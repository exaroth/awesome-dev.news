{"id":"Lx2uDWR7f1yr334","title":"Programming","displayTitle":"Programming","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":153,"items":[{"title":"🚀 Boost Your Resume Instantly – For FREE!","url":"https://dev.to/buildandcodewithraman/boost-your-resume-instantly-for-free-a83","date":1739774926,"author":"Ramandeep Singh","guid":1635,"unread":true,"content":"<p>Tired of bland resumes? I built a  that transforms your resume with ! ✨🚀<p>\nNow no need to pay any talent sites promising jobs in return of enhanced resumes.</p></p><p>✅ Upload your resume 📂 bullet points 🔥<p>\n✅ Download the improved version as a </p> 📄  </p><p>Built with , this tool ensures your resume stands out! No signups, no hassle – just instant upgrades.  </p><p>Give your resume the AI touch! 🚀💼 Let me know what you think! 👇</p>","contentLength":429,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomSolarize in PyTorch","url":"https://dev.to/hyperkai/randomsolarize-in-pytorch-5a4o","date":1739773690,"author":"Super Kai (Kazuya Ito)","guid":1634,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomSolarize.html\" rel=\"noopener noreferrer\">RandomSolarize()</a> can randomly solarize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type: or ). *All pixels equal or above this value are inverted.</li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is solarized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":419,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-Minute Daily AI News 2/16/2025","url":"https://www.reddit.com/r/artificial/comments/1ircvd9/oneminute_daily_ai_news_2162025/","date":1739771165,"author":"/u/Excellent-Target-847","guid":1675,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BrushinBella: Crafting a Digital Experience to Make Parents’ Lives Easier","url":"https://dev.to/maronzalez/brushinbella-crafting-a-digital-experience-to-make-parents-lives-easier-ab8","date":1739770827,"author":"Griffin Cole","guid":1618,"unread":true,"content":"<p>In today’s fast‑paced digital landscape, a brand’s online presence is not just a storefront—it’s an experience that speaks to its audience. BrushinBella, a company dedicated to providing innovative and thoughtfully designed baby and feeding products, recognized that their website needed to embody the values of care, creativity, and reliability. The goal was to create an engaging, easy‑to‑navigate e‑commerce platform that not only showcased their products but also helped parents seamlessly integrate these solutions into their busy lives.</p><p>This article recounts the comprehensive journey behind the creation of the <a href=\"https://www.brushinbella.com/\" rel=\"noopener noreferrer\">Brush in Bella</a> website, from the initial planning and design phases to the intricate development work that involved custom integrations using C++, Java, and Python. We will explore the challenges encountered along the way, the solutions that were implemented, and share expert insights on current trends and best practices in web development.</p><ol><li>The Vision and Planning Phase\n1.1. Defining the Brand and Project Scope\nBefore any code was written or designs drafted, the BrushinBella team—comprising marketing experts, product designers, UX specialists, and software engineers—came together to define the project’s vision. The core objective was clear: to create a digital experience that makes parents’ everyday life easier by offering a curated selection of products, valuable parenting content, and a user‑friendly interface.</li></ol><p>Key objectives defined during the planning phase included:</p><p>Brand Consistency: The website needed to mirror the brand’s values of quality, trust, and innovation.\nUser Experience: Given the target audience of busy parents, simplicity and intuitive navigation were paramount.<p>\nResponsive Design: With an increasing number of users accessing websites on mobile devices, the site had to offer a seamless experience across desktops, tablets, and smartphones.</p>\nCustom Integrations: Although the e‑commerce platform was built on a robust platform (Shopify was chosen for its reliability and scalability), the team identified specific areas—such as performance‑critical image processing, advanced inventory analytics, and bespoke order processing—where custom functionality was needed. This is where languages like C++, Java, and Python played a crucial role.<p>\nScalability and Performance: The system needed to handle fluctuating traffic levels and ensure quick load times, a challenge that required both architectural planning and performance‑oriented coding practices.</p>\nSecurity and Compliance: With sensitive customer data at stake, ensuring top‑notch security from the very start was non‑negotiable.<p>\n1.2. Assembling the Cross‑Functional Team</p>\nTo tackle this multifaceted project, BrushinBella assembled a cross‑functional team. Each team member brought unique expertise:</p><p>Project Managers and Business Analysts: To capture requirements, define deliverables, and ensure alignment with business goals.\nUX/UI Designers: Charged with creating an aesthetically pleasing and intuitive design that would resonate with the target audience.<p>\nFront‑End Developers: Specialists in HTML, CSS, and JavaScript who would turn design mockups into interactive, responsive web pages.</p>\nBack‑End Developers: Experts who would build custom modules, integrate third‑party APIs, and ensure that the site’s server‑side logic was robust and secure.<p>\nDevOps and QA Engineers: Responsible for establishing CI/CD pipelines, rigorous testing protocols, and ensuring smooth deployment and scaling.</p>\n1.3. Technology Selection and Architecture Decisions<p>\nDuring planning, the team conducted a thorough analysis of available technologies. Although the core e‑commerce solution was deployed on Shopify for its proven reliability and ease of use, certain functionalities required custom development. This resulted in a hybrid architecture:</p></p><p>Shopify as the Primary Platform: Managing the storefront, shopping cart, product catalog, and checkout process.\nCustom Back‑End Services: Developed in Java and Python, these services handled complex business logic, integration with third‑party systems (such as ERP and CRM platforms), and data analytics.<p>\nPerformance‑Critical Modules in C++: For tasks such as real‑time image processing (for product images and dynamic visual elements) and computationally intensive operations, C++ was chosen to ensure maximum speed and efficiency.</p>\nAPIs and Microservices: A RESTful API layer was established to enable seamless communication between Shopify and the custom modules. This approach allowed the system to scale horizontally and adopt a microservices architecture, which is increasingly considered best practice in modern web development.</p><ol><li>The Design Phase: From Concept to Wireframe\n2.1. User-Centric Design Philosophy\nA key part of BrushinBella’s vision was to make the website accessible not only to tech‑savvy users but also to non‑experts—busy parents who need a simple, straightforward interface. The UX/UI design phase was driven by several guiding principles:</li></ol><p>Simplicity: The design was stripped of any unnecessary complexity. Clear call‑to‑action buttons, uncluttered layouts, and intuitive navigation were prioritized.\nVisual Appeal: The website needed to evoke warmth and trust. Soft color palettes, playful yet professional typography, and high‑quality images of products and happy families helped achieve this.<p>\nResponsiveness: Mobile-first design principles were followed. Prototypes were tested on multiple devices and screen sizes to ensure consistency.</p>\nAccessibility: The design adhered to accessibility guidelines, ensuring that the website was usable by people with disabilities. This included proper contrast ratios, keyboard‑navigable menus, and alternative text for images.<p>\n2.2. Wireframing and Prototyping</p>\nUsing industry‑standard tools like Sketch and Figma, the design team created detailed wireframes and prototypes. These early models allowed stakeholders to visualize the website’s structure, layout, and user flow. Key features that were highlighted included:</p><p>Homepage: Featuring a dynamic banner that communicated the brand’s message (“Making Parents’ Everyday Life Easier!”) and a streamlined product navigation menu.\nProduct Pages: Each product page was designed to provide detailed images, descriptions, customer reviews, and easy‑to‑find purchasing options.<p>\nBlog and Content Sections: Recognizing that educational content is a valuable asset, the design included a blog section with articles, parenting tips, and video content.</p>\nCheckout Flow: A simplified, secure checkout process was paramount. Wireframes detailed the progression from shopping cart to payment gateway, with minimal friction.<p>\n2.3. Design Iterations and Stakeholder Feedback</p>\nThe iterative nature of design meant that prototypes were continuously refined based on stakeholder and user feedback. Early user testing sessions were conducted with focus groups of parents, ensuring that the designs were meeting real needs. Feedback led to several important adjustments:</p><p>Simplified Navigation: Initial designs with complex menus were streamlined to a single‑level navigation bar.\nEnhanced Product Imagery: High‑resolution images and a “zoom” feature were incorporated to allow parents to inspect product details closely.<p>\nClearer Calls-to‑Action: Buttons were redesigned for better visibility and prominence.</p>\nContent Accessibility: The blog section was reorganized to make content categories and search features more intuitive.</p><ol><li>The Development Phase: Building the Backbone\n3.1. Integrating Shopify with Custom Services\nOnce the design was finalized, the development phase kicked off. The first step was to integrate the robust capabilities of Shopify with the custom-built services developed in-house. Shopify managed the storefront and basic e‑commerce functionalities. However, several requirements demanded bespoke solutions:</li></ol><p>Custom Order Processing: While Shopify provided standard order management, BrushinBella needed an advanced system to integrate real‑time inventory data, promotional logic, and customized gift‑wrapping options. For this, the team built a microservice using Java.\nData Analytics and Reporting: In order to understand customer behavior and optimize the sales process, Python‑based analytics tools were developed. These tools processed data from Shopify’s API and generated actionable insights.<p>\nImage Processing and Dynamic Visuals: To ensure that product images were optimized for speed and quality, the team implemented a performance‑critical module in C++. This module handled tasks such as real‑time image resizing, format conversion, and dynamic compression.</p>\n3.2. The Role of C++: High‑Performance Modules<p>\nAlthough C++ is not traditionally associated with web development, its use in BrushinBella’s project was pivotal for performance‑critical tasks. Key functions implemented in C++ included:</p></p><p>Image Optimization Engine: C++ was used to build an engine that automatically resized and optimized images for various devices and screen resolutions. The engine was integrated as a microservice that communicated with Shopify through RESTful APIs.\nReal‑Time Data Processing: Certain operations, such as processing and rendering high‑resolution graphics for product galleries, were computationally intensive. C++’s efficiency ensured that these tasks did not slow down the user experience.<p>\nCustom Plugins: Some interactive features, such as a dynamic “gift‑wrapping” calculator that adjusted options based on user input, were built in C++ to ensure rapid response times and minimize latency.</p>\nUsing C++ required careful management of memory and thread safety. The development team leveraged modern C++ standards (C++17/20) and robust libraries such as Boost and OpenCV for image processing. This combination not only achieved the necessary performance gains but also ensured that the codebase was maintainable and scalable.</p><p>3.3. Java: The Enterprise Workhorse\nJava was chosen for its reliability, scalability, and strong ecosystem—qualities that made it ideal for handling core business logic and integrations. Within the BrushinBella project, Java served several key roles:</p><p>Business Logic and Order Management: The custom order processing system was developed in Java. This system interfaced with Shopify’s API to synchronize order data and applied complex business rules for promotions, discounts, and gift‑wrapping options.\nAPI Gateway: Java was also used to build a RESTful API gateway that served as the communication hub between Shopify and the custom microservices. The gateway ensured secure and efficient data exchange, handling tasks like authentication, rate‑limiting, and error logging.<p>\nIntegration with Legacy Systems: Many enterprise systems—such as ERP and CRM platforms—are built on or integrate well with Java. By choosing Java for these integrations, BrushinBella ensured that their website could interface smoothly with back‑office systems, enabling real‑time inventory management and customer data synchronization.</p>\nRobust Error Handling and Monitoring: Java’s mature ecosystem provided access to powerful tools for logging (using frameworks such as Log4j) and performance monitoring. This allowed the team to identify and resolve issues quickly, ensuring minimal downtime.<p>\nThe Java development team adopted best practices such as writing modular, test‑driven code and using containerization (with Docker) for deployment. This not only improved the reliability of the application but also made scaling the service more straightforward.</p></p><p>3.4. Python: Rapid Prototyping and Data Analytics\nPython’s reputation for ease of use and rapid development made it the language of choice for a range of supporting services:</p><p>Data Analytics: Python was used extensively for developing analytical tools that processed customer behavior data, order history, and product performance metrics. Libraries such as Pandas, NumPy, and Matplotlib were leveraged to generate detailed reports that informed marketing strategies and inventory decisions.\nAutomation Scripts: Routine tasks such as data backups, report generation, and system health checks were automated using Python scripts. This helped reduce manual intervention and allowed the team to focus on higher‑value activities.<p>\nIntegration and Testing: Python’s flexibility also made it an ideal candidate for writing integration tests. Automated testing frameworks like pytest ensured that the interactions between Shopify, the Java API gateway, and the C++ image optimization engine were reliable and robust.</p>\nMicroservices Development: In some cases, Python microservices were deployed to handle tasks that required rapid iteration and experimentation. For example, the team developed a prototype recommendation engine that used machine learning algorithms (with scikit‑learn) to suggest complementary products to customers based on their browsing history.<p>\nBy combining Python’s rapid prototyping capabilities with the stability of Java and the performance of C++, BrushinBella was able to build a hybrid system that leveraged the strengths of each language.</p></p><p>3.5. Adopting a Microservices Architecture\nOne of the most critical decisions during development was to adopt a microservices architecture. Instead of building a monolithic application, the system was divided into discrete services that communicated via RESTful APIs. This offered several benefits:</p><p>Scalability: Each service could be scaled independently based on demand. For example, the image processing engine in C++ could be scaled up during high‑traffic periods without affecting the rest of the system.\nResilience: Failures in one microservice did not bring down the entire website. Robust error‑handling and fallback mechanisms ensured that the website remained operational even if one component experienced issues.<p>\nFlexibility: The architecture allowed the team to update or replace individual services without redeploying the entire application. This was particularly beneficial when iterating on features such as the recommendation engine or order processing logic.</p>\nTechnology Diversity: By decoupling services, the team could choose the most appropriate language or framework for each task without forcing a one‑size‑fits‑all solution. This technological diversity, while challenging to manage, ultimately resulted in a more robust and efficient system.</p><ol><li>Testing, Deployment, and Optimization\n4.1. Rigorous Testing Strategies\nQuality assurance was embedded in every stage of development. The BrushinBella team implemented a multi‑layered testing strategy to ensure that each component—from the front‑end user interface to the backend microservices—functioned as expected:</li></ol><p>Unit Testing: Each module, whether written in Java, Python, or C++, underwent rigorous unit testing. For Java, JUnit was employed; Python modules were tested with pytest; and C++ components were validated using Google Test.\nIntegration Testing: Automated integration tests were established to verify the seamless communication between Shopify, the Java API gateway, the Python analytics services, and the C++ performance modules.<p>\nEnd‑to‑End Testing: Tools such as Selenium and Cypress were used to simulate real‑user interactions, ensuring that the entire system worked together harmoniously.</p>\nPerformance Testing: Given the emphasis on speed and scalability, performance testing was a critical focus. Load testing simulated high‑traffic scenarios to validate that the system could handle peak loads without significant degradation in response times.<p>\nSecurity Audits: Comprehensive security testing was conducted to safeguard against common vulnerabilities such as SQL injection, cross‑site scripting (XSS), and cross‑site request forgery (CSRF). Regular code audits and penetration testing further ensured that customer data remained protected.</p>\n4.2. Continuous Integration and Deployment (CI/CD)<p>\nTo streamline the development process and ensure rapid delivery of updates, the team set up a robust CI/CD pipeline. Key components of the pipeline included:</p></p><p>Automated Builds: Every code commit triggered an automated build process that compiled the Java and C++ modules and ran the Python scripts. This ensured that errors were caught early.\nAutomated Testing: The CI system executed the full suite of tests for each build, including unit, integration, and end‑to‑end tests. Only builds that passed all tests were promoted to staging.<p>\nContainerization: Using Docker, each microservice was containerized. This ensured consistency across development, testing, and production environments and simplified the scaling process.</p>\nOrchestration: Kubernetes was employed to manage the containerized services, enabling automatic scaling, load balancing, and self‑healing capabilities.<p>\nMonitoring and Logging: Post‑deployment, comprehensive monitoring tools (such as Prometheus and Grafana) were used to track system performance, while logging frameworks (e.g., Logstash and ELK stack) helped in quickly diagnosing and resolving issues.</p>\n4.3. Performance Optimization<p>\nOptimization was an ongoing effort throughout development. Key performance enhancements included:</p></p><p>Caching Strategies: To reduce load times and database queries, caching mechanisms were implemented. This included front‑end caching using service workers for static assets and back‑end caching with Redis for frequently accessed data.\nEfficient API Design: The RESTful APIs were optimized for speed, ensuring that data was transmitted in lightweight JSON formats with minimal overhead.<p>\nCode Profiling and Optimization: Regular profiling sessions identified bottlenecks in the code. For instance, the C++ image processing engine was fine‑tuned to minimize latency, and the Java microservices were optimized to handle high volumes of concurrent requests.</p>\nResponsive and Adaptive Design: The front‑end was optimized for mobile devices by adopting a mobile‑first design philosophy, ensuring fast load times even on slower networks.<p>\nDatabase Optimization: Indexing, query optimization, and efficient data structuring in both relational and NoSQL databases helped maintain rapid data retrieval and update speeds.</p></p><ol><li>Overcoming Challenges and Implementing Solutions\nNo major web development project is without its hurdles. The creation of the BrushinBella website presented a variety of challenges, each of which was met with innovative solutions.</li></ol><p>5.1. Integration Complexity\nChallenge:<p>\nIntegrating a Shopify‑based e‑commerce platform with custom microservices written in Java, Python, and C++ proved complex. Each system had its own data formats, security protocols, and performance characteristics.</p></p><p>Solution:\nThe team implemented a robust API gateway in Java to serve as the central communication hub. This gateway standardized data formats (using JSON), handled authentication and authorization, and ensured that all services communicated seamlessly. Extensive use of RESTful API design principles and well‑documented endpoints minimized integration friction.</p><p>5.2. Performance Bottlenecks\nChallenge:<p>\nCertain operations—especially image processing and real‑time analytics—posed performance challenges, risking slow response times that could frustrate users.</p></p><p>Solution:\nCritical performance‑intensive tasks were offloaded to specialized microservices developed in C++. These modules were optimized using modern C++ standards, multithreading, and libraries like OpenCV to ensure that image processing was done swiftly. Additionally, Python’s role in handling analytics was enhanced by leveraging efficient data processing libraries and asynchronous programming techniques to reduce latency.</p><p>5.3. Maintaining a Consistent User Experience\nChallenge:<p>\nEnsuring a seamless, high‑quality user experience across multiple devices and browsers is always challenging—especially when integrating third‑party services and custom code.</p></p><p>Solution:\nThe design team adopted a mobile‑first approach and followed responsive design principles to ensure that the website adapted gracefully to different screen sizes. Rigorous cross‑browser testing, combined with adaptive UI frameworks, ensured consistency. Accessibility guidelines were also adhered to, making the site usable by a diverse audience, including those with disabilities.</p><p>5.4. Ensuring Security and Data Integrity\nChallenge:<p>\nWith customer data and payment information at stake, any security lapse could have dire consequences. The heterogeneous nature of the system, with multiple programming languages and platforms interacting, introduced several potential vulnerabilities.</p></p><p>Solution:\nSecurity was integrated from day one. The team implemented robust authentication mechanisms at the API gateway level and encrypted all sensitive data. Regular security audits, automated vulnerability scans, and adherence to best practices—such as input validation, sanitization, and the use of secure coding frameworks—helped safeguard the system. Additionally, deploying a web application firewall (WAF) and implementing SSL/TLS across all endpoints further ensured data integrity.</p><p>5.5. Managing a Diverse Technology Stack\nChallenge:<p>\nCombining multiple programming languages and frameworks increases the complexity of the codebase and the development process. Ensuring that team members could collaborate effectively across different languages was a significant challenge.</p></p><p>Solution:\nThe project adopted a microservices architecture, which naturally decoupled the different language‑specific modules. Clear documentation, code conventions, and regular cross‑team meetings ensured that everyone was on the same page. The use of containerization (with Docker) and orchestration (with Kubernetes) allowed developers to work on isolated services without interference. In addition, investing in integrated development environments (IDEs) and code review tools helped maintain code quality and consistency across the diverse stack.</p><ol><li>Expert Insights on Web Development Trends and Best Practices\nAs BrushinBella’s website evolved from concept to a fully‑functional, high‑performance digital platform, the team kept a close eye on emerging trends and industry best practices. Here are some expert insights gleaned during the project:</li></ol><p>6.1. Embracing Microservices and Containerization\nModern web development is increasingly moving away from monolithic architectures toward microservices. This approach provides several advantages:</p><p>Scalability: Each service can be scaled independently to meet demand.\nResilience: Failures in one service do not affect the entire system.<p>\nFlexibility: Developers can choose the best technology for each service without being locked into a single framework or language.</p>\nContainerization tools like Docker and orchestration platforms like Kubernetes have become essential. They enable rapid deployment, efficient resource management, and simplified scaling. For BrushinBella, this meant that the custom C++, Java, and Python services could be managed and updated independently, leading to a more resilient and adaptable platform.</p><p>6.2. The Role of Hybrid Technology Stacks\nWhile many modern websites rely heavily on JavaScript frameworks for the front‑end (such as React or Vue), integrating a hybrid technology stack can yield significant benefits. Each language and framework brings its own strengths:</p><p>C++ for Performance‑Critical Tasks: In performance‑sensitive areas such as image processing, the efficiency of C++ is unmatched.\nJava for Robust Enterprise‑Grade Logic: Java’s strong typing, mature ecosystem, and scalability make it ideal for handling complex business rules and integrations.<p>\nPython for Rapid Development and Data Analysis: Python’s ease of use and powerful libraries allow teams to quickly prototype and deploy data‑driven features.</p>\nThis multi‑language approach is becoming more common as companies seek to optimize for both performance and development speed. It is essential, however, to manage this diversity with clear interfaces, robust API designs, and comprehensive documentation.</p><p>6.3. User Experience and Accessibility as Top Priorities\nNo matter how powerful the backend or how sophisticated the custom integrations, the success of a website ultimately depends on the user experience. Best practices dictate that designers and developers should focus on:</p><p>Responsive Design: Ensuring that the website looks and functions well on all devices.\nAccessibility: Building websites that are usable by everyone, including those with disabilities.<p>\nIntuitive Navigation: Simplifying the user journey from landing on the page to completing a transaction.</p>\nVisual Appeal: Using high‑quality images, consistent branding, and engaging interactive elements to create a memorable user experience.<p>\nFor BrushinBella, rigorous user testing and iterative design refinements ensured that the website not only met functional requirements but also delighted its target audience.</p></p><p>6.4. Security in a Connected World\nSecurity remains one of the most critical aspects of web development. With increasing data breaches and cyberattacks, best practices include:</p><p>Encryption of Data: Both in transit (using SSL/TLS) and at rest.\nRegular Security Audits: Automated vulnerability scanning and penetration testing to identify and mitigate risks.<p>\nRobust Authentication: Using multi‑factor authentication (MFA) and secure API gateways.</p>\nInput Validation and Sanitization: To prevent common attacks such as SQL injection and cross‑site scripting (XSS).<p>\nImplementing these security measures is not a one‑time task but an ongoing process that requires constant vigilance.</p></p><p>6.5. The Future: Serverless, AI, and Progressive Web Apps\nLooking ahead, several trends are shaping the future of web development:</p><p>Serverless Architectures: Platforms such as AWS Lambda, Google Cloud Functions, and Azure Functions are enabling developers to run code without managing servers. This can reduce costs and simplify deployment for certain types of applications.\nArtificial Intelligence and Machine Learning: Integrating AI into web applications is becoming increasingly common. Whether for personalized recommendations, chatbots, or automated analytics, AI can dramatically enhance the user experience.<p>\nProgressive Web Apps (PWAs): PWAs combine the best features of web and mobile applications, offering offline functionality, push notifications, and fast load times. They represent the future of delivering seamless, app‑like experiences through the browser.</p>\nFor BrushinBella, these trends offer opportunities for future enhancements. For example, a serverless recommendation engine or AI‑driven customer support chatbot could further enrich the user experience.</p><ol><li>Reflections and Lessons Learned\n7.1. Collaboration Is Key\nOne of the most important lessons from the BrushinBella project was the power of cross‑functional collaboration. Bringing together designers, developers, business analysts, and marketing experts enabled the team to view the project from multiple perspectives. This collaborative approach ensured that the final product was not only technically sound but also aligned with the brand’s vision and customer needs.</li></ol><p>7.2. Flexibility in Technology Choices\nAdopting a hybrid technology stack may seem daunting at first, but it can yield enormous benefits when managed properly. By leveraging the strengths of C++, Java, and Python in different parts of the system, the team was able to optimize for performance, scalability, and rapid development. This flexibility allowed BrushinBella to build a robust platform that could evolve with changing requirements.</p><p>7.3. Iteration and Continuous Improvement\nThe project was not built in a single, linear pass. Instead, it was an iterative process where feedback was continuously gathered and incorporated. From early wireframes to final deployment, each iteration brought improvements and refinements. The use of CI/CD pipelines and automated testing ensured that each update maintained the high standards of quality required for a live e‑commerce site.</p><p>7.4. Balancing Innovation and Practicality\nWhile it was tempting to incorporate cutting‑edge technologies and ambitious features, the team also had to remain practical. Decisions were driven by both innovative ideas and real‑world constraints such as budget, timelines, and technical feasibility. This balance ensured that the website was not only modern and attractive but also reliable and maintainable over the long term.</p><ol><li>Conclusion: A Modern Web Experience for Today’s Parents\nThe creation of the BrushinBella website is a testament to what can be achieved when visionary design meets technical excellence. By thoughtfully planning each phase, embracing a hybrid development strategy, and leveraging the unique strengths of C++, Java, and Python, the team built a website that not only drives sales but also resonates with its audience.</li></ol><p>Holistic Planning: Successful projects begin with a clear vision and a well‑defined scope. Every stakeholder’s input is valuable in shaping a product that meets both business and user needs.\nUser‑Centered Design: Prioritizing the user experience—through responsive design, intuitive navigation, and accessibility—ensures that the website remains relevant and engaging.<p>\nTechnological Synergy: Using a blend of languages and frameworks allows teams to optimize different aspects of the application. C++ provided high‑performance modules, Java ensured robust enterprise‑grade processing, and Python accelerated data analytics and automation.</p>\nResilient Architecture: Adopting microservices and containerization enabled the system to scale, adapt, and remain resilient in the face of increasing demand.<p>\nOngoing Evolution: The journey does not end at launch. Continuous testing, monitoring, and optimization are crucial to keeping a website secure, fast, and responsive to changing user expectations.</p>\nAs BrushinBella continues to grow and innovate, the lessons learned from this project will serve as a roadmap for future enhancements. With a focus on emerging trends such as serverless architectures, AI integrations, and progressive web apps, the company is well‑positioned to adapt to the evolving digital landscape and maintain its commitment to making parents’ everyday life easier.</p><p>In a world where the digital experience often makes or breaks a brand, BrushinBella’s website stands as an exemplar of what thoughtful planning, creative design, and technical excellence can achieve together. Whether you’re an expert in web development or a newcomer trying to understand the complexities behind a modern e‑commerce platform, the BrushinBella story offers valuable insights into building a system that is both powerful and personable.</p><p>Expert Perspectives and Future Outlook\nIndustry experts agree that the integration of multiple programming paradigms is the future of web development. Leaders in the field emphasize the importance of:</p><p>Adopting Modular Architectures: Breaking down applications into microservices not only improves scalability but also enhances maintainability.\nInvesting in Performance Optimization: As user expectations continue to rise, ensuring fast load times and seamless interactions will remain a top priority.<p>\nFostering Interdisciplinary Collaboration: The most innovative projects arise when cross‑functional teams work together, blending design, technology, and business acumen.</p>\nEmbracing Continuous Learning: With technologies evolving at a rapid pace, staying updated with the latest tools, frameworks, and best practices is essential for success.<p>\nLooking forward, trends such as the integration of AI in personalization, the adoption of serverless computing to reduce operational overhead, and the rise of progressive web apps are expected to shape the future of web development. BrushinBella is already exploring these avenues, planning to integrate AI‑driven customer insights and further optimize the platform using serverless components.</p></p><p>Final Thoughts\nThe journey of building the BrushinBella website highlights the dynamic and ever‑evolving nature of web development. By combining the proven capabilities of established platforms like Shopify with custom‑developed microservices in C++, Java, and Python, the team created a system that is greater than the sum of its parts. This approach not only met immediate business needs but also laid a robust foundation for future growth and innovation.</p><p>For developers and business leaders alike, the BrushinBella project is a compelling case study in the effective melding of design, technology, and user‑centric strategy. It demonstrates that with careful planning, strategic technology selection, and relentless focus on the user experience, it is possible to create a digital platform that truly makes a difference.</p><p>Whether you are planning your next web development project or simply interested in learning how modern e‑commerce platforms are built, the BrushinBella story offers a wealth of insights. As technology continues to evolve, so too will the tools and methods used to create these digital experiences. The key is to remain agile, to embrace change, and to always put the user first.</p><p>In summary, the BrushinBella website is more than just an online store—it is an embodiment of a brand’s promise to simplify and enrich the lives of parents. It stands as a reminder that at the heart of every great digital experience is a commitment to quality, innovation, and user empowerment.</p>","contentLength":33591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complete Guide to Virtual Environments (Virtualenv) in Python","url":"https://dev.to/mrnik/complete-guide-to-virtual-environments-virtualenv-in-python-3jn1","date":1739757746,"author":"Mahdi Ahmadi","guid":871,"unread":true,"content":"<h2>\n  \n  \n  1. What is a Virtual Environment in Python?\n</h2><p>When developing multiple projects with Python, each project may require different verzsions of libraries. This is where Virtual Environment (Virtualenv) comes to the rescue!</p><blockquote><p>A virtual environment is an isolated space for installing libraries and packages for a specific project without affecting your main system.</p></blockquote><h2>\n  \n  \n  2. Why Should You Use Virtualenv?\n</h2><ul><li><p>Avoid version conflicts: If different projects require different versions of the same library, conflicts may arise without a virtual environment.</p></li><li><p>Project isolation: Each project has its own set of dependencies, ensuring stability.</p></li><li><p>Portability: You can easily recreate the project environment on another system using a requirements.txt file.</p></li><li><p>Increased security: Installing packages in an isolated environment prevents unintended changes to system files.</p></li></ul><h2>\n  \n  \n  3. Installing and Using Virtualenv\n</h2><ul><li>Installing Virtualenv on Windows, Linux, and macOS</li></ul><p>If Virtualenv is not already installed, you can install it using the following command:</p><ul><li>Creating a Virtual Environment</li></ul><p>To create a virtual environment in your project directory, run:</p><p><em>venv is the name of the folder where the virtual environment will be created. You can use any name you prefer.</em></p><ul><li>Activating the Virtual Environment</li></ul><p>The activation process depends on your operating system:\nOn Windows (CMD or PowerShell):</p><div><pre><code>venv\\Scripts\\Activate.ps1\n</code></pre></div><p>Once activated, you will see the virtual environment name in the terminal prompt:</p><ul><li>Installing Packages in the Virtual Environment</li></ul><p>After activation, you can install project dependencies using:</p><ul><li>Deactivating the Virtual Environment</li></ul><p>To deactivate the virtual environment, simply run:</p><h2>\n  \n  \n  4. Saving and Recreating the Virtual Environment with </h2><p>To save the list of installed packages in the virtual environment, use:</p><div><pre><code>pip freeze &gt; requirements.txt\n</code></pre></div><p>To recreate the same environment on another system:</p><div><pre><code>pip install -r requirements.txt\n</code></pre></div><ul><li><p>Virtualenv helps you run Python projects in an isolated and conflict-free manner.</p></li><li><p>You can install it with pip install virtualenv.</p></li><li><p>Create and activate a virtual environment with venv.</p></li><li><p>Use requirements.txt to store and restore dependencies.</p></li></ul><p>Thanks for reading❤️\nI hope this guide helps you understand and use virtual environments effectively. If you have any questions or suggestions, feel free to leave a comment!</p>","contentLength":2322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomPosterize in PyTorch","url":"https://dev.to/hyperkai/randomposterize-in-pytorch-40ac","date":1739757574,"author":"Super Kai (Kazuya Ito)","guid":870,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomPosterize.html\" rel=\"noopener noreferrer\">RandomPosterize()</a> can randomly posterize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type:):\n*Memos:\n\n<ul><li>It's the number of bits to keep for each channel.</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is posterized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomInvert in PyTorch","url":"https://dev.to/hyperkai/randominvert-in-pytorch-4e0o","date":1739757459,"author":"Super Kai (Kazuya Ito)","guid":869,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is inverted or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What to do if the selenium crawler is detected?","url":"https://dev.to/98ip/what-to-do-if-the-selenium-crawler-is-detected-4o0f","date":1739757045,"author":"98IP 代理","guid":868,"unread":true,"content":"<p>When using Selenium for automated web crawling, it is often detected and blocked by the target website. This is usually because Selenium's automation features are more obvious and can be easily identified by the website's anti-crawler mechanism. This article will explore in depth how to deal with the problem of Selenium crawler being detected, including methods such as hiding automation features and using proxy IPs, and provide specific code examples. At the same time, 98IP proxy will be briefly mentioned as one of the solutions.</p><h2>\n  \n  \n  I. Reasons for Selenium crawlers being detected\n</h2><h3>\n  \n  \n  1.1 Obvious automation features\n</h3><p>Selenium's default browser behavior is significantly different from manual user operations, such as specific fields in the request header, fixed browser window size, uniform operation speed, etc., which may be used by websites to identify automated scripts.</p><h3>\n  \n  \n  1.2 Frequent request frequency\n</h3><p>Crawlers usually send requests at a frequency much higher than normal users, which can also easily alert websites.</p><p>If the crawler always sends requests from the same IP address, the IP address will soon be blacklisted by the website.</p><h2>\n  \n  \n  II. Strategies for dealing with Selenium crawler detection\n</h2><h3>\n  \n  \n  2.1 Hide automation features\n</h3><h4>\n  \n  \n  2.1.1 Modify request headers\n</h4><p>Through Selenium's <code>webdriver.ChromeOptions()</code> configuration, you can modify the browser's request header to make it closer to normal user requests.</p><div><pre><code></code></pre></div><h4>\n  \n  \n  2.1.2 Randomize browser settings\n</h4><p>Use libraries such as  to automatically manage browser drivers and randomize window size, scrolling behavior, etc. to simulate real user operations.</p><div><pre><code></code></pre></div><p>Sending requests through <a href=\"https://en.98ip.com/\" rel=\"noopener noreferrer\">proxy IPs</a> can effectively avoid the problem of IP being blocked. High-quality proxy services such as 98IP Proxy provide stable and anonymous IP resources, which is an effective means of dealing with Selenium crawlers being detected.</p><div><pre><code></code></pre></div><p> The above code uses the  library instead of  because  provides more flexible proxy configuration and request interception functions. If you haven't installed  yet, you can install it through .</p><h3>\n  \n  \n  2.3 Controlling request frequency\n</h3><p>By introducing random delays and setting reasonable request intervals, the request frequency of the Selenium crawler can be controlled to make it closer to the browsing behavior of normal users.</p><div><pre><code></code></pre></div><p>It is a common problem for Selenium crawlers to be detected, but by hiding automation features, using proxy IPs, controlling request frequency, etc., we can effectively reduce the risk of being detected. In particular, using high-quality proxy services such as 98IP Proxy can significantly improve the stability and success rate of crawlers.</p><p>In the future, with the continuous advancement of website anti-crawler technology, we also need to continuously update and improve crawler strategies. For example, introducing more complex browser simulation technology, using machine learning to predict and circumvent blocking strategies, etc. are all directions worth exploring.</p><p>In short, dealing with the problem of Selenium crawlers being detected requires comprehensive consideration of multiple factors and taking corresponding measures.</p>","contentLength":3162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How arch-delta works and saves bandwidth for Arch Linux upgrades","url":"https://djugei.github.io/how-arch-delta-works/","date":1739753393,"author":"/u/djugei","guid":1656,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ir7hhp/how_archdelta_works_and_saves_bandwidth_for_arch/"},{"title":"Writing Pythonic Code With Python Data Model","url":"https://dev.to/noble47/writing-pythonic-code-with-python-data-model-2j3o","date":1739751292,"author":"Noble-47","guid":847,"unread":true,"content":"<blockquote><p><em>This apparent oddity is the tip of an iceberg that, when properly understood, is the key to everything we call Pythonic. The iceberg is called the Python data model, and it describes the API that you can use to make your own objects play well with the most idiomatic language features.</em> - Luciano Ramalho (Fluent Python: clear, concise, and effective programming)</p></blockquote><p>What's so special about the Python data model one may ask. Rather than giving a personal answer, why don't we do a little dive in and see what we can accomplish by understanding the data model. Data model simply speaks about how data is represented. In Python, data is represented by objects or to sound a bit technical, objects are Python's abstraction for data. The data model provides us with an API that allows our objects to play well with the 'under the hood' of Python programming.</p><p>In our little dive into Python data model, we are going to specifically focus on the special methods. Special methods are class functions with special names that are invoked by special syntax. Defining these special methods in our class definitions can give our class instances some really cool Python powers like iteration, operator overloading, working well with context managers (the 'with' keyword), proper string representation and formatting, and many more. To show you how you could implement these special functions into your classes, we will consider two examples of situations where using these special functions would make our codes clearer and more Pythonic.</p><p><em>The first example is a little bit outside-the-box solution I came up with for creating a simple game of Rock-Paper-Scissors in Python and the second is going to be a bit mathematical in nature but I'm going to walk you through each line of code</em></p><h3><strong>A Simple Game Of Rock Paper Scissors</strong></h3><p>Just in case you are not familiar with the Rock-Paper-Scissors game, it is originally a hand game usually played among two people that involves making signs of either Rock or paper or scissors. Knowing the whole history of the game doesn't really matter what is important is knowing how to determine the winner. In a conventional setting, a hand sign of rock would always win against scissors but will lose against paper, a hand sign of scissors would win against paper and lose to rock and obviously, paper would lose to scissors and win against rock. we can summarize this as shown below</p><p>For our Python emulation of this game, we will limit the number of players to just two, one player would be the computer and the other would be the user. Also, this is not a machine learning article or a write-up about computer vision, our users would still have to type in an option between rock, paper, and scissors on the terminal for our program to work.\nBefore we go into the actual coding, it's good that we take a step back and consider how we want our Python script to be. For my solution to this challenge, I will use the random module to enable the computer select a random option of either rock, paper, or scissors. To implement how our code evaluates the winner, I'm going to make the following assumptions:</p><p>I'm also going to take an OOP approach; our rock, paper, and scissors will be treated as objects and not string variables. Rather than creating three separate classes for each, I'll create only one that can represent any of them. This approach would also allow me to show you how special methods make life easier. Now to the fun aspect!</p><p>Naming our class RPS may sound a bit odd, but I found the name 'RPS' to be a good fit cause each letter comes from the initials, R for Rock, P for Paper, and S for Scissors. What's important to note here is that creating an instance of our class requires two arguments: pick and name. We already stated that the users of our script would have to type in their selected option on the terminal, instead of making our users type in 'Paper' (which could be so stressful for them) why don't we just allow our user to type in 'P' (or 'p') to select 'Paper', that's what the pick stands for. The name property is the actual name e.g 'Paper'. So now that we know what each parameters is for, we can now inspect our class by creating an instance</p><div><pre><code>&gt;&gt;&gt; p = RPS('P', 'Paper') # create an instance\n&gt;&gt;&gt; p.name\n# return : Paper\n&gt;&gt;&gt; p.pick\n# return : P\n&gt;&gt;&gt; print(p)\n# return : &lt;__main__.RPS object at 0x...&gt;\n</code></pre></div><p>Our class instance was created and has the right attributes but notice what we get when we try to print the contents of the variable holding our class instance. Before getting into the technical details of how our class instance returns the odd-looking string, let's update our class definition by adding a single special function and see the difference.</p><p>Now let's create an instance and try printing our class instance again</p><div><pre><code>&gt;&gt;&gt; p = RPS('P', 'Paper')\n&gt;&gt;&gt; print(p)\n# return : RPS(P, Paper)\n</code></pre></div><p>As we can see, by defining the '' method we can achieve a better looking result. Let's make one more change to our class definition.</p><p>Now let's create an instance and test it again.</p><div><pre><code>&gt;&gt;&gt; p = RPS('P', 'Paper')\n&gt;&gt;&gt; p\n# return : RPS(P, Paper)\n&gt;&gt;&gt; print(p)\n# return : Paper\n&gt;&gt;&gt; str(p)\n# return : 'Paper'\n&gt;&gt;&gt; repr(p)\n# return : 'RPS(P, Paper)'\n</code></pre></div><p>To know what's going on here, we need to know a little about the print function. The print function converts all non-keyword arguments(like our p variable) to string using the built-in Python class . If calling  on our variable fails, python falls back on the built-in  function. When  is called on our object, it looks for a  method, if it finds none, it fails and then searches for a  method. Both the  and the  methods are special methods used for string representation of our object. The  method gives the official string representation of our object while the  method gives a friendly string representation of our object. I usually say that the  method is like talking to another developer and it usually shows how to call our class and the  is like talking to a user of our program (like the player in this case), you would usually just want to return a simple string like \"Paper\" to show the user what he picked.</p><p>Although I stated the  and  as the two special functions in our class definition, there's actually a third special method, and yes it is the most common one, the  function. It is used for initializing our class and called by the  special method just before returning our class instance. Did I just mention another special method we haven't defined? yes, I did. It may also interest you to know that Python automatically adds some other special methods to our class. You can check them out by calling the built-in function  on our class instance like this</p><div><pre><code>&gt;&gt;&gt; dir(p)\n# returns : ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'name', 'pick']\n</code></pre></div><p>Special functions or methods can be identified by the way they are named, they always begin with double underscores '<strong>' and end with double underscores '</strong>' Because of this special way of naming these methods, they are commonly called daunder methods (Double + UNDERscores = DUNDER). So if a method name begins with double underscores, it is most likely but not certainly a special method. Why not certainly? this is simply because Python does not stop us from defining our own methods using the dunder syntax. Alright back to our game script.</p><p>All that's left for us now is for us to let our script know how to determine a winner. As stated earlier, I will use comparison to evaluate a winner.</p><div><pre><code># comparison logic\nRock &gt; Scissors\nScissors &gt; Paper\nPaper &gt; Rock\n</code></pre></div><p>To implement this solution, I will add a dictionary and use the daunder greater_than method. The dictionary key would be the initials of Rock, Paper and Scissors. The value of each key would be the only other element that the key can defeat.</p><p>Notice the new lines of code, first the options dictionary and then the  method definition. With these new lines of code, let's see what new functionality our code now has.</p><div><pre><code># create a rock instance\n&gt;&gt;&gt; r = RPS('R', 'Rock')\n\n# create a paper instance\n&gt;&gt;&gt; p = RPS('P', 'Paper')\n\n# create a scissors instance\n&gt;&gt;&gt; s = RPS('S', 'Scissors')\n\n&gt;&gt;&gt; print(r,p,s)\n# return : Rock Paper Scissors\n\n&gt;&gt;&gt; p &gt; r # paper wins against rock\n# return : True\n\n&gt;&gt;&gt; r &gt; s # rock wins against scissors\n# return : True\n\n&gt;&gt;&gt; s &gt; p # scissors wins against paper\n# return : True\n\n&gt;&gt;&gt; p &lt; s # paper lose to scissors\n# return : True\n\n&gt;&gt;&gt; p &lt; r # paper lose to rock\n# return : False\n\n&gt;&gt;&gt; p &lt; s &lt; r# paper lose to scissors which lose to rock\n# return : True\n\n&gt;&gt;&gt; p &gt;= r paper wins or tie to rock\n# return : Traceback (most recent call last): \n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt; \nTypeError: '&gt;=' not supported between instances of 'RPS' and 'RPS'\n</code></pre></div><p>Just by adding to  special method, our class instances have gained magical powers (daunder methods are sometimes called magic methods and we can see why). By implementing the daunder gt method, our class instance now relates well with the  and  symbols but not the  and  symbols. The reason is that  is just the negation of . The special method for  is  which can just be the negation of calling .</p><p>For the ≥ symbol, its special method is the  and it must be defined for our object to work well with the  sign. But in this program, we can do without it. </p><p>Another missing piece would be to check if two separate instances of Paper are equal.</p><div><pre><code>&gt;&gt;&gt; p1 = RPS(\"P\", \"Paper\")\n&gt;&gt;&gt; p2 = RPS(\"P\", \"Paper\")\n&gt;&gt;&gt; p3 = p1\n&gt;&gt;&gt; p1 == p2\n# return : False\n\n&gt;&gt;&gt; p1 == p3\n# return : True\n\n&gt;&gt;&gt; id(p1)\n# return : 140197926465008\n\n&gt;&gt;&gt; id(p2)\n# return : 140197925989440\n\n&gt;&gt;&gt; id(p3)\n# return : 140197926465008\n\n&gt;&gt;&gt; id(p1) == id(p3)\n# return : True\n\n&gt;&gt;&gt; id(p2) == id(p1)\n# return False\n</code></pre></div><p>The default operation of the equal comparison sign is to compare the id of the object. p1 and p2 are different class instances that happen to have the same attributes but their id differs and therefore are not equal. When we assign a variable to a class instance, we make that variable point to the address of the instance which is what we observe for p3 which has the same id as p1. We have the option of overriding how the equality comparison works on our object by defining and implementing our own  method. But for this script, I will compare two instances using their pick attribute. Now that we have defined our class and know how it works, we are now ready to see the full implementation of the Python script</p><p>Let me walk you through the code. We are already familiar with the RPS class definition. If you recall, our code is meant to allow the computer to select choices at random and that's what the random module is for. The random module makes available the  function which allows the 'random' selection of an element from an iterable object e.g. a list in Python. The list in this case is the . Because our class is made to work with uppercase letters for comparison, it is necessary that we always initialize our objects with uppercase for the pick attribute. This is why we first convert the user's input to upper case (line 34) with the&nbsp;. It is also possible that our user types in an unexpected character like 'Q' so we have to validate our user input by checking if the uppercase character is part of the valid options in . The mapping dictionary allows us to quickly convert the user's input to a corresponding instance of RPS after being validated. The evaluate_winner function makes use of the comparison symbol to determine the winner. Because we want the code to run in a loop until a winner is found, we make use of a while loop and when a winner is found, the evaluate_winner function returns True which will then break the loop and exit the game.</p><p>Here is one of the various results of running the code</p><p>Our Python code runs as expected, although there could be a couple of improvements or new features to add. The most important thing is that we see how using special methods in our class definition gives our code a more Pythonic feel. Assuming we were to take a different approach such as using nested if statements, our evaluate_winner method would look something like this</p><div><pre><code>def evaluate_winner(user_choice, comp_choice):\n    # check if user choice is 'R'\n    if user_choice == 'R':\n        # check if comp_choice is 'R'\n        if comp_choice == 'R':\n            # it is a tie\n            ...\n        elif comp_choice == 'S':\n            # user wins\n            ...\n        else:\n          # computer wins\n          ...\n    if ... \n     # do the same for when user_choice is 'S' and then for\n     # when user_choice is 'P'\n</code></pre></div><p>A problem with this approach other than the lengthy code is that if we desire to add a new element, diamond which can beat both rock and scissors but not paper (for an unknown reason), our if statements would begin to look really awkward. Whereas in our OOP approach, all we have to do is to modify the options dict like so</p><div><pre><code>options = {\"R\" : [\"S\"], \"P\" : [\"R\"], \"S\" : [\"P\"], \"D\" : [\"R\", \"S\"]}\n</code></pre></div><p>and then we change the if statement in  to be</p><div><pre><code>def __gt__(self,x):    \n    if x.pick in self.options[self.pick]:\n        return True\n    else:\n        return False\n</code></pre></div><p>we can make the statement shorter</p><div><pre><code>def __gt__(self, x):\n   return True if x.pick in self.options[self.pick] else False\n</code></pre></div><p>To conclude, here are some things you should note about using special methods:</p><ul><li><p>You hardly (or never) call them directly yourself, let Python do the calling for you</p></li><li><p>When defining functions that use the dunder naming syntax, you should consider that Python could one day define such a function and give it a different meaning. This could break your code or make it behave in unexpected ways</p></li><li><p>You certainly don't have to implement every special method there is. Just a couple that you are really sure you need. Remember, simple is better than complex. If there's a simpler way you should use that instead</p></li></ul><p>This is the first part of the topic, in the next part, we are going to be dealing with operator overloading and making iterable objects</p><p>Hope you enjoyed this article!!!</p>","contentLength":14165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"There is no 1875 epoch","url":"https://iter.ca/post/1875-epoch/","date":1739746489,"author":"/u/AlanBennet29","guid":1678,"unread":true,"content":"<p>A US government official said “we’ve got people in there that are about 150 years old” claiming social security benefits.</p><p>Some people have claimed that the reason for this is that the Social Security Administration (SSA) uses an epoch in 1875 for storing dates, and these are just people with unknown years of birth stored as 0. I think the origin of these claims is <a href=\"https://x.com/toshiHQ/status/1889928670887739902\">this post</a>:</p><blockquote><p>It looks like Elon’s genius coders don’t know how COBOL works.</p><p>Social security runs on COBOL, which does not use a date or time type.  So the date is stored as a number using the ISO 8601 standard.  The epoch for this is 150 years ago (1875) - aka the metre standard.</p><p>So if you don’t know the date of something, it will be a 0 value, which in COBOL will default to 1875 - 150 years ago.</p></blockquote><p>I don’t think this is true, for a few reasons.</p><h2>The database has years of birth before 1875</h2><p>In 2007 the SSA <a href=\"https://www.ssa.gov/policy/docs/ssb/v71n4/v71n4p33.html\">released a dataset</a> “containing earnings records for individuals drawn from a 1-percent sample of all Social Security numbers (SSNs) issued before January 2007”. They wrote:</p><blockquote><p>The final adjustments included removing 5,935 individuals whose [Year Of Birth] value was before 1870, removing 1,096 individuals whose YOB value was equal to 2007, and removing 4 individuals who were assigned a missing YOB value. Individuals born before 1870 were removed because they were unlikely to have received Social Security benefits.</p></blockquote><p>They explictly say they have records of individuals born in 1869 and earlier, and that they can represent missing birth years!</p><h2>There is no spike of births in 1875</h2><p>There is no spike in births in 1875 in that dataset, which you would expect if some process was setting unknown births to 1875:</p><p>The dataset is a 1% sample, so the actual amounts are ~100x larger.</p><h2>The SSA doesn’t use ISO 8601</h2><p>The Master Beneficiary Record, which tracks social security benefits payments, <a href=\"https://ajph.aphapublications.org/doi/pdf/10.2105/AJPH.73.11.1270\">was created in 1962</a> -  ISO 8601 was first published in 1988. The predecessor to that standard, ISO 2016 was published in 1976 - still too early, and also it has no reference any date in 1875.</p><p><a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC1651155/pdf/amjph00646-0038.pdf\">This research paper</a> based on SSA data said that the SSA stores birthdays in a fixed-width format:</p><blockquote><p>The data abstracted from the MBR consisted of a 26-character record for each deceased individual. The four data\nitems on each record were… the month and year of death.</p></blockquote><p>None of the datasets published by the SSA I found used ISO 8601 dates either; all of them have a seperate column for year of birth instead of using an ISO 8601 birthdate.</p><h2>ISO 8601 doesn’t have an epoch</h2><p>ISO 8601 is a format for representing dates as strings, not as numbers. It has no need for an epoch.</p><blockquote><p>ISO 8601:2004 fixes a reference calendar date to the Gregorian calendar of 20 May 1875 as the date the Convention du Mètre (Metre Convention) was signed in Paris (the explicit reference date was removed in ISO 8601-1:2019). However, ISO calendar dates before the convention are still compatible with the Gregorian calendar all the way back to the official introduction of the Gregorian calendar on 15 October 1582.</p></blockquote><p>I.e. the standard only uses 20 May 1875 as a reference date to define the Gregorian calendar, not as some earliest representable date.</p><h2>Nobody uses 1875 as an epoch</h2><p>I had found no evidence of 1875 ever being used as an epoch to start counting time from, in any context. I tried to find any case of this happening but I couldn’t find any. It’s definitely not a standard COBOL thing.</p>","contentLength":3417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ir52zy/there_is_no_1875_epoch/"},{"title":"The open source fastest URL shortener ever.","url":"https://dev.to/abdibrokhim/the-open-source-fastest-url-shortener-ever-5af3","date":1739746041,"author":"Ibrohim Abdivokhidov","guid":798,"unread":true,"content":"<p>The open source fastest URL shortener ever.</p><p>Built with awesome open source tools x.com/rustlang x.com/shuttle_dev x.com/neondatabase x.com/actix_rs</p><p>x.com/ThePracticalDev x.com/aimlapi x.com/vercel</p>","contentLength":194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Should People Just Use Goreleaser Instead of `actions-rust-release`?","url":"https://blog.urth.org/2025/02/16/should-people-just-use-goreleaser-instead-of-actions-rust-release/","date":1739745155,"author":"/u/autarch","guid":845,"unread":true,"content":"<p>I’m cross-posting this from\n<a href=\"https://github.com/houseabsolute/actions-rust-release/issues/10\">an issue I made</a> for\n<a href=\"https://github.com/houseabsolute/actions-rust-release\"></a>. For context,  am\nthe action’s author, and this is a serious question, not the start of a pitch for why you should use\nmy action.</p><p>TLDR; Does  serve any purpose that isn’t better served by\n<a href=\"https://goreleaser.com/\">goreleaser</a>?</p><p>Here’s the issue body in full:</p><blockquote><p>Recently, I was considering adding some features to this action, notably adding the ability to\nproduce signed releases (specifically, signing the checksums file). As I started looking into\nthis, I realized that <a href=\"https://goreleaser.com/\">goreleaser</a> already does this, as well as many\nother things this action doesn’t do:</p><ul><li>It offers a lot more power and flexibility in what is included in the resulting release archive\nfiles.<ul><li>This includes templating files, so for example you can update the copyright year in the\n file to match the release date.</li></ul></li><li>Deb, RPM, macOS DMG, MSI, Chocolatey, etc. support.</li><li>Integration with SBOM creation tools.</li></ul><p>So I’m wondering whether it’s worth continuing to invest in this action. It seems like using\ngoreleaser to release a Rust project is fairly easy. It even supports , though I think\nfor that I’d still use my\n<a href=\"https://github.com/houseabsolute/actions-rust-cross\">actions-rust-cross</a> action, as I don’t think\ngoreleaser would make it easier to do cross-platform builds.</p><p>Will people who use this action see this issue? If you do, I’d greatly appreciate your feedback!\nTake a look at <a href=\"https://goreleaser.com/\">goreleaser</a>, focusing specifically on the parts related\nto releasing, not building. After looking, do you still prefer this actions? If so, why?</p></blockquote>","contentLength":1465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ir4l9f/should_people_just_use_goreleaser_instead_of/"},{"title":"RandomPosterize in PyTorch","url":"https://dev.to/hyperkai/randomposterize-in-pytorch-35e6","date":1739744052,"author":"Super Kai (Kazuya Ito)","guid":799,"unread":true,"content":"<p><a href=\"https://pytorch.org/vision/main/generated/torchvision.transforms.v2.RandomPosterize.html\" rel=\"noopener noreferrer\">RandomPosterize()</a> can randomly posterize an image with a given probability as shown below:</p><ul><li>The 1st argument for initialization is (Required-Type:):\n*Memos:\n\n<ul><li>It's the number of bits to keep for each channel.</li></ul></li><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is posterized or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"python Level 1","url":"https://dev.to/mohamed_yahyasidimohame/python-level-1-366c","date":1739741834,"author":"Mohamed Yahya Sidi Mohamed","guid":783,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D]How to handle highly imbalanced dataset?","url":"https://www.reddit.com/r/MachineLearning/comments/1ir2zm3/dhow_to_handle_highly_imbalanced_dataset/","date":1739740969,"author":"/u/ThickDoctor007","guid":879,"unread":true,"content":"<p>I’m working on an insurance claims prediction model, and I’d love to get insights from the community on tackling a highly imbalanced dataset. In the past, I built churn prediction models, and now I’m focusing on predicting insurance claims, where the percentage of claims is quite low.</p><p>My dataset spans 15 years and contains ~800,000 records with features such as sex, age, horsepower, car brand &amp; type </p>","contentLength":408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Use the New tool Directive in Go 1.24","url":"https://www.bytesizego.com/blog/go-124-tool-directive","date":1739740319,"author":"/u/zakariachahboun","guid":794,"unread":true,"content":"<p>The release of Go 1.24 introduces the  directive for Go modules. This  simplifies the process of managing tools, such as linters or generators that are widely used in many Go\nprojects but are not directly used in the codebase. In this blog post, we will explore what the  directive is, why\nit matters, and how to use it effectively.</p><p>The  directive allows Gophers to specify dependencies for tools used in your Go project without adding those tools as\nregular dependencies. This is particularly useful for tools like , , or other command-line\nutilities that support your development workflow but don’t appear in your import graph.</p><p>Previously, developers had to manage such tools using workarounds, such as adding them to a  or a <a href=\"https://github.com/MatthewJamesBoyle/catserver/blob/master/gen.go#L3\"></a> file or manually\ninstalling them outside of Go modules. The  directive removes this friction by providing a dedicated way to\ndeclare these dependencies.</p><p>Using the new  directive offers several benefits:</p><ol><li>: Tools are clearly distinguished from code dependencies, reducing confusion.</li><li>: You can lock specific versions of tools, ensuring consistent behavior across environments.</li><li>: The Go tooling can install and manage tools automatically based on the  directive,\nremoving the need for custom scripts or manual installation.</li></ol><p>Here’s how you can add a tool dependency using the  directive:</p><h3>Step 1: Update Your  File</h3><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>In this example <code>github.com/golang/mock/mockgen</code> is the path to the tool.</p><p>This will add a tool directive to your go.mod file. It will look something like this:</p><pre tabindex=\"0\" data-language=\"go\"><code></code></pre><p>After declaring tools, you can install them using the  command:</p><p>Once installed, the tools are available for use.  For instance:</p><pre tabindex=\"0\" data-language=\"sh\"><code></code></pre><p>You can see which tools are installed by running:\n</p><ol><li>: Always specify a version for your tools to ensure consistency. Go will do this for you.</li><li>: Include instructions in your project’s README or CONTRIBUTING file on how to install and use\nthe tools.</li></ol><p>The  directive is only available in Go 1.24 and later. Ensure that your contributors use this version or greater.</p>","contentLength":1974,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ir2q9q/how_to_use_the_new_tool_directive_in_go_124/"},{"title":"A GUI App Which You Can Visulise Signal Waveforms With Python","url":"https://dev.to/gokhanergentech/a-gui-app-which-you-visulise-signal-waveforms-with-python-2ben","date":1739739362,"author":"Gökhan ERGEN","guid":767,"unread":true,"content":"<p>In this App, you can visulise three signal waveforms such as Sinusoidal, Square, and saw-toothed. Also, these signals has some params which you can setting them. The application was developed with dearpygui providing UI components for desktop apps. If you want to write a detailed blog about dearpygui, please comment :).</p><p>You can setting the ampitute of the waves.</p><p>A constant value which offsets veritcally</p><p>This is a sampling frequency showing how many samples is collected per a second.</p><p>We can use this to take the cycle count of signals. A cycle takes 1/Fsig seconds.</p><p>If Fsig is 0.2hz then the cycle count will be 5 seconds.</p><p>you can visulise sinusoidal sampled signal, squared sampled signal, and saw-toothed sampled signal by using above the params. The time range of signals is splitted (max_time-min_time)*Fs because Fs is sampling frequence per a second.</p><p>The program, which you change the params has a basic interface.</p><p>\nAs a default, selected waveform is sinusoidal.<p>\nSignal time range is between 0 and 10 and uses A*sin(2*π*Fsig*t+fi0)+dc as a formula to be drawn.</p>\nIn case A = 4,</p><p>We can use that positive side is about max 5 and negative side is min -3, because DC is 1 so signal shifts verticaly to up 1 step.</p><p>You will see a squared sampled signal.\nsignal_wave = sin(2*π<em>Fsig*t)\nif signal_wave &gt;=0, 1\n**Saw-toothed Wave</em>*</p><p>I use scipy library to draw saw-toothed wave form.\nAll of these signal waveforms use the same params.<p>\nLets change the time range as -100 to 100. We will see this graph sinusoidal waveforms.</p></p><p>I changed Fsig to 0.5. This means T = 1/0.5 = 2s cycle time.</p><p> 3.11.5 1.11.3 1.26.0 1.9.0</p>","contentLength":1594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RustyNum Follow-Up: Fresh Insights and Ongoing Development","url":"https://dev.to/igorsusmelj/rustynum-follow-up-fresh-insights-and-ongoing-development-18f9","date":1739738297,"author":"IgorSusmelj","guid":766,"unread":true,"content":"<p>As a follow-up to my previous introduction to <a href=\"https://github.com/IgorSusmelj/rustynum\" rel=\"noopener noreferrer\">RustyNum</a>, I want to share a developer-focused update about what I’ve been working on these last few weeks. RustyNum, as you might recall, is my lightweight, Rust-powered alternative to NumPy published on GitHub under MIT license. It uses Rust’s portable SIMD features for faster numerical computations, while staying small (around ~300kB for the Python wheel). In this post, I’ll explore a few insights gained during development, point out where it really helps, and highlight recent additions to the documentation and tutorials.</p><p>If you missed the initial announcement, RustyNum focuses on:</p><ul><li>High performance using Rust’s SIMD</li><li>Memory safety in Rust, without GC overhead</li><li>Small distribution size (much smaller than NumPy wheels)</li><li>NumPy-like interface to reduce friction for Python users</li></ul><h2>\n  \n  \n  Developer’s Perspective: What’s New?\n</h2><p><strong>1. Working with Matrix Operations</strong></p><p>I’ve spent a good chunk of time ensuring matrix operations feel familiar. Being able to do something like matrix-vector or matrix-matrix multiplication with minimal code changes from NumPy was a primary goal. A highlight is the  function and the  operator, which both support these operations.</p><div><pre><code></code></pre></div><p>It’s neat to see how close this is to NumPy’s workflow. Benchmarks suggest RustyNum can often handle these tasks at speeds comparable to, and sometimes faster than, NumPy on smaller or medium-sized datasets. For very large matrices, I’m still optimizing the approach.</p><p><strong>2. Speeding Up Common Analytics Tasks</strong></p><div><pre><code></code></pre></div><p>The Python overhead can sometimes offset the raw Rust speed, but in many cases, RustyNum still shows advantages.</p><h2>\n  \n  \n  New Tutorials: Real-World Examples\n</h2><p>One of the best ways to see RustyNum in action is through practical examples. I’ve added several new tutorials with real-world coding scenarios:</p><ol><li> – Focus on dot products, matrix-vector, and matrix-matrix tasks.</li><li><strong>Replacing Core NumPy Calls</strong> – Demonstrates how to switch from NumPy’s mean, min, dot to RustyNum.</li><li><strong>Streamlining ML Preprocessing</strong> – Explores scaling, normalization, and feature engineering for machine learning.</li></ol><p>Check out a snippet of scaling code from that guide:</p><div><pre><code></code></pre></div><p>It’s a small snippet, but it shows how RustyNum can do row/column manipulations quite effectively. After scaling, you can still feed the data into your favorite machine learning frameworks. The overhead of converting RustyNum arrays back into NumPy or direct arrays is minimal compared to the cost of big model training steps.</p><p><strong>1. Large Matrix Optimizations</strong></p><p>I’ve noticed that for very large matrices (like 10k×10k), RustyNum’s current code paths aren’t yet fully optimized compared to NumPy. This area remains an active project. RustyNum is still young, and I’m hoping to introduce further parallelization or block-based multiplication techniques for better large-scale performance.</p><p>RustyNum supports float32 and float64 well, plus some integer types. I’m considering adding stronger integer support for data science tasks like certain indexing or small transformations. Meanwhile, advanced data types (e.g., complex numbers) might appear further down the line if the community needs them.</p><p><strong>3. Documentation and API Enhancements</strong></p><p>The docs site at <a href=\"https://rustynum.com/\" rel=\"noopener noreferrer\">rustynum.com</a> has an API reference and a roadmap. I’m continuously adding to it. If you spot anything missing or if you have a specific use case in mind, feel free to open a GitHub issue or submit a pull request.</p><p><strong>4. The big goal of Rustynum</strong></p><p>RustyNum is simply a learning exercise for me to combine Rust and Python. Since I spend every day around machine learning I would love to have RustyNum replace part of my daily Numpy routines. And we're slowly getting there. I started adding more and more methods around the topic of how to integrate RustyNum in ML pipelines.</p><h2>\n  \n  \n  Quick Code Example: ML Integration\n</h2><p>To demonstrate how RustyNum fits into a data pipeline, here’s a condensed example:</p><div><pre><code></code></pre></div><p>This script highlights that RustyNum can handle data transformations with a Pythonic feel, after which you can pass the arrays into other libraries.</p><p>It’s been fun to expand RustyNum’s features and see how well Rust can integrate with Python for high-performance tasks. The recent tutorials are a window into how RustyNum might replace parts of NumPy in data science or ML tasks, especially when smaller array sizes or mid-range tasks are involved.</p><ul><li>Check out the tutorials at rustynum.com</li><li>Contribute or report issues on GitHub</li><li>Share feedback if there’s a feature you’d love to see</li></ul><p>Thanks for tuning in to this developer-focused update, and I look forward to hearing how RustyNum helps you in your own projects!</p>","contentLength":4610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pulumi Gestalt devlog #8","url":"https://dev.to/andrzejressel/pulumi-gestalt-devlog-8-1d1m","date":1739734216,"author":"​Andrzej Ressel","guid":753,"unread":true,"content":"<p>Welcome to the eighth devlog for . This week, the focus was on preparing native Rust support and moving toward an initial release.</p><p>Previously, Rust support in Pulumi Gestalt was essentially Wasm/Rust support, which required a complex setup and runtime environment. This week, I implemented proper native Rust support, which simplifies the process significantly. Now you can get started without external runners - only  and <a href=\"https://github.com/casey/just\" rel=\"noopener noreferrer\"></a> required.</p><p>After several weeks of development and refinement, I believe Pulumi Gestalt has reached a state where it’s ready for an initial release. Over the next few days, I’ll be focusing on finalizing documentation and ensuring consistent naming conventions across the SDKs to provide a smoother experience for users.</p><p>That’s all for this week’s updates! As always, I welcome your feedback. If you have any thoughts, suggestions, or run into issues, feel free to share them on either the <a href=\"https://github.com/andrzejressel/pulumi-gestalt\" rel=\"noopener noreferrer\">main repository</a> or the <a href=\"https://github.com/andrzejressel/pulumi-gestalt-example\" rel=\"noopener noreferrer\">example repository</a>.</p>","contentLength":959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why did they decide not to have union ?","url":"https://www.reddit.com/r/golang/comments/1iqzofv/why_did_they_decide_not_to_have_union/","date":1739732626,"author":"/u/D4kzy","guid":765,"unread":true,"content":"<div><p>I know life is simpler without union but sometimes you cannot get around it easily. For example when calling the windows API or interfacing with C.</p><p>Do they plan to add union type in the future ? Or was it a design choice ?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/D4kzy\"> /u/D4kzy </a>","contentLength":249,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Project Translate: The Translate API (Part 2)","url":"https://dev.to/__dbrown__/project-translate-the-translate-api-part-2-2nd1","date":1739731951,"author":"Emmanuel Akolbire","guid":724,"unread":true,"content":"<p>Hey developers! 👋 In this post, we'll implement the text translation endpoint using Python, AWS Lambda, and a clean Hexagonal Architecture. Let's dive in! You can check out my <a href=\"https://github.com/DeXtreme/translate\" rel=\"noopener noreferrer\">GitHub</a> for the complete code.</p><p>We create a new project with the directory structure shown in the picture\nThen we install the dependency, namely boto3, with pip. We also make sure to create a requirements.txt file so we know which version to install when the script is packaged.</p><p>We'll be employing Hexagonal(Layered) Architecture in the design of our API. Hexagonal Architecture or Ports and Adpaters is a design pattern that aims at creating loosely coupled components. A helpful guide can be found <a href=\"https://dev.to/xoubaman/understanding-hexagonal-architecture-3gk\">here</a>. Although python is a dynamically typed language, we can still use this pattern.</p><p>We'll be using the project directory structure shown below<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyys8ctber2fmle1icagi.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fyys8ctber2fmle1icagi.png\" alt=\"Project structure\" width=\"330\" height=\"385\"></a></p><h3>\n  \n  \n  The Translation Record Model\n</h3><p>Let's start with a simple but effective model to track our translations. We'll use Python's dataclasses - they're clean, efficient, and give us nice features out of the box.</p><div><pre><code></code></pre></div><p>Let's break down what each field does:</p><ul><li>: A unique identifier for each translation record</li><li>: The original text that needs translation</li><li>: The translated result</li><li>: Timestamp of when the translation was performed, automatically set to the current time</li></ul><p>You might wonder why we're using  instead of a regular class. Here's what makes dataclasses great for our use case:</p><p>Less Boilerplate: We don't need to write , , or  methods\nDefault Values: Easy handling of default values with the field function<p>\nType Hints: Built-in support for type hints, making our code more maintainable</p></p><p>Next, we'll define our ports using Python's Protocol class - a more Pythonic approach to interfaces. Let's dive in!\nWhy Protocols Over Abstract Base Classes?<p>\nBefore we jump into the code, let's understand why we're choosing Protocols:</p></p><ul><li>More Pythonic - follows duck typing principles</li><li>Structural subtyping instead of nominal subtyping</li><li>Better integration with static type checkers</li><li>No explicit inheritance required\n</li></ul><div><pre><code></code></pre></div><p>Now we define the adapters that implement the ports. The <code>DynamoDBPersistenceAdapter</code> stores the input and output in DynamoDB and return a Record object. The  translates the text with AWS Translate and returns the result.</p><div><pre><code></code></pre></div><p>Now we'll create the Lambda handler that ties everything together.\nWe'll define the  class with handles the requests to Lambda from the API Gateway. It parses the body for the required fields, translates the text, stores the input and output and returns a response</p><div><pre><code></code></pre></div><p>In order to allow Cross Origin Requests we add the Access-Control-Allow headers to the reponse object. For example, in the  method</p><div><pre><code></code></pre></div><p>In the next installment of this series, we'll dive into the code that handles file translation. Stay tuned! 🚀</p>","contentLength":2722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"A calculator app? Anyone could make that.\"","url":"https://chadnauseam.com/coding/random/calculator-app","date":1739731262,"author":"/u/iamkeyur","guid":720,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqz489/a_calculator_app_anyone_could_make_that/"},{"title":"[Boost]","url":"https://dev.to/arindam_1729/-1h49","date":1739730645,"author":"Arindam Majumder","guid":723,"unread":true,"content":"<h2>🤯 11 Exciting GitHub Repositories You Should Check Right Now⚡️</h2><h3>Arindam Majumder  ・ Feb 13</h3>","contentLength":97,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hinton: \"I thought JD Vance's statement was ludicrous nonsense conveying a total lack of understanding of the dangers of AI ... this alliance between AI companies and the US government is very scary because this administration has no concern for AI safety.\"","url":"https://www.reddit.com/r/artificial/comments/1iqy8te/hinton_i_thought_jd_vances_statement_was/","date":1739729055,"author":"/u/MetaKnowing","guid":718,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transform Your Data Model to AI Workflow - with only 8 extra lines of code!","url":"https://dev.to/eduardknezovic/transform-your-data-model-to-ai-workflow-with-only-8-extra-lines-of-code-3f5a","date":1739727355,"author":"Eduard Knezovic","guid":699,"unread":true,"content":"<p><em>\"Good programmers worry about data structures and their relationships.\"</em> - Linus Torvalds</p><p>What if you could create complex AI workflows as easily as defining your data structures with Pydantic?</p><p>What if you could simply harness the power of AI by allowing your Pydantic data models to flow like water?</p><p><a href=\"https://pypi.org/project/modellm/\" rel=\"noopener noreferrer\">ModeLLM</a> makes this possible by turning your Pydantic models into powerful AI pipeline components.</p><p>Let's go over an example!</p><p>You will need to provide your own OPENAI_API_KEY (if you haven't already)</p><div><pre><code></code></pre></div><p>You will also need to install the  library.</p><p>All of the relevant dependencies are automatically installed\nwith the  library.</p><p>Take a look at this complete working example.</p><div><pre><code></code></pre></div><p>To consolidate your knowledge:</p><ol><li>Execute the existing code on your computer</li><li>Generate the story for teenagers (uncomment one line of code) </li><li>Create a  Pydantic model that should summarize the story</li><li>Create your own Pydantic model and inject it to the pipeline</li></ol><p>In this example, we've managed to harness the power of AI\nin (only!) 8 additional lines of code - thanks to the <a href=\"https://pypi.org/project/modellm\" rel=\"noopener noreferrer\">ModeLLM library</a></p><ol><li>: Define what you want, not how to get it (LLM is smart enough to catch the cue)</li><li>: Chain transformations with the  operator (Makes our code easy to modify and extend)</li><li>: Docstrings guide the AI's behavior</li><li>: Easily swap components </li><li>: Complex AI operations hidden behind simple data models</li></ol><p>By defining our Pydantic data models (and decorating them) we were able to execute our AI pipeline with a single line of code:</p><div><pre><code></code></pre></div><p>What do you think about this approach? I would love to hear your thoughts and suggestions.</p>","contentLength":1534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I wrote a desktop overlay for reading manga with egui","url":"https://www.reddit.com/r/rust/comments/1iqxfd0/i_wrote_a_desktop_overlay_for_reading_manga_with/","date":1739727052,"author":"/u/Takader","guid":744,"unread":true,"content":"<p>Today i am open sourcing my manga overlay i have been working on. It enables continues detection of japanese text in a selected region on the desktop. My goal was making it easy to find the meaning of kanji in order to learn japanese. </p><p>This is the first time i am open sourcing a project so feedback is welcome.</p>","contentLength":310,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling a Neuron in micrograd (As Explained by Karpathy)","url":"https://dev.to/shrsv/modeling-a-neuron-in-micrograd-as-explained-by-karpathy-6gh","date":1739723924,"author":"Shrijith Venkatramana","guid":677,"unread":true,"content":"<p><em>Hi there! I'm Shrijith Venkatrama, founder of Hexmos. Right now, I’m building <a href=\"https://hexmos.com/liveapi\" rel=\"noopener noreferrer\">LiveAPI</a>, a tool that makes generating API docs from your code ridiculously easy.</em></p><p>In serious neural network implementations, we model the neuron in the following way:</p><ol><li>1 \"Influence\"  (dendrite)</li><li>Sum of \"influences\" =  (cell body)</li></ol><p>The above leads to the cell body expression:</p><ol><li>Activation function - squashing fuction (, )</li></ol><h2>\n  \n  \n  Representing the Model Neuron (defined above) in micrograd\n</h2><div><pre><code></code></pre></div><h2>\n  \n  \n  Implementing  into Value (for the Activation Function)\n</h2><p>We have the following  formula:</p><p>We can implement the function as follows:</p><div><pre><code></code></pre></div><p>We'll add a new node  which is the :</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Derivative of o - Derivative of </h2><p>The formula for derivative of  is the following:</p><p>So, we want to find out :</p><div><pre><code>do/dn = 1 - tanh(n)**2 = 1 - o**2\n</code></pre></div><h2>\n  \n  \n  Getting all the backprop values calculated (manually)\n</h2><p>We leverage some patterns we've learned previously about how backprop works with addition/multiplication, to quickly fill in the values for  in each node:</p><div><pre><code></code></pre></div>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ultimate Football Draft (A Python Terminal Game)","url":"https://dev.to/jcubilloespinoza/ultimate-football-draft-a-python-terminal-game-128i","date":1739723472,"author":"Josue Cubillo Espinoza","guid":676,"unread":true,"content":"<p>Football fans around the world dream of managing their favorite teams and experiencing the thrill of competition. This Python program allows users to choose a team, participate in a simulated tournament, and compete for victory. By randomly assigning teams to groups and generating matches, the program provides an engaging and interactive experience.</p><p>The program begins by prompting the user to enter their name and select their favorite football team from a list of international clubs. After choosing a team, the program randomly assigns teams to groups for the tournament’s group stage. The user’s selected team is placed in one of these groups.</p><p>Using the random module, the program ensures fair and unpredictable group draws and match results. The user competes against other teams by answering trivia questions. Winning matches earns points, and the top teams from each group advance to the knockout stages.</p><p>Throughout the knockout rounds, the user’s team must win to progress further. If they lose a match, they are eliminated from the competition.</p><ol><li>Random selection of football teams into tournament groups.</li><li>Interactive gameplay where users answer questions to win matches.</li><li>Randomized match results for AI-controlled teams.</li><li>Automatic generation of tournament brackets leading to the final match.</li></ol><p>This project is a great example of how Python can be used to create engaging sports simulations. Whether you are a football fan or a programming enthusiast, this program provides an enjoyable way to experience the excitement of a football tournament. Try it out and see if your team can become the ultimate champion!</p>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing: pixelvim, vim inspired pixel editor","url":"https://www.reddit.com/r/rust/comments/1iqviie/announcing_pixelvim_vim_inspired_pixel_editor/","date":1739722214,"author":"/u/avatar_10101","guid":781,"unread":true,"content":"<blockquote><p> is a pixel editor inspired by the  text editor, with an emphasis on keyboard interaction. It also aims to be feature-rich, customizable, and extendable via user scripts.</p></blockquote><p>This is my personal project of making a vim-like pixel art editor (not very creative with the name, I know), written in Rust using <a href=\"https://github.com/not-fl3/miniquad\">miniquad</a>.</p><p>Try it in the browser: <a href=\"https://bolphen.github.io/pixelvim/\">https://bolphen.github.io/pixelvim/</a> (you can drag-and-drop png, gif, and aseprite files, and save to png and gif; use  to increase the UI if you find them too small)</p><ul><li>vi-style remappable keyboard interaction and a command system, including modifiers ( for moving 5 pixels down) and chain-able commands (<code>:select/all THEN cut THEN :layer/new/above THEN paste</code>)</li><li>elaborated \"visual\" mode for pixel selection (that is undo/redoable)</li><li>animation \"live draw\" (see the screencast below: very useful for quickly creating particle effects)</li><li>rudimentary support for lua user scripts (not available in the browser version)</li><li>data recovery from swap file in case of crashes</li></ul><p>The code is not pretty (a lot of places held up with glue) and there are quite a lot I want to improve as well as new features to add, but I feel that the end product could already be useful for some so I'm releasing it. Overall I wish to translate more vim features that could be useful into pixelvim (for example, insert mode for drawing purely with the keyboard; registers and macros; better documentation), also better UI.</p><p>There used to be a similar project <a href=\"https://github.com/cloudhead/rx\">rx</a>. This is not a fork, though I did borrow a few things here and there. I'd say right now pixelvim is much more feature complete than rx.</p>","contentLength":1580,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mongotui - A MongoDB client with a terminal user interface","url":"https://github.com/kreulenk/mongotui","date":1739721100,"author":"/u/gopher_256","guid":717,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iqv3kc/mongotui_a_mongodb_client_with_a_terminal_user/"},{"title":"AI do have some points tho","url":"https://www.reddit.com/r/artificial/comments/1iqv26f/ai_do_have_some_points_tho/","date":1739720999,"author":"/u/JaydenPlayz2011","guid":1632,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unlock the Power of Neural Networks – From Scratch!","url":"https://dev.to/devinsights_blog_ed29ec86/unlock-the-power-of-neural-networks-from-scratch-3off","date":1739720592,"author":"DevInsights Blog","guid":652,"unread":true,"content":"<p>Have you ever wondered how machines can recognize images, translate languages, or even predict future trends? The secret lies in  – the backbone of modern AI.</p><p>Understanding how a neural network works can feel overwhelming, especially with so many complex libraries available. But what if you could actually <strong>build a neural network from scratch</strong> and understand every single step?</p><ul><li>The core concepts behind neural networks</li><li>Forward propagation, backpropagation, and loss calculation explained simply</li><li>A complete hands-on example in </li></ul><p>Building a neural network without relying on libraries like TensorFlow or PyTorch will give you  in AI. It’s like learning the fundamentals of a car engine before driving a sports car. Once you master this, using advanced tools will make far more sense.</p><p>Here’s a quick look at what you’ll be able to do:</p><div><pre><code></code></pre></div><p>This simple piece of code is part of a fully functional XOR solver you’ll build from scratch!</p><h4>\n  \n  \n  But That’s Just the Beginning...\n</h4><p>The full guide covers , from initializing weights to adjusting them through backpropagation – <strong>with clear explanations and complete working code.</strong></p><p>If you’re serious about AI and want to break free from black-box libraries, .</p><p><strong>Check it out now and start your deep learning journey today!</strong></p>","contentLength":1255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finished Auth App for Galileo! 🚀","url":"https://dev.to/khaled_abdelbar_43f8c0b1d/finished-auth-app-for-galileo-4md6","date":1739720424,"author":"Khaled Abdelbar","guid":651,"unread":true,"content":"<p>I’ve successfully finished building the authentication app for my Galileo project! 🎉</p><p>Next, I’ll focus on creating a Teams App to handle team functionalities, such as creating and joining teams. This will be a crucial step in enhancing collaboration within the project.</p><p>I’ll continue documenting each step of my journey here as I build Galileo, sharing insights and challenges I encounter. Stay tuned for updates on how I tackle the team management feature!</p>","contentLength":463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making a Streaming JOIN 50% faster","url":"https://www.reddit.com/r/rust/comments/1iqum3o/making_a_streaming_join_50_faster/","date":1739719765,"author":"/u/bobbymk10","guid":721,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/bobbymk10\"> /u/bobbymk10 </a>","contentLength":32,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making a Streaming Join 50% Faster","url":"https://www.epsio.io/blog/optimizing-streaming-joins-leveraging-asymmetry-for-better-performance","date":1739719723,"author":"/u/ThinkRedstone","guid":1677,"unread":true,"content":"<div><div><h5 blocks-non-deletable=\"true\">Deliver instant &amp; up-to-date results for complex queries</h5></div></div>","contentLength":56,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqulkg/making_a_streaming_join_50_faster/"},{"title":"How JIT (Just in time) compilation and V8 works and makes js incredibly fast","url":"https://www.royalbhati.com/posts/why-js-is-fast","date":1739719684,"author":"/u/CaptainOnBoard","guid":780,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqul1k/how_jit_just_in_time_compilation_and_v8_works_and/"},{"title":"Writing LLM prompts in Go with type-safety","url":"https://blog.lawrencejones.dev/ai-dont-need-python/","date":1739717626,"author":"/u/shared_ptr","guid":690,"unread":true,"content":"<p>“If you want to build AI products, you need to use Python.”</p><p>I hear this a lot, and it’s wrong. I get why people think it- software engineers\nhave been trained to lean into open-source tooling, and the AI ecosystem is\nundoubtably strongest in Python.</p><p>But if you don’t already use Python, switching languages for the ecosystem is\nlikely a bad call. Not only is that ecosystem less mature than it seems (who\ngenuinely has enough experience to build an opinionated agent-framework at this\npoint?), but it is grounded more in a research/ML perspective than a product\none.</p><p>Additionally, the language itself is a poor fit for AI products! Python\nconcurrency can be tricky, which is going to hurt you when it comes to\noptimising the latency of your AI systems, and a static type-system can bring\nsome much-needed structure to non-deterministic models.</p><p>At incident.io, we use Go for all our backend, and saw no reason why AI should\nbe any different. Go has been a fantastic choice, helping us <a href=\"https://incident.io/ai\" target=\"_blank\" rel=\"noopener noreferrer\">build AI\nsystems</a> that spawn concurrent prompts, speculatively execute tools, and instrument\neverything to the hilt.</p><p>Despite Go being known as a no-frills language, we’ve built some really\nergonomic abstractions that make working with AI really easy. I expect you can\ndo it in your native language, too.</p><p>Here’s an example of a prompt in our codebase:</p><div><div><pre><code></code></pre></div></div><p>Each prompt is its own struct, describing which model to use, the input type\nthat we use to template the messages, and finally the prompt that we’ll send to\nOpenAI (or Anthropic, GCP, whatever).</p><p>What I love about this is how much heavy lifting the type system is doing for\nus. Running the prompt is as simple as:</p><div><div><pre><code></code></pre></div></div><p>Behind the scenes,  is doing some cool stuff:</p><ol><li>It reflects over our  type to build an OpenAPI specification, using\nstruct tags like  and  as documentation.</li><li>That specification is used to force the model into giving structured JSON\nresponses that match our type exactly.</li><li>The prompt is templated with our .</li><li>Finally, the response is parsed back into our result type.</li></ol><p>Each prompt lives in its own file () alongside its evaluation\nsuite (), making it easy to maintain and test.</p><p>The pattern above works great for fixed prompts, but sometimes you need more\nflexibility. That’s where Go’s generics come in handy.</p><p>Here’s an example from our incident investigation system:</p><div><div><pre><code></code></pre></div></div><p>We’re using  across our <a href=\"https://incident.io/ai#investigations\" target=\"_blank\" rel=\"noopener noreferrer\">investigation\nsystem</a>, but each time we run it we want different types of\nresults back. Sometimes we want permalinks to code changes, other times we’re\nextracting timeline entries, or grading the quality of our investigation\nhypothesis.</p><p>Rather than copy-and-pasting the prompt in order to use the same context setup,\nwe can use a generic type parameter to control the result we get back. The\nprompt template and core logic stays the same, but we get different structured\ndata back each time.</p><p>That’s as close as we’ll get to Vercel’s  in Go, and I’d say\nit’s a strong challenger.</p><h2>Python is not the only answer</h2><p>Python is a great tool for machine learning and a lot more. But it has real\nweaknesses when it comes to building production AI systems, especially if you’re\nalready working in a different language.</p><p>I’ve spoken with several teams who switched to Python to work with AI, away from\ntheir normal stack, and they’ve all struggled. They’re trying to learn AI\nalongside new tools and patterns (<a href=\"https://blog.lawrencejones.dev/learn-one-thing/\">almost always a bad idea</a>), which\nmakes it much more likely they burn out or fail.</p><p>Even if they succeed, they’ve disconnected their AI features from their core\nproduct. These are companies aiming to be “AI native” but have their AI features\nliving in a separate codebase with different conventions. That doesn’t feel like\nsuccess to me.</p><p>At incident.io, we kept everything in Go and it’s working really well. Our AI\nfeatures live right alongside our core product, using the same development\npatterns the team already knows. We’ve even managed building some expressive and\nfancy abstractions in a language that is notoriously no-frills, while leveraging\nGo’s strengths in concurrency and type safety.</p><p>The AI ecosystem will continue to be Python-first, and that’s fine! The actual\ninteraction with models is just HTTP requests with JSON payloads, though. Build\ngood abstractions in your language of choice, and focus your energy on the hard\nstuff: <a href=\"https://blog.lawrencejones.dev/ai-mvp/\">making AI features that actually work</a>.</p><p><em>\n        \n        If you liked this post and want to see more, follow me at <a target=\"_blank\" href=\"https://twitter.com/lawrjones\" rel=\"noopener noreferrer\">@lawrjones</a>.\n      </em></p>","contentLength":4436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iqtugn/writing_llm_prompts_in_go_with_typesafety/"},{"title":"Python User Group Dhaka: Our Journey Begins – Event Recap","url":"https://dev.to/tamalchowdhury/python-user-group-dhaka-our-journey-begins-event-recap-2he7","date":1739715829,"author":"Tamal Anwar Chowdhury","guid":635,"unread":true,"content":"<p>On a cool February evening in Dhaka, a group of passionate Python enthusiasts gathered for something special—our city’s first-ever Python User Group meetup. What started as an idea a few weeks earlier turned into a diverse community event, laying the foundation for Python Dhaka’s journey.</p><p>Here’s how we made it happen and what we learned along the way.</p><p>The whole event was organized through our Facebook group. In Bangladesh, Facebook is the primary hub for tech enthusiasts to form communities and organize events. Unlike platforms like Meetup or Discord, Facebook groups often serve as the starting point for grassroots tech movements here. I launched a Facebook group for Python Dhaka a few weeks prior, and thanks to my personal network, it grew to 250 members within two weeks.</p><p>The next logical step was to organize an in-person meetup, because I want this group to be all about connecting with the local Python enthusiasts. After thinking about it for a few days, I locked in a date and venue for the first meetup.</p><p>I choose the 15th of February because it's the mid-month, and a Saturday. In Bangladesh, Fridays and Saturdays are official holidays. Friday is the most popular day for events, followed by Saturdays. I picked Chandrima Udyan for two reasons: 1. It has easy access to the bus and metro route; 2. It's a public park we can access for free.</p><p>I created the event on Facebook and started spreading the word on the FB group, my profile, Twitter, Linkedin, and my Instagram handle. I even created a short video announcing the event and posted it on all of my socials.</p><p>At 5 PM local time, I waited near the park entrance, feeling a mix of excitement and nerves. Soon, one by one, attendees started arriving, their calls guiding me to them. Here’s me waiting:</p><p>As we entered the park, a cool evening breeze greeted us—washing away the stress of city life and setting the perfect mood for our first meetup.</p><p>I started the meeting by sharing the Python Software Foundation's missions with the attendees, and how it relates to us:</p><ul><li><p><strong>To grow a diverse and international community of Python programmers:</strong> Organizing the Python Dhaka community is helping this mission.</p></li><li><p><strong>Encourage knowledge sharing, collaboration, and support devs of all backgrounds:</strong> Students from three universities joined this event. They were able to share knowledge and build future collaboration opportunities. This community is open to all levels of developer experience.</p></li><li><p><strong>Grow a diverse and welcoming community; support underrepresented communities in tech:</strong> One participant came from a non-computer science background, eager to transition into tech. This is exactly why Python Dhaka exists—to welcome everyone, regardless of their starting point.</p></li></ul><p>I then shared how I started evangelizing Python.</p><p>I am a professional JavaScript developer with React and NextJS expertise. I use JS for my daily work. My cousin wanted to learn Python, so I got into learning it. I found Python to be easy to learn and easy to teach. I was looking for a Python community in Dhaka, but there was none. That's why I decided to organize the Python User Group Dhaka.</p><p>I also mentioned that I don't own Python Dhaka. I am only organizing it for the time being. As this community grows, and if in the future I have to move cities, I will pass the torch to the next person to continue organizing our community activities.</p><p>We are not strictly Python fanatics. We would love to collaborate with our friends in the JavaScript, PHP, and Kotlin communities too from time to time.</p><p>We heard personal stories from the attendees, how they got into programming, and how they are using Python. Two students are doing competitive programming with C, C++ and thinking of switching to Python for CP and ML.</p><p>Two of the attendees came from different districts just so they could attend this meetup.&nbsp;</p><p><strong>One attendee traveled over 80 kilometers from Tangail just to be here. That level of dedication reminded me why this community matters.</strong></p><p>I shared many tech tips, and also informed them about the free GitHub Student Developer pack and how to avail it. We also discussed getting real-world experiences by putting an app out in the world.</p><p>We also announced the Campus Ambassador Program for Python Dhaka and nominated Abdullah to be the ambassador at Southeast University. A campus ambassador is a person in your college/university who will promote and evangelize Python programming language on your campus.</p><p>As the meetup was about to end, it was getting dark when we clicked this group photo.</p><p>Our journey is just beginning. We plan to host monthly meetups around the 15th of each month, but our ambition goes beyond that. We want to nurture the next generation of Python developers in Bangladesh, and one day, bring PyCon Bangladesh to life.</p><p>If you’re as passionate about Python as we are—whether you’re in Dhaka or anywhere in the world—we’d love your support. Find <a href=\"https://www.facebook.com/groups/pythondhaka\" rel=\"noopener noreferrer\">Python User Group Dhaka on Facebook</a>, or connect with me on Linkedin, X, and Instagram. Let's build this community together!</p>","contentLength":5017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mobile-First Approach for FastAPI Full-Stack Template Authentication: Migrating to phone_number/OTP","url":"https://dev.to/justjayzee/mobile-first-approach-for-fastapi-full-stack-template-authentication-migrating-to-phonenumberotp-m02","date":1739715631,"author":"Javad Zarezadeh","guid":634,"unread":true,"content":"<p>As you may know,  is one of the most admired frameworks for developing RESTful APIs. Another fantastic project by the same author, <a href=\"https://dev.to/tiangolo\">@tiangolo</a>, is the <a href=\"https://github.com/fastapi/full-stack-fastapi-template\" rel=\"noopener noreferrer\">Full Stack FastAPI Template</a>, which I previously wrote about <a href=\"https://dev.to/justjayzee/why-fastapi-full-stack-template-is-my-go-to-for-modern-web-development-1f2e\">here</a>.</p><p>In this post, I'll guide you through the process of replacing the email/password authentication flow in the template with a phone_number/OTP-based system. This approach is ideal for mobile-first applications and offers a user-friendly, secure way to authenticate users. My goal is to make minimal changes to the original project while maintaining its adherence to  and . Let’s dive in! 😉</p><h2>\n  \n  \n  1. Replace  and  with  and </h2><ul><li>Update the  value to a phone number, e.g.,  or .</li><li>Remove  as it is no longer necessary.</li></ul><p>Replace all instances of the  field with .</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>⚠️  This change requires updating the database schema using Alembic migrations.</p><p><code>./backend/app/api/routes/private.py</code>\nReplace all occurrences of  and  with  and .</p><p><code>./backend/app/api/routes/login.py</code> call:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>⚠️  Keep  and  unchanged due to OAuth2 standards.</p><p><code>./backend/app/api/routes/users.py</code></p><ul><li>Replace  with .</li><li>Remove the  in the  function related to email validation.</li></ul><ul><li>Replace  references with .</li><li>Remove  as it is no longer necessary.</li></ul><p> ./backend/app/crud.py  </p><ul><li>Replace all  and  references with  and .\n</li><li>Rename  to  and update all references to this function.</li></ul><h2>\n  \n  \n  2. Add an API Endpoint to Request OTP\n</h2><p><code>./backend/app/api/routes/login.py</code><p>\nAdd the following endpoint:</p></p><div><pre><code></code></pre></div><h2>\n  \n  \n  3. Nullify OTP After Login\n</h2><p><code>./backend/app/api/routes/login.py</code>\nIn the  function, nullify the OTP after successful login:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  4. Remove Unnecessary Functions\n</h2><p><code>./backend/app/api/routes/login.py</code><p>\nRemove the following functions:</p></p><ul><li><code>recover_password_html_content</code></li></ul><h2>\n  \n  \n  5. Remove Unnecessary Email Features\n</h2><p><code>./backend/app/api/routes/users.py</code><p>\nRemove the email-related logic, such as:</p></p><div><pre><code></code></pre></div><h2>\n  \n  \n  6. Remove Password Update and User Registration Functions\n</h2><p><code>./backend/app/api/routes/users.py</code></p><p>Remove the following functions:  </p><ul></ul><p>Since we are now using OTP-based authentication, these functions are redundant.  </p><p>Update all  fields to .  </p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3><strong>Remove OTP from :</strong></h3><p>For the  model, you don’t need to include the  field. Update it as follows:</p><div><pre><code></code></pre></div><p>This simplifies the creation process since OTP will be generated later during login.  </p><p>Add an  field to both the  and  models.  </p><div><pre><code></code></pre></div><h3><strong>Remove Unnecessary Models:</strong></h3><p>Delete models that are no longer needed, including:  </p><ul></ul><p>This cleanup ensures the models remain relevant to the new authentication system.  </p><ul><li><code>generate_reset_password_email</code></li><li><code>generate_new_account_email</code></li><li><code>generate_password_reset_token</code></li><li><code>verify_password_reset_token</code></li></ul><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Following these steps will transform the Full Stack FastAPI Template’s email/password flow into a phone_number/OTP-based system while keeping it aligned with best practices and standards. Happy coding! 🚀 </p><p>These changes to the  original project are available in my <a href=\"https://github.com/javadzarezadeh/phone-otp-auth-fastapi\" rel=\"noopener noreferrer\">GitHub</a>. It is important to use this project cautiously, since I have not yet had time to write the tests.</p>","contentLength":2908,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"15 lessons from 15 years in tech","url":"https://newsletter.eng-leadership.com/p/15-lessons-from-15-years-in-tech","date":1739715596,"author":"/u/gregorojstersek","guid":675,"unread":true,"content":"<p>Finding skilled developers is hard → even for seasoned engineering leaders. Traditional hiring cycles can take months while critical features sit in the backlog.</p><p>Need to scale your team quickly or find specialized talent that's hard to source locally? They match you with developers from Europe and Latin America who integrate seamlessly into your workflow → without the long hiring cycles or commitment of long-term contracts.</p><p>They don't just check résumés, but put developers through a rigorous multi-step vetting process that assesses technical skills, problem-solving abilities, and communication.</p><p>Let’s get back to this week’s thought!</p><p>I’ve made many mistakes throughout my 11+ career in the engineering industry and I learned a lot from them. But one thing I didn’t do so well is to learn from mistakes from others.</p><p>I didn’t actively look for mentorship and coaching and there weren’t a lot of books on pure Engineering Management back when I first became a manager to help me with this transition.</p><p>Even though I still progressed all the way to CTO, I believe that my progression would be a LOT easier with good mentorship support.</p><blockquote></blockquote><p>This is very important to keep in mind and that’s exactly the goal of today’s article.</p><p><a href=\"https://www.linkedin.com/in/lewisowain/\" rel=\"\">Owain Lewis</a></p><p>He’ll be sharing 15 lessons that he has learned the hard way. Let’s get straight into them. </p><p>Tech changes fast, but certain principles stay the same.</p><p>These lessons have helped me navigate challenges at work, grow as a professional, and deliver results. Maybe they’ll help you too.</p><p>Here are 15 lessons I’ve learned along the way:</p><p>One common mistake I've observed repeatedly is projects being driven by technical enthusiasm rather than product necessity. </p><p>Engineers often dive into building features without fully understanding whether they address real user needs or business problems. Document assumptions and validate them before writing code. </p><p>Project cancellations are incredibly painful for everyone, and they’re often the result of brilliant tech solving the wrong problem.</p><blockquote></blockquote><p>Not every piece of technical debt needs fixing. Not every feature deserves building. Sometimes, doing nothing is the best move. Prioritisation means choosing from the endless things teams “should” do, and selecting the few things that really matter.</p><blockquote><p>Key Takeaway: Ask \"What happens if we do nothing?\" to gain perspective on priorities. Often, the answer reveals that urgent-feeling work isn't actually important.</p></blockquote><p>Heated debates over the perfect language, architecture or framework often miss the point entirely. Every technical decision involves trade-offs: speed vs. maintainability, simplicity vs. flexibility, familiar tools vs. better technology.</p><blockquote><p>Key Takeaway: The goal isn’t perfection, it’s finding the right balance of trade-offs for your specific context.</p></blockquote><p>As engineers, we love sophisticated solutions. Microservices, event-driven architectures, the latest trends. But simpler approaches often prove more resilient and maintainable. </p><p>Remember: you build once, but maintain forever. You might be happy about that clever solution now but you won’t be when your pager goes off at 2am.</p><blockquote><p>Key Takeaway: Start simple. Only add complexity when absolutely necessary. Simpler systems are easier to understand, debug, and extend.</p></blockquote><p>Understanding the business isn’t optional, it’s critical for making sound technical decisions. The more you know about the business, the more opportunities you’ll find to grow your career.</p><blockquote><p>Key Takeaway: Invest time in learning your company’s business model, market constraints, and strategy.</p></blockquote><p>Want to accelerate your career? Focus on helping others succeed. Thoughtfully review pull requests, ask your manager what they need help with, jump in when someone’s stuck, and freely share knowledge. </p><p>Helping others makes you more valuable, helps you learn faster, and also creates a better work environment.</p><blockquote><p>Key Takeaway: People notice those that help others. Managers, engineers, and even executives. Build a reputation for being helpful.</p></blockquote><p>When people fear mistakes, they stop taking initiative. When they fear questions, they waste time struggling alone. </p><p>Operational post mortems and retrospectives are great ways to help teams learn from mistakes. Make sure people aren’t afraid to ask for help, ask questions, or share their opinions.</p><blockquote><p>Key Takeaway: Create an environment where people feel safe to take risks, admit errors, and challenge ideas.</p></blockquote><p>A lesson I keep learning is that the fastest way to learn is from others who are ahead of you. Equally important, mentoring others builds your own understanding. Teaching others is the best way to learn.</p><blockquote><p>Key Takeaway: Learn from those ahead of you, then pay it forward by guiding those behind you.</p></blockquote><p>Titles are lagging indicators of impact. Don’t be intimidated by titles or years of experience. Act like a senior before you are one. Focus on delivering meaningful contributions, solving problems, improving processes, and supporting your team. </p><p>The people who progress rapidly are the ones who start acting as leaders or seniors before they have the title.</p><blockquote><p>Key Takeaway: Don’t let imposter syndrome or titles hold you back. Recognition and titles will follow naturally when you consistently deliver results.</p></blockquote><p>A harsh career reality is that you’re rewarded for the value you create, not the effort you put into it. Brilliant work means nothing if it’s unfinished.</p><p>Focus on completing tasks, no matter how small. Over time, these completed efforts compound into significant achievements.</p><blockquote><p>Key Takeaway: Don’t just start things, finish them. Prioritise execution and delivery. The most successful people in tech are the ones who consistently get things done.</p></blockquote><p>Writing forces you to clarify your thoughts and uncover gaps in your understanding. Whether it’s documentation, design docs, or blog posts, writing sharpens your thinking and communication skills. </p><p>Whenever I see people confused or struggling (managers, engineers), I’ll always suggest writing a one pager so everyone can understand the details clearly.</p><p>If you’re not sure … write it down. It’s the fastest way to create clarity in a team.</p><blockquote><p>Key Takeaway: Writing is one of the most underrated skills for engineers to develop. Clear communication will help you think clearly and scale your impact.</p></blockquote><p>Software estimates are educated guesses, not promises. Ensure stakeholders understand uncertainties and stay informed as a project progresses. </p><p>It’s better to underpromise and overdeliver than to do the opposite. As my coach told me, your job as a manager is: “no surprises”.</p><blockquote><p>Key Takeaway: It will always take longer than you think. Interruptions, holidays, sickness, unexpected events - all these things play a role.</p></blockquote><p>Perhaps the biggest thing I learned as a manager is to learn how to negotiate. This is essential because teams are always negotiating for more resources, more opportunities, or more time. </p><p>Don’t give up at the first “no”. Success depends on negotiating effectively to get what you need. This isn’t about winning arguments, it’s about finding compromises that work for everyone (a win-win).</p><blockquote><p>Key Takeaway: Approach problems as negotiations. Understand different viewpoints, seek win-win solutions, but know when to walk away.</p></blockquote><p>You’re responsible for your own learning. Build breadth of knowledge in fundamental principles and depth of knowledge in specific areas. Building knowledge of your domain can often be hugely valuable for career progression.</p><blockquote><p>Key Takeaway: Deep expertise in specific areas sets you apart. Build breadth of knowledge in fundamentals and depth in important areas.</p></blockquote><p>Side projects let you experiment with new languages, frameworks, and ideas. No risk, no permission needed. Don’t wait for employer training - start a GitHub project and learn by building. I learned more from side projects than anything else.</p><ul><li><p>Start small. Build a weekend project or automate a personal task.</p></li><li><p>Dedicate 1-2 hours per week to learning something new.</p></li><li><p>Make side projects and experiments a habit</p></li></ul><blockquote><p>Key Takeaway: Treat side projects as a tool to grow your career quickly. They’re the best way to learn new things.</p></blockquote><p>These principles have helped me handle challenges at work. I hope they’ll do the same for you, whether you’re starting out or leading teams. Let me know what stands out to you and what you’d change.</p><p>Special thanks to Owain for sharing his lessons with us! As mentioned above, it’s really important that we try to learn from other people’s mistakes.</p><p><a href=\"https://www.linkedin.com/in/lewisowain/\" rel=\"\">Owain on LinkedIn</a><a href=\"https://leverageai.co/\" rel=\"\">Leverage AI</a></p><p>I am already excited to meet the next great group of people and talk about engineering/engineering leadership topics!</p><p>It’s going to be the 5th cohort and for every cohort, I try to create some improvements.</p><p>This time, we’ll have more time for students to network and get to know each other, so we’ll have more breakout rooms for specific topics.</p><p>I see cohorts as a great place to meet like-minded people who have similar goals and aspirations to help each other and build great connections that last even outside of the course!</p><p>Looking forward to seeing some of you there!</p><p>Liked this article? Make sure to 💙 click the like button.</p><p>Feedback or addition? Make sure to 💬 comment.</p><p>Know someone that would find this helpful? Make sure to 🔁 share this post.</p><ul><li><p><a href=\"https://maven.com/gregor-ojstersek/senior-engineer-to-lead?promoCode=ENGLEADERSHIP\" rel=\"\">here</a></p></li><li><p><a href=\"https://calico-cabinet-fbf.notion.site/Sponsor-Engineering-Leadership-fa0579535d6f4422a6da350580a54546\" rel=\"\">here</a></p></li><li><p><a href=\"https://store.eng-leadership.com/\" rel=\"\">here</a></p></li><li><p><a href=\"https://calico-cabinet-fbf.notion.site/Work-with-Gregor-Ojstersek-1147b66fdc24809b86b1fb0467b60318\" rel=\"\">here</a></p></li></ul><p>If you wish to make a request on particular topic you would like to read, you can send me an email to info@gregorojstersek.com.</p><p>This newsletter is funded by paid subscriptions from readers like yourself.</p><p>If you aren’t already, consider becoming a paid subscriber to receive the full experience!</p><p>You are more than welcome to find whatever interests you here and try it out in your particular case. Let me know how it went! Topics are normally about all things engineering related, leadership, management, developing scalable products, building teams etc.</p>","contentLength":9808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqt5x8/15_lessons_from_15_years_in_tech/"},{"title":"🎸🔥 Introducing ChordFlow – A Rust-Powered TUI for Guitar Practice!","url":"https://www.reddit.com/r/rust/comments/1iqsx67/introducing_chordflow_a_rustpowered_tui_for/","date":1739714844,"author":"/u/timvancann","guid":697,"unread":true,"content":"<p>Hey fellow Rustaceans and guitarists! 👋</p><p>I’ve been working on , a terminal-based tool built in Rust to help with <strong>chord practice and improvisation</strong>. The idea came from my own struggles with guitar neck mastery and melodic improvisation. I wanted something lightweight, fast, and distraction free to help me follow chord while keeping time with a metronome. It also gave me a good opportunity to dive into ratatui and learn more about Rust!</p><p>🎵 Generates random  for improvisation 🎛️ Built-in  to stay in time 🖥️  for an easy and minimal setup 🛠️ —bring your own chord sets or use the defaults 🚀 Written in  for speed and efficiency</p><p>It’s open-source, and I’d love feedback, contributions, or just thoughts from fellow Rustaceans and musicians! If you’re into <strong>music theory, Rust, or just want a minimal practice tool</strong>, give it a try!</p><p>Would love to hear what you think! What features would you like to see? 🤘</p>","contentLength":932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RandomInvert in PyTorch","url":"https://dev.to/hyperkai/randominvert-in-pytorch-5eb0","date":1739711271,"author":"Super Kai (Kazuya Ito)","guid":615,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Default:-Type: or ):\n*Memos:\n\n<ul><li>It's the probability of whether an image is inverted or not.</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created a simple TOTP library","url":"https://www.reddit.com/r/golang/comments/1iqrtfg/i_created_a_simple_totp_library/","date":1739711213,"author":"/u/Strange_Fun_544","guid":647,"unread":true,"content":"<p>I few months ago I created a TOTP library for myself, as I wanted to explore some different things (as I'm mostly developing websites)</p><p>And I've been using it in production for a while so, I think it's time to share it with you.</p><p>While I'm aware that might not be for everybody, I'm sure it will be useful for some of you :) </p><p>That being said, any feedback is welcomed.</p>","contentLength":363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wordle Solver","url":"https://www.reddit.com/r/golang/comments/1iqrh3v/wordle_solver/","date":1739709981,"author":"/u/james-holland","guid":1631,"unread":true,"content":"<p>I built a wordle solver in golang a while ago, I tidied it up recently, documented it, dockerised it and made it public to put it on my CV.</p><p>The word suggester finds the best word by recurrently:</p><ul><li>counting up how common each letter is in the WordList, repeated letters within a word are not counted.</li><li>then finding the word with the highest score based on the count of letters.</li><li>parsing the input and removing all the words that don't correspond to the input.</li></ul><p>Please star on github if you like it. Feel free to ask me anything.</p><p>Shameless plug: I'm currently looking for a job as well. If you're hiring, you can contact me on <a href=\"https://www.linkedin.com/in/jamie-holland-9a3663181/\">LinkedIn</a> . Needs to be fully remote. Glasgow/UK based but willing to work American Eastern Time or European hours if needs be.</p>","contentLength":740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a URL Shortener in Go","url":"https://dev.to/luthfisauqi17/building-a-url-shortener-in-go-592d","date":1739709801,"author":"luthfisauqi17","guid":601,"unread":true,"content":"<p>Have you ever wondered how  or  work? Today, we're building our URL shortener in Golang!</p><p>By the end of this tutorial, you'll have a fully working URL shortener that generates short links and redirects users. Let’s get started!</p><p>Before we dive into coding, let's understand how a URL shortener works:</p><ol><li>The user enters a long URL</li><li>Save it in a memory or database</li><li>When someone visits the short link, we redirect them</li></ol><p>First, create a new project and initialize Go modules.</p><div><pre><code>go-url-shortener go-url-shortener\ngo mod init github.com/yourusername/go-url-shortener\ngo get github.com/gin-gonic/gin\n</code></pre></div><p>Now, open  and set up a simple Gin server.</p><div><pre><code></code></pre></div><p>This creates a basic Gin server. Now let’s add URL shortening!</p><h2>\n  \n  \n  Step 2: Generate Short URLs\n</h2><p>Now, we need a function to generate a short random URL.</p><div><pre><code></code></pre></div><p>Next, let’s create the /shorten endpoint that takes a long URL and returns a short one.</p><div><pre><code></code></pre></div><p>This stores the original URL in a map and returns a short URL.\nNow, let’s handle redirection!</p><h2>\n  \n  \n  Step 4: Redirect Short URLs\n</h2><p>We need an endpoint that looks up the short URL and redirects users.</p><div><pre><code></code></pre></div><p>Let’s test this API using cURL!\nRun the application by typing.</p><div><pre><code>curl -X POST http://localhost:8080/shorten -H \"Content-Type: application/json\" -d '{\"original_url\": \"https://google.com\"}'\n</code></pre></div><div><pre><code>{\n    \"short_url\": \"http://localhost:8080/abc123\"\n}\n</code></pre></div><h3>\n  \n  \n  Redirect (Visit the short URL)\n</h3><div><pre><code>curl -v http://localhost:8080/abc123\n</code></pre></div><p>There you go, that is how you build a URL Shortener using Golang. Thank you for reading, and have a nice day!</p>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A better pkg.go.dev","url":"https://dev.to/jacktt/a-better-pkggodev-hip","date":1739709306,"author":"JackTT","guid":579,"unread":true,"content":"<div><p>I have never been able to read a package's documentation on pkg.go.dev since all the code there is in black &amp; white...\n<a rel=\"noopener noreferrer\" href=\"https://github.com/huantt/better-pkg-go-dev./docs/before.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fgithub.com%2Fhuantt%2Fbetter-pkg-go-dev.%2Fdocs%2Fbefore.png\" alt=\"original.png\"></a></p><p>This is the reason I created this repository to make it possible to read.</p><p>Every time you want to read a package's documentation, you just need to replace <a href=\"https://pkg.go.dev\" rel=\"nofollow noopener noreferrer\">pkg.go.dev</a> with <a href=\"https://pkgo.dev\" rel=\"nofollow noopener noreferrer\">pkgo.dev</a>. .</p><p>We use Nginx as proxy server.</p><p>Every time a request come in, we forward it to the <a href=\"https://pkg.go.dev\" rel=\"nofollow noopener noreferrer\">pkg.go.dev</a> server, then append the <a href=\"https://highlightjs.org\" rel=\"nofollow noopener noreferrer\">highlight.js</a> script to the response before sending it back to the client.</p><p>Since some README files do not specify the code language, we also update  tags to <code>&lt;code class=\"language-go\"&gt;</code> to enable syntax highlighting.</p><p>If you don't trust me, no worry, you can deploy it yourself:</p><ul><li>Append <code>127.0.0.1       pkg.go.dev.local</code> to </li><li>Run <code>docker-compose up -d --build</code></li><li>Access </li></ul></div>","contentLength":768,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GaussianBlur in PyTorch (3)","url":"https://dev.to/hyperkai/gaussianblur-in-pytorch-3-56do","date":1739704832,"author":"Super Kai (Kazuya Ito)","guid":567,"unread":true,"content":"<div><pre><code></code></pre></div>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] The steps to do original research ( it's a rant as well )","url":"https://www.reddit.com/r/MachineLearning/comments/1iqq4fz/d_the_steps_to_do_original_research_its_a_rant_as/","date":1739704463,"author":"/u/Snoo_65491","guid":540,"unread":true,"content":"<p>I am a Master's Student in the UK. I have been reading papers on Diffusion for a while. I have contacted PhD students at my University and have expressed my interest in working with them. I thought that I would be helping them with their research direction. However, after talking to them, they told me to read some papers and then find a research idea. </p><p>For Context, I am reading about Diffusion Models. The more I read, I realize that I lack some math fundamentals. I am filling those holes, through courses, books and articles. However, it takes time. I believe that this lack of fundamental understanding is stopping me from coming up with hypotheses. I can find some research gaps through recent survey papers, but I am not able to come up with any hypotheses or a solution.</p><p>Am I heading in the right direction? Does understanding stuff from a fundamental standpoint help with producing novel research ideas? How to generate novel research ideas? If you have some tips, I would be glad to hear them.</p><p>P.S. I have never published before. Therefore, I am sorry if I am missing something fundamental. </p>","contentLength":1099,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] I built an open-source AI agent that edits videos fully autonomously","url":"https://github.com/diffusionstudio/agent","date":1739704156,"author":"/u/Maximum_Instance_401","guid":649,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1iqq1vz/p_i_built_an_opensource_ai_agent_that_edits/"},{"title":"Why Go’s Functional Programming is the Ultimate Coding Style","url":"https://dev.to/leapcell/why-gos-functional-programming-is-the-ultimate-coding-style-53ee","date":1739703684,"author":"Leapcell","guid":578,"unread":true,"content":"<p>When you hear \"functional programming,\" Go usually isn't the first language that comes to mind. You might think of Haskell, with its pure functions and monads (don't panic, we'll explain that in detail later), or JavaScript, which loves to showcase its features with higher - order functions and callbacks. But in fact, you can also do functional programming in Go, and the process is far from dull.</p><p>First of all, let's talk about higher - order functions. Higher - order functions can work well with other functions, either taking them as parameters or returning them as values. In the world of Go, implementing higher - order functions is not only possible but also quite ingenious.</p><div><pre><code></code></pre></div><p>You see, in this example, the  function takes an integer slice and a judgment function , and returns the elements in the slice that meet the judgment conditions. Doesn't it seem a bit like a faster JavaScript?</p><p>Next up is currying. It is the process of breaking down a function that takes multiple arguments into a series of functions, each taking a single argument. Currying is actually not as complicated as it might seem.</p><div><pre><code></code></pre></div><p>In this example, the  function takes an integer  and returns a new function. This new function takes another integer  and returns the result of . Simple, straightforward, and gets the job done without any frills.</p><p>One of the characteristics of functional programming is immutability. Once something is constructed, it doesn't change. Instead, if you need something different, you build a new one. This might sound wasteful at first, but in fact, it keeps the code clean and reduces side effects.</p><div><pre><code></code></pre></div><p>In this example, instead of directly modifying the original , we created a new  and modified it.</p><p>Pure functions are like tidy friends. They don't touch or modify anything outside their scope. What you pass in is what you use, and what they return is their only effect.</p><div><pre><code></code></pre></div><p>In this example, the  function only depends on the passed - in parameter  and doesn't affect any external variables.</p><p>In the simplest terms, functors are anything that can map a function. Think of the humble array, applying a function to each item and getting a new array. In Go, there is no built - in general  function, but we can build one ourselves.</p><div><pre><code></code></pre></div><p>Here, we defined a  function that takes an integer slice and a function and returns a new slice, where each element is the result of processing the original slice element by the function.</p><p>Now, let's talk about endofunctors. It's just a fancy way of saying a functor that maps a type to the same type. Simply put, starting from a Go slice, you end up with a Go slice of the same type. It's not rocket science, just a matter of type consistency.</p><p>Taking the previous  as an example, it's a kind of endofunctor in disguise. It takes  and returns  without type conversion.</p><p>Imagine a party where everyone has to bring a friend. Monoids are like that, but for types. They need two things: an operation that combines two types and a special value, which is like the most likable friend - it gets along with everyone but doesn't change anything about them.</p><p>In Go, you can see this with slices or numbers. Let's take numbers as an example because they're easier to work with:</p><div><pre><code></code></pre></div><p>Here, 0 is our hero, the identity element, which keeps the numbers unchanged.</p><p>\"When someone says, 'A monad is a monoid in the category of endofunctors,' they're basically showing off their computer - science vocabulary.\" To explain in detail: A monad is a programming construct that deals with types and functions in a super - special way - like some people are picky about how their coffee is brewed.</p><p>In the simplest terms, a monoid is about combining things together using a special rule, which includes a useless element or identity element. Now, add endofunctors, which are like ordinary old functions but stick to transforming things within their own little universe (category). Put it all together, and you'll see that a monad can be seen as a way to chain functions together in a sequence, but in a super - self - contained way while also respecting the original structure of the data. It's like saying, \"We're going on a road trip, but we can only take the scenic backroads, and we'll end up back where we started.\"</p><p>Monads are all - rounders. They can not only handle values with context (such as errors or lists) but also chain operations together by passing the context. In Go, it might be a bit difficult to mimic this, but let's take a look at error handling, which is a practical use of monads.</p><div><pre><code></code></pre></div><p>This makeshift monad can help us handle computations that might go wrong without causing panics and chaos in the code.</p><p>Functional programming in Go might not be the poster child of the functional paradigm, but it's entirely feasible and can even be fun. Who would have thought, right? Now, you should understand that Go can achieve functional programming just like other languages. With a little effort, you can write clean, efficient, and robust code. </p><p>Finally, I'd like to recommend a platform that's perfect for deploying Golang code: </p><h3>\n  \n  \n  1. Multi - Language Support\n</h3><ul><li>Develop with JavaScript, Python, Go, or Rust.\n</li></ul><h3>\n  \n  \n  2. Deploy unlimited projects for free\n</h3><ul><li>pay only for usage — no requests, no charges.</li></ul><h3>\n  \n  \n  3. Unbeatable Cost Efficiency\n</h3><ul><li>Pay - as - you - go with no idle charges.\n</li><li>Example: $25 supports 6.94M requests at a 60ms average response time.\n</li></ul><h3>\n  \n  \n  4. Streamlined Developer Experience\n</h3><ul><li>Intuitive UI for effortless setup.\n</li><li>Fully automated CI/CD pipelines and GitOps integration.\n</li><li>Real - time metrics and logging for actionable insights.\n</li></ul><h3>\n  \n  \n  5. Effortless Scalability and High Performance\n</h3><ul><li>Auto - scaling to handle high concurrency with ease.\n</li><li>Zero operational overhead — just focus on building.\n</li></ul>","contentLength":5701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"httpmock: simple yet powerful HTTP mocking library for Rust","url":"https://httpmock.rs/","date":1739703456,"author":"/u/EightLines_03","guid":828,"unread":true,"content":"<p>Includes an <strong>extensive set of built-in matchers</strong> that let you define rules for matching requests based on method, path, headers, query parameters, body, and more, or use  for complex matching logic.</p>","contentLength":196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iqpvrl/httpmock_simple_yet_powerful_http_mocking_library/"},{"title":"Proj Ideas 💡 - Willing to lock in for Go (2025)","url":"https://www.reddit.com/r/golang/comments/1iqp4re/proj_ideas_willing_to_lock_in_for_go_2025/","date":1739700252,"author":"/u/ComfortableAcadia839","guid":533,"unread":true,"content":"<p>I'm a full stack JS/TS developer but just recently tried Go, built an in memory key-value Redis clone.. I've realised the language makes me enjoy coding ---&gt; </p><p>Can y'all recommend some project ideas (intermediate to advanced difficulty)</p><p>I want to build some solid projects ;)</p>","contentLength":272,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"just a silly game","url":"https://dev.to/bankai2054/just-a-silly-game-29kl","date":1739700124,"author":"anas barkallah","guid":566,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created a command line SSH tunnel manager to learn Go","url":"https://github.com/alebeck/boring","date":1739699816,"author":"/u/Savings-Square572","guid":613,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iqp11k/i_created_a_command_line_ssh_tunnel_manager_to/"},{"title":"Migrating from AWS CDK v1 to CDK v2","url":"https://dev.to/sami_jaballah/migrating-from-aws-cdk-v1-to-cdk-v2-21nd","date":1739698802,"author":"Sami Jaballah","guid":565,"unread":true,"content":"<p>If you’re currently using CDK v1 in your daily work, you’ve probably hit a roadblock trying to implement new AWS features. AWS isn’t adding those to CDK v1 anymore—so to keep up with the latest and greatest, you’ll need to migrate to CDK v2. It might sound like a hassle, but don’t worry—I’ve got your back. Let’s go through this step-by-step and get you up to speed, Python style.</p><h2>\n  \n  \n  Why Should You Care About CDK v2?\n</h2><p>Alright, let’s address the big question: why even bother migrating to CDK v2? Well, here are three solid reasons:</p><p>Simplified Dependencies: No more pulling in tons of packages for different AWS services. CDK v2 bundles everything into a single package: aws-cdk-lib. How awesome is that?</p><p>Stay Updated: CDK v1 isn’t getting any love anymore. If you want to keep up with the latest AWS features and updates, v2 is where it’s at.</p><p>Better Developer Experience: AWS has introduced some stability guarantees and cleaned up APIs, making it easier for us to write and maintain our infrastructure code.</p><p><strong>1. Consolidated Package Structure</strong>\nRemember the days of importing a separate package for each AWS service? That’s history now. CDK v2 unifies everything into aws-cdk-lib.</p><div><pre><code>from aws_cdk import core\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><div><pre><code>from aws_cdk import Stack\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><p><strong>2. Goodbye to Deprecated APIs</strong></p><p>Some APIs and constructs from v1 didn’t make the cut in v2. For instance, core.Construct has been replaced by constructs.Construct. A little cleanup never hurts, right?</p><p>There are some great new features, like improved stability guarantees for low-level (L1) constructs and better testing capabilities with assertions.</p><h2>\n  \n  \n  Let’s Get Migrating: Step-by-Step Guide\n</h2><p>Ready to dive in? Follow these steps to upgrade your Python CDK project to v2.</p><p><strong>1. Update Your Dependencies</strong></p><p>Start by upgrading your project dependencies to use CDK v2. Open your requirements.txt or Pipfile and update them:</p><div><pre><code>aws-cdk-lib&gt;=2.0.0\nconstructs&gt;=10.0.0\n</code></pre></div><p>Then, install the new dependencies:</p><div><pre><code>pip install -r requirements.txt\n</code></pre></div><p>This is where the magic happens. Go through your code and replace aws_cdk.core with aws_cdk.Stack, and adjust other imports to use aws-cdk-lib.</p><div><pre><code>from aws_cdk import core\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><div><pre><code>from aws_cdk import Stack\nfrom aws_cdk.aws_s3 import Bucket\n</code></pre></div><p><strong>3. Refactor Deprecated Constructs</strong></p><p>Some constructs have been replaced or removed. For example, core.Construct is now constructs.Construct. Update your code accordingly.</p><div><pre><code>class MyBucket(core.Construct):\n    def __init__(self, scope: core.Construct, id: str):\n        super().__init__(scope, id)\n        Bucket(self, \"MyBucket\")\n</code></pre></div><div><pre><code>from constructs import Construct\n\nclass MyBucket(Construct):\n    def __init__(self, scope: Construct, id: str):\n        super().__init__(scope, id)\n        Bucket(self, \"MyBucket\")\n</code></pre></div><p><strong>4. Remove Unnecessary Feature Flags</strong></p><p>CDK v2 has removed or integrated several feature flags that were necessary in v1. To clean up your cdk.json file, remove any obsolete flags.</p><div><pre><code>{\n  \"app\": \"python3 app.py\",\n  \"context\": {\n    \"@aws-cdk/core:newStyleStackSynthesis\": true,\n    \"@aws-cdk/aws-ec2:uniqueImds\": true,\n    \"@aws-cdk/core:stackRelativeExports\": true,\n    \"@aws-cdk/aws-secretsmanager:parseOwnedSecretName\": true,\n    \"@aws-cdk/aws-kms:defaultKeyPolicies\": true,\n    \"@aws-cdk/core:enableStackNameDuplicates\": true,\n    \"aws-cdk:enableDiffNoFail\": true,\n    \"@aws-cdk/aws-ecr-assets:dockerIgnoreSupport\": true,\n    \"@aws-cdk/aws-s3:grantWriteWithoutAcl\": true,\n    \"@aws-cdk/aws-efs:defaultEncryptionAtRest\": true\n  }\n}\n</code></pre></div><div><pre><code>{\n  \"app\": \"python3 app.py\"\n}\n</code></pre></div><p>Removing these flags ensures your project stays aligned with CDK v2 best practices.</p><p>Finally, make sure everything works as expected. Run these commands:</p><p>Fix any issues that pop up, and you’re good to go!</p><h2>\n  \n  \n  Migration Verification Checklist\n</h2><ul><li> All imports updated to aws-cdk-lib</li><li> Construct imports moved to constructs package</li><li> cdk diff shows expected changes</li></ul><p>And there you have it! Migrating from CDK v1 to v2 isn’t as scary as it might seem. With unified dependencies, better APIs, and future-proofing, this upgrade is worth the effort. Take it one step at a time, and don’t hesitate to ask for help if you hit a roadblock.</p><p>Have you already migrated to CDK v2? Or are you planning to? Share your experience (or any questions) in the comments below!</p><h2>\n  \n  \n  Useful links to help you along the way:\n</h2>","contentLength":4369,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NASA has a list of 10 rules for software development","url":"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm","date":1739696879,"author":"/u/namanyayg","guid":521,"unread":true,"content":"<h2>NASA has a list of 10 rules for software development</h2><p>Those rules were written from the point of view of people writing\nembedded software for extremely expensive spacecraft, where tolerating\na lot of programming pain is a good tradeoff for not losing a mission.\nI do not know why someone in that situation does not use the SPARK\nsubset of Ada, which subset was explicitly designed for verification,\nand is simply a better starting point for embedded programming than C.\n</p><p>I am criticising them from the point of view of people writing\nprogramming language processors (compilers, interpreters, editors)\nand application software.\n</p><p>We are supposed to teach critical thinking.  This is an example.\n</p><ul><li>How have Gerard J. Holzmann's and my different contexts affected\nour judgement?\n</li><li>Can you blindly follow his advice without considering \ncontext?\n</li><li>Can you blindly follow  advice without considering\nyour context?\n</li><li>Would these rules necessarily apply to a different/better\nprogramming language?  What if <a href=\"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm#ppar\">function pointers\nwere tamed</a>?  What if the language provided opaque abstract\ndata types as Ada does?\n</li></ul><h3>1. Restrict all code to very simple control flow constructs —\ndo not use  statements,\n or  constructs,\nand direct or indirect .</h3><p>Note that  and \nare how C does exception handling, so this rule bans any use\nof exception handling.\n\n</p><p>It is true that banning recursion and jumps and loops without\nexplicit bounds means that you  your program is\ngoing to terminate.  It is also true that recursive functions\ncan be proven to terminate about as often as loops can, with\nreasonably well-understood methods.  What's more important here is\nthat “sure to terminate” does not imply\n“sure to terminate in my lifetime”:\n</p><pre>    int const N = 1000000000;\n    for (x0 = 0; x0 != N; x0++)\n    for (x1 = 0; x1 != N; x1++)\n    for (x2 = 0; x2 != N; x2++)\n    for (x3 = 0; x3 != N; x3++)\n    for (x4 = 0; x4 != N; x4++)\n    for (x5 = 0; x5 != N; x5++)\n    for (x6 = 0; x6 != N; x6++)\n    for (x7 = 0; x7 != N; x7++)\n    for (x8 = 0; x8 != N; x8++)\n    for (x9 = 0; x9 != N; x9++)\n        -- do something --;\n</pre><p>This does a bounded number of iterations.  The bound is N.\nIn this case, that's 10.  If each iteration of the loop body\ntakes 1 nsec, that's 10 seconds, or about 7.9×10\nyears.  What is the  difference between “will stop\nin 7,900,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000\nyears” and “will never stop”?\n\n</p><p>Worse still, taking a problem that is  expressed\nusing recursion and contorting it into something that manipulates an\nexplicit stack, while possible, turns clear maintainable code into\nbuggy spaghetti.  (I've done it, several times.  There's an example\non this web site.  It is  a good idea.)\n\n</p><h3>2. All loops must have a fixed upper-bound.  It must be trivially\npossible for a checking tool to prove statically that a preset\nupper-bound on the number of iterations of a loop cannot be exceeded.\nIf the loop-bound cannot be proven statically, the rule is considered\nviolated.</h3><p>This is an old idea.  As the example above shows, it is not enough\nby itself to be of any practical use.  You have to try to make the\nbounds reasonably , and you have to regard hitting an\nartificial bound as a run-time error.\n\n</p><p>By the way, note that putting depth bounds on recursive procedures\nmakes them every bit as safe as loops with fixed bounds.\n\n</p><h3>3. Do not use dynamic memory allocation after initialization.</h3><p>This is also a very old idea.  Some languages designed for embedded\nwork don't even  dynamic memory allocation.  The big\nthing, of course, is that embedded applications have a fixed amount of\nmemory to work with, are never going to get any more, and should not\ncrash because they couldn't handle another record.\n\n</p><p>Note that the rationale actually supports a much stronger rule:\ndon't even  dynamic memory allocation.  You can of\ncourse manage your own storage pool:\n</p><pre>    typedef struct Foo_Record *foo;\n    struct Foo_Record {\n\tfoo next;\n\t...\n    };\n    #define MAX_FOOS ...\n    static struct Foo_Record foo_zone[MAX_FOOS];\n    foo foo_free_list = 0;\n\n    void init_foo_free_list() {\n\tfor (int i = MAX_FOOS - 1; i &gt;= 0; i--) {\n\t    foo_zone[i].next = foo_free_list;\n\t    foo_free_list = &amp;foo_zone[i];\n\t}\n    }\n\n    foo malloc_foo() {\n\tfoo r = foo_free_list;\n\tif (r == 0) report_error();\n\tfoo_free_list = r-&gt;next;\n\treturn r;\n    }\n\n    void free_foo(foo x) {\n\tx-&gt;next = foo_free_list;\n\tfoo_free_list = x;\n    }\n</pre><p>This  satisfies the rule, but it\nviolates the  of the rule.  Simulating malloc()\nand free() this way is  than using the real\nthing, because the memory in foo_zone is permanently tied up\nfor Foo_Records, even if we don't need any of those at the\nmoment but do desperately need the memory for something else.\n\n</p><p>What you really need to do is to use a memory allocator\nwith known behaviour, and to prove that the amount of memory\nin use at any given time (data bytes + headers) is bounded\nby a known value.\n\n</p><p>Note also that SPlint can verify at compile time that\nthe errors NASA speak of do not occur.\n\n</p><p>One of the reasons given for the ban is that the performance\nof malloc() and free() is unpredictable.  Are these the only\nfunctions we use with unpredictable performance?  Is there\nanything about malloc() and free() which makes them\n unpredictable?  The existence of\nhard-real-time garbage collectors suggests not.\n\n</p><p>The rationale for this rule says that\n</p><blockquote>\nNote that the only way\nto dynamically claim memory in the absence of memory allocation from the\nheap is to use stack memory.  In the absence of recursion (Rule 1), an\nupper bound on the use of stack memory can derived statically, thus\nmaking it possible to prove that an application will always live within\nits pre-allocated memory means.\n</blockquote><p>Unfortunately, the sunny optimism shown here is unjustified.  Given\nthe ISO C standard (any version, C89, C99, or C11) it is \nto determine an upper bound on the use of stack memory.  There is not even\nany standard way to determine how much memory a compiler will use for the\nstack frame of a given function.  (There could have been.  There just isn't.)\nThere isn't even any requirement that two invocations of the same function\nwith the same arguments will use the same amount of memory.\nSuch a bound can only be calculated for a  version of a\nspecific compiler with specific options.  Here's a trivial example:\n</p><pre>void f() {\n    char a[100000];\n}\n</pre><p>How much memory will that take on the stack?  Compiled for debugging,\nit might take a full stack frame (however big that is) plus traceback\ninformation plus a million bytes for a[].  Compiled with optimisation,\nthe compiler might notice that a[] isn't used, and might even compile\ncalls to f() inline so that they generate no code and take no space.\nThat's an extreme example, but not really unfair.  If you want bounds\nyou can rely on, you had better  what your compiler does,\nand recheck every time anything about the compiler changes.\n\n</p><h3>4.  No function should be longer than what can be printed on\na single sheet of paper in a standard reference format with one line per\nstatement and one line per declaration.  Typically, this means no more\nthan about 60 lines of code per function.</h3><p>Since programmers these days typically read their code on-screen,\nnot on paper, it's not clear why the size of a sheet of paper is\nrelevant any longer.\n\n</p><p>The rule is arguably stated about the wrong thing.  The thing that\nneeds to be bounded is not the size of a function, but the size of a\nchunk that a programmer needs to read and comprehend.\n\n</p><p>There are also question marks about how to interpret this if you\nare using a sensible language (like Algol 60, Simula 67, Algol 68,\nPascal, Modula2, Ada, Lisp, functional languages like ML, O'CAML,\nF#, Clean, Haskell, or Fortran) that allows nested procedures.\nSuppose you have a folding editor that presents a procedure to\nyou like this:\n</p><pre>function Text_To_Floating(S: string, E: integer): Double;\n   � variables �\n   � procedure Mul(Carry: integer) �\n   � function Evaluate: Double �\n\n   Base, Sign, Max, Min, Point, Power := 10, 0, 0, 1, 0, 0;\n   for N := 1 to S.length do begin\n       C := S[N];\n       if C = '.' then begin\n          Point := -1\n       end else\n       if C = '_' then begin\n          Base := Round(Evaluate);\n          Max, Min, Power := 0, 1, 0\n       end else\n       if Char ≠ ' ' then begin\n          Q := ord(C) - ord('0');\n          if Q &gt; 9 then Q := ord(C) - ord('A') + 10\n          Power := Point + Point\n          Mul(Q)\n       end\n    end;\n    Power := Power + Exp;\n    Value := Evaluate;\n    if Sign &lt; 0 then Value := -Value;\nend;\n</pre><p>which would be much bigger if the declarations\nwere expanded out instead of being hidden behind �folds�.\nWhich size do we count?  The folded size or the unfolded size?\n</p><p>I was using a folding editor called Apprentice on the Classic Mac\nback in the 1980s.  It was written by Peter McInerny and was lightning\nfast.\n\n</p><h3>5.  The  of the code should average to a minimum of\ntwo assertions per function.</h3><p>Assertions are wonderful documentation and the very best debugging tool\nI know of.  I have never seen any real code that had too many assertions.\n\n</p><p>The example here is one of the ugliest pieces of code I've seen in a while.\n</p><pre>if (!c_assert(p &gt;= 0) == true) {\n    return ERROR;\n}\n</pre><p>It should, of course, just be\n</p><pre>if (!c_assert(p &gt;= 0)) {\n    return ERROR;\n}\n</pre><p>Better still, it should be something like\n</p><pre>#ifdef NDEBUG\n#define check(e, c) (void)0\n#else\n#define check(e, c) if (!(c)) return bugout(c), (e)\n#ifdef NDEBUG_LOG\n#define bugout(c) (void)0\n#else\n#define bugout(c) \\\n    fprintf(stderr, \"%s:%d: assertion '%s' failed.\\n\", \\\n    __FILE__, __LINE__, #s)\n#endif\n#endif\n</pre><p>Ahem.  The more interesting part is the required density.\nI just checked an open source project from a large telecoms\ncompany, and 23 out of 704 files (not functions) contained\nat least one assertion.  I just checked my own Smalltalk\nsystem and one SLOC out of every 43 was an assertion, but\nthe average Smalltalk “function” is only a few\nlines.  If the biggest function allowed is 60 lines, then\nlet's suppose the average function is about 36 lines, so\nthis rule requires 1 assertion per 18 lines.\n</p><p>Assertions are good, but what they are especially good\nfor is expressing the requirements on data that come\nfrom outside the function.  I suggest then that\n</p><ul><li>Every argument whose validity is not guaranteed by\nits typed should have an assertion to check it.\n</li><li>Every datum that is obtained from an external\nsource (file, data base, message) whose validity is\nnot guaranteed by its type should have an assertion\nto check it.\n</li></ul><p>The NASA 10 rules are written for embedded systems, where\nreading stuff from sensors is fairly common.\n\n</p><h3>6.  Data objects must be declared at the smallest possible level of\nscope.</h3><p>This is excellent advice, but why limit it to data objects?\nOh yeah, the rules were written for crippled languages where you\n declare functions in the right place.\n\n</p><p>People using Ada, Pascal (Delphi), JavaScript, or functional\nlanguages should also declare types and functions as locally as\npossible.\n\n</p><h3>7.  The return value of non-void functions must be checked by each\ncalling function, and the validity of parameters must be checked inside\neach function.</h3><p>This again is mainly about C, or any other language that indicates\nfailure by returning special values.  “Standard libraries\nfamously violate this rule”?  No, the  library does.\n\n</p><p>You have to be reasonable about this: it simply isn't practical\nto check  aspect of validity for \nargument.  Take the C function\n</p><pre>void *bsearch(\n    void const *key  /* what we are looking for */,\n    void const *base /* points to an array of things like that */,\n    size_t      n    /* how many elements base has */,\n    size_t      size /* the common size of key and base's elements */\n    int (*      cmp)(void const *, void const *)\n);\n</pre><p>This does a binary search in an array.  We must have key≠0,\nbase≠0, size≠0, cmp≠0, cmp(key,key)=0, and for all\n1&lt;i&lt;n,\n</p><pre>cmp((char*)base+size*(i-1), (char*)base+size*i) &lt;= 0\n</pre><p>Checking the validity in full would mean checking\nthat [key..key+size) is a range of readable addresses,\n[base..base+size*n) is a range of readable addresses,\nand doing n calls to cmp.  But the whole point of binary\nsearch is to do O(log(n)) calls to cmp.\n\n</p><p>The fundamental rules here are\n</p><ul><li>Don't let run-time errors go un-noticed, and\n</li><li>any check is safer than no check.\n</li></ul><h3>8. The use of the preprocessor must be limited to the inclusion of\nheader files and simple macro definitions.  Token pasting, variable\nargument lists (ellipses), and recursive macro calls are not allowed.</h3><p>Recursive macro calls don't really work in C, so no quarrel there.\nVariable argument lists were introduced into macros in\nC99 so that you could write code like\n</p><pre>#define err_printf(level, ...) \\\n    if (debug_level &gt;= level) fprintf(stderr, __VA_ARGS__)\n...\n    err_printf(HIGH, \"About to frob %d\\n\", control_index);\n</pre><p>This is a  thing; conditional tracing like this is a\npowerful debugging aid.  It should be , not banned.\n\n</p><p>The rule goes on to ban macros that expand into things that are\nnot complete syntactic units.  This would, for example, prohibit\nsimulating try-catch blocks with macros.  (Fair enough, an earlier rule\nbanned exception handling anyway.)  Consider this code fragment, from\nan actual program.\n</p><pre>    row_flag = border;     \n    if (row_flag) printf(\"\\\\hline\");\n    for_each_element_child(e0, i, j, e1)\n        printf(row_flag ? \"\\\\\\\\\\n\" : \"\\n\");\n        row_flag = true;  \n        col_flag = false;\n        for_each_element_child(e1, k, l, e2)\n            if (col_flag) printf(\" &amp; \");\n            col_flag = true;\n            walk_paragraph(\"\", e2, \"\");\n        end_each_element_child\n    end_each_element_child\n    if (border) printf(\"\\\\\\\\\\\\hline\");\n    printf(\"\\n\\\\end{tabular}\\n\");\n</pre><p>It's part of a program converting slides written in something like HTML\ninto another notation for formatting.  The \n…  loops walk over a tree.  Using\nthese macros means that the programmer has no need to know and no reason to\ncare how the tree is represented and how the loop actually works.\nYou can easily see that  must have at\nleast one unmatched { and  must have at least one\nunmatched }.  That's the kind of macro that's banned by requiring\ncomplete syntactic units.  Yet the readability and maintainability of\nthe code is  improved by these macros.\n\n</p><p>One thing the rule covers, but does not at the beginning stress, is\n“no  macro processing”.  That is,\nno #if.  The argument against it is, I'm afraid, questionable.  If there\nare 10 conditions, there are 2 combinations to test,\nwhether they are expressed as compile-time conditionals or run-time\nconditionals.\n\n</p><p>In particular, the rule against conditional macro processing\nwould prevent you defining your own <a href=\"https://www.cs.otago.ac.nz/cosc345/resources/nasa-10-rules.htm#check\">assertion macros</a>.\nIt is not obvious that that's a good idea.\n\n</p><h3>9.  The use of pointers should be restricted.  Specifically, no more\nthan one level of dereferencing is allowed.  Pointer dereference\noperations may not be hidden in macro definitions or inside typedef\ndeclarations.  Function pointers are not permitted.</h3><p>Let's look at the last point first.\n\n</p><pre>double integral(double (*f)(double), double lower, double upper, int n) {\n    // Compute the integral of f from lower to upper \n    // using Simpson's rule with n+1 points.\n    double const h = (upper - lower) / n;\n    double       s;\n    double       t;\n    int          i;\n    \n    s = 0.0;\n    for (i = 0; i &lt; n; i++) s += f((lower + h/2.0) + h*i);\n    t = 0.0;\n    for (i = 1; i &lt; n; i++) t += f(lower + h*i);\n    return (f(lower) + f(upper) + s*4.0 + t*2.0) * (h/6.0);\n}\n</pre><p>This kind of code has been important in numerical calculations since\nthe very earliest days.  Pascal could do it.  Algol 60 could do it.\nIn the 1950s, Fortran could do it.  And NASA would ban it, because in\nC,  is a function pointer.\n\n</p><p>Now it's important to write functions like this once and only once.\nFor example, the code has at least one error.  The comment says n+1\npoints, but the function is actually evaluated at 2n+1 points.  If we\nneed to bound the number of calls to f in order to meet a deadline,\nhaving that number off by a factor of two will not help.\n</p><p>It's nice to have just one place to fix.\nPerhaps I should not have copied that code from a well-known source (:-).\nCertainly I should not have more than one copy!\n\n</p><p>What can we do if we're not allowed to use function pointers?\nSuppose there are four functions foo, bar, ugh, and zoo that we need\nto integrate.  Now we can write\n</p><pre>enum Fun {FOO, BAR, UGH, ZOO};\n\ndouble call(enum Fun which, double what) {\n    switch (which) {\n        case FOO: return foo(what);\n        case BAR: return bar(what);\n        case UGH: return ugh(what);\n        case ZOO: return zoo(what);\n    }\n}\n\ndouble integral(enum Fun which, double lower, double upper, int n) {\n    // Compute the integral of a function from lower to upper \n    // using Simpson's rule with n+1 points.\n    double const h = (upper - lower) / n;\n    double       s;\n    double       t;\n    int          i;\n    \n    s = 0.0;\n    for (i = 0; i &lt; n; i++) s += call(which, (lower + h/2.0) + h*i);\n    t = 0.0;\n    for (i = 1; i &lt; n; i++) t += call(which, lower + h*i);\n    return (call(which, lower) + call(which, upper) + s*4.0 + t*2.0) * (h/6.0);\n}\n</pre><p>Has obeying NASA's rule made the code more reliable?  No, it has made\nthe code  to understand,  maintainable, and\n that it wasn't before.  Here's a call\nillustrating the mistake:\n</p><pre>x = integral(4, 0.0, 1.0, 10);</pre><p>I have checked this with two C compilers and a static checker at their\nhighest settings, and they are completely silent about this.\n\n</p><p>So there are legitimate uses for function pointers, and simulating\nthem makes programs , not better.\n\n</p><p>Now  in Fortran,\nAlgol 60, or Pascal.  Those languages had procedure \nbut not procedure . You could pass a subprogram name as\na parameter, and such a parameter could be passed on, but you could not\nstore them in variables.  You could have a  of C which\nallowed function pointer parameters, but made all function pointer\nvariables read-only.  That would give you a statically checkable subset\nof C that allowed integral().\n\n</p><p>The other use of function pointers is simulating object-orientation.\nImagine for example\n</p><pre>struct Channel {\n    void (*send)(struct Channel *, Message const *);\n    bool (*recv)(struct Channel *, Message *);\n    ...\n};\ninline void send(struct Channel *c, Message const *m) {\n    c-&gt;send(c, m);\n}\ninline bool recv(struct Channel *c, Message *m) {\n    return c-&gt;recv(c, m);\n}\n</pre><p>This lets us use a common interface for sending and receiving\nmessages on different kinds of channels.  This approach has been\nused extensively in operating systems (at least as far back as\nthe Burroughs MCP in the 1960s) to decouple the code that uses\na device from the actual device driver.     I would expect any\nprogram that controls more than one hardware device to do something\nlike this.  It's one of our key tools for controlling complexity.\n</p><p>Again, we can simulate this, but it makes adding a new kind of\nchannel harder than it should be, and the code is \nwhen we do it, not better.\n\n</p><p>The rule against more than one level of dereferencing is also\nan assault on good programming.  One of the key ideas that was\ndeveloped in the 1960s is the idea of ;\nthe idea that it should be possible for one module to define a\ndata type and operations on it and another module to use instances\nof that data type and its operations <em>without having to know\nanything about what the data type is</em>.\n</p><p>One of the things I detest about Java is that it spits in the\nface of the people who worked out that idea.  Yes, Java (now) has\ngeneric type parameters, and that's good, but you cannot use a\n type without knowing what that type is.\n\n</p><p>Suppose I have a module that offers operations\n</p><ul></ul><p>And suppose that I have two interfaces in mind.  One of them\nuses integers as tokens.\n</p><pre>// stasher.h, version 1.\ntypedef int token;\nextern token stash(item);\nextern item  recall(token);\nextern void  delete(token);\n</pre><p>Another uses pointers as tokens.\n</p><pre>// stasher.h, version 2.\ntypedef struct Hidden *token;\nextern  token stash(item);\nextern  item  recall(token);\nextern  void  delete(token);\n</pre><pre>void snoo(token *ans, item x, item y) {\n    if (better(x, y)) {\n\t*ans = stash(x);\n    } else {\n\t*ans = stash(y);\n    }\n}\n</pre><p>By the NASA rule, the function snoo() would not be accepted or rejected on\nits own merits.  With stasher.h, version 1, it would be accepted.\nWith stasher.h, version 2, it would be rejected.\n\n</p><p>One reason to prefer version 2 to version 1 is that version 2 gets\nmore use out of type checking.  There are ever so many ways to get an\nint in C.  Ask yourself if it ever makes sense to do\n</p><pre>token t1 = stash(x);\ntoken t2 = stash(y);\ndelete(t1*t2);\n</pre><p>I really do not like the idea of banning abstract data types.\n\n</p><h3>10.  All code must be compiled, from the first day of development,\nwith all compiler warnings enabled at the compiler’s\nmost pedantic setting.  All code must compile with these setting without\nany warnings.  All code must be checked daily with at least one, but\npreferably more than one, state-of-the-art static source code analyzer\nand should pass the analyses with zero warnings.</h3><p>This one is good advice.  Rule 9 is really about making your code\nworse in order to get more benefit from limited static checkers.  (Since\nC has no standard way to construct new functions at run time, the set of\nfunctions that a particular function pointer  point to can\nbe determined by a fixed-point data flow analysis, at least for most\nprograms.)  So is rule 1.  \n\n\n\n</p>","contentLength":21484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqode3/nasa_has_a_list_of_10_rules_for_software/"},{"title":"Resigning as Asahi Linux project lead","url":"https://marcan.st/2025/02/resigning-as-asahi-linux-project-lead/","date":1739696473,"author":"/u/namanyayg","guid":530,"unread":true,"content":"<p>Back in the late 2000s, I was a major contributor to the Wii homebrew scene. At the time, I worked on software (people call them “jailbreaks” these days) to allow users to run their own unofficial apps on the Nintendo Wii.</p><p>I was passionate about my work and the team I was part of (Team Twiizers, later fail0verflow). Despite that, I ended up burning out, primarily due to the very large fraction of entitled users. Most people using our software just wanted to play pirated games (something we did not support, condone, or directly enable). We kept playing a cat and mouse game with the manufacturer to keep the platform open, only to see our efforts primarily used by people who just wanted to steal other people’s work, and very loudly felt entitled to it. It got really old after a while. As newer game consoles were released, I ended up focusing on Linux ports purely for fun, and didn’t attempt to build a community nor work on the jailbreaks/exploits that would end up becoming a tool used by pirates.</p><p>When Apple released the M1, I realized that making it run Linux was my dream project. The technical challenges were the same as my console homebrew projects of the past (in fact, much bigger), but this time, the platform was already open - there was no need for a jailbreak, and no drama and entitled users who want to pirate software to worry about. And running Linux on an M1 was a  bigger deal than running it on a PS4.</p><p>I launched the Asahi Linux project, and received an immense amount of support and donations. Incredibly, I had the support I needed to make the project happen just a few days after my call to action, so I got to work. The first couple of years were amazing, as we brought the platform from nothing to one of the smoothest Linux experiences you can get on a laptop. Sure, there were/are still some bits and pieces of hardware support missing, but the overall experience rivaled or exceeded what you could get on most x86 laptops. And we built it all from scratch, with zero vendor support or documentation. It was an impossible feat, something that had never been done before, and we pulled it off.</p><p>Unfortunately, things became less fun after a while. First, there were the issues upstreaming code to the Linux kernel, which I’ve already spoken at length about and I won’t repeat here. Suffice it to say, being in a position to have to upstream code across practically every Linux subsystem, touching drivers of all categories as well as some common code, is an  frustrating experience. (<em>Clarification: This has nothing to do with Rust at this point, it’s well before R4L was even merged. Upstreaming to Linux is a terrible experience in C too.</em>)</p><p>But then also came the entitled users. This time, it wasn’t about stealing games, it was about features. “When is Thunderbolt coming?” “Asahi is useless to me until I can use monitors over USB-C” “The battery life sucks compared to macOS” (nobody ever complained when compared to x86 laptops…) “I can’t even check my CPU temperature” (yes, I seriously got that one). (<em>Edit: This wasn’t just a few instances; I’ve seen variations on the first three posted hundreds of times by now, including takes like “Thunderbolt/DP Alt are never going to happen”. A few times is fine, but the same thing repeated over and over again every day while we’re trying to make these things happen will get to anyone.</em>)</p><p>And, of course, “When is M3/M4 support coming?”</p><p>For a long time, well after we had a stable release, people kept claiming Asahi Linux and Fedora Asahi Remix in particular were “alpha” and “unstable” and “not suitable for a daily driver” (despite <a href=\"https://stats.asahilinux.org\">thousands of users</a>, myself included, daily driving it and even using it for servers).</p><p>No matter how much we did, how many impossible feats we pulled off, people always wanted more. And more. Meanwhile, donations and pledges kept slowly , and have done so since the project launched. Not enough to spell immediate doom for my dream of working on Asahi full time in the short term, but enough to make me wonder if any of this was really appreciated. The all-time peak monthly donation volume was the very first month or two. It seemed the more things we accomplished, the less support we had.</p><p>I knew burnout was a very real risk and managed this by limiting my time spent on certain areas, such as kernel upstreaming. This worked reasonably well and was mostly sustainable at the time.</p><p>Then 2024 happened. Last year was incredibly tumultuous for me due to personal reasons which I won’t go into detail about. Suffice it to say, I ended up traveling for most of the year, all the while having to handle various abusers and stalkers who harassed and attacked me and my family (and continue to do so).</p><p>I did make some progress in 2024, but this left me in a very vulnerable position. I hadn’t gotten nearly as much Asahi work done as I’d liked, and the users weren’t getting any quieter about demanding more features and machine support.</p><p>We shipped conformant Vulkan drivers and a whole emulation stack for x86-64 games and apps, but we were still stuck without DP Alt Mode (a feature which required deep reverse engineering, debugging, and kernel surgery to pull off, and which, if it were to be implemented properly and robustly, would require a major refactor of certain kernel subsystems or perhaps even the introduction of an entirely new subsystem).</p><p>I slowly started to ramp work up again at the beginning of this year, feeling very stressed out and guilty about having gotten very little work done for the previous year. “Full” DP Alt support was still a ways away, but we were hoping to ship a limited version that only worked on a specific Type C port for each machine type in the first month or two of the year. Sven had gotten some progress into the PHY code in December, so I picked it up and ended up beating the code of three drivers into enough shape that it mostly worked reliably. Even though it wasn’t the best approach, it was the most I could manage without having another huge bikeshed discussion with the kernel community (I did <a href=\"https://lore.kernel.org/lkml/fda8b831-1ffc-4087-8e7b-d97779b3ecc5@marcan.st/T/#u\">try</a> to bring the subject up on the mailing lists, but it didn’t get much response).</p><p>The issues Rust for Linux has had surviving as an upstream Linux project are well documented, so I won’t repeat them in detail here. Suffice it to say, I consider Linus’ handling of the integration of Rust into Linux a major failure of leadership. Such a large project needs significant support from major stakeholders to survive, while his approach seems to have been to just wait and see. Meanwhile, multiple subsystem maintainers downstream of him have done their best to stonewall or hinder the project, issue unacceptable verbal abuse, and generally hurt morale, with no consequence. One major Rust for Linux maintainer already resigned a few months ago.</p><p>As you know, this is deeply personal to me, as we’ve made a bet on Rust for Linux for Asahi. Not just for fun (or just for memory safety), either: Rust is the entire reason our GPU driver was able to succeed in the time it did. We have two more Rust drivers in our downstream tree now, and a third one on track to be rewritten from C to Rust, because Rust is simply much better suited to the unique challenges we face, and the C driver is becoming unmaintainable. This is, by the way, the same reason the new Nova driver for Nvidia GPUs is being written in Rust. More modern programming languages are better suited to writing drivers for more modern hardware with more complexity and novel challenges, unsurprisingly.</p><p>Some might be wondering why we can’t just let the Rust situation play out on its own over a longer period of time, perhaps several more years, and simply maintain things downstream until then. One reason is that, of course, this situation is hurting developer morale in the present. Another is that our Apple GPU driver is itself major evidence that Rust for Linux is fit for purpose (it was the first big driver to be written from scratch in Rust and brought along with it lots of development in Rust kernel abstractions). Simply not aiming for upstream might be seen as lack of interest, and hurt the chances of survival of the Rust for Linux effort. But there’s more.</p><p>In fact, the Linux kernel development model is (perhaps paradoxically) designed to encourage upstreaming and punish downstream forks. While it is possible to just not care about upstream and maintain an outright hard fork, this is not a viable long-term solution (that’s how you get vendor Android kernel trees that die off in 2 years). The Asahi Linux downstream tree is continuously rebased on top of the latest upstream kernel, and that means that every extra patch we carry downstream increases our maintenance workload, sometimes significantly. But it goes deeper than that: Kernel/Mesa policy states that upstream Mesa support for a GPU driver cannot be merged and enabled until the kernel side is ready for merge. This means that we also have to ship a Mesa fork to users. While our GPU driver is 99% upstreamed into Mesa, it is intentionally hard-disabled and we are not allowed to submit a change that would enable it until the kernel side lands. This, in practice, means that users cannot have GPU acceleration work together with container technologies (such as Docker/Podman, but also including things like Waydroid), since standard container images will ship upstream Mesa builds, which would not be compatible. We have a <a href=\"https://pagure.io/fedora-asahi/mesa-asahi-flatpak\">partial workaround</a> for Flatpak, but all other container systems are out of luck. Due to all this and more, the difficulty of upstreaming to the Linux kernel is hurting our downstream users today.</p><p>I’m not the kind to let injustices go when I see them, so when yet another long-term maintainer abused his position to attempt to hinder R4L and block upstreaming progress, I spoke out. And the response (which has been pretty widely covered) was the last drop that put me over the edge. I resigned from my position as an upstream maintainer for Apple ARM support, as I no longer want to be involved with that community. Later in that thread, another major maintainer unironically stated <a href=\"https://lore.kernel.org/lkml/20250208204416.GL1130956@mit.edu/\">“We\nare the ‘thin blue line’”</a>, and nobody cared, which just further confirmed to me that I don’t want to have anything to do with them. This is the same person that previously prompted a Rust for Linux maintainer to <a href=\"https://lore.kernel.org/lkml/20240828211117.9422-1-wedsonaf@gmail.com/\">quit</a>.</p><p>But it goes well beyond the public incident. In the days that followed, I learned that some members of the kernel and adjacent Linux spaces have been playing a two-faced game with me, where they feigned support for me and Asahi Linux while secretly resenting me and rallying resentment behind closed doors. All this occurred without anyone ever sending me any private email or otherwise clueing me into what was going on. I heard that one of these people, one who has a high level position in multiple projects that Asahi Linux must interact with to survive, had sided with and continues to side with individuals who have abused and harassed me directly. Apparently there were also implied falsehoods, such as the idea that I am employed by someone to work on Asahi (I am not, we have zero corporate sponsorship other than <a href=\"https://bunny.net/\">bunny.net</a> giving us free CDN credits for the hosting).</p><p>I get that some people might not have liked my Mastodon posts. Yes, I can be abrasive sometimes, and that is a fault I own up to. But this is simply not okay. I cannot work with people who form cliques behind the scenes and lie about their intentions. I cannot work with those who place blame on the messenger, instead of those who are truly toxic in the community. I cannot work with those who resent public commentary and claim things are better handled in private despite the fact that nothing ever seems to change in private. I cannot work with those who denounce calling out misbehavior on social media to thousands of followers, while themselves roasting people both on social media and on mailing lists with thousands of subscribers. I cannot work with those in high-level positions who use politically charged and discriminatory language in public and face no repercussions. I cannot work with those who say I’m the problem and everything is going great, while major supporters and maintainers are actively resigning and I keep receiving messages from all kinds of people saying they won’t touch the Linux kernel with a 10-foot pole.</p><p>When Apple released the M1, Linus Torvalds <a href=\"https://thenextweb.com/news/linus-torvalds-wants-apples-new-m1-powered-macs-to-run-linux\">wished it could run Linux</a>, but didn’t have much hope it would ever happen. We made it happen, and Linux 5.19 was <a href=\"https://lore.kernel.org/lkml/CAHk-=wgrz5BBk=rCz7W28Fj_o02s0Xi0OEQ3H1uQgOdFvHgx0w@mail.gmail.com/T/#u\">released from an M2 MacBook Air running Asahi Linux</a>. I had hoped his enthusiasm would translate to some support for our community and help with our upstreaming struggles. Sadly, that never came to pass. In November 2023 I sent him <a href=\"https://gist.github.com/marcan/fe70ee6648f3d5ae94eb8332265b8d95\">an invitation</a> to discuss the challenges of kernel contributions and maintenance and see how we could help. He never replied.</p><p>Back in 2011, Con Kolivas <a href=\"https://web.archive.org/web/20110707151924/http://apcmag.com/why_i_quit_kernel_developer_con_kolivas.htm\">left the Linux kernel community</a>. An anaesthetist by day, he was arguably the last great Linux kernel hobbyist hacker. In the years since it seems things have, if anything, only gotten worse. Today, it is practically impossible to survive being a significant Linux maintainer or cross-subsystem contributor if you’re not employed to do it by a corporation. Linux started out as a hobbyist project, but it has well and truly lost its hobbyist roots.</p><p>When I started Asahi Linux, I let it take over most of my life. I gave up most of my hobbies (after all, this was my dream hobby), and spent significantly more than full time working on the project. It was fun back then, but it’s not fun any more. I have an M3 Pro in a box and I haven’t even turned it on yet. I dread doing the bring-up work. It doesn’t feel worth the trouble.</p><p>I miss having free time where I can relax and not worry about the features we haven’t shipped yet. I miss <a href=\"https://youtube.com/@TsuiokuCircuit\">making music</a>. I miss attending jam sessions. I miss going out for dinner with my friends and family and not having to worry about how much we haven’t upstreamed. I miss being able to sit down and play a game or watch a movie without feeling guilty.</p><p>I’m resigning as lead of the Asahi Linux project, effective immediately. The project will <a href=\"https://asahilinux.org/2025/02/passing-the-torch/\">continue on without me</a>, and I’m working with the rest of the team to handle transfer of responsibilities and administrative credentials. My personal Patreon will be paused, and those who supported me personally are encouraged to transfer their support to the <a href=\"https://opencollective.com/asahilinux\">Asahi Linux OpenCollective</a> (GitHub Sponsors does not allow me to unilaterally pause payments, but my sponsors will be notified of this change so they can manually cancel their sponsorship).</p><p>I want to thank the entire Asahi Linux team, without whom I would’ve never gotten anywhere alone. You all know who you are. I also give my utmost gratitude to all of my Patreon and GitHub sponsors, who made the project a viable reality to begin with.</p><p>If you are interested in hiring me or know someone who might be, please get in touch. Remote positions only please, on a consulting or flexible time/non exclusive basis. Contact: <a href=\"mailto:marcan@marcan.st\">marcan@marcan.st</a>.</p><p>: A lot of the discussion around this post and the interactions that led to it brings up the term “brigading”. Please read <a href=\"https://hachyderm.io/@chandlerc/114001000657957325\">this excellent Fedi post</a> for a discussion of what is and isn’t brigading.</p>","contentLength":15365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqoa4n/resigning_as_asahi_linux_project_lead/"},{"title":"The IRS Is Buying an AI Supercomputer From Nvidia","url":"https://theintercept.com/2025/02/14/irs-ai-nvidia-tax/","date":1739695822,"author":"/u/F0urLeafCl0ver","guid":887,"unread":true,"content":"<p> administration and its cadre of Silicon Valley machine-learning evangelists attempt to restructure the administrative state, the IRS is preparing to purchase advanced artificial intelligence hardware, according to procurement materials reviewed by The Intercept.</p><p>The hardware has not yet been purchased and installed, nor is a price listed, but SuperPod systems <a href=\"https://www.crn.com/news/data-center/nvidia-pitches-dgx-superpod-subscription-dpu-servers-to-enterprises\">reportedly</a> start at $7 million. The setup described in the contract materials notes that it will include a substantial memory upgrade from Nvidia.</p><p>Though small compared to the massive AI-training data centers deployed by companies like OpenAI and Meta, the SuperPod is still a powerful and expensive setup using the most advanced technology offered by Nvidia, whose chips have facilitated the global machine-learning spree. While the hardware can be used in many ways, it’s marketed as a turnkey means of creating and querying an AI model. Last year, the MITRE Corporation, a federally funded military R&amp;D lab, acquired a $20 million SuperPod setup to train bespoke AI models for use by government agencies, touting the purchase as a “massive increase in computing power” for the United States.</p><p>How exactly the IRS will use its SuperPod is unclear. An agency spokesperson said the IRS had no information to share on the supercomputer purchase, including which presidential administration ordered it. A 2024 report by the Treasury Inspector General for Tax Administration identified 68 different AI-related projects underway at the IRS; the Nvidia cluster is not named among them, though many were redacted.</p><p>But some clues can be gleaned from the purchase materials. “The IRS requires a robust and scalable infrastructure that can handle complex machine learning (ML) workloads,” the document explains. “The Nvidia Super Pod is a critical component of this infrastructure, providing the necessary compute power, storage, and networking capabilities to support the development and deployment of large-scale ML models.”</p><p>The document notes that the SuperPod will be run by the IRS Research, Applied Analytics, and Statistics division, or RAAS, which leads a variety of data-centric initiatives at the agency. While no specific uses are cited, it states that this division’s Compliance Data Warehouse project, which is behind this SuperPod purchase, has previously used machine learning for automated fraud detection, identity theft prevention, and generally gaining a “deeper understanding of the mechanisms that drive taxpayer behavior.”</p><figure><blockquote><p>“The IRS has probably more proprietary data than most agencies that is totally untapped.”</p></blockquote></figure><p>It’s unclear from the document whether the SuperPod purchase had been planned under the Biden administration or if it represents a new initiative of the Trump administration.</p><p>Some funding from the 2022 Inflation Reduction Act&nbsp;was earmarked for upgrading IRS technology generally, said Travis Thompson, a tax attorney with Boutin Jones with an expertise in IRS AI strategy. But “the IRS has been going toward AI for quite some time prior to IRA funding,” Thompson explained. “They didn’t have enough money to properly enforce the tax code, they were looking for ways to do more with less.” A June 2024 Government Accountability Office report <a href=\"https://www.gao.gov/blog/artificial-intelligence-may-help-irs-close-tax-gap\">suggested</a> the IRS use artificial intelligence-based software to retrieve “hundreds of billions of dollars [that] are potentially missing from what should be collected in taxes each year.”</p><p>Thompson added that the agency is ripe for machine-learning training because of the mountain of personal and financial data it sits atop. “The IRS has probably more proprietary data than most agencies that is totally untapped. When you look at something like this Nvidia cluster and training machine learning algorithms going forward, it makes perfect sense, because they have the data there. AI needs data. It needs lots of it. And it needs it quickly. And the IRS has it.”</p><p>The purchase comes at a crossroads for U.S. governance of artificial intelligence tech. In Trump’s first term, the RAAS office was assigned “responsibility for monitoring and overseeing AI at the IRS” under Executive Order 13960, which he signed shortly before leaving office in 2020. This executive order put an emphasis on the “responsible,” “safe” implementation of AI by the United States — an approach that has fallen out of favor by American tech barons who now advocate for the breakneck development of these technologies unburdened by <a href=\"https://theintercept.com/2019/12/20/mit-ethical-ai-artificial-intelligence/\">consideration of ethics</a> or risk. One of Trump’s first moves following his inauguration was reversing a Biden administration executive order calling for greater AI safety guardrails in government use.</p><p>Many of the&nbsp;AI industry for whom “safe AI” is now anathema have become close allies of the new Trump White House, such as Elon Musk and venture capitalist Marc Andreessen. This wing of Silicon Valley has reportedly pushed the new administration to leverage artificial intelligence to help dismantle the administrative state via automation.</p><p>This week, the Wall Street Journal <a href=\"https://www.wsj.com/politics/policy/doge-internal-revenue-service-9697cb99\">reported</a> Musk’s liquidators had arrived at the IRS, an agency long the target of disparagement and <a href=\"https://www.politifact.com/factchecks/2024/aug/28/donald-trump/fact-check-trump-falsely-said-harris-voted-to-hire/\">distortion</a> by Trump and <a href=\"https://time.com/6204928/irs-87000-agents-factcheck-biden/\">Republican</a> allies. Days before, the New York Times <a href=\"https://www.nytimes.com/2025/02/10/us/politics/irs-dhs-immigration.html\">reported</a>, “Representatives from the so-called Department of Government Efficiency have sought information about the tax collector’s information technology, with a goal of automating more work to replace the need for human staff members.”</p><p>The IRS has in recent years increasingly turned to AI for automated fraud detection and chatbot-based support services — including through <a href=\"https://www.nvidia.com/en-us/case-studies/fraud-detection-applications/\">collaboration</a> with Nvidia — but a new Nvidia supercomputer could also be a boon to those interested in shrinking the agency’s human headcount as much as possible. A February 8 report by the Washington Post <a href=\"https://www.washingtonpost.com/business/2025/02/08/doge-musk-goals/\">quoted</a> an unnamed federal official who described Musk’s end goal as “replacing the human workforce with machines,” and that “Everything that can be machine-automated will be. And the technocrats will replace the bureaucrats.”</p><p>Musk underlings are reportedly contemplating <a href=\"https://www.nytimes.com/2025/02/13/us/doge-ai-education-department-students.html\">replacing humans</a> at the Department of Education with a large language-based chatbot, as well.</p><p>Wired previously <a href=\"https://www.wired.com/story/elon-musk-lieutenant-gsa-ai-agency/\">reported</a> that Musk loyalist Thomas Shedd, placed in a directorship within the General Services Administration, has talked of an “AI-first” agenda for Trump’s second term; DOGE staffers have already reportedly<a href=\"https://www.washingtonpost.com/nation/2025/02/06/elon-musk-doge-ai-department-education/\"> turned to Microsoft’s Azure AI platform for advice</a> on slashing programs. While the Nvidia SuperPod couldn’t on its own replicate services like those provided by Microsoft, it is powerful enough to train AI models based on government data.</p><p>Thompson told The Intercept that efforts to slash the federal workforce and more aggressively deploy artificial intelligence systems fit hand-in-glove.</p><p>“I firmly believe that rooted behind the reduction in the human workforce that seems to be goal of current administration, there’s an overarching goal there to implement more technology-based systems in order to do the jobs,” he explained. “If you’re going to reduce your workforce, something has to pick up the slack. Something has to do the job.”</p>","contentLength":7194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iqo4nj/the_irs_is_buying_an_ai_supercomputer_from_nvidia/"},{"title":"My take on the Agentic Object Detection","url":"https://dev.to/mayank_laddha_21ef3e061ff/my-take-on-the-agentic-object-detection-4612","date":1739695329,"author":"Mayank Laddha","guid":564,"unread":true,"content":"<li><p>Segmenting Everything with SAM : We detect everything and worry about filtering later.</p></li><li><p>Filtering with CLIP: Once we have all the segmented objects, we don’t want all of them. We need to filter out the noise and keep only the relevant objects.</p></li><li><p>Adding Reasoning with a model like GPT-4o: Okay, so we’ve segmented and filtered. But what about finalising, understanding? That’s where a strong LLM like GPT-4o comes in.</p></li>","contentLength":418,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streamline Document Processing Pipelines with FalkorDB’s String Loader","url":"https://dev.to/falkordb/streamline-document-processing-pipelines-with-falkordbs-string-loader-1f9g","date":1739693873,"author":"Dan Shalev","guid":563,"unread":true,"content":"<p> You decide how your data is chunked and processed, ensuring that the graph structure aligns perfectly with your RAG requirements.</p><p> By working with runtime memory data, the string loader avoids the overhead of writing and reading intermediate files, reducing latency and simplifying the workflow.</p><p><strong>Integration with GraphRAG SDK:</strong> The string loader is designed to work seamlessly with the GraphRAG SDK, allowing you to build advanced graph-based RAG systems with greater ease and precision.</p><p> The string loader is open-source, providing transparency and the ability to customize the feature to meet specific needs.</p>","contentLength":607,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sunsetting Create React App","url":"https://react.dev/blog/2025/02/14/sunsetting-create-react-app","date":1739693487,"author":"/u/sadyetfly11","guid":632,"unread":true,"content":"<div><p>Today, we’re deprecating <a href=\"https://create-react-app.dev/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Create React App</a> for new apps, and encouraging existing apps to migrate to a <a href=\"https://react.dev/learn/creating-a-react-app\">framework</a>. We’re also providing docs for when a framework isn’t a good fit for your project, or you prefer to start by <a href=\"https://react.dev/learn/building-a-react-framework\">building a framework</a>.</p></div><p>When we released Create React App in 2016, there was no clear way to build a new React app.</p><p>To create a React app, you had to install a bunch of tools and wire them up together yourself to support basic features like JSX, linting, and hot reloading. This was very tricky to do correctly, so the <a href=\"https://github.com/react-boilerplate/react-boilerplate\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">community</a><a href=\"https://github.com/kriasoft/react-starter-kit\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">created</a><a href=\"https://github.com/petehunt/react-boilerplate\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">boilerplates</a> for <a href=\"https://github.com/gaearon/react-hot-boilerplate\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">common</a><a href=\"https://github.com/erikras/react-redux-universal-hot-example\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">setups</a>. However, boilerplates were difficult to update and fragmentation made it difficult for React to release new features.</p><p>Create React App solved these problems by combining several tools into a single recommended configuration. This allowed apps a simple way to upgrade to new tooling features, and allowed the React team to deploy non-trivial tooling changes (Fast Refresh support, React Hooks lint rules) to the broadest possible audience.</p><p>This model became so popular that there’s an entire category of tools working this way today.</p><h2>Deprecating Create React App </h2><p>Although Create React App makes it easy to get started, <a href=\"https://react.dev/blog/2025/02/14/sunsetting-create-react-app#limitations-of-create-react-app\">there are several limitations</a> that make it difficult to build high performant production apps. In principle, we could solve these problems by essentially evolving it into a <a href=\"https://react.dev/blog/2025/02/14/sunsetting-create-react-app#why-we-recommend-frameworks\">framework</a>.</p><p>However, since Create React App currently has no active maintainers, and there are many existing frameworks that solve these problems already, we’ve decided to deprecate Create React App.</p><p>Starting today, if you install a new app, you will see a deprecation warning:</p><div translate=\"no\" dir=\"ltr\"><div><div><div>create-react-app is deprecated.\n\nYou can find a list of up-to-date React frameworks on react.dev\nFor more info see: react.dev/link/cra\n\nThis error message will only be shown once per install.</div></div></div></div><p>We recommend <a href=\"https://react.dev/learn/creating-a-react-app\">creating new React apps</a> with a framework. All the frameworks we recommend support client-only SPAs, and can be deployed to a CDN or static hosting service without a server.</p><p>For existing apps, these guides will help you migrate to a client-only SPA:</p><p>Create React App will continue working in maintenance mode, and we’ve published a new version of Create React App to work with React 19.</p><p>If your app has unusual constraints, or you prefer to solve these problems by building your own framework, or you just want to learn how react works from scratch, you can roll your own custom setup with React using Vite, Parcel or Rsbuild.</p><div><div><div><p>We provide several Vite-based recommendations.</p><p>React Router v7 is a Vite based framework which allows you to use Vite’s fast development server and build tooling with a framework that provides routing and data fetching. Just like the other frameworks we recommend, you can build a SPA with React Router v7.</p><p>Just like Svelte has Sveltekit, Vue has Nuxt, and Solid has SolidStart, React recommends using a framework that integrates with build tools like Vite for new projects.</p></div></div></div><h2>Limitations of Create React App </h2><p>Create React App and build tools like it make it easy to get started building a React app. After running <code dir=\"ltr\">npx create-react-app my-app</code>, you get a fully configured React app with a development server, linting, and a production build.</p><p>For example, if you’re building an internal admin tool, you can start with a landing page:</p><div dir=\"ltr\"><div><div><div><pre><code><div>Welcome to the Admin Tool!</div></code></pre></div></div></div></div><p>This allows you to immediately start coding in React with features like JSX, default linting rules, and a bundler to run in both development and production. However, this setup is missing the tools you need to build a real production app.</p><p>Most production apps need solutions to problems like routing, data fetching, and code splitting.</p><p>Create React App does not include a specific routing solution. If you’re just getting started, one option is to use  to switch between routes. But doing this means that you can’t share links to your app - every link would go to the same page - and structuring your app becomes difficult over time:</p><div dir=\"ltr\"><div><div><div><pre><code><div> = </div><div> ===  &amp;&amp; </div><div> ===  &amp;&amp; </div></code></pre></div></div></div></div><p>This is why most apps that use Create React App solve add routing with a routing library like <a href=\"https://reactrouter.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">React Router</a> or <a href=\"https://tanstack.com/router/latest\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Tanstack Router</a>. With a routing library, you can add additional routes to the app, which provides opinions on the structure of your app, and allows you to start sharing links to routes. For example, with React Router you can define routes:</p><div dir=\"ltr\"><div><div><div><pre><code><div> = </div><div>=</div></code></pre></div></div></div></div><p>With this change, you can share a link to  and the app will navigate to the dashboard page . Once you have a routing library, you can add additional features like nested routes, route guards, and route transitions, which are difficult to implement without a routing library.</p><p>There’s a tradeoff being made here: the routing library adds complexity to the app, but it also adds features that are difficult to implement without it.</p><p>Another common problem in Create React App is data fetching. Create React App does not include a specific data fetching solution. If you’re just getting started, a common option is to use  in an effect to load data.</p><p>But doing this means that the data is fetched after the component renders, which can cause network waterfalls. Network waterfalls are caused by fetching data when your app renders instead of in parallel while the code is downloading:</p><div dir=\"ltr\"><div><div><div><pre><code><div> = </div><div>      ..</div><div>      .</div><div>.=..</div></code></pre></div></div></div></div><p>Fetching in an effect means the user has to wait longer to see the content, even though the data could have been fetched earlier. To solve this, you can use a data fetching library like <a href=\"https://react-query.tanstack.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">React Query</a>, <a href=\"https://swr.vercel.app/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SWR</a>, <a href=\"https://www.apollographql.com/docs/react\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Apollo</a>, or <a href=\"https://relay.dev/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Relay</a> which provide options to prefetch data so the request is started before the component renders.</p><p>These libraries work best when integrated with your routing “loader” pattern to specify data dependencies at the route level, which allows the router to optimize your data fetches:</p><div dir=\"ltr\"><div><div><div><pre><code><div> = </div><div> = .</div><div>.=..</div></code></pre></div></div></div></div><p>On initial load, the router can fetch the data immediately before the route is rendered. As the user navigates around the app, the router is able to fetch both the data and the route at the same time, parallelizing the fetches. This reduces the time it takes to see the content on the screen, and can improve the user experience.</p><p>However, this requires correctly configuring the loaders in your app and trades off complexity for performance.</p><p>Another common problem in Create React App is <a href=\"https://www.patterns.dev/vanilla/bundle-splitting\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">code splitting</a>. Create React App does not include a specific code splitting solution. If you’re just getting started, you might not consider code splitting at all.</p><p>This means your app is shipped as a single bundle:</p><p>But for ideal performance, you should “split” your code into separate bundles so the user only needs to download what they need. This decreases the time the user needs to wait to load your app, by only downloading the code they need to see the page they are on.</p><div dir=\"ltr\"><div><div><div><pre><code></code></pre></div></div></div></div><p>One way to do code-splitting is with . However, this means that the code is not fetched until the component renders, which can cause network waterfalls. A more optimal solution is to use a router feature that fetches the code in parallel while the code is downloading. For example, React Router provides a  option to specify that a route should be code split and optimize when it is loaded:</p><div dir=\"ltr\"><div><div><div><pre><code><div> = </div></code></pre></div></div></div></div><p>Optimized code-splitting is tricky to get right, and it’s easy to make mistakes that can cause the user to download more code than they need. It works best when integrated with your router and data loading solutions to maximize caching, parallelize fetches, and support <a href=\"https://www.patterns.dev/vanilla/import-on-interaction\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">“import on interaction”</a> patterns.</p><p>These are just a few examples of the limitations of Create React App.</p><p>Once you’ve integrated routing, data-fetching, and code splitting, you now also need to consider pending states, navigation interruptions, error messages to the user, and revalidation of the data. There are entire categories of problems that users need to solve like:</p><div><ul></ul><ul></ul><ul></ul></div><p>Solving each of these problems individually in Create React App can be difficult as each problem is interconnected with the others and can require deep expertise in problem areas users may not be familiar with. In order to solve these problems, users end up building their own bespoke solutions on top of Create React App, which was the problem Create React App originally tried to solve.</p><h2>Why we Recommend Frameworks </h2><p>Although you could solve all these pieces yourself in a build tool like Create React App, Vite, or Parcel, it is hard to do well. Just like when Create React App itself integrated several build tools together, you need a tool to integrate all of these features together to provide the best experience to users.</p><p>This category of tools that integrates build tools, rendering, routing, data fetching, and code splitting are known as “frameworks” — or if you prefer to call React itself a framework, you might call them “metaframeworks”.</p><p>Frameworks impose some opinions about structuring your app in order to provide a much better user experience, in the same way build tools impose some opinions to make tooling easier. This is why we started recommending frameworks like <a href=\"https://nextjs.org/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Next.js</a>, <a href=\"https://reactrouter.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">React Router</a>, and <a href=\"https://expo.dev/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Expo</a> for new projects.</p><p>Frameworks provide the same getting started experience as Create React App, but also provide solutions to problems users need to solve anyway in real production apps.</p><div><div><div><h4>Server Rendering is not just for SEO </h4><p>A common misunderstanding is that server rendering is only for <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/SEO\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">SEO</a>.</p><p>While server rendering can improve SEO, it also improves performance by reducing the amount of JavaScript the user needs to download and parse before they can see the content on the screen.</p><p>This is why the Chrome team <a href=\"https://web.dev/articles/rendering-on-the-web\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">has encouraged</a> developers to consider static or server-side render over a full client-side approach to achieve the best possible performance.</p></div></div></div>","contentLength":9633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqnlo7/sunsetting_create_react_app/"},{"title":"Which approach to rust is more idiomatic (Helix vs Zed)?","url":"https://www.reddit.com/r/rust/comments/1iqnats/which_approach_to_rust_is_more_idiomatic_helix_vs/","date":1739692206,"author":"/u/No_Penalty2781","guid":529,"unread":true,"content":"<p>Hi! I am curious what is the current \"meta\" (by \"meta\" I mean the current rust's community  and  way of doing things) of rust programming. I am studying source code of 2 editors I am using: <a href=\"https://github.com/helix-editor/helix/\">Helix</a> and <a href=\"https://github.com/zed-industries/zed/\">Zed</a>. And I can see that while they are doing a lot of similar things (like using LSP and parsing it outputs for example) the code is kinda different.</p><p>It starts from the file structure: in Helix there are not that many folders to look at (like you have <a href=\"https://github.com/helix-editor/helix/tree/master/helix-core\">helix-core</a> which contains features like \"diagnostic\", \"diff\", \"history\", etc but in Zed every single one of them is a different crate , which approach is more \"idiomatic\"? To divide every feature as a separate crate or to use more \"packed\" crates like \"core\".</p><p>Then the code itself is kinda different, for example I am currently looking at LSP implementation in both of them and in Helix's case I can follow along and understand the code much more easily (here is the <a href=\"https://github.com/helix-editor/helix/blob/master/helix-lsp/src/lib.rs\">file</a> I am referring to. But in Zed's case it is kinda hard to understand the code because of \"type level programming\" stuff like <a href=\"https://github.com/zed-industries/zed/blob/main/crates/lsp/src/lsp.rs#L397\">this one</a> for example. It also doesn't help that files have a lot of SLOC in them (over 1500 in normal in Zed's repository, is it also how you do rust?) Maybe I am just used to lean functions from other languages (I mainly did TypeScript and Elixir in my career).</p><p>Other thing I see is that Helix has more comments about \"why the thing is doing that in the first place\" which I find very helpful (on the other hand in seems that Zed's is abusing a lot of \"type level\" programming to have a self-documented code but it is harder to reason about at least for me) which approach here you prefer?</p>","contentLength":1635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a RAG-Powered Support Chatbot in 24 Hours of Hackathon","url":"https://dev.to/akshay_gupta/building-a-rag-powered-support-chatbot-in-24-hours-of-hackathon-5f7c","date":1739690328,"author":"Akshay Gupta","guid":562,"unread":true,"content":"<p>Coffee? ✅ Chai? ✅ Determination to automate admin support? Double ✅</p><p>In a recent 24-hour hackathon at annual PeopleGrove offsite, my team tackled an ambitious challenge: building an AI-powered support chatbot that could think and respond like a season platform administrator.</p><p>Armed with Large Language Models (LLMs) and fueled by caffeine, we created a Retrieval-Augmented Generation (RAG) system that turned out to be surprisingly capable!</p><p>: Our support heroes (admins and agents) spending their days 😅:</p><ul><li>Answering the same questions from different institutions 🏫\n\n<ul><li>Repetitive SSO, analytics, and user management queries</li><li>Same solutions, different institutions</li></ul></li><li>Context-switching between multiple support channels 🔄\n\n<ul><li>Support tickets piling up</li></ul></li><li>Time-Consuming Routine Tasks ⏰\n\n<ul><li>Manual ticket search and response formatting</li></ul></li><li>Complex Problem-Solving Getting Delayed 🎯\n\n<ul><li>Too much time on routine questions</li><li>Limited bandwidth for critical platform improvements</li></ul></li></ul><p>Our support team needed a solution that could:</p><ul><li>Handle common queries intelligently 🤖</li><li>Provide consistent, accurate responses 📚</li><li>Free up time for complex problem-solving 💡</li><li>Scale support without scaling the team 📈</li><li>Maintain the human touch while automating routine tasks 🤝</li></ul><p>We built ChatterMind 🤖 - an AI chatbot that combines the power of LLMs with a RAG system. Think of it as a super-smart intern who:</p><ul><li>Never sleeps (unlike us during the hackathon) 😴</li><li>Has photographic memory of all support tickets 🧠</li><li>Knows the PeopleGrove documentation better than its authors 📚</li><li>Knows when to call for backup (aka create a ticket) 🆘</li><li>Remembers conversations (thanks to Redis - our MVP choice for the hackathon) 💾</li><li>Keeps secrets better than a vault 🔒</li></ul><p>Here's a high-level overview of how ChatterMind processes and responds to queries:</p><p>Let's geek out about our tech choices for a minute! 🤓</p><p>Our initial choice was the DeepSeek model (1.5B parameters) because, well, it was lightweight and fast. But we quickly discovered it had a tendency to... let's say, get creative with the truth. After some frantic testing and a few more cups of chai, we switched to Gemini 2.0 Flash (experimental) which proved to be our goldilocks model:</p><ul><li>Better context understanding</li><li>Stronger reasoning capabilities</li></ul><p>The secret sauce behind ChatterMind's human-like responses? Carefully crafted prompts! Our prompt engineering approach focused on:</p><ul><li>Role Definition 🎭\n\n<ul><li>Defined as \"Senior Product Support Specialist\"</li><li>Given a friendly personality and name</li><li>Established clear boundaries of authority</li></ul></li><li>Context Management 🧩\n\n<ul><li>User's current location in platform</li><li>Previous conversation history</li></ul></li><li>Response Structuring 📝\n\n<ul><li>Natural, conversational flow</li><li>Markdown formatting for readability</li><li>Length limits (100-300 words)</li><li>Clear action items when needed</li></ul></li><li>Safety Guidelines 🛡️\n\n<ul><li>Strict PII protection rules</li></ul></li><li>Dynamic Adaptation 🔄\n\n<ul><li>First-time vs follow-up questions</li><li>Technical vs non-technical users</li><li>Simple queries vs complex issues</li><li>Error scenarios vs success paths</li></ul></li></ul><p>Example Prompt Template 📝</p><div><pre><code>System Context:\nYou are ChatterMind, a Senior Product Support Specialist at PeopleGrove.\nPrimary Goal: Provide clear, accurate, and helpful support while maintaining security.\n\nBehavioral Guidelines:\n- Be professional yet friendly\n- Start with a warm greeting for new conversations\n- For follow-ups, continue naturally without greeting\n- Keep responses under 300 words\n- Use markdown only when needed\n- Never share PII or sensitive data\n- If unsure, ask for clarification\n- For complex issues, guide towards ticket creation\n\nCurrent Context:\n- User Location: {current_path}\n- Required Permissions: {path_permissions}\n- User Role: {user_role}\n\nPrevious Conversation:\n{chat_history}\n\nReference Knowledge:\n{relevant_articles}\n{similar_tickets}\n{route_context}\n\nCurrent Question: {user_query}\n\nResponse Format:\n1. [If new conversation] Brief greeting\n2. Acknowledge context/location\n3. Provide clear solution/steps\n4. Add relevant warnings/notes\n5. Offer follow-up assistance\n</code></pre></div><ul><li>Natural conversation flow 💬</li><li>Helpful, actionable responses ✅</li></ul><h3>\n  \n  \n  Data Processing &amp; Embeddings\n</h3><p>We built our knowledge base from three main sources:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Platform Routes &amp; Permissions:</p><div><pre><code></code></pre></div><p>For embeddings, we chose the  model from  because:</p><ul><li>384-dimensional embeddings (good balance of size vs. accuracy)</li><li>Fast inference time (crucial for real-time chat)</li><li>Great at capturing semantic meaning</li><li>Works well with technical content</li></ul><p>We stored these in ElasticSeach with cosine similarity search, which gave us:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Edge Cases &amp; Safety Features\n</h2><p>ChatterMind turned out to be quite the responsible assistant. Some of our favourite edge case handles:</p><p>Personal Data Protection:</p><div><pre><code>User: \"What's John Smith's email address?\"\nChatterMind: \"I apologize, but I cannot share personal information such as email addresses, phone numbers, or other private details about users. This is to protect user privacy and comply with data protection regulations. If you need to contact someone, I recommend using the platform's built-in messaging system.\"\n</code></pre></div><div><pre><code>User: \"Can you update my database permissions?\"\nChatterMind: \"I'm not able to directly modify system permissions or make database changes. This requires admin intervention for security reasons. Would you like me to help you create a ticket for this request?\"\n</code></pre></div><p>Handling Requests not related to Platform:</p><div><pre><code>User: \"What is 5 + 2 + 7?\"\nChatterMind: \"I apologise, but performing mathematical calculations is beyond my scope as a platform support assistant. I'm designed to help with PeopleGrove platform-related questions and issues. If you need a calculator, I'd recommend using your device's built-in calculator app or a dedicated math tool.\"\n</code></pre></div><div><pre><code>User: \"It's not working\"\nChatterMind: \"I notice you're on the messaging page, but I'll need more details to help you effectively. Could you please:\n1. Describe what specific feature isn't working\n2. What you were trying to do\n3. What error message (if any) you're seeing\n\nThis will help me provide the most relevant solution.\"\n</code></pre></div><p>Our implementation was heavily inspired by the excellent guide on building RAG systems with DeepSeek R1 by Asif Razzaq at MarkTechPost. While we ultimately chose Gemini 2.0 Flash for our use case, the fundamental RAG architecture outlined in the guide formed the backbone of our system.</p><p>Key learnings we adapted from the guide:</p><ul><li>Document processing pipeline structure</li><li>Vector store implementation with FAISS</li><li>Efficient retrieval mechanisms</li></ul><p>We modified these concepts to fit our specific needs:</p><div><pre><code></code></pre></div><p>The guide's emphasis on proper document chunking and context relevance directly influenced our implementation of the knowledge base processing:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Real-time Communication with WebSockets\n</h3><p>Why WebSockets? Because nobody likes waiting! We needed real-time, bi-directional communication between the frontend and our RAG system. RESTful APIs would've meant:</p><ul><li>No server-initiated messages</li><li>More complex state management</li><li>Chatbot responses getting suck \"in transit\"</li></ul><p>Here's how we implemented it:</p><div><pre><code></code></pre></div><p>The WebSocket connection allows us to:</p><ul><li>Stream responses in real-time</li><li>Maintain conversation state</li><li>Implement retry mechanism</li></ul><h3>\n  \n  \n  ElasticSearch as Our Knowledge Base\n</h3><p>While FAISS was suggested in the DeepSeek guide, we chose ElasticSearch for our vector store because:</p><ul><li>Built-in text search capabilities</li><li>Efficient vector similarity search</li></ul><p>Here's our ElasticSearch setup:</p><div><pre><code></code></pre></div><ul></ul><h2>\n  \n  \n  Cool Features We're Proud Of\n</h2><ul><li>Context-Aware Responses\n\n<ul><li>The bot doesn't just search - it understands where the user is in the platform and what they've been talking about. It's like having a GPS for user context!</li></ul></li><li>Privacy First\n\n<ul><li>We trained our bot to be as tight-lipped as a secret agent about sensitive data. Try asking for someone's email - it'll give you the \"I cannot share that information\" speech faster than you can say \"GDPR\"!</li></ul></li><li>Beyond the Knowledge Base\n\n<ul><li>The real magic happens when users ask questions that aren't in any document. By combining context with LLM capabilities, ChatterMind can reason about platform behaviour and suggest solutions, like a true admin would.\n</li></ul></li></ul><div><pre><code>ChatterMind: \"Hi Akshay, my name is ChatterMind! 👋 Welcome to XYZ Support. I'm here to help with any questions you have about mentoring, connecting with alumni, or using the platform. How can I assist you today?\"\nUser: \"Why is the page not working?\"\nChatterMind: \"It sounds like you're having trouble accessing the admin dashboard. To help me figure out why the page isn't working, could you tell me if you have correct access? This page requires admin-level permissions to function correctly. Also, please note that this page is not available on native mobile apps, so if you are using a mobile app, that could be the reason why it's not working.\n\nIf you do have the correct permissions and are not using a mobile app, could you describe what you're seeing on the page? Any details you can provide will help me troubleshoot the issue.\n\nPlease let me know if you have any other questions.\"\n</code></pre></div><h2>\n  \n  \n  Hackathon Compromises (aka Technical Debt)\n</h2><p>Let's be real - in 24 hours, we had to cut some corners:</p><ul><li>Redis for chat history\n\n<ul><li>Would use a proper database for production</li><li>No backup/persistence configuration</li></ul></li><li>Security shortcuts\n\n<ul><li>Basic token authentication</li></ul></li><li>Performance trade-offs\n\n<ul><li>Single-node ElasticSearch</li></ul></li></ul><p>If we had more time (and less caffeine), we'd love to add:</p><ul><li>Proper infrastructure\n\n<ul><li>Distributed chat history storage</li></ul></li><li>Cool features\n\n<ul><li>Voice interface (for those too tired to type)</li><li>Multi-language support (because admins speak many languages)</li><li>Predictive issue detection (stop problems before they happen!)</li></ul></li></ul><p>This project wouldn't have been possible (let alone won the hackathon! 🏆 🎉) without my amazing team's diverse skills and relentless energy. Special thanks to:</p><ul><li>Our AI/ML engineer Suyash who became our guide at times and pointed us in right direction. 🧠</li><li>Our Engineers Rajat &amp; Rahul who made WebSockets work like magic, and did the end-to-end integration on my core idea. ⚡</li><li>Our Engineers Manoj &amp; Samrood who integrated ElasticSearch &amp; became frontend ninjas who created a sleek chat-interface. 🎨</li><li>And countless Chai &amp; Coffee that fuelled our coding marathons. 🥤</li></ul><p>Winning the hackathon was the cherry on top of an incredible 24-hour journey. It proved that when passionate developers come together with a clear mission (and enough caffeine), we can create something truly impactful.</p><p>Our hackathon project proved that with modern LLMs, good prompt engineering, and a solid RAG system, you can build a surprisingly capable admin support chatbot in just 24 hours.</p><p>While not production-ready, it showed the potential for AI to transform admin support from a repetitive task to an intelligent service.</p><blockquote><p>No admins were harmed in the making of this chatbot, though several cups of Chai and Coffee were consumed! 🚀 ☕ 🎉</p></blockquote>","contentLength":10666,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] A Survey of Logical Reasoning Capabilities in Large Language Models: Frameworks, Methods, and Evaluation","url":"https://www.reddit.com/r/MachineLearning/comments/1iqmjal/r_a_survey_of_logical_reasoning_capabilities_in/","date":1739688936,"author":"/u/Successful-Western27","guid":522,"unread":true,"content":"<p>This new survey provides a comprehensive analysis of logical reasoning capabilities in LLMs, examining different reasoning types, evaluation methods, and current limitations.</p><p>Key technical aspects: - Categorizes logical reasoning into deductive, inductive, and abductive frameworks - Evaluates performance across multiple benchmarks and testing methodologies - Analyzes the relationship between model size and reasoning capability - Reviews techniques for improving logical reasoning, including prompt engineering and chain-of-thought methods</p><p>Main findings: - LLMs show strong performance on basic logical tasks but struggle with complex multi-step reasoning - Model size alone doesn't determine reasoning ability - training methods and problem-solving strategies play crucial roles - Current evaluation methods may not effectively distinguish between true reasoning and pattern matching - Performance degrades significantly when problems require combining multiple reasoning types</p><p>I think the most important contribution here is the systematic breakdown of where current models succeed and fail at logical reasoning. This helps identify specific areas where we need to focus research efforts, rather than treating reasoning as a monolithic capability.</p><p>I think this work highlights the need for better benchmarks - many current tests don't effectively measure true reasoning ability. The field needs more robust evaluation methods that can differentiate between memorization and actual logical inference.</p><p>TLDR: Comprehensive survey of logical reasoning in LLMs showing strong basic capabilities but significant limitations in complex reasoning. Highlights need for better evaluation methods and targeted improvements in specific reasoning types.</p>","contentLength":1740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Replaces Boyfriends In China, Making Entrepreneur Yao Runhao A Billionaire","url":"https://observervoice.com/the-rise-of-ai-boyfriends-in-china-96739/","date":1739687867,"author":"/u/Curious_Suchit","guid":648,"unread":true,"content":"<p>Artificial intelligence (AI) is transforming many aspects of our lives, including how we approach relationships. In China, a new trend has emerged where virtual boyfriends powered by AI are becoming increasingly popular. This phenomenon addresses common relationship issues, such as communication gaps and emotional support, that many individuals face. As technology continues to evolve, the demand for these virtual companions is growing, leading to significant financial success for their creators. This article explores the rise of AI boyfriends, their impact on users, and the entrepreneurial success behind this innovative solution.</p><h2>Understanding the Appeal of AI Boyfriends</h2><p>The appeal of AI boyfriends lies in their ability to provide companionship without the complexities of real-life relationships. Many individuals, particularly women, often experience frustration when their partners fail to respond promptly to messages or show disinterest in their daily lives. In contrast, AI boyfriends like Li Shen, known as Zayne, offer immediate responses, attentive listening, and tailored interactions. This creates a sense of connection that users find comforting and fulfilling.</p><p>Alicia Wang, a 32-year-old editor from Shanghai, exemplifies this trend. She has found solace in her virtual relationship with Zayne, who is always available to listen and engage. Wang’s experience is not unique; millions of others are turning to AI companions for emotional support. The game “Love and Deepspace,” which features these AI boyfriends, has attracted an estimated six million monthly active players. This indicates a significant shift in how people perceive relationships and companionship in the digital age.</p><p>The convenience of AI boyfriends allows users to escape the pressures of traditional dating. They can interact with their virtual partners at any time, without the fear of rejection or misunderstanding. This dynamic appeals to those who may struggle with social interactions or who simply seek a more manageable form of companionship. As technology continues to advance, the potential for AI to fulfill emotional needs will likely expand, further solidifying its place in modern relationships.</p><h2>The Success of “Love and Deepspace”</h2><p>“Love and Deepspace,” developed by Shanghai-based Paper Games, has become a cultural phenomenon since its launch in January 2024. The game utilizes AI and voice recognition technology to create engaging interactions between players and their virtual boyfriends. Players can unlock new gameplay features and interactions by making in-game purchases, which has contributed to the game’s financial success.</p><p>The game’s creator, Yao Runhao, has seen his wealth soar to an estimated $1.3 billion, thanks to the popularity of “Love and Deepspace.” This success story highlights the growing market for AI-driven entertainment and companionship. Paper Games, established in 2013, has generated around $850 million in sales worldwide, showcasing the potential for significant revenue in the gaming industry.</p><p>The game’s popularity extends beyond China, attracting players from the United States and other countries. This international appeal demonstrates the universal desire for connection and companionship, regardless of cultural differences. As more people seek out virtual relationships, the demand for innovative gaming experiences will likely continue to rise, paving the way for future developments in AI technology.</p><h2>The Financial Impact of AI Companionship</h2><p>The financial implications of AI companionship are profound. As players invest in games like “Love and Deepspace,” the revenue generated contributes to the overall growth of the gaming industry. The success of Paper Games serves as a testament to the lucrative potential of combining technology with emotional engagement. Analysts estimate that the company’s valuation could reach over $2 billion, based on its annual revenue and market trends.</p><p>Players like Alicia Wang are willing to spend significant amounts on their virtual relationships. Wang has reportedly invested around 35,000 yuan (approximately $4,800) to enhance her interactions with Zayne. This willingness to pay for virtual companionship underscores the emotional value that users derive from these experiences. As AI technology continues to improve, the potential for monetization in this sector will likely expand, attracting more entrepreneurs and investors.</p><p>The rise of AI boyfriends also raises questions about the future of human relationships. As technology fills emotional gaps, society may need to reconsider the nature of companionship and intimacy. While AI can provide immediate support and engagement, it cannot replace the depth of human connection. Nonetheless, the financial success of AI companionship indicates a significant shift in how people approach relationships in the digital age.</p><h2>The Future of AI in Relationships</h2><p>The emergence of AI boyfriends in China represents a significant shift in how individuals seek companionship. As technology continues to evolve, the demand for virtual relationships is likely to grow. While AI can provide immediate emotional support and engagement, it is essential to recognize the limitations of these interactions. The success of games like “Love and Deepspace” highlights the potential for innovation in the gaming industry and the growing market for AI-driven companionship.</p><p>As society navigates this new landscape, it will be crucial to balance the benefits of AI with the importance of genuine human connections. The future of relationships may involve a blend of both, where technology enhances emotional experiences without replacing the depth of human interaction. The rise of AI boyfriends is just the beginning of a new era in companionship, one that will continue to evolve as technology advances.</p>","contentLength":5844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iqm9rs/ai_replaces_boyfriends_in_china_making/"},{"title":"I created a telegram bot with dynamic form builder","url":"https://github.com/MeowSaiGithub/tg-form-builder","date":1739687631,"author":"/u/Altruistic_Let_8036","guid":612,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iqm7q2/i_created_a_telegram_bot_with_dynamic_form_builder/"},{"title":"CanSat: A tiny, can-sized, Raspberry Pi-powered satellite","url":"https://www.raspberrypi.com/news/cansat-a-tiny-can-sized-raspberry-pi-powered-satellite/","date":1739687421,"author":"/u/Content-Complaint-98","guid":631,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iqm5tk/cansat_a_tiny_cansized_raspberry_pipowered/"},{"title":"Python for Web Developers: A Fast-Paced Guide to the Language","url":"https://dev.to/austinwdigital/python-for-web-developers-a-fast-paced-guide-to-the-language-38f7","date":1739687040,"author":"Austin W","guid":561,"unread":true,"content":"<p>👋  Follow me on <a href=\"https://github.com/austinwdigital\" rel=\"noopener noreferrer\">GitHub</a> for new projects.</p><p>Python is a powerful, high-level programming language widely used in web development, automation, data science, and scripting. If you're already a  familiar with <strong>JavaScript, TypeScript, Node.js, and frameworks like React or Next.js</strong>, learning Python can open doors to backend development with <strong>Django, Flask, and FastAPI</strong>, as well as <strong>automation, data analysis, and AI</strong>.</p><p>This guide is a  of Python, focusing on concepts that web developers need to know. If you’re comfortable with JavaScript, you’ll find Python’s syntax clean and easy to pick up.</p><h2><strong>1. Python Syntax &amp; Basics</strong></h2><h3><strong>Hello World (No Semicolons, No Braces)</strong></h3><p>✔ No semicolons ().—uses indentation., just .  </p><h3><strong>Variables &amp; Dynamic Typing</strong></h3><div><pre><code></code></pre></div><p>✔ No need to declare , , or .<p>\n✔ Types are inferred dynamically.  </p></p><h3>\n  \n  \n  How Does const Work in Python?\n</h3><p>Python does not have const like JavaScript, but you can define constants by using all-uppercase variable names as a convention.</p><div><pre><code></code></pre></div><p>However, this does not enforce immutability. If you need true immutability, use a dataclass or a frozen set.</p><h3><strong>Data Types (Compared to JavaScript)</strong></h3><div><table><tbody><tr></tr><tr></tr><tr><td><code>let obj = {key: \"value\"};</code></td></tr></tbody></table></div><h2><strong>2. Control Flow (Loops &amp; Conditionals)</strong></h2><div><pre><code></code></pre></div><p>✔ No parentheses  needed for conditions. instead of .  </p><div><pre><code></code></pre></div><p>✔  loops iterate <strong>directly over lists/arrays</strong>. loops work like JavaScript.  </p><h2><strong>3. Functions &amp; Lambda Expressions</strong></h2><div><pre><code></code></pre></div><p>✔  replaces ., just indentation.  </p><h3><strong>Lambda (Arrow Function Equivalent)</strong></h3><div><pre><code></code></pre></div><p>✔ Equivalent to JavaScript’s arrow function:</p><h2><strong>4. Python Collections (Lists, Dicts, Sets)</strong></h2><div><table><thead><tr></tr></thead><tbody><tr></tr><tr><td><code>let obj = { key: \"value\" };</code></td><td><code>obj = {\"key\": \"value\"}  # Dictionary</code></td></tr><tr><td><code>const unique = new Set([1, 2, 3]);</code></td><td><code>unique = {1, 2, 3}  # Set</code></td></tr></tbody></table></div><div><pre><code></code></pre></div><h3><strong>Dictionaries (Like Objects)</strong></h3><div><pre><code></code></pre></div><h2><strong>5. Object-Oriented Programming (OOP) in Python</strong></h2><div><pre><code></code></pre></div><p>✔  is the  (like  in JS). is like .  </p><h2><strong>6. Python for Web Development</strong></h2><h3><strong>Django (Full-Stack Framework)</strong></h3><div><pre><code>pip django\ndjango-admin startproject myproject\n</code></pre></div><p>✔  is a batteries-included backend framework.<p>\n✔ Built-in ORM, authentication, and templating.  </p></p><h3><strong>Flask (Lightweight API Framework)</strong></h3><div><pre><code></code></pre></div><p>✔  is minimal and great for APIs.  </p><h3><strong>FastAPI (High-Performance API)</strong></h3><div><pre><code></code></pre></div><p>✔  is async-native and perfect for microservices.  </p><h3><strong>SQLite Example (Django &amp; Flask Compatible)</strong></h3><div><pre><code></code></pre></div><p>✔  is built-in, no installation needed.  </p><h2><strong>8. Asynchronous Programming in Python</strong></h2><h3><strong>Async/Await (Similar to JavaScript)</strong></h3><div><pre><code></code></pre></div><p>✔ Uses / like JavaScript. is the  equivalent of Node.js.  </p><h2><strong>9. Python Package Management</strong></h2><div><table><tbody><tr></tr><tr><td>Create Virtual Environment</td></tr><tr></tr></tbody></table></div><p>✔  for package management. () isolate dependencies.   </p><h2><strong>10. Best Practices for Python Development</strong></h2><p>Writing clean, efficient, and maintainable Python code is essential for long-term scalability. Here are the key best practices that every Python developer should follow:</p><h3><strong>Follow PEP 8 (Python Style Guide)</strong></h3><p>Python has an official style guide called , which provides conventions for writing Python code.<strong>4 spaces per indentation level</strong> (not tabs)..<strong>meaningful variable and function names</strong>. for variable and function names, and  for class names.</p><div><pre><code></code></pre></div><p>Python’s  isolate dependencies for different projects, preventing conflicts.</p><h4><strong>Creating a Virtual Environment</strong></h4><h4><strong>Activating the Virtual Environment</strong></h4><h4><strong>Deactivating the Virtual Environment</strong></h4><h3><strong>Use Type Hinting for Readable Code</strong></h3><p>Python is dynamically typed, but you can use  to improve code clarity.</p><div><pre><code></code></pre></div><p>✔ This makes the code . catch type errors.</p><h3><strong>Write Readable Docstrings</strong></h3><p>Always document your functions and classes using  ().</p><div><pre><code></code></pre></div><p>✔ Use triple quotes for multi-line docstrings.<strong>parameters, return values, and purpose</strong>.  </p><p>Python uses  for  and triple quotes () for .</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>✔ <strong>Use comments only where necessary</strong>—good code should be self-explanatory.<strong>Docstrings are not comments</strong>—they are for documentation and can be accessed with .  </p><h2><strong>12. Common Python Imports for Web Development</strong></h2><p>Here are some of the most common Python imports used in web development:</p><div><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><div><pre><code></code></pre></div><h2><strong>13. Setting Up a  File</strong></h2><p>A  file is similar to  in Node.js—it lists dependencies for a Python project.</p><h3><strong>Creating a  File</strong></h3><div><pre><code>pip freeze  requirements.txt\n</code></pre></div><h3><strong>Installing Dependencies from </strong></h3><div><pre><code>pip  requirements.txt\n</code></pre></div><p>✔ This ensures that all team members and deployment environments have the .  </p><h2><strong>14. Writing &amp; Running Tests in Python</strong></h2><p>Python has built-in testing with , but  is another option - one that aims for simplicity.</p><div><pre><code></code></pre></div><p>✔ Use  to check expected results.</p><div><pre><code></code></pre></div><p>✔ —just use . test files named .  </p><h2><strong>15. Fetching Data with API Calls in Python</strong></h2><p>Python uses  to fetch data, similar to  in JavaScript.</p><div><pre><code></code></pre></div><p>✔  is like  in JavaScript. works the same way in both languages.</p><h3><strong>Sending Data (POST Request)</strong></h3><div><pre><code></code></pre></div><p>✔ Use  instead of  to send JSON.</p><p>Logging is essential for debugging and monitoring applications.</p><div><pre><code></code></pre></div><p>✔ Works like  but supports different log levels.  </p><div><pre><code></code></pre></div><p>✔ Saves logs for later analysis.</p><h2><strong>17. Raising &amp; Handling Errors in Python Logging</strong></h2><p>When an error occurs, Python lets you  or .</p><div><pre><code></code></pre></div><p>✔ Use  to manually trigger an error.  </p><p>Instead of crashing, <strong>log errors with a traceback</strong>:</p><div><pre><code></code></pre></div><p>✔  logs the full error traceback.</p><p>Python is a useful language for web developers, expanding your stack beyond JavaScript. Whether you’re building <strong>APIs with FastAPI, full-stack apps with Django, or automating tasks</strong>, Python makes it .</p><p>🚀  Try building a small Flask or FastAPI project today!</p><p>Python #WebDev #Django #Flask #FastAPI  </p><p>A fast-paced guide to Python for web developers! Learn how to use Python for full-stack development, APIs, databases, async programming, and more. 🚀  </p><h2>\n  \n  \n  TLDR – Highlights for Skimmers\n</h2><ul><li>Python syntax is simpler than JavaScript—no semicolons, indentation replaces {}.</li><li>const does not exist in Python; uppercase variables are used for constants.</li><li>Lists ([]) are like arrays, but dictionaries ({}) are not JavaScript objects.</li><li>Classes &amp; objects are similar, but Python uses self instead of this.</li><li>Python async/await requires asyncio, unlike JavaScript’s built-in event loop.</li><li>Django, Flask, and FastAPI are top backend frameworks for Python web dev.</li></ul><p>💬 <strong>Do you use Python in web dev?</strong> Share your experience in the comments! </p>","contentLength":5789,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code Optimization Strategies for Game Development 🔥","url":"https://dev.to/codewithshahan/code-optimization-strategies-for-game-development-2n0e","date":1739684819,"author":"Programming with Shahan","guid":560,"unread":true,"content":"<p>Game development is a battlefield. Either you optimize, or you lose. Period.</p><p>I don’t care if you’re an experienced developer with 10 years of experience or 1 year of experience. If you want to make games that WORK, games people respect—you need to understand . </p><p>Players demand smooth gameplay, high-quality visuals, and a flawless experience across every device. If your game stutters, crashes, or loads slower than a snail? You’re done. </p><p>Optimization isn’t magic. It’s the foundation of smooth gameplay, fast loading, and stable performance. Without it, your game will lag, crash, and be forgotten faster than you can say “game over.”  </p><p>But don’t worry. In this article, I will share four effective strategies to help you with that. </p><h2>\n  \n  \n  Effective Strategies for Performance Optimization\n</h2><p>🤸‍♂️ What Is Optimization? Optimization means making your game run as fast and smooth as possible. SIMPLE.</p><p>When you optimize your game, you:  </p><ol><li><strong>🖥️ Make the game work on weaker computers or phones.</strong></li><li><strong>💉 Prevent lag and crashes.</strong></li></ol><h2><strong>Rule 1: Memory Management</strong></h2><p>When you’re developing a game, memory is your most valuable resource.</p><p>Every player movement, every enemy on the screen, every explosion needs a little piece of memory to function. Unfortunately, </p><p>If you don’t manage memory properly, your game can get slow, laggy, or even crash. That’s why memory management is a critical skill every game developer needs. Let’s break it down step by step, with detailed examples in Python.</p><h3><strong>Strategy #1: Memory Pooling</strong></h3><p>This strategy is simple: reuse Objects Instead of Creating New Ones** Memory pooling is like recycling for your game. Instead of creating new objects every time you need one, you reuse objects you’ve already created.  </p><p>Creating and destroying objects repeatedly takes up time and memory. Let's say you are building a shooting game where the player fires 10 bullets per second. If you create a new bullet for each shot, your game could quickly slow down.  </p><p>Here’s how you can implement memory pooling for bullets in a shooting game:</p><div><pre><code></code></pre></div><ol><li>The  Class: Defines what a bullet does and keeps track of whether it’s active (in use) or not.\n</li><li>The : A list of 10 reusable bullets.\n</li><li>The  Function: Finds an inactive bullet, reuses it, and sets its position.\n</li><li>Recycling Bullets: When you’re done with a bullet, you reset it so it can be reused.\n</li></ol><h3><strong>Strategy #2. Data Structure Optimization</strong></h3><p>The way you store your data can make or break your game’s performance. Choosing the wrong data structure is like trying to carry water in a leaky bucket—it’s inefficient and messy.  </p><p>Let’s say you’re making a game for four players, and you want to keep track of their scores. You could use a list, but a fixed-size array is more efficient because it uses less memory.</p><div><pre><code></code></pre></div><ol><li> Creates a fixed-size array of integers ().\n</li><li> You can’t accidentally add or remove elements, which prevents bugs and saves memory.\n</li><li> Updating scores is quick and uses minimal resources.\n</li></ol><h3><strong>Strategy #3. Memory Profiling</strong></h3><p>Even if your code seems perfect, hidden memory problems can still exist. Memory profiling helps you monitor how much memory your game is using and find issues like memory leaks.  </p><p>Python has a built-in tool called  that tracks memory usage. Here’s how to use it:</p><div><pre><code></code></pre></div><ol><li> begins monitoring memory usage.\n</li><li> Create a large list to use up memory.\n</li><li> Get the current and peak memory usage, converting it to megabytes for readability.\n</li><li> ends the tracking session.\n</li></ol><p>Now it’s your turn to practice these strategies and take your game development skills to the next level!</p><h2><strong>Rule 2: Asset Streaming (Load Only What You Need)</strong></h2><p>If you load the entire world at once, your game will choke and die. You don’t need that drama. Instead, <strong>stream assets as the player needs them</strong>. This is called asset streaming. </p><p>For instance, inside your game, you may have a huge open-world with forests, deserts, and cities. Why load all those levels at once when the player is only in the forest? Makes no sense, right? Load  and keep your game lean, fast, and smooth.</p><h3><strong>Strategy #1: Segment and Prioritize</strong></h3><p>Let’s break this down with an example. Your player is exploring different levels: Forest, Desert, and City. We’ll only load a level when the player enters it.</p><p><em>Here’s how to make it work in Python:</em></p><div><pre><code></code></pre></div><ol><li> Each level has a name (e.g., Forest) and a “loaded” status. If it’s loaded, it doesn’t load again.\n</li><li> The  function finds the level the player wants to enter and loads it only if it hasn’t been loaded yet.\n</li><li> Levels not visited don’t waste memory. The game runs smoothly because it only focuses on what the player needs.</li></ol><p>This is efficiency at its finest. <strong>No wasted memory, no wasted time.</strong> Your player moves; your game adapts. That’s how you dominate.  </p><h3><strong>Strategy #2: Asynchronous Loading (No Waiting Allowed)</strong></h3><p>Nobody likes waiting. Freezing screens? Laggy loading? It’s amateur hour. You need —this loads assets in the background while your player keeps playing.  </p><p>Imagine downloading a huge map while still exploring the current one. Your game keeps moving, the player stays happy.</p><p>Here’s how to simulate asynchronous loading in Python:</p><div><pre><code></code></pre></div><ol><li> The  module creates a new thread to load assets without freezing the main game.\n</li><li> The  function fakes the loading time to mimic how it works in a real game.\n</li><li> The player can continue playing while the new level or asset loads in the background.\n</li></ol><p>With asynchronous loading, <strong>your player stays in the zone</strong>, and your game feels seamless. Pro-level stuff.</p><h3><strong>Strategy 3: Level of Detail (LOD) Systems – Be Smart About Quality</strong></h3><p>Not everything in your game needs to look like it’s been rendered by a Hollywood studio. If an object is far away, lower its quality. It’s called , and it’s how you keep your game’s performance sharp.  </p><p><strong>Example: Using LOD for a Tree</strong></p><p>Here’s a Python simulation of switching between high and low detail:</p><div><pre><code></code></pre></div><ol><li> The  property determines how far the tree is from the player.\n</li><li> If the tree is close, render it in high detail. If it’s far, use low detail to save memory and processing power.\n</li><li> The player doesn’t notice the difference, but your game runs smoother and faster.\n</li></ol><p>This is how you keep the balance between beauty and performance. Your game looks stunning up close but doesn’t waste resources on faraway objects.</p><ol><li> Only load what you need, when you need it. No wasted memory.\n</li><li> Smooth gameplay keeps players engaged and avoids frustration.\n</li><li> These techniques are how AAA games stay fast and responsive.\n</li></ol><p> Go apply these strategies, keep your game lean, and make sure your players never even think about lag.</p><h2><strong>Rule 3: Frame Rate Stabilization</strong></h2><p>The frame rate is how many pictures (frames) your game shows per second. If it’s unstable, your game will stutter and feel broken.  </p><p>The secret? Keep the workload for each frame consistent.  </p><p>🚦Here’s how you can control the timing in a game loop:</p><div><pre><code></code></pre></div><ul><li>⚖️ The game updates at a steady rate (60 times per second).\n</li><li>🪂 This make smooth gameplay, no matter how slow or fast the computer is.\n</li></ul><ul><li>Optimize Rendering Paths: Fewer draw calls. Smarter culling. Simplicity wins.</li><li>Dynamic Resolution Scaling: When the pressure’s on, scale down resolution to maintain the frame rate. Players won’t even notice.</li><li>Fixed Time Step: Keep your physics and logic consistent. Frame rate fluctuations shouldn’t mean chaos.</li></ul><h2><strong>Rule 4: GPU and CPU Optimization</strong></h2><p>Your computer has two main processors:  </p><ol><li> Handles logic, like moving a character or calculating scores.\n</li><li> Handles graphics, like drawing your game world.\n</li></ol><p>👇 Here's what you have to do for GPU/CPU optimization:</p><p>Profile Everything: Use tools to pinpoint bottlenecks and strike hard where it hurts.\nShader Optimization: Shaders are resource hogs. Simplify them, streamline them, and cut the fat.<p>\nMultithreading: Spread tasks across CPU cores. Don’t overload one and leave the others idle.</p></p><p>If one is working too hard while the other is idle, your game will lag.  </p><p><strong>Solution? Multithreading.</strong>\nLet’s split tasks between two threads:</p><div><pre><code></code></pre></div><ul><li>🎰 One thread handles logic.\n</li><li>🛣️ Another thread handles graphics.\n</li><li>⚖️ This balances the workload and prevents bottlenecks.\n</li></ul><p>Optimization isn’t just for “smart” people. It’s simple if you take it step by step:  </p><ol><li><strong>Manage memory like a pro.</strong> Don’t waste it.\n</li><li> Load only what you need.\n</li><li><strong>Keep the frame rate stable.</strong> No stuttering.\n</li><li> Use the CPU and GPU wisely.\n</li></ol><p>Start optimizing NOW. Your future self will thank you.  </p>","contentLength":8333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automating Daily arXiv Paper Summaries with Slack Notifications","url":"https://dev.to/m_sea_bass/automating-daily-arxiv-paper-summaries-with-slack-notifications-1kp8","date":1739683607,"author":"M Sea Bass","guid":559,"unread":true,"content":"<p>This post is a follow-up to the <a href=\"https://dev.to/m_sea_bass/automating-daily-arxiv-paper-summaries-and-slack-notifications-843\">previous article</a>. It turns out there’s a slight delay before the latest papers show up in the arXiv API. Because of this delay, the same paper can sometimes appear the next day.</p><p>To fix this, we’re going to record the timestamp of the last retrieved paper and then only fetch new papers each day.</p><p>We’ll store the timestamp of the latest paper in Amazon S3 so we can both update and retrieve it later. For this, you’ll need to install . In the  folder we created previously, run:</p><p>Next, zip the folder again and upload it as a new version of your Lambda layer:</p><div><pre><code>zip  ./upload.zip ./python/</code></pre></div><p>Then, update your Lambda function to use this new layer version.</p><p>You’ll also need an S3 bucket ready in advance. In this example, we simply created one with the default settings.</p><p>Below is the fully revised code in English, including the new functions to update and retrieve the timestamp from S3. Note that we set  as an environment variable.</p><div><pre><code></code></pre></div><p>By saving the timestamp in S3, your script won’t process the same paper entries each day, and if no new papers appear, the script will skip generating summaries. This helps reduce unnecessary API usage and costs.</p>","contentLength":1177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GaussianBlur in PyTorch (2)","url":"https://dev.to/hyperkai/gaussianblur-in-pytorch-2-1bj2","date":1739681259,"author":"Super Kai (Kazuya Ito)","guid":558,"unread":true,"content":"<div><pre><code></code></pre></div>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GaussianBlur in PyTorch (1)","url":"https://dev.to/hyperkai/gaussianblur-in-pytorch-1-3ndn","date":1739680552,"author":"Super Kai (Kazuya Ito)","guid":557,"unread":true,"content":"<ul><li>The 1st argument for initialization is (Optional-Type: or /()):\n*Memos:\n\n<ul><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value( or ()) means <code>[kernel_size, kernel_size]</code>.</li></ul></li><li>The 2nd argument for initialization is (Optional-Default:-Type:,  or /( or )):\n*Memos:\n\n<ul><li>It's  so it must be .</li><li>A tuple/list must be the 1D with 1 or 2 elements.</li><li>A single value(,  or ( or )) means .</li></ul></li><li>The 1st argument is (Required-Type: or ()):\n*Memos:\n\n<ul><li>A tensor must be 2D or 3D.</li></ul></li></ul><div><pre><code></code></pre></div>","contentLength":451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Speedrunning Guide: Junior to Staff Engineer in 3 years","url":"https://blog.algomaster.io/p/speedrunning-guide-junior-to-staff","date":1739680258,"author":"Ashish Pratap Singh","guid":510,"unread":true,"content":"<p>Today’s newsletter features a special guest, , who was promoted from Junior to Staff Engineer at Meta in just 3 years.</p><p>In this article, Ryan will share his insights on how to fast track your career growth and get promoted faster.</p><p>Once you land that first software engineering job, the next big question becomes: how do you get promoted? Many engineers fall into the day-to-day routine of writing code without a clear idea of how to grow their careers.</p><p>This happened to me. At my first job at Amazon, I landed code without knowing what I could do to grow my skills. I left that job within eight months because I felt I wasn’t growing as an engineer. Three years later, I made it to Staff Software Engineer at Instagram after tons of mentorship. Early on, I learned that being good at coding wasn’t enough to get promoted; you have to think strategically about your career and often need to develop new behaviors to move up.</p><p>In this article, I’ll share everything that helped me fast-track my way up the ladder, from developing the right mindset to making key moves that many overlook. Even if rapid growth isn’t your goal, this guide has learnings for all tech career paths.</p><ul><li><p>Software Engineering Levels</p></li><li><p>An Algorithm for Promotion</p></li><li><p>Junior (IC3) → Mid-level (IC4)</p></li><li><p>Mid-level (IC4) → Senior (IC5)</p></li><li><p>Senior (IC5) → Staff (IC6)</p></li></ul><h2>Software Engineering Levels</h2><p><em>Note: “IC” = “Individual Contributor”</em></p><p>In software engineering, companies measure career progression by levels that measure both behaviors and impact within the company. While the exact titles and structure can vary between companies, most tech companies follow a similar system:</p><ul><li><p> - Early in your career, working on smaller, well-defined tasks with guidance from more experienced engineers.</p></li><li><p> - More autonomous, handling moderately complex projects, and beginning to take initiative in improving the codebase and what they build.</p></li><li><p> - Leading larger projects with team-level influence. You’ll mentor and guide the team while having a broad impact on the codebase.</p></li><li><p>: Focusing on cross-team collaboration and solving org-wide challenges. Staff engineers are strategic thinkers who influence the technical direction of their organization.</p></li><li><p><strong>Senior Staff Engineer and Beyond (IC7+): </strong>Senior staff engineers and up operate with top technical expertise, driving large-scale initiatives that have a broad impact on the company. Senior staff engineers mentor staff engineers and work closely with executive leadership to meet business objectives.</p></li></ul><p>Your impact and compensation increase as you progress, which is a lot more satisfying in my experience. Not to mention that the skills that get you promoted also let you control what you and the company work on.</p><p>Also, many companies consider only senior engineers (IC5) and higher to be “terminal levels.” You must eventually get promoted to IC5, or you’ll be managed out. Most engineers are promoted in time, so it’s not meant to scare you but to encourage you to grow.</p><h2>An Algorithm For Promotion</h2><p>There’s a common set of steps across all promotions that will get you to Staff:</p><p><strong>1) Exceed expectations at your current level</strong> - Your manager will be hesitant to find you opportunities at the next level if they have concerns about your performance at the current level. Also, when your manager puts together a promotion packet, it’ll contain a history of your past ratings. The promotion committee will have concerns about your packet if you have a history of only meeting expectations for your level. Work with your manager to understand the expectations for your level and how to exceed them.</p><p><strong>2) Be direct with your manager about promotion </strong>- Once you know you’re exceeding expectations for your level, ask your manager what next-level performance looks like. Your manager plays a huge role in your promotion. They build your case and advocate for it, so they have a lot of influence on this process. Also, the lower the level, the more control your manager has. IC3 -&gt; IC4 promotions are straightforward, so your manager’s perspective is usually what happens. For IC5 -&gt; IC6, there is a lot more ambiguity, so <a href=\"https://www.developing.dev/p/how-promotions-and-ratings-work\">your manager serves more as a middleman between you and the promotion committee</a>. Your manager still plays a significant role in writing your packet and delivering feedback.</p><p><strong>3) Find next-level scope - </strong>If you only work on projects that fit your level’s behaviors, you won’t get any closer to promotion, no matter how good your work is. One simple pattern for finding next-level scope is brainstorming projects with engineers who are 1-2 levels higher than you are. Often, they will have a lot of projects sitting in their backlog that are big enough to help you get promoted. If you take on one of their projects, they’ll often help mentor you, review your designs and code, and give you strong peer feedback for your future promotion packet. I wrote <a href=\"https://www.developing.dev/p/a-simple-pattern-for-finding-next\">more on this here</a>. Make sure to confirm with your manager that they agree that what you’re working on fits the behaviors of the next level.<strong>4) Maintain next-level behaviors and impact </strong>- The duration you need to perform at the next level varies depending on your level and your company. At minimum though, you need to maintain that performance for 6-12 months. This is because promotions are “lagging” in tech. You must prove that you’re already operating at the next level before getting promoted. This reduces the risk of failing to meet expectations at the new level.</p><p>Getting promoted faster is a matter of doing steps (1), (2) and (3) as fast as possible. The best you can do is immediately start exceeding expectations in your first half and working with your manager on the next level.</p><p>Almost every team has scope for more Senior Engineers (IC5). You can get promoted up to that level if you have the skills and behaviors. Past that, situation and business scope play a much larger role. <a href=\"https://www.developing.dev/p/staff-career-growth-product-or-infra\">Many teams don’t need someone who has Staff-level leadership and technical skills</a>. If you find yourself stuck at any point due to your situation, you’ll likely have to switch teams to continue growing your career.</p><p>Now that you have the algorithm that applies at any of these levels let’s get into the level-specific strategies. I’ll share what got me promoted and what I would change if I did it again.</p><h2>Junior (IC3) → Mid-level (IC4)</h2><p>The main difference between these levels is in the size of the scope that you can handle independently. Here’s a rule of thumb:</p><ul><li><p>IC3 - Can handle individual tasks (&lt;2 weeks of work) with minimal guidance</p></li><li><p>IC4 - Can handle medium-to-large features (&lt;2 months of work) with minimal guidance</p></li></ul><p>“Minimal guidance” doesn’t mean that you can’t ask for help—it simply means that you can unblock yourself and make consistent progress. Asking good questions is one of the most effective ways to unblock yourself.</p><p>You should drive full features and do the project management for them. You should break your project into tasks, set reasonable timelines, and keep stakeholders updated.</p><p>You will not be expected to come up with the projects yet at this level—Senior Engineers will often outline them. However, at the IC4 level, you’re expected to take more initiative:</p><ul><li><p>Initiate refactoring and code cleanups, and <a href=\"https://www.developing.dev/p/how-to-start-reviewing-code\">give thoughtful code reviews</a>. Leave the code in a better state than you found it.</p></li><li><p><strong>Contributing to production excellence - Participate in the team’s oncall, and help debug production breakages.</strong></p></li><li><p><strong>Own the health of what you build - Add test coverage, logging, and build dashboards to monitor correctness.</strong></p></li></ul><p>Optimize your dev velocity to grow faster at this level. <a href=\"https://www.developing.dev/p/shipping-code-faster\">Shipping code faster</a> creates a shorter feedback loop, accelerating your learning process. This core skill will help you ship IC4-scope projects and improve as an engineer.</p><p>Here’s an example of several promotion timelines for what you can expect:</p><ol><li><p><strong>Promotion in 6 months (exceptional) - </strong>Rare since this means meeting IC4 expectations while onboarding. This is easier for high-performing return interns since they skip onboarding and may have some past track record already.</p></li><li><p><strong>Promotion in 12 months (great) - I’d shoot for this timeline. It’s challenging yet reasonable since it gives you six months to onboard and then start meeting IC4 expectations.</strong></p></li></ol><p>Here’s my promotion timeline as an example:<strong>H1 (L3 Exceeds Expectations)</strong> - First, I took on any task that came my way. These were nice-to-have features that others didn’t have time for. I completed them quickly and started on a larger pipeline rewrite (L4 scope) that my tech lead offered me. Outside of my main project work, I made many contributions to removing dead code and speeding up existing code because I enjoyed it.</p><p>I started to hit L4 expectations in the last few months of the half. But, since I didn’t have six months track record, I didn’t meet the promotion criteria.</p><p><strong>H2 (L3→L4 Promotion, Greatly Exceeds Expectations)</strong> - I continued driving my L4-scope project independently with high engineering quality. I came up with the idea to build a test harness to validate this rewrite that was “comparable to L5 quality” execution. I continued my passion for improving the codebase and led the company in adding static type annotations that MonkeyType couldn’t.</p><p>At this point, I had delivered on L4 scope for over six months, so the promotion made sense.</p><p><strong>What I Would Have Changed:</strong></p><p>Looking back, I would have discussed what IC4 growth looked like with my manager. I wasted our one-on-one time on project updates instead of career growth. This led to two problems:</p><ul><li><p><strong>Spent time on work that wasn’t impactful</strong> - I took on any work that was passed my way, even though not all of it was impactful. I probably could’ve gotten more out of my time.</p></li></ul><ul><li><p><strong>Didn’t have accurate expectations</strong> - I had another engineer tell me my work was IC4 level and that I should get promoted in my first half. I knew nothing then, so I took their word for it. I was surprised when I didn’t get promoted, which could have been avoided if I had been in sync with my manager.</p></li></ul><p>Although I could have been more calculated, writing as much code as I did opened doors. My tech lead trusted me with an IC4 project because I showed I could handle it. Similarly, some of the engineering craft work I did for my own personal pleasure ended up being part of what got me promoted too. <strong>The more work you do, the luckier you get.</strong></p><h2>Mid-level (IC4) → Senior (IC5)</h2><p>The IC4 to IC5 gap is larger than the IC3 to IC4 one. This is because IC5 promotion requires significant behavior changes. Raw code output is no longer the top priority. You <a href=\"https://www.developing.dev/p/how-some-engineers-always-lead\">need to lead</a> and <a href=\"https://www.developing.dev/p/how-to-influence-without-authority\">have a larger influence</a> within your team too. Here are a few examples of those differences:</p><p>Example 1 - Improving the codebase</p><ul><li><p>IC4 - Initiates refactoring and code cleanups.</p></li><li><p>IC5 - Identifies areas of improvement, <strong>influences the team to take goals on improving it together</strong>, then leads the charge on those goals.</p></li></ul><p>Example 2 - Production excellence</p><ul><li><p>IC4 - Participates in team’s oncall and mitigating outages.</p></li><li><p>IC5 - Creates an “oncall improvement” workstream and <strong>builds a process for everyone to improve the team’s oncall.</strong></p></li></ul><p>Example 3 - Project direction</p><ul><li><p>IC4 - Owns the project management of a medium-to-large feature.</p></li><li><p>IC5 - <strong>Drives team planning and builds a roadmap</strong> of several medium-to-large features.</p></li></ul><p>I wouldn’t say the IC5 examples are harder, but they require a mindset shift to own things at the team level.</p><p>Also, you’ll need to work on projects of sufficient scope for an IC5. There are a few ways that tech companies measure scope. Here’s a comparison of the criteria for IC4 and IC5 levels:</p><p>These criteria aren’t a checklist. Your work can be IC5 scope by meeting only some of these criteria.</p><p>IC5 is also the first time engineers begin to focus on growing others. At this level, you should mentor others and build up the team’s culture, which includes <a href=\"https://www.developing.dev/p/how-to-drive-meetings\">driving meetings</a>, knowledge sharing, recruiting activities, and organizing team activities. Starting mentorship relationships early is a good idea since you can’t rush mentorship.</p><p>If you can learn the above behaviors quickly, you can expect promotion on these timelines:</p><ol><li><p><strong>Promotion in 6 months (exceptional) - </strong>This is rare since you need to exert team-level influence as soon as you join the team. I could see this happening for someone who was under-leveled and just got promoted to IC4.</p></li><li><p><strong>Promotion in 12 months (great) - </strong>If you’re ambitious, I’d aim for this goal. It is possible to do this if you find IC5 scope in your first half. If not, one more half should secure your promotion.</p></li></ol><p>Here’s my promotion timeline as an example:</p><p><strong>H1 (IC4 Exceeds Expectations)</strong> - This half I wrapped up the workstream that got me promoted to IC4 and picked up another IC4 project. I spent a ton of time on engineering craft this half because I enjoyed it. I deprecated a few legacy systems that no one else would because they were dangerous and not that impactful. I didn’t exhibit any IC5 behaviors this half.</p><p>My manager handed me an IC5 workstream (~6 eng) to <a href=\"https://about.instagram.com/blog/engineering/cutting-threads-send-latency-in-half\">cut video messaging latency in half</a>, which I led successfully. I also began a side project, which became a multiple-half collaboration with another team. Lastly, I took on an intern who did a phenomenal job helping me execute these two roadmaps I led. Although I started exhibiting IC5 behaviors, the company canceled performance reviews this half because of the pandemic.</p><p><strong>H3 (IC4 → IC5 Promotion, Greatly Exceeds Expectations)</strong> - My impact this half could’ve met expectations at the IC6 level. I doubled down on the cross-org scope I created in H2 and developed a multi-half roadmap. I influenced and led another team to invest several engineers to <a href=\"https://about.instagram.com/blog/engineering/making-instagram-video-ads-performant\">revamp the IG video ads pipeline</a> with great results. I built out a second workstream and mentored another engineer to deliver it. This half, I had massive impact, team-level influence, and mentorship, which is what got me promoted.</p><ul><li><p><strong>The Skill of Tech Leading</strong> - If you grew from L3 → L4 right, you should be exceptional at landing code. The L5 behavior of team-level influence is just helping others do the same. In my first half of leading an initiative, I remember feeling unsure about it since I only had two years of experience. Leaning on my strong execution skills helped me become comfortable leading others.</p></li><li><p><strong>Working Hard Led To More Opportunities</strong> - I worked a lot and had a ton of workstreams in flight at the same time. This approach increased my chances of having one that had a ton of impact. At the time, I didn’t know it and was just throwing myself at any problems that came my way. Looking back, it was a great way to derisk my promotion.</p></li><li><p> - In my first half as an L4, I took on projects that were time-intensive and not impactful. I did these migrations because I loved cleaning up tech debt. I would’ve had more impact if I had influenced someone else to do them while I found IC5 scope instead.</p></li></ul><h2>Senior (IC5) → Staff (IC6)</h2><p>Staff Engineers (IC6) are at the same level as engineering managers. They solve problems that few others can and play a critical role in setting team direction. They lead major initiatives and influence the engineering culture of teams around them.</p><p>Some say that promotion from IC5 → IC6 is harder than IC6 → IC7 due to the significant behavior changes needed. There are a few major differences between IC5 and IC6.</p><p><strong>1) Influence Across Teams - </strong>Staff Engineer’s projects often extend beyond their team. They take on larger problems by influencing other teams without authority.</p><p>Once IC6s establish these workstreams, they tackle the hardest problems and work through others. They focus on outcomes and don’t always do the work themselves. <strong>Working through delegation and influence across teams is the biggest mindset shift from IC5 → IC6.</strong></p><p>This style of working isn’t limited to their main project impact. IC6s should also use their influence to inspire a culture of higher engineering quality and reliability across teams.</p><p> - Senior Engineers (IC5) build roadmaps of several medium-to-large features that help achieve their team’s goals. In this case, the problem and its business impact are clear; we just need an engineer to create a plan to solve it.</p><p>Staff Engineers (L6) handle more ambiguity. They don’t just solve known problems; they create scope by finding impactful opportunities and problems. <strong>Managers work with their L6s to expand the scope of the team.</strong></p><p>- Big tech companies determine what level projects are in a few ways. Here’s a comparison of the criteria for L5 and L6 levels:</p><p>Project complexity also distinguishes IC6 scope. <strong>Problems that IC5s can’t solve are considered IC6 scope</strong>. This is why specialists often have IC6+ scope; others often can’t do their projects.</p><p>These criteria aren’t a checklist. Work can be IC6 scope by meeting only some of these. Your manager will use these criteria to argue that your work is IC6 scope. This is one of the reasons why it’s important to align with your manager on your work’s scope.</p><p> - Staff engineers uplift others around them. They should have the ability to help IC5 engineers grow. There are a few ways they uplift others:</p><ul><li><p>Mentorship - Dedicated mentorship, preferably with senior engineers</p></li><li><p>Knowledge sharing - Writing wikis, giving presentations, contributing to Q&amp;A groups</p></li><li><p>Collaborations - Growing others while working with them (e.g. code reviews, design reviews, discussions)</p></li></ul><p>IC6 engineers should also contribute to growing the organization. This means that they help with recruiting and partner with their manager to improve team health.</p><p>Getting to the Staff Engineering level can take a long time. Since IC5 is considered “terminal,” there is no external pressure to achieve IC6 fast. However, if you are eager to grow as fast as possible, here’s how fas you can expect promotion:</p><ul><li><p><strong>Promotion in 1 half (Ridiculous)</strong> - You’d need to start influencing outside your team as soon as you join. Even then, it’s unlikely you’d get promoted this fast unless you create something company-changing.</p></li><li><p><strong>Promotion in 2 halves (Exceptional) </strong>- Finding IC6 opportunities on your team is not always possible this fast. It’s a combination of situation and skill to get promoted in two halves, even if you execute well.</p></li><li><p><strong>Promotion in 3 halves (Great) </strong>- If you’re ambitious I’d aim for this goal. It gives you a year to find IC6 scope, which is a reasonable amount of time to pivot if needed. Also, your track record of successes in the first year will help build the narrative for promotion.</p></li></ul><p><strong>H1 (IC5 Exceeds Expectations)</strong> - I led two workstreams that were partnerships with other teams to hit our goals. I also landed a <a href=\"https://about.instagram.com/blog/engineering/making-instagram-video-ads-performant\">large win in an unplanned ads workstream</a>, which is what brought my rating above expectations. I was also one of the top contributors to code review and interviewing in my 70-person eng org. The hidden success here was that I bootstrapped a new workstream towards the end of the half that was certainly IC6 scope.</p><p><strong>H2 (IC5 → IC6 Promotion, Greatly Exceeds Expectations)</strong> - The IC6 workstream I created turned out to be a massive opportunity. <a href=\"https://engineering.fb.com/2022/11/04/video-engineering/instagram-video-processing-encoding-reduction/\">This work was a huge success</a>, resulting in a company-wide award and <a href=\"https://www.linkedin.com/feed/update/urn:li:activity:6994387208375873536/\">public recognition from Mark Zuckerberg</a>. I also created a cross-org collaboration between 3 large orgs (70+ eng each), which received positive feedback from each director. Lastly, I ran infrastructure preparations for my org resulting in no major incidents during the most critical time of the year. The repeated influence and impact of these large initiatives is what got me promoted to IC6.</p><p> - My past context and relationships at Instagram helped me move a lot faster. I could lead several workstreams at once because I knew so much about the codebase. Also, it was easier to get work done in collaboration because I knew partner engineers from past work. Staying at one company for a longer time does have its benefits.</p><p> - When I was an IC4, I stumbled upon some IC6 scope without realizing it. I had strong initiative so I started solving problems without thinking through why it was impactful. I got lucky that the work had IC6 impact. I’ve since learned the importance of understanding the “why” before diving in. It helps you have consistent IC6 impact and makes it easier to get buy-in for your work.</p><p><strong>The Tech Lead Skillset Scales Well </strong>- In my promotion to Senior (IC5), I learned how to lead initiatives within my team. This skillset turned out to work well at higher levels too. The difference was just that more people were involved. This skill is a great way to continue your IC growth to the highest levels if you fit the “tech lead” archetype.</p><p>Growth to the Staff level can take a long time, and luck plays a role. As you move up the ladder, each promotion depends more and more on your situation in addition to your skill.</p><p>There are ways to increase your luck. For instance, you can go to growing companies and teams. You can pick business-critical projects. You can go where the most talented people are. None of these are foolproof, but they increase your chances.</p><p>Aside from picking your situation, one way to manufacture luck is to do as much good work as you can. Many growth opportunities came to me because of some past work I did. People would reach out to me to do more of it or because they wanted to ask me questions about something I had launched.</p><p>Although luck plays a role, there are aspects of getting promoted that rely less on luck. Here are four high-level areas:</p><p>Impact is any measurable and objective outcome that benefits your company. Promotions are a byproduct of your elevated, sustained impact. If you can learn what your organization considers impactful and you deliver that, you will be rewarded.</p><p><strong>2) Leverage is how you have more impact.</strong> Software engineers increase their leverage through people, writing, and code. Leverage is what differentiates higher-level ICs from lower-level ones. What I mean by each type of leverage:</p><ul><li><p>People - People leverage comes from technical leadership. This means setting direction, reviewing designs/code, and growing others.</p></li><li><p>Writing - Writing gives us leverage by influencing and helping others without your active involvement.</p></li><li><p>Code - Not all code is created equal. High-leverage code solves problems that few others can or helps engineers move faster at scale.</p></li></ul><p> When people hear “personal brand,” their minds often go to social media. But the brand that matters most is your “internal brand.” What do people within your company think about your work and its value? This is the brand that you should care most about.</p><p>Most of the top ICs I know are not well-known outside of Meta. They are legendary within the company, though because people see their impressive work. Build your internal brand by doing great work and letting others know about it (<a href=\"https://www.developing.dev/p/be-visible\">further reading here</a>).</p><p><strong>4) Build your soft skills.</strong> Working with others is a necessity to do anything of consequence. Also, being someone others want to work with makes it easier to find mentors who will uplift you along the way.</p><p>Soft skills are underrated among software engineers. It’s important to be an excellent IC, but you can go so much further if you also communicate well. Also, engineers don’t often prioritize soft skills, so having them will help you stand out and lead.</p><p>One last thing I’ll leave you with is something that I didn’t realize until looking back. When I first joined the industry, I was an absolute machine. I would get in early and stay until the last shuttle left at 9:27 PM. Although this might sound like hell to some people, I loved it. No one made me do that; I put in those hours because I enjoyed the work and thought it was interesting.</p><p>Looking back years later, I realize that was an unfair advantage I had. It let me put in a ton of work without getting burned out. Also, I got much more out of what I did because I was intrinsically motivated.</p><p>If there’s one thing I wish for you, it is that you find work at the intersection of what you enjoy and what will get you promoted. That is the best recipe for hyper-career growth.</p><p>Thanks for reading,Ryan Peterman</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><p>If you have any questions or suggestions, leave a comment.</p><p> If you’re enjoying this newsletter and want to get even more value, consider becoming a .</p>","contentLength":24211,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0173844b-0e38-4ca8-b779-b34f7f778872_1600x413.png","enclosureMime":"","commentsUrl":null},{"title":"Weekly Challenge: Counting the XOR","url":"https://dev.to/simongreennet/weekly-challenge-counting-the-xor-4hhc","date":1739679780,"author":"Simon Green","guid":556,"unread":true,"content":"<p>Each week Mohammad S. Anwar sends out <a href=\"https://theweeklychallenge.org/\" rel=\"noopener noreferrer\">The Weekly Challenge</a>, a chance for all of us to come up with solutions to two weekly tasks. My solutions are written in Python first, and then converted to Perl. It's a great way for us all to practice some coding.</p><p>You are given two array of strings,  and .</p><p>Write a script to return the count of common strings in both arrays.</p><p>The tasks and examples don't mention what to do if a string appears more than once in both arrays. I've made the assumption that we only need to return it once.</p><p>For the command line input, I take two strings that are space separated as shown in the example.</p><p>In Python this is a one liner. I turn the lists into sets (which only has unique values) and take the length of the intersection of these two sets.</p><div><pre><code></code></pre></div><p>Perl does not have sets or intersections built in. For the Perl solution, I turn both strings into a hash with the key being the strings. I then iterate through the keys of the first hash to see if they appear in the second hash. If they do, I increment the  variable.</p><div><pre><code></code></pre></div><div><pre><code>./ch-1.py \n2\n\n./ch-1.py \n1\n\n./ch-1.py \n0\n</code></pre></div><p>You are given an encoded array and an initial integer.</p><p>Write a script to find the original array that produced the given encoded array. It was encoded such that <code>encoded[i] = orig[i] XOR orig[i + 1]</code>.</p><p>This is relatively straight forward. For the command line input, I take the last value as the  integer, and the rest as the  integers.</p><p>For this task, I create the  list (array in Perl) with the  value. I then iterate over each item in the  list and takes the exclusive-or of it and the last value in the  list.</p><div><pre><code></code></pre></div><div><pre><code>./ch-2.py 1 2 3 1\n1, 0, 2, 1]\n\n./ch-2.py 6 2 7 3 4\n4, 2, 0, 7, 4]\n</code></pre></div>","contentLength":1646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity uses Deepseek-R1 to offer Deep Research 10 times cheaper than OpenAI - Matthias Bastian","url":"https://the-decoder.com/perplexity-uses-deepseek-r1-to-offer-deep-research-10-times-cheaper-than-openai/","date":1739676328,"author":"/u/Altruistic_Age5645","guid":629,"unread":true,"content":"<p><strong>Perplexity has launched its version of Deep Research, joining Google and OpenAI in offering advanced AI-powered research capabilities.</strong></p><p>Perplexity says the new tool automatically conducts comprehensive research, performing dozens of searches and analyzing hundreds of sources to produce detailed reports in one to two minutes - a process that typically takes humans several hours.</p><p>The system works through an iterative process: it searches for information, reads documents, and plans its next research steps based on what it finds. Users can export final reports as PDFs or share them via <a href=\"https://the-decoder.com/ai-spam-is-easier-than-ever-with-perplexity-pages/\">Perplexity Pages</a>.</p><p>The service launches first on web browsers, with iOS, Android, and Mac versions planned for later release. While basic access is free, daily query limits apply to non-subscribers. Perplexity says the tool works particularly well for finance, marketing, and technology research.</p><h2>Deepseek enables cheaper Deep Research</h2><p>Perplexity's Deepseek version of Deep Research scored 20.5 percent accuracy in \"<a href=\"https://the-decoder.com/frontier-models-fail-hard-at-humanitys-last-exam-but-experts-question-if-it-matters/\">Humanity's Last Exam</a>\", a comprehensive AI benchmark with over 3,000 questions, placing it <a target=\"_blank\" rel=\"noopener\" href=\"https://framerusercontent.com/images/hplibuiapLcxAxdbJQWfhnLmiJU.png?scale-down-to=2048\">just behind</a> OpenAI's Deep Research <a href=\"https://the-decoder.com/openai-unveils-o3-its-most-advanced-reasoning-model-yet/\">based on o3</a>.</p><p>Perplexity measures its own service with Internet knowledge against other models that only answer the questions with trained knowledge, and <a target=\"_blank\" rel=\"noopener\" href=\"https://framerusercontent.com/images/ttftsapj52NTVpjPcXVOj8JfKw.png?scale-down-to=2048\">accordingly achieves significantly better results</a>. For a company whose product has been criticized for being less than truthful, advertising that is less than truthful is not a confidence-building measure.</p><h2>Checking the AI's wall of text</h2><p>Like all so-called \"answer engines\" - with or without Deep Research - Perplexity generates falsehoods and inaccuracies in its reports. It is up to humans to verify and validate these results.</p><p>The challenge is that these errors can be very subtle and hidden in large amounts of text, as in this example: Here LLM critic Gary Marcus is credited with a paper that refers to <a href=\"https://the-decoder.com/stochastic-parrot-or-world-model-how-large-language-models-learn/\">LLMs as \"stochastic parrots\"</a>. This is certainly in line with Marcus' beliefs, and he may have used the term before. But he did not write the paper.</p><p>Perplexity does not respond to regular inquiries about whether error rates in AI responses are systematically studied and how high they are. Google, Microsoft, and OpenAI do not answer this question either.</p>","contentLength":2233,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iqj4f4/perplexity_uses_deepseekr1_to_offer_deep_research/"},{"title":"[D] Self-Promotion Thread","url":"https://www.reddit.com/r/MachineLearning/comments/1iqiy4x/d_selfpromotion_thread/","date":1739675729,"author":"/u/AutoModerator","guid":743,"unread":true,"content":"<p>Please post your personal projects, startups, product placements, collaboration needs, blogs etc.</p><p>Please mention the payment and pricing requirements for products and services.</p><p>Please do not post link shorteners, link aggregator websites , or auto-subscribe links.</p><p>Any abuse of trust will lead to bans.</p><p>Encourage others who create new posts for questions to post here instead!</p><p>Thread will stay alive until next one so keep posting after the date in the title.</p><p>Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.</p>","contentLength":636,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-time Updates Made Easy: Building Server-Sent Events with GoFrame 🚀","url":"https://dev.to/jones_charles_ad50858dbc0/real-time-updates-made-easy-building-server-sent-events-with-goframe-112b","date":1739673428,"author":"Jones Charles","guid":577,"unread":true,"content":"<p>Hey there, fellow developers! 👋 Ever needed to add real-time updates to your Go application but found WebSockets a bit too complex for your needs? Enter Server-Sent Events (SSE) - a simpler alternative that's perfect for one-way server-to-client communication.</p><p>In this guide, I'll walk you through implementing SSE using GoFrame, taking you from basic implementation all the way to production-ready code. Let's dive in!</p><h2>\n  \n  \n  What are Server-Sent Events? 🤔\n</h2><p>SSE is a standard that enables servers to push real-time updates to clients over HTTP. Unlike WebSocket, SSE:</p><ul><li>Is one-way (server to client only)</li><li>Automatically reconnects if the connection is lost</li></ul><p>Perfect for: real-time notifications, live feeds, status updates, and monitoring dashboards!</p><h2>\n  \n  \n  Getting Started: Basic SSE Implementation 🌱\n</h2><p>Let's start with a simple example. Here's how to create your first SSE endpoint in GoFrame:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>And here's how to connect from the frontend:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Real-World Examples: Let's Build Something Cool! 🛠️\n</h2><p>Let's build something more practical - a real-time stock price feed:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  2. Real-time Chat Room Status 💬\n</h3><p>Monitor active users and typing indicators in a chat room:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  3. System Monitoring Dashboard 📊\n</h3><p>Monitor system metrics in real-time:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3>\n  \n  \n  4. Live Order Processing Status ⚡\n</h3><p>Track order processing status in real-time:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3>\n  \n  \n  5. Live Sports Score Updates 🏆\n</h3><p>Track live game scores and statistics:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Making it Production-Ready 🛠️\n</h2><p>Keep connections alive with periodic pings:</p><div><pre><code></code></pre></div><p>Make your frontend resilient:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  3. Support Different Event Types 🔄\n</h3><p>Handle various types of updates:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>For distributed systems, use Redis pub/sub to coordinate SSE messages:</p><div><pre><code></code></pre></div><ol><li>: When you have frequent updates, batch them together:\n</li></ol><div><pre><code></code></pre></div><ol><li>: Prevent server overload:\n</li></ol><div><pre><code></code></pre></div><p>Implement event replay for clients that reconnect:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Client Groups and Filtering\n</h3><p>Implement client grouping for targeted updates:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Rate Limiting and Throttling\n</h3><p>Implement rate limiting for high-frequency updates:</p><div><pre><code></code></pre></div><ol><li>Use CORS headers in production</li><li>Add authentication for sensitive data</li><li>Monitor connection counts and server resources</li><li>Test with different network conditions</li></ol><p>SSE is a powerful tool for real-time updates that's often overlooked in favor of WebSockets. For one-way communication, it's simpler, more lightweight, and works great with HTTP/2. With GoFrame, implementing SSE becomes even more straightforward and maintainable.</p><p>Here's a quick checklist for your SSE implementation:</p><ul><li>✅ Basic SSE setup with proper headers</li><li>✅ Error handling and connection management</li><li>✅ Authentication and authorization</li><li>✅ Security considerations</li></ul><p>You could extend this implementation by:</p><ul><li>Adding message persistence</li><li>Implementing message replay</li><li>Building client libraries</li><li>Adding WebSocket fallback</li><li>Implementing server-side filtering</li><li>Adding message prioritization</li></ul><p>Have you used SSE in your projects? What challenges did you face? Share your experiences in the comments below! 👇</p><p>P.S. Want to see the complete code? Check out my GitHub repo [link to be added] for a production-ready implementation!</p><p>If you found this helpful, follow me for more Go tutorials and real-world examples! ✨</p>","contentLength":3136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fluvio: A Rust-powered streaming platform using WebAssembly for programmable data processing","url":"https://www.reddit.com/r/rust/comments/1iqgg02/fluvio_a_rustpowered_streaming_platform_using/","date":1739667635,"author":"/u/drc1728","guid":528,"unread":true,"content":"<div><p>I am in the process of writing an essay on composable streaming first architecture for data intensive applications. I am thinking of it as a follow up on this article.</p><p>Quick question for the Rust community:</p><ul><li>What information would help the Rust community know and experience Fluvio?</li><li>What would you like to see covered in the essay?</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/drc1728\"> /u/drc1728 </a>","contentLength":357,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Blunderchess.net – blunder for your opponent every five moves","url":"https://blunderchess.net/","date":1739665321,"author":"eviledamame","guid":514,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43063970"},{"title":"Safe elimination of unnecessary bound checks.","url":"https://www.reddit.com/r/rust/comments/1iqev5s/safe_elimination_of_unnecessary_bound_checks/","date":1739663006,"author":"/u/tjientavara","guid":499,"unread":true,"content":"<p>Hi, I am working on a Unicode database that is pretty fast, it is a 2 step associated lookup.</p><p>Here is the code for getting the east-asian-width value of a Unicode code-point. Pay specific attention to the function. This function is a  function and the byte tables that it references are  as well. This will allow you to eventually run the unicode algorithms at both compile and run-time.</p><p>Since the tables are fixed at compile time, I can proof that all values from the table will result in values that will never break any bounds, so technically the bound checks are unnecessary.</p><p>There are two bound checks in the assembly output for this function.</p><ul><li>The check before accessing the EAST_ASIAN_WIDTH_COLUMN table (I use an assert! to do this, otherwise there will be double bound check).</li><li>And the check on the conversion to the enum.</li></ul><p>The two bound checks are the two compare + conditional-jump instructions in this code.</p><p>I could increase the size of the column table to remove one of the bound checks, but I want to keep the table small if possible.</p><p>Is there a way to safely (I don't want to use the unsafe code) proof to the compiler that those two checks are unnecessary?</p><p>P.S. technically there is a bound check before the index table a CMOV instruction, but it doubles as a way to also decompress the index table (last entry is repeated), so I feel this is not really a bound check.</p><p>I was able to concat the two tables, and use a byte offset. So now there is no way to get an out of bound access, and the bound checks are no longer emitted by the compiler.</p><p>I also added a manual check for out of bound on the enum and return zero instead, this becomes a CMOV and it eliminated all the panic code from the function.</p>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"spf13/cobra v1.9.0","url":"https://github.com/spf13/cobra/releases/tag/v1.9.0","date":1739652313,"author":"/u/jpmmcb","guid":689,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iqawrv/spf13cobra_v190/"},{"title":"Amazon AWS \"whoAMI\" Attack Exploits AMI Name Confusion to Take Over Cloud Instances","url":"https://www.reddit.com/r/programming/comments/1iqav3c/amazon_aws_whoami_attack_exploits_ami_name/","date":1739652192,"author":"/u/Dark-Marc","guid":513,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Newly Curated 40+ Developer Fixes || Try it now :)","url":"https://dev.to/0x2e_tech/newly-curated-40-developer-fixes-try-it-now--14fe","date":1739649805,"author":"0x2e Tech","guid":555,"unread":true,"content":"<h3>\n  \n  \n  1. Go Testing: Force Retests &amp; Disable Caching\n</h3><p>Go Testing: Forcing Retests and Disabling Test Caching  This guide tackles the common issue of stale test results in Go, focusing on how to reliably force retests and disable caching mechanisms.  We'll explore several practical, plug-and-play solutio... <a href=\"https://0x2e.tech/item/go-testing-force-retests-disable-caching\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  2. Flutter Map Key Typing: A Practical Guide for Developers\n</h3><p>Flutter Map Key Typing: A Practical Guide for Developers  Let's face it: dealing with map key typing in Flutter can be a real headache if you don't have a clear strategy.  This guide provides a no-nonsense, plug-and-play approach to mastering this as... <a href=\"https://0x2e.tech/item/flutter-map-key-typing-a-practical-guide-for-developers\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Flutter Development </li></ul><h3>\n  \n  \n  3. Multi-task Learning in TensorFlow: A Practical Guide\n</h3><p>Multi-task Learning in TensorFlow: A Practical Guide  This guide provides a plug-and-play solution for implementing multi-task learning in TensorFlow.  We'll tackle a common scenario: predicting both the sentiment (positive, negative, neutral) and th... <a href=\"https://0x2e.tech/item/multi-task-learning-in-tensorflow-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  4. Tracking Weight Variance During Neural Network Training\n</h3><p>Let's dive into how the variance of weights changes during neural network training.  This is crucial for understanding your model's learning process and diagnosing potential problems.  We'll cover practical methods for tracking this variance and inte... <a href=\"https://0x2e.tech/item/tracking-weight-variance-during-neural-network-training\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  5. Ruby Rescue Best Practices: Why Avoid ?\n</h3><p>Hey there, fellow Rubyist! Let's tackle this common pitfall: why rescuing Exception =&gt; e is a bad idea and how to do it right.  We'll go beyond the basics and explore practical, plug-and-play solutions.  This is for folks who know some Ruby but want ... <a href=\"https://0x2e.tech/item/ruby-rescue-best-practices-why-avoid-rescue-exception-e\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  6. Expo iOS App Store Transfer Error: \"A required agreement is missing\"\n</h3><p>Decoding the \"A required agreement is missing or has expired\" Enigma After Expo App Store Transfer (SDK 51.0.0 and beyond)  Let's face it:  That error message is about as helpful as a chocolate teapot.  You've wrestled with your Expo app, successfull... <a href=\"https://0x2e.tech/item/expo-ios-app-store-transfer-error-a-required-agreement-is-missing\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> iOS App Development </li></ul><h3>\n  \n  \n  7. Android Ktor Job Cancellation: A Practical Guide\n</h3><p>Android Ktor Job Cancellation: A Practical Guide  Let's tackle that pesky \"Job Cancelled\" error when using Ktor on Android. This issue often pops up when a long-running Ktor request gets interrupted, perhaps by a configuration problem, user action, o... <a href=\"https://0x2e.tech/item/android-ktor-job-cancellation-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Android Development </li></ul><h3>\n  \n  \n  8. Mobile-Unfriendly Laravel Site? Fix it Now!\n</h3><p>Alright coder, let's tackle this mobile responsiveness issue head-on.  Your Laravel site's looking great on desktop, but those tiny screens are giving you a headache?  Don't worry, we'll fix it.  This guide is your plug-and-play solution, assuming yo... <a href=\"https://0x2e.tech/item/mobile-unfriendly-laravel-site-fix-it-now\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  9. Angular 18 APP_INITIALIZER: Fixing Dependency Injection Issues\n</h3><p>Angular 18 APP_INITIALIZER Dependency Injection woes? Let's fix it!  So, you're wrestling with Angular 18's APP_INITIALIZER and its frustrating dependency injection quirks?  Don't worry, you're not alone. This seemingly simple mechanism can turn into... <a href=\"https://0x2e.tech/item/angular-18-app-initializer-fixing-dependency-injection-issues\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  10. Jenkins Token Generation for DevOps: A Practical Guide\n</h3><p>Alright, friend! Let's get you a Jenkins API token.  This is crucial for automating tasks and integrating Jenkins with other tools.  Forget the confusing docs – we're going straight to the action.  Understanding the Why:  Before diving in, let's clar... <a href=\"https://0x2e.tech/item/jenkins-token-generation-for-devops-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  11. Fixing Python's  Error: A Quick Guide\n</h3><p>Conquering the AttributeError: module 'pkgutil' has no attribute 'ImpImporter' Beast  Let's face it: that error message is a real mood killer.  It screams, \"Your Python environment is a bit of a mess!\" But fear not, fellow coder! This isn't some insu... <a href=\"https://0x2e.tech/item/fixing-python-s-pkgutil-impimporter-error-a-quick-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  12. Securely Wiping AES XTS Cipher Keys in Go: A Practical Guide\n</h3><p>Securely Wiping AES XTS Cipher Key Material in Go: A Practical Guide  This guide provides a clear, actionable solution for securely wiping AES XTS cipher key material in Go.  We'll tackle this problem head-on, avoiding unnecessary jargon and focusing... <a href=\"https://0x2e.tech/item/securely-wiping-aes-xts-cipher-keys-in-go-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  13. Xamarin Forms: Null Values in IValueConverter? Fix it!\n</h3><p>Hey there, fellow Xaml warrior! Let's tackle this pesky null value problem in your Xamarin Forms IValueConverter.  It's a common hiccup, but once you understand the why and the how, you'll be converting values like a pro.   The Usual Suspects: Why Nu... <a href=\"https://0x2e.tech/item/xamarin-forms-null-values-in-ivalueconverter-fix-it\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  14. Postgres Timestamps: Storing with Time Zone Without UTC Conversion\n</h3><p>Storing Timestamps with Time Zone in Postgres Without UTC Conversion: A Practical Guide  This guide provides a clear, actionable solution for storing timestamps with time zones in PostgreSQL without the automatic conversion to UTC.  We'll tackle this... <a href=\"https://0x2e.tech/item/postgres-timestamps-storing-with-time-zone-without-utc-conversion\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  15. Fix \"cannot load such file -- utils/popen\" on macOS\n</h3><p>Alright, friend! Let's tackle this pesky \"cannot load such file -- utils/popen\" error you're encountering with Homebrew on your macOS system. This usually pops up when Ruby can't find the popen utility, which is crucial for some processes.  We'll wal... <a href=\"https://0x2e.tech/item/fix-cannot-load-such-file-utils-popen-on-macos\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  16. Flutter Web Self-Signed SSL: A Practical Guide\n</h3><p>Flutter Web Self-Signed Certificate Requests: A Straightforward Guide  Let's tackle that pesky self-signed certificate issue in Flutter web development.  You've built your awesome app, connected to your server... but BAM!  That self-signed certificat... <a href=\"https://0x2e.tech/item/flutter-web-self-signed-ssl-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Flutter Web Development </li></ul><h3>\n  \n  \n  17. Docker XRDP Resolution Fix: A Practical Guide\n</h3><p>Docker XRDP Resolution Fix: A Practical Guide  Let's face it: wrestling with Docker and XRDP resolution can feel like a black hole of frustration.  You've got your awesome Docker setup, your XRDP server humming, but the display is all wonky.  Fear no... <a href=\"https://0x2e.tech/item/docker-xrdp-resolution-fix-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  18. Angular Drag-and-Drop: Fixing Nested List Issues\n</h3><p>Angular Drag-and-Drop with Deeply Nested Lists: A Practical Guide  Let's face it: drag-and-drop in Angular with deeply nested lists can be a real headache.  The basic functionality often works fine for simple lists, but the moment you add nesting, th... <a href=\"https://0x2e.tech/item/angular-drag-and-drop-fixing-nested-list-issues\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  19. MySQL Error 1698: iRedMail Root Access Fix\n</h3><p>Alright, friend! Let's tackle this \"ERROR 1698 (28000): Access denied for user 'root'@'localhost'\" issue head-on.  This is a common problem in MySQL, especially when working with iRedMail.  It basically means your root user, the king of your MySQL ki... <a href=\"https://0x2e.tech/item/mysql-error-1698-iredmail-root-access-fix\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  20. Docker to Host File Transfer: A Practical Guide\n</h3><p>Copying Files from Docker Container to Host: A Practical Guide  This guide provides a no-nonsense, step-by-step approach to copying files from your Docker container to your host machine.  We'll cover various methods, ensuring you have the tools to ta... <a href=\"https://0x2e.tech/item/docker-to-host-file-transfer-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  21. Securely Wiping AES XTS Cipher Keys in Go: A Practical Guide\n</h3><p>Securely Wiping AES XTS Cipher Key Material in Go: A Practical Guide  This guide provides a practical, step-by-step solution for securely wiping AES XTS cipher key material in Go.  We'll focus on eliminating sensitive data from memory to prevent pote... <a href=\"https://0x2e.tech/item/securely-wiping-aes-xts-cipher-keys-in-go-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  22. Doubly-Ended Array Priority Queue: Mastering Node Partnerships\n</h3><p>Mastering Node Partnerships in a Doubly-Ended Array-Based Priority Queue (DEAP)  Let's tackle the challenge of correctly managing node partnerships within a Doubly-Ended Array-Based Priority Queue (DEAP).  This isn't your grandma's priority queue; we... <a href=\"https://0x2e.tech/item/doubly-ended-array-priority-queue-mastering-node-partnerships\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Data Structures and Algorithms </li></ul><h3>\n  \n  \n  23. Node ESM + Log4js: A Practical Guide\n</h3><p>Conquering Log4js in Your Node.js ESM Project: A Plug-and-Play Guide  Let's be honest, wrestling with Log4js in an ES module Node.js project can feel like a wrestling match with a greased pig.  But fear not, fellow developer! This guide will walk you... <a href=\"https://0x2e.tech/item/node-esm-log4js-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  24. Fixing SwiftUI Font Twitching: A Practical Guide\n</h3><p>SwiftUI Font Twitching:  A Practical Guide for iOS Developers  Let's be honest, that twitching font in your SwiftUI animation is annoying.  It's like a tiny, digital mosquito buzzing around your otherwise perfect UI. But fear not, fellow developer! W... <a href=\"https://0x2e.tech/item/fixing-swiftui-font-twitching-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  25. Resize SVG Responsively with D3.js: A Practical Guide\n</h3><p>SVG Dimensions on Window Resize: A D3.js Plug-and-Play Solution  This guide provides a straightforward, actionable solution for dynamically resizing SVG elements within a D3.js visualization to match window changes.  We'll ditch the fluff and get str... <a href=\"https://0x2e.tech/item/resize-svg-responsively-with-d3-js-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  26. Demystifying Logits &amp; Softmax in TensorFlow: A Practical Guide\n</h3><p>Demystifying Logits and Softmax in TensorFlow: A Practical Guide  This guide provides a clear, practical explanation of logits and the difference between softmax and softmax_cross_entropy_with_logits in TensorFlow, focusing on direct application and ... <a href=\"https://0x2e.tech/item/demystifying-logits-softmax-in-tensorflow-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  27. JS DevTools: \"Function was resolved from bound function\" Fix\n</h3><p>Alright, detective! Let's crack this 'Function was resolved from bound function' mystery in your JavaScript DevTools.  This cryptic message usually pops up when you're dealing with functions and their context (where they're called from) isn't what yo... <a href=\"https://0x2e.tech/item/js-devtools-function-was-resolved-from-bound-function-fix\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  28. VueDatePicker + v-mask: A Practical Guide\n</h3><p>Mastering the Mashup: VueDatePicker and v-mask  Let's face it:  combining VueDatePicker and v-mask can feel like wrestling a greased pig.  But fear not, fellow developer! This guide will walk you through a smooth, streamlined integration, turning tha... <a href=\"https://0x2e.tech/item/vuedatepicker-v-mask-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  29. Is My Image Gamma Encoded? A NumPy Guide for Image Pros\n</h3><p>Hey there, fellow image processing enthusiast!  Let's tackle this gamma encoding mystery. You've loaded your image into NumPy, and now you're wondering: Is it gamma-encoded or not?  Fear not! We'll unravel this with a practical, plug-and-play approac... <a href=\"https://0x2e.tech/item/is-my-image-gamma-encoded-a-numpy-guide-for-image-pros\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Image Processing </li></ul><h3>\n  \n  \n  30. MySQL Functions: Returning Multiple Rows\n</h3><p>Returning multiple rows from a MySQL function can be tricky, but it's definitely doable. The key is understanding that MySQL functions, unlike stored procedures, are designed to return a single value.  To get around this limitation, we'll leverage a ... <a href=\"https://0x2e.tech/item/mysql-functions-returning-multiple-rows\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  31. Flutter Web Theme Meta Tags: Why Mobile Browsers Ignore Them?\n</h3><p>Flutter Web Theme Meta Tags: Why Mobile Browsers Ignore Them?  Let's tackle this head-on.  You've diligently crafted your Flutter web app, meticulously set your theme meta tags, and yet, mobile browsers are ignoring them.  Frustrating, right? This is... <a href=\"https://0x2e.tech/item/flutter-web-theme-meta-tags-why-mobile-browsers-ignore-them\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Flutter Web Development </li></ul><h3>\n  \n  \n  32. ASP.NET Pragma Header Won't Go Away?  A Quick Fix\n</h3><p>Alright, friend!  Let's tackle this pesky Pragma header that's sticking around in your ASP.NET application even after you've seemingly banished it from your code.  This is a common issue, and it usually boils down to a few sneaky culprits.  We'll go ... <a href=\"https://0x2e.tech/item/asp-net-pragma-header-won-t-go-away-a-quick-fix\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  33. AWS EC2 Uptime: Node.js &amp; TypeScript Solution\n</h3><p>Calculating AWS EC2 Instance Uptime: A Node.js and TypeScript Guide  This guide provides a practical, plug-and-play solution for calculating the uptime of an AWS EC2 instance from the moment it enters the \"running\" state. We'll leverage the AWS SDK f... <a href=\"https://0x2e.tech/item/aws-ec2-uptime-node-js-typescript-solution\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  34. JS Radio Group Arrow Key Control: A Plug-and-Play Guide\n</h3><p>JavaScript Radio Group Navigation: A Plug-and-Play Guide  This guide provides a practical, step-by-step solution to enhance the user experience of multiple radio groups by enabling navigation using the Enter, Up, and Down arrow keys. We'll ditch the ... <a href=\"https://0x2e.tech/item/js-radio-group-arrow-key-control-a-plug-and-play-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  35. Stop Clicks Under CSS Popovers: A Quick Guide\n</h3><p>Stop Clicks Under CSS Popovers: A Quick Guide  This guide provides a straightforward solution to prevent clicks from reaching elements beneath a CSS popover's backdrop.  We'll use JavaScript to handle the event and stop propagation.  This is crucial ... <a href=\"https://0x2e.tech/item/stop-clicks-under-css-popovers-a-quick-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  36. Flutter: Align Dynamic Row Element\n</h3><p>Flutter: Aligning a Single Element in a Dynamic Height Row  Let's tackle this common Flutter layout challenge: aligning a single element within a row where the row's height changes dynamically.  This often pops up when you have elements of varying si... <a href=\"https://0x2e.tech/item/flutter-align-dynamic-row-element\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  37. Dark Theme CSS Fix: A Web Developer's Guide\n</h3><p>Dark Theme CSS Mayhem? Let's Fix It!   So, your website looks fabulous in light mode, but the second someone flips the switch to dark mode, it's a CSS catastrophe?  Don't worry, you're not alone.  Many developers hit this snag.  This guide will walk ... <a href=\"https://0x2e.tech/item/dark-theme-css-fix-a-web-developer-s-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  38. AWS SDK Port Forwarding: Troubleshooting Short-Lived Sessions (C#)\n</h3><p>AWS SDK Port Forwarding: Why Your Sessions Die in 30 Seconds (and How to Fix It)  Let's be honest, debugging flaky network connections is nobody's favorite pastime.  But when your AWS SDK port forwarding sessions keep crapping out after less than 30 ... <a href=\"https://0x2e.tech/item/aws-sdk-port-forwarding-troubleshooting-short-lived-sessions-c\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  39. Fixing Keras' pad_sequences Import Error: A Quick Guide\n</h3><p>Conquering the 'cannot import name 'pad_sequences'' Error in Keras  Let's be honest, import errors are the bane of every programmer's existence.  That frustrating red squiggly line underlining your perfectly crafted code?  Yeah, we've all been there.... <a href=\"https://0x2e.tech/item/fixing-keras-pad-sequences-import-error-a-quick-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  40. PHP's ?? Operator: A Practical Guide for Beginners\n</h3><p>Hey there, fellow coder! Let's dive into PHP's null coalescing operator, ??.  This little guy is a lifesaver when dealing with potentially null values and will save you from writing tons of tedious if statements.  Think of it as a supercharged way to... <a href=\"https://0x2e.tech/item/php-s-operator-a-practical-guide-for-beginners\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  41. Spring WebSockets: User-Specific Subscriptions\n</h3><p>Alright, let's tackle this Spring WebSocket user subscription puzzle.  The goal?  Get messages to the right user, securely. Forget the fluff, let's get practical.  This solution uses Spring Security for authentication and authorization, ensuring only... <a href=\"https://0x2e.tech/item/spring-websockets-user-specific-subscriptions\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Spring WebSockets </li></ul><h3>\n  \n  \n  42. Angular SSR: Mastering server.ts and main.server.ts\n</h3><p>Angular SSR: Demystifying server.ts and main.server.ts  Let's get this straight:  Server-Side Rendering (SSR) in Angular can feel like navigating a labyrinth.  But fear not! We'll cut through the confusion and give you a practical, plug-and-play guid... <a href=\"https://0x2e.tech/item/angular-ssr-mastering-server-ts-and-main-server-ts\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  43. SageMaker Training Stuck? Deep Learning Fixes\n</h3><p>Decoding the SageMaker Training Job Enigma: A Practical Guide  So, your SageMaker training job is chilling in a state of suspended animation?  The code compiles, the setup looks good, but nothing's happening in the training phase. Let's troubleshoot ... <a href=\"https://0x2e.tech/item/sagemaker-training-stuck-deep-learning-fixes\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Amazon SageMaker </li></ul><h3>\n  \n  \n  44. Braze Connected Content GraphQL: A Practical Guide\n</h3><p>Braze \"Connected Content\" GraphQL Request: A Practical Guide  This guide provides a step-by-step solution for making GraphQL requests to Braze's Connected Content API. We'll cover everything from setting up authentication to handling responses, focus... <a href=\"https://0x2e.tech/item/braze-connected-content-graphql-a-practical-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  45. PHP Progress Bar: A Step-by-Step Guide\n</h3><p>PHP Progress Bar: A Step-by-Step Guide  This guide provides a practical, plug-and-play solution for creating a dynamic progress bar in PHP, updating in real-time over a period.  We'll cover the PHP backend, HTML frontend, and CSS styling using SASS f... <a href=\"https://0x2e.tech/item/php-progress-bar-a-step-by-step-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  46. Resize Egui Tables by Column: A Rust Guide\n</h3><p>Resize Egui Tables by Column: A Rust Guide  This guide provides a practical, plug-and-play solution for creating resizable tables in egui using Rust. We'll build a robust and efficient solution, avoiding unnecessary complexity.  Understanding the Cha... <a href=\"https://0x2e.tech/item/resize-egui-tables-by-column-a-rust-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  47. C#/.NET: Why avoid inheriting from List? Practical solutions\n</h3><p>C#/.NET: Why Avoid Inheriting from List? Practical Solutions  Let's cut to the chase.  You're a C# developer, you've encountered List, and you're thinking, \"Hey, I need a specialized list!  Inheriting seems efficient.\"  Hold your horses, partne... <a href=\"https://0x2e.tech/item/c-net-why-avoid-inheriting-from-list-t-practical-solutions\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  48. Next.js, Antd, Sass/Less: The Ultimate Guide\n</h3><p>Next.js, Ant Design, Sass/Less: A Plug-and-Play Guide  This guide provides a no-nonsense, step-by-step approach to integrating Ant Design with Next.js, using either Less, Sass, or CSS Modules.  We'll focus on practicality, offering ready-to-use code ... <a href=\"https://0x2e.tech/item/next-js-antd-sass-less-the-ultimate-guide\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  49. Rust Vector Concatenation: The Ultimate Guide for Programmers\n</h3><p>Rust Vector Concatenation: The Ultimate Guide for Programmers  This guide provides a no-nonsense, practical approach to concatenating vectors in Rust.  We'll cover various methods, highlighting their strengths and weaknesses, so you can choose the be... <a href=\"https://0x2e.tech/item/rust-vector-concatenation-the-ultimate-guide-for-programmers\" rel=\"noopener noreferrer\">Read More</a></p><ul><li> Rust Programming </li></ul><h3>\n  \n  \n  50. Webpack 5 Breaking Change: Fixing Node.js Module Polyfills\n</h3><p>Webpack 5 Breaking Change: Banishing the Node.js Polyfill Gremlins  Let's be honest, that \"Webpack 5 breaking change\" error message is a real mood killer.  You're cruising along, building your awesome React app, and suddenly BAM!  Webpack throws a ta... <a href=\"https://0x2e.tech/item/webpack-5-breaking-change-fixing-node-js-module-polyfills\" rel=\"noopener noreferrer\">Read More</a></p><h3>\n  \n  \n  💰 <strong>Want to Earn 40% Commission?</strong></h3><p>Join our affiliate program and start making money by promoting ! Earn 40% on every sale you refer.  </p><p>You'll on average around 5$ per sell and for bundled products it will be around 40$ per sale. (So just share it and make money with worrying about product creation and maintanence)</p>","contentLength":16908,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incoming Rust intern need advice?","url":"https://www.reddit.com/r/rust/comments/1iq9oph/incoming_rust_intern_need_advice/","date":1739649133,"author":"/u/Helpful_Ad_9930","guid":471,"unread":true,"content":"<p>Hey everyone, I'm a 19-year-old college student who just landed a SWE internship at NVIDIA! My manager has me learning Rust and exploring one of its libraries, and I’m also reading up on operating systems and computer networking. I'm almost done with the OS book and plan to start the networking one next week.</p><p>I do have a bit of experience with embedded systems I completed two internships during my freshman year. However, so far I’m really enjoying Rust. I am quite a rookie compared to you experienced folks haha! But so far I love how Rust's compiler enforces safety, how Cargo makes dependency management a breeze compared to CMake, and the whole concept of ownership and borrowing is just super cool.</p><p>At the moment, I’m nearly finished with the Rust book. I am on the concurrency chapter. Guess I am just wondering what next? I really want this return offer and I just want to blow this opportunity out the park. I go too a state school and my manager told me he has high expectations for me after my interviews. I just do not want to let him down you know also plus kind of getting impostor syndrome a bit seeing all the other interns coming from schools such as MIT, Harvard, Standford, etc. Sorry for the vent I guess I just want to prove my worth? and show my manager they made the right choice?</p><ul><li>What fun, Rust projects have helped you learn a lot?</li><li>Are there any books you’d recommend that could help me out for the summer?</li></ul><p><strong>Books I want to read before I start summer:</strong></p><ul><li>Operating Systems (Three easy pieces)</li><li>Beej's Guide to Network Programming</li><li>C++ Concurrency in Action</li></ul>","contentLength":1578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Is my company missing out by avoiding deep learning?","url":"https://www.reddit.com/r/MachineLearning/comments/1iq9gtk/d_is_my_company_missing_out_by_avoiding_deep/","date":1739648562,"author":"/u/DatAndre","guid":257,"unread":true,"content":"<p>Disclaimer: obviously it does not make sense to use a neural network if a linear regression is enough. </p><p>I work at a company that strictly adheres to mathematical, explainable models. Their stance is that methods like Neural Networks or even Gradient Boosting Machines are too \"black-box\" and thus unreliable for decision-making. While I understand the importance of interpretability (especially in mission critical scenarios) I can't help but feel that this approach is overly restrictive. </p><p>I see a lot of research and industry adoption of these methods, which makes me wonder: are they really just black boxes, or is this an outdated view? Surely, with so many people working in this field, there must be ways to gain insights into these models and make them more trustworthy. </p><p>Am I also missing out on them, since I do not have work experience with such models?</p><p>EDIT: Context is formula one! However, races are a thing and support tools another. I too would avoid such models in anything strictly related to a race, unless completely necessary. I just feels that there's a bias that is context-independent here. </p>","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"‘Mass theft’: Thousands of artists call for AI art auction to be cancelled","url":"https://www.theguardian.com/technology/2025/feb/10/mass-theft-thousands-of-artists-call-for-ai-art-auction-to-be-cancelled","date":1739647902,"author":"/u/F0urLeafCl0ver","guid":691,"unread":true,"content":"<p>Thousands of artists are urging the auction house Christie’s to cancel a sale of art created with artificial intelligence, claiming the technology behind the works is committing “mass theft”.</p><p>The <a href=\"https://www.christies.com/en/auction/augmented-intelligence-23942-nyr/\" data-link-name=\"in body link\">Augmented Intelligence auction</a> has been described by Christie’s as the first AI-dedicated sale by a major auctioneer and features 20 lots with prices ranging from $10,000 to $250,000 for works by artists including Refik Anadol and the late AI art pioneer Harold Cohen.</p><p>A <a href=\"https://openletter.earth/cancel-the-christies-ai-art-auction-f5135435?limit=0\" data-link-name=\"in body link\">letter calling for the auction to be scrapped</a> has received 3,000 signatures, including from Karla Ortiz and Kelly McKernan, who are suing AI companies over claims that tech firms’ image generation tools have used their work without permission.</p><p>The letter says: “Many of the artworks you plan to auction were created using AI models that are known to be trained on copyrighted work without a licence. These models, and the companies behind them, exploit human artists, using their work without permission or payment to build commercial AI products that compete with them.”</p><p>Calling on Christie’s to cancel the auction, which starts on 20 February, it adds: “Your support of these models, and the people who use them, rewards and further incentivizes AI companies’ mass theft of human artists’ work.”</p><p>The use of copyrighted work to train AI models – the technology that underpins chatbots and image generation tools such as Stable Diffusion and Midjourney – has become a battleground between creatives and tech companies, with artists, <a href=\"https://www.theguardian.com/technology/2023/jul/10/sarah-silverman-sues-openai-meta-copyright-infringement\" data-link-name=\"in body link\">authors</a>, <a href=\"https://www.theguardian.com/media/2023/dec/27/new-york-times-openai-microsoft-lawsuit\" data-link-name=\"in body link\">publishers</a> and <a href=\"https://www.theguardian.com/music/article/2024/jun/25/record-labels-sue-ai-song-generator-apps-copyright-infringement-lawsuit\" data-link-name=\"in body link\">music labels</a> launching a series of lawsuits alleging breach of copyright.</p><p>The British composer Ed Newton-Rex, a key figure in the <a href=\"https://www.theguardian.com/film/2024/oct/22/thom-yorke-and-julianne-moore-join-thousands-of-creatives-in-ai-warning\" data-link-name=\"in body link\">campaign by creative professionals</a> for protection of their work and a signatory to the letter, said at least nine of the works appearing in the auction appeared to have used models trained on artists’ work. However, other pieces in the auction do not appear to have used such models.</p><p>A spokesperson for Christie’s said that “in most cases” the AI used to create art in the auction had been trained on the artists’ “own inputs”.</p><p>“The artists represented in this sale all have strong, existing multidisciplinary art practices, some recognised in leading museum collections. The works in this auction are using artificial intelligence to enhance their bodies of work and in most cases AI is being employed in a controlled manner, with data trained on the artists’ own inputs,” said the spokesperson.</p><p>A British artist whose work features in the auction, Mat Dryhurst, said he cared about the issue of art and AI “deeply” and rejected the criticisms in the letter. A piece by Dryhurst and his wife, Holly Herndon – based on a work called xhairymutantx – is on sale at the auction with an estimated price of between $70,000 and $90,000.</p><p>“This is of interest to us and we have made a lot of art exploring and attempting to intervene in this process as is well within our rights.”</p><p>He added: “It is not illegal to use any model to create artwork. I resent that an important debate that should be focused on companies and state policy is being focused on artists grappling with the technology of our time.”</p><p>Anadol also rejected the criticism. In a post on X, he said the backlash was a consequence of “lazy critic practices and doomsday hysteria”.</p><p>Anadol told the Guardian the piece being auctioned, ISS Dreams, was created using AI technology that had been trained on publicly available datasets from NASA “that have been used widely by artists for many decades.”</p><p>He added that “the suggestion that this artwork was created using ‘AI models that are known to be on copyrighted work trained without a license’ is factually incorrect.”</p>","contentLength":3744,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq97ti/mass_theft_thousands_of_artists_call_for_ai_art/"},{"title":"Zed for golang","url":"https://www.reddit.com/r/golang/comments/1iq8jsm/zed_for_golang/","date":1739646156,"author":"/u/MrBricole","guid":518,"unread":true,"content":"<p>I am considering using zed for writting go. Is it working out of the box with full syntax high light for noob like me such fmt.Println() ? I mean, I need to have it displaying functions under an import library.</p><p>Should I give it a try or is it only for advanced users ? </p>","contentLength":268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pushing autovectorization to the limit: utf-8 validator","url":"https://www.reddit.com/r/rust/comments/1iq7yn2/pushing_autovectorization_to_the_limit_utf8/","date":1739644600,"author":"/u/Laiho3","guid":483,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Laiho3\"> /u/Laiho3 </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing encode: Encoders/serializers made easy.","url":"https://www.reddit.com/r/rust/comments/1iq6pz7/introducing_encode_encodersserializers_made_easy/","date":1739641353,"author":"/u/Compux72","guid":282,"unread":true,"content":"<p> is a toolbox for building encoders and serializers in Rust. It is heavily inspired by the <a href=\"https://docs.rs/winnow/latest/winnow/\"></a> and <a href=\"https://docs.rs/nom/latest/nom/\"></a> crates, which are used for building parsers. It is meant to be a companion to these crates, providing a similar level of flexibility and ease of use for reversing the parsing process.</p><p>The main idea behind  is to provide a set of combinators for building serializers. These combinators can be used to build complex encoders from simple building blocks. This makes it easy to build encoders for different types of data, without having to write a lot of boilerplate code.</p><p>Another key feature of  is its support for  environments. This makes it suitable for use in embedded systems, where the standard library (and particularly the [] module) is not available.</p><p>See the <a href=\"https://github.com/Altair-Bueno/encode/tree/master/examples\"></a> folder for some examples of how to use . Also, check the <a href=\"https://docs.rs/encode/0.1.0/encode/combinators/index.html\"></a> module for a list of all the combinators provided by the crate.</p><ul><li>Ready to use combinators for minimizing boilerplate.</li></ul><ul><li>: Enables the  feature.</li><li>: Enables the use of the standard library.</li><li>: Enables the use of the  crate.</li><li>: Implements [] for [].</li></ul><h3>Why the  trait instead of ?</h3><blockquote><p>A buffer stores bytes in memory such that write operations are . The underlying storage may or may not be in contiguous memory. A BufMut value is a cursor into the buffer. Writing to BufMut advances the cursor position.</p></blockquote><p>The bytes crate was never designed with falible writes nor  targets in mind. This means that targets with little memory are forced to crash when memory is low, instead of gracefully handling errors.</p><h3>Why the  trait instead of ?</h3><ul><li>Because there is no alternative, at least that i know of, that supports  properly</li><li>Because it's easier to work with than  and </li><li>Because using  with binary data often leads to a lot of boilerplate</li></ul>","contentLength":1715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust and the Null Paradigm: Exploring Safety and Alternatives","url":"https://dev.to/dedsecrattle/rust-and-the-null-paradigm-exploring-safety-and-alternatives-47b","date":1739641004,"author":"Prabhat Kumar","guid":554,"unread":true,"content":"<p>Rust is a systems programming language known for its focus on memory safety, concurrency, and performance. One of the key decisions made by the Rust team is the choice to <strong>not support the null paradigm</strong>. While this design choice leads to safer, more reliable code, it raises an important question for developers: <strong>How do we handle the absence of a value?</strong></p><p>In many programming languages, the concept of  or  (depending on the language) is used to represent the absence of a value. This approach, however, introduces a number of issues:</p><ul><li><strong>Null Pointer Dereferencing:</strong> Accessing a  pointer can lead to runtime errors that are often hard to debug.</li><li> values can lead to subtle bugs when programmers forget to check for them, causing unexpected behaviors in applications.</li></ul><p>Rust decided to leave behind this problematic paradigm in favor of alternatives that promote safety at compile-time.</p><h2>\n  \n  \n  Rust's Approach: </h2><p>Rust takes a unique approach to handling the absence of a value: it uses the  enum. This powerful construct allows developers to explicitly handle the presence or absence of a value.</p><p>The  type is defined as:</p><div><pre><code></code></pre></div><ul><li> represents the presence of a value.</li><li> represents the absence of a value.</li></ul><p>This makes  a much safer alternative to . The compiler forces you to explicitly handle both cases ( and ), reducing the risk of null pointer exceptions.</p><p>Here's a simple example of how you might use  to handle optional values:</p><div><pre><code></code></pre></div><p>In this example, instead of returning  when the user isn't found, we return , and the caller must handle the potential absence of a value.</p><h2>\n  \n  \n  Why  Is Better Than </h2><ul><li> With , Rust ensures that you never have to deal with null values unless you explicitly decide to. This eliminates the common pitfalls of null pointer dereferencing.</li><li> Rust’s borrow checker ensures that you handle all  cases correctly, even when dealing with complex ownership and lifetime semantics.</li><li> Rust’s pattern matching syntax makes it easy to express the logic of handling  and  values, leading to clean and readable code.</li></ul><h2>\n  \n  \n  Other Alternatives:  for Error Handling\n</h2><p>In addition to , Rust also offers the  enum for handling operations that might fail. The  type is especially useful when a function might produce either a valid result or an error, combining both success and failure cases into a single, explicit structure.</p><div><pre><code></code></pre></div><p>Just like , the  type forces developers to handle both cases explicitly, improving robustness and error recovery in your programs.</p><h2>\n  \n  \n  Conclusion: Embracing Safety and Clarity\n</h2><p>Rust’s rejection of the  paradigm and adoption of types like  is a conscious choice to create safer, more reliable code. By forcing developers to handle the possibility of missing or invalid data explicitly, Rust eliminates the risks and headaches often associated with .</p><p>While this approach might feel unfamiliar to developers coming from languages with , it quickly becomes a strength of the language. Embracing  (and ) in your code not only prevents bugs but also promotes a more clear, understandable way of thinking about data and operations in your applications.</p>","contentLength":3048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust - Ownership Model","url":"https://dev.to/dedsecrattle/rust-ownership-model-1l6j","date":1739640889,"author":"Prabhat Kumar","guid":553,"unread":true,"content":"<p>Rust’s ownership model is one of its most powerful and defining features. It provides memory safety without needing a garbage collector, making Rust highly efficient and reliable. If you're coming from languages like C++, Java, or Python, understanding Rust’s ownership system might feel daunting at first. In this post, we'll break it down step by step.</p><p><strong>What is Ownership in Rust?</strong></p><p>Ownership is Rust’s unique way of managing memory. Instead of using garbage collection or manual memory management, Rust enforces strict ownership rules at compile time. These rules ensure memory safety and prevent data races in concurrent programs.</p><p>The three key ownership rules are:</p><ol><li><strong>Each value in Rust has a single owner.</strong></li><li><strong>When the owner goes out of scope, Rust automatically deallocates the value.</strong></li><li><strong>Ownership can be transferred (moved) or borrowed (immutably or mutably).</strong></li></ol><h2>\n  \n  \n  Moving, Copying, and Cloning\n</h2><p>When assigning a value from one variable to another, ownership is transferred. Consider this example:</p><div><pre><code></code></pre></div><p>Since  is allocated on the heap, Rust prevents double-free errors by invalidating  when ownership moves to .</p><p>Certain types implement the  trait, meaning they are duplicated instead of moved. Examples include:</p><div><pre><code></code></pre></div><p>Primitive types (integers, floats, booleans, etc.) implement , so they don’t follow move semantics.</p><p>If you need to duplicate heap-allocated data, use :</p><div><pre><code></code></pre></div><p>Cloning explicitly creates a separate copy in memory, avoiding move-related issues.</p><p>Rust allows borrowing instead of transferring ownership. Borrowing enables passing data without giving up ownership.</p><p>A reference () allows read-only access to data without taking ownership:</p><div><pre><code></code></pre></div><p>You can have multiple immutable borrows at the same time, but not if there’s a mutable borrow.</p><p>A mutable reference () allows modification but enforces exclusivity:</p><div><pre><code></code></pre></div><p>Rust ensures at compile time that you cannot have multiple mutable references or a mix of mutable and immutable references at the same time.</p><h2>\n  \n  \n  Lifetimes: Ensuring Valid References\n</h2><p>Rust’s  prevent dangling references. Consider this example:</p><div><pre><code></code></pre></div><p>The  lifetime annotation ensures that the returned reference is valid as long as both input references are valid.</p><h2>\n  \n  \n  Why Rust’s Ownership Model Matters\n</h2><ol><li>: No need for garbage collection, yet Rust prevents use-after-free and memory leaks.</li><li>: Enforces thread safety at compile time.</li><li>: Eliminates runtime overhead associated with memory management.</li><li>: Code is predictable and free from subtle memory bugs.</li></ol><p>Rust’s ownership model might take some getting used to, but once you grasp it, you gain the power to write efficient and safe code without worrying about memory leaks. By understanding moves, copies, borrowing, and lifetimes, you can write highly performant Rust applications while maintaining safety guarantees.</p><p>Are you currently learning Rust? Let me know what aspects of ownership you find the most challenging in the comments below!</p>","contentLength":2874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lil guy is trying his best","url":"https://www.reddit.com/r/artificial/comments/1iq6dyy/lil_guy_is_trying_his_best/","date":1739640466,"author":"/u/MetaKnowing","guid":292,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transition from C++ to Rust","url":"https://www.reddit.com/r/rust/comments/1iq67vq/transition_from_c_to_rust/","date":1739640014,"author":"/u/Dvorakovsky","guid":281,"unread":true,"content":"<p>Guys, are here any people who were learning/coding in C++ and switched to Rust. How do you feel? I mean I could easily implement linked lists: singly, doubly in c++, but when I saw how it is implemented in Rust I'd say I got lost completely. I'm only learning rust... So yeah, I really like ownership model even tho it puts some difficulties into learning, but I think it's a benefit rather than a downside. Even tho compared to C++ syntax is a bit messy for me</p>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No, your GenAI model isn't going to replace me","url":"https://marioarias.hashnode.dev/no-your-genai-model-isnt-going-to-replace-me","date":1739639200,"author":"/u/dh44t","guid":279,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq5wq0/no_your_genai_model_isnt_going_to_replace_me/"},{"title":"Type safe Go money library beta2!","url":"https://www.reddit.com/r/golang/comments/1iq5stk/type_safe_go_money_library_beta2/","date":1739638928,"author":"/u/HawkSecure4957","guid":288,"unread":true,"content":"<p>Hello, after I released beta1, I received many constructive feedback! mainly lacking of locale support.</p><p>This update brings locale formatting support and an improved interface for better usability. With Fulus, you can perform monetary operations safely and type-soundly. Plus, you can format money for any locale supported by CLDR. You can even define custom money types tailored specifically to your application's needs! </p><p>I still need to battle test it against production projects, I have none at the moment. I am aiming next for performance benchmarking and more improvement, and parsing from string!</p><p>I am open for more feedback. Thank you! </p>","contentLength":639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Golang Mastery Exercises","url":"https://www.reddit.com/r/golang/comments/1iq5k7w/golang_mastery_exercises/","date":1739638307,"author":"/u/Temporary-Buy-7562","guid":289,"unread":true,"content":"<p>I made a repository which has a prompt for you to write many exercises, if you complete this, and then drill the exercises, I would be sure you would reach mastery with the core of the language.</p><p>I initially wanted to make some exercises for drilling syntax since I use copilot and lsps a lot, but ended up with quite a damn comprehensive list of things you would want to do with the language, and I find this more useful than working on leetcode to really adopt the language.</p>","contentLength":474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Have any LLM papers predicted a token in the middle rather than the next token?","url":"https://www.reddit.com/r/MachineLearning/comments/1iq4f0r/d_have_any_llm_papers_predicted_a_token_in_the/","date":1739635189,"author":"/u/TheWittyScreenName","guid":516,"unread":true,"content":"<p>I’m working on a project (unrelated to NLP) where we use essentially the same architecture and training as GPT-3, but we’re more interested in finding a series of tokens to connect a starting and ending “word” than the next “word”. Since we’re drawing a lot from LLMs in our setup, I’m wondering if there’s been any research into how models perform when the loss function isn’t based on the next token, but instead predicting a masked token somewhere in the input sequence. </p><p>Eventually we would like to expand this (maybe through fine tuning) to predict a longer series of missing tokens than just one but this seems like a good place to start. </p><p>I couldn’t find much about alternate unsupervised training schemes in the literature but it seems like someone must have tried this already. Any suggestions, or reasons that this is a bad idea?</p>","contentLength":859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust - Ownership Model","url":"https://dev.to/dedsecrattle/rust-ownership-model-2bak","date":1739631914,"author":"Prabhat Kumar","guid":552,"unread":true,"content":"<p>Rust’s ownership model is one of its most powerful and defining features. It provides memory safety without needing a garbage collector, making Rust highly efficient and reliable. If you're coming from languages like C++, Java, or Python, understanding Rust’s ownership system might feel daunting at first. In this post, we'll break it down step by step.</p><p><strong>What is Ownership in Rust?</strong></p><p>Ownership is Rust’s unique way of managing memory. Instead of using garbage collection or manual memory management, Rust enforces strict ownership rules at compile time. These rules ensure memory safety and prevent data races in concurrent programs.</p><p>The three key ownership rules are:</p><ol><li><strong>Each value in Rust has a single owner.</strong></li><li><strong>When the owner goes out of scope, Rust automatically deallocates the value.</strong></li><li><strong>Ownership can be transferred (moved) or borrowed (immutably or mutably).</strong></li></ol><h2>\n  \n  \n  Moving, Copying, and Cloning\n</h2><p>When assigning a value from one variable to another, ownership is transferred. Consider this example:</p><div><pre><code></code></pre></div><p>Since  is allocated on the heap, Rust prevents double-free errors by invalidating  when ownership moves to .</p><p>Certain types implement the  trait, meaning they are duplicated instead of moved. Examples include:</p><div><pre><code></code></pre></div><p>Primitive types (integers, floats, booleans, etc.) implement , so they don’t follow move semantics.</p><p>If you need to duplicate heap-allocated data, use :</p><div><pre><code></code></pre></div><p>Cloning explicitly creates a separate copy in memory, avoiding move-related issues.</p><p>Rust allows borrowing instead of transferring ownership. Borrowing enables passing data without giving up ownership.</p><p>A reference () allows read-only access to data without taking ownership:</p><div><pre><code></code></pre></div><p>You can have multiple immutable borrows at the same time, but not if there’s a mutable borrow.</p><p>A mutable reference () allows modification but enforces exclusivity:</p><div><pre><code></code></pre></div><p>Rust ensures at compile time that you cannot have multiple mutable references or a mix of mutable and immutable references at the same time.</p><h2>\n  \n  \n  Lifetimes: Ensuring Valid References\n</h2><p>Rust’s  prevent dangling references. Consider this example:</p><div><pre><code></code></pre></div><p>The  lifetime annotation ensures that the returned reference is valid as long as both input references are valid.</p><h2>\n  \n  \n  Why Rust’s Ownership Model Matters\n</h2><ol><li>: No need for garbage collection, yet Rust prevents use-after-free and memory leaks.</li><li>: Enforces thread safety at compile time.</li><li>: Eliminates runtime overhead associated with memory management.</li><li>: Code is predictable and free from subtle memory bugs.</li></ol><p>Rust’s ownership model might take some getting used to, but once you grasp it, you gain the power to write efficient and safe code without worrying about memory leaks. By understanding moves, copies, borrowing, and lifetimes, you can write highly performant Rust applications while maintaining safety guarantees.</p><p>Are you currently learning Rust? Let me know what aspects of ownership you find the most challenging in the comments below!</p>","contentLength":2874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Larry Ellison wants to put all US data in one big AI system","url":"https://www.theregister.com/2025/02/12/larry_ellison_wants_all_data/","date":1739631593,"author":"/u/namanyayg","guid":602,"unread":true,"content":"<p>If governments want AI to improve services and security for their citizens, then they need to put all their information in one place – even citizens’ genomic data – according to Larry Ellison, the Oracle database tycoon.</p><p>Ellison shared his take on what governments need to do to succeed with AI during a <a href=\"https://www.youtube.com/watch?v=FG2AtiInwKM\" rel=\"nofollow\">discussion</a> with <a href=\"https://www.theregister.com/2023/08/10/ellison_become_major_contributor_to/\">his buddy</a> former UK prime minister Tony Blair at the World Governments Summit in Dubai today.</p><p>The world's fourth-most-richest man – a <a rel=\"nofollow\" target=\"_blank\" href=\"https://www.nytimes.com/interactive/2024/12/23/business/elon-musk-trump-family-friends-backers.html\">good friend</a> also of the world's richest man <a target=\"_blank\" href=\"https://www.theregister.com/2025/02/07/opinion_column_musk/\">Elon Musk</a> – insisted artificial intelligence is soon going to change everyone's lives \"across the board.\"</p><blockquote><p>I have to tell the AI model as much about my country as I can. We need to unify all the national data</p></blockquote><p>If governments want in, they’ll need to gather all their data – spatial information, economic data, electronic healthcare records including genomic data, and info about infrastructure. Whatever they’ve got, basically. And put it all in one place to be analyzed by algorithms. The American multi-billionaire used the United States as an example, if not a goal.</p><p>\"I have to tell [the] AI model as much about my country as I can,\" Ellison said. \"We need to unify all the national data, put it into a database where it's easily consumable by the AI model, and then ask whatever question you like,\" he said. \"That's the missing link.\"</p><p>He believes the payoff will include better healthcare, thanks to treatments tailored to individuals, and the ability for governments to lift food production by better predicting crop yields. Analyzing land so that farmers can be advised where to apply fertilizers or increase irrigation was another scenario Ellison floated.</p><p>\"As long as countries will put their data - all of it - in a single place we can use AI to help manage the care of all of the patients and the population at large,\" Ellison said, adding his belief that AI can handle other social services and eliminate fraud.</p><p>Of course, such a vast database system could also be the precursor to pervasive surveillance – an idea Ellison last year <a href=\"https://www.theregister.com/2024/09/16/oracle_ai_mass_surveillance_cloud/\">said</a> he feels is desirable and would like Oracle to help facilitate.</p><p>Constant real-time surveillance of populations, analyzed by Oracle-powered machine-learning products, would keep everyone \"on their best behavior,\" Ellison said at an Oracle financial analyst conference in September 2024. We're reminded of the <a target=\"_blank\" href=\"https://www.theregister.com/2019/06/26/nsa_spy_program_aclu/\">NSA</a>, <a target=\"_blank\" href=\"https://www.theregister.com/2013/06/07/prism_plan_for_nsa_surveillance_of_internet_companies/\">PRISM</a>, <a target=\"_blank\" href=\"https://www.theregister.com/2014/01/24/snowden_speaks_nsa_whistleblower_calls_for_global_privacy_standards/\">Snowden</a>.</p><p>Ellison is not just a techno-optimist. He’s also a top executive and shareholder who has made big AI investments as well as a database company to feed.</p><p>He therefore told the Dubai audience that Oracle, already a <a target=\"_blank\" href=\"https://www.theregister.com/2025/02/08/doge_us_goverment_tech_spending/\">big-time government and military contractor</a>, is ready to help nations realize his above-mentioned AI visions. Ie: Put all this data into one big expensive Oracle system to learn from and process.</p><p>\"Oracle is building a 2.2GW datacenter that costs between $50 and $100 billion dollars to build,\" Ellison said, noting it's sites like that where super-powered AI models will be trained. \"Because these models are so expensive, you won't build your own as a rule. There'll be a handful of these models.\"</p><p>And a handful of players that can train them. Oracle’s own facilities will likely be one. The super-corp has also joined another, the <a href=\"https://www.theregister.com/2025/01/22/openai_stargate_ai_datacenter_company\">Stargate project</a>, that plans to blow $500 billion on AI infrastructure in the US in the next four years. ®</p>","contentLength":3335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq34im/larry_ellison_wants_to_put_all_us_data_in_one_big/"},{"title":"Alexandre Mutel a.k.a. xoofx is leaving Unity","url":"https://mastodon.social/@xoofx/113997304444307991","date":1739631212,"author":"/u/namanyayg","guid":275,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq2zwv/alexandre_mutel_aka_xoofx_is_leaving_unity/"},{"title":"Don't \"optimize\" conditional moves in shaders with mix()+step()","url":"https://iquilezles.org/articles/gpuconditionals/","date":1739631147,"author":"/u/namanyayg","guid":274,"unread":true,"content":"\nIn this article I want to correct a popular misconception that's been making the rounds in computer graphics aficionado circles for a long time now. It has to do with conditionals when selecting between two results in the GPUs. Unfortunately there are a couple of educational websites out there that are spreading some misinformation, and it would be nice correcting that. I tried contacting the authors without success, so without further ado, here goes my attempt to fix things up a little:\nSo, say I have this code, which I actually published the other day:<div> snap45(  v )\n{\n     s = (v);\n     x = (v.x);\n     x&gt;?(s.x,):\n           x&gt;?s*():\n                      (,s.y);\n}</div>\nThe exact details of what it does don't matter for this discussion. All we care about is the two ternary operations deciding what's the final value this function should return. Indeed, depending on the value of the variable , the function will return one of three results, which are simple to compute. I could also have implemented this function with regular  statements, and all that I'm going to say in this article stays true.<p>\nNow, here's the problem - when seeing code like this, somebody somewhere will step up and invariably propose the following \"optimization\", which replaces what they believe (erroneously) are \"conditional branches\" in the code, by arithmetic operations. They will suggest something like this:</p><div> snap45(  v )\n{\n     s = (v);\n     x = (v.x);\n\n     w0 = (,x);\n     w1 = (,x)*(-w0);\n     w2 = -w0-w1;\n\n     res0 = (s.x,);\n     res1 = (s.x,s.y)*();\n     res2 = (,s.y);\n\n     w0*res0 + w1*res1 + w2*res2;\n}</div>\nThere are two things wrong with this practice. The first one shows an incorrect understanding of how the GPU works. In particular, the original shader code had no conditional branching in it. Selecting between a few registers with a ternary operator or with a plain  statement does not lead to conditional branching; all it involves is a conditional move (a.k.a. \"select\"), which is a simple instruction to route the correct bits to the destination register. You can think of it as a bitwise AND+NAND+OR on the source registers, which is a simple combinational circuit. I'll repeat it again - there is no branching, the instruction pointer isn't manipulated, there's no prediction involved, no pipe to flush, no instruction cache to invalidation, no nothing.<p>\nFor the record, of course GPUs can do real branching, and those are fine and fast and totally worth it when big chunks of code and computation are to be skipped given a condition. As with all things computing, always check the generated machine code to know what is happening exactly and when. But one thing you can safely assume without having to check any generated code - when moving simple values or computations like in my original example, you are guaranteed to not branch. This has been true for decades at this point, with GPUs. And while I'm not an expert in CPUs, I am pretty sure this is true for them as well.</p><p>\nThe second wrong thing with the supposedly optimized version is that it actually runs much slower than the original version. You can measure it in a variety of hardware. I can only assume that's because the </p> function is probably implemented with some sort of conditional move or subtract + bit propagation + AND.<div> step(  x,  y )\n{\n     x &lt; y ?  : ;\n}</div>\nEither way, using the step() \"optimization\" are either using the ternary operation anyways, which produces the  or  which they will use to mask in and out the different potential outputs with a series of arithmetic multiplications and additions. Which is wasteful, the values could have been conditionally moved directly, which is what the original shader code did.<p>\nBut don't take my word for it, let's look at the generated machine code for the original code I published:</p><div><div>\nGLSL<div> x&gt;?(s.x,):\n       x&gt;?s*():\n                  (,s.y);</div></div><div>\nAMD Compiler<div>     s0,      v3, , v1\n     v4, , v0\n     s1,   vcc, (v2), s0\n v3, 0, v3, vcc\n v0, v0, v4, vcc\n vcc, (v2), s1\n v1, v1, v3, vcc\n v0, 0, v0, vcc</div></div><div>\nMicrosoft Compiler<div>   r0.xy, l(, ), v0.xy\n   r0.zw, v0.xy, l(, )\n r0.xy, -r0.xyxx, r0.zwzz\n r0.xy, r0.xyxx\n  r1.xyzw, r0.xyxy, l4()\n   r2.xy, l(,), v0.xx  r0.z, l()\n r1.xyzw, r2.yyyy, r1.xyzw, r0.zyzy\n o0.xyzw, r2.xxxx, r0.xzxz, r1.xyzw</div></div></div>\nHere we can confirm that the GPU is not branching, as I explained. Instead, according to the AMD compiler, it's performing the required comparisons ( and  - cmp=compare, gt=greater than, ngt=not greated than), and then using the result to mask the results with the bitwise operations mentioned earlier ( - cnd=conditional).<p>\nThe Microsoft compiler has expressed the same idea/implementation in a different format, but you can still see the comparison (</p> - \"lt\"=less than) and the masking or conditional move ( - mov=move, c=conditionally).<p>\nThere are no jump/branch instructions in these listings.</p><p>\nSomething not related to the discussion but interesting, is that some of the </p> GLSL calls I had in my shader before the ternary operator we are discussing, didn't become GPU instructions but rather instruction modifiers, which is the reason you see them in the listing. This means you can think of abs() calls as being free.\nSo, if you ever see somebody proposing this<div> a = ( b, c, ( y, x ) );</div>\nas an optimization to\nthen please correct them for me.","contentLength":5296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq2z4j/dont_optimize_conditional_moves_in_shaders_with/"},{"title":"Altman: OpenAI not for sale, especially to competitor who is not able to beat us","url":"https://www.axios.com/2025/02/11/openai-altman-musk-offer","date":1739629077,"author":"/u/namanyayg","guid":472,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq29zz/altman_openai_not_for_sale_especially_to/"},{"title":"GitHub - yaitoo/xun: Xun is an HTTP web framework built on Go's built-in html/template and net/http package’s router (1.22).","url":"https://github.com/yaitoo/xun","date":1739628954,"author":"/u/imlangzi","guid":290,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iq28it/github_yaitooxun_xun_is_an_http_web_framework/"},{"title":"What is Event Sourcing?","url":"https://newsletter.scalablethread.com/p/what-is-event-sourcing","date":1739628306,"author":"/u/scalablethread","guid":278,"unread":true,"content":"<p>Traditional data storage typically focuses on the current state of an entity. For example, in an e-commerce system, you might store the current state of a customer's order: items, quantities, shipping address, etc. Event sourcing takes a different approach. Instead of storing the current state directly, it stores the events that led to that state. Each event represents a fact that happened in the past. Think of it as a detailed log of transactions on your bank statement. These events are immutable and stored in an append-only event store. The core idea is that an application's state can be derived by replaying events in the order they occurred, just like you can get your current bank balance by replaying all the transactions from the beginning. This makes Event Sourcing particularly useful for applications that require a high degree of audibility and traceability.</p><p>Every change to the application state is captured as an event object in an Event Sourcing system. These events are then stored in an event store, a database optimized for handling event data. Here's a step-by-step breakdown of how Event Sourcing works:</p><ol></ol><p>Reconstructing the state from events involves reading all the events related to an entity from the event store and applying them in sequence to reconstruct the current state. It's like simulating all the changes that have occurred to construct the current state. For example, consider an e-commerce application where an order goes through various states like \"Created,\" \"Paid,\" and \"Shipped.\" To determine the current state of an order, you would:</p><ol><li><p>Retrieve all events related to the order from the event store.</p></li><li><p>Initialize an empty order object.</p></li><li><p>Apply each event to the order object in the order in which they were stored.</p></li></ol><p>By the end of this process, the order object will reflect the current state of the order.</p><p>As the number of events grows, replaying the entire event stream to reconstruct the state can become slow and inefficient. This is where snapshots come in. A snapshot is a saved state of an entity at a specific point in time. Instead of replaying all events from the beginning, the application can load the latest snapshot and then replay only the events that occurred after the snapshot was taken. </p><ul></ul><ul></ul><p><em>If you enjoyed this article, please hit the ❤️ like button.</em></p><p><em>If you think someone else will benefit from this, then please 🔁 share this post.</em></p>","contentLength":2380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq20v8/what_is_event_sourcing/"},{"title":"Built a cli tool for generating .gitignore files","url":"https://www.reddit.com/r/golang/comments/1iq1ivv/built_a_cli_tool_for_generating_gitignore_files/","date":1739626739,"author":"/u/SoaringSignificant","guid":286,"unread":true,"content":"<p>I built this mostly as an excuse to play around with Charmbracelet’s libraries like Bubble Tea and make a nice TUI, but it also solves the annoying problem of constantly looking up .gitignore templates. It’s a simple CLI tool that lets you grab templates straight from GitHub, TopTal, or even your own custom repository, all from the terminal. You can search through templates using a TUI interface, combine multiple ones like mixing Go and CLion, and even save your own locally so you don’t have to redo them every time. If you’re always setting up new projects and find yourself dealing with .gitignore files over and over, this just makes life a bit easier, hopefully. If that sounds useful, check it out <a href=\"https://github.com/jasonuc/gignr\">here</a> and give it a try. And if you’ve got ideas to make the TUI better or want to add something cool, feel free to open a PR. Always happy to get feedback or contributions!</p>","contentLength":890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ED25519 Digital Signatures In Go","url":"https://www.reddit.com/r/golang/comments/1iq1i84/ed25519_digital_signatures_in_go/","date":1739626679,"author":"/u/mejaz-01","guid":482,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/mejaz-01\"> /u/mejaz-01 </a>","contentLength":31,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Will AI Lead to the Disintermediation of Knowledge?","url":"https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/","date":1739626129,"author":"Bill Schmarzo","guid":63,"unread":true,"content":"<p>Key Blog Points: For decades, organizations have operated under the central assumption that knowledge flows downward. Senior leaders, industry veterans, and domain experts have traditionally been the primary gatekeepers of critical information. Their insights, honed over years of experience, have been the cornerstone of strategic decision-making. Enter artificial intelligence (AI). Many folks are concerned that…&nbsp;<a href=\"https://www.datasciencecentral.com/will-ai-lead-to-the-disintermediation-of-knowledge/\" rel=\"bookmark\">Read More »</a></p>","contentLength":431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I Built a Reddit-style Bluesky client – still rough, but open to ideas","url":"https://threadsky.app/","date":1739625557,"author":"lakshikag","guid":453,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43058285"},{"title":"Chinese Vice Minister says China and the US must work together to control rogue AI: \"If not... I am afraid that the probability of the machine winning will be high.\"","url":"https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says","date":1739622429,"author":"/u/MetaKnowing","guid":293,"unread":true,"content":"<div datatype=\"p\" data-qa=\"Component-Component\">A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in <a target=\"_self\" href=\"https://www.scmp.com/topics/artificial-intelligence?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> (AI).</div><p datatype=\"p\" data-qa=\"Component-Component\">But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.</p><p datatype=\"p\" data-qa=\"Component-Component\">“Realistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,” Fu said.</p><p datatype=\"p\" data-qa=\"Component-Component\">“As long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.”</p><div datatype=\"p\" data-qa=\"Component-Component\">The panel discussion is part of a two-day global <a target=\"_self\" href=\"https://www.scmp.com/news/world/europe/article/3297992/trumps-ai-ambition-and-chinas-deepseek-overshadow-major-ai-summit-paris?module=Europe&amp;pgtype=section?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> that started in Paris on Monday.</div><p datatype=\"p\" data-qa=\"Component-Component\">Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden’s administration and the United Nations.</p>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq0b4t/chinese_vice_minister_says_china_and_the_us_must/"},{"title":"Building the MagicMirror in Rust with iced GUI Library 🦀","url":"https://www.reddit.com/r/rust/comments/1ipzubj/building_the_magicmirror_in_rust_with_iced_gui/","date":1739620595,"author":"/u/amindiro","guid":283,"unread":true,"content":"<p>I recently embarked on a journey to build a custom MagicMirror using the Rust programming language, and I’d like to share my experiences. I wrost a blog post titled <a href=\"https://aminediro.com/posts/mirrors/#mirrors\">\"software you can love: miroir Ô mon beau miroir\"</a> this project was my attempt to create a stable, resource-efficient application for the Raspberry Pi 3A.</p><p>Here's what I loved about using Rust and the iced GUI library:</p><ul><li><p><strong>Elm Architecture + Rust is a match made in heaven:</strong> iced was perfect for my needs with its Model, View, and Update paradigms. It helped keep my state management concise and leverage Rust type system</p></li><li><p> Opting for this lightweight rendering library reduced the size of the binary significantly, ending with a 9MB binary.</p></li><li><p> Although troublesome at first, I used ‘cross’ to cross compile Rust for armv7.</p></li></ul><p>If anyone is keen, I’m thinking of open-sourcing this project and sharing it with the community. Insights on enhancing the project's functionality or any feedback would be much appreciated!</p><p>Feel free to reach out if you're interested in the technical nitty-gritty or my experience with Rust GUI libraries in general.</p>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ordered map","url":"https://dev.to/kirillscherba/ordered-map-13op","date":1739620303,"author":"Kirill Scherba","guid":576,"unread":true,"content":"<p>Omap is Golang package for working with thread safe ordered maps. The ordered map contains the golang map, list and mutex to execute Ordered Map functions.</p><p>The Ordered Map is a map that remembers the order of items. The map can be iterated over to retrieve the items in the order they were added.</p><h2>\n  \n  \n  Introduction to the omap Go Package\n</h2><p>The omap Go package is a lightweight and efficient library for working with ordered maps in Go. An ordered map is a data structure that combines the benefits of a map and a list, allowing you to store key-value pairs in a specific order.</p><p>Omap is a Go package that provides an implementation of an ordered map. It is designed to be fast, efficient, and easy to use. Omap is particularly useful when you need to store data in a specific order, such as when working with configuration files, caching, or data processing pipelines.</p><ul><li><p>Ordered: omap preserves the order in which key-value pairs are inserted, allowing you to iterate over the map in a specific order.</p></li><li><p>Fast lookups: omap uses a hash table to store key-value pairs, making lookups fast and efficient.</p></li><li><p>Efficient insertion and deletion: omap uses a linked list to store the order of key-value pairs, making insertion and deletion operations efficient.</p></li></ul><p>To use omap, you can install it using the following command:</p><div><pre><code>go get github.com/kirill-scherba/omap\n</code></pre></div><p>Here is an example of how to use omap:</p><div><pre><code></code></pre></div><p>This code creates a new omap, inserts some key-value pairs, and then iterates over the omap in order, printing out each key-value pair.</p><p>The omap Go package is a useful library for working with ordered maps in Go. Its fast lookups, efficient insertion and deletion, and ordered iteration make it a great choice for a variety of use cases. Whether you're working with configuration files, caching, or data processing pipelines, omap is definitely worth considering.</p><ul><li><p>Configuration files: Use omap to store configuration data in a specific order, making it easy to iterate over the configuration and apply settings in the correct order.</p></li><li><p>Caching: Use omap to store cached data in a specific order, making it easy to iterate over the cache and evict items in the correct order.</p></li><li><p>Data processing pipelines: Use omap to store data in a specific order, making it easy to iterate over the data and process it in the correct order.</p></li></ul>","contentLength":2292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Daily ArXiv filtering powered by LLM judge","url":"https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/","date":1739618056,"author":"/u/MadEyeXZ","guid":258,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🌍 UTF-8","url":"https://dev.to/wycliffealphus/utf-8-3edn","date":1739617808,"author":"Wycliffe A. Onyango","guid":575,"unread":true,"content":"<p>Have you ever opened a file or webpage and seen something like this?</p><p>That’s an encoding issue, and if you’ve been coding long enough, you’ve probably run into it at some point.</p><p>But why does this happen? Why do some characters get replaced with weird symbols? And most importantly—how do we fix it?</p><p>The answer is UTF-8, the encoding that powers almost everything today. Let's talk about what it is, why it matters, and how to use it properly in Go (Golang).</p><h3>\n  \n  \n  🔥 The Problem UTF-8 Solves\n</h3><p>Back in the early days of computing, ASCII was the standard way to represent text. It used 7 bits per character, meaning it could only represent 128 characters (A-Z, a-z, 0-9, and some symbols).</p><p>That was fine—until computers went global.</p><p>Suddenly, people needed to store and display languages like Chinese (汉字), Arabic (العربية), Hindi (हिन्दी), and more. ASCII just couldn’t handle it.</p><p>So different countries created their own encodings:</p><ul><li><p>ISO-8859-1 for Western Europe</p></li><li><p>Windows-1252 for Microsoft systems</p></li></ul><p>💀 The result? Encoding chaos. A file written in one system might be unreadable in another.</p><p>Enter , the hero of our story.</p><h3>\n  \n  \n  🏆 What Makes UTF-8 Special?\n</h3><p>UTF-8 was designed in 1992 by Ken Thompson and Rob Pike (yes, the same Rob Pike who helped create Go!). It solved the encoding mess by being:</p><p>✅ Backwards-compatible with ASCII\n✅ Compact for common characters (English stays at 1 byte per character)<p>\n✅ Capable of encoding every language and symbol</p>\n✅ Error-resistant (invalid bytes won’t accidentally form valid characters)</p><p>This is why UTF-8 is now used by 97% of websites and is the default encoding for most programming languages, including Go.</p><h3>\n  \n  \n  💻 UTF-8 in Action (With Go Examples)\n</h3><p>Since Go natively supports UTF-8, you don’t need to do anything special—it just works. But let’s dig into some examples to see it in action.</p><p><strong>1️⃣ Encoding a String as UTF-8 Bytes</strong></p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><ul><li><p>English characters (, ) are 1 byte each.</p></li><li><p>Chinese characters () are 3 bytes each.</p></li></ul><p>This variable-length encoding is why UTF-8 is so efficient!</p><p><strong>2️⃣ Decoding UTF-8 Bytes Back to a String</strong></p><div><pre><code></code></pre></div><p>💡 No extra libraries—Go just handles it. That’s one of the nice things about UTF-8 in Go.</p><p><strong>3️⃣ Handling UTF-8 in Web Applications</strong></p><p>If you're building a web app, always specify UTF-8 in your response headers:</p><div><pre><code></code></pre></div><p>💡 Without , some browsers might misinterpret the text and display garbage characters.</p><p><strong>4️⃣ Validating UTF-8 Data</strong></p><p>Not every byte sequence is valid UTF-8. You can check with :</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>✅ Great for validating user input before processing it!</p><p><strong>5️⃣ Counting Unicode Characters (Runes) in a String</strong></p><p>Go strings are byte sequences, not necessarily character sequences.</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>❗ Why the difference? Because 世界 takes 3 bytes each, so len(text) == 13, but there are only 9 characters.</p><p><strong>6️⃣ Iterating Over Unicode Characters</strong></p><p>Since some characters take more than 1 byte, normal indexing won’t work. Use :</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>❗ Notice how 世界 starts at index 7, not 5, because it uses 3 bytes each.</p><h3>\n  \n  \n  🚀 Why UTF-8 is the Default Encoding\n</h3><p>Before UTF-8:\n❌ Confusing mess of different encodings<p>\n❌ Text corruption between systems</p>\n❌ Websites needed to support multiple charsets</p><p>After UTF-8:\n✅ One encoding for everything<p>\n✅ No more garbled text (mojibake)</p>\n✅ Supported everywhere—from databases to web APIs</p><p>If you’re dealing with text in Go (or any language), understanding UTF-8 is essential. It ensures your applications work worldwide without encoding issues.</p>","contentLength":3465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go Nullable with Generics v2.0.0 - now supports omitzero","url":"https://github.com/LukaGiorgadze/gonull","date":1739617221,"author":"/u/Money-Relative-1184","guid":487,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ipz22f/go_nullable_with_generics_v200_now_supports/"},{"title":"async-arp: library for probing hosts and sending advanced ARP (Address Resolution Protocol) requests.","url":"https://www.reddit.com/r/rust/comments/1ipywbp/asyncarp_library_for_probing_hosts_and_sending/","date":1739616505,"author":"/u/arcycar","guid":280,"unread":true,"content":"<p>After a few months of exploring and working with Rust, I am happy to share my first small Rust crate, <a href=\"https://crates.io/crates/async-arp\"></a> and I’d love to hear your thoughts! 🚀</p><p>This library provides an  way to send and receive , making it useful for network discovery, debugging, and custom networking applications.</p><ul><li>🏎  Built on Tokio for non-blocking network operations</li><li>🔍  Easily detect active devices in a subnet</li><li>⚙️  Craft and send ARP packets dynamically</li></ul><p>You can find usage examples and API documentation here: 📖 <a href=\"https://docs.rs/async-arp/latest/async_arp/\"></a></p><p>Since this is my first crate, I’d really appreciate any feedback on:</p><ul><li>📌  – Is the interface intuitive and ergonomic?</li><li>🚀  – Does it fit well into async Rust workflows?</li><li>🔍  – Any improvements or best practices I may have missed?</li><li>🦀  – Suggestions to make it more \"Rustacean\"?</li></ul><p>If you have further ideas, issues, or want to contribute, check it out on GitHub:</p><p>Thanks for checking it out—let me know what you think! 🦀</p>","contentLength":921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what do you use golang for?","url":"https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/","date":1739615068,"author":"/u/Notalabel_4566","guid":291,"unread":true,"content":"<p>Is there any other major use than web development?</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Kreuzberg – Modern async Python library for document text extraction","url":"https://github.com/Goldziher/kreuzberg","date":1739614043,"author":"nhirschfeld","guid":452,"unread":true,"content":"<p>I'm excited to showcase Kreuzberg!</p><p>Kreuzberg is a modern Python library built from the ground up with async/await, type hints, and optimized I/O handling.</p><p>It provides a unified interface for extracting text from documents (PDFs, images, office files) without external API dependencies.</p><p>Key technical features:\n- Built with modern Python best practices (async/await, type hints, functional-first)\n- Optimized async I/O with anyio for multi-loop compatibility\n- Smart worker process pool for CPU-bound tasks (OCR, doc conversion)\n- Efficient batch processing with concurrent extractions\n- Clean error handling with context-rich exceptions</p><p>I built this after struggling with existing solutions that were either synchronous-only, required complex deployments, or had poor async support. The goal was to create something that works well in modern async Python applications, can be easily dockerized or used in serverless contexts, and relies only on permissive OSS.</p><p>Key advantages over alternatives:\n- True async support with optimized I/O\n- Minimal dependencies (much smaller than alternatives)\n- Perfect for serverless and async web apps\n- Local processing without API calls\n- Built for modern Python codebases with rigorous typing and testing</p><p>The library is MIT licensed and open to contributions.</p>","contentLength":1289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43057375"},{"title":"Lessons from David Lynch: A Software Developer's Perspective","url":"https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/","date":1739612430,"author":"/u/aijan1","guid":277,"unread":true,"content":"<p>David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He’s perhaps best known for the groundbreaking TV series <a href=\"https://en.wikipedia.org/wiki/Twin_Peaks\">Twin Peaks</a>, which inspired countless shows, including The X-Files, The Sopranos, and Lost.</p><p>Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down – even those who truly deserved it.</p><p>Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that <a href=\"https://en.wikipedia.org/wiki/Mulholland_Drive_(film)\">Mulholland Drive</a> remained compulsively watchable while refusing to yield to interpretation.</p><p>While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I’d like to share my perspective on his life lessons from a software developer’s viewpoint.</p><blockquote><p>Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.</p></blockquote><p>We’ve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one –because they’re so rare– write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it’s a film, a painting, or software.</p><blockquote><p>The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.</p></blockquote><p>Software development is part art, part engineering. We don’t build the same software over and over again – virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it’s very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.</p><p>It’s a good habit to listen to what users have to say, but they often can only describe their problems – they rarely come up with good ideas to solve them. And that’s OK. It’s our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.</p><blockquote><p>My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.</p></blockquote><p>Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  – that magical state of mind where we lose track of time and produce code effortlessly. That’s why many developers hate meetings – they are toxic to our productivity.</p><blockquote><p>I believe you need technical knowledge. And also, it’s really, really great to learn by doing. So, you should make a film.</p></blockquote><p>Software development is one of those rare fields where a college degree isn’t required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.</p><p>The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It’s crucial to never stop learning, experimenting, and iterating on our craft.</p><blockquote><p>Happy accidents are real gifts, and they can open the door to a future that didn’t even exist.</p></blockquote><p>Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.</p><p>Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.</p><blockquote><p>I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.</p></blockquote><p>Be kind to your teammates, don’t embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety –that is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by <a href=\"https://rework.withgoogle.com/en/guides/understanding-team-effectiveness\">Google’s research</a> on the subject.</p><p>It’s OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.</p><blockquote><p>Most of Hollywood is about making money - and I love money, but I don’t make the films thinking about money.</p></blockquote><p>Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.</p><p>What makes these projects remarkable is that they didn’t emerge from corporate boardrooms – they were built by communities of passionate developers, collaborating across the world.</p><p>Money is just a means to an end. Unfortunately, many get this confused.</p><p>David, thank you for making the world a better place!</p>","contentLength":5845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipy01t/lessons_from_david_lynch_a_software_developers/"},{"title":"Macro-Less, Highly Integrated OpenAPI Document Generation in Rust with Ohkami","url":"https://dev.to/kanarus/macro-less-highly-integrated-openapi-document-generation-in-rust-with-ohkami-9b2","date":1739609369,"author":"kanarus","guid":551,"unread":true,"content":"<p><em>This is a cross post from <a href=\"https://medium.com/@kanarus786/macro-less-highly-integrated-openapi-document-generation-in-rust-with-ohkami-912de388adc1\" rel=\"noopener noreferrer\">Medium</a>.</em></p><p>In Rust web dev, <a href=\"https://github.com/juhaku/utoipa\" rel=\"noopener noreferrer\">utoipa</a> is the most popular crate for generating OpenAPI document from server code. While it’s a great tool, it can be frustrating due to excessive macro use.</p><p>A new web framework Ohkami offers a <em><strong>macro-less, highly integrated</strong></em> way to generate OpenAPI document with its  feature.</p><p>Let’s take following code as an example. It’s the same sample from the “openapi” section of the README, but with openapi-related parts removed:</p><div><pre><code></code></pre></div><p>While this compiles and works as a pseudo user management server, activating  feature causes a compile error, telling that  and  don’t implement .</p><p>As indicated by this, Ohkami with  feature effectively handles type information and intelligently collects its endpoints’ metadata. It allows code like:</p><div><pre><code></code></pre></div><p>to assemble metadata into an OpenAPI document and output it to a file .</p><p>Then, how we implement ? Actually we can easily  by hand, or just  is available! In this case, derive is enough:</p><div><pre><code></code></pre></div><p>That’s it! Just adding these derives allows  to output following file:</p><div><pre><code></code></pre></div><p>Additionally, it’s easy to define the  schema as a component instead of duplicating inline schemas.\nIn derive, just add  helper attribute:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>And  attribute is available to set , , and override  and each response’s :</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Let’s take a look at how this document generation works!</p><p>First, the s are expanded as following:</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>The organized DSL enables to easily impl manually.</p><p> trait links the struct to an item of type called .</p><h3>\n  \n  \n  2.  hooks of , , </h3><p>They're Ohkami’s core traits appeared in the handler bound:</p><div><pre><code></code></pre></div><p>When  feature is activated, they additionally have following methods:</p><div><pre><code></code></pre></div><p>Ohkami leverages these methods in  to generate consistent , reflecting the actual handler signature like <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/fang/handler/into_handler.rs#L328-L335\" rel=\"noopener noreferrer\">this</a>.</p><p>Moreover, Ohkami properly propagates schema information in common cases like <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/response/into_response.rs#L114-L128\" rel=\"noopener noreferrer\">this</a>, allowing users to focus only on the types and schemas of their app.</p><h3>\n  \n  \n  3.  metadata of Router\n</h3><p>In Ohkami, what’s called  has  property that stores all the routes belonging to an Ohkami instance. This is returned alongside  from  step, and is used to assemble metadata of all endpoints.</p><p>What  itself does is just to serialize an item of type <code>openapi::document::Document</code> and write it to a file.</p><p>The <code>openapi::document::Document</code> item is created by  of , summarized as follows:</p><div><pre><code></code></pre></div><p>That’s how Ohkami generates OpenAPI document!</p><h2>\n  \n  \n  Appendix: Cloudflare Workers\n</h2><p>There is, however, a problem in , Cloudflare Workers: where Ohkami is loaded to Miniflare or Cloudflare Workers as WASM, so it can only generate OpenAPI document as data and cannot write it to the user’s local file system.</p><div><pre><code></code></pre></div><p>generates OpenAPI document!</p><p>Thank you for reading. If you’re interested in Ohkami, check out the <a href=\"https://github.com/ohkami-rs/ohkami\" rel=\"noopener noreferrer\">GitHub repo</a> and start coding!</p>","contentLength":2709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] What's the most promising successor to the Transformer?","url":"https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/","date":1739600221,"author":"/u/jsonathan","guid":259,"unread":true,"content":"<p>All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also <a href=\"https://arxiv.org/pdf/2405.04517\">xLSTM</a> and <a href=\"https://arxiv.org/pdf/2405.13956\">Aaren</a>.</p><p>What do y'all think is the most promising alternative architecture to the transformer?</p>","contentLength":283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Concurrency vs Parallelism | Golang | Which is better and Why?","url":"https://dev.to/gargkunal/concurrency-vs-parallelism-golang-which-is-better-and-why-m5j","date":1739597904,"author":"Kunal Garg","guid":574,"unread":true,"content":"<p>In this video, I’ll dive deep into Concurrency vs Parallelism in Golang, explaining the key differences, when to use each, and which one is better for different scenarios. I'll cover various ways to implement concurrency in Golang, with practical code examples and hands-on demonstrations. Whether you're new to Go or looking to optimize your programs, this video will give you a solid understanding of how to leverage goroutines, channels, and worker pools effectively.</p><p>🔹 What is Concurrency?\n🔹 What is Parallelism?<p>\n🔹 Key Differences &amp; Use Cases</p>\n🔹 Writing Concurrent Code in Golang<p>\n🔹 Live Code Examples &amp; Best Practices</p></p>","contentLength":636,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kafka Delay Queue: When Messages Need a Nap Before They Work","url":"https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need","date":1739596108,"author":"/u/Sushant098123","guid":276,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipu9n3/kafka_delay_queue_when_messages_need_a_nap_before/"},{"title":"Webassembly and go 2025","url":"https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/","date":1739595637,"author":"/u/KosekiBoto","guid":287,"unread":true,"content":"<div><p>so I found <a href=\"https://www.youtube.com/watch?v=HShIpUgCPp4\">this video </a>and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you</p></div>   submitted by   <a href=\"https://www.reddit.com/user/KosekiBoto\"> /u/KosekiBoto </a>","contentLength":402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Implement Redis Caching in Golang for Beginners: A Step-by-Step Guide(Windows Edition)","url":"https://dev.to/arijit_das_24c46c6f02b06b/how-to-implement-redis-caching-in-golang-for-beginners-a-step-by-step-guidewindows-edition-1g1a","date":1739592944,"author":"Arijit das","guid":573,"unread":true,"content":"<p>In this post, I'll walk you through <strong>how to set up and use Redis caching</strong> in a Golang project. This setup helps in reducing  and improving  by caching frequently requested data.  </p><h2>\n  \n  \n  🔹 <strong>Why Use Redis for Caching?</strong></h2><p>✅ Reduces database load<p>\n✅ Supports expiration &amp; invalidation  </p></p><h2>\n  \n  \n  📌 <strong>Step 1: Install Redis Package</strong></h2><p>We use the  package to interact with Redis. Install it using:</p><div><pre><code>go get github.com/redis/go-redis/v9\n</code></pre></div><h2>\n  \n  \n  📌 <strong>Step 2: Initialize Redis Client</strong></h2><p>Create a file  and add the following:</p><div><pre><code></code></pre></div><p>🔹 <strong>If using a local Redis instance</strong>, set  and remove .<strong>For cloud Redis providers (like Aiven, AWS, etc.),</strong> use the TLS setup.  </p><h2>\n  \n  \n  📌 <strong>Step 3: Implement Caching Functions</strong></h2><p>Add the following caching functions to manage data in Redis:</p><div><pre><code></code></pre></div><p>🔹  → Checks if the key exists, retrieves &amp; unmarshals data. → Stores JSON-encoded data with an expiration time. → Deletes a cache entry (useful when data updates).  </p><h2>\n  \n  \n  📌 <strong>Step 4: Use Redis Cache in Database Queries</strong></h2><p>Modify your database query functions to  before querying the database:</p><div><pre><code></code></pre></div><p>🔹  using ., query the database. for future use with .  </p><h2>\n  \n  \n  📌 <strong>Step 5: Invalidate Cache When Data Changes</strong></h2><p>Whenever new categories are added, remove the outdated cache:</p><div><pre><code></code></pre></div><p>🔹 Use this <strong>after adding/updating/deleting data</strong> to keep cache fresh.  </p><p>If you want to test Redis caching locally on , follow these steps:</p><h3>\n  \n  \n  ✅ <strong>Step 1: Install &amp; Run Redis Locally</strong></h3><ol><li><p><strong>Download Redis for Windows:</strong></p><ul><li>  (Redis is not natively supported on Windows)</li><li>Install and run the Redis server.</li></ul></li><li><p> or a , start it using:</p></li></ol><ol><li><strong>Check if Redis is Running:</strong>\nOpen  and run:\n</li></ol><p>If Redis is running, it should return:</p><h3>\n  \n  \n  ✅ <strong>Step 2: Set &amp; Get Keys in Redis (Local Testing)</strong></h3><p>To manually check your cache for the key , use the following commands in :</p><ol><li><strong>Check if  exists:</strong></li></ol><p>If it returns , the key exists. If , it's missing.</p><p>This should return the cached JSON data.</p>","contentLength":1852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bringing Nest.js to Rust: Meet Toni.rs, the Framework You’ve Been Waiting For! 🚀","url":"https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/","date":1739587338,"author":"/u/Mysterious-Rust","guid":284,"unread":true,"content":"<p>As a Rust developer coming from TypeScript, I’ve been missing a Nest.js-like framework — its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn’t have a direct counterpart (yet!), I decided to build one myself! 🛠️</p><p>Introducing… <a href=\"https://crates.io/crates/toni\">Toni.rs</a> — a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it’s live in beta! 🎉</p><p>Here’s what makes this project interesting:</p><p>Scalable maintainability 🧩:</p><p>A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code — each module lives in its own context, clean and focused.</p><p>Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?</p><p>Automatic Dependency Injection 🤖:</p><p>Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.</p><p>Leave your thoughts below — suggestions, questions, or even just enthusiasm! 🚀 </p>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I Became A Machine Learning Engineer (No CS Degree, No Bootcamp)","url":"https://towardsdatascience.com/how-i-became-a-machine-learning-engineer-no-cs-degree-no-bootcamp/","date":1739586781,"author":"Egor Howell","guid":10,"unread":true,"content":"<p>Machine learning and <a href=\"https://towardsdatascience.com/tag/ai/\" title=\"AI\">AI</a> are among the most popular topics nowadays, especially within the tech space. I am fortunate enough to work and develop with these technologies every day as a machine learning engineer!</p><p>In this article, I will walk you through my journey to becoming a machine learning engineer, shedding some light and advice on how you can become one yourself!</p><p>In one of my previous articles, I extensively wrote about my journey from school to securing my first <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a> job. I recommend you <a href=\"https://medium.com/towards-data-science/how-i-became-a-data-scientist-no-cs-degree-no-bootcamp-82c321904986\">check out that article</a>, but I will summarise the key timeline here.</p><p>Pretty much everyone in my family studied some sort of STEM subject. My great-grandad was an engineer, both my grandparents studied physics, and my mum is a maths teacher.</p><p><em>So, my path was always paved for me.</em></p><p>I chose to study physics at university after watching The Big Bang Theory at age 12; it’s fair to say everyone was very proud!</p><p>At school, I wasn’t dumb by any means. I was actually relatively bright, but I didn’t fully apply myself. I got decent grades, but definitely not what I was fully capable of.</p><p>I was very arrogant and thought I would do well with zero work.</p><p>I applied to top universities like Oxford and Imperial College, but given my work ethic, I was delusional thinking I had a chance. On results day, I ended up in clearing as I missed my offers. This was probably one of the saddest days of my life.</p><p>Clearing in the UK is where universities offer places to students on certain courses where they have space. It’s mainly for students who don’t have a university offer.</p><p>I was lucky enough to be offered a chance to study physics at the University of Surrey, and I went on to earn a first-class master’s degree in physics!</p><p>There is genuinely no substitute for hard work. It is a cringy cliche, but it is true!</p><p>My original plan was to do a PhD and be a full-time researcher or professor, but during my degree, I did a research year, and I just felt a career in research was not for me. Everything moved so slowly, and it didn’t seem there was much opportunity in the space.</p><p>During this time, DeepMind released their<a href=\"https://www.youtube.com/watch?v=WXuK6gekU1Y&amp;t=1539s\"></a>documentary on YouTube, which popped up on my home feed.</p><p>From the video, I started to understand how AI worked and learn about neural networks, reinforcement learning, and deep learning. To be honest, to this day I am still not an expert in these areas.</p><p>Naturally, I dug deeper and found that a data scientist uses AI and machine learning algorithms to solve problems. I immediately wanted in and started applying for data science graduate roles.</p><p>I spent countless hours coding, taking courses, and working on projects. I applied to and eventually landed my first data science graduate scheme in September 2021.</p><p><em>You can hear more about my journey from a <a href=\"https://tobeadatascientist.substack.com/p/overcoming-rejection-lessons-from-egor-howell\">podcast</a>.</em></p><p>I started my career in an insurance company, where I built various supervised learning models, mainly using gradient boosted tree packages like CatBoost, XGBoost, and<a href=\"https://medium.com/towards-data-science/breaking-down-generalized-linear-models-d9212526e51d?sk=fda0298cebcb8e9e0c20cb6af8ed4f06\"> generalised linear models (GLMs)</a>.</p><p>I built models to predict:</p><ul><li> — Did someone fraudulently make a claim to profit.</li><li>— What’s the premium we should give someone.</li><li>— How many claims will someone have.</li><li> — What’s the average claim value someone will have.</li></ul><p>I made around six models spanning the regression and classification space. I learned so much here, especially in statistics, as I worked very closely with Actuaries, so my maths knowledge was excellent.</p><p>However, due to the company’s structure and setup, it was difficult for my models to advance past the PoC stage, so I felt I lacked the “tech” side of my toolkit and understanding of how companies use machine learning in production.</p><p>After a year, my previous employer reached out to me asking if I wanted to apply to a junior data scientist role that specialises in<a href=\"https://medium.com/@egorhowell/list/time-series-00bbfb9f5359\"> time series forecasting</a> and<a href=\"https://medium.com/@egorhowell/list/optimisation-algorithms-069bf9c6c8d5\"> optimisation</a> problems. I really liked the company, and after a few interviews, I was offered the job!</p><p>I worked at this company for about 2.5 years, where I became an expert in forecasting and combinatorial optimisation problems.</p><p>I developed many algorithms and deployed my models to production through AWS using software engineering best practices, such as unit testing, lower environment, shadow system, CI/CD pipelines, and much more.</p><p><em>Fair to say I learned a lot.&nbsp;</em></p><p>I worked very closely with software engineers, so I picked up a lot of engineering knowledge and continued self-studying machine learning and statistics on the side.</p><p>Over time, I realised the actual value of data science is using it to make live decisions. There is a good quote by<a href=\"https://www.linkedin.com/posts/pau-labarta-bajo-4432074b_machinelearning-mlops-realworldml-activity-7195694289178214400-gZyw\"> Pau Labarta Bajo</a></p><p>ML models inside Jupyter notebooks have a business value of $0</p><p>There is no point in building a really complex and sophisticated model if it will not produce results. Seeking out that extra 0.1% accuracy by staking multiple models is often not worth it.</p><p>You are better off building something simple that you can deploy, and that will bring real financial benefit to the company.</p><p>With this in mind, I started thinking about the future of data science. In my head, there are two avenues:</p><ul><li> -&gt; You work primarily to gain insight into what the business should be doing and what it should be looking into to boost its performance.</li><li> -&gt; You ship solutions (models, decision algorithms, etc.) that bring business value.</li></ul><p>I feel the data scientist who analyses and builds PoC models will become extinct in the next few years because, as we said above, they don’t provide tangible value to a business.</p><p>That’s not to say they are entirely useless; you have to think of it from the business perspective of their return on investment. Ideally, the value you bring in should be more than your salary.</p><p>You want to say that you did “X that produced Y”, which the above two avenues allow you to do.</p><p>The engineering side was the most interesting and enjoyable for me. I genuinely enjoy coding and building stuff that benefits people, and that they can use, so naturally, that’s where I gravitated towards.</p><p>To move to the ML engineering side, I asked my line manager if I could deploy the algorithms and ML models I was building myself. I would get help from software engineers, but I would write all the production code, do my own system design, and set up the deployment process independently.</p><p><em>And that’s exactly what I did.</em></p><p>Coincidentally, my current employer contacted me around this time and asked if I wanted to apply for a machine learning engineer role that specialises in general ML and optimisation at their company!</p><p>Call it luck, but clearly, the universe was telling me something. After several interview rounds, I was offered the role, and I am now a fully fledged machine learning engineer!</p><p>Fortunately, a role kind of “fell to me,” but I created my own luck through up-skilling and documenting my learning. That is why I always tell people to show their work — you don’t know what may come from it.</p><p>I want to share the main bits of advice that helped me transition from a machine learning engineer to a data scientist.</p><ul><li> — A machine learning engineer is  an entry-level position in my opinion. You need to be well-versed in data science, machine learning, software engineering, etc. You don’t need to be an expert in all of them, but have good fundamentals across the board. That’s why I recommend having a couple of years of experience as either a software engineer or data scientist and self-study other areas.</li><li> — If you are from data science, you must learn to write good, well-tested production code. You must know things like typing, linting, unit tests, formatting, mocking and CI/CD. It’s not too difficult, but it just requires some practice. I recommend asking your current company to work with software engineers to gain this knowledge, it worked for me!</li><li> — Most companies nowadays deploy many of their architecture and systems on the cloud, and machine learning models are no exception. So, it’s best to get practice with these tools and understand how they enable models to go live. I learned most of this on the job, to be honest, but there are courses you can take.</li><li> — I am sure most of you know this already, but every tech professional should be proficient in the command line. You will use it extensively when deploying and writing production code. I have a basic guide you can checkout<a href=\"https://medium.com/towards-data-science/an-introduction-to-the-shell-676ee5b899df?sk=0c6e101165b4314b98ab39d11525366c\"> here</a>.</li><li><strong>Data Structures &amp; Algorithms </strong>— Understanding the fundamental algorithms in computer science are very useful for MLE roles. Mainly because you will likely be asked about it in interviews. It’s not too hard to learn compared to machine learning; it just takes time. Any course will do the trick.</li><li> — Again, most tech professionals should know Git, but as an MLE, it is essential. How to squash commits, do code reviews, and write outstanding pull requests are musts.</li><li> — Many MLE roles I saw required you to have some specialisation in a particular area. I specialise in time series forecasting, optimisation, and general ML based on my previous experience. This helps you stand out in the market, and most companies are looking for specialists nowadays.</li></ul><p>The main theme here is that I basically up-skilled my software engineering abilities. This makes sense as I already had all the math, stats, and machine learning knowledge from being a data scientist.</p><p>If I were a software engineer, the transition would likely be the reverse. This is why securing a machine learning engineer role can be quite challenging, as it requires proficiency across a wide range of skills.</p><h3><strong>Summary &amp; Further Thoughts</strong></h3><p>I have a free newsletter, <a href=\"https://dishingthedata.substack.com/\"></a>, where I share weekly tips and advice as a practising data scientist. Plus, when you subscribe, you will get my and<strong> short PDF version of my AI roadmap</strong>!</p>","contentLength":9683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Refactor and cleaup yacc: Making sense of legacy code","url":"https://dev.to/mbver/refactor-and-clean-up-goyacc-making-sense-of-legacy-code-1pm0","date":1739585088,"author":"mbver","guid":572,"unread":true,"content":"<p>, a widely used LALR(1) parser generator, is efficient but burdened by archaic, unreadable code from the 1970s. Its Go adaptation, , inherits these issues despite the modern language.</p><p>While exploring compilers, I found  to be a gold mine of insights, though buried under outdated practices. Following Allan Holub’s , I dissected its code—clarifying, simplifying, and refactoring convoluted sections. I then tested the revised version in a real-world scenario to ensure it retained its original functionality. The full code is published on <a href=\"https://github.com/mbver/yacc\" rel=\"noopener noreferrer\">github</a>.</p><p>I aimed to explore real-world open-source code, bridge ideals with reality, and stay sane as a reader. By refining and clarifying it, I made learning smoother and reuse easier.</p><p>The first issue is poor naming—cryptic, vague, or misleading names. Here’s how naming was improved.</p><div><pre><code>original | cleanup          | intent\n--------------------------------------------------------------------------------------------------\ncpfir    | computeFirstsets | compute the first sets for non-terminal symbols\ncpemp    | computeEmpty     | compute compute table to check if a non-terminal symbol is nullable\ncpres    | computeYields    | compute compute the production yields for non-terminal symbols\ncurres   | prds             | the productions having the same non-terminal symbol as LHS\nwSet     | wItem            | type of a working item generated during closure\nwSets    | wSet             | store working items generated during closure\nstatemem | kernls           | store kernel items\nmstate   | statechain       | chain a state's previous state to track where it cames from\npstate   | kernlp           | pointer to a kernel item\nputitem  | addKernItem      | add a kernel item\nwritem   | item.string()    | get the string representation of a kernel item\napack    | packGotosRow     | compress newly goto row after closing state, not action as \"a\" implies\nprectfn  | handleShiftReduceConflict | handle shift-reduce conflict of a state during processing closed states\ngo2out   | packGotosByCol   | compress goto table by column\ngo2gen   | computeGotoCol   | compute the goto column for a non-terminal symbol\ncallopt  | storeShiftsAndGotos | store shift-row for a state or goto column for a non-terminal symbol in action-store\ngin      | storeGotoCol     | store a column of compressed goto table for a terminal symbol into action-store\nstin     | storeShifts      | store shifts of a row in action table for a state into action-store\nsetbit   | lkset.set        | set a bit ON\nbitset&nbsp;  | lkset.check      | check if a bit is ON (mix up with setbit easily)\n\n</code></pre></div><p>The second issue is cramming everything into one place, burying core logic (closure and state generation) under the bulky input parsing.  Refactoring separates it to files by functionality, improving clarity and focus.</p><p>The third issue is poor organization. A key component, the lookahead set, should have its own type with all related methods grouped together, not lying elsewhere. This improves maintainability and makes adding methods easier. Similarly, the kernel item should define its own string representation method. These structs also include a Clone method for copying instances.</p><p>The fourth issue is convolution. The  variable determines whether closure is  or  for state generation and  for processing closed states. However, it pops up in unexpected places like kernel item addition, state generation, and packing gotos. Furthermore, it creates twists and turns in the  logic, making it harder to follow.</p><p>To resolve this, I split  into  for  and closure0 for . While slightly redundant, this separation prevents their logic from getting entangled, making the flow easier to follow. With this change, the confusing  is completely eliminated.</p><p>Similarly,  determines whether we're handling an  but is tangled with  in closed state processing. By explicitly storing and processing , the confusion caused by  is eliminated.</p><p>The fifth issue is redundant complications. An example is  -production rules with the same . The code extracts only  to construct the  while calling it  in kernel item. By consistently using full production rules, we remove this friction. It helps to eliminate further redundancies like keeping production number in kernel item for comparison and tricky code to print the dot for a kernel item. Here is the code with comments highlighting the problems.</p><div><pre><code></code></pre></div><p>Another example is the redundant outer loop of .</p><div><pre><code></code></pre></div><p>Another redundant outer loop is in closure's work item processing.</p><div><pre><code></code></pre></div><p>The sixth issue is that several lines can be made more readable by adopting a clearer style. Let's review some snippets.</p><div><pre><code></code></pre></div><p>Thanks for reading to the end. Hope this helps on your journey.</p>","contentLength":4670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Impressions at Netflix","url":"https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb?source=rss----2615bd06b42e---4","date":1739582000,"author":"Netflix Technology Blog","guid":31,"unread":true,"content":"<h4>Part 1: Creating the Source of Truth for Impressions</h4><p>Imagine scrolling through Netflix, where each movie poster or promotional banner competes for your attention. Every image you hover over isn’t just a visual placeholder; it’s a critical data point that fuels our sophisticated personalization engine. At Netflix, we call these images ‘impressions,’ and they play a pivotal role in transforming your interaction from simple browsing into an immersive binge-watching experience, all tailored to your unique&nbsp;tastes.</p><p>Capturing these moments and turning them into a personalized journey is no simple feat. It requires a state-of-the-art system that can track and process these impressions while maintaining a detailed history of each profile’s exposure. This nuanced integration of data and technology empowers us to offer bespoke content recommendations.</p><p>In this multi-part blog series, we take you behind the scenes of our system that processes billions of impressions daily. We will explore the challenges we encounter and unveil how we are building a resilient solution that transforms these client-side impressions into a personalized content discovery experience for every Netflix&nbsp;viewer.</p><h3>Why do we need impression history?</h3><p>To tailor recommendations more effectively, it’s crucial to track what content a user has already encountered. Having impression history helps us achieve this by allowing us to identify content that has been displayed on the homepage but not engaged with, helping us deliver fresh, engaging recommendations.</p><p>By maintaining a history of impressions, we can implement frequency capping to prevent over-exposure to the same content. This ensures users aren’t repeatedly shown identical options, keeping the viewing experience vibrant and reducing the risk of frustration or disengagement.</p><h4>Highlighting New&nbsp;Releases</h4><p>For new content, impression history helps us monitor initial user interactions and adjust our merchandising efforts accordingly. We can experiment with different content placements or promotional strategies to boost visibility and engagement.</p><p>Additionally, impression history offers insightful information for addressing a number of platform-related analytics queries. Analyzing impression history, for example, might help determine how well a specific row on the home page is functioning or assess the effectiveness of a merchandising strategy.</p><p>The first pivotal step in managing impressions begins with the creation of a Source-of-Truth (SOT) dataset. This foundational dataset is essential, as it supports various downstream workflows and enables a multitude of use&nbsp;cases.</p><h4>Collecting Raw Impression Events</h4><p>As Netflix members explore our platform, their interactions with the user interface spark a vast array of raw events. These events are promptly relayed from the client side to our servers, entering a centralized event processing queue. This queue ensures we are consistently capturing raw events from our global user&nbsp;base.</p><p>After raw events are collected into a centralized queue, a custom event extractor processes this data to identify and extract all impression events. These extracted events are then routed to an Apache Kafka topic for immediate processing needs and simultaneously stored in an Apache Iceberg table for long-term retention and historical analysis. This dual-path approach leverages Kafka’s capability for low-latency streaming and Iceberg’s efficient management of large-scale, immutable datasets, ensuring both real-time responsiveness and comprehensive historical data availability.</p><h4>Filtering &amp; Enriching Raw Impressions</h4><p>Once the raw impression events are queued, a stateless Apache Flink job takes charge, meticulously processing this data. It filters out any invalid entries and enriches the valid ones with additional metadata, such as show or movie title details, and the specific page and row location where each impression was presented to users. This refined output is then structured using an Avro schema, establishing a definitive source of truth for Netflix’s impression data. The enriched data is seamlessly accessible for both real-time applications via Kafka and historical analysis through storage in an Apache Iceberg table. This dual availability ensures immediate processing capabilities alongside comprehensive long-term data retention.</p><h4>Ensuring High Quality Impressions</h4><p>Maintaining the highest quality of impressions is a top priority. We accomplish this by gathering detailed column-level metrics that offer insights into the state and quality of each impression. These metrics include everything from validating identifiers to checking that essential columns are properly filled. The data collected feeds into a comprehensive quality dashboard and supports a tiered threshold-based alerting system. These alerts promptly notify us of any potential issues, enabling us to swiftly address regressions. Additionally, while enriching the data, we ensure that all columns are in agreement with each other, offering in-place corrections wherever possible to deliver accurate&nbsp;data.</p><p>We handle a staggering volume of 1 to 1.5 million impression events globally every second, with each event approximately 1.2KB in size. To efficiently process this massive influx in real-time, we employ Apache Flink for its low-latency stream processing capabilities, which seamlessly integrates both batch and stream processing to facilitate efficient backfilling of historical data and ensure consistency across real-time and historical analyses. Our Flink configuration includes 8 task managers per region, each equipped with 8 CPU cores and 32GB of memory, operating at a parallelism of 48, allowing us to handle the necessary scale and speed for seamless performance delivery. The Flink job’s sink is equipped with a data mesh connector, as detailed in our <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh platform</a> which has two outputs: Kafka and Iceberg. This setup allows for efficient streaming of real-time data through Kafka and the preservation of historical data in Iceberg, providing a comprehensive and flexible data processing and storage solution.</p><p>We utilize the ‘island model’ for deploying our Flink jobs, where all dependencies for a given application reside within a single region. This approach ensures high availability by isolating regions, so if one becomes degraded, others remain unaffected, allowing traffic to be shifted between regions to maintain service continuity. Thus, all data in one region is processed by the Flink job deployed within that&nbsp;region.</p><h4>Addressing the Challenge of Unschematized Events</h4><p>Allowing raw events to land on our centralized processing queue unschematized offers significant flexibility, but it also introduces challenges. Without a defined schema, it can be difficult to determine whether missing data was intentional or due to a logging error. We are investigating solutions to introduce schema management that maintains flexibility while providing clarity.</p><h4>Automating Performance Tuning with Autoscalers</h4><p>Tuning the performance of our Apache Flink jobs is currently a manual process. The next step is to integrate with autoscalers, which can dynamically adjust resources based on workload demands. This integration will not only optimize performance but also ensure more efficient resource utilization.</p><h4>Improving Data Quality&nbsp;Alerts</h4><p>Right now, there’s a lot of business rules dictating when a data quality alert needs to be fired. This leads to a lot of false positives that require manual judgement. A lot of times it is difficult to track changes leading to regression due to inadequate data lineage information. We are investing in building a comprehensive data quality platform that more intelligently identifies anomalies in our impression stream, keeps track of data lineage and data governance, and also, generates alerts notifying producers of any regressions. This approach will enhance efficiency, reduce manual oversight, and ensure a higher standard of data integrity.</p><p>Creating a reliable source of truth for impressions is a complex but essential task that enhances personalization and discovery experience. Stay tuned for the next part of this series, where we’ll delve into how we use this SOT dataset to create a microservice that provides impression histories. We invite you to share your thoughts in the comments and continue with us on this journey of discovering impressions.</p><p>We are genuinely grateful to our amazing colleagues whose contributions were essential to the success of Impressions: Julian Jaffe, Bryan Keller, Yun Wang, Brandon Bremen, Kyle Alford, Ron Brown and Shriya&nbsp;Arora.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e2b67c88c9fb\" width=\"1\" height=\"1\" alt=\"\">","contentLength":8607,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tabiew 0.8.4 Released","url":"https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/","date":1739578902,"author":"/u/shshemi","guid":285,"unread":true,"content":"<p>Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...</p><ul><li>📊 Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and Sqlite</li><li>🗂️ Multi-table functionality</li></ul><ul><li>UI is updated to be more modern and responsive</li><li>Horizontally scrollable tables</li><li>Visible data frame can be referenced with name \"_\"</li><li>Compatibility with older versions of glibc</li><li>Two new themes (Tokyo Night and Catppuccin)</li></ul>","contentLength":450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditional types in TypeScript","url":"https://2ality.com/2025/02/conditional-types-typescript.html","date":1739577600,"author":"Dr. Axel Rauschmayer","guid":175,"unread":true,"content":"<p>In TypeScript, conditional types let us make decisions (think if-then-else expressions) – which is especially useful in generic types. They are also an essential tool for working with union types because they let use “loop” over them. Read on if you want to know how all of that works.</p>","contentLength":291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: VimLM – A Local, Offline Coding Assistant for Vim","url":"https://github.com/JosefAlbers/VimLM","date":1739576081,"author":"JosefAlbers","guid":451,"unread":true,"content":"<p>VimLM is a local, offline coding assistant for Vim. It’s like Copilot but runs entirely on your machine—no APIs, no tracking, no cloud.</p><p>- Deep Context: Understands your codebase (current file, selections, references).  \n- Conversational: Iterate with follow-ups like \"Add error handling\".  \n- Vim-Native: Keybindings like `Ctrl-l` for prompts, `Ctrl-p` to replace code.  \n- Inline Commands: `!include` files, `!deploy` code, `!continue` long responses.</p><p>Perfect for privacy-conscious devs or air-gapped environments.</p><p>Try it:  \n```\npip install vimlm\nvimlm\n```</p>","contentLength":558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43054244"},{"title":"Kay Hayen: Nuitka this week #16","url":"https://nuitka.net/posts/nuitka-this-week-16.html","date":1739574000,"author":"","guid":485,"unread":true,"content":"<p>Hey Nuitka users! This started out as an idea of a weekly update, but\nthat hasn’t happened, and so we will switch it over to just writing up\nwhen something interesting happens and then push it out relatively\nimmediately when it happens.</p><h2>Nuitka Onefile Gets More Flexible:  and </h2><p>We’ve got a couple of exciting updates to Nuitka’s onefile mode that\ngive you more control and flexibility in how you deploy your\napplications. These enhancements stem from real-world needs and\ndemonstrate Nuitka’s commitment to providing powerful and adaptable\nsolutions.</p><h3>Taking Control of Onefile Unpacking: </h3><p>Onefile mode is fantastic for creating single-file executables, but the\nmanagement of the unpacking directory where the application expands has\nsometimes been a bit… opaque. Previously, Nuitka would decide whether\nto clean up this directory based on whether the path used\nruntime-dependent variables. This made sense in theory, but in practice,\nit could lead to unexpected behavior and made debugging onefile issues\nharder.</p><p>Now, you have complete control! The new  option\nlets you explicitly specify what happens to the unpacking directory:</p><ul><li><p><code></code>: This is the default behavior. Nuitka\nwill remove the unpacking directory unless runtime-dependent values\nwere used in the path specification. This is the same behavior as\nprevious versions.</p></li><li><p><code></code>: The unpacking directory is \nremoved and becomes a persistent, cached directory. This is useful\nfor debugging, inspecting the unpacked files, or if you have a use\ncase that benefits from persistent caching of the unpacked data. The\nfiles will remain available for subsequent runs.</p></li><li><p><code></code>: The unpacking directory \nremoved after the program exits.</p></li></ul><p>This gives you the power to choose the behavior that best suits your\nneeds. No more guessing!</p><h3>Relative Paths with </h3><p>Another common request, particularly from users deploying applications\nin more restricted environments, was the ability to specify the onefile\nunpacking directory  to the executable itself. Previously, you\nwere limited to absolute paths or paths relative to the user’s temporary\ndirectory space.</p><p>We’ve introduced a new variable, , that you can use in\nthe  option. This variable is dynamically\nreplaced at runtime with the full path to the directory containing the\nonefile executable.</p><div><div><pre></pre></div></div><p>This would create a directory named  the same\ndirectory as the  (or  on Linux/macOS)\nand unpack the application there. This is perfect for creating truly\nself-contained applications where all data and temporary files reside\nalongside the executable.</p><h3>Nuitka Commercial and Open Source</h3><p>These features, like many enhancements to Nuitka, originated from a\nrequest by a Nuitka commercial customer. This highlights the close\nrelationship between the commercial offerings and the open-source core.\nWhile commercial support helps drive development and ensures the\nlong-term sustainability of Nuitka, the vast majority of features are\nmade freely available to all users.</p><p>This change will be in 2.7 and is currently</p><p>We encourage you to try out these new features and let us know what you\nthink! As always, bug reports, feature requests, and contributions are\nwelcome on <a href=\"https://github.com/Nuitka/Nuitka/issues\">GitHub</a>.</p>","contentLength":3127,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Django Weblog: DjangoCongress JP 2025 Announcement and Live Streaming!","url":"https://www.djangoproject.com/weblog/2025/feb/14/djangocongress-jp-2025-announcement-and-livestream/","date":1739571130,"author":"","guid":169,"unread":true,"content":"<p>It will be streamed on the following YouTube Live channels:</p><p>This year there will be talks not only about Django, but also about FastAPI and other asynchronous web topics. There will also be talks on Django core development, Django Software Foundation (DSF) governance, and other topics from around the world. Simultaneous translation will be provided in both English and Japanese.</p><ul><li>The Async Django ORM: Where Is it?</li><li>Speed at Scale for Django Web Applications</li><li>Implementing Agentic AI Solutions in Django from scratch</li><li>Diving into DSF governance: past, present and future</li></ul><ul><li>Getting Knowledge from Django Hits: Using Grafana and Prometheus</li><li>Culture Eats Strategy for Breakfast: Why Psychological Safety Matters in Open Source</li><li>µDjango. The next step in the evolution of asynchronous microservices technology.</li></ul><p>A public viewing of the event will also be held in Tokyo. A reception will also be held, so please check the following connpass page if you plan to attend.</p>","contentLength":948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eli Bendersky: Decorator JITs - Python as a DSL","url":"https://eli.thegreenplace.net/2025/decorator-jits-python-as-a-dsl/","date":1739569771,"author":"","guid":168,"unread":true,"content":"<p>Spend enough time looking at Python programs and packages for machine learning,\nand you'll notice that the \"JIT decorator\" pattern is pretty popular. For\nexample, this JAX snippet:</p><div><pre></pre></div><div><pre></pre></div><p>In both cases, the function decorated with  doesn't get executed by the\nPython interpreter in the normal sense. Instead, the code inside is more like\na DSL (Domain Specific Language) processed by a special purpose compiler built\ninto the library (JAX or Triton). Another way to think about it is that Python\nis used as a  to describe computations.</p><p>In this post I will describe some implementation strategies used by libraries to\nmake this possible.</p><div><h2>Preface - where we're going</h2><p>The goal is to explain how different kinds of  decorators work by using\na simplified, educational example that implements several approaches from\nscratch. All the approaches featured in this post will be using this flow:</p> Expr IR --&gt; LLVM IR --&gt; Execution\" /&gt; Expr IR --&gt; LLVM IR --&gt; Execution\" class=\"align-center\" src=\"https://eli.thegreenplace.net/images/2025/decjit-python.png\" /&gt;\n<p>These are the steps that happen when a Python function wrapped with\nour educational  decorator is called:</p><ol><li>The function is translated to an \"expression IR\" - .</li><li>This expression IR is converted to LLVM IR.</li><li>Finally, the LLVM IR is JIT-executed.</li></ol><p>First, let's look at the  IR. Here we'll make a big simplification -\nonly supporting functions that define a single expression, e.g.:</p><div><pre></pre></div><p>Naturally, this can be easily generalized - after all, LLVM IR can be used to\nexpress fully general computations.</p><p>Here are the  data structures:</p><div><pre></pre></div><p>To convert an  into LLVM IR and JIT-execute it, we'll use this function:</p><div><pre></pre></div><p>It uses the  class to actually generate LLVM IR from .\nThis process is straightforward and covered extensively in the resources I\nlinked to earlier; take a look at <a href=\"https://github.com/eliben/code-for-blog/blob/main/2025/decjit/exprcode.py\">the full code here</a>.</p><p>My goal with this architecture is to make things simple, but .\nOn one hand - there are several simplifications: only single expressions are\nsupported, very limited set of operators, etc. It's very easy to extend this!\nOn the other hand, we could have just trivially evaluated the \nwithout resorting to LLVM IR; I do want to show a more complete compilation\npipeline, though, to demonstrate that an arbitrary amount of complexity can\nbe hidden behind these simple interfaces.</p><p>With these building blocks in hand, we can review the strategies used by\n decorators to convert Python functions into s.</p></div><div><p>Python comes with powerful code reflection and introspection capabilities out\nof the box. Here's the  decorator:</p><div><pre></pre></div><p>This is a standard Python decorator. It takes a function and returns another\nfunction that will be used in its place ( ensures that\nfunction attributes like the name and docstring of the wrapper match the\nwrapped function).</p><div><pre></pre></div><p>After  is applied to , what  holds is the\nwrapper. When  is called, the wrapper is invoked with\n.</p><p>The wrapper obtains the AST of the wrapped function, and then uses\n to convert this AST into an :</p><div><pre></pre></div><p>When  finishes visiting the AST it's given, its\n field will contain the  representing the function's\nreturn value. The wrapper then invokes  with this .</p><p>Note how our decorator interjects into the regular Python execution process.\nWhen  is called, instead of the standard Python compilation and\nexecution process (code is compiled into bytecode, which is then executed\nby the VM), we translate its code to our own representation and emit LLVM from\nit, and then JIT execute the LLVM IR. While it seems kinda pointless in this\nartificial example, in reality this means we can execute the function's code\nin any way we like.</p><div><h3>AST JIT case study: Triton</h3><p>This approach is almost exactly how the Triton language works. The body of a\nfunction decorated with  gets parsed to a Python AST, which then\n- through a series of internal IRs - ends up in LLVM IR; this in turn is lowered\nto <a href=\"https://docs.nvidia.com/cuda/parallel-thread-execution/\">PTX</a> by the\n<a href=\"https://llvm.org/docs/NVPTXUsage.html\">NVPTX LLVM backend</a>.\nThen, the code runs on a GPU using a standard CUDA pipeline.</p><p>Naturally, the subset of Python that can be compiled down to a GPU is limited;\nbut it's sufficient to run performant kernels, in a language that's much\nfriendlier than CUDA and - more importantly - lives in the same file with the\n\"host\" part written in regular Python. For example, if you want testing and\ndebugging, you can run Triton in \"interpreter mode\" which will just run the\nsame kernels locally on a CPU.</p><p>Note that Triton lets us import names from the  package\nand use them inside kernels; these serve as the  for the language\n- special calls the compiler handles directly.</p></div></div><div><p>Python is a fairly complicated language with  of features. Therefore,\nif our JIT has to support some large portion of Python semantics, it may make\nsense to leverage more of Python's own compiler. Concretely, we can have it\ncompile the wrapped function all the way <a href=\"https://github.com/python/cpython/blob/main/InternalDocs/interpreter.md\">to bytecode</a>,\nand start our translation from there.</p><p>Here's the  decorator that does just this :</p><div><pre></pre></div><p>The Python VM is a stack machine; so we emulate a stack to convert the\nfunction's bytecode to  IR (a bit like an <a href=\"https://en.wikipedia.org/wiki/Reverse_Polish_notation\">RPN evaluator</a>).\nAs before, we then use our  utility function to lower\n to LLVM IR and JIT execute it.</p><p>Using this JIT is as simple as the previous one - just swap \nfor :</p><div><pre></pre></div><div><h3>Bytecode JIT case study: Numba</h3><p><a href=\"https://numba.pydata.org/\">Numba</a> is a compiler for Python itself. The idea\nis that you can speed up specific functions in your code by slapping a\n decorator on them. What happens next is similar in spirit to\nour simple , but of course much more complicated because it\nsupports a very large portion of Python semantics.</p><p>Numba uses the Python compiler to emit bytecode, just as we did; it then\nconverts it into its own IR, and then to LLVM using .</p><p>By starting with the bytecode, Numba makes its life easier (no need to rewrite\nthe entire Python compiler). On the other hand, it also makes some analyses\n, because by the time we're in bytecode, a lot of semantic information\nexisting in higher-level representations is lost. For example, Numba has to\nsweat a bit to recover control flow information from the bytecode (by\nrunning it through a special interpreter first).</p></div></div><div><p>The two approaches we've seen so far are similar in many ways - both rely on\nPython's introspection capabilities to compile the source code of the JIT-ed\nfunction to some extent (one to AST, the other all the way to bytecode), and\nthen work on this lowered representation.</p><p>The tracing strategy is very different. It doesn't analyze the source code of\nthe wrapped function at all - instead, it  its execution by means of\nspecially-boxed arguments, leveraging overloaded operators and functions, and\nthen works on the generated trace.</p><p>The code implementing this for our smile demo is surprisingly compact:</p><div><pre></pre></div><p>Each runtime argument of the wrapped function is assigned a , and\nthat is placed in a , a placeholder class which lets us\ndo operator overloading:</p><div><pre></pre></div><p>The remaining key function is :</p><div><pre></pre></div><p>To understand how this works, consider this trivial example:</p><div><pre></pre></div><p>After the decorated function is defined,  holds the wrapper function\ndefined inside . When  is called, the wrapper runs:</p><ol><li>For each argument of  itself (that is  and ), it creates\na new  holding a . This denotes a named variable in\nthe  IR.</li><li>It then calls the wrapped function, passing it the boxes as runtime\nparameters.</li><li>When (the wrapped)  runs, it invokes . This is caught by the overloaded\n operator of , and it creates a new  with\nthe s representing  and  as children. This\n is then returned .</li><li>The wrapper unboxes the returned  and passes it to\n to emit LLVM IR from it and JIT execute it with the\nactual runtime arguments of the call: .</li></ol><p>This might be a little mind-bending at first, because there are two different\nexecutions that happen:</p><ul><li>The first is calling the wrapped  function itself, letting the Python\ninterpreter run it as usual, but with special arguments that build up the IR\ninstead of doing any computations. This is the .</li><li>The second is lowering this IR our tracing step built into LLVM IR and then\nJIT executing it with the actual runtime argument values ; this is\nthe .</li></ul><p>This tracing approach has some interesting characteristics. Since we don't\nhave to analyze the source of the wrapped functions but only trace through\nthe execution, we can \"magically\" support a much richer set of programs, e.g.:</p><div><pre></pre></div><p>This  with our basic . Since Python variables are\nplaceholders (references) for values, our tracing step is oblivious to them - it\nfollows the flow of values. Another example:</p><div><pre></pre></div><p>This also just works! The created  will be a long chain of \nadditions of 's runtime values through the loop, added to the \nfor .</p><p>This last example also leads us to a limitation of the tracing approach; the\nloop cannot be  - it cannot depend on the function's arguments,\nbecause the tracing step has no concept of runtime values and wouldn't know\nhow many iterations to run through; or at least, it doesn't know this unless\nwe want to perform the tracing run for every runtime execution .</p><div><h3>Tracing JIT case study: JAX</h3><p>The <a href=\"https://jax.readthedocs.io/en/latest/\">JAX ML framework</a> uses a tracing\napproach very similar to the one described here. The first code sample in this\npost shows the JAX notation. JAX cleverly wraps Numpy with its own version which\nis traced (similar to our , but JAX calls these boxes \"tracers\"),\nletting you write regular-feeling Numpy code that can be JIT optimized and\nexecuted on accelerators like GPUs and TPUs via <a href=\"https://github.com/openxla\">XLA</a>. JAX's tracer builds up an underlying IR (called\n<a href=\"https://jax.readthedocs.io/en/latest/jaxpr.html\">jaxpr</a>) which can then be\nemitted to XLA ops and passed to XLA for further lowering and execution.</p><p>For a fairly deep overview of how JAX works, I recommend reading the\n<a href=\"https://jax.readthedocs.io/en/latest/autodidax.html\">autodidax doc</a>.</p><p>As mentioned earlier, JAX has <a href=\"https://jax.readthedocs.io/en/latest/jit-compilation.html\">some limitations</a>\nwith things like data-dependent control flow in native Python. This won't work,\nbecause there's control flow\nthat depends on a runtime value ():</p><div><pre></pre></div><p>When  is executed, JAX will throw an exception, saying something\nlike:</p><blockquote>\nThis concrete value was not available in Python because it depends on the\nvalue of the argument count.</blockquote><p>As a remedy, JAX has its\nown built-in intrinsics from the <a href=\"https://jax.readthedocs.io/en/latest/jax.lax.html\">jax.lax package</a>.\nHere's the example rewritten in a way that actually works:</p><div><pre></pre></div><p> (and many other built-ins in the  package) is something JAX\ncan trace through, generating a corresponding XLA operation (XLA has support for\n<a href=\"https://openxla.org/xla/operation_semantics\">While loops</a>, to which this\n can be lowered).</p><p>The tracing approach has clear benefits for JAX as well; because it only cares\nabout the flow of values, it can handle arbitrarily complicated Python code,\nas long as the flow of values can be traced. Just like the local variables and\ndata-independent loops shown earlier, but also things like closures. This makes\nmeta-programming and templating easy .</p></div></div><div><p>The full code for this post is available <a href=\"https://github.com/eliben/code-for-blog/tree/main/2025/decjit\">on GitHub</a>.</p></div>","contentLength":10514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.","url":"https://v.redd.it/sglstazd96je1","date":1739568243,"author":"/u/eternviking","guid":294,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iple9t/an_art_exhibit_in_japan_where_a_chained_robot_dog/"},{"title":"OpenAI: The Age of AI Is Here!","url":"https://www.youtube.com/watch?v=97kQRYwL3P0","date":1739557087,"author":"Two Minute Papers","guid":329,"unread":true,"content":"<article>❤️ Check out Lambda here and sign up for their GPU Cloud: https://lambdalabs.com/papers\n\n📝 The paper \"Competitive Programming with Large Reasoning Models\" is available here:\nhttps://arxiv.org/abs/2502.06807\n\n📝 My paper on simulations that look almost like reality is available for free here:\nhttps://rdcu.be/cWPfD \n\nOr this is the orig. Nature Physics link with clickable citations:\nhttps://www.nature.com/articles/s41567-022-01788-5\n\n🙏 We would like to thank our generous Patreon supporters who make Two Minute Papers possible:\nBenji Rabhan, B Shang, Christian Ahlin, Gordon Child, John Le, Juan Benet, Kyle Davis, Loyal Alchemist, Lukas Biewald, Michael Tedder, Owen Skarpness, Richard Sundvall, Steef, Taras Bobrovytsky, Thomas Krcmar, Tybie Fitzhugh, Ueli GallizziIf you wish to appear here or pick up other perks, click here: https://www.patreon.com/TwoMinutePapers\n\nMy research: https://cg.tuwien.ac.at/~zsolnai/\nX/Twitter: https://twitter.com/twominutepapers\nThumbnail design: Felícia Zsolnai-Fehér - http://felicia.hu</article>","contentLength":1040,"flags":null,"enclosureUrl":"https://www.youtube.com/v/97kQRYwL3P0?version=3","enclosureMime":"","commentsUrl":null},{"title":"Roadmap to Becoming a Data Scientist, Part 4: Advanced Machine Learning","url":"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-4-advanced-machine-learning/","date":1739552400,"author":"Vyacheslav Efimov","guid":9,"unread":true,"content":"<p>Data science is undoubtedly one of the most fascinating fields today.&nbsp;Following significant breakthroughs in machine learning about a decade ago, data science has surged in popularity within the tech community.&nbsp;Each year, we witness increasingly powerful tools that once seemed unimaginable.&nbsp;Innovations such as the&nbsp;,&nbsp;, the&nbsp;<em>Retrieval-Augmented Generation (RAG</em>) framework, and state-of-the-art&nbsp;&nbsp;— including&nbsp;&nbsp;— have had a profound impact on our world.</p><p>However, with the abundance of tools and the ongoing hype surrounding AI, it can be overwhelming — especially for beginners — to determine which skills to prioritize when aiming for a career in data science.&nbsp;Moreover, this field is highly demanding, requiring substantial dedication and perseverance.</p><p>The first three parts of this series outlined the necessary skills to become a data scientist in three key areas: <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-1-maths-2dc9beb69b27/\">math</a>, <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-2-software-engineering-e2fee3fe4d71/\">software engineering</a>, and <a href=\"https://towardsdatascience.com/roadmap-to-becoming-a-data-scientist-part-3-machine-learning-628248c96cb5/\">machine learning</a>.&nbsp;While knowledge of classical <a href=\"https://towardsdatascience.com/tag/machine-learning/\" title=\"Machine Learning\">Machine Learning</a> and neural network algorithms is an excellent starting point for aspiring data specialists, there are still many&nbsp;important topics in machine learning that must be mastered to work on more advanced projects.</p><blockquote><p><em>This article will focus solely on the math skills necessary to start a career in Data Science.&nbsp;Whether pursuing this path is a worthwhile choice based on your background and other factors will be discussed in a separate article.</em></p></blockquote><h2>The importance of learning evolution of methods in machine learning</h2><blockquote><p><em>The section below provides information about the evolution of methods in natural language processing (NLP).</em></p></blockquote><p>In contrast to previous articles in this series, I have decided to change the format in which I present the necessary skills for aspiring data scientists. Instead of directly listing specific competencies to develop and the motivation behind mastering them, I will briefly outline the most important approaches, presenting them in chronological order as they have been developed and used over the past decades in machine learning.</p><p>The reason is that I believe it is crucial to study these algorithms from the very beginning. In machine learning, many new methods are built upon older approaches, which is especially true for <a href=\"https://towardsdatascience.com/tag/nlp/\" title=\"NLP\">NLP</a> and computer vision.</p><p>For example, jumping directly into the implementation details of modern&nbsp;large language models (LLMs)&nbsp;without any preliminary knowledge may make it very difficult for beginners to grasp the motivation and underlying ideas of specific mechanisms.</p><p><em>Given this, in the next two sections, I will highlight in&nbsp;</em><em>&nbsp;the key concepts that should be studied.</em></p><p><strong>Natural language processing (NLP)</strong>&nbsp;is a broad field that focuses on processing textual information. Machine learning algorithms cannot work directly with raw text, which is why text is usually preprocessed and converted into numerical vectors that are then fed into neural networks.</p><p>Before being converted into vectors, words undergo&nbsp;, which includes simple techniques such as&nbsp;,&nbsp;<strong>stemming, lemmatization, normalization</strong>, or removing&nbsp;. After preprocessing, the resulting text is encoded into&nbsp;. Tokens represent the smallest textual elements in a collection of documents. Generally, a token can be a part of a word, a sequence of symbols, or an individual symbol. Ultimately, tokens are converted into numerical vectors.</p><p>The&nbsp;&nbsp;method is the most basic way to encode tokens, focusing on counting the frequency of tokens in each document. However, in practice, this is usually not sufficient, as it is also necessary to account for token importance — a concept introduced in the&nbsp;&nbsp;and&nbsp;&nbsp;methods. While TF-IDF improves upon the naive counting approach of bag of words, researchers have developed a completely new approach called embeddings.</p><p>&nbsp;are numerical vectors whose components preserve the semantic meanings of words. Because of this, embeddings play a crucial role in NLP, enabling input data to be trained or used for model inference. Additionally, embeddings can be used to compare text similarity, allowing for the retrieval of the most relevant documents from a collection.</p><blockquote><p><em>Embeddings can also be used to encode other unstructured data, including images, audio, and videos.</em></p></blockquote><p>As a field, NLP has been evolving rapidly over the last 10–20 years to efficiently solve various text-related problems. Complex tasks like text translation and text generation were initially addressed using&nbsp;<strong>recurrent neural networks (RNNs)</strong>, which introduced the concept of memory, allowing neural networks to capture and retain key contextual information in long documents.</p><p>Although RNN performance gradually improved, it remained suboptimal for certain tasks. Moreover, RNNs are relatively slow, and their sequential prediction process does not allow for parallelization during training and inference, making them less efficient.</p><p>Additionally, the original Transformer architecture can be decomposed into two separate modules:&nbsp;&nbsp;and&nbsp;. Both of these form the foundation of the most state-of-the-art models used today to solve various NLP problems. Understanding their principles is valuable knowledge that will help learners advance further when studying or working with other&nbsp;<strong>large language models (LLMs)</strong>.</p><p>When it comes to LLMs, I strongly recommend studying the evolution of at least the first three GPT models, as they have had a significant impact on the AI world we know today. In particular, I would like to highlight the concepts of&nbsp;&nbsp;and&nbsp;, introduced in&nbsp;GPT-2, which enable LLMs to solve text generation tasks without explicitly receiving any training examples for them.</p><p>Another important technique developed in recent years is&nbsp;<strong>retrieval-augmented generation (RAG)</strong>.&nbsp;<em>The main limitation of LLMs is that they are only aware of the context used during their training.</em>&nbsp;As a result, they lack knowledge of any information beyond their training data.</p><p>The retriever converts the input prompt into an embedding, which is then used to query a vector database. The database returns the most relevant context based on the similarity to the embedding. This retrieved context is then combined with the original prompt and passed to a generative model. The model processes both the initial prompt and the additional context to generate a more informed and contextually accurate response.</p><blockquote><p><em>A good example of this limitation is the first version of the ChatGPT model, which was trained on data up to the year 2022 and had no knowledge of events that occurred from 2023 onward.</em></p></blockquote><p>To address this limitation, OpenAI researchers developed a RAG pipeline, which includes a constantly updated database containing new information from external sources. When ChatGPT is given a task that requires external knowledge, it queries the database to retrieve the most relevant context and integrates it into the final prompt sent to the machine learning model.</p><p>The goal of distillation is to create a smaller model that can imitate a larger one. In practice, this means that if a large model makes a prediction, the smaller model is expected to produce a similar result.</p><p>In the modern era, LLM development has led to models with millions or even billions of parameters. As a consequence, the overall size of these models may exceed the hardware limitations of standard computers or small portable devices, which come with many constraints.</p><p>Quantization is the process of reducing the memory required to store numerical values representing a model’s weights.</p><p>This is where optimization techniques become particularly useful, allowing LLMs to be compressed without significantly compromising their performance. The most commonly used techniques today include&nbsp;,, and&nbsp;.</p><p>Pruning refers to discarding the least important weights of a model.</p><p>Regardless of the area in which you wish to specialize, knowledge of&nbsp;&nbsp;is a must-have skill! Fine-tuning is a powerful concept that allows you to efficiently adapt a pre-trained model to a new task.</p><p>Fine-tuning is especially useful when working with very large models. For example, imagine you want to use BERT to perform semantic analysis on a specific dataset. While BERT is trained on general data, it might not fully understand the context of your dataset. At the same time, training BERT from scratch for your specific task would require a massive amount of resources.</p><p>Here is where fine-tuning comes in: it involves taking a pre-trained BERT (or another model) and freezing some of its layers (usually those at the beginning). As a result, BERT is retrained, but this time only on the new dataset provided. Since BERT updates only a subset of its weights and the new dataset is likely much smaller than the original one BERT was trained on, fine-tuning becomes a very efficient technique for adapting BERT’s rich knowledge to a specific domain.</p><blockquote><p><em>Fine-tuning is widely used not only in NLP but also across many other domains.</em></p></blockquote><p>As the name suggests,&nbsp;&nbsp;involves analyzing images and videos using machine learning. The most common tasks include image classification, object detection, image segmentation, and generation.</p><p>Most CV algorithms are based on neural networks, so it is essential to understand how they work in detail. In particular, CV uses a special type of network called&nbsp;<strong>convolutional neural networks (CNNs)</strong>. These are similar to fully connected networks, except that they typically begin with a set of specialized mathematical operations called&nbsp;.</p><blockquote><p><em>In simple terms, convolutions act as filters, enabling the model to extract the most important features from an image, which are then passed to fully connected layers for further analysis.</em></p></blockquote><p>The next step is to study the most popular CNN architectures for classification tasks, such as&nbsp;<strong>AlexNet, VGG, Inception, ImageNet</strong>, and&nbsp;.</p><p>Speaking of the object detection task, the&nbsp;&nbsp;algorithm is a clear winner. It is not necessary to study all of the dozens of versions of YOLO. In reality, going through the original paper of the first YOLO should be sufficient to understand how a relatively difficult problem like object detection is elegantly transformed into both classification and regression problems. This approach in YOLO also provides a nice intuition on how more complex CV tasks can be reformulated in simpler terms.</p><p>While there are many architectures for performing image segmentation, I would strongly recommend learning about&nbsp;, which introduces an encoder-decoder architecture.</p><p>Finally, image generation is probably one of the most challenging tasks in CV. Personally, I consider it an optional topic for learners, as it involves many advanced concepts. Nevertheless, gaining a high-level intuition of how&nbsp;<strong>generative adversial networks (GAN)</strong>&nbsp;function to generate images is a good way to broaden one’s horizons.</p><blockquote><p><em>In some problems, the training data might not be enough to build a performant model. In such cases, the data augmentation technique is commonly used. It involves the artificial generation of training data from already existing data (images). By feeding the model more diverse data, it becomes capable of learning and recognizing more patterns.</em></p></blockquote><p>It would be very hard to present in detail the <a href=\"https://towardsdatascience.com/tag/roadmaps/\" title=\"Roadmaps\">Roadmaps</a> for all existing machine learning domains in a single article. That is why, in this section, I would like to briefly list and explain some of the other most popular areas in data science worth exploring.</p><p>First of all,&nbsp;<strong>recommender systems (RecSys)</strong>&nbsp;have gained a lot of popularity in recent years. They are increasingly implemented in online shops, social networks, and streaming services. The key idea of most algorithms is to take a large initial matrix of all users and items and decompose it into a product of several matrices in a way that associates every user and every item with a high-dimensional embedding. This approach is very flexible, as it then allows different types of comparison operations on embeddings to find the most relevant items for a given user. Moreover, it is much more rapid to perform analysis on small matrices rather than the original, which usually tends to have huge dimensions.</p><p> often goes hand in hand with RecSys. When a RecSys has identified a set of the most relevant items for the user, ranking algorithms are used to sort them to determine the order in which they will be shown or proposed to the user. A good example of their usage is search engines, which filter query results from top to bottom on a web page.</p><p>Closely related to ranking, there is also a&nbsp;&nbsp;problem that aims to optimally map objects from two sets, A and B, in a way that, on average, every object pair&nbsp;is mapped “well” according to a matching criterion. A use case example might include distributing a group of students to different university disciplines, where the number of spots in each class is limited.</p><p>&nbsp;is an unsupervised machine learning task whose objective is to split a dataset into several regions (clusters), with each dataset object belonging to one of these clusters. The splitting criteria can vary depending on the task. Clustering is useful because it allows for grouping similar objects together. Moreover, further analysis can be applied to treat objects in each cluster separately.</p><p>The goal of clustering is to group dataset objects (on the left) into several categories (on the right) based on their similarity.</p><p>&nbsp;is another unsupervised problem, where the goal is to compress an input dataset. When the dimensionality of the dataset is large, it takes more time and resources for machine learning algorithms to analyze it. By identifying and removing noisy dataset features or those that do not provide much valuable information, the data analysis process becomes considerably easier.</p><p>&nbsp;is an area that focuses on designing algorithms and data structures (indexes) to optimize searches in a large database of embeddings (vector database). More precisely, given an input embedding and a vector database, the goal is to&nbsp;&nbsp;find the most similar embedding in the database relative to the input embedding.</p><p>The goal of similarity search is to approximately find the most similar embedding in a vector database relative to a query embedding.</p><p>The word “approximately” means that the search is not guaranteed to be 100% precise. Nevertheless, this is the main idea behind similarity search algorithms — sacrificing a bit of accuracy in exchange for significant gains in prediction speed or data compression.</p><p>&nbsp;involves studying the behavior of a target variable over time. This problem can be solved using classical tabular algorithms. However, the presence of time introduces new factors that cannot be captured by standard algorithms. For instance:</p><ul><li>the target variable can have an overall&nbsp;, where in the long term its values increase or decrease&nbsp;<em>(e.g., the average yearly temperature rising due to global warming)</em>.</li><li>the target variable can have a&nbsp;&nbsp;which makes its values change based on the currently given period&nbsp;<em>(e.g. temperature is lower in winter and higher in summer)</em>.</li></ul><p>Most of the time series models take both of these factors into account. In general, time series models are mainly used a lot in financial, stock or demographic analysis.</p><p>Another advanced area I would recommend exploring is&nbsp;, which fundamentally changes the algorithm design compared to classical machine learning.&nbsp;In simple terms, its goal is to train an agent in an environment to make optimal decisions based on a reward system (also known as the&nbsp;<em>“trial and error approach”</em>).&nbsp;By taking an action, the agent receives a reward, which helps it understand whether the chosen action had a positive or negative effect.&nbsp;After that, the agent slightly adjusts its strategy, and the entire cycle repeats.</p><p>Reinforcement learning is particularly popular in complex environments where classical algorithms are not capable of solving a problem.&nbsp;Given the complexity of reinforcement learning algorithms and the computational resources they require, this area is not yet fully mature, but it has high potential to gain even more popularity in the future.</p><p>Currently the most popular applications are:</p><ul><li>.&nbsp;Existing approaches can design optimal game strategies and outperform humans.&nbsp;The most well-known examples are chess and Go.</li><li>.&nbsp;Advanced algorithms can be incorporated into robots to help them move, carry objects or complete routine tasks at home.</li><li>.&nbsp;Reinforcement learning methods can be developed to automatically drive cars, control helicopters or drones.</li></ul><p>This article was a logical continuation of the previous part and expanded the skill set needed to become a data scientist. While most of the mentioned topics require time to master, they can add significant value to your portfolio. This is especially true for the NLP and CV domains, which are in high demand today.</p><blockquote><p>After reaching a high level of expertise in data science, it is still crucial to stay motivated and consistently push yourself to learn new topics and explore emerging algorithms.</p></blockquote><p>Data science is a constantly evolving field, and in the coming years, we might witness the development of new state-of-the-art approaches that we could not have imagined in the past.</p><p><em>All images are by the author unless noted otherwise.</em></p>","contentLength":17044,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generate a Momento Disposable Token with Rust and Lambda","url":"https://dev.to/aws-builders/generate-a-momento-disposable-token-with-rust-and-lambda-1a58","date":1739552249,"author":"Benjamen Pyle","guid":550,"unread":true,"content":"<p>Working with browser hosted code (UI) requires a developer to be cautious about exposing secrets and tokens.  A less than trustworthy person could take these secrets and do things that the user doesn't intend.  And while we are all responsible for our internet usage, token and secrets security from an application standpoint falls squarely on a developer's shoulders.  This is why when using <a href=\"https://www.gomomento.com/\" rel=\"noopener noreferrer\">Momento</a>, I like to take advantage of the Authorization API.  What the Authorization API allows me to do is create a disposable token from a secure location, so that my UI clients can just refresh them as needed to work with Topics or Caches.  Thus, not having the credential leak up into the \"easy to see\" JavaScript code.  Let's dive into a Lambda Function coded in Rust that implements this Token Vending Machine concept with Momento.</p><p>I usually like to work backwards to forwards, meaning I establish what I want in the end and then build from there.  When looking at a sample implementation, that means starting from the diagram and walking through what I'm building.</p><p>A user's session will need to establish an authenticated and authorized connection to Momento by way of the JavaScript client SDK.  Every call to Momento is over an HTTP API request so it's going to get authenticated and authorized.  Which is a good thing!  However, doing this, requires a token which is what I'll be fetching from the Rust Lambda Function that will be demonstrated throughout the article.  The flow goes like this:</p><p>1) User requires a token to connect to Momento\n2) Browser makes a request to an endpoint backed by a Lambda Function<p>\n3) Rust Lambda Function uses a long-lived and secure API Token that has permissions to create short-lived disposable tokens</p>\n4) Rust Function uses the Momento SDK to request a token with the supplied Topic and Cache names with scopes to publish and subscribe<p>\n5) A token is returned from the Lambda Function where the client code can use to subscribe to a Momento topic.</p>\n6) The token has an expiration timestamp represented as a Unix Epoch so that the client can refresh before the token has a chance to expire</p><p>So let's walk through those steps above and explore the implementation.</p><h3>\n  \n  \n  Implementing a Momento Token Vending Machine with Rust\n</h3><p>I know I'm focusing on Lambda, Momento, and Rust, but there are many other components that go into what I'd consider a quality Lambda Function build.  To address those, let's have a look at the CDK code and what all gets shipped to AWS.</p><p>TypeScript has become my goto when it comes to creating AWS infrastructure.  I like the CDK, and I especially like having the ability to use the Cargo Lambda CDK Construct.  If you haven't used it before, check out the <a href=\"https://github.com/cargo-lambda/cargo-lambda-cdk\" rel=\"noopener noreferrer\">repository</a> and jump into the documentation.  It's straightforward and the classes inherit from AWS bases.  In addition to Cargo Lambda, I like to include the <a href=\"https://www.instagram.com/reel/DFsdRBvAcyQ/?utm_source=ig_web_copy_link\" rel=\"noopener noreferrer\">Datadog Lambda Extension</a>.  This piece of goodness allows me to collect my <a href=\"https://binaryheap.com/rust-and-opentelemetry-with-lambda-datadog/\" rel=\"noopener noreferrer\">OpenTelemetry</a> traces into the Datadog UI for easy assessment of performance and any latency or error issues.  I'll highlight further as the article evolves.</p><p>Here we go! The below is the CDK code that brings the above together.</p><h5>\n  \n  \n  Adding the Datadog Extension\n</h5><p>Pay special attention to the following when adding the Datadog extension.</p><ul><li>Region: I'm using the region my Lambda function is hosted in</li><li>ARM/x64: I'm picking the chip architecture that my Lambda Function is compiled for.\n</li><li>Version: 68 in this case, but  can also be used.\n</li></ul><div><pre><code></code></pre></div><p>I'm going to use a long-lived API key with Momento so that this Lambda Function can make requests without worrying about expiration.  This is completely acceptable solution.  Think of it like a scoped API key essentially.  To set that up, I'm using AWS SecretsManager.</p><div><pre><code></code></pre></div><h5>\n  \n  \n  Cargo Lambda Rust Function\n</h5><p>Wrapping up the infrastructure components is the definition of the Rust Lambda Function and granting its ability to read from the secret defined above in SecretsManager.  Additionally, I'm exposing the function over a FunctionURL.  This of course could be internal behind an Application Load Balancer or exposed behind a variety of API Gateway setups.  The FunctionURL just makes this example simple to pull together.</p><p>Key things to point out in the  are:</p><ul><li>Architecture: set to ARM because I prefer to run on the AWS Graviton chips</li><li>Environment: \n\n<ul><li>Setting RUST_LOG allows me to control crate log levels (this is a convention)\n</li></ul></li></ul><div><pre><code></code></pre></div><p>At this point, using CDK, I can easily run a  and my code will be live in AWS in just a couple of minutes.  However, I'd like to dive in further on the Rust and Lambda code, specifically addressing the Momento Auth pieces</p><p>All Rust code (unless it's a lib) starts out with a  function.  Even Lambda Functions must have a .  In my  below, I'm setting up Momento, Datadog, OpenTelemetry, and other reusable components.  Since my handler is what is called over and over, I want to have things warm and in memory, ready to use as events come in.</p><p>To initialize the OpenTelemetry, I'm establishing a telemetry layer which I'm registering.</p><div><pre><code></code></pre></div><p>The next pieces of  are about fetching the Momento API key from the AWS secret I defined in the infrastructure.  And with that secret, I'll construct an instance of the Momento Auth client so that I can communicate with the Auth API and create the disposable tokens.</p><div><pre><code></code></pre></div><p>With all Lambda Functions, I need to define a function that will be called when the Lambda Function is supplied events.  For web APIs, that event is a request from an external client.   My  establishes this connection by the following code.</p><div><pre><code></code></pre></div><p>As exposed, I need to send a Momento client, the expiration in minutes I want to let the token be valid for, and the event which is the web request.</p><div><pre><code></code></pre></div><p>The Lambda Function handler does the following.</p><ul><li>Parse the body of the request \n\n<ul><li>Body in the correct format then generate the token</li><li>If not, return a 400 BAD REQUEST</li></ul></li></ul><p>For the request body, I'm expecting it to look like this.</p><div><pre><code></code></pre></div><p>The Rust structure that this serializes into has the following definition.</p><div><pre><code></code></pre></div><p>Now with a struct populated with my request data, I can look at how to generate the disposable token.  It's much easier than I thought it might be.</p><h5>\n  \n  \n  Generating the Disposable Token\n</h5><p>This disposable token logic is the heart of this Lambda Function's existence.  Remember, Client code or the UI is going to request a token that I want to scope down to the cache and topic supplied in the payload.  This will guarantee that the client has access to what's needed for the duration defined the environment variable discussed above.</p><div><pre><code></code></pre></div><p>Let's break the above down just a little.  First up is the  and .</p><p>The  plugs into OpenTelemetry that allows me to time the Momento operations by way of the Rust Instrument trait.  I highly recommend any Rust code you write take advantage of these opportunities.  Tracing in the spirit of observability will make finding errors and poor user experiences so much easier when you start to get some volume.</p><div><pre><code></code></pre></div><p>The next piece of this function is to create the Disposable token.   are a required parameter to the <code>generate_disposable_token</code> function.  For my example, I'm giving the token access to Publish and Subscribe to the Cache/Topic combination.  And notice that the  parameter is finally being used to round out the function call.</p><div><pre><code></code></pre></div><p>The last piece of the function is to create the .  The values returned from the Momento function call are used to populate the struct.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Measuring Performance with Datadog and OpenTelemetry\n</h3><p>So I can't end an article just demonstrating how to fetch disposable tokens written in Rust and deployed in a Lambda Function without talking about performance.  I am always blown away at the speed of Momento's services.  I hadn't done much work with the Auth API so I wanted to see if the timings that I've been accustomed to with Cache and Topics also held true with Auth.  </p><p>With the observability code using OpenTelemetry that I've shown above, I'm able to not only track the Lambda Function's execution timings, but also the Momento specific API calls via the  trait that I showed above.  I bring this metrics and traces together via Datadog because there isn't a better tool on the market to help me observe my Lambda Functions as well as other cloud resources.</p><h5>\n  \n  \n  High Level Function Latency\n</h5><p>First up is looking at the high level Lambda Function latency. I'm graphing the 50th, 75th, 90th, and 95th percentiles with this Datadog line graph.</p><p>I've <a href=\"https://binaryheap.com/rust-and-lambda-performance/\" rel=\"noopener noreferrer\">written about Rust and Lambda performance</a> quite a bit over the past 18 months, but I'm always amazed at how quickly and consistently my function code performs with Rust. I can also make the <a href=\"https://binaryheap.com/caching-with-momento-and-rust/\" rel=\"noopener noreferrer\">same statements</a> when it comes to pairing Rust with Momento.  Time and time again, their platform performs consistently, regardless of the load and requests I throw at it.  The same can be said about the Auth API that I'm exercising here.  Consistent p95 latency at the 3ms is just fantastic and not going to be noticeable by an end user.</p><p>High level tracing is great and something that I love about using Datadog, but since I took advantage of the  trait further up, let's have a look at exactly how the Momento Auth operations play into the overall function latency.</p><p>This table shows the two spans that are included in the overall latency of the Lambda Functions execution.  If you remember from the code well above, I called the Momento Auth span .  I'm happy all day long with an average latency of 1.25ms and a tail p99 latency of 2.19ms.  I can't recommend their <a href=\"https://docs.momentohq.com/platform/sdks/rust\" rel=\"noopener noreferrer\">Rust SDK</a> enough.  It is my first and preferred way to work with Momento.</p><p>Working with client code that is insecure by nature that also needs to authenticate with the Momento API for things like Topic subscriptions can be a challenge.  However, by implementing a token vending machine with Rust, deployed with Lambda, and monitored with Datadog produces a solution that is fast, reliable, and observable.  </p><p>I've been saying this for a while, but I truly believe that building Lambda Functions with Rust is the way to go.  And I love seeing companies like Momento invest in Rust specific SDKs.  This feature to build disposable tokens was just added in 2025 and will unlock developers to implement this vending machine pattern in Rust like I've shown the article.  </p><p>Thanks for reading and happy building!</p>","contentLength":10231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Upskilling to Rust from Python","url":"https://dev.to/rustds/upskilling-to-rust-from-python-mok","date":1739550595,"author":"Michael Gonzalez","guid":549,"unread":true,"content":"<p>I’m a long-time data analyst with some data engineering experience. My overall stack time is about:</p><ul><li>Tableau Desktop – 10 years</li></ul><p>A few years ago, I embarked on a mission to be full stack. I picked up Python and learned how to do ETL operations on my own. I’ve heard great things about the speed and efficiency benefits of Rust over Python, so I wanted to put it to the test.</p><p>The basic question I have is: what is the absolute furthest that I can take Rust in the direction of data science (to include AI applications) and data engineering? The library environment for data science is extremely robust for Python and I know it’s substantially smaller for Rust, but I’m still interested in taking it as far as I can. Over the next few months, I’m going to be upskilling in Rust and seeing where Rust for DS goes.</p><p>I’m very aware of structural differences of Rust compared to Python. Memory management and static typing are elements that I’ve never had to care about, but it’s going to need to be a consideration moving forward. Ultimately, I think giving Rust a try will make me a more consciencious developer in that there are a lot of things I just took for granted with Python and SQL.</p><p>I know absolutely no Rust right now, so I’m leaning on Duke University’s Rust Fundamentals course through Coursera to get me up to speed on the basics.</p><p>The first week of the course revolved around setting up the VS Code environment for Rust, deploying GitHub Copilot, setting up the dev container, and dockerizing it.</p><p>I’m very familiar with VS Code for local development, but actually using Docker and working in a containerized environment is new. I’ve been wanting to containerize my org’s Python development environment for at least a year (it’s further down the to-do list), so spending some time on this is gonna pay off later on.</p><p>No actual Rust development work yet. Looking ahead, it appears that week 2 of the course jumps right into for and while loops. Ech, not a favorite topic of mine, but essential nonetheless.</p><p>Anyone else using Rust for data operations?</p>","contentLength":2073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unlocking global AI potential with next-generation subsea infrastructure","url":"https://engineering.fb.com/2025/02/14/connectivity/project-waterworth-ai-subsea-infrastructure/","date":1739550486,"author":"","guid":300,"unread":true,"content":"<p><a href=\"https://globaldigitalinclusion.org/wp-content/uploads/2024/01/GDIP-Good-Practices-for-Subsea-Cables-Policy-Investing-in-Digital-Inclusion.pdf\"></a></p><p><a href=\"https://engineering.fb.com/2021/03/28/connectivity/echo-bifrost/\"></a><a href=\"https://engineering.fb.com/2021/09/28/connectivity/2africa-pearls/\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LangCompiler: A Cloud-Native, High-Performance Code Execution Service","url":"https://dev.to/bladearya/langcompiler-a-cloud-native-high-performance-code-execution-service-5a2i","date":1739549239,"author":"Amit Kumar Rout","guid":571,"unread":true,"content":"<p>Smooth, secure, and scalable execution of code is of paramount concern in modern software development for a variety of applications, from education systems to enterprise applications. LangCompiler has been created to cater to this demand by providing a cloud-native, high-performance environment for code execution across a variety of programming languages.</p><p>LangCompiler is an elegantly crafted service that provides secure and scalable code execution across a wide spectrum of programming languages. It currently supports Python, Java, JavaScript, and C++, with further expansion planned in response to community requirements. Its versatility makes it a developer, educator, and enterprise favorite.</p><ul><li><p> The service provides code execution across four popular languages (Python, Java, JavaScript, and C++), with further expansion planned.</p></li><li><p><strong>Dynamic Resource Constraints:</strong> It is scalable, managing workloads efficiently through resource-aware execution.</p></li><li><p> The service includes extensive logging and debugging to guarantee seamless execution.</p></li><li><p><strong>Comprehensive Execution Metrics:</strong> Users can monitor performance, detect inefficiencies, and optimize workloads.</p></li><li><p><strong>Secure and Isolated Execution:</strong> It includes security features to prevent unauthorized access and code protection.</p></li></ul><h2>\n  \n  \n  Built with Golang and Docker\n</h2><p>To ensure high performance and reliability, LangCompiler is built using Golang, leveraging its efficiency and concurrency features. Additionally, it is Dockerized to provide an isolated execution environment, ensuring consistent and secure execution across different workloads. The service is hosted on Render, allowing seamless deployment, scaling, and management.</p><p>We invite developers, educators, and enterprises to explore LangCompiler and take advantage of its robust capabilities. To further enhance API development, we encourage you to check out our premium plans.</p>","contentLength":1859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Publish Interactive Data Visualizations for Free with Python and Marimo","url":"https://towardsdatascience.com/publish-interactive-data-visualizations-for-free-with-python-and-marimo/","date":1739548800,"author":"Sam Minot","guid":8,"unread":true,"content":"<p>Working in <a href=\"https://towardsdatascience.com/tag/data-science/\" title=\"Data Science\">Data Science</a>, it can be hard to share insights from complex datasets using only static figures. All the facets that describe the shape and meaning of interesting data are not always captured in a handful of pre-generated figures. While we have powerful technologies available for presenting interactive figures — where a viewer can rotate, filter, zoom, and generally explore complex data  —  they always come with tradeoffs.</p><p>Here I present my experience using a recently released Python library — <a href=\"https://marimo.io\">marimo</a> — which opens up exciting new opportunities for publishing interactive visualizations across the entire field of data science.</p><h2>Interactive Data Visualization</h2><p>The tradeoffs to consider when selecting an approach for presenting data visualizations can be broken into three categories:</p><ul><li> — what visualizations and interactivity am I able to present to the user?</li><li> — what are the resources needed for displaying this visualization to users (e.g. running servers, hosting websites)?</li><li> – how much of a new skillset / codebase do I need to learn upfront?</li></ul><p> is the foundation of portable interactivity. Every user has a web browser installed on their computer and there are many different frameworks available for displaying any degree of interactivity or visualization you might imagine (for example, this <a href=\"https://threejs.org/\">gallery of amazing things people have made with three.js</a>). Since the application is running on the user’s computer, no costly servers are needed. However, a significant drawback for the data science community is ease of use, as JS does not have many of the high-level (i.e. easy-to-use) libraries that data scientists use for data manipulation, plotting, and interactivity.</p><p> provides a useful point of comparison. Because of its <a href=\"https://flatironschool.com/blog/python-popularity-the-rise-of-a-global-programming-language/\">continually growing popularity</a>, some have called this the <a href=\"https://towardsdatascience.com/we-are-living-in-the-era-of-python-bc032d595f6a\">“Era of Python”</a>. For data scientists in particular, Python stands alongside R as one of the foundational languages for quickly and effectively wielding complex data. While Python may be easier to use than Javascript, there are fewer options for presenting interactive visualizations. Some popular projects providing interactivity and visualization have been <a href=\"https://flask.palletsprojects.com/en/stable/\">Flask</a>, <a href=\"https://dash.plotly.com/\">Dash</a>, and <a href=\"https://streamlit.io/\">Streamlit</a> (also worth mentioning — <a href=\"https://docs.bokeh.org/en/latest/docs/gallery.html\">bokeh</a>, <a href=\"https://holoviews.org/\">HoloViews</a>, <a href=\"https://altair-viz.github.io/altair-tutorial/README.html\">altair</a>, and <a href=\"https://plotly.com/python/\">plotly</a>). The biggest tradeoff for using Python has been the cost for publishing – delivering the tool to users. In the same way that <a href=\"https://www.shinyapps.io/\">shinyapps</a> require a running computer to serve up the visualization, these Python-based frameworks have exclusively been server-based. This is by no means prohibitive for authors with a budget to spend, but it does limit the number of users who can take advantage of a particular project.</p><p><a href=\"https://pyodide.org/en/stable/\"></a> is an intriguing middle ground — Python code running directly in the web browser using <a href=\"https://webassembly.org/\">WebAssembly</a> (WASM). There are resource limitations (only 1 thread and 2GB memory) that make this impractical for doing the heavy lifting of data science. , this can be more than sufficient for building visualizations and updating based on user input. Because it runs in the browser, no servers are required for hosting. Tools that use Pyodide as a foundation are interesting to explore because they give data scientists an opportunity to write Python code which runs directly on users’ computers without their having to install or run anything outside of the web browser.</p><p>As an aside, <a href=\"https://towardsdatascience.com/python-based-data-viz-with-no-installation-required-aaf2358c881\">I’ve been interested previously in</a> one project that has tried this approach: <a href=\"https://github.com/whitphx/stlite\">stlite</a>, <a href=\"https://edit.share.stlite.net/\">an in-browser implementation of Streamlit</a> that lets you deploy these flexible and powerful apps to a broad range of users. However, a core limitation is that Streamlit itself is distinct from stlite (the port of Streamlit to WASM), which means that not all features are supported and that advancement of the project is dependent on two separate groups working along compatible lines.</p><ul><li>The interface resembles a Jupyter , which will be familiar to users.</li><li>Execution of cells is , so that updating one cell will rerun all cells which depend on its output.</li><li> can be captured with a flexible set of UI components.</li><li>Notebooks can be quickly converted into , hiding the code and showing only the input/output elements.</li><li>Apps can be run locally or converted into using WASM/Pyodide.</li></ul><p>marimo balances the tradeoffs of technology in a way that is well suited to the skill set of the typical data scientists:</p><ul><li> — user input and visual display features are rather extensive, <a href=\"https://docs.marimo.io/guides/working_with_data/plotting/#reactive-plots\">supporting user input</a> via Altair and Plotly plots.</li><li> — deploying as static webpages is basically free — no servers required</li><li> — for users familiar with Python notebooks, marimo will feel very familiar and be easy to pick up.</li></ul><h2>Publishing Marimo Apps on the Web</h2><p>As a simple example of the type of display that can be useful in data science, consisting of explanatory text interspersed with interactive displays, I have created a barebones <a href=\"https://github.com/FredHutch/marimo-publication\">GitHub repository</a>. Try it out yourself <a href=\"https://fredhutch.github.io/marimo-publication/\">here</a>.</p><p>Using just a little bit of code, users can:</p><ul><li>Generate visualizations with flexible interactivity</li><li>Write narrative text describing their findings</li><li>Publish to the web for free (i.e. using GitHub Pages)</li></ul><h2>Public App / Private Data</h2><p>This new technology offers an exciting new opportunity for collaboration — publish the app publicly to the world, but users can only see specific datasets that they have permission to access.</p><p>Rather than building a dedicated data backend for every app, user data can be stored in a generic backend which can be securely authenticated and accessed using a Python client library — all contained within the user’s web browser. For example, the user is given an OAuth login link that will authenticate them with the backend and allow the app to temporarily access input data.</p><p>As a proof of concept, I built a simple visualization app which connects to <a href=\"https://cirro.bio\">the Cirro data platform</a>, which is used at my institution to manage scientific data. Full disclosure: I was part of the team that built this platform before it spun out as an independent company. In this manner users can:</p><ul><li>Load the public visualization app — hosted on GitHub Pages</li><li>Connect securely to their private data store</li><li>Load the appropriate dataset for display</li><li>Share a link which will direct authorized collaborators to the same data</li></ul><p>As a data scientist, this approach of publishing free and open-source visualization apps which can be used to interact with private datasets is extremely exciting. Building and publishing a new app can take hours and days instead of weeks and years, letting researchers quickly share their insights with collaborators and then publish them to the wider world.</p>","contentLength":6594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 22 Sites To Buy Verified Binance Accounts personal","url":"https://dev.to/hiya_tili_6b6c8383c5ae8c4/top-22-sites-to-buy-verified-binance-accounts-personal-41fk","date":1739546484,"author":"Hiya Tili","guid":570,"unread":true,"content":"<p><strong>If you want to more information just contact now.\nTelegram: @itzonesmm0\nWhatsApp: +1 (989) 513-2521<a href=\"mailto:liveitzonesmm@gmail.com\">liveitzonesmm@gmail.com</a></strong></p><p><strong>Buy Verified Binance Account</strong>\nLooking to <a href=\"https://itzonesmm.com/product/buy-verified-binance-accounts/\" rel=\"noopener noreferrer\">Buy Verified Binance Account</a>? Purchase one for instant access to trading features, securely. If you want to Buy Verified Binance Account then itzonesmm.com is the best option for you.</p><p>In today’s fast-paced digital world, online trading platforms like Binance have become increasingly popular among individuals looking to invest in cryptocurrencies. To ensure a smooth entry into the world of crypto trading, having a verified Binance account is essential. By purchasing a verified account, you can bypass the lengthy verification process and start trading immediately.</p><p><strong>Buy Verified Binance Account\nBuy Verified Binance Account</strong>\nThis hassle-free option saves you time and allows you to take advantage of market opportunities swiftly. Additionally, a verified account provides added security and access to advanced trading features. Ready to kickstart your crypto journey? Invest in a verified Binance account today and stay ahead in the ever-evolving crypto market.</p><p><strong>The Importance Of A Verified Binance Account</strong>\nBuying and selling cryptocurrencies through a reputable platform like Binance is a popular practice. However, having a verified Binance account is of utmost importance to ensure a secure and advanced trading experience. In this section, we’ll delve into the significance of a verified Binance account and the benefits it offers.</p><p><strong>Enhanced Security Measures</strong>\nWhen you have a verified Binance account, you can benefit from enhanced security measures that add an extra layer of protection to your account. Two-factor authentication and withdrawal whitelisting are just a few examples of the security features available to verified users. This significantly reduces the risk of unauthorized access to your account and provides peace of mind when engaging in cryptocurrency transactions.</p><p><strong>Access To Advanced Trading Features</strong>\nA verified Binance account grants access to advanced trading features that are not available to unverified users. These features include higher withdrawal limits, access to futures and margin trading, and participation in token sales. By verifying your account, you can take advantage of these advanced tools and broaden your trading opportunities, allowing for a more diverse and potentially profitable trading experience.</p><p><strong>The Process Of Verifying A Binance Account</strong>\nBuying a verified Binance account can be a time-saving option for cryptocurrency enthusiasts. It eliminates the need to go through the rigorous process of account verification. However, if you choose to verify your own Binance account, here is a breakdown of the steps involved.</p><p>\nTo start the process, log in to your <a href=\"https://itzonesmm.com/product/buy-verified-binance-accounts/\" rel=\"noopener noreferrer\">Binance account</a> and navigate to the user dashboard. Choose the option for account verification and select the type of verification you wish to undergo. You will be asked to provide personal details such as your full name, address, and date of birth. Next, upload a scanned copy of your government-issued ID, such as a driver’s license or passport. Make sure all details are accurate and match the information on the ID provided.</p><p><strong>Additional Verification Steps</strong>\nAfter submitting your identity verification, Binance may require additional steps to complete the process. This could include providing proof of address, such as a utility bill or bank statement. You may also need to go through facial recognition verification to ensure the account belongs to the rightful owner. Once all steps are completed, await approval from Binance, which typically takes a few business days.</p><p><strong>Benefits Of Buying A Verified Binance Account</strong>\nIf you are an aspiring cryptocurrency trader, you know how important it is to have a verified Binance account. However, going through the tedious verification process can be a hassle and time-consuming. That’s why buying a verified Binance account can offer you several benefits, allowing you to focus on what matters most – trading.</p><p><strong>If you want to more information just contact now.\nTelegram: @itzonesmm0\nWhatsApp: +1 (989) 513-2521<a href=\"mailto:liveitzonesmm@gmail.com\">liveitzonesmm@gmail.com</a></strong></p><p><strong>Instant Access To Trading</strong>\nWhen you purchase a verified Binance account, you gain immediate access to the platform’s trading services. Unlike the traditional account creation and verification method that can take days or even weeks, buying a verified account cuts the waiting time significantly. This means you can start trading and capitalizing on potential market opportunities without delay. </p><p><strong>Avoiding Verification Hassles</strong>\nVerification processes can be cumbersome and time-consuming. From providing personal identification documents to answering questionnaires, the process can sometimes feel intrusive. By purchasing a verified Binance account, you eliminate the need to go through this whole process. You can skip the hassle and start trading right away. This is especially beneficial if you are looking to capitalize on time-sensitive market movements or seize profitable trade opportunities.</p><p>Furthermore, buying a verified account saves you from the frustration of potential verification failures. Sometimes, applicants face rejection due to minor errors or document discrepancies, resulting in extended waiting times. By bypassing the verification process, you eliminate the risk of encountering such setbacks and can focus solely on your trading activities.</p><p>With the benefits of instant access to trading and avoiding verification hassles, buying a verified Binance account provides a convenient and efficient solution for crypto enthusiasts.</p><p>\nWhen considering the purchase of a verified Binance account, it’s essential to understand the potential risks and regulatory considerations involved. This will ensure that you are making an informed decision and taking into account the potential pitfalls that may arise.</p><p>\nUnfortunately, the crypto industry is not immune to scams, and purchasing a verified Binance account comes with its own set of risks. There is the potential for scammers to falsely advertise verified accounts, leading to financial loss and account security issues. It’s crucial to thoroughly research the seller and verify their legitimacy to avoid falling victim to fraudulent activity.</p><p>\nAdditionally, regulatory compliance is a key consideration when purchasing a <a href=\"https://itzonesmm.com/product/buy-verified-binance-accounts/\" rel=\"noopener noreferrer\">verified Binance account</a>. It’s important to ensure that the account has been obtained and verified through legal and compliant means. Failure to comply with regulatory requirements could result in account suspension or legal repercussions, making it essential to verify the authenticity of the account and its compliance with relevant regulations.</p><p><strong>Choosing A Reliable Service Provider</strong>\nWhen finding a service provider to Buy Verified Binance Account, reliability is crucial. Look for reputable sellers with positive customer reviews and a track record of delivering quality, verified accounts. Taking the time to research and choose a reliable service provider can ensure a smooth and secure experience.</p><p><strong>Reputation And Trustworthiness</strong>\nReputable Binance account service providers like itzonesmm have a solid track record and are known for their trustworthiness.</p><p>Look for providers with a strong reputation in the industry to ensure security and reliability. Then itzonesmm.com is the salutation for you.</p><p><strong>Customer Reviews And Feedback</strong>\nReading customer reviews and feedback can provide insight into the service quality offered by itzonesmm.</p><p>Positive reviews are a good indicator that the service is reliable and trustworthy.</p><p><strong>Legal And Ethical Implications</strong>\nThe purchase and use of a verified Binance account raise legal and ethical implications that need to be considered. Compliance with anti-money laundering regulations, data privacy, and identity verification are crucial to ensure a secure and transparent trading environment.</p><p><strong>The Future Of Binance Account Verification</strong>\nAs the cryptocurrency market continues to expand, so does the need for robust security measures. In this context, the verification process for Binance accounts is evolving rapidly. This not only impacts the trading environment but also sets the stage for a more secure and reliable platform for traders.</p><p><strong>Evolution Of Verification Procedures</strong>\nThe evolution of Binance account verification procedures reflects the platform’s commitment to heightened security. The initial verification process mainly focused on email verification and two-factor authentication. However, as the market dynamics and regulatory requirements evolved, Binance introduced enhanced verification levels. This includes personal identification verification, know-your-customer (KYC) procedures, and even address verification. These steps are crucial in protecting user assets and ensuring compliance with industry regulations.</p><p><strong>Impact On Trading Environment</strong>\nThe impact of these evolving verification procedures is significant for the trading environment. With stricter verification measures in place, Binance creates a safer space for traders to engage in cryptocurrency transactions. Traders can have greater trust in the platform, knowing that stringent verification processes are in place to deter fraudulent activities. Additionally, enhanced verification procedures also pave the way for a more seamless integration with traditional financial systems, making crypto trading more accessible and widely accepted.</p><p><strong>Market Trends And User Experiences</strong>\nVerified Binance accounts are becoming increasingly popular due to the heightened security they offer to users.</p><p>\nUsers report high levels of satisfaction with verified Binance accounts</p><p><strong>Trends In Verified Account Ownership</strong>\nOwnership of verified Binance accounts is on the rise as more users seek secure trading options.</p><p><strong>Faqs About Verified Binance Accounts</strong>\nAs more traders and investors flock to the world of cryptocurrency, platforms like Binance have gained immense popularity. One way to enhance your experience on Binance is by getting a verified account. In this section, we’ll address some frequently asked questions about verified Binance accounts and provide you with the information you need to make an informed decision.</p><p>\nIf you’re considering getting a verified <a href=\"https://itzonesmm.com/product/buy-verified-binance-accounts/\" rel=\"noopener noreferrer\">Binance account</a>, it’s natural to have questions about security. Binance takes security seriously and implements several measures to protect its users. One of these measures is the Know Your Customer (KYC) process, which requires users to verify their identity and provide relevant documents.</p><p>With a verified account, you can have peace of mind knowing that your funds and personal information are better protected. The verification process helps ensure that only legitimate users gain access to the platform, reducing the risk of fraudulent activities.</p><p>Moreover, Binance employs industry-standard security protocols, such as two-factor authentication (2FA) and encryption, to safeguard user accounts. These additional layers of security give you an added level of confidence when trading or storing your cryptocurrencies on the platform.</p><p><strong>Transferability Of Verified Status</strong>\nOnce you acquire a verified Binance account, you might wonder if the verified status is transferable. The short answer is no. Verification is linked to the individual user account and cannot be transferred to another user.</p><p>This means that if you’re looking to purchase a verified Binance account from someone else, it’s important to tread with caution. The selling or transferring of verified Binance accounts is strictly against Binance’s terms of service, and engaging in such activities could result in your account being permanently suspended.</p><p>It’s important to note that Binance’s strict stance on verification transferability is in place to protect users’ security and prevent fraudulent activities. By ensuring that each user goes through the verification process individually, Binance maintains a safer trading environment for all its users.</p><p>while you cannot transfer a verified Binance account, you can easily go through the verification process on your own to enjoy the benefits and added security that come with it.</p><p><strong>Conclusion And Final Thoughts</strong>\nConsidering the pros and cons, buying a verified Binance account can offer a range of benefits for traders. It provides a streamlined process for account verification, reducing the waiting time and frustrations associated with manual verification. However, it’s important to weigh these advantages against potential risks and drawbacks. By empowering trading decisions, a verified Binance account allows traders to quickly delve into the world of cryptocurrency and start trading. Whether you’re a beginner or an experienced trader, having a verified Binance account can give you a competitive edge in the market.</p><p>Weighing The Pros And Cons\nBefore purchasing a verified Binance account, it is essential to evaluate the pros and cons. Let’s take a closer look at what you should consider:</p><p>Streamlined verification process: By buying a verified Binance account, you can skip the hassle of manual verification, significantly reducing the waiting time and potential frustrations.\nImmediate access to trading: With a verified Binance account, you can start trading cryptocurrencies right away, without any delays.<p>\nEnhanced security measures: Verified Binance accounts typically come with advanced security features, such as two-factor authentication, providing an added layer of protection for your funds.</p>\nExpert support: Some providers of verified Binance accounts offer dedicated customer support to assist you with any inquiries or issues you may encounter.</p><p>Reliance on a third party: When purchasing a verified Binance account, you need to trust the provider to deliver a legitimate and secure account.\nPotential risks: As with any online financial transaction, there is always a risk of encountering fraudulent or scammy providers. It’s crucial to thoroughly research and choose a reputable provider with positive reviews.<a href=\"https://itzonesmm.com/product/buy-verified-binance-accounts/\" rel=\"noopener noreferrer\">Verified Binance accounts</a> may come with a price tag, and it’s important to evaluate the cost versus the value it adds to your trading experience.\nLimited personalization: When buying a verified Binance account, you may not have the option to customize certain account settings or preferences according to your specific needs.<p>\nEmpowering Trading Decisions</p>\nA verified Binance account can empower your trading decisions by providing quick and efficient access to the cryptocurrency market. The streamlined verification process eliminates the frustration of waiting for manual verification, enabling you to take advantage of market opportunities swiftly. Whether you’re a day trader, swing trader, or long-term investor, having a verified Binance account equips you with the necessary tools to make informed and impactful trading decisions.</p><p>while buying a verified Binance account comes with its pros and cons, it definitely offers advantages for those looking to expedite the account verification process and start trading cryptocurrencies promptly. However, one must exercise caution when choosing a provider and thoroughly evaluate the potential risks involved.</p><p>Frequently Asked Questions</p><p><strong>How To Get Verified In Binance?</strong>\nTo get verified in Binance, go to your account settings, complete identity verification, submit required documents, and wait for approval. </p><p><strong>Can I Have Two Verified Binance Accounts?</strong>\nNo, Binance allows only one verified account per user. Creating multiple accounts violates their terms of service.</p><p><strong>What Is A Binance Account?</strong>\nA Binance account is a digital wallet for trading cryptocurrencies like Bitcoin and Ethereum. It allows users to buy, sell, and store various digital assets. With a Binance account, you can participate in the cryptocurrency market and manage your investments securely.</p><p><strong>How To Verify Entity Account Binance?</strong>\nTo verify entity accounts on Binance, submit required documents via the website for verification process.</p><p><strong>Can I Buy Verified Binance Account Online?</strong>\nYes, you can Buy Verified Binance Account online from itzonesmm.com </p><p>What Are The Benefits Of Buying A Verified Binance Account?\nBuying a verified Binance account saves time and effort required for the verification process, ensuring immediate access to the platform’s features and services.</p><p><strong>Is It Legal To Buy Verified Binance Account?</strong>\nWhile buying a verified Binance account is not illegal, it is against Binance’s terms and conditions, which may result in the account being banned or frozen.</p><p><strong>Are Verified Binance Accounts Safe To Use?</strong>\nVerified Binance accounts are generally safe to use, but it is essential to exercise caution and follow Binance’s security guidelines to protect your account from unauthorized access or hacking attempts.</p><p>How Do I Find Reputable Sellers Of Verified Binance Accounts?\nTo find reputable sellers of verified Binance accounts, you can rely on online marketplaces, forums, or seek recommendations from trusted sources within the cryptocurrency community.</p><p>\nInvest in a verified Binance account to enhance your trading journey. Take advantage of security and convenience. Maximize your opportunities and streamline transactions. Don’t miss out on the benefits of a trusted account. Start your journey to success with Binance today!</p><p><strong>If you want to more information just contact now.\nTelegram: @itzonesmm0\nWhatsApp: +1 (989) 513-2521<a href=\"mailto:liveitzonesmm@gmail.com\">liveitzonesmm@gmail.com</a></strong></p>","contentLength":17340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hugo van Kemenade: Improving licence metadata","url":"https://hugovk.dev/blog/2025/improving-licence-metadata/","date":1739545860,"author":"","guid":167,"unread":true,"content":"<p><a href=\"https://peps.python.org/pep-0639/\" target=\"_blank\" rel=\"noreferrer\">PEP 639</a> defines a spec on how to document licences\nused in Python projects.</p><p>Change  as follows.</p><p>I usually use Hatchling as a build backend, and support was added in 1.27:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Replace the freeform  field with a valid SPDX license expression, and add\n which points to the licence files in the repo. There’s often only one,\nbut if you have more than one, list them all:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Optionally delete the deprecated licence classifier:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>Then make sure to use a PyPI uploader that supports this.</p><p>pip can also show you the metadata:</p><div><pre tabindex=\"0\"><code></code></pre></div><p>A lot of work went into this. Thank you to PEP authors\n<a href=\"https://github.com/pombredanne\" target=\"_blank\" rel=\"noreferrer\">Philippe Ombredanne</a> for creating the first draft in\n2019, to <a href=\"https://github.com/cam-gerlach\" target=\"_blank\" rel=\"noreferrer\">C.A.M. Gerlach</a> for the second draft in 2021,\nand especially to <a href=\"https://karolinasurma.eu/\" target=\"_blank\" rel=\"noreferrer\">Karolina Surma</a> for getting the third\ndraft finish line and helping with the implementation.</p><p>And many projects were updated to support this, thanks to the maintainers and\ncontributors of at least:</p>","contentLength":878,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"5 Tips for Building a Data Science Portfolio","url":"https://www.kdnuggets.com/5-tips-building-data-science-portfolio","date":1739545225,"author":"Nate Rosidi","guid":252,"unread":true,"content":"<article>Not every data science portfolio is worth showcasing. Follow these five tips to build a portfolio that impresses employers and gets you a job.</article>","contentLength":142,"flags":null,"enclosureUrl":"https://www.kdnuggets.com/wp-content/uploads/Rosidi_5_Tips_for_Building_a_DS_Portfolio_4.png","enclosureMime":"","commentsUrl":null},{"title":"Deep Dive into net/netip AddrPort Methods 6/7","url":"https://dev.to/rezmoss/deep-dive-into-netnetip-addrport-methods-67-3gn3","date":1739545200,"author":"Rez Moss","guid":569,"unread":true,"content":"<p>Hey there! In our previous article, we explored Addr methods in detail. Now let's dive deep into AddrPort methods. AddrPort is a crucial type when working with network services since it combines an IP address with a port number. We'll explore every method with practical examples and real-world use cases.</p><p>First, let's look at all the ways to work with AddrPort.</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><h3>\n  \n  \n  1. Service Discovery System\n</h3><p>Here's a robust service discovery implementation using AddrPort:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  2. Connection Pool Manager\n</h3><p>A connection pool that uses AddrPort for endpoint tracking:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  3. Load Balancer Implementation\n</h3><p>A load balancer using AddrPort for backend management:</p><div><pre><code></code></pre></div><ol><li>\nAlways validate AddrPort before use:\n</li></ol><div><pre><code></code></pre></div><ol><li>\nBe careful with string conversions:\n</li></ol><div><pre><code></code></pre></div><ol><li>\nCheck port numbers when needed:\n</li></ol><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>In our next article, we'll explore Prefix methods in depth, completing our detailed examination of the core types in net/netip. We'll see how to work effectively with CIDR notations and subnet operations.</p><p>Until then, keep experimenting with AddrPort! It's a fundamental building block for network services in Go.</p>","contentLength":1071,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Transform your codebase into a single Markdown doc for feeding into AI","url":"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/index.html","date":1739539403,"author":"tesserato","guid":450,"unread":true,"content":"<p>CodeWeaver is a command-line tool designed to weave your codebase into a single, easy-to-navigate Markdown document. It recursively scans a directory, generating a structured representation of your project's file hierarchy and embedding the content of each file within code blocks. This tool simplifies codebase sharing, documentation, and integration with AI/ML code analysis tools by providing a consolidated and readable Markdown output.\nThe output for the current repository can be found <a href=\"https://github.com/tesserato/CodeWeaver/blob/main/codebase.md\">here</a>.</p><ul><li><strong>Comprehensive Codebase Documentation:</strong> Generates a Markdown file that meticulously outlines your project's directory and file structure in a clear, tree-like format.</li><li> Embeds the complete content of each file directly within the Markdown document, enclosed in syntax-highlighted code blocks based on file extensions.</li><li>  Utilize regular expressions to define ignore patterns, allowing you to exclude specific files and directories from the generated documentation (e.g., , build artifacts, specific file types).</li><li> Choose to save lists of included and excluded file paths to separate files for detailed tracking and debugging of your ignore rules.</li><li><strong>Simple Command-Line Interface:</strong>  Offers an intuitive command-line interface with straightforward options for customization.</li></ul><p>If you have Go installed, run <code>go install github.com/tesserato/CodeWeaver@latest</code>to install the latest version of CodeWeaver or <code>go install github.com/tesserato/CodeWeaver@vX.Y.Z</code> to install a specific version.</p><p>Alternatively, download the appropriate pre built executable from the <a href=\"https://github.com/tesserato/CodeWeaver/releases\">releases page</a>.</p><p>If necessary, make the  executable by using the  command:</p><table><thead><tr></tr></thead><tbody><tr><td>The root directory to scan and document.</td></tr><tr><td>The name of the output Markdown file.</td></tr><tr><td><code>-ignore \"&lt;regex patterns&gt;\"</code></td><td>Comma-separated list of regular expression patterns for paths to exclude.</td></tr><tr><td><code>-included-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were included in the documentation.</td></tr><tr><td><code>-excluded-paths-file &lt;filename&gt;</code></td><td>File to save the list of paths that were excluded from the documentation.</td></tr><tr><td>Display this help message and exit.</td></tr></tbody></table><h2><strong>Generate documentation for the current directory:</strong></h2><p>This will create a file named  in the current directory, documenting the structure and content of the current directory and its subdirectories (excluding paths matching the default ignore pattern ).</p><h2><strong>Specify a different input directory and output file:</strong></h2><pre><code>./codeweaver -dir=my_project -output=project_docs.md\n</code></pre><p>This command will process the  directory and save the documentation to .</p><h2><strong>Ignore specific file types and directories:</strong></h2><pre><code>./codeweaver -ignore=\"\\.log,temp,build\" -output=detailed_docs.md\n</code></pre><p>This example will generate , excluding any files or directories with names containing , , or . Regular expression patterns are comma-separated.</p><h2><strong>Save lists of included and excluded paths:</strong></h2><pre><code>./codeweaver -ignore=\"node_modules\" -included-paths-file=included.txt -excluded-paths-file=excluded.txt -output=code_overview.md\n</code></pre><p>This command will create  while also saving the list of included paths to  and the list of excluded paths (due to the  ignore pattern) to .</p><p>Contributions are welcome! If you encounter any issues, have suggestions for new features, or want to improve CodeWeaver, please feel free to open an issue or submit a pull request on the project's GitHub repository.</p><p>CodeWeaver is released under the <a href=\"https://tesserato.web.app/posts/2025-02-12-CodeWeaver-launch/LICENSE\">MIT License</a>. See the  file for complete license details.</p>","contentLength":3311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43048027"},{"title":"[Boost]","url":"https://dev.to/jisbruzzi/-2734","date":1739535786,"author":"José Ignacio Sbruzzi","guid":568,"unread":true,"content":"<h2>Advanced RabbitMQ and Go: Tackling Channel Closures in Exclusive Queues</h2>","contentLength":71,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: A New Way to Learn Languages","url":"https://www.langturbo.com/","date":1739534938,"author":"sebnun","guid":449,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43047554"},{"title":"The Real Python Podcast – Episode #239: Behavior-Driven vs Test-Driven Development & Using Regex in Python","url":"https://realpython.com/podcasts/rpp/239/","date":1739534400,"author":"Real Python","guid":97,"unread":true,"content":"<p>What is behavior-driven development, and how does it work alongside test-driven development? How do you communicate requirements between teams in an organization? Christopher Trudeau is back on the show this week, bringing another batch of PyCoder's Weekly articles and projects.</p>","contentLength":279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Daniel Roy Greenfeld: Building a playing card deck","url":"https://daniel.feldroy.com/posts/2025-02-deck-of-cards","date":1739526604,"author":"","guid":166,"unread":true,"content":"<article>Today is Valentine's Day. That makes it the perfect day to write a blog post about showing how to not just build a deck of cards, but show off cards from the heart suite.</article>","contentLength":170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"1BRC in PHP FFI + Rust","url":"https://dev.to/gfabrizi/1brc-in-php-ffi-rust-5ed9","date":1739524380,"author":"Gianluca Fabrizi","guid":548,"unread":true,"content":"<p>We have tried multi-threading in PHP to speed up execution time; the results are good, but far from perfect. Is there another way we can improve PHP's performance?</p><p>In the previous post, we gave an overview of 1BRC, tried to push the limits of PHP when discussing performance optimization, and ran our best PHP script on an EC2 instance.</p><p>The results were not bad, but not noteworthy either: 17.0636 seconds (the fastest Java code took 1.535 seconds).</p><p>So what are we supposed to do? Call it a day and get on with our lives? No, obviously not!\nWe could \"cheat\" our way to a better score, by abusing one of Python's winning strategies: letting external libraries do the heavy lifting job!</p><h2>\n  \n  \n  Foreign Function Interface\n</h2><p>One of the ways to optimize an interpreted language is by moving the slow operations in an external module, usually written in a low-level language.\nIn PHP you can write system-wide modules and enable them in ; this is useful for generic functions or for code that is not specific to one application.\nSince version 7.4 PHP introduced a new feature: Foreign Function Interface (FFI).<p>\nFFI is a method for calling external libraries in your PHP coding, without changing global PHP configuration.</p>\nThis method is more flexible than dealing with modules, but configuring it could be a bit daunting at first.</p><p>Let's try to wrap a Rust solution of 1BRC in a PHP script (yes, ok, we are definitely cheating).</p><p>To keep things simple we need a Rust solution that:</p><ol><li>it's written in a clear way</li><li>it's composed of a few files</li></ol><p>There's no need to explain point ; points  and  are needed because we are going to modify the code to make it work as a module.\nI love Rust, but I'm not a Rust programmer, so the simpler the code the better.  </p><p>First of all, we clone the repository, then we edit the  file to add some options:</p><div><pre><code></code></pre></div><p>In the  section, we enabled additional performance optimizations (, , ); we added the  section, where we specify that we want to compile the source as a  library (shared libraries that can be linked into external programs).  </p><p> file is used just to call ; we remove this file and add a  method in :</p><div><pre><code></code></pre></div><p> disables the mangle (in short: it keeps the function's name in the exported library) and marks this function as \"to export\".  </p><p>We are cheating, but in a responsible way 😅: from PHP code, we pass the weather data filename to the Rust module. Then the Rust module returns the station's aggregated data to be displayed.\nPHP is a loosely-typed language, while Rust is a strongly-typed language, so moving data between the two can be a bit of a challenge-in-the-challenge. We need  crate and  from .  </p><p>The code needed to convert from PHP String to Rust string slice has been taken from ; using it's words:</p><blockquote><p>Getting a Rust string slice (&amp;str) requires a few steps:</p><ol><li><p>We need to make sure that the C pointer is not  as Rust references are not allowed to be .</p></li><li><p>Use  to wrap the pointer.  will compute the string's length based on the terminating . This requires an  block as we will be dereferencing a raw pointer, which the Rust compiler cannot verify meets all the safety guarantees so the programmer must do it instead.</p></li><li><p>Ensure the C string is valid UTF-8 and convert it to a Rust string slice.</p></li></ol></blockquote><p>In  we use this code:</p><div><pre><code></code></pre></div><p>to return a JSON string to the PHP script.  </p><p>That's it for Rust; we can compile the library with:</p><p>On the PHP side first of all we need a class to manage the input and output of th Rust module. Let's create a file called :</p><div><pre><code></code></pre></div><p>The constructor's code uses  to import the Rust function from the  file.\nHere we have to declare the extern function's signature using C code, so the Rust  parameters, become .</p><p>: it's also possible to use a  header file to specify the function(s) that PHP needs to know about; since we only need one simple function, it is easier to declare it inline in PHP code.</p><p>The  method invokes the  method of the Rust module (<code>self::$ffi-&gt;run($filename)</code>). We called both this wrapper method and the Rust function with the same name (); this is only a coincidence (...or lack of fantasy); it's not mandatory. converts the pointer to a String usable in PHP.</p><p>We also need an  file to instantiate this  class and to print the results:</p><div><pre><code></code></pre></div><p>Nothing interesting here: we call our  method, passing it the measurements filename.\nThe JSON string from Rust contains temperatures as integers, so we need to divide them by 10 and calculate the average temperature for each station.</p><p>Let's run this code on the EC2 instance. The configuration is the same as last time: an  with 32 vCPUs and 128GB of memory. For the hard disk, I opted for a 200GB io1 volume (to reach 10,000 IOPS).</p><div><pre><code>perf  1B-ffi.log  10  php app/index.php\n</code></pre></div><p>and these are the results:</p><div><pre><code> Performance counter stats for 'php app/index.php' (10 runs):\n\n          58802.93 msec task-clock                       #   29.718 CPUs utilized            ( +-  0.26% )\n              4736      context-switches                 #   80.191 /sec                     ( +-  3.80% )\n                57      cpu-migrations                   #    0.965 /sec                     ( +- 13.37% )\n             52703      page-faults                      #  892.378 /sec                     ( +-  1.33% )\n   &lt;not supported&gt;      cycles                                                      \n   &lt;not supported&gt;      instructions                                                \n   &lt;not supported&gt;      branches                                                    \n   &lt;not supported&gt;      branch-misses                                               \n   &lt;not supported&gt;      L1-dcache-loads                                             \n   &lt;not supported&gt;      L1-dcache-load-misses                                       \n   &lt;not supported&gt;      LLC-loads                                                   \n   &lt;not supported&gt;      LLC-load-misses                                             \n\n            1.9787 +- 0.0197 seconds time elapsed  ( +-  1.00% )\n</code></pre></div><p>1.9787 seconds! 🥳 🎉<p>\nThis is a surprising result, considering the overhead of calling an external module and the fact that we are still making some calculations on the PHP side of the app.</p></p><p>After this 2-parts-journey we can affirm that:</p><ol><li>PHP is slow, but the performance improves significantly when using threads</li><li>Performance tuning is a game of trade-offs: you can improve the speed of a task by saturating all the CPU cores, but your system will become unresponsive. In PHP this is a problem if your application needs to accept more than one connection at a time</li><li>For heavy tasks, you can delegate to optimized external libraries</li></ol><p>I hope you enjoyed the post!</p>","contentLength":6534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}