{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":36,"items":[{"title":"Career transition in to Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/","date":1739626865,"author":"/u/Similar-Secretary-86","guid":646,"unread":true,"content":"<p>\"I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated </p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why OOP & FP are the Two Main Paradigms","url":"https://www.youtube.com/watch?v=l_3AGwVwP_k","date":1739624173,"author":"/u/OkMemeTranslator","guid":658,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iq0rlf/why_oop_fp_are_the_two_main_paradigms/"},{"title":"Chinese Vice Minister says China and the US must work together to control rogue AI: \"If not... I am afraid that the probability of the machine winning will be high.\"","url":"https://www.scmp.com/news/china/diplomacy/article/3298267/china-and-us-should-team-rein-risks-runaway-ai-former-diplomat-says","date":1739622429,"author":"/u/MetaKnowing","guid":651,"unread":true,"content":"<div datatype=\"p\" data-qa=\"Component-Component\">A former senior Chinese diplomat has called for China and the US to work together to head off the risks of rapid advances in <a target=\"_self\" href=\"https://www.scmp.com/topics/artificial-intelligence?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> (AI).</div><p datatype=\"p\" data-qa=\"Component-Component\">But the prospect of cooperation was bleak as geopolitical tensions rippled out through the technological landscape, former Chinese foreign vice-minister Fu Ying told a closed-door AI governing panel in Paris on Monday.</p><p datatype=\"p\" data-qa=\"Component-Component\">“Realistically, many are not optimistic about US-China AI collaboration, and the tech world is increasingly subject to geopolitical distractions,” Fu said.</p><p datatype=\"p\" data-qa=\"Component-Component\">“As long as China and the US can cooperate and work together, they can always find a way to control the machine. [Nevertheless], if the countries are incompatible with each other ... I am afraid that the probability of the machine winning will be high.”</p><div datatype=\"p\" data-qa=\"Component-Component\">The panel discussion is part of a two-day global <a target=\"_self\" href=\"https://www.scmp.com/news/world/europe/article/3297992/trumps-ai-ambition-and-chinas-deepseek-overshadow-major-ai-summit-paris?module=Europe&amp;pgtype=section?module=inline&amp;pgtype=article\" data-qa=\"BaseLink-renderAnchor-StyledAnchor\"></a> that started in Paris on Monday.</div><p datatype=\"p\" data-qa=\"Component-Component\">Other panel members included Yoshua Bengio, the Canadian computer scientist recognised as a pioneer in the field, and Alondra Nelson, a central AI policy adviser to former US president Joe Biden’s administration and the United Nations.</p>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iq0b4t/chinese_vice_minister_says_china_and_the_us_must/"},{"title":"Karol Herbst steps down as Nouveau maintainer due to “thin blue line comment”","url":"https://www.reddit.com/r/linux/comments/1iq09g6/karol_herbst_steps_down_as_nouveau_maintainer_due/","date":1739622250,"author":"/u/mdedetrich","guid":626,"unread":true,"content":"<p>\"I was pondering with myself for a while if I should just make it official that I'm not really involved in the kernel community anymore, neither as a reviewer, nor as a maintainer.</p><p>Most of the time I simply excused myself with \"if something urgent comes up, I can chime in and help out\". Lyude and Danilo are doing a wonderful job and I've put all my trust into them.</p><p>However, there is one thing I can't stand and it's hurting me the most. I'm convinced, no, my core believe is, that inclusivity and respect, working with others as equals, no power plays involved, is how we should work together within the Free and Open Source community.</p><p>I can understand maintainers needing to learn, being concerned on technical points. Everybody deserves the time to understand and learn. It is my true belief that most people are capable of change eventually. I truly believe this community can change from within, however this doesn't mean it's going to be a smooth process.</p><p>The moment I made up my mind about this was reading the following words written by a maintainer within the kernel community:</p><p>\"we are the thin blue line\"</p><p>This isn't okay. This isn't creating an inclusive environment. This isn't okay with the current political situation especially in the US. A maintainer speaking those words can't be kept. No matter how important or critical or relevant they are. They need to be removed until they learn. Learn what those words mean for a lot of marginalized people. Learn about what horrors it evokes in their minds.</p><p>I can't in good faith remain to be part of a project and its community where those words are tolerated. Those words are not technical, they are a political statement. Even if unintentionally, such words carry power, they carry meanings one needs to be aware of. They do cause an immense amount of harm.</p><p>I wish the best of luck for everybody to continue to try to work from within. You got my full support and I won't hold it against anybody trying to improve the community, it's a thankless job, it's a lot of work. People will continue to burn out.</p><p>I got burned out enough by myself caring about the bits I maintained, but eventually I had to realize my limits. The obligation I felt was eating me from inside. It stopped being fun at some point and I reached a point where I simply couldn't continue the work I was so motivated doing as I've did in the early days.</p><p>Please respect my wishes and put this statement as is into the tree. Leaving anything out destroys its entire meaning.</p>","contentLength":2492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[P] Daily ArXiv filtering powered by LLM judge","url":"https://www.reddit.com/r/MachineLearning/comments/1ipz934/p_daily_arxiv_filtering_powered_by_llm_judge/","date":1739618056,"author":"/u/MadEyeXZ","guid":653,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My new blog post comparing networking in EKS vs. GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/","date":1739617569,"author":"/u/jumiker","guid":647,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Richard Stallman in Polytechnic University of Turin, Italy","url":"https://www.reddit.com/r/linux/comments/1ipz4wy/richard_stallman_in_polytechnic_university_of/","date":1739617538,"author":"/u/ShockleyTransistor","guid":630,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go Nullable with Generics v2.0.0 - now supports omitzero","url":"https://github.com/LukaGiorgadze/gonull","date":1739617221,"author":"/u/Money-Relative-1184","guid":632,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ipz22f/go_nullable_with_generics_v200_now_supports/"},{"title":"Deep Dive into VPA Recommender","url":"https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/","date":1739615164,"author":"/u/erik_zilinsky","guid":648,"unread":true,"content":"<p>I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.</p><p>Based on my findings, I wrote a <a href=\"https://erikzilinsky.com/posts/vpa1.html\">blog post</a> about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.</p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what do you use golang for?","url":"https://www.reddit.com/r/golang/comments/1ipykyd/what_do_you_use_golang_for/","date":1739615068,"author":"/u/Notalabel_4566","guid":636,"unread":true,"content":"<p>Is there any other major use than web development?</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux in any distribution is unobtainable for most people because the first two installation steps are basically impossible.","url":"https://www.reddit.com/r/linux/comments/1ipyc1o/linux_in_any_distribution_is_unobtainable_for/","date":1739613947,"author":"/u/trollfinnes","guid":628,"unread":true,"content":"<p>Recently, just before Christmas, I decided to check out Linux again (tried it ~20 years ago) because Windows 11 was about to cause an aneurysm.</p><p>I was expecting to spend the \"weekend\" getting everything to work; find hardware drivers, installing various open source software and generally just 'hack together something that works'.</p><p>To my surprise everything worked flawlessly first time booting up. I had WiFi, sound, usb, webcam, memory card reader, correct screen resolution. I even got battery status and management! It even came with a nice litte 'app center' making installation of a bunch of software as simple as a click!</p><p>And I remember thinking any Windows user could  install Linux and would get comfortable using it in an afternoon.</p><p>I'm pretty 'comfortable' in anything PC and have changed boot orders and created bootable things since the early 90's and considered that part of the installation the easiest part.</p><p>However, most people have never heard about any of them, and that makes the two steps seem 'impossible'.</p><p>I recently convinced a friend of mine, who also couldn't stand Window11, to install Linux instead as it would easily cover all his PC needs. </p><p>And while he is definitely in the upper half of people in terms of 'tech savvyness', both those \"two easy first steps\" made it virtually impossible for him to install it. </p><p>He easily managed downloading the .iso, but turning that iso into a bootable USB-stick turned out to be too difficult. But after guiding him over the phone he was able to create it.</p><p>But he wasn't able to get into bios despite all my attempts explaining what button to push and when</p><p>Next day he came over with his laptop. And just out of reflex I just started smashing the F2 key (or whatever it was) repeatingly and got right into bios where I enabled USB boot and put it at the top at the sequence.</p><p>After that he managed to install Linux just fine without my supervision.</p><p>But it made me realise that the two first steps in installing Linux, that are second nature to me and probably everyone involved with Linux from people just using it to people working on huge distributions, makes them virtually impossible for most people to install it.</p><p>I don't know enough about programming to know of this is possible:</p><p>Instead of an .iso file for download some sort of .exe file can be downloaded that is able to create a bootable USB-stick and change the boot order?</p><p>That would 'open up' Linux to  more people, probably orders of magnitude..</p>","contentLength":2460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lessons from David Lynch: A Software Developer's Perspective","url":"https://lackofimagination.org/2025/02/lessons-from-david-lynch-a-software-developers-perspective/","date":1739612430,"author":"/u/aijan1","guid":661,"unread":true,"content":"<p>David Lynch passed away in January 2025, shortly after being evacuated from his Los Angeles home due to the Southern California wildfires. He’s perhaps best known for the groundbreaking TV series <a href=\"https://en.wikipedia.org/wiki/Twin_Peaks\">Twin Peaks</a>, which inspired countless shows, including The X-Files, The Sopranos, and Lost.</p><p>Lynch was genuinely a good human being who cared deeply for his actors and crew. He discovered extraordinary talent like Naomi Watts, who had struggled to land a major role in a Hollywood movie after 10 years of auditioning. From the interviews he gave, it quickly becomes apparent that he respected people of all kinds and never put anyone down – even those who truly deserved it.</p><p>Lynch is famous for refusing to explain his movies. Although not a fan of his previous work, the great film critic Roger Ebert once wrote that <a href=\"https://en.wikipedia.org/wiki/Mulholland_Drive_(film)\">Mulholland Drive</a> remained compulsively watchable while refusing to yield to interpretation.</p><p>While Lynch offered very little in terms of what his movies meant, he was generous in sharing his views on creativity, work, and life in general. As a tribute to Lynch, I’d like to share my perspective on his life lessons from a software developer’s viewpoint.</p><blockquote><p>Ideas are like fish. If you want to catch little fish, you can stay in the shallow water. But if you want to catch the big fish, you’ve got to go deeper.</p></blockquote><p>We’ve all got hundreds or even thousands of ideas floating around in our brains. But the really big ones are few and far between. Once you catch a good one –because they’re so rare– write it down immediately, says Lynch. From there, ideas attract other ideas and start to grow from their initial seed state. The final job is to translate those ideas into a medium, whether it’s a film, a painting, or software.</p><blockquote><p>The idea is the whole thing. If you stay true to the idea, it tells you everything you need to know, really. You just keep working to make it look like that idea looked, feel like it felt, sound like it sounded, and be the way it was.</p></blockquote><p>Software development is part art, part engineering. We don’t build the same software over and over again – virtually all software is crafted by hand, sometimes with help from AI. If you ask two developers to create a non-trivial program, it’s very likely that the programs they produce will be different, even if the functionality is the same. Under the hood, the programming language, data structures, and overall architecture may be completely different. And on the surface, the user interfaces may look nothing alike.</p><p>It’s a good habit to listen to what users have to say, but they often can only describe their problems – they rarely come up with good ideas to solve them. And that’s OK. It’s our job to find the right ideas, implement them well, and solve tricky problems in a way we, and hopefully the users, will love.</p><blockquote><p>My friend Bushnell Keeler, who was really responsible for me wanting to be a painter, said you need four hours of uninterrupted time to get one hour of good painting in, and that is really true.</p></blockquote><p>Like other creative fields, writing code requires deep concentration. We need to hold complex structures in our minds while working through problems. Switching between coding and other tasks disrupts  – that magical state of mind where we lose track of time and produce code effortlessly. That’s why many developers hate meetings – they are toxic to our productivity.</p><blockquote><p>I believe you need technical knowledge. And also, it’s really, really great to learn by doing. So, you should make a film.</p></blockquote><p>Software development is one of those rare fields where a college degree isn’t required to succeed. Yes, we should all know the basics, but in my experience, new college graduates often lack the practical knowledge to be effective developers.</p><p>The real learning happens through hands-on experience: building real projects, debugging tricky problems, collaborating with teams, and maintaining code over time. It’s crucial to never stop learning, experimenting, and iterating on our craft.</p><blockquote><p>Happy accidents are real gifts, and they can open the door to a future that didn’t even exist.</p></blockquote><p>Tim Berners-Lee invented the web in 1989, while working at CERN, the European Organization for Nuclear Research. Originally conceived to meet the demand for information sharing between scientists around the world, the web went mainstream within just a few years.</p><p>Linus Torvalds created Git due to a licensing dispute over BitKeeper, the original version control system used for Linux development. The need for a new tool led to Git becoming the most widely used version control system today.</p><blockquote><p>I feel that a set should be like a happy family. Almost like Thanksgiving every day, happily going down the road together.</p></blockquote><p>Be kind to your teammates, don’t embarrass them. They may not be perfect, but accept them for who they are. The most important trait of an effective software development team is psychological safety –that is, team members feel safe to take risks and be vulnerable in front of each other, as corroborated by <a href=\"https://rework.withgoogle.com/en/guides/understanding-team-effectiveness\">Google’s research</a> on the subject.</p><p>It’s OK to make mistakes, as long as you learn from them. Knowing that your team has your back when things go south is a wonderful feeling.</p><blockquote><p>Most of Hollywood is about making money - and I love money, but I don’t make the films thinking about money.</p></blockquote><p>Just like Lynch prioritizes creativity over financial gain, some of the most impactful software projects started with an open source model, and they literally changed the world, such as Linux, PostgreSQL, and Node.js, just to name a few.</p><p>What makes these projects remarkable is that they didn’t emerge from corporate boardrooms – they were built by communities of passionate developers, collaborating across the world.</p><p>Money is just a means to an end. Unfortunately, many get this confused.</p><p>David, thank you for making the world a better place!</p>","contentLength":5845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipy01t/lessons_from_david_lynch_a_software_developers/"},{"title":"Container Networking - Kubernetes with Calico","url":"https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/","date":1739604355,"author":"/u/tkr_2020","guid":645,"unread":true,"content":"<ul><li>: VLAN 10</li><li>: VLAN 20</li></ul><p>When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:</p><p>The inner IP header reflects:</p><p>The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.</p><p>Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?</p>","contentLength":502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] What's the most promising successor to the Transformer?","url":"https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/","date":1739600221,"author":"/u/jsonathan","guid":655,"unread":true,"content":"<p>All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also <a href=\"https://arxiv.org/pdf/2405.04517\">xLSTM</a> and <a href=\"https://arxiv.org/pdf/2405.13956\">Aaren</a>.</p><p>What do y'all think is the most promising alternative architecture to the transformer?</p>","contentLength":283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kafka Delay Queue: When Messages Need a Nap Before They Work","url":"https://beyondthesyntax.substack.com/p/kafka-delay-queue-when-messages-need","date":1739596108,"author":"/u/Sushant098123","guid":660,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipu9n3/kafka_delay_queue_when_messages_need_a_nap_before/"},{"title":"Webassembly and go 2025","url":"https://www.reddit.com/r/golang/comments/1ipu4wd/webassembly_and_go_2025/","date":1739595637,"author":"/u/KosekiBoto","guid":635,"unread":true,"content":"<div><p>so I found <a href=\"https://www.youtube.com/watch?v=HShIpUgCPp4\">this video </a>and was thinking about doing something similar for my game as a means to implement modding, however I also stumbled upon a 3 y/o post when looking into it essentially stating that it's a bad idea and I wasn't able to really find anything on the state of go wasm, so can someone please enlighten me as to the current state of WASM and Go, thank you</p></div>   submitted by   <a href=\"https://www.reddit.com/user/KosekiBoto\"> /u/KosekiBoto </a>","contentLength":402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vq: A Vector Quantization Library for Rust 🦀","url":"https://www.reddit.com/r/rust/comments/1ipu2jg/vq_a_vector_quantization_library_for_rust/","date":1739595392,"author":"/u/West-Bottle9609","guid":638,"unread":true,"content":"<p>I've created a Rust library called Vq that implements several <a href=\"https://en.wikipedia.org/wiki/Vector_quantization\">vector quantization</a> algorithms. At the moment, these algorithms include binary, scalar, product, optimized product, tree-structured, and residual quantization. I think the library can be useful for tasks like data compression, similarity search, creating RAG pipelines, and speeding up machine learning computations.</p><p>This is my second Rust project, as I'm currently learning Rust. I'd like to get some feedback from the community and hear about any use cases you might have for the library, so I'm making this announcement.</p><p>The library is available on crates.io: <a href=\"https://crates.io/crates/vq\">vq</a>, and the source code is on GitHub: <a href=\"https://github.com/habedi/vq\">vq</a>.</p>","contentLength":664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modern Java Deep Dive","url":"https://www.youtube.com/watch?v=z4qsidg261E","date":1739591964,"author":"/u/BlueGoliath","guid":657,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipt4pe/modern_java_deep_dive/"},{"title":"Bringing Nest.js to Rust: Meet Toni.rs, the Framework You’ve Been Waiting For! 🚀","url":"https://www.reddit.com/r/rust/comments/1iprsmo/bringing_nestjs_to_rust_meet_tonirs_the_framework/","date":1739587338,"author":"/u/Mysterious-Rust","guid":641,"unread":true,"content":"<p>As a Rust developer coming from TypeScript, I’ve been missing a Nest.js-like framework — its modularity, dependency injection, and CLI superpowers. But since the Rust ecosystem doesn’t have a direct counterpart (yet!), I decided to build one myself! 🛠️</p><p>Introducing… <a href=\"https://crates.io/crates/toni\">Toni.rs</a> — a Rust framework inspired by the Nest.js architecture, designed to bring the same developer joy to our favorite language. And it’s live in beta! 🎉</p><p>Here’s what makes this project interesting:</p><p>Scalable maintainability 🧩:</p><p>A modular architecture keeps your business logic decoupled and organized. Say goodbye to spaghetti code — each module lives in its own context, clean and focused.</p><p>Need a complete CRUD setup? Just run a single CLI command. And I have lots of ideas for CLI ease. Who needs copy and paste?</p><p>Automatic Dependency Injection 🤖:</p><p>Stop wasting time wiring dependencies. Declare your providers, add them to your structure, and let the framework magically inject them. Less boilerplate, more coding.</p><p>Leave your thoughts below — suggestions, questions, or even just enthusiasm! 🚀 </p>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do I configure Minikube to use my local IP address instead of the cluster IP?","url":"https://www.reddit.com/r/kubernetes/comments/1ipr3i1/how_do_i_configure_minikube_to_use_my_local_ip/","date":1739584978,"author":"/u/Own_Appointment5630","guid":644,"unread":true,"content":"<p>Hi there!! How can I configure Minikube on Windows (using Docker) to allow my Spring Boot pods to connect to a remote database on the same network as my local machine? When I create the deployment, the pods use the same IP as the Minikube cluster which gets rejected by the database. Is there any way that Minikube uses my local IP in order to connect correctly?.</p>","contentLength":363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"what was the Linux expirance like in the 90's and 00's?","url":"https://www.reddit.com/r/linux/comments/1ipql9k/what_was_the_linux_expirance_like_in_the_90s_and/","date":1739583310,"author":"/u/mrcrabs6464","guid":627,"unread":true,"content":"<div><p>I started using Linux about 2 years ago really right at the beginning of the proton revolution. And I know that Gaming in specif was the biggest walls for mass adaption of Linux throughout the 2010's and late 2000's but Ive heard things about how most software ran through WINE until Direct x and other API's became more common. but gaming aside what was the expirance and community like at the time?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mrcrabs6464\"> /u/mrcrabs6464 </a>","contentLength":434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tabiew 0.8.4 Released","url":"https://www.reddit.com/r/rust/comments/1ipp72r/tabiew_084_released/","date":1739578902,"author":"/u/shshemi","guid":642,"unread":true,"content":"<p>Tabiew is a lightweight TUI application that allows users to view and query tabular data files, such as CSV, Parquet, Arrow, Sqlite, and ...</p><ul><li>📊 Support for CSV, Parquet, JSON, JSONL, Arrow, FWF, and Sqlite</li><li>🗂️ Multi-table functionality</li></ul><ul><li>UI is updated to be more modern and responsive</li><li>Horizontally scrollable tables</li><li>Visible data frame can be referenced with name \"_\"</li><li>Compatibility with older versions of glibc</li><li>Two new themes (Tokyo Night and Catppuccin)</li></ul>","contentLength":450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GOGC & GOMEMLIMIT ?","url":"https://www.reddit.com/r/golang/comments/1ipnxxk/gogc_gomemlimit/","date":1739575182,"author":"/u/mistyrouge","guid":633,"unread":true,"content":"<div><p>If the GC cost is fixed with regards to the amount of memory being freed up. Why would I not want to put  and  to say 70% of the memory I have available? Specially in an application that is known to be cpu bound.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/mistyrouge\"> /u/mistyrouge </a>","contentLength":245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dell Nears $5 Billion AI Server Deal for Elon Musk’s xAI","url":"https://www.bnnbloomberg.ca/business/technology/2025/02/14/dell-nears-us5-billion-ai-server-deal-for-elon-musks-xai/","date":1739571937,"author":"/u/F0urLeafCl0ver","guid":650,"unread":true,"content":"<p>(Bloomberg) -- Dell Technologies Inc. is in advanced stages of securing a deal worth more than $5 billion to provide Elon Musk’s xAI with servers optimized for artificial intelligence work.</p><p>The company will sell servers containing Nvidia Corp. GB200 semiconductors to Musk’s AI startup for delivery this year, according to people familiar with the matter, who asked to not to be named because the work is private. Some details are being finalized and still may change, some of the people added.&nbsp;</p><p>Demand for computing to run AI workloads has led to a boom for makers of high-powered servers like Dell, Super Micro Computer Inc. and Hewlett Packard Enterprise Co. Musk’s companies, including carmaker Tesla Inc. and xAI, have emerged as major customers for the hardware.&nbsp;</p><p>Dell and Nvidia declined to comment. xAI didn’t respond to a request for comment.</p><p>Dell shares jumped as much as 6% to $116.88 Friday on the news before paring some gains. The stock had slipped 4.3% this year through Thursday’s close.</p><p>A supercomputer project being built by xAI in Memphis has used a mix of Dell and Super Micro servers. In December, Dell said it had deployed tens of thousands of graphics processing units, or GPUs, there and was working to win an “unfair share” of the remaining build-out. GPUs are the key chips to power AI workloads and Nvidia is the top maker of those processing units.</p><p>Analysts expect Dell will have shipped more than $10 billion of AI servers in the fiscal year ending last month and project that value will jump to $14 billion in the fiscal year ending in January 2026. Dell is scheduled to report fiscal fourth-quarter earnings on Feb. 27, with the AI server business a major focus for investors.</p><p>The deal with xAI “would firmly establish the company as a leading AI-server provider and boost sales, though the impact on profitability is less clear,” wrote Woo Jin Ho, an analyst at Bloomberg Intelligence.</p><p>AI startup xAI’s main product, a chatbot called Grok, has primarily been available to paying users of X, the social network formerly known as Twitter. Firms that Musk runs are known to share employees, technology and computing power.</p><p>--With assistance from Ian King and Kurt Wagner.</p><p>(Updates with comments from analyst in the eighth paragraph.)</p>","contentLength":2274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ipmsdv/dell_nears_5_billion_ai_server_deal_for_elon/"},{"title":"An art exhibit in Japan where a chained robot dog will try to attack you to showcase the need for AI safety.","url":"https://v.redd.it/sglstazd96je1","date":1739568243,"author":"/u/eternviking","guid":652,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iple9t/an_art_exhibit_in_japan_where_a_chained_robot_dog/"},{"title":"GitHub - Clivern/Peanut: 🐺 Deploy Databases and Services Easily for Development and Testing Pipelines.","url":"https://github.com/Clivern/Peanut","date":1739567268,"author":"/u/Clivern","guid":634,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ipl12p/github_clivernpeanut_deploy_databases_and/"},{"title":"Shutdown Go server","url":"https://www.reddit.com/r/golang/comments/1ipj5zn/shutdown_go_server/","date":1739562402,"author":"/u/Kennedy-Vanilla","guid":637,"unread":true,"content":"<div><p>Hi, recently I saw that many people shutdown their servers like this or similar</p><pre><code>serverCtx, serverStopCtx serverCtx, serverStopCtx := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { &lt;-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { &lt;-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal(\"graceful shutdown timed out.. forcing exit.\") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf(\"error shutting down server: %v\", err) } serverStopCtx() }() log.Printf(\"Server starting on port %s...\\n\", port) err = server.ListenAndServe() if err != nil &amp;&amp; err != http.ErrServerClosed { log.Printf(\"error starting server: %v\", err) os.Exit(1) } &lt;-serverCtx.Done() log.Println(\"Server stopped\") } := context.WithCancel(context.Background()) sig := make(chan os.Signal, 1) signal.Notify(sig, syscall.SIGHUP, syscall.SIGINT, syscall.SIGTERM, syscall.SIGQUIT) go func() { &lt;-sig shutdownCtx, cancelShutdown := context.WithTimeout(serverCtx, 30*time.Second) defer cancelShutdown() go func() { &lt;-shutdownCtx.Done() if shutdownCtx.Err() == context.DeadlineExceeded { log.Fatal(\"graceful shutdown timed out.. forcing exit.\") } }() err := server.Shutdown(shutdownCtx) if err != nil { log.Printf(\"error shutting down server: %v\", err) } serverStopCtx() }() log.Printf(\"Server starting on port %s...\\n\", port) err = server.ListenAndServe() if err != nil &amp;&amp; err != http.ErrServerClosed { log.Printf(\"error starting server: %v\", err) os.Exit(1) } &lt;-serverCtx.Done() log.Println(\"Server stopped\") </code></pre><p>Is it necessary? Like it's so many code for the simple operation</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Kennedy-Vanilla\"> /u/Kennedy-Vanilla </a>","contentLength":1805,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"the ref keyword","url":"https://www.reddit.com/r/rust/comments/1ipixny/the_ref_keyword/","date":1739561817,"author":"/u/Tickstart","guid":640,"unread":true,"content":"<p>I've made a quick mock situation which is analogous to my situation the other day:</p><pre><code>fn main() { let mut v: Option&lt;Vec&lt;usize&gt;&gt; = None; let mut h = 20; while h.ne(&amp;0) { if (h % 3).ge(&amp;1) { match v { Some(ref mut v) =&gt; (*v).push(h), None =&gt; v = Some(vec![h]) } } h -= 1 } println!(\"{v:?}\") } </code></pre><p>I was a bit confused on how it \"should\" be solved. My issue is the \"ref mut\". It made sense to me that I didn't want to consume the vector v, just add to it if it existed and I tried adding ref (then mut), which worked. When I goodled, it seemed ref was a legacy thing and not needed anymore. My question is, how is the idiomatic way to write this? Perhaps it's possible to do in a much simpler way and I just found a way to complicate it for no reason.</p><p>Also, don't worry I know this is a terrible pattern, it was mostly for tesing something.</p>","contentLength":828,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Macro-Less, Highly Integrated OpenAPI Document Generation in Rust with Ohkami","url":"https://medium.com/@kanarus786/macro-less-highly-integrated-openapi-document-generation-in-rust-with-ohkami-912de388adc1","date":1739557605,"author":"/u/kanarus","guid":639,"unread":true,"content":"<p>In Rust web dev, <a href=\"https://github.com/juhaku/utoipa\" rel=\"noopener ugc nofollow\" target=\"_blank\">utoipa</a> is the most popular crate for generating OpenAPI document from server code. While it’s a great tool, it can be frustrating due to excessive macro use.</p><p>A new web framework Ohkami offers a ,  way to generate OpenAPI document with its “openapi” feature.</p><p>Let’s take following code as an example. It’s the same sample from the “openapi” section of the README, but with openapi-related parts removed:</p><pre></pre><p>While this compiles and works as a pseudo user management server, activating “openapi” feature causes a compile error, telling that User and CreateUser don’t implement ohkami::openapi::Schema.</p><p>As indicated by this, Ohkami with “openapi” feature effectively handles type information and intelligently collects its endpoints’ metadata. It allows code like:</p><pre></pre><p>to assemble metadata into an OpenAPI document and output it to a file .</p><p>Then, how we implement Schema? Actually we can easily impl Schema by hand, or just #[derive(Schema)] is available! In this case, derive is enough:</p><pre></pre><p>That’s it! Just adding these derives allows Ohkami::generate to output following file:</p><pre></pre><p>Additionally, it’s easy to define the User schema as a component instead of duplicating inline schemas. In derive, just add #[openapi(component)] helper attribute:</p><pre></pre><pre></pre><p>And  #[operation] attribute is available to set summary, description, and override operationId and each response’s description:</p><pre></pre><pre></pre><p>Let’s take a look at how this document generation works!</p><p>First, the #[derive(Schema)]s are expanded as following:</p><pre></pre><pre></pre><p>The DSL enables to easily impl manually.</p><p>Schema trait links the struct to an item of type called “SchemaRef”.</p><h2>2. openapi_* hooks of FromParam, FromRequest, IntoResponse</h2><p>FromParam, FromRequest and IntoResponse are Ohkami’s core traits appeared in the handler bound:</p><pre></pre><p>When “openapi” feature is activated, they additionally have following methods:</p><pre></pre><p>Ohkami leverages these methods in IntoHandler to generate consistent openapi::Operation, reflecting the actual handler signature like <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/fang/handler/into_handler.rs#L328-L335\" rel=\"noopener ugc nofollow\" target=\"_blank\">this</a>.</p><p>Moreover, Ohkami properly propagates schema information in common cases like <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/response/into_response.rs#L114-L128\" rel=\"noopener ugc nofollow\" target=\"_blank\">this</a>, allowing users to focus only on the types and schemas of their app.</p><h2>3. routes metadata of Router</h2><p>In Ohkami, what’s called router::base::Router <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/router/base.rs#L8-L18\" rel=\"noopener ugc nofollow\" target=\"_blank\">has “routes” property</a> that stores all the routes belonging to an Ohkami instance. This is returned alongside router::final::Router from “finalize” step, and is used to assemble metadata of all endpoints.</p><p>What Ohkami::generate itself does is just to serialize an item of type openapi::document::Document and write it to a file.</p><p>The openapi::document::Document item is created by<a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/ohkami/src/router/final.rs#L54-L59\" rel=\"noopener ugc nofollow\" target=\"_blank\"> “gen_openapi_doc” of router::final::Router</a>, summarized as follows:</p><pre></pre><p>That’s how Ohkami generates OpenAPI document!</p><p>There is, however, a problem in , Cloudflare Workers: Ohkami is loaded to Miniflare or Cloudflare Workers as WASM, so it can only generate OpenAPI document andcannot write it to the user’s local file system.</p><p>To work around this, Ohkami provides a CLI tool <a href=\"https://github.com/ohkami-rs/ohkami/blob/6e243ac823e21f286aca2660f9d38f7bde381c5a/scripts/workers_openapi.js\" rel=\"noopener ugc nofollow\" target=\"_blank\">scripts/workers_openapi.js</a>. This is, for example, used in package.json of Cloudflare Workers + OpenAPI template:</p><pre></pre><p>generates OpenAPI document!</p>","contentLength":3119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iph9lk/macroless_highly_integrated_openapi_document/"},{"title":"Dynamic triple/double buffering merge request for GNOME was just merged!","url":"https://gitlab.gnome.org/GNOME/mutter/-/merge_requests/1441","date":1739556559,"author":"/u/joojmachine","guid":631,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ipguyt/dynamic_tripledouble_buffering_merge_request_for/"},{"title":"[P] GNNs for time series anomaly detection","url":"https://www.reddit.com/r/MachineLearning/comments/1ipgk8p/p_gnns_for_time_series_anomaly_detection/","date":1739555819,"author":"/u/Important-Gear-325","guid":654,"unread":true,"content":"<p>For the past few months, my partner and I have been working on a project exploring the use of Graph Neural Networks (GNNs) for Time Series Anomaly Detection (TSAD). As we are near the completion of our work, I’d love to get feedback from this amazing community!</p><p>Any comments, suggestions, or discussions are more than welcome! If you find the repo interesting, dropping a ⭐ would mean a lot. : )</p><p>We're also planning to publish a detailed report with our findings and insights in the coming months, so stay tuned!</p><p>The repo is still under development so don't be too harsh :)</p><p>Looking forward to hearing your thoughts!</p>","contentLength":615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/","date":1739555053,"author":"/u/trouphaz","guid":649,"unread":true,"content":"<p>I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.</p><p>So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.</p><pre><code>sc() { CLUSTER_NAME=\"${1}\" export KUBECONFIG=\"~/.kube/${CLUSTER_NAME}\" } </code></pre>","contentLength":592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incremental Archival from Postgres to Parquet for Analytics","url":"https://www.crunchydata.com/blog/incremental-archival-from-postgres-to-parquet-for-analytics","date":1739554943,"author":"/u/gtobbe","guid":656,"unread":true,"content":"<p>PostgreSQL is commonly used to store event data coming from various kinds of devices. The data often arrives as individual events or small batches, which requires an operational database to capture. Features like <a href=\"https://www.crunchydata.com/blog/native-partitioning-with-postgres\">time partitioning</a> help optimize the storage layout for time range filtering and efficient deletion of old data.</p><p>The PostgreSQL feature set gives you a lot of flexibility for handling a variety of IoT scenarios, but there are certain scenarios for it is less suitable, namely:</p><ul><li>Long-term archival of historical data</li><li>Fast, interactive analytics on the source data</li></ul><p>Ideally, data would get automatically archived in cheap storage, in a format optimized for large analytical queries.</p><p>We developed two open source Postgres extensions that help you do that:</p><ul><li><a href=\"https://github.com/CrunchyData/pg_parquet\">pg_parquet</a> can export (and import) query results to the <a href=\"https://parquet.apache.org/\">Parquet</a> file format in object storage using regular COPY commands</li><li><a href=\"https://github.com/crunchydata/pg_incremental\">pg_incremental</a> can run a command for a never-ending series of time intervals or files, built on top of <a href=\"https://github.com/citusdata/pg_cron\">pg_cron</a></li></ul><p>With some simple commands, you can set up a reliable, fully automated pipeline to export time ranges to the columnar Parquet format in S3.</p><p>Then, you can use a variety of analytics tools to query or import the data. My favorite is of course <a href=\"https://www.crunchydata.com/blog/crunchy-data-warehouse-postgres-with-iceberg-for-high-performance-analytics\">Crunchy Data Warehouse</a>.</p><p>On any PostgreSQL server that has the pg_parquet and pg_incremental extensions, you can set up a pipeline that periodically exports data to in S3 in two steps.</p><p>The pg_incremental extension has a create_time_interval_pipeline function that will run a given command once the time interval has passed, with 2 timestamp parameters set to the start and end of the hour. We cannot directly use query parameters in a COPY command, but we can define a simple PL/pgSQL function that generates and executes a custom COPY command using the parameters.</p><pre><code>-- existing raw data table\ncreate table events (\n  event_id bigint not null generated by default as identity,\n  event_time timestamptz not null default now(),\n  device_id bigint not null,\n  sensor_1 double precision\n);\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n\ninsert into events (device_id, sensor_1)\nvalues (297, 20.4);\n\n-- define an export function that wraps a COPY command\ncreate or replace function export_events(start_time timestamptz, end_time timestamptz)\nreturns void language plpgsql as $function$\nbegin\n  execute format(\n    $$\n      copy (select * from events where event_time &gt;= %L and event_time &lt; %L)\n      to 's3://mybucket/events/%s.parquet' with (format 'parquet');\n    $$,\n    start_time, end_time, to_char(start_time, 'YYYY-MM-DD-HH')\n  );\nend;\n$function$;\n\n-- export events hourly from the start of the year, and keep exporting in the future\nselect incremental.create_time_interval_pipeline('event-export',\n  time_interval := '1 hour',                      /* export data by the hour                */\n  batched := false,                               /* process 1 hour at a time               */\n  start_time := '2025-01-01',                     /* backfill from the start of the year    */\n  source_table_name := 'events',                  /* wait for writes on events to finish    */\n  command := $$ select export_events($1, $2) $$   /* run export_events with start/end times */\n);\n\n</code></pre><p>By running these commands, Postgres will export all the data from the start of the year into hourly Parquet files in S3, and will keep doing so after every hour and automatically retry on failure.</p><p>To use pg_parquet Crunchy Bridge, you can add your S3 credentials for pg_parquet to your Postgres server via the dashboard under Settings -&gt; Data lake.</p><p>Once data is in Parquet, you can use a variety of tools and approaches to query the data. If you want to keep using Postgres, you can use <a href=\"https://www.crunchydata.com/products/warehouse\">Crunchy Data Warehouse</a> which has two different ways of working with Parquet data.</p><p>The simplest way to start querying Parquet files in S3 in Crunchy Data Warehouse is to use a lake analytics table. You can easily create a table for all Parquet files that match a wildcard pattern:</p><pre><code>create foreign table events_parquet ()\nserver crunchy_lake_analytics\noptions (path 's3://mybucket/events/*.parquet');\n</code></pre><p>You can then immediately query the data and the files get cached in the background, so queries will quickly get faster.</p><p>A downside of querying Parquet directly is that eventually we will have a lot of hourly files that match the pattern, and there will be some overhead from listing them for each query (listing is not cached). We also cannot easily change the schema later.</p><p>A more flexible approach is to import the Parquet files into an Iceberg table. Iceberg tables are also backed by Parquet in S3, but the files are compacted and optimized, and the table supports transactions and schema changes.</p><p>You can create an Iceberg table that has the same schema as a set of Parquet files using the definition_from option. You could also load the data using load_from, but we’ll do that separately.</p><pre><code>create table events_iceberg () using iceberg\nwith (definition_from = 's3://mybucket/events/*.parquet');\n</code></pre><p>Now we need a way to import all existing Parquet files and also import new files that show up in S3 into Iceberg. This is another job for pg_incremental. Following a similar approach as before, we create a function to generate a COPY command using a parameter.</p><pre><code>-- define an import function that wraps a COPY command to import from a URL\ncreate function import_events(path text)\nreturns void language plpgsql as $function$\nbegin\n  execute format($$copy events_iceberg from %L$$, path);\nend;\n$function$;\n\n-- create a pipeline to import new files into a table, one by one.\n-- $1 will be set to the path of a new file\nselect incremental.create_file_list_pipeline('event-import',\n   file_pattern := 's3://mybucket/events/*.parquet',\n   list_function := 'crunchy_lake.list_files',\n   command := $$ select import_events($1) $$,\n);\n\n-- optional: do compaction immediately\nvacuum events_iceberg;\n\n</code></pre><p>After running these commands, your data will be continuously archived from your source Postgres server into Iceberg in S3. You can then run fast analytical queries directly from Crunchy Data Warehouse, which uses a combination of parallel, vectorized query processing and file caching to speed up queries. You can additionally set up (materialized) views and assign read permissions to the relevant users.</p><p>No complex ETL pipelines required.</p><p>To give you a sense of the performance benefit of using Parquet, we loaded 100M rows into the source table, which got automatically mirrored in Parquet and Iceberg via our pipelines. We then ran a simple analytical query on each table:</p><pre><code>select device_id, avg(sensor_1) from events group by 1;\n</code></pre><p>The runtimes in milliseconds are shown in the following chart:</p><p>In this case the source server is a standard-16 instance (4 vcpus) on Crunchy Bridge, and the warehouse is a warehouse-standard-16 instance (4 vcpus). So, using Crunchy Data Warehouse we can analyze 100M rows in well under a second on a small machine, and get &gt;10x speedup with Iceberg.</p><p>The use of compression also means the size went from 8.9GB in PostgreSQL to 1.2GB in Iceberg using object storage.</p><p>With pg_parquet and pg_incremental, you can incrementally export data from PostgreSQL into Parquet in S3, and with Crunchy Data Warehouse you can process and analyze that data very quickly while still using PostgreSQL.</p><p>One of the nice characteristics of the approach described in this blog is that the pipelines are fully transactional. It means that every import or export step either fully succeeds or fails and then it will be retried until it does succeed. That’s how we can create production-ready pipelines with a few simple SQL commands.</p><p>Under the covers, pg_incremental keeps track of which time ranges or files have been processed. The bookkeeping happens in the same transaction as the COPY commands. So if a command fails because of an ephemeral S3 issue, the data will not end up being ingested twice or go missing. Having transactions takes away a huge amount of complexity for building reliable pipelines. There can of course be other reasons for pipeline failures that cannot be resolved through retries (e.g. changing data format), so it is still important to <a href=\"https://github.com/crunchydata/pg_incremental#monitoring-pipelines\">monitor</a> your pipelines.</p>","contentLength":8175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipg7oy/incremental_archival_from_postgres_to_parquet_for/"},{"title":"Siren Call of SQLite on the Server","url":"https://pid1.dev/posts/siren-call-of-sqlite-on-the-server/","date":1739551579,"author":"/u/sausagefeet","guid":659,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ipevoh/siren_call_of_sqlite_on_the_server/"},{"title":"I'm very impressed by how Rust supports both beginners and pro's","url":"https://www.reddit.com/r/rust/comments/1ipe6m7/im_very_impressed_by_how_rust_supports_both/","date":1739549777,"author":"/u/ConstructionShot2026","guid":643,"unread":true,"content":"<p>I would go as far saying it supports a syntax abstraction that is simpler than python to read.</p><p>I just find it amazing, with a performance level so close to C++.</p><p>Its your choice how many complex features you want to add for control and optimization, and the compiler is so cool, that it can add them automatically if I don't see it necessary.</p><p>I believe if more knew how simple it could be, more would use it outside systems programming :D</p>","contentLength":434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why does Linux open large file bases much faster than windows?","url":"https://www.reddit.com/r/linux/comments/1ipd8a7/why_does_linux_open_large_file_bases_much_faster/","date":1739547296,"author":"/u/AlternativeCarpet494","guid":629,"unread":true,"content":"<p>So I have a 4TB hard drive with around a 100 GB dataset on it. I was going to some useless uni classes today and thought oh I’ll just work on some of my code to process the data set on my windows laptop. Anyways, the file explorer crashed. Why is the windows file system so much worse?</p>","contentLength":287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["reddit"]}