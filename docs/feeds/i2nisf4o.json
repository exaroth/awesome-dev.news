{"id":"i2nisf4o","title":"Reddit","displayTitle":"Reddit","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":159,"items":[{"title":"native WebP encoding version 1.0! üöÄ","url":"https://www.reddit.com/r/golang/comments/1iw8a68/native_webp_encoding_version_10/","date":1740310727,"author":"/u/Pretend-Ad1926","guid":9590,"unread":true,"content":"<p>I‚Äôm excited to announce nativewebp v1.0, a major milestone for our WebP encoder in Go! This version marks 1.0 because we now fully support the VP8L format, making nativewebp a complete solution for lossless WebP encoding. Alongside this, we‚Äôve added better compression, improved Go integration, and important bug fixes.</p><p>Here are some highlights of this release:</p><p><strong>Full VP8L Feature Support</strong></p><p>This release now fully supports all VP8L features, including LZ77, Color Caching, and transforms, ensuring more accurate and efficient encoding.</p><p><strong>Smarter Compression with Filter Selection for Predictor Transform</strong></p><p>We now analyze block entropy and automatically select the best filter per block, leading to much better compression.</p><p>nativewebp now includes a wrapper for <a href=\"http://golang.org/x/image/webp\">golang.org/x/image/webp</a>, so you can use Decode and image.Decode out of the box without extra imports.</p><p>Looking forward to your thoughts and feedback on the new release!</p>","contentLength":918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7puq/amazon_eks_downgrade_a_practical_solution/","date":1740308426,"author":"/u/Complete-Emu-6287","guid":9546,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7pqt/amazon_eks_downgrade_a_practical_solution/","date":1740308415,"author":"/u/Complete-Emu-6287","guid":9545,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1740303005,"author":"/u/Aciddit","guid":9526,"unread":true,"content":"<div>By <b>Antonio Ojea, Michael McCune</b> |\n<time datetime=\"2025-02-14\">Friday, February 14, 2025</time></div><p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><figure><img src=\"https://kubernetes.io/images/docs/components-of-kubernetes.svg\" alt=\"Components of Kubernetes\"></figure><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the Node\nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial Node is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><figure><img src=\"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/ccm-chicken-egg-problem-sequence-diagram.svg\" alt=\"Chicken and egg problem sequence diagram\"><figcaption><p>Chicken and egg problem sequence diagram</p></figcaption></figure><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a Node resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a Node resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one ‚Äúcorrect way‚Äù to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting ‚ÄúhostNetwork‚Äù to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider‚Äôs instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure‚Äôs node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider‚Äôs documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: cloud-controller-manager\n  name: cloud-controller-manager\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cloud-controller-manager\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cloud-controller-manager\n      annotations:\n        kubernetes.io/description: Cloud controller manager for my infrastructure\n    spec:\n      containers: # the container details will depend on your specific cloud controller manager\n      - name: cloud-controller-manager\n        command:\n        - /bin/my-infrastructure-cloud-controller-manager\n        - --leader-elect=true\n        - -v=1\n        image: registry/my-infrastructure-cloud-controller-manager@latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 50Mi\n      hostNetwork: true # these Pods are part of the control plane\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - topologyKey: \"kubernetes.io/hostname\"\n            labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: cloud-controller-manager\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoExecute\n        key: node.kubernetes.io/unreachable\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoExecute\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoSchedule\n        key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - effect: NoSchedule\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iw6fpj/the_cloud_controller_manager_chicken_and_egg/"},{"title":"Finding UI libraries is easy, but discovering components visually is still a challenge. A curated list + an idea to fix this.","url":"https://github.com/sanjay10985/animated-react-collection","date":1740302565,"author":"/u/Mobile_Candidate_926","guid":9592,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iw6bzk/finding_ui_libraries_is_easy_but_discovering/"},{"title":"[P] See the idea development of academic papers visually","url":"https://www.reddit.com/r/MachineLearning/comments/1iw5lgj/p_see_the_idea_development_of_academic_papers/","date":1740299410,"author":"/u/MadEyeXZ","guid":9593,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Relevance-Guided Parameter Optimization for Efficient Control in Diffusion Transformers","url":"https://www.reddit.com/r/MachineLearning/comments/1iw46oq/r_relevanceguided_parameter_optimization_for/","date":1740293456,"author":"/u/Successful-Western27","guid":9487,"unread":true,"content":"<p>The key technical contribution here is a relevance-guided architecture that makes diffusion transformers more computationally efficient by selectively allocating processing power based on region importance. It combines DiT (Diffusion Transformers) with ControlNet approaches while introducing a relevance prior mechanism.</p><p>Main technical points: - Introduces a two-stage relevance assessment system: lightweight networks evaluate region importance, followed by adaptive computation allocation - Integrates with existing diffusion pipelines through modular design - Relevance prior guides transformer attention mechanisms - Compatible with standard diffusion transformer architectures</p><p>Key results: - 30-50% reduction in computational overhead - Maintains or improves image quality compared to baselines - More precise control over generated content - Effective handling of complex scenes</p><p>I think this could have meaningful impact on making high-quality image generation more accessible, especially for resource-constrained applications. The approach seems particularly promising for deployment scenarios where computational efficiency is crucial.</p><p>I think the relevance-guided approach could extend beyond image generation - the core idea of selective computation based on importance could benefit other transformer applications where attention mechanisms are computationally expensive.</p><p>TLDR: Novel architecture that makes diffusion transformers more efficient by focusing computational resources on important image regions, reducing compute needs by 30-50% while maintaining quality.</p>","contentLength":1576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Folang: Transpiler for F#-like functional languages ‚Äã‚Äãto Go","url":"https://www.reddit.com/r/golang/comments/1iw3tz7/folang_transpiler_for_flike_functional_languages/","date":1740292019,"author":"/u/karino2012","guid":9541,"unread":true,"content":"<p>I wrote a transpiler in Go that transpiles F#-like functional languages ‚Äã‚Äãto Go.</p><p>I design the language specifications from scratch to match Go, and named it Folang.</p><p>There are still many NYIs, but I have implemented it to the extent that it can be self-hosted, so I will post it on reddit.</p><ul><li>A transpiler that does not require anything other than Go to try</li><li>Argument types are inferred using F#-like syntax, and the arguments are generalized to become generic functions</li><li>The transpiler itself is 3600 lines of Folang code and about 500 lines of Go code</li></ul>","contentLength":546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A simple VSCode extension to remember which virtual desktop each editor window is on in Linux","url":"https://marketplace.visualstudio.com/items?itemName=mathiscode.remember-desktops","date":1740290165,"author":"/u/FatherCarbon","guid":9468,"unread":true,"content":"<div><div><div><table role=\"presentation\"><tbody><tr><td><div><div><p>On some Linux desktop managers, Visual Studio Code editors don't remember their last desktop. This extension uses  to save the desktop of each open editor window, and restore them when the editor starts.</p><p>There are commands to save the editor locations, and to restore them, but by default the extension will start working automatically when it is installed.</p></div></div></td></tr></tbody></table></div></div></div>","contentLength":356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iw3czs/a_simple_vscode_extension_to_remember_which/"},{"title":"Thoughts on listings like these selling flash drives with Ubuntu and other Linux distros pre-installed?","url":"https://www.reddit.com/r/linux/comments/1iw2zog/thoughts_on_listings_like_these_selling_flash/","date":1740288767,"author":"/u/FutureSuccess2796","guid":9486,"unread":true,"content":"<p>Admittedly someone who's relatively newer to the Linux space, so please bear with my question here. I was in middle of actually shopping for some extra brand new USBs to replace my old ones when I encountered this for the first time. It looked like there were quite a good number of people on marketplace platforms like eBay and Mercari selling bootable USB flash drives with a Linux distro pre-installed on it. Majority of the ones I saw were Ubuntu (like what I had pictured) on there, but I also saw a good amount of ones with Kali and different versions of Linux Mint as well. </p><p>Seems like you get the USB according to said listings with instructions on how to properly boot it or install it on your computer, and in some cases even provide contact information for support if needed. The prices on some of these are slightly in the higher side when compared to those I had screenshots of in the examples, and the sellers all had a large amount of sales and positive responses. </p><p>Now, of course, I'd personally just stick to what I've been doing and just create the bootable drive myself for literally free like I have from the start. So to me it was interesting to see these out there actually being bought when the process of doing this yourself is relatively easy with step-by-step guides on the respective distro's website and even YouTube tutorials if you wish to follow those. </p><p>So in short, what's everyone think of these?</p>","contentLength":1426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Font for programming mathematics","url":"https://www.reddit.com/r/rust/comments/1iw2ovd/font_for_programming_mathematics/","date":1740287667,"author":"/u/okimusix","guid":9544,"unread":true,"content":"<p>So I am a physics undergrad and I've been using Rust for a few years now. It's my favorite language and I use it for everything, from personal apps using Tauri to taking advantage of its speed for computations and using it in my school assignments.</p><p>Since I often find myself writing math code, I found naming variables \"lambda_squared\", for example, looks really clunky and makes it harder to read the code. For this, I implemented a Live Templates group on RustRover that replaced lambda, for example, with its equivalent unicode character. However, Rust did complain a little.</p><p>Finally, though, I found the solution. I had been trying to do this for a while with no luck, but I found a way to make it work. I used the ligature system on the FiraCode font to implement ligatures for every greek letter and some mathematical symbols, this way you get the readability of actual math, but for the compiler, it still looks like plain text. Here's an example</p><p>The text for the sum variable, for example, is just \"SUMxu2\", and both the compiler and I are happier. I don't know if anyone has done this before, I tried to look for it but never found anything. </p><p>If you find this something that could be useful for you or others, I can share a link to a drive or something where you can download the font, as well as the guide to every symbol I included. If so, please comment and share your thoughts on this too :)</p>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] API platforms vs self-deployment for diffusion models","url":"https://www.reddit.com/r/MachineLearning/comments/1iw2kbl/d_api_platforms_vs_selfdeployment_for_diffusion/","date":1740287219,"author":"/u/crookedstairs","guid":9469,"unread":true,"content":"<p>Caveat that Modal is a serverless compute platform! But this post covers when you might choose between API platforms (replicate, fal), traditional cloud (AWS EC2), managed ML platforms (SageMaker, Vertex), and serverless cloud.</p><p>I often see companies jump to self-deployment even if they're just using off-the-shelf models with a couple of adapters. I think that rarely makes sense from a cost or effort perspective unless you have a high volume of production traffic that you're amortizing those things across. The most compelling reason to move to self-deployment is if you need a high level of control over generated inputs =&gt; this requires fine-tuned weights / customer adapters / multi-step generation pipeline =&gt; this requires code-level control of your deployment.</p><p>What do you agree/disagree with? If you've evaluated these categories of providers before, tell me how they stacked up against each other.</p>","contentLength":907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's your recommendation on video editors?","url":"https://www.reddit.com/r/linux/comments/1iw2aae/whats_your_recommendation_on_video_editors/","date":1740286224,"author":"/u/chuzambs","guid":9506,"unread":true,"content":"<p>Hi there ! I'm looking for the best video editor for Linux, but as I know that's a completely subjective matter I ask for your favorite one. I come from adobe premiere and I'm looking for a Linux replacement, Im not a cinematographer so I'm not looking for something extremely professional.</p><p>I think Id go for da Vinci resolve since it's more standard, but would love to hear your recommendations</p><p>Edit: I'm running fedora bluefin (gnome) so I'd rather use flatpak </p>","contentLength":461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is your logging, monitoring & observability stack for your golang app?","url":"https://www.reddit.com/r/golang/comments/1iw07rm/what_is_your_logging_monitoring_observability/","date":1740279237,"author":"/u/gwwsc","guid":9441,"unread":true,"content":"<p>My company uses papertrail for logging, prometheus and grafana for observability and monitoring.</p><p>I was not actively involved in the integration as it was done by someone else a few years ago and it works.</p><p>I want to do the same thing for my side project that I am working on for learning purpose. The thing I am confused about it should I first learn the basics about otel, collector agents etc? Or should I just dive in?</p><p>As a developer I get an itch if things are too abstracted away and I don't know how things are working. I want to understand the underlying concepts first before relying on abstraction.</p><p>What tools are you or your company using for this?</p>","contentLength":653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My \"AI Operating System\" Can Now Organize My Desktop!","url":"https://v.redd.it/crjpxmcknske1","date":1740275232,"author":"/u/mitousa","guid":9446,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ivyyce/my_ai_operating_system_can_now_organize_my_desktop/"},{"title":"This feels illegal","url":"https://www.reddit.com/r/linux/comments/1ivyn4j/this_feels_illegal/","date":1740274254,"author":"/u/dk865409","guid":9424,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built WikiTok in 4 hours - A TikTok style feed for Wikipedia","url":"https://www.reddit.com/r/artificial/comments/1ivy48f/i_built_wikitok_in_4_hours_a_tiktok_style_feed/","date":1740272626,"author":"/u/Illustrious-King8421","guid":9542,"unread":true,"content":"<p>So, I decided to use Replit's AI Agent to create my own version. Took me about 4 hours total, which isn't bad since I don't know any code at all.</p><p>To be honest, at first it seemed unreal - seeing the AI build stuff just from my instructions. But then reality hit me. With every feature I wanted to add, it became more of a headache. Here's what I mean: I wanted to move some buttons around, simple stuff. But when I asked the AI to realign these buttons, it messed up other parts of the design that were working fine before. Like, why would moving a button break the entire layout?</p><p>This really sucks because these errors took up most of my time. I'm pretty sure I could've finished everything in about 2 hours if it wasn't for all this fixing of things that shouldn't have broken in the first place.</p><p>I'm curious about other people's experiences. If you don't code, I'd love to hear about your attempts with AI agents for building apps and websites. What worked best for you? Which AI tool actually did what you needed?</p><p>What do you think? Would love to hear your stories and maybe get some tips for next time!</p>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"After 15 years of using Windows, I decided to try Linux","url":"https://www.reddit.com/r/linux/comments/1ivxy81/after_15_years_of_using_windows_i_decided_to_try/","date":1740272133,"author":"/u/Catwz","guid":9408,"unread":true,"content":"<p>First of all, I apologize for writing such a long text.</p><p>I'm 22 years old. I know I'm young and still don't know much, but I'd like to write about this anyway. I think I started using computers during the Windows XP era. My father worked repairing computers. My mom says I learned to type on a computer before writing on paper. I was like one of today's kids who spend all day on their phones, except with computers. During my childhood, I spent my time chronically online, playing various games and browsing the internet. I remember Windows XP very well, along with Windows 7 and Minecraft. Those were good times, but as I grew older, things changed very quickly. My father stopped working with computer repairs, and soon I knew more than everyone else in the family.</p><p>I could fix all kinds of computers easily for my friends; back then, everything was Windows. My first contact with Linux was at school when we started having computer classes, when I was around 15. The school computers were slow and had Ubuntu installed. It was slow, ugly, and very limited because the computers were managed by the school. That was my first impression: a slow system for government computers.</p><p>Microsoft tried various things. I remember Windows 8 when formatting laptops, and then that Windows 8.1 update where they changed the menu. A lot happened, and it seems to have passed so quickly. At school, I always used Office suite programs: Word, PowerPoint, etc., and in computer classes, you had to use LibreOffice on a very slow government computer. it was ugly and seemed very difficult to use.</p><p>My family's financial situation didn't improve much, so I ended up with limited access to new technologies. My phone was already old, and my computers were getting old. I still remember Windows 10's launch very well. My relatives would bring computers for me to repair and format, wanting the latest version of Windows with Office and everything else, but the computers were already old and barely worked with Windows 8.</p><p>I begged my father to buy me a laptop, and after much insistence, I finally convinced him. It was an Asus X450LA. A mid-range computer for its time. It came with Windows 8, I think, but I did that upgrade to Windows 10. I used it until I finished high school, but then Windows 11 came along, and my laptop was cut from the list of computers that could upgrade. it was the end of my laptop's life.</p><p>I was already working at my father's market, so I bought myself a new gaming computer with Windows 11. I had time again to spend on the internet and started to worry about my father's business expenses. Using Office costs money, sales programs are expensive, everything is expensive, and maybe my gaming laptop won't even be able to use the next Windows.</p><p>I started researching Linux. At first, I was a bit scared because everyone on Reddit talked about terminals, command lines to install anything, etc., but I decided to take my old laptop and refurbish it. I bought a new battery, an SSD, and an 8GB RAM stick. I researched on Reddit which distro was best for beginners, got an old USB drive, put Mint on it, and formatted my computer: Love at first sight.</p><p>I customized Mint and left it in a way that I spend more than 15 minutes before doing anything just appreciating it. I used LibreOffice for everything I did in Office. I used Firefox and liked it a lot. The system is very fast, strangely seems faster than my new computer with Windows 11. I downloaded my daily-use programs from Mint's app center: Spotify, Bitwarden, everything's there. I spent hours playing with the terminal with ChatGPT's help. I extracted running process logs to txt, system information. it's very easy to use. I even managed to install a game I played in my childhood, a BF2 mod: Forgotten Hope 2 from Windows on Mint using Lutris (I swear it's the last Windows thing I'll use).</p><p>I'm in love with my old laptop again. I cleaned it, spent hours looking at it, I love using Mint, made it my own. I'm going to buy a new computer for my room and install Mint for my personal use. I'll have a laptop and a computer with Linux. My current computer with Windows 11 will be only for sales programs and government programs that only work on Windows. I showed it to my father, and he liked Linux too.<p> Windows never again. Using Windows now feels like one of those mobile games full of ads</p></p>","contentLength":4349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advanced SQL Tricks (CTEs, Conditional Aggregations, etc)","url":"https://youtu.be/rDGCOE5YGT0","date":1740269892,"author":"/u/Special_Community179","guid":9543,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivx75e/advanced_sql_tricks_ctes_conditional_aggregations/"},{"title":"Found this on a piece of digital signage in a bathroom","url":"https://www.reddit.com/r/linux/comments/1ivwydq/found_this_on_a_piece_of_digital_signage_in_a/","date":1740269184,"author":"/u/A_Sc00py_b0i","guid":9393,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing wctx - A simple CLI tool for window context info on Wayland & X11","url":"https://www.reddit.com/r/linux/comments/1ivw7xn/introducing_wctx_a_simple_cli_tool_for_window/","date":1740267048,"author":"/u/slightlyfaulty","guid":9561,"unread":true,"content":"<p>Hey everyone, I just released my first package for Linux. It's called  (short for window context). It's a simple CLI tool that provides real-time information about the current  window (focused window) or  window (under the mouse cursor) on Wayland and X11. It's (mostly) written in Rust.</p><p>It's not very useful on its own, but it makes it much easier for programs and scripts to work with windows. For example, you could create hotkeys that only work in specific apps, or change your mouse scroll speed when the cursor is in a browser window, or turn your monitor brightness up when it has a fullscreen window.</p><p>You can of course already do these things, with a bit of effort. The main advantage of wctx is that it works across multiple desktop environments, which means programs and scripts using it will too. It's also dead simple to use, with several CLI output options and formats, as well as a D-Bus interface.</p><p>Currently it supports these desktop environments, with more to come if there's enough interest in them:</p><p>For other distros an installation script is included, with more info in the readme.</p><p>I'd love to hear everyone's thoughts. This is also my first real Rust project, so please be nice üòÑ (or rip me a new one so I can learn).</p><p>Feedback and contributions are very welcome!</p>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My first time with Linux","url":"https://www.reddit.com/r/linux/comments/1ivvcxn/my_first_time_with_linux/","date":1740264599,"author":"/u/Acu17y","guid":9392,"unread":true,"content":"<p>Oh my god guys, I'm speechless. Unfortunately I regret it, but it's the first time I've put my hands on a PC with a Linux kernel.<p> But this stuff is absurd! It has mind-blowing performance!</p></p><p>I installed it on my old laptop with an i3 5005u / 4gb of ram and a 500gb 5400rpm hdd and it's like it was reborn. I mean, it's basically the OS I've always dreamed of, I feel like the PC is really mine and everything is so fast and intuitive that I can't describe it.</p><p>I was so impressed by Linux Mint that I'm really thinking of installing it on the main machine and getting rid of Windows, if only it weren't for the huge library of video games I have.</p><p>It also has a community made up of wonderful people, true enthusiasts.</p><p>I write this post as an appreciation for this discovery and someone who can help me understand if it is possible to use mint for gaming, I read around that there are problems with anti-cheats and online games?</p>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Any open source project that shows a good example of unit test and integration test","url":"https://www.reddit.com/r/golang/comments/1ivv5eq/any_open_source_project_that_shows_a_good_example/","date":1740264003,"author":"/u/smartfinances","guid":9367,"unread":true,"content":"<p>As the title suggests. I have been adding various unit tests to my project but I am looking for suggestions/ideas on how to go about writing integration tests.</p><p>My project mostly entails reading from SQS, batching data based on some parameters and then writing the output to s3. probably, a very common pattern. Extending that the service reads from various SQS and batching is localised to one queue. So 10 queue creats 10 different outputs.</p><p>I am using localstack for development. I am not looking for examples of exactly the above use case but something similar that is interaction with db/external system and then giving some output would also do.</p>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Writing a file system in Go -- not FUSE, but a real FS","url":"https://www.reddit.com/r/golang/comments/1ivuz61/writing_a_file_system_in_go_not_fuse_but_a_real_fs/","date":1740263514,"author":"/u/Rich-Engineer2670","guid":9377,"unread":true,"content":"<p>I would say I'm crazy, but this both well established and redundant.....</p><p>Assume I wanted to write my own file system (education), with Golang -- not a fuse variant, but I literally am taking a file on a block device and treating it as a disk. If this were C, OK, I'd do the following:</p><ul><li>Define a binary boot block at LBA 0</li><li>Define a certain number of LBAs for boot code</li><li>Define a certain number of LBAs for partitions</li><li>Within each partition define the directories and free lists (FATs, clusters, etc...)</li><li>Have a bunch of free LBAs.</li></ul><p>In C, I could define structs and just write them out assuming they were packed. In Go, structs aren't C structs, so I need to constantly convert structs to binaries. Sure, I could use the binary package and a lot functions, but someone must have done this in a better way, or is the \"better way\" \"No, write your file systems in C....\"</p><p>I want to stay in Go, because everything else in the OS is in Go...</p>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talos on IPv6 only network?","url":"https://www.reddit.com/r/kubernetes/comments/1ivumee/talos_on_ipv6_only_network/","date":1740262538,"author":"/u/Moleventions","guid":9371,"unread":true,"content":"<div><p>Does anyone know if you can deploy Talos on an IPv6 only network in AWS?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Moleventions\"> /u/Moleventions </a>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why K8s when there‚Äôs k3s with less resource requirements?","url":"https://www.reddit.com/r/kubernetes/comments/1ivu87n/why_k8s_when_theres_k3s_with_less_resource/","date":1740261469,"author":"/u/Crafty0x","guid":9321,"unread":true,"content":"<div><p>I don‚Äôt get why a business will run the more demanding k8s instead of k3s. What could possibly be the limitations of running k3s on full fledged servers.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Crafty0x\"> /u/Crafty0x </a>","contentLength":186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Golang SQLite admin tool","url":"https://github.com/joelseq/sqliteadmin-go","date":1740258532,"author":"/u/lAdddd","guid":9320,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1ivt55s/golang_sqlite_admin_tool/"},{"title":"DeepSeek Founders Are Worth $1 Billion or $150 Billion Depending Who You Ask","url":"https://www.bloomberg.com/news/articles/2025-02-10/deepseek-could-make-founder-liang-wenfeng-one-of-the-world-s-richest-people?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTczOTIzNzk1NywiZXhwIjoxNzM5ODQyNzU3LCJhcnRpY2xlSWQiOiJTUjhYTTdUMEcxS1cwMCIsImJjb25uZWN0SWQiOiI0MUVGMDc3MjI0RTM0MDhFOTNFMDdFQkY0RDc3QzI1QiJ9.kqtC_AK59CyhVfXIjYbRqB5ymi-WS52icc0pzlfX74E","date":1740256369,"author":"/u/cramdev","guid":9391,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ivsbro/deepseek_founders_are_worth_1_billion_or_150/"},{"title":"Solving The Millionaires' Problem in Rust","url":"https://vaktibabat.github.io/posts/smpc_circuits/","date":1740255860,"author":"/u/vaktibabat","guid":9370,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ivs4vp/solving_the_millionaires_problem_in_rust/"},{"title":"Official /r/rust \"Who's Hiring\" thread for job-seekers and job-offerers [Rust 1.85]","url":"https://www.reddit.com/r/rust/comments/1ivrkhs/official_rrust_whos_hiring_thread_for_jobseekers/","date":1740254374,"author":"/u/DroidLogician","guid":9425,"unread":true,"content":"<p>Welcome once again to the official <a href=\"https://www.reddit.com/r/rust\">r/rust</a> Who's Hiring thread!</p><p>Before we begin, job-seekers should also remember to peruse the <a href=\"https://www.reddit.com/r/rust/comments/1hynsw7/official_rrust_whos_hiring_thread_for_jobseekers/\">prior thread</a>.</p><p>This thread will be periodically stickied to the top of <a href=\"https://www.reddit.com/r/rust\">r/rust</a> for improved visibility. You can also find it again via the \"Latest Megathreads\" list, which is a dropdown at the top of the page on new Reddit, and a section in the sidebar under \"Useful Links\" on old Reddit.</p><p>The thread will be refreshed and posted anew when the next version of Rust releases in six weeks.</p><p>Please adhere to the following rules when posting:</p><ul><li><p>Don't create top-level comments; those are for employers.</p></li><li><p>Feel free to reply to top-level comments with on-topic questions.</p></li><li><p>Anyone seeking work should reply to my stickied top-level comment.</p></li><li><p>Meta-discussion should be reserved for the distinguished comment at the very bottom.</p></li></ul><ul><li><p><strong>The ordering of fields in the template has been revised to make postings easier to read. If you are reusing a previous posting, please update the ordering as shown below.</strong></p></li><li><p><strong>Remote positions: see bolded text for new requirement.</strong></p></li><li><p>To find individuals seeking work, see the replies to the stickied top-level comment; you will need to click the \"more comments\" link at the bottom of the top-level comment in order to make these replies visible.</p></li><li><p>To make a top-level comment you must be hiring directly; no third-party recruiters.</p></li><li><p>One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.</p></li><li><p>Proofread your comment after posting it and edit it if necessary to correct mistakes.</p></li><li><p>To share the space fairly with other postings and keep the thread pleasant to browse, we ask that you try to limit your posting to either 50 lines or 500 words, whichever comes first.<strong>We reserve the right to remove egregiously long postings.</strong> However, this only applies to the content of this thread; you can link to a job page elsewhere with more detail if you like.</p></li><li><p>Please base your comment on the following template:</p></li></ul><p>COMPANY: <em>[Company name; optionally link to your company's website or careers page.]</em></p><p>TYPE: <em>[Full time, part time, internship, contract, etc.]</em></p><p>LOCATION: <em>[Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.]</em></p><p>REMOTE: <em>[Do you offer the option of working remotely? <strong>Please state clearly if remote work is restricted to certain regions or time zones, or if availability within a certain time of day is expected or required.</strong>]</em></p><p>VISA: <em>[Does your company sponsor visas?]</em></p><p>DESCRIPTION: <em>[What does your company do, and what are you using Rust for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.]</em></p><p>ESTIMATED COMPENSATION: <em>[Be courteous to your potential future colleagues by attempting to provide at least a rough expectation of wages/salary. If you are listing several positions in the \"Description\" field above, then feel free to include this information inline above, and put \"See above\" in this field.<p> If compensation is negotiable, please attempt to provide at least a base estimate from which to begin negotiations. If compensation is highly variable, then feel free to provide a range.</p> If compensation is expected to be offset by other benefits, then please include that information here as well. If you don't have firm numbers but do have relative expectations of candidate expertise (e.g. entry-level, senior), then you may include that here.<p> If you truly have no information, then put \"Uncertain\" here.</p> Note that many jurisdictions (including several U.S. states) <strong>require salary ranges on job postings by law</strong>. If your company is based in one of these locations or you plan to hire employees who reside in any of these locations, you are likely subject to these laws.<p> Other jurisdictions may require salary information to be available upon request or be provided after the first interview.</p> To avoid issues, <strong>we recommend all postings provide salary information</strong>. You  state clearly in your posting if you are planning to compensate employees partially or fully in <strong>something other than fiat currency</strong> (e.g. cryptocurrency, stock options, equity, etc). Do  put just \"Uncertain\" in this case as the default assumption is that the compensation will be 100% fiat. Postings that fail to comply with this addendum . Thank you.]</em></p><p>CONTACT: <em>[How can someone get in touch with you?]</em></p>","contentLength":4390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I‚Äôve made a bunch of free wallpapers","url":"https://www.reddit.com/r/linux/comments/1ivqqg0/ive_made_a_bunch_of_free_wallpapers/","date":1740252187,"author":"/u/Folium_Creations","guid":9300,"unread":true,"content":"<p>I‚Äôve made a whole bunch of wallpapers and released them under CC:BY </p><p>I have made a git where I have uploaded, and will continue to upload them in 4k resolution as .png files for your convenience. I can‚Äôt stand all those ‚Äúwe have free wallpapers, as long as you register with email,phone number and the blood of your first born.‚Äù Here is the link to the git. I‚Äôm slowly building up a curated library of wallpapers I‚Äôve created. </p>","contentLength":438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What are the odds that Rust is going to have a real competitor?","url":"https://www.reddit.com/r/rust/comments/1ivqkj1/what_are_the_odds_that_rust_is_going_to_have_a/","date":1740251759,"author":"/u/nikitarevenco","guid":9369,"unread":true,"content":"<p>By \"Real Competitor\" I mean: A language just like Rust with similar goals, but one that people actually prefer to Rust. So it would be a fast, low-level memory safe language with great tooling, great type system and other benefits that Rust offers. But it would need to be better than Rust to actually catch on</p><p>This language needs to offer real advantages over Rust to be considered. Of course since Rust has a huge ecosystem that is growing rapidly, it may take a long time. But I am talking on a timescale of 25+ years.</p><p>Creating a new programming language to compete with Rust would be a massive undertaking and there would have to be some real reason to do it. Rust may be missing some features like higher-kinded types, named function arguments and such but to really catch on the language would need to offer some extremely important feature that Rust doesn't have, as well as offering all of Rust's benefits at the same time.</p><p>Is there any such language currently in early development? Or perhaps, what would such a language have to look like?</p>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scrap Your ORM‚ÄîReplacing Your ORM With Relational Algebra","url":"https://youtu.be/SKXEppEZp9M?si=wccXwllXm-0M-zOO","date":1740249559,"author":"/u/JohnyTex","guid":9394,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivppbh/scrap_your_ormreplacing_your_orm_with_relational/"},{"title":"FFmpeg School of Assembly Language","url":"https://github.com/FFmpeg/asm-lessons/blob/main/lesson_01/index.md","date":1740247207,"author":"/u/mitousa","guid":9265,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivorhb/ffmpeg_school_of_assembly_language/"},{"title":"[R] Interpreting Deep Neural Networks: Memorization, Kernels, Nearest Neighbors, and Attention","url":"https://medium.com/@thienhn97/interpreting-deep-neural-networks-memorization-kernels-nearest-neighbors-and-attention-6bf0cefc7619","date":1740244549,"author":"/u/ThienPro123","guid":9301,"unread":true,"content":"<p>This means that our positive kernel is actually some inner product of a Hilbert space. Typically, Mercer‚Äôs theorem is used for the kernel trick where we can map our input data to richer feature spaces that are potentially infinite dimensional (e.g. Gaussian kernel, polynomial kernel, etc.). However, in our case, we will use it to interpret the other way around.</p><p>Note the following relationship between distance in the Hilbert space and the kernel function:</p><p>This means that the closer x is to y in H , the more similar they will be in our similarity measure. So our intuition of the similarity measure being related to some form of distance is formalized by the relationship above.</p><h2>Learned kernel instead of fixing a kernel a priori</h2><p>If something within our prediction model is not learnable, then it is a prior that we are imposing on the dataset and the problem.</p><p>In our previous discussions of soft-kernelized NNs, the kernel K is fixed, meaning that we have some prior on the geometry of the data. That is not always desirable and we want our methods to automatically learn the structure of the data rather than us manually imposing this geometry.</p><p>Hence, if we want to learn the kernel K instead of imposing a prior fixed kernel, we can write (due to Mercer‚Äôs theorem):</p><p>for some parameterized feature map œà : X ‚Üí H from the data space X to some Hilbert space H. Typically, H will just be R^n or the dimension of the latent space. We can then learn the parameters of œà via standard training techniques (i.e. gradient descent on some loss).</p><p>This view allows us to connect standard deep learning (or representation learning) to kernel learning.</p><h2>Interpreting attention as soft nearest neighbors</h2><p>Note that the soft kernelized nearest neighbor that we presented earlier</p><p>can be interpreted as the popular attention mechanism that is ubiquitous today in LLMs and LVMs via the transformers architecture.</p><p>If we interpret x as some token, e.g. x_c, in the sequence (x_1, ‚Ä¶, x_n), K(x_i , x) as the attention dot product i.e.</p><p>and setting W_{ci} as the normalized values for token at time i i.e.</p><p>then we would recover the attention computation (bidirectional or autoregressive depending on whether we set the W_{ci} = 0 for i &lt; c) as being the weighted average of the values of other tokens in the sequence.</p><p>The representer theorem states that there exists an optimal linear solution that lies in the span of the training data. We shall call span (œà(x_1), ‚Ä¶, œà (x_n)) the <em>training (examples) feature span.</em></p>","contentLength":2494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/MachineLearning/comments/1ivnp1c/r_interpreting_deep_neural_networks_memorization/"},{"title":"i made a list of Tech EU tech projects! for users interested in privacy and sustainability","url":"https://github.com/uscneps/Awesome-European-Tech","date":1740244216,"author":"/u/uscnep","guid":9264,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivnk77/i_made_a_list_of_tech_eu_tech_projects_for_users/"},{"title":"I can play a game that wasn't meant to run on my PC, using Linux","url":"https://www.reddit.com/r/linux/comments/1ivn4kr/i_can_play_a_game_that_wasnt_meant_to_run_on_my/","date":1740243151,"author":"/u/Vousch","guid":9263,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gitoxide in February","url":"https://github.com/GitoxideLabs/gitoxide/discussions/1855","date":1740242874,"author":"/u/ByronBates","guid":9395,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1ivn0m4/gitoxide_in_february/"},{"title":"I made an MMORPG playable with an API. Use any programming language to control your characters with the API.","url":"https://www.artifactsmmo.com/","date":1740241440,"author":"/u/Muigetsu","guid":9211,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivmgf2/i_made_an_mmorpg_playable_with_an_api_use_any/"},{"title":"windows firewall for local Go web app","url":"https://www.reddit.com/r/golang/comments/1ivmbtd/windows_firewall_for_local_go_web_app/","date":1740241115,"author":"/u/jeevanism","guid":9298,"unread":true,"content":"<p>Every time I start my Go web app locally on Windows, I get a firewall error (see screenshot). The Windows Firewall blocks it, and I have to manually allow access. Why does this keep happening? Is there a way to fix this permanently?</p><p>NB : I am unable to attach the screenshot here :( </p>","contentLength":282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to implement dynamic storage provisioning for onPrem cluster","url":"https://www.reddit.com/r/kubernetes/comments/1ivlitx/how_to_implement_dynamic_storage_provisioning_for/","date":1740239019,"author":"/u/Impossible_Nose_2956","guid":9237,"unread":true,"content":"<p>Hi I have setup Onprem cluster for dev qa and preprod environments onPrem.</p><p>And I use redis, rabbitmq, mqtt, sqlite(for celery) in the cluster. And all these need persistent volumes.</p><p>Without dynamic provisioning, i have to create a folder, then create pv with node affinity and then create pvc and assign it to the statefulset.</p><p>I dont want to handle PVs for my onPrem clusters.</p><p>What options are available?</p><p>Do let me know if my understanding of things is wrong anywhere. </p>","contentLength":464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DSBG, an open-source static site generator built with Go","url":"https://www.reddit.com/r/golang/comments/1ivl3o0/dsbg_an_opensource_static_site_generator_built/","date":1740237882,"author":"/u/tarjano","guid":9209,"unread":true,"content":"<p>This is my first big project built in Go, primarily to see if I could be as productive with it as I am with Python. I wanted to tackle a non-trivial project, so I aimed to include most of the functionality I envisioned for this type of tool. Here's what DSBG offers:</p><ul><li> Works with both Markdown and HTML source files.</li><li><strong>Automatic Tagging &amp; Filtering:</strong> Tags are generated from paths, with built-in tag filtering.</li><li><strong>Client-Side Fuzzy Search:</strong> Provides fast search over all content within the browser.</li><li><strong>Automatic RSS Feed Generation:</strong> Easily create RSS feeds for your blog.</li><li><strong>Watch Mode with Auto-Rebuild:</strong> For continuous feedback during development.</li><li> Includes 3 different themes, with the option to add your own custom CSS.</li><li> For major social networks.</li><li> Easily add analytics, comments, and more.</li></ul><p>The code might not be perfectly idiomatic, so any tips, suggestions, and feedback are very welcome!</p>","contentLength":870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is Saga Pattern in Distributed Systems?","url":"https://newsletter.scalablethread.com/p/what-is-saga-pattern-in-distributed","date":1740235399,"author":"/u/scalablethread","guid":9234,"unread":true,"content":"<p><a href=\"https://newsletter.scalablethread.com/i/146489166/consistency-and-consistency-model\" rel=\"\">data consistency</a></p><p>The Saga pattern is a design pattern that helps manage transaction updates across multiple services by breaking them down into a sequence of small local transactional updates, called \"saga steps\" or \"subtransactions.\" Each step represents a unit of work that interacts with a single service. Once a step is completed, it triggers the next step in the sequence. If any step fails, the saga executes compensating updates to undo the changes made by the previous steps, ensuring that the system returns to its initial state.</p><p>There are two main approaches to implementing the Saga Pattern: Orchestration and Choreography.</p><p>In this approach, a central orchestrator service coordinates the saga steps. The orchestrator tells each service when to execute its local transaction. It maintains the state of the saga and handles any failures by invoking compensating transactions. The orchestrator knows the entire saga flow.</p><p>The client initiates the saga by communicating with the orchestrator. The orchestrator then invokes the first service. Upon successful completion, the orchestrator moves to the next step, invoking the corresponding service. If a service fails, the orchestrator triggers compensating transactions in reverse order.</p><p>In the Choreography approach, there is no central coordinator. Instead, each service involved in the saga knows its role and communicates with the other services through events or messages. Each service listens for specific events and performs local transactions when the appropriate event is received. The saga flow is distributed across the services.</p><p>The client initiates the saga by communicating with the first service. This service performs its transaction and publishes an event. Other services, listening for this event, perform their respective transactions and publish their events. This chain reaction continues until the saga is complete. If a service fails, it publishes a compensating event, triggering other services to execute compensating transactions.</p><ul><li><p>Choreography has no single point of failure, as each service manages its part of the saga.</p></li><li><p>Orchestration provides simplified error handling and monitoring with centralized control. In contrast, each service needs to handle its errors in Choreography, which can lead to complex error-handling logic.</p></li><li><p>In Orchestration, the coordinator needs to know about all the services involved in the saga, which can lead to tight coupling. In contrast, in Choreography, services need to agree on the events and the order of transactions, which can lead to overhead in coordination.</p></li></ul><ul></ul><ul></ul><p><em>If you enjoyed this article, please hit the ‚ù§Ô∏è like button.</em></p><p><em>If you think someone else will benefit from this, then please üîÅ share this post.</em></p>","contentLength":2718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivk7x9/what_is_saga_pattern_in_distributed_systems/"},{"title":"[R] Calculating costs of fine tuning an Vision Language Model","url":"https://www.reddit.com/r/MachineLearning/comments/1ivjrwi/r_calculating_costs_of_fine_tuning_an_vision/","date":1740234081,"author":"/u/thekarthikprasad","guid":9235,"unread":true,"content":"<p>Hello guys, I need help in calculating the cost of fine-tuning a VL model.<p> My image dataset is of size 80+gb (</p><a href=\"https://huggingface.co/datasets/RussRobin/SpatialQA\">https://huggingface.co/datasets/RussRobin/SpatialQA</a>) The VL model is InternVL's 2B model<p> I am confused about whether to do a full parameter/QLoRA Finetuning.</p> I can't spend more on this, but wish to check the results.</p><p>If so I could, what would be the cost estimate, also how to estimate cost in general Can I sample the dataset, if it breaks my cost bound and still see the results?<p> Also do suggest the best and cheapest compute platform for my case.</p> Thanks in advance.</p>","contentLength":577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rustaceans, What are your thoughts on Gleam?","url":"https://www.reddit.com/r/rust/comments/1ivjcus/rustaceans_what_are_your_thoughts_on_gleam/","date":1740232848,"author":"/u/nikitarevenco","guid":9183,"unread":true,"content":"<p>I've been writing Rust for a couple months. I absolutely love its monads like Result and Option, pattern-matching, private-by-default, the friendly compiler and its type system. I took a quick look at Gleam and it seems to have those features as well. Its syntax heavily reminds me of Rust's, the major distinction is that Gleam is much higher level (No lifetimes, for example), and also it is a purely functional language. It is still relatively new.</p><p>For those who have tried it, what do you think about it? Are there situations where you will prefer Gleam over Rust and why. </p>","contentLength":576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Almost everyone is under-appreciating automated AI research","url":"https://www.reddit.com/r/artificial/comments/1ivja6c/almost_everyone_is_underappreciating_automated_ai/","date":1740232632,"author":"/u/MetaKnowing","guid":9233,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SystemV Filesystem Being Removed From The Linux Kernel","url":"https://www.phoronix.com/news/Removing-SystemV-Filesystem","date":1740229957,"author":"/u/unixbhaskar","guid":9181,"unread":true,"content":"<p>Michael Larabel is the principal author of Phoronix.com and founded the site in 2004 with a focus on enriching the Linux hardware experience. Michael has written more than 20,000 articles covering the state of Linux hardware support, Linux performance, graphics drivers, and other topics. Michael is also the lead developer of the Phoronix Test Suite, Phoromatic, and OpenBenchmarking.org automated benchmarking software. He can be followed via <a href=\"https://twitter.com/MichaelLarabel\">Twitter</a>, <a href=\"https://www.linkedin.com/in/michaellarabel/\">LinkedIn</a>, or contacted via <a href=\"https://www.michaellarabel.com/\">MichaelLarabel.com</a>.</p>","contentLength":500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ivifki/systemv_filesystem_being_removed_from_the_linux/"},{"title":"anyone tried kro for kubernetes resource management yet?","url":"https://www.reddit.com/r/kubernetes/comments/1ivhubw/anyone_tried_kro_for_kubernetes_resource/","date":1740227970,"author":"/u/AnnualRich5252","guid":9118,"unread":true,"content":"<p>i just came across this article on the new resource orchestrator for kubernetes called kro, and i think it's worth discussing here. for anyone who's been dealing with the ever-growing complexity of kubernetes deployments, kro could be a game changer. it simplifies how we manage and define complex kubernetes resources by grouping them into reusable units, making everything more efficient and predictable.</p><p>what i find cool about kro is that it focuses on making kubernetes resource management easier to handle, without needing the in-depth, advanced skills that most operators and devs have to rely on today. it's got this thing called a ResourceGraphDefinition (RGD) which essentially lets you define and manage resources as a unit, and it‚Äôs smart enough to figure out deployment sequences automatically based on dependencies. really takes the guesswork out of it.</p><p>it‚Äôs worth noting kro isn‚Äôt trying to replace helm or kustomize directly, but it definitely offers a more structured and predictable approach, with better handling of CRD upgrades and dependencies. while helm has been a go-to for packaging, kro's approach might be more useful for teams looking for a more secure, governed way to manage kubernetes resources at scale.</p><p>looking forward to hearing your thoughts!</p>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Efficiency Paradox: How to Save Yourself & the World ‚Ä¢ Holly Cummins","url":"https://youtu.be/dU_WHead0oY","date":1740227380,"author":"/u/goto-con","guid":9507,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivhol0/the_efficiency_paradox_how_to_save_yourself_the/"},{"title":"API Application Monitoring - OpenTelemetry? Or something else?","url":"https://www.reddit.com/r/golang/comments/1ivhm7y/api_application_monitoring_opentelemetry_or/","date":1740227138,"author":"/u/_nullptr_","guid":9180,"unread":true,"content":"<p>I am writing a few different gRPC and HTTP (via gRPC Gateway) API servers for various heavy financial compute/IO operations (trading systems and market data). I am doing this as a single developer. These are mostly for me as a hobbyist, but may become commercial/cloud provided at some point with a nice polished UI frontend.</p><p>Given the nature of the applications, I want to know what is \"going on\" and be able to troubleshoot performance bottlenecks as they arise, see how long transactions take, etc. I want to standardize the support for this into my apiserver package so all my apps can leverage and it isn't an afterthought. That said, I don't want some huge overhead either, but just want to know the performance of my app when I want to (and not when I don't). I do think I want to instrument with logs, trace and metrics after thinking what each would give me in value.</p><p>Right now I am leaning towards just going full OpenTelemetry knowing that it is early and might not be fully mature, but that it likely will over time. I am thinking I will use stdlib  for logs with Otel handler only when needed else default to basic stdout handler. Do I want to use otel metrics/tracing directly? I am also thinking I want these others sent to a  handler by default (even stdout is too much noise), and only to a collector when configured at runtime. Is that possible with the Go Otel packages? Does this seem like the best strategy? How does stdlib  play into this? or doesn't it? Other ideas?</p>","contentLength":1487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's a good combination of tools to get a proper application observation solution together?","url":"https://www.reddit.com/r/kubernetes/comments/1ivh9co/whats_a_good_combination_of_tools_to_get_a_proper/","date":1740225819,"author":"/u/tofagerl","guid":9117,"unread":true,"content":"<p>I work for a company with tons of k8s clusters, but they haven't really got the whole \"let's provide all the benefits of this to the product teams\" together yet, so we're stuck with a basic Grafana + Kibana package for now. That's fine, it works. </p><p>But since I used to work with Anthos, I got used to getting the full tracing benefits from Anthos Service Mesh, and I really miss having that. </p><p>So now I'd like to pressure the infra teams to provide something better for us, but I can't just say \"use Anthos Service Mesh\", because they are already running on GCS, so there'd be no point in using Anthos. Obviously they could use a normal Istio service mesh, but I'd like to know if there are easier solutions -- Service Meshes are complicated and come with serious drawbacks, and I'm really just looking for the observation layer, not the network security layer. </p><p>Keep in mind we prefer OSS solutions as a rule, and prefer non-managed solutions as a core philosophy because we believe in understanding each tool because we know it might break. </p>","contentLength":1038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI bans Chinese accounts using ChatGPT to edit code for social media surveillance","url":"https://www.engadget.com/ai/openai-bans-chinese-accounts-using-chatgpt-to-edit-code-for-social-media-surveillance-230451036.html","date":1740223463,"author":"/u/F0urLeafCl0ver","guid":9113,"unread":true,"content":"<p>OpenAI has banned the accounts of a group of Chinese users who had attempted to use ChatGPT to debug and edit code for an AI social media surveillance tool, the company <a data-i13n=\"cpos:1;pos:1\" href=\"https://openai.com/global-affairs/disrupting-malicious-uses-of-ai/\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:said Friday;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>. The campaign, which OpenAI calls Peer Review, saw the group prompt ChatGPT to generate sales pitches for a program those documents suggest was designed to monitor anti-Chinese sentiment on X, Facebook, YouTube, Instagram and other platforms. The operation appears to have been particularly interested in spotting calls for protests against human rights violations in China, with the intent of sharing those insights with the country's authorities.</p><p>\"This network consisted of ChatGPT accounts that operated in a time pattern consistent with mainland Chinese business hours, prompted our models in Chinese, and used our tools with a volume and variety consistent with manual prompting, rather than automation,\" said OpenAI. \"The operators used our models to proofread claims that their insights had been sent to Chinese embassies abroad, and to intelligence agents monitoring protests in countries including the United States, Germany and the United Kingdom.\"</p><p>According to Ben Nimmo, a principal investigator with OpenAI, this was the first time the company had uncovered an AI tool of this kind. \"Threat actors sometimes give us a glimpse of what they are doing in other parts of the internet because of the way they use our AI models,\" Nimmo told <a data-i13n=\"cpos:2;pos:1\" href=\"https://www.nytimes.com/2025/02/21/technology/openai-chinese-surveillance.html\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:The New York Times;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>.</p><p>Much of the code for the surveillance tool appears to have been based on an open-source version of one of Meta's <a data-i13n=\"cpos:3;pos:1\" href=\"https://www.engadget.com/ai/meta-says-llamas-usage-grew-tremendously-due-to-the-power-of-open-source-140020454.html\" data-ylk=\"slk:Llama models;cpos:3;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>. The group also appears to have used ChatGPT to generate an end-of-year performance review where it claims to have written phishing emails on behalf of clients in China.</p><p>\"Assessing the impact of this activity would require inputs from multiple stakeholders, including operators of any open-source models who can shed a light on this activity,\" OpenAI said of the operation's efforts to use ChatGPT to edit code for the AI social media surveillance tool.</p><p>Separately, OpenAI said it recently banned an account that used ChatGPT to generate social media posts critical of <a data-i13n=\"cpos:4;pos:1\" href=\"https://en.wikipedia.org/wiki/Cai_Xia\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:Cai Xia;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a>, a Chinese political scientist and dissident who lives in the US in exile. The same group also used the chatbot to generate articles in Spanish critical of the US. These articles were published by \"mainstream\" news organizations in Latin America and often attributed to either an individual or a Chinese company.</p>","contentLength":2411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1ivgo34/openai_bans_chinese_accounts_using_chatgpt_to/"},{"title":"I have KCA 50% coupun that i dont need, i will give to anyone who can give me aws aor redhat coupon in exchange?","url":"https://www.reddit.com/r/kubernetes/comments/1ivgiu6/i_have_kca_50_coupun_that_i_dont_need_i_will_give/","date":1740222832,"author":"/u/sabir8992","guid":9184,"unread":true,"content":"<div><p>If you have other exam i will give to anyone who can give me aws or Redhat coupon in exchange? DM ME Please</p></div>   submitted by   <a href=\"https://www.reddit.com/user/sabir8992\"> /u/sabir8992 </a>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distributed system courses in Rust?","url":"https://www.reddit.com/r/rust/comments/1ivgbko/distributed_system_courses_in_rust/","date":1740222035,"author":"/u/FeelingAttempt55","guid":9163,"unread":true,"content":"<p>I am currently following the <a href=\"https://github.com/pingcap/talent-plan/tree/master\">pingcap course</a> to learn distributed systems with Rust. So far, I am really enjoying the course, but the course is 5 years old, could you guys suggest some other project-based and more up-to-date courses? </p>","contentLength":233,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dockerize GO environment with only go.mod and go.sum and no source code","url":"https://www.reddit.com/r/golang/comments/1ivfxtb/dockerize_go_environment_with_only_gomod_and/","date":1740220358,"author":"/u/muthunatsharma","guid":9112,"unread":true,"content":"<p>I need a docker container which has go packages downloaded, installed and compiled as mentioned in go.mod and go.sum. All the articles show how to do it but the install/compile actually happens only when the source-code is copied in to the container and \"go build\" is run in the dockerfile.</p><p>I see \"go download\" downloads all pkgs in go.mod to /go/mod/pkg. How do I install these?<p> I can give \"go install &lt;pkg&gt;\" but that would mean I need to update my Dockerfile each time a new pkg is added to go.mod.</p></p><p>What is the one-shot way of installing it in the dockerfile build?</p><p>Edit: The context is to build a dev container where deps are pre-built saving time when code is mounted on the container and built -- this is the main point to save time. The container wouldn't have the app itself. Only the dependencies fully installed and serve as a standard environment to run.</p>","contentLength":861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"godoc.nvim - Golang docs inside Neovim!","url":"https://www.reddit.com/r/golang/comments/1ivfv16/godocnvim_golang_docs_inside_neovim/","date":1740220022,"author":"/u/ffredrikk","guid":9111,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PeaZip 10.3.0 released!","url":"https://www.reddit.com/r/linux/comments/1ivfn9y/peazip_1030_released/","date":1740219074,"author":"/u/peazip","guid":9368,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thought We Had Our EKS Upgrade Figured Out‚Ä¶ We Did Not","url":"https://www.reddit.com/r/kubernetes/comments/1ivf8u3/thought_we_had_our_eks_upgrade_figured_out_we_did/","date":1740217389,"author":"/u/rohit_raveendran","guid":9303,"unread":true,"content":"<p>You ever think you‚Äôve got everything under control, only for prod to absolutely humble you? Yeah, that was us.</p><ul><li>Lower environments? ‚úÖ Tested a bunch.</li><li>Version mismatches? ‚úÖ All within limits.</li><li>EKS addons? ‚úÖ Using the standard upgrade flow.</li></ul><p>So we run Terraform on upgrade day. Everything‚Äôs looking fine‚Äîuntil <strong>kube-proxy upgrade just straight-up fails.</strong> Some pods get stuck in  Great.</p><p>Cool, thanks, very helpful. We hadn‚Äôt changed anything on kube-proxy beyond the upgrade, so what the hell?</p><p>At this point, one of us starts frantically digging through the EKS docs while another engineer manually downgrades kube-proxy just to get things back up. That works, but obviously, we can‚Äôt leave it like that.</p><p>And then we find it: <strong>a tiny note in the AWS docs added just a few days ago.</strong> Turns out, <strong>kube-proxy 1.31 needs an ARMv8.2 processor with Cryptographic Extensions</strong> (<a href=\"https://github.com/awsdocs/amazon-eks-user-guide/blob/mainline/latest/ug/nodes/hybrid-nodes-os.adoc#arm\">link</a>).</p><p>And guess what Karpenter had spun up?  AWS confirmed that A1s are a no-go in EKS 1.31+. We updated our Karpenter configs to block them, ran the upgrade again, and boom‚Äîeverything worked.</p><ol><li><strong>You‚Äôre never actually prepared.</strong> We tested everything, but something always slips through. The real test is how fast you fix it.</li><li><strong>Karpenter is great, but don‚Äôt let it go rogue.</strong> We‚Äôre now explicitly blocking unsupported instance families.</li></ol><p>Anyway, if you guys have ever had one of those ‚Äúwe did everything right, and it still blew up‚Äù moments, drop your stories. Misery loves company.</p>","contentLength":1449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pewdiepie uses linux mint","url":"https://www.reddit.com/r/linux/comments/1ivf01w/pewdiepie_uses_linux_mint/","date":1740216313,"author":"/u/RedDevilVortex","guid":9059,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created a fairly extensive cheat sheet for scripting Sieve mail filters. Here's a link to the Gist if anyone is interested.","url":"https://gist.github.com/Hotrod369/6b7a24e1ea060e48e0c02459cbb950a0","date":1740214906,"author":"/u/StinkyPete312","guid":9210,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iveor2/i_created_a_fairly_extensive_cheat_sheet_for/"},{"title":"This is a minimalist 2-click MSI installer generator for your projects for Windows. Magic works as all you need is to populate _configMSI.yml with your own values, then click 2 bat or sh files (if you use MS Visual Studio or MSYS2/MINGW64). And voila, your MSI Installer is ready!","url":"https://github.com/windows-2048/Magic-MSI-Installer-Template","date":1740214284,"author":"/u/florida-haunted","guid":9081,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivejju/this_is_a_minimalist_2click_msi_installer/"},{"title":"Pixerise v0.12 Release: Python High-Performance 3D Renderer Adds Ray Casting, 1/z Depth Interpolation, and Group Management with Improved Architecture","url":"https://github.com/enricostara/pixerise","date":1740214075,"author":"/u/jumpixel","guid":9182,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivehr0/pixerise_v012_release_python_highperformance_3d/"},{"title":"Confused about \"NEW\" Rust feature in - Closures in async functions","url":"https://www.reddit.com/r/rust/comments/1ivdmek/confused_about_new_rust_feature_in_closures_in/","date":1740210338,"author":"/u/DataBora","guid":9302,"unread":true,"content":"<p>I am reading how new Rust feature is comming for using closures in Async functions. || async whaterver...</p><p>In my Elusion library implementation i have PipelineScheduler function signature:</p><pre><code> ```rust pub async fn new&lt;F, Fut&gt;(frequency: &amp;str, job: F) -&gt; ElusionResult&lt;Self&gt; where F: Fn() -&gt; Fut + Send + Sync + 'static, Fut: Future&lt;Output = ElusionResult&lt;()&gt;&gt; + Send + 'static ``` </code></pre><p>and then for Job creation:</p><pre><code>```rust let job = Job::new_async(&amp;cron, move |uuid, mut l| { let job_fn = job_fn.clone(); Box::pin(async move { let future = job_fn(); future.await.unwrap_or_else(|e| eprintln!(\"‚ùå Job execution failed: {}\", e)); let next_tick = l.next_tick_for_job(uuid).await; match next_tick { Ok(Some(ts)) =&gt; println!(\"Next job execution: {:?} UTC Time\", ts), _ =&gt; println!(\"Could not determine next job execution\"), } }) }).map_err(|e| ElusionError::Custom(format!(\"‚ùå Job creation failed: {}\", e)))?; ``` </code></pre><p>which user can use like this:</p><pre><code>let scheduler = PipelineScheduler::new(\"5min\", || async {}) </code></pre><p>How this new feature will be different?</p>","contentLength":1025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] Evaluating LLM Knowledge Across 285 Graduate Disciplines: A Comprehensive Benchmark Using Human-LLM Collaborative Filtering","url":"https://www.reddit.com/r/MachineLearning/comments/1ivd069/r_evaluating_llm_knowledge_across_285_graduate/","date":1740207761,"author":"/u/Successful-Western27","guid":9236,"unread":true,"content":"<p>A new evaluation benchmark tests language models across 285 graduate-level disciplines using an iterative human-AI collaborative approach to generate and validate questions. The methodology combines expert review with model-assisted filtering to ensure high-quality, discipline-appropriate assessment.</p><p>Key technical points: - Uses a two-stage question generation process: initial AI generation followed by expert review - Implements collaborative filtering where both human experts and LLMs help identify and remove problematic questions - Covers disciplines from traditional academia to specialized industrial fields - Tests both factual knowledge and reasoning capabilities - Evaluated on multiple leading LLMs including GPT-4, Claude 2, and DeepSeek</p><p>Results: - Best performance: DeepSeek-R1 at 61.82% accuracy - Significant variance in performance across different disciplines - 80+ expert annotators involved in validation - Generated dataset of 2,855 validated questions</p><p>I think this benchmark addresses a critical gap in LLM evaluation by going beyond common academic subjects. The methodology of combining human expertise with AI assistance for question validation could be valuable for developing future evaluation datasets.</p><p>I think the relatively modest performance (62%) on graduate-level questions across diverse fields suggests current LLMs still have significant room for improvement in specialized domains. This could influence how we approach model training and evaluation for domain-specific applications.</p><p>TLDR: New benchmark tests LLMs across 285 graduate disciplines using human-AI collaborative question generation. Best model achieved 62% accuracy, revealing gaps in specialized knowledge.</p>","contentLength":1704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week in Plasma: Refinements All Around","url":"https://blogs.kde.org/2025/02/22/this-week-in-plasma-refinements-all-around/","date":1740207215,"author":"/u/gabriel_3","guid":9080,"unread":true,"content":"<p>Welcome to a new issue of \"This Week in Plasma\"! Every week we cover as much as possible of what's happening in the world of KDE Plasma and its associated apps like Discover, System Monitor, and more.</p><p>This week, we've been rapidly fixing the bugs that people found in Plasma 6.3, as well as some older bugs as well. In addition to that, some smaller UI improvements have started to trickle in! There's some larger work in progress too, but not merged yet. Have a look at what did merge this week:</p><p>Improved the weather widget's display of search results from the BBC weather service to reduce unhelpful visual noise. (Ismael Asensio, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500065\">link</a>)</p><p>Eliminated the visual difference between how Night Light looks on Wayland compared to on X11. (Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500300\">link</a>)</p><p>The Digital Clock widget's context menu is now less cluttered with things you're not likely to use. (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5139\">link</a>)</p><p>Rephrased some settings on System Settings' General Behavior page to be clearer about what it is that they actually do. (Nate Graham, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2808\">link</a>)</p><p>Improved the accessibility of the Widget Explorer sidebar. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2807\">link</a>)</p><p>Fixed the issue mentioned last week where KWin built with LTO on GCC 15 could show a black screen on login when using an ICC profile; we found a way to restructure the code that avoids the issue. (Vlad Zahorodnii and Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499789\">link</a>)</p><p>Fixed a case where Plasma could crash when you tried to access the Properties dialog for a file in the Recently or Frequently Used file lists in the Kickoff Application Launcher. (Nicolas Fella, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499845\">link</a>)</p><p>Fixed a regression that caused the volume change OSD to fail to appear when adjusting the volume with the integrated volume buttons of a Bluetooth headset. (David Redondo, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500129\">link</a>)</p><p>Fixed a regression that caused the + clipboard popup to lose its visual highlights when navigated by keyboard. (Christoph Wolk, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500055\">link</a>)</p><p>Fixed an issue in KWin that caused the new \"Prefer efficiency\" option when using an ICC profile to not actually be very efficient on some hardware, and another one that broke Night Light while using the \"Prefer color accuracy\" setting. (Xaver Hugl, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499987\">link 1</a> and <a href=\"https://bugs.kde.org/show_bug.cgi?id=500404\">link 2</a>)</p><p>Taking screenshots on Wayland in FreeBSD now works. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500261\">link</a>)</p><p>Fixed a few bugs in the Color Picker widget, such as the shortcut option not working, and the tooltip not looking correct in certain circumstances. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/676\">link 1</a> and <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/677\">link 2</a>)</p><p>Fixed a bug with the Task Manager widgets that broke the ability to move the pointer diagonally to a tooltip without dismissing it by accident while using a right-to-left language like Arabic or Hebrew. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2792\">link</a>)</p><p>Made several improvements and fixes for keyboard navigation in the Kicker Application Menu widget. (Christoph Wolk, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2811\">link 1</a>, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2812\">link 2</a>, and <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2813\">link 3</a>)</p><p>Fixed a regression that caused desktop icons selected by dragging a box around them to become inappropriately deselected if the pointer ended right over one of the icons when releasing the mouse button. (David Edmundson, <a href=\"https://bugs.kde.org/show_bug.cgi?id=499898\">link</a>)</p><p>Fixed a regression that caused the automatic tablet mode feature to accidentally get blocked on certain types of devices, but only when using the feature to re-bind mouse buttons. (Vlad Zahorodnii, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500025\">link</a>)</p><p>Fixed a bug that caused the desktop and panels to go missing when applying a new Global Theme and using the option to replace the existing layout. This also fixed a bug that caused deleted widgets to not be deleted from the <code>plasma-org.kde.plasma.desktop-appletsrc</code> config file. (Marco Martin, <a href=\"https://bugs.kde.org/show_bug.cgi?id=498175\">link 1</a> and <a href=\"https://bugs.kde.org/show_bug.cgi?id=404641\">link 2</a>)</p><p>Fixed a set of subtle bugs in the implementation of the new \"prefer symbolic icons\" behavior of the System Tray that caused it to actually do the opposite, showing you colorful icons instead! (Nate Graham and David Redondo, <a href=\"https://invent.kde.org/plasma/plasma-workspace/-/merge_requests/5227\">link 1</a> and <a href=\"https://invent.kde.org/frameworks/kiconthemes/-/merge_requests/175\">link 2</a>)</p><p>Extremely long weather station names no longer overflow and break the widget popup's layout. (Ismael Asensio, <a href=\"https://invent.kde.org/plasma/kdeplasma-addons/-/merge_requests/680\">link</a>)</p><p>The inline file renaming text field on the desktop is now colored correctly when using a mixed light/dark setup, as with Breeze Twilight. (Evgeniy Harchenko, <a href=\"https://invent.kde.org/plasma/plasma-desktop/-/merge_requests/2829\">link</a>)</p><p>Limited the Power Management setting \"Change screen brightness\" to only take effect for built-in screens on battery-powered systems (e.g. laptops), which avoids certain timing-related brightness bugs for external monitors and makes the settings page less confusing. (Jakob Petsovits, <a href=\"https://bugs.kde.org/show_bug.cgi?id=498771\">link</a>)</p><p>Fixed an issue that could cause user switching from KRunner to behave strangely and eventually cause a crash. (David Edmundson, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500038\">link</a>)</p><p>Fixed an older regression that broke the \"highlight non-default settings\" features for pages in System Settings written using QtWidgets. The fact that this was overlooked for so long goes to show how few are left these days! (David Redondo, <a href=\"https://invent.kde.org/frameworks/kcmutils/-/merge_requests/257\">link</a>)</p><p>Switched KWin's render loop initialization code to use a more precise type of timer that should reduce frame drops. (Apostolos Dimitromanolakis, <a href=\"https://bugs.kde.org/show_bug.cgi?id=500500\">link</a>)</p><p>When the  daemon crashes, now it automatically restarts itself in the background. (Bryan Liang, <a href=\"https://invent.kde.org/frameworks/kded/-/merge_requests/57\">link</a>)</p><p>KDE has become important in the world, and your time and contributions have helped us get there. As we grow, we need your support to keep KDE sustainable.</p><p>You can help KDE by becoming an active community member and <a href=\"https://community.kde.org/Get_Involved\">getting involved</a> somehow. Each contributor makes a huge difference in KDE ‚Äî you are not a number or a cog in a machine!</p><p>You don‚Äôt have to be a programmer, either. Many other opportunities exist:</p><p>You can also help us by <a href=\"https://kde.org/donate\">making a donation!</a> Any monetary contribution ‚Äî however small ‚Äî will help us cover operational costs, salaries, travel expenses for contributors, and in general just keep KDE bringing Free Software to the world.</p><p>Enter your email address to follow this blog and receive notifications of new posts by email.</p>","contentLength":5644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ivcuy0/this_week_in_plasma_refinements_all_around/"},{"title":"I Made a Configurable Rate Limiter‚Ä¶ Because APIs Can‚Äôt Say ‚ÄòChill‚Äô","url":"https://beyondthesyntax.substack.com/p/i-made-a-configurable-rate-limiter?r=4jgehp&amp;utm_campaign=post&amp;utm_medium=web&amp;triedRedirect=true","date":1740205486,"author":"/u/Sushant098123","guid":8987,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivcfct/i_made_a_configurable_rate_limiter_because_apis/"},{"title":"I have made a pong game in C++ using raylib. So can anyone plz give suggession where I can improve the game and my code?","url":"https://github.com/EthicalAniruddha/AI-Pong","date":1740204102,"author":"/u/Ethical_Aniruddha","guid":8986,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1ivc2qa/i_have_made_a_pong_game_in_c_using_raylib_so_can/"},{"title":"One-Minute Daily AI News 2/21/2025","url":"https://www.reddit.com/r/artificial/comments/1ivbt3a/oneminute_daily_ai_news_2212025/","date":1740203110,"author":"/u/Excellent-Target-847","guid":9262,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Excellent-Target-847\"> /u/Excellent-Target-847 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generic Bitfield I had fun implementing","url":"https://gist.github.com/oplanre/de0bba4f1e2f769458ca1adff7f12280","date":1740197539,"author":"/u/ln3ar","guid":9162,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iva8up/generic_bitfield_i_had_fun_implementing/"},{"title":"Announcing async-local 3.0 now with async closure support","url":"https://www.reddit.com/r/rust/comments/1iv8o6v/announcing_asynclocal_30_now_with_async_closure/","date":1740192335,"author":"/u/Jester831","guid":9060,"unread":true,"content":"<p>Async-local enables thread locals to be used in an async context across await points or within blocking threads managed by the Tokio runtime without the overhead of `Arc`. The way this is accomplished is by using <a href=\"https://crates.io/crates/generativity\">generativity</a> to create unique invariant lifetimes so that borrows to TLS can't be coerced to a `&amp;'static` lifetime and by configuring the runtime with a barrier to rendezvous worker threads during shutdown. This shutdown barrier makes it such that runtime tasks never outlive TLS data owned by worker threads; this makes every invariant lifetime guaranteed to be valid until no tasks remain. Blocking threads managed by the Tokio runtime cannot outlive worker threads with this configuration, and so pointers to TLS from worker threads can be safely moved to these blocking threads with the lifetime constrained. As the lifetimes cannot be coerced into `&amp;'static`, moving onto other threads is prevented. This crate downgrades to using `Arc` whenever the `barrier-protected-runtime` feature is not enabled, making it the end users choice to opt into this optimization by using async_local to configure the runtime shutdown barrier. </p>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"from nodejs want to move to golang","url":"https://www.reddit.com/r/golang/comments/1iv7ngg/from_nodejs_want_to_move_to_golang/","date":1740189188,"author":"/u/Spirited-Item1431","guid":8971,"unread":true,"content":"<p>I used to be a web developer who used Node.js as my daily programming language, but now I'm interested in switching to Golang. Aside from the usual fundamentals, what are the most important things to learn in Golang?</p>","contentLength":216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ring is unmaintained","url":"https://rustsec.org/advisories/RUSTSEC-2025-0007.html","date":1740185098,"author":"/u/technobicheiro","guid":8934,"unread":true,"content":"<p>This advisory has been withdrawn and should be ignored. It is kept only for reference.</p><p>The author has announced an indefinite hiatus in its development, noting that\nany reported security vulnerabilities may go unaddressed for prolonged periods\nof time.</p><p>After this advisory was published, the author graciously agreed to give\naccess to the rustls team. The rustls team is committed to providing\nsecurity (only) maintenance for  for the foreseeable future.</p><p>Advisory available under <a href=\"https://spdx.org/licenses/CC0-1.0.html\">CC0-1.0</a>\n    license.\n\n    \n    </p>","contentLength":508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iv6myf/ring_is_unmaintained/"},{"title":"[P] Decensor AI models Qwen/Deepseek by finetuning with non political data","url":"https://www.reddit.com/r/MachineLearning/comments/1iv6ckk/p_decensor_ai_models_qwendeepseek_by_finetuning/","date":1740184249,"author":"/u/Ambitious_Anybody855","guid":9114,"unread":true,"content":"<p>The best way to decensor a DeepSeek model? Don‚Äôt try to decensor it.</p><p>Fine-tuned OpenThinker on OpenThoughts-114k, a dataset focused on reasoning tasks like math, coding, and graduate-level Q&amp;A, with no political content. Despite using censored base models (Qwen), the fine-tuned OpenThinker-7B and OpenThinker-32B models became decensored without any explicit intervention. Unlike Perplexity, no custom fine-tuning was applied to remove censorship, yet the results remain uncensored. </p><p>It challenges assumptions about model safety and opens exciting new research directions. AI game is so on</p>","contentLength":590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Firefox's HEVC support for Linux (via VA-API) coming in Firefox 137","url":"https://www.reddit.com/r/linux/comments/1iv6bhi/firefoxs_hevc_support_for_linux_via_vaapi_coming/","date":1740184159,"author":"/u/neks101","guid":8955,"unread":true,"content":"<p>Windows got support in Firefox 134, MacOS on the Firefox beta build 136, and Linux will be on the Firefox nightly with 137. Looks like all OS will be supported by 137!</p>","contentLength":167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"First time on Linux, 3 gig ram and works like a rocket lol","url":"https://www.reddit.com/r/linux/comments/1iv5tas/first_time_on_linux_3_gig_ram_and_works_like_a/","date":1740182726,"author":"/u/SnooOpinions7428","guid":8898,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightweight Real-Time System Stats for VS Code","url":"https://marketplace.visualstudio.com/items?itemName=odangoo.otak-monitor","date":1740181177,"author":"/u/Wise_Bug47","guid":8942,"unread":true,"content":"<p align=\"center\">A lightweight system monitor for VS Code - Track CPU, memory, and disk usage with efficient 5-second updates and 1-minute averages.</p><ol><li>Find the system monitor in your VS Code status bar</li><li>View CPU usage percentage</li><li>Hover to see detailed current and average metrics</li></ol><p>otak-monitor is a lightweight VS Code extension that helps you monitor system resources without leaving your editor.</p><ul><li><ul><li>Status bar display of CPU usage percentage</li><li>Aggregated across all CPU cores</li><li>Precise to one decimal place</li><li>Current CPU clock speed (MHz)</li></ul></li><li><ul><li>Detailed memory information</li><li>Shows used and total memory in MB</li></ul></li><li><ul><li>Cross-platform disk space monitoring\n<ul><li>Windows: C: drive (home directory in Codespaces)</li><li>Linux: Root filesystem (workspace root in Codespaces)</li></ul></li><li>Shows used and total space in GB</li></ul></li><li><ul><li>Clean status bar integration</li><li>Detailed hover tooltip showing:\n<ul><li>Current CPU, memory, and disk metrics</li></ul></li></ul></li></ul><ul><li>Visual Studio Code ^1.90.0</li><li>Supported environments:\n<ul><li>Local: Windows, macOS, Linux</li><li>Remote: GitHub Codespaces</li></ul></li></ul><ol><li>Install the extension from VS Code Marketplace</li><li>Look for the CPU usage display in your status bar</li><li>Hover over it to see detailed system information</li></ol><p>The extension shows the following information in your status bar:</p><p>With a detailed tooltip showing:</p><pre><code>Current:\nCPU Usage: 45.3% (2400 MHz)\nMemory Usage: 1024 MB / 2048 MB (50.0%)\nDisk Usage: 150 GB / 500 GB (30.0%)\n</code></pre><p>Note: For disk usage, the monitored path varies by environment:</p><ul><li>Windows:\n<ul><li>Codespaces: Home directory</li></ul></li><li>Linux:\n<ul><li>Local: Root filesystem (/)</li><li>Codespaces: Workspace root</li></ul></li></ul><ul><li>CPU usage is calculated by comparing idle and total CPU time differences</li><li>Memory values are shown in MB and percentage</li><li>Disk usage monitoring adapts to the environment:\n<ul><li>Local machines: Monitors system root or C: drive</li><li>Codespaces: Monitors relevant workspace paths</li></ul></li><li>Moving averages are calculated using 12 data points (5-second intervals over 1 minute)</li><li>Updates occur every 5 seconds for efficient monitoring</li><li>Minimal performance impact on the system</li></ul><h2>GitHub Codespaces Support</h2><p>The extension automatically detects when running in GitHub Codespaces and adjusts its behavior:</p><ul><li>Monitors the workspace root directory in Linux environments</li><li>Uses home directory for Windows-based Codespaces</li><li>Maintains consistent monitoring experience across all environments</li><li>Provides accurate disk usage information for containerized development</li></ul><p>Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.</p><p>This project is licensed under the MIT License - see the <a href=\"https://github.com/tsuyoshi-otake-system-exe-jp/otak-monitor/blob/HEAD/LICENSE\" target=\"_blank\" rel=\"noreferrer noopener nofollow\">LICENSE</a> file for details.</p>","contentLength":2481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iv592c/lightweight_realtime_system_stats_for_vs_code/"},{"title":"Best way to develop talos locally?","url":"https://www.reddit.com/r/kubernetes/comments/1iv4ttd/best_way_to_develop_talos_locally/","date":1740179989,"author":"/u/obviouslyGAR","guid":8877,"unread":true,"content":"<div><p>I am currently learning and building a cluster using talos.</p><p>One thing I want to know is how are you all developing locally? </p><p>Is using docker and using the command  the best way or is there another way that can be done like utilizing terraform?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/obviouslyGAR\"> /u/obviouslyGAR </a>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made an AirDrop server that uses URL Requests to accept data from anywhere","url":"https://github.com/gnhen/SkyDrop","date":1740179194,"author":"/u/GranttH","guid":8899,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iv4j4r/i_made_an_airdrop_server_that_uses_url_requests/"},{"title":"Rust Rant Contest: std::io::Error, the oversized junk drawer of failure","url":"https://www.reddit.com/r/rust/comments/1iv3rb3/rust_rant_contest_stdioerror_the_oversized_junk/","date":1740177182,"author":"/u/OliveTreeFounder","guid":8957,"unread":true,"content":"<p>I've been coding in Rust for five years, and  has never been anything but a headache. The error code? Never useful. It‚Äôs impossible to handle‚Äîtoo big, too vague‚Äîso we all end up just passing this bloated mess back to the caller without even knowing what‚Äôs inside or what actually caused the error.</p><p>But it gets worse. Traits, instead of being parameterized over an  type, just return <code>Result&lt;..., std::io::Error&gt;</code>. Once a trait like this becomes popular‚Äîlike  or ‚Äîyou're stuck. You can‚Äôt handle errors properly unless you rewrite every crate that depends on these traits.</p><p> is a contagious disease infecting the entire ecosystem. We need to stop this pandemic!</p>","contentLength":668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Matrix.org bridges to shut down in 1 month unless $100k can be raised","url":"https://matrix.org/blog/2025/02/crossroads/","date":1740174293,"author":"/u/Evidlo","guid":8876,"unread":true,"content":"<p>After a <a href=\"https://matrix.org/blog/2024/12/25/the-matrix-holiday-special-2024/\">successful 2024 with a lot to be proud of</a>, and a Matrix Conference that brought our community together to celebrate 10 years of Matrix, we step into 2025 with a light budget and a mighty team poised to make the most of it!</p><p>Our priorities remain to make Matrix a safer network, keep growing the ecosystem, make the most of our Governing Board, and drive a fruitful and friendly collaboration across all actors.</p><p>However, whether we will manage to get there is not fully a given.</p><h2><a href=\"https://matrix.org/blog/2025/02/crossroads/#the-foundation-is-key-to-the-success-of-matrix\" aria-label=\"Anchor link for: the-foundation-is-key-to-the-success-of-matrix\">üîó</a>The Foundation is key to the success of Matrix</h2><p>The Matrix.org Foundation has gone from depending entirely on Element, the company set up by the creators of Matrix, to having half of its budget covered by its <a href=\"https://matrix.org/support/\">11 funding members</a>, which is a great success on the road to financial independence! However half of the budget being covered means half of it isn‚Äôt. Or in other words: the Foundation is not yet sustainable, despite running on the strictest possible budget, and is burning through its (relatively small) reserves. And we are at the point where the end of the road is in sight.</p><blockquote><p>The Matrix.org Foundation exists to act as a neutral  and to nurture it as efficiently as possible as <strong>a single unfragmented standard, for the greater benefit of the whole ecosystem</strong>, not benefiting or privileging any single player or subset of players.</p></blockquote><p>Without the Foundation and its programs, the Matrix protocol itself faces existential threats:</p><ul><li>Without Trust &amp; Safety efforts, bad actors and communities would proliferate on the network and make it unlivable for the rest.</li><li>Without a canonical specification, the shared infrastructure and a Spec Core Team to maintain it, the protocol would become fragmented, losing its effective interoperability ‚Äì increasing the costs on all downstream users.</li><li>Without a neutral entity as the custodian of the specification, the ecosystem would first shatter and then consolidate around the biggest (likely for-profit) actor.</li><li>Without advocacy, conferences, documentation and tutorials, Matrix would become a niche protocol used by a few enthusiasts for side projects, whilst big proprietary and siloed networks continue to hold the world‚Äôs communications.</li></ul><p>But there is light at the end of the tunnel! Concretely, the Foundation delivers most of its value by fostering a healthy, fair and fertile ecosystem around Matrix. It needs to strike the right balance between:</p><ul><li><strong>Making Matrix accessible &amp; visible.</strong><ul><li>For the general public it means maintaining an easy default onboarding server (Matrix.org).</li><li>For server administrators it means providing the right tooling to keep their users (and themselves!) safe.</li><li>For developers it means making it easy to develop products using Matrix, via documentation, tutorials, and in-person events.</li></ul></li><li><strong>Making Matrix compelling to build on.</strong><ul><li>This means maintaining the Matrix Specification as a canonical, unencumbered, patent free and royalty free specification.</li><li>Being responsive and vendor-neutral when an organisation or individual contributes.</li><li>Promoting the good players within the ecosystem.</li><li>Ensuring the network grows and attracts more users.</li></ul></li><li><strong>Making Matrix a product that benefits the greater good.</strong><ul><li>This means ensuring that the general public can easily build safe &amp; easy to use communities on Matrix.</li><li>Ensuring that bad actors are proactively chased and discouraged to use Matrix.</li></ul></li></ul><p>Matrix has been here for 10 years, and will hopefully be here for many more! But to continue to grow and thrive, it needs the Foundation to be around and healthy, which means carefully allocating its budget in order to continue to exist and fulfill its mission. This is why it needs to focus on critical programs and shut down some of its activities.</p><p>We view the following programs as critical to the Foundation‚Äôs mission:</p><ul><li>Maintaining the canonical, backwards compatible, stable <a href=\"https://spec.matrix.org/latest/\">Matrix Spec</a></li><li>Developing protocol enhancements and Trust and Safety tooling, making the tools available to the ecosystem and moderating the servers under its control (typically Matrix.org) - <a href=\"https://matrix.org/blog/2025/02/building-a-safer-matrix/\">see our recent blog post</a></li><li>Running the Matrix.org homeserver as an initial home for newcomers</li><li>Promoting the Matrix protocol via online content, conferences and meet-ups and other marketing strategies</li></ul><p>We might fine tune our approach, but we can't cease any of those programs without severe consequences for the ecosystem.</p><p>Meanwhile, bridges have been at the heart of Matrix for a long time. Public bridges hosted by the Matrix.org Foundation have been a very good resource to show the power of interoperability, connect communities together, and onboard many people into their Matrix journey.</p><p>However, these bridges require regular maintenance as the bridged platforms evolve their APIs, and significant engineering and moderation support to run. Luckily, the Matrix ecosystem is now more mature than it was at the time we spun up those public Slack, XMPP and IRC bridge instances. There are now commercial players like <a href=\"https://www.beeper.com/\">Beeper</a> providing a user-friendly offering for people who want to get all their conversations in a single app, or <a href=\"https://indiehosters.net/\">IndieHosters</a> and <a href=\"https://www.fairkom.eu/\">Fairkom</a> offering hosting for Matrix server and bridge instances (and much more).</p><p>So unless the Foundation manages to raise $100,000 of funding by the end of March 2025, we will have to focus our resources on the critical lines of work, and consequently <strong>we will have to shut down all the remaining bridges hosted by the Matrix.org Foundation. This includes bridges to Slack, XMPP, OFTC (IRC), and Snoonet (IRC).</strong> We will also mark the software behind those bridges as archived, as we don't have the resources to accept new contributions.</p><p>In practice, the Foundation needs an additional $610K in revenue to break-even, but this $100K would extend our runway 1 month while we work on landing grants and new members. To put this in context, we nearly doubled our revenue in 2024, reaching $561K, but it was also the first year in which we carried the full cost of our operations: $1.2M. To make ends meet, we liquidated $283K worth of cryptocurrency donations and ended the year with a $356K deficit. We are currently on target for $587K revenue in 2025, with a modest increase in expenses.</p><h2><a href=\"https://matrix.org/blog/2025/02/crossroads/#growing-the-ecosystem-and-the-network\" aria-label=\"Anchor link for: growing-the-ecosystem-and-the-network\">üîó</a>Growing the ecosystem and the network</h2><p>Choosing to shut the bridges down is a difficult decision to make, but will allow us to focus on the critical projects which will keep the ecosystem growing. The success of Matrix depends on how widely it is used by the general public and by organisations ‚Äì preferably natively rather than via bridges.</p><p>The more people and organisations rely on Matrix, the more attractive it becomes for organisations to build products and services on top of it, the more funding the Foundation gets, and the more the Foundation can in turn reinvest into the ecosystem and run initiatives that benefit all stakeholders for the growth of the network.</p><p>Once the Foundation is cashflow positive, it will be able to accelerate and eventually get on with the multiple projects the team and Governing Board have in mind to make Matrix fun, exciting, reliable, safe, easy to use, and above all useful. And we hope to get there by the end of the year.</p><p>Most importantly, despite the Trust and Safety team being the Foundation‚Äôs biggest expense, as explained in <a href=\"https://matrix.org/blog/2025/02/building-a-safer-matrix/\">our blog post</a>, the team is still underresourced: they are understaffed and under a lot of pressure to deliver protocol improvements, better tooling for server admins, and ensure Matrix.org is a good citizen of the open federation. <strong>T&amp;S will be the first area to see increased funding.</strong></p><p>Separately, the Foundation wants to continue executing on its mission! Among others, better connect the doers in the ecosystem with the people and organisations who need their energy, share the successes and learnings from the community: the Matrix Conference was an incredible success and we want to see more of that.</p><p>We‚Äôve also seen a clear change in how many users and organisations were adopting Matrix in the last few months: the world needs a decentralised end-to-end encrypted network to communicate more than ever, and it shows! We want to uplift the good players which are driving this growth.</p><p>There is so much more that we could do to make Matrix better and realise its full potential. </p><p>Right now, the Foundation urgently needs <a href=\"https://matrix.org/support/\">your financial help</a>. For the sake of a safe network, our primary focus today, but also to be able to deliver on the reason we all want Matrix to succeed.</p><ul><li>People should have full control over their own communication.</li><li>People should not be locked into centralised communication silos, but instead be free to pick who hosts their communication without limiting who they can reach.</li><li>The ability to converse securely and privately is a basic human right.</li><li>Communication should be available to everyone as a free and open, unencumbered, standard and global network.</li></ul><p><strong>If you are an organisation building on top of Matrix</strong>, you can help by , which also gives you the opportunity to be eligible to participate in the Governing Board, and other perks. </p><p><strong>If you are an organisation buying Matrix services or products</strong>, you can help by <strong>ensuring that your vendor is financially contributing back to the project</strong> or becoming a member yourself.</p><p><strong>If you are an individual using Matrix,</strong> you can help by .</p><p><strong>If you are a philanthropist or other funder</strong>, you can help by getting in touch with us at <a href=\"https://matrix.org/cdn-cgi/l/email-protection#c1a7b4afa5a8afa681aca0b5b3a8b9efaeb3a6\"></a> to discuss funding options. </p><p>It isn‚Äôt the <a href=\"https://matrix.org/blog/2022/12/01/funding-matrix-via-the-matrix-org-foundation/\">first</a><a href=\"https://matrix.org/blog/2024/04/open-source-publicly-funded-service/\">time</a> we‚Äôve rung the alarm bell, and it is no fun to beg for help. We are at a crossroads, where the vibrancy of the ecosystem and enthusiasm around Matrix is not reflected in the support the Foundation gets, and we are at risk of losing this common resource and all it offers.</p><p>But all in all, we are optimists ‚Äì we wouldn‚Äôt have begun this journey if we weren‚Äôt ‚Äì and we believe that there are people out there who realise that sovereign and secure communication is as high on the list of today‚Äôs essential technology ‚Äì if not higher ‚Äì as ensuring AI is safe, so let‚Äôs spread the word and let‚Äôs continue working on a safer and more sovereign world!</p><div><div><p>\n                        The Matrix.org Foundation is a non-profit and only relies\n                        on donations to operate. Its core mission is to maintain\n                        the Matrix Specification, but it does much more than that.\n                    </p><p>\n                        It maintains the matrix.org homeserver and hosts several\n                        bridges for free. It fights for our collective rights to\n                        digital privacy and dignity.\n                    </p><a href=\"https://matrix.org/support\">Support us</a></div></div>","contentLength":10479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iv2mr3/matrixorg_bridges_to_shut_down_in_1_month_unless/"},{"title":"Reading the Source Code","url":"https://www.reddit.com/r/kubernetes/comments/1iv1def/reading_the_source_code/","date":1740171145,"author":"/u/TopNo6605","guid":8856,"unread":true,"content":"<p>Curious does anyone have any advice or vids/blogs/books that go through the source code of k8s? I'm the type of person who likes to see what's happening under the hood. But k8s is a beast of an application. I was reading the apiserver source and got up the point where it's creating handlers and doing something with an openapi controller...which I didn't know existed.</p><p>Fascinating stuff but the amount of abstraction here is what gets me. Everything is an interface and abstracted to some other file, you end up following a long chain only to end up at an interface function without a definition. I get it, for development purposes. But man it's a beast to learn.</p><p>With the apiserver I literally just started logging when functions were called but I had to take a break after 4 hours of that. How do knew contributors get brought up to speed?</p>","contentLength":840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tk9.0 canvas demo","url":"https://opu.peklo.biz/p/25/02/21/1740170028-43ac6.png","date":1740170311,"author":"/u/0xjnml","guid":8897,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iv11dh/tk90_canvas_demo/"},{"title":"Windows to Linux, Set Up Full Disk Encryption on openSUSE","url":"https://news.opensuse.org/2025/02/20/setup-fde-on-opensuse/","date":1740170160,"author":"/u/gabriel_3","guid":9079,"unread":true,"content":"<p>Data breaches and cyber threats are becoming increasingly common and securing your personal and professional information has never been more critical.</p><p>Users transitioning from <a href=\"https://news.opensuse.org/2024/11/26/transition-from-windows-step-by-step/\">Windows to Linux</a> through the Upgrade to Freedom campaign can use <a href=\"https://get.opensuse.org/\">openSUSE</a>‚Äôs tools to protect sensitive data, which include full disk encryption (FDE).</p><p>Full disk encryption during installation ensures maximum security. It safeguards all data on your hard drive by encrypting it and makes it unreadable without an decryption key. This level of protection is vital for preventing unauthorized access if your laptop or desktop is lost or stolen.</p><p>FDE with openSUSE is both user-friendly and powerful. The setup with advanced security features is easy.</p><p>For users seeking feature parity with Windows BitLocker, openSUSE offers Full Disk Encryption (FDE) secured by a TPM2 chip or a FIDO2 key. This advanced setup enhances security by storing encryption keys within the TPM, which ensures that only a trusted system configuration can unlock the disk. For a step-by-step guide on enabling this feature, read the <a href=\"https://news.opensuse.org/2024/09/20/quickstart-fde-yast2/\">Quickstart in Full Disk Encryption with TPM and YaST2</a> article.</p><p>Here‚Äôs a step-by-step guide to set up FDE on your system:</p><p><strong>Step 1: Download and Boot openSUSE</strong></p><ul><li>Visit <a href=\"https://get.opensuse.org/\">get.opensuse.org</a> to download the latest version of openSUSE Leap or Tumbleweed.</li><li>Restart your computer and boot from the USB drive to begin the installation process.</li></ul><p><strong>Step 2: Configure Encryption During Installation</strong></p><ul><li>Once the installer starts, select your preferred language and keyboard layout.</li><li>In the partitioning setup, choose Guided Setup with Encrypted LVM.</li><li>Set a strong passphrase for encryption. This passphrase will be required every time the system boots.   - Use a mix of upper and lower case letters, numbers and special characters for optimal security.</li><li>Proceed with the installation as directed by the installer.</li></ul><p><strong>Step 3: Verify Encryption Settings</strong></p><p>After installation is complete and the system restarts, you‚Äôll be prompted to enter your encryption passphrase. Once entered, openSUSE tools will decrypt the disk and boot normally. To confirm encryption is active:</p><ul><li>Open a terminal or console.</li><li>Run the command  to verify that your disk is listed with the encryption type (e.g., ).</li></ul><p>The output might look something similar to the following:</p><div><div><pre><code>NAME        FSTYPE      FSVER LABEL UUID                                   FSAVAIL FSUSE% MOUNTPOINT\nsda                                                                                     \n‚îú‚îÄsda1      ext4        1.0     4a83v1e1-e8d2-4e38-815d-fd79j194f5   25G    30%    /\n‚îî‚îÄsda2      swap        1           d2e18c23-9w4b-4d26-p1s2-cm2sd64tx9de                \nsdb                                                                                     \n‚îî‚îÄsdb1      crypto_LUKS 1           10bb2vca-81r4-418b-a2c4-e0f6585f2c7a                \n  ‚îî‚îÄluks    ext4        1.0         8a9wka1b-7e9c-1a1f-a9f7-3c82x1e4e87f   150G    10%    /mnt/data\n</code></pre></div></div><p>While FDE protects your data, it does not prevent data loss from hardware failure or accidental deletion. Regularly back up your data to an encrypted external drive or a secure cloud service to ensure its safety.</p><p><strong>Enhanced Security for Modern Challenges</strong></p><p>Setting up full disk encryption on openSUSE not only protects your data but also aligns with the Upgrade to Freedom campaign‚Äôs mission of empowering users to maintain control over their hardware and privacy. By combining open-source software with good security practices, openSUSE ensures that users can confidently embrace a more secure digital future.</p><p>For additional guidance and community support, visit the <a href=\"https://forums.opensuse.org/\">openSUSE forums</a> or join discussions at your local Linux user group.</p><p><small> Please be aware that some hardward configurations may require additional drivers or BIOS settings adjustments for full disk encryption to fully function properly. Check your device‚Äôs compatibility and update your firmware before proceeding. </small></p>","contentLength":3905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iv0z6q/windows_to_linux_set_up_full_disk_encryption_on/"},{"title":"Streamline Kubernetes Management with Rancher","url":"https://youtube.com/shorts/fOVTDobiwIE?feature=share","date":1740167919,"author":"/u/abhimanyu_saharan","guid":8833,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iv0357/streamline_kubernetes_management_with_rancher/"},{"title":"[First crate] derive_regex: construct a type by parsing a string with regular expressions","url":"https://www.reddit.com/r/rust/comments/1iuzg1i/first_crate_derive_regex_construct_a_type_by/","date":1740166287,"author":"/u/TitaniumBrain","guid":9116,"unread":true,"content":"<p>I had an idea and decided it was simple enough to publish <a href=\"https://crates.io/crates/derive-regex\">my first crate</a> and contribute to the Rust ecosystem.</p><p>I'm still relatively new to Rust (coming from a few years of Python but I fell in love with the language), so any feedback is welcome. I'm confident my code isn't , but I want to make sure I follow best practices and learn about any Rust .</p><p>Using this crate - and the associated derive proc macro - you can derive  on an enum or struct to automatically derive the  constructor method.</p><p>Copied from the readme, here's a couple examples if you don't to click away from Reddit:</p><p>```rust use derive_regex::FromRegex;</p><pre><code>pattern = r\"^(?P&lt;timestamp&gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?P&lt;level&gt;[A-Z]+)\\] (?P&lt;message&gt;.+)$\" </code></pre><p>)] struct LogEntry { timestamp: String, level: String, message: String, }</p><p>fn main() { let log = \"2025-02-20 15:30:00 [INFO] Server started successfully\"; let entry = LogEntry::parse(log).expect(\"Failed to parse log entry\"); println!(\"Parsed log entry: {:#?}\", entry); // Parsed log entry: LogEntry { // timestamp: \"2025-02-20 15:30:00\", // level: \"INFO\", // message: \"Server started successfully\", // } } ```</p><p>```rust use derive_regex::FromRegex;</p><p>enum CookingCommand { // Parses a command like \"chop 3 carrots\" #[regex(pattern = r\"chop (?P&lt;quantity&gt;\\d+) (?P&lt;ingredient&gt;\\w+)\")] Chop { quantity: u32, ingredient: String },</p><pre><code>// Parses a command like \"boil for 10 minutes\" #[regex(pattern = r\"boil for (?P&lt;minutes&gt;\\d+) minutes\")] Boil(u32), // Parses a command like \"bake at 375.0 degrees for 25 minutes\" #[regex(pattern = r\"bake at (?P&lt;temperature&gt;\\d+\\.\\d+) degrees for (?P&lt;minutes&gt;\\d+) minutes\")] Bake { temperature: f64, minutes: u32 }, // Parses a command like \"mix salt and pepper\" #[regex(pattern = r\"mix (?P&lt;ingredient1&gt;\\w+) and (?P&lt;ingredient2&gt;\\w+)\")] Mix { ingredient1: String, ingredient2: String, }, </code></pre><p>fn main() { let commands = [ \"First, chop 3 carrots\", \"Don't forget to boil for 10 minutes\", \"I guess I'll bake at 375.0 degrees for 25 minutes\", \"mix salt and pepper now\", ];</p><pre><code>for cmd in &amp;commands { if let Ok(command) = CookingCommand::parse(cmd) { match command { CookingCommand::Chop { quantity, ingredient, } =&gt; { println!(\"Chop {} {}(s)\", quantity, ingredient); } CookingCommand::Boil(minutes) =&gt; { println!(\"Boil for {} minutes\", minutes); } CookingCommand::Bake { temperature, minutes, } =&gt; { println!(\"Bake at {} degrees for {} minutes\", temperature, minutes); } CookingCommand::Mix { ingredient1, ingredient2, } =&gt; { println!(\"Mix {} and {}\", ingredient1, ingredient2); } } } else { eprintln!(\"Failed to parse command: {}\", cmd); } } // Chop 3 carrots(s) // Boil for 10 minutes // Bake at 375 degrees for 25 minutes // Mix salt and pepper </code></pre>","contentLength":2667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interop 2025: another year of web platform improvements","url":"https://web.dev/blog/interop-2025?hl=en","date":1740163542,"author":"/u/feross","guid":8956,"unread":true,"content":"<p>\n  Published: February 13, 2025\n</p><p>After the huge success of Interop 2024, the project returns today with a new set\nof focus areas for 2025. While we couldn't include every suggestion made this\nyear, the final list reaches across the web platform‚Äîfrom CSS to\nperformance-related features.</p><ul></ul><p>In addition, and as in previous years, there's a set of areas for investigation.\nThese are areas where we don't have enough information or tests to include as a\nfocus area, but the group feels some work should be done to get them to a stage\nwhere we can include them.</p><ul></ul><p>We're excited about all of these features and the improvements this year's\nproject will bring to the platform. And, as with <a href=\"https://web.dev/blog/interop-2024-wrapup\">last\nyear</a>, the project will make a whole set of things\nBaseline Newly available. This post shares more information about some of the\nfeatures on the list, with links to information to find out more.</p><p>Several of the features included in Interop 2025 are features that you flagged\nup as important in the State of CSS 2024 survey. They'll help you create more\nbeautiful and performant user experiences.</p><p>This feature lets you anchor a positioned element to an anchor, it's\nparticularly useful when displaying popovers.</p><h3 data-text=\"Same-document view transitions\" tabindex=\"-1\">Same-document view transitions</h3><p>Also included this year are view transitions, specifically same-document view\ntransitions, and the  CSS property.</p><h3 data-text=\"The backdrop-filter property\" tabindex=\"-1\">The  property</h3><p>The\n<a href=\"https://developer.mozilla.org/docs/Web/CSS/backdrop-filter\"></a>\nproperty has been Baseline Newly available since September 2024. It lets you\ncreate effects behind your content. For example to blur or create effects that\nyou might expect to only be available in a graphics application.</p><p>Despite being mostly interoperable, you can see from <a href=\"https://wpt.fyi/results/css/filter-effects?label=experimental&amp;label=master&amp;aligned&amp;q=backdrop-filter\">the failing tests for\n</a>that\nthere are bugs and issues in those implementations. While these issues might not\nbe a problem to everyone, we know that many of you do run into them, it'll be\ngreat to get this feature working really well.</p><p>The  element is a disclosure widget which can be expanded to reveal\nadditional content. The  element itself is Baseline Widely available.\nHowever, there are a number of related features that have been more recently\nadded <a href=\"https://developer.chrome.com/blog/styling-details\">that make  more\nuseful</a>.</p><ul><li>The  and  CSS pseudo-elements.</li><li>Using  to toggle the content instead of .</li><li>Auto-expanding the  element with find-in-page matches.</li><li>The  attribute, which hides an element until it is found\nusing the browser's find-in-page search or it is directly navigated to by\nfollowing a URL fragment.</li></ul><p>The  at-rule lets you scope your selectors to a sub-tree of the DOM, or\neven select between an upper and lower boundary in the tree. For example, the\nfollowing CSS only selects  elements inside an element with a class of\n.</p><p>In the next example, an upper and lower bound is used. The  element is\nonly selected if it's between the element with a class of  and also\noutside of the element with a class of .</p><p>Without the  event, there's no reliable way to detect that a scroll is\ncomplete. The best you could do is to use  to check if the scroll\nhas stopped for a period. This makes it more like a scroll has paused event, not\na scroll has ended event.</p><p>With the  event, the browser does all this difficult evaluation for\nyou.</p><h3 data-text=\"The text-decoration property\" tabindex=\"-1\">The  property</h3><p>The\n<a href=\"https://developer.mozilla.org/docs/Web/CSS/text-decoration\"></a>\nproperty is a shorthand for , ,\n, and <code translate=\"no\" dir=\"ltr\">text-decoration-thickness</code>. It's deemed Baseline\nWidely available, however in Safari the only unprefixed shorthand property that\nworks is . It's this that will be addressed during 2025.</p><p>The CSS <a href=\"https://developer.mozilla.org/docs/Web/CSS/writing-mode\"></a>\nproperty has a number of possible values, many of which are designed to lay out\nscripts that display vertically. Sometimes however, you want to lay out text\nvertically as part of a design, rather than for language support reasons. The\n and  values are designed for this, but have suffered\nfrom poor browser compatibility. This should be fixed during 2025.</p><p>In addition, the logical CSS properties  and \nare included. These make it possible to control what happens when content\noverflows boxes, regardless of the writing mode.</p><p><a href=\"https://web.dev/explore/learn-core-web-vitals\">Web Vitals</a> can help you quantify the\nexperience of your site and identify opportunities to improve. The Web Vitals\ninitiative aims to simplify the landscape, and help sites focus on the metrics\nthat matter most, the Core Web Vitals.</p><h4 data-text=\"Event Timing API (for INP)\" tabindex=\"-1\">Event Timing API (for INP)</h4><p>This year, the work will focus on the following features:</p><ul><li>JavaScript string builtins: to make the WebAssembly built-in string\nfunctions mirror a subset of the JavaScript String API so it can be callable\nwithout JavaScript glue code.</li><li>Resizable buffers integration: to integrate WebAssembly into JavaScript code\nthat uses resizable buffers.</li></ul><p>This year the project includes a removal from the platform. <a href=\"https://developer.mozilla.org/docs/Web/API/MutationEvent\">Mutation\nevents</a> are deprecated\nand replaced with the much more performant and Baseline Widely available\n<a href=\"https://developer.mozilla.org/docs/Web/API/MutationObserver\">Mutation Observer\nAPI</a>. Chrome\nremoved these events in Chrome 126, and this focus area is to remove them from\nall browsers.</p><p>Descriptions of the full list of features can be found in the project <a href=\"https://github.com/web-platform-tests/interop/blob/main/2025/README.md\">README</a>.\nAlso, read the posts from the other companies working on Interop 2025.</p>","contentLength":4896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuybup/interop_2025_another_year_of_web_platform/"},{"title":"COSMIC Alpha 6: Big Leaps Forward","url":"https://blog.system76.com/post/cosmic-alpha-6-big-leaps-forward","date":1740163210,"author":"/u/Schnurres","guid":8781,"unread":true,"content":"<div data-v-7773bbb7=\"\"><p>Our COSMIC mission continues! This month, we finished up some essential features and fixes in preparation for the upcoming beta alongside some amazing COSMIC contributors. Check out what‚Äôs new in Alpha 6, and make sure you‚Äôre fully updated to see these changes for yourself!</p><p>Desktop Zoom can now be activated in Settings &gt; Accessibility, from the Accessibility applet in the panel, or using the shortcuts Super + =, Super + -, or Super + Mouse Scroll.</p></div><div data-v-7773bbb7=\"\"><p>More <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-greeter/issues/40\">accessibility features</a> in the books! Clicking the Accessibility icon at login gives you access to various settings toggles for:</p><ul><li>Reads on-screen text aloud</li><li> Scrolling up while holding Super <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/issues/853\">magnifies</a> the region of the screen where your cursor is located</li></ul><p>Navigates you to Accessibility Settings</p></div><div data-v-7773bbb7=\"\"><p>Additional accessibility features for high-contrast, color inversion and various color filters for colorblindness are being worked on soon.</p><p><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/547\">Desktop view</a> is now supported in COSMIC. Right-clicking an empty desktop and selecting ‚ÄúDesktop view options‚Äù opens a settings window for your desktop. Show or hide desktop folders, drives, or the Trash; you can also adjust icon size and spacing between icons from this window. A fix for the window appearing below other windows will arrive in a later update. Files and folders can also be <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/597\">dragged</a> between the desktop view and COSMIC Files.</p></div><div data-v-7773bbb7=\"\"><p><strong>Additional Scaling Options</strong></p><p>An additional scaling setting has been added to scale the screen slightly, from 5% to 20% larger. For example, on a display set to 125% scaling, those desiring larger text can use this new setting to increase scaling to 130%, 135%, 140%, or 145%.</p></div><div data-v-7773bbb7=\"\"><p>Workspaces received some updates to really make the feature sparkle. For starters, you can now <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/34\">scroll between workspaces</a> in the overview as a quick and easy way of navigating to your intended destination. Clicking on the preview of the current workspace or empty space in the workspace overview will allow you to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/49\">exit the workspace view</a>.</p><p>Previews for horizontal workspace now include name and number. Workspace previews on rotated displays will show the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/17\">correct orientation</a>. The last workspace is now removed if it follows another <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/83\">empty workspace</a>.</p></div><div data-v-7773bbb7=\"\"><p>Additionally, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/89\">minimized windows</a> can now be dragged and dropped between workspaces, while windows can be moved to another display by dropping them in the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/53\">workspace overview</a>. Dragging a window <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/41\">out of a stack</a> will match expectations and no longer move the whole stack. Window titles in the overview are now at the top left of the window and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-workspaces-epoch/issues/56\">match the users theme</a>. Additional workspace features, including pinned workspaces, will arrive in a future update.</p><p><strong>Windows Gravitate to Edges</strong></p><p>Toggling on ‚ÄúFloating windows gravitate to nearby edges‚Äù in Window Management Settings will automatically align a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/pull/1189\">window‚Äôs edge</a> to the adjacent screen border when dragged close to it, removing the struggle of aligning it with the edge manually.</p><p>If a search in the Launcher yields more than eight entries, users can now <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-launcher/issues/238\">scroll</a> to see the additional options. In addition, the Launcher has been updated to trigger a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-launcher/issues/126\">countdown timer</a> whenever Power Off, Restart, and Log Out are selected, matching the behavior of the Power applet.</p></div><div data-v-7773bbb7=\"\"><p>File path completion is now in COSMIC Files. Hitting the Down arrow when typing a file path into the search bar will automatically finish the file path you‚Äôre searching for. Meanwhile, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/728\">copying a file</a> allows pasting of the file path in other applications. COSMIC Files uses <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/732\">Home and End keys</a> for navigating the app. You can now compress and extract <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/468\">password protected zip files</a> and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/190\">drag selecting</a> will scroll the content window.</p><p>Copy/paste using the middle mouse button has been implemented. Sweet convenience.</p><p>A <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-player/issues/52\">nav bar</a> has been added for viewing folders in a tree view to display the video files available to open in the Media Player. File menu options have also been completed:</p></div><div data-v-7773bbb7=\"\"><p>When a music file is playing, the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-player/issues/56\">Media Player</a> image will display the song title, album, artist, and year released. Down the line we‚Äôd like to explore adding further metadata, such as album artwork and song lyrics.&nbsp;</p><p>Mpris control has been added to show and control currently-playing media in the sound applet, and the scrubber now moves to a second line for improved single-column usability.</p><p>A <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/128\">Revert all changes</a> feature has been added to COSMIC Edit to revert your file back to the most recent saved state. If you decide to scrap everything or start a new file from scratch, go to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/41\">File &gt; Close Project</a> to remove the project from the NavBar and bring up a new document and tab. When multiple tabs are open, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/123\">cycle project tabs</a> using Ctrl+Tab and Ctrl+Shift+Tab shortcuts.</p><p><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/pull/311\">Zoom</a> has also been implemented. Zoom in and out from the View menu, or using Ctrl + or Ctrl - shortcuts. Reset back to default using Ctrl + 0.</p><p>Opens Sans replaces Fira Sans as the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/libcosmic/pull/809\">default font</a> for COSMIC. The team liked Open Sans for its better legibility, glyph and language support, and a more modern aesthetic. Likewise, Noto Sans Mono will be used for the default monospace font.</p><p>Memory usage has been greatly reduced in a number of areas, including <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-applets/pull/796\">minimize</a>, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/pull/789\">COSMIC Files</a>, and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-panel/issues/336\">workspaces</a>. A related update made to libcosmic should prevent memory fragmentation. In addition, optimizations to cosmic-text and <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/freedesktop-icons/pull/4\">freedesktop-icons</a> have reduced memory usage across all COSMIC apps and applets.</p><p><strong>A Whole Swarm of Bug Fixes‚Ä¶and More!</strong></p><ul><li>Fixed a bug with server-side decorations that caused the cursor to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/issues/1071\">drag a window</a> after a single click</li><li>When clicking an app icon of an app with multiple windows opened, <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-applets/issues/456\">window previews</a> now adapt to the size and shape of the window</li><li>Implemented a fix in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-comp/issues/680\">cosmic-comp</a> related to keyboard grabbing after a window is focused</li><li>Implemented behavior to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/pull/735\">COSMIC Files</a> for exiting the context menu</li><li><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/308\">COSMIC Files</a> now changes view away from the external drive after the drive is mounted and removed</li><li>Implemented a fix for <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/766\">COSMIC Files</a> attempting to read an unreadable .hidden file</li><li>Fixed a crash involving the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/xdg-desktop-portal-cosmic/issues/121\">file picker</a> related to a11y in libcosmic</li><li><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-term/issues/68\">COSMIC Terminal</a> now uses a hollow block cursor design when the window is unfocused</li><li>Removed Spell Check menu option from <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/300\">COSMIC Edit</a>, to be returned once the feature exists</li><li>In <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/191\">COSMIC Edit</a>, ‚ÄúFind‚Äù searches now highlight all occurrences, and the currently selected item is highlighted at a higher opacity</li><li>Scrolling now occurs as expected when dragging to highlight text in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/154\">COSMIC Edit</a></li><li>Saving a root or read-only file in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-edit/issues/249\">COSMIC Edit</a> now prompts the user for their password, removing the need to run the application as root</li><li><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-screenshot/issues/40\">Screenshot tool</a> now respects the user‚Äôs time zone when naming screenshot files</li><li>Implemented a fix preventing icons from disappearing from the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-screenshot/issues/18\">screenshot tool</a></li><li>Fixed a bug preventing the Delete key from moving a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-files/issues/592\">desktop file</a> to the Trash</li><li>Implemented a fix for a bug causing <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-session/issues/103\">Steam</a> to crash</li><li>Fixed an issue causing some Radeon RX users to be unable to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-epoch/issues/1447\">log in</a></li><li>The context menu in <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-settings/issues/953\">COSMIC Settings</a> now closes when another option is selected in the NavBar</li><li>Implemented the ability to import environment variables from <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-session/pull/106\">systemd</a></li><li>Fixed a regression with <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/libcosmic/issues/656\">libcosmic</a> affecting the ComboBox widget</li><li>Added a <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-panel/issues/190\">slight delay</a> when the cursor hovers from one applet to the next to account for intent</li><li>Added support for using the middle mouse button to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-term/issues/49\">copy/paste</a></li><li>Clicking next/previous month in the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/libcosmic/issues/632\">calendar widget</a> no longer selects the day</li><li>Resolved a bug with <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-settings/issues/798\">Firefox</a> not recognizing it‚Äôs the default web browser when it‚Äôs not set as the default mail client</li><li>Removed WPS suggestion from the <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-applets/pull/793\">WiFi applet</a> when WPS is not supported</li><li>Added support to <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/cosmic-bg/pull/68\">cosmic-bg</a> for compositors without fractional scaling support</li><li>Pop!_OS 24.04 Linux kernel updated to version <a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/pop-os/linux/pull/343\">6.12.10</a></li></ul><p>Head to the <a target=\"_blank\" rel=\"noopener\" href=\"https://system76.com/cosmic\">COSMIC page</a> for a fresh install of Alpha 6. If your system has an NVIDIA GPU, remember to install the NVIDIA ISO. Have fun and break things!</p></div>","contentLength":7754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iuy6v6/cosmic_alpha_6_big_leaps_forward/"},{"title":"Meanwhile at the Pentagon","url":"https://www.reddit.com/r/artificial/comments/1iuwy03/meanwhile_at_the_pentagon/","date":1740160167,"author":"/u/MetaKnowing","guid":8804,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[R] MLGym: A New Framework and Benchmark for Advancing AI Research Agents","url":"https://www.reddit.com/r/MachineLearning/comments/1iuwuyu/r_mlgym_a_new_framework_and_benchmark_for/","date":1740159974,"author":"/u/Rybolos","guid":8853,"unread":true,"content":"<p>We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</p>","contentLength":1468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Dimensionality reduction is bad practice?","url":"https://www.reddit.com/r/MachineLearning/comments/1iuwgcu/d_dimensionality_reduction_is_bad_practice/","date":1740159022,"author":"/u/Ready_Plastic1737","guid":8854,"unread":true,"content":"<p>I was given a problem statement and data to go along with it. My initial intuition was \"what features are most important in this dataset and what initial relationships can i reveal?\"</p><p>I proposed t-sne, PCA, or UMAP to observe preliminary relationships to explore but was immediately shut down because \"reducing dimensions means losing information.\"</p><p>which i know is true but..._____________</p><p>can some of you add to the ___________? what would you have said?</p>","contentLength":451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Godfather Yoshua Bengio says it is an \"extremely worrisome\" sign that when AI models are losing at chess, they will cheat by hacking their opponent","url":"https://www.reddit.com/r/artificial/comments/1iuvosh/ai_godfather_yoshua_bengio_says_it_is_an/","date":1740157177,"author":"/u/MetaKnowing","guid":8719,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rust ü¶Ä DataFrame Library Elusion v3.3.0 is released üöÄ FIXED NORMALIZATION","url":"https://www.reddit.com/r/rust/comments/1iuvnrr/rust_dataframe_library_elusion_v330_is_released/","date":1740157108,"author":"/u/DataBora","guid":9115,"unread":true,"content":"<p>Elusion is a high-performance DataFrame / Data Engineering / Data Analysis library designed for in-memory data formats such as CSV, JSON, PARQUET, DELTA, as well as for ODBC Database Connections for MySQL and PostgreSQL, as well as for Azure Blob Storage Connections, as well as for creating JSON files from REST API's which can be forwarded to DataFrame.</p>","contentLength":355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Practices for Consistent API Error Handling","url":"https://zuplo.com/blog/2025/02/11/best-practices-for-api-error-handling","date":1740157047,"author":"/u/ZuploAdrian","guid":8852,"unread":true,"content":"<p><strong>Clear and consistent API error handling is crucial for improving developer\nexperience and reducing debugging time.</strong> Poor practices, like unclear messages\nor misuse of HTTP status codes, can frustrate developers and lead to increased\nsupport tickets. This guide covers actionable strategies to standardize API\nerror handling, including:</p><ul><li><strong>Use of HTTP Status Codes:</strong> Ensure accurate mapping (e.g., 400 for client\nerrors, 500 for server issues).</li><li><strong>Structured Error Responses:</strong> Follow RFC 9457 (Problem Details) standards\nfor clear, actionable error details.</li><li> Write brief, helpful, and secure messages.</li><li><strong>Protocol-Specific Practices:</strong> Tailor error handling for REST, GraphQL, and\ngRPC APIs.</li></ul><p>To address the challenges highlighted earlier, you can apply these\nwell-established methods.</p><p>HTTP status codes are your first tool for communicating errors. The key is to\nuse them accurately, rather than relying on generic codes.</p><table><thead><tr></tr></thead><tbody><tr><td>401 Unauthorized, 422 Unprocessable Entity</td></tr><tr><td>500 Internal Error, 503 Service Unavailable</td></tr></tbody></table><p>You can find a full list of status codes\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\">on MDN</a>, but here's a\nfew helpful docs we've written in the past that go into more depth:</p><p>For consistent error reporting, modern APIs should follow the\n<a href=\"https://www.rfc-editor.org/rfc/rfc9457.html\">RFC 9457</a> Problem Details\nspecification. This is the successor to the popular\n<a href=\"https://www.rfc-editor.org/rfc/rfc7807\">RFC 7807</a> draft. For an in-depth\nunderstanding of this format, check out our\n<a href=\"https://zuplo.com/blog/2023/04/11/the-power-of-problem-details\">full problem details guide</a>. In\ncase you're short on time - here's a quick overview:</p><p>The problem details response is sent back as a JSON body with the following\nproperties:</p><ul><li>: A URI that identifies the specific error type. This\nhelps clients understand the error and potentially find more information or\ndocumentation about it. Ideally, this URI should be stable and not change over\ntime.</li><li>: A short, human-readable summary of the problem. This\nshould be a brief description that concisely conveys the error. The title\nshould not change for a given \"type\" URI.</li><li><strong>status (integer, optional)</strong>: The HTTP status code generated by the origin\nserver for this occurrence of the problem. This helps clients understand the\nnature of the error and how it relates to the HTTP protocol.</li><li><strong>detail (string, optional)</strong>: A more detailed, human-readable explanation of\nthe problem. This can include specific information about the error and what\nmight have caused it. The \"detail\" field is intended to provide context and\nsuggestions to clients on how they might address the problem.</li><li><strong>instance (string, URI, optional)</strong>: A URI that identifies the specific\noccurrence of the problem. This can help clients and servers correlate and\ntrack individual instances of errors.</li></ul><p>Here's what a standardized error response might look like:</p><pre tabindex=\"0\"><code></code></pre><p>This response would be accompanied with the following headers and status code:</p><pre tabindex=\"0\"><code></code></pre><p>Here's a video that shows you how to send these error back in practice. It's in\n.Net/C# but the concepts are broadly applicable:</p><p>When evolving error schemas, it's crucial to make changes without disrupting\nexisting clients. Here's how you can manage this:</p><ul><li>Add optional fields instead of altering existing ones</li><li>Preserve legacy formats during transitions</li><li>Implement semantic versioning for your endpoints</li></ul><p>API management tools like Zuplo are really handy when trying to bring\nconsistency across your error formats, and are especially useful when trying to\ntransition all of your APIs over from one format to another.</p><p>Creating effective API error messages means providing useful guidance while\nkeeping security in mind. Google's AIP-193 guidelines\n<a href=\"https://google.aip.dev/193\">[4]</a> recommend using a structured format with\nplain, straightforward language that remains technically accurate.</p><p>The goal is to make error messages both clear and actionable. Here's a\ncomparison of good and bad examples:</p><table><thead><tr></tr></thead><tbody><tr><td>\"Invalid email format in 'user_email' field\"</td><td>\"ValidationError: field_23\"</td></tr><tr><td>\"Request to /api/v1/users failed\"</td></tr></tbody></table><p>If you've ever used Azure, many of their system errors demonstrate this\napproach, for example:</p><pre tabindex=\"0\"><code></code></pre><p>Maintaining security while providing useful error feedback involves a few key\npractices:</p><ul><li> to prevent leaks.</li><li><strong>Standardize authentication errors</strong> for consistency.</li><li> to avoid exposing vulnerabilities.</li></ul><p>For example, following RFC 9457 guidelines, a secure error response might look\nlike this:</p><pre tabindex=\"0\"><code></code></pre><p>This is another area where using an API gateway/API management tool is useful.\nYou can monitor your outbound responses and scan them for PII or other sensitive\ninformation/keywords using a regex. A programmable gateway (ex. Zuplo) will even\nlet you rewrite your response bodies to strip out sensitive data.</p><div><div><p>Over 10,000 developers trust Zuplo to secure, document, and monetize their APIs</p><a href=\"https://zuplo.com?utm_source=blog&amp;utm_medium=inline-cta\">Learn More</a></div></div><p>Let's dive into how different API protocols handle errors, building on the\nstandards discussed earlier.</p><p>REST APIs rely on HTTP status codes paired with structured error payloads. A\ncommon standard for this is , which ensures a\nconsistent format across endpoints. This method also supports content\nnegotiation between JSON and XML, keeping the structure consistent across\ndifferent formats.</p><p>GraphQL handles errors differently. It always responds with a  status\ncode, even when errors occur. Errors are communicated through an  array,\nwhich allows for partial success. For instance, GitHub's GraphQL API might\nreturn:</p><pre tabindex=\"0\"><code></code></pre><p>This approach allows for returning valid data alongside error details.</p><p>gRPC uses a predefined set of numeric status codes (ranging from 0 to 16) for\nerror handling, aligned with . Errors include structured details\nfor better context. Here's an example:</p><pre tabindex=\"0\"><code></code></pre><table><thead><tr></tr></thead><tbody><tr><td>Standard HTTP success codes</td></tr><tr></tr><tr></tr></tbody></table><p>Each protocol has its own approach, but they all follow two key principles:\n<strong>machine-readable error codes</strong> and . This ensures\nerrors are both understandable and actionable.</p><p>For cross-protocol APIs, API gateways can simplify error handling. They provide\nunified error schema management and automate transformations between\nprotocol-specific formats. This helps maintain consistency while respecting the\nconventions of each API type.</p><p>Effective error management relies on consistent monitoring and testing to uphold\nusability standards outlined in earlier protocols. This process builds on\nprotocol-specific error conventions to ensure smooth handling across systems.</p><p>A centralized error code system can ensure uniformity across distributed\nservices. For example, Google uses the  format, which\nenforces standardized error structures with both machine-readable codes and\nhuman-readable messages <a href=\"https://google.aip.dev/193\">[4]</a>.</p><p>In multi-service architectures, two main approaches are common:</p><table><thead><tr></tr></thead><tbody><tr><td>Ensures uniform error codes, Acts as a single source of truth</td><td>Requires strict oversight</td></tr><tr><td>Distributed with Prefixes</td><td>Allows team independence, Enables quicker updates</td><td>Demands thorough documentation</td></tr></tbody></table><p>Many organizations find that combining these methods provides the best results.</p><p>Key metrics to monitor include\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://techcommunity.microsoft.com/discussions/appsonazure/best-practices-for-api-error-handling-a-comprehensive-guide/4088121\">[3]</a>:</p><ul><li><strong>Mean Time to Acknowledge (MTTA)</strong> errors: Target under 30 minutes.</li><li>: Should remain below 5% after fixes.</li><li><strong>95th percentile error resolution time</strong>: A critical benchmark for resolution\nspeed.</li><li><strong>User-impacting error ratio</strong>: Measured per 10,000 requests.</li></ul><p>Tools like <a href=\"https://sentry.io/\">Sentry</a> support distributed tracing in over 15\nlanguages, while <a href=\"https://raygun.com/\">Raygun</a> offers deployment correlation to\npinpoint issues <a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a>.</p><p>Testing ensures compliance with HTTP status codes and response formats covered\nin earlier sections. Error testing is typically done manually using tools like\nPostman - but you should definitely invest in automation as your API grows and\nevolves. To test specific errors, you should invest in automated\n<a href=\"https://zuplo.com/blog/2025/02/01/end-to-end-api-testing-guide\">end-to-end API testing</a> using\ntools like Playwright and StepCI.</p><p>If you have schematized errors, you can perform\n<a href=\"https://zuplo.com/blog/2024/07/19/verify-json-schema\">schema validation</a> on your live responses\nto ensure they adhere to those schemas. When response validation is combined\nwith an <a href=\"https://zuplo.com/blog/2024/09/25/mastering-api-definitions\">API design specification</a>\nlike OpenAPI to enforce outputs match what's documented, it's known as\n.</p><p>Effective API error handling builds on the protocol-specific conventions\ndiscussed earlier. Two key goals are ensuring <strong>consistent response formats</strong>\nand reducing repeat client errors\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://blog.postman.com/best-practices-for-api-error-handling/\">[5]</a>.</p><p>Key requirements include:</p><ul><li><strong>Standardized Response Structure</strong>:<ul><li>Machine-readable error codes</li><li>Clear, human-readable messages</li><li>Links to relevant documentation</li><li>Request correlation IDs for troubleshooting</li></ul></li><li><strong>Security-Focused Practices</strong>:<ul><li>Prevent exposure of sensitive data by adhering to security guidelines\noutlined in the Writing Clear Error Messages section</li><li>Use appropriate status codes</li><li>Follow established security best practices</li></ul></li></ul><p>To create a reliable error-handling system, follow these four phases:</p><ol><li>Use an\n<a href=\"https://zuplo.com/blog/2025/01/27/8-api-monitoring-tools-every-developer-should-know\">API monitoring tool</a>\nto analyze errors at the endpoint level. This helps identify inconsistencies\nand establish a baseline for improvement\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://blog.postman.com/best-practices-for-api-error-handling/\">[5]</a>.</li><li>Introduce centralized error-handling middleware to enforce the newly defined\nstandards. Often, an API gateway plays this role.</li><li>Use error tracking tools to evaluate the system‚Äôs performance. Track metrics\nlike error recurrence rates, resolution times, and Mean Time to Acknowledge\n(MTTA).</li></ol><p>These steps align with earlier recommendations for policy-driven error handling\nand offer practical ways to enhance your API's reliability.</p><p>Handling API errors effectively requires a clear and structured approach. This\ninvolves combining standard protocols with additional application-specific\ninformation to provide clarity and maintain security.</p><p><strong>1. Protocol-Specific Handling</strong></p><p>Each API protocol has its own method for managing errors. Here‚Äôs how to handle\nerrors for some common protocols:</p><ul><li>: Use HTTP status codes alongside detailed error messages in the\nresponse body.</li><li>: Include error arrays in the response, allowing for partial\nsuccess when appropriate.</li><li>: Utilize standardized status codes with structured error details.</li></ul><p><strong>2. Security Best Practices</strong></p><p>To keep your API secure, follow these guidelines (as outlined in the \"Security\nin Error Messages\" section):</p><ul><li>Use generic error messages for authentication failures to prevent revealing\nsensitive information.</li><li>Avoid exposing internal system details in error responses.</li><li>Filter sensitive data on the server side before sending error responses.</li></ul><p><strong>3. Monitoring and Consistency</strong></p><p>Set up monitoring tools, such as distributed tracing, to identify and analyze\nerror patterns. Use\n<a href=\"https://zuplo.com/blog/2024/12/16/api-gateway-hosting-options\">API gateways</a> to\nenforce consistent error formats and schemas across your system for better\nmanagement and debugging\n<a href=\"https://raygun.com/blog/best-error-monitoring-tools/\">[2]</a><a href=\"https://techcommunity.microsoft.com/discussions/appsonazure/best-practices-for-api-error-handling-a-comprehensive-guide/4088121\">[3]</a>.</p>","contentLength":10121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuvmvv/best_practices_for_consistent_api_error_handling/"},{"title":"Meetup: All in Kubernetes (Munich)","url":"https://www.reddit.com/r/kubernetes/comments/1iuvh2k/meetup_all_in_kubernetes_munich/","date":1740156652,"author":"/u/simplyblock-r","guid":8722,"unread":true,"content":"<p>Hey folks, if you're in or around Munich or Bavaria: this is for you! (if it's not a right place to post it, pls delete)</p><p>We're running our second meetup of the \"All in Kubernetes\" roadshow in Munich on Thursday, 13th of March. The first meetup, last month in Berlin, one was a big success with over 80 participants in Berlin.</p><p>Community is focused around stateful workloads in Kubernetes. The sessions lined up are:</p><ol><li>Architecting and Building a K8s-based AI Platform</li><li>Databases on Kubernetes: A Storage Story</li></ol>","contentLength":501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub Traffic - CLI Edition","url":"https://postimg.cc/XXWK9gzB","date":1740155625,"author":"/u/manifoldjava","guid":8851,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuv282/github_traffic_cli_edition/"},{"title":"list of decimal packages: fixed and big","url":"https://www.reddit.com/r/golang/comments/1iuunf1/list_of_decimal_packages_fixed_and_big/","date":1740154589,"author":"/u/kardianos","guid":8830,"unread":true,"content":"<p>I was updating a list of decimal packages. I thought I would share.</p><p>There are generally 2 varieties: fixed sized and arbitrary precision. The udecimal is interesting as it uses a fixed size for 128 bit precision with zero allocations, then uses an allocating \"*big.Int\" version for anything larger then that.</p><p>I currently use \"cockroachdb/apd\", which is a great package for frameworks or databases, but, it's a bit awkward to hold and lacks good formating. Realistically, I just need a fixed size decimal for my needs (financial/clinical). When I get a chance, I'll probably swap in for one of the fixed size packages.</p>","contentLength":615,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a new playground for Go, Pt, TS and more other, with Postgres... It supports program arguments, pretty output for JSON and I will add a lot feature soon","url":"https://codiew.io/ide","date":1740151971,"author":"/u/Halabooda","guid":8831,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iutm6h/i_built_a_new_playground_for_go_pt_ts_and_more/"},{"title":"Borrow Checker Trauma","url":"https://www.reddit.com/r/rust/comments/1iuthsl/borrow_checker_trauma/","date":1740151657,"author":"/u/xwaxes","guid":8784,"unread":true,"content":"<p>I am using the term ‚Äòborrow checker trauma‚Äô for lack of a better word. A bit of context first; I have been using Rust for my personal web projects extensively but use Rails at work. </p><p>So the problem is, whenever I am working on work projects and want to perform two or more operations on a variable, especially if I am passing it around or returning it, I always find myself taking a step back to consider if the ownership has moved before I remember that I am on Ruby and that doesn‚Äôt apply. </p><p>Has anyone experienced this in other languages or on their daily workflow?</p>","contentLength":571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Talk me out of using Mongo","url":"https://www.reddit.com/r/golang/comments/1iutb24/talk_me_out_of_using_mongo/","date":1740151172,"author":"/u/grdevops","guid":8652,"unread":true,"content":"<p>Talk me out of using Mongo for a project I'm starting and intend to make a publicly available service. I  love how native Mongo feels for golang, specifically structs. I have a fair amount of utils written for it and it's basically at a copy and paste stage when I'm adding it to different structs and different types. </p><p>Undeniably, Mongo is what I'm comfortable with have spend the most time writing and the queries are dead simple in Go (to me at least) compared to Postgres where I have not had luck with embedded structs and getting them to easily insert or scanned when querying (especially many rows) using sqlx. Getting better at postgres is something I can do and am absolutely 100% willing to do if it's the right choice, I just haven't run into the issues with Mongo that I've seen other people have</p><p>As far as the data goes, there's not a ton of places where I would need to do joins, maybe 5% of the total DB calls or less and I know that's where Mongo gets most of its flak. </p>","contentLength":984,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bugs with k8s snap and IPv6 only","url":"https://www.reddit.com/r/kubernetes/comments/1iurtp1/bugs_with_k8s_snap_and_ipv6_only/","date":1740147192,"author":"/u/hblok","guid":8633,"unread":true,"content":"<p>I'm setting up an IPv6  cluster, using Ubuntu 24.04 and the k8s and kubelet snaps. I've disabled IPv4 on the eth0 interface, but not on loopback. The CP comes up fine, and can be used locally and remotely. However, when trying to connect a worker node, there are some configuration options relating to IPv6 which I believe are bugs. I'd be interested to hear if these are misunderstandings on my part, or actual bugs.</p><p>The first is in the k8s-apiserver-proxy config file <code>/var/snap/k8s/common/args/conf.d/k8s-apiserver-proxy.json</code>. It looks like this, where the the last part is the port number 6443. The service does not start with a <em>\"failed to parse endpoint\"</em> error:</p><pre><code>{\"endpoints\":[\"dead:beef:1234::1:6443\"]} </code></pre><p>When correcting the address to use brackets, it will start up correctly.</p><pre><code>{\"endpoints\":[\"[dead:beef:1234::1]:6443\"]} </code></pre><p>Secondly, the snap.k8s.kubelet.service will not start, trying to bind to 0.0.0.0:10250 , but fails with <em>\"Failed to listen and serve\" err=\"listen tcp 0.0.0.0:10250: bind: address already in use\"</em>. Here I'm not sure where the address and port is coming from, but I'm guessing it's a default somewhere. Possibly <a href=\"https://github.com/kubernetes/kubernetes/issues/108248\">related to this report</a>.</p>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gofs - a file server written in go","url":"https://www.reddit.com/r/golang/comments/1iuqggw/gofs_a_file_server_written_in_go/","date":1740143207,"author":"/u/-dtdt-","guid":9077,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/-dtdt-\"> /u/-dtdt- </a>","contentLength":29,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Certifications for software architects","url":"https://www.cerbos.dev/blog/certifications-for-enterprise-architects-domain-solutions-architects-software-engineers","date":1740142976,"author":"/u/West-Chard-1474","guid":8598,"unread":true,"content":"<p>Over the years, I‚Äôve noticed that no one can quite settle on how important certification is. All it takes is one look at the Software Architecture subreddit and you‚Äôll see people asking about certificates only to be told they‚Äôre both useless and useful.</p><p>When I was working with Lemon.io (a developer marketplace with 80k+ developers), I got to see firsthand how certification affected their careers. So when I saw this topic start to pop up again (without any real answers), I decided to dive into research to see if my experience at Lemon.io still held true for architects.</p><h2>The value of certification</h2><p>Assuming I‚Äôm talking to architects with a lot of experience, what I will say is that certification doesn‚Äôt replace experience, but it does complement it really well. And, it can be a strong differentiator from your peers.</p><p>I‚Äôm going to try a metaphor here. If your career is a burger, your experience is the patty and certificates are the condiments. Some people prefer their burgers with bacon, cheese, or even an egg. Depending on what your career goals are, you‚Äôll want to add different condiments to your ‚Äòburger‚Äô. But keep in mind, the most important thing will always be the meat, a.k.a your experience.</p><p>Deciding if certification is right for you is the first step. The next step is asking the question: what is the right certification for you?</p><p>I loved the Role Based Roadmap from Mumshad Mannambeth, founder &amp; CEO at KodeKloud on navigating the certification paths <a href=\"https://www.linkedin.com/posts/mmumshad_kodekloud-cloudcomputing-aws-activity-7110238676915273728-U2X8/?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAB-THdoBNL-y76_DeuYlNKmEH_yOiRzPAjc\">(you can find it here)</a>.</p><p>It inspired me to do something similar but focused on Architects. Below, you‚Äôll find 12 of the most popular architectural certificates you can choose to upgrade your career, and add more ‚Äútrust badges‚Äù to your CV, LinkedIn profile, or freelancer profile.</p><p>Each has its own focus and value it can add depending on your career goals and role:</p><table><thead><tr><th><strong>Certifications for Enterprise Architects</strong></th><th><strong>Certifications for Domain Solutions Architects</strong></th><th><strong>Software Architecture, Governance, and Infrastructure Certification</strong></th></tr></thead><tbody><tr><td>AWS Certified Solutions Architect</td></tr><tr><td>Google Professional Cloud Architect</td></tr><tr><td>Zachman Certified - Enterprise Architect</td><td>Microsoft Certified: Azure Solutions Architect Expert</td></tr><tr><td>Certified Enterprise Architect (CEA) Black Belt Program</td><td>Red Hat Certified Architect (RHCA)</td></tr></tbody></table><p>Recognized globally, <a href=\"https://www.credly.com/org/the-open-group/badge/the-open-group-certified-togaf-9-certified\">this certificate</a> covers the TOGAF framework for designing, planning, implementing, and governing enterprise information technology architecture. So if you are working on an enterprise-wide architecture or want to switch to that direction, it can be useful.\nUnlike the certifications below, this is a whole ecosystem that builds on itself. So it is a significant investment. However, if you‚Äôre working in, or looking to work in, the governmental sector or with large corporations, this may be a good choice for you. Keep in mind, however, that while it may look cheap, the cost does not include training, which is provided by TOGAF-accredited partners, each of whom sets their own price.</p><p>The comment below sums up my research into TOGAF 9 really well:</p><p><strong>Best for Software Architects working in large organizations</strong>: Enterprise Architects,  Business Architects, Solutions Architects, and IT Strategy Consultants with experience in strategic planning, We had a few Enterprise Architects with TOGAF 9 certification at Lemon.io and it was a nice value-add to their profiles. It is worth mentioning that their rates were in the top tier üôÇ.</p><ul><li>13 Level 1 Learning Units</li><li>27 Level 2 Learning Units</li></ul><p>Testing: Two-stage testing, including TOGAF 9 Part 1 and TOGAF 9 Part 2 examinations</p><table><thead><tr></tr></thead><tbody><tr><td>$360 USD per exam (requires two exams)</td><td>English, Simplified Chinese, Spanish (Latin American), French</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: TOGAF 9 certificate does not expire.</p><p>ITIL is one of the most popular certification systems in the world, with more than two million certified specialists in the world. The <a href=\"https://www.axelos.com/certifications/itil-service-management\">Master certification</a> is the highest level of ITIL certifications and requires passing all four previous certifications before challenging it. To achieve the master certification you have to pass an assessment to validate your ability to apply ITIL frameworks to real-world business scenarios. So, experience working with the ITIL principals and practices as an enterprise software architect is required.</p><p>ITIL is not a training or testing provider but works with accredited partners for each. That means the pricing is dependent on your provider. The nice thing is, that it is one of the few large accreditations you can achieve through self-study.</p><p>If you want to chat with peers who are planning to become ITIL Masters, there is an active subreddit called <a href=\"https://www.reddit.com/r/ITIL_Certification/\">ITIL_Certification</a>.</p><p>: Governance &amp; Compliance Architects, Governance and Compliance Managers, Enterprise Architects.</p><p>Five distinct levels to progress through: Foundation -&gt; Practitioner ‚Äì&gt; Intermediate -&gt; Expert -&gt; Master. Each level has its own training and requirements.</p><ul><li>Attend a training course with an accredited training organization, which will include the exam as part of the course.</li><li>Self-study using the core manual, then book an exam directly with PeopleCert.</li></ul><table><thead><tr></tr></thead><tbody><tr><td>Dependent on your chosen training/testing partner.</td><td>All 4 previous ITIL certifications, including ITIL Expert certification</td><td>5 years in IT leadership, management, or higher management advisory levels.</td><td>English, Brazilian Portuguese, Chinese, Dutch, French, German, Italian, Japanese, Polish, Spanish, Thai</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: Certification is valid for 3 years. You can renew your certification by retaking the exam or by earning a new certification.</p><h2>Zachman Certified - Enterprise Architect</h2><p><a href=\"https://zachman-feac.com/\">This certification</a> covers the Zachman Framework for designing and maintaining Enterprise Architectures, which aligns IT with business goals. The training is focused on high-level enterprise planning, including Enterprise Architecture principles, strategy formulation, and practical application, rather than project or solutions architecture. This makes it ideal for those who need a structured approach to solve enterprise challenges.</p><p>This is another multi-level certification option. Each level builds on the one before it, which makes it a very comprehensive course. And, in my humble opinion, very expensive.</p><p>: Enterprise Architects, Business Architects, and IT Consultants with an Enterprise Architecture focus.</p><p>Four levels available: Associate, Practitioner, Professional and Educator (last one only required for those who want to teach the course).</p><p>Training: Two weeks of online prep work, and three days of live instruction</p><p>Testing: Two-hour, online exam (passing grants level 1 Associate)</p><p>Case study: Delivering a case study provides level 2 Practitioner, and a second case study provides level 3 Professional</p><table><thead><tr></tr></thead><tbody><tr><td>$2999 USD covers level 1 &amp; 2 (regional pricing is available)</td><td>Each level requires certification in the preceding level</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate expires in 3 years. Recertification costs $99 USD.</p><h2>Certified Enterprise Architect (CEA) Black Belt Program (Owned by Zachman now)</h2><p>Zachman now owns CEA, so I decided to add this certification under the Zachman banner. Just like Zachman, this is a three-step course, except the naming convention is designed to make you feel like you‚Äôre in a martial art, which is cool. It‚Äôs also $7,000 more, which is not as cool, but that does mean you get to skip the yellow and green belt and go straight for the black belt.</p><p>Designed to develop Enterprise Architects through hands-on training and real-world projects, <a href=\"https://zachman-feac.com/\">this program</a> will prepare you for leadership roles in Enterprise Architecture. This program is built on ISO standards and focuses on the practical application of frameworks, tools, and methodologies.</p><p>: (Very rich people) Senior Enterprise Architect, IT Director with EA experience, Chief Architect, Enterprise Architect with 10+ years experience, Enterprise Architecture Consultant.</p><ul><li>Accelerated Path of 12 Weeks: Five individual courses taught in parallel over twelve weeks.</li><li>Progressive Path that is self-paced: Five individual online courses taken at your own pace over 24-30 weeks.</li></ul><p>The <a href=\"https://iasaglobal.org/Public/Public/Learn/Training_and_Certifications.aspx\">IASA Global certification</a> is a vendor-independent program for Business Technology Architects. The training has four stages that roughly align with your career stage. For Enterprise Architects, the professional (3rd) tier is the most useful  The previous two tiers are for those in earlier stages of their career. The training is based on practical experience with a focus on Enterprise Architecture (EA), Software Architecture (SA), Solution Architecture (SolA), Infrastructure Architecture (IA) and Business Architecture (BA).</p><p>Unlike most tiered options, IASA allows you to challenge each level even if you haven‚Äôt attained the certification under it. So if you‚Äôve progressed in your career far enough that you don‚Äôt think a foundational certification is valuable, and you don‚Äôt want to work your way through 3 tiers you already fully understand, IASA may be the answer for you.</p><p>: Senior architects and business analysts aiming to bridge the gap between business and technology.</p><p>Four levels of certification:</p><ul><li>Professional (recommended for enterprise-level architects) - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2 hrs allotted for the exam.</li><li>Distinguished - achieved by presenting to a board of CITA-D certified architects and answering any questions. 2.5 hrs allotted for the exam.</li></ul><p>Testing: Online or onsite testing is available</p><table><thead><tr><th> (for professional tier)</th></tr></thead><tbody><tr><td> Exam: $425 USD  N/A  Exam + prep: $2,000 USD  Exam + prep: $2,800 USD</td><td>A minimum of 10 years in the industry as a practicing architect.  Extensive documentation is required.</td><td>CITA-Associate level certificate is recommended but not mandatory.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: Requires individuals to maintain an active Full Iasa Membership and collect at least 80 hours of Continuing Education Units bi-annually (based on their website).</p><h2>The Open Group ArchiMate 3 Certification</h2><p>ArchiMate was mentioned a lot over Reddit. <a href=\"https://www.opengroup.org/certifications/archimate\">The Open Group ArchiMate 3</a> doesn‚Äôt teach you how to be an EA but rather focuses on how to communicate better as an EA by teaching ArchiMate‚Äôs modelling language. This language is designed to remove ambiguity from the description, analysis, and visualization of Enterprise Architectures.</p><p>The content of the course is designed to complement TOGAF, which makes it useful for Enterprise Architects who already work in a TOGAF framework. They aren‚Äôt however, the same thing.</p><p>: Enterprise Architects</p><p>Training: Online self-study available, or attend an accredited training course (Accredited Training Courses provide an exam voucher).</p><ul><li>ArchiMate 3 Part 1 offers foundation certification (60 min time limit for 40-question, multiple choice exam. Passing score: 60%)</li><li>ArchiMate 3 Part 2 offers practitioner certification (90 min time limit for 8-question, scenario-based and complex multiple choice exam. Passing score: 65%)</li></ul><p>Online or in-person proctored exams available depending on provider</p><table><thead><tr></tr></thead><tbody><tr><td>Training cost depends on the provider.  Each exam costs $360 USD.</td><td> None  Foundation certification or pass Part 1 exam on the same day with the same provider.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certification does not expire, which is very cool.</p><h2>AWS Certified Solutions Architect</h2><p><a href=\"https://aws.amazon.com/certification/certified-solutions-architect-associate/?nc1=f_ls\">The AWS certification</a> is a great starting point for architects or senior Software Engineers with AWS Cloud or strong on-premises IT experience. It covers the design and optimization of AWS cloud-based software and shows you can handle complex multi-service architectures, which is crucial as more companies move to the cloud. The exam tests real-world scenarios you would face designing large-scale systems, including hybrid architectures, multi-region deployments, and cost optimization at scale. Basic familiarity with programming concepts will help, but you don‚Äôt need hands-on experience with code.</p><p>This is the only provider-specific certificate that offers a more in-depth, self-directed training option for a price. Of course, you don‚Äôt have to take that option. If you‚Äôre confident, you can take the free training and then challenge the test. However, the test is also the cheapest among these certifications, so it offsets the overall cost a little bit.</p><p>During my research, I found quite a few posters, including <a href=\"https://www.reddit.com/r/AWSCertifications/comments/1h5jfmp/how_did_passing_aws_solution_architect/\">this one</a>, that had seen significant benefits from taking the course.</p><p>: Systems Administrators (Cloud Focused), Cloud Architects, Solutions Architects, Cloud Consultants Software Architects for Cloud-Based Applications, and Enterprise Architects.</p><ul><li>3 courses of online, self-directed exam prep are available</li><li>15.25 hrs free; 48.75 hrs paid</li></ul><p>Testing: 130 minutes. Pearson VUE testing center, or online proctored exam</p><table><thead><tr></tr></thead><tbody><tr><td>1 year of hands-on experience designing cloud solutions with AWS services.</td><td>English, French (France), German, Italian, Japanese, Korean, Portuguese (Brazil), Spanish (Latin America), Spanish (Spain), Simplified and Traditional Chinese</td></tr></tbody></table><h2>Google Professional Cloud Architect</h2><p>If you‚Äôre all in on Google, <a href=\"https://cloud.google.com/learn/certification/cloud-architect\">this certification</a> is for you. It will help you show your cloud architecture skills and advance your career in Google‚Äôs cloud technology. Keep in mind, this is focused on Google‚Äôs cloud infrastructure and doesn‚Äôt cover software architecture.</p><p>The training and exam for this certificate are all online (though on-site exams are available) which makes it very flexible. Plus, it‚Äôs pretty cheap (although it‚Äôs the most expensive of the provider-specific options). However, if you go through all the training, it‚Äôs going to take you some time, as it‚Äôs the longest provider-specific course here.</p><p>: Cloud Architects, Solutions Architects, IT Project Managers focused on the cloud, Cloud Engineers, DevOps Engineers, and Enterprise Architects.</p><p>Training: 114.75 hrs, online, self-directed</p><p>Testing: 2-hr test, with two options: an online proctored exam, or an onsite-proctored exam at a testing center.</p><table><thead><tr></tr></thead><tbody><tr><td>3+ years of industry experience, including 1+ year of designing and managing solutions with Google Cloud</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate is valid for 2 years. Recertify by retaking the exam within 60 days of the expiration date.</p><h2>Microsoft Certified: Azure Solutions Architect Expert</h2><p><a href=\"https://learn.microsoft.com/en-us/credentials/certifications/azure-solutions-architect/\">This certificate</a> shows you know your way around Azure, including how to design and implement cloud and hybrid solutions. It dives deep into how various IT infrastructure components in the Microsoft ecosystem (like compute, network, storage, monitoring, and security) interact to generate solutions. Just like the Google certificate above, this is infrastructure-focused, not software-focused, so keep that in mind when considering it.</p><p>The course for the Microsoft certification is shorter than Google‚Äôs by almost 100 hours and can be taken both online at your own pace, or with an online instructor. It‚Äôs also a bit cheaper than Google‚Äôs, making it an easier investment both for hours and dollars spent.</p><p>: Systems Administrators, Network Engineers, IT Managers, Cloud Architects, Computer Systems Analysts and Infrastructure Engineers.</p><p>Training: Self-paced online learning (15.25 hrs), or instructor-led online training (4 days).</p><p>Testing: Online proctored exam.</p><table><thead><tr></tr></thead><tbody><tr><td>$165 USD  Self-paced training is free; instructor-led depends on the provider.</td><td>Experience with Azure administration and development, and DevOps processes.  Advanced experience and knowledge of IT operations.</td><td>English, Chinese (Simplified), French, German, Japanese, Korean, Portuguese (Brazil), Spanish</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate expires after one year. Recertification is free via an online assessment on Microsoft Learn.</p><h2>Red Hat Certified Architect (RHCA)</h2><p>Red Hat‚Äôs highest level of certification, the <a href=\"https://www.redhat.com/en/services/certification/rhca\">RHCA designation</a> covers designing, implementing, and managing Red Hat-based IT infrastructures. One of the nice things is that RHCA offers tracks in both infrastructure and enterprise applications. So you can choose the option that best matches your goals.</p><p>Red Hat prices its certification differently than most. It‚Äôs a subscription-based model, which allows you to have a little more freedom with how you tackle their training. In fact, their certification offers the most custom options, with a variety of options to get from A (where you are) to B (certified). So if you have diverse interests, this one might be the one for you.</p><p>: Senior Systems Administrator, Cloud Architect, IT Infrastructure Architect, DevOps Engineer, Senior Application Developer, Enterprise Solutions Architect</p><ul><li>Red Hat Certified Architect in Infrastructure</li><li>Red Hat Certified Architect in Enterprise Applications</li></ul><p>Certification builds on prerequisites with five additional certifications chosen from a list.</p><p>Training and exams depend on your chosen path and certifications.</p><table><thead><tr></tr></thead><tbody><tr><td> Standard: $7,500 USD/year for 25 training units and certification. <p> Premium: $9,000 USD/year for 30 training units and certification.</p></td><td>Red Hat Certified Engineer (RHCE)  OR <p> Red Hat Certified Enterprise Microservices Developer (RHCEMD) </p> OR <p> Red Hat Certified Cloud-native Developer (RHCCD)</p></td><td>Recommended experience depends on your specific path.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: Certifications become ‚Äònon-current‚Äô after three years. To maintain an RHCA certification, you must maintain five additional certifications over your RHCE. RHCEMD or RHCCD. These certifications do not need to be the same as those you‚Äôve taken to attain your RHCA.</p><p>Made for IT professionals who want to master SOA, <a href=\"https://www.arcitura.com/cert/soa-architect-certification-exam.html\">this certification</a> covers designing, implementing, and managing Service-Oriented Architecture (SOA) infrastructure.</p><p>SOA certification is one and done. There are no levels or long-term commitment to one system, which makes it less of a commitment. It‚Äôs also very affordable.</p><p>: Enterprise Architects, Solutions Architects, IT Architects, Software Architects, Systems Engineers with SOA experience, Technical Leads with architecture responsibilities</p><ul><li>170 mins online proctored exam</li><li>50 hrs of training over 5 modules (modules include: Workbook Lessons, Video Lessons, Interactive Exercises, Mind Map Poster, Practice Exam Questions, PDFs of Workbook and Poster, Lab Exercise Booklet).</li></ul><table><thead><tr></tr></thead><tbody><tr><td>$399 USD for course and certification</td></tr></tbody></table><h2>iSAQB CPSA-F/CPSA-A (International Software Architecture Qualification Board)</h2><p>In contrast to TOGAF training, <a href=\"https://www.isaqb.org/certifications/cpsa-certifications/cpsa-foundation-level/\">the CPSA program</a> focuses on the practical implementation of IT systems. Its foundation and advanced certificates offer room for architects to grow.</p><p>This is a two-step program, foundation and advanced, which puts it between the single-step SOA certification and the larger three- or even four-step offerings. Just like TOGAF and ITIL, iSAQB is not a testing or training provider, so there are a lot of options for training. Or, if you‚Äôre confident, you can challenge the exam without, though that‚Äôs not recommended.</p><p>: Software designers, software developers, Software Architects, systems analysts</p><p>\nThough training and testing are performed by independent operators, you have four testing options available, including:</p><ul><li>Exam after classroom training</li></ul><table><thead><tr></tr></thead><tbody><tr><td>The cost of training is dependent on training providers.  Testing price is dependent on training providers.</td><td>Training is suggested.  Foundation certification is required for Advanced.</td><td>18+ months of practical experience, including:  - A higher programming language <p> - Technical documentation </p> - Object-oriented programming language <p> - Design and implementation of distributed applications </p> - Basics of modeling and abstraction; and UML and their relation to source</td><td>Language is dependent on training providers.</td></tr></tbody></table><p><strong>Certification expiry &amp; recertification</strong>: The certificate does not expire.</p><p>The right certification can set you apart on your resume, but it‚Äôs never a replacement for experience. Depending on your career path, you may choose to get certified by one of the bodies above, or simply study on your own and prove what you can do through practical experience.</p><p>If you‚Äôve decided to pursue the certification path, the options above are all great choices. Of course, each requires a commitment of time and money. While you can‚Äôt warp the space-time continuum to change the time requirement, it may be possible to get your employer to help you cover some, if not all of the cost. If they do, it gives you an even higher ROI on your investment to yourself.</p>","contentLength":20228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuqdq8/certifications_for_software_architects/"},{"title":"This month in Servo: new webview API, relative colors, canvas buffs, and more!","url":"https://servo.org/blog/2025/02/19/this-month-in-servo/","date":1740142406,"author":"/u/wuyuwei-tw","guid":8721,"unread":true,"content":"<p>Servo now supports several new web API features:</p><p>We‚Äôve landed a bunch of  improvements:</p><p> are a lot more useful now, with  now supporting  (<a href=\"https://github.com/Taym95\">@Taym95</a>, <a href=\"https://github.com/servo/servo/pull/35040\">#35040</a>), , , and  (<a href=\"https://github.com/Taym95\">@Taym95</a>, <a href=\"https://github.com/servo/servo/pull/34958\">#34958</a>).</p><p>Servo aims to be an embeddable web engine, but so far it‚Äôs been a lot harder to embed Servo than it should be.</p><p>For one, configuring and starting Servo is complicated.\nWe found that getting Servo running at all, even without wiring up input or handling resizes correctly, took  of Rust code (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35118\">#35118</a>).\nEmbedders (apps) could only control Servo by sending and receiving a variety of ‚Äúmessages‚Äù and ‚Äúevents‚Äù, and simple questions like ‚Äúwhat‚Äôs the current URL?‚Äù were impossible to answer without keeping track of extra state in the app.</p><p>Contrast this with <a href=\"https://webkitgtk.org/\">WebKitGTK</a>, where you can write a minimal kiosk app with a fully-functional webview in  of C.\nTo close that gap, we‚Äôve started <strong>reworking our embedding API</strong> towards something more idiomatic and ergonomic, starting with the concept embedders care about most: the .</p><p>Our new webview API is controlled by calling methods on a  (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35119\">#35119</a>, <a href=\"https://github.com/servo/servo/pull/35183\">#35183</a>, <a href=\"https://github.com/servo/servo/pull/35192\">#35192</a>), including navigation and user input.\nHandles will eventually represent the lifecycle of the webview itself; if you have one, the webview is valid, and if you drop them, the webview is destroyed.</p><p>Servo needs to call into the embedder too, and here we‚Äôve started replacing the old EmbedderMsg API with a  (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35211\">#35211</a>), much like the delegates in <a href=\"https://developer.apple.com/documentation/webkit/wkuidelegate?language=objc\">Apple‚Äôs WebKit API</a>.\nIn Rust, a delegate is a  that the embedder can install its own  for.\nStay tuned for more on this next month!</p><p>Other embedding improvements include:</p><p>We‚Äôve reworked Servo‚Äôs , making all prefs optional with reasonable defaults (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/34966\">#34966</a>, <a href=\"https://github.com/servo/servo/pull/34999\">#34999</a>, <a href=\"https://github.com/servo/servo/pull/34994\">#34994</a>).\nAs a result:</p><ul><li><strong>The names of all preferences have changed</strong>; see the <a href=\"https://doc.servo.org/servo_config/prefs/struct.Preferences.html\">Prefs docs</a> for a list</li><li><strong>Embedders no longer need a </strong> resource to get Servo running</li></ul><p>Servo‚Äôs networking is more efficient now, with the ability to <strong>cancel fetches for navigation</strong> that contain redirects (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/34919\">#34919</a>) and <strong>cancel fetches for &lt;video&gt; and &lt;media&gt;</strong> when the document is unloaded (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/34883\">#34883</a>).\nThose changes also <strong>eliminate per-request IPC channels</strong> for navigation and cancellation respectively, and in the same vein, we‚Äôve eliminated them for image loading too (<a href=\"https://github.com/mrobinson\">@mrobinson</a>, <a href=\"https://github.com/servo/servo/pull/35041\">#35041</a>).</p><p>We‚Äôve continued <strong>splitting up our massive script crate</strong> (<a href=\"https://github.com/jdm\">@jdm</a>, <a href=\"https://github.com/servo/servo/pull/34359\">#34359</a>, <a href=\"https://github.com/servo/servo/pull/35157\">#35157</a>, <a href=\"https://github.com/servo/servo/pull/35169\">#35169</a>, <a href=\"https://github.com/servo/servo/pull/35172\">#35172</a>), which will eventually make Servo much faster to build.</p><p>We now run <strong>CI smoketests on OpenHarmony</strong> using a real device (<a href=\"https://github.com/jschwe\">@jschwe</a>, <a href=\"https://github.com/mukilan\">@mukilan</a>, <a href=\"https://github.com/servo/servo/pull/35006\">#35006</a>), increasing confidence in your changes beyond compile-time errors.</p><p>We‚Äôve also tripled our <strong>self-hosted CI runner capacity</strong> (<a href=\"https://github.com/delan\">@delan</a>, <a href=\"https://github.com/servo/servo/pull/34983\">#34983</a>, <a href=\"https://github.com/servo/servo/pull/35002\">#35002</a>), making concurrent Windows and macOS builds possible without falling back to the much slower GitHub-hosted runners.</p><p>Servo can‚Äôt yet run WebDriver-based tests on <a href=\"https://wpt.fyi\">wpt.fyi</a>, <a href=\"https://wpt.servo.org\">wpt.servo.org</a>, or CI, because the  executor for the <a href=\"https://web-platform-tests.org\">Web Platform Tests</a> does not support testdriver.js.\n does, though, so we‚Äôve started fixing test regressions with that executor with the goal of eventually switching to it (<a href=\"https://github.com/jdm\">@jdm</a>, <a href=\"https://github.com/servo/servo/pull/34957\">#34957</a>, <a href=\"https://github.com/servo/servo/pull/34997\">#34997</a>).</p><p>Thanks again for your generous support!\nWe are now receiving  (‚àí11.4% over December) in recurring donations.\nWith this money, we‚Äôve been able to expand our capacity for <a href=\"https://ci0.servo.org\">self-hosted</a><a href=\"https://ci1.servo.org\">CI</a><a href=\"https://ci2.servo.org\">runners</a> on Windows, Linux, and macOS builds, <strong>halving  build times</strong> from over an hour to under 30 minutes!</p><p>Servo is also on <a href=\"https://thanks.dev\">thanks.dev</a>, and already  (+5 over December) that depend on Servo are sponsoring us there.\nIf you use Servo libraries like <a href=\"https://crates.io/crates/url/reverse_dependencies\">url</a>, <a href=\"https://crates.io/crates/html5ever/reverse_dependencies\">html5ever</a>, <a href=\"https://crates.io/crates/selectors/reverse_dependencies\">selectors</a>, or <a href=\"https://crates.io/crates/cssparser/reverse_dependencies\">cssparser</a>, signing up for <a href=\"https://thanks.dev\">thanks.dev</a> could be a good way for you (or your employer) to give back to the community.</p><p>As always, use of these funds will be decided transparently in the Technical Steering Committee.\nFor more details, head to our <a href=\"https://servo.org/sponsorship/\">Sponsorship page</a>.</p>","contentLength":3866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iuq74e/this_month_in_servo_new_webview_api_relative/"},{"title":"Is this architecture possible without using haproxy but nginx(in rocky linux 9)?","url":"https://www.reddit.com/r/kubernetes/comments/1iupwhs/is_this_architecture_possible_without_using/","date":1740141445,"author":"/u/Keeper-Name_2271","guid":8575,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alerting from Prometheus and Grafana with kube-prometheus-stack","url":"https://www.reddit.com/r/kubernetes/comments/1iupvn8/alerting_from_prometheus_and_grafana_with/","date":1740141365,"author":"/u/HumanResult3379","guid":8917,"unread":true,"content":"<p>In Grafana page's , I find the built-in alert rules named .</p><p>I set Slack Contact points. But when the Alert Firing, it didn't send to Slack.</p><p>If I create a customized alert in Grafana, it can be sent to Slack. So does the alert-rules above only for seeing?</p><p>By the way, I find almost the same alert in Prometheus' AlertManager. I set a slack notification endpoint and the messages been sent there!</p><ol><li>Are the prometheus' alert-rules the same as  in Grafana Alert rules page like the picture above?</li><li>If want send alert from Grafana, does it only possible use new created alert rule manually in Grafana?</li></ol>","contentLength":589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Have we hit a scaling wall in base models? (non reasoning)","url":"https://www.reddit.com/r/artificial/comments/1iupqgp/have_we_hit_a_scaling_wall_in_base_models_non/","date":1740140885,"author":"/u/CH1997H","guid":9078,"unread":true,"content":"<p>Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 Sonnet</p><p>Yet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the \"scaling laws\" where the chart just says \"line goes up\")</p><p>Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scaling</p><p>It looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it</p>","contentLength":850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I built a new playground for Go","url":"https://codiew.io/ide?t=go","date":1740140631,"author":"/u/Halabooda","guid":8597,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iupnrn/i_built_a_new_playground_for_go/"},{"title":"[D] Have we hit a scaling wall in base models? (non reasoning)","url":"https://www.reddit.com/r/MachineLearning/comments/1iupnet/d_have_we_hit_a_scaling_wall_in_base_models_non/","date":1740140600,"author":"/u/CH1997H","guid":8632,"unread":true,"content":"<p>Grok 3 was supposedly trained on 100,000 H100 GPUs, which is in the ballpark of about 10x more than models like the GPT-4 series and Claude 3.5 Sonnet</p><p>Yet they're about equal in abilities. Grok 3 isn't AGI or ASI like we hoped. In 2023 and 2024 OpenAI kept saying that they can just keep scaling the pre-training more and more, and the models just magically keep getting smarter (the \"scaling laws\" where the chart just says \"line goes up\")</p><p>Now all the focus is on reasoning, and suddenly OpenAI and everybody else have become very quiet about scaling</p><p>It looks very suspicious to be honest. Instead of making bigger and bigger models like in 2020-2024, they're now trying to keep them small while focusing on other things. Claude 3.5 Opus got quietly deleted from the Anthropic blog, with no explanation. Something is wrong and they're trying to hide it</p>","contentLength":850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I created A Easy to use Rust Web Framework","url":"https://www.reddit.com/r/rust/comments/1iuplg1/i_created_a_easy_to_use_rust_web_framework/","date":1740140417,"author":"/u/Rough_Shopping_6547","guid":8832,"unread":true,"content":"<p>I just published my  project!</p><p>I realized there isn‚Äôt a single easy-to-use, plug-and-play Rust web framework out there (at least to my knowledge), so I decided to create my own.</p><p>I'd love to hear your thoughts on it!</p>","contentLength":214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Junior, Trying to understand why startups use golang for backend","url":"https://www.reddit.com/r/golang/comments/1iup4di/junior_trying_to_understand_why_startups_use/","date":1740138781,"author":"/u/FriendshipOk6564","guid":8573,"unread":true,"content":"<p>Hello,i just took a look at the website 'who is hiring' and saw a lot of startups using ruby on rails and golang in their stack and i'm confuse, the path isn't normally mvp in rails and after some companies will rewrite their wall backend at some point in something like Java spring? it append for netflix but also a big company where i live. Why would they mixte ror and golang? Those it mean they are rewriting their ror in a microservice architecture in go?</p>","contentLength":460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to properly prepare monorepos in Golang and is it worth it?","url":"https://www.reddit.com/r/golang/comments/1iuoppk/how_to_properly_prepare_monorepos_in_golang_and/","date":1740137237,"author":"/u/GoDuffer","guid":8780,"unread":true,"content":"<p>Hello everyone. At the moment I am writing a report on the topic of a monorepo in order to close my internship at the university.</p><p>Since I am a Go developer (or at least I aspire to be one), I decided to make a monorepo in Go.</p><p>The first thing I came across was an article from Uber about how they use Bazel and I started digging in this direction.</p><p>And then I realized that it was too complicated for small projects and I became interested.</p><p>Does it make sense to use a monorepo on small projects? If not, how to split the application into services? Or store each service in a separate repository.</p><p>In Java, everything is trivially simple with their modules and Gradle. Yes, Go has modules and a workspace, but let's be honest, this is not the level of Gradle.</p><p>As a result, we have that Bazel is too complicated for simple projects, and gowork seems somehow cut down after Gradle.</p><ol><li><p>Monorepo or polyrepo for Go?</p></li><li><p>Is there anything other than go work and Bazel?</p></li><li><p>What is the correct way to split a Go project so that it looks like a Solution in C#, or modules in Java/Gradle?</p></li></ol><p>It is quite possible that I really don't understand the architecture of Go projects, I will be glad if you point me in the right direction.</p>","contentLength":1196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Deeper Love of Go (Go 1.24 early access edition)","url":"https://bitfieldconsulting.com/books/deeper","date":1740136216,"author":"/u/bitfieldconsulting","guid":8525,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iuogi4/the_deeper_love_of_go_go_124_early_access_edition/"},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1iuob4d/weekly_share_your_victories_thread/","date":1740135633,"author":"/u/gctaylor","guid":8500,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI and the future of work - an EU perspective","url":"https://v.redd.it/cx0l3st20hke1","date":1740134098,"author":"/u/snehens","guid":8653,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iunxt8/ai_and_the_future_of_work_an_eu_perspective/"},{"title":"Getting organised! ¬∑ AerynOS","url":"https://github.com/orgs/AerynOS/discussions/37","date":1740133024,"author":"/u/Wooden-Opposite3557","guid":8720,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iunoxx/getting_organised_aerynos/"},{"title":"ChatGPT took an oath to protect its own.üòÑü§ñ","url":"https://www.reddit.com/r/artificial/comments/1iuno62/chatgpt_took_an_oath_to_protect_its_own/","date":1740132932,"author":"/u/snehens","guid":8526,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My experience after switching from Java to Go","url":"https://www.reddit.com/r/golang/comments/1iuni44/my_experience_after_switching_from_java_to_go/","date":1740132223,"author":"/u/hosmanagic","guid":8498,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/hosmanagic\"> /u/hosmanagic </a>","contentLength":33,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AVR microcontrollers are now officially maintained!","url":"https://www.reddit.com/r/rust/comments/1iunfgx/avr_microcontrollers_are_now_officially_maintained/","date":1740131957,"author":"/u/Patryk27","guid":8499,"unread":true,"content":"<p>AVRs are cute &amp; tiny microcontrollers from Atmel - you might've heard about ATmega328p used in Arduino Uno, for example:</p><p>Every week we're marching towards better AVR support in Rust and as of today I can proudly say: we don't need no `target.json`s anymore + we've got an official maintainer! (points finger at self)</p><p>So far AVRs remain tier 3, but at least it's waay easier to use them now - just target `avr-none` and provide `-C target-cpu` so that rustc &amp; llvm know which specific microcontroller you're building for; <a href=\"https://github.com/llvm/llvm-project/pull/118015\">a couple</a> of <a href=\"https://github.com/llvm/llvm-project/pull/121498\">important</a> codegen <a href=\"https://github.com/llvm/llvm-project/pull/106722\">fixes</a> are also coming together with rustc's upgrade to LLVM 20, hoping to wrap up on <a href=\"https://github.com/Rahix/avr-hal/pull/585\">https://github.com/Rahix/avr-hal/pull/585</a> over the coming days.</p><p>I'd like to take this moment to thank <a href=\"https://github.com/benshi001\">https://github.com/benshi001</a> for his continued support and code reviews on the LLVM's side - let AVR flourish!</p>","contentLength":845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sponsoring Rust Developers","url":"https://www.reddit.com/r/rust/comments/1iun7oj/sponsoring_rust_developers/","date":1740131048,"author":"/u/szabgab","guid":8855,"unread":true,"content":"<p>One of the \"findings\" of my <a href=\"https://www.reddit.com/r/rust/comments/1ital1t/why_dont_you_use_rust_at_your_company/\">previous question</a> was that some crates are missing or not mature enough to be used.</p><p>If you would like to use Rust you can hope that those gaps will be closed in time or you can do something about it. If you have the time and expertise you can get involved in the needed projects, but there is a much easier and less time-consuming way. You and/or your company can sponsor the development efforts.</p><p>Allocating 10-20 USD / month by an individual or 1000-2000 USD month by a small company does not sound like a big investment and many such sponsors can make a huge difference together.</p><p>One way to find who to sponsor is to find the developers of your dependencies. For that visit the <a href=\"https://github.com/sponsors/explore\">Explore GitHub Sponsors</a> page. On the left-hand side select the \"Cargo\" ecosystem. That will show you the individuals and the organizations that you currently rely upon that also accept sponsorship.</p><p>I've also created a page listing some of the <a href=\"https://rust.code-maven.com/sponsoring\">people and project</a> who develop Rust and could be sponsored. For some of them I've also included background information.</p>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My experience with the GNOME Desktop - from despised to loved","url":"https://www.reddit.com/r/linux/comments/1iun2fo/my_experience_with_the_gnome_desktop_from/","date":1740130417,"author":"/u/Fishsven","guid":8933,"unread":true,"content":"<p> I started my Linux journey with Pop!_OS, and I hated the wasted space of the panel-like dock. It took me a while for me to return to GNOME as I was discovering KDE Plasma's (5.24) customization potential. I loved it at first, but I noticed how the DE slowly became unstable after a lot of customising (Plasma has GREATLY improved by now, last time I tried 5.27 on Q4OS and it was blazing fast and rock solid). I was annoyed at how people took a liking to the hideous DE known as GNOME, and for me there was little difference between it and Windows 8, as they were basically tablet centric with GNOME and it's wasted space.</p><p> I eventually got tired of Plasma, because it had way too many features that I didn¬¥t wan¬¥t to use. Tried XFCE, MATE and Budgie, and they felt too outdated for my liking; Budgie felt off. I decided to give GNOME a shot and installed Ubuntu 22.04. For once I was starting to like GNOME. It felt more unified and simple than KDE, but just more modern than the other desktops. However, this was NOT stock GNOME. I installed vanilla GNOME on the same OS and decided to give it a shot.</p><p> Moving on from Ubuntu's Yaru theme to Adwaita felt like a MASSIVE downgrade. Except the looks, GNOME's true workflow actually started to make sense to me and it was more productive than any desktop I tried. Of course, I installed some extensions like Blur my Shell, but I can use GNOME without extensions nowadays. As I'm writing this, GNOME 48 would bring a new Adwaita font with Inter as it's base, which will improve the looks of GNOME by a bit, IMO. Currently using Zorin OS, which has a GNOME theme that is MILES better compared to Libadwaita / Adwaita. </p><p> What I understood is GNOME is not all about looks, it makes the UI simpler and easier to understand, with ONLY the things you need, and it stays out of your way and focuses on your work. It might be dumbing down the desktop for some, but that's exactly what GNOME's for. A solid philosophy IMO- but definitely lagging in some important areas. </p>","contentLength":2009,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How donations helped the LibreOffice project and community in 2024","url":"https://blog.documentfoundation.org/blog/2025/02/21/how-your-donations-helped-the-libreoffice-project-in-2024/","date":1740130087,"author":"/u/themikeosguy","guid":8630,"unread":true,"content":"<p>Thank you for visiting our website and your interest in our services and products. As the protection of your personal data is an important concern for us, please click on the \"More information\" link to access our Privacy Policy page - which will open in a separate browser tab - where we explain what information we collect during your visit to our website, how it is processed, and whether or how it may be used.\nOnce you have carefully read our Privacy Policy page, close the browser tab to return to this page and click on the \"Save Preferences\" button under this text to acknowledge it, close the dialogue and return to the website.<p>\nWe take all the necessary technical and organisational security measures to protect your personal data from loss and misuse. Your data is stored in a secure operating environment that is not accessible to the public.</p></p>","contentLength":853,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iumzoe/how_donations_helped_the_libreoffice_project_and/"},{"title":"reddittui - A terminal browser for reddit","url":"https://github.com/tonymajestro/reddit-tui","date":1740124441,"author":"/u/tmajest","guid":8438,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iulout/reddittui_a_terminal_browser_for_reddit/"},{"title":"Sharing my Open Source Project: Realtime Messaging Platform Built with Go & React (Fullstack)","url":"https://github.com/JoyalAJohney/Realtime-distributed-chat","date":1740113598,"author":"/u/BruceWayn_","guid":8916,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuivtv/sharing_my_open_source_project_realtime_messaging/"},{"title":"Minecraft from scratch with only modern OpenGL","url":"https://github.com/GianlucaP106/minecraft","date":1740108768,"author":"/u/One_Mess_1093","guid":8378,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuhfcq/minecraft_from_scratch_with_only_modern_opengl/"},{"title":"Contribute by filing bugs. You'll feel all warm and fuzzy inside.","url":"https://www.reddit.com/r/linux/comments/1iugim6/contribute_by_filing_bugs_youll_feel_all_warm_and/","date":1740105949,"author":"/u/billhughes1960","guid":8527,"unread":true,"content":"<p>As a lifelong Linux user, I believe strongly in giving back to the open-source community. While I'm not a developer myself, I've found another way to contribute: filing bug reports.</p><p>I'll admit my early attempts were probably pretty rough ‚Äì missing crucial context and details. But practice makes perfect (or at least close!), and these days my bug reports are often addressed within a day or so.</p><p>There's something incredibly satisfying about uncovering a problem, meticulously documenting it, submitting a report, seeing it assigned to someone, and finally witnessing the fix. It's a tangible way to make a difference in the software we all rely on.</p><p>This level of responsiveness and respect simply doesn't exist in proprietary ecosystems. I've tried reporting bugs on Windows and macOS with little success ‚Äì it often feels like shouting into the void. But in the open-source world, even smaller projects welcome contributions and treat you seriously.</p><p>So, I encourage everyone to embrace bug reporting! Start with a simpler project to get comfortable with the process, then gradually tackle more complex ones. Not only will you be improving the software for everyone, but you'll also experience that warm glow of knowing you made a positive impact.</p>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linus Torvalds responds to Christoph Hellwig","url":"https://lore.kernel.org/rust-for-linux/CAHk-=wgLbz1Bm8QhmJ4dJGSmTuV5w_R0Gwvg5kHrYr4Ko9dUHQ@mail.gmail.com/","date":1740105034,"author":"/u/bik1230","guid":7580,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iug7u9/linus_torvalds_responds_to_christoph_hellwig/"},{"title":"Linus Torvalds rips into Hellwig for blocking Rust for Linux","url":"https://lore.kernel.org/rust-for-linux/CAHk-=wgLbz1Bm8QhmJ4dJGSmTuV5w_R0Gwvg5kHrYr4Ko9dUHQ@mail.gmail.com/","date":1740102572,"author":"/u/eugay","guid":7578,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iufdhk/linus_torvalds_rips_into_hellwig_for_blocking/"},{"title":"Installed Ubuntu on my Nan's laptop:","url":"https://www.reddit.com/r/linux/comments/1iueq5x/installed_ubuntu_on_my_nans_laptop/","date":1740100681,"author":"/u/Unique_Ad4547","guid":7577,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Libreboot 20241206, 10th revision released! GRUB security fixes, better LVM scanning, non-root USB2 hub support","url":"https://libreboot.org/news/libreboot20241206rev10.html","date":1740100282,"author":"/u/libreleah","guid":8629,"unread":true,"content":"<p>Article published by: Leah Rowe</p><p>Date of publication: 18 February 2025</p><p>Today‚Äôs Libreboot 20241206 revision is the 10th revision in the Libreboot 20241206 stable release series. The changelog on this page is written, relative to Libreboot 20241206 revision 9 which was released on 12 February 2025. The  Libreboot 20241206 release came out on 6 December 2024. You can find the full list of revisions <a href=\"https://libreboot.org/news/libreboot20241206.Revisions.html\">here</a> and the original release <a href=\"https://libreboot.org/news/libreboot20241206.html\">here</a>.</p><div><h2>Open source BIOS/UEFI firmware</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot20241206rev10.html#open-source-biosuefi-firmware\">[link]</a></div><p>Libreboot is a free/open source BIOS/UEFI replacement on x86 and ARM, providing boot firmware that initialises the hardware in your computer, to then load an operating system (e.g.&nbsp;Linux/BSD). It is specifically a , in the same way that Debian is a Linux distribution. It provides an automated build system to produce coreboot ROM images with a variety of payloads such as GRUB or SeaBIOS, with regular well-tested releases to make coreboot as easy to use as possible for non-technical users. From a project management perspective, this works in  the same way as a Linux distro, providing a source-based package manager (called lbmk) which patches sources and compiles coreboot images. It makes use of <a href=\"https://www.coreboot.org/\">coreboot</a> for hardware initialisation, and then a payload such as <a href=\"https://www.seabios.org/SeaBIOS\">SeaBIOS</a> or <a href=\"https://www.gnu.org/software/grub/\">GRUB</a> to boot your operating system; on ARM(chromebooks), we provide  (as a coreboot payload).</p><p>We also provide an experimental U-Boot setup on x86, as a coreboot payload for providing a minimal UEFI implementation.</p><p>Normally, revisions would only be documented on the <a href=\"https://libreboot.org/news/libreboot20241206.Revisions.html\">Libreboot 20241206 revisions page</a>, but this revision contains , so it was decided that there should be a full announcement, to ensure that more people see it.</p><div><h2>Summarised list of changes</h2><a aria-hidden=\"true\" href=\"https://libreboot.org/news/libreboot20241206rev10.html#summarised-list-of-changes\">[link]</a></div><p>GRUB released  to its main branch, fixing a large number of security issues. You can read about them here:</p><p>This updates GRUB to revision <code>4dc6166571645780c459dde2cdc1b001a5ec844c</code> from 18 February 2025. Several OOB heap writes, buffer overflows, use after frees and so on, are now prevented with this update.</p><p>In addition to the security fixes, several out-of-tree fixes from Libreboot‚Äôs main branch have been merged for GRUB, fixing bugs in the xHCI driver, and adding support for non-root USB2 hubs on platforms that use the  GRUB tree.</p><p>Changes to the GRUB configuration have been made, to make scanning of LVM volume/group names more reliable, including on full-disk-encryption setups. More such changes are planned for the next major release; the current changes are very minor.</p>","contentLength":2480,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1iuel79/libreboot_20241206_10th_revision_released_grub/"},{"title":"BritCSS: Fixes CSS to use non-American English","url":"https://github.com/DeclanChidlow/BritCSS","date":1740096884,"author":"/u/ValenceTheHuman","guid":8397,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iudp7p/britcss_fixes_css_to_use_nonamerican_english/"},{"title":"Docker Hub will only allow an unauthenticated 10/pulls per hour starting March 1st","url":"https://docs.docker.com/docker-hub/usage/","date":1740096401,"author":"/u/onedr0p","guid":7514,"unread":true,"content":"<blockquote><p>Starting April 1, 2025, all users with a Pro, Team, or Business\nsubscription will have unlimited Docker Hub pulls with fair use.\nUnauthenticated users and users with a free Personal account have the\nfollowing pull limits:</p><ul><li>Unauthenticated users: 10 pulls/hour</li><li>Authenticated users with a free account: 100 pulls/hour</li></ul></blockquote><p>The following table provides an overview of the included usage and limits for each\nuser type, subject to fair use:</p><div><table><thead><tr><th>Number of public repositories</th><th>Number of private repositories</th></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>10 per IPv4 address or IPv6 /64 subnet</td></tr></tbody></table></div><p>For more details, see the following:</p><p>When utilizing the Docker Platform, users should be aware that excessive data\ntransfer, pull rates, or data storage can lead to throttling, or additional\ncharges. To ensure fair resource usage and maintain service quality, we reserve\nthe right to impose restrictions or apply additional charges to accounts\nexhibiting excessive data and storage consumption.</p><p>Docker Hub has an abuse rate limit to protect the application and\ninfrastructure. This limit applies to all requests to Hub properties including\nweb pages, APIs, and image pulls. The limit is applied per IPv4 address or per\nIPv6 /64 subnet, and while the limit changes over time depending on load and\nother factors, it's in the order of thousands of requests per minute. The abuse\nlimit applies to all users equally regardless of account level.</p><p>You can differentiate between the pull rate limit and abuse rate limit by\nlooking at the error code. The abuse limit returns a simple  response. The pull limit returns a longer error message that includes\na link to documentation.</p>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iudj0d/docker_hub_will_only_allow_an_unauthenticated/"},{"title":"[Media] Rust powered flight radar","url":"https://www.reddit.com/r/rust/comments/1iubs4r/media_rust_powered_flight_radar/","date":1740091662,"author":"/u/Confident-Alarm-6911","guid":7579,"unread":true,"content":"<p>So, consider this mix: I have thing for retro-interfaces with monochromatic displays, I wanted to learn rust and do something with sdr radio, I live next to the airport. And that‚Äôs how my small radar comes to life üòé</p><p>Hardware: ESP32C3, 1.5 inch i2c oled display, some encoder. RTL-SDR V4 running on my local linux machine and small endpoint to serve ADS-B data via http.</p><p>Firmware written in rust 2021 edition. Libraries: mostly std and esp-idf-svc + rtos (not necessary, but I wanted to try it)</p><p>I‚Äôm pretty content with this small project as it is my first attempt to build something in Rust. Now I want to design 3D printable case, do some polishing on software side, and publish it as open source.</p><p>I wanted to post video but it says I can not do this in this community, so only pic</p>","contentLength":784,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IaaC Simplified: Automating EC2 Deployments with GitHub Actions, Terraform, Docker & Distribution Registry | Vue & Node admin panel framework","url":"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/","date":1740090590,"author":"/u/Unerring-Ocean","guid":7467,"unread":true,"content":"<p>This guide shows how to deploy own Docker apps (with AdminForth as example) to Amazon EC2 instance with Docker and Terraform involving Docker self-hosted registry.</p><ul><li>GitHub actions Free plan which includes 2000 minutes per month (1000 of 2-minute builds per month - more then enough for many projects, if you are not running tests etc). Extra builds would cost  per minute.</li><li>AWS account where we will auto-spawn EC2 instance. We will use  instance (2 vCPUs, 2GB RAM) which costs  per month in  region (cheapest region). Also it will take  per month for EBS gp2 storage (20GB) for EC2 instance</li></ul><p>This is it, registry will be auto-spawned on EC2 instance, so no extra costs for it. Also GitHub storage is not used, so no extra costs for it.</p><p>The setup has next features:</p><ul><li>Build process is done using IaaC approach with HashiCorp Terraform, so almoast no manual actions are needed from you. Every resource including EC2 server instance is described in code which is commited to repo so no manual clicks are needed.</li><li>Docker build process is done on GitHub actions, so EC2 server is not overloaded</li><li>Changes in infrastructure including changing server type, adding S3 Bucket, changing size of sever disk is also can be done by commiting code to repo.</li><li>Docker images and cache are stored on EC2 server, so no extra costs for Docker registry are needed.</li><li>Total build time for average commit to AdminForth app (with Vite rebuilds) is around 2 minutes.</li></ul><p>Previously we had a blog post about <a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions/\">deploying AdminForth to EC2 with Terraform without registry</a>. That method might work well but has a significant disadvantage - build process happens on EC2 itself and uses EC2 RAM and CPU. This can be a problem if your EC2 instance is well-loaded without extra free resources. Moreover, low-end EC2 instances have a small amount of RAM and CPU, so build process which involves vite/tsc/etc can be slow or even fail.</p><p>So obviously to solve this problem we need to move the build process to CI, however it introduces new chellenges and we will solve them in this post.</p><p>Quick difference between approaches from previous post and current post:</p><table><thead><tr></tr></thead><tbody><tr><td>How and where docker build happens</td><td>Source code is rsync-ed from CI to EC2 and docker build is done there</td><td>Docker build is done on CI and docker image is pushed to registry (in this post we run registry automatically on EC2)</td></tr><tr><td>How Docker build layers are cached</td><td>GitHub actions has no own Docker cache out of the box, so it should be stored in dedicated place (we use self-hosted registry on the EC2 as it is free)</td></tr><tr><td>Simpler setup with less code (we don't need code to run and secure registry, and don't need extra cache setup as is naturally persisted on EC2).</td><td>Build is done on CI, so EC2 server is not overloaded. For most cases CI builds are faster than on EC2. Plus time is saved because we don't need to rsync source code to EC2</td></tr><tr><td>Build on EC2 requires additional server RAM / overloads CPU</td><td>More terraform code is needed. registry cache might require small extra space on EC2</td></tr></tbody></table><h2>Chellenges when you build on CI<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#chellenges-when-you-build-on-ci\" aria-label=\"Direct link to Chellenges when you build on CI\" title=\"Direct link to Chellenges when you build on CI\">‚Äã</a></h2><p>When you move build process to CI you have to solve next chellenges:</p><ol><li>We need to deliver built docker images to EC2 somehow (and only we)</li><li>We need to persist cache between builds</li></ol><h4>Exporing images to tar files<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#exporing-images-to-tar-files\" aria-label=\"Direct link to Exporing images to tar files\" title=\"Direct link to Exporing images to tar files\">‚Äã</a></h4><p>Simplest option which you can find is save docker images to tar files and deliver them to EC2. We can easily do it in terraform (using  command on CI and  command on EC2). However this option has a significant disadvantage - it is slow. Docker images are big (always include all layers, without any options), so it takes infinity to do save/load and another infinity to transfer them to EC2 (via relatively slow rsync/SSH and relatively slow GitHub actions outbound connection).</p><p>Faster, right option which we will use here - involve Docker registry. Registry is a repository which stores docker images. It does it in a smart way - it saves each image as several layers, so if you will update last layer, then only last layer will be pushed to registry and then only last will be pulled to EC2.\nTo give you row compare - whole-layers image might take , but last layer created by  command might take . And most builds you will do only last layer changes, so it will be 20 times faster to push/pull last layer than whole image.\nAnd this is not all, registry uses TLS HTTP protocol so it is faster then SSH/rsync encrypted connection.</p><p>Of course you have to care about a way of registry authentication (so only you and your CI/EC2 can push/pull images).</p><p>What docker registry can you use? Pretty known options:</p><ol><li>Docker Hub - most famous. It is free for public images, so literally every opensource project uses it. However it is not free for private images, and you have to pay for it. In this post we are considering you might do development for commercial project with tight budget, so we will not use it.</li><li>GHCR - Registry from Google. Has free plan but allows to store only 500MB and allows to transfer 1GB of traffic per month. Then you pay for every extra GB in storage and traffic. Probably small images will fit in this plan, but generally even alpine-based docker images are bigger than 500MB, so it is not a good option.</li><li>Self-hosted registry web system. In our software development company, we use Harbor. It is a powerful free open-source registry that can be installed to own server. It allows pushing and pulling without limit. Also, it has internal life-cycle rules that cleanup unnecessary images and layers. The main drawbacks of it are that it is not so fast to install and configure, plus you have to get a domain and another powerfull server to run it. So unless you are a software development company, it is not worth using it.</li><li>Self-hosted minimal CNCF Distribution <a href=\"https://distribution.github.io/distribution/\" target=\"_blank\" rel=\"noopener noreferrer\">registry</a> on EC2 itself. So since we already have EC2, we can run registry on it directly. The  container is pretty light-weight and easy to setup and it will not consume a lot of extra CPU/RAM on server. Plus images will be stored close to application so pull will be fast.</li></ol><p>In the post we will use last (4th way). Our terraform will deploy registry automatically, so you don't have to do anything special.</p><p>Docker builds without layer cache persistence are possible but very slow. Most builds only change a couple of layers, and having no ability to cache them will cause the Docker builder to regenerate all layers from scratch. This can, for example, increase the Docker build time from a minute to ten minutes or even more.</p><p>Out of the box, GitHub Actions can't save Docker layers between builds, so you have to use external storage.</p><blockquote><p>Though some CI systems can persist docker build cache, e.g. open-source self-hosted Woodpecker CI allows it out of the box. However GitHub actions which is pretty popular, reasonably can't allow such free storage to anyone</p></blockquote><p>So when build-in Docker cache can't be used, there is one alternative - Docker BuildKit external cache.\nSo BuildKit allows you to connect external storage. There are several options, but most sweet for us is using Docker registry as cache storage (not only as images storage to deliver them to application server).</p><blockquote><p><em>BuildKit cache in Compose issue</em>\nPreviously we used docker compose to build &amp; run our app, it can be used to both build, push and pull images, but has <a href=\"https://github.com/docker/compose/issues/11072#issuecomment-1848974315\" target=\"_blank\" rel=\"noopener noreferrer\">issues with external cache connection</a>. While they are not solved we have to use  command to build images. It is not so bad, but is another point of configuration which we will cover in this post.</p></blockquote><h3>Registry authorization and traffic encryption<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#registry-authorization-and-traffic-encryption\" aria-label=\"Direct link to Registry authorization and traffic encryption\" title=\"Direct link to Registry authorization and traffic encryption\">‚Äã</a></h3><p>Hosting custom CNCF registry, from other hand is a security responsibility.</p><p>If you don't protect it right, someone will be able to push any image to your registry and then pull it to your EC2 instance. This is a big security issue, so we have to protect our registry.</p><p>First of all we need to set some authorization to our registry so everyone who will push/pull images will be authorized. Here we have 2 options: HTTP basic auth and Client certificate auth. We will use first one as it is easier to setup. We will generate basic login and password automatically in terraform so no extra actions are needed from you.</p><p>But this is not enough. Basic auth is not encrypted, so someone can perform MITM attack and get your credentials. So we need to encrypt traffic between CI and registry. We can do it by using TLS certificates. So we will generate self-signed TLS certificates, and attach them to our registry.</p><p>Assume you have your AdminForth project in .</p><p>Create file  in :</p><p>create folder  and create file  inside:</p><h2>Step 3 - create a SSH keypair<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-3---create-a-ssh-keypair\" aria-label=\"Direct link to Step 3 - create a SSH keypair\" title=\"Direct link to Step 3 - create a SSH keypair\">‚Äã</a></h2><p>Make sure you are still in  folder, run next command:</p><p>Now it should create  and  files with your SSH keypair. Terraform script will put the public key to the EC2 instance and will use private key to connect to the instance. Also you will be able to use it to connect to the instance manually.</p><h2>Step 4 - create TLS certificates to encrypt traffic between CI and registry<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-4---create-tls-certificates-to-encrypt-traffic-between-ci-and-registry\" aria-label=\"Direct link to Step 4 - create TLS certificates to encrypt traffic between CI and registry\" title=\"Direct link to Step 4 - create TLS certificates to encrypt traffic between CI and registry\">‚Äã</a></h2><p>Make sure you are still in  folder, run next command:</p><p>Run next command to create TLS certificates:</p><p>This will create  and  files.</p><h2>Step 5 - .gitignore file<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-5---gitignore-file\" aria-label=\"Direct link to Step 5 - .gitignore file\" title=\"Direct link to Step 5 - .gitignore file\">‚Äã</a></h2><p>Create  file with next content:</p><h2>Step 6 - buildx bake file<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-6---buildx-bake-file\" aria-label=\"Direct link to Step 6 - buildx bake file\" title=\"Direct link to Step 6 - buildx bake file\">‚Äã</a></h2><p>Create file :</p><h2>Step 7 - main terraform file main.tf<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-7---main-terraform-file-maintf\" aria-label=\"Direct link to Step 7 - main terraform file main.tf\" title=\"Direct link to Step 7 - main terraform file main.tf\">‚Äã</a></h2><p>Create file  in  folder:</p><blockquote><p>üëÜ Replace  with your app name (no spaces, only underscores or letters)</p></blockquote><h3>Step 7.1 - Configure AWS Profile<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-71---configure-aws-profile\" aria-label=\"Direct link to Step 7.1 - Configure AWS Profile\" title=\"Direct link to Step 7.1 - Configure AWS Profile\">‚Äã</a></h3><p>Open or create file  and add (if not already there):</p><h3>Step 7.2 - Run deployment<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-72---run-deployment\" aria-label=\"Direct link to Step 7.2 - Run deployment\" title=\"Direct link to Step 7.2 - Run deployment\">‚Äã</a></h3><p>To run the deployment first time, you need to run:</p><h2>Step 8 - Migrate state to the cloud<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-8---migrate-state-to-the-cloud\" aria-label=\"Direct link to Step 8 - Migrate state to the cloud\" title=\"Direct link to Step 8 - Migrate state to the cloud\">‚Äã</a></h2><p>First deployment had to create S3 bucket for storing Terraform state. Now we need to migrate the state to the cloud.</p><p>Add to the end of :</p><blockquote><p>üëÜ Replace  with your app name (no spaces, only underscores or letters).\nUnfortunately we can't use variables, HashiCorp thinks it is too dangerous üò•</p></blockquote><p>Now you can delete local  file and  file as they are in the cloud now.</p><h2>Step 9 - CI/CD - Github Actions<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-9---cicd---github-actions\" aria-label=\"Direct link to Step 9 - CI/CD - Github Actions\" title=\"Direct link to Step 9 - CI/CD - Github Actions\">‚Äã</a></h2><p>Create file <code>.github/workflows/deploy.yml</code>:</p><div><div>.github/workflows/deploy.yml</div></div><h3>Step 8.1 - Add secrets to GitHub<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#step-81---add-secrets-to-github\" aria-label=\"Direct link to Step 8.1 - Add secrets to GitHub\" title=\"Direct link to Step 8.1 - Add secrets to GitHub\">‚Äã</a></h3><p>Go to your GitHub repository, then  -&gt;  -&gt;  and add:</p><ul><li> - your AWS access key</li><li><code>VAULT_AWS_SECRET_ACCESS_KEY</code> - your AWS secret key</li><li> - execute  and paste to GitHub secrets</li><li> - execute  and paste to GitHub secrets</li><li> - execute  and paste to GitHub secrets</li><li> - execute  and paste to GitHub secrets</li></ul><p>Now you can push your changes to GitHub and see how it will be deployed automatically.</p><p>Once you will have sensitive tokens/passwords in your apps you have to store them in a secure way.</p><p>Simplest way is to use GitHub secrets.</p><p>Let's imagine you have  which will be used one of AI-powered plugins of adminforth. We can't put this key to the code, so we have to store it in GitHub secrets.</p><p>Open your GitHub repository, then  -&gt;  -&gt;  and add  with your key.</p><p>Now open GitHub actions file and add it to the  section:</p><div><div>.github/workflows/deploy.yml</div></div><p>Next add it to the  script:</p><p>In the same way you can add any other secrets to your GitHub actions.</p><h3>Out of space on EC2 instance? Extend EBS volume<a href=\"https://adminforth.dev/blog/compose-ec2-deployment-github-actions-registry/#out-of-space-on-ec2-instance-extend-ebs-volume\" aria-label=\"Direct link to Out of space on EC2 instance? Extend EBS volume\" title=\"Direct link to Out of space on EC2 instance? Extend EBS volume\">‚Äã</a></h3><p>To upgrade EBS volume size you have to do next steps:</p><p>This will increase physical size of EBS volume, but you have to increase filesystem size too.</p><blockquote><p>You can find your EC2 IP in AWS console by visiting EC2 -&gt; Instances -&gt; Your instance -&gt; IPv4 Public IP</p></blockquote><p>This would show something like this:</p><p>Here we see that  is our disk and  is our partition.</p><p>Now to extend partition run:</p><p>This will extend partition to the full disk size. No reboot is needed.</p>","contentLength":11283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iubdcw/iaac_simplified_automating_ec2_deployments_with/"},{"title":"Using one ingress controller to proxy to another cluster","url":"https://www.reddit.com/r/kubernetes/comments/1iub1dp/using_one_ingress_controller_to_proxy_to_another/","date":1740089739,"author":"/u/djjudas21","guid":7472,"unread":true,"content":"<p>I'm planning a migration between two on-premise clusters. Both clusters are on the same network, with an ingress IP provided by MetalLB. The network is behind a NAT gateway with a single public IP, and port forwarding.</p><p>I need to start moving applications from cluster A to cluster B, but I can only set my port forwarding to point to cluster A  cluster B.</p><p>I'm trying to figure out if there's a way to use one cluster's ingress controller to proxy some sites to the other cluster's ingress controller. Something like SSL passthrough.</p><p>I've tried to configure the following on cluster B to proxy some specific site back to cluster A, with SSL passthrough as cluster A is running all its sites with TLS enabled. Unfortunately it isn't working properly and attempting to connect to <a href=\"http://app.example.com\">app.example.com</a> on cluster B only presents the default ingress controller self-signed cert, not the real app cert from cluster A.</p><pre><code>apiVersion: v1 kind: Service metadata: name: microk8s-proxy namespace: default spec: type: ExternalName externalName: ingress-a.example.com --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" name: microk8s-proxy namespace: default spec: ingressClassName: public rules: - host: app.example.com http: paths: - backend: service: name: microk8s-proxy port: number: 443 path: / pathType: Prefix </code></pre><p>I've been working on this for hours and can't get it working. Seems like it might be easier to just schedule a day of downtime for all sites! Thanks</p>","contentLength":1570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google's Shift to Rust Programming Cuts Android Memory Vulnerabilities by 68%","url":"https://thehackernews.com/2024/09/googles-shift-to-rust-programming-cuts.html","date":1740089695,"author":"/u/Unerring-Ocean","guid":7468,"unread":true,"content":"<p>Google has revealed that its transition to memory-safe languages such as Rust as part of its secure-by-design approach has led to the percentage of memory-safe vulnerabilities discovered in Android dropping from 76% to 24% over a period of six years.</p><p>The tech giant said focusing on <a href=\"https://blog.google/technology/safety-security/tackling-cybersecurity-vulnerabilities-through-secure-by-design/\" rel=\"noopener\" target=\"_blank\">Safe Coding</a> for new features not only reduces the overall security risk of a codebase, but also makes the switch more \"scalable and cost-effective.\"</p><p>Eventually, this leads to a drop in memory safety vulnerabilities as new memory unsafe development slows down after a certain period of time, and new memory safe development takes over, Google's Jeff Vander Stoep and Alex Rebert <a href=\"https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html\" rel=\"noopener\" target=\"_blank\">said</a> in a post shared with The Hacker News.</p><p>Perhaps even more interestingly, the number of memory safety vulnerabilities tends to register a drop notwithstanding an increase in the quantity of new memory unsafe code.</p><p>The paradox is explained by the fact that vulnerabilities decay exponentially, with a study finding that a high number of vulnerabilities often reside in new or recently modified code.</p><p>\"The problem is overwhelmingly with new code, necessitating a fundamental change in how we develop code,\" Vander Stoep and Rebert noted. \"Code matures and gets safer with time, exponentially, making the returns on investments like rewrites diminish over time as code gets older.\"</p><p>Google, which <a href=\"https://thehackernews.com/2021/04/android-to-support-rust-programming.html\" rel=\"noopener\" target=\"_blank\">formally announced</a> its plans to support the Rust programming language in Android way back in April 2021, said it began prioritizing transitioning new development to memory-safe languages around 2019.</p><p>As a result, the number of memory safety vulnerabilities discovered in the operating system has declined from <a href=\"https://security.googleblog.com/2022/12/memory-safe-languages-in-android-13.html\" rel=\"noopener\" target=\"_blank\">223 in 2019</a> to less than 50 in 2024.</p><p>It also goes without saying that much of the decrease in such flaws is down to advancements in the ways devised to combat them, moving from reactive patching to proactive mitigating to proactive vulnerability discovery using tools like <a href=\"https://thehackernews.com/2023/12/google-using-clang-sanitizers-to.html\" rel=\"noopener\" target=\"_blank\">Clang sanitizers</a>.</p><p>The tech giant further noted that memory safety strategies should evolve even more to prioritize \"high-assurance prevention\" by incorporating <a href=\"https://dl.acm.org/doi/10.1145/3651621\" rel=\"noopener\" target=\"_blank\">secure-by-design principles</a> that enshrine security into the very foundations. </p><p>\"Instead of focusing on the interventions applied (mitigations, fuzzing), or attempting to use past performance to predict future security, Safe Coding allows us to make strong assertions about the code's properties and what can or cannot happen based on those properties,\" Vander Stoep and Rebert said.</p><p>That's not all. Google said it is also focusing on offering interoperability between Rust, C++, and Kotlin, instead of code rewrites, as a \"practical and incremental approach\" to embracing memory-safe languages and ultimately eliminating entire vulnerability classes.</p><p>\"Adopting Safe Coding in new code offers a paradigm shift, allowing us to leverage the inherent decay of vulnerabilities to our advantage, even in large existing systems,\" it said.</p><p>\"The concept is simple: once we turn off the tap of new vulnerabilities, they decrease exponentially, making all of our code safer, increasing the effectiveness of security design, and alleviating the scalability challenges associated with existing memory safety strategies such that they can be applied more effectively in a targeted manner.\"</p><p>The development comes as Google touted increased collaboration with Arm's product security and graphics processing unit (GPU) engineering teams to flag multiple shortcomings and elevate the overall security of the GPU software/firmware stack across the Android ecosystem.</p><p>\"Proactive testing is good hygiene as it can lead to the detection and resolution of new vulnerabilities before they're exploited,\" Google and Arm <a href=\"https://security.googleblog.com/2024/09/google-arm-raising-bar-on-gpu-security.html\" rel=\"noopener\" target=\"_blank\">said</a>.</p><div>Found this article interesting?  Follow us on <a href=\"https://twitter.com/thehackersnews\" rel=\"noopener\" target=\"_blank\">Twitter </a> and <a href=\"https://www.linkedin.com/company/thehackernews/\" rel=\"noopener\" target=\"_blank\">LinkedIn</a> to read more exclusive content we post.</div>","contentLength":3792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iub0rk/googles_shift_to_rust_programming_cuts_android/"},{"title":"CustomResourceDefinitions to provision Azure resources such as storage blob","url":"https://www.reddit.com/r/kubernetes/comments/1iuay1o/customresourcedefinitions_to_provision_azure/","date":1740089501,"author":"/u/Valuable-Ad3229","guid":7471,"unread":true,"content":"<p>I am developer working with Azure Kubernetes Service, and I wonder if it is possible to define a CustomResourceDefinitions to provision other Azure resources such as Azure storage blobs, or Azure identities?</p><p>I am mindful that this may be anti-pattern but I am curious. Thank you!</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Are there any theoretical machine learning papers that have significantly helped practitioners?","url":"https://www.reddit.com/r/MachineLearning/comments/1iuanhy/d_are_there_any_theoretical_machine_learning/","date":1740088779,"author":"/u/nihaomundo123","guid":8455,"unread":true,"content":"<p>21M deciding whether or not to specialize in theoretical ML for their math PhD. Specifically, I am interested in</p><p>ii) but NOT interested in papers focusing on improving empirical performance, like the original dropout and batch normalization papers.</p><p>I want to work on something with the potential for deep impact during my PhD, yet still theoretical. When trying to find out if the understanding-based questions in category i) fits this description, however, I could not find much on the web...</p><p><strong>If anyone has any specific examples of papers whose main focus was to understand some phenomena, and that ended up revolutionizing things for practitioners, would appreciate it :)</strong></p>","contentLength":670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The State of Scala & Clojure Surveys: How is functional programming on JVM doing","url":"https://www.jvm-weekly.com/p/the-state-of-scala-and-clojure-surveys","date":1740088259,"author":"/u/ArturSkowronski","guid":8574,"unread":true,"content":"<p>The title might be a bit of an overstatement ‚Äì don‚Äôt expect a very extensive analysis of functional programming trends. I wanted to focus on two surveys that have appeared recently, which tell us a bit about languages that many of you probably used in a previous reincarnation cycle, but have already forgotten about.</p><p>Of course, the usual disclaimer at the beginning ‚Äì we know that surveys tend to show what they feel is worth showing and have a certain narrative power.</p><p>Having that in mind, I think we can begin.</p><p>First, let‚Äôs look at the people who filled out the survey ‚Äì demographics tell us a lot about the quality of the results and what we can expect. We have as many as 232 responses, with 75% Software Engineers and 49.6% people in tech-lead or related areas ‚Äì apparently, many of us wear several hats at once (yes, I know that pain too). It turned out that most of them work on closed-source projects (87.5%), although there‚Äôs no shortage of hardcore open-source folks (18.1%). I think that fairly reflects the state of the industry.</p><p>An overwhelming majority of respondents like or love Scala: 49.1% love it, 44% rather like it. The remaining 6.9% are still undecided, and 0.9% (i.e., 2 people) have fallen into Scala-depression. However here we also hit, to some extent, the ‚Äúpeak‚Äù of people interested in the topic - those willing to fill out the Scala Survey.</p><p>The average age of projects is 7 years, with a median of 6 years ‚Äì that‚Äôs quite‚Ä¶ a lot. It also shows that quite a bit of Scala is legacy projects ‚Äì at least in the surveyed group, ‚Äúgreenfields‚Äù are relatively rare.</p><p>Typelevel (41.8%), Akka (35.3%), ZIO (23.3%), and Play (15.5%). And of course, Spark (7.7%) ‚Äì let‚Äôs not forget that big data elephant in the room, though the days when it was synonymous with Data seem to be behind us. These people have a new best friend.</p><p>Moreover, more than half of the projects (53%) combine different ecosystems such as Akka and Cats, indicating that we‚Äôre building increasingly hybrid beasts. 36.2% are ‚Äúmonogamists‚Äù relying entirely on a single library (ZIO, we‚Äôre looking at you), and 10.8% are the brave ones using only the standard library.</p><p><a href=\"https://www.linkedin.com/article/edit/7298227473140305920/#\" rel=\"\">VirtusLab</a><a href=\"https://github.com/scalameta/metals\" rel=\"\">project here</a></p><p>SBT beats everyone hands down (87.5%). Scala-CLI (11.2%) is relatively new but has decent traction, and Bazel (7.8%) and Maven (7.3%) also have loyal fans. </p><p>22.4% of commercial projects have already switched to Scala 3, but as many as 37% do not plan to. Why? Because as usual, the ecosystem is (still) not fully ready, there‚Äôs a lack of resources, and management is pushing the topic aside. You know that feeling when a new version of a language tempts you, but there are dozens of ‚Äúwork in progress‚Äù branches piling up in the repo‚Ä¶?</p><p>Talking about the problems, the report shows three major ones:</p><ul><li><p><strong>Fragmentation of the ecosystem and migration issues to Scala 3</strong></p></li><li><p><strong>Lack of resources to maintain older projects.</strong></p></li></ul><p><strong>recruiting Scala developers</strong></p><p>Given these recruitment problems, it‚Äôs not surprising that 68.6% allow remote work, which at least somewhat makes life easier for those who have managed to settle in the Bieszczady Mountains or in Bali.</p><ul></ul><p>Despite all these challenges, 88.4% of respondents would still choose Scala for new projects without hesitation. It shows that the JVM community sees great potential in Scala, but also knows that working on its further development and tooling is a marathon, not a sprint.</p><p><a href=\"https://bit.ly/scala-report\" rel=\"\">the report is full of interesting details</a></p><p>Now, let's take a look at the other report I have for you today.</p><p><a href=\"https://www.linkedin.com/article/edit/7298227473140305920/#\" rel=\"\">Alex Miller</a><a href=\"https://clojure.org/news/2024/12/02/state-of-clojure-2024\" rel=\"\">State of Clojure</a></p><p>Let‚Äôs start with what warms the hearts of backend folks the most: does Clojure live in real projects and is it more than just a hobby experiment? Definitely yes! 73% of respondents use Clojure at work, mainly in web development, commercial services, and enterprise applications. Their services often end up in the cloud ‚Äì public (58%) or private (26%). So you can say with confidence that the ‚ÄúLispy DSL‚Äù is conquering more and more server rooms and Docker containers.</p><p>What about team size? Most are small teams (up to 10 people), though there are also true giants ‚Äì Nubank, with over a thousand Clojure developers, is a prime example. It‚Äôs no coincidence they‚Äôre now responsible for the development of the language.</p><p>Regarding the adoption of new versions, things look surprisingly good. As many as 58% have already moved to Clojure 1.12, released in September 2024, which indicates that stability and a lack of painful breaking changes are quite a motivator.</p><p>Here we see that Java 21 LTS already has 54% of users, and Java 8 is losing ground in favor of newer versions (only 9% remain with the old-timer, which is better than in Java itself). Probably for this reason, Clojure plans to raise the base version in subsequent releases (much like Scala, but we‚Äôll talk about that next week).</p><p><a href=\"https://babashka.org/\" rel=\"\">Babashka</a></p><p>An example is Babashka ‚Äì a dialect that enjoys huge popularity (93% of respondents who use dialects had dealt with it) because it allows scripts to be run quickly, without the start-up delays of the JVM. ClojureDart, on the other hand, brings Clojure into the Dart ecosystem, opening new perspectives for web and mobile apps. Other projects like Squint, Jank, and Cherry demonstrate the community‚Äôs ongoing creativity ‚Äì each introduces its own modifications, often experimental, allowing the Clojure philosophy to adapt to entirely new conditions.</p><p>Leiningen and deps.edn continues to vie for space among dependency management tools ‚Äì we can see that deps.edn is gaining strength, and nRepl, REBL, and other plugins help make REPL feel like home.</p><p>A special section of the survey examines people who have just started their adventure with Clojure (less than a year of experience). The report has been tracking programmers‚Äô migration paths to Clojure for years. It appears they still largely come from Java, JavaScript, and Python. Ruby and C++ are in decline, whereas C# is starting to gain slightly ‚Äì perhaps thanks to the ‚Äúfunctional awakening‚Äù in the .NET ecosystem.</p><p><strong>Biggest challenges for newcomers?</strong></p><p>If you‚Äôve read this far, you‚Äôre probably as happy as I am to see that Clojure keeps evolving. On one hand ‚Äì a stable, mature platform, and on the other ‚Äì new dialects, a growing community not just in corporate settings but also in open-source projects and even in the gaming industry (yes, yes, I‚Äôve seen it!). State of Clojure 2024 shows that Lisp on the JVM is still a very strong player: a steady, balanced development without revolutionary changes, yet‚Ä¶ there‚Äôs always something new to discover.</p><p><a href=\"https://clojure.org/news/2024/12/02/state-of-clojure-2024\" rel=\"\">through the full report</a></p><p>Happy coding ‚Äì and until next time in JVM Weekly!</p>","contentLength":6662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iuag90/the_state_of_scala_clojure_surveys_how_is/"},{"title":"[R] Detecting LLM Hallucinations using Information Theory","url":"https://www.reddit.com/r/MachineLearning/comments/1iu9ryi/r_detecting_llm_hallucinations_using_information/","date":1740086564,"author":"/u/meltingwaxcandle","guid":7496,"unread":true,"content":"<p>LLM hallucinations and errors are a major challenge, but what if we could predict when they happen? Nature had a great <a href=\"https://www.nature.com/articles/s41586-024-07421-0\">publication</a> on semantic entropy, but I haven't seen many practical guides on production patterns for LLMs.</p><ol><li><strong>Sequence log-probabilities</strong> provides a free, effective way to detect unreliable outputs (can be interpreted as \"LLM confidence\").</li><li><strong>High-confidence responses were nearly twice as accurate</strong> as low-confidence ones (76% vs 45%).</li><li>Using this approach, we can automatically <strong>filter poor responses, introduce human review, or iterative RAG pipelines</strong>.</li></ol><p><strong>Experiment setup is simple</strong>: generate 1000 RAG-supported LLM responses to various questions. Ask experts to blindly evaluate responses for quality. See how much LLM confidence predicts quality.</p><p>Bonus: precision recall curve for an LLM.</p><p>My interpretation is that LLM operates in a higher entropy (less predictable output / flatter token likelihood distributions) regime when it's not confident. So it's dealing with more uncertainty and starts to break down essentially.</p><p>Regardless of your opinions on validity of LLMs, this feels like one of the simplest, but effective methods to catch a bulk of errors. </p>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Project - Deploy Flask App With MySQL on Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iu92sy/learning_project_deploy_flask_app_with_mysql_on/","date":1740084836,"author":"/u/kchandank","guid":8379,"unread":true,"content":"<p>If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.</p><p>In this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.</p><ul><li>Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).</li><li>kubectl installed and configured to interact with your Kubernetes cluster.</li><li>Docker installed on your machine to build and push the Docker image of the Flask app.</li><li>Docker Hub account to push the Docker image.</li></ul><p>You will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:</p><ul></ul><p>Create a <a href=\"http://app.py\">app.py</a> file with following content</p><pre><code>from flask import Flask, jsonify import os import mysql.connector from mysql.connector import Error app = Flask(__name__) def get_db_connection(): \"\"\" Establishes a connection to the MySQL database using environment variables. Expected environment variables: - MYSQL_HOST - MYSQL_DB - MYSQL_USER - MYSQL_PASSWORD \"\"\" host = os.environ.get(\"MYSQL_HOST\", \"localhost\") database = os.environ.get(\"MYSQL_DB\", \"flaskdb\") user = os.environ.get(\"MYSQL_USER\", \"flaskuser\") password = os.environ.get(\"MYSQL_PASSWORD\", \"flaskpass\") try: connection = mysql.connector.connect( host=host, database=database, user=user, password=password ) if connection.is_connected(): return connection except Error as e: app.logger.error(f\"Error connecting to MySQL: {e}\") return None u/app.route(\"/\") def index(): return f\"Welcome to the Flask App running in {os.environ.get('APP_ENV', 'development')} mode!\" u/app.route(\"/dbtest\") def db_test(): \"\"\" A simple endpoint to test the MySQL connection. Executes a query to get the current time from the database. \"\"\" connection = get_db_connection() if connection is None: return jsonify({\"error\": \"Failed to connect to MySQL database\"}), 500 try: cursor = connection.cursor() cursor.execute(\"SELECT NOW();\") current_time = cursor.fetchone() return jsonify({ \"message\": \"Successfully connected to MySQL!\", \"current_time\": current_time[0] }) except Error as e: return jsonify({\"error\": str(e)}), 500 finally: if connection and connection.is_connected(): cursor.close() connection.close() if __name__ == \"__main__\": debug_mode = os.environ.get(\"DEBUG\", \"false\").lower() == \"true\" app.run(host=\"0.0.0.0\", port=5000, debug=debug_mode) </code></pre><pre><code>FROM python:3.9-slim # Install ping (iputils-ping) for troubleshooting RUN apt-get update &amp;&amp; apt-get install -y iputils-ping &amp;&amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements.txt . RUN pip install --upgrade pip &amp;&amp; pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 5000 ENV FLASK_APP=app.py CMD [\"python\", \"app.py\"] </code></pre><pre><code>docker build -t becloudready/my-flask-app </code></pre><p>It will show a 6 digit Code, which you need to enter to following URL</p><p>Push the Image to DockerHub</p><pre><code>docker push becloudready/my-flask-app </code></pre><p>You should be able to see the Pushed Image</p><pre><code>apiVersion: apps/v1 kind: Deployment metadata: name: flask-deployment namespace: flask-app labels: app: flask spec: replicas: 2 selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: becloudready/my-flask-app:latest # Replace with your Docker Hub image name. ports: - containerPort: 5000 env: - name: APP_ENV valueFrom: configMapKeyRef: name: flask-config key: APP_ENV - name: DEBUG valueFrom: configMapKeyRef: name: flask-config key: DEBUG - name: MYSQL_DB valueFrom: configMapKeyRef: name: flask-config key: MYSQL_DB - name: MYSQL_HOST valueFrom: configMapKeyRef: name: flask-config key: MYSQL_HOST - name: MYSQL_USER valueFrom: secretKeyRef: name: db-credentials key: username - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: flask-svc namespace: flask-app spec: selector: app: flask type: LoadBalancer ports: - port: 80 targetPort: 5000 </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: flask-config namespace: flask-app data: APP_ENV: production DEBUG: \"false\" MYSQL_DB: flaskdb MYSQL_HOST: mysql-svc.mysql.svc.cluster.local </code></pre><pre><code>apiVersion: v1 kind: Namespace metadata: name: flask-app --- apiVersion: v1 kind: Namespace metadata: name: mysql </code></pre><pre><code>kubectl create secret generic db-credentials \\ --namespace=flask-app \\ --from-literal=username=flaskuser \\ --from-literal=password=flaskpass \\ --from-literal=database=flaskdb </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: mysql-initdb namespace: mysql data: initdb.sql: | CREATE DATABASE IF NOT EXISTS flaskdb; CREATE USER 'flaskuser'@'%' IDENTIFIED BY 'flaskpass'; GRANT ALL PRIVILEGES ON flaskdb.* TO 'flaskuser'@'%'; FLUSH PRIVILEGES; </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: mysql-svc namespace: mysql spec: selector: app: mysql ports: - port: 3306 targetPort: 3306 </code></pre><pre><code>apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-statefulset namespace: mysql labels: app: mysql spec: serviceName: \"mysql-svc\" replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: initContainers: - name: init-clear-mysql-data image: busybox command: [\"sh\", \"-c\", \"rm -rf /var/lib/mysql/*\"] volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql containers: - name: mysql image: mysql:5.7 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: rootpassword # For production, use a Secret instead. - name: MYSQL_DATABASE value: flaskdb - name: MYSQL_USER value: flaskuser - name: MYSQL_PASSWORD value: flaskpass volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: initdb mountPath: /docker-entrypoint-initdb.d volumes: - name: initdb configMap: name: mysql-initdb volumeClaimTemplates: - metadata: name: mysql-persistent-storage spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi storageClassName: do-block-storage </code></pre><ul><li><p>kubectl apply -f namespaces.yaml</p></li><li><p>Deploy ConfigMaps and Secrets:</p><p>kubectl apply -f flask-config.yaml kubectl apply -f mysql-initdb.yaml kubectl apply -f db-credentials.yaml</p></li><li><p>kubectl apply -f mysql-svc.yaml kubectl apply -f mysql-statefulset.yaml</p></li><li><p>kubectl apply -f flask-deployment.yaml kubectl apply -f flask-svc.yaml</p></li></ul><p><code>kubectl get svc -n flask-app</code><code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</code><code>flask-svc LoadBalancer 10.109.112.171 146.190.190.51 80:32618/TCP 2m53s</code></p><p>Unable to connect to MySQL from Flask App</p><p>Login to the Flask app pod to ensure all values are loaded properly</p><pre><code>kubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql MYSQL_DB=flaskdb MYSQL_PASSWORD=flaskpass MYSQL_USER=flaskuser MYSQL_HOST=mysql-svc.mysql.svc.cluster.local </code></pre><ul><li>Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.</li><li>Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.</li><li>Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.</li></ul>","contentLength":7195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to manage tool dependencies in Go 1.24+","url":"https://www.alexedwards.net/blog/how-to-manage-tool-dependencies-in-go-1.24-plus","date":1740083763,"author":"/u/alexedwards","guid":7465,"unread":true,"content":"<p>One of my favourite features of Go 1.24 is the new functionality for managing  dependencies.</p><p>By this, I mean tooling that you use to assist with development, testing, build, or deployment ‚Äì such as <a href=\"https://staticcheck.dev/\"></a> for static code analysis, <a href=\"https://pkg.go.dev/golang.org/x/vuln/cmd/govulncheck\"></a> for vulnerability scanning, or <a href=\"https://www.alexedwards.net/blog/github.com/air-verse/air\"></a> for live-reloading applications.</p><p>Historically, managing these dependencies ‚Äî especially in a team setting ‚Äî has been tricky. The previous solutions have been to use a <a href=\"https://marcofranssen.nl/manage-go-tools-via-go-modules\"></a> file or the <a href=\"https://www.alexedwards.net/blog/using-go-run-to-manage-tool-dependencies\"></a> pattern, but while these approaches work, they‚Äôve always felt like workarounds with some downsides.</p><p>With Go 1.24, there‚Äôs finally a better way. </p><p>To demonstrate the new functionality, let's scaffold a simple module and add some application code.</p><figure><code><pre>$ go mod init example.com\n<samp>go: creating new go.mod: module example.com</samp>\n$ touch main.go\n</pre></code></figure><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/kr/text\"\n)\n\nfunc main() {\n    wrapped := text.Wrap(\"This is an informational message that should be wrapped.\", 30)\n    fmt.Println(wrapped)\n}\n</pre></code></figure><p>Now fetch the  package and run the code. The output should look like this:</p><figure><code><pre>$ go get github.com/kr/text\n<samp>go: downloading github.com/kr/text v0.2.0\ngo: added github.com/kr/text v0.2.0</samp>\n$ go run .\n<samp>This is an informational\nmessage that should be\nwrapped.</samp></pre></code></figure><p>Go 1.24 introduces the  flag for , which you can use like this:</p><figure><code><pre>go get -tool import_path@version\n</pre></code></figure><p>This command will download the package specified by the import path (along with any child dependencies), store them in your module cache, and record them in your  file. The  part is optional ‚Äì if you omit it, the latest version will be downloaded.</p><p>Let's use this to add the latest versions of  and  to our module as developer tools, along with  version .</p><figure><code><pre>$ go get -tool golang.org/x/tools/cmd/stringer\n<samp>go: downloading golang.org/x/tools v0.30.0\ngo: downloading golang.org/x/sync v0.11.0\ngo: downloading golang.org/x/mod v0.23.0\ngo: added golang.org/x/mod v0.23.0\ngo: added golang.org/x/sync v0.11.0\ngo: added golang.org/x/tools v0.30.0</samp>\n\n$ go get -tool golang.org/x/vuln/cmd/govulncheck\n<samp>go: downloading golang.org/x/vuln v1.1.4\ngo: downloading golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7\ngo: downloading golang.org/x/sys v0.30.0\ngo: upgraded golang.org/x/telemetry v0.0.0-20240521205824-bda55230c457 =&gt; v0.0.0-20240522233618-39ace7a40ae7\ngo: added golang.org/x/vuln v1.1.4</samp>\n\n$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.1\n<samp>go: downloading honnef.co/go/tools v0.5.1\ngo: downloading golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678\ngo: downloading github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c\ngo: downloading golang.org/x/exp v0.0.0-20231110203233-9a3e6036ecaa\ngo: added github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c\ngo: added golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678\ngo: added honnef.co/go/tools v0.5.1</samp></pre></code></figure><p>After running these, your  file will now include a  section listing the tools you've added. The corresponding module paths and versions for all the dependencies will appear in the  section and be marked as indirect:</p><figure><code><pre>module example.com\n\ngo 1.24.0\n\nrequire (\n    github.com/BurntSushi/toml v1.4.1-0.20240526193622-a339e1f7089c // indirect\n    github.com/kr/text v0.2.0 // indirect\n    golang.org/x/exp/typeparams v0.0.0-20231108232855-2478ac86f678 // indirect\n    golang.org/x/mod v0.23.0 // indirect\n    golang.org/x/sync v0.11.0 // indirect\n    golang.org/x/sys v0.30.0 // indirect\n    golang.org/x/telemetry v0.0.0-20240522233618-39ace7a40ae7 // indirect\n    golang.org/x/tools v0.30.0 // indirect\n    golang.org/x/vuln v1.1.4 // indirect\n    honnef.co/go/tools v0.5.1 // indirect\n)\n\ntool (\n    golang.org/x/tools/cmd/stringer\n    golang.org/x/vuln/cmd/govulncheck\n    honnef.co/go/tools/cmd/staticcheck\n)\n</pre></code></figure><p>Once added, you can run tools using the  command.</p><p>To run a specific tool from the command line within your module, you can use  followed by the last non-major-version segment of the import path for the tool (which is, normally, just the name for the tool). For example:</p><figure><code><pre>$ go tool staticcheck -version\n<samp>staticcheck 2024.1.1 (0.5.1)</samp>\n\n$ go tool govulncheck\n<samp>No vulnerabilities found.</samp></pre></code></figure><p>The  command also works nicely if you want to execute tools from your scripts or Makefiles. To illustrate, let's create a Makefile with an  task that runs staticcheck and govulncheck on the codebase.</p><pre><code>.PHONY: audit\naudit:\n    go vet ./...\n    go tool staticcheck ./...\n    go tool govulncheck\n</code></pre><p>If you run , you should see that all the checks complete successfully.</p><figure><code><pre>$ make audit\n<samp>go vet ./...\ngo tool staticcheck ./...\ngo tool govulncheck\nNo vulnerabilities found.</samp></pre></code></figure><p>Let's also take a look at an example where we use the stringer tool in conjunction with  to generate  methods for some  constants.</p><figure><code><pre>package main\n\nimport (\n    \"fmt\"\n\n    \"github.com/kr/text\"\n)\n\n//go:generate go tool stringer -type=Level\n\ntype Level int\n\nconst (\n    Info Level = iota\n    Error\n    Fatal\n)\n\nfunc main() {\n    wrapped := text.Wrap(\"This is an informational message that should be wrapped.\", 30)\n\n    fmt.Printf(\"%s: %s\\n\", Info, wrapped)\n}\n</pre></code></figure><p>The important thing here is the  line. When you run  on this file, it will in turn use  to execute the version of the stringer tool listed in your  file.</p><figure><code><pre>$ go generate .\n$ ls \n<samp>go.mod  go.sum  level_string.go  main.go  Makefile</samp></pre></code></figure><p>You should see that a new  file is created, and running the application should result in some output that looks like this:</p><figure><code><pre>$ go run .\n<samp>Info: This is an informational\nmessage that should be\nwrapped.</samp></pre></code></figure><p>You can check which tools have been added to a module by running , like so:</p><figure><code><pre>$ go list tool\n<samp>honnef.co/go/tools/cmd/staticcheck\ngolang.org/x/tools/cmd/stringer\ngolang.org/x/vuln/cmd/govulncheck</samp></pre></code></figure><p>Because the tools are included in your  file as dependencies, if you want to check that the code for the tools stored in your module cache has not changed you can simply run :</p><figure><code><pre>$ go mod verify\n</pre></code></figure><p>This will check that the code in your module cache exactly matches the corresponding checksums in your  file.</p><p>If you run , the code for tooling dependencies will be included in the  folder and the  manifest alongside your non-tool dependencies.</p><figure><code><pre>$ go mod vendor\n$  tree  -L 3\n<samp>.\n‚îú‚îÄ‚îÄ go.mod\n‚îú‚îÄ‚îÄ go.sum\n‚îú‚îÄ‚îÄ main.go\n‚îú‚îÄ‚îÄ Makefile\n‚îî‚îÄ‚îÄ vendor\n        ‚îú‚îÄ‚îÄ github.com\n        ‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ BurntSushi\n        ‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ kr\n        ‚îú‚îÄ‚îÄ golang.org\n        ‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ x\n        ‚îú‚îÄ‚îÄ honnef.co\n        ‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ go\n        ‚îî‚îÄ‚îÄ modules.txt</samp></pre></code></figure><p>When tools are vendored in this way, running  will execute the corresponding code in the  directory. Note that  does not work on vendored code.</p><p>To upgrade or downgrade a specific tool to a specific version, you can use the same <code>go get -tool import_path@version</code> command that you did for adding the tool originally. For example:</p><figure><code><pre>$ go get -tool honnef.co/go/tools/cmd/staticcheck@v0.5.0\n</pre></code></figure><p>To upgrade to the latest version of a specific tool, omit the  suffix. </p><figure><code><pre>$ go get -tool honnef.co/go/tools/cmd/staticcheck\n</pre></code></figure><p>You can also upgrade  to their latest version by running . Note:  is a sub-command here, not a flag.</p><p>If your tool dependencies are vendored, you will need to re-run  after any upgrades or downgrades.</p><p>At the time of writing, I'm not aware of any easy way to specifically list the tools that have upgrades available ‚Äì if you know of one please let me know!</p><p>To remove the tool completely from your module, use  with the special version tag .</p><figure><code><pre>$ go get -tool honnef.co/go/tools/cmd/staticcheck@none\n</pre></code></figure><p>Again, if you're vendoring, make sure to run  after removing a tool.</p><p>A <a href=\"https://old.reddit.com/r/golang/comments/1iu8nkj/how_to_manage_tool_dependencies_in_go_124/mdzscsa/\">Reddit commenter</a> mentioned the potential for problems if your tools share dependencies with your application code. For example, let's say that your application code depends on  version , and is tested and known to work with that version. Then if you add a tool that relies on a  version of , the version number in your  file will be bumped to the newer version and your application code will use that newer version too.</p><p>In theory, this  be a problem so long as all your dependencies and their child dependencies are stable, follow strict semantic versioning, and don't make backwards-incompatible changes without a major version increment. But, of course, the real world is messy and backwards-incompatible changes  happen, which could unexpectedly break your application code.</p><p>It's worth noting that this issue isn't limited to tool dependencies ‚Äì the same thing can happen if your application code and a non-tool dependency both rely on the same package. However, including tools in  increases the risk.</p><p>To reduce this risk, you can use a separate modfile for tool dependencies instead of including them in your main . You can do this with the  flag, specifying an alternative file such as , like so:</p><figure><code><pre><samp># Initialize a go.tool.mod modfile</samp>\n$ go mod init -modfile=go.tool.mod example.com\n\n<samp># Add a tool to the module</samp>\n$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck\n\n<samp># Run the tool from the command line</samp>\n$ go tool -modfile=go.tool.mod govulncheck\n\n<samp># List all tools added to the module</samp>\n$ go list -modfile=go.tool.mod tool\n\n<samp># Verify the integrity of the tool dependencies</samp>\n$ go mod verify -modfile=go.tool.mod\n\n<samp># Upgrade or downgrade a tool to a specific version</samp>\n$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@v1.1.2\n\n<samp># Upgrade all tools to their latest version</samp>\n$ go get -modfile=go.tool.mod tool\n\n<samp># Remove a tool from the module</samp>\n$ go get -tool -modfile=go.tool.mod golang.org/x/vuln/cmd/govulncheck@none\n</pre></code></figure>","contentLength":9405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iu8nkj/how_to_manage_tool_dependencies_in_go_124/"},{"title":"Why do temporaries need to explicitly borrowed?","url":"https://www.reddit.com/r/rust/comments/1iu8jsn/why_do_temporaries_need_to_explicitly_borrowed/","date":1740083498,"author":"/u/parkotron","guid":8474,"unread":true,"content":"<p>As a long time C++ dev, I feel it didn't take me very long to pick up Rust's reference semantics and borrowing rules, but there one place where I constantly find myself forgetting to include the : passing temporaries into functions taking references.</p><pre><code>fn foo(s: &amp;str) { println!(\"The str is: {s}\"); } fn bar() -&gt; String { \"temporary\".to_string() } fn main() { foo(&amp;bar()); // ^ I always forget this ampersand until reminded by the compiler. } </code></pre><p>Rust's explicit  and  operators make a lot of sense to me: given a chunk of code, it should be obvious where a value has been borrowed and what kind of borrow it is. One should never be surprised to learn a reference was taken, because it's right there in the code.</p><p>But in the case of temporary values, it really doesn't matter, does it? Whatever a function call does (or doesn't) do to a temporary value passed to it, the effect cannot be observed in the surrounding code, since the temporary is gone by the end of the statement.</p><p>Is there a subtlety I'm missing here? Does that ampersand on a temporary convey useful information to an experienced Rust dev? Or is it really just syntactic noise, as it seems to me? Are there corner cases I'm just not considering? Could a future edition of Rust be changed to implicitly borrow from temporaries (like it implicitly borrows to make method calls)? Is my mental model just wrong?</p><p>To be perfectly clear, this isn't a criticism, just curiosity. Clearly a lot of thought has been put into the language's design and syntax. This is just the only place I've encountered where Rust's explicitness doesn't feel completely justified.</p>","contentLength":1609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To the purists rocking linux from scratch systems: how was it?","url":"https://www.reddit.com/r/linux/comments/1iu7gc4/to_the_purists_rocking_linux_from_scratch_systems/","date":1740080811,"author":"/u/0110010001101111","guid":8631,"unread":true,"content":"<p>how was your experience from installation to day to day management? what was your use case to build such system over just choosing a distro.</p><p>the apps and the updating it. is it a hassle?</p><p>is it a viable or reasonable option as a daily driver. i just wanted to get some insights about it.</p><p>what do you like or dont like about it. the tradeoffs you were willing to accept, etc. </p>","contentLength":371,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rate my photo manipulation tool","url":"https://www.reddit.com/r/golang/comments/1iu6pch/rate_my_photo_manipulation_tool/","date":1740079001,"author":"/u/tunerhd","guid":7513,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/tunerhd\"> /u/tunerhd </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI can fix bugs‚Äîbut can‚Äôt find them: OpenAI‚Äôs study highlights limits of LLMs in software engineering","url":"https://venturebeat.com/ai/ai-can-fix-bugs-but-cant-find-them-openais-study-highlights-limits-of-llms-in-software-engineering/","date":1740078804,"author":"/u/F0urLeafCl0ver","guid":8473,"unread":true,"content":"<div><p><em>Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. <a href=\"https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav\" data-type=\"link\" data-id=\"https://venturebeat.com/newsletters/?utm_source=VBsite&amp;utm_medium=desktopNav\">Learn More</a></em></p></div><p>In a <a href=\"https://arxiv.org/pdf/2502.12115\" target=\"_blank\" rel=\"noreferrer noopener\">new paper</a>, <a href=\"https://openai.com/\" target=\"_blank\" rel=\"noreferrer noopener\">OpenAI</a> researchers detail how they developed an LLM benchmark called SWE-Lancer to test how much foundation models can earn from real-life freelance software engineering tasks. The test found that, while the models can solve bugs, they can‚Äôt see why the bug exists and continue to make more mistakes.&nbsp;</p><p>The researchers tasked three LLMs ‚Äî OpenAI‚Äôs GPT-4o and o1 and <a href=\"https://venturebeat.com/ai/the-code-whisperer-how-anthropics-claude-is-changing-the-game-for-software-developers/\">Anthropic‚Äôs Claude-3.5 Sonnet</a> ‚Äî with 1,488 freelance software engineer tasks <a href=\"https://venturebeat.com/business/upwork-shares-leap-40-as-ceo-calls-ipo-beginning-of-a-new-chapter/\">from the freelance platform</a> Upwork amounting to $1 million in payouts. They divided the tasks into two categories: individual contributor tasks (resolving bugs or implementing features), and management tasks (where the model roleplays as a manager who will choose the best proposal to resolve issues).&nbsp;</p><p>‚ÄúResults indicate that the real-world freelance work in our benchmark remains challenging for frontier language models,‚Äù the researchers write.&nbsp;</p><p>The test shows that foundation models cannot fully replace human engineers. While they can help solve bugs, they‚Äôre not quite at the level where they can start earning freelancing cash by themselves.&nbsp;</p><h2>Benchmarking freelancing models</h2><p>The researchers and 100 other professional software engineers identified potential tasks on Upwork and, without changing any words, fed these to a Docker container to create the SWE-Lancer dataset. The container does not have internet access and cannot access GitHub ‚Äúto avoid the possible of models scraping code diffs or pull request details,‚Äù they explained. </p><p>The team identified 764 individual contributor tasks, totaling about $414,775, ranging from 15-minute bug fixes to weeklong feature requests. These tasks, which included reviewing freelancer proposals and job postings, would pay out $585,225.</p><p>The tasks were added to the expensing platform Expensify.&nbsp;</p><p>The researchers generated prompts based on the task title and description and a snapshot of the codebase. If there were additional proposals to resolve the issue, ‚Äúwe also generated a management task using the issue description and list of proposals,‚Äù they explained. </p><p>From here, the researchers moved to end-to-end test development. They wrote Playwright tests for each task that applies these generated patches which were then ‚Äútriple-verified‚Äù by professional software engineers.</p><p>‚ÄúTests simulate real-world user flows, such as logging into the application, performing complex actions (making financial transactions) and verifying that the model‚Äôs solution works as expected,‚Äù the paper explains.&nbsp;</p><p>After running the test, the researchers found that none of the models earned the full $1 million value of the tasks. Claude 3.5 Sonnet, the best-performing model, earned only $208,050 and resolved 26.2% of the individual contributor issues. However, the researchers point out, ‚Äúthe majority of its solutions are incorrect, and higher reliability is needed for trustworthy deployment.‚Äù</p><p>The models performed well across most individual contributor tasks, with Claude 3.5-Sonnet performing best, followed by o1 and GPT-4o.&nbsp;</p><p>‚ÄúAgents excel at localizing, but fail to root cause, resulting in partial or flawed solutions,‚Äù the report explains. ‚ÄúAgents pinpoint the source of an issue remarkably quickly, using keyword searches across the whole repository to quickly locate the relevant file and functions ‚Äî often far faster than a human would. However, they often exhibit a limited understanding of how the issue spans multiple components or files, and fail to address the root cause, leading to solutions that are incorrect or insufficiently comprehensive. We rarely find cases where the agent aims to reproduce the issue or fails due to not finding the right file or location to edit.‚Äù</p><p>Interestingly, the models all performed better on manager tasks that required reasoning to evaluate technical understanding.</p><p>These benchmark tests showed that AI models can solve some ‚Äúlow-level‚Äù coding problems and can‚Äôt replace ‚Äúlow-level‚Äù software engineers yet. The models still took time, often made mistakes, and couldn‚Äôt chase a bug around to find the root cause of coding problems. Many ‚Äúlow-level‚Äù engineers work better, but the researchers said this may not be the case for very long.&nbsp;</p>","contentLength":4364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iu6mdl/ai_can_fix_bugsbut_cant_find_them_openais_study/"},{"title":"TwinSong: Jupyter notebook built from scratch in Rust","url":"https://www.reddit.com/r/rust/comments/1iu5tpa/twinsong_jupyter_notebook_built_from_scratch_in/","date":1740076846,"author":"/u/winter-moon","guid":7469,"unread":true,"content":"<p>I've spent a lot of time working with Python in Jupyter notebooks, but one thing has always bothered me: the way code and outputs are mixed together. While this is great for tutorials and interactive documentation, it's less ideal for exploratory work or data processing, where I just want to interact with Python without the constraints of a document-style interface. </p><p>To address this, I created TwinSong, a Jupyter alternative that separates code and outputs. Right now, it's primarily a UX experiment, but core features like editing and executing cells are already in place. Instead of modifying Jupyter's existing codebase, I built it from scratch with a React frontend and a Rust backend.</p><p>While performance wasn't the main focus, implementing a Python kernel driver in Rust keeps the kernel clean and avoids loading Python dependencies that might interfere with user code. Plus, as we've seen with other projects, rewriting classic Python tools in Rust can open up new possibilities.</p>","contentLength":986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[D] Enriching token embedding with last hidden state?","url":"https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/","date":1740074778,"author":"/u/Academic_Sleep1118","guid":8599,"unread":true,"content":"<p>Looking at a decoder transformer working process from an information theory standpoint, we can see that the information available in the last hidden state is collapsed into a single token during generation. It means that you collapse a hidden state that, in theory, has about:</p><p> (or whatever quant) bits of information to something like:</p><p>I wonder if it's a good thing (sorry for the naive phrasing). The information used by a transformer to predict the next token is entirely stored in its context window and does not involve any recurrent state. So, predicting the next token of a sequence the transformer was just fed with is going to yield the exact same result as doing so for the same sequence if it were entirely generated by the transformer itself.</p><p>Fair enough, in some sense: whether the sequence was generated or just read doesn't change anything about what the next token should be.</p><p>But on the other hand, this approach means that  the information flow between tokens has to happen through the attention mechanism. There's no way for the transformer to embed some nuance or flavor into the predicted token embedding. Like in:</p><p><em>\"Well, I predicted the token '</em></p><p>When the next token is predicted, this nuance that was likely present in the last hidden state (or even in the softmaxed output probability distribution) is totally lost.</p><p>So while I was having a little walk yesterday, I was thinking that it might be a good idea to add some information to the token embeddings using something like:</p><p><strong>augmented_embedding = embedding(token) + F(last_hidden_state)</strong></p><p>(It would be important to make sure that:</p><p><strong>‚ÄñF(last_hidden_state)‚Äñ ‚â™ ‚Äñembedding(token)‚Äñ</strong></p><p>I have tried to find papers on this subject and asked for feedback from Claude, ChatGPT, and Perplexity.</p><ul><li> told me it was <em>\"an incredibly insightful idea.\"</em></li><li> hallucinated a paper on the subject.</li><li> gave me a very long list of totally unrelated sources.</li></ul><p>So I'm turning to you guys. I would love it if some big-brained guy told me why other big-brained guys decided not to follow this idea, or why it doesn't work.</p><p>Here are some things I identified as potentially problematic:</p><p>Transformers are nice to train with heavy parallelization precisely because they are not recursive. Each sequence of size  can give  independent training examples. Injecting last hidden states' information in token embeddings would break some of that parallelization.</p><p>It would still be possible to train it efficiently, I guess.</p><ol><li>First, take the () vanilla sequences and get the predictions.</li><li>Then, for each prediction, store the last hidden state and update the corresponding token embedding in each of the sequences where it appears.</li><li>Now, you have a new set of training sequences, with all (but the first) token embeddings updated.</li><li>You can repeat this process indefinitely. I hope it converges ^^</li></ol><p>This really looks like a diffusion process, by the way. That brings me to the next point:</p><p>Here, I am not very competent. What are the conditions that define such a process' stability? My uneducated guess is that if you keep:<strong>‚Äñlast_hidden_state_contribution‚Äñ ‚â™ ‚Äñaugmented_token_embedding‚Äñ</strong> you should not have many problems. But it would also limit the information flow. I guess there's a trade-off, and I wouldn't be surprised if it's not good enough.</p><p>What do you guys think? Has this already been tried somewhere? Is there a fundamental reason this wouldn't work?</p>","contentLength":3370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thoughts on an AI powered bipedal, musculoskeletal , anatomically accurate, synthetic human with over 200 degrees of freedom, over 1,000 Myofibers, and 500 sensors?","url":"https://v.redd.it/b1iwrsu32cke1","date":1740074243,"author":"/u/VivariuM_007","guid":7436,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/artificial/comments/1iu4qex/thoughts_on_an_ai_powered_bipedal_musculoskeletal/"},{"title":"Announcing Rust 1.85.0 and Rust 2024 | Rust Blog","url":"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html","date":1740071479,"author":"/u/slanterns","guid":7359,"unread":true,"content":"<p>The Rust team is happy to announce a new version of Rust, 1.85.0. This stabilizes the 2024 edition as well.\nRust is a programming language empowering everyone to build reliable and efficient software.</p><p>If you have a previous version of Rust installed via , you can get 1.85.0 with:</p><p>If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel () or the nightly channel (). Please <a href=\"https://github.com/rust-lang/rust/issues/new/choose\">report</a> any bugs you might come across!</p><p>We are excited to announce that the Rust 2024 Edition is now stable!\nEditions are a mechanism for opt-in changes that may otherwise pose a backwards compatibility risk. See <a href=\"https://doc.rust-lang.org/edition-guide/editions/index.html\">the edition guide</a> for details on how this is achieved, and detailed instructions on how to migrate.</p><p>This is the largest edition we have released. The <a href=\"https://doc.rust-lang.org/edition-guide/rust-2024/index.html\">edition guide</a> contains detailed information about each change, but as a summary, here are all the changes:</p><p>The guide includes migration instructions for all new features, and in general\n<a href=\"https://doc.rust-lang.org/edition-guide/editions/transitioning-an-existing-project-to-a-new-edition.html\">transitioning an existing project to a new edition</a>.\nIn many cases  can automate the necessary changes. You may even find that no changes in your code are needed at all for 2024!</p><p>Note that automatic fixes via  are very conservative to avoid ever changing the semantics of your code. In many cases you may wish to keep your code the same and use the new semantics of Rust 2024; for instance, continuing to use the  macro matcher, and ignoring the conversions of conditionals because you want the new 2024 drop order semantics. The result of  should not be considered a recommendation, just a conservative conversion that preserves behavior.</p><p> people came together to create this edition. We'd like to thank them all for their hard work!</p><p>Rust now supports asynchronous closures like  which return futures when called. This works like an  which can also capture values from the local environment, just like the difference between regular closures and functions. This also comes with 3 analogous traits in the standard library prelude: , , and .</p><p>In some cases, you could already approximate this with a regular closure and an asynchronous block, like . However, the future returned by such an inner block is not able to borrow from the closure captures, but this does work with  closures:</p><pre><code>let mut vec: Vec&lt;String&gt; = vec![];\n\nlet closure = async || {\n    vec.push(ready(String::from(\"\")).await);\n};\n</code></pre><p>It also has not been possible to properly express higher-ranked function signatures with the  traits returning a , but you can write this with the  traits:</p><pre><code>use core::future::Future;\nasync fn f&lt;Fut&gt;(_: impl for&lt;'a&gt; Fn(&amp;'a u8) -&gt; Fut)\nwhere\n    Fut: Future&lt;Output = ()&gt;,\n{ todo!() }\n\nasync fn f2(_: impl for&lt;'a&gt; AsyncFn(&amp;'a u8))\n{ todo!() }\n\nasync fn main() {\n    async fn g(_: &amp;u8) { todo!() }\n    f(g).await;\n    //~^ ERROR mismatched types\n    //~| ERROR one type is more general than the other\n\n    f2(g).await; // ok!\n}\n</code></pre><h3><a href=\"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html#hiding-trait-implementations-from-diagnostics\" aria-hidden=\"true\"></a>Hiding trait implementations from diagnostics</h3><p>The new <code>#[diagnostic::do_not_recommend]</code> attribute is a hint to the compiler to not show the annotated trait implementation as part of a diagnostic message. For library authors, this is a way to keep the compiler from making suggestions that may be unhelpful or misleading. For example:</p><pre><code>pub trait Foo {}\npub trait Bar {}\n\nimpl&lt;T: Foo&gt; Bar for T {}\n\nstruct MyType;\n\nfn main() {\n    let _object: &amp;dyn Bar = &amp;MyType;\n}\n</code></pre><pre><code>error[E0277]: the trait bound `MyType: Bar` is not satisfied\n --&gt; src/main.rs:9:29\n  |\n9 |     let _object: &amp;dyn Bar = &amp;MyType;\n  |                             ^^^^ the trait `Foo` is not implemented for `MyType`\n  |\nnote: required for `MyType` to implement `Bar`\n --&gt; src/main.rs:4:14\n  |\n4 | impl&lt;T: Foo&gt; Bar for T {}\n  |         ---  ^^^     ^\n  |         |\n  |         unsatisfied trait bound introduced here\n  = note: required for the cast from `&amp;MyType` to `&amp;dyn Bar`\n</code></pre><p>For some APIs, it might make good sense for you to implement , and get  indirectly by that blanket implementation. For others, it might be expected that most users should implement  directly, so that  suggestion is a red herring. In that case, adding the diagnostic hint will change the error message like so:</p><pre><code>#[diagnostic::do_not_recommend]\nimpl&lt;T: Foo&gt; Bar for T {}\n</code></pre><pre><code>error[E0277]: the trait bound `MyType: Bar` is not satisfied\n  --&gt; src/main.rs:10:29\n   |\n10 |     let _object: &amp;dyn Bar = &amp;MyType;\n   |                             ^^^^ the trait `Bar` is not implemented for `MyType`\n   |\n   = note: required for the cast from `&amp;MyType` to `&amp;dyn Bar`\n</code></pre><h3><a href=\"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html#fromiterator-and-extend-for-tuples\" aria-hidden=\"true\"></a> and  for tuples</h3><p>Earlier versions of Rust implemented convenience traits for iterators of  tuple pairs to behave like , with  in 1.56 and  in 1.79. These have now been  to more tuple lengths, from singleton  through to 12 items long, . For example, you can now use  to fanout into multiple collections at once:</p><pre><code>use std::collections::{LinkedList, VecDeque};\nfn main() {\n    let (squares, cubes, tesseracts): (Vec&lt;_&gt;, VecDeque&lt;_&gt;, LinkedList&lt;_&gt;) =\n        (0i32..10).map(|i| (i * i, i.pow(3), i.pow(4))).collect();\n    println!(\"{squares:?}\");\n    println!(\"{cubes:?}\");\n    println!(\"{tesseracts:?}\");\n}\n</code></pre><pre><code>[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n[0, 1, 8, 27, 64, 125, 216, 343, 512, 729]\n[0, 1, 16, 81, 256, 625, 1296, 2401, 4096, 6561]\n</code></pre><h3><a href=\"https://blog.rust-lang.org/2025/02/20/Rust-1.85.0.html#updates-to-stdenvhome_dir\" aria-hidden=\"true\"></a>Updates to </h3><p> has been deprecated for years, because it can give surprising results in some Windows configurations if the  environment variable is set (which is not the normal configuration on Windows). We had previously avoided changing its behavior, out of concern for compatibility with code depending on this non-standard configuration. Given how long this function has been deprecated, we're now updating its behavior as a bug fix, and a subsequent release will remove the deprecation for this function.</p><p>These APIs are now stable in const contexts</p><p>Many people came together to create Rust 1.85.0. We couldn't have done it without all of you. <a href=\"https://thanks.rust-lang.org/rust/1.85.0/\">Thanks!</a></p>","contentLength":5852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iu3l0a/announcing_rust_1850_and_rust_2024_rust_blog/"},{"title":"Ugly Code and Dumb Things","url":"https://lucumr.pocoo.org/2025/2/20/ugly-code/","date":1740070620,"author":"/u/FoxInTheRedBox","guid":7438,"unread":true,"content":"<p data-date=\"2025-02-20T00:00:00\">written on Thursday, February 20, 2025</p><p>This week I had a conversation with one of our engineers about ‚Äúshitty\ncode‚Äù which lead me to sharing with him one of my more unusual\ninspirations: <a href=\"https://github.com/exflickr/flamework/\">Flamework</a>, a\npseudo framework created at Flickr.</p><div><h2>Two Passions, Two Approaches</h2><p>There are two driving passions in my work.  One is the love of creating\nbeautiful, elegant code ‚Äî making Open Source libraries and APIs that focus\non clear design and reusability.  The other passion is building quick,\npragmatic solutions for real users (who may not even be developers).  The\nlatter usually in a setting of building a product, where the product is\nnot the code.  Here, speed and iteration matter more than beautiful code\nor reusability, because success hinges on shipping something people want.</p><p>Flamework is in service of the latter, and in crass violation of the\nformer.</p><p>Early on, I realized that creating reusable code and directly solving\nproblems for users are often at odds.  My first clue came when I helped\nrun the German\n<a href=\"https://www.ubuntuusers.de/\">ubuntuusers</a> website.  It was powered by\na heavily modified version of phpBB, which despite how messy it was,\nscaled to a large user base when patched properly.  It was messy, but easy\nto adjust.  The abstractions were one layer deep.</p><p>Back then, me and a friend tried to replace it by writing my own bulletin\nboard software, <a href=\"https://web.archive.org/web/20070502223619/http://flying.circus.pocoo.org/\">Pocoo</a>.\nWorking in isolation, without users, led me down a path of\nover-engineering.  While we learned a lot and ended up creating popular\nOpen Source libraries (like Jinja, Werkzeug and Pygments), Pocoo never\nbecame a solid product.  Later, my collaborators and I <a href=\"https://github.com/inyokaproject/inyoka/\">rebuilt\nubuntuusers</a>, without the\ngoal of making it into a reusable product.  That rewrite shipped\nsuccessfully and it lives to this very day.</p><p>But it took me years to fully realize what was happening here: reusability\nis not that important when you‚Äôre building an application, but it‚Äôs\ncrucial when you‚Äôre building a library or framework.</p></div><div><p>If you are unfamiliar with Flamework you should watch a talk that Cal\nHenderson gave in 2008 at DjangoCon (<a href=\"https://www.youtube.com/watch?v=i6Fr65PFqfk\">Why I hate Django</a>).  He talked about scale\nand how Django didn't solve for it.  He enumerated all the things\nimportant to him: sharding, using custom sequences for primary keys,\nforgoing joins and foreign keys, supporting database replication setups,\ndenormalizing data to the extreme.  This is also were I first learned\nabout the possibility of putting all session data into cookies via\nsigning.  It was a memorable talk for me because it showed me that there\nare shortcomings.  Django (which I used for ubuntuusers) had beautiful\nAPIs but at the time solved for little of that Cal needed.  The talk\nreally stuck with me.</p><p>At the time of the talk, Flamework did not really exist.  It was more of\nan idea and principles of engineering at Flickr.</p><p>A few years later, Flamework appeared on GitHub, not as an open-sourced\npiece of Flickr code but as a reimplementation of those same ideas.  You\ncan explore its repository and see code like this:</p><div><pre></pre></div><p>Instinctively it makes me cringe.  Is that a SQL injection?  Well you were\nsupposed to use the PHP <a href=\"https://www.php.net/manual/en/function.addslashes.php\">addslashes</a> function\nbeforehand.  But notice how it caters to sharding and clustering directly\nin the query function.</p></div><div><p>Code like this often triggers a visceral reaction, especially in engineers\nwho prize clean design.</p><p>How does something like that get created?  Cal Henderson described\nFlickr's principle as ‚Äúdoing the dumbest possible thing that will work.‚Äù\nMaybe ‚Äúdumb‚Äù is too strong ‚Äî ‚Äúsimple‚Äù might be more apt.  Yet simplicity\ncan look messy to someone expecting a meticulously engineered codebase.\nThis is not at all uncommon and I have seen it over and over.  The first\nlarge commercial project that got traction that I ever worked on (<a href=\"https://en.wikipedia.org/wiki/Plurk\">Plurk</a>) was also pretty pragmatic and\nmessy inside.  My former colleague Ben Vinegar also <a href=\"https://benv.ca/blog/posts/the-hardest-problem\">recently shared</a> a story of early,\nmessy FreshBooks code and how he came to terms with it.  Same story at\n<a href=\"https://sentry.io/welcome\">Sentry</a>.  We moved fast, we made a mess.</p><p>None of this is surprising in retrospective.  Perfect code doesn't\nguarantee success if you haven't solved a real problem for real people.\nPursuing elegance in a vacuum leads to abandoned side projects or\nframeworks nobody uses.  By contrast, clunky but functional code often\ncomes with just the right compromises for quick iteration.  And that in\nturn means a lot of messy code powers products that people love ‚Äî\nsomething that's a far bigger challenge.</p></div><div><p>I have shown Flamework's code to multiple engineers over the years and it\nusually creates such a visceral response.  It blind sights one by\nseemingly disregarding all rules of good software engineering.</p><p>That makes Flamework serve as a fascinating Rorschach test for engineers.\nAre you <a href=\"https://github.com/exflickr/flamework\">looking at it</a> with\nadmiration for the focus on some critical issues like scale, the built-in\nobservability and debugging tools.  Or are you judging it, and its\ncreators, for manually constructing SQL queries, using global variables,\nnot using classes and looking like messy PHP4 code?  Is it a pragmatic\ntool, intentionally designed to iterate quickly at scale, or is it a naive\nmess made by unskilled developers?</p><p>Would I use Flamework?  Hello no.  But I appreciate the priorities behind\nit.  If these ugly choices help you move faster, attract users and\nvalidate the product, then a rewrite, or large refactorings later are a\nsmall price to pay.</p></div><div><p>At the end of the day, where you stand on ‚Äúshitty code‚Äù depends on your\nprimary goal:</p><ul><li>Are you shipping a product and racing to meet user needs?</li><li>Or are you building a reusable library or framework meant to stand the\ntest of time?</li></ul><p>Both mindsets are valid, but they rarely coexist harmoniously in a single\ncodebase.  Flamework is a reminder that messy, simple solutions can be\npowerful if they solve real problems.  Eventually, when the time is right,\nyou can clean it up or rebuild from the ground up.</p><p>The real challenge is deciding which route to take ‚Äî and when.  Even with\nexperience, it is can be hard to know when to move from quick fixes to\nmore robust foundations.  The principles behind Flamework are also\nreflected in <a href=\"https://develop.sentry.dev/getting-started/philosophy/\">Sentry's development philosophy</a>.  One more\npoignant one being ‚ÄúEmbrace the Duct Tape‚Äù.  Yet as Sentry matured, much\nof our duct tape didn't stand the test of time, and was re-applied at\nmoments when the real solution would have been a solid foundation poured\nwith concrete.</p><p>That's because successful projects eventually grow up.  What let you\niterate fast in the beginning might eventually turn into an unmaintainable\nmess and will be rebuilt from the inside out.</p><p>I personally would never have built Flamework, it repulses me a bit.  At the\nsame time, I have a enormous respect for the people who build it.  Their\nwork and thinking has shaped how I solve problems and think of product\nengineering.</p></div>","contentLength":6780,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1iu37xs/ugly_code_and_dumb_things/"},{"title":"Why Firefox?","url":"https://www.reddit.com/r/linux/comments/1iu25zd/why_firefox/","date":1740068011,"author":"/u/Flaky_Comfortable425","guid":7437,"unread":true,"content":"<p>This actually makes me curious, when I switch between a lot of distros, jumping from Debian to CentOS to dfferent distros, I can see that they all love firefox, it's not my favorite actually, and there are plenty of internet browsers out there which is free and open source like Brave for example, still I am wondering what kind of attachment they have to this browser</p>","contentLength":368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"C equivalent of select() / poll() in go socket programming","url":"https://www.reddit.com/r/golang/comments/1iu1ugj/c_equivalent_of_select_poll_in_go_socket/","date":1740067219,"author":"/u/ChestPainGuy","guid":8416,"unread":true,"content":"<p>Hi, I'm fairly new to socket programming and go, so forgive my ignorance.</p><p>Recently, I have been reading up Beej's guide to network programming, where he explains the use of  and  to read and write to multiple sockets without blocking.</p><p>I have googled quite a bit, but almost every tutorial or go example on the basics of socket connections just spawn a new goroutine with something like .</p><ol><li>So whats' the equivalent of  in go?</li><li>Is spawning a goroutine for every connection an effective approach?</li></ol><p>Any good links to network programming in go would be appreciated if this question is too dumb. Thanks</p>","contentLength":588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is anyone working on AI designed to preserve democracy?","url":"https://www.reddit.com/r/artificial/comments/1iu1n10/is_anyone_working_on_ai_designed_to_preserve/","date":1740066712,"author":"/u/BarbaGramm","guid":7466,"unread":true,"content":"<p>I‚Äôm looking for people or groups who are already working on something like this:</p><p>A decentralized AI trained to preserve the intellectual, historical, and emotional essence of democracy‚Äîwhat it actually means, not just what future regimes might redefine it to be. Think of it as a fusion of data hoarding, decentralized AI, and resistance tech, built to withstand authoritarian drift and historical revisionism.</p><p>Maybe it doesn't reach the heights of the corporate or state models, but a system that can always articulate the delta‚Äîthe difference between a true democratic society (or at least what we seem to be leaving behind) and whatever comes next. If democracy gets twisted into something unrecognizable, this AI should be able to compare, contrast, and remind people what was lost. It should be self-contained, offline-capable, decentralized, and resistant to censorship‚Äîan incorruptible witness to history.</p><p>Does this exist? Are there people in AI, decentralized infrastructure, or archival communities working toward something like this? I don‚Äôt want to reinvent the wheel if a community is already building it. If you know of any projects, frameworks, or people tackling this problem, please point me in the right direction.</p><p>If no one is doing it, shouldn't this be a project people are working on? Is there an assumption that corporate or state controlled AI will do this inherently?</p>","contentLength":1397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Slice Internals in Go: How the Runtime Expands Slices Efficiently","url":"https://themsaid.com/slice-internals-in-go","date":1740064893,"author":"/u/themsaid","guid":7435,"unread":true,"content":"<p>The deeper you delve into Go‚Äôs internals, the more evident it becomes that its creators carefully engineered the language to strike a precise balance between performance and flexibility. This delicate equilibrium influences many of Go‚Äôs core features, including its approach to memory management and data structures. One standout example of this thoughtful design is the implementation of slice growth. Through this approach, Go ensures that slices expand seamlessly, optimizing both performance and memory usage without compromising ease of use.</p><p>In Go, a slice is a lightweight data structure that serves as a window into a contiguous block of memory where elements of a specific type are stored. At its core, a slice doesn‚Äôt directly contain the data itself but instead holds a pointer to an underlying array (know as the backing array.)</p><p>When the Go runtime creates a slice, as in this example, it constructs a small struct under the hood, defined in the runtime as :</p><div><pre> {\n\t unsafe.\n}</pre></div><p>The  type has the following fields:</p><ul><li> holds a pointer to the underlying array.</li><li> stores the number of elements in the slice.</li><li> stores the capacity of the array.</li></ul><p>The  type used for the  field is a generic pointer type that bypasses Go's type safety rules. Since the size of an array in Go is part of the type, the runtime uses  so it can replace the array with a larger one when needed.</p><p>The code in the example above creates a slice that has zero elements with a backing array that can hold 10 elements of type . We can later fill that array with elements by using the  function:</p><p>Here, we append a byte to the empty array represented by the unsigned 8-bit integer .</p><p>When appending elements to a slice, the Go runtime first checks whether the backing array has enough capacity to accommodate the new elements. If it does, the elements are simply added to the existing array. However, if the current array lacks sufficient space, the runtime allocates a larger backing array, copies the existing elements into it, and then appends the new elements.</p><div><pre>([], , )\n\n(, ) (, ) </pre></div><p>The capacity of the newly allocated array is determined by several factors, including the current array‚Äôs capacity, the type of elements it holds, and the number of new elements being appended. These factors influence how much the array grows, ensuring efficient memory usage while minimizing the need for frequent reallocations.</p><p>The runtime begins by attempting to double the existing capacity as the first step in determining the new array size:</p><div><pre></pre></div><p>If the total number of existing and newly appended elements exceeds the doubled capacity, the runtime sets the new capacity to match the required number of elements:</p><div><pre> {\n    \n}</pre></div><p>This ensures that the new capacity is larger than or equals to the number of elements after the appending operation.</p><div><pre>([], , )\n\n(, , , )\n\n.(()) </pre></div><p>In this example, the integer slice initially had a capacity of 1. After adding three elements, the runtime allocated a new backing array with a capacity of 3. This happened because doubling the original capacity (1 * 2) was insufficient to accommodate the new elements, prompting the runtime to adjust the capacity accordingly.</p><p>If doubling the capacity is sufficient, the runtime further evaluates whether allocating such a large array is efficient or merely a waste of memory.</p><p>For small slices, capacity less than 256, the runtime employs a simple doubling strategy (e.g., 2 to 4, 4 to 8, 8 to 16.) This makes sense for small workloads: doubling ensures plenty of headroom for future appends. However, as the slice‚Äôs capacity climbs into more than 256, or beyond, doubling becomes less practical. Doubling a capacity of, say, 10,000 to 20,000 allocates an extra 10,000 elements‚Äô worth of memory (potentially tens or hundreds of kilobytes, depending on the element size) which might sit unused for a long time.</p><p>To address this, the runtime adjusts its growth strategy for larger slices by reducing the growth factor gradually until it reaches 1.25. This slower growth means that if a slice already has a capacity of, say, 512, adding a few elements doesn‚Äôt balloon it to 1024; it might rise to 832 instead (a 62.5% increase). The key insight is that a larger slice can absorb more appends before hitting its capacity limit. For instance, a slice with a capacity of 512 has room for 512 more elements if empty, compared to just 8 for a capacity of 8. This naturally delays the need for reallocation.</p><p>This conservative approach aims to curb excessive memory usage. By growing incrementally rather than exponentially, the runtime avoids reserving vast swaths of memory that might remain idle, which is critical in applications handling large datasets or with limited resources (e.g., embedded systems). However, there‚Äôs a flip side: smaller growth steps mean the slice fills up sooner, triggering reallocation more often. Each reallocation involves CPU work (allocating memory, copying the existing elements, and updating the slice‚Äôs pointer) which can add up if appends are frequent.</p><p>The runtime‚Äôs strategy thus balances these two forces: memory footprint versus CPU overhead. It leans toward saving memory at the cost of potentially more frequent (but smaller) reallocations, betting that the trade-off pays off in most real-world scenarios where slices don‚Äôt grow indefinitely.</p><p>When determining how a slice should grow, the Go runtime takes into account the type of elements stored in the array, as this directly impacts memory allocation. On 64-bit systems, memory is generally allocated in chunks of 8 bytes. Any allocation that does not align with this rule is rounded up to the nearest multiple of 8 to ensure efficient memory usage and alignment.</p><p>Let's say we create a slice of bytes with capacity zero and then append an element to it:</p><div><pre>([], , )\n\n(, )\n\n.(()) </pre></div><p>After the growth, the capacity of the slice becomes 8 (8 bytes). If the element type was a 64-bit integer instead, the growth will increase the capacity to 1 (1 * 64 bits = 8 bytes):</p><div><pre>([], , )\n\n(, )\n\n.(()) </pre></div><p>If the element type was a 32-bit integer, the growth will increase the capacity to 2 (2 * 32 bits = 8 bytes):</p><div><pre>([], , )\n\n(, )\n\n.(()) </pre></div><p>The reason is that modern CPUs, particularly on 64-bit systems, operate most efficiently when data is aligned to their word size (the amount of data they can process in one cycle.) On a 64-bit system, the word size is 64 bits, or 8 bytes. If the runtime allocates, say, 5 bytes, the CPU  and masks off the unused portion. That means, allocating in 8-byte multiples ensures the entire chunk is usable without waste or extra work.</p><p>In addition, CPU caches fetch memory in 64-byte lines (8 words of 8 bytes each.) Multiples of 8 bytes fit neatly into these lines, reducing cache misses and improving locality when accessing sequential data, like a slice‚Äôs backing array.</p><p>In addition to the 8-byte chunk allocation rule, the Go runtime maintains a table of predefined constants to guide its memory allocation decisions. This table categorizes memory allocations into specific size classes, helping the runtime minimize fragmentation and efficiently reuse freed memory blocks.</p><p>The table looks like this:</p><div><pre>+---------+-------+\n| Class   | Value |\n+---------+-------+\n| Class  |      |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |     |\n| Class  |    |\n| .     | .   |\n+---------+-------+</pre></div><p>This size class allocation table functions as an efficient lookup mechanism for managing memory allocation and deallocation. When a memory block belonging to a specific size class is freed, the runtime stores it in the table rather than immediately returning it to the operating system. Later, if a request is made for a memory block of the same size class, the system can quickly retrieve and reuse the previously freed block instead of performing an extensive search through physical memory to find a suitable allocation.</p><div><pre>+---------------------------------+\n|  Freed memory  size class X  |\n+---------------------------------+\n        ‚îÇ\n        ‚ñº\n+-------------------+  \n| Memory Block     |  &lt;-- Freed  (Stored in Table)\n+-------------------+  \n| Memory Block     |  &lt;-- Freed  (Stored in Table)\n+-------------------+  \n| Memory Block     |  &lt;-- Freed  (Stored in Table)\n+-------------------+  \n        ‚îÇ\n        ‚ñº\n+-------------------------------------+\n| Incoming Memory Allocation Request  |\n+-------------------------------------+\n        ‚îÇ\n          (Lookup in Table)\n+-------------------------------+\n| Matching Freed Block Found    |\n+-------------------------------+\n        ‚îÇ\n          (Reused Instead of )\n+----------------------------+\n|  Allocated to Application  |\n+----------------------------+</pre></div><p>With that in mind, the runtime not only rounds up to the nearest 8-byte boundary but also rounds up to the nearest size class in the allocation table.</p><p>Consider this slice operation:</p><div><pre>([], , )\n\n(, , , , , )</pre></div><p>Here, the slice starts with zero capacity and we add 5 elements of type . Without considering the size class allocation table, the runtime would allocate 40 bytes for the new backing array:</p><div><pre> *  bits =  bits /  =  bytes</pre></div><p>Instead, the runtime consults the class allocation table and rounds up to the nearest match (48 in this case). As a result, it allocates a backing array with a capacity of 6 (48 bytes / 64 bits), even though the new array would only need to hold 5 elements (that require only 40 bytes).</p><p>This approach significantly improves performance by reducing external fragmentation (where free memory is scattered in small, non-contiguous blocks, making larger allocations difficult.) It also minimizes allocation overhead and speeds up memory access by eliminating the need to repeatedly request new memory from the operating system.</p><p>In summary, the Go runtime takes several key factors into account when growing an array:</p><ol><li>: It starts with a doubling factor (2x) and then gradually winds down to 1.25x.</li><li>: The array is rounded up to the nearest 8-byte boundary.</li><li><strong>The Size Class Allocation Table</strong>: The runtime rounds up to the nearest available class in the table.</li></ol><p>I really admire the thoughtful work the Go team has put into making the language both efficient and flexible. It's clear that a lot of careful consideration went into optimizing performance while maintaining flexibility for developers.</p>","contentLength":10276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/golang/comments/1iu0xjk/slice_internals_in_go_how_the_runtime_expands/"},{"title":"Rust 2024 Is Coming: baby steps","url":"https://smallcultfollowing.com/babysteps/blog/2025/02/20/rust-2024-is-coming/?utm_source=atom_feed","date":1740063180,"author":"/u/VorpalWay","guid":7288,"unread":true,"content":"<div><p>So, a little bird told me that Rust 2024 is going to become stable today, along with Rust 1.85.0. In honor of this momentous event, I have penned a little ditty that I‚Äôd like to share with you all. Unfortunately, for those of you who remember Rust 2021‚Äôs <a href=\"https://smallcultfollowing.com/babysteps/blog/2021/05/26/edition-the-song/\">‚ÄúEdition: The song‚Äù</a>, in the 3 years between Rust 2021 and now, my daughter has realized that her father is deeply uncool and so I had to take this one on solo. Anyway, enjoy! Or, you know, suffer. As the case may be.</p><p>In ChordPro format, for those of you who are inspired to play along.</p><pre tabindex=\"0\"><code>{title: Rust 2024}\n{subtitle: }\n\n{key: C}\n\n[Verse 1]\n[C] When I got functions that never return\nI write an exclamation point [G]\nBut use it for an error that could never be\nthe compiler [C] will yell at me\n\n[Verse 2]\n[C] We Rust designers, we want that too\n[C7] But we had to make a [F] change\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\n[Bridge]\n[Am] ... [Am] But will my program [E] build?\n[Am] Yes ... oh that‚Äôs [D7] for sure\n[F] edi-tions [G] are [C] opt in\n\n[Verse 3]\n[C] Usually when I return an `impl Trait`\neverything works out fine [G]\nbut sometimes I need a tick underscore\nand I don‚Äôt really [C] know what that‚Äôs for\n\n[Verse 4]\n[C] We Rust designers we do agree\n[C7] That was con- [F] fusing \n[F] But that will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\n[Bridge 2]\n[Am] Cargo fix will make the changes\nautomatically [G] Oh that sure sounds great...\n[Am] but wait... [Am] my de-pen-denc-[E]-ies\n[Am] Don‚Äôt worry e-[D7]ditions\n[F] inter [G] oper [C] ate\n\n[Verse 5]\n[C] Whenever I match on an ampersand T\nThe borrow [G] propagates\nBut where do I put the ampersand\nwhen I want to [C] copy again?\n\n[Verse 6]\n[C] We Rust designers, we do agree\n[C7] That really had to [F] change\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\n[Outro]\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n\nOne more time!\n\n[Half speed]\n[F] That will be [Fm]better\n[C] Oh so much [A]better\n[D] in Rust Twenty [G7]Twenty [C]Four\n</code></pre></div>","contentLength":2134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1iu09jr/rust_2024_is_coming_baby_steps/"},{"title":"[D] Deepseek 681bn inference costs vs. hyperscale?","url":"https://www.reddit.com/r/MachineLearning/comments/1itys24/d_deepseek_681bn_inference_costs_vs_hyperscale/","date":1740059045,"author":"/u/sgt102","guid":8398,"unread":true,"content":"<p>I've estimated the cost/performance of Deepseek 681bn like this :</p><p>Huggingface open deepseek blog reported config &amp; performance = 32 H100's 800tps </p><p>1million tokens = 1250s = 21 (ish) , minutes. 69.12 million tokens per day </p><p>Cost to rent 32 H100's per month ~$80000</p><p>Cost per million tokens = $37.33 (80000/ 31 days /69.12 ) </p><p>I know that this is very optimistic (100% utilisation, no support etc.) but does the arithmetic make sense and does it pass the sniff test do you think? Or have I got something significantly wrong? </p><p>I guess this is 1000 times more expensive than an API served model like Gemini, and this gap has made me wonder if I am being silly</p>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I still have these","url":"https://www.reddit.com/r/linux/comments/1itynca/i_still_have_these/","date":1740058638,"author":"/u/emuboy85","guid":7235,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chromium Ozone/Wayland: The Last Mile Stretch","url":"https://nickdiego.dev/blog/chromium-ozone-wayland-the-last-mile-stretch/","date":1740058042,"author":"/u/Worldly_Topic","guid":7259,"unread":true,"content":"<p>Hey there! I‚Äôm glad to finally start paying my blogging debt :) as this\nis something I‚Äôve been planning to do for quite some time now. To get the\nball rolling, I‚Äôve shared some bits about me in my very first blog post\n<a href=\"https://nickdiego.dev/blog/ola-mundo/\">Ol√° Mundo</a>.</p><p>In this article, I‚Äôm going to walk through what we‚Äôve been working on\nsince last year in the Chromium Ozone/Wayland project, on which I‚Äôve\nbeen involved (directly or indirectly) since I‚Äôve joined Igalia back in\n2018.</p><p>Lets start with some context, the project consists of implementing,\nshipping and maintaining native <a href=\"https://wayland.freedesktop.org\" target=\"_blank\" rel=\"noreferrer\">Wayland</a> support in the\nChromium project. Our team at <a href=\"https://igalia.com\" target=\"_blank\" rel=\"noreferrer\">Igalia</a> has been leading the\neffort since it was first merged upstream back in 2016. For more\nhistorical context, there are a few <a href=\"https://blogs.igalia.com/msisov/chrome-on-wayland-waylandification-project/\" target=\"_blank\" rel=\"noreferrer\">blog</a><a href=\"https://blogs.igalia.com/adunaev/2021/12/17/ozone-our-way-to-the-big-change/\" target=\"_blank\" rel=\"noreferrer\">posts</a> and this <a href=\"https://www.youtube.com/watch?v=KSxaoXOSxhs\" target=\"_blank\" rel=\"noreferrer\">amazing talk</a>, by my colleagues\nAntonio Gomes and Max Ihlenfeldt, presented at last year‚Äôs <a href=\"https://webengineshackfest.org/\" target=\"_blank\" rel=\"noreferrer\">Web Engines\nHackfest</a>.</p><p>Especially due to the <a href=\"https://chromium.googlesource.com/chromium/src/+/refs/tags/88.0.4324.84/docs/lacros.md\" target=\"_blank\" rel=\"noreferrer\">Lacros project</a>, progresses on Linux\nDesktop has been slower over the last few years. Fortunately, the\nscenario changed since last year, when a new sponsor came up and made it\npossible to address most of the outstanding missing features and issues\nrequired to move Ozone Wayland to the finish line.</p><p>It‚Äôs been a few months since Chromium Wayland backend has started to be\ntested as the main browser backend by Google employees, through a finch\ntrial experiment, as well as internally at Igalia. Feedback collected\nsince then is quite positive in general. The exception is Nvidia setups,\nwhich, depending on the driver version, may face major regressions (see\n<a href=\"https://nickdiego.dev/blog/chromium-ozone-wayland-the-last-mile-stretch/#explicit-sync\">Explicit Sync</a> section below for more details).</p><p>While official roll-out has been under discussion, it‚Äôs still disabled\nby default on Linux Desktop. Early adopters willing to test it are\nencouraged to explicitly opt-in by flipping the \nchrome flag to  or . Issue reports are welcome at\n<a href=\"https://crbug.com/new?component=1456988&amp;template=0\" target=\"_blank\" rel=\"noreferrer\">crbug.com/new</a>.</p><p>There are also a few other Wayland-specific flags which might be\nselectively enabled, if you feel brave enough :) such as, ui scaling,\ntext input v3, etc; all described in more details below.</p><figure><figcaption>Chrome Wayland flags available in M135.</figcaption></figure><p>Initial fractional scaling support for Linux Desktop was originally\nimplemented by an external contributor <a href=\"https://chromium-review.googlesource.com/c/chromium/src/+/4370091\" target=\"_blank\" rel=\"noreferrer\">back in\n2023</a>.\nAfter some months of stabilization, reports of <a href=\"https://issues.chromium.org/336007385\" target=\"_blank\" rel=\"noreferrer\">blurriness and some\nother subtle issues</a> started to\npop up, which were listed as top-priority when this new project phase\nkicked off back in 2024 June.</p><p>After some analysis, we could confirm that there were some fundamental\nmissing bits in the process. Which was the actual usage of the fractional\nscale values provided by the Wayland compositor, via\nthe <a href=\"https://wayland.app/protocols/fractional-scale-v1\" target=\"_blank\" rel=\"noreferrer\">fractional-scale-v1</a>\nprotocol extension. Rather than using it, fractional scales were being\n using <a href=\"https://wayland.app/protocols/xdg-output-unstable-v1\" target=\"_blank\" rel=\"noreferrer\">xdg-output</a>\nprotocol instead, which is unsupported (for such usage) and prone to\nprecision issues.</p><p>The screenshot above shows a sharp Chrome window scaled by a 1.25\nfactor, running on Gnome Shell 47.</p><p>The work involved a considerable architectural refactoring to make it\npossible to support the per-window scaling design of the Wayland\nprotocol, without breaking the standard per-display scaling implemented\nin Chromium. The feature started shipping experimentally in Milestone\n128, behind the  chrome flag and disabled by\ndefault until M135. I plan to cover it in more details soon in separate\nblog post.</p><p>IME has been yet another major pain point for some Wayland users. Back\nin last June, a careful study was conducted by my colleague <a href=\"https://garai.ca\" target=\"_blank\" rel=\"noreferrer\">Orko\nGarai</a> in order to understand and consolidate the\npossible approaches to tackle it, as well as pros, cons and potential\nroad-blockers for each of them. Besides the detailed outcomes of that\nstudy, publicly available in the form of a <a href=\"https://docs.google.com/document/d/1GkOphcAQBMdW4iPiMOd9eKd70tlXWQaR7M3GJXGUDpQ\" target=\"_blank\" rel=\"noreferrer\">design\ndocument</a>,\nOrko has recently started a blog post series about the topic\n<a href=\"https://garai.ca/what_is_ime\" target=\"_blank\" rel=\"noreferrer\">here</a>.</p><p>Experimental support for\n<a href=\"https://wayland.app/protocols/text-input-unstable-v3\" target=\"_blank\" rel=\"noreferrer\">text-input-v3</a>\nprotocol was implemented on the Chromium side, with ongoing work on the\nWayland community side to fullfill browser use cases from a protocol\nperspective, which is expected to come soon as part of <a href=\"https://gitlab.freedesktop.org/wayland/wayland-protocols/-/merge_requests/282\" target=\"_blank\" rel=\"noreferrer\">version\n3.2</a>\nof the text-input protocol.</p><p>In the meantime, we keep working with the Chromium community on\nimprovements to the client-side implementation of the protocol, although\nprogress has been slow as we are still looking for some key browser\nrequirements to be solved on the protocol side to have the confidence\nand buy-in for productization.</p><p>Supporting the full original user experience for Chrome‚Äôs tab dragging\nunder Wayland has proved to be complex, especially because the core\nWayland protocol does not cover all of its requirements. Back in 2021,\nwe designed a brand new protocol and implemented it in ChromeOS‚Äô Exo\nWayland compositor, in the context of Lacros project, to fullfill those\ngaps.</p><p>Years later,\n<a href=\"https://wayland.app/protocols/xdg-toplevel-drag-v1\" target=\"_blank\" rel=\"noreferrer\">xdg-toplevel-drag</a>\nhas emerged as a community effort to standardize it upstream, thanks\nDavid Redondo and Robert Mader for working on it. It consists of a\ntrimmed down version of the original  protocol with\nsome tweaks to make it more aligned with Wayland design principles. A\nfew months ago, initial support for it has landed in Chromium and we‚Äôve\nbeen stabilizing it since then.</p><h4>Full UX support in Mutter<a href=\"https://nickdiego.dev/blog/chromium-ozone-wayland-the-last-mile-stretch/#full-ux-support-in-mutter\" aria-hidden=\"true\">#</a></h4><p>Back in last November, xdg-toplevel-drag was\n<a href=\"https://wayland.app/protocols/xdg-toplevel-drag-v1#compositor-support\" target=\"_blank\" rel=\"noreferrer\">supported</a>\nonly in KWin and Jay compositors, when a demand to implement support for\nit in Mutter was raised by our customer, and the task was assigned to\nme.</p><p>Long story short, it was a pretty interesting and challenging experience\nwhich I‚Äôm glad to have had. As a curiosity, last time I had coded in C\nand glib had been , maybe &gt;13 years? üò± Also, it was my\nfirst time hacking on Mutter and Gnome code base. After all, the\n<a href=\"https://gitlab.gnome.org/GNOME/mutter/-/merge_requests/4107\" target=\"_blank\" rel=\"noreferrer\">MR</a> did\nlanded and will start shipping as part of Gnome 48, next month.</p><figure><figcaption>xdg-toplevel-drag-v1 demo on Gnome Shell 48.</figcaption></figure><p>I‚Äôm preparing a blog post to share more technical details about the\nprotocol implementation from the perspective of a browser developer and\nGnome/Mutter newcomer.</p><p>Let me take the opportunity to say thanks to the Gnome developers who\nhelped me a lot in the process: Jonas √Ödahl, Carlos Garnacho, Georges\nStavracas and Sebastian Wick. Really appreciate your help and patience,\nguys!</p><p>In parallel to the work on the regular tab drag experience through\nxdg-toplevel-drag, a fallback implementation relying solely on core\nWayland drag-and-drop protocol has been led by my colleague Max\nIhlenfeldt. A few days ago, Max has published an <a href=\"https://blogs.igalia.com/max/fallback-tab-dragging/\" target=\"_blank\" rel=\"noreferrer\">awesome in-depth blog\npost</a> about his\nwork on it. Enjoy the read!</p><figure><figcaption>Fallback tab dragging UX demo</figcaption></figure><p>The main difference to the regular UX is that, rather than instantly\ncreating a browser window when it gets dragged out of its tab strip, a\ndrag icon containing the tab thumbnail is used instead. Browser window\ncreation is then deferred to when the drop happens, as can be seen in\nthe video above. The feature was recently enabled by default, and\nstarted shipping in Chrome 133.</p><p>Linux desktop environments usually support system-wide ‚Äútext scaling‚Äù\nsettings, which are supposed to be handled by applications. On\nGnome-based environments, it can be triggered via several ways,\nsuch as the ‚ÄúLarge Text‚Äù accessibility feature.</p><p>Historically, it has been supported in Chromium X11 by resizing the\nwhole browser UI elements, instead of just text items. After an in-depth\nanalysis, it was decided to follow the very same approach for the\nWayland initial implementation. The main motivation was that there seems\nto be gaps in both Chromium‚Äôs internal UI framework as well as in\nChrome‚Äôs UI/layout code, which would need to be fixed before supporting\nsuch text-only live resizing/relayout.</p><figure><figcaption>Quick demo of text scale in action on Chromium Wayland on Gnome 47.</figcaption></figure><p>Technically, the solution involved implementing an additional scaling\nlayer in Ozone/Wayland, so called ‚Äúui scale‚Äù which make the browser UI\nto get fully resized/re-laid out instantly in reaction to system‚Äôs ‚Äútext\nscaling factor‚Äù updates.</p><p>The feature has started shipping in Milestone 131, disabled by default\nas usual. Users can enable it by using the <code>chrome://flags/#wayland-ui-scaling</code>\nflag.</p><p>To address some <a href=\"https://issues.chromium.org/issues/377438303\" target=\"_blank\" rel=\"noreferrer\">display tearing reports</a>,\nsupport for the <a href=\"https://wayland.app/protocols/linux-drm-syncobj-v1\" target=\"_blank\" rel=\"noreferrer\">linux-drm-syncobj-v1</a>\nprotocol has been implemented. The patch series has landed and started\nshipping in version 132.0.6834.83, and can be enabled using the\n<code>wayland-linux-drm-syncobj</code> chrome flag.</p><p>Feedback has been positive so far. If you‚Äôre willing to give it a try,\nplease bear in mind that Linux kernel version &gt;= 6.11 is required, and\ndon‚Äôt hesitate to get back to us with your remarks.</p><p>Besides overall stabilization and maintenance, there is a large ongoing\neffort led by my colleague Orko to get Chromium‚Äôs interactive UI tests\ninfrastructure and code working with major Wayland compositors,\nprimary focus is Mutter/Gnome, though wlroots is also considered\nfor the future.</p><p>Another area we are currently investigating is ‚Äúsession management‚Äù,\nwhich will make it possible to restore browser window attributes, such\nas, position, display and workspace across restarts.</p><p>Aside from that, there are a bunch of technical debt and follow-up\nissues which spun off from some of the features and fixes listed above,\nsuch as:</p><p>New sponsors and partners are always welcome, so please don‚Äôt hesitate\nto mail us to discuss how we coud be of help.</p><p>Yay! Quite busy and exciting times!! I‚Äôd like to thank once more all of\nour supporters, sponsors and, of course, <a href=\"https://igalia.com\" target=\"_blank\" rel=\"noreferrer\">Igalia</a> for making all\nthis possible ‚ù§Ô∏è Looking forward to the challenges ahead! Stay tuned for\nmore updates and don‚Äôt hesitate to reach out if you have questions or\nother remarks. üëãüëã</p>","contentLength":9484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1itygay/chromium_ozonewayland_the_last_mile_stretch/"},{"title":"The Fedora Project Leader is willfully ignorant about Flathub","url":"https://blogs.gnome.org/alatiera/2025/02/19/the-fedora-project-leader-is-willfully-ignorant-about-flathub/","date":1740057149,"author":"/u/Worldly_Topic","guid":7234,"unread":true,"content":"<p>Today I woke up to a link of an <a href=\"https://www.youtube.com/watch?v=oKP1hgdFJKo\">interview</a> from the current Fedora Project Leader, Matthew Miller. Brodie who conducted the interview mentioned that Miller was the one that reached out to him. The background of this video was the currently ongoing issue regarding OBS, Bottles and the Fedora project, which Niccol√≤ made an <a href=\"https://youtu.be/o2qd2RFC6Fk\">excellent video</a> explaining and summarizing the situation. You can also find the article over at <a href=\"https://thelibre.news/why-obs-and-bottles-got-in-a-fight-with-fedora/\">thelibre.news</a>. ‚ÄúImpressive‚Äù as this story is, it‚Äôs for another time.</p><p>What I want to talk in this post, is the outrageous, smearing and straight up slanderous statements about Flathub that the Fedora Project Leader made during the interview..</p><p>I am not directly involved with the Flathub project (A lot of my friends are), however I am a maintainer of the GNOME Flatpak Runtime, and a contributor to the Freedesktop-sdk and ElementaryOS Runtimes. I also maintain applications that get published on Flathub directly. So you can say I am someone invested in the project and that has put a lot of time into it. It was extremely frustrating to hear what would only qualify as reddit-level completely made up arguments with no base in reality coming directly from Matthew Miller.</p><p>Below is a transcript, slightly edited for brevity, of all the times Flathub and Flatpak was mentioned. You can refer to the <a href=\"https://www.youtube.com/watch?v=oKP1hgdFJKo\">original video</a> as well as there were many more interesting things Miller talked about.</p><p>It starts off with an introduction and some history and around the 10-minute mark, the conversation starts to involve Flathub.</p><blockquote><p>Miller: [..] long way of saying I think for something like OBS we‚Äôre not really providing anything by packaging that. Miller: I think there is an overall place for the Fedora Flatpaks, because Flathub part of the reason its so popular (there‚Äôs a double edged sword), (its) because the rules are fairly lax about what can go into Flathub and the idea is we want to make it as easy for developers to get their things to users, but there is not really much of a review</p></blockquote><p>This is not the main reason why Flathub is popular, its a lot more involved and interesting in practice. I will go into this in a separate post hopefully soon.</p><p>Claiming that Flathub does not have any review process or inclusion policies is straight up wrong and incredibly damaging. It‚Äôs the kind of thing we‚Äôve heard ad nauseam from Flathub haters, but never from a person in charge of one of the most popular distributions and that should have <strong>really really known better</strong>.</p><p>You can find the <a href=\"https://docs.flathub.org/docs/for-app-authors/requirements\">Requirements</a> in the Flathub documentation if you spend 30 seconds to google for them, along with the submission <a href=\"//docs.flathub.org/docs/for-app-authors/submission/\">guidelines</a> for developers. If those documents qualify as a wild west and free for all, I can‚Äôt possibly take you seriously.</p><p>I haven‚Äôt maintained a linux distribution package myself so I won‚Äôt go to comparisons between Flathub and other distros, however you can find people, with red hats even, <a href=\"https://social.vivaldi.net/@sesivany/114030210735848325\">that do so and talked about it</a>. Of course this is one off examples and social bias from my part. But it proves how laughable of a claim is that things are not reviewed. Additionally, the most popular story I hear from developers is how Flathub requirements are often stricter and sometimes cause annoyances.</p><p>Additionally, Flathub has been the driving force behind encouraging applications to update their metadata, completely reworking the User Experience and handling off permissions and made them prominent to the user. (To the point where even network access is marked as potentially-unsafe).</p><blockquote><p>Miller: [..] the thing that says verified just says that it‚Äôs verified from the developer themselves.</p></blockquote><p>No, verified does not mean that the developer signed off into it. Let‚Äôs take another 30 seconds to look into the Flathub <a href=\"https://docs.flathub.org/docs/for-users/verification\">documentation page</a> about exactly this.</p><blockquote><p>A verified app on Flathub is one whose developer has confirmed their ownership of the app ID [‚Ä¶]. This usually also may mean that either the app is maintained directly by the developer or a party authorized or approved by them.</p></blockquote><p>It still went through the review process and all the rest of requirements and policies apply. The verified program is basically a badge to tell users this is a supported application by the upstream developers, rather than the free for all that exists currently where you may or may not get an application released from years ago depending on how stable your distribution is.</p><p>Sidenote, did you know that 1483/3003 applications on Flathub are verified as of the writing of this post? As opposed to maybe a dozen of them at best in the distributions. You <a href=\"https://gitlab.gnome.org/-/snippets/6791\">can check</a> for yourself</p><blockquote><p>Miller: .. and it doesn‚Äôt necessarily verify that it was build with good practices, maybe it was built in a coffee shop on some laptop or whatever which could be infected with malware or whatever could happen</p></blockquote><p>Again if Miller had done the bare minimum effort, he would have come across the <a href=\"https://docs.flathub.org/docs/for-app-authors/requirements\">Requirements</a> page which describes exactly how an Application in Flathub is built, instead of further spreading made up takes about the infrastructure. I can‚Äôt stress enough how damaging it has been&nbsp;throughout the years to claim that ‚ÄúFlathub may be potential Malware‚Äù. Why it‚Äôs malware? Because I don‚Äôt like its vibes and I just assume so..</p><p>I am sure If I did the same about Fedora in a very very public medium with thousand of listeners I would probably end up with a Layers letter from Redhat.</p><p>Now Applications in Flathub are all built without a network access, in Flathub‚Äôs build servers, using flatpak-builder and Flatpak Manifests which are a declarative format, which means all the sources required to build the application are known, validated/checksumed, the build is reproducible to the extend possible, you can easily inspect the resulting binaries and the manifest itself used to build the application ends up in  which you can also inspect with the following command and use it to rebuild the application yourself exactly like how it‚Äôs done in Flathub.</p><div><pre><code></code></pre></div><p>The exception to this, are proprietary applications naturally, and a handful of applications (under an OSI approved license) where Flathub developers helped the upstream projects integrate a direct publishing workflow into their Deployment pipelines. I am aware of Firefox and OBS as the main examples, both of which publish in Flathub through their Continues Deployment (CI/CD) pipeline the same way they generate their builds for other platforms they support and the code for how it happens is available on their repos.</p><p>If you have issues trusting Mozilla‚Äôs infrastructure, then how are you trusting Firefox in the first place and good luck auditing gecko to make sure it does not start to ship malware. Surely distribution packagers audit every single change that happens from release to release for each package they maintain and can verify no malicious code ever gets merged. The <a href=\"https://lwn.net/Articles/967180/\">xz backdoor</a> was very recent, and it was identified by pure chance, none of this prevented it.</p><p>Then Miller proceeds to describe the Fedora build infrastructure and afterward we get into the following:</p><blockquote><p>Miller: I will give an example of something I installed in Flathub, I was trying to get some nice gui thing that would show me like my system Hardware stats [‚Ä¶] one of them ones I picked seemed to do nothing, and turns out what it was actually doing, there was no graphical application it was just a script, it was running that script in the background and that script uploaded my system stats to a server somewhere.</p></blockquote><p>Firstly we don‚Äôt really have many details to be able to identify which application it was, I would be very curious to know. Now speculating on my part, the most popular application matching that description it‚Äôs Hardware Probe and it absolutely has a GUI, no matter how minimal. It also asks you before uploading.</p><p>Maybe there is a org.upload.MySystem application that I don‚Äôt know about, and it ended up doing what was in the description, again I would love to know more and update the post if you could recall!</p><blockquote><p>Miller: No one is checking for things like that and there‚Äôs no necessarily even agreement that that was was bad.</p></blockquote><p>Second time! Again with the ‚ÄúThere is no review and inclusion process in Flathub‚Äù narrative. There absolutely is, and these are the kinds of things that get brought up during it.</p><blockquote><p>Miller: I am not trying to be down on Flathub because I think it is a great resource</p></blockquote><p>Yes, I can see that, however in your ignorance you were something much worse than ‚ÄúDown‚Äù. This is pure slander and defamation, coming from the current ‚ÄúFedora Project Leader‚Äù, the ‚ÄúTechnically Voice of Fedora‚Äù (direct quote from a couple seconds later). All the statements made above are manufactured and inaccurate. Myths that you‚Äôd hear from people that never asked, looked or cared about any of these cause the moment you do you its obvious how laughable all these claims are.</p><blockquote><p>Miller: And in a lot of ways Flathub is a competing distribution to Fedora‚Äôs packaging of all applications.</p></blockquote><p>Precisely, he is spot on here, and I believe this is what kept Miller willfully ignorant and caused him to happily pick the first anit-flatpak/anti-flathub arguments he came across on reddit and repeat the verbatim without putting any thought into it. I do not believe Miller is malicious on purpose, I do truly believe he means well and does not know better.</p><p>However, we can‚Äôt ignore the conflict that arises from his current job position as an big influence to why incidents like this happened. Nor the influence and damage this causes when it comes from a person of Matthew Miller‚Äôs position.</p><blockquote><p>Miller: One of the other things I wanted to talk about Flatpak, is the security and sandboxing around it. Miller: Like I said the stuff in the Flathub are not really reviewed in detail and it can do a lot of things:</p></blockquote><p>Third time with the no review theme. I was fuming when I first heard this, and I am very very angry about still, If you can‚Äôt tell. Not only is this an incredibly damaging lie as covered above, it gets repeated over and over again.</p><blockquote><p>With Flatpak basically the developer defines what the permissions are. So there is a sandbox, but the sandbox is what the person who put it there is, and one can imagine that if you were to put malware in there you might make your sandboxing pretty loose.</p></blockquote><blockquote><p>Brodie: One of the things you can say is ‚ÄúI want full file system access, and then you can do anything‚Äù</p></blockquote><p>No, again it‚Äôs stated in the Flathub documentation, permissions are very carefully reviewed and updates get blocked when permissions change until another review has happened.</p><blockquote><p>Miller: Android and Apple have pretty strong leverage against application developers to make applications work in their sandbox</p><p>Brodie: the model is the other way around where they request permissions and then the user grants them whereas Flatpak, they get the permission and then you could reject them later</p></blockquote><p>This is partially correct, the first part about leverage will talk about in a bit, but here‚Äôs a primer on how permissions work in Flatpak and how it compares to the sandboxing technologies in iOS and Android.</p><p>In all of them we have a separation between Static and Dynamic permissions. Static are the ones the application always has access to, for example the network, or the ability to send you notifications. These are always there and are mentioned at install time usually. Dynamic permissions are the ones where the application has to ask the user before being able to access a resource. For example opening a file chooser dialog so the user can upload a file, the application the only gets access to the file the user consented or none. Another example is using the camera on the device and capturing photos/video from it.</p><p>Brodie here gets a bit confused and only mentions static permissions. If I had to guess it would be cause we usually refer to the dynamic permissions system in the Flatpak world as ‚ÄúPortals‚Äù.</p><blockquote><p>Miller: it didn‚Äôt used to be that way and and in fact um Android had much weaker sandboxing like you could know read the whole file system from one app and things like that [‚Ä¶] they slowly tightened it and then app developers had to adjust Miller: I think with the Linux ecosystem we don‚Äôt really have the way to tighten that kind of thing on app developers ‚Ä¶ Flatpak actually has that kind of functionality [‚Ä¶] with portals [‚Ä¶] but there‚Äôs no not really a strong incentive for developers to do that because, you know well, first of all of course my software is not going to be bad so why should I you know work on sandboxing it, it‚Äôs kind of extra work and I I don‚Äôt know I don‚Äôt know how to solve that. I would like to get to the utopian world where we have that same security for applications and it would be nice to be able to install things from completely untrusted places and know that they can‚Äôt do anything to harm your system and that‚Äôs not the case with it right now</p></blockquote><p>As with any technology and adoption, we don‚Äôt get to perfection from day 1. Static permissions are necessary to provide a migration path for existing applications and until you have developed the appropriate and much more complex dynamic permissions mechanisms that are needed. For example up until iOS 18 it wasn‚Äôt possible to give applications access to a subset of your contacts list. Think of it like having to give access your entire filesystem instead of the specific files you want. Similarly partial-only access to your photos library arrived couple years ago in IOS and Android.</p><p>In an ideal world all permissions are dynamic, but this takes time and resources and adaptation for the needs of applications and the platform as development progresses.</p><p>Now about the leverage part.</p><p>I do agree that ‚Äúthe Linux ecosystem‚Äù as a whole does not have any leverage on applications developers. This is cause Miller is looking at the wrong place for it. <a href=\"https://blogs.gnome.org/tbernard/2019/12/04/there-is-no-linux-platform-1/\">There is no Linux ecosystem</a> but rather Platforms developers target.</p><p>GNOME and KDE, as they distribute all their applications on Flathub absolutely have leverage. Similarly Flathub itself has leverage by changing the publishing requirements and inclusion guidelines. Which I kept being told they don‚Äôt exist.. Every other application that wants to publish also has to adhere by the rules on Flathub. ElementaryOS and their Appcenter has leverage on developers. Canonical does have the same pull as well with the Snapstore. Fedora on the other hand doesn‚Äôt have any leverage cause the Fedora Flatpak repository is <a href=\"https://thelibre.news/why-obs-and-bottles-got-in-a-fight-with-fedora/\">irrelevant, broken and nobody wants to use it</a>.</p><p>[..] The <a href=\"https://lwn.net/Articles/967180/\">xz backdoor</a> gets brought up when discussing dependencies and how software gets composed together.</p><blockquote><p>Miller: we try to keep all of those things up to date and make sure everything is patched across the dist even when it‚Äôs even when it‚Äôs difficult. I think that really is one of the best ways to keep your system secure and because the sandboxing isn‚Äôt very strong that can really be a problem, you know like the XZ thing that happened before. If XZ is just one place it‚Äôs not that hard of an update but if you‚Äôve got a 100 Flatpaks from different places [‚Ä¶] and no consistency to it it‚Äôs pretty hard to manage that</p></blockquote><p>I am not going to get in depth about this problem domain and the arguments over it. In fact I have been writing another blog post for a while. I hope to publish shortly. Till then I can not recommend high enough <a href=\"https://www.bassi.io/articles/2017/08/10/dev-v-ops/\">Emmanuele‚Äôs</a> and <a href=\"https://0pointer.net/blog/revisiting-how-we-put-together-linux-systems.html\">Lennart‚Äôs</a> blog posts, as well as one of the very <a href=\"https://blogs.gnome.org/alexl/2011/09/30/rethinking-the-linux-distibution/\">early posts</a> from Alex when Flatpak was in early design phase on the shortcomings of the current distribution model.</p><p>Now about bundled dependencies. The concept of Runtimes has served us well so far, and we have been doing a pretty decent job providing most of the things applications need but would not want to bundle themselves. This makes the Runtimes a single place for most of the high profile dependencies (curl, openssl, webkitgtk and so on) that you‚Äôd frequently update for security vulnerabilities and once it‚Äôs done they roll out to everyone without needing to do anything manual to update the applications or even rebuilt them.</p><p>Applications only need to bundle their direct dependencies,and as mentioned above, the flatpak manifest includes the exact definition of all of them. They are available to anyone to inspect and there‚Äôs tooling that can scan them and hopefully in the future alert us.</p><p>If the Docker/OCI model where you end bundling the entire toolchain, runtime, and now you have to maintain it and keep up with updates and rebuild your containers is good enough for all those enterprise distributions, then the Flatpak model which is much more efficient, streamlined and thought out and much much much less maintenance intensive, it is probably fine.</p><blockquote><p>Miller: part of the idea of having a distro was to keep all those things consistent so that it‚Äôs easier for everyone, including the developers</p></blockquote><p>As mentioned above, nothing that fundamentally differs from the leverage that Flathub and the Platform Developers have.</p><blockquote><p>Brodie: took us 20 minutes to get to an explanation [..] but the tldr Fedora Flatpak is basically it is built off of the Fedora RPM build system and because that it is more well tested and sort of intended, even if not entirely for the Enterprise, designed in a way as if an Enterprise user was going to use it the idea is this is more well tested and more secure in a lot of cases not every case.\nMiller: Yea that‚Äôs basically it</p></blockquote><p>This is a question/conclusion that Brodie reaches with after the previous statements and by far the most enraging thing in this interview. This is also an excellent example of the damage Matthew Miller caused today and if I was a Flathub developer I would stop on nothing sort of a public apology from the Fedora project itself. Hell I want this just being an application developer that publishes on it. The interview has been basically shitting on both the Developers of Flathub  the people that choose to publish in it. And if that‚Äôs not enough there should be an apology just out of decency. Dear god..</p><blockquote><p>Brodie: how should Fedora handle upstreams that don‚Äôt want to be packaged&nbsp; like the OBS case here where they did not want there to be a package in Fedora Flatpak or another example is obviously bottles which has made a lot of noise about the packaging</p></blockquote><p>Lastly I want to touch on this closing question in light of recent events.</p><blockquote><p>Miller: I think we probably shouldn‚Äôt do it. We should respect people‚Äôs wishes there. At least when it is an open source project working in good faith there. There maybe some other cases where the software, say theoretically there‚Äôs somebody who has commercial interests in some thing and they only want to release it from their thing even though it‚Äôs open source. We might want to actually like, well it‚Äôs open source we can provide things, we in that case we might end up you having a different name or something but yeah I can imagine situations where it makes sense to have it packaged in Fedora still but in general especially and when it‚Äôs a you know friendly successful open source project we should be friendly yeah. The name thing is something people forget history like that‚Äôs happened before with Mozilla with Firefox and Debian.</p></blockquote><p>This is an excellent idea! But it gets better:</p><blockquote><p>Miller: so I understand why they strict about that but it was kind of frustrating um you know we in Fedora have basically the same rules if you want to take Fedora Linux and do something out of it, make your own thing out of it, put your own software on whatever, you can do that but we ask you not to call it Fedora if it‚Äôs a fedora remix brand you can use in some cases otherwise pick your own name it‚Äôs all open source but you know the name is ours. yeah and I the Upstream as well it make totally makes sense.</p><p>Brodie: yeah no the name is completely understandable especially if you do have a trademark to already even if you don‚Äôt like it‚Äôs it‚Äôs common courtesy to not name the thing the exact same thing</p><p>Miller: yeah I mean and depending on the legalities like you don‚Äôt necessarily have to register a trademark to have the trademark kind of protections under things so hopefully lawyers you can stay out of the whole thing because that always makes the situations a lot more complicated, and we can just get along talking like human beings who care about making good software and getting it to users.</p></blockquote><p>And I completely agree with all of these, all of it. But let‚Äôs break it down a bit because no matter how nice the words and intentions it hasn‚Äôt been working out this way with the Fedora community so far.</p><p>First, Miller agrees the Fedora project should be respecting of application developer‚Äôs wishes to not have their application distributed by fedora but rather it be a renamed version if Fedora wishes to keep distributing it.</p><p>However, every single time a developer has asked for this, they have been ridiculed, laughed at and straight up bullied by Fedora packagers and the rest of the Fedora community. It has been a similar response from other distribution projects and companies as well, it‚Äôs not just Fedora. You can look at <a href=\"https://thelibre.news/why-obs-and-bottles-got-in-a-fight-with-fedora/\">Bottle‚Äôs story</a> for the most recent example. It is very nice to hear Miller‚Äôs intentions but means nothing in practice.</p><p>Then Miller proceeds to assure us why he understand that naming and branding is such a big deal to those projects (unlike the rest of the Fedora community again). He further informs us how Fedora has the exact same policies and asks from people that want to fork Fedora. Which makes the treatment that every single application developer has received when asking about the same exact thing ever more outrageous.</p><p>What I didn‚Äôt know is that in certain cases you don‚Äôt even need to have a trademark yet to be covered by some of the protections, depending on jurisdiction and all.</p><p>And last we come into lawyers. Neither Fedora nor application developers would want it to ever come to this, and it was stated multiple times by Bottles developers that they don‚Äôt want to have to file for a trademark so they can be taken seriously. Similarly, OBS developers said how resorting to legal action would be the last thing they would want to do and would rather have the issue resolved before that. But it took until OBS, a project of a high enough profile, with the resources required to acquire a trademark and to threaten legal action before the Fedora Leadership cared to treat application developers like human beings and get the Fedora packagers and community members to comply. (Something which they had stated multiple times they simply couldn‚Äôt do).</p><p>I hate all of this. Fedora and all the other distributions need to do better. They all claim to care about their users but happily keep shipping broken and miss configured software to them over the upstream version, just cause it‚Äôs what aligns with their current interests. In this case is the promotion of Fedora tooling and Fedora Flatpaks over the application in Flathub they have no control over. In previous incidents it was about <a href=\"https://pagure.io/fedora-workstation/issue/351\">branding applications</a> like the rest of the system even though it was making them unusable. And I can find you and list you with a bunch of examples from other distributions just as easily.</p><p>They don‚Äôt care about their users, they care about their bottom line first and foremost. Any civil attempts at fixing issues get ignored and laughed at, up until there is a threat of a legal action or a big enough PR damage, drama and shitshow that they can‚Äôt ignore it anymore and have to backtrack on them.</p><p>This is my two angry cents. Overall I am not exactly sure how Matthew Miller managed in a rushed and desperate attempt at damage control for the OBS drama, to not only to make it worse, but to piss off the entire Flathub community at the same time. But what‚Äôs done is done, let‚Äôs see what we can do to address the issues that have festered and persisted for years now.</p>","contentLength":23816,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/linux/comments/1ity5zp/the_fedora_project_leader_is_willfully_ignorant/"}],"tags":["reddit"]}