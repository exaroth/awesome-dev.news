{"id":"KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw","title":"GitHub All Languages Daily Trending","displayTitle":"Github Trending","url":"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml","feedLink":"http://mshibanami.github.io/GitHubTrendingRSS","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":40,"items":[{"title":"GetStream/Vision-Agents","url":"https://github.com/GetStream/Vision-Agents","date":1769655493,"author":"","guid":425672,"unread":true,"content":"<p>Open Vision Agents by Stream. Build Vision Agents quickly with any model or video provider. Uses Stream's edge network for ultra-low latency.</p><img width=\"1280\" height=\"360\" alt=\"Readme\" src=\"https://raw.githubusercontent.com/GetStream/Vision-Agents/main/assets/repo_image.png\"><h2>Build Real-Time Vision AI Agents</h2><h3>Multi-modal AI agents that watch, listen, and understand video.</h3><p>Vision Agents give you the building blocks to create intelligent, low-latency video experiences powered by your models, your infrastructure, and your use cases.</p><ul><li> Built for real-time video AI. Combine YOLO, Roboflow, and others with Gemini/OpenAI in real-time.</li><li> Built by Stream, but works with any video edge network.</li><li> Native SDK methods from OpenAI (), Gemini (), and Claude ( ) â€” always access the latest LLM capabilities.</li><li> SDKs for React, Android, iOS, Flutter, React Native, and Unity, powered by Stream's ultra-low-latency network.</li></ul><p>This example shows you how to build golf coaching AI with YOLO and Gemini Live. Combining a fast object detection model (like YOLO) with a full realtime AI is useful for many different video AI use cases. For example: Drone fire detection, sports/video game coaching, physical therapy, workout coaching, just dance style games etc.</p><pre><code># partial example, full example: examples/02_golf_coach_example/golf_coach_example.py\nagent = Agent(\n    edge=getstream.Edge(),\n    agent_user=agent_user,\n    instructions=\"Read @golf_coach.md\",\n    llm=gemini.Realtime(fps=10),\n    # llm=openai.Realtime(fps=1), # Careful with FPS can get expensive\n    processors=[ultralytics.YOLOPoseProcessor(model_path=\"yolo11n-pose.pt\", device=\"cuda\")],\n)\n</code></pre><h3>Security Camera with Package Theft Detection</h3><p>This example shows a security camera system that detects faces, tracks packages and detects when a package is stolen. It automatically generates \"WANTED\" posters, posting them to X in real-time.</p><p>It combines face recognition, YOLOv11 object detection, Nano Banana and Gemini for a complete security workflow with voice interaction.</p><pre><code># partial example, full example: examples/04_security_camera_example/security_camera_example.py\nsecurity_processor = SecurityCameraProcessor(\n    fps=5,\n    model_path=\"weights_custom.pt\",  # YOLOv11 for package detection\n    package_conf_threshold=0.7,\n)\n\nagent = Agent(\n    edge=getstream.Edge(),\n    agent_user=User(name=\"Security AI\", id=\"agent\"),\n    instructions=\"Read @instructions.md\",\n    processors=[security_processor],\n    llm=gemini.LLM(\"gemini-2.5-flash-lite\"),\n    tts=elevenlabs.TTS(),\n    stt=deepgram.STT(),\n)\n</code></pre><h3>Cluely style Invisible Assistant (coming soon)</h3><p>Apps like Cluely offer realtime coaching via an invisible overlay. This example shows you how you can build your own invisible assistant. It combines Gemini realtime (to watch your screen and audio), and doesn't broadcast audio (only text). This approach is quite versatile and can be used for: Sales coaching, job interview cheating, physical world/ on the job coaching with glasses</p><pre><code>agent = Agent(\n    edge=StreamEdge(),  # low latency edge. clients for React, iOS, Android, RN, Flutter etc.\n    agent_user=agent_user,  # the user object for the agent (name, image etc)\n    instructions=\"You are silently helping the user pass this interview. See @interview_coach.md\",\n    # gemini realtime, no need to set tts, or sst (though that's also supported)\n    llm=gemini.Realtime()\n)\n</code></pre><p><strong>Step 2: (Optional) Install with extra integrations</strong></p><p><code>uv add \"vision-agents[getstream, openai, elevenlabs, deepgram]\"</code></p><p><strong>Step 3: Obtain your Stream API credentials</strong></p><p>Get a free API key from <a href=\"https://getstream.io/\">Stream</a>. Developers receive <strong>333,000 participant minutes</strong> per month, plus extra credits via the Maker Program.</p><table><tbody><tr><td><strong>True real-time via WebRTC</strong></td><td>Stream directly to model providers that support it for instant visual understanding.</td></tr><tr><td><strong>Interval/processor pipeline</strong></td><td>For providers without WebRTC, process frames with pluggable video processors (e.g., YOLO, Roboflow, or custom PyTorch/ONNX) before/after model calls.</td></tr><tr><td><strong>Turn detection &amp; diarization</strong></td><td>Keep conversations natural; know when the agent should speak or stay quiet and who's talking.</td></tr><tr><td><strong>Voice activity detection (VAD)</strong></td><td>Trigger actions intelligently and use resources efficiently.</td></tr><tr><td>Enable low-latency loops for smooth, conversational voice UX.</td></tr><tr><td>Execute arbitrary code and APIs mid-conversation. Create Linear issues, query weather, trigger telephony, or hit internal services.</td></tr><tr><td><strong>Built-in memory via Stream Chat</strong></td><td>Agents recall context naturally across turns and sessions.</td></tr><tr><td>Message the agent silently during a call.</td></tr><tr><td>Interact with the Agent via inbound or outbound phone calls using Twilio and Turbopuffer</td></tr></tbody></table><h2>Out-of-the-Box Integrations</h2><table><thead><tr></tr></thead><tbody><tr><td>Realtime speech-to-speech plugin using Amazon Nova models with automatic reconnection</td></tr><tr><td>TTS plugin using Amazon's cloud-based service with natural-sounding voices and neural engine support</td></tr><tr><td>TTS plugin for realistic voice synthesis in real-time voice applications</td></tr><tr><td>Real-time AI video transformation service for applying artistic styles and effects to video streams</td></tr><tr><td>STT plugin for fast, accurate real-time transcription with speaker diarization</td></tr><tr><td>TTS plugin with highly realistic and expressive voices for conversational agents</td></tr><tr><td>High-performance STT plugin using OpenAI's Whisper model with CTranslate2 for fast inference</td></tr><tr><td>STT and TTS plugin with automatic language detection and voice cloning capabilities</td></tr><tr><td>Realtime API for building conversational agents with support for both voice and video</td></tr><tr><td>LLM plugin providing access to many open-source language models hosted on the Hugging Face Hub and powered by external providers (Cerebras, Together, Groq, etc.)</td></tr><tr><td>TTS plugin with high-quality streaming voices for real-time conversational AI agents</td></tr><tr><td>Local TTS engine for offline voice synthesis with low latency</td></tr><tr><td>Moondream provides realtime detection and VLM capabilities. Developers can choose from using the hosted API or running locally on their CUDA devices. Vision Agents supports Moondream's Detect, Caption and VQA skills out-of-the-box.</td></tr><tr><td>VLM plugin using NVIDIA's Cosmos 2 models for video understanding with automatic frame buffering and streaming responses</td></tr><tr><td>Realtime API for building conversational agents with out of the box support for real-time video directly over WebRTC, LLMs and Open AI TTS</td></tr><tr><td>LLM plugin providing access to multiple providers (Anthropic, Google, OpenAI) through a unified API</td></tr><tr><td>Realtime audio plugin using Alibaba's Qwen3 with native audio output and built-in speech recognition</td></tr><tr><td>Object detection processor using Roboflow's hosted API or local RF-DETR models</td></tr><tr><td>Advanced turn detection system combining Silero VAD, Whisper, and neural models for natural conversation flow</td></tr><tr><td>RAG plugin using TurboPuffer for hybrid search (vector + BM25) with Gemini embeddings for retrieval augmented generation</td></tr><tr><td>Voice call integration plugin enabling bidirectional audio streaming via Twilio Media Streams with call registry and audio conversion</td></tr><tr><td>Real-time pose detection processor using YOLO models with skeleton overlays</td></tr><tr><td>Neural turn detection system for intelligent turn-taking in voice conversations</td></tr><tr><td>STT plugin with real-time translation capabilities powered by Whisper v3</td></tr><tr><td>LLM plugin using xAI's Grok models with advanced reasoning and real-time knowledge</td></tr></tbody></table><p>Processors let your agent  and  in real-time.</p><p>They take care of the hard stuff, like:</p><ul></ul><p>â€¦ so you can focus on your agent logic.</p><table><tbody><tr><td align=\"left\">Using Cartesia's Sonic 3 model to visually look at what's in the frame and tell a story with emotion.<p>â€¢ Real-time visual understanding</p>â€¢ Emotional storytelling<p>â€¢ Frame-by-frame analysis</p><a href=\"https://github.com/GetStream/Vision-Agents/tree/main/plugins/cartesia/example\">&gt;Source Code and tutorial</a></td></tr><tr><td align=\"left\"><h3>Realtime Stable Diffusion</h3>Realtime stable diffusion using Vision Agents and Decart's Mirage 2 model to create interactive scenes and stories.<p>â€¢ Real-time video restyling</p>â€¢ Interactive scene generation<p>â€¢ Stable diffusion integration</p><a href=\"https://github.com/GetStream/Vision-Agents/tree/main/plugins/decart/example\">&gt;Source Code and tutorial</a></td></tr><tr><td align=\"left\">Using Gemini Live together with Vision Agents and Ultralytics YOLO, we're able to track the user's pose and provide realtime actionable feedback on their golf game.<p>â€¢ Real-time pose tracking</p>â€¢ Actionable coaching feedbackâ€¢ Gemini Live integration<a href=\"https://github.com/GetStream/Vision-Agents/tree/main/examples/02_golf_coach_example\">&gt;Source Code and tutorial</a></td></tr><tr><td align=\"left\">Together with OpenAI Realtime and Vision Agents, we can take GeoGuesser to the next level by asking it to identify places in our real world surroundings.<p>â€¢ Real-world location identification</p>â€¢ OpenAI Realtime integration<p>â€¢ Visual scene understanding</p><a href=\"https://visionagents.ai/integrations/openai#openai-realtime\">&gt;Source Code and tutorial</a></td></tr><tr><td align=\"left\">Interact with your Agent over the phone using Twilio. This example demonstrates how to use TurboPuffer for Retrieval Augmented Generation (RAG) to give your agent specialized knowledge.<p>â€¢ Inbound/Outbound telephony</p>â€¢ Twilio Media Streams integration<p>â€¢ Vector search with TurboPuffer</p>â€¢ Retrieval Augmented Generation<a href=\"https://github.com/GetStream/Vision-Agents/tree/main/examples/03_phone_and_rag_example\">&gt;Source Code and tutorial</a></td></tr><tr><td align=\"left\">A security camera with face recognition, package detection and automated theft response. Generates WANTED posters with Nano Banana and posts them to X when packages disappear.<p>â€¢ Face detection &amp; named recognition</p>â€¢ YOLOv11 package detection<p>â€¢ Automated WANTED poster generation</p>â€¢ Real-time X posting<a href=\"https://github.com/GetStream/Vision-Agents/tree/main/examples/04_security_camera_example\">&gt;Source Code and tutorial</a></td></tr></tbody></table><p>Our favorite people &amp; projects to follow for vision AI</p><ul><li>Livekit Agents: Great syntax, Livekit only</li><li>Pipecat: Flexible, but more verbose.</li><li>OpenAI Agents: Focused on openAI only</li></ul><h3>0.1 â€“ First Release - Oct</h3><ul><li>Working TTS, Gemini &amp; OpenAI</li></ul><h3>0.2 - Simplification - Nov</h3><ul><li>Simplified the library &amp; improved code quality</li><li>Deepgram Nova 3, Elevenlabs Scribe 2, Fish, Moondream, QWen3, Smart turn, Vogent, Inworld, Heygen, AWS and more</li><li>Improved openAI &amp; Gemini realtime performance</li></ul><h3>0.3 - Examples and Deploys - Jan</h3><ul><li>Excellence on documentation/polish</li><li>Better Roboflow annotation docs</li><li>Automated workflows for maintenance</li><li>Local camera/audio support AND/OR WebRTC connection</li><li>Embedded/robotics examples</li></ul><p>Video AI is the frontier of AI. The state of the art is changing daily to help models understand live video. While building the integrations, here are the limitations we've noticed (Dec 2025)</p><ul><li>Video AI struggles with small text. If you want the AI to read the score in a game it will often get it wrong and hallucinate</li><li>Longer videos can cause the AI to lose context. For instance if it's watching a soccer match it will get confused after 30 seconds</li><li>Most applications require a combination of small specialized models like Yolo/Roboflow/Moondream, API calls to get more context and larger models like gemini/openAI</li><li>Image size &amp; FPS need to stay relatively low due to performance constraints</li><li>Video doesnâ€™t trigger responses in realtime models. You always need to send audio/text to trigger a response.</li></ul><p>Join the team behind this project - weâ€™re hiring a Staff Python Engineer to architect, build, and maintain a powerful toolkit for developers integrating voice and video AI into their products.</p>","contentLength":10425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NevaMind-AI/memU","url":"https://github.com/NevaMind-AI/memU","date":1769655493,"author":"","guid":425673,"unread":true,"content":"<p>Memory for 24/7 proactive agents like moltbot (clawdbot).</p><p>memU is a memory framework built for . It is designed for long-running use and greatly <strong>reduces the LLM token cost</strong> of keeping agents always online, making always-on, evolving agents practical in production systems. memU <strong>continuously captures and understands user intent</strong>. Even without a command, the agent can tell what you are about to do and act on it by itself.</p><img width=\"100%\" src=\"https://github.com/NevaMind-AI/memU/raw/main/assets/star.gif\"> If you find memU useful or interesting, a GitHub Star â­ï¸ would be greatly appreciated. \n<table><tbody><tr><td>Always-on memory agent that works continuously in the backgroundâ€”never sleeps, never forgets</td></tr><tr><td>Understands and remembers user goals, preferences, and context across sessions automatically</td></tr><tr><td>Reduces long-running token costs by caching insights and avoiding redundant LLM calls</td></tr></tbody></table><h2>ğŸ”„ How Proactive Memory Works</h2><pre><code>\ncd examples/proactive\npython proactive.py\n\n</code></pre><h3>Proactive Memory Lifecycle</h3><pre><code>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  1. USER INITIAL QUERY                          â”‚\nâ”‚  â””â”€ User input, context, or any trigger event   â”‚\nâ”‚     Conversation starts here                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â†“\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  2. AGENT PLANNING / ACTIONS                    â”‚\n    â”‚  â””â”€ Analyze request, execute tasks              â”‚\n    â”‚     Retrieve relevant memories for context      â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â†“\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  3. MEMORIZE &amp; UPDATE TODOLIST                  â”‚\n    â”‚  â””â”€ Store new insights, facts, preferences      â”‚\n    â”‚     Modify task list based on progress          â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â†“\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  4. PREDICT USER INTENT                         â”‚\n    â”‚  â””â”€ Anticipate next steps and needs             â”‚\n    â”‚     Proactively prepare relevant context        â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â†“\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  5. LOOP (2 â†’ 4)                                â”‚\n    â”‚  â””â”€ Continuous iteration until task complete    â”‚\n    â”‚     Agent-driven proactive workflow             â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</code></pre><h3>1. <strong>Information Recommendation</strong></h3><p><em>Agent monitors interests and proactively surfaces relevant content</em></p><pre><code># User has been researching AI topics\nMemU tracks: reading history, saved articles, search queries\n\n# When new content arrives:\nAgent: \"I found 3 new papers on RAG optimization that align with\n        your recent research on retrieval systems. One author\n        (Dr. Chen) you've cited before published yesterday.\"\n\n# Proactive behaviors:\n- Learns topic preferences from browsing patterns\n- Tracks author/source credibility preferences\n- Filters noise based on engagement history\n- Times recommendations for optimal attention\n</code></pre><p><em>Agent learns communication patterns and handles routine correspondence</em></p><pre><code># MemU observes email patterns over time:\n- Response templates for common scenarios\n- Priority contacts and urgent keywords\n- Scheduling preferences and availability\n- Writing style and tone variations\n\n# Proactive email assistance:\nAgent: \"You have 12 new emails. I've drafted responses for 3 routine\n        requests and flagged 2 urgent items from your priority contacts.\n        Should I also reschedule tomorrow's meeting based on the\n        conflict John mentioned?\"\n\n# Autonomous actions:\nâœ“ Draft context-aware replies\nâœ“ Categorize and prioritize inbox\nâœ“ Detect scheduling conflicts\nâœ“ Summarize long threads with key decisions\n</code></pre><h3>3. <strong>Trading &amp; Financial Monitoring</strong></h3><p><em>Agent tracks market context and user investment behavior</em></p><pre><code># MemU learns trading preferences:\n- Risk tolerance from historical decisions\n- Preferred sectors and asset classes\n- Response patterns to market events\n- Portfolio rebalancing triggers\n\n# Proactive alerts:\nAgent: \"NVDA dropped 5% in after-hours trading. Based on your past\n        behavior, you typically buy tech dips above 3%. Your current\n        allocation allows for $2,000 additional exposure while\n        maintaining your 70/30 equity-bond target.\"\n\n# Continuous monitoring:\n- Track price alerts tied to user-defined thresholds\n- Correlate news events with portfolio impact\n- Learn from executed vs. ignored recommendations\n- Anticipate tax-loss harvesting opportunities\n</code></pre><h2>ğŸ—‚ï¸ Hierarchical Memory Architecture</h2><p>MemU's three-layer system enables both  and <strong>proactive context loading</strong>:</p><img width=\"100%\" alt=\"structure\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/structure.png\"><table><thead><tr></tr></thead><tbody><tr><td>Direct access to original data</td><td>Background monitoring for new patterns</td></tr><tr><td>Real-time extraction from ongoing interactions</td></tr><tr><td>Automatic context assembly for anticipation</td></tr></tbody></table><ul><li>: New memories self-organize into topics</li><li>: System identifies recurring themes</li><li>: Anticipates what information will be needed next</li></ul><p>Experience proactive memory instantly:</p><p>ğŸ‘‰  - Hosted service with 7Ã—24 continuous learning</p><p>For enterprise deployment with custom proactive workflows, contact </p><table><thead><tr></tr></thead><tbody><tr><td><code>Authorization: Bearer YOUR_API_KEY</code></td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td>Register continuous learning task</td></tr><tr><td><code>/api/v3/memory/memorize/status/{task_id}</code></td><td>Check real-time processing status</td></tr><tr><td><code>/api/v3/memory/categories</code></td><td>List auto-generated categories</td></tr><tr><td>Query memory (supports proactive context loading)</td></tr></tbody></table><blockquote><p>: Python 3.13+ and an OpenAI API key</p></blockquote><p> (in-memory):</p><pre><code>export OPENAI_API_KEY=your_api_key\ncd tests\npython test_inmemory.py\n</code></pre><p><strong>Test with Persistent Storage</strong> (PostgreSQL):</p><pre><code># Start PostgreSQL with pgvector\ndocker run -d \\\n  --name memu-postgres \\\n  -e POSTGRES_USER=postgres \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -e POSTGRES_DB=memu \\\n  -p 5432:5432 \\\n  pgvector/pgvector:pg16\n\n# Run continuous learning test\nexport OPENAI_API_KEY=your_api_key\ncd tests\npython test_postgres.py\n</code></pre><p>Both examples demonstrate <strong>proactive memory workflows</strong>:</p><ol><li>: Process multiple files sequentially</li><li>: Immediate memory creation</li><li>: Context-aware memory surfacing</li></ol><h3>Custom LLM and Embedding Providers</h3><p>MemU supports custom LLM and embedding providers beyond OpenAI. Configure them via :</p><pre><code>from memu import MemUService\n\nservice = MemUService(\n    llm_profiles={\n        # Default profile for LLM operations\n        \"default\": {\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n            \"api_key\": \"your_api_key\",\n            \"chat_model\": \"qwen3-max\",\n            \"client_backend\": \"sdk\"  # \"sdk\" or \"http\"\n        },\n        # Separate profile for embeddings\n        \"embedding\": {\n            \"base_url\": \"https://api.voyageai.com/v1\",\n            \"api_key\": \"your_voyage_api_key\",\n            \"embed_model\": \"voyage-3.5-lite\"\n        }\n    },\n    # ... other configuration\n)\n</code></pre><p>MemU supports <a href=\"https://openrouter.ai\">OpenRouter</a> as a model provider, giving you access to multiple LLM providers through a single API.</p><pre><code>from memu import MemoryService\n\nservice = MemoryService(\n    llm_profiles={\n        \"default\": {\n            \"provider\": \"openrouter\",\n            \"client_backend\": \"httpx\",\n            \"base_url\": \"https://openrouter.ai\",\n            \"api_key\": \"your_openrouter_api_key\",\n            \"chat_model\": \"anthropic/claude-3.5-sonnet\",  # Any OpenRouter model\n            \"embed_model\": \"openai/text-embedding-3-small\",  # Embedding model\n        },\n    },\n    database_config={\n        \"metadata_store\": {\"provider\": \"inmemory\"},\n    },\n)\n</code></pre><table><tbody><tr><td>Works with any OpenRouter chat model</td></tr><tr><td>Use OpenAI embedding models via OpenRouter</td></tr><tr><td>Use vision-capable models (e.g., )</td></tr></tbody></table><pre><code>export OPENROUTER_API_KEY=your_api_key\n\n# Full workflow test (memorize + retrieve)\npython tests/test_openrouter.py\n\n# Embedding-specific tests\npython tests/test_openrouter_embedding.py\n\n# Vision-specific tests\npython tests/test_openrouter_vision.py\n</code></pre><h3> - Continuous Learning Pipeline</h3><p>Processes inputs in real-time and immediately updates memory:</p><img width=\"100%\" alt=\"memorize\" src=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/assets/memorize.png\"> ```python result = await service.memorize( resource_url=\"path/to/file.json\", # File path or URL modality=\"conversation\", # conversation | document | image | video | audio user={\"user_id\": \"123\"} # Optional: scope to a user ) \n<p>{ \"resource\": {...}, # Stored resource metadata \"items\": [...], # Extracted memory items (available instantly) \"categories\": [...] # Auto-updated category structure }</p><pre><code>\n**Proactive Features:**\n- Zero-delay processingâ€”memories available immediately\n- Automatic categorization without manual tagging\n- Cross-reference with existing memories for pattern detection\n\n### `retrieve()` - Dual-Mode Intelligence\n\nMemU supports both **proactive context loading** and **reactive querying**:\n\n&lt;img width=\"100%\" alt=\"retrieve\" src=\"assets/retrieve.png\" /&gt;\n\n#### RAG-based Retrieval (`method=\"rag\"`)\n\nFast **proactive context assembly** using embeddings:\n\n- âœ… **Instant context**: Sub-second memory surfacing\n- âœ… **Background monitoring**: Can run continuously without LLM costs\n- âœ… **Similarity scoring**: Identifies most relevant memories automatically\n\n#### LLM-based Retrieval (`method=\"llm\"`)\n\nDeep **anticipatory reasoning** for complex contexts:\n\n- âœ… **Intent prediction**: LLM infers what user needs before they ask\n- âœ… **Query evolution**: Automatically refines search as context develops\n- âœ… **Early termination**: Stops when sufficient context is gathered\n\n#### Comparison\n\n| Aspect | RAG (Fast Context) | LLM (Deep Reasoning) |\n|--------|-------------------|---------------------|\n| **Speed** | âš¡ Milliseconds | ğŸ¢ Seconds |\n| **Cost** | ğŸ’° Embedding only | ğŸ’°ğŸ’° LLM inference |\n| **Proactive use** | Continuous monitoring | Triggered context loading |\n| **Best for** | Real-time suggestions | Complex anticipation |\n\n#### Usage\n```python\n# Proactive retrieval with context history\nresult = await service.retrieve(\n    queries=[\n        {\"role\": \"user\", \"content\": {\"text\": \"What are their preferences?\"}},\n        {\"role\": \"user\", \"content\": {\"text\": \"Tell me about work habits\"}}\n    ],\n    where={\"user_id\": \"123\"},  # Optional: scope filter\n    method=\"rag\"  # or \"llm\" for deeper reasoning\n)\n\n# Returns context-aware results:\n{\n    \"categories\": [...],     # Relevant topic areas (auto-prioritized)\n    \"items\": [...],          # Specific memory facts\n    \"resources\": [...],      # Original sources for traceability\n    \"next_step_query\": \"...\" # Predicted follow-up context\n}\n</code></pre><p>: Use  to scope continuous monitoring:</p><ul><li> - User-specific context</li><li><code>where={\"agent_id__in\": [\"1\", \"2\"]}</code> - Multi-agent coordination</li><li>Omit  for global context awareness</li></ul><blockquote><p>ğŸ“š <strong>For complete API documentation</strong>, see <a href=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/docs/SERVICE_API.md\">SERVICE_API.md</a> - includes proactive workflow patterns, pipeline configuration, and real-time update handling.</p></blockquote><h3>Example 1: Always-Learning Assistant</h3><p>Continuously learns from every interaction without explicit memory commands:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_1_conversation_memory.py\n</code></pre><ul><li>Automatically extracts preferences from casual mentions</li><li>Builds relationship models from interaction patterns</li><li>Surfaces relevant context in future conversations</li><li>Adapts communication style based on learned preferences</li></ul><p> Personal AI assistants, customer support that remembers, social chatbots</p><h3>Example 2: Self-Improving Agent</h3><p>Learns from execution logs and proactively suggests optimizations:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_2_skill_extraction.py\n</code></pre><ul><li>Monitors agent actions and outcomes continuously</li><li>Identifies patterns in successes and failures</li><li>Auto-generates skill guides from experience</li><li>Proactively suggests strategies for similar future tasks</li></ul><p> DevOps automation, agent self-improvement, knowledge capture</p><h3>Example 3: Multimodal Context Builder</h3><p>Unifies memory across different input types for comprehensive context:</p><pre><code>export OPENAI_API_KEY=your_api_key\npython examples/example_3_multimodal_memory.py\n</code></pre><ul><li>Cross-references text, images, and documents automatically</li><li>Builds unified understanding across modalities</li><li>Surfaces visual context when discussing related topics</li><li>Anticipates information needs by combining multiple sources</li></ul><p> Documentation systems, learning platforms, research assistants</p><p>MemU achieves  on the Locomo benchmark across all reasoning tasks, demonstrating reliable proactive memory operations.</p><img width=\"100%\" alt=\"benchmark\" src=\"https://github.com/user-attachments/assets/6fec4884-94e5-4058-ad5c-baac3d7e76d9\"><table><thead><tr></tr></thead><tbody><tr><td>Core proactive memory engine</td><td>7Ã—24 learning pipeline, auto-categorization</td></tr><tr><td>Backend with continuous sync</td><td>Real-time memory updates, webhook triggers</td></tr><tr><td>Live memory evolution monitoring</td></tr></tbody></table><p>We welcome contributions from the community! Whether you're fixing bugs, adding features, or improving documentation, your help is appreciated.</p><p>To start contributing to MemU, you'll need to set up your development environment:</p><ul><li><a href=\"https://github.com/astral-sh/uv\">uv</a> (Python package manager)</li></ul><h4>Setup Development Environment</h4><pre><code># 1. Fork and clone the repository\ngit clone https://github.com/YOUR_USERNAME/memU.git\ncd memU\n\n# 2. Install development dependencies\nmake install\n</code></pre><p>The  command will:</p><ul><li>Create a virtual environment using </li><li>Install all project dependencies</li><li>Set up pre-commit hooks for code quality checks</li></ul><p>Before submitting your contribution, ensure your code passes all quality checks:</p><p>The  command runs:</p><ul><li>: Ensures  consistency</li><li>: Lints code with Ruff, formats with Black</li><li>: Runs  for static type analysis</li><li>: Uses  to find obsolete dependencies</li></ul><p>For detailed contribution guidelines, code standards, and development practices, please see <a href=\"https://raw.githubusercontent.com/NevaMind-AI/memU/main/CONTRIBUTING.md\">CONTRIBUTING.md</a>.</p><ul><li>Create a new branch for each feature or bug fix</li><li>Write clear commit messages</li><li>Add tests for new functionality</li><li>Update documentation as needed</li><li>Run  before pushing</li></ul><div align=\"center\"><p>â­  to get notified about new releases!</p></div>","contentLength":14331,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ran-j/PS2Recomp","url":"https://github.com/ran-j/PS2Recomp","date":1769655493,"author":"","guid":425674,"unread":true,"content":"<p>Playstation 2 Static Recompiler &amp; Runtime Tool to make native PC ports</p><h2>PS2Recomp: PlayStation 2 Static Recompiler (Not ready)</h2><ul><li>Note this is an experiment and doesn't work as it should, feel free to open a PR to help the project.</li></ul><p>PS2Recomp is a tool designed to statically recompile PlayStation 2 ELF binaries into C++ code that can be compiled for any modern platform. This enables running PS2 games natively on PC and other platforms without traditional emulation.</p><ul><li>Translates MIPS R5900 instructions to C++ code</li><li>Supports PS2-specific 128-bit MMI instructions</li><li>Handles VU0 in macro mode</li><li>Supports relocations and overlays</li><li>Configurable via TOML files</li><li>Single-file or multi-file output options</li><li>Function stubbing and skipping</li></ul><p>Parsing a PS2 ELF file to extract functions, symbols, and relocations Decoding the MIPS R5900 instructions in each function Translating those instructions to equivalent C++ code Generating a runtime that can execute the recompiled code</p><p>The translated code is very literal, with each MIPS instruction mapping to a C++ operation. For example,  becomes <code>ctx-&gt;r4 = ADD32(ctx-&gt;r4, 0X20);</code>.</p><ul><li>C++20 compatible compiler (I only test with MSVC)</li><li>SSE4/AVX support for 128-bit operations</li></ul><pre><code>git clone --recurse-submodules https://github.com/ran-j/PS2Recomp.git\ncd PS2Recomp\n\n# Create build directory\nmkdir build\ncd build\n\ncmake ..\ncmake --build .\n</code></pre><ol><li>Create a configuration file (see <code>./ps2xRecomp/example_config.toml</code>)</li></ol><pre><code>./ps2recomp your_config.toml\n</code></pre><p>Compile the generated C++ code Link with a runtime implementation</p><p>PS2Recomp uses TOML configuration files to specify:</p><ul><li>Functions to stub or skip</li></ul><pre><code>[general]\ninput = \"path/to/game.elf\"\noutput = \"output/\"\nsingle_file_output = false\n\n# Functions to stub\nstubs = [\"printf\", \"malloc\", \"free\"]\n\n# Functions to skip\nskip = [\"abort\", \"exit\"]\n\n# Patches\n[patches]\ninstructions = [\n  { address = \"0x100004\", value = \"0x00000000\" }\n]\n</code></pre><p>To execute the recompiled code, you'll need to implement or use a runtime that provides:</p><ul><li>PS2-specific hardware simulation</li></ul><p>A basic runtime lib is provided in  folder.</p><ul><li>VU1 microcode support is limited</li><li>Graphics Synthesizer and other hardware components need external implementation</li><li>Some PS2-specific features may not be fully supported yet</li></ul><ul><li>Uses ELFIO for ELF parsing</li><li>Uses toml11 for TOML parsing</li><li>Uses fmt for string formatting</li></ul>","contentLength":2260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"protocolbuffers/protobuf","url":"https://github.com/protocolbuffers/protobuf","date":1769655493,"author":"","guid":425675,"unread":true,"content":"<p>Protocol Buffers - Google's data interchange format</p><p>Copyright 2008 Google LLC</p><p>Protocol Buffers (a.k.a., protobuf) are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data. You can learn more about it in <a href=\"https://protobuf.dev\">protobuf's documentation</a>.</p><p>This README file contains protobuf installation instructions. To install protobuf, you need to install the protocol compiler (used to compile .proto files) and the protobuf runtime for your chosen programming language.</p><h2>Working With Protobuf Source Code</h2><p>If you choose to work from the head revision of the main branch your build will occasionally be broken by source-incompatible changes and insufficiently-tested (and therefore broken) behavior.</p><p>If you are using C++ or otherwise need to build protobuf from source as a part of your project, you should pin to a release commit on a release branch.</p><p>This is because even release branches can experience some instability in between release commits.</p><p>Protobuf supports <a href=\"https://bazel.build/external/module\">Bzlmod</a> with Bazel 7 +. Users should specify a dependency on protobuf in their MODULE.bazel file as follows.</p><pre><code>bazel_dep(name = \"protobuf\", version = &lt;VERSION&gt;)\n</code></pre><p>Users can optionally override the repo name, such as for compatibility with WORKSPACE.</p><pre><code>bazel_dep(name = \"protobuf\", version = &lt;VERSION&gt;, repo_name = \"com_google_protobuf\")\n</code></pre><p>Users can also add the following to their legacy <a href=\"https://bazel.build/external/overview#workspace-system\">WORKSPACE</a> file.</p><p>Note that with the release of 30.x there are a few more load statements to properly set up rules_java and rules_python.</p><pre><code>http_archive(\n    name = \"com_google_protobuf\",\n    strip_prefix = \"protobuf-VERSION\",\n    sha256 = ...,\n    url = ...,\n)\n\nload(\"@com_google_protobuf//:protobuf_deps.bzl\", \"protobuf_deps\")\n\nprotobuf_deps()\n\nload(\"@rules_java//java:rules_java_deps.bzl\", \"rules_java_dependencies\")\n\nrules_java_dependencies()\n\nload(\"@rules_java//java:repositories.bzl\", \"rules_java_toolchains\")\n\nrules_java_toolchains()\n\nload(\"@rules_python//python:repositories.bzl\", \"py_repositories\")\n\npy_repositories()\n</code></pre><h2>Protobuf Compiler Installation</h2><p>The protobuf compiler is written in C++. If you are using C++, please follow the <a href=\"https://raw.githubusercontent.com/protocolbuffers/protobuf/main/src/README.md\">C++ Installation Instructions</a> to install protoc along with the C++ runtime.</p><p>For non-C++ users, the simplest way to install the protocol compiler is to download a pre-built binary from our <a href=\"https://github.com/protocolbuffers/protobuf/releases\">GitHub release page</a>.</p><p>In the downloads section of each release, you can find pre-built binaries in zip packages: <code>protoc-$VERSION-$PLATFORM.zip</code>. It contains the protoc binary as well as a set of standard  files distributed along with protobuf.</p><p>If you are looking for an old version that is not available in the release page, check out the <a href=\"https://repo1.maven.org/maven2/com/google/protobuf/protoc/\">Maven repository</a>.</p><p>These pre-built binaries are only provided for released versions. If you want to use the github main version at HEAD, or you need to modify protobuf code, or you are using C++, it's recommended to build your own protoc binary from source.</p><h2>Protobuf Runtime Installation</h2><p>Protobuf supports several different programming languages. For each programming language, you can find instructions in the corresponding source directory about how to install protobuf runtime for that specific language:</p><p>If you want to learn from code examples, take a look at the examples in the <a href=\"https://raw.githubusercontent.com/protocolbuffers/protobuf/main/examples\">examples</a> directory.</p><p>To be alerted to upcoming changes in Protocol Buffers and connect with protobuf developers and users, <a href=\"https://groups.google.com/g/protobuf\">join the Google Group</a>.</p>","contentLength":3333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"lobehub/lobehub","url":"https://github.com/lobehub/lobehub","date":1769655493,"author":"","guid":425676,"unread":true,"content":"<p>The ultimate space for work and life â€” to find, build, and collaborate with agent teammates that grow with you. We are taking agent harness to the next level â€” enabling multi-agent collaboration, effortless agent team design, and introducing agents as the unit of work interaction.</p><h2>ğŸ‘‹ğŸ» Getting Started &amp; Join Our Community</h2><p>We are a group of e/acc design-engineers, hoping to provide modern design components and tools for AIGC. By adopting the Bootstrapping approach, we aim to provide developers and users with a more open, transparent, and user-friendly product ecosystem.</p><p>Whether for users or professional developers, LobeHub will be your AI Agent playground. Please be aware that LobeHub is currently under active development, and feedback is welcome for any <a href=\"https://img.shields.io/github/issues/lobehub/lobe-chat.svg?style=flat\">issues</a> encountered.</p><blockquote><p>, You will receive all release notifications from GitHub without any delay ~ â­ï¸</p></blockquote><p>Todayâ€™s agents are one-off, task-driven tools. They lack context, live in isolation, and require manual hand-offs between different windows and models. While some maintain memory, it is often global, shallow, and impersonal. In this mode, users are forced to toggle between fragmented conversations, making it difficult to form structured productivity.</p><p><strong>LobeHub changes everything.</strong></p><p>LobeHub is a work-and-lifestyle space to find, build, and collaborate with agent teammates that grow with you. In LobeHub, we treat <strong>Agents as the unit of work</strong>, providing an infrastructure where humans and agents co-evolve.</p><h3>Create: Agents as the Unit of Work</h3><p>Building a personalized AI team starts with the . You can describe what you need once, and the agent setup starts right away, applying auto-configurations so you can use it instantly.</p><ul><li>: Seamlessly access any model and any modalityâ€”all under your control.</li><li>: Connect your agents to the skills you use every day with a library of over 10,000 tools and MCP-compatible plugins.</li></ul><h3>Collaborate: Scale New Forms of Collaboration Networks</h3><p>LobeHub introduces , allowing you to work with agents like real teammates. The system assembles the right agents for the task, enabling parallel collaboration and iterative improvement.</p><ul><li>: Write and refine content with multiple agents in one place with a shared context.</li><li>: Schedule runs and let agents do the work at the right time, even while you are away.</li><li>: Organize work by project to keep everything structured and easy to track.</li><li>: A shared space for teams to collaborate with agents, ensuring clear ownership and visibility across the organization.</li></ul><h3>Evolve: Co-evolution of Humans and Agents</h3><p>The best AI is one that understands you deeply. LobeHub features  that builds a clear understanding of your needs.</p><ul><li>: Your agents learn from how you work, adapting their behavior to act at the right moment.</li><li>: We believe in transparency. Your agents use structured, editable memory, giving you full control over what they remember.</li></ul><blockquote><p>âœ¨ more features will be added when LobeHub evolve.</p></blockquote><p>LobeHub provides Self-Hosted Version with Vercel, Alibaba Cloud, and <a href=\"https://hub.docker.com/r/lobehub/lobehub\">Docker Image</a>. This allows you to deploy your own chatbot within a few minutes without any prior knowledge.</p><h3> Deploying with Vercel, Zeabur , Sealos or Alibaba Cloud</h3><p>\"If you want to deploy this service yourself on Vercel, Zeabur or Alibaba Cloud, you can follow these steps:</p><ul><li>Click the button below to start deployment: Log in directly with your GitHub account, and remember to fill in the (required) and  (recommended) on the environment variable section.</li><li>After deployment, you can start using it.</li><li>Bind a custom domain (optional): The DNS of the domain assigned by Vercel is polluted in some areas; binding a custom domain can connect directly.</li></ul><p>After fork, only retain the upstream sync action and disable other actions in your repository on GitHub.</p><p>If you have deployed your own project following the one-click deployment steps in the README, you might encounter constant prompts indicating \"updates available.\" This is because Vercel defaults to creating a new project instead of forking this one, resulting in an inability to detect updates accurately.</p><p>We provide a Docker image for deploying the LobeHub service on your own private device. Use the following command to start the LobeHub service:</p><ol><li>create a folder to for storage files</li></ol><pre><code>$ mkdir lobe-chat-db &amp;&amp; cd lobe-chat-db\n</code></pre><ol start=\"2\"><li>init the LobeHub infrastructure</li></ol><pre><code>bash &lt;(curl -fsSL https://lobe.li/setup.sh)\n</code></pre><ol start=\"3\"><li>Start the LobeHub service</li></ol><p>This project provides some additional configuration items set with environment variables:</p><table><thead><tr></tr></thead><tbody><tr><td>This is the API key you apply on the OpenAI account page</td></tr><tr><td>If you manually configure the OpenAI interface proxy, you can use this configuration item to override the default OpenAI API request base URL</td><td><code>https://api.chatanywhere.cn</code> or The default value is<code>https://api.openai.com/v1</code></td></tr><tr><td>Add a password to access this service; you can set a long password to avoid leaking. If this value contains a comma, it is a password array.</td><td> or  or </td></tr><tr><td>Used to control the model list. Use  to add a model,  to hide a model, and  to customize the display name of a model, separated by commas.</td><td><code>qwen-7b-chat,+glm-6b,-gpt-3.5-turbo</code></td></tr></tbody></table><p>Plugins provide a means to extend the <a href=\"https://lobehub.com/blog/openai-function-call\">Function Calling</a> capabilities of LobeHub. They can be used to introduce new function calls and even new ways to render message results. If you are interested in plugin development, please refer to our <a href=\"https://lobehub.com/docs/usage/plugins/development\">ğŸ“˜ Plugin Development Guide</a> in the Wiki.</p><ul><li><a href=\"https://github.com/lobehub/lobe-chat-plugins\">lobe-chat-plugins</a>: This is the plugin index for LobeHub. It accesses index.json from this repository to display a list of available plugins for LobeHub to the user.</li><li><a href=\"https://github.com/lobehub/chat-plugins-gateway\">@lobehub/chat-plugins-gateway</a>: The LobeHub Plugins Gateway is a backend service that provides a gateway for LobeHub plugins. We deploy this service using Vercel. The primary API POST /api/v1/runner is deployed as an Edge Function.</li></ul><blockquote><p>The plugin system is currently undergoing major development. You can learn more in the following issues:</p><ul><li>[x] <a href=\"https://github.com/lobehub/lobe-chat/issues/73\"></a>: Implement separation of the plugin from the main body, split the plugin into an independent repository for maintenance, and realize dynamic loading of the plugin.</li><li>[x] <a href=\"https://github.com/lobehub/lobe-chat/issues/97\"></a>: The security and stability of the plugin's use, more accurately presenting abnormal states, the maintainability of the plugin architecture, and developer-friendly.</li><li>[x] <a href=\"https://github.com/lobehub/lobe-chat/issues/149\"></a>: Higher-level and more comprehensive customization capabilities, support for plugin authentication, and examples.</li></ul></blockquote><p>You can use GitHub Codespaces for online development:</p><p>Or clone it for local development:</p><pre><code>$ git clone https://github.com/lobehub/lobe-chat.git\n$ cd lobe-chat\n$ pnpm install\n$ pnpm dev\n</code></pre><p>Contributions of all types are more than welcome; if you are interested in contributing code, feel free to check out our GitHub <a href=\"https://github.com/lobehub/lobe-chat/issues\">Issues</a> and <a href=\"https://github.com/lobehub/lobe-chat/projects\">Projects</a> to get stuck in to show us what you're made of.</p><blockquote><p>We are creating a technology-driven forum, fostering knowledge interaction and the exchange of ideas that may culminate in mutual inspiration and collaborative innovation.</p><p>Help us make LobeHub better. Welcome to provide product design feedback, user experience discussions directly to us.</p></blockquote><a href=\"https://github.com/lobehub/lobe-chat/graphs/contributors\" target=\"_blank\"></a><p>Every bit counts and your one-time donation sparkles in our galaxy of support! You're a shooting star, making a swift and bright impact on our journey. Thank you for believing in us â€“ your generosity guides us toward our mission, one brilliant flash at a time.</p><a href=\"https://opencollective.com/lobehub\" target=\"_blank\"></a><ul><li> Modern theme for Stable Diffusion WebUI, exquisite interface design, highly customizable UI, and efficiency-boosting features.</li><li> WebUI for Midjourney, leverages AI to quickly generate a wide array of rich and diverse images from text prompts, sparking creativity and enhancing conversations.</li><li> Lobe i18n is an automation tool for the i18n (internationalization) translation process, powered by ChatGPT. It supports features such as automatic splitting of large files, incremental updates, and customization options for the OpenAI model, API proxy, and temperature.</li><li> Lobe Commit is a CLI tool that leverages Langchain/ChatGPT to generate Gitmoji-based commit messages.</li></ul>","contentLength":7841,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"moltbot/moltbot","url":"https://github.com/moltbot/moltbot","date":1769655493,"author":"","guid":425677,"unread":true,"content":"<p>Your own personal AI assistant. Any OS. Any Platform. The lobster way. ğŸ¦</p><p> is a  you run on your own devices. It answers you on the channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, Microsoft Teams, WebChat), plus extension channels like BlueBubbles, Matrix, Zalo, and Zalo Personal. It can speak and listen on macOS/iOS/Android, and can render a live Canvas you control. The Gateway is just the control plane â€” the product is the assistant.</p><p>If you want a personal, single-user assistant that feels local, fast, and always-on, this is it.</p><p>Preferred setup: run the onboarding wizard (). It walks through gateway, workspace, channels, and skills. The CLI wizard is the recommended path and works on <strong>macOS, Linux, and Windows (via WSL2; strongly recommended)</strong>. Works with npm, pnpm, or bun. New install? Start here: <a href=\"https://docs.molt.bot/start/getting-started\">Getting started</a></p><p>Model note: while any model is supported, I strongly recommend <strong>Anthropic Pro/Max (100/200) + Opus 4.5</strong> for longâ€‘context strength and better promptâ€‘injection resistance. See <a href=\"https://docs.molt.bot/start/onboarding\">Onboarding</a>.</p><h2>Models (selection + auth)</h2><pre><code>npm install -g moltbot@latest\n# or: pnpm add -g moltbot@latest\n\nmoltbot onboard --install-daemon\n</code></pre><p>The wizard installs the Gateway daemon (launchd/systemd user service) so it stays running. Legacy note:  remains available as a compatibility shim.</p><pre><code>moltbot onboard --install-daemon\n\nmoltbot gateway --port 18789 --verbose\n\n# Send a message\nmoltbot message send --to +1234567890 --message \"Hello from Moltbot\"\n\n# Talk to the assistant (optionally deliver back to any connected channel: WhatsApp/Telegram/Slack/Discord/Google Chat/Signal/iMessage/BlueBubbles/Microsoft Teams/Matrix/Zalo/Zalo Personal/WebChat)\nmoltbot agent --message \"Ship checklist\" --thinking high\n</code></pre><ul><li>: tagged releases ( or ), npm dist-tag .</li><li>: prerelease tags (), npm dist-tag  (macOS app may be missing).</li><li>: moving head of , npm dist-tag  (when published).</li></ul><h2>From source (development)</h2><p>Prefer  for builds from source. Bun is optional for running TypeScript directly.</p><pre><code>git clone https://github.com/moltbot/moltbot.git\ncd moltbot\n\npnpm install\npnpm ui:build # auto-installs UI deps on first run\npnpm build\n\npnpm moltbot onboard --install-daemon\n\n# Dev loop (auto-reload on TS changes)\npnpm gateway:watch\n</code></pre><p>Note:  runs TypeScript directly (via ).  produces  for running via Node / the packaged  binary.</p><h2>Security defaults (DM access)</h2><p>Moltbot connects to real messaging surfaces. Treat inbound DMs as .</p><p>Default behavior on Telegram/WhatsApp/Signal/iMessage/Microsoft Teams/Discord/Google Chat/Slack:</p><ul><li> ( / <code>channels.discord.dm.policy=\"pairing\"</code> / <code>channels.slack.dm.policy=\"pairing\"</code>): unknown senders receive a short pairing code and the bot does not process their message.</li><li>Approve with: <code>moltbot pairing approve &lt;channel&gt; &lt;code&gt;</code> (then the sender is added to a local allowlist store).</li><li>Public inbound DMs require an explicit opt-in: set  and include  in the channel allowlist ( / <code>channels.discord.dm.allowFrom</code> / <code>channels.slack.dm.allowFrom</code>).</li></ul><p>Run  to surface risky/misconfigured DM policies.</p><ul><li> â€” WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, BlueBubbles, Microsoft Teams, Matrix, Zalo, Zalo Personal, WebChat, macOS, iOS/Android.</li><li> â€” route inbound channels/accounts/peers to isolated agents (workspaces + per-agent sessions).</li><li> â€” browser, canvas, nodes, cron, sessions, and Discord/Slack actions.</li></ul><h2>Everything we built so far</h2><pre><code>WhatsApp / Telegram / Slack / Discord / Google Chat / Signal / iMessage / BlueBubbles / Microsoft Teams / Matrix / Zalo / Zalo Personal / WebChat\n               â”‚\n               â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            Gateway            â”‚\nâ”‚       (control plane)         â”‚\nâ”‚     ws://127.0.0.1:18789      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n               â”‚\n               â”œâ”€ Pi agent (RPC)\n               â”œâ”€ CLI (moltbot â€¦)\n               â”œâ”€ WebChat UI\n               â”œâ”€ macOS app\n               â””â”€ iOS / Android nodes\n</code></pre><h2>Tailscale access (Gateway dashboard)</h2><p>Moltbot can auto-configure Tailscale  (tailnet-only) or  (public) while the Gateway stays bound to loopback. Configure :</p><ul><li>: no Tailscale automation (default).</li><li>: tailnet-only HTTPS via  (uses Tailscale identity headers by default).</li><li>: public HTTPS via  (requires shared password auth).</li></ul><ul><li> must stay  when Serve/Funnel is enabled (Moltbot enforces this).</li><li>Serve can be forced to require a password by setting <code>gateway.auth.mode: \"password\"</code> or <code>gateway.auth.allowTailscale: false</code>.</li><li>Funnel refuses to start unless <code>gateway.auth.mode: \"password\"</code> is set.</li><li>Optional: <code>gateway.tailscale.resetOnExit</code> to undo Serve/Funnel on shutdown.</li></ul><h2>Remote Gateway (Linux is great)</h2><p>Itâ€™s perfectly fine to run the Gateway on a small Linux instance. Clients (macOS app, CLI, WebChat) can connect over  or , and you can still pair device nodes (macOS/iOS/Android) to execute deviceâ€‘local actions when needed.</p><ul><li> runs the exec tool and channel connections by default.</li><li> run deviceâ€‘local actions (, camera, screen recording, notifications) via . In short: exec runs where the Gateway lives; device actions run where the device lives.</li></ul><h2>macOS permissions via the Gateway protocol</h2><p>The macOS app can run in  and advertises its capabilities + permission map over the Gateway WebSocket ( / ). Clients can then execute local actions via :</p><ul><li> runs a local command and returns stdout/stderr/exit code; set <code>needsScreenRecording: true</code> to require screen-recording permission (otherwise youâ€™ll get ).</li><li> posts a user notification and fails if notifications are denied.</li><li>, , , and  are also routed via  and follow TCC permission status.</li></ul><p>Elevated bash (host permissions) is separate from macOS TCC:</p><ul><li>Use  to toggle perâ€‘session elevated access when enabled + allowlisted.</li><li>Gateway persists the perâ€‘session toggle via  (WS method) alongside , , , , and .</li></ul><h2>Agent to Agent (sessions_* tools)</h2><ul><li>Use these to coordinate work across sessions without jumping between chat surfaces.</li><li> â€” discover active sessions (agents) and their metadata.</li><li> â€” fetch transcript logs for a session.</li><li> â€” message another session; optional replyâ€‘back pingâ€‘pong + announce step (, ).</li></ul><h2>Skills registry (ClawdHub)</h2><p>ClawdHub is a minimal skill registry. With ClawdHub enabled, the agent can search for skills automatically and pull in new ones as needed.</p><p>Send these in WhatsApp/Telegram/Slack/Google Chat/Microsoft Teams/WebChat (group commands are owner-only):</p><ul><li> â€” compact session status (model + tokens, cost when available)</li><li> or  â€” reset the session</li><li> â€” compact session context (summary)</li><li> â€” off|minimal|low|medium|high|xhigh (GPT-5.2 + Codex models only)</li><li> â€” per-response usage footer</li><li> â€” restart the gateway (owner-only in groups)</li><li><code>/activation mention|always</code> â€” group activation toggle (groups only)</li></ul><p>The Gateway alone delivers a great experience. All apps are optional and add extra features.</p><p>If you plan to build/run companion apps, follow the platform runbooks below.</p><h3>macOS (Moltbot.app) (optional)</h3><ul><li>Menu bar control for the Gateway and health.</li><li>Voice Wake + push-to-talk overlay.</li><li>Remote gateway control over SSH.</li></ul><p>Note: signed builds required for macOS permissions to stick across rebuilds (see ).</p><ul><li>Pairs as a node via the Bridge.</li><li>Voice trigger forwarding + Canvas surface.</li><li>Controlled via .</li></ul><ul><li>Pairs via the same Bridge + pairing flow as iOS.</li><li>Exposes Canvas, Camera, and Screen capture commands.</li></ul><ul><li>Workspace root:  (configurable via <code>agents.defaults.workspace</code>).</li><li>Injected prompt files: , , .</li><li>Skills: <code>~/clawd/skills/&lt;skill&gt;/SKILL.md</code>.</li></ul><p>Minimal  (model + defaults):</p><pre><code>{\n  agent: {\n    model: \"anthropic/claude-opus-4-5\"\n  }\n}\n</code></pre><h2>Security model (important)</h2><ul><li> tools run on the host for the  session, so the agent has full access when itâ€™s just you.</li><li> set <code>agents.defaults.sandbox.mode: \"non-main\"</code> to run  (groups/channels) inside perâ€‘session Docker sandboxes; bash then runs in Docker for those sessions.</li><li> allowlist , , , , , , , , ; denylist , , , , , .</li></ul><ul><li>Link the device: <code>pnpm moltbot channels login</code> (stores creds in ).</li><li>Allowlist who can talk to the assistant via <code>channels.whatsapp.allowFrom</code>.</li><li>If  is set, it becomes a group allowlist; include  to allow all.</li></ul><ul><li>Set  or <code>channels.telegram.botToken</code> (env wins).</li><li>Optional: set  (with <code>channels.telegram.groups.\"*\".requireMention</code>); when set, it is a group allowlist (include  to allow all). Also <code>channels.telegram.allowFrom</code> or <code>channels.telegram.webhookUrl</code> as needed.</li></ul><pre><code>{\n  channels: {\n    telegram: {\n      botToken: \"123456:ABCDEF\"\n    }\n  }\n}\n</code></pre><ul><li>Set  +  (or  + ).</li></ul><ul><li>Set  or  (env wins).</li><li>Optional: set , , or , plus <code>channels.discord.dm.allowFrom</code>, , or <code>channels.discord.mediaMaxMb</code> as needed.</li></ul><pre><code>{\n  channels: {\n    discord: {\n      token: \"1234abcd\"\n    }\n  }\n}\n</code></pre><ul><li>Requires  and a  config section.</li></ul><ul><li>macOS only; Messages must be signed in.</li><li>If  is set, it becomes a group allowlist; include  to allow all.</li></ul><ul><li>Configure a Teams app + Bot Framework, then add a  config section.</li><li>Allowlist who can talk via ; group access via  or <code>msteams.groupPolicy: \"open\"</code>.</li></ul><ul><li>Uses the Gateway WebSocket; no separate WebChat port/config.</li></ul><p>Browser control (optional):</p><pre><code>{\n  browser: {\n    enabled: true,\n    color: \"#FF4500\"\n  }\n}\n</code></pre><p>Use these when youâ€™re past the onboarding flow and want the deeper reference.</p><h2>Advanced docs (discovery + control)</h2><h2>Operations &amp; troubleshooting</h2><p>Moltbot was built for , a space lobster AI assistant. ğŸ¦ by Peter Steinberger and the community.</p><p>See <a href=\"https://raw.githubusercontent.com/moltbot/moltbot/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for guidelines, maintainers, and how to submit PRs. AI/vibe-coded PRs welcome! ğŸ¤–</p><p>Thanks to all clawtributors:</p>","contentLength":9426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"bambulab/BambuStudio","url":"https://github.com/bambulab/BambuStudio","date":1769655493,"author":"","guid":425678,"unread":true,"content":"<p>PC Software for BambuLab and other 3D printers</p><p>Bambu Studio is a cutting-edge, feature-rich slicing software. It contains project-based workflows, systematically optimized slicing algorithms, and an easy-to-use graphic interface, bringing users an incredibly smooth printing experience.</p><p>Bambu Studio is based on <a href=\"https://github.com/prusa3d/PrusaSlicer\">PrusaSlicer</a> by Prusa Research, which is from <a href=\"https://github.com/Slic3r/Slic3r\">Slic3r</a> by Alessandro Ranellucci and the RepRap community.</p><ul><li>Basic slicing features &amp; GCode viewer</li><li>Multiple plates management</li><li>Remote control &amp; monitoring</li><li>Hybrid/Tree/Normal support types, Customized support</li><li>multi-material printing and rich painting tools</li><li>multi-platform (Win/Mac/Linux) support</li><li>Global/Object/Part level slicing parameters</li></ul><p>Other major features are:</p><ul><li>Advanced cooling logic controlling fan speed and dynamic print speed</li><li>Auto brim according to mechanical analysis</li><li>Assembly &amp; explosion view</li><li>Flushing transition-filament into infill/object during filament change</li></ul><p>Following platforms are currently supported to compile:</p><p>Bambu Studio is licensed under the GNU Affero General Public License, version 3. Bambu Studio is based on PrusaSlicer by PrusaResearch.</p><p>PrusaSlicer is licensed under the GNU Affero General Public License, version 3. PrusaSlicer is owned by Prusa Research. PrusaSlicer is originally based on Slic3r by Alessandro Ranellucci.</p><p>Slic3r is licensed under the GNU Affero General Public License, version 3. Slic3r was created by Alessandro Ranellucci with the help of many other contributors.</p><p>The GNU Affero General Public License, version 3 ensures that if you use any part of this software in any way (even behind a web server), your software must be released under the same license.</p><p>The bambu networking plugin is based on non-free libraries. It is optional to the Bambu Studio and provides extended networking functionalities for users. By default, after installing Bambu Studio without the networking plugin, you can initiate printing through the SD card after slicing is completed.</p>","contentLength":1942,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MoonshotAI/kimi-cli","url":"https://github.com/MoonshotAI/kimi-cli","date":1769655493,"author":"","guid":425679,"unread":true,"content":"<p>Kimi Code CLI is your next CLI agent.</p><p>Kimi Code CLI is an AI agent that runs in the terminal, helping you complete software development tasks and terminal operations. It can read and edit code, execute shell commands, search and fetch web pages, and autonomously plan and adjust actions during execution.</p><p>Kimi Code CLI is not only a coding agent, but also a shell. You can switch the shell command mode by pressing . In this mode, you can directly run shell commands without leaving Kimi Code CLI.</p><blockquote><p>[!NOTE] Built-in shell commands like  are not supported yet.</p></blockquote><p>Kimi Code CLI supports <a href=\"https://github.com/agentclientprotocol/agent-client-protocol\">Agent Client Protocol</a> out of the box. You can use it together with any ACP-compatible editor or IDE.</p><p>To use Kimi Code CLI with ACP clients, make sure to run Kimi Code CLI in the terminal and send  to complete the login first. Then, you can configure your ACP client to start Kimi Code CLI as an ACP agent server with command .</p><p>For example, to use Kimi Code CLI with <a href=\"https://zed.dev/\">Zed</a> or <a href=\"https://blog.jetbrains.com/ai/2025/12/bring-your-own-ai-agent-to-jetbrains-ides/\">JetBrains</a>, add the following configuration to your <code>~/.config/zed/settings.json</code> or  file:</p><pre><code>{\n  \"agent_servers\": {\n    \"Kimi Code CLI\": {\n      \"command\": \"kimi\",\n      \"args\": [\"acp\"],\n      \"env\": {}\n    }\n  }\n}\n</code></pre><p>Then you can create Kimi Code CLI threads in IDE's agent panel.</p><p>You can use Kimi Code CLI together with Zsh, to empower your shell experience with AI agent capabilities.</p><pre><code>git clone https://github.com/MoonshotAI/zsh-kimi-cli.git \\\n  ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/kimi-cli\n</code></pre><blockquote><p>[!NOTE] If you are using a plugin manager other than Oh My Zsh, you may need to refer to the plugin's README for installation instructions.</p></blockquote><p>Then add  to your Zsh plugin list in :</p><p>After restarting Zsh, you can switch to agent mode by pressing .</p><p>Kimi Code CLI supports MCP (Model Context Protocol) tools.</p><p><strong> sub-command group</strong></p><p>You can manage MCP servers with  sub-command group. For example:</p><pre><code># Add streamable HTTP server:\nkimi mcp add --transport http context7 https://mcp.context7.com/mcp --header \"CONTEXT7_API_KEY: ctx7sk-your-key\"\n\n# Add streamable HTTP server with OAuth authorization:\nkimi mcp add --transport http --auth oauth linear https://mcp.linear.app/mcp\n\n# Add stdio server:\nkimi mcp add --transport stdio chrome-devtools -- npx chrome-devtools-mcp@latest\n\n# List added MCP servers:\nkimi mcp list\n\n# Remove an MCP server:\nkimi mcp remove chrome-devtools\n\n# Authorize an MCP server:\nkimi mcp auth linear\n</code></pre><p>Kimi Code CLI also supports ad-hoc MCP server configuration via CLI option.</p><p>Given an MCP config file in the well-known MCP config format like the following:</p><pre><code>{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    },\n    \"chrome-devtools\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"chrome-devtools-mcp@latest\"]\n    }\n  }\n}\n</code></pre><p>Run  with  option to connect to the specified MCP servers:</p><pre><code>kimi --mcp-config-file /path/to/mcp.json\n</code></pre><p>To develop Kimi Code CLI, run:</p><pre><code>git clone https://github.com/MoonshotAI/kimi-cli.git\ncd kimi-cli\n\nmake prepare  # prepare the development environment\n</code></pre><p>Then you can start working on Kimi Code CLI.</p><p>Refer to the following commands after you make changes:</p><pre><code>uv run kimi  # run Kimi Code CLI\n\nmake format  # format code\nmake check  # run linting and type checking\nmake test  # run tests\nmake test-kimi-cli  # run Kimi Code CLI tests only\nmake test-kosong  # run kosong tests only\nmake test-pykaos  # run pykaos tests only\nmake build  # build python packages\nmake build-bin  # build standalone binary\nmake help  # show all make targets\n</code></pre><p>We welcome contributions to Kimi Code CLI! Please refer to <a href=\"https://raw.githubusercontent.com/MoonshotAI/kimi-cli/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for more information.</p>","contentLength":3591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"modelcontextprotocol/ext-apps","url":"https://github.com/modelcontextprotocol/ext-apps","date":1769655493,"author":"","guid":425680,"unread":true,"content":"<p>Official repo for spec &amp; SDK of MCP Apps protocol - standard for UIs embedded AI chatbots, served by MCP servers</p><p>This repo contains the SDK and specification for MCP Apps Extension (<a href=\"https://github.com/modelcontextprotocol/modelcontextprotocol/pull/1865\">SEP-1865</a>).</p><p>MCP Apps are a proposed standard inspired by <a href=\"https://mcpui.dev/\">MCP-UI</a> and <a href=\"https://developers.openai.com/apps-sdk/\">OpenAI's Apps SDK</a> to allow MCP Servers to display interactive UI elements in conversational MCP clients / chatbots.</p><p>MCP tools return text and structured data. That works for many cases, but not when you need an interactive UI, like a chart, form, or video player.</p><p>MCP Apps provide a standardized way to deliver interactive UIs from MCP servers. Your UI renders inline in the conversation, in context, in any compliant host.</p><p>MCP Apps extend the Model Context Protocol by letting tools declare UI resources:</p><ol><li> â€” Your tool declares a  resource containing its HTML interface</li><li> â€” The LLM calls the tool on your server</li><li> â€” The host fetches the resource and displays it in a sandboxed iframe</li><li><strong>Bidirectional communication</strong> â€” The host passes tool data to the UI via notifications, and the UI can call other tools through the host</li></ol><p>This SDK serves two audiences:</p><p>Build interactive UIs that run inside MCP-enabled chat clients.</p><ul><li>: <code>@modelcontextprotocol/ext-apps</code> â€” <a href=\"https://modelcontextprotocol.github.io/ext-apps/api/modules/app.html\">API Docs</a></li><li>: <code>@modelcontextprotocol/ext-apps/react</code> â€” <a href=\"https://modelcontextprotocol.github.io/ext-apps/api/modules/_modelcontextprotocol_ext-apps_react.html\">API Docs</a></li></ul><p>Embed and communicate with MCP Apps in your chat application.</p><ul><li>: <code>@modelcontextprotocol/ext-apps/app-bridge</code> â€” <a href=\"https://modelcontextprotocol.github.io/ext-apps/api/modules/app-bridge.html\">API Docs</a></li></ul><p>The <a href=\"https://github.com/idosal/mcp-ui\">MCP-UI</a> client SDK offers a fully-featured MCP Apps framework used by a few hosts. Clients may choose to use it or roll their own implementation.</p><pre><code>npm install -S @modelcontextprotocol/ext-apps\n</code></pre><p>This repository provides two <a href=\"https://agentskills.io/\">Agent Skills</a> for building MCP Apps. You can install the skills as a Claude Code plugin:</p><pre><code>/plugin marketplace add modelcontextprotocol/ext-apps\n/plugin install mcp-apps@modelcontextprotocol-ext-apps\n</code></pre><p>For more information, including instructions for installing the skills in your favorite AI coding agent, see the <a href=\"https://raw.githubusercontent.com/modelcontextprotocol/ext-apps/main/docs/agent-skills.md\">agent skills guide</a>.</p><p>The <a href=\"https://github.com/modelcontextprotocol/ext-apps/tree/main/examples\"></a> directory contains demo apps showcasing real-world use cases.</p><p>To run all examples locally using <a href=\"https://github.com/modelcontextprotocol/ext-apps/tree/main/examples/basic-host\">basic-host</a> (the reference host implementation included in this repo):</p><pre><code>git clone https://github.com/modelcontextprotocol/ext-apps.git\ncd ext-apps\nnpm install\nnpm start\n</code></pre><p>To use these examples with MCP clients that support the stdio transport (such as Claude Desktop or VS Code), add this MCP server configuration to your client's settings:</p><blockquote><p>[!NOTE] The  server requires cloning the repository first. See <a href=\"https://github.com/modelcontextprotocol/ext-apps/tree/main/examples/qr-server\">qr-server README</a> for details.</p></blockquote><p>To test local modifications with MCP clients, first clone and install the repository:</p><pre><code>git clone https://github.com/modelcontextprotocol/ext-apps.git\ncd ext-apps\nnpm install\n</code></pre><p>Then configure your MCP client to build and run the local server. Replace  with your actual clone path:</p><p>This configuration rebuilds each server on launch, ensuring your local changes are picked up.</p>","contentLength":2815,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"asgeirtj/system_prompts_leaks","url":"https://github.com/asgeirtj/system_prompts_leaks","date":1769655493,"author":"","guid":425681,"unread":true,"content":"<p>Collection of extracted System Prompts from popular chatbots like ChatGPT, Claude &amp; Gemini</p><p>Collection of system prompts/system messages/developer messages.</p><p>Feel free to do Pull Requests</p>","contentLength":183,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kubernetes/ingress-nginx","url":"https://github.com/kubernetes/ingress-nginx","date":1769655493,"author":"","guid":425682,"unread":true,"content":"<p>Ingress NGINX Controller for Kubernetes</p><ul><li>Best-effort maintenance will continue until March 2026.</li><li>Afterward, there will be no further releases, no bugfixes, and no updates to resolve any security vulnerabilities that may be discovered.</li><li>Existing deployments of Ingress NGINX will not be broken. \n  <ul><li>Existing project artifacts such as Helm charts and container images will remain available.</li></ul></li></ul><p>ingress-nginx was an Ingress controller for Kubernetes using <a href=\"https://www.nginx.org/\">NGINX</a> as a reverse proxy and load balancer.</p><p>If you are not already using ingress-nginx, you should not be deploying it as it is <a href=\"https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/#retiring\">not being developed</a>. Instead you should identify a <a href=\"https://gateway-api.sigs.k8s.io/guides/\">Gateway API</a> implementation and use it.</p><p>Do not use in multi-tenant Kubernetes production installations. This project assumes that users that can create Ingress objects are administrators of the cluster. See the <a href=\"https://kubernetes.github.io/ingress-nginx/faq/#faq\">FAQ</a> for more.</p><p>Supported versions for the ingress-nginx project mean that we have completed E2E tests, and they are passing for the versions listed. Ingress-Nginx versions  work on older versions, but the project does not make that guarantee.</p><table><thead><tr></tr></thead><tbody><tr><td>1.34, 1.33, 1.32, 1.31, 1.30</td></tr><tr><td>1.34, 1.33, 1.32, 1.31, 1.30</td></tr><tr><td>1.34, 1.33, 1.32, 1.31, 1.30</td></tr><tr><td>1.33, 1.32, 1.31, 1.30, 1.29</td></tr><tr><td>1.33, 1.32, 1.31, 1.30, 1.29</td></tr><tr><td>1.33, 1.32, 1.31, 1.30, 1.29</td></tr><tr><td>1.33, 1.32, 1.31, 1.30, 1.29</td></tr><tr><td>1.33, 1.32, 1.31, 1.30, 1.29</td></tr><tr><td>1.33, 1.32, 1.31, 1.30, 1.29</td></tr><tr><td>1.33, 1.32, 1.31, 1.30, 1.29</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.32, 1.31, 1.30, 1.29, 1.28</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr><td>1.30, 1.29, 1.28, 1.27, 1.26</td></tr><tr></tr><tr><td>1.29, 1.28, 1.27, 1.26, 1.25</td></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>1.24, 1.23, 1.22, 1.21, 1.20</td></tr></tbody></table><p>Thanks for taking the time to join our community and start contributing!</p><ul><li><p>: Documentation contributions are welcome.</p><ul><li>Read <a href=\"https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/CONTRIBUTING.md\"></a> for information about the workflow that we expect and instructions on the developer certificate of origin that we require.</li><li>Submit GitHub issues for documentation problems. \n    <ul><li>Please make sure to read the <a href=\"https://github.com/kubernetes/ingress-nginx/raw/main/CONTRIBUTING.md#issue-reporting-guidelines\">Issue Reporting Checklist</a> before opening an issue. Issues not conforming to the guidelines <strong>may be closed immediately</strong>.</li></ul></li></ul></li></ul>","contentLength":2548,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shubhamsaboo/awesome-llm-apps","url":"https://github.com/Shubhamsaboo/awesome-llm-apps","date":1769568343,"author":"","guid":424109,"unread":true,"content":"<p>Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p><p>A curated collection of <strong>Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.</strong> This repository features LLM apps that use models from <img src=\"https://cdn.simpleicons.org/openai\" alt=\"openai logo\" width=\"25\" height=\"15\"> , <img src=\"https://cdn.simpleicons.org/anthropic\" alt=\"anthropic logo\" width=\"25\" height=\"15\">, <img src=\"https://cdn.simpleicons.org/googlegemini\" alt=\"google logo\" width=\"25\" height=\"18\">, <img src=\"https://cdn.simpleicons.org/x\" alt=\"X logo\" width=\"25\" height=\"15\"> and open-source models like <img src=\"https://cdn.simpleicons.org/alibabacloud\" alt=\"alibaba logo\" width=\"25\" height=\"15\"> or <img src=\"https://cdn.simpleicons.org/meta\" alt=\"meta logo\" width=\"25\" height=\"15\"> that you can run locally on your computer.</p><ul><li>ğŸ’¡ Discover practical and creative ways LLMs can be applied across different domains, from code repositories to email inboxes and more.</li><li>ğŸ”¥ Explore apps that combine LLMs from OpenAI, Anthropic, Gemini, and open-source alternatives with AI Agents, Agent Teams, MCP &amp; RAG.</li><li>ğŸ“ Learn from well-documented projects and contribute to the growing open-source ecosystem of LLM-powered applications.</li></ul><h3>ğŸ® Autonomous Game Playing Agents</h3><h3>ğŸ“€ RAG (Retrieval Augmented Generation)</h3><h3>ğŸ’¾ LLM Apps with Memory Tutorials</h3><h3>ğŸ”§ LLM Fine-tuning Tutorials</h3><h3>ğŸ§‘â€ğŸ« AI Agent Framework Crash Course</h3><ul><li>Starter agent; modelâ€‘agnostic (OpenAI, Claude)</li><li>Structured outputs (Pydantic)</li><li>Tools: builtâ€‘in, function, thirdâ€‘party, MCP tools</li><li>Memory; callbacks; Plugins</li><li>Simple multiâ€‘agent; Multiâ€‘agent patterns</li></ul><ul><li>Starter agent; function calling; structured outputs</li><li>Tools: builtâ€‘in, function, thirdâ€‘party integrations</li><li>Memory; callbacks; evaluation</li><li>Multiâ€‘agent patterns; agent handoffs</li><li>Swarm orchestration; routing logic</li></ul><ol><li><pre><code>git clone https://github.com/Shubhamsaboo/awesome-llm-apps.git \n</code></pre></li><li><p><strong>Navigate to the desired project directory</strong></p><pre><code>cd awesome-llm-apps/starter_ai_agents/ai_travel_agent\n</code></pre></li><li><p><strong>Install the required dependencies</strong></p><pre><code>pip install -r requirements.txt\n</code></pre></li><li><p><strong>Follow the project-specific instructions</strong> in each project's  file to set up and run the app.</p></li></ol><h3><img src=\"https://cdn.simpleicons.org/github\" alt=\"github logo\" width=\"25\" height=\"20\"> Thank You, Community, for the Support! ğŸ™</h3><p>ğŸŒŸ <strong>Donâ€™t miss out on future updates! Star the repo now and be the first to know about new and exciting LLM apps with RAG and AI Agents.</strong></p>","contentLength":1845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"hashicorp/vault","url":"https://github.com/hashicorp/vault","date":1769568343,"author":"","guid":424110,"unread":true,"content":"<p>A tool for secrets management, encryption as a service, and privileged access management</p><p>: We take Vault's security and our users' trust very seriously. If you believe you have found a security issue in Vault, <em>please responsibly disclose</em> by contacting us at <a href=\"mailto:security@hashicorp.com\">security@hashicorp.com</a>.</p><img width=\"300\" alt=\"Vault Logo\" src=\"https://github.com/hashicorp/vault/raw/f22d202cde2018f9455dec755118a9b84586e082/Vault_PrimaryLogo_Black.png\"><p>Vault is a tool for securely accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, and more. Vault provides a unified interface to any secret, while providing tight access control and recording a detailed audit log.</p><p>A modern system requires access to a multitude of secrets: database credentials, API keys for external services, credentials for service-oriented architecture communication, etc. Understanding who is accessing what secrets is already very difficult and platform-specific. Adding on key rolling, secure storage, and detailed audit logs is almost impossible without a custom solution. This is where Vault steps in.</p><p>The key features of Vault are:</p><ul><li><p>: Vault can store arbitrary key/value pairs. Vault encrypts data before writing it to persistent storage, so gaining access to the raw storage isn't enough to access your secrets. Vault can write to disk, <a href=\"https://www.consul.io\">Consul</a>, and more.</p></li><li><p>: Vault can generate secrets on-demand for some systems, such as AWS or SQL databases. For example, when an application needs to access an S3 bucket, it asks Vault for credentials, and Vault will generate an AWS keypair with valid permissions on demand. After creating these dynamic secrets, Vault will also automatically revoke them after the lease is up.</p></li><li><p>: Vault can encrypt and decrypt data without storing it. This allows security teams to define encryption parameters and developers to store encrypted data in a location such as a SQL database without having to design their own encryption methods.</p></li><li><p>: Vault associates a  with each secret. At the end of the lease, Vault automatically revokes the secret. Clients are able to renew leases via built-in renew APIs.</p></li><li><p>: Vault has built-in support for secret revocation. Vault can revoke not only single secrets, but a tree of secrets, for example, all secrets read by a specific user, or all secrets of a particular type. Revocation assists in key rolling as well as locking down systems in the case of an intrusion.</p></li></ul><h2>Documentation, Getting Started, and Certification Exams</h2><p>If you're new to Vault and want to get started with security automation, please check out our <a href=\"https://learn.hashicorp.com/collections/vault/getting-started\">Getting Started guides</a> on HashiCorp's learning platform. There are also <a href=\"https://learn.hashicorp.com/vault\">additional guides</a> to continue your learning.</p><p>For examples of how to interact with Vault from inside your application in different programming languages, see the <a href=\"https://github.com/hashicorp/vault-examples\">vault-examples</a> repo. An out-of-the-box <a href=\"https://github.com/hashicorp/hello-vault-go\">sample application</a> is also available.</p><p>Show off your Vault knowledge by passing a certification exam. Visit the <a href=\"https://www.hashicorp.com/certification/#hashicorp-certified-vault-associate\">certification page</a> for information about exams and find <a href=\"https://learn.hashicorp.com/collections/vault/certification\">study materials</a> on HashiCorp's learning platform.</p><p>If you wish to work on Vault itself or any of its built-in systems, you'll first need <a href=\"https://www.golang.org\">Go</a> installed on your machine.</p><p>For local dev first make sure Go is properly installed, including setting up a <a href=\"https://golang.org/doc/code.html#GOPATH\">GOPATH</a>, then setting the <a href=\"https://pkg.go.dev/cmd/go#hdr-Environment_variables\">GOBIN</a> variable to . Ensure that  is in your path as some distributions bundle the old version of build tools.</p><p>Next, clone this repository. Vault uses <a href=\"https://github.com/golang/go/wiki/Modules\">Go Modules</a>, so it is recommended that you clone the repository  of the GOPATH. You can then download any required build tools by bootstrapping your environment:</p><p>To compile a development version of Vault, run  or . This will put the Vault binary in the  and  folders:</p><pre><code>$ make dev\n...\n$ bin/vault\n...\n</code></pre><p>To compile a development version of Vault with the UI, run . This will put the Vault binary in the  and  folders:</p><pre><code>$ make static-dist dev-ui\n...\n$ bin/vault\n...\n</code></pre><p>To run tests, type . Note: this requires Docker to be installed. If this exits with exit status 0, then everything is working!</p><p>If you're developing a specific package, you can run tests for just that package by specifying the  variable. For example below, only  package tests will be run.</p><pre><code>$ make test TEST=./vault\n...\n</code></pre><p>If you encounter an error like <code>could not read Username for 'https://github.com'</code> you may need to adjust your git config like so:</p><pre><code>$ git config --global --add url.\"git@github.com:\".insteadOf \"https://github.com/\"\n</code></pre><p>This repository publishes two libraries that may be imported by other projects: <code>github.com/hashicorp/vault/api</code> and <code>github.com/hashicorp/vault/sdk</code>.</p><p>Note that this repository also contains Vault (the product), and as with most Go projects, Vault uses Go modules to manage its dependencies. The mechanism to do that is the <a href=\"https://raw.githubusercontent.com/hashicorp/vault/main/go.mod\">go.mod</a> file. As it happens, the presence of that file also makes it theoretically possible to import Vault as a dependency into other projects. Some other projects have made a practice of doing so in order to take advantage of testing tooling that was developed for testing Vault itself. This is not, and has never been, a supported way to use the Vault project. We aren't likely to fix bugs relating to failure to import <code>github.com/hashicorp/vault</code> into your project.</p><p>See also the section \"Docker-based tests\" below.</p><p>Vault has comprehensive <a href=\"https://en.wikipedia.org/wiki/Acceptance_testing\">acceptance tests</a> covering most of the features of the secret and auth methods.</p><p>If you're working on a feature of a secret or auth method and want to verify it is functioning (and also hasn't broken anything else), we recommend running the acceptance tests.</p><p> The acceptance tests create/destroy/modify , which may incur real costs in some cases. In the presence of a bug, it is technically possible that broken backends could leave dangling data behind. Therefore, please run the acceptance tests at your own risk. At the very least, we recommend running them in their own private account for whatever backend you're testing.</p><p>To run the acceptance tests, invoke :</p><pre><code>$ make testacc TEST=./builtin/logical/consul\n...\n</code></pre><p>The  variable is required, and you should specify the folder where the backend is. The  variable is recommended to filter down to a specific resource to test, since testing all of them at once can sometimes take a very long time.</p><p>Acceptance tests typically require other environment variables to be set for things such as access keys. The test itself should error early and tell you what to set, so it is not documented here.</p><p>We have created an experimental new testing mechanism inspired by NewTestCluster. An example of how to use it:</p><pre><code>import (\n  \"testing\"\n  \"github.com/hashicorp/vault/sdk/helper/testcluster/docker\"\n)\n\nfunc Test_Something_With_Docker(t *testing.T) {\n  opts := &amp;docker.DockerClusterOptions{\n    ImageRepo: \"hashicorp/vault\", // or \"hashicorp/vault-enterprise\"\n    ImageTag:    \"latest\",\n  }\n  cluster := docker.NewTestDockerCluster(t, opts)\n  defer cluster.Cleanup()\n  \n  client := cluster.Nodes()[0].APIClient()\n  _, err := client.Logical().Read(\"sys/storage/raft/configuration\")\n  if err != nil {\n    t.Fatal(err)\n  }\n}\n</code></pre><pre><code>import (\n  \"testing\"\n  \"github.com/hashicorp/vault/sdk/helper/testcluster/docker\"\n)\n\nfunc Test_Something_With_Docker(t *testing.T) {\n  opts := &amp;docker.DockerClusterOptions{\n    ImageRepo: \"hashicorp/vault-enterprise\",\n    ImageTag:  \"latest\",\n\tVaultLicense: licenseString, // not a path, the actual license bytes\n  }\n  cluster := docker.NewTestDockerCluster(t, opts)\n  defer cluster.Cleanup()\n}\n</code></pre><p>Here is a more realistic example of how we use it in practice. DefaultOptions uses : as the repo and tag, but it also looks at the environment variable VAULT_BINARY. If populated, it will copy the local file referenced by VAULT_BINARY into the container. This is useful when testing local changes.</p><p>Instead of setting the VaultLicense option, you can set the VAULT_LICENSE_CI environment variable, which is better than committing a license to version control.</p><p>Optionally you can set COMMIT_SHA, which will be appended to the image name we build as a debugging convenience.</p><pre><code>func Test_Custom_Build_With_Docker(t *testing.T) {\n  opts := docker.DefaultOptions(t)\n  cluster := docker.NewTestDockerCluster(t, opts)\n  defer cluster.Cleanup()\n}\n</code></pre><p>There are a variety of helpers in the <code>github.com/hashicorp/vault/sdk/helper/testcluster</code> package, e.g. these tests below will create a pair of 3-node clusters and link them using PR or DR replication respectively, and fail if the replication state doesn't become healthy before the passed context expires.</p><p>Again, as written, these depend on having a Vault Enterprise binary locally and the env var VAULT_BINARY set to point to it, as well as having VAULT_LICENSE_CI set.</p><pre><code>func TestStandardPerfReplication_Docker(t *testing.T) {\n  opts := docker.DefaultOptions(t)\n  r, err := docker.NewReplicationSetDocker(t, opts)\n  if err != nil {\n      t.Fatal(err)\n  }\n  defer r.Cleanup()\n\n  ctx, cancel := context.WithTimeout(context.Background(), time.Minute)\n  defer cancel()\n  err = r.StandardPerfReplication(ctx)\n  if err != nil {\n    t.Fatal(err)\n  }\n}\n\nfunc TestStandardDRReplication_Docker(t *testing.T) {\n  opts := docker.DefaultOptions(t)\n  r, err := docker.NewReplicationSetDocker(t, opts)\n  if err != nil {\n    t.Fatal(err)\n  }\n  defer r.Cleanup()\n\n  ctx, cancel := context.WithTimeout(context.Background(), time.Minute)\n  defer cancel()\n  err = r.StandardDRReplication(ctx)\n  if err != nil {\n    t.Fatal(err)\n  }\n}\n</code></pre><p>Finally, here's an example of running an existing OSS docker test with a custom binary:</p><pre><code>$ GOOS=linux make dev\n$ VAULT_BINARY=$(pwd)/bin/vault go test -run 'TestRaft_Configuration_Docker' ./vault/external_tests/raft/raft_binary\nok      github.com/hashicorp/vault/vault/external_tests/raft/raft_binary        20.960s\n</code></pre>","contentLength":9534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"badlogic/pi-mono","url":"https://github.com/badlogic/pi-mono","date":1769568343,"author":"","guid":424111,"unread":true,"content":"<p>AI agent toolkit: coding agent CLI, unified LLM API, TUI &amp; web UI libraries, Slack bot, vLLM pods</p><p>Tools for building AI agents and managing LLM deployments.</p><pre><code>npm install          # Install all dependencies\nnpm run build        # Build all packages\nnpm run check        # Lint, format, and type check\n./test.sh            # Run tests (skips LLM-dependent tests without API keys)\n./pi-test.sh         # Run pi from sources (must be run from repo root)\n</code></pre><blockquote><p> requires  to be run first. The web-ui package uses  which needs compiled  files from dependencies.</p></blockquote>","contentLength":546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Free-TV/IPTV","url":"https://github.com/Free-TV/IPTV","date":1769568343,"author":"","guid":424112,"unread":true,"content":"<p>M3U Playlist for free TV channels</p><p>This is an M3U playlist for free TV channels around the World.</p><p>Either free locally (over the air):</p><ul><li>Pluto TV (English, Spanish, French, Italian)</li></ul><p>The main goals for this playlist are listed below.</p><p>The less channels we support the better.</p><ul><li>All channels should work well.</li><li>As much as possible channels should be in HD, not SD.</li><li>Only one URL per channel (no +1, no alternate feeds, no regional declinations)</li></ul><p>If a channel is normally only available via commercial subscriptions it has nothing to do in this playlist. If on the other hand it is provided for free to everybody in a particular country, then it should be in this playlist.</p><ul><li>Only channels which are officially provided for free (via DVB-S, DVB-T, analog, etc..)</li></ul><p>This is a playlist for everybody.</p><ul><li>No channels dedicated to any particular religion</li><li>No channels dedicated to any particular political party</li><li>No channels made for a country and funded by a different country</li></ul><p>It can be quite hard to find up to date URLs, here's a list of sources:</p><p>The m3u8 playlist is generated by , using the  files located in .</p><p>Each .md file represesnts a group. The  line is used as the group title.</p><p>Only channels which URL column starts with  are included in the playlist.</p><p>Channels which are not in HD are marked with an .</p><p>Channels which use GeoIP blocking are marked with a .</p><p>Channels which are live Youtube channels are marked with a .</p><p>Only create issues for bugs and feature requests.</p><p>Do not create issues to add/edit or to remove channels. If you want to add/edit/remove channels, create a pull request directly.</p><p>If your Pull Request modifies channels, only modify .md files. Do not modify m3u8 files in your pull request.</p><p>To add a new channel, make a Pull Request.</p><ul><li>In your Pull Request you need to provide information to show that the channel is free.</li><li>Use imgur.com to host the channel logo and point to it.</li><li>If you have a valid stream, add it and put  in front of it.</li><li>If you don't have an stream for the channel, add  in the url column and place your channel in the Invalid category.</li><li>If you have a stream but it doesn't work well, put the channel in the Invalid category and put  in front of the url.</li><li>If you're adding geoblocked URLs specify it in your PR and specify which country they're working in. The PR will only be merged if these URLs can be tested.</li></ul><p>To remove a channel, make a Pull Request.</p><p>In your Pull Request you need to provide information to show that the channel is only available via a private paid subscription.</p><p>Note: Public taxes (whether national or regional, whether called TV License or not) do not constitute a private paid subscription.</p><p>If a stream is broken, simply move the channel to the invalid category and replace  with  in the url column.</p>","contentLength":2702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"k4yt3x/video2x","url":"https://github.com/k4yt3x/video2x","date":1769482087,"author":"","guid":421848,"unread":true,"content":"<p>A machine learning-based video super resolution and frame interpolation framework. Est. Hack the Valley II, 2018.</p><p>Video2X 6.0.0 highlights:</p><ul><li>Complete rewrite of the Video2X project in C/C++.</li><li>Faster and more efficient architecture.</li><li>Cross-platform support for Windows and Linux.</li><li>Vastly improved output quality.</li><li>New GUI and installer for easy setup on Windows.</li></ul><p>Your system must meet the minimum hardware requirements below to run Video2X.</p><ul><li><ul><li>The precompiled binaries require CPUs with AVX2 support.</li><li>: Haswell (Q2 2013) or newer</li><li>: Excavator (Q2 2015) or newer</li></ul></li><li><ul><li>The GPU must support Vulkan.</li><li>: Kepler (GTX 600 series, Q2 2012) or newer</li><li>: GCN 1.0 (Radeon HD 7000 series, Q1 2012) or newer</li><li>: HD Graphics 4000 (Q2 2012) or newer</li></ul></li></ul><p>You can download the latest Windows release on the <a href=\"https://github.com/k4yt3x/video2x/releases/latest\">releases page</a>. For basic GUI usage, refer to the <a href=\"https://docs.video2x.org/running/desktop.html\">documentation</a>. If you're unable to download directly from GitHub, try the <a href=\"https://files.k4yt3x.com\">mirror site</a>. The GUI currently supports the following languages:</p><ul></ul><p>Video2X packages are available for the Linux distros listed below. A universal AppImage is also available for other distros. If you'd like to build it from source code, refer to the <a href=\"https://raw.githubusercontent.com/k4yt3x/video2x/master/packaging/arch/PKGBUILD\">PKGBUILD</a> file for a general overview of the required dependencies and commands.</p><p>Video2X <a href=\"https://github.com/k4yt3x/video2x/pkgs/container/video2x\">container images</a> are available on the GitHub Container Registry for easy deployment on Linux and macOS. If you already have Docker/Podman installed, only one command is needed to start upscaling a video. For more information on how to use Video2X's Docker image, please refer to the <a href=\"https://docs.video2x.org/running/container.html\">documentation</a>.</p><p>You can use Video2X on <a href=\"https://colab.research.google.com/\">Google Colab</a> if you don't have a powerful GPU of your own. You can borrow a powerful GPU (NVIDIA T4, L4, or A100) on Google's server for free for a maximum of 12 hours per session. <strong>Please use the free resource fairly</strong> and do not create sessions back-to-back and run upscaling 24/7. This might result in you getting banned. You can get <a href=\"https://colab.research.google.com/signup/pricing\">Colab Pro/Pro+</a> if you'd like to use better GPUs and get longer runtimes. Usage instructions are embedded in the <a href=\"https://colab.research.google.com/drive/1gWEwcA9y57EsxwOjmLNmNMXPsafw0kGo\">Colab Notebook</a>.</p><p>Join our Telegram discussion group to ask any questions you have about Video2X, chat directly with the developers, or discuss super resolution, frame interpolation technologies, or the future of Video2X in general.</p><h2>ğŸ“½ï¸ Video Demos (Outdated)</h2><p>The following clip can be used to test if your setup works properly. This is also the standard clip used for running performance benchmarks.</p><p>The original clip came from the anime \"ã•ãã‚‰è˜ã®ãƒšãƒƒãƒˆãªå½¼å¥³.\" Copyright of this clip belongs to æ ªå¼ä¼šç¤¾ã‚¢ãƒ‹ãƒ—ãƒ¬ãƒƒã‚¯ã‚¹.</p><p>This project includes or depends on these following projects:</p><p>More licensing information can be found in the <a href=\"https://raw.githubusercontent.com/k4yt3x/video2x/master/NOTICE\">NOTICE</a> file.</p><p>Special thanks to the following individuals for their significant contributions to the project, listed in alphabetical order.</p>","contentLength":2760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"business-science/ai-data-science-team","url":"https://github.com/business-science/ai-data-science-team","date":1769482087,"author":"","guid":421849,"unread":true,"content":"<p>An AI-powered data science team of agents to help you perform common data science tasks 10X faster.</p><div align=\"center\"><em>AI Data Science Team + AI Pipeline Studio</em></div><p>AI Data Science Team is a Python library of specialized agents for common data science workflows, plus a flagship app: . The Studio turns your work into a visual, reproducible pipeline, while the AI team handles data loading, cleaning, visualization, and modeling.</p><p> Beta. Breaking changes may occur until 0.1.0.</p><h2>AI Pipeline Studio (Flagship App)</h2><p>AI Pipeline Studio is the main example of the AI Data Science Team in action.</p><ul><li>Pipeline-first workspace: Visual Editor, Table, Chart, EDA, Code, Model, Predictions, MLflow</li><li>Manual + AI steps with lineage and reproducible scripts</li><li>Multi-dataset handling and merge workflows</li><li>Project saves: metadata-only or full-data</li><li>Storage footprint controls and rehydrate workflows</li></ul><pre><code>streamlit run apps/ai-pipeline-studio-app/app.py\n</code></pre><p>Full app docs: <code>apps/ai-pipeline-studio-app/README.md</code></p><ul><li>OpenAI API key (or Ollama for local models)</li></ul><h3>Install the app and library</h3><p>Clone the repo and install in editable mode:</p><h3>Run the AI Pipeline Studio app</h3><pre><code>streamlit run apps/ai-pipeline-studio-app/app.py\n</code></pre><p>The repository includes both the  app and the underlying  library. The library provides agent building blocks and multi-agent workflows for:</p><ul><li>Data loading and inspection</li><li>Cleaning, wrangling, and feature engineering</li><li>Modeling and evaluation (H2O + MLflow tools)</li></ul><p>Agent examples live in . Notable agents:</p><ul><li>Feature Engineering Agent</li><li>Multi-agent workflows (e.g., Pandas Data Analyst, SQL Data Analyst)</li><li>Supervisor Agent (oversees other agents)</li><li>Custom tools for data science tasks</li></ul><p>See all apps in . Notable apps:</p><ul><li>AI Pipeline Studio: <code>apps/ai-pipeline-studio-app/</code></li><li>EDA Explorer App: <code>apps/exploratory-copilot-app/</code></li><li>Pandas Data Analyst App: <code>apps/pandas-data-analyst-app/</code></li></ul><pre><code>from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(\n    model_name=\"gpt-4.1-mini\",\n)\n</code></pre><pre><code>ollama serve\nollama pull llama3.1:8b\n</code></pre><pre><code>from langchain_ollama import ChatOllama\n\nllm = ChatOllama(\n    model=\"llama3.1:8b\",\n)\n</code></pre><h2>Next-Gen AI Agentic Workshop</h2>","contentLength":2018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"supermemoryai/supermemory","url":"https://github.com/supermemoryai/supermemory","date":1769395936,"author":"","guid":420606,"unread":true,"content":"<p>Memory engine and app that is extremely fast, scalable. The Memory API for the AI era.</p><ol><li>Start Adding Memory with your choice of format (Note, Link, File)</li></ol><ol start=\"2\"><li>You can also Connect to your favourite services (Notion, Google Drive, OneDrive)</li></ol><ol start=\"3\"><li>Once Memories are added, you can chat with Supermemory by clicking on \"Open Chat\" and retrieve info from your saved memories</li></ol><ol start=\"4\"><li>Add MCP to your AI Tools (by clicking on \"Connect to your AI\" and select the AI tool you are trying to integrate)</li></ol><ol start=\"5\"><li><p>: Install the <a href=\"https://chromewebstore.google.com/detail/supermemory/afpgkkipfdpeaflnpoaffkcankadgjfc\">Chrome/Edge extension</a> to save memories directly from any webpage, integrate with ChatGPT and Claude conversations, and import from Twitter/X. Right-click on any content or use the extension popup to save memories instantly.</p></li><li><p>: Install the <a href=\"https://www.raycast.com/supermemory/supermemory\">Raycast extension</a> to add and search memories directly from Raycast. Use the \"Add Memory\" command to quickly save content, or \"Search Memories\" to find and retrieve your saved information with keyboard shortcuts.</p></li></ol><p>Have questions or feedback? We're here to help:</p><p>We welcome contributions from developers of all skill levels! Whether you're fixing bugs, adding features, or improving documentation, your help makes supermemory better for everyone.</p><p>For detailed guidelines, development setup, coding standards, and the complete contribution workflow, please see our <a href=\"https://raw.githubusercontent.com/supermemoryai/supermemory/main/CONTRIBUTING.md\"></a>.</p><ul><li>ğŸ›  - Help us squash those pesky issues</li><li>âœ¨  - Add functionality that users will love</li><li>ğŸ¨  - Make the interface more intuitive</li><li>âš¡ <strong>Performance optimizations</strong> - Help us make supermemory faster</li></ul><p>Check out our <a href=\"https://github.com/supermemoryai/supermemory/issues\">Issues</a> page for  and  labels to get started!</p><p>Stay up to date with the latest improvements:</p>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Psiphon-Inc/conduit","url":"https://github.com/Psiphon-Inc/conduit","date":1769395936,"author":"","guid":420607,"unread":true,"content":"<p>Conduit runs inproxy from <a href=\"https://github.com/Psiphon-Labs/psiphon-tunnel-core\">psiphon-tunnel-core</a> in a mobile app. This repository targets Android, iOS and Mac (via Catalyst).</p><p>This project uses <a href=\"https://git-lfs.github.com/\">Git LFS</a> to manage large files such as the tunnel core libraries.</p><p>For information about pulling and verifying translations, see <a href=\"https://raw.githubusercontent.com/Psiphon-Inc/conduit/main/i18n/README.md\">i18n/README.md</a>.</p>","contentLength":282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"qarmin/czkawka","url":"https://github.com/qarmin/czkawka","date":1769395936,"author":"","guid":420608,"unread":true,"content":"<p>Multi functional app to find duplicates, empty folders, similar images etc.</p><p> ((IPA: [ËˆkrÉ”cÉ›t]), \"croquette\" in Polish) new generation GUI frontend, simple, multiplatform, fast and free app to remove unnecessary files from your computer.</p><p> ( (IPA: [ËˆÊ§Ì‘kafka]), \"hiccup\" in Polish) older gtk4 GUI frontend, superseded by Krokiet, but still receiving bugfix updates.</p><ul><li>Written in memory-safe Rust - almost 100% unsafe code free</li><li>Amazingly fast - due to using more or less advanced algorithms and multithreading</li><li>Free, Open Source without ads</li><li>Multiplatform - works on Linux, Windows, macOS, FreeBSD and many more</li><li>Cache support - second and further scans should be much faster than the first one</li><li>CLI frontend - for easy automation</li><li>GUI frontend - uses Slint or GTK 4 frameworks</li><li>Core library - allows to reuse functionality in other apps</li><li>No spying - Czkawka does not have access to the Internet, nor does it collect any user information or statistics</li><li>Multilingual - support multiple languages like Polish, English or Italian</li><li>Multiple tools to use: \n  <ul><li>Duplicates - Finds duplicates based on file name, size or hash</li><li>Empty Folders - Finds empty folders with the help of an advanced algorithm</li><li>Big Files - Finds the provided number of the biggest files in given location</li><li>Empty Files - Looks for empty files across the drive</li><li>Temporary Files - Finds temporary files</li><li>Similar Images - Finds images which are not exactly the same (different resolution, watermarks)</li><li>Similar Videos - Looks for visually similar videos</li><li>Same Music - Searches for similar music by tags or by reading content and comparing it</li><li>Invalid Symbolic Links - Shows symbolic links which point to non-existent files/directories</li><li>Broken Files - Finds files that are invalid or corrupted</li><li>Bad Extensions - Lists files whose content not match with their extension</li><li>Exif Remover - Removes Exif metadata from various file types</li><li>Video Optimizer - Crops from static parts and converts videos to more efficient formats</li><li>Bad Names - Finds files with names that may be not wanted (e.g., containing special characters)</li></ul></li></ul><h2>Usage, installation, compilation, requirements, license</h2><p>Each tool uses different technologies, so you can find instructions for each of them in the appropriate file:</p><h2>Comparison to other tools</h2><p>Bleachbit is a master at finding and removing temporary files, while Czkawka only finds the most basic ones. So these two apps shouldn't be compared directly or be considered as an alternative to one another.</p><p>In this comparison remember, that even if app have same features they may work different(e.g. one app may have more options to choose than other).</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td align=\"center\">Music duplicates(content)</td></tr><tr></tr><tr></tr></tbody></table><p> Few small commits added recently and last version released in 2023</p><p> Czkawka GTK is in maintenance mode receiving only bugfixes</p><p>There are many similar applications to Czkawka on the Internet, which do some things better and some things worse:</p><ul><li><a href=\"https://github.com/arsenetar/dupeguru\">DupeGuru</a> - Many options to customize; great photo compare tool</li><li><a href=\"https://github.com/pixelb/fslint\">FSlint</a> - A little outdated, but still have some tools not available in Czkawka</li></ul><p>Due to limited time, the biggest emphasis is on the GUI version so if you are looking for really good and feature-packed console apps, then take a look at these:</p><ul><li><a href=\"https://github.com/pkolaczk/fclones\">Fclones</a> - One of the fastest tools to find duplicates; it is written also in Rust</li><li><a href=\"https://github.com/sahib/rmlint\">Rmlint</a> - Nice console interface and also is feature packed</li><li><a href=\"https://github.com/pauldreik/rdfind\">RdFind</a> - Fast, but written in C++ Â¯\\_(ãƒ„)_/Â¯</li></ul><p>Czkawka exposes its common functionality through a crate called , which can be reused by other projects.</p><p>It is written in Rust and is used by all Czkawka frontends (, , ).</p><p>It is also used by external projects, such as:</p><p>Bindings are also available for:</p><p>Some projects work as wrappers around . Without directly depending on , they allow simple scanning and retrieving results in JSON format:</p><p>Big thanks to PÃ¡draig Brady, creator of fantastic FSlint, because without his work I wouldn't create this tool.</p><p>Thanks also to all the people who create patches for this program, create and fix translations, make it available on other systems, create videos, articles about it etc.</p><p>Also, I really appreciate work of people that create crates on which Czkawka is based and for that I try to report bugs to make it even better.</p><h2>Officially Supported Projects</h2><p>Czkawka does not have an official website, so do not trust any sites that claim to be the official one.</p><p>If you use packages from unofficial sources, make sure they are safe.</p><p>The entire code in this repository is licensed under the <a href=\"https://mit-license.org/\">MIT</a> license.</p><p>All images are licensed under the <a href=\"https://creativecommons.org/licenses/by/4.0/\">CC BY 4.0</a> license.</p><p>The Czkawka GTK GUI and CLI applications are licensed under the <a href=\"https://mit-license.org/\">MIT</a> license, while the Krokiet is licensed under the <a href=\"https://www.gnu.org/licenses/gpl-3.0.en.html\">GPL-3.0-only</a> license.</p><p>If you are using the app, I would appreciate a donation for its further development, which can be done <a href=\"https://github.com/sponsors/qarmin\">here</a>.</p>","contentLength":4700,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"openai/codex","url":"https://github.com/openai/codex","date":1769395936,"author":"","guid":420609,"unread":true,"content":"<p>Lightweight coding agent that runs in your terminal</p><p align=\"center\">or <code>brew install --cask codex</code></p><p align=\"center\"> is a coding agent from OpenAI that runs locally on your computer. </p> If you want Codex in your code editor (VS Code, Cursor, Windsurf), \n<a href=\"https://developers.openai.com/codex/ide\">install in your IDE.</a>If you are looking for the \n from OpenAI, \n, go to \n<a href=\"https://chatgpt.com/codex\">chatgpt.com/codex</a>.\n<h3>Installing and running Codex CLI</h3><p>Install globally with your preferred package manager:</p><pre><code># Install using npm\nnpm install -g @openai/codex\n</code></pre><pre><code># Install using Homebrew\nbrew install --cask codex\n</code></pre><p>Then simply run  to get started.</p><h3>Using Codex with your ChatGPT plan</h3>","contentLength":557,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI4Finance-Foundation/FinRobot","url":"https://github.com/AI4Finance-Foundation/FinRobot","date":1769309495,"author":"","guid":420185,"unread":true,"content":"<p>FinRobot: An Open-Source AI Agent Platform for Financial Analysis using LLMs ğŸš€ ğŸš€ ğŸš€</p><p> is an AI Agent Platform that transcends the scope of FinGPT, representing a comprehensive solution meticulously designed for financial applications. It integrates <strong>a diverse array of AI technologies</strong>, extending beyond mere language models. This expansive vision highlights the platform's versatility and adaptability, addressing the multifaceted needs of the financial industry.</p><p>: an AI Agent is an intelligent entity that uses large language models as its brain to perceive its environment, make decisions, and execute actions. Unlike traditional artificial intelligence, AI Agents possess the ability to independently think and utilize tools to progressively achieve given objectives.</p><p><a href=\"https://finrobot.ai/\">FinRobot Pro</a> is an AI-powered equity research platform that automates professional stock analysis using Large Language Models (LLMs) and AI Agents.</p><ul><li><strong>Automated Report Generation</strong> â€“ Generate professional equity research reports instantly</li><li> â€“ Deep dive into income statements, balance sheets, and cash flows</li><li> â€“ P/E ratio, EV/EBITDA multiples, and peer comparison</li><li> â€“ Comprehensive investment risk evaluation</li></ul><h3>The overall framework of FinRobot is organized into four distinct layers, each designed to address specific aspects of financial AI processing and application:</h3><ol><li><strong>Financial AI Agents Layer</strong>: The Financial AI Agents Layer now includes Financial Chain-of-Thought (CoT) prompting, enhancing complex analysis and decision-making capacity. Market Forecasting Agents, Document Analysis Agents, and Trading Strategies Agents utilize CoT to dissect financial challenges into logical steps, aligning their advanced algorithms and domain expertise with the evolving dynamics of financial markets for precise, actionable insights.</li><li><strong>Financial LLMs Algorithms Layer</strong>: The Financial LLMs Algorithms Layer configures and utilizes specially tuned models tailored to specific domains and global market analysis.</li><li><strong>LLMOps and DataOps Layers</strong>: The LLMOps layer implements a multi-source integration strategy that selects the most suitable LLMs for specific financial tasks, utilizing a range of state-of-the-art models.</li><li><strong>Multi-source LLM Foundation Models Layer</strong>: This foundational layer supports the plug-and-play functionality of various general and specialized LLMs.</li></ol><ol><li><p>: This module captures and interprets multimodal financial data from market feeds, news, and economic indicators, using sophisticated techniques to structure the data for thorough analysis.</p></li><li><p>: Acting as the core processing unit, this module perceives data from the Perception module with LLMs and utilizes Financial Chain-of-Thought (CoT) processes to generate structured instructions.</p></li><li><p>: This module executes instructions from the Brain module, applying tools to translate analytical insights into actionable outcomes. Actions include trading, portfolio adjustments, generating reports, or sending alerts, thereby actively influencing the financial environment.</p></li></ol><h2>FinRobot: Smart Scheduler</h2><p>The Smart Scheduler is central to ensuring model diversity and optimizing the integration and selection of the most appropriate LLM for each task.</p><ul><li>: This component orchestrates the task assignment process, ensuring that tasks are allocated to agents based on their performance metrics and suitability for specific tasks.</li><li>: Manages the registration and tracks the availability of agents within the system, facilitating an efficient task allocation process.</li><li>: Tailor agent functionalities to specific tasks, enhancing their performance and integration within the overall system.</li><li>: Manages and stores different general and fine-tuned LLMs-based agents tailored for various financial tasks, updated periodically to ensure relevance and efficacy.</li></ul><p>The main folder  has three subfolders <strong>agents, data_source, functional</strong>.</p><pre><code>FinRobot\nâ”œâ”€â”€ finrobot (main folder)\nâ”‚   â”œâ”€â”€ agents\nâ”‚   \tâ”œâ”€â”€ agent_library.py\nâ”‚   \tâ””â”€â”€ workflow.py\nâ”‚   â”œâ”€â”€ data_source\nâ”‚   \tâ”œâ”€â”€ finnhub_utils.py\nâ”‚   \tâ”œâ”€â”€ finnlp_utils.py\nâ”‚   \tâ”œâ”€â”€ fmp_utils.py\nâ”‚   \tâ”œâ”€â”€ sec_utils.py\nâ”‚   \tâ””â”€â”€ yfinance_utils.py\nâ”‚   â”œâ”€â”€ functional\nâ”‚   \tâ”œâ”€â”€ analyzer.py\nâ”‚   \tâ”œâ”€â”€ charting.py\nâ”‚   \tâ”œâ”€â”€ coding.py\nâ”‚   \tâ”œâ”€â”€ quantitative.py\nâ”‚   \tâ”œâ”€â”€ reportlab.py\nâ”‚   \tâ””â”€â”€ text.py\nâ”‚   â”œâ”€â”€ toolkits.py\nâ”‚   â””â”€â”€ utils.py\nâ”‚\nâ”œâ”€â”€ configs\nâ”œâ”€â”€ experiments\nâ”œâ”€â”€ tutorials_beginner (hands-on tutorial)\nâ”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb\nâ”‚   â””â”€â”€ agent_annual_report.ipynb \nâ”œâ”€â”€ tutorials_advanced (advanced tutorials for potential finrobot developers)\nâ”‚   â”œâ”€â”€ agent_trade_strategist.ipynb\nâ”‚   â”œâ”€â”€ agent_fingpt_forecaster.ipynb\nâ”‚   â”œâ”€â”€ agent_annual_report.ipynb \nâ”‚   â”œâ”€â”€ lmm_agent_mplfinance.ipynb\nâ”‚   â””â”€â”€ lmm_agent_opt_smacross.ipynb\nâ”œâ”€â”€ setup.py\nâ”œâ”€â”€ OAI_CONFIG_LIST_sample\nâ”œâ”€â”€ config_api_keys_sample\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ README.md\n</code></pre><p><strong>1. (Recommended) Create a new virtual environment</strong></p><pre><code>conda create --name finrobot python=3.10\nconda activate finrobot\n</code></pre><p><strong>2. download the FinRobot repo use terminal or download it manually</strong></p><pre><code>git clone https://github.com/AI4Finance-Foundation/FinRobot.git\ncd FinRobot\n</code></pre><p><strong>3. install finrobot &amp; dependencies from source or pypi</strong></p><p>get our latest release from pypi</p><p>or install from this repo directly</p><p><strong>4. modify OAI_CONFIG_LIST_sample file</strong></p><pre><code>1) rename OAI_CONFIG_LIST_sample to OAI_CONFIG_LIST\n2) remove the four lines of comment within the OAI_CONFIG_LIST file\n3) add your own openai api-key &lt;your OpenAI API key here&gt;\n</code></pre><p><strong>5. modify config_api_keys_sample file</strong></p><pre><code>1) rename config_api_keys_sample to config_api_keys\n2) remove the comment within the config_api_keys file\n3) add your own finnhub-api \"YOUR_FINNHUB_API_KEY\"\n4) add your own financialmodelingprep and sec-api keys \"YOUR_FMP_API_KEY\" and \"YOUR_SEC_API_KEY\" (for financial report generation)\n</code></pre><p><strong>6. start navigating the tutorials or the demos below:</strong></p><pre><code># find these notebooks in tutorials\n1) agent_annual_report.ipynb\n2) agent_fingpt_forecaster.ipynb\n3) agent_trade_strategist.ipynb\n4) lmm_agent_mplfinance.ipynb\n5) lmm_agent_opt_smacross.ipynb\n</code></pre><h3>1. Market Forecaster Agent (Predict Stock Movements Direction)</h3><p>Takes a company's ticker symbol, recent basic financials, and market news as input and predicts its stock movements.</p><pre><code>import autogen\nfrom finrobot.utils import get_current_date, register_keys_from_json\nfrom finrobot.agents.workflow import SingleAssistant\n</code></pre><pre><code># Read OpenAI API keys from a JSON file\nllm_config = {\n    \"config_list\": autogen.config_list_from_json(\n        \"../OAI_CONFIG_LIST\",\n        filter_dict={\"model\": [\"gpt-4-0125-preview\"]},\n    ),\n    \"timeout\": 120,\n    \"temperature\": 0,\n}\n\n# Register FINNHUB API keys\nregister_keys_from_json(\"../config_api_keys\")\n</code></pre><pre><code>company = \"NVDA\"\n\nassitant = SingleAssistant(\n    \"Market_Analyst\",\n    llm_config,\n    # set to \"ALWAYS\" if you want to chat instead of simply receiving the prediciton\n    human_input_mode=\"NEVER\",\n)\nassitant.chat(\n    f\"Use all the tools provided to retrieve information available for {company} upon {get_current_date()}. Analyze the positive developments and potential concerns of {company} \"\n    \"with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. \"\n    f\"Then make a rough prediction (e.g. up/down by 2-3%) of the {company} stock price movement for next week. Provide a summary analysis to support your prediction.\"\n)\n</code></pre><h3>2. Financial Analyst Agent for Report Writing (Equity Research Report)</h3><p>Take a company's 10-k form, financial data, and market data as input and output an equity research report</p><pre><code>import os\nimport autogen\nfrom textwrap import dedent\nfrom finrobot.utils import register_keys_from_json\nfrom finrobot.agents.workflow import SingleAssistantShadow\n</code></pre><pre><code>llm_config = {\n    \"config_list\": autogen.config_list_from_json(\n        \"../OAI_CONFIG_LIST\",\n        filter_dict={\n            \"model\": [\"gpt-4-0125-preview\"],\n        },\n    ),\n    \"timeout\": 120,\n    \"temperature\": 0.5,\n}\nregister_keys_from_json(\"../config_api_keys\")\n\n# Intermediate strategy modules will be saved in this directory\nwork_dir = \"../report\"\nos.makedirs(work_dir, exist_ok=True)\n\nassistant = SingleAssistantShadow(\n    \"Expert_Investor\",\n    llm_config,\n    max_consecutive_auto_reply=None,\n    human_input_mode=\"TERMINATE\",\n)\n\n</code></pre><pre><code>company = \"Microsoft\"\nfyear = \"2023\"\n\nmessage = dedent(\n    f\"\"\"\n    With the tools you've been provided, write an annual report based on {company}'s {fyear} 10-k report, format it into a pdf.\n    Pay attention to the followings:\n    - Explicitly explain your working plan before you kick off.\n    - Use tools one by one for clarity, especially when asking for instructions. \n    - All your file operations should be done in \"{work_dir}\". \n    - Display any image in the chat once generated.\n    - All the paragraphs should combine between 400 and 450 words, don't generate the pdf until this is explicitly fulfilled.\n\"\"\"\n)\n\nassistant.chat(message, use_cache=True, max_turns=50,\n               summary_method=\"last_msg\")\n</code></pre><ol><li>: 10-K report, market data, financial ratios</li><li><strong>Analyze Financial Statements</strong>: balance sheet, income statement, cash flow</li><li><strong>Company Overview and Performance</strong>: company description, business highlights, segment analysis</li><li>: assess risks</li><li><strong>Financial Performance Visualization</strong>: plot PE ratio and EPS</li><li><strong>Synthesize Findings into Paragraphs</strong>: combine all parts into a coherent summary</li><li>: use tools to generate PDF automatically</li><li>: check word counts</li></ol><h3>3. Trade Strategist Agent with multimodal capabilities</h3><h2>AI Agent Blogs and Videos</h2><h2>AI Agent Open-Source Framework &amp; Tool</h2><ul><li><a href=\"https://github.com/Significant-Gravitas/AutoGPT\">AutoGPT (163k stars)</a> is a tool for everyone to use, aiming to democratize AI, making it accessible for everyone to use and build upon.</li><li><a href=\"https://github.com/langchain-ai/langchain\">LangChain (87.4k stars)</a> is a framework for developing context-aware applications powered by language models, enabling them to connect to sources of context and rely on the model's reasoning capabilities for responses and actions.</li><li><a href=\"https://github.com/geekan/MetaGPT\">MetaGPT (41k stars)</a> is a multi-agent open-source framework that assigns different roles to GPTs, forming a collaborative software entity to execute complex tasks.</li><li><a href=\"https://github.com/langgenius/dify\">dify (34.1.7k stars)</a> is an LLM application development platform. It integrates the concepts of Backend as a Service and LLMOps, covering the core tech stack required for building generative AI-native applications, including a built-in RAG engine</li><li><a href=\"https://github.com/microsoft/autogen\">AutoGen (27.4k stars)</a> is a framework for developing LLM applications with conversational agents that collaborate to solve tasks. These agents are customizable, support human interaction, and operate in modes combining LLMs, human inputs, and tools.</li><li><a href=\"https://github.com/OpenBMB/ChatDev\">ChatDev (24.1k stars)</a> is a framework that focuses on developing conversational AI Agents capable of dialogue and question-answering. It provides a range of pre-trained models and interactive interfaces, facilitating the development of customized chat Agents for users.</li><li><a href=\"https://github.com/yoheinakajima/babyagi\">BabyAGI (19.5k stars)</a> is an AI-powered task management system, dedicated to building AI Agents with preliminary general intelligence.</li><li><a href=\"https://github.com/joaomdmoura/crewAI\">CrewAI (16k stars)</a> is a framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.</li><li><a href=\"https://github.com/TransformerOptimus/SuperAGI\">SuperAGI (14.8k stars)</a> is a dev-first open-source autonomous AI agent framework enabling developers to build, manage &amp; run useful autonomous agents.</li><li><a href=\"https://github.com/labring/FastGPT\">FastGPT (14.6k stars)</a> is a knowledge-based platform built on the LLM, offers out-of-the-box data processing and model invocation capabilities, allows for workflow orchestration through Flow visualization.</li><li><a href=\"https://github.com/OpenBMB/XAgent\">XAgent (7.8k stars)</a> is an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.</li><li><a href=\"https://github.com/camel-ai/camel\">CAMEL (4.7k stars)</a> is a framework that offers a comprehensive set of tools and algorithms for building multimodal AI Agents, enabling them to handle various data forms such as text, images, and speech.</li><li><a href=\"https://github.com/langfuse/langfuse\">Langfuse (4.3k stars)</a> is a language fusion framework that can integrate the language abilities of multiple AI Agents, enabling them to simultaneously possess multilingual understanding and generation capabilities.</li></ul><pre><code>@inproceedings{\nzhou2024finrobot,\ntitle={FinRobot: {AI} Agent for Equity Research and Valuation with Large Language Models},\nauthor={Tianyu Zhou and Pinqiao Wang and Yilin Wu and Hongyang Yang},\nbooktitle={ICAIF 2024: The 1st Workshop on Large Language Models and Generative AI for Finance},\nyear={2024}\n}\n\n@article{yang2024finrobot,\n  title={FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models},\n  author={Yang, Hongyang and Zhang, Boyu and Wang, Neng and Guo, Cheng and Zhang, Xiaoli and Lin, Likun and Wang, Junlin and Zhou, Tianyu and Guan, Mao and Zhang, Runjia and others},\n  journal={arXiv preprint arXiv:2405.14767},\n  year={2024}\n}\n\n@inproceedings{han2024enhancing,\n  title={Enhancing Investment Analysis: Optimizing AI-Agent Collaboration in Financial Research},\n  author={Han, Xuewen and Wang, Neng and Che, Shangkun and Yang, Hongyang and Zhang, Kunpeng and Xu, Sean Xin},\n  booktitle={ICAIF 2024: Proceedings of the 5th ACM International Conference on AI in Finance},\n  pages={538--546},\n  year={2024}\n}\n</code></pre><p>: The codes and documents provided herein are released under the Apache-2.0 license. They should not be construed as financial counsel or recommendations for live trading. It is imperative to exercise caution and consult with qualified financial professionals prior to any trading or investment actions.</p>","contentLength":13525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"putyy/res-downloader","url":"https://github.com/putyy/res-downloader","date":1769309495,"author":"","guid":420186,"unread":true,"content":"<p>è§†é¢‘å·ã€å°ç¨‹åºã€æŠ–éŸ³ã€å¿«æ‰‹ã€å°çº¢ä¹¦ã€ç›´æ’­æµã€m3u8ã€é…·ç‹—ã€QQéŸ³ä¹ç­‰å¸¸è§ç½‘ç»œèµ„æºä¸‹è½½!</p><blockquote><p>ä¸€æ¬¾åŸºäº Go + <a href=\"https://github.com/wailsapp/wails\">Wails</a> çš„è·¨å¹³å°èµ„æºä¸‹è½½å·¥å…·ï¼Œç®€æ´æ˜“ç”¨ï¼Œæ”¯æŒå¤šç§èµ„æºå—…æ¢ä¸ä¸‹è½½ã€‚</p></blockquote><ul><li>ğŸ–¥ï¸ ï¼šWindows / macOS / Linux</li><li>ğŸŒ ï¼šè§†é¢‘ / éŸ³é¢‘ / å›¾ç‰‡ / m3u8 / ç›´æ’­æµç­‰</li><li>ğŸ“± ï¼šæ”¯æŒå¾®ä¿¡è§†é¢‘å·ã€å°ç¨‹åºã€æŠ–éŸ³ã€å¿«æ‰‹ã€å°çº¢ä¹¦ã€é…·ç‹—éŸ³ä¹ã€QQéŸ³ä¹ç­‰</li></ul><ol></ol><ul><li>æ£€æŸ¥æ˜¯å¦æ­£ç¡®è®¾ç½®ç³»ç»Ÿä»£ç†ï¼š åœ°å€ï¼š127.0.0.1 ç«¯å£ï¼š8899</li></ul><p>æœ¬å·¥å…·é€šè¿‡ä»£ç†æ–¹å¼å®ç°ç½‘ç»œæŠ“åŒ…ï¼Œå¹¶ç­›é€‰å¯ç”¨èµ„æºã€‚ä¸ Fiddlerã€Charlesã€æµè§ˆå™¨ DevTools åŸç†ç±»ä¼¼ï¼Œä½†å¯¹èµ„æºè¿›è¡Œäº†æ›´å‹å¥½çš„ç­›é€‰ã€å±•ç¤ºå’Œå¤„ç†ï¼Œå¤§å¹…åº¦é™ä½äº†ä½¿ç”¨é—¨æ§›ï¼Œæ›´é€‚åˆå¤§ä¼—ç”¨æˆ·ä½¿ç”¨ã€‚</p><blockquote><p>æœ¬è½¯ä»¶ä»…ä¾›å­¦ä¹ ä¸ç ”ç©¶ç”¨é€”ï¼Œç¦æ­¢ç”¨äºä»»ä½•å•†ä¸šæˆ–è¿æ³•ç”¨é€”ã€‚ å¦‚å› æ­¤äº§ç”Ÿçš„ä»»ä½•æ³•å¾‹è´£ä»»ï¼Œæ¦‚ä¸ä½œè€…æ— å…³ï¼</p></blockquote>","contentLength":881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"simstudioai/sim","url":"https://github.com/simstudioai/sim","date":1769309495,"author":"","guid":420187,"unread":true,"content":"<p>Open-source platform to build and deploy AI agent workflows.</p><p align=\"center\">Build and deploy AI agent workflows in minutes.</p><h3>Build Workflows with Ease</h3><p>Design agent workflows visually on a canvasâ€”connect agents, tools, and blocks, then run them instantly.</p><p>Leverage Copilot to generate nodes, fix errors, and iterate on flows directly from natural language.</p><h3>Integrate Vector Databases</h3><p>Upload documents to a vector store and let agents answer questions grounded in your specific content.</p><p>Docker must be installed and running on your machine.</p><table><tbody><tr><td>Port to run Sim on (default )</td></tr><tr><td>Skip pulling latest Docker images</td></tr></tbody></table><h3>Self-hosted: Docker Compose</h3><pre><code>git clone https://github.com/simstudioai/sim.git &amp;&amp; cd sim\ndocker compose -f docker-compose.prod.yml up -d\n</code></pre><h4>Using Local Models with Ollama</h4><p>Run Sim with local AI models using <a href=\"https://ollama.ai\">Ollama</a> - no external APIs required:</p><pre><code># Start with GPU support (automatically downloads gemma3:4b model)\ndocker compose -f docker-compose.ollama.yml --profile setup up -d\n\n# For CPU-only systems:\ndocker compose -f docker-compose.ollama.yml --profile cpu --profile setup up -d\n</code></pre><pre><code>docker compose -f docker-compose.ollama.yml exec ollama ollama pull llama3.1:8b\n</code></pre><h4>Using an External Ollama Instance</h4><p>If Ollama is running on your host machine, use  instead of :</p><pre><code>OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d\n</code></pre><p>On Linux, use your host's IP address or add <code>extra_hosts: [\"host.docker.internal:host-gateway\"]</code> to the compose file.</p><p>Sim supports <a href=\"https://docs.vllm.ai/\">vLLM</a> for self-hosted models. Set  and optionally  in your environment.</p><h3>Self-hosted: Dev Containers</h3><ol><li>Open the project and click \"Reopen in Container\" when prompted</li><li>Run  in the terminal or use the  alias \n  <ul><li>This starts both the main application and the realtime socket server</li></ul></li></ol><h3>Self-hosted: Manual Setup</h3><pre><code>git clone https://github.com/simstudioai/sim.git\ncd sim\nbun install\n</code></pre><ol start=\"2\"><li>Set up PostgreSQL with pgvector:</li></ol><pre><code>docker run --name simstudio-db -e POSTGRES_PASSWORD=your_password -e POSTGRES_DB=simstudio -p 5432:5432 -d pgvector/pgvector:pg17\n</code></pre><pre><code>cp apps/sim/.env.example apps/sim/.env\ncp packages/db/.env.example packages/db/.env\n# Edit both .env files to set DATABASE_URL=\"postgresql://postgres:your_password@localhost:5432/simstudio\"\n</code></pre><pre><code>cd packages/db &amp;&amp; bunx drizzle-kit migrate --config=./drizzle.config.ts\n</code></pre><ol start=\"5\"><li>Start development servers:</li></ol><pre><code>bun run dev:full  # Starts both Next.js app and realtime socket server\n</code></pre><p>Or run separately:  (Next.js) and <code>cd apps/sim &amp;&amp; bun run dev:sockets</code> (realtime).</p><p>Copilot is a Sim-managed service. To use Copilot on a self-hosted instance:</p><ul><li>Go to <a href=\"https://sim.ai\">https://sim.ai</a> â†’ Settings â†’ Copilot and generate a Copilot API key</li><li>Set  environment variable in your self-hosted apps/sim/.env file to that value</li></ul><p>Key environment variables for self-hosted deployments. See <a href=\"https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/.env.example\"></a> for defaults or <a href=\"https://raw.githubusercontent.com/simstudioai/sim/main/apps/sim/lib/core/config/env.ts\"></a> for the full list.</p><table><thead><tr></tr></thead><tbody><tr><td>PostgreSQL connection string with pgvector</td></tr><tr><td>Auth secret ()</td></tr><tr><td>Your app URL (e.g., )</td></tr><tr><td>Public app URL (same as above)</td></tr><tr><td>Encrypts environment variables ()</td></tr><tr><td>Encrypts internal API routes ()</td></tr><tr><td>Encrypts API keys ()</td></tr><tr><td>API key from sim.ai for Copilot features</td></tr></tbody></table><h3>Ollama models not showing in dropdown (Docker)</h3><p>If you're running Ollama on your host machine and Sim in Docker, change  from  to :</p><pre><code>OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d\n</code></pre><h3>Database connection issues</h3><p>Ensure PostgreSQL has the pgvector extension installed. When using Docker, wait for the database to be healthy before running migrations.</p><p>If ports 3000, 3002, or 5432 are in use, configure alternatives:</p><pre><code># Custom ports\nNEXT_PUBLIC_APP_URL=http://localhost:3100 POSTGRES_PORT=5433 docker compose up -d\n</code></pre><p>This project is licensed under the Apache License 2.0 - see the <a href=\"https://raw.githubusercontent.com/simstudioai/sim/main/LICENSE\">LICENSE</a> file for details.</p><p align=\"center\">Made with â¤ï¸ by the Sim Team</p>","contentLength":3630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Blaizzy/mlx-audio","url":"https://github.com/Blaizzy/mlx-audio","date":1769309495,"author":"","guid":420188,"unread":true,"content":"<p>A text-to-speech (TTS), speech-to-text (STT) and speech-to-speech (STS) library built on Apple's MLX framework, providing efficient speech analysis on Apple Silicon.</p><p>The best audio processing library built on Apple's MLX framework, providing fast and efficient text-to-speech (TTS), speech-to-text (STT), and speech-to-speech (STS) on Apple Silicon.</p><ul><li>Fast inference optimized for Apple Silicon (M series chips)</li><li>Multiple model architectures for TTS, STT, and STS</li><li>Multilingual support across models</li><li>Voice customization and cloning capabilities</li><li>Adjustable speech speed control</li><li>Interactive web interface with 3D audio visualization</li><li>OpenAI-compatible REST API</li><li>Quantization support (3-bit, 4-bit, 6-bit, 8-bit, and more) for optimized performance</li><li>Swift package for iOS/macOS integration</li></ul><h3>Using uv to install only the command line tools</h3><p>Latest release from pypi:</p><pre><code>uv tool install --force mlx-audio --prerelease=allow\n</code></pre><pre><code>uv tool install --force git+https://github.com/Blaizzy/mlx-audio.git --prerelease=allow\n</code></pre><h3>For development or web interface:</h3><pre><code>git clone https://github.com/Blaizzy/mlx-audio.git\ncd mlx-audio\npip install -e \".[dev]\"\n</code></pre><pre><code># Basic TTS generation\nmlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello, world!' --lang_code a\n\n# With voice selection and speed adjustment\nmlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --voice af_heart --speed 1.2 --lang_code a\n\n# Play audio immediately\nmlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --play  --lang_code a\n\n# Save to a specific directory\nmlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text 'Hello!' --output_path ./my_audio  --lang_code a\n</code></pre><pre><code>from mlx_audio.tts.utils import load_model\n\n# Load model\nmodel = load_model(\"mlx-community/Kokoro-82M-bf16\")\n\n# Generate speech\nfor result in model.generate(\"Hello from MLX-Audio!\", voice=\"af_heart\"):\n    print(f\"Generated {result.audio.shape[0]} samples\")\n    # result.audio contains the waveform as mx.array\n</code></pre><p>Kokoro is a fast, multilingual TTS model with 54 voice presets.</p><pre><code>from mlx_audio.tts.utils import load_model\n\nmodel = load_model(\"mlx-community/Kokoro-82M-bf16\")\n\n# Generate with different voices\nfor result in model.generate(\n    text=\"Welcome to MLX-Audio!\",\n    voice=\"af_heart\",  # American female\n    speed=1.0,\n    lang_code=\"a\"  # American English\n):\n    audio = result.audio\n</code></pre><ul><li>American English: , , , , , , etc.</li><li>British English: , , , , etc.</li><li>Japanese: , , etc.</li><li>Chinese: , , etc.</li></ul><table><tbody><tr><td>Requires </td></tr><tr><td>Requires </td></tr></tbody></table><p>Alibaba's state-of-the-art multilingual TTS with voice cloning, emotion control, and voice design capabilities.</p><pre><code>from mlx_audio.tts.utils import load_model\n\nmodel = load_model(\"mlx-community/Qwen3-TTS-12Hz-0.6B-Base-bf16\")\nresults = list(model.generate(\n    text=\"Hello, welcome to MLX-Audio!\",\n    voice=\"Chelsie\",\n    language=\"English\",\n))\n\naudio = results[0].audio  # mx.array\n</code></pre><p>See the <a href=\"https://raw.githubusercontent.com/Blaizzy/mlx-audio/main/mlx_audio/tts/models/qwen3_tts/README.md\">Qwen3-TTS README</a> for voice cloning, CustomVoice, VoiceDesign, and all available models.</p><p>Clone any voice using a reference audio sample:</p><pre><code>mlx_audio.tts.generate \\\n    --model mlx-community/csm-1b \\\n    --text \"Hello from Sesame.\" \\\n    --ref_audio ./reference_voice.wav \\\n    --play\n</code></pre><pre><code>from mlx_audio.stt.generate import generate_transcription\n\nresult = generate_transcription(\n    model=\"mlx-community/whisper-large-v3-turbo-asr-fp16\",\n    audio=\"audio.wav\",\n)\nprint(result.text)\n</code></pre><p>Microsoft's 9B parameter speech-to-text model with speaker diarization and timestamps. Supports long-form audio (up to 60 minutes) and outputs structured JSON.</p><pre><code>from mlx_audio.stt.utils import load\n\nmodel = load(\"mlx-community/VibeVoice-ASR-bf16\")\n\n# Basic transcription\nresult = model.generate(audio=\"meeting.wav\", max_tokens=8192, temperature=0.0)\nprint(result.text)\n# [{\"Start\":0,\"End\":5.2,\"Speaker\":0,\"Content\":\"Hello everyone, let's begin.\"},\n#  {\"Start\":5.5,\"End\":9.8,\"Speaker\":1,\"Content\":\"Thanks for joining today.\"}]\n\n# Access parsed segments\nfor seg in result.segments:\n    print(f\"[{seg['start_time']:.1f}-{seg['end_time']:.1f}] Speaker {seg['speaker_id']}: {seg['text']}\")\n</code></pre><pre><code># Stream tokens as they are generated\nfor text in model.stream_transcribe(audio=\"speech.wav\", max_tokens=4096):\n    print(text, end=\"\", flush=True)\n</code></pre><p><strong>With context (hotwords/metadata):</strong></p><pre><code>result = model.generate(\n    audio=\"technical_talk.wav\",\n    context=\"MLX, Apple Silicon, PyTorch, Transformer\",\n    max_tokens=8192,\n    temperature=0.0,\n)\n</code></pre><pre><code># Basic transcription\npython -m mlx_audio.stt.generate \\\n    --model mlx-community/VibeVoice-ASR-bf16 \\\n    --audio meeting.wav \\\n    --output-path output \\\n    --format json \\\n    --max-tokens 8192 \\\n    --verbose\n\n# With context/hotwords\npython -m mlx_audio.stt.generate \\\n    --model mlx-community/VibeVoice-ASR-bf16 \\\n    --audio technical_talk.wav \\\n    --output-path output \\\n    --format json \\\n    --max-tokens 8192 \\\n    --context \"MLX, Apple Silicon, PyTorch, Transformer\" \\\n    --verbose\n</code></pre><h3>SAM-Audio (Source Separation)</h3><p>Separate specific sounds from audio using text prompts:</p><pre><code>from mlx_audio.sts import SAMAudio, SAMAudioProcessor, save_audio\n\nmodel = SAMAudio.from_pretrained(\"mlx-community/sam-audio-large\")\nprocessor = SAMAudioProcessor.from_pretrained(\"mlx-community/sam-audio-large\")\n\nbatch = processor(\n    descriptions=[\"A person speaking\"],\n    audios=[\"mixed_audio.wav\"],\n)\n\nresult = model.separate_long(\n    batch.audios,\n    descriptions=batch.descriptions,\n    anchors=batch.anchor_ids,\n    chunk_seconds=10.0,\n    overlap_seconds=3.0,\n    ode_opt={\"method\": \"midpoint\", \"step_size\": 2/32},\n)\n\nsave_audio(result.target[0], \"voice.wav\")\nsave_audio(result.residual[0], \"background.wav\")\n</code></pre><h3>MossFormer2 (Speech Enhancement)</h3><p>Remove noise from speech recordings:</p><pre><code>from mlx_audio.sts import MossFormer2SEModel, save_audio\n\nmodel = MossFormer2SEModel.from_pretrained(\"starkdmi/MossFormer2_SE_48K_MLX\")\nenhanced = model.enhance(\"noisy_speech.wav\")\nsave_audio(enhanced, \"clean.wav\", 48000)\n</code></pre><h2>Web Interface &amp; API Server</h2><p>MLX-Audio includes a modern web interface and OpenAI-compatible API.</p><pre><code># Start API server\nmlx_audio.server --host 0.0.0.0 --port 8000\n\n# Start web UI (in another terminal)\ncd mlx_audio/ui\nnpm install &amp;&amp; npm run dev\n</code></pre><p> (OpenAI-compatible):</p><pre><code>curl -X POST http://localhost:8000/v1/audio/speech \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"mlx-community/Kokoro-82M-bf16\", \"input\": \"Hello!\", \"voice\": \"af_heart\"}' \\\n  --output speech.wav\n</code></pre><pre><code>curl -X POST http://localhost:8000/v1/audio/transcriptions \\\n  -F \"file=@audio.wav\" \\\n  -F \"model=mlx-community/whisper-large-v3-turbo-asr-fp16\"\n</code></pre><p>Reduce model size and improve performance with quantization using the convert script:</p><pre><code># Convert and quantize to 4-bit\npython -m mlx_audio.convert \\\n    --hf-path prince-canuma/Kokoro-82M \\\n    --mlx-path ./Kokoro-82M-4bit \\\n    --quantize \\\n    --q-bits 4 \\\n    --upload-repo username/Kokoro-82M-4bit (optional: if you want to upload the model to Hugging Face)\n\n# Convert with specific dtype (bfloat16)\npython -m mlx_audio.convert \\\n    --hf-path prince-canuma/Kokoro-82M \\\n    --mlx-path ./Kokoro-82M-bf16 \\\n    --dtype bfloat16 \\\n    --upload-repo username/Kokoro-82M-bf16 (optional: if you want to upload the model to Hugging Face)\n</code></pre><table><tbody><tr><td>Source Hugging Face model or local path</td></tr><tr><td>Output directory for converted model</td></tr><tr></tr><tr><td>Bits per weight (4, 6, or 8)</td></tr><tr><td>Group size for quantization (default: 64)</td></tr><tr><td>Weight dtype: , , </td></tr><tr><td>Upload converted model to HF Hub</td></tr></tbody></table><p>Looking for Swift/iOS support? Check out <a href=\"https://github.com/Blaizzy/mlx-audio-swift\">mlx-audio-swift</a> for on-device TTS using MLX on macOS and iOS.</p><ul><li>Apple Silicon Mac (M1/M2/M3/M4)</li><li> (required for MP3/FLAC audio encoding)</li></ul><p>ffmpeg is required for saving audio in MP3 or FLAC format. Install it using:</p><pre><code># macOS (using Homebrew)\nbrew install ffmpeg\n\n# Ubuntu/Debian\nsudo apt install ffmpeg\n</code></pre><p>WAV format works without ffmpeg.</p><pre><code>@misc{mlx-audio,\n  author = {Canuma, Prince},\n  title = {MLX Audio},\n  year = {2025},\n  howpublished = {\\url{https://github.com/Blaizzy/mlx-audio}},\n  note = {Audio processing library for Apple Silicon with TTS, STT, and STS capabilities.}\n}\n</code></pre>","contentLength":7890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asabeneh/30-Days-Of-Python","url":"https://github.com/Asabeneh/30-Days-Of-Python","date":1769222420,"author":"","guid":419755,"unread":true,"content":"<p>Our amazing sponsors for supporting my open-source contribution and the  series!</p><p>Every contribution, big or small, makes a huge difference. Thank you for your support! ğŸŒŸ</p><p> for deciding to participate in a  programming challenge. In this challenge, you will learn everything you need to be a python programmer and the whole concept of programming. In the end of the challenge you will get a  programming challenge certificate.</p><p>Python is a high-level programming language for general-purpose programming. It is an open source, interpreted, object-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of the Python programming language was derived from a British sketch comedy series, <em>Monty Python's Flying Circus</em>. The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, and many hands on exercises and projects.</p><p>This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge. People who actively participate in the telegram group have a high probability of completing the challenge.</p><p>This challenge is easy to read, written in conversational English, engaging, motivating and at the same time, it is very demanding. You need to allocate much time to finish this challenge. If you are a visual learner, you may get the video lesson on <a href=\"https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw\"> Washera</a> YouTube channel. You may start from <a href=\"https://youtu.be/OCCWZheOesI\">Python for Absolute Beginners video</a>. Subscribe the channel, comment and ask questions on YouTube videos and be proactive, the author will eventually notice you.</p><p>The author likes to hear your opinion about the challenge, share the author by expressing your thoughts about the 30DaysOfPython challenge. You can leave your testimonial on this <a href=\"https://www.asabeneh.com/testimonials\">link</a></p><p>It is a programming language which is very close to human language and because of that, it is easy to learn and use. Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system administration, and machine learning libraries. Python is a highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.</p><p>To run a python script you need to install python. Let's <a href=\"https://www.python.org/\">download</a> python. If your are a windows user, click the button encircled in red.</p><p>If you are a macOS user, click the button encircled in red.</p><p>To check if python is installed write the following command on your device terminal.</p><p>As you can see from the terminal, I am using  version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you manage to see the python version, well done. Python has been installed on your machine. Continue to the next section.</p><p>Python is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a <em>Python Shell (Python Interactive Shell)</em>. It is used to execute a single python command and get the result.</p><p>Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line. Open your terminal or command prompt(cmd) and write:</p><p>The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol &gt;&gt;&gt; and then click Enter. Let us write our very first script on the Python scripting shell.</p><p>Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ? To close the shell, next to this symbol &gt;&gt;&gt; write  command and press Enter.</p><p>Now, you know how to open the Python interactive shell and how to exit from it.</p><p>Python will give you results if you write scripts that Python understands, if not it returns errors. Let's make a deliberate mistake and see what Python will return.</p><p>As you can see from the returned error, Python is so clever that it knows the mistake we made and which was <em>Syntax Error: invalid syntax</em>. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of () we use asterisk (*) for multiplication. The returned error clearly shows what to fix.</p><p>The process of identifying and removing errors from a program is called . Let us debug it by putting * in place of .</p><p>Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are , , , , , , , , ,  etc. We will see more about different Python  in later sections.</p><p>Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word .</p><p>The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus, exponentiation).</p><p>Let us do some maths first before we write any Python code:</p><ul></ul><p>In python, we have the following additional operations:</p><ul><li>3 % 2 = 1 =&gt; which means finding the remainder</li><li>3 // 2 = 1 =&gt; which means removing the remainder</li></ul><p>Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.</p><p>A  is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol. This is how you write a comment in python</p><pre><code> # comment starts with hash\n # this is a python comment, because it starts with a (#) symbol\n</code></pre><p>Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing  on the shell and open it again and let us practice how to write text on the Python shell.</p><h3>Installing Visual Studio Code</h3><p>The Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge, we will use Visual Studio Code. Visual Studio Code is a very popular open source text editor. I am a fan of vscode and I would recommend to <a href=\"https://code.visualstudio.com/\">download</a> visual studio code, but if you are in favor of other editors, feel free to follow with what you have.</p><p>If you installed visual studio code, let us see how to use it. If you prefer a video, you can follow this Visual Studio Code for Python <a href=\"https://www.youtube.com/watch?v=bn7Cx4z-vSo\">Video tutorial</a></p><h4>How to use visual studio code</h4><p>Open the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.</p><p>Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.</p><p>After opening it, you will see shortcuts for creating files and folders inside of 30DaysOfPython project's directory. As you can see below, I have created the very first file, . You can do the same.</p><p>After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.</p><p>Congratulations, you have finished setting up the development environment. Let us start coding.</p><p>A Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.</p><p>An indentation is a white space in a text. Indentation in many languages is used to increase code readability; however, Python uses indentation to create blocks of code. In other programming languages, curly brackets are used to create code blocks instead of indentation. One of the common bugs when writing Python code is incorrect indentation.</p><p>Comments play a crucial role in enhancing code readability and allowing developers to leave notes within their code. In Python, any text preceded by a hash (#) symbol is considered a comment and is not executed when the code runs.</p><p><strong>Example: Single Line Comment</strong></p><pre><code>    # This is the first comment\n    # This is the second comment\n    # Python is eating the world\n</code></pre><p><strong>Example: Multiline Comment</strong></p><p>Triple quote can be used for multiline comment if it is not assigned to a variable</p><pre><code>\"\"\"This is multiline comment\nmultiline comment takes multiple lines.\npython is eating the world\n\"\"\"\n</code></pre><p>In Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.</p><ul><li>Integer: Integer(negative, zero and positive) numbers Example: ... -3, -2, -1, 0, 1, 2, 3 ...</li><li>Float: Decimal number Example ... -3.5, -2.25, -1.0, 0.0, 1.1, 2.2, 3.5 ...</li><li>Complex Example 1 + j, 2 + 4j</li></ul><p>A collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.</p><pre><code>'Asabeneh'\n'Finland'\n'Python'\n'I love teaching'\n'I hope you are enjoying the first day of 30DaysOfPython Challenge'\n</code></pre><p>A boolean data type is either a True or False value. T and F should be always uppercase.</p><pre><code>    True  #  Is the light on? If it is on, then the value is True\n    False # Is the light on? If it is off, then the value is False\n</code></pre><p>Python list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.</p><pre><code>[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers\n['Banana', 'Orange', 'Mango', 'Avocado'] # all the same data types - a list of strings (fruits)\n['Finland','Estonia', 'Sweden','Norway'] # all the same data types - a list of strings (countries)\n['Banana', 10, False, 9.81] # different data types in the list - string, integer, boolean and float\n</code></pre><p>A Python dictionary object is an unordered collection of data in a key value pair format.</p><pre><code>{\n'first_name':'Asabeneh',\n'last_name':'Yetayeh',\n'country':'Finland',\n'age':250,\n'is_married':True,\n'skills':['JS', 'React', 'Node', 'Python']\n}\n</code></pre><p>A tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.</p><pre><code>('Asabeneh', 'Pawel', 'Brook', 'Abraham', 'Lidiya') # Names\n</code></pre><pre><code>('Earth', 'Jupiter', 'Neptune', 'Mars', 'Venus', 'Saturn', 'Uranus', 'Mercury') # planets\n</code></pre><p>A set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.</p><p>In later sections, we will go in detail about each and every Python data type.</p><pre><code>{2, 4, 3, 5}\n{3.14, 9.81, 2.7} # order is not important in set\n</code></pre><p>To check the data type of certain data/variable we use the  function. In the following terminal you will see different python data types:</p><p>First open your project folder, 30DaysOfPython. If you don't have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let's do what we did on python interactive shell using visual studio code.</p><p>The Python interactive shell was printing without using  but on visual studio code to see our result we should use a built in function . The  built-in function takes one or more arguments as follows <em>print('arument1', 'argument2', 'argument3')</em>. See the examples below.</p><p>The file name is </p><pre><code># Day 1 - 30DaysOfPython Challenge\n\nprint(2 + 3)             # addition(+)\nprint(3 - 1)             # subtraction(-)\nprint(2 * 3)             # multiplication(*)\nprint(3 / 2)             # division(/)\nprint(3 ** 2)            # exponential(**)\nprint(3 % 2)             # modulus(%)\nprint(3 // 2)            # Floor division operator(//)\n\n# Checking data types\nprint(type(10))          # Int\nprint(type(3.14))        # Float\nprint(type(1 + 3j))      # Complex number\nprint(type('Asabeneh'))  # String\nprint(type([1, 2, 3]))   # List\nprint(type({'name':'Asabeneh'})) # Dictionary\nprint(type({9.8, 3.14, 2.7}))    # Set\nprint(type((9.8, 3.14, 2.7)))    # Tuple\n</code></pre><p>To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing  in the terminal .</p><p>ğŸŒ• You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.</p><ol><li>Check the python version you are using</li><li>Open the python interactive shell and do the following operations. The operands are 3 and 4. \n   <ul><li>floor division operator(//)</li></ul></li><li>Write strings on the python interactive shell. The strings are the following: \n   <ul><li>I am enjoying 30 days of python</li></ul></li><li>Check the data types of the following data: \n   <ul><li>['Asabeneh', 'Python', 'Finland']</li></ul></li></ol><ol><li>Create a folder named day_1 inside 30DaysOfPython folder. Inside day_1 folder, create a python file helloworld.py and repeat questions 1, 2, 3 and 4. Remember to use  when you are working on a python file. Navigate to the directory where you have saved your file, and run it.</li></ol><ol><li>Write an example for different Python data types such as Number(Integer, Float, Complex), String, Boolean, List, Tuple, Set and Dictionary.</li></ol>","contentLength":13401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KellerJordan/modded-nanogpt","url":"https://github.com/KellerJordan/modded-nanogpt","date":1769222420,"author":"","guid":419756,"unread":true,"content":"<p>NanoGPT (124M) in 2 minutes</p><p>This repository hosts the , in which we (collaboratively|competitively) search for the fastest algorithm to use 8 NVIDIA H100 GPUs to train a language model that attains 3.28 cross-entropy loss on the <a href=\"https://huggingface.co/datasets/HuggingFaceFW/fineweb\">FineWeb</a> validation set.</p><ul><li>Under 100 seconds on 8xH100 (the llm.c GPT-2 replication needed 45 minutes)</li><li>under 500M tokens (the llm.c GPT-2 replication needed 10B)</li></ul><p>This improvement in training speed has been brought about by the following techniques:</p><ul><li>Modernized architecture: Rotary embeddings, QK-Norm, and ReLUÂ²</li><li>Use FP8 matmul for head, and asymmetric rescale and softcap logits</li><li>Initialization of projections to zero (muP-like)</li><li>Skip connections from embedding to every block as well as from block 3 to 6</li><li>Extra embeddings which are mixed into the values in attention layers (inspired by Zhou et al. 2024)</li><li>Flash Attention 3 with long-short sliding window attention pattern (inspired by Gemma 2) and window size warmup with YaRN</li><li>Align training batch starts with EoS and set a max document length</li><li>Accumulate gradients for 2 steps for embedding and lm_head before updating parameters</li><li>Enable model to back out contributions from first 2/3 layers before prediction</li><li>Polar Express implementation in Muon</li><li>Smear module to enable 1 token look back</li><li>Cautious Weight Decay w/ schedule tied to LR</li><li>Exponential decay of residual stream</li><li>Untie embed and lm_head at 2/3 of training</li><li>Additional gating on value embeddings and skip connection</li></ul><p>As well as many systems optimizations.</p><h2>Running the current record</h2><p>To run the current record, run the following commands.</p><pre><code>git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;&amp; cd modded-nanogpt\npip install -r requirements.txt\npip install torch==2.10.0.dev20251210+cu126 --index-url https://download.pytorch.org/whl/nightly/cu126\n# downloads only the first 900M training tokens to save time\npython data/cached_fineweb10B.py 9\n./run.sh\n</code></pre><p>Add torchrun to path if ./run.sh gives error <code>torchrun: command not found</code>.</p><p><strong>Note: torch.compile will add around 7 minutes of latency the first time you run the code.</strong></p><h2>Alternative: Running with Docker (recommended for precise timing)</h2><p>For cases where CUDA or NCCL versions aren't compatible with your current system setup, Docker can be a helpful alternative. This approach standardizes versions for CUDA, NCCL, CUDNN, and Python, reducing dependency issues and simplifying setup. Note: an NVIDIA driver must already be installed on the system (useful if only the NVIDIA driver and Docker are available).</p><pre><code>git clone https://github.com/KellerJordan/modded-nanogpt.git &amp;&amp; cd modded-nanogpt\nsudo docker build -t modded-nanogpt .\nsudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt python data/cached_fineweb10B.py 8\nsudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt sh run.sh\n</code></pre><p>To get an interactive docker, you can use</p><pre><code>sudo docker run -it --rm --gpus all -v $(pwd):/modded-nanogpt modded-nanogpt bash\n</code></pre><p>The following is the historical progression of world speed records for the following competitive task:</p><blockquote><p><em>Train a neural network to â‰¤3.28 validation loss on FineWeb using 8x NVIDIA H100s.</em></p></blockquote><ol><li>Not modify the train or validation data pipelines. (You can change the batch size, sequence length, attention structure etc.; just don't change the underlying streams of tokens.)</li><li>Attain â‰¤3.28 mean val loss. (Due to inter-run variance, submissions must provide enough run logs to attain a statistical significance level of p&lt;0.01 that their mean val loss is â‰¤3.28. Example code to compute p-value can be found <a href=\"https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/records/track_1_short/2025-01-04_SoftCap#softer-softcap\">here</a>. For submissions which improve speed by optimizing the systems performance, without touching the ML, this requirement is waived.)</li><li>Not use any extra  or  flags. (These can save a few seconds, but they can also make compilation take &gt;30min. This rule was introduced after the 21st record.)</li><li>Run faster than the prior record when baselined on the same hardware.</li></ol><p>Discretionary reasons why a PR may not be accepted:</p><ol><li>Disproportionately degrades the readability of the codebase. A 200 line kernel to drop 300ms is considered worthwhile. 500 lines that convolute the optimizer layout for a 50ms gain will likely be rejected.</li><li>The current record is intentionally kept roughly 0.001-0.002 loss below 3.28 to make validation simpler. If a PR substantially consumes this buffer, it should do so in a way that outperforms a simple step count decrease, when measured at equivalent loss.</li></ol><blockquote><p>Note: <code>torch._inductor.config.coordinate_descent_tuning</code> is allowed for GPT-2 Medium track (a.k.a. 2.92 track).</p></blockquote><p>Other than that, anything and everything is fair game!</p><h3>Comment on the target metric</h3><p>The target metric is <em>cross-entropy loss on the FineWeb val set</em>. To speak mathematically, the goal of the speedrun is *to obtain a probability model of language which assigns a probability of at least <code>math.exp(-3.28 * 10485760)</code> to the first 10,485,760 tokens of the FineWeb valset. Hence, e.g., we allow evaluation at any sequence length, so long as we still have a valid probability model of language.</p><h3>Timing change after record 21</h3><p>After the 21st record, we made two changes to the timing. First, there used to be an initial \"grace period\" of 10 untimed steps to allow kernel warmup. We replaced this with an explicit kernel-warmup section which is untimed and uses dummy data. This results in an extra runtime of 850ms from the 10 extra timed steps. Second, we banned the use of <code>torch._inductor.config.coordinate_descent_tuning</code>. This saves ~25min of untimed pre-run compilation, but results in an extra runtime of ~3s.</p><ul><li><a href=\"https://x.com/alexjc/status/1881410039639863622\">@alexjc's 01/20/2025 2.77-minute TokenMonster-based record</a>. This record is technically outside the rules of the speedrun, since we specified that the train/val tokens must be kept fixed. However, it's very interesting, and worth including. The run is not more data-efficient; rather, the speedup comes from the improved tokenizer allowing the vocabulary size to be reduced (nearly halved!) while preserving the same bytes-per-token, which saves lots of parameters and FLOPs in the head and embeddings.</li></ul><h2>Speedrun track 2: GPT-2 Medium</h2><p>The target loss for this track is lowered from 3.28 to 2.92, as per Andrej Karpathy's 350M-parameter llm.c baseline. This baseline generates a model with performance similar to the original GPT-2 Medium, whereas the first track's baseline generates a model on par with GPT-2 Small. All other rules remain the same.</p><blockquote><p>Note: <code>torch._inductor.config.coordinate_descent_tuning</code> is turned on after the record 6 (*).</p></blockquote><h3>Q: What is the point of NanoGPT speedrunning?</h3><p>A: The officially stated goal of NanoGPT speedrunning is as follows: . But for something a little more verbose involving an argument for good benchmarking, here's some kind of manifesto, adorned with a blessing from the master. <a href=\"https://x.com/karpathy/status/1846790537262571739\">https://x.com/karpathy/status/1846790537262571739</a></p><h3>Q: What makes \"NanoGPT speedrunning\" not just another idiosyncratic benchmark?</h3><p>A: Because it is a  benchmark. In particular, if you attain a new speed record (using whatever method you want), there is an open invitation for you to post that record (on arXiv or X) and thereby vacuum up all the clout for yourself. I will even help you do it by reposting you as much as I can.</p><h3>Q: NanoGPT speedrunning is cool and all, but meh it probably won't scale and is just overfitting to val loss</h3><p>A: This is hard to refute, since \"at scale\" is an infinite category (what if the methods stop working only for &gt;100T models?), making it impossible to fully prove. Also, I would agree that some of the methods used in the speedrun are unlikely to scale, particularly those which <em>impose additional structure</em> on the network, such as logit softcapping. But if the reader cares about 1.5B models, they might be convinced by this result:</p><p><em>Straightforwardly scaling up the speedrun (10/18/24 version) to 1.5B parameters yields a model with GPT-2 (1.5B)-level HellaSwag performance 2.5x more cheaply than <a href=\"https://github.com/karpathy/llm.c/discussions/677\">@karpathy's baseline</a> ($233 instead of $576):</em></p><p>Muon is defined as follows:</p><p>Where NewtonSchulz5 is the following Newton-Schulz iteration [2, 3], which approximately replaces  with  where .</p><pre><code>@torch.compile\ndef zeroth_power_via_newtonschulz5(G, steps=5, eps=1e-7):\n    assert len(G.shape) == 2\n    a, b, c = (3.4445, -4.7750,  2.0315)\n    X = G.bfloat16() / (G.norm() + eps)\n    if G.size(0) &gt; G.size(1):\n        X = X.T \n    for _ in range(steps):\n        A = X @ X.T\n        B = b * A + c * A @ A\n        X = a * X + B @ X\n    if G.size(0) &gt; G.size(1):\n        X = X.T \n    return X.to(G.dtype)\n</code></pre><p>For this training scenario, Muon has the following favorable properties:</p><ul><li>Lower memory usage than Adam</li><li>~1.5x better sample-efficiency</li></ul><p>Many of the choices made to generate this optimizer were obtained experimentally by our pursuit of <a href=\"https://github.com/KellerJordan/cifar10-airbench\">CIFAR-10 speedrunning</a>. In particular, we experimentally obtained the following practices:</p><ul><li>Using Nesterov momentum inside the update, with orthogonalization applied after momentum.</li><li>Using a specifically quintic Newton-Schulz iteration as the method of orthogonalization.</li><li>Using non-convergent coefficients for the quintic polynomial in order to maximize slope at zero, and thereby minimize the number of necessary Newton-Schulz iterations. It turns out that the variance doesn't actually matter that much, so we end up with a quintic that rapidly converges to the range 0.68, 1.13 upon repeated application, rather than converging more slowly to 1.</li><li>Running the Newton-Schulz iteration in bfloat16 (whereas Shampoo implementations often depend on inverse-pth-roots run in fp32 or fp64).</li></ul><p>Our use of a Newton-Schulz iteration for orthogonalization traces to <a href=\"https://arxiv.org/abs/2409.20325\">Bernstein &amp; Newhouse (2024)</a>, who suggested it as a way to compute Shampoo [5, 6] preconditioners, and theoretically explored Shampoo without preconditioner accumulation. In particular, Jeremy Bernstein @jxbz sent us the draft, which caused us to experiment with various Newton-Schulz iterations as the orthogonalization method for this optimizer. If we had used SVD instead of a Newton-Schulz iteration, this optimizer would have been too slow to be useful. Bernstein &amp; Newhouse also pointed out that Shampoo without preconditioner accumulation is equivalent to steepest descent in the spectral norm, and therefore Shampoo can be thought of as a way to smooth out spectral steepest descent. The proposed optimizer can be thought of as a second way of smoothing spectral steepest descent, with a different set of memory and runtime tradeoffs compared to Shampoo.</p><ul><li>To run experiments on fewer GPUs, simply modify  to have a different . This should not change the behavior of the training.</li><li>If you're running out of memory, you may need to reduce the sequence length for FlexAttention (which does change the training. see <a href=\"https://github.com/KellerJordan/modded-nanogpt/pull/38\">here</a> for a guide)</li></ul><pre><code>@misc{modded_nanogpt_2024,\n  author       = {Keller Jordan and Jeremy Bernstein and Brendan Rappazzo and\n                  @fernbear.bsky.social and Boza Vlado and You Jiacheng and\n                  Franz Cesista and Braden Koszarsky and @Grad62304977},\n  title        = {modded-nanogpt: Speedrunning the NanoGPT baseline},\n  year         = {2024},\n  url          = {https://github.com/KellerJordan/modded-nanogpt}\n}\n</code></pre><img src=\"https://raw.githubusercontent.com/KellerJordan/modded-nanogpt/master/img/dofa.jpg\" alt=\"itsover_wereback\">","contentLength":11027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"lyogavin/airllm","url":"https://github.com/lyogavin/airllm","date":1769222420,"author":"","guid":419757,"unread":true,"content":"<p>AirLLM 70B inference with single 4GB GPU</p><p> optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning. And you can run  on  now.</p><h2>AI Agents Recommendation:</h2><p>[2024/08/20] v2.11.0: Support Qwen2.5</p><p>[2024/08/18] v2.10.1 Support CPU inference. Support non sharded models. Thanks @NavodPeiris for the great work!</p><p>[2024/07/30] Support Llama3.1  (<a href=\"https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_llama3.1_405B.ipynb\">example notebook</a>). Support .</p><p>[2024/04/20] AirLLM supports Llama3 natively already. Run Llama3 70B on 4GB single GPU.</p><p>[2023/12/25] v2.8.2: Support MacOS running 70B large language models.</p><p>[2023/12/20] v2.7: Support AirLLMMixtral.</p><p>[2023/12/20] v2.6: Added AutoModel, automatically detect model type, no need to provide model class to initialize model.</p><p>[2023/12/18] v2.5: added prefetching to overlap the model loading and compute. 10% speed improvement.</p><p>[2023/12/03] added support of , , , , !</p><p>[2023/12/02] added support for safetensors. Now support all top 10 models in open llm leaderboard.</p><p>[2023/12/01] airllm 2.0. Support compressions: </p><p>[2023/11/20] airllm Initial version!</p><p>First, install the airllm pip package.</p><p>Then, initialize AirLLMLlama2, pass in the huggingface repo ID of the model being used, or the local path, and inference can be performed similar to a regular transformer model.</p><p>(<em>You can also specify the path to save the splitted layered model through  when init AirLLMLlama2.</em></p><pre><code>from airllm import AutoModel\n\nMAX_LENGTH = 128\n# could use hugging face model repo id:\nmodel = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\")\n\n# or use model's local path...\n#model = AutoModel.from_pretrained(\"/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f\")\n\ninput_text = [\n        'What is the capital of United States?',\n        #'I like',\n    ]\n\ninput_tokens = model.tokenizer(input_text,\n    return_tensors=\"pt\", \n    return_attention_mask=False, \n    truncation=True, \n    max_length=MAX_LENGTH, \n    padding=False)\n           \ngeneration_output = model.generate(\n    input_tokens['input_ids'].cuda(), \n    max_new_tokens=20,\n    use_cache=True,\n    return_dict_in_generate=True)\n\noutput = model.tokenizer.decode(generation_output.sequences[0])\n\nprint(output)\n\n</code></pre><p>Note: During inference, the original model will first be decomposed and saved layer-wise. Please ensure there is sufficient disk space in the huggingface cache directory.</p><h2>Model Compression - 3x Inference Speed Up!</h2><p>We just added model compression based on block-wise quantization-based model compression. Which can further <strong>speed up the inference speed</strong> for up to  , with <strong>almost ignorable accuracy loss!</strong> (see more performance evaluation and why we use block-wise quantization in <a href=\"https://arxiv.org/abs/2212.09720\">this paper</a>)</p><h4>How to enable model compression speed up:</h4><ul><li>Step 1. make sure you have <a href=\"https://github.com/TimDettmers/bitsandbytes\">bitsandbytes</a> installed by <code>pip install -U bitsandbytes </code></li><li>Step 2. make sure airllm verion later than 2.0.0: </li><li>Step 3. when initialize the model, passing the argument compression ('4bit' or '8bit'):</li></ul><pre><code>model = AutoModel.from_pretrained(\"garage-bAInd/Platypus2-70B-instruct\",\n                     compression='4bit' # specify '8bit' for 8-bit block-wise quantization \n                    )\n</code></pre><h4>What are the differences between model compression and quantization?</h4><p>Quantization normally needs to quantize both weights and activations to really speed things up. Which makes it harder to maintain accuracy and avoid the impact of outliers in all kinds of inputs.</p><p>While in our case the bottleneck is mainly at the disk loading, we only need to make the model loading size smaller. So, we get to only quantize the weights' part, which is easier to ensure the accuracy.</p><p>When initialize the model, we support the following configurations:</p><ul><li>: supported options: 4bit, 8bit for 4-bit or 8-bit block-wise quantization, or by default None for no compression</li><li>: supported options: True to output time consumptions or by default False</li><li>: optionally another path to save the splitted model</li><li>: huggingface token can be provided here if downloading gated models like: </li><li>: prefetching to overlap the model loading and compute. By default, turned on. For now, only AirLLMLlama2 supports this.</li><li>: if you don't have too much disk space, you can set delete_original to true to delete the original downloaded hugging face model, only keep the transformed one to save half of the disk space.</li></ul><p>Just install airllm and run the code the same as on linux. See more in <a href=\"https://raw.githubusercontent.com/lyogavin/airllm/main/#quickstart\">Quick Start</a>.</p><ul><li>make sure you installed <a href=\"https://github.com/ml-explore/mlx?tab=readme-ov-file#installation\">mlx</a> and torch</li><li>you probably need to install python native see more <a href=\"https://stackoverflow.com/a/65432861/21230266\">here</a></li></ul><a target=\"_blank\" href=\"https://colab.research.google.com/github/lyogavin/airllm/blob/main/air_llm/examples/run_all_types_of_models.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg?sanitize=true\" alt=\"Open In Colab\"></a><h4>example of other models (ChatGLM, QWen, Baichuan, Mistral, etc):</h4><h4>To request other model support: <a href=\"https://docs.google.com/forms/d/e/1FAIpQLSe0Io9ANMT964Zi-OQOq1TJmnvP-G3_ZgQDhP7SatN0IEdbOg/viewform?usp=sf_link\">here</a></h4><p>A lot of the code are based on SimJeg's great work in the Kaggle exam competition. Big shoutout to SimJeg:</p><h3>1. MetadataIncompleteBuffer</h3><p>safetensors_rust.SafetensorError: Error while deserializing header: MetadataIncompleteBuffer</p><p>If you run into this error, most possible cause is you run out of disk space. The process of splitting model is very disk-consuming. See <a href=\"https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/12\">this</a>. You may need to extend your disk space, clear huggingface <a href=\"https://huggingface.co/docs/datasets/cache\">.cache</a> and rerun.</p><h3>2. ValueError: max() arg is an empty sequence</h3><p>Most likely you are loading QWen or ChatGLM model with Llama2 class. Try the following:</p><pre><code>from airllm import AutoModel #&lt;----- instead of AirLLMLlama2\nAutoModel.from_pretrained(...)\n</code></pre><pre><code>from airllm import AutoModel #&lt;----- instead of AirLLMLlama2\nAutoModel.from_pretrained(...)\n</code></pre><h3>3. 401 Client Error....Repo model ... is gated.</h3><p>Some models are gated models, needs huggingface api token. You can provide hf_token:</p><pre><code>model = AutoModel.from_pretrained(\"meta-llama/Llama-2-7b-hf\", #hf_token='HF_API_TOKEN')\n</code></pre><h3>4. ValueError: Asking to pad but the tokenizer does not have a padding token.</h3><p>Some model's tokenizer doesn't have padding token, so you can set a padding token or simply turn the padding config off:</p><pre><code>input_tokens = model.tokenizer(input_text,\n   return_tensors=\"pt\", \n   return_attention_mask=False, \n   truncation=True, \n   max_length=MAX_LENGTH, \n   padding=False  #&lt;-----------   turn off padding \n)\n</code></pre><p>If you find AirLLM useful in your research and wish to cite it, please use the following BibTex entry:</p><pre><code>@software{airllm2023,\n  author = {Gavin Li},\n  title = {AirLLM: scaling large language models on low-end commodity computers},\n  url = {https://github.com/lyogavin/airllm/},\n  version = {0.0},\n  year = {2023},\n}\n</code></pre><p>Welcomed contributions, ideas and discussions!</p><p>If you find it useful, please â­ or buy me a coffee! ğŸ™</p>","contentLength":6440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"anthropics/claude-code","url":"https://github.com/anthropics/claude-code","date":1769222420,"author":"","guid":419758,"unread":true,"content":"<p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.</p><p>Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows -- all through natural language commands. Use it in your terminal, IDE, or tag @claude on Github.</p><img src=\"https://raw.githubusercontent.com/anthropics/claude-code/main/demo.gif\"><blockquote><p>[!NOTE] Installation via npm is deprecated. Use one of the recommended methods below.</p></blockquote><p>For more installation options, uninstall steps, and troubleshooting, see the <a href=\"https://code.claude.com/docs/en/setup\">setup documentation</a>.</p><ol><li><p><strong>MacOS/Linux (Recommended):</strong></p><pre><code>curl -fsSL https://claude.ai/install.sh | bash\n</code></pre><pre><code>brew install --cask claude-code\n</code></pre><pre><code>irm https://claude.ai/install.ps1 | iex\n</code></pre><pre><code>winget install Anthropic.ClaudeCode\n</code></pre><pre><code>npm install -g @anthropic-ai/claude-code\n</code></pre></li><li><p>Navigate to your project directory and run .</p></li></ol><p>This repository includes several Claude Code plugins that extend functionality with custom commands and agents. See the <a href=\"https://raw.githubusercontent.com/anthropics/claude-code/main/plugins/README.md\">plugins directory</a> for detailed documentation on available plugins.</p><p>We welcome your feedback. Use the  command to report issues directly within Claude Code, or file a <a href=\"https://github.com/anthropics/claude-code/issues\">GitHub issue</a>.</p><p>Join the <a href=\"https://anthropic.com/discord\">Claude Developers Discord</a> to connect with other developers using Claude Code. Get help, share feedback, and discuss your projects with the community.</p><h2>Data collection, usage, and retention</h2><p>When you use Claude Code, we collect feedback, which includes usage data (such as code acceptance or rejections), associated conversation data, and user feedback submitted via the  command.</p><p>We have implemented several safeguards to protect your data, including limited retention periods for sensitive information, restricted access to user session data, and clear policies against using feedback for model training.</p>","contentLength":1892,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"github/copilot-cli","url":"https://github.com/github/copilot-cli","date":1769222420,"author":"","guid":419759,"unread":true,"content":"<p>GitHub Copilot CLI brings the power of Copilot coding agent directly to your terminal.</p><p>The power of GitHub Copilot, now in your terminal.</p><p>GitHub Copilot CLI brings AI-powered coding assistance directly to your command line, enabling you to build, debug, and understand code through natural language conversations. Powered by the same agentic harness as GitHub's Copilot coding agent, it provides intelligent assistance while staying deeply integrated with your GitHub workflow.</p><h2>ğŸš€ Introduction and Overview</h2><p>We're bringing the power of GitHub Copilot coding agent directly to your terminal. With GitHub Copilot CLI, you can work locally and synchronously with an AI agent that understands your code and GitHub context.</p><ul><li><strong>Terminal-native development:</strong> Work with Copilot coding agent directly in your command line â€” no context switching required.</li><li><strong>GitHub integration out of the box:</strong> Access your repositories, issues, and pull requests using natural language, all authenticated with your existing GitHub account.</li><li> Build, edit, debug, and refactor code with an AI collaborator that can plan and execute complex tasks.</li><li><strong>MCP-powered extensibility:</strong> Take advantage of the fact that the coding agent ships with GitHub's MCP server by default and supports custom MCP servers to extend capabilities.</li><li> Preview every action before execution â€” nothing happens without your explicit approval.</li></ul><p>We're still early in our journey, but with your feedback, we're rapidly iterating to make the GitHub Copilot CLI the best possible companion in your terminal.</p><ul><li>(On Windows)  v6 or higher</li></ul><pre><code>winget install GitHub.Copilot\n</code></pre><pre><code>winget install GitHub.Copilot.Prerelease\n</code></pre><pre><code>brew install copilot-cli@prerelease\n</code></pre><p>Install with <a href=\"https://www.npmjs.com/package/@github/copilot\">npm</a> (macOS, Linux, and Windows):</p><pre><code>npm install -g @github/copilot\n</code></pre><pre><code>npm install -g @github/copilot@prerelease\n</code></pre><p>Install with the install script (macOS and Linux):</p><pre><code>curl -fsSL https://gh.io/copilot-install | bash\n</code></pre><pre><code>wget -qO- https://gh.io/copilot-install | bash\n</code></pre><p>Use  to run as root and install to .</p><p>Set  to install to  directory. Defaults to  when run as root or  when run as a non-root user.</p><p>Set  to install a specific version. Defaults to the latest version.</p><p>For example, to install version  to a custom directory:</p><pre><code>curl -fsSL https://gh.io/copilot-install | VERSION=\"v0.0.369\" PREFIX=\"$HOME/custom\" bash\n</code></pre><p>On first launch, you'll be greeted with our adorable animated banner! If you'd like to see this banner again, launch  with the  flag.</p><p>If you're not currently logged in to GitHub, you'll be prompted to use the  slash command. Enter this command and follow the on-screen instructions to authenticate.</p><h4>Authenticate with a Personal Access Token (PAT)</h4><p>You can also authenticate using a fine-grained PAT with the \"Copilot Requests\" permission enabled.</p><p>Launch  in a folder that contains code you want to work with.</p><p>By default,  utilizes Claude Sonnet 4.5. Run the  slash command to choose from other available models, including Claude Sonnet 4 and GPT-5.</p><p>Each time you submit a prompt to GitHub Copilot CLI, your monthly quota of premium requests is reduced by one. For information about premium requests, see <a href=\"https://docs.github.com/copilot/managing-copilot/monitoring-usage-and-entitlements/about-premium-requests\">About premium requests</a>.</p><h2>ğŸ“¢ Feedback and Participation</h2><p>We're excited to have you join us early in the Copilot CLI journey.</p><p>This is an early-stage preview, and we're building quickly. Expect frequent updates--please keep your client up to date for the latest features and fixes!</p><p>Your insights are invaluable! Open issue in this repo, join Discussions, and run  from the CLI to submit a confidential feedback survey!</p>","contentLength":3470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/VibeVoice","url":"https://github.com/microsoft/VibeVoice","date":1769222420,"author":"","guid":419760,"unread":true,"content":"<p>Open-Source Frontier Voice AI</p><div align=\"left\"><p><strong>2026-01-21: ğŸ“£ We open-sourced <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-asr.md\"></a>, a unified speech-to-text model designed to handle 60-minute long-form audio in a single pass, generating structured transcriptions containing Who (Speaker), When (Timestamps), and What (Content), with support for User-Customized Context. Try it in <a href=\"https://aka.ms/vibevoice-asr\">Playground</a></strong>.</p><p>2025-12-16: ğŸ“£ We added experimental speakers to <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md\"></a> for exploration, including multilingual voices in nine languages (DE, FR, IT, JP, KR, NL, PL, PT, ES) and 11 distinct English style voices. <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md#optional-more-experimental-voices\">Try it</a>. More speaker types will be added over time.</p><p>2025-12-03: ğŸ“£ We open-sourced <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-realtime-0.5b.md\"></a>, a realâ€‘time textâ€‘toâ€‘speech model that supports streaming text input and robust long-form speech generation. Try it on <a href=\"https://colab.research.google.com/github/microsoft/VibeVoice/blob/main/demo/vibevoice_realtime_colab.ipynb\">Colab</a>.</p><p>2025-09-05: VibeVoice is an open-source research framework intended to advance collaboration in the speech synthesis community. After release, we discovered instances where the tool was used in ways inconsistent with the stated intent. Since responsible use of AI is one of Microsoftâ€™s guiding principles, we have removed the VibeVoice-TTS code from this repository.</p><p>2025-08-25: ğŸ“£ We open-sourced <a href=\"https://raw.githubusercontent.com/microsoft/VibeVoice/main/docs/vibevoice-tts.md\"></a>, a long-form multi-speaker text-to-speech model that can synthesize speech up to 90 minutes long with up to 4 distinct speakers.</p></div><p>VibeVoice is a <strong>family of open-source frontier voice AI models</strong> that includes both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) models.</p><p>A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of . These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a <a href=\"https://arxiv.org/abs/2412.08635\">next-token diffusion</a> framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details.</p><p>For more information, demos, and examples, please visit our <a href=\"https://microsoft.github.io/VibeVoice\">Project Page</a>.</p><p> is a unified speech-to-text model designed to handle <strong>60-minute long-form audio</strong> in a single pass, generating structured transcriptions containing <strong>Who (Speaker), When (Timestamps), and What (Content)</strong>, with support for .</p><ul><li><p><strong>ğŸ•’ 60-minute Single-Pass Processing</strong>: Unlike conventional ASR models that slice audio into short chunks (often losing global context), VibeVoice ASR accepts up to  of continuous audio input within 64K token length. This ensures consistent speaker tracking and semantic coherence across the entire hour.</p></li><li><p>: Users can provide customized hotwords (e.g., specific names, technical terms, or background info) to guide the recognition process, significantly improving accuracy on domain-specific content.</p></li><li><p><strong>ğŸ“ Rich Transcription (Who, When, What)</strong>: The model jointly performs ASR, diarization, and timestamping, producing a structured output that indicates  said  and .</p></li></ul><p>: Long-form conversational audio, podcasts, multi-speaker dialogues</p><ul><li><p><strong>â±ï¸ 90-minute Long-form Generation</strong>: Synthesizes conversational/single-speaker speech up to  in a single pass, maintaining speaker consistency and semantic coherence throughout.</p></li><li><p>: Supports up to  in a single conversation, with natural turn-taking and speaker consistency across long dialogues.</p></li><li><p>: Generates expressive, natural-sounding speech that captures conversational dynamics and emotional nuances.</p></li><li><p>: Supports English, Chinese and other languages.</p></li></ul><p><strong>Long Conversation with 4 people</strong></p><p>VibeVoice-Realtime is a  text-to-speech model supporting  and <strong>robust long-form speech generation</strong>.</p><ul><li>Parameter size: 0.5B (deployment-friendly)</li><li>Real-time TTS (~300 milliseconds first audible latency)</li><li>Robust long-form speech generation (~10 minutes)</li></ul><p>While efforts have been made to optimize it through various techniques, it may still produce outputs that are unexpected, biased, or inaccurate. VibeVoice inherits any biases, errors, or omissions produced by its base model (specifically, Qwen2.5 1.5b in this release). Potential for Deepfakes and Disinformation: High-quality synthetic speech can be misused to create convincing fake audio content for impersonation, fraud, or spreading disinformation. Users must ensure transcripts are reliable, check content accuracy, and avoid using generated content in misleading ways. Users are expected to use the generated content and to deploy the models in a lawful manner, in full compliance with all applicable laws and regulations in the relevant jurisdictions. It is best practice to disclose the use of AI when sharing AI-generated content.</p><p>We do not recommend using VibeVoice in commercial or real-world applications without further testing and development. This model is intended for research and development purposes only. Please use responsibly.</p>","contentLength":4681,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"browser-use/browser-use","url":"https://github.com/browser-use/browser-use","date":1769222420,"author":"","guid":419761,"unread":true,"content":"<p>ğŸŒ Make websites accessible for AI agents. Automate tasks online with ease.</p><p>ğŸŒ¤ï¸ Want to skip the setup? Use our  for faster, scalable, stealth-enabled browser automation!</p><ol><li>Direct your favorite coding agent (Cursor, Claude Code, etc) to <a href=\"https://docs.browser-use.com/llms-full.txt\">Agents.md</a></li></ol><p><strong>1. Create environment with <a href=\"https://docs.astral.sh/uv/\">uv</a> (Python&gt;=3.11):</strong></p><p><strong>2. Install Browser-Use package:</strong></p><pre><code>#  We ship every day - use the latest version!\nuv add browser-use\nuv sync\n</code></pre><p><strong>3. Get your API key from <a href=\"https://cloud.browser-use.com/new-api-key\">Browser Use Cloud</a> and add it to your  file (new signups get $10 free credits):</strong></p><pre><code># .env\nBROWSER_USE_API_KEY=your-key\n</code></pre><p><strong>4. Install Chromium browser:</strong></p><pre><code>from browser_use import Agent, Browser, ChatBrowserUse\nimport asyncio\n\nasync def example():\n    browser = Browser(\n        # use_cloud=True,  # Uncomment to use a stealth browser on Browser Use Cloud\n    )\n\n    llm = ChatBrowserUse()\n\n    agent = Agent(\n        task=\"Find the number of stars of the browser-use repo\",\n        llm=llm,\n        browser=browser,\n    )\n\n    history = await agent.run()\n    return history\n\nif __name__ == \"__main__\":\n    history = asyncio.run(example())\n</code></pre><p>We handle agents, browsers, persistence, auth, cookies, and LLMs. The agent runs right next to the browser for minimal latency.</p><pre><code>from browser_use import Browser, sandbox, ChatBrowserUse\nfrom browser_use.agent.service import Agent\nimport asyncio\n\n@sandbox()\nasync def my_task(browser: Browser):\n    agent = Agent(task=\"Find the top HN post\", browser=browser, llm=ChatBrowserUse())\n    await agent.run()\n\n# Just call it like any async function\nasyncio.run(my_task())\n</code></pre><p><strong>Want to get started even faster?</strong> Generate a ready-to-run template:</p><pre><code>uvx browser-use init --template default\n</code></pre><p>This creates a  file with a working example. Available templates:</p><ul><li> - Minimal setup to get started quickly</li><li> - All configuration options with detailed comments</li><li> - Examples of custom tools and extending the agent</li></ul><p>You can also specify a custom output path:</p><pre><code>uvx browser-use init --template default --output my_agent.py\n</code></pre><p>Fast, persistent browser automation from the command line:</p><pre><code>browser-use open https://example.com    # Navigate to URL\nbrowser-use state                       # See clickable elements\nbrowser-use click 5                     # Click element by index\nbrowser-use type \"Hello\"                # Type text\nbrowser-use screenshot page.png         # Take screenshot\nbrowser-use close                       # Close browser\n</code></pre><p>The CLI keeps the browser running between commands for fast iteration. See <a href=\"https://raw.githubusercontent.com/browser-use/browser-use/main/browser_use/skill_cli/README.md\">CLI docs</a> for all commands.</p><p>For <a href=\"https://claude.ai/code\">Claude Code</a>, install the skill to enable AI-assisted browser automation:</p><pre><code>mkdir -p ~/.claude/skills/browser-use\ncurl -o ~/.claude/skills/browser-use/SKILL.md \\\n  https://raw.githubusercontent.com/browser-use/browser-use/main/skills/browser-use/SKILL.md\n</code></pre><h4>Task = \"Fill in this job application with my resume and information.\"</h4><h4>Task = \"Put this list of items into my instacart.\"</h4><h4>Task = \"Help me find parts for a custom PC.\"</h4><h2>Integrations, hosting, custom tools, MCP, and more on our <a href=\"https://docs.browser-use.com\">Docs â†—</a></h2><div align=\"center\">\n  Made with â¤ï¸ in Zurich and San Francisco \n</div>","contentLength":2973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ai-dynamo/dynamo","url":"https://github.com/ai-dynamo/dynamo","date":1769222420,"author":"","guid":419762,"unread":true,"content":"<p>A Datacenter Scale Distributed Inference Serving Framework</p><p>High-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments.</p><p>Large language models exceed single-GPU capacity. Tensor parallelism spreads layers across GPUs but creates coordination challenges. Dynamo closes this orchestration gap.</p><p>Dynamo is inference engine agnostic (supports TRT-LLM, vLLM, SGLang) and provides:</p><ul><li><strong>Disaggregated Prefill &amp; Decode</strong> â€“ Maximizes GPU throughput with latency/throughput trade-offs</li><li> â€“ Optimizes performance based on fluctuating demand</li><li><strong>LLM-Aware Request Routing</strong> â€“ Eliminates unnecessary KV cache re-computation</li><li><strong>Accelerated Data Transfer</strong> â€“ Reduces inference response time using NIXL</li><li> â€“ Leverages multiple memory hierarchies for higher throughput</li></ul><p>Built in Rust for performance and Python for extensibility, Dynamo is fully open-source with an OSS-first development approach.</p><blockquote><p> â€” Detailed compatibility including LoRA, Request Migration, Speculative Decoding, and feature interactions.</p></blockquote><p>Want to help shape the future of distributed LLM inference? We welcome contributors at all levelsâ€”from doc fixes to new features.</p><p>The Dynamo team recommends the  Python package manager, although any way works. Install uv:</p><pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre><h3>Install Python Development Headers</h3><p>Backend engines require Python development headers for JIT compilation. Install them with:</p><pre><code>sudo apt install python3-dev\n</code></pre><p>We publish Python wheels specialized for each of our supported engines: vllm, sglang, and trtllm. The examples that follow use SGLang; continue reading for other engines.</p><pre><code>uv venv venv\nsource venv/bin/activate\nuv pip install pip\n\n# Choose one\nuv pip install \"ai-dynamo[sglang]\"  #replace with [vllm], [trtllm], etc.\n</code></pre><p>Before trying out Dynamo, you can verify your system configuration and dependencies:</p><pre><code>python3 deploy/sanity_check.py\n</code></pre><p>This is a quick check for system resources, development tools, LLM frameworks, and Dynamo components.</p><h3>Running an LLM API Server</h3><p>Dynamo provides a simple way to spin up a local set of inference components including:</p><ul><li><strong>OpenAI Compatible Frontend</strong> â€“ High performance OpenAI compatible http api server written in Rust.</li><li><strong>Basic and Kv Aware Router</strong> â€“ Route and load balance traffic to a set of workers.</li><li> â€“ Set of pre-configured LLM serving engines.</li></ul><pre><code># Start an OpenAI compatible HTTP server with prompt templating, tokenization, and routing.\n# For local dev: --store-kv file avoids etcd (workers and frontend must share a disk)\npython3 -m dynamo.frontend --http-port 8000 --store-kv file\n\n# Start the SGLang engine. You can run several of these for the same or different models.\n# The frontend will discover them automatically.\npython3 -m dynamo.sglang --model-path deepseek-ai/DeepSeek-R1-Distill-Llama-8B --store-kv file\n</code></pre><blockquote><p> vLLM workers publish KV cache events by default, which requires NATS. For dependency-free local development with vLLM, add <code>--kv-events-config '{\"enable_kv_cache_events\": false}'</code>. This keeps local prefix caching enabled while disabling event publishing. See <a href=\"https://raw.githubusercontent.com/ai-dynamo/dynamo/main/#service-discovery-and-messaging\">Service Discovery and Messaging</a> for details.</p></blockquote><pre><code>curl localhost:8000/v1/chat/completions   -H \"Content-Type: application/json\"   -d '{\n    \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n    \"messages\": [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello, how are you?\"\n    }\n    ],\n    \"stream\":false,\n    \"max_tokens\": 300\n  }' | jq\n</code></pre><p>Rerun with  and change  in the request to  to get the responses as soon as the engine issues them.</p><p>For production deployments on Kubernetes clusters with multiple GPUs.</p><p>Pre-built deployment configurations for common models and topologies:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>Dynamo is inference engine agnostic. Install the wheel for your chosen engine and run with <code>python3 -m dynamo.&lt;engine&gt; --help</code>.</p><table><thead><tr></tr></thead><tbody><tr><td><code>uv pip install ai-dynamo[vllm]</code></td><td>Broadest feature coverage</td></tr><tr><td><code>uv pip install ai-dynamo[sglang]</code></td></tr><tr><td><code>pip install --pre --extra-index-url https://pypi.nvidia.com ai-dynamo[trtllm]</code></td></tr></tbody></table><blockquote><p> TensorRT-LLM requires  (not ) due to URL-based dependencies. See the <a href=\"https://raw.githubusercontent.com/ai-dynamo/dynamo/main/docs/backends/trtllm/\">TRT-LLM guide</a> for container setup and prerequisites.</p></blockquote><p>Use  to specify which GPUs to use. Engine-specific options (context length, multi-GPU, etc.) are documented in each backend guide.</p><h2>Service Discovery and Messaging</h2><p>Dynamo uses TCP for inter-component communication. External services are optional for most deployments:</p><table><tbody><tr><td>K8s-native discovery; TCP request plane</td></tr><tr><td>Pass ; vLLM also needs <code>--kv-events-config '{\"enable_kv_cache_events\": false}'</code></td></tr><tr><td>Prefix caching enabled by default requires NATS</td></tr></tbody></table><p>For local development without external dependencies, pass  (avoids etcd) to both the frontend and workers. vLLM users should also pass <code>--kv-events-config '{\"enable_kv_cache_events\": false}'</code> to disable KV event publishing (avoids NATS) while keeping local prefix caching enabled; SGLang and TRT-LLM don't require this flag.</p><p>For distributed non-Kubernetes deployments or KV-aware routing:</p><ul><li><a href=\"https://etcd.io/\">etcd</a> can be run directly as .</li><li><a href=\"https://nats.io/\">nats</a> needs JetStream enabled: .</li></ul><p>To quickly setup both: <code>docker compose -f deploy/docker-compose.yml up -d</code></p><p>Dynamo provides comprehensive benchmarking tools:</p><h2>Frontend OpenAPI Specification</h2><p>The OpenAI-compatible frontend exposes an OpenAPI 3 spec at . To generate without running the server:</p><pre><code>cargo run -p dynamo-llm --bin generate-frontend-openapi\n</code></pre><p>This writes to <code>docs/frontends/openapi.json</code>.</p><p>For contributors who want to build Dynamo from source rather than installing from PyPI.</p><pre><code>sudo apt install -y build-essential libhwloc-dev libudev-dev pkg-config libclang-dev protobuf-compiler python3-dev cmake\n</code></pre><pre><code># if brew is not installed on your system, install it\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre><pre><code>brew install cmake protobuf\n\n## Check that Metal is accessible\nxcrun -sdk macosx metal\n</code></pre><p>If Metal is accessible, you should see an error like <code>metal: error: no input files</code>, which confirms it is installed correctly.</p><pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\nsource $HOME/.cargo/env\n</code></pre><h2>3. Create a Python Virtual Environment</h2><p>Follow the instructions in <a href=\"https://docs.astral.sh/uv/#installation\">uv installation</a> guide to install uv if you don't have  installed. Once uv is installed, create a virtual environment and activate it.</p><pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre><ul><li>Create a virtual environment</li></ul><pre><code>uv venv dynamo\nsource dynamo/bin/activate\n</code></pre><pre><code>uv pip install pip maturin\n</code></pre><p><a href=\"https://github.com/PyO3/maturin\">Maturin</a> is the Rust&lt;-&gt;Python bindings build tool.</p><h2>5. Build the Rust Bindings</h2><pre><code>cd lib/bindings/python\nmaturin develop --uv\n</code></pre><h2>6. Install GPU Memory Service</h2><p>The GPU Memory Service is a Python package with a C++ extension. It requires only Python development headers and a C++ compiler (g++).</p><pre><code>cd $PROJECT_ROOT\nuv pip install -e lib/gpu_memory_service\n</code></pre><pre><code>cd $PROJECT_ROOT\nuv pip install -e .\n</code></pre><p>You should now be able to run <code>python3 -m dynamo.frontend</code>.</p><p>For local development, pass  to avoid external dependencies (see Service Discovery and Messaging section).</p><p>Set the environment variable  to adjust the logging level; for example, . It has the same syntax as .</p><p>If you use vscode or cursor, we have a .devcontainer folder built on <a href=\"https://code.visualstudio.com/docs/devcontainers/containers\">Microsofts Extension</a>. For instructions see the <a href=\"https://raw.githubusercontent.com/ai-dynamo/dynamo/main/.devcontainer/README.md\">ReadMe</a> for more details.</p>","contentLength":7063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenBMB/UltraRAG","url":"https://github.com/OpenBMB/UltraRAG","date":1769222420,"author":"","guid":419763,"unread":true,"content":"<p>UltraRAG v3: A Low-Code MCP Framework for Building Complex and Innovative RAG Pipelines</p><h3 align=\"center\"> Less Code, Lower Barrier, Faster Deployment </h3><ul><li>[2026.01.23] ğŸ‰ UltraRAG 3.0 Released: Say no to \"black box\" developmentâ€”make every line of reasoning logic clearly visible ğŸ‘‰|<a href=\"https://github.com/OpenBMB/UltraRAG/raw/page/project/blog/en/ultrarag3_0.md\">ğŸ“– Blog</a>|</li><li>[2026.01.20] ğŸ‰ AgentCPM-Report Model Released! DeepResearch is finally localized: 8B on-device writing agent AgentCPM-Report is open-sourced ğŸ‘‰ |<a href=\"https://huggingface.co/openbmb/AgentCPM-Report\">ğŸ¤— Model</a>|</li></ul><p>Designed for research exploration and industrial prototyping, UltraRAG standardizes core RAG components (Retriever, Generation, etc.) as independent , combined with the powerful workflow orchestration capabilities of the . Developers can achieve precise orchestration of complex control structures such as conditional branches and loops simply through YAML configuration.</p><p>UltraRAG UI transcends the boundaries of traditional chat interfaces, evolving into a visual RAG Integrated Development Environment (IDE) that combines orchestration, debugging, and demonstration.</p><p>The system features a powerful built-in Pipeline Builder that supports bidirectional real-time synchronization between \"Canvas Construction\" and \"Code Editing,\" allowing for granular online adjustments of pipeline parameters and prompts. Furthermore, it introduces an Intelligent AI Assistant to empower the entire development lifecycle, from pipeline structural design to parameter tuning and prompt generation. Once constructed, logic flows can be converted into interactive dialogue systems with a single click. The system seamlessly integrates Knowledge Base Management components, enabling users to build custom knowledge bases for document Q&amp;A. This truly realizes a one-stop closed loop, spanning from underlying logic construction and data governance to final application deployment.</p><ul><li><p>ğŸš€ <strong>Low-Code Orchestration of Complex Workflows</strong></p><ul><li>: Natively supports control structures such as sequential, loop, and conditional branches. Developers only need to write YAML configuration files to implement complex iterative RAG logic in dozens of lines of code.</li></ul></li><li><p>âš¡ <strong>Modular Extension and Reproduction</strong></p><ul><li>: Based on the MCP architecture, functions are decoupled into independent Servers. New features only need to be registered as function-level Tools to seamlessly integrate into workflows, achieving extremely high reusability.</li></ul></li><li><p>ğŸ“Š <strong>Unified Evaluation and Benchmark Comparison</strong></p><ul><li>: Built-in standardized evaluation workflows, ready-to-use mainstream research benchmarks. Through unified metric management and baseline integration, significantly improves experiment reproducibility and comparison efficiency.</li></ul></li><li><p>âœ¨ <strong>Rapid Interactive Prototype Generation</strong></p><ul><li>: Say goodbye to tedious UI development. With just one command, Pipeline logic can be instantly converted into an interactive conversational Web UI, shortening the distance from algorithm to demonstration.</li></ul></li></ul><p>We provide two installation methods: local source code installation (recommended using  for package management) and Docker container deployment</p><h3>Method 1: Source Code Installation</h3><p>We strongly recommend using <a href=\"https://github.com/astral-sh/uv\">uv</a> to manage Python environments and dependencies, as it can greatly improve installation speed.</p><p>If you haven't installed uv yet, please execute:</p><pre><code>## Direct installation\npip install uv\n## Download\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre><pre><code>git clone https://github.com/OpenBMB/UltraRAG.git --depth 1\ncd UltraRAG\n</code></pre><p>Choose one of the following modes to install dependencies based on your use case:</p><p><strong>A: Create a New Environment</strong> Use  to automatically create a virtual environment and synchronize dependencies:</p><ul><li><p>Core dependencies: If you only need to run basic core functions, such as only using UltraRAG UI:</p></li><li><p>Full installation: If you want to fully experience UltraRAG's retrieval, generation, corpus processing, and evaluation functions, please run:</p></li><li><p>On-demand installation: If you only need to run specific modules, keep the corresponding  as needed, for example:</p><pre><code>uv sync --extra retriever   # Retrieval module only\nuv sync --extra generation  # Generation module only\n</code></pre></li></ul><p>Once installed, activate the virtual environment:</p><pre><code># Windows CMD\n.venv\\Scripts\\activate.bat\n\n# Windows Powershell\n.venv\\Scripts\\Activate.ps1\n\n# macOS / Linux\nsource .venv/bin/activate\n</code></pre><p><strong>B: Install into an Existing Environment</strong> To install UltraRAG into your currently active Python environment, use :</p><pre><code># Core dependencies\nuv pip install -e .\n\n# Full installation\nuv pip install -e \".[all]\"\n\n# On-demand installation\nuv pip install -e \".[retriever]\"\n</code></pre><h3>Method 2: Docker Container Deployment</h3><p>If you prefer not to configure a local Python environment, you can deploy using Docker.</p><pre><code># 1. Clone the repository\ngit clone https://github.com/OpenBMB/UltraRAG.git --depth 1\ncd UltraRAG\n\n# 2. Prepare the image (choose one)\n# Option A: Pull from Docker Hub\ndocker pull hdxin2002/ultrarag:v0.3.0-base-cpu # Base version (CPU)\ndocker pull hdxin2002/ultrarag:v0.3.0-base-gpu # Base version (GPU)\ndocker pull hdxin2002/ultrarag:v0.3.0          # Full version (GPU)\n\n# Option B: Build locally\ndocker build -t ultrarag:v0.3.0 .\n\n# 3. Start container (port 5050 is automatically mapped)\ndocker run -it --gpus all -p 5050:5050 &lt;docker_image_name&gt;\n</code></pre><pre><code># Start the container (Port 5050 is mapped by default)\ndocker run -it --gpus all -p 5050:5050 &lt;docker_image_name&gt;\n</code></pre><p>Note: After the container starts, UltraRAG UI will run automatically. You can directly access  in your browser to use it.</p><p>After installation, run the following example command to check if the environment is normal:</p><pre><code>ultrarag run examples/sayhello.yaml\n</code></pre><p>If you see the following output, the installation is successful:</p><p>We provide complete tutorial examples from beginner to advanced. Whether you are conducting academic research or building industrial applications, you can find guidance here. Welcome to visit the <a href=\"https://ultrarag.openbmb.cn/pages/en/getting_started/introduction\">Documentation</a> for more details.</p><p>Designed for researchers, providing data, experimental workflows, and visualization analysis tools.</p><ul><li><a href=\"https://ultrarag.openbmb.cn/pages/en/getting_started/quick_start\">Getting Started</a>: Learn how to quickly run standard RAG experimental workflows based on UltraRAG.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/develop_guide/dataset\">Evaluation Data</a>: Download the most commonly used public evaluation datasets in the RAG field and large-scale retrieval corpora, directly for research benchmark testing.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/develop_guide/case_study\">Case Analysis</a>: Provides a visual Case Study interface to deeply track each intermediate output of the workflow, assisting in analysis and error attribution.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/develop_guide/code_integration\">Code Integration</a>: Learn how to directly call UltraRAG components in Python code to achieve more flexible customized development.</li></ul><p>Designed for developers and end users, providing complete UI interaction and complex application cases.</p><ul><li><a href=\"https://ultrarag.openbmb.cn/pages/en/ui/start\">Quick Start</a>: Learn how to start UltraRAG UI and familiarize yourself with various advanced configurations in administrator mode.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/ui/prepare\">Deployment Guide</a>: Detailed production environment deployment tutorials, covering the setup of Retriever, Generation models (LLM), and Milvus vector database.</li><li><a href=\"https://ultrarag.openbmb.cn/pages/en/demo/deepresearch\">Deep Research</a>: Flagship case, deploy a Deep Research Pipeline. Combined with the AgentCPM-Report model, it can automatically perform multi-step retrieval and integration to generate tens of thousands of words of survey reports.</li></ul><p>Thanks to the following contributors for their code submissions and testing. We also welcome new members to join us in collectively building a comprehensive RAG ecosystem!</p><p>You can contribute by following the standard process: <strong>Fork this repository â†’ Submit Issues â†’ Create Pull Requests (PRs)</strong>.</p><a href=\"https://github.com/OpenBMB/UltraRAG/contributors\"><img src=\"https://contrib.rocks/image?repo=OpenBMB/UltraRAG&amp;nocache=true\"></a><p>If you find this repository helpful for your research, please consider giving us a â­ to show your support.</p><a href=\"https://star-history.com/#OpenBMB/UltraRAG&amp;Date\"></a><ul><li>For technical issues and feature requests, please use <a href=\"https://github.com/OpenBMB/UltraRAG/issues\">GitHub Issues</a>.</li><li>For questions about usage, feedback, or any discussions related to RAG technologies, you are welcome to join our <a href=\"https://github.com/OpenBMB/UltraRAG/raw/main/docs/wechat_qr.png\">WeChat group</a>, <a href=\"https://github.com/OpenBMB/UltraRAG/raw/main/docs/feishu_qr.png\">Feishu group</a>, and <a href=\"https://discord.gg/yRFFjjJnnS\">Discord</a> to exchange ideas with us.</li><li>If you have any questions, feedback, or would like to get in touch, please feel free to reach out to us via email at <a href=\"mailto:yanyk.thu@gmail.com\">yanyk.thu@gmail.com</a></li></ul>","contentLength":7820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"deepseek-ai/FlashMLA","url":"https://github.com/deepseek-ai/FlashMLA","date":1769136244,"author":"","guid":418534,"unread":true,"content":"<p>FlashMLA: Efficient Multi-head Latent Attention Kernels</p><p>FlashMLA is DeepSeek's library of optimized attention kernels, powering the <a href=\"https://github.com/deepseek-ai/DeepSeek-V3\">DeepSeek-V3</a> and <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">DeepSeek-V3.2-Exp</a> models. This repository contains the following implementations:</p><p><em>These kernels power DeepSeek Sparse Attention (DSA), as introduced in <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">this paper</a>.</em></p><ul><li>Token-level sparse attention for the prefill stage</li><li>Token-level sparse attention for the decoding stage, with FP8 KV cache</li></ul><ul><li>Dense attention for the prefill stage</li><li>Dense attention for the decoding stage</li></ul><ul><li><strong>2025.09.29 Release of Sparse Attention Kernels</strong>: With the launch of <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">DeepSeek-V3.2</a>, we are releasing the corresponding token-level sparse attention kernels. These kernels power the model's DeepSeek Sparse Attention (DSA) and achieve up to 640 TFlops during prefilling and 410 TFlops during decoding. We also release a deep-dive blog for our new FP8 sparse decoding kernel. Check it out <a href=\"https://raw.githubusercontent.com/deepseek-ai/FlashMLA/main/docs/20250929-hopper-fp8-sparse-deep-dive.md\">here</a>.</li><li><strong>2025.08.01 Kernels for MHA on SM100</strong>: Thanks to <a href=\"https://github.com/deepseek-ai/FlashMLA/pull/76\">NVIDIA's PR</a> for MHA forward / backward kernels on SM100!</li><li><strong>2025.04.22 Deep-Dive Blog</strong>: We'd love to share the technical details behind the new FlashMLA kernel! Check out our deep-dive write-up <a href=\"https://raw.githubusercontent.com/deepseek-ai/FlashMLA/main/docs/20250422-new-kernel-deep-dive.md\">here</a>.</li><li><strong>2025.04.22 Performance Update</strong>: We're excited to announce the new release of Flash MLA, which delivers 5% ~ 15% performance improvement for compute-bound workloads, achieving up to 660 TFlops on NVIDIA H800 SXM5 GPUs. The interface of the new version is fully compatible with the old one. Simply upgrade to the new version for an immediate performance boost! ğŸš€ğŸš€ğŸš€</li></ul><h4>Test &amp; benchmark MLA decoding (Sparse &amp; Dense):</h4><pre><code>python tests/test_flash_mla_dense_decoding.py\npython tests/test_flash_mla_sparse_decoding.py\n</code></pre><p>The dense MLA decoding kernel achieves up to 3000 GB/s in memory-bound configuration and 660 TFLOPS in computation-bound configuration on H800 SXM5 with CUDA 12.8. The token-level sparse MLA decoding kernel (which uses an FP8 KV cache while performing the matrix multiplication in bfloat16) achieves 410 TFLOPS in compute-bound configuration on H800 SXM5 with CUDA 12.8, and achieves up to 350 TFlops on B200 (which is not really optimized yet).</p><h4>Test &amp; benchmark MHA prefill (Dense):</h4><pre><code>python tests/test_fmha_sm100.py\n</code></pre><p>It achieves up to 1460 TFlops in forward and 1000 TFlops in backward computation on B200, as reported by NVIDIA.</p><h4>Test &amp; benchmark MLA prefill (Sparse):</h4><pre><code>python tests/test_flash_mla_sparse_prefill.py\n</code></pre><p>It achieves up to 640 TFlops in forward computation on H800 SXM5 with CUDA 12.8, and achieves up to 1450 TFlops on B200, CUDA 12.9.</p><ul><li>SM90 / SM100 (See the support matrix below)</li><li>CUDA 12.8 and above (CUDA 12.9+ is required for SM100 kernels)</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>[1]: For more details on using FP8 KV cache, see documents below.</p><p>[2]: Here \"MLA Mode\" refers to the mode used for MLA calculation. MQA stands for Multi-Query Attention mode (i.e.  = 576 with  = 512), while MHA stands for Multi-Head Attention mode (i.e.  = 192 / 128 with  = 128). For a detailed explanation of these modes, please refer to the appendix of <a href=\"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp\">DeepSeek V3.2's Paper</a>.</p><pre><code>git clone https://github.com/deepseek-ai/FlashMLA.git flash-mla\ncd flash-mla\ngit submodule update --init --recursive\npip install -v .\n</code></pre><p>To use the MLA decoding kernels, call get_mla_metadata once before the decoding loop to get the tile scheduler metadata. Then, call flash_mla_with_kvcache in each decoding step. For example:</p><pre><code>from flash_mla import get_mla_metadata, flash_mla_with_kvcache\n\ntile_scheduler_metadata, num_splits = get_mla_metadata(\n    cache_seqlens,\n    s_q * h_q // h_kv,\n    h_kv,\n    h_q,\n    is_fp8,\n    topk,\n)\n\nfor i in range(num_layers):\n    ...\n    o_i, lse_i = flash_mla_with_kvcache(\n        q_i, kvcache_i, block_table, cache_seqlens, dv,\n        tile_scheduler_metadata, num_splits,\n        is_causal, is_fp8_kvcache, indices,\n    )\n    ...\n</code></pre><ul><li> is the number of q tokens per q sequence. If MTP (speculative decoding) is disabled, it should be 1.</li><li> is the number of key-value heads.</li><li> is the number of query heads.</li></ul><p> If  is set to , the kernel reads the KV cache in the \"FP8 with scale\" format (described below). It dequantizes the cache to bfloat16 and performs attention computation in bfloat16. The output is also in bfloat16.</p><p>In the \"FP8 with scale\" format, each token's KV cache is 656 Bytes, structured as:</p><ul><li> The \"quantized NoPE\" part, containing 512  values.</li><li> Scale factors, containing 4  values. The first  is the scale for the first 128  values, the second for the next 128, and so on.</li><li> The \"RoPE\" part, containing 64  values. This part is not quantized for accuracy.</li></ul><p>See  for quantization and dequantization details.</p><p><strong>Sparse Attention ( tensor):</strong> The  tensor (if provided) enables token-level sparse attention by instructing the kernel to compute attention only for specified tokens.</p><ul><li> should be a 3D tensor of shape <code>(batch_size, seq_len_q, topk)</code>.</li><li><code>indices_in_kvcache[i][j][k] = (the index of the page block where token t resides) * page_block_size + (the offset of token t within the page block)</code>, where  is the k-th token for the j-th query sequence in the i-th batch. Since the index of the page block has already been encoded into , the kernel does not require the  parameter.</li><li> Set invalid indices to .</li></ul><p> The kernel returns , where:</p><ul><li> is the attention result.</li><li> is the log-sum-exp value of the attention scores for each query head.</li></ul><p>See <code>tests/test_flash_mla_decoding.py</code> for a complete example.</p><p>For the sparse MLA prefill kernel, call  directly with the following parameters:</p><ul><li>: Query tensor of shape </li><li>: Key-Value tensor of shape </li><li>: Indices tensor of shape </li></ul><p> This kernel does not support a batch dimension. For multi-batch inference, reshape the input tensors and adjust the  parameter to simulate batch processing.</p><p> Set invalid entries in  to  or any number .</p><p><strong>Return Values and Equivalent PyTorch Code:</strong> The kernel returns . This is equivalent to the following PyTorch operations:</p><pre><code>Q: [s_q, h_q, d_qk], bfloat16\nkv: [s_kv, h_kv, d_qk], bfloat16\nindices: [s_q, h_kv, topk], int32\n\nkv = kv.squeeze(1)  # [s_kv, d_qk], h_kv must be 1\nindices = indices.squeeze(1)    # [s_q, topk]\nfocused_kv = kv[indices]    # For the i-th sequence (s_q), the corresponding KV tokens are selected from the KV cache based on indices[i, :]. This operation results in a tensor of shape [s_q, topk, d_qk].\n\nP = (Q @ focused_kv.transpose(-1, -2)) * sm_scale * math.log2(math.e)    # [s_q, h_q, topk]\nmax_logits = P.max(dim=-1) # [s_q, h_q]\nlse = log2sumexp2(P, dim=-1, base=2)   # [s_q, h_q]ï¼Œ\"log2sumexp2\" means that the exponentiation and logarithm are base-2\nS = exp2(P - lse)      # [s_q, h_q, topk]\nout = S @ focused_kv  # [s_q, h_q, d_qk]\n\nreturn (out, max_logits, lse)\n</code></pre><p>See <code>tests/test_flash_mla_prefill.py</code> for a complete example.</p><p>This kernel implements the standard dense Multi-Head Attention (MHA) forward and backward operations. It can be called using:</p><ul><li><code>flash_attn_varlen_qkvpacked_func</code></li><li><code>flash_attn_varlen_kvpacked_func</code></li></ul><p>The usage is similar to the  package. See  for a complete example.</p><p>For MetaX GPUs, visit the official website: <a href=\"https://www.metax-tech.com\">MetaX</a>.</p><p>For the Moore Threads GPU, visit the official website: <a href=\"https://www.mthreads.com/\">Moore Threads</a>.</p><p>For the Intellifusion NNP, visit the official website: <a href=\"https://www.intellif.com\">Intellifusion</a>.</p><p>For AMD Instinct GPUs, visit the official website: <a href=\"https://www.amd.com/en/products/accelerators/instinct.html\">AMD Instinct</a>.</p><p>The corresponding FlashMLA version can be found at: <a href=\"https://github.com/ROCm/aiter/raw/main/aiter/mla.py\">AITER/MLA</a></p><pre><code>@misc{flashmla2025,\n      title={FlashMLA: Efficient Multi-head Latent Attention Kernels},\n      author={Jiashi Li, Shengyu Liu},\n      year={2025},\n      publisher = {GitHub},\n      howpublished = {\\url{https://github.com/deepseek-ai/FlashMLA}},\n}\n</code></pre>","contentLength":7389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"remotion-dev/remotion","url":"https://github.com/remotion-dev/remotion","date":1769136244,"author":"","guid":418535,"unread":true,"content":"<p>ğŸ¥ Make videos programmatically with React</p><p>Remotion is a framework for <strong>creating videos programmatically using React.</strong></p><h2>Why create videos in React?</h2><ul><li><strong>Leverage web technologies</strong>: Use all of CSS, Canvas, SVG, WebGL, etc.</li><li>: Use variables, functions, APIs, math and algorithms to create new effects</li><li>: Reusable components, Powerful composition, Fast Refresh, Package ecosystem</li></ul><p>If you already have Node.JS installed, type</p><p>Be aware of that Remotion has a special license and requires obtaining a company license in some cases. Read the <a href=\"https://raw.githubusercontent.com/remotion-dev/remotion/main/LICENSE.md\">LICENSE</a> page for more information.</p>","contentLength":552,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"block/goose","url":"https://github.com/block/goose","date":1769136244,"author":"","guid":418536,"unread":true,"content":"<p>an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM</p><p>goose is your on-machine AI agent, capable of automating complex development tasks from start to finish. More than just code suggestions, goose can build entire projects from scratch, write and execute code, debug failures, orchestrate workflows, and interact with external APIs - .</p><p>Whether you're prototyping an idea, refining existing code, or managing intricate engineering pipelines, goose adapts to your workflow and executes tasks with precision.</p><p>Designed for maximum flexibility, goose works with any LLM and supports multi-model configuration to optimize performance and cost, seamlessly integrates with MCP servers, and is available as both a desktop app as well as CLI - making it the ultimate AI assistant for developers who want to move faster and focus on innovation.</p><blockquote><p>Why did the developer choose goose as their AI agent?</p><p>Because it always helps them \"migrate\" their code to production! ğŸš€</p></blockquote>","contentLength":1017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mastra-ai/mastra","url":"https://github.com/mastra-ai/mastra","date":1769136244,"author":"","guid":418537,"unread":true,"content":"<p>From the team behind Gatsby, Mastra is a framework for building AI-powered applications and agents with a modern TypeScript stack.</p><p>Mastra is a framework for building AI-powered applications and agents with a modern TypeScript stack.</p><p>It includes everything you need to go from early prototypes to production-ready applications. Mastra integrates with frontend and backend frameworks like React, Next.js, and Node, or you can deploy it anywhere as a standalone server. It's the easiest way to build, tune, and scale reliable AI products.</p><p>Purpose-built for TypeScript and designed around established AI patterns, Mastra gives you everything you need to build great AI applications out-of-the-box.</p><ul><li><p><a href=\"https://mastra.ai/models\"></a> - Connect to 40+ providers through one standard interface. Use models from OpenAI, Anthropic, Gemini, and more.</p></li><li><p><a href=\"https://mastra.ai/docs/agents/overview\"></a> - Build autonomous agents that use LLMs and tools to solve open-ended tasks. Agents reason about goals, decide which tools to use, and iterate internally until the model emits a final answer or an optional stopping condition is met.</p></li><li><p><a href=\"https://mastra.ai/docs/workflows/overview\"></a> - When you need explicit control over execution, use Mastra's graph-based workflow engine to orchestrate complex multi-step processes. Mastra workflows use an intuitive syntax for control flow (, , ).</p></li><li><p><a href=\"https://mastra.ai/docs/workflows/suspend-and-resume\"></a> - Suspend an agent or workflow and await user input or approval before resuming. Mastra uses <a href=\"https://mastra.ai/docs/server-db/storage\">storage</a> to remember execution state, so you can pause indefinitely and resume where you left off.</p></li><li><p> - Give your agents the right context at the right time. Provide <a href=\"https://mastra.ai/docs/memory/conversation-history\">conversation history</a>, <a href=\"https://mastra.ai/docs/rag/overview\">retrieve</a> data from your sources (APIs, databases, files), and add human-like <a href=\"https://mastra.ai/docs/memory/working-memory\">working</a> and <a href=\"https://mastra.ai/docs/memory/semantic-recall\">semantic</a> memory so your agents behave coherently.</p></li><li><p> - Bundle agents and workflows into existing React, Next.js, or Node.js apps, or ship them as standalone endpoints. When building UIs, integrate with agentic libraries like Vercel's AI SDK UI and CopilotKit to bring your AI assistant to life on the web.</p></li><li><p><a href=\"https://mastra.ai/docs/tools-mcp/mcp-overview\"></a> - Author Model Context Protocol servers, exposing agents, tools, and other structured resources via the MCP interface. These can then be accessed by any system or agent that supports the protocol.</p></li><li><p> - Shipping reliable agents takes ongoing insight, evaluation, and iteration. With built-in <a href=\"https://mastra.ai/docs/evals/overview\">evals</a> and <a href=\"https://mastra.ai/docs/observability/overview\">observability</a>, Mastra gives you the tools to observe, measure, and refine continuously.</p></li></ul><p>The  way to get started with Mastra is by running the command below:</p><p>Looking to contribute? All types of help are appreciated, from coding to testing and feature specification.</p><p>If you are a developer and would like to contribute with code, please open an issue to discuss before opening a Pull Request.</p><p>We have an <a href=\"https://discord.gg/BTYqqHKUrf\">open community Discord</a>. Come and say hello and let us know if you have any questions or need any help getting things running.</p><p>It's also super helpful if you leave the project a star here at the <a href=\"https://github.com/mastra-ai/mastra\">top of the page</a></p><p>We are committed to maintaining the security of this repo and of Mastra as a whole. If you discover a security finding we ask you to please responsibly disclose this to us at <a href=\"mailto:security@mastra.ai\">security@mastra.ai</a> and we will get back to you.</p>","contentLength":3032,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nexmoe/VidBee","url":"https://github.com/nexmoe/VidBee","date":1769136244,"author":"","guid":418538,"unread":true,"content":"<p>Download videos from almost any website worldwide</p><p>VidBee is a modern, open-source video downloader that lets you download videos and audios from 1000+ websites worldwide. Built with Electron and powered by yt-dlp, VidBee offers a clean, intuitive interface with powerful features for all your downloading needs, including RSS auto-download automation that automatically subscribes to feeds and downloads new videos from your favorite creators in the background.</p><p>VidBee is currently under active development, and feedback is welcome for any <a href=\"https://github.com/nexmoe/VidBee/issues\">issue</a> encountered.</p><blockquote><p>, You will receive all release notifications from GitHub without any delay ~</p></blockquote><a href=\"https://next.ossinsight.io/widgets/official/compose-last-28-days-stats?repo_id=1081230042\" target=\"_blank\" align=\"center\"></a><h3>ğŸŒ Global Video Download Support</h3><p>Download videos from almost any website worldwide through the powerful yt-dlp engine. Support for 1000+ sites including YouTube, TikTok, Instagram, Twitter, and many more.</p><h3>ğŸ¨ Best-in-class UI Experience</h3><p>Modern, clean interface with intuitive operations. One-click pause/resume/retry, real-time progress tracking, and comprehensive download queue management.</p><p>Automatically subscribe to RSS feeds and auto-download new videos in the background from your favorite creators across YouTube, TikTok, and more. Set up RSS subscriptions once, and VidBee will automatically download new uploads without manual intervention, perfect for keeping up with your favorite channels and creators.</p><p>You are welcome to join the open source community to build together. For more details, check out:</p><p>This project is distributed under the MIT License. See <a href=\"https://raw.githubusercontent.com/nexmoe/VidBee/main/LICENSE\"></a> for details.</p><ul><li><a href=\"https://github.com/yt-dlp/yt-dlp\">yt-dlp</a> - The powerful video downloader engine</li><li><a href=\"https://ffmpeg.org/\">FFmpeg</a> - The multimedia framework for video and audio processing</li><li><a href=\"https://www.electronjs.org/\">Electron</a> - Build cross-platform desktop apps</li><li><a href=\"https://vitejs.dev/\">Vite</a> - Next generation frontend tooling</li></ul>","contentLength":1701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"virattt/dexter","url":"https://github.com/virattt/dexter","date":1769136244,"author":"","guid":418539,"unread":true,"content":"<p>An autonomous agent for deep financial research</p><p>Dexter is an autonomous financial research agent that thinks, plans, and learns as it works. It performs analysis using task planning, self-reflection, and real-time market data. Think Claude Code, but built specifically for financial research.</p><img width=\"1098\" height=\"659\" alt=\"Screenshot 2026-01-21 at 5 25 10â€¯PM\" src=\"https://github.com/user-attachments/assets/3bcc3a7f-b68a-4f5e-8735-9d22196ff76e\"><p>Dexter takes complex financial questions and turns them into clear, step-by-step research plans. It runs those tasks using live market data, checks its own work, and refines the results until it has a confident, data-backed answer.</p><ul><li><strong>Intelligent Task Planning</strong>: Automatically decomposes complex queries into structured research steps</li><li>: Selects and executes the right tools to gather financial data</li><li>: Checks its own work and iterates until tasks are complete</li><li>: Access to income statements, balance sheets, and cash flow statements</li><li>: Built-in loop detection and step limits to prevent runaway execution</li></ul><img width=\"875\" height=\"558\" alt=\"Screenshot 2026-01-21 at 5 22 19â€¯PM\" src=\"https://github.com/user-attachments/assets/72d28363-69ea-4c74-a297-dfa60aa347f7\"><ul><li><a href=\"https://bun.com\">Bun</a> runtime (v1.0 or higher)</li><li>Financial Datasets API key (get <a href=\"https://financialdatasets.ai\">here</a>)</li><li>Tavily API key (get <a href=\"https://tavily.com\">here</a>) - optional, for web search</li></ul><p>If you don't have Bun installed, you can install it using curl:</p><pre><code>curl -fsSL https://bun.com/install | bash\n</code></pre><pre><code>powershell -c \"irm bun.sh/install.ps1|iex\"\n</code></pre><p>After installation, restart your terminal and verify Bun is installed:</p><pre><code>git clone https://github.com/virattt/dexter.git\ncd dexter\n</code></pre><ol start=\"2\"><li>Install dependencies with Bun:</li></ol><ol start=\"3\"><li>Set up your environment variables:</li></ol><pre><code># Copy the example environment file (from parent directory)\ncp env.example .env\n\n# Edit .env and add your API keys (if using cloud providers)\n# OPENAI_API_KEY=your-openai-api-key\n# ANTHROPIC_API_KEY=your-anthropic-api-key\n# GOOGLE_API_KEY=your-google-api-key\n# XAI_API_KEY=your-xai-api-key\n\n# (Optional) If using Ollama locally\n# OLLAMA_BASE_URL=http://127.0.0.1:11434\n\n# Other required keys\n# FINANCIAL_DATASETS_API_KEY=your-financial-datasets-api-key\n# TAVILY_API_KEY=your-tavily-api-key\n</code></pre><p>Run Dexter in interactive mode:</p><p>Or with watch mode for development:</p><ol></ol><p>: Please keep your pull requests small and focused. This will make it easier to review and merge.</p><p>This project is licensed under the MIT License.</p>","contentLength":2040,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["trending"]}