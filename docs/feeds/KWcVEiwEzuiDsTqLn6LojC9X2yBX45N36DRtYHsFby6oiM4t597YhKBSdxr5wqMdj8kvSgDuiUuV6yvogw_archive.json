{"id":"KWcVEiwEzuiDsTqLn6LojC9X2yBX45N36DRtYHsFby6oiM4t597YhKBSdxr5wqMdj8kvSgDuiUuV6yvogw","title":"GitHub All Languages Daily Trending","displayTitle":"Github Trending","url":"https://mshibanami.github.io/GitHubTrendingRSS/daily/all.xml","feedLink":"http://mshibanami.github.io/GitHubTrendingRSS","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":32,"items":[{"title":"microsoft/generative-ai-for-beginners","url":"https://github.com/microsoft/generative-ai-for-beginners","date":1751510306,"author":"","guid":181645,"unread":true,"content":"<p>21 Lessons, Get Started Building with Generative AI üîó https://microsoft.github.io/generative-ai-for-beginners/</p><h3>21 Lessons teaching everything you need to know to start building Generative AI applications</h3><h4>Supported via GitHub Action (Automated &amp; Always Up-to-Date)</h4><p>Learn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.</p><p>This course has 21 lessons. Each lesson covers its own topic so start wherever you like!</p><p>Lessons are labeled either \"Learn\" lessons explaining a Generative AI concept or \"Build\" lessons that explain a concept and code examples in both  and  when possible.</p><p>Each lesson also includes a \"Keep Learning\" section with additional learning tools.</p><h3>To run the code of this course, you can use either:</h3><p>We have created a  lesson to help you with setting up your development environment.</p><h2>üó£Ô∏è Meet Other Learners, Get Support</h2><ul><li>A short video introduction to the topic</li><li>A written lesson located in the README</li><li>Python and TypeScript code samples supporting Azure OpenAI and OpenAI API</li><li>Links to extra resources to continue your learning</li></ul><p>Special thanks to <a href=\"https://www.linkedin.com/in/john0isaac/\"></a> for creating all of the GitHub Actions and workflows</p><p><a href=\"https://www.linkedin.com/in/bernhard-merkle-738b73/\"></a> for making key contributions to each lesson to improve the learner and code experience.</p><p>Our team produces other courses! Check out:</p>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mrdoob/three.js","url":"https://github.com/mrdoob/three.js","date":1751510306,"author":"","guid":181646,"unread":true,"content":"<p>The aim of the project is to create an easy-to-use, lightweight, cross-browser, general-purpose 3D library. The current builds only include WebGL and WebGPU renderers but SVG and CSS3D renderers are also available as addons.</p><p>This code creates a scene, a camera, and a geometric cube, and it adds the cube to the scene. It then creates a  renderer for the scene and camera, and it adds that viewport to the  element. Finally, it animates the cube within the scene for the camera.</p><pre><code>import * as THREE from 'three';\n\nconst width = window.innerWidth, height = window.innerHeight;\n\n// init\n\nconst camera = new THREE.PerspectiveCamera( 70, width / height, 0.01, 10 );\ncamera.position.z = 1;\n\nconst scene = new THREE.Scene();\n\nconst geometry = new THREE.BoxGeometry( 0.2, 0.2, 0.2 );\nconst material = new THREE.MeshNormalMaterial();\n\nconst mesh = new THREE.Mesh( geometry, material );\nscene.add( mesh );\n\nconst renderer = new THREE.WebGLRenderer( { antialias: true } );\nrenderer.setSize( width, height );\nrenderer.setAnimationLoop( animate );\ndocument.body.appendChild( renderer.domElement );\n\n// animation\n\nfunction animate( time ) {\n\n\tmesh.rotation.x = time / 2000;\n\tmesh.rotation.y = time / 1000;\n\n\trenderer.render( scene, camera );\n\n}\n</code></pre><p>If everything goes well, you should see <a href=\"https://jsfiddle.net/v98k6oze/\">this</a>.</p><p>Cloning the repo with all its history results in a ~2 GB download. If you don't need the whole history you can use the  parameter to significantly reduce download size.</p><pre><code>git clone --depth=1 https://github.com/mrdoob/three.js.git\n</code></pre>","contentLength":1500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"microsoft/Mastering-GitHub-Copilot-for-Paired-Programming","url":"https://github.com/microsoft/Mastering-GitHub-Copilot-for-Paired-Programming","date":1751510306,"author":"","guid":181647,"unread":true,"content":"<p>A multi-module course teaching everything you need to know about using GitHub Copilot as an AI Peer Programming resource.</p><p>Unlock the next generation of collaborative coding with our newly updated, in-depth course: Mastering GitHub Copilot. This multi-module, 10-hour program now features GitHub Copilot's revolutionary Agent Mode, transforming Copilot from a passive assistant into a proactive AI coding partner that works with you‚Äîand for you.</p><p>Whether you're just starting out or an experienced developer, this course equips you to fully harness GitHub Copilot‚Äôs AI capabilities, including real-time autonomous code execution, intelligent problem-solving, and workflow automation. You'll learn how to collaborate with AI using natural-language prompts that initiate multi-step solutions‚Äîfrom initial planning and architecture suggestions to code generation, testing, and iteration.</p><p>To get started, make sure to follow the instructions on how to fork the lessons into your own GitHub account. This will allow you to modify the code and complete the challenges at your own pace.</p><p>To use GitHub Copilot, you must have an active GitHub Copilot subscription.</p><p>To make it easier to revisit this repository in the future, you can also <a href=\"https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-113596-abartolo\">star (üåü) this repo</a> this repo.</p><p>Below are links to each lesson‚Äîfeel free to explore and dive into any topic that interests you the most!</p><h2>üöÄ Are you a startup or got an idea you want to launch?</h2><p>Here are ways you can contribute to this course:</p><ul><li>Send us your ideas, maybe your ideas for new lessons or exercises, and let us know how we can improve.</li></ul><ul><li>a written lesson located in the README</li><li>a challenge or assignment to apply your learning</li><li>links to extra resources to continue your learning</li></ul><table><thead><tr></tr></thead><tbody><tr><td align=\"center\">GitHub Copilot is an AI coding assistant that can help you write code faster and with less effort, allowing you to focus more energy on problem solving and collaboration.</td><td>In this exercise, you'll unlock the potential of this AI-powered coding assistant to accelerate your development process.</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"center\">Use GitHub Copilot, an AI pair programmer that offers autocomplete-style suggestions as you code, to work with JavaScript.</td><td>Enable the GitHub Copilot extension in Visual Studio Code. Craft prompts that can generate useful suggestions from GitHub Copilot. Use GitHub Copilot to improve a JavaScript project.</td></tr><tr><td align=\"center\">Use GitHub Copilot, an AI pair programmer that offers autocomplete-style suggestions as you code, to work with Python.</td><td>Enable the GitHub Copilot extension in Visual Studio Code. Craft prompts that can generate useful suggestions from GitHub Copilot. Use GitHub Copilot to improve a Python project.</td></tr><tr><td align=\"center\">Use GitHub Copilot, an AI pair programmer that offers autocomplete-style suggestions as you code, to work with C#.</td><td>Enable the GitHub Copilot extension in Visual Studio Code. Craft prompts that can generate useful suggestions from GitHub Copilot. Use GitHub Copilot to improve a C# Minimal API project.</td></tr><tr><td align=\"center\">Use GitHub Copilot to assist you in building a Python-based mini game.</td><td>Craft prompts that can generate useful suggestions from GitHub Copilot to incorporate gaming logic and improve your Python-based game.</td></tr></tbody></table><table><thead><tr></tr></thead><tbody><tr><td align=\"center\">Use advanced GitHub Copilot features like inline chat, slash commands, and agents.</td><td>Interact with GitHub Copilot with deeper context on your project and ask questions about it.</td></tr><tr><td align=\"center\">Apply advanced GitHub Copilot features to work with a challenging application working with a complex SQL query</td><td>Gain a clear understanding of how to work with extremely challenging SQL and yield better results when simple prompts don't work well</td></tr><tr><td align=\"center\">Leverage GitHub Copilot to upgrade a legacy Python project to the latest version of Python.</td><td>Apply techniques to overcome the challenges involved in working with legacy projects</td></tr><tr><td align=\"center\">Rewrite an existing application using a different language with the guidance of GitHub Copilot</td><td>Use advanced workflows with GitHub Copilot applicable when translating projects to different programming languages</td></tr></tbody></table><p>Our team produces other courses! Check out:</p>","contentLength":3947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"openssl/openssl","url":"https://github.com/openssl/openssl","date":1751510306,"author":"","guid":181648,"unread":true,"content":"<p>TLS/SSL and crypto library</p><p>OpenSSL is a robust, commercial-grade, full-featured Open Source Toolkit for the TLS (formerly SSL), DTLS and QUIC protocols.</p><p>The protocol implementations are based on a full-strength general purpose cryptographic library, which can also be used stand-alone. Also included is a cryptographic module validated to conform with FIPS standards.</p><p>OpenSSL is descended from the SSLeay library developed by Eric A. Young and Tim J. Hudson.</p><p>The OpenSSL toolkit includes:</p><ul><li><p> an implementation of all TLS protocol versions up to TLSv1.3 (<a href=\"https://tools.ietf.org/html/rfc8446\">RFC 8446</a>), DTLS protocol versions up to DTLSv1.2 (<a href=\"https://tools.ietf.org/html/rfc6347\">RFC 6347</a>) and the QUIC version 1 protocol (<a href=\"https://tools.ietf.org/html/rfc9000\">RFC 9000</a>).</p></li><li><p> a full-strength general purpose cryptographic library. It constitutes the basis of the TLS implementation, but can also be used independently.</p></li><li><p> the OpenSSL command line tool, a swiss army knife for cryptographic tasks, testing and analyzing. It can be used for</p><ul><li>creation of key parameters</li><li>creation of X.509 certificates, CSRs and CRLs</li><li>calculation of message digests</li><li>encryption and decryption</li><li>SSL/TLS/DTLS and client and server tests</li><li>handling of S/MIME signed or encrypted mail</li></ul></li></ul><p>Source code tarballs of the official releases can be downloaded from <a href=\"https://openssl-library.org/source/\">openssl-library.org/source/</a>. The OpenSSL project does not distribute the toolkit in binary form.</p><p>However, for a large variety of operating systems precompiled versions of the OpenSSL toolkit are available. In particular, on Linux and other Unix operating systems, it is normally recommended to link against the precompiled shared libraries provided by the distributor or vendor.</p><p>We also maintain a list of third parties that produce OpenSSL binaries for various Operating Systems (including Windows) on the <a href=\"https://github.com/openssl/openssl/wiki/Binaries\" title=\"List of third party OpenSSL binaries\">Binaries</a> page on our wiki.</p><h2>For Testing and Development</h2><p>Although testing and development could in theory also be done using the source tarballs, having a local copy of the git repository with the entire project history gives you much more insight into the code base.</p><p>The main OpenSSL Git repository is private. There is a public GitHub mirror of it at <a href=\"https://github.com/openssl/openssl\" title=\"OpenSSL GitHub Mirror\">github.com/openssl/openssl</a>, which is updated automatically from the former on every commit.</p><p>A local copy of the Git repository can be obtained by cloning it from the GitHub mirror using</p><pre><code>git clone https://github.com/openssl/openssl.git\n</code></pre><p>If you intend to contribute to OpenSSL, either to fix bugs or contribute new features, you need to fork the GitHub mirror and clone your public fork instead.</p><pre><code>git clone https://github.com/yourname/openssl.git\n</code></pre><p>This is necessary because all development of OpenSSL nowadays is done via GitHub pull requests. For more details, see <a href=\"https://raw.githubusercontent.com/openssl/openssl/master/#contributing\">Contributing</a>.</p><p>After obtaining the Source, have a look at the <a href=\"https://raw.githubusercontent.com/openssl/openssl/master/INSTALL.md\">INSTALL</a> file for detailed instructions about building and installing OpenSSL. For some platforms, the installation instructions are amended by a platform specific document.</p><p>There are some README.md files in the top level of the source distribution containing additional information on specific topics.</p><p>There are some tutorial and introductory pages on some important OpenSSL topics within the <a href=\"https://docs.openssl.org/master/man7/ossl-guide-introduction\" title=\"An introduction to OpenSSL\">OpenSSL Guide</a>.</p><p>The manual pages for the master branch and all current stable releases are available online.</p><p>There are numerous source code demos for using various OpenSSL capabilities in the <a href=\"https://raw.githubusercontent.com/openssl/openssl/master/demos\">demos subfolder</a>.</p><p>There is a <a href=\"https://github.com/openssl/openssl/wiki\" title=\"OpenSSL Wiki\">GitHub Wiki</a> which is currently not very active.</p><p>OpenSSL is licensed under the Apache License 2.0, which means that you are free to get and use it for commercial and non-commercial purposes as long as you fulfill its conditions.</p><p>There are various ways to get in touch. The correct channel depends on your requirement. See the <a href=\"https://raw.githubusercontent.com/openssl/openssl/master/SUPPORT.md\">SUPPORT</a> file for more details.</p><p>If you are interested and willing to contribute to the OpenSSL project, please take a look at the <a href=\"https://raw.githubusercontent.com/openssl/openssl/master/CONTRIBUTING.md\">CONTRIBUTING</a> file.</p><p>A number of nations restrict the use or export of cryptography. If you are potentially subject to such restrictions, you should seek legal advice before attempting to develop or distribute cryptographic code.</p><p>Copyright (c) 1998-2025 The OpenSSL Project Authors</p><p>Copyright (c) 1995-1998 Eric A. Young, Tim J. Hudson</p>","contentLength":4046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"tadata-org/fastapi_mcp","url":"https://github.com/tadata-org/fastapi_mcp","date":1751510306,"author":"","guid":181649,"unread":true,"content":"<p>Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p><p align=\"center\">Expose your FastAPI endpoints as Model Context Protocol (MCP) tools, with Auth!</p><ul><li><p> built in, using your existing FastAPI dependencies!</p></li><li><p> Not just another OpenAPI -&gt; MCP converter</p></li><li><p><strong>Zero/Minimal configuration</strong> required - just point it at your FastAPI app and it works</p></li><li><p> of your request models and response models</p></li><li><p> of all your endpoints, just as it is in Swagger</p></li><li><p> - Mount your MCP server to the same app, or deploy separately</p></li><li><p> - Uses FastAPI's ASGI interface directly for efficient communication</p></li></ul><p>If you prefer a managed hosted solution check out <a href=\"https://tadata.com\">tadata.com</a>.</p><p>We recommend using <a href=\"https://docs.astral.sh/uv/\">uv</a>, a fast Python package installer:</p><p>Alternatively, you can install with pip:</p><p>The simplest way to use FastAPI-MCP is to add an MCP server directly to your FastAPI application:</p><pre><code>from fastapi import FastAPI\nfrom fastapi_mcp import FastApiMCP\n\napp = FastAPI()\n\nmcp = FastApiMCP(app)\n\n# Mount the MCP server directly to your FastAPI app\nmcp.mount()\n</code></pre><p>That's it! Your auto-generated MCP server is now available at .</p><h2>Documentation, Examples and Advanced Usage</h2><p>FastAPI-MCP is designed as a native extension of FastAPI, not just a converter that generates MCP tools from your API. This approach offers several key advantages:</p><ul><li><p>: Secure your MCP endpoints using familiar FastAPI  for authentication and authorization</p></li><li><p>: Communicates directly with your FastAPI app using its ASGI interface, eliminating the need for HTTP calls from the MCP to your API</p></li><li><p>: Your FastAPI app doesn't need to run separately from the MCP server (though <a href=\"https://fastapi-mcp.tadata.com/advanced/deploy#deploying-separately-from-original-fastapi-app\">separate deployment</a> is also supported)</p></li></ul><p>This design philosophy ensures minimum friction when adding MCP capabilities to your existing FastAPI services.</p><h2>Development and Contributing</h2><p>Thank you for considering contributing to FastAPI-MCP! We encourage the community to post Issues and create Pull Requests.</p><p>Join <a href=\"https://join.slack.com/t/themcparty/shared_invite/zt-30yxr1zdi-2FG~XjBA0xIgYSYuKe7~Xg\">MCParty Slack community</a> to connect with other MCP enthusiasts, ask questions, and share your experiences with FastAPI-MCP.</p><ul><li>Python 3.10+ (Recommended 3.12)</li></ul><p>MIT License. Copyright (c) 2025 Tadata Inc.</p>","contentLength":2045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PaddlePaddle/ERNIE","url":"https://github.com/PaddlePaddle/ERNIE","date":1751510306,"author":"","guid":181650,"unread":true,"content":"<p>The official repository for ERNIE 4.5 and ERNIEKit ‚Äì its industrial-grade development toolkit based on PaddlePaddle.</p><h2>Introduction to ERNIE 4.5</h2><p>We introduce ERNIE 4.5, a new family of large-scale multimodal models comprising 10 distinct variants. The model family consist of Mixture-of-Experts (MoE) models with 47B and 3B active parameters, with the largest model having 424B total parameters, as well as a 0.3B dense model. For the MoE architecture, we propose a novel heterogeneous modality structure, which supports parameter sharing across modalities while also allowing dedicated parameters for each individual modality. This MoE architecture has the advantage to enhance multimodal understanding without compromising, and even improving, performance on text-related tasks. All of our models are trained with optimal efficiency using the <a href=\"https://github.com/PaddlePaddle/Paddle\">PaddlePaddle</a> deep learning framework, which also enables high-performance inference and streamlined deployment for them. We achieve 47% Model FLOPs Utilization (MFU) in our largest ERNIE 4.5 language model pre-training. Experimental results show that our models achieve state-of-the-art performance across multiple text and multimodal benchmarks, especially in instruction following, world knowledge memorization, visual understanding and multimodal reasoning. All models are publicly accessible under Apache 2.0 to support future research and development in the field. Additionally, we open source the development toolkits for ERNIE 4.5, featuring industrial-grade capabilities, resource-efficient training and inference workflows, and multi-hardware compatibility.</p><div align=\"center\"><table><thead><tr></tr><tr></tr></thead><tbody><tr><td rowspan=\"4\">Large Language Models (LLMs)</td></tr><tr><td rowspan=\"4\"> Vision-Language Models (VLMs)</td><td>ERNIE-4.5-VL-424B-A47B-Base</td></tr><tr><td>ERNIE-4.5-VL-28B-A3B-Base</td></tr><tr></tr></tbody></table></div><p><em>Note: All models (including pre-trained weights and inference code) have been released on <a href=\"https://huggingface.co/baidu\">ü§óHugging Face</a>, and <a href=\"https://aistudio.baidu.com/index\">AI Studio</a>. Check our <a href=\"https://yiyan.baidu.com/blog/posts/ernie4.5\">blog</a> for more details.</em></p><p>Our model family is characterized by three key innovations:</p><ol><li><p><strong>Multimodal Heterogeneous MoE Pre-Training:</strong> Our models are jointly trained on both textual and visual modalities to better capture the nuances of multimodal information and improve performance on tasks involving text understanding and generation, image understanding, and cross-modal reasoning. To achieve this without one modality hindering the learning of another, we designed a <em>heterogeneous MoE structure</em>, incorporated <em>modality-isolated routing</em>, and employed  and <em>multimodal token-balanced loss</em>. These architectural choices ensure that both modalities are effectively represented, allowing for mutual reinforcement during training.</p></li><li><p><strong>Scaling-Efficient Infrastructure:</strong> We propose a novel heterogeneous hybrid parallelism and hierarchical load balancing strategy for efficient training of ERNIE 4.5 models. By using intra-node expert parallelism, memory-efficient pipeline scheduling, FP8 mixed-precision training and finegrained recomputation methods, we achieve remarkable pre-training throughput. For inference, we propose <em>multi-expert parallel collaboration</em> method and <em>convolutional code quantization</em> algorithm to achieve 4-bit/2-bit lossless quantization. Furthermore, we introduce PD disaggregation with dynamic role switching for effective resource utilization to enhance inference performance for ERNIE 4.5 MoE models. Built on <a href=\"https://github.com/PaddlePaddle/Paddle\">PaddlePaddle</a>, ERNIE 4.5 delivers high-performance inference across a wide range of hardware platforms.</p></li><li><p><strong>Modality-Specific Post-Training:</strong> To meet the diverse requirements of real-world applications, we fine-tuned variants of the pre-trained model for specific modalities. Our LLMs are optimized for general-purpose language understanding and generation. The VLMs focuses on visuallanguage understanding and supports both thinking and non-thinking modes. Each model employed a combination of <em>Supervised Fine-tuning (SFT)</em>, <em>Direct Preference Optimization (DPO)</em> or a modified reinforcement learning method named <em>Unified Preference Optimization (UPO)</em> for post-training.</p></li></ol><h2>Performance and Benchmark Results</h2><p>ERNIE-4.5-300B-A47B-Base surpasses DeepSeek-V3-671B-A37B-Base on 22 out of 28 benchmarks, demonstrating leading performance across all major capability categories. This underscores the substantial improvements in generalization, reasoning, and knowledge-intensive tasks brought about by scaling up the ERNIE-4.5-Base model relative to other state-of-the-art large models. With a total parameter size of 21B (approximately 70% that of Qwen3-30B), ERNIE-4.5-21B-A3B-Base outperforms Qwen3-30B-A3B-Base on several math and reasoning benchmarks, including BBH and CMATH. ERNIE-4.5-21B-A3B-Base remains highly competitive given its significantly smaller model size, demonstrating notable parameter efficiency and favorable performance trade-offs.</p><p>ERNIE-4.5-300B-A47B, the post trained model, demonstrates significant strengths in instruction following and knowledge tasks, as evidenced by the state-of-the-art scores on benchmarks such as IFEval, Multi-IF, SimpleQA, and ChineseSimpleQA. The lightweight model ERNIE-4.5-21B-A3B achieves competitive performance compared to Qwen3-30B-A3B, despite having approximately 30% fewer total parameters.</p><p>In the non-thinking mode, ERNIE-4.5-VL exhibits outstanding proficiency in visual perception, document and chart understanding, and visual knowledge, performing strongly across a range of established benchmarks. Under the thinking mode, ERNIE-4.5-VL not only demonstrates enhanced reasoning abilities compared to the non-thinking mode, but also retains the strong perception capabilities of the latter. ERNIE-4.5-VL-424B-A47B delivers consistently strong results across the various multimodal evaluation benchmarks. Its thinking mode offers a distinct advantage on challenging benchmarks such as MathVista, MMMU, and VisualPuzzle, while maintaining competitive performance on perception-focused datasets like CV-Bench and RealWorldQA. The lightweight vision-language model ERNIE-4.5-28B-A3B achieves competitive or even superior performance compared to Qwen2.5-VL-7B and Qwen2.5-VL-32B across most benchmarks, despite using significantly fewer activation parameters. Notably, our lightweight model also supports both thinking and non-thinking modes, offering functionalities consistent with ERNIE-4.5-VL-424B-A47B.</p><h3>Performace of ERNIE-4.5 pre-trained models</h3><h3>Performance of post-trained model ERNIE-4.5-300B-A47B</h3><h3>Performance of post-trained model ERNIE-4.5-21B-A3B</h3><h3>Performance of post-trained multimodal models in thinking mode</h3><h3>Performance of post-trained multimodal models in non-thinking mode</h3><p>ERNIE 4.5 models are trained and deployed for inference using the <a href=\"https://github.com/PaddlePaddle/(https://github.com/PaddlePaddle/Paddle)\">PaddlePaddle</a> framework. The full workflow of training, compression, and inference for ERNIE 4.5 is supported through the <a href=\"https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/docs/erniekit.md\">ERNIEKit</a> and <a href=\"https://github.com/PaddlePaddle/FastDeploy\">FastDeploy</a> toolkit. The table below details the feature matrix of the ERNIE 4.5 model family for training and inference.</p><div align=\"center\"><table><tbody><tr><td>SFT/SFT-LoRA/DPO/DPO-LoRA</td><td>BF16 / W4A16C16 / W8A16C16 / FP8</td></tr><tr><td>SFT/SFT-LoRA/DPO/DPO-LoRA/QAT</td><td>BF16 / W4A16C16 / W8A16C16 / W4A8C8 / FP8 / 2Bits</td></tr><tr><td>SFT/SFT-LoRA/DPO/DPO-LoRA</td><td>BF16 / W4A16C16 / W8A16C16 / FP8</td></tr><tr><td>SFT/SFT-LoRA/DPO/DPO-LoRA</td><td>BF16 / W4A16C16 / W8A16C16 / FP8</td></tr><tr><td>ERNIE-4.5-VL-424B-A47B-Base</td><td>BF16 / W4A16C16 / W8A16C16 / FP8</td></tr><tr><td>BF16 / W4A16C16 / W8A16C16 / FP8</td></tr><tr><td>ERNIE-4.5-VL-28B-A3B-Base</td><td>BF16 / W4A16C16 / W8A16C16 / FP8</td></tr><tr><td>BF16 / W4A16C16 / W8A16C16 / FP8</td></tr><tr><td>SFT/SFT-LoRA/DPO/DPO-LoRA</td></tr><tr><td>SFT/SFT-LoRA/DPO/DPO-LoRA</td></tr></tbody></table></div><p><em>Note: For different ERNIE 4.5 model, we provide diverse quantization schemes using the notation WxAxCx, where: W indicates weight precision, A indicates activation precision, C indicates KV Cache precision, x represents numerical precision.</em></p><h3>ERNIEKit: ERNIE Development Toolkit Based on PaddlePaddle</h3><p> is an industrial-grade training and compression development toolkit for ERNIE models based on PaddlePaddle, offering full-cycle development support for the ERNIE 4.5 model family. Key capabilities include:</p><ul><li>High-performance pre-training implementation</li><li>Full-parameter supervised fine-tuning (SFT)</li><li>Direct Preference Optimization (DPO)</li><li>Parameter-efficient fine-tuning and alignment (SFT-LoRA/DPO-LoRA)</li><li>Quantization-Aware Training (QAT)</li><li>Post-Training Quantization (PTQ) [WIP]</li></ul><p>Minimum hardware requirements for training each model are documented <a href=\"https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/docs/erniekit.md\">here</a>.</p><p>When you install ERNIEKit successfully, you can start training ERNIE 4.5 models with the following command:</p><pre><code># download model from huggingface\nhuggingface-cli download baidu/ERNIE-4.5-0.3B-Paddle --local-dir baidu/ERNIE-4.5-0.3B-Paddle\n# 8K Sequence Length, SFT\nerniekit train examples/configs/ERNIE-4.5-0.3B/sft/run_sft_8k.yaml\n</code></pre><p>For detailed guides on installation, CLI usage, WebUI, multi-node training, and advanced features, please refer to <a href=\"https://raw.githubusercontent.com/PaddlePaddle/ERNIE/develop/docs/erniekit.md\">ERNIEKit Training Document</a>.</p><h3>FastDeployÔºöHigh-performance Inference and Deployment Toolkit for LLMs and VLMs Based on PaddlePaddle</h3><p> is an inference and deployment toolkit for large language models and visual language models, developed based on PaddlePaddle. It delivers production-ready, easy-to-use multi-hardware deployment solutions with multi-level load-balanced PD disaggregation, comprehensive quantization format support, OpenAI API server and vLLM compatible etc.</p><pre><code>from fastdeploy import LLM, SamplingParams\n\nprompt = \"Write me a poem about large language model.\"\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\nllm = LLM(model=\"baidu/ERNIE-4.5-0.3B-Paddle\", max_model_len=32768)\n\noutputs = llm.generate(prompt, sampling_params)\n</code></pre><pre><code>python -m fastdeploy.entrypoints.openai.api_server \\\n    --model \"baidu/ERNIE-4.5-0.3B-Paddle\" \\\n    --max-model-len 32768 \\\n    --port 9904\n</code></pre><p>For more inference and deployment guides, please refer to <a href=\"https://github.com/PaddlePaddle/FastDeploy\">FastDeploy</a>.</p><p>Discover best-practice guides showcasing ERNIE‚Äôs capabilities across multiple domains:</p><p>The ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. </p><p>If you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:</p><pre><code>@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu ERNIE Team},\n      year={2025},\n      eprint={},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={}\n}\n</code></pre>","contentLength":10068,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"binwiederhier/ntfy","url":"https://github.com/binwiederhier/ntfy","date":1751510306,"author":"","guid":181651,"unread":true,"content":"<p>Send push notifications to your phone or desktop using PUT/POST</p><p> (pronounced \"\") is a simple HTTP-based <a href=\"https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern\">pub-sub</a> notification service. With ntfy, you can <strong>send notifications to your phone or desktop via scripts</strong> from any computer, <strong>without having to sign up or pay any fees</strong>. If you'd like to run your own instance of the service, you can easily do so since ntfy is open source.</p><p>I now offer paid plans for <a href=\"https://ntfy.sh/\">ntfy.sh</a> if you don't want to self-host, or you want to support the development of ntfy (‚Üí <a href=\"https://ntfy.sh/app\">Purchase via web app</a>). You can <strong>buy a plan for as low as $5/month</strong>. You can also donate via <a href=\"https://github.com/sponsors/binwiederhier\">GitHub Sponsors</a>, and <a href=\"https://liberapay.com/ntfy\">Liberapay</a>. I would be very humbled by your sponsorship. ‚ù§Ô∏è</p><p>There are a few ways to get in touch with me and/or the rest of the community. Feel free to use any of these methods. Whatever works best for you:</p><h2>Announcements/beta testers</h2><p>For announcements of new releases and cutting-edge beta versions, please subscribe to the <a href=\"https://ntfy.sh/announcements\">ntfy.sh/announcements</a> topic. If you'd like to test the iOS app, join <a href=\"https://testflight.apple.com/join/P1fFnAm9\">TestFlight</a>. For Android betas, join Discord/Matrix (I'll eventually make a testing channel in Google Play).</p><p>If you'd like to support the ntfy maintainers, please consider donating to <a href=\"https://github.com/sponsors/binwiederhier\">GitHub Sponsors</a> or and <a href=\"https://liberapay.com/ntfy\">Liberapay</a>. We would be humbled if you helped carry the server and developer account costs. Even small donations are very much appreciated.</p><p>Thank you to our commercial sponsors, who help keep the service running and the development going:</p><p>And a big fat  to the individuals who have sponsored ntfy in the past, or are still sponsoring ntfy:</p><p>I welcome any contributions. Just create a PR or an issue. For larger features/ideas, please reach out on Discord/Matrix first to see if I'd accept them. To contribute code, check out the <a href=\"https://ntfy.sh/docs/develop/\">build instructions</a> for the server and the Android app. Or, if you'd like to help translate üá©üá™ üá∫üá∏ üáßüá¨, you can start immediately in <a href=\"https://hosted.weblate.org/projects/ntfy/\">Hosted Weblate</a>.</p><a href=\"https://hosted.weblate.org/engage/ntfy/\"><img src=\"https://hosted.weblate.org/widgets/ntfy/-/multi-blue.svg?sanitize=true\" alt=\"Translation status\"></a><p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p><p><strong>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</strong></p><p>Third-party libraries and resources:</p>","contentLength":2453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NginxProxyManager/nginx-proxy-manager","url":"https://github.com/NginxProxyManager/nginx-proxy-manager","date":1751510306,"author":"","guid":181652,"unread":true,"content":"<p>Docker container for managing Nginx proxy hosts with a simple, powerful interface</p><p>This project comes as a pre-built docker image that enables you to easily forward to your websites running at home or otherwise, including free SSL, without having to know too much about Nginx or Letsencrypt.</p><p>I created this project to fill a personal need to provide users with an easy way to accomplish reverse proxying hosts with SSL termination and it had to be so easy that a monkey could do it. This goal hasn't changed. While there might be advanced options they are optional and the project should be as simple as possible so that the barrier for entry here is low.</p><ul><li>Beautiful and Secure Admin Interface based on <a href=\"https://tabler.github.io/\">Tabler</a></li><li>Easily create forwarding domains, redirections, streams and 404 hosts without knowing anything about Nginx</li><li>Free SSL using Let's Encrypt or provide your own custom SSL certificates</li><li>Access Lists and basic HTTP Authentication for your hosts</li><li>Advanced Nginx configuration available for super users</li><li>User management, permissions and audit log</li></ul><h2>Hosting your home network</h2><p>I won't go in to too much detail here but here are the basics for someone new to this self-hosted world.</p><ol><li>Your home router will have a Port Forwarding section somewhere. Log in and find it</li><li>Add port forwarding for port 80 and 443 to the server hosting this project</li><li>Configure your domain name details to point to your home, either with a static ip or a service like DuckDNS or <a href=\"https://github.com/jc21/route53-ddns\">Amazon Route53</a></li><li>Use the Nginx Proxy Manager as your gateway to forward to your other web based services</li></ol><ol><li>Install Docker and Docker-Compose</li></ol><ol start=\"2\"><li>Create a docker-compose.yml file similar to this:</li></ol><pre><code>services:\n  app:\n    image: 'docker.io/jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      - '80:80'\n      - '81:81'\n      - '443:443'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n</code></pre><p>This is the bare minimum configuration required. See the <a href=\"https://nginxproxymanager.com/setup/\">documentation</a> for more.</p><ol start=\"3\"><li>Bring up your stack by running</li></ol><pre><code>docker-compose up -d\n\n# If using docker-compose-plugin\ndocker compose up -d\n\n</code></pre><p>When your docker container is running, connect to it on port  for the admin interface. Sometimes this can take a little bit because of the entropy of keys.</p><pre><code>Email:    admin@example.com\nPassword: changeme\n</code></pre><p>Immediately after logging in with this default user you will be asked to modify your details and change your password.</p><p>All are welcome to create pull requests for this project, against the  branch. Official releases are created from the  branch.</p><p>CI is used in this project. All PR's must pass before being considered. After passing, docker builds for PR's are available on dockerhub for manual verifications.</p>","contentLength":2643,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"zaidmukaddam/scira","url":"https://github.com/zaidmukaddam/scira","date":1751510306,"author":"","guid":181653,"unread":true,"content":"<p>Scira (Formerly MiniPerplx) is a minimalistic AI-powered search engine that helps you find information on the internet and cites it too. Powered by Vercel AI SDK! Search with models like xAI's Grok 3.</p><p>A minimalistic AI-powered search engine that helps you find information on the internet.</p><ul><li><a href=\"https://tavily.com\">Tavily AI</a> - For search grounding and web search capabilities</li></ul><h3>Core Search &amp; Information</h3><ul><li>: Get answers to your questions using multiple AI models including xAI's Grok, Anthropic's Claude, Google's Gemini, and OpenAI's GPT models</li><li>: Search the web using Tavily's API with support for multiple queries, search depths, and topics</li><li>: Extract and analyze content from any URL using Exa AI with live crawling capabilities</li><li>: Search Reddit content with time range filtering using Tavily API</li><li>: Search X posts with date ranges and specific handle filtering using xAI Live Search</li><li>: Advanced multi-step search capability for complex queries</li></ul><ul><li>: Search for academic papers and research using Exa AI with abstracts and summaries</li><li>: Find YouTube videos with detailed information, captions, and timestamps powered by Exa AI</li></ul><ul><li>: Get detailed information about movies and TV shows using TMDB API</li><li>: Discover trending movies with cast, ratings, and detailed information</li><li>: Find popular TV shows with comprehensive metadata</li></ul><h3>Financial &amp; Data Analysis</h3><ul><li>: Generate interactive stock charts with news integration using yfinance and Tavily</li><li>: Convert between currencies with real-time exchange rates using yfinance</li><li>: Write and execute Python code with chart generation capabilities using Daytona sandbox</li></ul><ul><li>: Get current weather and forecasts for any location using OpenWeather API</li><li>: Find places and get coordinates using Google Maps API</li><li>: Discover nearby restaurants, attractions, and services with Google Places API</li><li>: Track real-time flight information using Aviation Stack API</li></ul><ul><li>: Translate text between languages using AI models</li><li>: Get current date and time in user's timezone with multiple format options</li><li>: Add and search personal memories using Mem0 AI</li><li>: Search for Model Context Protocol servers using Smithery Registry</li></ul><ul><li>: Search across the entire internet powered by Tavily</li><li>: Your personal memory companion (requires authentication)</li><li>: Code execution, stock charts, and currency conversion</li><li>: Direct conversation with AI models</li><li>: Search X (Twitter) posts</li><li>: Search Reddit posts</li><li>: Search academic papers powered by Exa</li><li>: Search YouTube videos powered by Exa</li><li>: Deep research with multiple sources and analysis</li></ul><ul><li>: Grok 3, Grok 3 Mini, Grok 2 Vision</li><li>: Gemini 2.5 Flash (Preview), Gemini 2.5 Pro (Preview)</li><li>: Claude 4 Sonnet, Claude 4 Opus (with thinking capabilities)</li><li>: GPT-4o, o4-mini, o3 (with reasoning capabilities)</li><li>: Qwen QwQ 32B, Qwen 3 32B, Meta's Llama 4 Maverick</li></ul><h2>Set Scira as your default search engine</h2><ol><li><p><strong>Open the Chrome browser settings</strong>:</p><ul><li>Click on the three vertical dots in the upper right corner of the browser.</li><li>Select \"Settings\" from the dropdown menu.</li></ul></li><li><p><strong>Go to the search engine settings</strong>:</p><ul><li>In the left sidebar, click on \"Search engine.\"</li><li>Then select \"Manage search engines and site search.\"</li></ul></li><li><ul><li>Click on \"Add\" next to \"Site search.\"</li></ul></li><li><p><strong>Set the search engine name</strong>:</p><ul><li>Enter  in the \"Search engine\" field.</li></ul></li><li><p><strong>Set the search engine URL</strong>:</p><ul><li>Enter  in the \"URL with %s in place of query\" field.</li></ul></li><li><p><strong>Set the search engine shortcut</strong>:</p><ul><li>Enter  in the \"Shortcut\" field.</li></ul></li><li><ul><li>Click on the three dots next to the search engine you just added.</li><li>Select \"Make default\" from the dropdown menu.</li></ul></li></ol><p>After completing these steps, you should be able to use Scira as your default search engine in Chrome.</p><p>The application can be run using Docker in two ways:</p><h5>Using Docker Compose (Recommended)</h5><ol><li>Make sure you have Docker and Docker Compose installed on your system</li><li>Create a  file based on  with your API keys</li><li>Run the following command in the project root: </li><li>The application will be available at </li></ol><ol><li>Create a  file based on  with your API keys</li><li>Build the Docker image: <pre><code>docker build -t scira.app .\n</code></pre></li><li>Run the container: <pre><code>docker run --env-file .env -p 3000:3000 scira.app\n</code></pre></li></ol><p>The application uses a multi-stage build process to minimize the final image size and implements security best practices. The production image runs on Node.js LTS with Alpine Linux for a minimal footprint.</p><p>To run the application locally without Docker:</p><ol><li>Sign up for accounts with the required AI providers: \n  <ul><li>Tavily (required for web search feature)</li></ul></li><li>Copy  to  and fill in your API keys</li><li>Install dependencies: </li><li>Start the development server: </li><li>Open  in your browser</li></ol><p>This project is licensed under the Apache 2.0 License - see the <a href=\"https://raw.githubusercontent.com/zaidmukaddam/scira/main/LICENSE\">LICENSE</a> file for details.</p>","contentLength":4435,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"danielmiessler/Fabric","url":"https://github.com/danielmiessler/Fabric","date":1751510306,"author":"","guid":181654,"unread":true,"content":"<p>Fabric is an open-source framework for augmenting humans using AI. It provides a modular system for solving specific problems using a crowdsourced set of AI prompts that can be used anywhere.</p><p>Since the start of modern AI in late 2022 we've seen an  number of AI applications for accomplishing tasks. There are thousands of websites, chat-bots, mobile apps, and other interfaces for using all the different AI out there.</p><p>It's all really exciting and powerful, but <em>it's not easy to integrate this functionality into our lives.</em></p><h4>In other words, AI doesn't have a capabilities problem‚Äîit has an  problem.</h4><p><strong>Fabric was created to address this by creating and organizing the fundamental units of AI‚Äîthe prompts themselves!</strong></p><p>Fabric organizes prompts by real-world task, allowing people to create, collect, and organize their most important AI solutions in a single place for use in their favorite tools. And if you're command-line focused, you can use Fabric itself as the interface!</p><p>Keep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current <a href=\"https://raw.githubusercontent.com/danielmiessler/Fabric/main/#installation\">install instructions</a> below.</p><blockquote><ul><li><p>Fabric now supports Perplexity AI. Configure it by using  to add your Perplexity AI API Key, and then try:</p><pre><code>fabric -m sonar-pro \"What is the latest world news?\"\n</code></pre></li></ul><ul><li>Fabric's YouTube transcription now needs  to be installed. Make sure to install the latest version (2025.06.09 as of this note). The YouTube API key is only needed for comments (the  flag) and metadata extraction (the  flag).</li></ul></blockquote><blockquote><p>AI isn't a thing; it's a  of a thing. And that thing is .</p></blockquote><p>We believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the  problems we want to solve.</p><h3>Breaking problems into components</h3><p>Our approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.</p><img width=\"2078\" alt=\"augmented_challenges\" src=\"https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06\"><p>Prompts are good for this, but the biggest challenge I faced in 2023‚Äî‚Äîwhich still exists today‚Äîis <strong>the sheer number of AI prompts out there</strong>. We all have prompts that are useful, but it's hard to discover new ones, know if they are good or not, <em>and manage different versions of the ones we like</em>.</p><p>One of 's primary features is helping people collect and integrate prompts, which we call , into various parts of their lives.</p><p>Fabric has Patterns for all sorts of life and work activities, including:</p><ul><li>Extracting the most interesting parts of YouTube videos and podcasts</li><li>Writing an essay in your own voice with just an idea as an input</li><li>Summarizing opaque academic papers</li><li>Creating perfectly matched AI art prompts for a piece of writing</li><li>Rating the quality of content to see if you want to read/watch the whole thing</li><li>Getting summaries of long, boring content</li><li>Turning bad documentation into usable documentation</li><li>Creating social media posts from any content input</li></ul><p>To install Fabric, you can use the latest release binaries or install it from the source.</p><h3>Get Latest Release Binaries</h3><p><code>https://github.com/danielmiessler/fabric/releases/latest/download/fabric-windows-amd64.exe</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-arm64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-darwin-amd64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-amd64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><p><code>curl -L https://github.com/danielmiessler/fabric/releases/latest/download/fabric-linux-arm64 &gt; fabric &amp;&amp; chmod +x fabric &amp;&amp; ./fabric --version</code></p><p> using Homebrew or the Arch Linux package managers makes  available as , so add the following alias to your shell startup files to account for this:</p><pre><code># Install Fabric directly from the repo\ngo install github.com/danielmiessler/fabric@latest\n</code></pre><p>You may need to set some environment variables in your  on linux or  file on mac to be able to run the  command. Here is an example of what you can add:</p><p>For Intel based macs or linux</p><pre><code># Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\n\n# Update PATH to include GOPATH and GOROOT binaries\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n</code></pre><p>for Apple Silicon based macs</p><pre><code># Golang environment variables\nexport GOROOT=$(brew --prefix go)/libexec\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n</code></pre><p>Now run the following command</p><pre><code># Run the setup to set up your directories and keys\nfabric --setup\n</code></pre><p>If everything works you are good to go.</p><h3>Add aliases for all patterns</h3><p>In order to add aliases for all your patterns and use them directly as commands ie.  instead of <code>fabric --pattern summarize</code> You can add the following to your  or  file.</p><pre><code># Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in $HOME/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Create an alias in the form: alias pattern_name=\"fabric --pattern pattern_name\"\n    alias_command=\"alias $pattern_name='fabric --pattern $pattern_name'\"\n\n    # Evaluate the alias command to add it to the current shell\n    eval \"$alias_command\"\ndone\n\nyt() {\n    if [ \"$#\" -eq 0 ] || [ \"$#\" -gt 2 ]; then\n        echo \"Usage: yt [-t | --timestamps] youtube-link\"\n        echo \"Use the '-t' flag to get the transcript with timestamps.\"\n        return 1\n    fi\n\n    transcript_flag=\"--transcript\"\n    if [ \"$1\" = \"-t\" ] || [ \"$1\" = \"--timestamps\" ]; then\n        transcript_flag=\"--transcript-with-timestamps\"\n        shift\n    fi\n    local video_link=\"$1\"\n    fabric -y \"$video_link\" $transcript_flag\n}\n</code></pre><p>You can add the below code for the equivalent aliases inside PowerShell by running  inside a PowerShell window:</p><pre><code># Path to the patterns directory\n$patternsPath = Join-Path $HOME \".config/fabric/patterns\"\nforeach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {\n    $patternName = $patternDir.Name\n\n    # Dynamically define a function for each pattern\n    $functionDefinition = @\"\nfunction $patternName {\n    [CmdletBinding()]\n    param(\n        [Parameter(ValueFromPipeline = `$true)]\n        [string] `$InputObject,\n\n        [Parameter(ValueFromRemainingArguments = `$true)]\n        [String[]] `$patternArgs\n    )\n\n    begin {\n        # Initialize an array to collect pipeline input\n        `$collector = @()\n    }\n\n    process {\n        # Collect pipeline input objects\n        if (`$InputObject) {\n            `$collector += `$InputObject\n        }\n    }\n\n    end {\n        # Join all pipeline input into a single string, separated by newlines\n        `$pipelineContent = `$collector -join \"`n\"\n\n        # If there's pipeline input, include it in the call to fabric\n        if (`$pipelineContent) {\n            `$pipelineContent | fabric --pattern $patternName `$patternArgs\n        } else {\n            # No pipeline input; just call fabric with the additional args\n            fabric --pattern $patternName `$patternArgs\n        }\n    }\n}\n\"@\n    # Add the function to the current session\n    Invoke-Expression $functionDefinition\n}\n\n# Define the 'yt' function as well\nfunction yt {\n    [CmdletBinding()]\n    param(\n        [Parameter()]\n        [Alias(\"timestamps\")]\n        [switch]$t,\n\n        [Parameter(Position = 0, ValueFromPipeline = $true)]\n        [string]$videoLink\n    )\n\n    begin {\n        $transcriptFlag = \"--transcript\"\n        if ($t) {\n            $transcriptFlag = \"--transcript-with-timestamps\"\n        }\n    }\n\n    process {\n        if (-not $videoLink) {\n            Write-Error \"Usage: yt [-t | --timestamps] youtube-link\"\n            return\n        }\n    }\n\n    end {\n        if ($videoLink) {\n            # Execute and allow output to flow through the pipeline\n            fabric -y $videoLink $transcriptFlag\n        }\n    }\n}\n</code></pre><p>This also creates a  alias that allows you to use <code>yt https://www.youtube.com/watch?v=4b0iet22VIk</code> to get transcripts, comments, and metadata.</p><h4>Save your files in markdown using aliases</h4><p>If in addition to the above aliases you would like to have the option to save the output to your favorite markdown note vault like Obsidian then instead of the above add the following to your  or  file:</p><pre><code># Define the base directory for Obsidian notes\nobsidian_base=\"/path/to/obsidian\"\n\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in ~/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename \"$pattern_file\")\n\n    # Remove any existing alias with the same name\n    unalias \"$pattern_name\" 2&gt;/dev/null\n\n    # Define a function dynamically for each pattern\n    eval \"\n    $pattern_name() {\n        local title=\\$1\n        local date_stamp=\\$(date +'%Y-%m-%d')\n        local output_path=\\\"\\$obsidian_base/\\${date_stamp}-\\${title}.md\\\"\n\n        # Check if a title was provided\n        if [ -n \\\"\\$title\\\" ]; then\n            # If a title is provided, use the output path\n            fabric --pattern \\\"$pattern_name\\\" -o \\\"\\$output_path\\\"\n        else\n            # If no title is provided, use --stream\n            fabric --pattern \\\"$pattern_name\\\" --stream\n        fi\n    }\n    \"\ndone\n</code></pre><p>This will allow you to use the patterns as aliases like in the above for example  instead of <code>fabric --pattern summarize --stream</code>, however if you pass in an extra argument like this <code>summarize \"my_article_title\"</code> your output will be saved in the destination that you set in <code>obsidian_base=\"/path/to/obsidian\"</code> in the following format <code>YYYY-MM-DD-my_article_title.md</code> where the date gets autogenerated for you. You can tweak the date format by tweaking the  format.</p><p>If you have the Legacy (Python) version installed and want to migrate to the Go version, here's how you do it. It's basically two steps: 1) uninstall the Python version, and 2) install the Go version.</p><pre><code># Uninstall Legacy Fabric\npipx uninstall fabric\n\n# Clear any old Fabric aliases\n(check your .bashrc, .zshrc, etc.)\n# Install the Go version\ngo install github.com/danielmiessler/fabric@latest\n# Run setup for the new version. Important because things have changed\nfabric --setup\n</code></pre><p>The great thing about Go is that it's super easy to upgrade. Just run the same command you used to install it in the first place and you'll always get the latest version.</p><pre><code>go install github.com/danielmiessler/fabric@latest\n</code></pre><p>Fabric provides shell completion scripts for Zsh, Bash, and Fish shells, making it easier to use the CLI by providing tab completion for commands and options.</p><p>To enable Zsh completion:</p><pre><code># Copy the completion file to a directory in your $fpath\nmkdir -p ~/.zsh/completions\ncp completions/_fabric ~/.zsh/completions/\n\n# Add the directory to fpath in your .zshrc before compinit\necho 'fpath=(~/.zsh/completions $fpath)' &gt;&gt; ~/.zshrc\necho 'autoload -Uz compinit &amp;&amp; compinit' &gt;&gt; ~/.zshrc\n</code></pre><p>To enable Bash completion:</p><pre><code># Source the completion script in your .bashrc\necho 'source /path/to/fabric/completions/fabric.bash' &gt;&gt; ~/.bashrc\n\n# Or copy to the system-wide bash completion directory\nsudo cp completions/fabric.bash /etc/bash_completion.d/\n</code></pre><p>To enable Fish completion:</p><pre><code># Copy the completion file to the fish completions directory\nmkdir -p ~/.config/fish/completions\ncp completions/fabric.fish ~/.config/fish/completions/\n</code></pre><p>Once you have it all set up, here's how to use it.</p><pre><code>\nUsage:\n  fabric [OPTIONS]\n\nApplication Options:\n  -p, --pattern=                    Choose a pattern from the available patterns\n  -v, --variable=                   Values for pattern variables, e.g. -v=#role:expert -v=#points:30\n  -C, --context=                    Choose a context from the available contexts\n      --session=                    Choose a session from the available sessions\n  -a, --attachment=                 Attachment path or URL (e.g. for OpenAI image recognition messages)\n  -S, --setup                       Run setup for all reconfigurable parts of fabric\n  -t, --temperature=                Set temperature (default: 0.7)\n  -T, --topp=                       Set top P (default: 0.9)\n  -s, --stream                      Stream\n  -P, --presencepenalty=            Set presence penalty (default: 0.0)\n  -r, --raw                         Use the defaults of the model without sending chat options (like temperature etc.) and use the user role instead of the system role for patterns.\n  -F, --frequencypenalty=           Set frequency penalty (default: 0.0)\n  -l, --listpatterns                List all patterns\n  -L, --listmodels                  List all available models\n  -x, --listcontexts                List all contexts\n  -X, --listsessions                List all sessions\n  -U, --updatepatterns              Update patterns\n  -c, --copy                        Copy to clipboard\n  -m, --model=                      Choose model\n      --modelContextLength=         Model context length (only affects ollama)\n  -o, --output=                     Output to file\n      --output-session              Output the entire session (also a temporary one) to the output file\n  -n, --latest=                     Number of latest patterns to list (default: 0)\n  -d, --changeDefaultModel          Change default model\n  -y, --youtube=                    YouTube video or play list \"URL\" to grab transcript, comments from it and send to chat or print it put to the console and store it in the output file\n      --playlist                    Prefer playlist over video if both ids are present in the URL\n      --transcript                  Grab transcript from YouTube video and send to chat (it is used per default).\n      --transcript-with-timestamps  Grab transcript from YouTube video with timestamps and send to chat\n      --comments                    Grab comments from YouTube video and send to chat\n      --metadata                    Output video metadata\n  -g, --language=                   Specify the Language Code for the chat, e.g. -g=en -g=zh\n  -u, --scrape_url=                 Scrape website URL to markdown using Jina AI\n  -q, --scrape_question=            Search question using Jina AI\n  -e, --seed=                       Seed to be used for LMM generation\n  -w, --wipecontext=                Wipe context\n  -W, --wipesession=                Wipe session\n      --printcontext=               Print context\n      --printsession=               Print session\n      --readability                 Convert HTML input into a clean, readable view\n      --input-has-vars              Apply variables to user input\n      --dry-run                     Show what would be sent to the model without actually sending it\n      --serve                       Serve the Fabric Rest API\n      --serveOllama                 Serve the Fabric Rest API with ollama endpoints\n      --address=                    The address to bind the REST API (default: :8080)\n      --api-key=                    API key used to secure server routes\n      --config=                     Path to YAML config file\n      --version                     Print current version\n      --listextensions              List all registered extensions\n      --addextension=               Register a new extension from config file path\n      --rmextension=                Remove a registered extension by name\n      --strategy=                   Choose a strategy from the available strategies\n      --liststrategies              List all strategies\n      --listvendors                 List all vendors\n      --shell-complete-list         Output raw list without headers/formatting (for shell completion)\n\nHelp Options:\n  -h, --help                        Show this help message\n\n</code></pre><h2>Our approach to prompting</h2><p>Fabric  are different than most prompts you'll see.</p><ul><li><strong>First, we use  to help ensure maximum readability and editability</strong>. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. <em>Importantly, this also includes the AI you're sending it to!</em></li></ul><p>Here's an example of a Fabric Pattern.</p><pre><code>https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md\n</code></pre><img width=\"1461\" alt=\"pattern-example\" src=\"https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d\"><ul><li><p><strong>Next, we are extremely clear in our instructions</strong>, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.</p></li><li><p><strong>And finally, we tend to use the System section of the prompt almost exclusively</strong>. In over a year of being heads-down with this stuff, we've just seen more efficacy from doing that. If that changes, or we're shown data that says otherwise, we will adjust.</p></li></ul><blockquote><p>The following examples use the macOS  to paste from the clipboard. See the <a href=\"https://raw.githubusercontent.com/danielmiessler/Fabric/main/#pbpaste\">pbpaste</a> section below for Windows and Linux alternatives.</p></blockquote><p>Now let's look at some things you can do with Fabric.</p><ol><li><p>Run the  Pattern based on input from . In this case, the body of an article.</p><pre><code>pbpaste | fabric --pattern summarize\n</code></pre></li><li><p>Run the  Pattern with the  option to get immediate and streaming results.</p><pre><code>pbpaste | fabric --stream --pattern analyze_claims\n</code></pre></li><li><p>Run the  Pattern with the  option to get immediate and streaming results from any Youtube video (much like in the original introduction video).</p><pre><code>fabric -y \"https://youtube.com/watch?v=uXs-zPc63kM\" --stream --pattern extract_wisdom\n</code></pre></li><li><p>Create patterns- you must create a .md file with the pattern and save it to <code>~/.config/fabric/patterns/[yourpatternname]</code>.</p></li><li><p>Run a  pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.</p><pre><code>fabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims\n</code></pre></li></ol><img width=\"1173\" alt=\"fabric-patterns-screenshot\" src=\"https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8\"><p>If you're not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the <a href=\"https://github.com/danielmiessler/fabric/tree/main/patterns\"></a> directory and start exploring!</p><p>We hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.</p><p>You can use any of the Patterns you see there in any AI application that you have, whether that's ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we've published, and they will be way better than ours.</p><p>The wisdom of crowds for the win.</p><p>Fabric also implements prompt strategies like \"Chain of Thought\" or \"Chain of Draft\" which can be used in addition to the basic patterns.</p><p>Each strategy is available as a small  file in the <a href=\"https://github.com/danielmiessler/fabric/tree/main/strategies\"></a> directory.</p><p>The prompt modification of the strategy is applied to the system prompt and passed on to the LLM in the chat session.</p><p>Use  and select the option to install the strategies in your  directory.</p><p>You may want to use Fabric to create your own custom Patterns‚Äîbut not share them with others. No problem!</p><p>Just make a directory in <code>~/.config/custompatterns/</code> (or wherever) and put your  files in there.</p><p>When you're ready to use them, copy them into <code>~/.config/fabric/patterns/</code></p><p>You can then use them like any other Patterns, but they won't be public unless you explicitly submit them as Pull Requests to the Fabric project. So don't worry‚Äîthey're private to you.</p><p>Fabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:</p><p> is a helper command that converts LaTeX files to PDF format. You can use it like this:</p><p>This will create a PDF file from the input LaTeX file in the same directory.</p><p>You can also use it with stdin which works perfectly with the  pattern:</p><pre><code>echo \"ai security primer\" | fabric --pattern write_latex | to_pdf\n</code></pre><p>This will create a PDF file named  in the current directory.</p><p>To install , install it the same way as you install Fabric, just with a different repo name.</p><pre><code>go install github.com/danielmiessler/fabric/plugins/tools/to_pdf@latest\n</code></pre><p>Make sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as  requires  to be available in your system's PATH.</p><p> is used in conjunction with the  pattern. It generates a  representation of a directory of code that can be fed into an AI model with instructions to create a new feature or edit the code in a specified way.</p><pre><code>go install github.com/danielmiessler/fabric/plugins/tools/code_helper@latest\n</code></pre><p>The <a href=\"https://raw.githubusercontent.com/danielmiessler/Fabric/main/#examples\">examples</a> use the macOS program  to paste content from the clipboard to pipe into  as the input.  is not available on Windows or Linux, but there are alternatives.</p><p>On Windows, you can use the PowerShell command  from a PowerShell command prompt. If you like, you can also alias it to . If you are using classic PowerShell, edit the file <code>~\\Documents\\WindowsPowerShell\\.profile.ps1</code>, or if you are using PowerShell Core, edit <code>~\\Documents\\PowerShell\\.profile.ps1</code> and add the alias,</p><pre><code>Set-Alias pbpaste Get-Clipboard\n</code></pre><p>On Linux, you can use <code>xclip -selection clipboard -o</code> to paste from the clipboard. You will likely need to install  with your package manager. For Debian based systems including Ubuntu,</p><pre><code>sudo apt update\nsudo apt install xclip -y\n</code></pre><p>You can also create an alias by editing  or  and adding the alias,</p><pre><code>alias pbpaste='xclip -selection clipboard -o'\n</code></pre><p>Fabric now includes a built-in web interface that provides a GUI alternative to the command-line interface and an out-of-the-box website for those who want to get started with web development or blogging. You can use this app as a GUI interface for Fabric, a ready to go blog-site, or a website template for your own projects.</p><p>The  directory includes starter  and  directories, allowing you to open up the  directory as an <a href=\"https://obsidian.md\">Obsidian.md</a> vault. You can place your posts in the posts directory when you're ready to publish.</p><p>The GUI can be installed by navigating to the  directory and using , , or your favorite package manager. Then simply run the development server to start the app.</p><p><em>You will need to run fabric in a separate terminal with the  command.</em></p><p><strong>From the fabric project  directory:</strong></p><pre><code>npm run dev\n\n## or ##\n\npnpm run dev\n\n## or your equivalent\n</code></pre><p>To run the Streamlit user interface:</p><pre><code># Install required dependencies\npip install -r requirements.txt\n\n# Or manually install dependencies\npip install streamlit pandas matplotlib seaborn numpy python-dotenv pyperclip\n\n# Run the Streamlit app\nstreamlit run streamlit.py\n</code></pre><p>The Streamlit UI provides a user-friendly interface for:</p><ul><li>Running and chaining patterns</li><li>Creating and editing patterns</li><li>Analyzing pattern results</li></ul><p>The Streamlit UI supports clipboard operations across different platforms:</p><ul><li>: Uses  and  (built-in)</li><li>: Uses  library (install with )</li><li>: Uses  (install with <code>sudo apt-get install xclip</code> or equivalent for your Linux distribution)</li></ul><blockquote><p>[!NOTE] Special thanks to the following people for their inspiration and contributions!</p></blockquote><ul><li> for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!</li><li> for pushing me over the edge of whether to make this a public project or not.</li><li> and  for their invaluable contributions to the Go version</li><li> for his work on the web interface.</li><li> for super useful input on the project's Github directory structure..</li><li> for the idea of a  context flag that adds pre-created context in the  directory to all Pattern queries.</li><li> for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using  before sending on to  for analysis.</li><li> for assisting with numerous components to make things simpler and more maintainable.</li></ul><a href=\"https://github.com/danielmiessler/fabric/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=danielmiessler/fabric\" alt=\"contrib.rocks\"></a>","contentLength":23029,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The-Cool-Coders/Project-Ideas-And-Resources","url":"https://github.com/The-Cool-Coders/Project-Ideas-And-Resources","date":1751423796,"author":"","guid":179513,"unread":true,"content":"<p>A Collection of application ideas that can be used to improve your coding skills ‚ù§.</p><img src=\"https://user-images.githubusercontent.com/61475220/97093903-6d8aee80-166d-11eb-8799-13e119380d2b.jpg\" width=\"50%\" align=\"right\"><p>Have you ever wanted to build something but you had no idea what to do? Just as authors sometimes have \"writers block\" it's also true for developers. This list is intended to solve this issue once and for all! üëç</p><ul><li>Great for improving your coding skills ;</li><li>Great for experimenting with new technologies üÜï;</li><li>Great for adding to your portfolio to impress your next employer/client ;</li><li>Great for using as examples in tutorials (articles or videos) ;</li><li>Easy to complete and also easily extendable with new features ;</li></ul><p>This is not just a simple list of projects, but a collection that describes each project in enough detail so that you can develop it from the ground up!</p><p>Each project has the following :</p><ol><li>A clear and descriptive objective;</li><li>A list of  which should be implemented (these stories act more as a guideline than a forced list of . Feel free to add your own stories if you want);</li><li>A list of bonus features that not only improve the base project, but also your skills at the same time (these are optional and should be attempted only after you have completed the required user stories)</li><li>All the resources and links to help you find what you need to complete the project</li></ol><p>Projects are divided into three tiers based on the knowledge and experience required to complete them.</p><table><tbody><tr><td>Developers in the early stages of their learning journey. Those who are typically focused on creating user-facing applications.</td></tr><tr><td>Developers at an intermediate stage of learning and experience. They are comfortable in UI/UX, using development tools, and building apps that use API services.</td></tr><tr><td>Developers who have all of the above, and are learning more advanced techniques like implementing backend applications and database services.</td></tr></tbody></table><h3>Tier-1: Beginner Projects</h3><h3>Tier-2: Intermediate Projects</h3><h3>Tier-3: Advanced Projects</h3><h3>DApp and Blockchain Projects -</h3><p>We are planning to add more and more projects to this repository. For this, we need your help! Find out how to contribute below. üëá</p><p>We are also planning to create a website where you can easily browse through all of the projects.</p><p>Any contributions are highly appreciated.  You can contribute in two ways:</p><ol><li>create an issue and tell us your idea . Make sure that you use the  label in this case;</li><li>fork the project and submit a PR with your new idea. Before doing that, please make sure that you read and follow the <a href=\"https://raw.githubusercontent.com/The-Cool-Coders/Project-Ideas-And-Resources/main/CONTRIBUTION.md\">Contribution Guide</a>;</li></ol><p>You can also add your own examples to the projects after you have completed them. I highly encourage you to do this as it will show others what amazing things were built! üëç</p><h4>Awesome APIS for Front-End Developers</h4><p>Thanks goes to all these Wonderful People. Contributions of any kind are welcome!üöÄ</p><a href=\"https://github.com/The-Cool-Coders/Project-Ideas-And-Resources/contributors\"><img src=\"https://contrib.rocks/image?repo=The-Cool-Coders/Project-Ideas-And-Resources\"></a><p>Looking for Great Contributions from all the open source enthusiasts for making this repository even bigger! Do follow the steps mentioned in Contributing.md file for succesful PRs</p><p>If the information from this repo was useful to you in any way, make sure you give it a star üåü, this way others can find it and benefit too! Together we can grow and make our community better! </p><p align=\"center\" width=\"60%\"><b>Do you have any suggestions on how we could improve this project overall? Let us know! We'd love to hear your feedback ‚ù§!</b></p>","contentLength":3204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"confident-ai/deepeval","url":"https://github.com/confident-ai/deepeval","date":1751423796,"author":"","guid":179514,"unread":true,"content":"<p>The LLM Evaluation Framework</p><p> is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs  for evaluation.</p><p>Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.</p><blockquote><p>[!IMPORTANT] Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? <a href=\"https://confident-ai.com?utm_source=GitHub\">Sign up to the DeepEval platform</a> to compare iterations of your LLM app, generate &amp; share testing reports, and more.</p></blockquote><blockquote><p>ü•≥ You can now share DeepEval's test results on the cloud directly on <a href=\"https://confident-ai.com?utm_source=GitHub\">Confident AI</a>'s infrastructure</p></blockquote><ul><li>Supports both end-to-end and component-level LLM evaluation.</li><li>Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by  LLM of your choice, statistical methods, or NLP models that runs : \n  <ul><li><ul></ul></li><li><ul></ul></li><li><ul></ul></li><li><ul><li>Conversation Completeness</li></ul></li></ul></li><li>Build your own custom metrics that are automatically integrated with DeepEval's ecosystem.</li><li>Generate synthetic datasets for evaluation.</li><li>Integrates seamlessly with  CI/CD environment.</li><li><a href=\"https://deepeval.com/docs/red-teaming-introduction\">Red team your LLM application</a> for 40+ safety vulnerabilities in a few lines of code, including: \n  <ul><li>etc., using advanced 10+ attack enhancement strategies such as prompt injections.</li></ul></li><li>Easily benchmark  LLM on popular LLM benchmarks in <a href=\"https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub\">under 10 lines of code.</a>, which includes: \n  <ul></ul></li><li><a href=\"https://confident-ai.com?utm_source=GitHub\">100% integrated with Confident AI</a> for the full evaluation lifecycle: \n  <ul><li>Curate/annotate evaluation datasets on the cloud</li><li>Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best</li><li>Fine-tune metrics for custom results</li><li>Debug evaluation results via LLM traces</li><li>Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data</li></ul></li></ul><blockquote><p>[!NOTE] Confident AI is the DeepEval platform. Create an account <a href=\"https://app.confident-ai.com?utm_source=GitHub\">here.</a></p></blockquote><p>Let's pretend your LLM application is a RAG based customer support chatbot; here's how DeepEval can help test what you've built.</p><h2>Create an account (highly recommended)</h2><p>Using the  platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.</p><p>Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy <a href=\"https://deepeval.com/docs/data-privacy?utm_source=GitHub\">here</a>).</p><h2>Writing your first test case</h2><p>Open  and write your first test case to run an  evaluation using DeepEval, which treats your LLM app as a black-box:</p><pre><code>import pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import GEval\nfrom deepeval.test_case import LLMTestCase, LLMTestCaseParams\n\ndef test_case():\n    correctness_metric = GEval(\n        name=\"Correctness\",\n        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n        threshold=0.5\n    )\n    test_case = LLMTestCase(\n        input=\"What if these shoes don't fit?\",\n        # Replace this with the actual output from your LLM application\n        actual_output=\"You have 30 days to get a full refund at no extra cost.\",\n        expected_output=\"We offer a 30-day full refund at no extra costs.\",\n        retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n    )\n    assert_test(test_case, [correctness_metric])\n</code></pre><p>Set your  as an environment variable (you can also evaluate using your own custom model, for more details visit <a href=\"https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub\">this part of our docs</a>):</p><pre><code>export OPENAI_API_KEY=\"...\"\n</code></pre><p>And finally, run  in the CLI:</p><pre><code>deepeval test run test_chatbot.py\n</code></pre><p><strong>Congratulations! Your test case should have passed ‚úÖ</strong> Let's breakdown what happened.</p><ul><li>The variable  mimics a user input, and  is a placeholder for what your application's supposed to output based on this input.</li><li>The variable  represents the ideal answer for a given , and <a href=\"https://deepeval.com/docs/metrics-llm-evals\"></a> is a research-backed metric provided by  for you to evaluate your LLM output's on any custom custom with human-like accuracy.</li><li>In this example, the metric  is correctness of the  based on the provided .</li><li>All metric scores range from 0 - 1, which the  threshold ultimately determines if your test have passed or not.</li></ul><p><a href=\"https://deepeval.com/docs/getting-started?utm_source=GitHub\">Read our documentation</a> for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.</p><h2>Evaluating Nested Components</h2><p>If you wish to evaluate individual components within your LLM app, you need to run  evals - a powerful way to evaluate any component within an LLM system.</p><p>Simply trace \"components\" such as LLM calls, retrievers, tool calls, and agents within your LLM application using the  decorator to apply metrics on a component-level. Tracing with  is non-instrusive (learn more <a href=\"https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing\">here</a>) and helps you avoid rewriting your codebase just for evals:</p><pre><code>from deepeval.tracing import observe, update_current_span\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.dataset import Golden\nfrom deepeval.metrics import GEval\nfrom deepeval import evaluate\n\ncorrectness = GEval(name=\"Correctness\", criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\", evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])\n\n@observe(metrics=[correctness])\ndef inner_component():\n    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.\n    update_current_span(test_case=LLMTestCase(input=\"...\", actual_output=\"...\"))\n    return\n\n@observe\ndef llm_app(input: str):\n    inner_component()\n    return\n\nevaluate(observed_callback=llm_app, goldens=[Golden(input=\"Hi!\")])\n</code></pre><p>You can learn everything about component-level evaluations <a href=\"https://www.deepeval.com/docs/evaluation-component-level-llm-evals\">here.</a></p><h2>Evaluating Without Pytest Integration</h2><p>Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.</p><pre><code>from deepeval import evaluate\nfrom deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\nanswer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\ntest_case = LLMTestCase(\n    input=\"What if these shoes don't fit?\",\n    # Replace this with the actual output from your LLM application\n    actual_output=\"We offer a 30-day full refund at no extra costs.\",\n    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n)\nevaluate([test_case], [answer_relevancy_metric])\n</code></pre><p>DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:</p><pre><code>from deepeval.metrics import AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\n\nanswer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)\ntest_case = LLMTestCase(\n    input=\"What if these shoes don't fit?\",\n    # Replace this with the actual output from your LLM application\n    actual_output=\"We offer a 30-day full refund at no extra costs.\",\n    retrieval_context=[\"All customers are eligible for a 30 day full refund at no extra costs.\"]\n)\n\nanswer_relevancy_metric.measure(test_case)\nprint(answer_relevancy_metric.score)\n# All metrics also offer an explanation\nprint(answer_relevancy_metric.reason)\n</code></pre><p>Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.</p><h2>Evaluating a Dataset / Test Cases in Bulk</h2><p>In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:</p><pre><code>import pytest\nfrom deepeval import assert_test\nfrom deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric\nfrom deepeval.test_case import LLMTestCase\nfrom deepeval.dataset import EvaluationDataset\n\nfirst_test_case = LLMTestCase(input=\"...\", actual_output=\"...\", context=[\"...\"])\nsecond_test_case = LLMTestCase(input=\"...\", actual_output=\"...\", context=[\"...\"])\n\ndataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])\n\n@pytest.mark.parametrize(\n    \"test_case\",\n    dataset,\n)\ndef test_customer_chatbot(test_case: LLMTestCase):\n    hallucination_metric = HallucinationMetric(threshold=0.3)\n    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)\n    assert_test(test_case, [hallucination_metric, answer_relevancy_metric])\n</code></pre><pre><code># Run this in the CLI, you can also add an optional -n flag to run tests in parallel\ndeepeval test run test_&lt;filename&gt;.py -n 4\n</code></pre><p>Alternatively, although we recommend using , you can evaluate a dataset/test cases without using our Pytest integration:</p><pre><code>from deepeval import evaluate\n...\n\nevaluate(dataset, [answer_relevancy_metric])\n# or\ndataset.evaluate([answer_relevancy_metric])\n</code></pre><ol><li>Curate/annotate evaluation datasets on the cloud</li><li>Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best</li><li>Fine-tune metrics for custom results</li><li>Debug evaluation results via LLM traces</li><li>Monitor &amp; evaluate LLM responses in product to improve datasets with real-world data</li></ol><p>Everything on Confident AI, including how to use Confident is available <a href=\"https://documentation.confident-ai.com/docs?utm_source=GitHub\">here</a>.</p><p>To begin, login from the CLI:</p><p>Follow the instructions to log in, create your account, and paste your API key into the CLI.</p><p>Now, run your test file again:</p><pre><code>deepeval test run test_chatbot.py\n</code></pre><p>You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!</p><p>Please read <a href=\"https://github.com/confident-ai/deepeval/raw/main/CONTRIBUTING.md\">CONTRIBUTING.md</a> for details on our code of conduct, and the process for submitting pull requests to us.</p><p>DeepEval is licensed under Apache 2.0 - see the <a href=\"https://github.com/confident-ai/deepeval/raw/main/LICENSE.md\">LICENSE.md</a> file for details.</p>","contentLength":9937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TapXWorld/ChinaTextbook","url":"https://github.com/TapXWorld/ChinaTextbook","date":1751423796,"author":"","guid":179515,"unread":true,"content":"<p>ËôΩÁÑ∂ÂõΩÂÜÖÊïôËÇ≤ÁΩëÁ´ôÂ∑≤Êèê‰æõÂÖçË¥πËµÑÊ∫êÔºå‰ΩÜÂ§ßÂ§öÊï∞ÊôÆÈÄö‰∫∫Ëé∑Âèñ‰ø°ÊÅØÁöÑÈÄîÂæÑ‰æùÁÑ∂ÂèóÈôê„ÄÇÊúâ‰∫õ‰∫∫Âà©Áî®Ëøô‰∏ÄÁÇπÔºåÂú®ÊüêÁ´ô‰∏äÈîÄÂîÆËøô‰∫õÂ∏¶ÊúâÁßÅ‰∫∫Ê∞¥Âç∞ÁöÑËµÑÊ∫ê„ÄÇ‰∏∫‰∫ÜÂ∫îÂØπËøôÁßçÊÉÖÂÜµÔºåÊàëËÆ°ÂàíÂ∞ÜËøô‰∫õËµÑÊ∫êÈõÜ‰∏≠Âπ∂ÂºÄÊ∫êÔºå‰ª•‰øÉËøõ‰πâÂä°ÊïôËÇ≤ÁöÑÊôÆÂèäÂíåÊ∂àÈô§Âú∞Âå∫Èó¥ÁöÑÊïôËÇ≤Ë¥´Âõ∞„ÄÇ</p><p>ËøòÊúâ‰∏Ä‰∏™ÊúÄÈáçË¶ÅÁöÑÂéüÂõ†ÊòØÔºåÂ∏åÊúõÊµ∑Â§ñÂçé‰∫∫ËÉΩÂ§üËÆ©Ëá™Â∑±ÁöÑÂ≠©Â≠êÁªßÁª≠‰∫ÜËß£ÂõΩÂÜÖÊïôËÇ≤„ÄÇ</p><p>Áî±‰∫é GitHub ÂØπÂçï‰∏™Êñá‰ª∂ÁöÑ‰∏ä‰º†ÊúâÊúÄÂ§ßÈôêÂà∂ÔºåË∂ÖËøá 100MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãíÁªù‰∏ä‰º†ÔºåË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰∏ä‰º†Êó∂‰ºöÊî∂Âà∞Ë≠¶Âëä„ÄÇÂõ†Ê≠§ÔºåÊñá‰ª∂Â§ßÂ∞èË∂ÖËøá 50MB ÁöÑÊñá‰ª∂‰ºöË¢´ÊãÜÂàÜÊàêÊØè‰∏™ 35MB ÁöÑÂ§ö‰∏™Êñá‰ª∂„ÄÇ</p><ul></ul><p>Ë¶ÅÂêàÂπ∂Ëøô‰∫õË¢´ÊãÜÂàÜÁöÑÊñá‰ª∂ÔºåÊÇ®Âè™ÈúÄÊâßË°å‰ª•‰∏ãÊ≠•È™§(ÂÖ∂‰ªñÊìç‰ΩúÁ≥ªÁªüÂêåÁêÜ)Ôºö</p><ol><li>Â∞ÜÂêàÂπ∂Á®ãÂ∫è <code>mergePDFs-windows-amd64.exe</code> ‰∏ãËΩΩÂà∞ÂåÖÂê´ PDF Êñá‰ª∂ÁöÑÊñá‰ª∂Â§π‰∏≠„ÄÇ</li><li>Á°Æ‰øù <code>mergePDFs-windows-amd64.exe</code> ÂíåË¢´ÊãÜÂàÜÁöÑ PDF Êñá‰ª∂Âú®Âêå‰∏ÄÁõÆÂΩï‰∏ã„ÄÇ</li><li>ÂèåÂáª <code>mergePDFs-windows-amd64.exe</code> Á®ãÂ∫èÂç≥ÂèØËá™Âä®ÂÆåÊàêÊñá‰ª∂ÂêàÂπ∂„ÄÇ</li></ol><ul><li>mergePDFs-windows-amd64.exe</li></ul><ul><li>Â¶ÇÊûúÊÇ®‰Ωç‰∫éÂõΩÂ§ñÔºåÂíåÂÜÖÂú∞ÁΩëÁªúÈÄö‰ø°ÈÄüÂ∫¶ËæÉÊÖ¢ÔºåÂª∫ËÆÆ‰ΩøÁî®Êú¨Â≠òÂÇ®Â∫ìËøõË°åÁ≠æÂá∫„ÄÇ</li></ul><p>Â¶ÇÊûúËøô‰∏™È°πÁõÆÂ∏ÆÂä©ÊÇ®ÂÖçË¥πËé∑ÂèñÊïôËÇ≤ËµÑÊ∫êÔºåËØ∑ËÄÉËôëÊîØÊåÅÊàë‰ª¨Êé®ÂπøÂºÄÊîæÊïôËÇ≤ÁöÑÂä™ÂäõÔºÅÊÇ®ÁöÑÊçêÁåÆÂ∞ÜÂ∏ÆÂä©Êàë‰ª¨Áª¥Êä§ÂíåÊâ©Â±ïËøô‰∏™ËµÑÊ∫êÂ∫ì„ÄÇ</p><p>Â¶ÇÊûúÊÇ®ËßâÂæóËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåÊÇ®ÂèØ‰ª•Êâ´Êèè‰ª•‰∏ã‰∫åÁª¥Á†ÅËøõË°åÊçêËµ†Ôºö</p>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"onlook-dev/onlook","url":"https://github.com/onlook-dev/onlook","date":1751423796,"author":"","guid":179516,"unread":true,"content":"<p>The Cursor for Designers ‚Ä¢ An Open-Source Visual Vibecoding Editor ‚Ä¢ Visually build, style, and edit your React App with AI</p><p>Craft websites, prototypes, and designs with AI in Next.js + TailwindCSS. Make edits directly in the browser DOM with a visual editor. Design in realtime with code. An open-source alternative to Bolt.new, Lovable, V0, Replit Agent, Figma Make, Webflow, etc.</p><h3>üöß üöß üöß Onlook for Web is still under development üöß üöß üöß</h3><p>We're actively looking for contributors to help make Onlook for Web an incredible prompt-to-build experience. Check the <a href=\"https://github.com/onlook-dev/onlook/issues\">open issues</a> for a full list of proposed features (and known issues), and join our <a href=\"https://discord.gg/hERDfFZCsH\">Discord</a> to collaborate with hundreds of other builders.</p><h2>What you can do with Onlook:</h2><h3>Onlook for Desktop (aka Onlook Alpha)</h3><p>We're in early preview for Onlook Web. If you're looking for the downloadable desktop electron app, it's moved to <a href=\"https://github.com/onlook-dev/desktop\">Onlook Desktop</a>.</p><p>Onlook will run on any Next.js + TailwindCSS project, import your project into Onlook or start from scratch within the editor.</p><p>Use the AI chat to create or edit a project you're working on. At any time, you can always right-click an element to open up the exact location of the element in code.</p><img width=\"600\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4ad9f411-b172-4430-81ef-650f4f314666\"><p>Draw-in new divs and re-arrange them within their parent containers by dragging-and-dropping.</p><img width=\"600\" alt=\"image\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/insert-div.png\"><p>Preview the code side-by-side with your site design.</p><img width=\"600\" alt=\"image\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/code-connect.png\"><p>Use Onlook's editor toolbar to adjust Tailwind styles, directly manipulate objects, and experiment with layouts.</p><img width=\"600\" alt=\"image\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/text-styling.png\"><img width=\"676\" alt=\"architecture\" src=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/assets/architecture.png\"><ol><li>When you create an app, we load the code into a web container</li><li>The container runs and serves the code</li><li>Our editor receives the preview link and displays it in an iFrame</li><li>Our editor reads and indexes the code from the container</li><li>We instrument the code in order to map elements to their place in code</li><li>When the element is edited, we edit the element in our iFrame, then in code</li><li>Our AI chat also has code access and tools to understand and edit the code</li></ol><p>This architecture can theoretically scale to any language or framework that displays DOM elements declaratively (e.g. jsx/tsx/html). We are focused on making it work well with Next.js and TailwindCSS for now.</p><ul><li><a href=\"https://bun.sh/\">Bun</a> - Monorepo, runtime, bundler</li></ul><p>If you have a suggestion that would make this better, please fork the repo and create a pull request. You can also <a href=\"https://github.com/onlook-dev/onlook/issues\">open issues</a>.</p><a href=\"https://github.com/onlook-dev/onlook/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=onlook-dev/onlook\"></a><p>Distributed under the Apache 2.0 License. See <a href=\"https://raw.githubusercontent.com/onlook-dev/onlook/main/LICENSE.md\">LICENSE.md</a> for more information.</p>","contentLength":2340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NanmiCoder/MediaCrawler","url":"https://github.com/NanmiCoder/MediaCrawler","date":1751423796,"author":"","guid":179517,"unread":true,"content":"<p>Â∞èÁ∫¢‰π¶Á¨îËÆ∞ | ËØÑËÆ∫Áà¨Ëô´„ÄÅÊäñÈü≥ËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅÂø´ÊâãËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅB Á´ôËßÜÈ¢ë ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÂæÆÂçöÂ∏ñÂ≠ê ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÁôæÂ∫¶Ë¥¥ÂêßÂ∏ñÂ≠ê ÔΩú ÁôæÂ∫¶Ë¥¥ÂêßËØÑËÆ∫ÂõûÂ§çÁà¨Ëô´ | Áü•‰πéÈóÆÁ≠îÊñáÁ´†ÔΩúËØÑËÆ∫Áà¨Ëô´</p><blockquote><p>Êú¨‰ªìÂ∫ìÁöÑÊâÄÊúâÂÜÖÂÆπ‰ªÖ‰æõÂ≠¶‰π†ÂíåÂèÇËÄÉ‰πãÁî®ÔºåÁ¶ÅÊ≠¢Áî®‰∫éÂïÜ‰∏öÁî®ÈÄî„ÄÇ‰ªª‰Ωï‰∫∫ÊàñÁªÑÁªá‰∏çÂæóÂ∞ÜÊú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÁî®‰∫éÈùûÊ≥ïÁî®ÈÄîÊàñ‰æµÁäØ‰ªñ‰∫∫ÂêàÊ≥ïÊùÉÁõä„ÄÇÊú¨‰ªìÂ∫ìÊâÄÊ∂âÂèäÁöÑÁà¨Ëô´ÊäÄÊúØ‰ªÖÁî®‰∫éÂ≠¶‰π†ÂíåÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫éÂØπÂÖ∂‰ªñÂπ≥Âè∞ËøõË°åÂ§ßËßÑÊ®°Áà¨Ëô´ÊàñÂÖ∂‰ªñÈùûÊ≥ïË°å‰∏∫„ÄÇÂØπ‰∫éÂõ†‰ΩøÁî®Êú¨‰ªìÂ∫ìÂÜÖÂÆπËÄåÂºïËµ∑ÁöÑ‰ªª‰ΩïÊ≥ïÂæãË¥£‰ªªÔºåÊú¨‰ªìÂ∫ì‰∏çÊâøÊãÖ‰ªª‰ΩïË¥£‰ªª„ÄÇ‰ΩøÁî®Êú¨‰ªìÂ∫ìÁöÑÂÜÖÂÆπÂç≥Ë°®Á§∫ÊÇ®ÂêåÊÑèÊú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊâÄÊúâÊù°Ê¨æÂíåÊù°‰ª∂„ÄÇ</p></blockquote><p>‰∏Ä‰∏™ÂäüËÉΩÂº∫Â§ßÁöÑÔºåÊîØÊåÅÂ∞èÁ∫¢‰π¶„ÄÅÊäñÈü≥„ÄÅÂø´Êâã„ÄÅBÁ´ô„ÄÅÂæÆÂçö„ÄÅË¥¥Âêß„ÄÅÁü•‰πéÁ≠â‰∏ªÊµÅÂπ≥Âè∞ÁöÑÂÖ¨ÂºÄ‰ø°ÊÅØÊäìÂèñ„ÄÇ</p><ul><li>ÔºöÂà©Áî®‰øùÁïôÁôªÂΩïÊÄÅÁöÑÊµèËßàÂô®‰∏ä‰∏ãÊñáÁéØÂ¢ÉÔºåÈÄöËøá JS Ë°®ËææÂºèËé∑ÂèñÁ≠æÂêçÂèÇÊï∞</li><li>ÔºöÊó†ÈúÄÈÄÜÂêëÂ§çÊùÇÁöÑÂä†ÂØÜÁÆóÊ≥ïÔºåÂ§ßÂπÖÈôç‰ΩéÊäÄÊúØÈó®Êßõ</li></ul><table><thead><tr></tr></thead><tbody></tbody></table><blockquote><p>üí° <strong>ÂºÄÊ∫ê‰∏çÊòìÔºåÂ¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºÅ</strong></p></blockquote><p>Âú®ËøõË°å‰∏ã‰∏ÄÊ≠•Êìç‰Ωú‰πãÂâçÔºåËØ∑Á°Æ‰øùÁîµËÑë‰∏äÂ∑≤ÁªèÂÆâË£Ö‰∫Ü uvÔºö</p><ul><li>ÔºöÁªàÁ´ØËæìÂÖ•ÂëΩ‰ª§ ÔºåÂ¶ÇÊûúÊ≠£Â∏∏ÊòæÁ§∫ÁâàÊú¨Âè∑ÔºåËØÅÊòéÂ∑≤ÁªèÂÆâË£ÖÊàêÂäü</li><li>Ôºöuv ÊòØÁõÆÂâçÊúÄÂº∫ÁöÑ Python ÂåÖÁÆ°ÁêÜÂ∑•ÂÖ∑ÔºåÈÄüÂ∫¶Âø´„ÄÅ‰æùËµñËß£ÊûêÂáÜÁ°Æ</li></ul><pre><code># ËøõÂÖ•È°πÁõÆÁõÆÂΩï\ncd MediaCrawler\n\n# ‰ΩøÁî® uv sync ÂëΩ‰ª§Êù•‰øùËØÅ python ÁâàÊú¨ÂíåÁõ∏ÂÖ≥‰æùËµñÂåÖÁöÑ‰∏ÄËá¥ÊÄß\nuv sync\n</code></pre><pre><code># ÂÆâË£ÖÊµèËßàÂô®È©±Âä®\nuv run playwright install\n</code></pre><blockquote><p>ÔºöMediaCrawler ÁõÆÂâçÂ∑≤ÁªèÊîØÊåÅ‰ΩøÁî® playwright ËøûÊé•‰Ω†Êú¨Âú∞ÁöÑ Chrome ÊµèËßàÂô®‰∫ÜÔºå‰∏Ä‰∫õÂõ†‰∏∫ Webdriver ÂØºËá¥ÁöÑÈóÆÈ¢òËøéÂàÉËÄåËß£‰∫Ü„ÄÇ</p><p>ÁõÆÂâçÂºÄÊîæ‰∫Ü  Âíå  Ëøô‰∏§‰∏™‰ΩøÁî® CDP ÁöÑÊñπÂºèËøûÊé•Êú¨Âú∞ÊµèËßàÂô®ÔºåÂ¶ÇÊúâÈúÄË¶ÅÔºåÊü•Áúã  ‰∏≠ÁöÑÈÖçÁΩÆÈ°π„ÄÇ</p></blockquote><pre><code># È°πÁõÆÈªòËÆ§ÊòØÊ≤°ÊúâÂºÄÂêØËØÑËÆ∫Áà¨ÂèñÊ®°ÂºèÔºåÂ¶ÇÈúÄËØÑËÆ∫ËØ∑Âú® config/base_config.py ‰∏≠ÁöÑ ENABLE_GET_COMMENTS ÂèòÈáè‰øÆÊîπ\n# ‰∏Ä‰∫õÂÖ∂‰ªñÊîØÊåÅÈ°πÔºå‰πüÂèØ‰ª•Âú® config/base_config.py Êü•ÁúãÂäüËÉΩÔºåÂÜôÁöÑÊúâ‰∏≠ÊñáÊ≥®Èáä\n\n# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÂÖ≥ÈîÆËØçÊêúÁ¥¢Áõ∏ÂÖ≥ÁöÑÂ∏ñÂ≠êÂπ∂Áà¨ÂèñÂ∏ñÂ≠ê‰ø°ÊÅØ‰∏éËØÑËÆ∫\nuv run main.py --platform xhs --lt qrcode --type search\n\n# ‰ªéÈÖçÁΩÆÊñá‰ª∂‰∏≠ËØªÂèñÊåáÂÆöÁöÑÂ∏ñÂ≠êIDÂàóË°®Ëé∑ÂèñÊåáÂÆöÂ∏ñÂ≠êÁöÑ‰ø°ÊÅØ‰∏éËØÑËÆ∫‰ø°ÊÅØ\nuv run main.py --platform xhs --lt qrcode --type detail\n\n# ÊâìÂºÄÂØπÂ∫îAPPÊâ´‰∫åÁª¥Á†ÅÁôªÂΩï\n\n# ÂÖ∂‰ªñÂπ≥Âè∞Áà¨Ëô´‰ΩøÁî®Á§∫‰æãÔºåÊâßË°å‰∏ãÈù¢ÁöÑÂëΩ‰ª§Êü•Áúã\nuv run main.py --help\n</code></pre><ul><li>ÔºöÊîØÊåÅÂÖ≥Á≥ªÂûãÊï∞ÊçÆÂ∫ì MySQL ‰∏≠‰øùÂ≠òÔºàÈúÄË¶ÅÊèêÂâçÂàõÂª∫Êï∞ÊçÆÂ∫ìÔºâ \n  <ul><li>ÊâßË°å  ÂàùÂßãÂåñÊï∞ÊçÆÂ∫ìË°®ÁªìÊûÑÔºàÂè™Âú®È¶ñÊ¨°ÊâßË°åÔºâ</li></ul></li><li>ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ CSV ‰∏≠Ôºà ÁõÆÂΩï‰∏ãÔºâ</li><li>ÔºöÊîØÊåÅ‰øùÂ≠òÂà∞ JSON ‰∏≠Ôºà ÁõÆÂΩï‰∏ãÔºâ</li></ul><blockquote><p>Â¶ÇÊûúÊÉ≥Âø´ÈÄüÂÖ•Èó®ÂíåÂ≠¶‰π†ËØ•È°πÁõÆÁöÑ‰ΩøÁî®„ÄÅÊ∫êÁ†ÅÊû∂ÊûÑËÆæËÆ°Á≠â„ÄÅÂ≠¶‰π†ÁºñÁ®ãÊäÄÊúØ„ÄÅ‰∫¶ÊàñËÄÖÊÉ≥‰∫ÜËß£MediaCrawlerProÁöÑÊ∫ê‰ª£Á†ÅËÆæËÆ°ÂèØ‰ª•Áúã‰∏ãÊàëÁöÑÁü•ËØÜ‰ªòË¥πÊ†èÁõÆ„ÄÇ</p></blockquote><p>Â¶ÇÊûúËøô‰∏™È°πÁõÆÂØπÊÇ®ÊúâÂ∏ÆÂä©ÔºåËØ∑Áªô‰∏™ ‚≠ê Star ÊîØÊåÅ‰∏Ä‰∏ãÔºåËÆ©Êõ¥Â§öÁöÑ‰∫∫ÁúãÂà∞ MediaCrawlerÔºÅ</p><a href=\"https://www.swiftproxy.net/?ref=nanmi\"><img src=\"https://raw.githubusercontent.com/NanmiCoder/MediaCrawler/main/docs/static/images/img_5.png\"> **Swiftproxy** - 90M+ ÂÖ®ÁêÉÈ´òË¥®ÈáèÁ∫ØÂáÄ‰ΩèÂÆÖIPÔºåÊ≥®ÂÜåÂèØÈ¢ÜÂÖçË¥π 500MB ÊµãËØïÊµÅÈáèÔºåÂä®ÊÄÅÊµÅÈáè‰∏çËøáÊúüÔºÅ &gt; ‰∏ìÂ±ûÊäòÊâ£Á†ÅÔºö**GHB5** Á´ã‰∫´‰πùÊäò‰ºòÊÉ†ÔºÅ </a><p>Êàê‰∏∫ËµûÂä©ËÄÖÔºåÂèØ‰ª•Â∞ÜÊÇ®ÁöÑ‰∫ßÂìÅÂ±ïÁ§∫Âú®ËøôÈáåÔºåÊØèÂ§©Ëé∑ÂæóÂ§ßÈáèÊõùÂÖâÔºÅ</p><ul></ul><div><p>Êú¨È°πÁõÆÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÊú¨È°πÁõÆ‚ÄùÔºâÊòØ‰Ωú‰∏∫‰∏Ä‰∏™ÊäÄÊúØÁ†îÁ©∂‰∏éÂ≠¶‰π†Â∑•ÂÖ∑ËÄåÂàõÂª∫ÁöÑÔºåÊó®Âú®Êé¢Á¥¢ÂíåÂ≠¶‰π†ÁΩëÁªúÊï∞ÊçÆÈááÈõÜÊäÄÊúØ„ÄÇÊú¨È°πÁõÆ‰∏ìÊ≥®‰∫éËá™Â™í‰ΩìÂπ≥Âè∞ÁöÑÊï∞ÊçÆÁà¨ÂèñÊäÄÊúØÁ†îÁ©∂ÔºåÊó®Âú®Êèê‰æõÁªôÂ≠¶‰π†ËÄÖÂíåÁ†îÁ©∂ËÄÖ‰Ωú‰∏∫ÊäÄÊúØ‰∫§ÊµÅ‰πãÁî®„ÄÇ</p><p>Êú¨È°πÁõÆÂºÄÂèëËÄÖÔºà‰ª•‰∏ãÁÆÄÁß∞‚ÄúÂºÄÂèëËÄÖ‚ÄùÔºâÈÉëÈáçÊèêÈÜíÁî®Êà∑Âú®‰∏ãËΩΩ„ÄÅÂÆâË£ÖÂíå‰ΩøÁî®Êú¨È°πÁõÆÊó∂Ôºå‰∏•Ê†ºÈÅµÂÆà‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÔºåÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫é„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁΩëÁªúÂÆâÂÖ®Ê≥ï„Äã„ÄÅ„Ää‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÂèçÈó¥Ë∞çÊ≥ï„ÄãÁ≠âÊâÄÊúâÈÄÇÁî®ÁöÑÂõΩÂÆ∂Ê≥ïÂæãÂíåÊîøÁ≠ñ„ÄÇÁî®Êà∑Â∫îËá™Ë°åÊâøÊãÖ‰∏ÄÂàáÂõ†‰ΩøÁî®Êú¨È°πÁõÆËÄåÂèØËÉΩÂºïËµ∑ÁöÑÊ≥ïÂæãË¥£‰ªª„ÄÇ</p><p>Êú¨È°πÁõÆ‰∏•Á¶ÅÁî®‰∫é‰ªª‰ΩïÈùûÊ≥ïÁõÆÁöÑÊàñÈùûÂ≠¶‰π†„ÄÅÈùûÁ†îÁ©∂ÁöÑÂïÜ‰∏öË°å‰∏∫„ÄÇÊú¨È°πÁõÆ‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ï‰æµÂÖ•‰ªñ‰∫∫ËÆ°ÁÆóÊú∫Á≥ªÁªüÔºå‰∏çÂæóÁî®‰∫é‰ªª‰Ωï‰æµÁäØ‰ªñ‰∫∫Áü•ËØÜ‰∫ßÊùÉÊàñÂÖ∂‰ªñÂêàÊ≥ïÊùÉÁõäÁöÑË°å‰∏∫„ÄÇÁî®Êà∑Â∫î‰øùËØÅÂÖ∂‰ΩøÁî®Êú¨È°πÁõÆÁöÑÁõÆÁöÑÁ∫ØÂ±û‰∏™‰∫∫Â≠¶‰π†ÂíåÊäÄÊúØÁ†îÁ©∂Ôºå‰∏çÂæóÁî®‰∫é‰ªª‰ΩïÂΩ¢ÂºèÁöÑÈùûÊ≥ïÊ¥ªÂä®„ÄÇ</p><p>ÂºÄÂèëËÄÖÂ∑≤Â∞ΩÊúÄÂ§ßÂä™ÂäõÁ°Æ‰øùÊú¨È°πÁõÆÁöÑÊ≠£ÂΩìÊÄßÂèäÂÆâÂÖ®ÊÄßÔºå‰ΩÜ‰∏çÂØπÁî®Êà∑‰ΩøÁî®Êú¨È°πÁõÆÂèØËÉΩÂºïËµ∑ÁöÑ‰ªª‰ΩïÂΩ¢ÂºèÁöÑÁõ¥Êé•ÊàñÈó¥Êé•ÊçüÂ§±ÊâøÊãÖË¥£‰ªª„ÄÇÂåÖÊã¨‰ΩÜ‰∏çÈôê‰∫éÁî±‰∫é‰ΩøÁî®Êú¨È°πÁõÆËÄåÂØºËá¥ÁöÑ‰ªª‰ΩïÊï∞ÊçÆ‰∏¢Â§±„ÄÅËÆæÂ§áÊçüÂùè„ÄÅÊ≥ïÂæãËØâËÆºÁ≠â„ÄÇ</p><p>Êú¨È°πÁõÆÁöÑÁü•ËØÜ‰∫ßÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÊú¨È°πÁõÆÂèóÂà∞Ëëó‰ΩúÊùÉÊ≥ïÂíåÂõΩÈôÖËëó‰ΩúÊùÉÊù°Á∫¶‰ª•ÂèäÂÖ∂‰ªñÁü•ËØÜ‰∫ßÊùÉÊ≥ïÂæãÂíåÊù°Á∫¶ÁöÑ‰øùÊä§„ÄÇÁî®Êà∑Âú®ÈÅµÂÆàÊú¨Â£∞ÊòéÂèäÁõ∏ÂÖ≥Ê≥ïÂæãÊ≥ïËßÑÁöÑÂâçÊèê‰∏ãÔºåÂèØ‰ª•‰∏ãËΩΩÂíå‰ΩøÁî®Êú¨È°πÁõÆ„ÄÇ</p><p>ÂÖ≥‰∫éÊú¨È°πÁõÆÁöÑÊúÄÁªàËß£ÈáäÊùÉÂΩíÂºÄÂèëËÄÖÊâÄÊúâ„ÄÇÂºÄÂèëËÄÖ‰øùÁïôÈöèÊó∂Êõ¥ÊîπÊàñÊõ¥Êñ∞Êú¨ÂÖçË¥£Â£∞ÊòéÁöÑÊùÉÂà©ÔºåÊÅï‰∏çÂè¶Ë°åÈÄöÁü•„ÄÇ</p></div><p>ÊÑüË∞¢ JetBrains ‰∏∫Êú¨È°πÁõÆÊèê‰æõÂÖçË¥πÁöÑÂºÄÊ∫êËÆ∏ÂèØËØÅÊîØÊåÅÔºÅ</p><a href=\"https://www.jetbrains.com/?from=MediaCrawler\"><img src=\"https://www.jetbrains.com/company/brand/img/jetbrains_logo.png\" width=\"100\" alt=\"JetBrains\"></a>","contentLength":4915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ColorlibHQ/AdminLTE","url":"https://github.com/ColorlibHQ/AdminLTE","date":1751423796,"author":"","guid":179518,"unread":true,"content":"<p>AdminLTE - Free admin dashboard template based on Bootstrap 5</p><p> is a fully responsive administration template. Based on  framework and also the JavaScript plugins. Highly customizable and easy to use. Fits many screen resolutions from small mobile devices to large desktops.</p><p><strong>Production Deployment &amp; Cross-Platform Compatibility</strong> - This release resolves critical production deployment issues:</p><ul><li> - Resolved CSS/JS path issues, sidebar navigation, and image loading in all deployment scenarios</li><li> - Automatic relative path calculation works for root deployment, sub-folders, and CDN hosting</li><li> - Eliminated rtlcss interference with standard LTR production builds</li><li> - Bootstrap 5.3.7, Bootstrap Icons 1.13.1, OverlayScrollbars 2.11.0</li><li> - Fixed all CDN integrity mismatches and runtime issues</li><li> - Perfect compatibility with traditional hosting and modern static platforms</li></ul><ul><li>‚úÖ Development and production environments now behave identically</li><li>‚úÖ Images, CSS, and JavaScript load correctly in any deployment structure</li><li>‚úÖ Sidebar navigation displays properly with badges and arrow indicators</li><li>‚úÖ All CDN resources load without console errors</li><li>‚úÖ Complete production build included in repository for easy deployment</li></ul><h2>Looking for Premium Templates?</h2><p>AdminLTE.io just opened a new premium templates page. Hand picked to ensure the best quality and the most affordable prices. Visit <a href=\"https://adminlte.io/premium\">https://adminlte.io/premium</a> for more information.</p><p> has been carefully coded with clear comments in all of its JS, SCSS and HTML files. SCSS has been used to increase code customizability.</p><p>To start developing with AdminLTE:</p><ol><li> Files auto-compile and refresh on changes</li></ol><ol><li><em>(includes linting and optimization)</em></li><li><em>(faster for development/testing)</em></li></ol><ul><li> - Start development server with file watching</li><li> - Build all assets for development</li><li> - Full production build with linting and bundlewatch</li><li> - Run all linters (JS, CSS, docs, lockfile)</li><li> - Build CSS only</li><li> - Build JavaScript only</li></ul><p>AdminLTE supports all modern browsers with the latest Bootstrap 5.3.7:</p><ul></ul><ul><li>First thing first, you should have bit knowledge about NodeJS.</li><li>Install NodeJS LTS version.</li><li>Clone this Repository to your machine and change to  branch.</li><li>In cli/bash run  it will install dependency from .</li><li>After installation completes, run </li><li>Cool, Send your changes in PR to  branch.</li></ul><p>AdminLTE is an open source project by <a href=\"https://adminlte.io\">AdminLTE.io</a> that is licensed under <a href=\"https://opensource.org/licenses/MIT\">MIT</a>. AdminLTE.io reserves the right to change the license of future releases.</p>","contentLength":2385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"actualbudget/actual","url":"https://github.com/actualbudget/actual","date":1751337922,"author":"","guid":177070,"unread":true,"content":"<p>A local-first personal finance app</p><p>Actual is a local-first personal finance tool. It is 100% free and open-source, written in NodeJS, it has a synchronization element so that all your changes can move between devices without any heavy lifting.</p><p>If you are interested in contributing, or want to know how development works, see our <a href=\"https://actualbudget.org/docs/contributing/\">contributing</a> document we would love to have you.</p><p>Want to say thanks? Click the ‚≠ê at the top of the page.</p><p>There are four ways to deploy Actual:</p><h2>Ready to Start Budgeting?</h2><h3>Are you new to budgeting or want to start fresh?</h3><p>Check out the community's <a href=\"https://actualbudget.org/docs/getting-started/starting-fresh\">Starting Fresh</a> guide so you can quickly get up and running!</p><h3>Are you migrating from other budgeting apps?</h3><p>Check out the community's <a href=\"https://actualbudget.org/docs/migration/\">Migration</a> guide to start jumping on the Actual Budget train!</p><p>We have a wide range of documentation on how to use Actual, this is all available in our <a href=\"https://actualbudget.org/docs\">Community Documentation</a>, this includes topics on Budgeting, Account Management, Tips &amp; Tricks and some documentation for developers.</p><p>The Actual app is split up into a few packages:</p><ul><li>loot-core - The core application that runs on any platform</li><li>desktop-client - The desktop UI</li><li>desktop-electron - The desktop app</li></ul><p>Current feature requests can be seen <a href=\"https://github.com/actualbudget/actual/issues?q=is%3Aissue+label%3A%22needs+votes%22+sort%3Areactions-%2B1-desc\">here</a>. Vote for your favorite requests by reacting  to the top comment of the request.</p><p>To add new feature requests, open a new Issue of the \"Feature Request\" type.</p><p>Make Actual Budget accessible to more people by helping with the <a href=\"https://actualbudget.org/docs/contributing/i18n/\">Internationalization</a> of Actual. We are using a crowd sourcing tool to manage the translations, see our <a href=\"https://hosted.weblate.org/projects/actualbudget/\">Weblate Project</a>. Weblate proudly supports open-source software projects through their <a href=\"https://weblate.org/en/hosting/#libre\">Libre plan</a>.</p><a href=\"https://hosted.weblate.org/engage/actualbudget/\"><img src=\"https://hosted.weblate.org/widget/actualbudget/actual/287x66-grey.png\" alt=\"Translation status\"></a><p>Thanks to our wonderful sponsors who make Actual Budget possible!</p>","contentLength":1677,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"nextcloud/all-in-one","url":"https://github.com/nextcloud/all-in-one","date":1751337922,"author":"","guid":177071,"unread":true,"content":"<p>üì¶ The official Nextcloud installation method. Provides easy deployment and maintenance with most features included in this one Nextcloud instance.</p><p>The official Nextcloud installation method. Nextcloud AIO provides easy deployment and maintenance with most features included in this one Nextcloud instance.</p><ul><li>High performance backend for Nextcloud Files</li><li>Nextcloud Office (optional)</li><li>High performance backend for Nextcloud Talk and TURN-server (optional)</li><li>Nextcloud Talk Recording-server (optional)</li><li>Imaginary (optional, for previews of heic, heif, illustrator, pdf, svg, tiff and webp)</li><li>ClamAV (optional, Antivirus backend for Nextcloud)</li><li>Fulltextsearch (optional)</li></ul><blockquote><p>[!WARNING] You should first make sure that you are not using docker installed via snap. You can check this by running <code>sudo docker info | grep \"Docker Root Dir\" | grep \"/var/snap/docker/\"</code>. If the output should contain the mentioned string , you should first uninstall docker snap via  and then follow the instructions below. ‚ö†Ô∏è Attention: only run the command if this is a clean new docker installation and you are not running any service already using this.</p></blockquote><blockquote><p>[!NOTE] The following instructions are meant for installations without a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else) already being in place. If you want to run AIO behind a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else), see the <a href=\"https://github.com/nextcloud/all-in-one/raw/main/reverse-proxy.md\">reverse proxy documentation</a>. Also, the instructions below are especially meant for Linux. For macOS see <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-run-aio-on-macos\">this</a>, for Windows see <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-run-aio-on-windows\">this</a> and for Synology see <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-run-aio-on-synology-dsm\">this</a>.</p></blockquote><blockquote><p>[!WARNING] You could use the convenience script below to install docker. However we recommend to not blindly download and execute scripts as sudo. But if you feel like it, you can of course use it. See below:</p></blockquote><ol start=\"2\"><li><p>Run the command below in order to start the container on Linux and without a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else) already in place:</p><pre><code># For Linux and without a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else) already in place:\nsudo docker run \\\n--init \\\n--sig-proxy=false \\\n--name nextcloud-aio-mastercontainer \\\n--restart always \\\n--publish 80:80 \\\n--publish 8080:8080 \\\n--publish 8443:8443 \\\n--volume nextcloud_aio_mastercontainer:/mnt/docker-aio-config \\\n--volume /var/run/docker.sock:/var/run/docker.sock:ro \\\nghcr.io/nextcloud-releases/all-in-one:latest\n</code></pre><p>Note: You may be interested in adjusting Nextcloud‚Äôs datadir to store the files in a different location than the default docker volume. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p></li><li><p>After the initial startup, you should be able to open the Nextcloud AIO Interface now on port 8080 of this server. E.g. <code>https://ip.address.of.this.server:8080</code> ‚ö†Ô∏è  do always use an ip-address if you access this port and not a domain as HSTS might block access to it later! (It is also expected that this port uses a self-signed certificate due to security concerns which you need to accept in your browser)<p> If your firewall/router has port 80 and 8443 open/forwarded and you point a domain to your server, you can get a valid certificate automatically by opening the Nextcloud AIO Interface via:</p><code>https://your-domain-that-points-to-this-server.tld:8443</code></p></li><li><p>Please do not forget to open port  and  in your firewall/router for the Talk container!</p></li></ol><h3>Where can I find additional documentation?</h3><p>Nextcloud AIO is inspired by projects like Portainer that manage the docker daemon by talking to it through the docker socket directly. This concept allows a user to install only one container with a single command that does the heavy lifting of creating and managing all containers that are needed in order to provide a Nextcloud installation with most features included. It also makes updating a breeze and is not bound to the host system (and its slow updates) anymore as everything is in containers. Additionally, it is very easy to handle from a user perspective because a simple interface for managing your Nextcloud AIO installation is provided.</p><p>See <a href=\"https://github.com/nextcloud/all-in-one/issues/5251\">this issue</a> for a list of feature requests that need help by contributors.</p><h3>How many users are possible?</h3><h3>Are reverse proxies supported?</h3><h3>Which ports are mandatory to be open in your firewall/router?</h3><p>Only those (if you access the Mastercontainer Interface internally via port 8080):</p><ul><li> for the Apache container</li><li> if you want to enable http3 for the Apache container</li><li> and  for the Talk container</li></ul><h3>Explanation of used ports</h3><ul><li>: Mastercontainer Interface with self-signed certificate (works always, also if only access via IP-address is possible, e.g. <code>https://ip.address.of.this.server:8080/</code>) ‚ö†Ô∏è  do always use an ip-address if you access this port and not a domain as HSTS might block access to it later! (It is also expected that this port uses a self-signed certificate due to security concerns which you need to accept in your browser)</li><li>: redirects to Nextcloud (is used for getting the certificate via ACME http-challenge for the Mastercontainer)</li><li>: Mastercontainer Interface with valid certificate (only works if port 80 and 8443 are open/forwarded in your firewall/router and you point a domain to your server. It generates a valid certificate then automatically and access via e.g. <code>https://public.domain.com:8443/</code> is possible.)</li><li>: will be used by the Apache container later on and needs to be open/forwarded in your firewall/router</li><li>: will be used by the Apache container later on and needs to be open/forwarded in your firewall/router if you want to enable http3</li><li> and : will be used by the Turnserver inside the Talk container and needs to be open/forwarded in your firewall/router</li></ul><h3>Notes on Cloudflare (proxy/tunnel)</h3><p>Since Cloudflare Proxy/Tunnel comes with a lot of limitations which are listed below, it is rather recommended to switch to <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a> if possible.</p><ul><li>Cloudflare Proxy and Cloudflare Tunnel both require Cloudflare to perform TLS termination on their side and thus decrypt all the traffic on their infrastructure. This is a privacy concern and you will need to look for other solutions if it's unacceptable for you.</li><li>Using Cloudflare Tunnel might potentially slow down Nextcloud since local access via the configured domain is not possible because TLS termination is in that case offloaded to Cloudflare's infrastructure. There is no way to disable this behavior in Cloudflare Tunnel.</li><li>Cloudflare only supports uploading files up to 100&nbsp;MB in the free plan, if you try to upload bigger files you will get an error (413 - Payload Too Large) if no chunking is used (e.g. for public uploads in the web, or if chunks are configured to be bigger than 100 MB in the clients or the web). If you need to upload bigger files, you need to disable the proxy option in your DNS settings. Note that this will both disable Cloudflare DDoS protection and Cloudflare Tunnel as these services require the proxy option to be enabled.</li><li>Cloudflare only allows a max timeout of 100s for requests which is not configurable. This means that any server-side processing e.g. for assembling chunks for big files during upload that take longer than 100s will simply not work. See <a href=\"https://github.com/nextcloud/server/issues/19223\">https://github.com/nextcloud/server/issues/19223</a>. If you need to upload big files reliably, you need to disable the proxy option in your DNS settings. Note that this will both disable Cloudflare DDoS protection and Cloudflare Tunnel as these services require the proxy option to be enabled.</li><li>It is known that the in AIO included collabora (Nextcloud Office) does not work out of the box behind Cloudflare. To make it work, you need to add all <a href=\"https://www.cloudflare.com/ips/\">Cloudflare IP-ranges</a> to the wopi-allowlist in <code>https://yourdomain.com/settings/admin/richdocuments</code></li><li>The built-in turn-server for Nextcloud Talk will not work behind Cloudflare Tunnel since it needs a separate port (by default 3478 or as chosen) available on the same domain. If you still want to use the feature, you will need to install your own turnserver or use a publicly available one and adjust and test your stun and turn settings in <code>https://yourdomain.com/settings/admin/talk</code>.</li><li>If you get an error in Nextcloud's admin overview that the HSTS header is not set correctly, you might need to enable it in Cloudflare manually.</li><li>If you are using AIO's built-in Reverse Proxy and don't use your own, then the certificate issuing may possibly not work out-of-the-box because Cloudflare might block the attempt. In that case you need to disable the Proxy feature at least temporarily in order to make it work. Note that this isn't an option if you need Cloudflare Tunnel as disabling the proxy would also disable Cloudflare Tunnel which would in turn make your server unreachable for the verification. See <a href=\"https://github.com/nextcloud/all-in-one/discussions/1101\">https://github.com/nextcloud/all-in-one/discussions/1101</a>.</li></ul><h3>How to run Nextcloud behind a Cloudflare Tunnel?</h3><p>Although it does not seems like it is the case but from AIO perspective a Cloudflare Tunnel works like a reverse proxy. So please follow the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/reverse-proxy.md\">reverse proxy documentation</a> where is documented how to make it run behind a Cloudflare Tunnel. However please see the <a href=\"https://github.com/nextcloud/all-in-one#notes-on-cloudflare-proxytunnel\">caveats</a> before proceeding.</p><h3>How to run Nextcloud via Tailscale?</h3><h3>How to get Nextcloud running using the ACME DNS-challenge?</h3><h3>How to run Nextcloud locally? No domain wanted, or wanting intranet access within your LAN.</h3><p>If you do not want to open Nextcloud to the public internet, you may have a look at the following documentation on how to set it up locally: <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/local-instance.md\">local-instance.md</a>, but keep in mind you're still required to have https working properly.</p><h3>Can I use an ip-address for Nextcloud instead of a domain?</h3><p>No and it will not be added. If you only want to run it locally, you may have a look at the following documentation: <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/local-instance.md\">local-instance.md</a>. Recommended is to use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a>.</p><h3>Can I run AIO offline or in an airgapped system?</h3><p>No. This is not possible and will not be added due to multiple reasons: update checks, app installs via app-store, downloading additional docker images on demand and more.</p><h3>Are self-signed certificates supported for Nextcloud?</h3><p>No and they will not be. If you want to run it locally, without opening Nextcloud to the public internet, please have a look at the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/local-instance.md\">local instance documentation</a>. Recommended is to use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a>.</p><h3>Can I use AIO with multiple domains?</h3><p>No and it will not be added. However you can use <a href=\"https://github.com/nextcloud/all-in-one/raw/main/multiple-instances.md\">this feature</a> in order to create multiple AIO instances, one for each domain.</p><h3>Are other ports than the default 443 for Nextcloud supported?</h3><p>No and they will not be. If port 443 and/or 80 is blocked for you, you may use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a> if you want to publish it online. If you already run a different service on port 443, please use a dedicated domain for Nextcloud and set it up correctly by following the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/reverse-proxy.md\">reverse proxy documentation</a>. However in all cases the Nextcloud interface will redirect you to port 443.</p><h3>Can I run Nextcloud in a subdirectory on my domain?</h3><p>No and it will not be added. Please use a dedicated (sub-)domain for Nextcloud and set it up correctly by following the <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/reverse-proxy.md\">reverse proxy documentation</a>. Alternatively, you may use <a href=\"https://github.com/nextcloud/all-in-one/discussions/5439\">Tailscale</a> if you want to publish it online.</p><h3>How can I access Nextcloud locally?</h3><p>Please note that local access is not possible if you are running AIO behind Cloudflare Tunnel since TLS proxying is in that case offloaded to Cloudflares infrastructure. You can fix this by setting up your own reverse proxy that handles TLS proxying locally and will make the steps below work.</p><p>Please make sure that if you are running AIO behind a reverse proxy, that the reverse proxy is configured to use port 443 on the server that runs it. Otherwise the steps below will not work.</p><p>Now that this is out of the way, the recommended way how to access Nextcloud locally, is to set up a local dns-server like a pi-hole and set up a custom dns-record for that domain that points to the internal ip-adddress of your server that runs Nextcloud AIO. Below are some guides:</p><h3>How to skip the domain validation?</h3><p>If you are completely sure that you've configured everything correctly and are not able to pass the domain validation, you may skip the domain validation by adding <code>--env SKIP_DOMAIN_VALIDATION=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used).</p><h3>How to resolve firewall problems with Fedora Linux, RHEL OS, CentOS, SUSE Linux and others?</h3><p>It is known that Linux distros that use <a href=\"https://firewalld.org\">firewalld</a> as their firewall daemon have problems with docker networks. In case the containers are not able to communicate with each other, you may change your firewalld to use the iptables backend by running:</p><pre><code>sudo sed -i 's/FirewallBackend=nftables/FirewallBackend=iptables/g' /etc/firewalld/firewalld.conf\nsudo systemctl restart firewalld docker\n</code></pre><p>Afterwards it should work.</p><h3>What can I do to fix the internal or reserved ip-address error?</h3><p>If you get an error during the domain validation which states that your ip-address is an internal or reserved ip-address, you can fix this by first making sure that your domain indeed has the correct public ip-address that points to the server and then adding <code>--add-host yourdomain.com:&lt;public-ip-address&gt;</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) which will allow the domain validation to work correctly. And so that you know: even if the  record of your domain should change over time, this is no problem since the mastercontainer will not make any attempt to access the chosen domain after the initial domain validation.</p><h3>Which CPU architectures are supported?</h3><p>You can check this on Linux by running: </p><ul></ul><h3>Disrecommended VPS providers</h3><ul><li> Strato VPS using Virtuozzo caused problems though ones from Q3 2023 and later should work. If your VPS has a  file and a low  limit set in it your server will likely misbehave once it reaches this limit which is very quickly reached by AIO, see <a href=\"https://github.com/nextcloud/all-in-one/discussions/1747#discussioncomment-4716164\">here</a>.</li><li>Hostingers VPS seem to miss a specific Kernel feature which is required for AIO to run correctly. See <a href=\"https://help.nextcloud.com/t/help-installing-nc-via-aio-on-vps/153956\">here</a>.</li></ul><p>In general recommended VPS are those that are KVM/non-virtualized as Docker should work best on them.</p><ul><li>SD-cards are disrecommended for AIO since they cripple the performance and they are not meant for many write operations which is needed for the database and other parts</li><li>SSD storage is recommended</li><li>HDD storage should work as well but is of course much slower than SSD storage</li></ul><h3>Are there known problems when SELinux is enabled?</h3><p>Yes. If SELinux is enabled, you might need to add the <code>--security-opt label:disable</code> option to the docker run command of the mastercontainer in order to allow it to access the docker socket (or <code>security_opt: [\"label:disable\"]</code> in compose.yaml). See <a href=\"https://github.com/nextcloud/all-in-one/discussions/485\">https://github.com/nextcloud/all-in-one/discussions/485</a></p><h3>How to change the default location of Nextcloud's Datadir?</h3><blockquote><p>[!WARNING] Do not set or adjust this value after the initial Nextcloud installation is done! If you still want to do it afterwards, see <a href=\"https://github.com/nextcloud/all-in-one/discussions/890#discussioncomment-3089903\">this</a> on how to do it.</p></blockquote><p>You can configure the Nextcloud container to use a specific directory on your host as data directory. You can do so by adding the environmental variable  to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used). Allowed values for that variable are strings that start with  and are not equal to . The chosen directory or volume will then be mounted to  inside the container.</p><ul><li>An example for Linux is <code>--env NEXTCLOUD_DATADIR=\"/mnt/ncdata\"</code>. ‚ö†Ô∏è Please note: If you should be using an external BTRFS drive that is mounted to , make sure to choose a subfolder like e.g.  as datadir, since the root folder is not suited as datadir in that case. See <a href=\"https://github.com/nextcloud/all-in-one/discussions/2696\">https://github.com/nextcloud/all-in-one/discussions/2696</a>.</li><li>On macOS it might be <code>--env NEXTCLOUD_DATADIR=\"/var/nextcloud-data\"</code></li><li>For Synology it may be <code>--env NEXTCLOUD_DATADIR=\"/volume1/docker/nextcloud/data\"</code>.</li><li>On Windows it might be <code>--env NEXTCLOUD_DATADIR=\"/run/desktop/mnt/host/c/ncdata\"</code>. (This path is equivalent to  on your Windows host so you need to translate the path accordingly. Hint: the path that you enter needs to start with . Append to that the exact location on your windows host, e.g.  which is equivalent to .) ‚ö†Ô∏è : This does not work with external drives like USB or network drives and only with internal drives like SATA or NVME drives.</li><li>Another option is to provide a specific volume name here with: <code>--env NEXTCLOUD_DATADIR=\"nextcloud_aio_nextcloud_datadir\"</code>. This volume needs to be created beforehand manually by you in order to be able to use it. e.g. on Windows with: <pre><code>docker volume create ^\n--driver local ^\n--name nextcloud_aio_nextcloud_datadir ^\n-o device=\"/host_mnt/e/your/data/path\" ^\n-o type=\"none\" ^\n-o o=\"bind\"\n</code></pre> In this example, it would mount  into the volume so for a different location you need to adjust <code>/host_mnt/e/your/data/path</code> accordingly.</li></ul><h3>How to store the files/installation on a separate drive?</h3><p>‚ö†Ô∏è If you encounter errors from richdocuments in your Nextcloud logs, check in your Collabora container if the message \"Capabilities are not set for the coolforkit program.\" appears. If so, follow these steps:</p><ol><li>Stop all the containers from the AIO Interface.</li><li>Go to your terminal and delete the Collabora container (<code>docker rm nextcloud-aio-collabora</code>) AND the Collabora image (<code>docker image rm nextcloud/aio-collabora</code>).</li><li>You might also want to prune your Docker () (no data will be lost).</li><li>Restart your containers from the AIO Interface.</li></ol><p>This should solve the problem.</p><h3>How to allow the Nextcloud container to access directories on the host?</h3><p>By default, the Nextcloud container is confined and cannot access directories on the host OS. You might want to change this when you are planning to use local external storage in Nextcloud to store some files outside the data directory and can do so by adding the environmental variable  to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used). Allowed values for that variable are strings that start with  and are not equal to .</p><ul><li>Two examples for Linux are <code>--env NEXTCLOUD_MOUNT=\"/mnt/\"</code> and <code>--env NEXTCLOUD_MOUNT=\"/media/\"</code>.</li><li>On macOS it might be <code>--env NEXTCLOUD_MOUNT=\"/Volumes/your_drive/\"</code></li><li>For Synology it may be <code>--env NEXTCLOUD_MOUNT=\"/volume1/\"</code>.</li><li>On Windows it might be <code>--env NEXTCLOUD_MOUNT=\"/run/desktop/mnt/host/d/your-folder/\"</code>. (This path is equivalent to  on your Windows host so you need to translate the path accordingly. Hint: the path that you enter needs to start with . Append to that the exact location on your windows host, e.g.  which is equivalent to .) ‚ö†Ô∏è : This does not work with external drives like USB or network drives and only with internal drives like SATA or NVME drives.</li></ul><p>After using this option, please make sure to apply the correct permissions to the directories that you want to use in Nextcloud. E.g. <code>sudo chown -R 33:0 /mnt/your-drive-mountpoint</code> and <code>sudo chmod -R 750 /mnt/your-drive-mountpoint</code> should make it work on Linux when you have used <code>--env NEXTCLOUD_MOUNT=\"/mnt/\"</code>. On Windows you could do this e.g. with <code>docker exec -it nextcloud-aio-nextcloud chown -R 33:0 /run/desktop/mnt/host/d/your-folder/</code> and <code>docker exec -it nextcloud-aio-nextcloud chmod -R 750 /run/desktop/mnt/host/d/your-folder/</code>.</p><p>You can then navigate to <code>https://your-nc-domain.com/settings/apps/disabled</code>, activate the external storage app, navigate to <code>https://your-nc-domain.com/settings/admin/externalstorages</code> and add a local external storage directory that will be accessible inside the container at the same place that you've entered. E.g. <code>/mnt/your-drive-mountpoint</code> will be mounted to <code>/mnt/your-drive-mountpoint</code> inside the container, etc.</p><p>Be aware though that these locations will not be covered by the built-in backup solution - but you can add further Docker volumes and host paths that you want to back up after the initial backup is done.</p><blockquote><p>[!NOTE] If you can't see the type \"local storage\" in the external storage admin options, a restart of the containers from the AIO interface may be required.</p></blockquote><h3>How to adjust the Talk port?</h3><p>By default will the talk container use port  and  for connections. This should be set to something higher than 1024! You can adjust the port by adding e.g.  to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and adjusting the port to your desired value. Best is to use a port over 1024, so e.g. 3479 to not run into this: <a href=\"https://github.com/nextcloud/all-in-one/discussions/2517\">https://github.com/nextcloud/all-in-one/discussions/2517</a></p><h3>How to adjust the upload limit for Nextcloud?</h3><p>By default, public uploads to Nextcloud are limited to a max of 16G (logged in users can upload much bigger files using the webinterface or the mobile/desktop clients, since chunking is used in that case). You can adjust the upload limit by providing <code>--env NEXTCLOUD_UPLOAD_LIMIT=16G</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must start with a number and end with  e.g. .</p><h3>How to adjust the max execution time for Nextcloud?</h3><p>By default, uploads to Nextcloud are limited to a max of 3600s. You can adjust the upload time limit by providing <code>--env NEXTCLOUD_MAX_TIME=3600</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a number e.g. .</p><h3>How to adjust the PHP memory limit for Nextcloud?</h3><p>By default, each PHP process in the Nextcloud container is limited to a max of 512 MB. You can adjust the memory limit by providing <code>--env NEXTCLOUD_MEMORY_LIMIT=512M</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must start with a number and end with  e.g. .</p><h3>How to change the Nextcloud apps that are installed on the first startup?</h3><p>You might want to adjust the Nextcloud apps that are installed upon the first startup of the Nextcloud container. You can do so by adding <code>--env NEXTCLOUD_STARTUP_APPS=\"deck twofactor_totp tasks calendar contacts notes\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a string with small letters a-z, 0-9, spaces and hyphens or '_'. You can disable shipped and by default enabled apps by adding a hyphen in front of the appid. E.g. .</p><h3>How to add OS packages permanently to the Nextcloud container?</h3><p>Some Nextcloud apps require additional external dependencies that must be bundled within Nextcloud container in order to work correctly. As we cannot put each and every dependency for all apps into the container - as this would make the project quickly unmaintainable - there is an official way in which you can add additional dependencies into the Nextcloud container. However note that doing this is disrecommended since we do not test Nextcloud apps that require external dependencies.</p><p>You can do so by adding <code>--env NEXTCLOUD_ADDITIONAL_APKS=\"imagemagick dependency2 dependency3\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a string with small letters a-z, digits 0-9, spaces, dots and hyphens or '_'. You can find available packages here: <a href=\"https://pkgs.alpinelinux.org/packages?branch=v3.21\">https://pkgs.alpinelinux.org/packages?branch=v3.21</a>. By default  is added. If you want to keep it, you need to specify it as well.</p><h3>How to add PHP extensions permanently to the Nextcloud container?</h3><p>Some Nextcloud apps require additional php extensions that must be bundled within Nextcloud container in order to work correctly. As we cannot put each and every dependency for all apps into the container - as this would make the project quickly unmaintainable - there is an official way in which you can add additional php extensions into the Nextcloud container. However note that doing this is disrecommended since we do not test Nextcloud apps that require additional php extensions.</p><p>You can do so by adding <code>--env NEXTCLOUD_ADDITIONAL_PHP_EXTENSIONS=\"imagick extension1 extension2\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. It must be a string with small letters a-z, digits 0-9, spaces, dots and hyphens or '_'. You can find available extensions here: <a href=\"https://pecl.php.net/packages.php\">https://pecl.php.net/packages.php</a>. By default  is added. If you want to keep it, you need to specify it as well.</p><h3>What about the pdlib PHP extension for the facerecognition app?</h3><p>The <a href=\"https://apps.nextcloud.com/apps/facerecognition\">facerecognition app</a> requires the pdlib PHP extension to be installed. Unfortunately, it is not available on PECL nor via PHP core, so there is no way to add this into AIO currently. However you can use <a href=\"https://github.com/nextcloud/all-in-one/tree/main/community-containers/facerecognition\">this community container</a> in order to run facerecognition.</p><h3>How to enable hardware acceleration for Nextcloud?</h3><p>Some container can use GPU acceleration to increase performance like <a href=\"https://apps.nextcloud.com/apps/memories\">memories app</a> allows to enable hardware transcoding for videos.</p><h4>With open source drivers MESA for AMD, Intel and  drivers  for Nvidia</h4><blockquote><p>[!WARNING] This only works if the  device is present on the host! If it does not exist on your host, don't proceed as otherwise the Nextcloud container will fail to start! If you are unsure about this, better do not proceed with the instructions below. Make sure that your driver is correctly configured on the host.</p></blockquote><p>In order to use that, you need to add <code>--env NEXTCLOUD_ENABLE_DRI_DEVICE=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) which will mount the  device into the container.</p><h4>With proprietary drivers for Nvidia  BETA</h4><blockquote><p>[!WARNING] This only works if the Nvidia Toolkit is installed on the host and an NVIDIA GPU is enabled! Make sure that it is correctly configured on the host. If it does not exist on your host, don't proceed as otherwise the Nextcloud container will fail to start! If you are unsure about this, better do not proceed with the instructions below.</p><p>This feature is in beta. Since the proprietary, we haven't a lot of user using proprietary drivers, we can't guarantee the stability of this feature. Your feedback is welcome.</p></blockquote><p>In order to use that, you need to add <code>--env NEXTCLOUD_ENABLE_NVIDIA_GPU=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) which will enable the nvidia runtime.</p><h3>How to keep disabled apps?</h3><p>In certain situations you might want to keep Nextcloud apps that are disabled in the AIO interface and not uninstall them if they should be installed in Nextcloud. You can do so by adding <code>--env NEXTCLOUD_KEEP_DISABLED_APPS=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used).</p><blockquote><p>[!WARNING] Doing this might cause unintended problems in Nextcloud if an app that requires an external dependency is still installed but the external dependency not for example.</p></blockquote><h3>How to trust user-defined Certification Authorities (CA)?</h3><blockquote><p>[!NOTE] Please note, that this feature is only intended to make LDAPS connections with self-signed certificates work. It will not make other interconnectivity between the different containers work, as they expect a valid publicly trusted certificate like one from Let's Encrypt.</p></blockquote><p>For some applications it might be necessary to establish a secure connection to another host/server which is using a certificate issued by a Certification Authority that is not trusted out of the box. An example could be configuring LDAPS against a domain controller (Active Directory or Samba-based) of an organization.</p><p>You can make the Nextcloud container trust any Certification Authority by providing the environmental variable <code>NEXTCLOUD_TRUSTED_CACERTS_DIR</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used). The value of the variables should be set to the absolute paths of the directory on the host, which contains one or more Certification Authorities certificates. You should use X.509 certificates, Base64 encoded. (Other formats may work but have not been tested!) All the certificates in the directory will be trusted.</p><p>When using , the environmental variable can be set with <code>--env NEXTCLOUD_TRUSTED_CACERTS_DIR=/path/to/my/cacerts</code>.</p><p>In order for the value to be valid, the path should start with  and not end with  and point to an existing . Pointing the variable directly to a certificate  will not work and may also break things.</p><h3>How to disable Collabora's Seccomp feature?</h3><p>The Collabora container enables Seccomp by default, which is a security feature of the Linux kernel. On systems without this kernel feature enabled, you need to provide <code>--env COLLABORA_SECCOMP_DISABLED=true</code> to the initial docker run command in order to make it work. If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used.</p><h3>How to adjust the Fulltextsearch Java options?</h3><p>The Fulltextsearch Java options are by default set to  which might not be enough on some systems. You can adjust this by adding e.g. <code>--env FULLTEXTSEARCH_JAVA_OPTIONS=\"-Xms1024M -Xmx1024M\"</code> to the initial docker run command. If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used.</p><p>On macOS, there is only one thing different in comparison to Linux: instead of using <code>--volume /var/run/docker.sock:/var/run/docker.sock:ro</code>, you need to use <code>--volume /var/run/docker.sock.raw:/var/run/docker.sock:ro</code> to run it after you installed <a href=\"https://www.docker.com/products/docker-desktop/\">Docker Desktop</a> (and don't forget to <a href=\"https://github.com/nextcloud/all-in-one/raw/main/docker-ipv6-support.md\">enable ipv6</a> if you should need that). Apart from that it should work and behave the same like on Linux.</p><p>Also, you may be interested in adjusting Nextcloud's Datadir to store the files on the host system. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p><h3>How to run AIO on Windows?</h3><p>On Windows, install <a href=\"https://www.docker.com/products/docker-desktop/\">Docker Desktop</a> (and don't forget to <a href=\"https://github.com/nextcloud/all-in-one/raw/main/docker-ipv6-support.md\">enable ipv6</a> if you should need that) and run the following command in the command prompt:</p><pre><code>docker run ^\n--init ^\n--sig-proxy=false ^\n--name nextcloud-aio-mastercontainer ^\n--restart always ^\n--publish 80:80 ^\n--publish 8080:8080 ^\n--publish 8443:8443 ^\n--volume nextcloud_aio_mastercontainer:/mnt/docker-aio-config ^\n--volume //var/run/docker.sock:/var/run/docker.sock:ro ^\nghcr.io/nextcloud-releases/all-in-one:latest\n</code></pre><p>Also, you may be interested in adjusting Nextcloud's Datadir to store the files on the host system. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p><blockquote><p>[!NOTE] Almost all commands in this project's documentation use . Since  is not available on Windows, you simply remove  from the commands and they should work.</p></blockquote><h3>How to run AIO on Synology DSM</h3><p>On Synology, there are two things different in comparison to Linux: instead of using <code>--volume /var/run/docker.sock:/var/run/docker.sock:ro</code>, you need to use <code>--volume /volume1/docker/docker.sock:/var/run/docker.sock:ro</code> to run it. You also need to add <code>--env WATCHTOWER_DOCKER_SOCKET_PATH=\"/volume1/docker/docker.sock\"</code>to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>). Apart from that it should work and behave the same like on Linux. Obviously the Synology Docker GUI will not work with that so you will need to either use SSH or create a user-defined script task in the task scheduler as the user 'root' in order to run the command.</p><blockquote><p>[!NOTE] It is possible that the docker socket on your Synology is located in  like the default on Linux. Then you can just use the Linux command without having to change anything - you will notice this when you try to start the container and it says that the bind mount failed. E.g. <code>docker: Error response from daemon: Bind mount failed: '/volume1/docker/docker.sock' does not exists.</code></p></blockquote><p>Also, you may be interested in adjusting Nextcloud's Datadir to store the files on the host system. See <a href=\"https://github.com/nextcloud/all-in-one#how-to-change-the-default-location-of-nextclouds-datadir\">this documentation</a> on how to do it.</p><p>You'll also need to adjust Synology's firewall, see below:</p><h3>How to run AIO with Portainer?</h3><p>The easiest way to run it with Portainer on Linux is to use Portainer's stacks feature and use <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/compose.yaml\">this docker-compose file</a> in order to start AIO correctly.</p><h3>Can I run AIO on TrueNAS SCALE?</h3><p>With the Truenas Scale Release 24.10.0 (which was officially released on October 29th 2024 as a stable release) IX Systems ditched the Kubernetes integration and implemented a fully working docker environment.</p><p>On older TrueNAS SCALE releases with Kubernetes environment, there are two ways to run AIO. The preferred one is to run AIO inside a VM. This is necessary since they do not expose the docker socket for containers on the host, you also cannot use docker-compose on it thus and it is also not possible to run custom helm-charts that are not explicitly written for TrueNAS SCALE.</p><p>Simply run the following: <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ your-command</code>. Of course  needs to be exchanged with the command that you want to run.</p><h3>How to resolve <code>Security &amp; setup warnings displays the \"missing default phone region\" after initial install</code>?</h3><h3>How to run multiple AIO instances on one server?</h3><h3>Bruteforce protection FAQ</h3><p>Nextcloud features a built-in bruteforce protection which may get triggered and will block an ip-address or disable a user. You can unblock an ip-address by running <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ security:bruteforce:reset &lt;ip-address&gt;</code> and enable a disabled user by running <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ user:enable &lt;name of user&gt;</code>. See <a href=\"https://docs.nextcloud.com/server/latest/admin_manual/configuration_server/occ_command.html#security\">https://docs.nextcloud.com/server/latest/admin_manual/configuration_server/occ_command.html#security</a> for further information.</p><h3>How to switch the channel?</h3><p>You can switch to a different channel like e.g. the beta channel or from the beta channel back to the latest channel by stopping the mastercontainer, removing it (no data will be lost) and recreating the container using the same command that you used initially to create the mastercontainer. You simply need to change the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code> to <code>ghcr.io/nextcloud-releases/all-in-one:beta</code> and vice versa.</p><h3>How to update the containers?</h3><p>If we push new containers to , you will see in the AIO interface below the  section that new container updates were found. In this case, just press  and <code>Start and update containers</code> in order to update the containers. The mastercontainer has its own update procedure though. See below. And don't forget to back up the current state of your instance using the built-in backup solution before starting the containers again! Otherwise you won't be able to restore your instance easily if something should break during the update.</p><p>If a new  update was found, you'll see a note below the  button that allows to show the changelog. If you click that button and the containers are stopped, you will see a new button that allows to update the mastercontainer. After doing so and after the update is gone through, you will have the option again to <code>Start and update containers</code>. It is recommended to create a backup before clicking the <code>Start and update containers</code> button.</p><p>Additionally, there is a cronjob that runs once a day that checks for container and mastercontainer updates and sends a notification to all Nextcloud admins if a new update was found.</p><h3>How to easily log in to the AIO interface?</h3><p>If your Nextcloud is running and you are logged in as admin in your Nextcloud, you can easily log in to the AIO interface by opening <code>https://yourdomain.tld/settings/admin/overview</code> which will show a button on top that enables you to log in to the AIO interface by just clicking on this button.</p><blockquote><p>[!Note] You can change the domain/ip-address/port of the button by simply stopping the containers, visiting the AIO interface from the correct and desired domain/ip-address/port and clicking once on .</p></blockquote><h3>How to change the domain?</h3><blockquote><p>[!NOTE] Editing the configuration.json manually and making a mistake may break your instance so please create a backup first!</p></blockquote><p>If you set up a new AIO instance, you need to enter a domain. Currently there is no way to change this domain afterwards from the AIO interface. So in order to change it, you need to edit the configuration.json manually using <code>sudo docker run -it --rm --volume nextcloud_aio_mastercontainer:/mnt/docker-aio-config:rw alpine sh -c \"apk add --no-cache nano &amp;&amp; nano /mnt/docker-aio-config/data/configuration.json\"</code>, substitute each occurrence of your old domain with your new domain and save and write out the file. Afterwards restart your containers from the AIO interface and everything should work as expected if the new domain is correctly configured. If you are running AIO behind a web server or reverse proxy (like Apache, Nginx, Caddy, Cloudflare Tunnel and else), you need to obviously also change the domain in your reverse proxy config.</p><p>Additionally, after restarting the containers, you need to open the admin settings and update some values manually that cannot be changed automatically. Here is a list of some known places:</p><ul><li><code>https://your-nc-domain.com/settings/admin/talk</code> for Turn/Stun server and Signaling Server if you enabled Talk via the AIO interface</li><li><code>https://your-nc-domain.com/settings/admin/theming</code> for the theming URL</li><li><code>https://your-nc-domain.com/settings/admin/app_api</code> for the deploy daemon if you enabled the App API via the AIO interface</li></ul><h3>How to properly reset the instance?</h3><p>If something goes unexpected routes during the initial installation, you might want to reset the AIO installation to be able to start from scratch.</p><blockquote><p>[!NOTE] If you already have it running and have data on your instance, you should not follow these instructions as it will delete all data that is coupled to your AIO instance.</p></blockquote><p>Here is how to reset the AIO instance properly:</p><ol><li>Stop all containers if they are running from the AIO interface</li><li>Stop the mastercontainer with <code>sudo docker stop nextcloud-aio-mastercontainer</code></li><li>If the domaincheck container is still running, stop it with <code>sudo docker stop nextcloud-aio-domaincheck</code></li><li>Check that no AIO containers are running anymore by running <code>sudo docker ps --format {{.Names}}</code>. If no  containers are listed, you can proceed with the steps below. If there should be some, you will need to stop them with <code>sudo docker stop &lt;container_name&gt;</code> until no one is listed anymore.</li><li>Check which containers are stopped: <code>sudo docker ps --filter \"status=exited\"</code></li><li>Now remove all these stopped containers with <code>sudo docker container prune</code></li><li>Delete the docker network with <code>sudo docker network rm nextcloud-aio</code></li><li>Check which volumes are dangling with <code>sudo docker volume ls --filter \"dangling=true\"</code></li><li>Now remove all these dangling volumes: <code>sudo docker volume prune --filter all=1</code> (on Windows you might need to remove some volumes afterwards manually with <code>docker volume rm nextcloud_aio_backupdir</code>, <code>docker volume rm nextcloud_aio_nextcloud_datadir</code>).</li><li>If you've configured  to a path on your host instead of the default volume, you need to clean that up as well. (E.g. by simply deleting the directory).</li><li>Make sure that no volumes are remaining with <code>sudo docker volume ls --format {{.Name}}</code>. If no  volumes are listed, you can proceed with the steps below. If there should be some, you will need to remove them with <code>sudo docker volume rm &lt;volume_name&gt;</code> until no one is listed anymore.</li><li>Optional: You can remove all docker images with <code>sudo docker image prune -a</code>.</li><li>And you are done! Now feel free to start over with the recommended docker run command!</li></ol><h3>Can I use a CIFS/SMB share as Nextcloud's datadir?</h3><p>Sure. Add this to the  file on the host system: <code>&lt;your-storage-host-and-subpath&gt; &lt;your-mount-dir&gt; cifs rw,mfsymlinks,seal,credentials=&lt;your-credentials-file&gt;,uid=33,gid=0,file_mode=0770,dir_mode=0770 0 0</code> (Of course you need to modify <code>&lt;your-storage-host-and-subpath&gt;</code>,  and  for your specific case.)</p><p>One example could look like this:<code>//your-storage-host/subpath /mnt/storagebox cifs rw,mfsymlinks,seal,credentials=/etc/storage-credentials,uid=33,gid=0,file_mode=0770,dir_mode=0770 0 0</code> and add into :</p><pre><code>username=&lt;smb/cifs username&gt;\npassword=&lt;password&gt;\n</code></pre><p>(Of course you need to modify  and  for your specific case.)</p><p>Now you can use  as Nextcloud's datadir like described in the section above this one.</p><h3>Can I run this with Docker swarm?</h3><h3>Can I run this with Kubernetes?</h3><h3>How to run this with Docker rootless?</h3><p>You can run AIO also with docker rootless. How to do this is documented here: <a href=\"https://github.com/nextcloud/all-in-one/raw/main/docker-rootless.md\">docker-rootless.md</a></p><h3>Can I run this with Podman instead of Docker?</h3><h3>Access/Edit Nextcloud files/folders manually</h3><p>The files and folders that you add to Nextcloud are by default stored in the following docker directory: <code>nextcloud_aio_nextcloud:/mnt/ncdata/</code> (usually <code>/var/lib/docker/volumes/nextcloud_aio_nextcloud_data/_data/</code> on linux host systems). If needed, you can modify/add/delete files/folders there but : be very careful when doing so because you might corrupt your AIO installation! Best is to create a backup using the built-in backup solution before editing/changing files/folders in there because you will then be able to restore your instance to the backed up state.</p><p>After you are done modifying/adding/deleting files/folders, don't forget to apply the correct permissions by running: <code>sudo docker exec nextcloud-aio-nextcloud chown -R 33:0 /mnt/ncdata/</code> and <code>sudo docker exec nextcloud-aio-nextcloud chmod -R 750 /mnt/ncdata/</code> and rescan the files with <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ files:scan --all</code>.</p><h3>How to edit Nextclouds config.php file with a texteditor?</h3><p>You can edit Nextclouds config.php file directly from the host with your favorite text editor. E.g. like this: <code>sudo docker run -it --rm --volume nextcloud_aio_nextcloud:/var/www/html:rw alpine sh -c \"apk add --no-cache nano &amp;&amp; nano /var/www/html/config/config.php\"</code>. Make sure to not break the file though which might corrupt your Nextcloud instance otherwise. In best case, create a backup using the built-in backup solution before editing the file.</p><h3>How to change default files by creating a custom skeleton directory?</h3><p>All users see a set of <a href=\"https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/default_files_configuration.html\">default files and folders</a> as dictated by Nextcloud's configuration. To change these default files and folders a custom skeleton directory must first be created; this can be accomplished by copying your skeleton files <code>sudo docker cp --follow-link /path/to/nextcloud/skeleton/ nextcloud-aio-nextcloud:/mnt/ncdata/skeleton/</code>, applying the correct permissions with <code>sudo docker exec nextcloud-aio-nextcloud chown -R 33:0 /mnt/ncdata/skeleton/</code> and <code>sudo docker exec nextcloud-aio-nextcloud chmod -R 750 /mnt/ncdata/skeleton/</code> and setting the skeleton directory option with <code>sudo docker exec --user www-data -it nextcloud-aio-nextcloud php occ config:system:set skeletondirectory --value=\"/mnt/ncdata/skeleton\"</code>. Further information is available in the Nextcloud documentation on <a href=\"https://docs.nextcloud.com/server/stable/admin_manual/configuration_server/config_sample_php_parameters.html#skeletondirectory\">configuration parameters for the skeleton directory</a>.</p><h3>How to adjust the version retention policy and trashbin retention policy?</h3><h3>How to enable automatic updates without creating a backup beforehand?</h3><p>If you have an external backup solution, you might want to enable automatic updates without creating a backup first. However note that doing this is disrecommended since you will not be able to easily create and restore a backup from the AIO interface anymore and you need to make sure to shut down all the containers properly before creating the backup, e.g. by stopping them from the AIO interface first.</p><p>But anyhow, is here a guide that helps you automate the whole procedure:</p><p>You can simply copy and paste the script into a file e.g. named  e.g. here: .</p><p>Afterwards apply the correct permissions with <code>sudo chown root:root /root/shutdown-script.sh</code> and <code>sudo chmod 700 /root/shutdown-script.sh</code>. Then you can create a cronjob that runs it on a schedule e.g. runs the script at  each day like this:</p><ol><li>Open the cronjob with  (and choose your editor of choice if not already done. I'd recommend nano).</li><li>Add the following new line to the crontab if not already present: <code>0 4 * * * /root/shutdown-script.sh</code> which will run the script at 04:00 each day.</li><li>save and close the crontab (when using nano the shortcuts for this are  and then  to save, and close the editor with ).</li></ol><p><strong>After that is in place, you should schedule a backup from your backup solution that creates a backup after AIO is shut down properly. Hint: If your backup runs on the same host, make sure to at least back up all docker volumes and additionally Nextcloud's datadir if it is not stored in a docker volume.</strong></p><p><strong>Afterwards, you can create a second script that automatically updates the containers:</strong></p><p>You can simply copy and paste the script into a file e.g. named  e.g. here: <code>/root/automatic-updates.sh</code>.</p><p>Afterwards apply the correct permissions with <code>sudo chown root:root /root/automatic-updates.sh</code> and <code>sudo chmod 700 /root/automatic-updates.sh</code>. Then you can create a cronjob that runs e.g. at  each day like this:</p><ol><li>Open the cronjob with  (and choose your editor of choice if not already done. I'd recommend nano).</li><li>Add the following new line to the crontab if not already present: <code>0 5 * * * /root/automatic-updates.sh</code> which will run the script at 05:00 each day.</li><li>save and close the crontab (when using nano the shortcuts for this are  then  to save, and close the editor with ).</li></ol><h3>Securing the AIO interface from unauthorized ACME challenges</h3><p><a href=\"https://github.com/nextcloud/all-in-one/discussions/4882#discussioncomment-9858384\">By design</a>, Caddy that runs inside the mastercontainer, which handles automatic TLS certificate generation for the AIO interface on port 8443, is configured to accept traffic on any valid domain in order to make the AIO interface as convenient to use as possible. However due to this, it is vulnerable to receiving DNS challenges for arbitrary hostnames from anyone on the internet. While this does not compromise your server's security, it can result in cluttered logs and rejected certificate renewal attempts due to rate limit abuse. To mitigate this issue, it is recommended to place the AIO interface behind a VPN and/or limit its public exposure.</p><h3>How to migrate from an already existing Nextcloud installation to Nextcloud AIO?</h3><p>Nextcloud AIO provides a backup solution based on <a href=\"https://github.com/borgbackup/borg#what-is-borgbackup\">BorgBackup</a>. These backups act as a restore point in case the installation gets corrupted. By using this tool, backups are incremental, differential, compressed and encrypted ‚Äì so only the first backup will take a while. Further backups should be fast as only changes are taken into account.</p><p>It is recommended to create a backup before any container update. By doing this, you will be safe regarding any possible complication during updates because you will be able to restore the whole instance with basically one click.</p><p>For local backups, the restore process should be pretty fast as rsync is used to restore the chosen backup which only transfers changed files and deletes additional ones. For remote borg backups, the whole backup archive is extracted from the remote, which depending on how clever  is, may require downloading the whole archive.</p><p>If you connect an external drive to your host, and choose the backup directory to be on that drive, you are also kind of safe against drive failures of the drive where the docker volumes are stored on.</p><p>If you want to back up directly to a remote borg repository:</p><p>Backups can be created and restored in the AIO interface using the buttons  and . Additionally, a backup check is provided that checks the integrity of your backups but it shouldn't be needed in most situations.</p><p>The backups themselves get encrypted with an encryption key that gets shown to you in the AIO interface. Please save that at a safe place as you will not be able to restore from backup without this key.</p><p>Daily backups can get enabled after the initial backup is done. Enabling this also allows to enable an option that allows to automatically update all containers, Nextcloud and its apps.</p><p>Be aware that this solution does not back up files and folders that are mounted into Nextcloud using the external storage app - but you can add further Docker volumes and host paths that you want to back up after the initial backup is done.</p><h3>What is getting backed up by AIO's backup solution?</h3><p>Backed up will get all important data of your Nextcloud AIO instance required to restore the instance, like the database, your files and configuration files of the mastercontainer and else. Files and folders that are mounted into Nextcloud using the external storage app are not getting backed up. There is currently no way to exclude the data directory because it would require hacks like running files:scan and would make the backup solution much more unreliable (since the database and your files/folders need to stay in sync). If you still don't want your datadirectory to be backed up, see <a href=\"https://github.com/nextcloud/all-in-one#how-to-enable-automatic-updates-without-creating-a-backup-beforehand\">https://github.com/nextcloud/all-in-one#how-to-enable-automatic-updates-without-creating-a-backup-beforehand</a> for options (there is a hint what needs to be backed up in which order).</p><h3>How to adjust borgs retention policy?</h3><p>The built-in borg-based backup solution has by default a retention policy of <code>--keep-within=7d --keep-weekly=4 --keep-monthly=6</code>. See <a href=\"https://borgbackup.readthedocs.io/en/stable/usage/prune.html\">https://borgbackup.readthedocs.io/en/stable/usage/prune.html</a> for what these values mean. You can adjust the retention policy by providing <code>--env BORG_RETENTION_POLICY=\"--keep-within=7d --keep-weekly=4 --keep-monthly=6\"</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used) and customize the value to your fitting. ‚ö†Ô∏è Please make sure that this value is valid, otherwise backup pruning will bug out!</p><h3>How to migrate from AIO to AIO?</h3><p>If you have the borg backup feature enabled, you can copy it over to the new host and restore from the backup. This guide assumes the new installation data dir will be on , you can adjust the steps if it's elsewhere.</p><ol><li>Set the DNS entry to 60 seconds TTL if applicable</li><li>On your current installation, use the AIO interface to: \n  <ol><li>Update AIO and all containers</li><li>Stop all containers (from now on, your cloud is down)</li><li>Create a current borg backup</li><li>Note the path where the backups are stored and the encryption password</li></ol></li><li>Navigate to the backup folder</li><li>Create archive of the backup so it's easier to copy: <code>tar -czvf borg.tar.gz borg</code></li><li>Copy the archive over to the new host: <code>scp borg.tar.gz user@new.host:/mnt</code>. Make sure to replace  with your actual user and  with the IP or domain of the actual host. You can also use another way to copy the archive.</li><li>Go to the folder you put the backup archive and extract it with </li><li>Follow the installation guide to create a new aio instance, but do not start the containers yet (the  or  command)</li><li>Change the DNS entry to the new host's IP</li><li>Configure your reverse proxy if you use one</li><li>Start the AIO container and open the new AIO interface in your browser</li><li>Make sure to save the newly generated passphrase and enter it in the next step</li><li>Select the \"Restore former AIO instance from backup\" option and enter the encryption password from the old backup and the path in which the extracted  folder lies in (without the borg part) and hit <code>Submit location and password</code></li><li>Choose the latest backup in the dropdown and hit </li><li>Wait until the backup is restored</li><li>Start the containers in the AIO interface</li></ol><h3>Are remote borg backups supported?</h3><p>Backing up directly to a remote borg repository is supported. This avoids having to store a local copy of your backups, supports append-only borg keys to counter ransomware and allows using the AIO interface to manage your backups.</p><p>Some alternatives, which do not have all the above benefits:</p><h3>Failure of the backup container in LXC containers</h3><p>If you are running AIO in a LXC container, you need to make sure that FUSE is enabled in the LXC container settings. Also, if using Alpine Linux as host OS, make sure to add fuse via . Otherwise the backup container will not be able to start as FUSE is required for it to work.</p><h3>How to create the backup volume on Windows?</h3><p>As stated in the AIO interface, it is possible to use a docker volume as backup target. Before you can use that, you need to create it first. Here is an example how to create one on Windows:</p><pre><code>docker volume create ^\n--driver local ^\n--name nextcloud_aio_backupdir ^\n-o device=\"/host_mnt/e/your/backup/path\" ^\n-o type=\"none\" ^\n-o o=\"bind\"\n</code></pre><p>In this example, it would mount  into the volume so for a different location you need to adjust <code>/host_mnt/e/your/backup/path</code> accordingly. Afterwards enter  in the AIO interface as backup location.</p><h3>Pro-tip: Backup archives access</h3><p>You can open the BorgBackup archives on your host by following these steps: (instructions for Ubuntu Desktop)</p><pre><code># Install borgbackup on the host\nsudo apt update &amp;&amp; sudo apt install borgbackup\n\n# In any shell where you use borg, you must first export this variable\n# If you are using the default backup location /mnt/backup/borg\nexport BORG_REPO='/mnt/backup/borg'\n# or if you are using a remote repository\nexport BORG_REPO='user@host:/path/to/repo'\n\n# Mount the archives to /tmp/borg\nsudo mkdir -p /tmp/borg &amp;&amp; sudo borg mount \"$BORG_REPO\" /tmp/borg\n\n# After entering your repository key successfully, you should be able to access all archives in /tmp/borg\n# You can now do whatever you want by syncing them to a different place using rsync or doing other things\n# E.g. you can open the file manager on that location by running:\nxhost +si:localuser:root &amp;&amp; sudo nautilus /tmp/borg\n\n# When you are done, simply close the file manager and run the following command to unmount the backup archives:\nsudo umount /tmp/borg\n</code></pre><h3>Delete backup archives manually</h3><p>You can delete BorgBackup archives on your host manually by following these steps: (instructions for Debian based OS' like Ubuntu)</p><pre><code># Install borgbackup on the host\nsudo apt update &amp;&amp; sudo apt install borgbackup\n\n# In any shell where you use borg, you must first export this variable\n# If you are using the default backup location /mnt/backup/borg\nexport BORG_REPO='/mnt/backup/borg'\n# or if you are using a remote repository\nexport BORG_REPO='user@host:/path/to/repo'\n\n# List all archives (if you are using the default backup location /mnt/backup/borg)\nsudo borg list\n\n# After entering your repository key successfully, you should now see a list of all backup archives\n# An example backup archive might be called 20220223_174237-nextcloud-aio\n# Then you can simply delete the archive with:\nsudo borg delete --stats --progress \"::20220223_174237-nextcloud-aio\"\n\n# If borg 1.2.0 or higher is installed, you then need to run borg compact in order to clean up the freed space\nsudo borg --version\n# If version number of the command above is higher than 1.2.0 you need to run the command below:\nsudo borg compact\n\n</code></pre><p>After doing so, make sure to update the backup archives list in the AIO interface! You can do so by clicking on the  button or  button.</p><h3>Sync local backups regularly to another drive</h3><p>For increased backup security, you might consider syncing the local backup repository regularly to another drive.</p><p>To do that, first add the drive to  so that it is able to get automatically mounted and then create a script that does all the things automatically. Here is an example for such a script:</p><p>You can simply copy and paste the script into a file e.g. named  e.g. here: . Do not forget to modify the variables to your requirements!</p><p>Afterwards apply the correct permissions with <code>sudo chown root:root /root/backup-script.sh</code> and <code>sudo chmod 700 /root/backup-script.sh</code>. Then you can create a cronjob that runs e.g. at  each week on Sundays like this:</p><ol><li>Open the cronjob with  (and choose your editor of choice if not already done. I'd recommend nano).</li><li>Add the following new line to the crontab if not already present: <code>0 20 * * 7 /root/backup-script.sh</code> which will run the script at 20:00 on Sundays each week.</li><li>save and close the crontab (when using nano are the shortcuts for this  -&gt;  and close the editor with ).</li></ol><h3>How to exclude Nextcloud's data directory or the preview folder from backup?</h3><p>In order to speed up the backups and to keep the backup archives small, you might want to exclude Nextcloud's data directory or its preview folder from backup.</p><blockquote><p>[!WARNING] However please note that you will run into problems if the database and the data directory or preview folder get out of sync. <strong>So please only read further, if you have an additional external backup of the data directory!</strong> See <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-enable-automatic-updates-without-creating-a-backup-beforehand\">this guide</a> for example.</p></blockquote><blockquote><p>[!TIP] A better option is to use the external storage app inside Nextcloud as the data connected via the external storage app is not backed up by AIO's backup solution. See <a href=\"https://docs.nextcloud.com/server/latest/admin_manual/configuration_files/external_storage_configuration_gui.html\">this documentation</a> on how to configure the app.</p></blockquote><p>If you still want to proceed, you can exclude the data directory by simply creating a  file in the root directory of the specified  target. The same logic is implemented for the preview folder that is located inside the data directory, inside the  folder. So simply create a  file in there if you want to exclude the preview folder.</p><p>After doing a restore via the AIO interface, you might run into problems due to the data directory and database being out of sync. You might be able to fix this by running  and  and . See <a href=\"https://github.com/nextcloud/all-in-one#how-to-run-occ-commands\">https://github.com/nextcloud/all-in-one#how-to-run-occ-commands</a>. If only the preview folder is excluded, the command <code>occ files:scan-app-data preview</code> should be used.</p><h3>How to stop/start/update containers or trigger the daily backup from a script externally?</h3><blockquote><p>[!WARNING] The below script will only work after the initial setup of AIO. So you will always need to first visit the AIO interface, type in your domain and start the containers the first time or restore an older AIO instance from its borg backup before you can use the script.</p></blockquote><p>You can do so by running the  script that is stored in the mastercontainer. It accepts the following environment variables:</p><ul><li> if set to , it will automatically stop the containers, update them and start them including the mastercontainer. If the mastercontainer gets updated, this script's execution will stop as soon as the mastercontainer gets stopped. You can then wait until it is started again and run the script with this flag again in order to update all containers correctly afterwards.</li><li> if set to , it will automatically stop the containers and create a backup. If you want to start them again afterwards, you may have a look at the  option.</li><li> if set to , it will automatically start the containers without updating them.</li><li> if set to , it will automatically stop the containers.</li><li> if set to , it will start the backup check. This is not allowed to be enabled at the same time like . Please be aware that this option is non-blocking which means that the backup check is not done when the process is finished since it only start the borgbackup container with the correct configuration.</li></ul><p>One example for this would be <code>sudo docker exec -it --env DAILY_BACKUP=1 nextcloud-aio-mastercontainer /daily-backup.sh</code>, which you can run via a cronjob or put it in a script.</p><blockquote><p>[!NOTE] None of the option returns error codes. So you need to check for the correct result yourself.</p></blockquote><h3>How to disable the backup section?</h3><p>If you already have a backup solution in place, you may want to hide the backup section. You can do so by adding <code>--env AIO_DISABLE_BACKUP_SECTION=true</code> to the docker run command of the mastercontainer (but before the last line <code>ghcr.io/nextcloud-releases/all-in-one:latest</code>! If it was started already, you will need to stop the mastercontainer, remove it (no data will be lost) and recreate it using the docker run command that you initially used).</p><p>It is possible to connect to an existing LDAP server. You need to make sure that the LDAP server is reachable from the Nextcloud container. Then you can enable the LDAP app and configure LDAP in Nextcloud manually. If you don't have a LDAP server yet, recommended is to use this docker container: <a href=\"https://hub.docker.com/r/nitnelave/lldap\">https://hub.docker.com/r/nitnelave/lldap</a>. Make sure here as well that Nextcloud can talk to the LDAP server. The easiest way is by adding the LDAP docker container to the docker network . Then you can connect to the LDAP container by its name from the Nextcloud container. There is now a community container which allows to easily add LLDAP to AIO: <a href=\"https://github.com/nextcloud/all-in-one/tree/main/community-containers/lldap\">https://github.com/nextcloud/all-in-one/tree/main/community-containers/lldap</a></p><p>If you want to use the user_sql app, the easiest way is to create an additional database container and add it to the docker network . Then the Nextcloud container should be able to talk to the database container using its name.</p><h3>phpMyAdmin, Adminer or pgAdmin</h3><h3>Requirements for integrating new containers</h3><p>What are the requirements?</p><ol><li>New containers must be related to Nextcloud. Related means that there must be a feature in Nextcloud that gets added by adding this container.</li><li>It must be optionally installable. Disabling and enabling the container from the AIO interface must work and must not produce any unexpected side-effects.</li><li>The feature that gets added into Nextcloud by adding the container must be maintained by the Nextcloud GmbH.</li><li>It must be possible to run the container without big quirks inside docker containers. Big quirks means e.g. needing to change the capabilities or security options.</li><li>The container should not mount directories from the host into the container: only docker volumes should be used.</li><li>The container must be usable by more than 90% of the users (e.g. not too high system requirements and such)</li><li>No additional setup should be needed after adding the container - it should work completely out of the box.</li><li>If the container requires being exposed, only subfolders are supported. So the container should not require its own (sub-)domain and must be able to run in a subfolder.</li></ol><p>This project values stability over new features. That means that when a new major Nextcloud update gets introduced, we will wait at least until the first patch release, e.g.  is out before upgrading to it. Also we will wait with the upgrade until all important apps are compatible with the new major version. Minor or patch releases for Nextcloud and all dependencies as well as all containers will be updated to new versions as soon as possible but we try to give all updates first a good test round before pushing them. That means that it can take around 2 weeks before new updates reach the  channel. If you want to help testing, you can switch to the  channel by following <a href=\"https://raw.githubusercontent.com/nextcloud/all-in-one/main/#how-to-switch-the-channel\">this documentation</a> which will also give you the updates earlier.</p><h3>How often are update notifications sent?</h3><p>AIO ships its own update notifications implementation. It checks if container updates are available. If so, it sends a notification with the title <code>Container updates available!</code> on saturdays to Nextcloud users that are part of the  group. If the Nextcloud container image should be older than 90 days (~3 months) and thus badly outdated, AIO sends a notification to all Nextcloud users with the title . Thus admins should make sure to update the container images at least once every 3 months in order to make sure that the instance gets all security bugfixes as soon as possible.</p><p>If you should run into issues with huge docker logs, you can adjust the log size by following <a href=\"https://docs.docker.com/config/containers/logging/local/#usage\">https://docs.docker.com/config/containers/logging/local/#usage</a>. However for the included AIO containers, this should usually not be needed because almost all of them have the log level set to warn so they should not produce many logs.</p>","contentLength":65765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"visgl/deck.gl","url":"https://github.com/visgl/deck.gl","date":1751337922,"author":"","guid":177072,"unread":true,"content":"<p>WebGL2 powered visualization framework</p><h5 align=\"center\"> GPU-powered, highly performant large-scale data visualization</h5><p>deck.gl is designed to simplify high-performance, WebGL2/WebGPU based visualization of large data sets. Users can quickly get impressive visual results with minimal effort by composing existing layers, or leverage deck.gl's extensible architecture to address custom needs.</p><p>deck.gl maps  (usually an array of JSON objects) into a stack of visual  - e.g. icons, polygons, texts; and look at them with : e.g. map, first-person, orthographic.</p><p>deck.gl handles a number of challenges out of the box:</p><ul><li>Performant rendering and updating of large data sets</li><li>Interactive event handling such as picking, highlighting and filtering</li><li>Cartographic projections and integration with major basemap providers</li><li>A catalog of proven, well-tested layers</li></ul><p>Deck.gl is designed to be highly customizable. All layers come with flexible APIs to allow programmatic control of each aspect of the rendering. All core classes such are easily extendable by the users to address custom use cases.</p><pre><code>&lt;script src=\"https://unpkg.com/deck.gl@latest/dist.min.js\"&gt;&lt;/script&gt;\n</code></pre><p>Data sources are listed in each example.</p><h4>The deck.gl project is supported by</h4>","contentLength":1195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"vanshb03/Summer2026-Internships","url":"https://github.com/vanshb03/Summer2026-Internships","date":1751337922,"author":"","guid":177073,"unread":true,"content":"<p>Collection of Summer 2026 tech internships!</p><p>The torch has been passed. Use this repo to share and keep track of software, tech, CS, PM, quant internships for . The list is maintained collaboratively by Vansh and <a href=\"https://discord.gg/cscareers\">CSCareers</a>!</p><p> Please note that this repository is exclusively for internships/co-ops in the United States, Canada, or Remote positions </p><p>üôè <strong>Contribute by submitting an <a href=\"https://github.com/vanshb03/Summer2026-Internships/issues/new/choose\">issue</a>! See the contribution guidelines <a href=\"https://raw.githubusercontent.com/vanshb03/Summer2026-Internships/dev/CONTRIBUTING.md\">here</a>!</strong> üôè</p><div align=\"center\"><h3>Want notifications when new internships open? </h3><p> Join the ‚¨áÔ∏è <strong> discord  ‚¨áÔ∏è and get your internship applications in right when they open! <a href=\"https://redirect.cvrve.me/discord\"></a></strong></p><strong><sub><i>Join the Discord to connect with fellow peers and streamline your internship search.</i></sub></strong></div>","contentLength":651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"paperless-ngx/paperless-ngx","url":"https://github.com/paperless-ngx/paperless-ngx","date":1751337922,"author":"","guid":177074,"unread":true,"content":"<p>A community-supported supercharged document management system: scan, index and archive all your documents</p><p>Paperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, .</p><p>Paperless-ngx is the official successor to the original <a href=\"https://github.com/the-paperless-project/paperless\">Paperless</a> &amp; <a href=\"https://github.com/jonaswinkler/paperless-ng\">Paperless-ng</a> projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. <a href=\"https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/dev/#community-support\">Consider joining us!</a></p><p>Thanks to the generous folks at <a href=\"https://m.do.co/c/8d70b916d462\">DigitalOcean</a>, a demo is available at <a href=\"https://demo.paperless-ngx.com\">demo.paperless-ngx.com</a> using login  / . <em>Note: demo content is reset frequently and confidential information should not be uploaded.</em></p><p align=\"right\">This project is supported by:<a href=\"https://m.do.co/c/8d70b916d462\"></a></p><p>The easiest way to deploy paperless is . The files in the <a href=\"https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose\"> directory</a> are configured to pull the image from the GitHub container registry.</p><p>If you'd like to jump right in, you can configure a  environment with our install script:</p><pre><code>bash -c \"$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)\"\n</code></pre><p>More details and step-by-step guides for alternative installation methods can be found in <a href=\"https://docs.paperless-ngx.com/setup/#installation\">the documentation</a>.</p><p>If you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The <a href=\"https://docs.paperless-ngx.com/development/\">documentation</a> has some basic information on how to get started.</p><p>People interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the <a href=\"https://matrix.to/#/%23paperless:matrix.org\">Matrix Room</a>. If you would like to contribute to the project on an ongoing basis there are multiple <a href=\"https://github.com/orgs/paperless-ngx/people\">teams</a> (frontend, ci/cd, etc) that could use your help so please reach out!</p><p>Feature requests can be submitted via <a href=\"https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests\">GitHub Discussions</a>, you can search for existing ideas, add your own and vote for the ones you care about.</p><p>Please see <a href=\"https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects\">the wiki</a> for a user-maintained list of related projects and software that is compatible with Paperless-ngx.</p><blockquote><p>Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. <strong>Paperless-ngx should never be run on an untrusted host</strong> because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk. <strong>The safest way to run Paperless-ngx is on a local server in your own home with backups in place</strong>.</p></blockquote>","contentLength":2398,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"007revad/Synology_HDD_db","url":"https://github.com/007revad/Synology_HDD_db","date":1751337922,"author":"","guid":177075,"unread":true,"content":"<p>Add your HDD, SSD and NVMe drives to your Synology's compatible drive database and a lot more</p><p>Add your SATA or SAS HDDs and SSDs plus SATA and NVMe M.2 drives to your Synology's compatible drive databases, including your Synology M.2 PCIe card and Expansion Unit databases.</p><p>The script works in DSM 7, including DSM 7.2, and DSM 6.</p><p>It also has a restore option to undo all the changes made by the script.</p><ul><li>Gets the Synology NAS model and DSM version (so it knows which db files to edit).</li><li>Gets a list of the HDD, SSD, SAS and NVMe drives installed in your Synology NAS.</li><li>Gets each drive's model number and firmware version.</li><li>Backs up the database files if there is no backup already.</li><li>Checks if each drive is already in the Synology's compatible-drive database.</li><li>Adds any missing drives to the Synology's compatible-drive database.</li><li>Optionally prevents DSM auto updating the drive database.</li><li>Optionally disable DSM's \"support_disk_compatibility\".</li><li>Optionally edits max supported memory to match the amount of memory installed, if installed memory is greater than the current max memory setting. \n  <ul><li>DSM only uses the max memory setting when calculating the reserved RAM area size for SSD caches.</li></ul></li><li>Optionally set write_mostly for your internal HDDs so DSM will normally read from your faster internal SSD(s). \n  <ul><li>It can automatically set DSM to read from your internal SSDs.</li><li>Or you can tell the script which internal drive(s) DSM should read from.</li></ul></li><li>Enables M2D20, M2D18, M2D17 and E10M20-T1 if present on Synology NAS that don't officially support them. \n  </li><li>Checks that M.2 volume support is enabled (on models that have M.2 slots or PCIe slots).</li><li>Enables creating M.2 storage pools and volumes from within Storage Manager in DSM 7.2 and later . \n  <ul><li>Including M.2 drives in PCIe adaptor cards like M2D20, M2D18, M2D17 and E10M20-T1 for DSM 7.2 and above <strong>(schedule the script to run boot)</strong>.</li></ul></li><li>Optionally update IronWolf Health Monitor to v2.5.1 to support recent model IronWolf and IronWolf Pro drives. <strong>(NAS with x86_64 CPUs only)</strong>. \n  <ul><li>Also installs IronWolf Health Management on '22 series and newer models that don't have IronWolf Health Management .</li></ul></li><li>Makes DSM recheck disk compatibility so rebooting is not needed if you don't have M.2 drives (DSM 7 only). \n  <ul><li><strong>If you have M.2 drives you may need to reboot.</strong></li><li>Reminds you that you may need to reboot the Synology after running the script.</li></ul></li><li>Checks if there is a newer version of this script and offers to download it for you. \n  <ul><li>The new version available messages time out so they don't prevent the script running if it is scheduled to run unattended.</li></ul></li></ul><ol><li>Save the download zip file to a folder on the Synology. \n  <ul><li>Do  save the script to a M.2 volume. After a DSM or Storage Manager update the M.2 volume won't be available until after the script has run.</li></ul></li></ol><p>Or via SSH as your regular user:</p><pre><code>cd $HOME\nwget https://github.com/007revad/Synology_HDD_db/archive/refs/heads/main.zip -O syno_hdd_db.zip\n7z x syno_hdd_db.zip\ncd Synology_HDD_db-main &amp;&amp; ls -ali\n</code></pre><p>The following files from the downloaded zip file must be in the same folder:</p><ol><li>dtc or the bin folder containing dtc (only required if you have a E10M20-T1, M2D20 or M2D18 in a NAS that does not support them).</li></ol><p>You would need to re-run the script after a DSM update. If you have DSM set to auto update the best option is to run the script every time the Synology boots, and the best way to do that is to <a href=\"https://raw.githubusercontent.com/007revad/Synology_HDD_db/main/how_to_schedule.md/\">setup a scheduled task</a> to run the the script at boot-up.</p><p> After you first run the script you may need to reboot the Synology to see the effect of the changes.</p><h3>Options when running the script </h3><p>There are optional flags you can use when running the script:</p><pre><code>  -s, --showedits       Show edits made to &lt;model&gt;_host db and db.new file(s)\n  -n, --noupdate        Prevent DSM updating the compatible drive databases\n  -r, --ram             Disable memory compatibility checking (DSM 7.x only)\n                        and sets max memory to the amount of installed memory\n  -f, --force           Force DSM to not check drive compatibility\n                        Do not use this option unless absolutely needed\n  -i, --incompatible    Change incompatible drives to supported\n                        Do not use this option unless absolutely needed\n  -w, --wdda            Disable WD Device Analytics to prevent DSM showing\n                        a false warning for WD drives that are 3 years old\n                          DSM 7.2.1 and later already has WDDA disabled\n  -p, --pcie            Enable creating volumes on M2 in unknown PCIe adaptor\n  -e, --email           Disable colored text in output scheduler emails\n  -S, --ssd=DRIVE       Enable write_mostly on internal HDDs so DSM primarily \n                        reads from internal SSDs or your specified drives\n                          -S automatically sets internal SSDs as DSM preferred\n                          --ssd=DRIVE requires the fast drive(s) as argument,\n                          or restore as the argument to reset drives to default\n                          --ssd=sata1 or --ssd=sata1,sata2 or --ssd=sda etc\n                          --ssd=restore\n      --restore         Undo all changes made by the script (except -S --ssd)\n                        To restore all changes including write_mostly use\n                          --restore --ssd=restore\n      --autoupdate=AGE  Auto update script (useful when script is scheduled)\n                          AGE is how many days old a release must be before\n                          auto-updating. AGE must be a number: 0 or greater\n  -I, --ihm             Update IronWolf Health Management to 2.5.1 to support\n                        recent model IronWolf and IronWolf Pro drives.\n                        For NAS with x86_64 CPUs only.\n                        Also installs IHM on '22 series and newer models (untested)\n  -h, --help            Show this help message\n  -v, --version         Show the script version\n</code></pre><ul><li>The -f or --force option is only needed if for some reason your drives still show as unsupported in storage manager. \n  <ul><li>Only use this option as last resort.</li><li>Using this option will prevent data deduplication from being available, and prevent firmware updates on Synology brand drives.</li></ul></li><li>If you have some Synology drives and want to update their firmware run the script  --noupdate or -n then do the drive database update from Storage Manager and finally run the script again with your preferred options.</li></ul><h3>Scheduling the script in Synology's Task Scheduler</h3><h3>Running the script via SSH</h3><p>You run the script in a shell with sudo -s or as root.</p><pre><code>sudo -s /path-to-script/syno_hdd_db.sh -nr\n</code></pre><p> Replace /path-to-script/ with the actual path to the script on your Synology.</p><p>If you run the script with the --showedits flag it will show you the changes it made to the Synology's compatible-drive database. Obviously this is only useful if you run the script in a shell.</p><pre><code>sudo -s /path-to-script/syno_hdd_db.sh -nr --showedits\n</code></pre><p> Replace /path-to-script/ with the actual path to the script on your Synology.</p><p>If you get a \"No such file or directory\" error check the following:</p><ol><li>Make sure you downloaded the zip or rar file to a folder on your Synology (not on your computer).</li><li>Make sure you unpacked the zip or rar file that you downloaded and are trying to run the syno_hdd_db.sh file.</li><li>If the path to the script contains any spaces you need to enclose the path/scriptname in double quotes: <pre><code>sudo -s \"/volume1/my scripts/syno_hdd_db.sh -n\"\n</code></pre></li><li>Set the script file as executable: <pre><code>sudo chmod +x \"/volume1/scripts/syno_hdd_db.sh\"\n</code></pre></li></ol><p>You only need to edit syno_hdd_vendor_ids.txt if the script warns you about a missing vendor id.</p><p>If DSM doesn't know the brand of your NVMe drives they will show up in Storage Manager as Unknown brand, and Unrecognised firmware version.</p><p>In this case the script will show you the vendor ID and advise you to add it to the syno_hdd_vendor_ids.txt file.</p><p>Ironwolf Health working with the latest version of Ironwolf Health Monitor.</p><ul><li>The idea for this script came from a comment made by Empyrealist on the Synology subreddit.</li><li>Thanks for the assistance from Alex_of_Chaos on the Synology subreddit.</li><li>Thanks to dwabraxus and aferende for help detecting connected expansion units.</li><li>Thanks to bartoque on the Synology subreddit for the tip on making the script download the latest release from GitHub.</li><li>Thanks to nicolerenee for pointing out the easiest way to enable creating M.2 storage pools and volumes in Storage Manager.</li></ul><p>Thank you to the PayPal and Buy Me a Coffee donators, GitHub sponsors and hardware donators</p><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table>","contentLength":8450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"aldinokemal/go-whatsapp-web-multidevice","url":"https://github.com/aldinokemal/go-whatsapp-web-multidevice","date":1751337922,"author":"","guid":177076,"unread":true,"content":"<p>API for Whatsapp Web Multi Device Version, Support UI, Webhook &amp; MCP</p><p><a href=\"https://www.patreon.com/c/aldinokemal\"><img src=\"https://img.shields.io/badge/Support%20on-Patreon-orange.svg?sanitize=true\" alt=\"Patreon\"></a><strong>If you're using this tools to generate income, consider supporting its development by becoming a Patreon member!</strong> Your support helps ensure the library stays maintained and receives regular updates!</p><h2>Support for  &amp;  Architecture along with  Support</h2><ul><li><ul><li>For REST mode, you need to run  instead of <ul><li>for example:  instead of </li></ul></li><li>For MCP mode, you need to run <ul><li>for example: </li></ul></li></ul></li></ul><ul><li><p><strong>MCP (Model Context Protocol) Server Support</strong> - Integrate with AI agents and tools using standardized protocol</p></li><li><ul><li>example: <code>Hello @628974812XXXX, @628974812XXXX</code></li></ul></li><li><p>Compress image before send</p></li><li><p>Compress video before send</p></li><li><p>Change OS name become your app (it's the device name when connect via mobile)</p><ul><li> or </li></ul></li><li><p>Basic Auth (able to add multi credentials)</p><ul><li><code>--basic-auth=kemal:secret,toni:password,userName:secretPassword</code>, or you can simplify</li><li><code>-b=kemal:secret,toni:password,userName:secretPassword</code></li></ul></li><li><p>Customizable port and debug mode</p></li><li><ul><li><code>--autoreply=\"Don't reply this message\"</code></li></ul></li><li><p>Webhook for received message</p><ul><li><code>--webhook=\"http://yourwebhook.site/handler\"</code>, or you can simplify</li><li><code>-w=\"http://yourwebhook.site/handler\"</code></li></ul></li><li><p>Webhook Secret Our webhook will be sent to you with an HMAC header and a sha256 default key .</p><p>You may modify this by using the option below:</p><ul><li><code>--webhook-secret=\"secret\"</code></li></ul></li></ul><p>You can configure the application using either command-line flags (shown above) or environment variables. Configuration can be set in three ways (in order of priority):</p><ol><li>Command-line flags (highest priority)</li><li> file (lowest priority)</li></ol><p>You can configure the application using environment variables. Configuration can be set in three ways (in order of priority):</p><ol><li>Command-line flags (highest priority)</li><li> file (lowest priority)</li></ol><p>To use environment variables:</p><ol><li>Copy  to  in your project root (<code>cp src/.env.example src/.env</code>)</li><li>Modify the values in  according to your needs</li><li>Or set the same variables as system environment variables</li></ol><h4>Available Environment Variables</h4><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr><td>OS name (device name in WhatsApp)</td></tr><tr><td>Basic authentication credentials</td><td><code>APP_BASIC_AUTH=user1:pass1,user2:pass2</code></td></tr><tr><td>Chat flush interval in days</td><td><code>APP_CHAT_FLUSH_INTERVAL=30</code></td></tr><tr><td><code>file:storages/whatsapp.db?_foreign_keys=on</code></td><td><code>DB_URI=postgres://user:pass@host/db</code></td></tr><tr><td><code>WHATSAPP_AUTO_REPLY=\"Auto reply message\"</code></td></tr><tr><td>Webhook URL(s) for events (comma-separated)</td><td><code>WHATSAPP_WEBHOOK=https://webhook.site/xxx</code></td></tr><tr><td>Webhook secret for validation</td><td><code>WHATSAPP_WEBHOOK_SECRET=super-secret-key</code></td></tr><tr><td><code>WHATSAPP_ACCOUNT_VALIDATION</code></td><td>Enable account validation</td><td><code>WHATSAPP_ACCOUNT_VALIDATION=false</code></td></tr><tr><td><code>WHATSAPP_CHAT_STORAGE=false</code></td></tr></tbody></table><p>Note: Command-line flags will override any values set in environment variables or  file.</p><ul><li>For more command </li></ul><ul><li> (for building from source)</li><li> (for media processing)</li></ul><ul><li>macOS (Intel, Apple Silicon)</li><li>Windows (x86_64) - WSL recommended</li></ul><h3>Dependencies (without docker)</h3><ul><li>Mac OS: \n  <ul><li><code>export CGO_CFLAGS_ALLOW=\"-Xpreprocessor\"</code></li></ul></li><li>Linux: \n  <ul></ul></li></ul><ol><li>Clone this repo: <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run  (for REST API mode)</li><li>Open </li></ol><h3>Docker (you don't need to install in required)</h3><ol><li>Clone this repo: <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run <code>docker-compose up -d --build</code></li><li>open </li></ol><ol><li>Clone this repo <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run \n  <ol><li>Linux &amp; MacOS: </li><li>Windows (CMD / PowerShell): </li></ol></li><li>run \n  <ol><li>Linux &amp; MacOS:  (for REST API mode) \n    <ol><li>run  for more detail flags</li></ol></li><li>Windows:  (for REST API mode) \n    <ol><li>run  for more detail flags</li></ol></li></ol></li><li>open  in browser</li></ol><h3>MCP Server (Model Context Protocol)</h3><p>This application can also run as an MCP server, allowing AI agents and tools to interact with WhatsApp through a standardized protocol.</p><ol><li>Clone this repo <code>git clone https://github.com/aldinokemal/go-whatsapp-web-multidevice</code></li><li>Open the folder that was cloned via cmd/terminal.</li><li>run  or build the binary and run </li><li>The MCP server will start on  by default</li></ol><ul><li> - Set the host for MCP server (default: localhost)</li><li> - Set the port for MCP server (default: 8080)</li></ul><ul><li> - Send text messages</li><li> - Send contact cards</li><li> - Send links with captions</li><li> - Send location coordinates</li></ul><ul><li>SSE endpoint: <code>http://localhost:8080/sse</code></li><li>Message endpoint: <code>http://localhost:8080/message</code></li></ul><p>Make sure you have the MCP server running: </p><p>For AI tools that support MCP with SSE (like Cursor), add this configuration:</p><pre><code>{\n  \"mcpServers\": {\n    \"whatsapp\": {\n      \"url\": \"http://localhost:8080/sse\"\n    }\n  }\n}\n</code></pre><h3>Production Mode REST (docker)</h3><pre><code>docker run --detach --publish=3000:3000 --name=whatsapp --restart=always --volume=$(docker volume create --name=whatsapp):/app/storages aldinokemal2104/go-whatsapp-web-multidevice rest --autoreply=\"Dont't reply this message please\"\n</code></pre><h3>Production Mode REST (docker compose)</h3><p>create  file with the following configuration:</p><pre><code>services:\n  whatsapp:\n    image: aldinokemal2104/go-whatsapp-web-multidevice\n    container_name: whatsapp\n    restart: always\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - whatsapp:/app/storages\n    command:\n      - rest\n      - --basic-auth=admin:admin\n      - --port=3000\n      - --debug=true\n      - --os=Chrome\n      - --account-validation=false\n\nvolumes:\n  whatsapp:\n</code></pre><pre><code>services:\n  whatsapp:\n    image: aldinokemal2104/go-whatsapp-web-multidevice\n    container_name: whatsapp\n    restart: always\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - whatsapp:/app/storages\n    environment:\n      - APP_BASIC_AUTH=admin:admin\n      - APP_PORT=3000\n      - APP_DEBUG=true\n      - APP_OS=Chrome\n      - APP_ACCOUNT_VALIDATION=false\n\nvolumes:\n  whatsapp:\n</code></pre><p>You can fork or edit this source code !</p><h3>MCP (Model Context Protocol) API</h3><ul><li>MCP server provides standardized tools for AI agents to interact with WhatsApp</li><li>Supports Server-Sent Events (SSE) transport</li><li>Available tools: , , , </li><li>Compatible with MCP-enabled AI tools and agents</li></ul><table><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>/message/:message_id/revoke</td></tr><tr><td>/message/:message_id/reaction</td></tr><tr><td>/message/:message_id/delete</td></tr><tr><td>/message/:message_id/update</td></tr><tr><td>/message/:message_id/read</td></tr><tr><td>/message/:message_id/star</td></tr><tr><td>/message/:message_id/unstar</td></tr><tr></tr><tr></tr><tr><td>Add Participants in Group</td></tr><tr><td>Remove Participant in Group</td><td>/group/participants/remove</td></tr><tr><td>Promote Participant in Group</td><td>/group/participants/promote</td></tr><tr><td>Demote Participant in Group</td><td>/group/participants/demote</td></tr><tr><td>List Requested Participants in Group</td><td>/group/participant-requests</td></tr><tr><td>Approve Requested Participant in Group</td><td>/group/participant-requests/approve</td></tr><tr><td>Reject Requested Participant in Group</td><td>/group/participant-requests/reject</td></tr><tr></tr></tbody></table><pre><code>‚úÖ = Available\n‚ùå = Not Available Yet\n</code></pre><ul><li>Please do this if you have an error (invalid flag in pkg-config --cflags: -Xpreprocessor) <code>export CGO_CFLAGS_ALLOW=\"-Xpreprocessor\"</code></li></ul><ul><li>This project is unofficial and not affiliated with WhatsApp.</li><li>Please use official WhatsApp API to avoid any issues.</li><li>We only able to run MCP or REST API, this is limitation from whatsmeow library. independent MCP will be available in the future.</li></ul>","contentLength":6667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"snailyp/gemini-balance","url":"https://github.com/snailyp/gemini-balance","date":1751337922,"author":"","guid":177077,"unread":true,"content":"<li><p>: Supports configuring multiple Gemini API Keys () for automatic sequential polling, improving availability and concurrency.</p></li><li><p><strong>Visual Configuration Takes Effect Immediately</strong>: Configurations modified through the admin backend take effect without restarting the service. Remember to click save for changes to apply. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image4.png\" alt=\"Configuration Panel\"></p></li><li><p><strong>Dual Protocol API Compatibility</strong>: Supports forwarding CHAT API requests in both Gemini and OpenAI formats.</p><pre><code>openai baseurl `http://localhost:8000(/hf)/v1`\ngemini baseurl `http://localhost:8000(/gemini)/v1beta`\n</code></pre></li><li><p>: Supports web search.  configures which models can perform web searches. When actually calling, use the  model name to use this feature. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image8.png\" alt=\"Web Search\"></p></li><li><p>: Provides a  page (requires authentication) to view the status and usage of each Key in real-time. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image.png\" alt=\"Monitoring Panel\"></p></li><li><p><strong>Support for Custom Gemini Proxy</strong>: Supports custom Gemini proxies, such as those built on Deno or Cloudflare.</p></li><li><p><strong>OpenAI Image Generation API Compatibility</strong>: Adapts the  model interface to be compatible with the OpenAI image generation API, supporting client calls.</p></li><li><p>: Flexible way to add keys using regex matching for , with key deduplication. <img src=\"https://raw.githubusercontent.com/snailyp/gemini-balance/main/files/image5.png\" alt=\"Add Key\"></p></li><li><p><strong>OpenAI Format Embeddings API Compatibility</strong>: Perfectly adapts to the OpenAI format  interface, usable for local document vectorization.</p></li><li><p><strong>Streamlined Response Optimization</strong>: Optional stream output optimizer () to improve the experience of long-text stream responses.</p></li><li><p><strong>Failure Retry and Key Management</strong>: Automatically handles API request failures, retries (), automatically disables Keys after too many failures (), and periodically checks for recovery ().</p></li><li><p>: Supports AMD and ARM architecture Docker deployments. You can also build your own Docker image.</p><blockquote><p>Image address: docker pull ghcr.io/snailyp/gemini-balance:latest</p></blockquote></li><li><p><strong>Automatic Model List Maintenance</strong>: Supports fetching OpenAI and Gemini model lists, perfectly compatible with NewAPI's automatic model list fetching, no manual entry required.</p></li><li><p><strong>Support for Removing Unused Models</strong>: Too many default models are provided, many of which are not used. You can filter them out using .</p></li><li><p>: Supports configuring HTTP/SOCKS5 proxy servers () for accessing the Gemini API, convenient for use in special network environments. Supports batch adding proxies.</p></li>","contentLength":2169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mendableai/firecrawl","url":"https://github.com/mendableai/firecrawl","date":1751251237,"author":"","guid":175482,"unread":true,"content":"<p>üî• Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API.</p><p>Empower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.</p><p><em>This repository is in development, and we‚Äôre still integrating custom modules into the mono repo. It's not fully ready for self-hosted deployment yet, but you can run it locally.</em></p><p><a href=\"https://firecrawl.dev?ref=github\">Firecrawl</a> is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our <a href=\"https://docs.firecrawl.dev\">documentation</a>.</p><p><em>Pst. hey, you, join our stargazers :)</em></p><a href=\"https://github.com/mendableai/firecrawl\"><img src=\"https://img.shields.io/github/stars/mendableai/firecrawl.svg?style=social&amp;label=Star&amp;maxAge=2592000\" alt=\"GitHub stars\"></a><p>We provide an easy to use API with our hosted version. You can find the playground and documentation <a href=\"https://firecrawl.dev/playground\">here</a>. You can also self host the backend if you'd like.</p><p>Check out the following resources to get started:</p><p>To run locally, refer to guide <a href=\"https://github.com/mendableai/firecrawl/raw/main/CONTRIBUTING.md\">here</a>.</p><p>To use the API, you need to sign up on <a href=\"https://firecrawl.dev\">Firecrawl</a> and get an API key.</p><ul><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#scraping\"></a>: scrapes a URL and get its content in LLM-ready format (markdown, structured data via <a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#llm-extraction-beta\">LLM Extract</a>, screenshot, html)</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#crawling\"></a>: scrapes all the URLs of a web page and return content in LLM-ready format</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#map-alpha\"></a>: input a website and get all the website urls - extremely fast</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#search\"></a>: search the web and get full content from results</li><li><a href=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/#extract\"></a>: get structured data from single page, multiple pages or entire websites with AI.</li></ul><ul><li>: markdown, structured data, screenshot, HTML, links, metadata</li><li>: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration</li><li>: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...</li><li>: pdfs, docx, images</li><li>: designed to get the data you need - no matter how hard it is</li><li>: click, scroll, input, wait and more before extracting data</li><li>: scrape thousands of URLs at the same time with a new async endpoint.</li></ul><p>You can find all of Firecrawl's capabilities and how to use them in our <a href=\"https://docs.firecrawl.dev\">documentation</a></p><p>Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/crawl \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer fc-YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"limit\": 10,\n      \"scrapeOptions\": {\n        \"formats\": [\"markdown\", \"html\"]\n      }\n    }'\n</code></pre><p>Returns a crawl job id and the url to check the status of the crawl.</p><pre><code>{\n  \"success\": true,\n  \"id\": \"123-456-789\",\n  \"url\": \"https://api.firecrawl.dev/v1/crawl/123-456-789\"\n}\n</code></pre><p>Used to check the status of a crawl job and get its result.</p><pre><code>curl -X GET https://api.firecrawl.dev/v1/crawl/123-456-789 \\\n  -H 'Content-Type: application/json' \\\n  -H 'Authorization: Bearer YOUR_API_KEY'\n</code></pre><pre><code>{\n  \"status\": \"completed\",\n  \"total\": 36,\n  \"creditsUsed\": 36,\n  \"expiresAt\": \"2024-00-00T00:00:00.000Z\",\n  \"data\": [\n    {\n      \"markdown\": \"[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...\",\n      \"html\": \"&lt;!DOCTYPE html&gt;&lt;html lang=\\\"en\\\" class=\\\"js-focus-visible lg:[--scroll-mt:9.5rem]\\\" data-js-focus-visible=\\\"\\\"&gt;...\",\n      \"metadata\": {\n        \"title\": \"Build a 'Chat with website' using Groq Llama 3 | Firecrawl\",\n        \"language\": \"en\",\n        \"sourceURL\": \"https://docs.firecrawl.dev/learn/rag-llama3\",\n        \"description\": \"Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.\",\n        \"ogLocaleAlternate\": [],\n        \"statusCode\": 200\n      }\n    }\n  ]\n}\n</code></pre><p>Used to scrape a URL and get its content in the specified formats.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"markdown\": \"Launch Week I is here! [See our Day 2 Release üöÄ](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[üí• Get 2 months free...\",\n    \"html\": \"&lt;!DOCTYPE html&gt;&lt;html lang=\\\"en\\\" class=\\\"light\\\" style=\\\"color-scheme: light;\\\"&gt;&lt;body class=\\\"__variable_36bd41 __variable_d7dc5d font-inter ...\",\n    \"metadata\": {\n      \"title\": \"Home - Firecrawl\",\n      \"description\": \"Firecrawl crawls and converts any website into clean markdown.\",\n      \"language\": \"en\",\n      \"keywords\": \"Firecrawl,Markdown,Data,Mendable,Langchain\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Firecrawl\",\n      \"ogDescription\": \"Turn any website into LLM-ready data.\",\n      \"ogUrl\": \"https://www.firecrawl.dev/\",\n      \"ogImage\": \"https://www.firecrawl.dev/og.png?123\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Firecrawl\",\n      \"sourceURL\": \"https://firecrawl.dev\",\n      \"statusCode\": 200\n    }\n  }\n}\n</code></pre><p>Used to map a URL and get urls of the website. This returns most links present on the website.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\"\n    }'\n</code></pre><pre><code>{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://firecrawl.dev\",\n    \"https://www.firecrawl.dev/pricing\",\n    \"https://www.firecrawl.dev/blog\",\n    \"https://www.firecrawl.dev/playground\",\n    \"https://www.firecrawl.dev/smart-crawl\",\n  ]\n}\n</code></pre><p>Map with  param allows you to search for specific urls inside a website.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/map \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://firecrawl.dev\",\n      \"search\": \"docs\"\n    }'\n</code></pre><p>Response will be an ordered list from the most relevant to the least relevant.</p><pre><code>{\n  \"status\": \"success\",\n  \"links\": [\n    \"https://docs.firecrawl.dev\",\n    \"https://docs.firecrawl.dev/sdks/python\",\n    \"https://docs.firecrawl.dev/learn/rag-llama3\",\n  ]\n}\n</code></pre><p>Search the web and get full content from results</p><p>Firecrawl‚Äôs search API allows you to perform web searches and optionally scrape the search results in one operation.</p><ul><li>Choose specific output formats (markdown, HTML, links, screenshots)</li><li>Search the web with customizable parameters (language, country, etc.)</li><li>Optionally retrieve content from search results in various formats</li><li>Control the number of results and set timeouts</li></ul><pre><code>curl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5\n  }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"data\": [\n    {\n      \"url\": \"https://firecrawl.dev\",\n      \"title\": \"Firecrawl | Home Page\",\n      \"description\": \"Turn websites into LLM-ready data with Firecrawl\"\n    },\n    {\n      \"url\": \"https://docs.firecrawl.dev\",\n      \"title\": \"Documentation | Firecrawl\",\n      \"description\": \"Learn how to use Firecrawl in your own applications\"\n    }\n  ]\n}\n</code></pre><pre><code>curl -X POST https://api.firecrawl.dev/v1/search \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer fc-YOUR_API_KEY\" \\\n  -d '{\n    \"query\": \"what is firecrawl?\",\n    \"limit\": 5,\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\", \"links\"]\n    }\n  }'\n</code></pre><p>Get structured data from entire websites with a prompt and/or a schema.</p><p>You can extract structured data from one or multiple URLs, including wildcards:</p><p>When you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/extract \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\n        \"https://firecrawl.dev/*\", \n        \"https://docs.firecrawl.dev/\", \n        \"https://www.ycombinator.com/companies\"\n      ],\n      \"prompt\": \"Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.\",\n      \"schema\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"company_mission\": {\n            \"type\": \"string\"\n          },\n          \"is_open_source\": {\n            \"type\": \"boolean\"\n          },\n          \"is_in_yc\": {\n            \"type\": \"boolean\"\n          }\n        },\n        \"required\": [\n          \"company_mission\",\n          \"is_open_source\",\n          \"is_in_yc\"\n        ]\n      }\n    }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"id\": \"44aa536d-f1cb-4706-ab87-ed0386685740\",\n  \"urlTrace\": []\n}\n</code></pre><p>If you are using the sdks, it will auto pull the response for you:</p><pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"company_mission\": \"Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.\",\n    \"supports_sso\": false,\n    \"is_open_source\": true,\n    \"is_in_yc\": true\n  }\n}\n</code></pre><p>Used to extract structured data from scraped pages.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://www.mendable.ai/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"company_mission\": {\n                      \"type\": \"string\"\n            },\n            \"supports_sso\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_open_source\": {\n                      \"type\": \"boolean\"\n            },\n            \"is_in_yc\": {\n                      \"type\": \"boolean\"\n            }\n          },\n          \"required\": [\n            \"company_mission\",\n            \"supports_sso\",\n            \"is_open_source\",\n            \"is_in_yc\"\n          ]\n        }\n      }\n    }'\n</code></pre><pre><code>{\n  \"success\": true,\n  \"data\": {\n    \"content\": \"Raw Content\",\n    \"metadata\": {\n      \"title\": \"Mendable\",\n      \"description\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"robots\": \"follow, index\",\n      \"ogTitle\": \"Mendable\",\n      \"ogDescription\": \"Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide\",\n      \"ogUrl\": \"https://mendable.ai/\",\n      \"ogImage\": \"https://mendable.ai/mendable_new_og1.png\",\n      \"ogLocaleAlternate\": [],\n      \"ogSiteName\": \"Mendable\",\n      \"sourceURL\": \"https://mendable.ai/\"\n    },\n    \"json\": {\n      \"company_mission\": \"Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to\",\n      \"supports_sso\": true,\n      \"is_open_source\": false,\n      \"is_in_yc\": true\n    }\n  }\n}\n</code></pre><h3>Extracting without a schema (New)</h3><p>You can now extract without a schema by just passing a  to the endpoint. The llm chooses the structure of the data.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"url\": \"https://docs.firecrawl.dev/\",\n      \"formats\": [\"json\"],\n      \"jsonOptions\": {\n        \"prompt\": \"Extract the company mission from the page.\"\n      }\n    }'\n</code></pre><h3>Interacting with the page with Actions (Cloud-only)</h3><p>Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.</p><p>Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n        \"url\": \"google.com\",\n        \"formats\": [\"markdown\"],\n        \"actions\": [\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"click\", \"selector\": \"textarea[title=\\\"Search\\\"]\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"write\", \"text\": \"firecrawl\"},\n            {\"type\": \"wait\", \"milliseconds\": 2000},\n            {\"type\": \"press\", \"key\": \"ENTER\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"click\", \"selector\": \"h3\"},\n            {\"type\": \"wait\", \"milliseconds\": 3000},\n            {\"type\": \"screenshot\"}\n        ]\n    }'\n</code></pre><h3>Batch Scraping Multiple URLs (New)</h3><p>You can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.</p><pre><code>curl -X POST https://api.firecrawl.dev/v1/batch/scrape \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer YOUR_API_KEY' \\\n    -d '{\n      \"urls\": [\"https://docs.firecrawl.dev\", \"https://docs.firecrawl.dev/sdks/overview\"],\n      \"formats\" : [\"markdown\", \"html\"]\n    }'\n</code></pre><pre><code>from firecrawl.firecrawl import FirecrawlApp\nfrom firecrawl.firecrawl import ScrapeOptions\n\napp = FirecrawlApp(api_key=\"fc-YOUR_API_KEY\")\n\n# Scrape a website:\nscrape_status = app.scrape_url(\n  'https://firecrawl.dev', \n  formats=[\"markdown\", \"html\"]\n)\nprint(scrape_status)\n\n# Crawl a website:\ncrawl_status = app.crawl_url(\n  'https://firecrawl.dev',\n  limit=100,\n  scrape_options=ScrapeOptions(\n    formats=[\"markdown\", \"html\"],),\n  poll_interval=30\n)\nprint(crawl_status)\n</code></pre><h3>Extracting structured data from a URL</h3><p>With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:</p><pre><code>class ArticleSchema(BaseModel):\n    title: str\n    points: int \n    by: str\n    commentsURL: str\n\nclass TopArticlesSchema(BaseModel):\n    top: List[ArticleSchema] = Field(..., description=\"Top 5 stories\")\n\njson_config = JsonConfig(schema=TopArticlesSchema.model_json_schema())\n\nllm_extraction_result = app.scrape_url('https://news.ycombinator.com', formats=[\"json\"], json=json_config)\n\nprint(llm_extraction_result.json)\n</code></pre><p>To install the Firecrawl Node SDK, you can use npm:</p><pre><code>npm install @mendable/firecrawl-js\n</code></pre><ol><li>Set the API key as an environment variable named  or pass it as a parameter to the  class.</li></ol><pre><code>import FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';\n\nconst app = new FirecrawlApp({apiKey: \"fc-YOUR_API_KEY\"});\n\n// Scrape a website\nconst scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {\n  formats: ['markdown', 'html'],\n});\n\nif (scrapeResponse) {\n  console.log(scrapeResponse)\n}\n\n// Crawl a website\nconst crawlResponse = await app.crawlUrl('https://firecrawl.dev', {\n  limit: 100,\n  scrapeOptions: {\n    formats: ['markdown', 'html'],\n  }\n} satisfies CrawlParams, true, 30) satisfies CrawlStatusResponse;\n\nif (crawlResponse) {\n  console.log(crawlResponse)\n}\n</code></pre><h3>Extracting structured data from a URL</h3><p>With LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:</p><pre><code>import FirecrawlApp from \"@mendable/firecrawl-js\";\nimport { z } from \"zod\";\n\nconst app = new FirecrawlApp({\n  apiKey: \"fc-YOUR_API_KEY\"\n});\n\n// Define schema to extract contents into\nconst schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe(\"Top 5 stories on Hacker News\"),\n});\n\nconst scrapeResult = await app.scrapeUrl(\"https://news.ycombinator.com\", {\n  jsonOptions: { extractionSchema: schema },\n});\n\nconsole.log(scrapeResult.data[\"json\"]);\n</code></pre><h2>Open Source vs Cloud Offering</h2><p>Firecrawl is open source available under the AGPL-3.0 license.</p><p>To deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.</p><p>Firecrawl Cloud is available at <a href=\"https://firecrawl.dev\">firecrawl.dev</a> and offers a range of features that are not available in the open source version:</p><p><em>It is the sole responsibility of the end users to respect websites' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions.</em></p><a href=\"https://github.com/mendableai/firecrawl/graphs/contributors\"><img alt=\"contributors\" src=\"https://contrib.rocks/image?repo=mendableai/firecrawl\"></a><p>This project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.</p><ul><li>The AGPL-3.0 license applies to all parts of the project unless otherwise specified.</li><li>The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.</li><li>When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.</li></ul><p>For more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.</p>","contentLength":17168,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"jnsahaj/tweakcn","url":"https://github.com/jnsahaj/tweakcn","date":1751251237,"author":"","guid":175483,"unread":true,"content":"<p>A visual no-code theme editor for shadcn/ui components</p><p> is a powerful Visual Theme Editor for tailwind CSS &amp; shadcn/ui components. It comes with Beautiful theme presets to get started, while aiming to offer advanced customisation for each aspect of your UI</p><p>Websites made with shadcn/ui famously look the same. tweakcn is a tool that helps you customize shadcn/ui components visually, to make your components stand-out. The goal is to build a platform where a user can discover endless customization options and then have the ability to put their own twist on it. Check our roadmap for more information</p><ul></ul><pre><code>git clone https://github.com/jnsahaj/tweakcn.git\ncd tweakcn\n</code></pre><ol start=\"3\"><li>Start the development server:</li></ol><a href=\"https://github.com/jnsahaj/tweakcn/graphs/contributors\"><img src=\"https://contrib.rocks/image?repo=jnsahaj/tweakcn\"></a><h3>Interested in Contributing?</h3><p>Contributions are welcome! Please feel free to submit a Pull Request.</p>","contentLength":784,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LMCache/LMCache","url":"https://github.com/LMCache/LMCache","date":1751251237,"author":"","guid":175484,"unread":true,"content":"<p>Supercharge Your LLM with the Fastest KV Cache Layer</p><h3 align=\"center\"> Redis for LLMs - Infinite and Ultra-Fast </h3><p>LMCache is an  serving engine extension to  and , especially under long-context scenarios. By storing the KV caches of reusable texts across various locations, including (GPU, CPU DRAM, Local Disk), LMCache reuses the KV caches of  reused text (not necessarily prefix) in  serving engine instance. Thus, LMCache saves precious GPU cycles and reduces user response delay.</p><p>By combining LMCache with vLLM, LMCache achieves 3-10x delay savings and GPU cycle reduction in many LLM use cases, including multi-round QA and RAG.</p><p>Try LMCache with pre-built vllm docker images <a href=\"https://docs.lmcache.ai/developer_guide/docker_file.html\">here</a>.</p><p>The community meeting for LMCache is hosted weekly. Meeting Details:</p><p>Meetings  between the two times. All are welcome to join!</p><p>We welcome and value any contributions and collaborations. Please check out <a href=\"https://raw.githubusercontent.com/LMCache/LMCache/dev/CONTRIBUTING.md\">CONTRIBUTING.md</a> for how to get involved.</p><p>If you use LMCache for your research, please cite our papers:</p><pre><code>@inproceedings{liu2024cachegen,\n  title={Cachegen: Kv cache compression and streaming for fast large language model serving},\n  author={Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},\n  booktitle={Proceedings of the ACM SIGCOMM 2024 Conference},\n  pages={38--56},\n  year={2024}\n}\n\n@article{cheng2024large,\n  title={Do Large Language Models Need a Content Delivery Network?},\n  author={Cheng, Yihua and Du, Kuntai and Yao, Jiayi and Jiang, Junchen},\n  journal={arXiv preprint arXiv:2409.13761},\n  year={2024}\n}\n\n@inproceedings{10.1145/3689031.3696098,\n  author = {Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},\n  title = {CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion},\n  year = {2025},\n  url = {https://doi.org/10.1145/3689031.3696098},\n  doi = {10.1145/3689031.3696098},\n  booktitle = {Proceedings of the Twentieth European Conference on Computer Systems},\n  pages = {94‚Äì109},\n}\n\n  \n</code></pre><p>This project is licensed under Apache License 2.0. See the <a href=\"https://raw.githubusercontent.com/LMCache/LMCache/dev/LICENSE\">LICENSE</a> file for details.</p>","contentLength":2207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"octra-labs/wallet-gen","url":"https://github.com/octra-labs/wallet-gen","date":1751251237,"author":"","guid":175485,"unread":true,"content":"<p><strong>download and start wallet generator web UI with a single command:</strong></p><pre><code>curl -fsSL https://octra.org/wallet-generator.sh | bash\n</code></pre><pre><code>powershell -c \"irm octra.org/wallet-generator.ps1 | iex\"\n</code></pre><ul><li>download the latest source code and build the wallet generator</li><li>start the server and open the generator web UI page in your browser</li><li>install to <code>~/.octra/wallet-generator</code> for future use</li></ul>","contentLength":358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"m1k1o/neko","url":"https://github.com/m1k1o/neko","date":1751251237,"author":"","guid":175486,"unread":true,"content":"<p>A self hosted virtual browser that runs in docker and uses WebRTC.</p><p>Welcome to Neko, a self-hosted virtual browser that runs in Docker and uses WebRTC technology. Neko is a powerful tool that allows you to <strong>run a fully-functional browser in a virtual environment</strong>, giving you the ability to <strong>access the internet securely and privately from anywhere</strong>. With Neko, you can browse the web, , and perform other tasks just as you would on a regular browser, all within a <strong>secure and isolated environment</strong>. Whether you are a developer looking to test web applications, a <strong>privacy-conscious user seeking a secure browsing experience</strong>, or simply someone who wants to take advantage of the <strong>convenience and flexibility of a virtual browser</strong>, Neko is the perfect solution.</p><p>In addition to its security and privacy features, Neko offers the <strong>ability for multiple users to access it simultaneously</strong>. This makes it an ideal solution for teams or organizations that need to share access to a browser, as well as for individuals who want to use <strong>multiple devices to access the same virtual environment</strong>. With Neko, you can <strong>easily and securely share access to a browser with others</strong>, without having to worry about maintaining separate configurations or settings. Whether you need to , access shared resources, or simply want to <strong>share access to a browser with friends or family</strong>, Neko makes it easy to do so.</p><p>Neko is also a great tool for  and interactive presentations. With its virtual browser capabilities, Neko allows you to host watch parties and presentations that are , without the need for in-person gatherings. This makes it easy to <strong>stay connected with friends and colleagues</strong>, even when you are unable to meet in person. With Neko, you can easily host a watch party or give an , whether it's for leisure or work. Simply invite your guests to join the virtual environment, and you can share the screen and <strong>interact with them in real-time</strong>.</p><p>This app uses WebRTC to stream a desktop inside of a docker container, original author made this because <a href=\"https://en.wikipedia.org/wiki/Rabb.it\">rabb.it</a> went under and his internet could not handle streaming and discord kept crashing when his friend attempted to. He just wanted to watch anime with his friends ·Éö(‡≤†Áõä‡≤†·Éö) so he started digging throughout the internet and found a few  clones, but none of them had the virtual browser, then he found <a href=\"https://github.com/Khauri/Turtus\">Turtus</a> and he was able to figure out the rest.</p><p>Then I found <a href=\"https://github.com/nurdism/neko\">this</a> project and started to dig into it. I really liked the idea of having collaborative browser browsing together with multiple people, so I created a fork. Initially, I wanted to merge my changes to the upstream repository, but the original author did not have time for this project anymore and it got eventually archived.</p><p>Neko started as a virtual browser that is streamed using WebRTC to multiple users.</p><ul><li>It is <strong>not only limited to a browser</strong>; it can run anything that runs on linux (e.g. VLC). Browser only happens to be the most popular and widely used use-case.</li><li>In fact, it is not limited to a single program either; you can install a full desktop environment (e.g. XFCE, KDE).</li><li>Speaking of limits, it does not need to run in a container; you could install neko on your host, connect to your X server and control your whole VM.</li><li>Theoretically it is not limited to only X server, anything that can be controlled and scraped periodically for images could be used instead. \n  <ul><li>Like implementing RDP or VNC protocol, where neko would only act as WebRTC relay server. This is currently only future.</li></ul></li></ul><p>Primary use case is connecting with multiple people, leveraging real time synchronization and interactivity:</p><ul><li> - watching video content together with multiple people and reacting to it (chat, emotes) - open source alternative to <a href=\"https://giggl.app/\">giggl.app</a> or <a href=\"https://watch.hyperbeam.com\">hyperbeam</a>.</li><li> - not only screen sharing, but others can control the screen.</li><li> - brainstorming ideas, cobrowsing, code debugging together.</li><li> - interactively guiding people in controlled environment.</li><li> - embed virtual browser in your web app - open source alternative to <a href=\"https://hyperbeam.com/\">hyperbeam API</a>. \n  <ul><li>open any third-party website or application, synchronize audio and video flawlessly among multiple participants.</li></ul></li></ul><p>Other use cases that benefit from single-user:</p><ul><li> - streaming containerized apps and desktops to end-users - similar to <a href=\"https://www.kasmweb.com/\">kasm</a>.</li><li> - own browser with persistent cookies available anywhere - similar to <a href=\"https://www.mightyapp.com/\">mightyapp</a>. \n  <ul><li>no state is left on the host browser after terminating the connection.</li><li>sensitive data like cookies are not transferred - only video is shared.</li></ul></li><li> - a better solution for planning secret parties and buying birthday gifts off the internet. \n  <ul><li>use Tor Browser and <a href=\"https://github.com/m1k1o/neko-vpn\">VPN</a> for additional anonymity.</li><li>mitigates risk of OS fingerprinting and browser vulnerabilities by running in container.</li></ul></li><li> - broadcast room content using RTMP (to e.g. twitch or youtube...).</li><li> - broadcast RTMP can be saved to a file using e.g. <a href=\"https://www.nginx.com/products/nginx/modules/rtmp-media-streaming/\">nginx-rtmp</a><ul><li>have clean environment when recording tutorials.</li><li>no need to hide bookmarks or use incognito mode.</li></ul></li><li> - access your internal applications securely without the need for VPN.</li><li> - you can install <a href=\"https://playwright.dev/\">playwright</a> or <a href=\"https://pptr.dev/\">puppeteer</a> and automate tasks while being able to actively intercept them.</li></ul><ul><li> because it uses WebRTC and not images sent over WebSockets.</li><li> support, what is not part of Apache Guacamole or noVNC.</li><li><strong>Multi-participant control</strong>, what is not natively supported by Apache Guacamole or noVNC.</li></ul><p>I like cats üê± ( is the Japanese word for cat), I'm a weeb/nerd.</p><p> Because cats are , but you love them anyways.</p><p>For neko room management software, visit <a href=\"https://github.com/m1k1o/neko-rooms\">neko-rooms</a>.</p><p>Full documentation is available at <a href=\"https://neko.m1k1o.net/\">neko.m1k1o.net</a>. Key sections include:</p><p>If you find Neko useful, consider supporting the project via <a href=\"https://github.com/sponsors/m1k1o\">GitHub Sponsors</a>.</p>","contentLength":5601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"stanford-oval/storm","url":"https://github.com/stanford-oval/storm","date":1751251237,"author":"","guid":175487,"unread":true,"content":"<p>An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.</p> **Latest News** üî• \n<ul><li><p>[2025/01] We add <a href=\"https://github.com/BerriAI/litellm\">litellm</a> integration for language models and embedding models in  v1.1.0.</p></li><li><p>[2024/09] Co-STORM codebase is now released and integrated into  python package v1.0.0. Run <code>pip install knowledge-storm --upgrade</code> to check it out.</p></li><li><p>[2024/09] We introduce collaborative STORM (Co-STORM) to support human-AI collaborative knowledge curation! <a href=\"https://www.arxiv.org/abs/2408.15232\">Co-STORM Paper</a> has been accepted to EMNLP 2024 main conference.</p></li><li><p>[2024/07] You can now install our package with <code>pip install knowledge-storm</code>!</p></li><li><p>[2024/07] We add  to support grounding on user-provided documents, complementing existing support of search engines (, ). (check out <a href=\"https://github.com/stanford-oval/storm/pull/58\">#58</a>)</p></li><li><p>[2024/07] We release demo light for developers a minimal user interface built with streamlit framework in Python, handy for local development and demo hosting (checkout <a href=\"https://github.com/stanford-oval/storm/pull/54\">#54</a>)</p></li><li><p>[2024/06] We will present STORM at NAACL 2024! Find us at Poster Session 2 on June 17 or check our <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/assets/storm_naacl2024_slides.pdf\">presentation material</a>.</p></li><li><p>[2024/05] We add Bing Search support in <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/rm.py\">rm.py</a>. Test STORM with  - we now configure the article generation part in our demo using  model.</p></li><li><p>[2024/04] We release refactored version of STORM codebase! We define <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/interface.py\">interface</a> for STORM pipeline and reimplement STORM-wiki (check out <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/knowledge_storm/storm_wiki\"></a>) to demonstrate how to instantiate the pipeline. We provide API to support customization of different language models and retrieval/search integration.</p></li></ul> STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. Co-STORM further enhanced its feature by enabling human to collaborative LLM system to support more aligned and preferred information seeking and knowledge curation. \n<p>While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.</p><p><strong>More than 70,000 people have tried our <a href=\"https://storm.genie.stanford.edu/\">live research preview</a>. Try it out to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system üôè!</strong></p><h2>How STORM &amp; Co-STORM works</h2><p>STORM breaks down generating long articles with citations into two steps:</p><ol><li>: The system conducts Internet-based research to collect references and generates an outline.</li><li>: The system uses the outline and references to generate the full-length article with citations.</li></ol><p>STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:</p><ol><li><strong>Perspective-Guided Question Asking</strong>: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.</li><li>: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.</li></ol><p>Co-STORM proposes <strong>a collaborative discourse protocol</strong> which implements a turn management policy to support smooth collaboration among</p><ul><li>: This type of agent generates answers grounded on external knowledge sources and/or raises follow-up questions based on the discourse history.</li><li>: This agent generates thought-provoking questions inspired by information discovered by the retriever but not directly used in previous turns. Question generation can also be grounded!</li><li>: The human user will take the initiative to either (1) observe the discourse to gain deeper understanding of the topic, or (2) actively engage in the conversation by injecting utterances to steer the discussion focus.</li></ul><p>Co-STORM also maintains a dynamic updated , which organize collected information into a hierarchical concept structure, aiming to <strong>build a shared conceptual space between the human user and the system</strong>. The mind map has been proven to help reduce the mental load when the discourse goes long and in-depth.</p><p>Both STORM and Co-STORM are implemented in a highly modular way using <a href=\"https://github.com/stanfordnlp/dspy\">dspy</a>.</p><p>To install the knowledge storm library, use <code>pip install knowledge-storm</code>.</p><p>You could also install the source code which allows you to modify the behavior of STORM engine directly.</p><ol><li><p>Clone the git repository.</p><pre><code>git clone https://github.com/stanford-oval/storm.git\ncd storm\n</code></pre></li><li><p>Install the required packages.</p><pre><code>conda create -n storm python=3.11\nconda activate storm\npip install -r requirements.txt\n</code></pre></li></ol><p>Currently, our package support:</p><ul><li>Language model components: All language models supported by litellm as listed <a href=\"https://docs.litellm.ai/docs/providers\">here</a></li><li>Embedding model components: All embedding models supported by litellm as listed <a href=\"https://docs.litellm.ai/docs/embedding/supported_embedding\">here</a></li><li>retrieval module components: , , , , , , , , , and  as</li></ul><p>Both STORM and Co-STORM are working in the information curation layer, you need to set up the information retrieval module and language model module to create their  classes respectively.</p><p>The STORM knowledge curation engine is defined as a simple Python  class. Here is an example of using You.com search engine and OpenAI models.</p><pre><code>import os\nfrom knowledge_storm import STORMWikiRunnerArguments, STORMWikiRunner, STORMWikiLMConfigs\nfrom knowledge_storm.lm import LitellmModel\nfrom knowledge_storm.rm import YouRM\n\nlm_configs = STORMWikiLMConfigs()\nopenai_kwargs = {\n    'api_key': os.getenv(\"OPENAI_API_KEY\"),\n    'temperature': 1.0,\n    'top_p': 0.9,\n}\n# STORM is a LM system so different components can be powered by different models to reach a good balance between cost and quality.\n# For a good practice, choose a cheaper/faster model for `conv_simulator_lm` which is used to split queries, synthesize answers in the conversation.\n# Choose a more powerful model for `article_gen_lm` to generate verifiable text with citations.\ngpt_35 = LitellmModel(model='gpt-3.5-turbo', max_tokens=500, **openai_kwargs)\ngpt_4 = LitellmModel(model='gpt-4o', max_tokens=3000, **openai_kwargs)\nlm_configs.set_conv_simulator_lm(gpt_35)\nlm_configs.set_question_asker_lm(gpt_35)\nlm_configs.set_outline_gen_lm(gpt_4)\nlm_configs.set_article_gen_lm(gpt_4)\nlm_configs.set_article_polish_lm(gpt_4)\n# Check out the STORMWikiRunnerArguments class for more configurations.\nengine_args = STORMWikiRunnerArguments(...)\nrm = YouRM(ydc_api_key=os.getenv('YDC_API_KEY'), k=engine_args.search_top_k)\nrunner = STORMWikiRunner(engine_args, lm_configs, rm)\n</code></pre><p>The  instance can be evoked with the simple  method:</p><pre><code>topic = input('Topic: ')\nrunner.run(\n    topic=topic,\n    do_research=True,\n    do_generate_outline=True,\n    do_generate_article=True,\n    do_polish_article=True,\n)\nrunner.post_run()\nrunner.summary()\n</code></pre><ul><li>: if True, simulate conversations with difference perspectives to collect information about the topic; otherwise, load the results.</li><li>: if True, generate an outline for the topic; otherwise, load the results.</li><li>: if True, generate an article for the topic based on the outline and the collected information; otherwise, load the results.</li><li>: if True, polish the article by adding a summarization section and (optionally) removing duplicate content; otherwise, load the results.</li></ul><p>The Co-STORM knowledge curation engine is defined as a simple Python  class. Here is an example of using Bing search engine and OpenAI models.</p><pre><code>from knowledge_storm.collaborative_storm.engine import CollaborativeStormLMConfigs, RunnerArgument, CoStormRunner\nfrom knowledge_storm.lm import LitellmModel\nfrom knowledge_storm.logging_wrapper import LoggingWrapper\nfrom knowledge_storm.rm import BingSearch\n\n# Co-STORM adopts the same multi LM system paradigm as STORM \nlm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()\nopenai_kwargs = {\n    \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n    \"api_provider\": \"openai\",\n    \"temperature\": 1.0,\n    \"top_p\": 0.9,\n    \"api_base\": None,\n} \nquestion_answering_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\ndiscourse_manage_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\nutterance_polishing_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)\nwarmstart_outline_gen_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\nquestion_asking_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)\nknowledge_base_lm = LitellmModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\n\nlm_config.set_question_answering_lm(question_answering_lm)\nlm_config.set_discourse_manage_lm(discourse_manage_lm)\nlm_config.set_utterance_polishing_lm(utterance_polishing_lm)\nlm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)\nlm_config.set_question_asking_lm(question_asking_lm)\nlm_config.set_knowledge_base_lm(knowledge_base_lm)\n\n# Check out the Co-STORM's RunnerArguments class for more configurations.\ntopic = input('Topic: ')\nrunner_argument = RunnerArgument(topic=topic, ...)\nlogging_wrapper = LoggingWrapper(lm_config)\nbing_rm = BingSearch(bing_search_api_key=os.environ.get(\"BING_SEARCH_API_KEY\"),\n                     k=runner_argument.retrieve_top_k)\ncostorm_runner = CoStormRunner(lm_config=lm_config,\n                               runner_argument=runner_argument,\n                               logging_wrapper=logging_wrapper,\n                               rm=bing_rm)\n</code></pre><p>The  instance can be evoked with the  and  methods.</p><pre><code># Warm start the system to build shared conceptual space between Co-STORM and users\ncostorm_runner.warm_start()\n\n# Step through the collaborative discourse \n# Run either of the code snippets below in any order, as many times as you'd like\n# To observe the conversation:\nconv_turn = costorm_runner.step()\n# To inject your utterance to actively steer the conversation:\ncostorm_runner.step(user_utterance=\"YOUR UTTERANCE HERE\")\n\n# Generate report based on the collaborative discourse\ncostorm_runner.knowledge_base.reorganize()\narticle = costorm_runner.generate_report()\nprint(article)\n</code></pre><h2>Quick Start with Example Scripts</h2><p>We provide scripts in our <a href=\"https://raw.githubusercontent.com/stanford-oval/storm/main/examples\">examples folder</a> as a quick start to run STORM and Co-STORM with different configurations.</p><p>We suggest using  to set up the API keys. Create a file  under the root directory and add the following content:</p><pre><code># ============ language model configurations ============ \n# Set up OpenAI API key.\nOPENAI_API_KEY=\"your_openai_api_key\"\n# If you are using the API service provided by OpenAI, include the following line:\nOPENAI_API_TYPE=\"openai\"\n# If you are using the API service provided by Microsoft Azure, include the following lines:\nOPENAI_API_TYPE=\"azure\"\nAZURE_API_BASE=\"your_azure_api_base_url\"\nAZURE_API_VERSION=\"your_azure_api_version\"\n# ============ retriever configurations ============ \nBING_SEARCH_API_KEY=\"your_bing_search_api_key\" # if using bing search\n# ============ encoder configurations ============ \nENCODER_API_TYPE=\"openai\" # if using openai encoder\n</code></pre><p><strong>To run STORM with  family models with default configurations:</strong></p><p>Run the following command.</p><pre><code>python examples/storm_examples/run_storm_wiki_gpt.py \\\n    --output-dir $OUTPUT_DIR \\\n    --retriever bing \\\n    --do-research \\\n    --do-generate-outline \\\n    --do-generate-article \\\n    --do-polish-article\n</code></pre><p>To run Co-STORM with  family models with default configurations,</p><ol><li>Add <code>BING_SEARCH_API_KEY=\"xxx\"</code> and  to </li><li>Run the following command</li></ol><pre><code>python examples/costorm_examples/run_costorm_gpt.py \\\n    --output-dir $OUTPUT_DIR \\\n    --retriever bing\n</code></pre><h2>Customization of the Pipeline</h2><p>If you have installed the source code, you can customize STORM based on your own use case. STORM engine consists of 4 modules:</p><ol><li>Knowledge Curation Module: Collects a broad coverage of information about the given topic.</li><li>Outline Generation Module: Organizes the collected information by generating a hierarchical outline for the curated knowledge.</li><li>Article Generation Module: Populates the generated outline with the collected information.</li><li>Article Polishing Module: Refines and enhances the written article for better presentation.</li></ol><p>The interface for each module is defined in <code>knowledge_storm/interface.py</code>, while their implementations are instantiated in <code>knowledge_storm/storm_wiki/modules/*</code>. These modules can be customized according to your specific requirements (e.g., generating sections in bullet point format instead of full paragraphs).</p><p>If you have installed the source code, you can customize Co-STORM based on your own use case</p><ol><li>Co-STORM introduces multiple LLM agent types (i.e. Co-STORM experts and Moderator). LLM agent interface is defined in <code>knowledge_storm/interface.py</code> , while its implementation is instantiated in <code>knowledge_storm/collaborative_storm/modules/co_storm_agents.py</code>. Different LLM agent policies can be customized.</li><li>Co-STORM introduces a collaborative discourse protocol, with its core function centered on turn policy management. We provide an example implementation of turn policy management through  in <code>knowledge_storm/collaborative_storm/engine.py</code>. It can be customized and further improved.</li></ol><p>To facilitate the study of automatic knowledge curation and complex information seeking, our project releases the following datasets:</p><p>The FreshWiki Dataset is a collection of 100 high-quality Wikipedia articles focusing on the most-edited pages from February 2022 to September 2023. See Section 2.1 in <a href=\"https://arxiv.org/abs/2402.14207\">STORM paper</a> for more details.</p><p>You can download the dataset from <a href=\"https://huggingface.co/datasets/EchoShao8899/FreshWiki\">huggingface</a> directly. To ease the data contamination issue, we archive the <a href=\"https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup/FreshWiki\">source code</a> for the data construction pipeline that can be repeated at future dates.</p><p>To study users‚Äô interests in complex information seeking tasks in the wild, we utilized data collected from the web research preview to create the WildSeek dataset. We downsampled the data to ensure the diversity of the topics and the quality of the data. Each data point is a pair comprising a topic and the user‚Äôs goal for conducting deep search on the topic. For more details, please refer to Section 2.2 and Appendix A of <a href=\"https://www.arxiv.org/abs/2408.15232\">Co-STORM paper</a>.</p><p>The WildSeek dataset is available <a href=\"https://huggingface.co/datasets/YuchengJiang/WildSeek\">here</a>.</p><h2>Replicate STORM &amp; Co-STORM paper result</h2><p>For STORM paper experiments, please switch to the branch <a href=\"https://github.com/stanford-oval/storm/tree/NAACL-2024-code-backup\">here</a>.</p><p>For Co-STORM paper experiments, please switch to the branch  (placeholder for now, will be updated soon).</p><p>Our team is actively working on:</p><ol><li>Human-in-the-Loop Functionalities: Supporting user participation in the knowledge curation process.</li><li>Information Abstraction: Developing abstractions for curated information to support presentation formats beyond the Wikipedia-style report.</li></ol><p>If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!</p><p>We would like to thank Wikipedia for its excellent open-source content. The FreshWiki dataset is sourced from Wikipedia, licensed under the Creative Commons Attribution-ShareAlike (CC BY-SA) license.</p><p>We are very grateful to <a href=\"https://michelle123lam.github.io/\">Michelle Lam</a> for designing the logo for this project and <a href=\"https://dekun.me\">Dekun Ma</a> for leading the UI development.</p><p>Please cite our paper if you use this code or part of it in your work:</p><pre><code>@inproceedings{jiang-etal-2024-unknown,\n    title = \"Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations\",\n    author = \"Jiang, Yucheng  and\n      Shao, Yijia  and\n      Ma, Dekun  and\n      Semnani, Sina  and\n      Lam, Monica\",\n    editor = \"Al-Onaizan, Yaser  and\n      Bansal, Mohit  and\n      Chen, Yun-Nung\",\n    booktitle = \"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing\",\n    month = nov,\n    year = \"2024\",\n    address = \"Miami, Florida, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.emnlp-main.554/\",\n    doi = \"10.18653/v1/2024.emnlp-main.554\",\n    pages = \"9917--9955\",\n}\n\n@inproceedings{shao-etal-2024-assisting,\n    title = \"Assisting in Writing {W}ikipedia-like Articles From Scratch with Large Language Models\",\n    author = \"Shao, Yijia  and\n      Jiang, Yucheng  and\n      Kanell, Theodore  and\n      Xu, Peter  and\n      Khattab, Omar  and\n      Lam, Monica\",\n    editor = \"Duh, Kevin  and\n      Gomez, Helena  and\n      Bethard, Steven\",\n    booktitle = \"Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)\",\n    month = jun,\n    year = \"2024\",\n    address = \"Mexico City, Mexico\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2024.naacl-long.347/\",\n    doi = \"10.18653/v1/2024.naacl-long.347\",\n    pages = \"6252--6278\",\n}\n</code></pre>","contentLength":16494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ItzCrazyKns/Perplexica","url":"https://github.com/ItzCrazyKns/Perplexica","date":1751251237,"author":"","guid":175488,"unread":true,"content":"<p>Perplexica is an AI-powered search engine. It is an Open source alternative to Perplexity AI</p><p>Perplexica is an open-source AI-powered searching tool or an AI-powered search engine that goes deep into the internet to find answers. Inspired by Perplexity AI, it's an open-source option that not just searches the web but understands your questions. It uses advanced machine learning algorithms like similarity searching and embeddings to refine results and provides clear answers with sources cited.</p><p>Using SearxNG to stay current and fully open source, Perplexica ensures you always get the most up-to-date information without compromising your privacy.</p><p>Want to know more about its architecture and how it works? You can read it <a href=\"https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md\">here</a>.</p><ul><li>: You can make use local LLMs such as Llama3 and Mixtral using Ollama.</li><li><ul><li> (In development) Boosts search by generating different queries to find more relevant internet sources. Like normal search instead of just using the context by SearxNG, it visits the top matches and tries to find relevant sources to the user's query directly from the page.</li><li> Processes your query and performs a web search.</li></ul></li><li> Special modes to better answer specific types of questions. Perplexica currently has 6 focus modes: \n  <ul><li> Searches the entire web to find the best results.</li><li> Helpful for writing tasks that do not require searching the web.</li><li> Finds articles and papers, ideal for academic research.</li><li> Finds YouTube videos based on the search query.</li><li><strong>Wolfram Alpha Search Mode:</strong> Answers queries that need calculations or data analysis using Wolfram Alpha.</li><li> Searches Reddit for discussions and opinions related to the query.</li></ul></li><li> Some search tools might give you outdated info because they use data from crawling bots and convert them into embeddings and store them in a index. Unlike them, Perplexica uses SearxNG, a metasearch engine to get the results and rerank and get the most relevant source out of it, ensuring you always get the latest information without the overhead of daily data updates.</li><li>: Integrate Perplexica into your existing applications and make use of its capibilities.</li></ul><p>It has many more features like image and video search. Some of the planned features are mentioned in <a href=\"https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/#upcoming-features\">upcoming features</a>.</p><p>There are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.</p><h3>Getting Started with Docker (Recommended)</h3><ol><li><p>Ensure Docker is installed and running on your system.</p></li><li><p>Clone the Perplexica repository:</p><pre><code>git clone https://github.com/ItzCrazyKns/Perplexica.git\n</code></pre></li><li><p>After cloning, navigate to the directory containing the project files.</p></li><li><p>Rename the  file to . For Docker setups, you need only fill in the following fields:</p><ul><li><p>: Your OpenAI API key. <strong>You only need to fill this if you wish to use OpenAI's models</strong>.</p></li><li><p>: Your Ollama API URL. You should enter it as <code>http://host.docker.internal:PORT_NUMBER</code>. If you installed Ollama on port 11434, use <code>http://host.docker.internal:11434</code>. For other ports, adjust accordingly. <strong>You need to fill this if you wish to use Ollama's models instead of OpenAI's</strong>.</p></li><li><p>: Your Groq API key. <strong>You only need to fill this if you wish to use Groq's hosted models</strong>.</p></li><li><p>: Your Anthropic API key. <strong>You only need to fill this if you wish to use Anthropic models</strong>.</p></li><li><p>: Your Gemini API key. <strong>You only need to fill this if you wish to use Google's models</strong>.</p><p>: You can change these after starting Perplexica from the settings dialog.</p></li><li><p>: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)</p></li></ul></li><li><p>Ensure you are in the directory containing the  file and execute:</p></li><li><p>Wait a few minutes for the setup to complete. You can access Perplexica at <a href=\"http://localhost:3000\">http://localhost:3000</a> in your web browser.</p></li></ol><p>: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.</p><ol><li>Install SearXNG and allow  format in the SearXNG settings.</li><li>Clone the repository and rename the  file to  in the root directory. Ensure you complete all required fields in this file.</li><li>After populating the configuration run .</li><li>Install the dependencies and then execute .</li><li>Finally, start the app by running </li></ol><p>: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.</p><p>If you're encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama's API. To fix this issue you can:</p><ol><li><p><strong>Check your Ollama API URL:</strong> Ensure that the API URL is correctly set in the settings menu.</p></li><li><p><strong>Update API URL Based on OS:</strong></p><ul><li> Use <code>http://host.docker.internal:11434</code></li><li> Use <code>http://host.docker.internal:11434</code></li><li> Use <code>http://&lt;private_ip_of_host&gt;:11434</code></li></ul><p>Adjust the port number if you're using a different one.</p></li><li><p><strong>Linux Users - Expose Ollama to Network:</strong></p><ul><li><p>Inside <code>/etc/systemd/system/ollama.service</code>, you need to add <code>Environment=\"OLLAMA_HOST=0.0.0.0\"</code>. Then restart Ollama by . For more information see <a href=\"https://github.com/ollama/ollama/raw/main/docs/faq.md#setting-environment-variables-on-linux\">Ollama docs</a></p></li><li><p>Ensure that the port (default is 11434) is not blocked by your firewall.</p></li></ul></li></ol><p>If you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser's search bar, follow these steps:</p><ol><li>Open your browser's settings.</li><li>Navigate to the 'Search Engines' section.</li><li>Add a new site search with the following URL: <code>http://localhost:3000/?q=%s</code>. Replace  with your IP address or domain name, and  with the port number if Perplexica is not hosted locally.</li><li>Click the add button. Now, you can use Perplexica directly from your browser's search bar.</li></ol><p>Perplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.</p><p>For more details, check out the full documentation <a href=\"https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md\">here</a>.</p><h2>Expose Perplexica to network</h2><p>Perplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.</p><p>If you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.</p><p>We also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!</p><table><tbody><tr><td>Address: <code>0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD</code></td></tr></tbody></table><p>Perplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the <a href=\"https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/master/CONTRIBUTING.md\">CONTRIBUTING.md</a> file to learn more about Perplexica and how you can contribute to it.</p><p>If you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. <a href=\"https://discord.gg/EFwsmQDgAu\">Click here</a> to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at .</p><p>Thank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don't forget to check back for updates and new features!</p>","contentLength":7277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"zyronon/typing-word","url":"https://github.com/zyronon/typing-word","date":1751251237,"author":"","guid":175489,"unread":true,"content":"<p>ÂèØ‰ª•ÈÄâÊã©ËÆ∞ÂøÜÊàñÈªòÂÜôÂçïËØçÔºåÊèê‰æõ‰∫ÜÈü≥Ê†áÊòæÁ§∫„ÄÅÂèëÈü≥ÂäüËÉΩÔºàÂùáÂèØÈÄâÁæéÈü≥„ÄÅËã±Èü≥Ôºâ„ÄÅÈîôËØØÁªüËÆ°</p><p>ÂÜÖÁΩÆÁªèÂÖ∏ÊïôÊùê‰π¶Á±çÔºåÂèØ‰ª•ÁªÉ‰π†ÂíåËÉåËØµÊñáÁ´†ÔºåÈÄêÂè•ËæìÂÖ•ÔºåËá™Âä®ÂèëÈü≥„ÄÇ‰πüÂèØ‰ª•Ëá™Ë°åÊ∑ªÂä†„ÄÅÂØºÂÖ•ÊñáÁ´†ÔºåÊèê‰æõ‰∏ÄÈîÆÁøªËØë„ÄÅËØëÊñáÂØπÁÖßÂäüËÉΩ</p><p>ÈªòÂÜôÂçïËØçÊó∂ËæìÂÖ•ÈîôËØØ‰ºöËá™Âä®Ê∑ªÂä†Âà∞ÈîôËØçÊú¨Ôºå‰ª•‰æøÂêéÁª≠Â§ç‰π†„ÄÇ‰πüÂèØ‰ª•Ê∑ªÂä†Âà∞ÁÆÄÂçïËØçÔºå‰πãÂêéÂÜçÈÅáÂà∞Ëøô‰∏™ËØç‰æø‰ºöËá™Âä®Ë∑≥ËøáÔºåÂêåÊó∂‰πüÂèØ‰ª•Â∞ÜÂÖ∂Ê∑ªÂä†Âà∞ÁîüËØçÊú¨‰∏≠Ôºå‰ª•‰æøÂ∑©Âõ∫Â§ç‰π†</p><p>Âú®Áî®Êà∑ÂÆåÊàê‰∏Ä‰∏™Á´†ËäÇÁöÑÁªÉ‰π†ÂêéÔºåÂ¶ÇÊûúÊúâÈîôËØØËØçÔºåÈÇ£‰πà‰ºöÈáçÂ§çÁªÉ‰π†ÈîôËØØËØçÔºåÁõ¥Âà∞Ê≤°ÊúâÈîôËØØËØç‰∏∫Ê≠¢„ÄÇÂÆåÊàê‰πãÂêéÂºπÂá∫ÈÄâÈ°πÂèØÈÄâÊã©ÈªòÂÜôÊú¨Á´†„ÄÅÈáçÂ§çÊú¨Á´†„ÄÅ‰∏ã‰∏ÄÁ´†</p><p>ÂÜÖÁΩÆ‰∫ÜÂ∏∏Áî®ÁöÑ CET-4 „ÄÅCET-6 „ÄÅGMAT „ÄÅGRE „ÄÅIELTS „ÄÅSAT „ÄÅTOEFL „ÄÅËÄÉÁ†îËã±ËØ≠„ÄÅ‰∏ì‰∏öÂõõÁ∫ßËã±ËØ≠„ÄÅ‰∏ì‰∏öÂÖ´Á∫ßËã±ËØ≠Ôºå‰πüÊúâÁ®ãÂ∫èÂëòÂ∏∏ËßÅËã±ËØ≠ÂçïËØç‰ª•ÂèäÂ§öÁßçÁºñÁ®ãËØ≠Ë®Ä API Á≠âËØçÂ∫ì„ÄÇ Â∞ΩÂèØËÉΩÊª°Ë∂≥Â§ßÈÉ®ÂàÜÁî®Êà∑ÂØπËÉåÂçïËØçÁöÑÈúÄÊ±ÇÔºå‰πüÈùûÂ∏∏Ê¨¢ËøéÁ§æÂå∫Ë¥°ÁåÆÊõ¥Â§öÁöÑËØçÂ∫ì„ÄÇ</p><p>Êú¨È°πÁõÆÊòØÂü∫‰∫éÂºÄÂèëÁöÑÔºåÈúÄË¶Å node ÁéØÂ¢ÉÊù•ËøêË°å„ÄÇ</p><p>ÂàõÂª∫ÈïúÂÉè docker build -t typing-word:001 .</p><p>ÂêØÂä®ÂÆπÂô® docker run --name typing-word -p 3000:3000 -d typing-word:001</p><ul><li>CET-4„ÄÅCET-6„ÄÅGMAT„ÄÅGRE„ÄÅIELTS„ÄÅSAT„ÄÅTOEFL„ÄÅBEC</li></ul><ul><li>JavaScript API„ÄÅNode.js API„ÄÅJava API„ÄÅLinux Command„ÄÅC#: List API</li></ul><p>Â¶ÇÊûúÊÇ®ÈúÄË¶ÅËÉåËØµÂÖ∂‰ªñËØçÂ∫ìÔºåÊ¨¢ËøéÂú® Issue ‰∏≠ÊèêÂá∫</p><p>ÁõÆÂâçÈ°πÁõÆÂ§Ñ‰∫éÂºÄÂèëÂàùÊúüÔºåÊñ∞ÂäüËÉΩÊ≠£Âú®ÊåÅÁª≠Ê∑ªÂä†‰∏≠ÔºåÂ¶ÇÊûú‰Ω†ÂØπËΩØ‰ª∂Êúâ‰ªª‰ΩïÂäüËÉΩ‰∏éÂª∫ËÆÆÔºåÊ¨¢ËøéÂú® Issues ‰∏≠ÊèêÂá∫ Â¶ÇÊûú‰Ω†‰πüÂñúÊ¨¢Êú¨ËΩØ‰ª∂ÁöÑËÆæËÆ°ÊÄùÊÉ≥ÔºåÊ¨¢ËøéÊèê‰∫§ prÔºåÈùûÂ∏∏ÊÑüË∞¢‰Ω†ÂØπÊàë‰ª¨ÁöÑÊîØÊåÅÔºÅ</p><p><a href=\"https://github.com/RealKai42/qwerty-learner/\">qwerty-learner</a> ÂæàÂñúÊ¨¢‰ΩúËÄÖÁöÑËøô‰∏™È°πÁõÆÔºå‰ΩÜÊòØÂÆÉÊ≤°ÊúâËÉåÂçïËØçÊâÄÂøÖÂ§áÁöÑ  ÁöÑÂäüËÉΩÔºåÂèØËÉΩÊòØ‰ΩúËÄÖÂèçÂ§çÂº∫Ë∞ÉÂíåÊèêÈÜíËøô‰∏™È°πÁõÆÊòØ‚Äú‚ÄùËÄå‰∏çÊòØ‰∏Ä‰∏™‚Äú‚ÄùÁöÑËΩØ‰ª∂ÂêßÔºåÂ∞ΩÁÆ°ÁªùÂ§ßÂ§öÊï∞Áî®Êà∑ÈÉΩÊòØÁî®ÂÆÉÊù•ËÉåÂçïËØçüòÇüòÇüòÇ„ÄÇ</p><p>Êú¨È°πÁõÆÂèÇËÄÉÂÖ∂ÊÄùË∑Ø‰ΩøÁî® Vue ÈáçÂÜô‰∫ÜÔºåÂπ∂Ê∑ªÂä†‰∫Ü  „ÄÅ  Á≠âÂäüËÉΩ</p>","contentLength":1935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["trending"]}