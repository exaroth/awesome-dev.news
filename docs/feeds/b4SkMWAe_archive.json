{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":131,"items":[{"title":"Digest #161: GitLab 300GB Loss, DIY Data Center, OAuth Attacks, Netflix AWS Security, Docker Hub Limits & More!","url":"https://www.devopsbulletin.com/p/digest-161-gitlab-300gb-loss-diy","date":1740308599,"author":"Mohamed Labouardy","guid":9560,"unread":true,"content":"<p><strong>Welcome to this week‚Äôs edition of the DevOps Bulletin!</strong></p><p>First, see how GitLab lost 300GB of production data and look into what it takes to build your data center. We also share the findings from six months of research into OAuth application attacks, update you on Docker Hub‚Äôs new limits, explain Cortex Cloud‚Äôs merge with Prisma Cloud, and show how monday.com is managing its trace volume.</p><p>Discover how to secure thousands of AWS accounts in our featured podcast without slowing down developers from Netflix‚Äôs cloud security engineer.</p><p>In the tutorials section, learn how to replace Docker Compose with Quadlet for servers and organize Terraform code for better scalability to refactoring code with GitHub Copilot, understand Azure Data Transfer pricing, fix AWS Serverless image handlers, set up cross-region disaster recovery on AWS, explore multi-cluster fault tolerance with k8gb, storing Terraform state in Azure, and even a look at HTTP3.</p><p>We also highlight some cool open-source devtools:‚Ä¢  ‚Äì like Wireshark for Docker, letting you see all network requests.‚Ä¢  ‚Äì a handy script to switch from Docker to Podman.‚Ä¢  ‚Äì a tool to track which Chrome extensions make suspicious DNS requests.</p><p>All this and more in this week‚Äôs DevOps Bulletin‚Äîdon‚Äôt miss out!</p><ul><li><p> is an open-source project management platform focused on simplicity and efficiency.</p></li><li><p> is Wireshark for your Docker containers. It lets devs see all incoming and outgoing requests to resolve production issues faster.</p></li></ul><ul><li><p> is a small bash script that helps you migrate from Docker to Podman.</p></li><li><p> helps track which Chrome extensions are making suspicious DNS requests. </p></li></ul><div><p>If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":1768,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdd377709-8e11-4b69-9726-1029a1b30588_1447x991.jpeg","enclosureMime":"","commentsUrl":null},{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7puq/amazon_eks_downgrade_a_practical_solution/","date":1740308426,"author":"/u/Complete-Emu-6287","guid":9546,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon EKS Downgrade: A Practical Solution","url":"https://www.reddit.com/r/kubernetes/comments/1iw7pqt/amazon_eks_downgrade_a_practical_solution/","date":1740308415,"author":"/u/Complete-Emu-6287","guid":9545,"unread":true,"content":"<p>Amazon EKS makes Kubernetes upgrades seamless, but downgrading is not supported, leaving teams stuck if issues arise after an upgrade. In our latest article, we share a practical approach to downgrading an EKS cluster using Velero for backup &amp; restore, IAM adjustments for cross-cluster access, and EBS permissions for persistent storage recovery.</p><p>üîó Read the full article here: </p><p>If you've faced EKS downgrade challenges, let's discuss your experiences! ‚¨áÔ∏è #AWS #Kubernetes #EKS #DevOps</p>","contentLength":490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1740303005,"author":"/u/Aciddit","guid":9526,"unread":true,"content":"<div>By <b>Antonio Ojea, Michael McCune</b> |\n<time datetime=\"2025-02-14\">Friday, February 14, 2025</time></div><p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><figure><img src=\"https://kubernetes.io/images/docs/components-of-kubernetes.svg\" alt=\"Components of Kubernetes\"></figure><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the Node\nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial Node is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><figure><img src=\"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/ccm-chicken-egg-problem-sequence-diagram.svg\" alt=\"Chicken and egg problem sequence diagram\"><figcaption><p>Chicken and egg problem sequence diagram</p></figcaption></figure><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a Node resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a Node resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one ‚Äúcorrect way‚Äù to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting ‚ÄúhostNetwork‚Äù to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider‚Äôs instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure‚Äôs node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider‚Äôs documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: cloud-controller-manager\n  name: cloud-controller-manager\n  namespace: kube-system\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cloud-controller-manager\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: cloud-controller-manager\n      annotations:\n        kubernetes.io/description: Cloud controller manager for my infrastructure\n    spec:\n      containers: # the container details will depend on your specific cloud controller manager\n      - name: cloud-controller-manager\n        command:\n        - /bin/my-infrastructure-cloud-controller-manager\n        - --leader-elect=true\n        - -v=1\n        image: registry/my-infrastructure-cloud-controller-manager@latest\n        resources:\n          requests:\n            cpu: 200m\n            memory: 50Mi\n      hostNetwork: true # these Pods are part of the control plane\n      nodeSelector:\n        node-role.kubernetes.io/control-plane: \"\"\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - topologyKey: \"kubernetes.io/hostname\"\n            labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: cloud-controller-manager\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoExecute\n        key: node.kubernetes.io/unreachable\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoExecute\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n        tolerationSeconds: 120\n      - effect: NoSchedule\n        key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - effect: NoSchedule\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iw6fpj/the_cloud_controller_manager_chicken_and_egg/"},{"title":"Talos on IPv6 only network?","url":"https://www.reddit.com/r/kubernetes/comments/1ivumee/talos_on_ipv6_only_network/","date":1740262538,"author":"/u/Moleventions","guid":9371,"unread":true,"content":"<div><p>Does anyone know if you can deploy Talos on an IPv6 only network in AWS?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Moleventions\"> /u/Moleventions </a>","contentLength":107,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why K8s when there‚Äôs k3s with less resource requirements?","url":"https://www.reddit.com/r/kubernetes/comments/1ivu87n/why_k8s_when_theres_k3s_with_less_resource/","date":1740261469,"author":"/u/Crafty0x","guid":9321,"unread":true,"content":"<div><p>I don‚Äôt get why a business will run the more demanding k8s instead of k3s. What could possibly be the limitations of running k3s on full fledged servers.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/Crafty0x\"> /u/Crafty0x </a>","contentLength":186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to implement dynamic storage provisioning for onPrem cluster","url":"https://www.reddit.com/r/kubernetes/comments/1ivlitx/how_to_implement_dynamic_storage_provisioning_for/","date":1740239019,"author":"/u/Impossible_Nose_2956","guid":9237,"unread":true,"content":"<p>Hi I have setup Onprem cluster for dev qa and preprod environments onPrem.</p><p>And I use redis, rabbitmq, mqtt, sqlite(for celery) in the cluster. And all these need persistent volumes.</p><p>Without dynamic provisioning, i have to create a folder, then create pv with node affinity and then create pvc and assign it to the statefulset.</p><p>I dont want to handle PVs for my onPrem clusters.</p><p>What options are available?</p><p>Do let me know if my understanding of things is wrong anywhere. </p>","contentLength":464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"anyone tried kro for kubernetes resource management yet?","url":"https://www.reddit.com/r/kubernetes/comments/1ivhubw/anyone_tried_kro_for_kubernetes_resource/","date":1740227970,"author":"/u/AnnualRich5252","guid":9118,"unread":true,"content":"<p>i just came across this article on the new resource orchestrator for kubernetes called kro, and i think it's worth discussing here. for anyone who's been dealing with the ever-growing complexity of kubernetes deployments, kro could be a game changer. it simplifies how we manage and define complex kubernetes resources by grouping them into reusable units, making everything more efficient and predictable.</p><p>what i find cool about kro is that it focuses on making kubernetes resource management easier to handle, without needing the in-depth, advanced skills that most operators and devs have to rely on today. it's got this thing called a ResourceGraphDefinition (RGD) which essentially lets you define and manage resources as a unit, and it‚Äôs smart enough to figure out deployment sequences automatically based on dependencies. really takes the guesswork out of it.</p><p>it‚Äôs worth noting kro isn‚Äôt trying to replace helm or kustomize directly, but it definitely offers a more structured and predictable approach, with better handling of CRD upgrades and dependencies. while helm has been a go-to for packaging, kro's approach might be more useful for teams looking for a more secure, governed way to manage kubernetes resources at scale.</p><p>looking forward to hearing your thoughts!</p>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's a good combination of tools to get a proper application observation solution together?","url":"https://www.reddit.com/r/kubernetes/comments/1ivh9co/whats_a_good_combination_of_tools_to_get_a_proper/","date":1740225819,"author":"/u/tofagerl","guid":9117,"unread":true,"content":"<p>I work for a company with tons of k8s clusters, but they haven't really got the whole \"let's provide all the benefits of this to the product teams\" together yet, so we're stuck with a basic Grafana + Kibana package for now. That's fine, it works. </p><p>But since I used to work with Anthos, I got used to getting the full tracing benefits from Anthos Service Mesh, and I really miss having that. </p><p>So now I'd like to pressure the infra teams to provide something better for us, but I can't just say \"use Anthos Service Mesh\", because they are already running on GCS, so there'd be no point in using Anthos. Obviously they could use a normal Istio service mesh, but I'd like to know if there are easier solutions -- Service Meshes are complicated and come with serious drawbacks, and I'm really just looking for the observation layer, not the network security layer. </p><p>Keep in mind we prefer OSS solutions as a rule, and prefer non-managed solutions as a core philosophy because we believe in understanding each tool because we know it might break. </p>","contentLength":1038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I have KCA 50% coupun that i dont need, i will give to anyone who can give me aws aor redhat coupon in exchange?","url":"https://www.reddit.com/r/kubernetes/comments/1ivgiu6/i_have_kca_50_coupun_that_i_dont_need_i_will_give/","date":1740222832,"author":"/u/sabir8992","guid":9184,"unread":true,"content":"<div><p>If you have other exam i will give to anyone who can give me aws or Redhat coupon in exchange? DM ME Please</p></div>   submitted by   <a href=\"https://www.reddit.com/user/sabir8992\"> /u/sabir8992 </a>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thought We Had Our EKS Upgrade Figured Out‚Ä¶ We Did Not","url":"https://www.reddit.com/r/kubernetes/comments/1ivf8u3/thought_we_had_our_eks_upgrade_figured_out_we_did/","date":1740217389,"author":"/u/rohit_raveendran","guid":9303,"unread":true,"content":"<p>You ever think you‚Äôve got everything under control, only for prod to absolutely humble you? Yeah, that was us.</p><ul><li>Lower environments? ‚úÖ Tested a bunch.</li><li>Version mismatches? ‚úÖ All within limits.</li><li>EKS addons? ‚úÖ Using the standard upgrade flow.</li></ul><p>So we run Terraform on upgrade day. Everything‚Äôs looking fine‚Äîuntil <strong>kube-proxy upgrade just straight-up fails.</strong> Some pods get stuck in  Great.</p><p>Cool, thanks, very helpful. We hadn‚Äôt changed anything on kube-proxy beyond the upgrade, so what the hell?</p><p>At this point, one of us starts frantically digging through the EKS docs while another engineer manually downgrades kube-proxy just to get things back up. That works, but obviously, we can‚Äôt leave it like that.</p><p>And then we find it: <strong>a tiny note in the AWS docs added just a few days ago.</strong> Turns out, <strong>kube-proxy 1.31 needs an ARMv8.2 processor with Cryptographic Extensions</strong> (<a href=\"https://github.com/awsdocs/amazon-eks-user-guide/blob/mainline/latest/ug/nodes/hybrid-nodes-os.adoc#arm\">link</a>).</p><p>And guess what Karpenter had spun up?  AWS confirmed that A1s are a no-go in EKS 1.31+. We updated our Karpenter configs to block them, ran the upgrade again, and boom‚Äîeverything worked.</p><ol><li><strong>You‚Äôre never actually prepared.</strong> We tested everything, but something always slips through. The real test is how fast you fix it.</li><li><strong>Karpenter is great, but don‚Äôt let it go rogue.</strong> We‚Äôre now explicitly blocking unsupported instance families.</li></ol><p>Anyway, if you guys have ever had one of those ‚Äúwe did everything right, and it still blew up‚Äù moments, drop your stories. Misery loves company.</p>","contentLength":1449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Docker Hub Policies: Prioritizing Developer Experience","url":"https://www.docker.com/blog/revisiting-docker-hub-policies-prioritizing-developer-experience/","date":1740202910,"author":"Tushar Jain","guid":8970,"unread":true,"content":"<p>At Docker, we are committed to ensuring that Docker Hub remains the best place for developers, engineering teams, and operations teams to build, share, and collaborate. As part of this, we previously announced plans to introduce image pull consumption fees and storage-based billing. After further evaluating how developers use Docker Hub and what will best support the ecosystem, we have refined our approach‚Äîone that prioritizes developer experience and enables developers to scale with confidence while reinforcing Docker Hub as the foundation of the cloud-native ecosystem.</p><p>We‚Äôre making important updates to our previously announced pull limits and storage policies to ensure Docker Hub remains a valuable resource for developers:</p><ul><li><strong>No More Pull Count Limits or Consumption Charges</strong> ‚Äì We‚Äôre cancelling pull consumption charges entirely. Our focus is on making Docker Hub the best place for developers to build, share, and collaborate‚Äîensuring teams can scale with confidence.</li><li><strong>Unlimited Pull rates for Paid Users (As Announced Earlier) </strong>‚Äì Starting , all  will have unlimited image pulls (with fair use limits) to ensure a seamless experience.</li><li><strong>Updated Pull Rate Limits for Free &amp; Unauthenticated Users</strong> ‚Äì To ensure a reliable and seamless experience for all users, we are updating authenticated and free pull limits:\n<ul><li>: Limited to 10 pulls per hour (as announced previously)</li><li>: Increased to 100 pulls per hour (up from 40 pulls / hour)</li><li><strong>System accounts &amp; automation</strong>: As previously shared, automated systems and service accounts can easily authenticate using Personal Access Tokens (PATs) or Organizational Access Tokens (OATs), ensuring access to higher pull limits and a more reliable experience for automated authenticated pulls.</li></ul></li><li><strong>Storage Charges Delayed Indefinitely</strong> ‚Äì Previously, we announced plans to introduce , but we have decided to <strong>indefinitely delay any storage charges</strong>. Instead, we are focusing on delivering  that will allow users to actively manage their storage usage. Once these tools are available, we will assess storage policies in the best interest of our users. <strong>If and when storage charges are introduced, we will provide a six-month notice, ensuring teams have ample time to adjust.</strong></li></ul><ul><li><strong>The Best Place to Build and Share</strong> ‚Äì Docker Hub remains <strong>the world‚Äôs leading container registry</strong>, trusted by over 20 million developers and organizations. We‚Äôre committed to keeping it <strong>the best place to distribute and consume software</strong>.</li><li> ‚Äì We‚Äôre making these changes to <strong>support more developers, teams, and businesses as they scale</strong>, reinforcing Docker Hub as the foundation of the cloud-native world.</li><li> ‚Äì Our focus is on delivering more capabilities that help developers move faster, <strong>from better storage management to</strong><strong>strengthening security to better protect the software supply chain</strong>.</li><li> ‚Äì Every decision we make is about <strong>strengthening the platform and enabling developers to build, share, and innovate without unnecessary barriers</strong>.</li></ul><p>We appreciate your feedback, and we‚Äôre excited to keep evolving Docker Hub to meet the needs of developers and teams worldwide. Stay tuned for more updates, and as always‚Äîhappy building!&nbsp;</p>","contentLength":3132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best way to develop talos locally?","url":"https://www.reddit.com/r/kubernetes/comments/1iv4ttd/best_way_to_develop_talos_locally/","date":1740179989,"author":"/u/obviouslyGAR","guid":8877,"unread":true,"content":"<div><p>I am currently learning and building a cluster using talos.</p><p>One thing I want to know is how are you all developing locally? </p><p>Is using docker and using the command  the best way or is there another way that can be done like utilizing terraform?</p></div>   submitted by   <a href=\"https://www.reddit.com/user/obviouslyGAR\"> /u/obviouslyGAR </a>","contentLength":276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reading the Source Code","url":"https://www.reddit.com/r/kubernetes/comments/1iv1def/reading_the_source_code/","date":1740171145,"author":"/u/TopNo6605","guid":8856,"unread":true,"content":"<p>Curious does anyone have any advice or vids/blogs/books that go through the source code of k8s? I'm the type of person who likes to see what's happening under the hood. But k8s is a beast of an application. I was reading the apiserver source and got up the point where it's creating handlers and doing something with an openapi controller...which I didn't know existed.</p><p>Fascinating stuff but the amount of abstraction here is what gets me. Everything is an interface and abstracted to some other file, you end up following a long chain only to end up at an interface function without a definition. I get it, for development purposes. But man it's a beast to learn.</p><p>With the apiserver I literally just started logging when functions were called but I had to take a break after 4 hours of that. How do knew contributors get brought up to speed?</p>","contentLength":840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streamline Kubernetes Management with Rancher","url":"https://youtube.com/shorts/fOVTDobiwIE?feature=share","date":1740167919,"author":"/u/abhimanyu_saharan","guid":8833,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iv0357/streamline_kubernetes_management_with_rancher/"},{"title":"Meetup: All in Kubernetes (Munich)","url":"https://www.reddit.com/r/kubernetes/comments/1iuvh2k/meetup_all_in_kubernetes_munich/","date":1740156652,"author":"/u/simplyblock-r","guid":8722,"unread":true,"content":"<p>Hey folks, if you're in or around Munich or Bavaria: this is for you! (if it's not a right place to post it, pls delete)</p><p>We're running our second meetup of the \"All in Kubernetes\" roadshow in Munich on Thursday, 13th of March. The first meetup, last month in Berlin, one was a big success with over 80 participants in Berlin.</p><p>Community is focused around stateful workloads in Kubernetes. The sessions lined up are:</p><ol><li>Architecting and Building a K8s-based AI Platform</li><li>Databases on Kubernetes: A Storage Story</li></ol>","contentLength":501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing CDK Garbage Collection","url":"https://aws.amazon.com/blogs/devops/announcing-cdk-garbage-collection/","date":1740155867,"author":"Kaizen Conroy","guid":8684,"unread":true,"content":"<p>For CDK developers that leverage assets at scale, they may notice over time that the bootstrapped bucket or repository accumulated old or unused data. If users wanted to clean this data on their own, CDK didn‚Äôt provide a clear way of determining which data is safe to delete. To solve this problem, we are excited to announce the preview launch of <a href=\"https://docs.aws.amazon.com/cdk/v2/guide/ref-cli-cmd-gc.html\">CDK Garbage Collection</a>, a new feature of the CDK that automatically deletes old assets in your bootstrapped Amazon S3 Bucket and Amazon ECR Repository, saving users time and money. This feature is available starting in AWS CDK version 2.165.0.</p><p>We expect CDK Garbage Collection to help AWS CDK customers save on storage costs associated with using the product while not affecting how customers use CDK.</p><p>CDK Garbage Collection is exposed as a CDK CLI command named gc. To use CDK Garbage Collection in its default configuration, run the following command on a terminal in your CDK application.</p><p>The  flag is meant to acknowledge that CDK Garbage Collection is in preview mode. This indicates that the scope and API of the feature might still change, but otherwise the feature is generally production ready and fully supported.</p><p>CDK Garbage Collection works at the environment level, so it will attempt to delete isolated assets in the AWS account / region that you call it in. For the purposes of this walkthrough, you will be re-bootstrapping the environment with a custom qualifier so that you do not delete isolated assets before you are ready.</p><pre><code>cdk bootstrap --qualifier=abcdef --toolkit-stack-name=CDKToolkitDemo</code></pre><p>You now have a new bootstrap template under the name CDKToolkitDemo and bootstrap resources associated with it. Next, set up a CDK application with both Amazon S3 and Amazon ECR assets:</p><pre><code>mkdir garbage-collection-demo &amp;&amp; cd garbage-collection-demo\ncdk init -l typescript app\n</code></pre><p>Your next step is to replace the existing code In <code>lib/garbage-collection-demo-stack.ts</code> with the following CDK Stack:</p><pre><code>import * as path from 'path';\nimport * as cdk from 'aws-cdk-lib';\nimport { Construct } from 'constructs';\nimport * as lambda from 'aws-cdk-lib/aws-lambda';\n\nexport class GarbageCollectionDemoStack extends cdk.Stack {\n  constructor(scope: Construct, id: string, props?: cdk.StackProps) {\n    super(scope, id, props);\n\n    const fn1 = new lambda.Function(this, 'my-function-s3', {\n    code: lambda.Code.fromAsset(path.join(__dirname, '..', 'lambda')),\n    runtime: lambda.Runtime.NODEJS_LATEST,\n    handler: 'index.handler',\n    });\n\n    const fn2 = new lambda.Function(this, 'my-function-ecr', {\n    code: lambda.Code.fromAssetImage(path.join(__dirname, '..', 'docker')),\n    runtime: lambda.Runtime.FROM_IMAGE,\n    handler: lambda.Handler.FROM_IMAGE,\n    });\n  }\n}</code></pre><p>This creates two AWS Lambda functions, one which uses an Amazon S3 asset as its source code and one that uses an Amazon ECR image as its source code. You need to add the assets that are referenced to our CDK application. In  add a simple Lambda function:</p><pre><code>exports.handler = async function(event) {\n  const response = require('./response.json');\n  return response;\n};</code></pre><p>And in  add a simple Docker image:</p><div><pre><code>FROM public.ecr.aws/docker/library/alpine:latest</code></pre><p>Now you can run  and get your initial CDK application set up in your AWS Account.</p><pre><code>cdk deploy \\\n  --toolkit-stack-name=CDKToolkitDemo \\\n  --context='@aws-cdk/core:bootstrapQualifier=abcdef'</code></pre><p>At this point you can check to make sure that assets have been correctly added into the bootstrapped Amazon S3 bucket and Amazon ECR repository:</p><div><img aria-describedby=\"caption-attachment-21521\" loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/21/Screenshot-2024-11-18-at-2.37.59-PM.png\" alt=\"cdk assets inside s3 bucket\" width=\"2608\" height=\"998\"><p>Two objects exist in the bootstrapped Amazon S3 Bucket after the initial AWS CDK Deploy.</p></div><div><img aria-describedby=\"caption-attachment-21522\" loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/21/Screenshot-2024-11-18-at-2.36.32-PM.png\" alt=\"1 Image exists in the bootstrapped Amazon ECR Repository after the initial AWS CDK Deploy.\" width=\"2278\" height=\"580\"><p>One image exists in the bootstrapped Amazon ECR Repository after the initial AWS CDK Deploy.</p></div><p>The output shows that you have the data you expect in both bootstrapped resources. The Amazon S3 Bucket also stores the json file of the AWS CloudFormation Template that was generated when you ran cdk deploy.</p><p>You can now simulate a typical CDK development cycle by updating both assets. Add a small change to the Amazon S3 asset that lives in :</p><pre><code>exports.handler = async function(event) {\n  console.log('hello world');\n  const response = require('./response.json');\n  return response;\n};</code></pre><p>And do the same in :</p><div><pre><code>FROM public.ecr.aws/docker/library/alpine:latest\nCMD echo 'Hello World'</code></pre></div><p>You can now run  again, and both assets should be re-uploaded under a new hash.</p><div><img aria-describedby=\"caption-attachment-21523\" loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/21/Screenshot-2024-11-18-at-2.50.16-PM.png\" alt=\"4 Objects exist in the bootstrapped Amazon S3 Bucket after the second AWS CDK Deploy.\" width=\"2256\" height=\"1356\"><p>Four objects exist in the bootstrapped Amazon S3 Bucket after the second AWS CDK Deploy.</p></div><div><img aria-describedby=\"caption-attachment-21524\" loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/21/Screenshot-2024-11-18-at-2.50.52-PM.png\" alt=\"2 Images exist in the bootstrapped Amazon ECR Repository after the second AWS CDK Deploy.\" width=\"2334\" height=\"682\"><p>Two images exist in the bootstrapped Amazon ECR Repository after the second AWS CDK Deploy.</p></div><p>This output confirms that everything is as expected and the new assets have been added in. Because you are using new bootstrapped resources, you can still tell which resources are currently isolated and which are not. Right now, only the zipfile prefixed with 50f409b9 is referenced in AWS CloudFormation, and in Amazon ECR, only the image prefixed  is referenced. That means that every other asset ‚Äî 3 objects in Amazon S3 and 1 object in Amazon ECR ‚Äî are isolated and can be deleted.</p><p>One item to note is the additional files in Amazon S3 that are not your local assets ‚Äî these are AWS CloudFormation templates that are uploaded to Amazon S3 as an intermediary step before being sent to AWS CloudFormation. They are not needed after being copied over and are a perfect candidate for deletion via CDK Garbage Collection.</p><p>Here is where CDK Garbage Collection comes in. With the right parameters, you are able to clean up the isolated objects while not disturbing the assets that are actively in use.</p><div><pre><code>cdk gc \\\n  --unstable=gc \\\n  --bootstrap-stack-name=CDKToolkitDemo \\\n  --rollback-buffer-days=0 \\\n  --created-buffer-days=0</code></pre></div><p>Because you want to delete assets immediately, and not tag them for deletion later, set rollback-buffer-days to 0. You also want to delete assets that were just created, so be sure to set created-buffer-days to 0 as well. The default for created-buffer-days is 1.</p><pre><code> ‚è≥ Garbage Collecting environment aws://912331974472/us-east-1...\nFound 3 objects to delete based off of the following criteria:\n- objects have been isolated for &gt; 0 days\n- objects were created &gt; 0 days ago\n\nDelete this batch (yes/no/delete-all)? </code></pre><p>CDK Garbage Collection found three assets to be deleted from Amazon S3, which is to be expected. It prompts you to verify that you want to delete, which you do, so enter . You will then get this response:</p><div><pre><code>[100.00%] 4 files scanned: 0 assets (0.00 MiB) tagged, 3 assets (0.02 MiB) deleted.</code></pre></div><div><pre><code>Found 1 image to delete based off of the following criteria:\n- images have been isolated for &gt; 0 days\n- images were created &gt; 0 days ago\n\nDelete this batch (yes/no/delete-all)?</code></pre></div><p>Once again, this is to be expected for Amazon ECR, so you enter yes again. You then get the response:</p><div><pre><code>[100.00%] 2 files scanned: 0 assets (0.00 MiB) tagged, 1 assets (3.90 MiB) deleted.</code></pre></div><p>At this point, CDK Garbage Collection is finished.</p><p>CDK Garbage Collection exposes some parameters to help you customize the experience to your specific scenario. These options help you determine how aggressive you want your garbage collection to be.</p><ul><li>rollback-buffer-days: this is the amount of days an asset has to be marked as isolated before it is eligible for deletion.</li><li>created-buffer-days: this is the amount of days an asset must live before it is eligible for deletion.</li></ul><p>Rollback Buffer Days should be considered when you are not using  and instead use a deployment method that operates on templates only, like a pipeline. If your pipeline can rollback without any involvement of the CDK CLI, this parameter will help ensure that assets are not prematurely deleted. When used, instead of deleting unused objects, cdk gc tags them with the current date. Subsequent runs of  will check this tag and delete the asset only after it has been tagged for longer than the specified buffer days.</p><p>Created Buffer Days should be considered if you want to be extra safe about assets that have been recently uploaded. When used,  filters out any assets that have not persisted that number of days. Note that this may not include assets that have been shared across multiple CDK Apps CDK reuses assets that are identical, and its possible that a recent deploy of a CDK App references an asset that was uploaded earlier.</p><p>For example, if you want to ensure that only assets that are over a month old and have been isolated for a week are deleted, you can specify:</p><pre><code>cdk gc --unstable --rollback-buffer-days=7 --created-buffer-days=30.</code></pre><div><img aria-describedby=\"caption-attachment-21525\" loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/21/Screenshot-2025-02-13-at-1.38.54-PM.png\" alt=\"Decision flow diagram of an asset as it gets audited for garbage collection.\" width=\"1794\" height=\"738\"><p>Decision flow diagram of an asset as it gets audited for garbage collection.</p></div><h2>Limitations of CDK Garbage Collection</h2><p>During CDK Garbage Collection, we collect all stack templates to see what assets are in use. If garbage collection runs between the asset upload and stack deployment, there is a chance that it does not pick up the latest stack deployment, but it does pick up the latest asset. In this scenario, CDK Garbage Collection may delete those assets.</p><p>We recommend not deploying stacks while running CDK Garbage Collection. If that is unavoidable, setting  will help as garbage collection will avoid deleting assets that are recently created. Finally, if you do experience a failed deployment, the mitigation is to redeploy, as the asset upload step will be able to re-upload the missing asset. In practice, this race condition is only for a specific edge case and unlikely to happen. However, we are working on a new method of storing CDK Assets to reduce the risk of this race condition. That work is being tracked in this <a href=\"https://github.com/aws/aws-cdk/issues/32799\">issue</a>.</p><p>CDK Garbage Collection helps users manage the lifecycle of unused CDK Assets in their AWS account. As users continue to scale with the CDK, tools like CDK Garbage Collection will play a crucial role in maintaining clean, efficient, and cost-effective cloud environments. We encourage CDK users to explore this feature, provide <a href=\"https://github.com/aws/aws-cdk/issues/new/choose\">feedback</a>, and incorporate it into their workflows to optimize their AWS resource management.</p></div>","contentLength":9848,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bugs with k8s snap and IPv6 only","url":"https://www.reddit.com/r/kubernetes/comments/1iurtp1/bugs_with_k8s_snap_and_ipv6_only/","date":1740147192,"author":"/u/hblok","guid":8633,"unread":true,"content":"<p>I'm setting up an IPv6  cluster, using Ubuntu 24.04 and the k8s and kubelet snaps. I've disabled IPv4 on the eth0 interface, but not on loopback. The CP comes up fine, and can be used locally and remotely. However, when trying to connect a worker node, there are some configuration options relating to IPv6 which I believe are bugs. I'd be interested to hear if these are misunderstandings on my part, or actual bugs.</p><p>The first is in the k8s-apiserver-proxy config file <code>/var/snap/k8s/common/args/conf.d/k8s-apiserver-proxy.json</code>. It looks like this, where the the last part is the port number 6443. The service does not start with a <em>\"failed to parse endpoint\"</em> error:</p><pre><code>{\"endpoints\":[\"dead:beef:1234::1:6443\"]} </code></pre><p>When correcting the address to use brackets, it will start up correctly.</p><pre><code>{\"endpoints\":[\"[dead:beef:1234::1]:6443\"]} </code></pre><p>Secondly, the snap.k8s.kubelet.service will not start, trying to bind to 0.0.0.0:10250 , but fails with <em>\"Failed to listen and serve\" err=\"listen tcp 0.0.0.0:10250: bind: address already in use\"</em>. Here I'm not sure where the address and port is coming from, but I'm guessing it's a default somewhere. Possibly <a href=\"https://github.com/kubernetes/kubernetes/issues/108248\">related to this report</a>.</p>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Endor Labs Extends Microsoft SCA Alliance to GitHub","url":"https://devops.com/endor-labs-extends-microsoft-sca-alliance-to-github/","date":1740144015,"author":"Mike Vizard","guid":8594,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Has DevSecOps Failed?","url":"https://devops.com/why-has-devsecops-failed/","date":1740142593,"author":"Edouard Viot","guid":8571,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is this architecture possible without using haproxy but nginx(in rocky linux 9)?","url":"https://www.reddit.com/r/kubernetes/comments/1iupwhs/is_this_architecture_possible_without_using/","date":1740141445,"author":"/u/Keeper-Name_2271","guid":8575,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alerting from Prometheus and Grafana with kube-prometheus-stack","url":"https://www.reddit.com/r/kubernetes/comments/1iupvn8/alerting_from_prometheus_and_grafana_with/","date":1740141365,"author":"/u/HumanResult3379","guid":8917,"unread":true,"content":"<p>In Grafana page's , I find the built-in alert rules named .</p><p>I set Slack Contact points. But when the Alert Firing, it didn't send to Slack.</p><p>If I create a customized alert in Grafana, it can be sent to Slack. So does the alert-rules above only for seeing?</p><p>By the way, I find almost the same alert in Prometheus' AlertManager. I set a slack notification endpoint and the messages been sent there!</p><ol><li>Are the prometheus' alert-rules the same as  in Grafana Alert rules page like the picture above?</li><li>If want send alert from Grafana, does it only possible use new created alert rule manually in Grafana?</li></ol>","contentLength":589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Share your victories thread","url":"https://www.reddit.com/r/kubernetes/comments/1iuob4d/weekly_share_your_victories_thread/","date":1740135633,"author":"/u/gctaylor","guid":8500,"unread":true,"content":"<p>Got something working? Figure something out? Make progress that you are excited about? Share here!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 47 (external-secrets)","url":"https://www.youtube.com/watch?v=F1VRkXR1UG0","date":1740117732,"author":"CNCF [Cloud Native Computing Foundation]","guid":8399,"unread":true,"content":"<article>External-Secrets, a CNCF Sandbox project, is an open source project that simplifies the management and retrieval of secrets from external secret management systems (like AWS Secrets Manager, HashiCorp Vault, or Google Secret Manager). It bridges the gap between cloud native applications running in Kubernetes and secure external secret stores, ensuring secure and efficient secret management. \n\nJoin CNCF Ambassador, Edson Ferreira as he explores how external-secrets can be valuable for teams operating in hybrid or multi-cloud environments with stringent security needs.</article>","contentLength":573,"flags":null,"enclosureUrl":"https://www.youtube.com/v/F1VRkXR1UG0?version=3","enclosureMime":"","commentsUrl":null},{"title":"Docker Hub will only allow an unauthenticated 10/pulls per hour starting March 1st","url":"https://docs.docker.com/docker-hub/usage/","date":1740096401,"author":"/u/onedr0p","guid":7514,"unread":true,"content":"<blockquote><p>Starting April 1, 2025, all users with a Pro, Team, or Business\nsubscription will have unlimited Docker Hub pulls with fair use.\nUnauthenticated users and users with a free Personal account have the\nfollowing pull limits:</p><ul><li>Unauthenticated users: 10 pulls/hour</li><li>Authenticated users with a free account: 100 pulls/hour</li></ul></blockquote><p>The following table provides an overview of the included usage and limits for each\nuser type, subject to fair use:</p><div><table><thead><tr><th>Number of public repositories</th><th>Number of private repositories</th></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr><td>10 per IPv4 address or IPv6 /64 subnet</td></tr></tbody></table></div><p>For more details, see the following:</p><p>When utilizing the Docker Platform, users should be aware that excessive data\ntransfer, pull rates, or data storage can lead to throttling, or additional\ncharges. To ensure fair resource usage and maintain service quality, we reserve\nthe right to impose restrictions or apply additional charges to accounts\nexhibiting excessive data and storage consumption.</p><p>Docker Hub has an abuse rate limit to protect the application and\ninfrastructure. This limit applies to all requests to Hub properties including\nweb pages, APIs, and image pulls. The limit is applied per IPv4 address or per\nIPv6 /64 subnet, and while the limit changes over time depending on load and\nother factors, it's in the order of thousands of requests per minute. The abuse\nlimit applies to all users equally regardless of account level.</p><p>You can differentiate between the pull rate limit and abuse rate limit by\nlooking at the error code. The abuse limit returns a simple  response. The pull limit returns a longer error message that includes\na link to documentation.</p>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iudj0d/docker_hub_will_only_allow_an_unauthenticated/"},{"title":"Using one ingress controller to proxy to another cluster","url":"https://www.reddit.com/r/kubernetes/comments/1iub1dp/using_one_ingress_controller_to_proxy_to_another/","date":1740089739,"author":"/u/djjudas21","guid":7472,"unread":true,"content":"<p>I'm planning a migration between two on-premise clusters. Both clusters are on the same network, with an ingress IP provided by MetalLB. The network is behind a NAT gateway with a single public IP, and port forwarding.</p><p>I need to start moving applications from cluster A to cluster B, but I can only set my port forwarding to point to cluster A  cluster B.</p><p>I'm trying to figure out if there's a way to use one cluster's ingress controller to proxy some sites to the other cluster's ingress controller. Something like SSL passthrough.</p><p>I've tried to configure the following on cluster B to proxy some specific site back to cluster A, with SSL passthrough as cluster A is running all its sites with TLS enabled. Unfortunately it isn't working properly and attempting to connect to <a href=\"http://app.example.com\">app.example.com</a> on cluster B only presents the default ingress controller self-signed cert, not the real app cert from cluster A.</p><pre><code>apiVersion: v1 kind: Service metadata: name: microk8s-proxy namespace: default spec: type: ExternalName externalName: ingress-a.example.com --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" nginx.ingress.kubernetes.io/ssl-passthrough: \"true\" name: microk8s-proxy namespace: default spec: ingressClassName: public rules: - host: app.example.com http: paths: - backend: service: name: microk8s-proxy port: number: 443 path: / pathType: Prefix </code></pre><p>I've been working on this for hours and can't get it working. Seems like it might be easier to just schedule a day of downtime for all sites! Thanks</p>","contentLength":1570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CustomResourceDefinitions to provision Azure resources such as storage blob","url":"https://www.reddit.com/r/kubernetes/comments/1iuay1o/customresourcedefinitions_to_provision_azure/","date":1740089501,"author":"/u/Valuable-Ad3229","guid":7471,"unread":true,"content":"<p>I am developer working with Azure Kubernetes Service, and I wonder if it is possible to define a CustomResourceDefinitions to provision other Azure resources such as Azure storage blobs, or Azure identities?</p><p>I am mindful that this may be anti-pattern but I am curious. Thank you!</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing cost, performance, and security in K8s with policy-as-code","url":"https://www.youtube.com/watch?v=O5YBwJO6FCw","date":1740089431,"author":"CNCF [Cloud Native Computing Foundation]","guid":7474,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/O5YBwJO6FCw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Cloud Native Live: Insights from the Radar","url":"https://www.youtube.com/watch?v=Sxnqk6EoB-s","date":1740088802,"author":"CNCF [Cloud Native Computing Foundation]","guid":7473,"unread":true,"content":"<article>Insights from the Radar: Exploring trends &amp; ecosystem gaps from the CNCF AI &amp; Multicluster radar report</article>","contentLength":103,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Sxnqk6EoB-s?version=3","enclosureMime":"","commentsUrl":null},{"title":"Learning Project - Deploy Flask App With MySQL on Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iu92sy/learning_project_deploy_flask_app_with_mysql_on/","date":1740084836,"author":"/u/kchandank","guid":8379,"unread":true,"content":"<p>If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.</p><p>In this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.</p><ul><li>Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).</li><li>kubectl installed and configured to interact with your Kubernetes cluster.</li><li>Docker installed on your machine to build and push the Docker image of the Flask app.</li><li>Docker Hub account to push the Docker image.</li></ul><p>You will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:</p><ul></ul><p>Create a <a href=\"http://app.py\">app.py</a> file with following content</p><pre><code>from flask import Flask, jsonify import os import mysql.connector from mysql.connector import Error app = Flask(__name__) def get_db_connection(): \"\"\" Establishes a connection to the MySQL database using environment variables. Expected environment variables: - MYSQL_HOST - MYSQL_DB - MYSQL_USER - MYSQL_PASSWORD \"\"\" host = os.environ.get(\"MYSQL_HOST\", \"localhost\") database = os.environ.get(\"MYSQL_DB\", \"flaskdb\") user = os.environ.get(\"MYSQL_USER\", \"flaskuser\") password = os.environ.get(\"MYSQL_PASSWORD\", \"flaskpass\") try: connection = mysql.connector.connect( host=host, database=database, user=user, password=password ) if connection.is_connected(): return connection except Error as e: app.logger.error(f\"Error connecting to MySQL: {e}\") return None u/app.route(\"/\") def index(): return f\"Welcome to the Flask App running in {os.environ.get('APP_ENV', 'development')} mode!\" u/app.route(\"/dbtest\") def db_test(): \"\"\" A simple endpoint to test the MySQL connection. Executes a query to get the current time from the database. \"\"\" connection = get_db_connection() if connection is None: return jsonify({\"error\": \"Failed to connect to MySQL database\"}), 500 try: cursor = connection.cursor() cursor.execute(\"SELECT NOW();\") current_time = cursor.fetchone() return jsonify({ \"message\": \"Successfully connected to MySQL!\", \"current_time\": current_time[0] }) except Error as e: return jsonify({\"error\": str(e)}), 500 finally: if connection and connection.is_connected(): cursor.close() connection.close() if __name__ == \"__main__\": debug_mode = os.environ.get(\"DEBUG\", \"false\").lower() == \"true\" app.run(host=\"0.0.0.0\", port=5000, debug=debug_mode) </code></pre><pre><code>FROM python:3.9-slim # Install ping (iputils-ping) for troubleshooting RUN apt-get update &amp;&amp; apt-get install -y iputils-ping &amp;&amp; rm -rf /var/lib/apt/lists/* WORKDIR /app COPY requirements.txt . RUN pip install --upgrade pip &amp;&amp; pip install --no-cache-dir -r requirements.txt COPY app.py . EXPOSE 5000 ENV FLASK_APP=app.py CMD [\"python\", \"app.py\"] </code></pre><pre><code>docker build -t becloudready/my-flask-app </code></pre><p>It will show a 6 digit Code, which you need to enter to following URL</p><p>Push the Image to DockerHub</p><pre><code>docker push becloudready/my-flask-app </code></pre><p>You should be able to see the Pushed Image</p><pre><code>apiVersion: apps/v1 kind: Deployment metadata: name: flask-deployment namespace: flask-app labels: app: flask spec: replicas: 2 selector: matchLabels: app: flask template: metadata: labels: app: flask spec: containers: - name: flask image: becloudready/my-flask-app:latest # Replace with your Docker Hub image name. ports: - containerPort: 5000 env: - name: APP_ENV valueFrom: configMapKeyRef: name: flask-config key: APP_ENV - name: DEBUG valueFrom: configMapKeyRef: name: flask-config key: DEBUG - name: MYSQL_DB valueFrom: configMapKeyRef: name: flask-config key: MYSQL_DB - name: MYSQL_HOST valueFrom: configMapKeyRef: name: flask-config key: MYSQL_HOST - name: MYSQL_USER valueFrom: secretKeyRef: name: db-credentials key: username - name: MYSQL_PASSWORD valueFrom: secretKeyRef: name: db-credentials key: password </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: flask-svc namespace: flask-app spec: selector: app: flask type: LoadBalancer ports: - port: 80 targetPort: 5000 </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: flask-config namespace: flask-app data: APP_ENV: production DEBUG: \"false\" MYSQL_DB: flaskdb MYSQL_HOST: mysql-svc.mysql.svc.cluster.local </code></pre><pre><code>apiVersion: v1 kind: Namespace metadata: name: flask-app --- apiVersion: v1 kind: Namespace metadata: name: mysql </code></pre><pre><code>kubectl create secret generic db-credentials \\ --namespace=flask-app \\ --from-literal=username=flaskuser \\ --from-literal=password=flaskpass \\ --from-literal=database=flaskdb </code></pre><pre><code>apiVersion: v1 kind: ConfigMap metadata: name: mysql-initdb namespace: mysql data: initdb.sql: | CREATE DATABASE IF NOT EXISTS flaskdb; CREATE USER 'flaskuser'@'%' IDENTIFIED BY 'flaskpass'; GRANT ALL PRIVILEGES ON flaskdb.* TO 'flaskuser'@'%'; FLUSH PRIVILEGES; </code></pre><pre><code>apiVersion: v1 kind: Service metadata: name: mysql-svc namespace: mysql spec: selector: app: mysql ports: - port: 3306 targetPort: 3306 </code></pre><pre><code>apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql-statefulset namespace: mysql labels: app: mysql spec: serviceName: \"mysql-svc\" replicas: 1 selector: matchLabels: app: mysql template: metadata: labels: app: mysql spec: initContainers: - name: init-clear-mysql-data image: busybox command: [\"sh\", \"-c\", \"rm -rf /var/lib/mysql/*\"] volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql containers: - name: mysql image: mysql:5.7 ports: - containerPort: 3306 name: mysql env: - name: MYSQL_ROOT_PASSWORD value: rootpassword # For production, use a Secret instead. - name: MYSQL_DATABASE value: flaskdb - name: MYSQL_USER value: flaskuser - name: MYSQL_PASSWORD value: flaskpass volumeMounts: - name: mysql-persistent-storage mountPath: /var/lib/mysql - name: initdb mountPath: /docker-entrypoint-initdb.d volumes: - name: initdb configMap: name: mysql-initdb volumeClaimTemplates: - metadata: name: mysql-persistent-storage spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi storageClassName: do-block-storage </code></pre><ul><li><p>kubectl apply -f namespaces.yaml</p></li><li><p>Deploy ConfigMaps and Secrets:</p><p>kubectl apply -f flask-config.yaml kubectl apply -f mysql-initdb.yaml kubectl apply -f db-credentials.yaml</p></li><li><p>kubectl apply -f mysql-svc.yaml kubectl apply -f mysql-statefulset.yaml</p></li><li><p>kubectl apply -f flask-deployment.yaml kubectl apply -f flask-svc.yaml</p></li></ul><p><code>kubectl get svc -n flask-app</code><code>NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE</code><code>flask-svc LoadBalancer 10.109.112.171 146.190.190.51 80:32618/TCP 2m53s</code></p><p>Unable to connect to MySQL from Flask App</p><p>Login to the Flask app pod to ensure all values are loaded properly</p><pre><code>kubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql MYSQL_DB=flaskdb MYSQL_PASSWORD=flaskpass MYSQL_USER=flaskuser MYSQL_HOST=mysql-svc.mysql.svc.cluster.local </code></pre><ul><li>Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.</li><li>Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.</li><li>Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.</li></ul>","contentLength":7195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 51 (cert-manager)","url":"https://www.youtube.com/watch?v=UR64KulZDCM","date":1740081068,"author":"CNCF [Cloud Native Computing Foundation]","guid":7404,"unread":true,"content":"<article>Cert-manager is a recently graduated CNCF project designed to automate the management and issuance of TLS certificates. It simplifies the process of obtaining, renewing, and revoking certificates for applications running on Kubernetes.\n\nJoin CNCF Ambassador, Ronit Banerjee as he explores how cert-manager automates many aspects of managing TLS certificates, ensuring that they are always up to date, valid, and properly configured.</article>","contentLength":432,"flags":null,"enclosureUrl":"https://www.youtube.com/v/UR64KulZDCM?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 51 (WasmEdge)","url":"https://www.youtube.com/watch?v=Cxz7pC9Lq2k","date":1740080252,"author":"CNCF [Cloud Native Computing Foundation]","guid":7403,"unread":true,"content":"<article>WasmEdge is a CNCF Sandbox project that is a high-performance WebAssembly runtime optimized for cloud-native and edge computing use cases. It‚Äôs designed to run WebAssembly (Wasm) applications at the edge and in the cloud, offering a fast, efficient, and secure way to execute containerized applications.\n\nUnlike traditional container runtimes, WasmEdge is designed to work efficiently with cloud-native ecosystems. Join CNCF Ambassador, Faeka Ansari as she explores how WasmEdge allows developers to run Wasm modules with low overhead.</article>","contentLength":537,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Cxz7pC9Lq2k?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 50 (Tekton Pipelines)","url":"https://www.youtube.com/watch?v=vHnI_hty9zc","date":1740079094,"author":"CNCF [Cloud Native Computing Foundation]","guid":7402,"unread":true,"content":"<article>Tekton Pipelines is a project that focuses on providing CI/CD (Continuous Integration/Continuous Delivery) systems. Tekton is designed to facilitate the automation of the software development lifecycle, from code commit to deployment, and is deeply integrated into Kubernetes.\n\nJoin CNCF Ambassador, Chamod Perera as he explores this robust and flexible CI/CD solution suitable for teams using Kubernetes who want a Kubernetes-native approach to CI/CD.</article>","contentLength":452,"flags":null,"enclosureUrl":"https://www.youtube.com/v/vHnI_hty9zc?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 49 (Linkerd)","url":"https://www.youtube.com/watch?v=WltDqvMzZIw","date":1740078440,"author":"CNCF [Cloud Native Computing Foundation]","guid":7401,"unread":true,"content":"<article>Linkerd is a CNCF graduated project as of July 2021. This ultra light, simple, and powerful cloud native project adds security, observability, and reliability to Kubernetes, without the complexity.\n\nLinkerd is an advanced service mesh written in Rust, and has been in production for over 8 years with over 500 contributors. Join CNCF Ambassador ChengHao Yang as he explores Linkerd for the first time with the hopes of learning about its benefits, and why companies like Adidas and Xbox chose it for their tech stack.</article>","contentLength":517,"flags":null,"enclosureUrl":"https://www.youtube.com/v/WltDqvMzZIw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Arm Extension for GitHub Copilot: Accelerating Migration to Arm Architecture","url":"https://devops.com/arm-extension-for-github-copilot-accelerating-migration-to-arm-architecture/","date":1740074271,"author":"Tom Smith","guid":7318,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EKS vs. GKE differences in Services and Ingresses for their respective NLBs and ALBs","url":"https://www.reddit.com/r/kubernetes/comments/1itx2uh/eks_vs_gke_differences_in_services_and_ingresses/","date":1740053554,"author":"/u/jumiker","guid":7289,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: This Week I Learned (TWIL?) thread","url":"https://www.reddit.com/r/kubernetes/comments/1itvy0m/weekly_this_week_i_learned_twil_thread/","date":1740049229,"author":"/u/gctaylor","guid":7143,"unread":true,"content":"<p>Did you learn something new this week? Share here!</p>","contentLength":50,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EKS Auto Mode a.k.a managed Karpenter.","url":"https://www.reddit.com/r/kubernetes/comments/1itumdr/eks_auto_mode_aka_managed_karpenter/","date":1740043614,"author":"/u/lynxerious","guid":7360,"unread":true,"content":"<p>It's relatively new, has anyone tried it before? Someone just told me about it recently.</p><p><a href=\"https://aws.amazon.com/eks/pricing/\">https://aws.amazon.com/eks/pricing/</a> The pricing is a bit strange, it adds up cost to EC2 pricing instead of Karpenter pods. And there are many type of instance I can't search for in that list.</p>","contentLength":280,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Tools for Kubernetes: What Have I Missed?","url":"https://www.reddit.com/r/kubernetes/comments/1ittpj1/ai_tools_for_kubernetes_what_have_i_missed/","date":1740039627,"author":"/u/Electronic_Role_5981","guid":7051,"unread":true,"content":"<p><strong>karpor (kusionstack subproject)</strong></p><p>Intelligence for Kubernetes. World's most promising Kubernetes Visualization Tool for Developer and Platform Engineering teams</p><p><strong>kube-copilot (personal project from Azure)</strong></p><ul><li>Automate Kubernetes cluster operations using ChatGPT (GPT-4 or GPT-3.5).</li><li>Diagnose and analyze potential issues for Kubernetes workloads.</li><li>Generate Kubernetes manifests based on provided prompt instructions.</li><li>Utilize native  and  commands for Kubernetes cluster access and security vulnerability scanning.</li><li>Access the web and perform Google searches without leaving the terminal.</li></ul><p><strong>some cost related `observibility and analysis`</strong></p><p>I did not check if all below projects focus on k8s.</p><p>Are there any ai-for-k8s projects that I miss?</p>","contentLength":713,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Perform Cleanup Tasks When a Pod Crashes (Including OOM Errors)?","url":"https://www.reddit.com/r/kubernetes/comments/1itt3ja/how_to_perform_cleanup_tasks_when_a_pod_crashes/","date":1740037029,"author":"/u/SamaDinesh","guid":7099,"unread":true,"content":"<p>I have a requirement where I need to delete a specific file in a shared volume whenever a pod goes down.</p><p>I initially tried using the  lifecycle hook, and it works fine when the pod is deleted normally (e.g., via ). However, the problem is that  does not trigger when the pod crashes unexpectedly, such as due to an OOM error or a node failure. </p><p>I am looking for a reliable way to ensure that the file is deleted even when the pod crashes unexpectedly. Has anyone faced a similar issue or found a workaround?</p><pre><code>lifecycle: preStop: exec: command: [\"/bin/sh\", \"-c\", \"rm -f /data/your-file.txt\"] </code></pre>","contentLength":587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing Kyverno policy enforcement performance for large clusters","url":"https://www.youtube.com/watch?v=DWmCAUCs3bc","date":1740031543,"author":"CNCF [Cloud Native Computing Foundation]","guid":6980,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DWmCAUCs3bc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Cluster restoration","url":"https://www.reddit.com/r/kubernetes/comments/1itq9c8/cluster_restoration/","date":1740026321,"author":"/u/Upper-Aardvark-6684","guid":6959,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Upper-Aardvark-6684\"> /u/Upper-Aardvark-6684 </a>","contentLength":42,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to run VM using kubevirt in kind cluster in MacOS (M2)?","url":"https://www.reddit.com/r/kubernetes/comments/1itpzch/how_to_run_vm_using_kubevirt_in_kind_cluster_in/","date":1740025387,"author":"/u/Wooden_Departure1285","guid":6958,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Wooden_Departure1285\"> /u/Wooden_Departure1285 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"how advancements like Dynamic Resource Allocation (DRA) and the Container Device Interface (CDI) are shaping Kubernetes for AI workloads","url":"https://furiosa.ai/blog/the-next-chapter-of-kubernetes-enabling-ml-inference-at-scale","date":1740023767,"author":"/u/woowookim","guid":6206,"unread":true,"content":"<p dir=\"ltr\">CDI is designed to solve two problems. First, all device vendors expose their devices to containers in different ways. Second, device support in container runtimes is fragmented. </p><p dir=\"ltr\">This imposes high costs on both AI chip vendors and the open-source community. Yes, these circumstances clearly illustrate why a standard interface is necessary. </p><p dir=\"ltr\">Now AI chip vendors can support various container runtimes through standardized interfaces by implementing CDI. Docker also supports it as an experimental feature, but it is expected to become a default feature in the near future since containerd now supports it as a default feature. Originally, CDI was part of the DRA. However, it now has a greater influence on the entire container ecosystem.</p><p dir=\"ltr\">In addition to standardized device exposure, CDI delivers several benefits for enterprise deployments using different kinds of AI hardware (such as RNGD):</p><ul><li dir=\"ltr\"><p dir=\"ltr\">Simplified runtime support</p></li><li dir=\"ltr\"><p dir=\"ltr\">Reduced implementation costs</p></li></ul><p dir=\"ltr\">Furiosa has built RNGD to be the best accelerator for real world deployments using large language models, multimodal models and agentic AI systems. To achieve this, we will leverage new Kubernetes functionality like CDI and DRA.  RNGD uses the CDI to define how devices are assigned to containers. </p><p dir=\"ltr\">The CDI specification lays out the format, guidelines, and interfaces needed to properly describe these devices. For example, RNGD‚Äôs vendor-specific interfaces like device nodes and sysfs files can be expressed using the interfaces provided by the CDI. This makes it simpler for container-related systems to understand which system resources need to be present when running workloads on RNGD hardware. </p><p dir=\"ltr\">We‚Äôre currently building a DRA plugin for flexible and efficient resource scheduling. When we release the plugin in 2025, users will be able to request RNGD resources defined through the DRA interface. </p><p dir=\"ltr\">One key benefit of DRA is that it allows users to specify their desired hardware topology. For example, a user might say they need four RNGD devices under a single physical CPU socket, or two RNGD devices under a specific PCIe switch. They can even specify that multiple servers must be located beneath a particular network switch. </p><p>When the scheduler hands off the RNGD scheduling task to the Furiosa DRA plugin, the plugin calculates which server meets the user‚Äôs topology requirements. Among the servers that match these conditions, it then assigns the requested RNGD resources to the user‚Äôs container (reality is binding the user's pod on that server).</p><h2 dir=\"ltr\">Future outlook for Kubernetes and ML workloads</h2><p dir=\"ltr\">The evolution of Kubernetes, driven by advancements like DRA and CDI, is crucial for the future of AI. As the industry moves beyond traditional GPUs and embraces more efficient chip architectures like RNGD, the ability to effectively orchestrate and manage these resources will become even more critical.</p><p dir=\"ltr\">This will accelerate the adoption of specialized hardware, leading to faster, more efficient, and more cost-effective ML inference.</p><p dir=\"ltr\">The ongoing collaboration between the Kubernetes community and AI chip vendors is essential to ensure that Kubernetes continues to meet the evolving needs of the AI landscape. By working together, we can unlock the full potential of AI and drive innovation across industries. We believe that sharing our experience with developing and deploying RNGD will contribute to this important effort. We also hope to foster more open-source contributions that will speed progress toward an open ecosystem that supports a wide range of AI-specific hardware to serve different needs in the industry.</p>","contentLength":3579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1itphl0/how_advancements_like_dynamic_resource_allocation/"},{"title":"Dapr.io - enterprise enabler for boosting developer productivity","url":"https://www.youtube.com/watch?v=XoNcJYoJkRY","date":1740001867,"author":"CNCF [Cloud Native Computing Foundation]","guid":6062,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/XoNcJYoJkRY?version=3","enclosureMime":"","commentsUrl":null},{"title":"Master Node Migration","url":"https://www.reddit.com/r/kubernetes/comments/1itfclm/master_node_migration/","date":1739996081,"author":"/u/BrockWeekley","guid":6084,"unread":true,"content":"<p>Hello all, I've been running a k3s cluster for my home lab for several months now. My master node hardware has begun failing - it is always maxed out on CPU and is having all kinds of random failures. My question is, would it be easier to simply recreate a new cluster and apply all of my deployments there, or should mirroring the disk of the master to new hardware be fairly painless for the switch over?</p><p>I'd like to add HA with multiple master nodes to prevent this in the future, which is why I'm leaning towards just making a new cluster, as switching from an embedded sqlite DB to a shared database seems like a pain. </p>","contentLength":623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubemgr: Open-Source Kubernetes Config Merger","url":"https://www.reddit.com/r/kubernetes/comments/1ite641/kubemgr_opensource_kubernetes_config_merger/","date":1739993255,"author":"/u/RAPlDEMENT","guid":6139,"unread":true,"content":"<p>I'm excited to share a personal project I've been working on recently. My classmates and I found it tedious to manually change environment variables or modify Kubernetes configurations by hand. Merging configurations can be straightforward but often feels cumbersome and annoying.</p><p>To address this, I created Kubemgr, a Rust crate that abstracts a command for merging Kubernetes configurations:</p><pre><code>KUBECONFIG=config1:config2... kubectl config view --flatten </code></pre><p>Available on <a href=\"http://crates.io\">crates.io</a>, this CLI makes the process less painful and more intuitive.</p><p>But that's not all! For those who prefer not to install the crate locally, I also developed a user interface using Next.js and WebAssembly (WASM). The goal was to ensure that both the interface and the CLI use the exact same logic while keeping everything client-side for security reasons.</p><p>I understand that this project might not be useful for everyone, especially those who are already experienced with Kubernetes. However, it was primarily a learning exercise for me to explore new technologies and improve my skills. I'm eager to get feedback and hear any ideas for new features or improvements that could make Kubemgr more useful for the community.</p><p>The project is open-source, so feel free to check out the code and provide recommendations or suggestions for improvement on GitHub. Contributions are welcome!</p><p>If you like the project, please consider starring the GitHub repo!</p>","contentLength":1412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNCF Research End User Group: Managing Kubeflow deployments and updates at CERN (February 19, 2025)","url":"https://www.youtube.com/watch?v=QvNIS0M0VJE","date":1739988531,"author":"CNCF [Cloud Native Computing Foundation]","guid":5993,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/QvNIS0M0VJE?version=3","enclosureMime":"","commentsUrl":null},{"title":"TOC Meeting 2025-02-18","url":"https://www.youtube.com/watch?v=deyssJesSII","date":1739984032,"author":"CNCF [Cloud Native Computing Foundation]","guid":5910,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"https://www.youtube.com/v/deyssJesSII?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sawmills Emerges From Stealth to Apply AI to Managing Telemetry Data","url":"https://devops.com/sawmills-emerges-from-stealth-to-apply-ai-to-managing-telemetry-data/","date":1739978836,"author":"Mike Vizard","guid":5874,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"INE Security‚Äôs Cybersecurity and IT Training Enhances Career Stability in Tech","url":"https://devops.com/ine-securitys-cybersecurity-and-it-training-enhances-career-stability-in-tech/","date":1739977368,"author":"cybernewswire","guid":5843,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"C.K.A Exam Change - Did Anyone Take the Exam with the New Syllabus Effective 18th?","url":"https://www.reddit.com/r/kubernetes/comments/1it6rzx/cka_exam_change_did_anyone_take_the_exam_with_the/","date":1739975397,"author":"/u/fighting_cockroach","guid":5851,"unread":true,"content":"<p>I'm curious if anyone has taken the Certified Kubernetes Administrator exam with the new syllabus that became effective on February 18th. If so, how does it compare to the old exam? </p><p>Any insights on the changes and how they impacted your experience would be greatly appreciated!</p>","contentLength":277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 3 Resilience Patterns Every Developer Should Know","url":"https://blog.devops.dev/top-3-resilience-patterns-every-developer-should-know-28cb92082738?source=rss----33f8b2d9a328---4","date":1739968506,"author":"Rajeev Kumar","guid":5808,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RabbitMQ ‚Äî Pub-Sub Pattern: Implementation Of Fanout Exchange","url":"https://blog.devops.dev/rabbitmq-pub-sub-pattern-implementation-of-fanout-exchange-67a70d9f0cfb?source=rss----33f8b2d9a328---4","date":1739968499,"author":"Arijit Sarkar","guid":5807,"unread":true,"content":"<div><p>In this article, we will learn about the messaging patterns of message brokers. RabbitMQ supports a wide range of messaging patterns one‚Ä¶</p></div>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Practical Manual : Fundamentals of Bash Commands (Shell)‚ö°","url":"https://blog.devops.dev/practical-manual-fundamentals-of-bash-commands-shell-537900aa7581?source=rss----33f8b2d9a328---4","date":1739968488,"author":"Diego Esteban ‚Äç","guid":5806,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenTelemetry, Prometheus, and More: Which Is Better for Metrics Collection and Propagation?","url":"https://blog.devops.dev/opentelemetry-prometheus-and-more-which-is-better-for-metrics-collection-and-propagation-f83f8cda0b27?source=rss----33f8b2d9a328---4","date":1739968481,"author":"Zhu Jiekun","guid":5805,"unread":true,"content":"<h3>Prometheus and Remote Write&nbsp;<a href=\"https://victoriametrics.com/blog/opentelemetry-prometheus-and-more/#prometheus-and-remote-write\">#</a></h3><p>Prometheus is the de facto standard of cloud-native monitoring. The way it works is really simple: applications provide a /metrics HTTP API endpoint to expose metric data in text format. Prometheus collects application metrics from /metrics endpoints, stores the collected metrics on disk and allows analyzing the collected metrics via http-based querying&nbsp;API.</p><p>Although the Prometheus ecosystem has a rich amount of components, its core, the Prometheus Server (hereinafter referred to as Prometheus), is still shipped as a single binary that can run without additional dependencies, making its installation and deployment much&nbsp;simpler.</p><p>On the flip side, this also makes Prometheus difficult to scale. For example, initially, you can easily run Prometheus on a machine with 2 CPUs and 4GiB of memory to monitor 100 applications. But soon, you may need to monitor 10k or even up to 100k applications. In order to accomplish this Prometheus will require more resources than a limited single machine can provide. Moreover, these applications may be deployed in different clusters and availability zones. Using a single Prometheus to collect data from all over is inefficient.</p><p>Luckily, Prometheus also offers two important features:</p><ol><li>Remote write: This allows Prometheus to send metric data to , such as Thanos, Cortex, Mimir, and VictoriaMetrics. These remote storage solutions are born and designed to address issues such as scalability, high availability, multi-tenancy, and long-term storage.</li><li>Agent Mode: In this mode, Prometheus works as a data-collection agent. Features like querying, alerting, and local storage are disabled, resulting in lower resource usage. This makes Prometheus lighter and more efficient at collecting data from different sources.</li></ol><p>An example large scale multi-application architecture utilizing the above features may look&nbsp;like:</p><p>In 2019, OpenTelemetry was born. It provides a unified and open-source observability standard and helps users avoid migration pitfalls due to vendor&nbsp;lock-in.</p><p>OpenTelemetry comes with many concepts, such as Signal, which means a type of Telemetry, like Tracing Signal, Metric Signal, and Log Signal. The protocol for transmitting telemetry data between different components is called the OpenTelemetry Protocol&nbsp;(OTLP).</p><h3>Prometheus vs OpenTelemetry <a href=\"https://victoriametrics.com/blog/opentelemetry-prometheus-and-more/#prometheus-vs-opentelemetry\">#</a></h3><p>When talking about metrics, it‚Äôs easy to compare OpenTelemetry with Prometheus:</p><pre>|                              | Prometheus   | OpenTelemetry  ||------------------------------|--------------|----------------|<p>| Data Model                   | Metrics      | Metrics Signal |</p>| Protocol of Data Propagation | Remote Write | OTLP           |</pre><p>Have you ever wondered: <strong>Why should we consider OpenTelemetry when Prometheus data model and remote write protocol are already the widely adopted and supported de facto standard in cloud-native monitoring</strong>?</p><p>Suppose you are already running Kubernetes. Many components of the Kubernetes ecosystem expose metric data in the Prometheus text format. For&nbsp;example:</p><p>Since these components do not yet provide OpenTelemetry support (for pushing data in the OTLP format), we still have to <strong>scrape them and propagate the data to remote storage</strong>. In this scenario, which is more performant, <strong>Prometheus or OpenTelemetry</strong>?</p><p>We will run <a href=\"https://github.com/prometheus/prometheus\">Prometheus</a> (Agent Mode), <a href=\"https://github.com/open-telemetry/opentelemetry-collector\">OpenTelemetry Collector</a>, and <a href=\"https://github.com/VictoriaMetrics/VictoriaMetrics\">vmagent</a> (a lightweight agent from VictoriaMetrics) to scrape data from 1200 Node exporters scattered across 3 regions respectively, and send the data to the receiver via different protocols.</p><p>To support OTLP, remote write and its variants, along with various custom testing protocols without encountering any bottlenecks, we built this <a href=\"https://github.com/jiekun/metrics-noop-receiver\">No-op receiver</a> which only perform decompression and unmarshalling on the data, and record some statistical metrics, but without data persistence operations.</p><p>Here is the breakdown of the various components involved:</p><pre>|                         | Version  | Machine Type  | vCPUs   | Memory (GB) | Storage Type                  ||-------------------------|----------|---------------|---------|-------------|-------------------------------|<p>| Prometheus              | 2.53.3   | e2-highcpu-2  | 2       | 2           | Standard persistent disk(HDD) |</p>| Prometheus              | 3.0.1    | e2-highcpu-2  | 2       | 2           | Standard persistent disk(HDD) |<p>| OpenTelemetry Collector | v0.115.0 | e2-highcpu-2  | 2       | 2           | Standard persistent disk(HDD) |</p>| vmagent                 | v1.108.0 | e2-highcpu-2  | 2       | 2           | Standard persistent disk(HDD) |<p>| Node exporter           | 1.8.2    | e2-micro      | 2(0.25) | 1           | Standard persistent disk(HDD) |</p>| No-op Receiver          | N/A      | n2d-highcpu-4 | 4       | 4           | Balanced persistent disk(SSD) |</pre><p>The overall benchmark architecture is as&nbsp;follows:</p><p>Initially, as a baseline, we ran the benchmark for several days to ascertain the resource usage of different benchmark objects.</p><p>It seems upgrading  from version  to  won‚Äôt save any CPU or memory for you. And Prometheus uses the most memory among the benchmark objects. The , served as a data collection agent, has very high CPU overhead and relatively high memory&nbsp;usage.</p><p>The network traffic reflects the efficiency of the different protocols. Since benchmark objects were scraping the same targets, the in-traffic is mostly identical. The out-traffic shows that <strong>remote write 2.0 in Prometheus 3.x saves 40% of bandwidth compared with remote write 1.0</strong>. OTLP is also not very efficient. vmagent uses <a href=\"https://docs.victoriametrics.com/vmagent/#victoriametrics-remote-write-protocol\">VictoriaMetrics remote write protocol</a>, a variant of Prometheus remote write 1.0, which employs a different compression algorithm <a href=\"https://github.com/valyala/gozstd\">zstd</a> in replacement of the less efficient <a href=\"https://github.com/google/snappy\">Snappy</a>, resulting in significant bandwidth savings.</p><blockquote><em>Disk usage on the instance includes space taken up by the OS and persistent data generated by benchmark objects. Fluctuations in disk usage indicate usage by benchmark objects.</em></blockquote><p>In terms of disk usage, since the benchmark does not simulate the case when remote storage is unavailable, both the OpenTelemetry Collector and vmagent barely use any additional storage space. Prometheus, however, because WAL is enabled (even in Agent Mode), has to write the write-ahead log and purge it every two hours, which leads to the jagged pattern in the disk usage graph&nbsp;above.</p><p>After our initial analysis, we have identified some issues worth further discussion:</p><ol><li><strong>Why is the CPU usage of the OpenTelemetry Collector much higher than that of other&nbsp;agents</strong>?</li><li>The vmagent managed to reduce bandwidth consumption significantly by only changing the compression algorithm for Remote Write 1.0. <strong>Will it be useful for remote write 2.0 to use the zstd compression?</strong></li></ol><h3>Profiling OpenTelemetry Collector <a href=\"https://victoriametrics.com/blog/opentelemetry-prometheus-and-more/#profiling-opentelemetry-collector\">#</a></h3><p>The config of the OpenTelemetry Collector is very&nbsp;simple:</p><pre>service:  pipelines:      receivers: [prometheus]      exporters: [otlp]</pre><p>Therefore, the CPU overhead issue is either with the Prometheus receiver or the OTLP exporter.</p><p>By adding extensions: [pprof], we collected the <a href=\"https://victoriametrics.com/blog/202412-otlp-remote-write/otel-profile.prof\">profile sample</a> of the OpenTelemetry Collector. The profiling result told us that the OpenTelemetry Collector spends more time on the scrape operation, thus it can be reasonably assumed that the overhead is caused by the .</p><p>Considering that scraping metrics in the Prometheus text format is not the primary function of the OpenTelemetry (Collector), these performance flaws seem reasonable. Optimizing it could be an interesting process, but that is not the focus of this&nbsp;post.</p><p>But <strong>what if various infrastructure components (such as Node Exporter) could provide OTLP support? Would the OpenTelemetry Collector take advantage of&nbsp;it?</strong></p><p>We then redesigned the data collection process of the OpenTelemetry Collector and examined its resource usage when collecting data in the . The new architecture is as&nbsp;follows:</p><p><strong>OpenTelemetry Collector helper</strong> still uses Prometheus receiver to scrape Node exporter metrics. Data is then exported by OTLP exporter to the OpenTelemetry Collector at 40 req/s without batching.</p><p>By comparison, we found that in using this pattern, the OpenTelemetry Collector‚Äôs CPU usage has dropped by 1.56x and its memory usage decreased by&nbsp;3.23x.</p><p>Surprised by the extremely low bandwidth usage of the customized version of remote write 1.0 of vmagent, we tried to integrate zstd with remote write 2.0 as well. With some simple changes in Prometheus 3.x source code, we observed the following results:</p><p>With the help of zstd, the bandwidth usage of remote write 2.0 is reduced by 30%. We thought the change of the compression algorithm would affect the CPU usage. However, with the same workload, the CPU usage only increased by 4%, which is insignificant. It is reasonable to believe that zstd provides significant value for users who are concerned about bandwidth usage, distribute infrastructure across multiple cloud providers, and need to transfer data across availability zones.</p><p>The initial question we want to discuss is: <strong>why should we consider OpenTelemetry when Prometheus‚Äôs data model and remote write protocol are already supported by many vendors and projects?</strong></p><p>The conclusion is that, before most exporters (especially those who generate large amount of time series, like Istio and kube-state-metrics) can provide support for OTLP, integrating OpenTelemetry comes with extra&nbsp;costs.</p><p>In the Prometheus ecosystem, <strong>the new remote-write protocol in Prometheus 3.x reduces bandwidth usage by 40%</strong>. Enabling the zstd compression option can boost this figure to 60%. On the other hand, since Prometheus 2.x will also receive performance patches, <strong>the difference in resource usage between the two major versions seems rather indistinguishable</strong>.</p><p>And , as a special competitor to OpenTelemetry Collector and Prometheus 3.x in this benchmark&nbsp;, <strong>can scrape the metrics data in Prometheus text format with extremely low resource overhead, utilizing 3.2x/1.6x less CPU and 2.7x/3.0x less memory</strong>. Meanwhile, with the combination of remote write 1.0 and the zstd compression, it can also , <strong>saving 46% of network bandwidth compared with remote write 2.0</strong>, making it an excellent cost-effective choice for&nbsp;users.</p><p>With OpenTelemetry and Prometheus continuously rolling out new standards, the new protocols will show advantages if more exporters could add support for fully utilizing them. But for now, it seems that users still have reasons to stick to the existing standards. After all, migrations and upgrades come with not only benefits but also unignorable costs.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f83f8cda0b27\" width=\"1\" height=\"1\" alt=\"\">","contentLength":10430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Set Up a WireGuard VPN on Linux","url":"https://blog.devops.dev/how-to-manually-configure-a-vpn-with-wireguard-with-linux-781f5f106f8b?source=rss----33f8b2d9a328---4","date":1739968475,"author":"Jaecom","guid":5804,"unread":true,"content":"<h4>A Step-by-Step Guide to Connecting Servers with WireGuard VPN</h4><p>Hello everyone, I recently had to connect machines to the same VPN (virtual private network) for my manual k8s setup. I first did some testing on my EC2 container and raspberry pis, and I thought I would share my&nbsp;process.</p><p>I‚Äôll share how to configure a hub-and-peer VPN with WireGuard, an open-source protocol that uses cutting-edge cryptography for secure connections. For more details, refer to WireGuard‚Äôs <a href=\"https://www.wireguard.com\">official documentation</a>.</p><h4>Why Choose the Hub-and-Peer Model?</h4><p>In a hub-and-peer model, one central machine (the hub) routes traffic between multiple peers. This setup is simpler and more scalable compared to a fully meshed network, where every machine directly connects to every other machine. While a fully meshed network increases redundancy, it also complicates configuration as the number of machines grows. The hub-and-peer model strikes a balance, making it easier to add new&nbsp;peers.</p><ul></ul><ul><li>Dependency on the hub (single point of&nbsp;failure)</li><li>Slightly higher latency for peer-to-peer traffic</li></ul><p>For this guide, we‚Äôll configure an EC2 instance as the hub and two Raspberry Pi devices as peers. You can do this with any linux machines and even docker containers.</p><h4>Step 1: Install WireGuard and Generate&nbsp;Keys</h4><p>Start by installing WireGuard on all machines (hub and&nbsp;peers):</p><pre>sudo apt updatesudo apt install wireguard</pre><p>Generate private and public keys for each&nbsp;machine:</p><pre># Generate key for hubwg genkey | tee wireguard_hub_private.key | wg pubkey &gt; wireguard_hub_public.key<p># Generate key for Peer 1</p>wg genkey | tee wireguard_peer_1_private.key | wg pubkey &gt; wireguard_peer_1_public.key<p># Generate key for Peer 2</p>wg genkey | tee wireguard_peer_2_private.key | wg pubkey &gt; wireguard_peer_2_public.key</pre><p>Print out the contents of the key and save it in a text editor. You will need all three sets of private and public keys in the next&nbsp;step:</p><pre># Hub private &amp; public keycat wireguard_hub_private.key wireguard_hub_public.key<p># Peer 1 private &amp; public key</p>cat wireguard_peer_1_private.key wireguard_peer_1_public.key<p># Peer2 private &amp; public key</p>cat wireguard_peer_2_private.key wireguard_peer_2_public.key</pre><h4>Step 2. Create WireGuard config&nbsp;files</h4><p>In each of the machines, create a wg0.conffile in the /etc/wireguard directory:</p><pre>sudo vim /etc/wireguard/wg0.conf</pre><pre># Your hub machine /etc/wireguard/wg0.confAddress = 10.0.0.1/24PrivateKey = &lt;HUB_PRIVATE_KEY&gt;PublicKey = &lt;PEER1_PUBLIC_KEY&gt;PublicKey = &lt;PEER2_PUBLIC_KEY&gt;</pre><blockquote><em>w0g.conf file, we specified</em><em>ListenPort = 51820&nbsp;. You need to make sure that this </em><strong><em>51820 is enabled as an UDP inbound rule in your EC2 security group.</em></strong><em> Without this, your peers won‚Äôt be able to connect to your main&nbsp;hub.</em></blockquote><pre># Your peer 1 machine /etc/wireguard/wg0.confAddress = 10.0.0.2/24<p>PrivateKey = &lt;PEER_1_PRIVATE_KEY&gt;</p>PublicKey = &lt;HUB_PUBLIC_KEY&gt;<p>Endpoint = &lt;HUB_PUBLIC_IP&gt;:51820</p>AllowedIPs = 10.0.0.1/24</pre><pre># Your peer 2 machine /etc/wireguard/wg0.confAddress = 10.0.0.3/24<p>PrivateKey = &lt;PEER_2_PRIVATE_KEY&gt;</p>PublicKey = &lt;HUB_PUBLIC_KEY&gt;<p>Endpoint = &lt;HUB_PUBLIC_IP&gt;:51820</p>AllowedIPs = 10.0.0.1/24</pre><h4>Step 3. Start the Wireguard Interface</h4><p>Save the config files, and then run the following command in each of the interfaces:</p><pre># Hubsudo wg-quick up wg0sudo wg-quick up wg0sudo wg-quick up wg0</pre><h4>Step 4. Check Interface Status</h4><p>In your Hub, run the following command to view the status of your interface:</p><p>You should see something like&nbsp;this:</p><pre># Hub console output  public key: 7lEdXmpAJd6ObiZBogaBObbhu8UVwQTObXPWUMA+tnQ=  listening port: 51820<p>peer: Uk2MVpGkEv6sfFyy0kdBcksuzj82oVu112SwLB3NNyM=</p>  endpoint: PEER_1_PUBLIC_IP:46240  latest handshake: 38 seconds ago<p>  transfer: 88.66 KiB received, 24.43 KiB sent</p><p>peer: 3XageH4zmDPUfzLDbzuPT+epMNYdOep21oEFvuar82M=</p>  endpoint: PEER_2_PUBLIC_IP:35267  latest handshake: 1 minute, 21 seconds ago<p>  transfer: 92.23 KiB received, 27.93 KiB sent</p></pre><p>Here, not that the two peers we established have successfully established a handshake.</p><p>In you peers, check the wireguard interfaces as&nbsp;well:</p><pre># Peer 1 console output (Peer 2 should also look similiar)interface: wg0<p>  public key: 3XageH4zmDPUfzLDbzuPT+epMNYdOep21oEFvuar82M=</p>  private key: (hidden)<p>peer: 7lEdXmpAJd6ObiZBogaBObbhu8UVwQTObXPWUMA+tnQ=</p>  endpoint: HUB_PUBLIC_IP:51820  latest handshake: 1 minute ago<p>  transfer: 38.32 KiB received, 108.61 KiB sent</p>  persistent keepalive: every 25 seconds</pre><h4>Step 5. Ping your Hub from your&nbsp;Peers</h4><p>From your Peer 1, ping your Hub at&nbsp;10.0.0.1</p><p>You should see it successfully ping your hub via the VPN ip of 10.0.0.1 instead of the public&nbsp;IP.</p><pre>PING 10.0.0.1 (10.0.0.1) 56(84) bytes of data.64 bytes from 10.0.0.1: icmp_seq=1 ttl=64 time=6.51 ms<p>64 bytes from 10.0.0.1: icmp_seq=2 ttl=64 time=6.09 ms</p>64 bytes from 10.0.0.1: icmp_seq=3 ttl=64 time=5.63 ms<p>64 bytes from 10.0.0.1: icmp_seq=4 ttl=64 time=5.87 ms</p>^C<p>--- 10.0.0.1 ping statistics ---</p>4 packets transmitted, 4 received, 0% packet loss, time 3001ms<p>rtt min/avg/max/mdev = 5.629/6.023/6.511/0.324 ms</p></pre><h4>Step 6. Ping Between&nbsp;Peers</h4><p>When you try to ping peer 3 (10.0.0.3) from peer 2 (10.0.0.2), you will get a dead response:</p><pre># From 10.0.0.2 (peer 1)ping 10.0.0.3 </pre><pre>PING 10.0.0.4 (10.0.0.4) 56(84) bytes of data.^C<p>--- 10.0.0.3 ping statistics ---</p>3 packets transmitted, 0 received, 100% packet loss, time 2044ms</pre><p>This is because the peers are not connected to each other. They are both connected to the hub machine, but the hub machine cannot forward the ip unless you explicitly state&nbsp;it.</p><h4>Step 7. Enable IP Forwarding on the&nbsp;Hub</h4><p>In your hub, enable port forwarding:</p><pre># Temprorary enable port forwardingsudo sysctl -w net.ipv4.ip_forward=1</pre><p>To make the change permanent, you can change the /etc/systectl.conf file:</p><pre>sudo nano /etc/sysctl.conf</pre><p>Uncomment net.ipv4.ip_forward = 1 part of the config&nbsp;file:</p><pre>...<p># Uncomment the next line to enable TCP/IP SYN cookies</p># See http://lwn.net/Articles/277146/<p># Note: This may impact IPv6 TCP sessions too</p>#net.ipv4.tcp_syncookies=1<p># Uncomment the next line to enable packet forwarding for IPv4</p>net.ipv4.ip_forward=1</pre><h4>Step 8: Save Firewall&nbsp;Rules</h4><p>Then, add firewall rules to allow the hub to forward IPs in the subnet 10.0.0.0/24&nbsp;:</p><pre>sudo iptables -A FORWARD -i wg0 -o wg0 -s 10.0.0.0/24 -d 10.0.0.0/24 -j ACCEPT</pre><p>To ensure that firewall rules persist across reboots, use the netfilter-persistent package:</p><pre>sudo apt install netfilter-persistentsudo netfilter-persistent save<p>sudo netfilter-persistent reload</p>sudo systemctl enable netfilter-persistent</pre><h4>Step 8. Retest Peer-to-Peer Connectivity</h4><p>Now, you should be able to ping any of your peers, as the hub will forward that traffic to that&nbsp;IP:</p><pre># From 10.0.0.2ping 10.0.0.3</pre><pre>PING 10.0.0.3 (10.0.0.3) 56(84) bytes of data.64 bytes from 10.0.0.3: icmp_seq=1 ttl=63 time=10.8 ms<p>64 bytes from 10.0.0.3: icmp_seq=2 ttl=63 time=12.3 ms</p>64 bytes from 10.0.0.3: icmp_seq=3 ttl=63 time=10.2 ms--- 10.0.0.3 ping statistics ---<p>3 packets transmitted, 3 received, 0% packet loss, time 2003ms</p>rtt min/avg/max/mdev = 10.220/11.099/12.297/0.877 ms</pre><p>Using WireGuard‚Äôs hub-and-peer model simplifies VPN setup and management for connecting multiple machines. With IP forwarding and persistent firewall rules, your VPN remains robust and functional even after system reboots. WireGuard‚Äôs lightweight design and ease of use make it a top choice for secure, scalable networking, and in my case, it was simple to use for my manual k8s&nbsp;setup.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=781f5f106f8b\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Set Up Jenkins with Docker: From Installation to Agent Configuration","url":"https://blog.devops.dev/the-ultimate-guide-to-setting-up-jenkins-with-docker-from-installation-to-agent-configuration-852115190bef?source=rss----33f8b2d9a328---4","date":1739968469,"author":"Jaecom","guid":5803,"unread":true,"content":"<p> is the go-to automation server for continuous integration and delivery (CI/CD). Running Jenkins in Docker provides an efficient, isolated, and reproducible environment. In this guide, you‚Äôll&nbsp;learn:</p><ul><li>How to launch a  container for building Docker images inside&nbsp;Jenkins.</li><li>How to build and run a  with essential plugins.</li><li>How to  for the first&nbsp;time.</li><li>How to create and manage .</li><li>How to spin up  in Docker and connect them to your Jenkins controller.</li></ul><h4>1. Create a Docker Network for&nbsp;Jenkins</h4><p>Before we start, create a dedicated Docker network for Jenkins to keep containers organized and simplify internal name resolution:</p><pre>docker network create jenkins</pre><blockquote><em>This network will be shared by both the Jenkins controller and its&nbsp;agents.</em></blockquote><h4>2. Run Docker-in-Docker (dind)</h4><p>Jenkins needs access to Docker to build containerized applications. Launch a Docker-in-Docker container that allows the Jenkins controller to run docker commands:</p><pre>docker run \\  --name jenkins-docker \\  --detach \\  --network jenkins \\  --env DOCKER_TLS_CERTDIR=/certs \\<p>  --volume jenkins-docker-certs:/certs/client \\</p>  --volume jenkins-data:/var/jenkins_home \\  docker:dind \\<p>  --storage-driver overlay2</p></pre><blockquote><em>Docker-in-Docker is a method that provides Jenkins with its own isolated Docker environment without relying on the host‚Äôs Docker daemon, which can sometimes be unreliable or inconsistent across different environments. This ensures better stability and portability for CI/CD pipelines.</em></blockquote><h4>3. Build a Custom Jenkins&nbsp;Image</h4><p>Next, create a custom Jenkins image using the official jenkins image that includes the Docker CLI and essential Jenkins plugins (for Docker pipelines and Blue&nbsp;Ocean).</p><p>Create the JenkinsDockerfile:</p><p>Add the following content to your JenkinsDockerfile:</p><pre>FROM jenkins/jenkins:latestUSER root<p>RUN apt-get update &amp;&amp; apt-get install -y lsb-release</p>RUN curl -fsSLo /usr/share/keyrings/docker-archive-keyring.asc \\<p>  https://download.docker.com/linux/debian/gpg</p>RUN echo \"deb [arch=$(dpkg --print-architecture) \\<p>  signed-by=/usr/share/keyrings/docker-archive-keyring.asc] \\</p>  https://download.docker.com/linux/debian \\<p>  $(lsb_release -cs) stable\" &gt; /etc/apt/sources.list.d/docker.list</p>RUN apt-get update &amp;&amp; apt-get install -y docker-ce-cliRUN jenkins-plugin-cli --plugins \"blueocean docker-workflow\"</pre><pre>docker build -t myjenkins-blueocean:latest -f JenkinsDockerfile.</pre><h4>4. Start and Unlock&nbsp;Jenkins</h4><p>Run the  container, which serves the Jenkins web interface at localhost:8080&nbsp;:</p><pre>docker run \\  --name jenkins-blueocean \\  --detach \\  --env DOCKER_HOST=tcp://docker:2376 \\<p>  --env DOCKER_CERT_PATH=/certs/client \\</p>  --env DOCKER_TLS_VERIFY=1 \\  --publish 50000:50000 \\<p>  --volume jenkins-data:/var/jenkins_home \\</p>  --volume jenkins-docker-certs:/certs/client:ro \\<p>  myjenkins-blueocean:latest</p></pre><p>Navigate to localhost:8080. Youwill see the Jenkins initial setup page, asking for an admin password:</p><p>Retrieve the temporary password from your jenkins-blueocean container:</p><pre>sudo docker exec -it jenkins-blueocean bashcat /var/jenkins_home/secrets/initialAdminPassword</pre><p>Paste the password in the setup page, and install the recommended plugins:</p><p>Complete any additional steps to reach the Jenkins home&nbsp;screen:</p><h4>5. Generate SSH Keys for Jenkins&nbsp;Agents</h4><p>Jenkins agents will connect to the Jenkins controller over SSH. Generate a key pair on your&nbsp;host:</p><pre>ssh-keygen -t rsa -b 4096 -f jenkins-agent-key</pre><ul><li>jenkins-agent-key: Private&nbsp;key</li><li>jenkins-agent-key.pub: Public&nbsp;key</li></ul><p>Print the contents of the keys and copy it to your clipboard. You will need them&nbsp;later:</p><pre>cat jenkins-agent-key       # Private keycat jenkins-agent-key.pub   # Public key</pre><h4>6. Add SSH Credentials to&nbsp;Jenkins</h4><p>Now, we need create a Jenkins Credential. In Jenkins, navigate to the :</p><p>Fill in the required&nbsp;fields:</p><ul><li> SSH Username with private&nbsp;key</li><li> jenkins-agent-credentials</li><li><strong>Private Key (Enter directly):</strong> Paste content of the jenkins-agent-key</li></ul><h4>7. Run a Jenkins Agent in&nbsp;Docker</h4><p>Now that you‚Äôve created the credentials for your jenkins agent, spin up a container running the official Jenkins SSH agent image, which will use the public key for authentication.</p><p>Replace \"ssh-rsa&nbsp;...\" with the  from jenkins-agent.pub:</p><pre>sudo docker run -d \\  --rm \\  --network jenkins \\<p>  --env JENKINS_AGENT_SSH_PUBKEY=\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDGKfU7QAY...\" \\</p>  --env DOCKER_HOST=tcp://docker:2376 \\<p>  --env DOCKER_CERT_PATH=/certs/client \\</p>  --env DOCKER_TLS_VERIFY=1 \\<p>  --volume jenkins-docker-certs:/certs/client:ro \\</p>  jenkins/ssh-agent:latest</pre><h4>8. Configure the Jenkins Agent&nbsp;(Node)</h4><p>Navigate to <strong>Dashboard ‚Üí Manage Jenkins ‚Üí Nodes ‚Üí New Node</strong>. Enter a Node name, select Permanent Agent, and click&nbsp;OK:</p><p>Fill in the required&nbsp;fields:</p><ul><li>/home/jenkins</li><li>Launch agents via&nbsp;SSH</li><li> jenkins-agent-1 (name of the jenkins agent docker container)</li><li>jenkins-agent-credentials</li><li><strong>Host Key Verification Strategy:</strong> Manually trusted key Verification Strategy</li></ul><h4>9. Verify the Agent Connection</h4><p>Once saved, Jenkins will try to connect to the agent via&nbsp;SSH.</p><p>Go to<strong> Dashboard ‚Üí Nodes ‚Üí agent-1 ‚Üí&nbsp;Log</strong>:</p><p>Check the logs to confirm the Agent successfully connected and online&nbsp;message:</p><p>Now you should see agent-1 in your build executor&nbsp;status:</p><p>Congratulations! You‚Äôve successfully set up Jenkins with Docker, including:</p><ul><li> for building and running containers.</li><li> for a scalable, distributed build&nbsp;system.</li><li>A  that contains all necessary plugins for Docker workflows.</li></ul><p>I‚Äôll be uploading more Jenkins tutorials in the future, including pipeline configurations for automating builds and deployments, Jenkinsfile setups for streamlined workflows, and best practices for managing plugins, security, and scaling. Make sure to follow&nbsp;along!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=852115190bef\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deploying OWASP ZAP in Kubernetes: Automating Your Security Testing Pipeline","url":"https://blog.devops.dev/deploying-owasp-zap-in-kubernetes-automating-your-security-testing-pipeline-b5e307e3b8fb?source=rss----33f8b2d9a328---4","date":1739968464,"author":"Ashhadali","guid":5802,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Choosing Wisely: delete vs. destroy in Rails Record Deletion","url":"https://blog.devops.dev/choosing-wisely-delete-vs-destroy-in-rails-record-deletion-ba208b20b19d?source=rss----33f8b2d9a328---4","date":1739968459,"author":"Bhavesh Saluja","guid":5801,"unread":true,"content":"<div><p>Managing data efficiently is the backbone of any Ruby on Rails application. When it‚Äôs time to remove records from your database, Rails‚Ä¶</p></div>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mastering Angular UI Unit Testing: A Friendly Guide","url":"https://blog.devops.dev/mastering-angular-ui-unit-testing-a-friendly-guide-7c129d1a0f04?source=rss----33f8b2d9a328---4","date":1739968404,"author":"Adem KORKMAZ","guid":5800,"unread":true,"content":"<div><p>Unit testing is a crucial part of developing robust Angular applications. But let‚Äôs be honest‚Ää‚Äî‚Ääwriting UI tests can sometimes feel‚Ä¶</p></div>","contentLength":142,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Internet Performance Monitoring Wins the Day","url":"https://devops.com/internet-performance-monitoring-wins-the-day/","date":1739966525,"author":"Mitch Ashley","guid":5730,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Share your EXPLOSIONS thread","url":"https://www.reddit.com/r/kubernetes/comments/1it2vyy/weekly_share_your_explosions_thread/","date":1739962833,"author":"/u/gctaylor","guid":6138,"unread":true,"content":"<div><p>Did anything explode this week (or recently)? Share the details for our mutual betterment.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a>","contentLength":121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is future technology for Sr, Devops Engineer from now-2025,","url":"https://www.reddit.com/r/kubernetes/comments/1it2tme/what_is_future_technology_for_sr_devops_engineer/","date":1739962581,"author":"/u/sabir8992","guid":5734,"unread":true,"content":"<div><p>Can you list out the technology and certification that will be alteast in trend for next 5 to 8 years.</p></div>   submitted by   <a href=\"https://www.reddit.com/user/sabir8992\"> /u/sabir8992 </a>","contentLength":134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenTelemetry resource attributes: Best practices for Kubernetes","url":"https://www.dash0.com/blog/opentelemetry-resource-attributes-best-practices-for-kubernetes","date":1739958555,"author":"/u/Aciddit","guid":5733,"unread":true,"content":"<p><a target=\"_self\" href=\"https://www.dash0.com/faq/what-are-opentelemetry-resources\"> in OpenTelemetry</a> are used to document which systems the telemetry is describing, and they are often the difference between telemetry from which you can gain insights from, and ‚Äújust data‚Äù.</p><p>When instrumenting services with OpenTelemetry, adhering to semantic conventions ensures consistent, accurate, and meaningful telemetry data across systems.</p><p>A variety of attributes allow you to describe which workload on your Kubernetes cluster is emitting which telemetry. The <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/resource/k8s/\">Kubernetes resource semantic conventions</a> specify, among others, pairs of  and  attributes for pods ( and ), as well for all workloads ‚Äúhigher level‚Äù constructs (which in Kubernetes are also called ‚Äúresources‚Äù, but we don‚Äôt want to confuse the two terms), like deployment ( and ), daemonset ( and ) and so on.</p><p>Nevertheless, despite the well-defined attributes, it is not entirely straightforward to have all the right Kubernetes-related metadata attached to your telemetry.</p><p>Among all the resource attributes that describe telemetry coming from your workloads on Kubernetes,  is by far the most important: through it, you can add most other pieces of k8s-related metadata by funneling your telemetry through the <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md\"><code></code> component</a> of an OpenTelemetry collector running inside the same cluster. The <code></code> ‚Äúfills‚Äù the blanks using data it gets from Kubernetes API, and with the right tool, you can effortlessly filter and group your telemetry across all types of aggregations.</p><p>The fact that the <code></code> exists actually fills a very important gap in telemetry metadata: the OpenTelemetry SDKs running within your containers have access to virtually no metadata about the pod surrounding them is running. They are not going to know, unless you specifically add it to the pod, e.g., using environment variables, which daemonset scheduled the pod, or which replicaset of which deployment.</p><p>As a matter of fact, without additional assistance by you, for example via environment variables (either setting values yourself, or adding pod uid, pod name, and namespace name to the environment via Kubernetes‚Äô <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/workloads/pods/downward-api/\">Downward API</a>), an OpenTelemetry SDK within a container can pretty much know only the pod uid (though very esoteric magic involving the parsing of <a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cgroups\">cgroup</a> metadata, see e.g. this <a target=\"_blank\" href=\"https://github.com/dash0hq/opentelemetry-js-distribution/blob/main/src/detectors/node/opentelemetry-resource-detector-kubernetes-pod/index.ts\">resource detector</a> in the Dash0 distribution of OpenTelemetry for Node.js) and the pod name (via the networking hostname). And unfortunately, as of writing, OpenTelemetry SDKs do not even implement those consistently (see, e.g., <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-python-contrib/issues/1474\">this issue</a>).</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>By the way, the <code></code> is capable of identifying which pod is sending telemetry to it via the unique pod ip (see the <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md#configuration\">‚Äúassociations‚Äù configurations</a>): the processor in the OpenTelemetry Collector matches the IP address of the ‚Äúother side‚Äù of the TCP connection that sends the telemetry, and since each pod in a Kubernetes cluster has a unique IP address assigned to it, it can figure out from which pod it‚Äôs coming from. </p><p>And here, there is a caveat: the detection of the pod based on the IP address is known not to work reliably if you use a service mesh or some of the less conventional network setups. (Don‚Äôt ask us how we know. It was not fun to troubleshoot that.) So, better safe than sorry: ensure your telemetry is annotated with  and, if you are OK with proxying your telemetry through an OpenTelemetry Collector inside the cluster, have that fill most of the resource attributes mentioned in the remainder of this article for you using a <a target=\"_blank\" href=\"https://www.otelbin.io/s/249772c6c88ab31e77c168af6131df0248902bbf\">configuration like this</a>.</p><p>Pod UIDs are effectively unique across all your workloads. (And all Kubernetes workloads anywhere, ever.) But long, random strings of characters are not something we humans are good at remembering things by, or even searching for in lists.</p><p>So, in a similar way same way we set the  resource attribute, we can set the :</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>Did you know that the Kubernetes pod, i.e., one or more containers that share local networking and other resources like CPU and memory allotment and volumes, is called like that because:</p><ol><li>‚Äúpod‚Äù is the collective name of a group of whales</li><li>the logo of Docker, the original container runtime of Kubernetes, is a whale</li></ol><p>If your pod has more than one container, you are really going to need to know which of them is having issues. Usually, applications on Kubernetes have just one ‚Äúmain‚Äù container, running your application and may have one or more ‚Äúsidecars‚Äù, i.e., containers that have inside ancillary processes like log collectors or service mesh proxies. The <code></code> (see previous section) cannot tell containers apart (all your containers in the same pod share the same IP address and pod uid, among other things), so you should set the  yourself. It can be a bit toilsome, because you cannot do it generically using the Downward API the way we did for , but in the end is as simple as adding another entry to <code></code>:</p><p><em>A Kubernetes pod spec template snippet showing how to use the <code></code> environment variable to set the  resource attribute. Be sure to set it to the same value of the container name!</em></p><p>Note that  could be redundant: the  attribute is supposed to contain the same data, and there are detectors in most OpenTelemetry SDKs (see e.g. for <a target=\"_blank\" href=\"http://node.js/\">Node.js</a>), that can collect  for you by parsing, for example, the  metadata (<a target=\"_blank\" href=\"https://en.wikipedia.org/wiki/Cgroups\">cgroups</a> are one of the foundational Linux facilities for containers). However, it may not always be possible to add detectors to your containerized applications, especially when you are using sidecars from 3rd parties. But if they support the <code></code> environment variable, you can fill the gaps in resource attributes through the process environment.</p><p>The same way you are likely to have a  service in most applications (<code></code>) service, that service will likely be powered by a  deployment (<code></code>). Names in Kubernetes are unique only for resources of the same type (e.g., deployments) within the same Kubernetes namespace. If you want to avoid confusion and simplify your life aggregating data, set not only  but also , and use the UIDs to group.</p><p>After reading the previous paragraph, you might have wondered:</p><p>‚ÄúThe deployment name is unique inside the namespace, and the namespace name is unique inside the cluster. So why do I need unique identifiers anyhow?‚Äù</p><p>That logic works if you deploy your software in only one cluster. But that is seldom the case. Between development, testing, and various production clusters (which is a rather common type of layout), most organizations run the same software, at the same time, on  of Kubernetes clusters. And, to make matters more complicated, telling Kubernetes clusters apart is harder than it should (see next section).</p><p>The fun with identifiers is not over yet. Now let‚Äôs talk about some identifiers that . Specifically, the identifier of a specific Kubernetes cluster. In most Kubernetes setups, it literally does not exist. In other words, a<em> Kubernetes cluster has no own notion of identity</em>!</p><p>You surely have names for your clusters, but the name ‚Äúprod-eu-awesomesauce‚Äù that you define in your Kubernetes cluster management tool is more a matter of how you call the profile in  to connect to that cluster rather than some metadata you can find from within the cluster itself! (Your mileage may vary with specific setups and Kubernetes flavor; but in general, this is the case.) So, you should use the OpenTelemetry collector‚Äôs <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/resourceprocessor\"></a> to inject the value of  like this:</p><p><em>A snippet of the OpenTelemetry collector configuration to add the  resource attribute to all telemetry coming through.</em></p><p>Moreover, the same way most Kubernetes  attributes have matching  ones, so does  have a match in . The common practice in this case is to set  using as value the uid of the  namespace, which is pretty much the only namespace you are guaranteed to find in every Kubernetes cluster you will ever see (as it is the one that by default hosts the control plane components).</p><p>The  attribute, and its far lesser used  sibling, are really useful when investigating performance issues due to resource underprovisioning (pods on the same node fighting for resources like CPU or memory) or <a target=\"_blank\" href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/\">node-pressure evictions</a>. The only notable exception to this is when you run on AWS EKS on Fargate, where each pod, from the point of view of Kubernetes, runs on a dedicated node.</p><p>If you have already set up the <code></code> so that it can add resource attributes to your telemetry, it can also take care of  for you. Otherwise, setting the  attribute is pretty simple using Kubernetes‚Äô Downward API and ‚Äú<a target=\"_blank\" href=\"https://kubernetes.io/docs/tasks/inject-data-application/define-interdependent-environment-variables/\">dependent environment variables</a>‚Äù:</p><p><em>A Kubernetes pod spec template snippet showing how to use the Downward API together with the <code></code> environment variable to set the  resource attribute.</em></p><p>Resource attributes are a key ingredient to make your telemetry useful. In this blog post, we have looked at the OpenTelemetry resource semantic conventions for Kubernetes, and how you can ensure to always know which workload of which cluster is sending the errors.</p><p>In this post, we kept to the fundamental Kubernetes metadata your telemetry should have. There is, of course, a lot more to be said about OpenTelemetry semantic conventions, even just for resources on Kubernetes. For example, you will find more resource attributes specified in the <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/resource/k8s/\">Kubernetes resource semantic conventions</a>. Also, there are semantic conventions for <a target=\"_blank\" href=\"https://opentelemetry.io/docs/specs/semconv/system/k8s-metrics/\">metrics about Kubernetes</a>, which are, at the time of writing, in an experimental state (meaning: the convention is not declared stable, so it might still change in backwards-incompatible ways) and are based on what the OpenTelemetry Collector knows how to collect with various receivers like <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8sclusterreceiver\"></a> and <a target=\"_blank\" href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kubeletstatsreceiver\"></a>.</p><p>By the way, if you liked this article, you are probably going to love Ben's \"<a target=\"_self\" href=\"https://www.dash0.com/blog/top-10-opentelemetry-collector-components\">Top 10 OpenTelemetry Collector Components</a>\" blog post. It will likely give you a bunch more ideas about what you can do to ensure that your resource metadata is top-notch.</p><p>And if you want all the awesomeness of the resource metadata we covered in this post, but do none of the work to get them, give the <a target=\"_self\" href=\"https://www.dash0.com/documentation/dash0/dash0-kubernetes-operator\">Dash0 operator</a> a spin: it is open source, built on OpenTelemetry components with opinionation and an appliance-like philosophy (‚Äúit just works‚Ñ¢‚Äù), and under the hood it uses most of the techniques described in this post to get your telemetry annotated to the state of the art, with literally none of the toil from on your side.</p>","contentLength":10476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1it1u9k/opentelemetry_resource_attributes_best_practices/"},{"title":"Has anyone used Nginx Ingress controller with the AWS Load Balancer Controller service instead of the default service?","url":"https://www.reddit.com/r/kubernetes/comments/1it16ic/has_anyone_used_nginx_ingress_controller_with_the/","date":1739955716,"author":"/u/lynxerious","guid":7018,"unread":true,"content":"<p>So the nginx-ingress-controller creates a LoadBalancer service by default, this load balancer is created by the in-tree controller managed by EKS. And I want to manage the load balancer with the AWS Load Balancer Controller instead, using a custom service, it has more features than the default LoadBalancer service.</p><p>After I had successfully created the new load balancer, route the service to the nginx-ingress-controller pods, the target groups pods IPs are all correct, and change all domains DNS records to the new load balancer DNS name, change the publishService in the nginx pods to the new service. It was sure this has worked properly.</p><p>Then I tried to disable the default service of the nginx-ingress-controller, voila, everything went down, and I had to re-enable it quickly, after I checked the Monitoring sections of the load balancers, the old ones still got the traffic, while the old ones barely got any. This just doesn't make sense to me. I ping all domains and it goes to the correct IP of the new load balancer, yet the old one still got traffics and I don't even know why, could it be DNS records cache? But I don't think it would be cached for that long since it's been 2 days already.</p><p>Edit: I found out something really weird: dig <a href=\"http://domain.com\">domain.com</a> -&gt; new load balancer IP dig <a href=\"https://domain.com\">https://domain.com</a> -&gt; old load balancer IP I'm investigating why here.</p>","contentLength":1359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Khronoscope Pre-Alpha ‚Äì A New Way to Explore Your Kubernetes Cluster Over Time","url":"https://www.reddit.com/r/kubernetes/comments/1isz5gq/introducing_khronoscope_prealpha_a_new_way_to/","date":1739947088,"author":"/u/HoyleHoyle","guid":5616,"unread":true,"content":"<p>I'm excited to share , a  tool designed to give you a  view of your Kubernetes cluster. Inspired by k9s, it lets you pause, rewind, and fast-forward through historical states, making it easier to debug issues, analyze performance, and understand how your cluster evolves.</p><ul><li>Connects to your Kubernetes cluster and tracks resource states over time</li><li>Provides a  to navigate past events</li><li>Lets you <strong>filter, inspect, and interact</strong> with resources dynamically</li><li>Supports <strong>log collection and playback</strong> for deeper analysis</li></ul><p>üìñ <strong>Debugging the Past with Khronoscope</strong></p><p>Imagine inspecting your Kubernetes cluster when you notice something strange‚Äîa deployment with flapping pods. They start, crash, restart. Something‚Äôs off.</p><p>You pause the cluster state and check related resources. Nothing obvious. Rewinding a few minutes, you see the pods failing right after startup. Fast-forwarding, you mark one to start collecting logs. More crashes. Rewinding again, you inspect the logs just before failure‚Äîeach pod dies trying to connect to a missing service.</p><p>Jumping to another namespace, you spot the issue: a critical infrastructure pod failed to start earlier. A quick fix, a restart, and everything stabilizes.</p><p>With Khronoscope‚Äôs ability to navigate through time, track key logs, and inspect past states, you solve in minutes what could‚Äôve taken hours.</p><p>This is an , and I‚Äôm looking for  from anyone willing to try it out. I‚Äôd love to hear what works, what doesn‚Äôt, and how it could be improved.</p><pre><code>brew tap hoyle1974/homebrew-tap brew install khronoscope </code></pre><pre><code>git clone https://github.com/hoyle1974/khronoscope.git cd khronoscope go run cmd/khronoscope/main.go </code></pre>","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KubeVPN: Revolutionizing Kubernetes Local Development","url":"https://www.reddit.com/r/kubernetes/comments/1isxgo2/kubevpn_revolutionizing_kubernetes_local/","date":1739940995,"author":"/u/HamsterTall8168","guid":5599,"unread":true,"content":"<p>In the Kubernetes era, developers face a critical conflict between  and <strong>local development agility</strong>. Traditional workflows force developers to:</p><ol><li>Suffer frequent / operations</li><li>Set up mini Kubernetes clusters locally (e.g., minikube)</li><li>Risk disrupting shared dev environments</li></ol><p>KubeVPN solves this through <strong>cloud-native network tunneling</strong>, seamlessly extending Kubernetes cluster networks to local machines with three breakthroughs:</p><ul><li>üöÄ : Access cluster services without code changes</li><li>üíª <strong>Real-Environment Debugging</strong>: Debug cloud services in local IDEs</li><li>üîÑ <strong>Bidirectional Traffic Control</strong>: Route specific traffic to local or cloud</li></ul><h3>1. Direct Cluster Networking</h3><ul><li>‚úÖ Service name access (e.g., )</li><li>‚úÖ Native Kubernetes DNS resolution</li></ul><p><code>shell ‚ûú curl productpage:9080 # Direct cluster access &lt;!DOCTYPE html&gt; &lt;html&gt;...&lt;/html&gt; </code></p><h3>2. Smart Traffic Interception</h3><p>Precision routing via header conditions:</p><p><code>bash kubevpn proxy deployment/productpage --headers user=dev-team </code></p><ul><li>Requests with  ‚Üí Local service</li><li>Others ‚Üí Original cluster handling</li></ul><p>Connect two clusters simultaneously:</p><p><code>bash kubevpn connect -n dev --kubeconfig ~/.kube/cluster1 # Primary kubevpn connect -n prod --kubeconfig ~/.kube/cluster2 --lite # Secondary </code></p><h3>4. Local Containerized Dev</h3><p>Clone cloud pods to local Docker:</p><p><code>bash kubevpn dev deployment/authors --entrypoint sh </code></p><p>Launched containers feature:</p><ul><li>üåê Identical network namespace</li><li>‚öôÔ∏è Matching environment variables</li></ul><p>KubeVPN's three-layer architecture:</p><table><thead><tr></tr></thead><tbody><tr><td>Cluster-side interception</td><td>MutatingWebhook + iptables</td></tr><tr><td>Secure local-cluster channel</td></tr><tr></tr></tbody></table><p><code>mermaid graph TD Local[Local Machine] --&gt;|Encrypted Tunnel| Tunnel[VPN Gateway] Tunnel --&gt;|Service Discovery| K8sAPI[Kubernetes API] Tunnel --&gt;|Traffic Proxy| Pod[Workload Pods] subgraph K8s Cluster K8sAPI --&gt; TrafficManager[Traffic Manager] TrafficManager --&gt; Pod end </code></p><p>100QPS load test results:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>KubeVPN outperforms alternatives in overhead control.</p><p>kubectl krew install kubevpn/kubevpn ```</p><p><code>bash kubevpn connect --namespace dev </code></p><p>kubevpn proxy deployment/frontend --headers x-debug=true ```</p><p><code>bash curl -H \"x-debug: true\" frontend.dev.svc/cluster-api </code></p><p>KubeVPN's growing toolkit:</p><ul><li>üîå : Visual traffic management</li><li>üß© : Automated testing/deployment</li><li>üìä : Real-time network metrics</li></ul><p>Join developer community:</p><p>With KubeVPN, developers finally enjoy cloud-native debugging while sipping coffee ‚òïÔ∏èüöÄ</p>","contentLength":2282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best approach to manifests/infra?","url":"https://www.reddit.com/r/kubernetes/comments/1isql4s/best_approach_to_manifestsinfra/","date":1739921216,"author":"/u/RespectNo9085","guid":4474,"unread":true,"content":"<p>I've been provisioning various Kube clusters throughout the years, and now I\"m about to start a new project. </p><p>To me the best practice is to have a repo for the infrastructure using Terraform/Open Tofu, in this repo I usually set conditionals to provision either a Minikube for local or an EKS for prod, </p><p>Then I would create another repo to put together all cross-cutting concerns as a helm chart. That means I will use Grafana, Tempo, Vault Helm Charts and then I will package them in to one 'shared infrastructure' helm chart which is then applied to the clusters. </p><p>Each microservice will have its own helm chart that is generated on push to master and serverd on GIthub packages, there is also a dev manifest where people update the chart version for their microservice. The dev manifest has all they need to run the cluster, all the services.</p><p>The problem here is that sometimes I want to add a new technology to the cluster, for example recently I wanted to add the API gateway, Vault, Cillium or some other time I wanted to add a Mattermost instance, and some of these don't have proper helm charts. </p><p>Most of their instructions are simple cases where you apply a manifest from a URL into the cluster and that's no way to provision a cluster, because if I want to change things in the future, then should I apply again with a new values.yaml ? not fun, I like to see, understand and control what is going into my cluster. </p><p>So the question is, is the only option to read those manifest and create my own Helm charts? should I even Helm? is there a better approach? any opinion is appreciated. </p>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Configuration Languages (config DSLs) solve configuration complexity?","url":"https://www.reddit.com/r/kubernetes/comments/1isph9t/can_configuration_languages_config_dsls_solve/","date":1739918543,"author":"/u/wineandcode","guid":4448,"unread":true,"content":"<p>Configuration languages are not the best solution to configuration complexity. Each language has its pros and cons, but none moves the needle much. In this post, Brian Grant explores what they are. Why would someone create a new one? And do they reduce configuration complexity?</p>","contentLength":278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to stop SSL-Certs from being deleted when uninstalling a helm deployment","url":"https://www.reddit.com/r/kubernetes/comments/1ismivb/how_to_stop_sslcerts_from_being_deleted_when/","date":1739909981,"author":"/u/Eldiabolo18","guid":4432,"unread":true,"content":"<p>when trying a helm chart I often have to reinstall it a couple of times until it works the way I want it. If that Helm-Chart has an ingress and generates a SSL-Cert from Letsencrypt via Cert-Manager, the cert also gets deleted and regenerated. </p><p>I just ran into the issue, that I redployed the helm chart more than 5 times in 24 (48?) hrs for the same domain, so letsencrypt blocks the request. </p><p>Is there any way to stop the SSL-Certs from being deleted when in uninstall a helm chart, so it can be reused for the next deployment? Or is there any other way around this? </p>","contentLength":567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GPU nodes on-premise","url":"https://www.reddit.com/r/kubernetes/comments/1isk9ne/gpu_nodes_onpremise/","date":1739904616,"author":"/u/blu-base","guid":4342,"unread":true,"content":"<p>My company acquired a few GPU nodes with a couple of nvidia h100 cards each. The app team is likely wanting to use nvidias Trition interference server. For this purpose we need to operate kubernetes on those nodes. I am now wondering whether to maintain native kubernetes on these nodes. Or to use some suite, such as open shift or rancher. Running natively means a lot of work on reinventing the wheel, having an operation documentation/ process. However, using suites could mean an overhead of complexity relative to the few number of local nodes.</p><p>I am not experienced with doing the admin side of operating an on-premise kubernetes. Have you any recommendations how to run such GPU focused clusters?</p>","contentLength":701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Powered by Docker: Streamlining Engineering Operations as a Platform Engineer","url":"https://www.docker.com/blog/streamlining-engineering-operations-as-platform-engineer/","date":1739904013,"author":"Neal Patel","guid":4335,"unread":true,"content":"<p><em>The Powered by Docker is a series of blog posts featuring use cases and success stories from Docker partners and practitioners. This story was contributed by Neal Patel from Siimpl.io. Neal has more than ten years of experience developing software and is a Docker Captain.</em></p><p>As a platform engineer at a mid-size startup, I‚Äôm responsible for identifying bottlenecks and developing solutions to streamline engineering operations to keep up with the velocity and scale of the engineering organization. In this post, I outline some of the challenges we faced with one of our clients, how we addressed them, and provide guides on how to tackle these challenges at your company.</p><p>One of our clients faced critical engineering challenges, including poor synchronization between development and CI/CD environments, slow incident response due to inadequate rollback mechanisms, and fragmented telemetry tools that delayed issue resolution. Siimpl implemented strategic solutions to enhance development efficiency, improve system reliability, and streamline observability, turning obstacles into opportunities for growth.</p><p><strong>Let‚Äôs walk through the primary challenges we encountered.</strong></p><h3>Inefficient development and deployment</h3><ul><li> We lacked parity between developer tooling and CI/CD tooling, which made it difficult for engineers to test changes confidently.</li><li> We needed to ensure consistent environments across development, testing, and production.</li></ul><h3>Unreliable incident response</h3><ul><li> If a rollback was necessary, we did not have the proper infrastructure to accomplish this efficiently.</li><li> We wanted to revert to stable versions in case of deployment issues easily.</li></ul><h3>Lack of comprehensive telemetry</h3><ul><li> Our SRE team created tooling to simplify collecting and publishing telemetry, but distribution and upgradability were poor. Also, we found adoption to be extremely low.</li><li> We needed to standardize how we configure telemetry collection, and simplify the configuration of auto-instrumentation libraries so the developer experience is turnkey.</li></ul><h2>Solution: Efficient development and deployment</h2><h3>CI/CD configuration with self-hosted GitHub runners and Docker Buildx</h3><p>We had a requirement for multi-architecture support (arm64/amd64), which we initially implemented in CI/CD with Docker Buildx and QEMU. However, we noticed an extreme dip in performance due to the emulated architecture build times.</p><p>We were able to reduce build times by almost 90% by ditching QEMU (emulated builds), and targeting arm64 and amd64 self-hosted runners. This gave us the advantage of blazing-fast native architecture builds, but still allowed us to support multi-arch by publishing the manifest after-the-fact.&nbsp;</p><p>If you‚Äôd like to deploy this yourself, there‚Äôs a guide in the <a href=\"https://github.com/siimpl/multi-architecture-cicd?tab=readme-ov-file#multi-architecture-cicd\" rel=\"nofollow noopener\" target=\"_blank\">README.md</a>.</p><p>This project uses the following tools:</p><ul><li>A managed container orchestration service like Elastic Kubernetes Service (EKS), Azure Kubernetes Service (AKS), or&nbsp; Google Kubernetes Engine (GKE)</li></ul><p>Because this project uses industry-standard tooling like Terraform, Kubernetes, and Helm, it can be easily adapted to any CI/CD or cloud solution you need.</p><p>The secret sauce of this solution is provisioning the self-hosted runners in a way that allows our CI/CD to specify which architecture to execute the build on.</p><p>The first step is to provision two node pools ‚Äî an amd64 node pool and an arm64 node pool, which can be found in the <a href=\"https://github.com/siimpl/multi-architecture-cicd/blob/main/aks.tf\" rel=\"nofollow noopener\" target=\"_blank\">aks.tf</a>. In this example, the node_count is fixed at 1 for both node pools but for better scalability/flexibility you can also enable autoscaling for a dynamic pool.</p><pre><code>resource \"azurerm_kubernetes_cluster_node_pool\" \"amd64\" {\n  name                  = \"amd64pool\"\n  kubernetes_cluster_id = azurerm_kubernetes_cluster.cicd.id\n  vm_size               = \"Standard_DS2_v2\" # AMD-based instance\n  node_count            = 1\n  os_type               = \"Linux\"\n  tags = {\n    environment = \"dev\"\n  }\n}\n\nresource \"azurerm_kubernetes_cluster_node_pool\" \"arm64\" {\n  name                  = \"arm64pool\"\n  kubernetes_cluster_id = azurerm_kubernetes_cluster.cicd.id\n  vm_size               = \"Standard_D4ps_v5\" # ARM-based instance\n  node_count            = 1\n  os_type               = \"Linux\"\n  tags = {\n    environment = \"dev\"\n  }\n}\n</code></pre><p>Next, we need to update the self-hosted runners‚Äô <a href=\"https://github.com/siimpl/multi-architecture-cicd/blob/main/templatefiles/arc_runner_values.yaml#L196\" rel=\"nofollow noopener\" target=\"_blank\">values.yaml</a> to have a configurable nodeSelector. This will allow us to deploy one runner scale set to the <a href=\"https://github.com/siimpl/multi-architecture-cicd/blob/main/runners.tf#L40\" rel=\"nofollow noopener\" target=\"_blank\">arm64pool</a> and one to the <a href=\"https://github.com/siimpl/multi-architecture-cicd/blob/main/runners.tf#L22\" rel=\"nofollow noopener\" target=\"_blank\">amd64pool</a>.</p><p>Once the Terraform resources are successfully created, the runners should be registered to the organization or repository you specified in the GitHub config URL. We can now update the REGISTRY values for the <a href=\"https://github.com/siimpl/multi-architecture-cicd/blob/main/.github/workflows/emulated-build.yml#L16\" rel=\"nofollow noopener\" target=\"_blank\">emulated-build</a> and the <a href=\"https://github.com/siimpl/multi-architecture-cicd/blob/main/.github/workflows/native-build.yml#L10\" rel=\"nofollow noopener\" target=\"_blank\">native-build</a>.</p><p>After creating a pull request with those changes, navigate to the  tab to witness the results.</p><p>You should see two jobs kick off, one using the emulated build path with QEMU, and the other using the self-hosted runners for native node builds. Depending on cache hits or the Dockerfile being built, the performance improvements can be up to 90%. Even with this substantial improvement, utilizing Docker Build Cloud can improve performance 95%. More importantly, you can reap the benefits during development builds! Take a look at the <a href=\"https://github.com/siimpl/multi-architecture-cicd/blob/main/.github/workflows/docker-build-cloud.yml\" rel=\"nofollow noopener\" target=\"_blank\">docker-build-cloud.yml</a> workflow for more details. All you need is a Docker Build Cloud subscription and a cloud driver to take advantage of the improved pipeline.</p><p>5. Create a PR to validate pipelines</p><h3>Reliable Incident Response</h3><p><strong>Leveraging SemVer Tagged Containers for Easy Rollback</strong></p><p>Recognizing that deployment issues can arise unexpectedly, we needed a mechanism to quickly and reliably rollback production deployments. Below is an example workflow for properly rolling back a deployment based on the tagging strategy we implemented above.</p><ol><li><ul><li>In case of a problematic build, deployment was rolled back to a previous stable version using the tagged images.</li><li>AWS CLI commands were used to update ECS services with the desired image tag:</li></ul></li></ol><pre><code>on:\n  workflow_call:\n    inputs:\n      image-version:\n        required: true\n        type: string\njobs:\n  rollback:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      context: read\n    steps:\n     - name: Rollback to previous version\n       run: |\n         aws ecs update-service --cluster my-cluster --service my-service --force-new-deployment --image ${{ secrets.REGISTRY }}/myapp:${{ inputs.image-version }}</code></pre><p><strong>Configuring Sidecar Containers in ECS for Aggregating/Publishing Telemetry Data (OTEL)</strong></p><p>As we adopted a OpenTelemetry to standardize observability, we quickly realized that adoption was one of the toughest hurdles. As a team, we decided to bake in as much configuration as possible into the infrastructure (Terraform modules) so that we could easily distribute and maintain observability instrumentation.</p><ol><li><ul><li>Sidecar containers were defined in the ECS task definitions to run OpenTelemetry collectors.</li><li>The collectors were configured to aggregate and publish telemetry data from the application containers.</li></ul></li></ol><pre><code>{\n  \"containerDefinitions\": [\n    {\n      \"name\": \"myapp\",\n      \"image\": \"myapp:1.0.0\",\n      \"essential\": true,\n      \"portMappings\": [{ \"containerPort\": 8080 }]\n    },\n    {\n      \"name\": \"otel-collector\",\n      \"image\": \"otel/opentelemetry-collector:latest\",\n      \"essential\": false,\n      \"portMappings\": [{ \"containerPort\": 4317 }],\n      \"environment\": [\n        { \"name\": \"OTEL_RESOURCE_ATTRIBUTES\", \"value\": \"service.name=myapp\" }\n      ]\n    }\n  ],\n  \"family\": \"my-task\"\n}</code></pre><p><strong>Configuring Multi-Stage Dockerfiles for OpenTelemetry Auto-Instrumentation Libraries (Node.js)</strong></p><p>At the application level, configuring the auto-instrumentation posed a challenge since most applications varied in their build process. By leveraging multi-stage Dockerfiles, we were able to help standardize the way we initialized the auto-instrumentation libraries across microservices. We were primarily a nodejs shop, so below is an example Dockerfile for that.</p><ol><li><ul><li>The Dockerfile is divided into stages to separate the build environment from the final runtime environment, ensuring a clean and efficient image.</li><li>OpenTelemetry libraries are installed in the build stage and copied to the runtime stage:</li></ul></li></ol><pre><code># Stage 1: Build stage\nFROM node:20 AS build\nWORKDIR /app\nCOPY package.json package-lock.json ./\n# package.json defines otel libs (ex. @opentelemetry/node @opentelemetry/tracing)\nRUN npm install\nCOPY . .\nRUN npm run build\n\n# Stage 2: Runtime stage\nFROM node:20\nWORKDIR /app\nCOPY --from=build /app /app\nCMD [\"node\", \"dist/index.js\"]</code></pre><p>By addressing these challenges we were able to reduce build times by , which alone dropped our DORA metrics for  and  by With the rollback strategy and telemetry changes, we were able to reduce our Mean time to Detect (MTTD) and Mean time to resolve (MTTR) by . We believe that it could get to  with tuning of alerts and the addition of runbooks (automated and manual).</p><ol><li><strong>Enhanced Development Efficiency:</strong> Consistent environments across development, testing, and production stages sped up the development process, and roughly 90% faster build times with the native architecture solution.</li><li> Quick and efficient rollbacks minimized downtime and maintained system integrity.</li><li> Sidecar containers enabled detailed monitoring of system health and security without impacting application performance, and was baked right into the infrastructure developers were deploying. Auto-instrumentation of the application code was simplified drastically with the adoption of our Dockerfiles.</li></ol><h3>Siimpl: Transforming Enterprises with Cloud-First Solutions</h3><p>With Docker at the core, Siimpl.io‚Äôs solutions demonstrate how teams can build faster, more reliable, and scalable systems. Whether you‚Äôre optimizing CI/CD pipelines, enhancing telemetry, or ensuring secure rollbacks, Docker provides the foundation for success. Try Docker today to unlock new levels of developer productivity and operational efficiency.</p>","contentLength":9756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Install QEMU for Multi-Platform Builds in a Docker-in-Docker (DinD) Container","url":"https://blog.devops.dev/how-to-install-qemu-for-multi-platform-builds-in-a-docker-in-docker-dind-container-83eae2590ac8?source=rss----33f8b2d9a328---4","date":1739901820,"author":"Jaecom","guid":4367,"unread":true,"content":"<p>I recently encountered a platform error when building Docker images on my Raspberry Pi which uses arm64 architecture. The images couldn't be run on my EC2 container, which runs Ubuntu on amd64. To resolve this, I needed to build images for multiple architectures. The solution? Installing QEMU in my Docker-in-Docker (DinD) container for cross-platform builds.</p><p><strong>Step 1: Enter the DinD Docker Container Interactive Shell</strong></p><p>First, open an interactive shell inside the DinD container (or any host using&nbsp;docker):</p><pre>sudo docker exec -it dind-container sh</pre><p><strong>Step 2: Install QEMU and Enable Cross-Platform Builds</strong></p><pre>docker run --privileged --rm tonistiigi/binfmt --install all</pre><p>We use the tonistiigi/binfmt image to install QEMU and set up cross-platform builds. This image simplifies the process of configuring your host (or container) to run binaries built for different architectures using&nbsp;QEMU.</p><p><strong>Step 3: Verify Installation</strong></p><p>Once the installation completes, you‚Äôll see a list of supported architectures and installed emulators:</p><pre>{  \"supported\": [    \"linux/amd64\",    \"linux/ppc64le\",    \"linux/386\",    \"linux/mips64\",    \"linux/arm/v6\"  \"emulators\": [    \"qemu-i386\",    \"qemu-mips64el\",    \"qemu-riscv64\",    \"qemu-x86_64\"}</pre><p>This confirms that QEMU is ready to handle builds for architectures like linux/arm64 and linux/amd64, enabling cross-platform builds.</p><p><strong>Step 4: Building Multi-Architecture Images</strong></p><p>Now that QEMU is set up, we can use Docker‚Äôs buildx tool to build images for different architectures. The following command builds an image for the amd64 architecture, even though you're running on an arm64&nbsp;machine:</p><pre>docker buildx build --platform linux/amd64 -t MyDockerfile .</pre><p>With QEMU installed and configured in your Docker environment, you can easily build images for different architectures without changing your development setup. Whether you‚Äôre running on a Raspberry Pi or any other arm64 platform, this solution allows you to create platform compatible images effortlessly. Try this out for your own multi-platform projects and simplify your Docker&nbsp;builds!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=83eae2590ac8\" width=\"1\" height=\"1\" alt=\"\">","contentLength":2048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RTSP Proxy Setup using MediaMTX","url":"https://blog.devops.dev/rtsp-proxy-setup-using-mediamtx-3f4dff41446f?source=rss----33f8b2d9a328---4","date":1739901807,"author":"Basil Shaji","guid":4366,"unread":true,"content":"<p>Description&nbsp;: This project sets up an RTSP Proxy using MediaMTX to relay multiple RTSP streams over a network. The proxy enables efficient access to multiple IP camera streams while enforcing TCP-based transport for better stability.</p><p>Organization: <strong>Karunya Institute of Technology and&nbsp;Sciences</strong></p><ul><li>Proxy multiple RTSP streams (up to 4 streams configured)</li><li>Enforces  for reliability</li><li>Supports <strong>authentication for client&nbsp;access</strong></li><li>Allows on-demand streaming to reduce resource&nbsp;usage</li></ul><ul><li> installed on the host&nbsp;machine</li><li>ffmpeg package installed for verification of the&nbsp;streams.</li><li>Access to the RTSP source&nbsp;streams</li><li>Firewall rules allowing RTSP&nbsp;traffic</li></ul><h3>1. Make a Project Directory</h3><pre>mkdir mediamtx-rtsp-proxycd mediamtx-rtsp-proxy</pre><h3>2. Pull the Mediamtx Docker Container.</h3><pre>docker pull bluenviron/mediamtx</pre><h3>3. Create a Custom Configuration File</h3><p>Create a mediamtx.yml file in the project directory:</p><pre>rtsp: yesreadTimeout: 30s  all:    source: &lt;rtsp_link_1&gt;    sourceOnDemand: yes    readPass: proxypass    source: &lt;rtsp_link_2&gt;    sourceOnDemand: yes    readPass: proxypass    source: &lt;rtsp_link_3&gt;    sourceOnDemand: yes    readPass: proxypass    source: &lt;rtsp_link_4&gt;    sourceOnDemand: yes    readPass: proxypass</pre><h3>4. Create a Custom Docker Compose&nbsp;File</h3><p>Create a docker-compose.yml file in the project directory:</p><pre>version: '3'services:    image: bluenviron/mediamtx:latest    ports:      - \"8888:8888\"    volumes:<p>      - ./mediamtx.yml:/mediamtx.yml</p></pre><h3>5. Run MediaMTX Docker Container</h3><pre>Ouput:----------------------------------------------------------------------------- ‚úî Network mediamtx-rtsp-proxy_default  Created                         0.3s <p> ‚úî Container mediamtx                   Started                         2.2s</p></pre><h3>1. Streaming the Proxy&nbsp;Streams</h3><p>Once the proxy is running, clients can access the streams&nbsp;via:</p><pre>ffplay rtsp://proxyuser:proxypass@&lt;server_ip&gt;:8554/stream1ffplay rtsp://proxyuser:proxypass@&lt;server_ip&gt;:8554/stream2<p>ffplay rtsp://proxyuser:proxypass@&lt;server_ip&gt;:8554/stream3</p>ffplay rtsp://proxyuser:proxypass@&lt;server_ip&gt;:8554/stream4</pre><p>Replace server_ip with the IP of the machine running the&nbsp;proxy.</p><ul><li>Ensure the source RTSP URLs are correct and accessible.</li><li>Check firewall settings if streams fail to&nbsp;load.</li><li>Use docker logs mediamtx for debugging errors.</li><li>Ensure RTSP traffic is allowed on port&nbsp;.</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3f4dff41446f\" width=\"1\" height=\"1\" alt=\"\">","contentLength":2260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes CronJob + Sidecar: A Love Story Gone Wrong (And How to Fix It)","url":"https://blog.devops.dev/kubernetes-cronjob-sidecar-a-love-story-gone-wrong-and-how-to-fix-it-ae9fd1e95a9a?source=rss----33f8b2d9a328---4","date":1739901802,"author":"Vitaly Bykov","guid":4365,"unread":true,"content":"<p>I work at a large product company with a sprawling Kubernetes infrastructure. We run thousands of workloads, process massive amounts of data, and rely on automation to keep things running smoothly. So when we needed to execute a scheduled task in Kubernetes, using a CronJob seemed like a no-brainer.</p><p>At first, everything worked perfectly. Our CronJob fired up a Job, the task ran, completed, and exited&nbsp;cleanly.</p><p>But then, as always, the requirements changed:</p><p>‚Ä¢\tThe script was opening too many database connections, so we added an SQL proxy to optimize connection pooling.</p><p>‚Ä¢\tThe task became mission-critical, meaning we needed real-time monitoring to ensure failures wouldn‚Äôt go unnoticed.</p><p>‚Ä¢\tWe added sidecar containers for these enhancements‚Ä¶ and that‚Äôs when everything broke.</p><p><strong>The Problem: CronJob Stopped&nbsp;Running</strong></p><p>Kubernetes CronJobs work by creating Jobs, which spin up Pods to execute the actual work. A Job is considered complete only when all containers in the pod reach the  state.</p><p>Our main container was completing successfully, transitioning to .</p><p>But our sidecar containers ‚Äì SQL Proxy and Monitoring ‚Äì were running indefinitely.</p><p>Since they never exited, the Job never finished, and the CronJob never scheduled the next execution.</p><p><strong>Why We Needed These Sidecars in the First&nbsp;Place</strong></p><p>1.\tSQL Proxy: Our script was making hundreds of direct DB connections, overwhelming the database. Adding a SQL proxy helped pool connections, reducing the&nbsp;load.</p><p>2.\tMonitoring: The job wasn‚Äôt just some background task ‚Äì it was mission-critical. If it failed silently, key business processes would break. We needed real-time logs and metrics to ensure it was running correctly.</p><p>So removing the sidecars wasn‚Äôt an option. Instead, we needed to teach them when to&nbsp;exit.</p><p><strong>The Fix: Graceful Shutdown via File Signaling</strong></p><p>We needed a way to tell the sidecars:</p><p>‚ÄúHey, the main job is done. Time to shut&nbsp;down.‚Äù</p><p>1.\tThe main container creates a file (/shared-data/done) in a shared volume at&nbsp;startup.</p><p>2.\tThe sidecars monitor the file using inotifywait.</p><p>3.\tWhen the main job finishes, it deletes the&nbsp;file.</p><p>4.\tThe sidecars detect this, terminate gracefully using SIGTERM, and&nbsp;exit.</p><p>5.\tThe Job completes, and the CronJob can schedule the next&nbsp;run.</p><p>This issue has been around for a while, and various workarounds have been proposed. There are even specialized projects like <a href=\"https://github.com/cropse/K8S-job-sidecar-terminator\">K8S Job Sidecar Terminator</a>, which help manage sidecar shutdown for Kubernetes Jobs.</p><p>However, our approach is much simpler and doesn‚Äôt require any additional components ‚Äì just a shared volume and a simple script inside the containers.</p><p><strong>Implementation: The Helm&nbsp;Chart</strong></p><p>We‚Äôll use an emptyDir volume so all containers in the pod can access the same&nbsp;file.</p><pre>volumes:  - name: shared-data</pre><p>2. The Main Job Container</p><p>Our main job script&nbsp;will:</p><p>‚Ä¢\tExecute the actual&nbsp;task.</p><p>‚Ä¢ Create /pod/terminated (or /pod/error) when it have finished.</p><pre>containers:  - name: main-job<p>    image: my-job-image:latest</p>    command:      - -c        trap '[ $? -eq 0 ] &amp;&amp; touch /pod/terminated || touch /pod/error' EXIT;<p>        while [ ! -S /tmp/proxysql.sock ]; do sleep 1; done;  # Check sidecar service availability</p>        ./run-my-task.sh      - name: shared-data</pre><p>3. Sidecar Containers (SQL Proxy or Monitoring)</p><p>We‚Äôll use the same graceful shutdown approach for&nbsp;both.</p><pre>  - name: proxysql-sidecar    image: proxysql/proxysql:latest      - /bin/sh      - |<p>        proxysql -f -c \"$CONFIG_PATH\" &amp; CHILD_PID=$!</p>        (while true; do if [ -f \"/pod/terminated\" ] || [ -f \"/pod/error\" ]; then kill $CHILD_PID; echo \"Killed $CHILD_PID because the main container terminated.\"; fi; sleep 1; done) &amp;        if [ -f \"/pod/error\" ]; then<p>          echo \"Job completed with error. Exiting...\";</p>          exit 1;<p>        elif [ -f \"/pod/terminated\" ]; then</p>          echo \"Job completed. Exiting...\";        fi      - name: shared-data</pre><p>We just need to deploy two instances of this container, one as SQL Proxy and the other as Monitoring, using different images or configurations if&nbsp;needed.</p><p>2. The sidecar starts the main process in background and termination file waiting loop in foreground.</p><p>3. The main job finishes, creates /pod/terminated (or /pod/error), and&nbsp;exits.</p><p>4. The sidecars detect the termination file, catch the signal, and exit&nbsp;cleanly.</p><p>5.\tThe Job completes, and the CronJob schedules the next&nbsp;run.</p><p>No more stuck Jobs, no more missing CronJob executions.</p><p>Adding sidecars to Jobs and CronJobs can be tricky, but with a bit of clever process signaling, it‚Äôs totally manageable.</p><p>If your CronJob mysteriously stops running, check if your sidecars are stuck in Running state. If they are, they‚Äôre the&nbsp;problem.</p><p>This approach ‚Äì file signaling + SIGTERM traps ‚Äì is a simple, reliable&nbsp;fix.</p><p>For alternative solutions and further discussion, check out these resources:</p><p>Hope this helps! Now go forth and deploy with confidence. üöÄ</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ae9fd1e95a9a\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Fixed My Slow Nginx + Gunicorn Setup ‚Äî Here‚Äôs How It Became 3X Faster","url":"https://blog.devops.dev/i-fixed-my-slow-nginx-gunicorn-setup-heres-how-it-became-3x-faster-1c324eb9bbb5?source=rss----33f8b2d9a328---4","date":1739901767,"author":"Chimaobi|Roland Okeke","guid":4364,"unread":true,"content":"<h3>I Fixed My Slow Nginx + Gunicorn Setup‚Ää‚Äî‚ÄäHere‚Äôs How It Became 3X&nbsp;Faster</h3><p>Before we dive in, a quick disclaimer‚Ää‚Äî‚ÄäI‚Äôm not a professional DevOps engineer, just a developer who needed to fix a slow server. After spending hours tweaking, I want to share my experience so you don‚Äôt waste hours on something that should take five&nbsp;minutes.</p><p>Now, here‚Äôs my setup: I‚Äôm running a Django app on a CentOS 9 EC2 Server. For my web server, I chose Nginx for three key reasons: <strong>performance, load balancing, and ease of configuration</strong>, along with my familiarity with it. I also used Gunicorn as the proxy server to manage the Django services.</p><p>While testing, I noticed something odd‚Ää‚Äî‚Ääa  between running the Django development server with Nginx routing traffic to it versus running it through Gunicorn.</p><p>My first thought was that the application might be too heavy, so I enabled  in Gunicorn. This preloads the app before forking worker processes, which can improve memory efficiency but doesn‚Äôt necessarily boost request speed. I also <strong>increased the number of workers from 2 to 4</strong> to allow more concurrent requests and set the log level to  to reduce logging overhead.</p><p>Despite these tweaks, the requests were still , suggesting the bottleneck might be elsewhere‚Ää‚Äî‚Ääpossibly due to Gunicorn‚Äôs default synchronous workers, Nginx‚Äôs configuration, or database/I/O delays.</p><p>Below is my Nginx and Gunicorn configuration. By default, the Nginx config is located&nbsp;in:</p><p>/etc/nginx/conf.d/filename.conf</p><pre>server {    listen 80;<p>    server_name **.**.***.***;</p><p>    location = /favicon.ico { access_log off; log_not_found off; }</p>    location /static/ {<p>        root /home/ec2-user/src/g*****/media;</p>    }        proxy_pass http://localhost:8000;<p>        proxy_set_header Host $host;</p>        proxy_set_header X-Real-IP $remote_addr;<p>        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</p>        proxy_set_header X-Forwarded-Proto $scheme;}</pre><p>I created a service file to run Gunicorn as a <strong>detached background service</strong>, ensuring it starts automatically when the server restarts and can handle incoming requests reliably.</p><pre>[Unit]Description=gunicorn daemonUser=ec2-userWorkingDirectory=/home/ec2-user/src/g*****<p>ExecStart=/home/ec2-user/src/g*****/vone/bin/gunicorn \\</p> --workers 4 \\ --bind 0.0.0.0:8000 \\<p> --access-logfile /home/ec2-user/src/g*****/access.log \\</p> --error-logfile /home/ec2-user/src/g*****/error.log \\[Install]<p>WantedBy=multi-user.target</p></pre><p>After hours of debugging and tracing, I ruled out Nginx as the direct cause of the issue. The real culprit turned out to be a <strong>misconfigured binding address</strong> in Gunicorn.</p><p>in my Gunicorn config and a similar address in&nbsp;Nginx:</p><pre>proxy_pass http://localhost:8000;</pre><p>This single misconfiguration drastically slowed down my service. After a deep dive, I learned that this setup forced the server to communicate over the  instead of using a more efficient  or loopback interface&nbsp;:-/</p><p>To fix the issue, I followed :</p><h3>1. Create a Gunicorn Socket&nbsp;Service</h3><p>First, I created a  to allow Gunicorn to communicate via a Unix&nbsp;socket:</p><pre>sudo nano /etc/systemd/system/gunicorn.socket</pre><pre>[Unit]Description=gunicorn socketListenStream=/run/gunicorn.sockWantedBy=sockets.target</pre><h3>2. Update Gunicorn to Use the&nbsp;Socket</h3><p>Next, I modified the Gunicorn service file to  instead of a network&nbsp;address:</p><p><strong>‚Äî bind unix:/run/gunicorn.sock</strong></p><pre>[Unit]Description=gunicorn daemonUser=ec2-userWorkingDirectory=/home/ec2-user/src/g*****<p>ExecStart=/home/ec2-user/src/g*****/vone/bin/gunicorn \\</p> --workers 3 \\ --bind unix:/run/gunicorn.sock \\<p> --access-logfile /home/ec2-user/src/g*****/access.log \\</p> --error-logfile /home/ec2-user/src/g*****/error.log \\[Install]<p>WantedBy=multi-user.target</p></pre><h3>3. Update Nginx to Use the&nbsp;Socket</h3><p>Lastly, I then updated  to communicate via the Unix&nbsp;socket:</p><pre>server {    listen 80;<p>    server_name **.**.***.***;</p><p>    location = /favicon.ico { access_log off; log_not_found off; }</p>    location /static/ {<p>        root /home/ec2-user/src/g*****/media;</p>    }        proxy_pass http://unix:/run/gunicorn.sock;<p>        proxy_set_header Host $host;</p>        proxy_set_header X-Real-IP $remote_addr;<p>        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</p>        proxy_set_header X-Forwarded-Proto $scheme;}</pre><p>After restarting the services using the below commands, the issue was eliminated by removing unnecessary network overhead, making the application much&nbsp;faster.</p><pre>Sudo Systemctl restart NginxSudo Systemctl restart Gunicorn</pre><p>I ran the test again on Postman, and voila‚Ää‚Äî‚Ääthe response time for Gunicorn was ~.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1c324eb9bbb5\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating A Game Highlight Processor Using A S3, AWS MediaConvert, & Docker","url":"https://blog.devops.dev/creating-a-game-highlight-processor-using-a-s3-aws-mediaconvert-docker-b1d505ccaeca?source=rss----33f8b2d9a328---4","date":1739901758,"author":"Bri Rorie","guid":4363,"unread":true,"content":"<h3>Creating A Game Highlight Processor Using S3, AWS MediaConvert, &amp;&nbsp;Docker</h3><p>print(‚ÄúWelcome to another challenge&nbsp;!‚Äù)</p><p>I am back with my sports streak challenges to create another project&nbsp;!</p><p><strong>*This is not a super long read it‚Äôs long because I added actual code&nbsp;in*</strong></p><p> Today my objective is to create a NCAA game highlight processor using S3, AWS Media Convert &amp;&nbsp;Docker.</p><p>Once the code is written this project should be pretty straightforward but I‚Äôm excited to see what issues I might possibly face while going through the steps. So buckle up‚Ää‚Äî‚Äälets begin&nbsp;!</p><p>Here is a comprehensive diagram of the architecture&nbsp;:</p><p>I‚Äùll briefly explain the code&nbsp;:</p><p>The first file I‚Äôll be utilizing is . This file will be importing the json, boto3, os, load_dotenv, and requests modules to add more functionality to my Python code such as handling json data, interacting with AWS services, loading environment variables into my code, and making HTTP requests&nbsp;:</p><pre># Import the 'json' module for handling JSON dataimport json<p># Import the 'boto3' library for interacting with AWS services like S3</p>import boto3<p># Import the 'requests' library for making HTTP requests to external APIs</p>import requests<p>#Import the 'os' module to access environment variables</p>import os<p>#Imports the \"dotenv\" module to load environment variables from the .env file</p>from dotenv import load_dotenv</pre><p>Then it will load and import the environment variables from the config file&nbsp;:</p><pre>load_dotenv() #Loads the .env variables<p># Import specific configuration variables from the 'config.py' module</p>from config import (<p>    API_URL,             # The endpoint URL for fetching sports highlights</p>    RAPIDAPI_HOST,       # The host for the RapidAPI service<p>    RAPIDAPI_KEY,        # The API key for authenticating with RapidAPI</p>    DATE,                # The date for which to fetch highlights<p>    LEAGUE_NAME,         # The name of the basketball league (e.g., NCAA)</p>    LIMIT,               # The maximum number of highlights to fetch<p>    S3_BUCKET_NAME,      # The name of the S3 bucket where data will be stored</p>    AWS_REGION,          # The AWS region</pre><p>Debug and get the secret from Secret Manager&nbsp;:</p><pre># Debug prints for environment variablesprint(f\"SECRET_NAME: {os.getenv('SECRET_NAME')}\")<p>print(f\"API_URL: {os.getenv('API_URL')}\")</p>print(f\"AWS_REGION: {os.getenv('AWS_REGION')}\")<p>print(f\"S3_BUCKET_NAME: {os.getenv('S3_BUCKET_NAME')}\")</p><p>#Fetches the secret from AWS Secrets Manager</p>def get_secret():<p>    secret_name = os.getenv(\"SECRET_NAME\")</p>    region_name = \"us-east-1\"  <p>    client = boto3.client(\"secretsmanager\", region_name=region_name)</p>        response = client.get_secret_value(SecretId=secret_name)<p>        secret = json.loads(response[\"SecretString\"])</p><p>        print(f\"Fetched secret: {secret}\") #Prints fetched secret</p>        return secret.get(\"RAPIDAPI_KEY\")  #Secret api key name        print(f\"Error fetching secret: {e}\")print(f\"API_KEY fetched: {API_KEY}\")</pre><p>Fetch the highlights from the RapidApi website&nbsp;:</p><pre>def fetch_highlights():    \"\"\"<p>    Fetch basketball highlights from the API.</p><p>    This function makes a GET request to the specified API endpoint with the necessary</p>    headers and query parameters to retrieve basketball highlights. It handles any<p>    request-related exceptions and returns the fetched highlights as a JSON object.</p>        dict or None: The fetched highlights as a JSON dictionary if successful; otherwise, None.    try:<p>        # Define the query parameters for the API request</p>        query_params = {<p>            \"date\": DATE,            # The specific date for which to fetch highlights</p>            \"leagueName\": LEAGUE_NAME,  # The name of the league (e.g., NCAA)<p>            \"limit\": LIMIT            # The maximum number of highlights to retrieve</p>        }<p>        # Define the headers for the API request, including authentication details</p>        headers = {<p>            \"X-RapidAPI-Key\": API_KEY,      # API key for RapidAPI authentication</p>            \"X-RapidAPI-Host\": RAPIDAPI_HOST     # Hostname for the RapidAPI service<p>        # Make a GET request to the API endpoint with the specified headers and query parameters</p>        # Set a timeout of 120 seconds to prevent hanging requests<p>        response = requests.get(API_URL, headers=headers, params=query_params, timeout=120)</p><p>        # Raise an HTTPError if the HTTP request returned an unsuccessful status code</p>        response.raise_for_status()<p>        # Parse the JSON response from the API</p>        highlights = response.json()<p>        # Print a success message to indicate that highlights were fetched successfully</p>        print(\"Highlights fetched successfully!\")<p>        # Return the parsed highlights data</p>        return highlights<p>    except requests.exceptions.RequestException as e:</p>        # Catch any exceptions related to the HTTP request and print an error message<p>        print(f\"Error fetching highlights: {e}\")</p><p>        # Return None to indicate that fetching highlights failed</p>        return None</pre><p>The data that was fetched will be saved to S3&nbsp;:</p><pre>def save_to_s3(data, file_name):    \"\"\"<p>    Save data to an S3 bucket.</p><p>    This function uploads the provided data to a specified S3 bucket. It first checks</p>    whether the bucket exists and creates it if it does not. The data is then serialized<p>    to JSON and uploaded to the S3 bucket with the specified file name.</p>        data (dict): The data to be saved to S3.<p>        file_name (str): The name of the file (without extension) to be created in S3.</p>    \"\"\"        # Initialize the S3 client using the specified AWS region<p>        s3 = boto3.client(\"s3\", region_name=AWS_REGION)</p><p>        # Attempt to check if the specified S3 bucket exists by calling 'head_bucket'</p>        try:<p>            s3.head_bucket(Bucket=S3_BUCKET_NAME)</p>            # If the bucket exists, print a confirmation message<p>            print(f\"Bucket {S3_BUCKET_NAME} exists.\")</p>        except Exception:<p>            # If the bucket does not exist, print a message indicating creation</p>            print(f\"Bucket {S3_BUCKET_NAME} does not exist. Creating...\")<p>            if AWS_REGION == \"us-east-1\":</p>                # For the 'us-east-1' region, the 'LocationConstraint' is not required<p>                s3.create_bucket(Bucket=S3_BUCKET_NAME)</p>            else:<p>                # For other regions, specify the 'LocationConstraint' during bucket creation</p>                s3.create_bucket(                    CreateBucketConfiguration={\"LocationConstraint\": AWS_REGION}            # Print a success message after creating the bucket<p>            print(f\"Bucket {S3_BUCKET_NAME} created successfully.\")</p><p>        # Define the S3 key (path) where the JSON data will be stored</p>        s3_key = f\"highlights/{file_name}.json\"<p>        # Upload the JSON data to the specified S3 bucket and key</p>        s3.put_object(<p>            Bucket=S3_BUCKET_NAME,                     # The target S3 bucket</p>            Key=s3_key,                                # The S3 key (path) for the uploaded file<p>            Body=json.dumps(data),                      # The JSON-serialized data to upload</p>            ContentType=\"application/json\"             # The MIME type of the uploaded file<p>        # Print a success message indicating where the data was saved in S3</p>        print(f\"Highlights saved to S3: s3://{S3_BUCKET_NAME}/{s3_key}\")        # Catch any exceptions related to S3 operations and print an error message<p>        print(f\"Error saving to S3: {e}\")</p></pre><p>&amp; then the highlights are processed&nbsp;:</p><pre>def process_highlights():    \"\"\"<p>    Main function to fetch and process basketball highlights.</p><p>    This function orchestrates the workflow of fetching basketball highlights from the API</p>    and saving them to an S3 bucket. It first calls 'fetch_highlights' to retrieve the data,<p>    and if successful, proceeds to call 'save_to_s3' to store the data in S3.</p>    \"\"\"<p>    # Print a message indicating the start of the highlights fetching process</p>    print(\"Fetching highlights...\")<p>    # Call the 'fetch_highlights' function to retrieve highlights data from the API</p>    highlights = fetch_highlights()<p>    # Check if highlights were successfully fetched</p>    if highlights:<p>        # Print a message indicating the start of the S3 saving process</p>        print(\"Saving highlights to S3...\")<p>        # Call the 'save_to_s3' function to upload the fetched highlights to S3</p>        save_to_s3(highlights, \"basketball_highlights\")<p># Check if this script is being run as the main program</p># If so, execute the 'process_highlights' function<p>if __name__ == \"__main__\":</p>    process_highlights()</pre><p>The next Python script will be &nbsp;:</p><pre># process_one_video.py<p># Import the 'json' module for handling JSON data serialization and deserialization</p>import json<p># Import the 'boto3' library for interacting with AWS services like S3</p>import boto3<p># Import the 'requests' library for making HTTP requests to external URLs</p>import requests<p># Import 'BytesIO' from the 'io' module to handle in-memory binary streams</p>from io import BytesIO<p># Import specific configuration variables from the 'config.py' module</p>from config import (<p>    S3_BUCKET_NAME,  # The name of the Amazon S3 bucket used for input/output data</p>    AWS_REGION,      # The AWS region where the S3 bucket is located<p>    INPUT_KEY,       # The S3 key (path) for the input JSON file containing video URLs</p>    OUTPUT_KEY       # The S3 key (path) where the processed video will be saved    \"\"\"<p>    Fetch a highlight URL from the JSON file in S3, download the video,</p>    and save it back to S3.<p>    This function performs the following steps:</p>    1. Connects to the specified S3 bucket.<p>    2. Retrieves the input JSON file containing video URLs.</p>    3. Extracts the first video URL from the JSON data.<p>    4. Downloads the video from the extracted URL.</p>    5. Uploads the downloaded video to the specified S3 location.    try:<p>        # Initialize the S3 client with the specified AWS region</p>        s3 = boto3.client(\"s3\", region_name=AWS_REGION)<p>        # Inform the user that the JSON file retrieval process has started</p>        print(\"Fetching JSON file from S3...\")<p>        # Retrieve the JSON file from S3 using the specified bucket and key</p>        response = s3.get_object(Bucket=S3_BUCKET_NAME, Key=INPUT_KEY)<p>        # Read the content of the retrieved object and decode it from bytes to a UTF-8 string</p>        json_content = response['Body'].read().decode('utf-8')<p>        # Parse the JSON string into a Python dictionary</p>        highlights = json.loads(json_content)<p>        # Extract the first video URL from the JSON data</p>        # Adjust the key path ('[\"data\"][0][\"url\"]') based on the actual structure of your JSON<p>        video_url = highlights[\"data\"][0][\"url\"]</p><p>        # Inform the user about the video URL being processed</p>        print(f\"Processing video URL: {video_url}\")<p>        # Inform the user that the video download process has started</p>        print(\"Downloading video...\")<p>        # Make a GET request to the video URL to download the video content</p>        # 'stream=True' allows streaming the response content<p>        video_response = requests.get(video_url, stream=True)</p><p>        # Raise an HTTPError if the HTTP request returned an unsuccessful status code</p>        video_response.raise_for_status()<p>        # Read the content of the video response and store it in a BytesIO object</p>        # This allows handling the video data in memory without saving it to disk<p>        video_data = BytesIO(video_response.content)</p><p>        # Inform the user that the video upload process has started</p>        print(\"Uploading video to S3...\")<p>        # Upload the video data to S3 using the specified bucket and key</p>        s3.put_object(<p>            Bucket=S3_BUCKET_NAME,          # The target S3 bucket where the video will be stored</p>            Key=OUTPUT_KEY,                  # The S3 key (path) for the uploaded video<p>            Body=video_data,                 # The binary data of the video to upload</p>            ContentType=\"video/mp4\"          # The MIME type of the uploaded file<p>        # Inform the user that the video was uploaded successfully, including the S3 URL</p>        print(f\"Video uploaded successfully: s3://{S3_BUCKET_NAME}/{OUTPUT_KEY}\")        # Catch any exceptions that occur during the process and inform the user<p>        print(f\"Error during video processing: {e}\")</p><p># Check if this script is being run as the main program</p># If so, execute the 'process_one_video' function<p>if __name__ == \"__main__\":</p>    process_one_video()</pre><p>This will import the necessary libraries, and modules to make HTTP requests, interact with AWS services, converting Python code into Json / vice versa&nbsp;, and writes data in-memory instead of on the disk to process the video before uploading to&nbsp;S3.</p><p>It will process one video from the data fetched in the Rapidapi response&nbsp;, and then save it into S3. I already created a RapidApi account to get my api credential for this&nbsp;project.</p><p>Okay the next file is the <strong>mediaconvert_process.py&nbsp;:</strong></p><pre># mediaconvert_process.py<p># Import the 'json' module for handling JSON data serialization and deserialization</p>import json<p># Import the 'boto3' library for interacting with AWS services like MediaConvert and S3</p>import boto3<p># Import specific configuration variables from the 'config.py' module</p>from config import (<p>    AWS_REGION,               # AWS region where services are deployed (e.g., 'us-east-1')</p>    MEDIACONVERT_ENDPOINT,    # The endpoint URL for AWS MediaConvert service<p>    MEDIACONVERT_ROLE_ARN,    # The Amazon Resource Name (ARN) for the IAM role used by MediaConvert</p>    S3_BUCKET_NAME            # The name of the Amazon S3 bucket used for input/output data    \"\"\"<p>    Create a MediaConvert job to process a video.</p><p>    This function initializes the MediaConvert client, defines the job settings,</p>    and submits a job to AWS MediaConvert for processing a video file stored in S3.    try:<p>        # Initialize the MediaConvert client with specified region and endpoint</p>        mediaconvert = boto3.client(<p>            \"mediaconvert\",                    # AWS MediaConvert service</p>            region_name=AWS_REGION,            # AWS region from configuration<p>            endpoint_url=MEDIACONVERT_ENDPOINT  # MediaConvert endpoint URL from configuration</p>        )<p>        # Define the S3 URL for the input video file to be processed</p>        input_s3_url = f\"s3://{S3_BUCKET_NAME}/videos/first_video.mp4\"<p>        # Define the S3 URL where the processed videos will be saved</p>        output_s3_url = f\"s3://{S3_BUCKET_NAME}/processed_videos/\"<p>        # Define the job settings for MediaConvert</p>        job_settings = {<p>            \"Inputs\": [  # List of input sources for the MediaConvert job</p>                {<p>                    \"AudioSelectors\": {  # Define audio selection settings</p>                        \"Audio Selector 1\": {\"DefaultSelection\": \"DEFAULT\"}  # Select default audio track                    \"FileInput\": input_s3_url,  # Specify the input video file S3 URL<p>                    \"VideoSelector\": {}         # Video selection settings (empty means default)</p>                }            \"OutputGroups\": [  # Define output group settings                    \"Name\": \"File Group\",  # Name identifier for the output group<p>                    \"OutputGroupSettings\": {  # Settings specific to the output group</p>                        \"Type\": \"FILE_GROUP_SETTINGS\",  # Type of output group<p>                        \"FileGroupSettings\": {  # Settings related to file group outputs</p>                            \"Destination\": output_s3_url  # S3 destination URL for processed videos                    },<p>                    \"Outputs\": [  # List of output configurations within the output group</p>                        {<p>                            \"ContainerSettings\": {  # Container format settings</p>                                \"Container\": \"MP4\",       # Output container format (MP4)<p>                                \"Mp4Settings\": {}         # Additional MP4-specific settings (empty for defaults)</p>                            },<p>                            \"VideoDescription\": {  # Description of video settings</p>                                \"CodecSettings\": {  # Codec configuration for video<p>                                    \"Codec\": \"H_264\",  # Video codec to use (H.264)</p>                                    \"H264Settings\": {  # Specific settings for H.264 codec<p>                                        \"Bitrate\": 5000000,              # Bitrate in bits per second</p>                                        \"RateControlMode\": \"CBR\",        # Constant Bit Rate control mode<p>                                        \"QualityTuningLevel\": \"SINGLE_PASS\",  # Quality tuning level</p>                                        \"CodecProfile\": \"MAIN\"            # H.264 codec profile                                },<p>                                \"ScalingBehavior\": \"DEFAULT\",  # Behavior for scaling video resolution</p>                                \"TimecodeInsertion\": \"DISABLED\"  # Disable timecode insertion                            \"AudioDescriptions\": [  # List of audio configurations                                    \"CodecSettings\": {  # Codec configuration for audio<p>                                        \"Codec\": \"AAC\",  # Audio codec to use (AAC)</p>                                        \"AacSettings\": {  # Specific settings for AAC codec<p>                                            \"Bitrate\": 64000,           # Bitrate in bits per second</p>                                            \"CodingMode\": \"CODING_MODE_2_0\",  # Audio coding mode (2.0 channels)<p>                                            \"SampleRate\": 48000        # Audio sample rate in Hz</p>                                        }                                }                        }                }        }<p>        # Submit the MediaConvert job with the defined settings and additional parameters</p>        response = mediaconvert.create_job(<p>            Role=MEDIACONVERT_ROLE_ARN,                 # IAM role ARN that MediaConvert assumes</p>            Settings=job_settings,                      # Job settings defined above<p>            AccelerationSettings={\"Mode\": \"DISABLED\"},  # Disable acceleration settings</p>            StatusUpdateInterval=\"SECONDS_60\",           # Interval for status updates (every 60 seconds)<p>            Priority=0                                   # Priority of the job (0 is default)</p>        )<p>        # Print a success message indicating the job was created</p>        print(\"MediaConvert job created successfully:\")<p>        # Pretty-print the JSON response from MediaConvert</p>        print(json.dumps(response, indent=4))        # Catch any exceptions that occur during job creation and print an error message<p>        print(f\"Error creating MediaConvert job: {e}\")</p><p># Check if this script is being run as the main program</p>if __name__ == \"__main__\":<p>    # Call the 'create_job' function to initiate the MediaConvert job</p>    create_job()</pre><p>This one is pretty straightforward also‚Ää‚Äî‚Ääimporting the libraries&nbsp;, modules&nbsp;, and environment variables from the  file, and then creating a job to convert the existing media in the S3 bucket to a higher quality&nbsp;video.</p><p>Last but not least I have the  file&nbsp;, that will consolidate all the files and run them. The file will be in the Docker container I build which will actually execute the process&nbsp;:</p><pre># run_all.py<p># Import the 'subprocess' module to run external scripts as subprocesses</p>import subprocess<p># Import the 'time' module to handle delays between script executions</p>import time<p># Import specific configuration variables from the 'config.py' module</p>from config import (<p>    RETRY_COUNT,               # The number of retry attempts for failed scripts</p>    RETRY_DELAY,               # The delay (in seconds) between retry attempts<p>    WAIT_TIME_BETWEEN_SCRIPTS  # The waiting time (in seconds) between different scripts</p>)<p>def run_script(script_name, retries=RETRY_COUNT, delay=RETRY_DELAY):</p>    \"\"\"<p>    Run a script with retry logic and a delay.</p><p>    This function attempts to execute a given Python script using subprocess.</p>    If the script fails, it retries the execution up to a specified number of times,<p>    waiting for a specified delay between attempts.</p>        script_name (str): The name of the Python script to execute.<p>        retries (int, optional): The maximum number of retry attempts. Defaults to RETRY_COUNT.</p>        delay (int, optional): The delay in seconds between retry attempts. Defaults to RETRY_DELAY.        subprocess.CalledProcessError: If the script fails after all retry attempts.    # Initialize the attempt counter to zero<p>    # Continue attempting to run the script until the maximum number of retries is reached</p>    while attempt &lt; retries:            # Inform the user that the script is being run, including the current attempt number<p>            print(f\"Running {script_name} (attempt {attempt + 1}/{retries})...\")</p><p>            # Execute the script as a subprocess</p>            # 'check=True' ensures that a CalledProcessError is raised if the script exits with a non-zero status<p>            subprocess.run([\"python\", script_name], check=True)</p><p>            # Inform the user that the script completed successfully</p>            print(f\"{script_name} completed successfully.\")<p>            # Exit the function if the script ran successfully</p>            return<p>        except subprocess.CalledProcessError as e:</p>            # Inform the user that an error occurred while running the script<p>            print(f\"Error running {script_name}: {e}\")</p><p>            # Increment the attempt counter</p>            attempt += 1<p>            # Check if the maximum number of retries has not been reached</p>            if attempt &lt; retries:<p>                # Inform the user that the script will be retried after a delay</p>                print(f\"Retrying in {delay} seconds...\")<p>                # Wait for the specified delay before retrying</p>                time.sleep(delay)                # Inform the user that the script has failed after all retry attempts<p>                print(f\"{script_name} failed after {retries} attempts.\")</p><p>                # Re-raise the exception to propagate the error</p>                raise e    \"\"\"<p>    Main function to orchestrate the execution of multiple scripts in a pipeline.</p><p>    This function runs a series of Python scripts in a specific order, introducing</p>    delays between them to ensure that resources are stabilized before the next script runs.<p>    It handles any exceptions that occur during the execution of the scripts.</p>    \"\"\"        # Step 1: Run fetch.py<p>        # Buffer time between scripts to allow resources to stabilize</p>        print(\"Waiting for resources to stabilize...\")<p>        time.sleep(WAIT_TIME_BETWEEN_SCRIPTS)</p><p>        # Step 2: Run process_one_video.py</p>        run_script(\"process_one_video.py\")<p>        # Buffer time between scripts to allow resources to stabilize</p>        print(\"Waiting for resources to stabilize...\")<p>        time.sleep(WAIT_TIME_BETWEEN_SCRIPTS)</p><p>        # Step 3: Run mediaconvert_process.py</p>        run_script(\"mediaconvert_process.py\")<p>        # Inform the user that all scripts have been executed successfully</p>        print(\"All scripts executed successfully.\")        # Inform the user that the pipeline has failed and provide the error details<p>        print(f\"Pipeline failed: {e}\")</p><p># Check if this script is being run as the main program</p># If so, execute the 'main' function<p>if __name__ == \"__main__\":</p>    main()</pre><p>Now that all my code is ready&nbsp;, I can send my api key credentials to Secrets Manager‚Ää‚Äî‚Ää is a fully managed service that helps you store, manage, and retrieve sensitive information‚Ää‚Äî‚Äälike database credentials, API keys, that‚Äôs used by your applications and services.</p><p>It is more secure than a regular&nbsp; file because the sensitive data is encrypted at rest by the(KMS)&nbsp;, which is just a hardware security module that generates asynchronous(public and private key pair) and synchronous(single unit to encrypt and decrypt)keys.</p><p>Secrets Manager also automatically rotates credentials for higher security as well as auditing features to keep track of whose accessing them. This option is good for production environments, but since I‚Äôve never worked with Secrets Manager before I‚Äôll go try it out&nbsp;:</p><p>aws secretsmanager create-secret --name &lt;secret_name&gt; --description \"Api key for accessing the Sports Highlights Api\" --secret-string '{\"RapidApi_Key\":\"&lt;Rapid api key&gt;\" -- region us-east-1</p><p>For my&nbsp;variables&nbsp;, I set the  endpoint,  name, <strong>Amazon Resource Name (ARN)</strong>&nbsp;, &amp; &nbsp;,, , &nbsp;, ,  &amp; , <strong>Wait_Time_Between_Scripts</strong> and the  &amp; . I already set the api key in the secrets manager so for the Secret name I set the literal name for the value in the&nbsp;file.</p><p>I used aws mediaconvert describe-endpoints to get the endpoint&nbsp;:</p><p>I saved all the changes to my code, secured my&nbsp; file by setting the permissions to 600 so that only I have access to read &amp; write to it, and then redirected it to my.gitignore file for extra extra safety. The&nbsp;file just protects my sensitive data from being pushed to a repository.</p><p>Now I‚Äôll build and run my Docker container&nbsp;:</p><p>docker build -t highlight-processor&nbsp;.</p><p>docker run --env-file&nbsp;.env highlight-processor</p><p>So I quite literally tried doing everything to troubleshoot‚Ää‚Äî‚ÄäI checked my fetch.py code, config file, I checked the api key in Secrets Manager, I looked in my container to see if any credentials or files were missing, I looked into my&nbsp;.envfile to make sure there were no spaces or errors in the values, but when it all boiled down the solution came down to installing the , , and  modules with pip locally&nbsp;, and then remembering to include the  module into my  file.</p><p>Installing , allows my Python code to interact with AWS services, the  module is used to load the environment variables from the&nbsp;.env file into the code‚Ää‚Äî‚Ääwhich is why my api keys were not showing up, and then the  module is used to be able to make Http requests to the RapidApi&nbsp;website.</p><p>I had to include python_dotenv into my requirements.txt file as well so that my&nbsp;.env variables would be loaded into my code in my container.</p><p>I also had to rebuild my image and container with the new adjustments.</p><p>After that the api key&nbsp;, and Client errors went away and everything worked:</p><p>The previous Docker commands just grab the environment variables from the&nbsp; file, loads them (python_dotenv), copies them to the newly created image (Dockerfile) creates the image‚Ää‚Äî‚Ääand then starts the container.</p><p>The container will run , &nbsp;,  using the ‚Äúfile&nbsp;, install the dependencies, save the fetched data from the Rapidapi website into my S3 bucket, process one video, and then convert it with Mediaconvert into a higher&nbsp;quality.</p><p>A  is a portable way of sharing and running applications with their dependencies, libraries, and configuration files‚Ää‚Äî‚Ääso that no matter where it‚Äôs deployed&nbsp;, it‚Äôll run the same way. An  is an executable package that has everything needed to run an application which is created with a Dockerfile. A  is a set of instructions used to define the image. So the Dockerfile is used to create the image, and the image is used to create the container.</p><p> is a file based video processing service used to make higher quality content efficiently.</p><p>Since my code finally worked I can check the console to see everything that was created&nbsp;:</p><p>I‚Äôll look into the MediaConvert service to see the job it created&nbsp;:</p><p>Then check S3 to see the files that were created&nbsp;:</p><p>I‚Äôll check the and  folders:</p><p>Here is the first video&nbsp;:</p><p>&amp; here is the processed video by MediaConvert&nbsp;:</p><p>Alright that‚Äôs it‚Äôs for this project‚Ää‚Äî‚Ääuntil next time&nbsp;!!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b1d505ccaeca\" width=\"1\" height=\"1\" alt=\"\">","contentLength":27276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deploying Azure Pipelines Agents on AKS with Autoscaling Using KEDA","url":"https://blog.devops.dev/deploying-azure-pipelines-agents-on-aks-with-autoscaling-using-keda-07c19b1f1b99?source=rss----33f8b2d9a328---4","date":1739901630,"author":"Maciej","guid":4362,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Update API Server SANs and sync issue","url":"https://www.reddit.com/r/kubernetes/comments/1ishh0h/update_api_server_sans_and_sync_issue/","date":1739898036,"author":"/u/nfreder","guid":4275,"unread":true,"content":"<p>We have a use case to leverage a cloud provider I won't name that starts with an O. I have a requirement to setup a hostname and IP for connectivity. Is there a way to run kubeadm commands across all control plane nodes that I'm missing? I think a daemonset might work but, I have a chicken and egg issue to setup any kind of automation there. </p>","contentLength":344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"issue with ingress","url":"https://www.reddit.com/r/kubernetes/comments/1isgbi8/issue_with_ingress/","date":1739895206,"author":"/u/GeneEfficient1481","guid":4274,"unread":true,"content":"<p>hello everyone i am having trouble with this ingress exercise</p><p>Create an Ingress resource named web and configure it as follows:</p><p>Route traffic for the host web.kubernetes and all routes to the existing web service. Enable TLS termination using the existing Secret web certification.</p><p>Redirect HTTP requests to HTTPS. </p><p>I have configured /etc/hosts I will pair the node ip with the web.kubernetes host</p><p>[curl: (7) Unable to connect to web.k8s.local port 80: connection refused]</p><p>what should i do to solve the problem?</p><p>this my txt containing deploy,svc secret and ingress: # 1. Deployment</p><p>openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \"/CN=web.k8s.local/O=web.k8s.local\"</p><p>kubectl create secret tls web-cert --namespace=prod --cert=tls.crt --key=tls.key</p>","contentLength":776,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elixir in kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1isfy34/elixir_in_kubernetes/","date":1739894278,"author":"/u/Latter-Change-9228","guid":4243,"unread":true,"content":"<p>I'm currently learning elixir in order to use it in production. I heard of the node architecture that elixir provides thanks to the OTP but I can't find resources about some return on experienec on using distributed elixir in a kubernetes context. Any thoughts about that ?</p>","contentLength":273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Good books/video/article to understand ingress controllers","url":"https://www.reddit.com/r/kubernetes/comments/1isfeoc/good_booksvideoarticle_to_understand_ingress/","date":1739892908,"author":"/u/khaloudkhaloud","guid":4242,"unread":true,"content":"<div><p>Any good ressources to \"really\" understand how ingress controllers works</p></div>   submitted by   <a href=\"https://www.reddit.com/user/khaloudkhaloud\"> /u/khaloudkhaloud </a>","contentLength":109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intruder Enhances Free Vulnerability Intelligence Platform ‚ÄòIntel‚Äô with AI-Generated CVE Descriptions","url":"https://devops.com/intruder-enhances-free-vulnerability-intelligence-platform-intel-with-ai-generated-cve-descriptions/","date":1739887272,"author":"cybernewswire","guid":4183,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"2025 Predictions for DevOps and Application Development","url":"https://devops.com/2025-predictions-for-devops-and-application-development/","date":1739887240,"author":"Mitch Ashley","guid":4182,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RKE2-Agent and Cilium HostFirewall Blocking Port 9345","url":"https://www.reddit.com/r/kubernetes/comments/1isc6cm/rke2agent_and_cilium_hostfirewall_blocking_port/","date":1739883773,"author":"/u/zdeneklapes","guid":4186,"unread":true,"content":"<p>I'm setting up a Kubernetes cluster using Rancher RKE2 with Cilium as the CNI. Everything works fine on the <strong>RKE2 server (master node)</strong> with  and <strong>kube-proxy replacement activated</strong>.</p><p>However, when I try to add a , it seems that some rules are pulled to the worker node, and after approximately , port  is . This results in the following error on the worker node:</p><pre><code>Feb 18 09:45:28 compute-07 rke2[173412]: time=\"2025-02-18T09:45:28Z\" level=error msg=\"Failed to connect to proxy. Empty dialer response\" error=\"dial tcp &lt;my-public-server-ip&gt;:9345: connect: connection timed out\" </code></pre><p>To fix this, I tried allowing the port  before adding the new worker node by applying the following <strong>CiliumClusterwideNetworkPolicy</strong>:</p><pre><code>apiVersion: cilium.io/v2 kind: CiliumClusterwideNetworkPolicy metadata: name: allow-hostfirewall-9345 spec: nodeSelector: {} # Applies to all nodes ingress: - fromEntities: - all toPorts: - ports: - port: \"9345\" protocol: TCP egress: - toEntities: - all toPorts: - ports: - port: \"9345\" protocol: TCP </code></pre><p>Unfortunately, this did not resolve the issue.</p><p>Before starting , I confirmed that the port :</p><pre><code>root@compute-07:~# nc -zv &lt;ip&gt; 9345 Ncat: Version 7.92 () Ncat: Connected to &lt;ip&gt;:9345. Ncat: 0 bytes sent, 0 bytes received in 0.01 seconds.https://nmap.org/ncat </code></pre><p>After starting , the port :</p><pre><code>root@compute-07:~# nc -zv &lt;ip&gt; 9345 Ncat: Version 7.92 ( https://nmap.org/ncat ) Ncat: Connection timed out. </code></pre><ol><li><strong>Why is port 9345 being closed after the RKE2 agent starts?</strong></li><li><strong>Is there a better way to explicitly allow this port through Cilium's hostFirewall?</strong></li><li><strong>What additional troubleshooting steps should I take to debug this issue?</strong></li></ol>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simplifying Kubernetes deployments with a unified Helm chart","url":"https://www.reddit.com/r/kubernetes/comments/1isb1e2/simplifying_kubernetes_deployments_with_a_unified/","date":1739879866,"author":"/u/danielepolencic","guid":4122,"unread":true,"content":"<div><p>Managing  in  at scale often leads to inconsistent deployments and maintenance overhead. This episode explores a practical solution that standardizes service deployments while maintaining team autonomy.</p><p> discusses how a unified  chart approach can help platform teams support multiple development teams efficiently while maintaining consistent standards across services.</p><ul><li>Why inconsistent  chart configurations across teams create maintenance challenges and slow down deployments</li><li>How to implement a unified  chart that balances standardization with flexibility through override functions</li><li>How to maintain quality through automated documentation and testing with tools like  and </li></ul></div>   submitted by   <a href=\"https://www.reddit.com/user/danielepolencic\"> /u/danielepolencic </a>","contentLength":710,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Boost Your Website Core Web Vitals Through DevOps Best Practices","url":"https://devops.com/boost-your-website-core-web-vitals-through-devops-best-practices/","date":1739878045,"author":"Suzanne Harrison","guid":4117,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weekly: Questions and advice","url":"https://www.reddit.com/r/kubernetes/comments/1isa58r/weekly_questions_and_advice/","date":1739876419,"author":"/u/gctaylor","guid":4081,"unread":true,"content":"<p>Have any questions about Kubernetes, related tooling, or how to adopt or use Kubernetes? Ask away!</p>","contentLength":98,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empower Developers to Preserve Human Agency in Today‚Äôs World of Automation","url":"https://devops.com/empower-developers-to-preserve-human-agency-in-todays-world-of-automation/","date":1739875805,"author":"Michael Burch","guid":4078,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Longhorn does not recognize dm-crypt module in ubunti 24.04 vm.","url":"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/","date":1739872887,"author":"/u/MrSliff84","guid":4054,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Do i have to set up secrets first, to get rid of this warning in longhorn?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MrSliff84\"> /u/MrSliff84 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is9bbr/longhorn_does_not_recognize_dmcrypt_module_in/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Cgroup v2 Features Are You Using Beyond Basic CPU and Memory limit in Kubernetes? (Alpha features or customized plugins)","url":"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/","date":1739869690,"author":"/u/Electronic_Role_5981","guid":4053,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">https://kubernetes.io/docs/concepts/architecture/cgroups/</a> </p> <p><a href=\"https://kubernetes.io/docs/concepts/architecture/cgroups/\">cgroup v2 </a>is stable since v1.25.</p> <p><a href=\"https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#memory-qos-with-cgroup-v2\">MemoryQoS</a> started using memory.high, but it may cause throttling issue to hang the application sometimes. It is still alpha since 1.22.</p> <p>For OOMKill behavior change, kubelet added <a href=\"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/\">singleProcessOOMKill</a> to keep the behavior of cgroups v1 when users want. </p> <p><a href=\"https://github.com/kubernetes/enhancements/issues/4205\">PSI KEP</a> was merged recently for v1.33.</p> <p><a href=\"https://github.com/kubernetes/enhancements/issues/2400\">NodeSwap</a> was beta now.</p> <p>Cgroup v2 controller includes:</p> <ul> <li><strong><em>memory (since Linux 4.5)</em></strong></li> <li><em>pids</em> (since Linux 4.5)</li> <li><strong><em>io</em></strong> <strong>(since Linux 4.5)</strong></li> <li><em>rdma (since Linux 4.11)</em></li> <li><em>perf_event</em> (since Linux 4.11)</li> <li><strong><em>cpu (since Linux 4.15)</em></strong></li> <li><em>cpuset</em> (since Linux 5.0)</li> <li><strong><em>freezer</em></strong> <strong>(since Linux 5.2)</strong></li> <li><em>hugetlb</em> (since Linux 5.6)</li> <li><em>nsdelegate</em> (since Linux 4.15)</li> <li>PSI(since Linux 4.20)</li> </ul> <p>Anyone started using the blkio limit or other cgroup controllers? Are you enable the CgroupV2 related feature gates above or flags? </p> <ul> <li>Some related projects: <ul> <li> <a href=\"https://facebookmicrosites.github.io/oomd/\">https://facebookmicrosites.github.io/oomd/</a></li> <li> <a href=\"https://github.com/facebookincubator/oomd\">https://github.com/facebookincubator/oomd</a></li> </ul></li> </ul> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is8lfd/what_cgroup_v2_features_are_you_using_beyond/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"unexpected side effects in pod routing","url":"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/","date":1739862394,"author":"/u/Cyclonit","guid":4004,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi,</p> <p>I am working on hosting <a href=\"https://www.home-assistant.io/\">Home Assistant</a> in my Kubernetes Homelab. For Home Assistant being able to discover devices in my home network, I added a secondary bridged macvlan0 network interface using Multus. Given that my router manages IP addresses for my home network, I decided to use DHCP for the pod&#39;s second IP address too. This part works fine.</p> <pre><code>apiVersion: &quot;k8s.cni.cncf.io/v1&quot; kind: NetworkAttachmentDefinition metadata: name: eth0-macvlan-dhcp spec: config: | { &quot;cniVersion&quot;: &quot;0.3.0&quot;, &quot;type&quot;: &quot;macvlan&quot;, &quot;master&quot;: &quot;eth0&quot;, &quot;mode&quot;: &quot;bridge&quot;, &quot;ipam&quot;: { &quot;type&quot;: &quot;dhcp&quot; } } </code></pre> <p>However, using DHCP results in the pod receiving a second default route via my home network&#39;s router. This route takes precedence over the default route via the pod network and completely breaks pod-to-pod communication.</p> <p>This is how the routes look like inside of the container after deployment:</p> <pre><code>```sh $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 192.168.178.1 0.0.0.0 UG 0 0 0 net1 default 10.0.2.230 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 ``` </code></pre> <p>This is what happens after trying to delete the first route. As you can see, the default route via <a href=\"http://10.0.2.230\">10.0.2.230</a> was replaced by a default route via localhost. <a href=\"http://10.0.2.230\">10.0.2.230</a> is not an IP of the pod.</p> <pre><code>$ route del -net default gw 192.168.178.1 netmask 0.0.0.0 dev net1 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default localhost 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 </code></pre> <p>Interestingly, this is completely reversible by adding the undesired route back:</p> <pre><code>$ route add -net default gw 192.168.178.1 netmask 0.0.0.0 dev net1 $ route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default 192.168.178.1 0.0.0.0 UG 0 0 0 net1 default 10.0.2.230 0.0.0.0 UG 0 0 0 eth0 10.0.2.230 * 255.255.255.255 UH 0 0 0 eth0 192.168.178.0 * 255.255.255.0 U 0 0 0 net1 </code></pre> <p>Any ideas on what is going on?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Cyclonit\"> /u/Cyclonit </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1is6wqt/unexpected_side_effects_in_pod_routing/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS CloudFormation: 2024 Year in Review","url":"https://aws.amazon.com/blogs/devops/aws-cloudformation-2024-year-in-review/","date":1739832113,"author":"Idriss Laouali Abdou","guid":2054,"unread":true,"content":"<p><a href=\"https://aws.amazon.com/cloudformation/\">AWS CloudFormation</a> enables you to model and provision your cloud application infrastructure as code-base templates. Whether you prefer writing templates directly in JSON or YAML, or using programming languages like Python, Java, and TypeScript with the <a href=\"https://aws.amazon.com/cdk/\">AWS Cloud Development Kit (CDK)</a>, CloudFormation and CDK provide the flexibility you need. For organizations adopting <a href=\"https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/organizing-your-aws-environment.html\">multi-account strategies</a>, CloudFormation <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/what-is-cfnstacksets.html\">StackSets</a> offers a powerful capability to deploy resources across multiple regions and accounts in parallel.</p><p>Last year, we delivered broad set of enhancements that accelerated the development cycle, simplified troubleshooting, and introduced new deployment safety and configuration governance capabilities. Let‚Äôs dive into the key launches that shaped CloudFormation in 2024.</p><p>In March, we introduced <a href=\"https://aws.amazon.com/blogs/devops/peek-inside-your-aws-cloudformation-deployments-with-timeline-view/\">optimistic stabilization</a> with the new <a href=\"https://aws.amazon.com/blogs/devops/peek-inside-your-aws-cloudformation-deployments-with-timeline-view/\">CONFIGURATION_COMPLETE</a> event, delivering up to 40% faster stack creation times. This new event signals that CloudFormation has created the resource and applied the configuration as defined in the stack template, allowing us to begin parallel creation of dependent resources. For example, if your stack contains resource B that depends on resource A, CloudFormation will now start provisioning resource B when resource A reaches the CONFIGURATION_COMPLETE state, rather than waiting for full stabilization. Read <a href=\"https://aws.amazon.com/blogs/devops/how-we-sped-up-aws-cloudformation-deployments-with-optimistic-stabilization/\">How we sped up AWS CloudFormation deployments with optimistic stabilization</a> to learn more.</p><p><strong>Figure 1: CloudFormation‚Äôs old and new deployment strategy</strong></p><h2>Catch template errors before deployment with early validation</h2><p>In March, we launched <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/03/aws-cloudformation-new-validation-checks-stack-operations/\">early resource properties validation checks</a>. This feature validates your stack operation upfront for invalid resource property errors, helping you fail fast and minimize the steps required for a successful deployment. Previously, you had to wait until CloudFormation attempted to provision a resource before discovering property-related errors. Now, we validate your template before deploying the first resource and provide clear error messages upfront.</p><p><strong>Figure 2: CloudFormation‚Äôs early template properties validation feature</strong></p><h2><strong>Safely clean up failed stacks with enhanced deletion controls</strong></h2><p>In May, we enhanced the <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/05/aws-cloudformation-dev-test-cycle-new-parameter-deletestack-api/\">DeleteStack API</a> with a new DeletionMode parameter, allowing you to safely delete stacks that are in DELETE_FAILED state. By passing the FORCE_DELETE_STACK value to this parameter, you can now resolve stuck stacks more efficiently during your development and testing cycles.</p><h2><strong>Accelerate feedback loops with CloudFormation custom resource timeout controls</strong></h2><p>In June, we introduced the <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/06/aws-cloudformation-dev-test-cycle-timeouts-custom-resources/\">ServiceTimeout</a> property for <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudformation-customresource.html#aws-resource-cloudformation-customresource-properties\">custom resources</a>. This new capability allows you to set custom timeout values for your custom resource logic execution. Previously, custom resources had a fixed one-hour timeout, which could lead to long wait times when debugging custom resource logic. Now, you can set appropriate timeout values to accelerate your development feedback loops. Refer to the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudformation-customresource.html#aws-resource-cloudformation-customresource-properties\">custom resources</a><a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudformation-customresource.html#aws-resource-cloudformation-customresource-properties\">documentation</a> to learn more about the ServiceTimeout property.</p><p><strong>Figure 3: CloudFormation‚Äôs ServiceTimeout property for Custom resource</strong></p><h2>Resolve deployment issues faster with one-click CloudTrail access</h2><p>In May, we launched integration with <a href=\"https://aws.amazon.com/cloudtrail/\">AWS CloudTrail</a> in the <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/05/aws-cloudformation-deployment-troubleshooting-aws-cloudtrail-integration/\">Events tab of the CloudFormation console</a>. Troubleshooting some failed stack operations can be time-consuming, so we have streamlined this process by providing direct links from stack operation events to relevant CloudTrail events. When you click ‚ÄòDetect Root Cause‚Äô in the CloudFormation Console, you‚Äôll now see a pre-configured CloudTrail deep-link to the API events generated by your stack operation, eliminating multiple manual steps from the troubleshooting process.</p><h2>Visualize your entire deployment process with timeline view</h2><p>In November, we launched <a href=\"https://aws.amazon.com/blogs/devops/peek-inside-your-aws-cloudformation-deployments-with-timeline-view/\">deployment timeline view</a>. It gives you unprecedented visibility into your stack operations. This visual tool shows the sequence of actions CloudFormation takes during a deployment, helping you understand resource dependencies and provisioning duration. You can see which resources are being created in parallel, track their status through color-coding, and quickly identify bottlenecks in your deployments.</p><p><strong>Figure 5: CloudFormation‚Äôs deployment timeline view</strong></p><h2>Get instant troubleshooting help with Amazon Q Developer</h2><p>We integrated <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/11/cloudformation-troubleshooting-q-developer-assistance/\">Amazon Q Developer</a> to provide <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/11/cloudformation-troubleshooting-q-developer-assistance/\">AI-powered assistance for troubleshooting</a>. When you encounter a failed stack operation, you can now click ‚ÄúDiagnose with Q‚Äù to receive a clear, human-readable analysis of the error. Need more help? The ‚ÄúHelp me resolve‚Äù button provides actionable steps tailored to your specific scenario.</p><p>We‚Äôve also improved how <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html\">change sets</a> handle references. When referenced values are available before deployment, Change sets can now resolve them to their expected values, giving you a more accurate preview of your planned changes.</p><p><strong>Figure 7: CloudFormation‚Äôs change sets feature</strong></p><h2>Eliminate weeks of manual effort with IaC Generator</h2><p>In February, we launched the <a href=\"https://aws.amazon.com/blogs/devops/import-entire-applications-into-aws-cloudformation/\">CloudFormation IaC Generator</a>, a capability addressing one of our customers‚Äô biggest challenges: onboarding existing cloud resources to CloudFormation. This feature makes it easier to generate CloudFormation templates for existing AWS resources. You can now onboard workloads to IaC in minutes instead of spending weeks writing templates manually.</p><p>The IaC generator supports over <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/resource-import-supported-resources.html\">600 AWS resource types</a> and provides recommendations for related resources. For instance, when you select an S3 bucket, it automatically suggests including associated bucket policies. You can use the generated templates to import resources into CloudFormation, download them for deployment.</p><p><strong>Figure 8: CloudFormation‚Äôs IaC Generator</strong></p><p>In August, we enhanced the IaC Generator with two improvements. First, we added a <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/generate-IaC-view-scan-summary.html\">graphical summary view</a> that helps you quickly find resources after the account scan completes. Second, we integrated with AWS Infrastructure Composer to <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/infrastructure-composer-for-cloudformation.html\">visualize your application architecture</a>, making it easier to understand resource relationships and configurations.</p><p>In November, we launched major enhancements to CloudFormation Hooks, giving you easier ways to author proactive configuration controls and more points to enforce them with your cloud infrastructure provisioning.</p><h2>CloudFormation Hooks for stack and change set target invocation points</h2><p>First, we introduced <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/11/aws-cloudformation-hooks-stack-change-set-target-points/\">stack and change set target invocation points for CloudFormation Hooks</a>. This extends Hooks beyond individual resource validation, allowing you to run validation checks against entire templates and examine resource relationships. For example, you can now create hooks that validate architectural patterns across multiple resources or enforce team-specific deployment standards. With the change set invocation point, you can automate your change set reviews and reduce the time needed to resolve compliance issues. Refer to the <a href=\"https://docs.aws.amazon.com/cloudformation-cli/latest/hooks-userguide/what-is-cloudformation-hooks.html\">Hooks developer guide</a> to learn more.</p><h2>Managed hooks for the CloudFormation Guard domain specific language</h2><p><strong>Figure 11: CloudFormation Hooks‚Äô Guard language feature</strong></p><h2>Managed hooks for AWS Lambda functions</h2><h2>CloudFormation Hooks for AWS Cloud Control API target invocation points</h2><p>Lastly, we extended <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/11/aws-cloudformation-hooks-cloud-control-api-configurations-evaluation/\">Hooks to support AWS Cloud Control API (CCAPI) resource configurations</a>. This means your existing resource Hooks can now evaluate configurations from CCAPI create and update operations, allowing you to standardize your proactive control evaluation regardless your IaC tool. If you‚Äôre already using pre-built Lambda or Guard hooks, you simply need to specify ‚ÄúCloud_Control‚Äù as a target in your hooks‚Äô configuration to extend their coverage. Learn the detail of this feature from this <a href=\"https://aws.amazon.com/blogs/devops/introducing-aws-cloudformation-hooks-invoked-via-aws-cloud-control-api-ccapi/\">AWS DevOps Blog</a>.<img loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/17/CloudFormation-Hooks-for-AWS-Cloud-Control-API-target-invocation-point.png\" alt=\"CloudFormation Hooks for AWS Cloud Control API target invocation point\" width=\"1831\" height=\"754\"><strong>Figure 13: CloudFormation Hooks for AWS Cloud Control API target invocation point</strong></p><h2>StackSets ListStackSetAutoDeploymentTargets API</h2><p>In March, we enhanced <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/03/visibility-auto-deployment-configuration-stacksets-api/\">StackSets with the ListStackSetAutoDeploymentTargets API</a>. This new capability gives you better visibility into your auto-deployment configurations by allowing you to list existing target Organizational Units (OUs) and AWS Regions for a given stack set. Instead of logging into individual accounts to understand your deployment scope, you can now get this information in a single API call.</p><h2>CloudFormation Git sync with request review support</h2><p>In September, we improved <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/09/aws-cloudformation-git-sync-supports-pull-request-workflows/\">CloudFormation Git sync with pull request workflow support</a>. When you create or update a pull request in a linked repository, CloudFormation automatically posts change set information as PR comments. This integration provides a clear overview of proposed changes within your familiar Git workflow, allowing team members to review infrastructure changes alongside code changes. Visit our <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/git-sync.html\">user guide</a> and <a href=\"https://aws.amazon.com/blogs/devops/automate-safe-aws-cloudformation-deployments-from-github/\">launch blog</a> to learn more.</p><h2>Reshape your AWS CloudFormation stacks seamlessly with stack refactoring</h2><p>In February 2025, CloudFormation introduced a new capability called <a href=\"https://aws.amazon.com/about-aws/whats-new/2025/02/reshape-aws-cloudformation-stack-refactoring/\">stack refactoring</a> that makes it easy to reorganize cloud resources across your CloudFormation stacks. Stack refactoring enables you to move resources from one stack to another, split monolithic stacks into smaller components, and rename the logical name of resources within a stack. This enables you to adapt your stacks to meet architectural patterns, operational needs, or business requirements. To explore an example scenario, read <a href=\"https://aws.amazon.com/blogs/devops/introducing-aws-cloudformation-stack-refactoring/\">Introducing AWS CloudFormation Stack Refactoring.</a></p><p>Here are some resources to help you get started learning and using CloudFormation to manage your cloud infrastructure:</p><p>As we are starting 2025, our focus remains on making infrastructure deployment faster, safer, and more manageable. These enhancements reflect our commitment to solving real customer challenges and improving the CloudFormation experience. We are excited about the roadmap ahead and look forward to bringing you more innovations in 2025.</p><p>We encourage you to try these new features and share your feedback. For more detailed information about any of these launches, visit our <a href=\"https://docs.aws.amazon.com/cloudformation/\">documentation</a> or check out the <a href=\"https://aws.amazon.com/blogs/devops/\">AWS DevOps Blog.</a></p>","contentLength":10002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Whats the most kubefriendly pubsub messaging broker?","url":"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/","date":1739831242,"author":"/u/leeliop","guid":2089,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Like rabbitmq or even amazon sns?</p> <p>Or is it easier just using sns if we are in eks/amazon managed k8s land?</p> <p>Its for enterprise messaging volume, not particularly complex but just lots of it</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/leeliop\"> /u/leeliop </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irwur1/whats_the_most_kubefriendly_pubsub_messaging/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[ Removed by Reddit ]","url":"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/","date":1739826481,"author":"/u/Vw-Bee5498","guid":3977,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>[ Removed by Reddit on account of violating the <a href=\"/help/contentpolicy\">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vw-Bee5498\"> /u/Vw-Bee5498 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iruwhp/removed_by_reddit/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"North Korea‚Äôs Lazarus Group Targets Developers, Supply Chain","url":"https://devops.com/north-koreas-lazarus-group-targets-developers-supply-chain/","date":1739826050,"author":"Jeff Burt","guid":2013,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[ Removed by Reddit ]","url":"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/","date":1739823237,"author":"/u/Vw-Bee5498","guid":4025,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>[ Removed by Reddit on account of violating the <a href=\"/help/contentpolicy\">content policy</a>. ]</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Vw-Bee5498\"> /u/Vw-Bee5498 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irtk27/removed_by_reddit/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solving ISO 8583 Logon-Logoff Issues: Deployment Strategies & Industry Trends","url":"https://blog.devops.dev/solving-iso-8583-logon-logoff-issues-deployment-strategies-industry-trends-4324fe3cca04?source=rss----33f8b2d9a328---4","date":1739813923,"author":"M Mahdi Ramadhan, M. Si","guid":1973,"unread":true,"content":"<p>Working with  in banking and payment systems, I‚Äôve encountered numerous challenges‚Ää‚Äî‚Ääone of the most frustrating being  when using <strong>persistent TCP connections</strong>.</p><p>1. A client sends a ‚Äú0800 logoff‚Äù request to the server.2. The server acknowledges with ‚Äú0810 logoff‚Äù success.<p>3. However, the TCP connection remains open instead of closing.</p>4. The client attempts to ‚Äúlogon again‚Äù, but the session gets stuck.<p>5. The only way to recover is by ‚Äúforcing the client to send another logoff request‚Äù.</p></p><p>This issue can lead to failed transactions, system delays, and manual intervention‚Ää‚Äî‚Ääall of which are unacceptable in real-time financial systems. Let‚Äôs break down why this happens and how to fix it, along with the best deployment strategies (Dedicated Server vs. Kubernetes) and how Indonesia‚Äôs banking sector is shifting to REST APIs under SNAP (Standard Open API Pembayaran).</p><h3>How ISO 8583 Handles Logon &amp;&nbsp;Logoff</h3><p>ISO 8583 is a message-based protocol where communication occurs over a persistent TCP/IP connection. It uses Message Type Indicators (MTI) to differentiate between message&nbsp;types:</p><ul><li>0800‚Ää‚Äî‚ÄäLogon or Logoff&nbsp;Request</li><li>0810‚Ää‚Äî‚ÄäLogon or Logoff&nbsp;Response</li></ul><ol><li>Client ‚Üí Sends ‚Äú0800 Logoff Request‚Äù.</li><li>Server ‚Üí Responds with ‚Äú0810 Logoff Success Response‚Äù.</li><li>TCP connection should close properly.</li></ol><ol><li>Client ‚Üí Sends ‚Äú0800 Logon Request‚Äù.</li><li>Server ‚ÜíResponds with ‚Äú0810 Logon Success Response‚Äù.</li><li>Connection remains open for transaction processing.</li></ol><h4>What Actually Happens Sometimes</h4><ol><li>Client ‚Üí Sends ‚Äú0800 Logoff Request‚Äù.</li><li>Server ‚Üí Responds with ‚Äú0810 Logoff Success Response‚Äù.</li><li>TCP connection remains open instead of&nbsp;closing.</li><li>New logon attempts fail because the previous session is still&nbsp;active.</li></ol><h3>Common Causes of Logon-Logoff Issues</h3><ol><li>TCP Connection Not Closing&nbsp;Properly</li></ol><ul><li>The server does not explicitly close the socket, leaving the connection in an  state.</li><li>The client assumes the session is terminated, but the connection persists, causing conflicts.</li></ul><p>2. Client Does Not Send a FIN or RST&nbsp;Packet</p><ul><li>Proper TCP connection teardown requires a  from the client after receiving the logoff response.</li><li>If not sent, the connection remains , leading to session conflicts.</li></ul><p>3. Server Holding the Previous&nbsp;Session</p><ul><li>Some servers delay session termination, assuming the client may reconnect soon.</li><li>This prevents new logon attempts.</li></ul><p>4. TCP Keep-Alive Settings</p><ul><li>TCP Keep-Alive prevents inactive connections from closing.- If not properly managed, stale connections block new&nbsp;logons.</li></ul><p>5. Improper Exception Handling in Client&nbsp;Code</p><ul><li>If the client does not close sockets properly, the connection remains open indefinitely.</li></ul><h4>Solution 1: Ensure the Client Closes the TCP Connection</h4><p>After receiving logoff success, the client must explicitly close the connection.</p><pre>import socketdef send_logoff_request(host, port):        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)<p>        client_socket.connect((host, port))</p>        logoff_request = b'\\x00\\x10\\x08\\x00' # Example 0800 logoff message<p>        client_socket.send(logoff_request)</p>        response = client_socket.recv(1024)<p>        print(\"Received response:\", response)</p>        client_socket.shutdown(socket.SHUT_RDWR)        print(\"Connection closed.\")        print(\"Error:\", e)<p>send_logoff_request(\"127.0.0.1\", 5000)</p></pre><h4>Solution 2: Implement Server-Side Timeout</h4><ul><li>Ensure the server closes connections that remain idle for too&nbsp;long.</li><li>Configure timeout settings at both the server and network&nbsp;level.</li></ul><pre>import socketimport threading<p>def handle_client(client_socket):</p>    client_socket.settimeout(10) # Close inactive connections after 10 seconds        while True:<p>            data = client_socket.recv(1024)</p>            if not data:                print(\"Received:\", data)<p>                client_socket.send(b'\\x00\\x10\\x08\\x10') # Example 0810 response</p>    except socket.timeout:<p>        print(\"Timeout reached, closing connection.\")</p>    finally:server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)<p>server_socket.bind((\"127.0.0.1\", 5000))</p>server_socket.listen(5)<p>print(\"Server listening‚Ä¶\")</p>while True:<p>    client, addr = server_socket.accept()</p>    print(\"Accepted connection from:\", addr)<p>    threading.Thread(target=handle_client, args=(client,)).start()</p></pre><p>- Ensure the server closes connections that remain idle for too long.- Configure timeout settings at both the server and network&nbsp;level.</p><h4>Solution 3: Use Connection Pooling</h4><ul><li>Implement connection pooling to manage persistent connections efficiently.</li><li>Avoid excessive connection reuse, which may cause session conflicts.</li></ul><pre>import queueimport socket    def __init__(self, host, port, max_size=5):        self.port = port<p>        self.pool = queue.Queue(max_size)</p>        for _ in range(max_size):<p>            self.pool.put(self.create_connection())</p>    def create_connection(self):<p>        conn = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</p>        conn.connect((self.host, self.port))    def get_connection(self):    def release_connection(self, conn):pool = ConnectionPool(\"127.0.0.1\", 5000)<p>conn = pool.get_connection()</p>conn.send(b'Example Request')print(\"Received:\", data)<p>pool.release_connection(conn)</p></pre><ul><li>Implement connection pooling to manage persistent connections efficiently.</li><li>Avoid excessive connection reuse, which may cause session conflicts.</li></ul><h4>Solution 4: Enforce Explicit Disconnection Policies</h4><ul><li>Ensure that the server actively terminates lingering sessions after&nbsp;logoff.</li><li>Use logging and monitoring to track inactive connections.</li><li>Ensure that the server actively terminates lingering sessions after logoff.- Use logging and monitoring to track inactive connections.</li></ul><pre>import socketimport time<p>def enforce_disconnection(client_socket, idle_time=5):</p>    last_activity = time.time()        try:<p>            client_socket.settimeout(1)</p>            data = client_socket.recv(1024)                last_activity = time.time()<p>                if time.time()-last_activity &gt; idle_time:</p>                    print(\"Idle timeout reached, disconnecting client.\")                    break            continue<p>server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</p>server_socket.bind((\"127.0.0.1\", 5000))print(\"Server listening‚Ä¶\")    client, addr = server_socket.accept()<p>    print(\"Accepted connection from:\", addr)</p>    enforce_disconnection(client)</pre><h3>Dedicated Server vs. Kubernetes for ISO&nbsp;8583</h3><p>Many IT industries have adopted  as a powerful orchestration solution due to its ability to efficiently manage multiple nodes, optimizing performance and reducing operational costs. However, for services using , I strongly recommend deploying them on a . The following table summarizes the key differences:</p><ul><li><strong>For stable, long-lived ISO 8583 connections ‚Üí Use a Dedicated Server.</strong></li><li><strong>For scalable, cloud-native transactions (REST APIs) ‚Üí Use Kubernetes.</strong></li></ul><h3>Industry Trend: Migration to REST APIs &amp; SNAP (Standard Open API Pembayaran)</h3><p>Several Indonesian banking gateway are transitioning from ISO 8583 to REST APIs such as <a href=\"https://www.jaringanprima.co.id/id/tentang-rintis\">Rintis</a> for payment method Real Time Online Transfer and QRIS, following Bank Indonesia‚Äôs SNAP standard. According to <a href=\"https://www.aspi-indonesia.or.id\">ASPI Indonesia</a>, this transition is driven by the need for a more flexible and interoperable payment ecosystem that aligns with modern banking&nbsp;demands.</p><ul><li>Easier Scaling‚Äî REST APIs work well in cloud-native environments.</li><li>Interoperability‚Ää‚Äî‚ÄäSNAP ensures easy integration with fintech platforms.</li><li>Simplified Session Management‚Ää‚Äî‚ÄäNo need for persistent TCP connections.</li></ul><ul><li>ISO 8583 remains for ATM/POS transactions.</li><li>REST APIs handle mobile banking &amp; digital&nbsp;wallets.</li><li>API gateways convert ISO 8583 to REST, easing the transition.</li></ul><p>The ISO 8583 logon-logoff issue is mainly caused by stale TCP connections. Choosing between dedicated servers and Kubernetes depends on connection stability vs. scalability. Meanwhile, Indonesia‚Äôs banking industry is shifting towards REST APIs under SNAP, making hybrid models the best strategy moving&nbsp;forward.</p><p>In conclusion, you can either implement all the solutions together for a more comprehensive approach or choose the one that best fits your specific needs. Additionally, feel free to share any alternative solutions or insights in the comments, as collaboration can lead to even better optimization and problem-solving strategies.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4324fe3cca04\" width=\"1\" height=\"1\" alt=\"\">","contentLength":8160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quick and Easy Spring Security: Implement OAuth 2.0 & OpenID Connect in Minutes","url":"https://blog.devops.dev/quick-and-easy-spring-security-implement-oauth-2-0-openid-connect-in-minutes-64e2f1893199?source=rss----33f8b2d9a328---4","date":1739813915,"author":"Rajeev Kumar","guid":1972,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automating Linux Server Setup with Bash Scripts","url":"https://blog.devops.dev/automating-linux-server-setup-with-bash-scripts-5b9d367f6682?source=rss----33f8b2d9a328---4","date":1739813913,"author":"Obafemi","guid":1971,"unread":true,"content":"<div><p>Always setting up a Linux server manually can feel repetitive and time-consuming, especially when dealing with multiple servers or‚Ä¶</p></div>","contentLength":133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incorporating Custom Metrics in Python Flask E-Commerce Apps with Prometheus & Grafana","url":"https://blog.devops.dev/incorporating-custom-metrics-in-python-flask-e-commerce-apps-with-prometheus-grafana-60feec84e865?source=rss----33f8b2d9a328---4","date":1739813902,"author":"Subbareddysangham","guid":1970,"unread":true,"content":"<p>Monitoring is essential in modern applications to ensure system health, detect issues, and optimize performance. I have integrated Prometheus metrics using Python and Flask to achieve robust observability in our E-Commerce Application. These metrics provide insights into system behavior, performance, and user activity, which are visualized using .</p><ul><li> Metrics collection and&nbsp;storage</li><li>Visualization and dashboards</li><li><strong><em>Custom Python decorators:</em></strong> Metric instrumentation</li><li> Deployment and service monitoring</li></ul><h3>Why These Metrics Matter for SREs and Business&nbsp;Success?</h3><p><strong>For SREs (Site Reliability Engineers):</strong>These metrics act as the , helping SREs ensure <strong>high availability, reliability, and performance</strong>.‚úÖ  help detect slow responses, errors, and bottlenecks before users complain.‚úÖ  ensure queries run efficiently and prevent downtime due to overloaded connections.‚úÖ  track logins and active sessions, helping identify potential security issues or unusual spikes.‚úÖ  enable ‚Ää‚Äî‚ÄäKubernetes or monitoring tools can restart failing services instantly.</p><p><strong>For Business Decision-Making:</strong>Metrics aren‚Äôt just technical‚Ää‚Äî‚Ääthey üí∞  helps analyze customer purchasing behavior and identify checkout issues.üõí  show why users abandon carts, guiding UX improvements.üìä  ensures a fast, smooth experience, reducing drop-off rates and boosting sales.üö¶  allows proactive issue resolution, keeping customers happy and&nbsp;engaged.</p><h3>Python Flask E-Commerce Application Deployment in&nbsp;EKS:</h3><h3>Prometheus &amp; Grafana Setup Guide for&nbsp;EKS:</h3><h3>Metrics Implementation with Prometheus</h3><p>I use middleware and decorators to capture and log these metrics automatically.</p><p>‚úÖ ‚Ää‚Äî‚ÄäTracks API request count () and latency (<strong>http_request_duration_seconds</strong>).‚úÖ  ‚Äì Monitors orders (), cart operations ().‚úÖ  ‚Äì Tracks active DB connections () and query latency (<strong>db_query_duration_seconds</strong>).‚úÖ  ‚Äì Logs active sessions () and login attempts ().</p><ul><li> ‚Üí  (Ensures service is running).</li><li> ‚Üí  (Checks DB &amp; system&nbsp;health).</li><li> ‚Üí <strong>Prometheus scrape endpoint</strong> for collecting real-time data.</li></ul><h3>Here‚Äôs how to check these metrics internally:</h3><ol><li><strong><em>Using kubectl port-forward:</em></strong></li></ol><pre># Port forward the servicekubectl port-forward svc/ecommerce-backend -n shopeasy-dev 5000:80<p># In another terminal, test endpoints</p>curl http://localhost:5000/api/health/live<p>curl http://localhost:5000/api/health/ready</p>curl http://localhost:5000/api/metrics</pre><pre># Get into the podkubectl exec -it $(kubectl get pod -l app=ecommerce,tier=backend -n shopeasy-dev -o jsonpath='{.items[0].metadata.name}') -n shopeasy-dev -- bash<p># Check metrics internally</p>curl http://localhost:5000/api/health/live<p>curl http://localhost:5000/api/health/ready</p>curl http://localhost:5000/api/metrics</pre><p><strong><em>3. Using LoadBalancer URL:</em></strong></p><pre># Get your LoadBalancer URLexport LB_URL=$(kubectl get svc ecommerce-backend -n shopeasy-dev -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')curl http://$LB_URL/api/health/live<p>curl http://$LB_URL/api/health/ready</p>curl http://$LB_URL/api/metrics</pre><p><strong><em>4. Check using service&nbsp;DNS:</em></strong></p><pre># From another pod in the same namespacecurl http://ecommerce-backend.shopeasy-dev.svc.cluster.local/api/health/live<p>curl http://ecommerce-backend.shopeasy-dev.svc.cluster.local/api/health/ready</p>curl http://ecommerce-backend.shopeasy-dev.svc.cluster.local/api/metrics</pre><p>Tracking HTTP requests helps in understanding traffic patterns, API latency, and error&nbsp;rates.</p><ul><li> (): Captures the number of incoming API requests categorized by <strong>method, endpoint, and status&nbsp;code</strong>.</li><li> (<strong>http_request_duration_seconds</strong>): Records the duration of API responses for performance analysis.</li></ul><pre>from prometheus_client import Counter, Histogram, Gauge, CollectorRegistryfrom functools import wraps<p>from flask import request</p>import time<p># Create a global registry</p>REGISTRY = CollectorRegistry()REQUEST_COUNT = Counter(    'Total HTTP requests',<p>    ['method', 'endpoint', 'status'],</p>    registry=REGISTRY<p>REQUEST_LATENCY = Histogram(</p>    'http_request_duration_seconds',<p>    'HTTP request latency in seconds',</p>    ['method', 'endpoint'],)</pre><h3>1.2 Business-Specific Metrics</h3><p>Monitoring business-specific transactions provides insights into customer interactions.</p><ul><li> (): Tracks the number of successful and failed&nbsp;orders.</li><li> (): Records actions like <strong>adding, removing, and checking out items</strong> from the&nbsp;cart.</li></ul><pre># Business MetricsORDER_COUNT = Counter(    'Total orders placed',<p>    ['status'],  # success, failed</p>    registry=REGISTRY<p>CART_OPERATIONS = Counter(</p>    'cart_operations_total',    ['operation'],  # add, remove, checkout)</pre><p>Understanding user behavior allows us to track login trends and active sessions.</p><ul><li> (): Tracks the number of currently active&nbsp;users.</li><li> (): Records successful and failed&nbsp;logins.</li></ul><pre># User Metrics<p>USER_SESSION_COUNT = Gauge(</p>    'user_sessions_active',<p>    'Number of active user sessions',</p>    registry=REGISTRY<p>USER_LOGIN_COUNT = Counter(</p>    'user_logins_total',<p>    'Total number of user logins',</p>    ['status'],  # success, failed)</pre><h3>Grafana Dashboards for Insights:</h3><p>üìä  Request trends, error rates, response latency.üìä  Order success/failure rates, cart activities.üìä  Query response time, active DB connections.üìä  CPU, memory usage, uptime monitoring.</p><p>This  ensures <strong>scalability, reliability, and proactive issue detection</strong> in our E-Commerce platform. üöÄ</p><pre>kubectl get svc -n monitoring grafana-external</pre><ul><li>Password: (from kubectl get&nbsp;secret)</li></ul><ul><li>Click ‚Äò+ Create‚Äô &gt;&nbsp;‚ÄòImport‚Äô</li><li>Click ‚ÄòCreate New Dashboard‚Äô</li></ul><h3><strong><em>1. API Performance Panel:</em></strong></h3><pre>{  \"title\": \"API Response Times\",  \"datasource\": \"Prometheus\",    {<p>      \"expr\": \"rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])\",</p>      \"legendFormat\": \"{{endpoint}}\"  ]</pre><p><strong><em>First, create a new panel in your dashboard:</em></strong></p><ul><li>Click ‚ÄúAdd panel‚Äù (+&nbsp;icon)</li></ul><ul><li>Select ‚ÄúPrometheus‚Äù as data&nbsp;source</li><li>In the Metrics browser, enter this PromQL&nbsp;query:</li></ul><pre>rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])</pre><p><strong><em>Configure Panel Settings (right&nbsp;side):</em></strong></p><ul><li>Title: ‚ÄúAPI Response&nbsp;Times‚Äù</li><li>Panel options &gt; Description: ‚ÄúAverage response time by endpoint‚Äù</li></ul><p><strong><em>Under Visualization settings:</em></strong></p><ul><li>Title: ‚ÄúAPI Response&nbsp;Times‚Äù</li><li>Description: ‚ÄúAverage API response times in&nbsp;seconds‚Äù</li></ul><ul></ul><ul><li>Show points: When threshold is&nbsp;crossed</li></ul><ul></ul><ul><li>Add warning at 0.5 seconds&nbsp;(yellow)</li><li>Add critical at 1 second&nbsp;(red)</li></ul><ul><li>Click ‚ÄúApply‚Äù in the top-right corner</li><li>Then ‚ÄúSave‚Äù the dashboard</li></ul><pre># Per pod CPU usagesum by(pod) (<p>  rate(container_cpu_usage_seconds_total{namespace=\"shopeasy-dev\", container!=\"\"}[5m])</p>) * 100</pre><ul><li>Show CPU usage as a percentage (multiplied by&nbsp;100)</li><li>Focus on your shopeasy-dev namespace</li><li>Use a 5-minute rate to smooth out&nbsp;spikes</li></ul><pre>sum by(pod) (  kube_pod_container_resource_limits{namespace=\"shopeasy-dev\", resource=\"cpu\"}</pre><pre># Total requests by status codesum(rate(http_request_total[5m])) by (status)sum(rate(http_request_total{status=~\"5.*\"}[5m])) / sum(rate(http_request_total[5m])) * 100sum(rate(http_request_total[5m])) by (endpoint)</pre><h3><strong><em>HTTP Request Duration (Latency):</em></strong></h3><pre># Order rate (orders per minute)sum(rate(orders_total[5m]))sum(rate(orders_total{status=\"success\"}[5m])) / sum(rate(orders_total[5m])) * 100sum(rate(orders_total{status=\"failed\"}[5m])) / sum(rate(orders_total[5m])) * 100<p># Total orders in last 24 hours</p>sum(increase(orders_total[24h]))</pre><pre># Cart operations by typesum(rate(cart_operations_total[5m])) by (operation)<p># Cart abandonment rate (checkout vs add)</p>1 - (sum(rate(cart_operations_total{operation=\"checkout\"}[1h])) / sum(rate(cart_operations_total{operation=\"add\"}[1h])))sum(increase(cart_operations_total[1h])) by (operation)</pre><pre># Current active sessionsuser_sessions_activedelta(user_sessions_active[1h])<p># Peak sessions in last day</p>max_over_time(user_sessions_active[24h])</pre><pre># Login success ratesum(rate(user_logins_total{status=\"success\"}[5m])) / sum(rate(user_logins_total[5m])) * 100sum(rate(user_logins_total{status=\"failed\"}[5m]))sum(increase(user_logins_total[24h]))</pre><p>Integrating  with  in a <strong>Python Flask E-Commerce application</strong> provides  into user activity, API performance, and system health. By tracking <strong>user sessions, logins, orders, cart operations, and database performance</strong>, we ensure better <strong>reliability, scalability, and business&nbsp;growth</strong>.</p><p>With , teams can easily <strong>visualize trends, detect issues early, and improve user experience</strong>. This monitoring setup helps both  make data-driven decisions, ensuring a <strong>smooth, high-performance application</strong> for customers. üöÄ</p><p><em>I‚Äôd love to hear what you think about this article‚Ää‚Äî‚Ääfeel free to share your opinions in the comments below (or above, depending on your device!). If you found this helpful or enjoyable, a clap, a comment, or a highlight of your favourite sections would mean a&nbsp;lot.</em></p><p><em>For more insights into the world of technology and data, visit </em><a href=\"http://www.subbutechops.com/\"></a><em> There‚Äôs plenty of exciting content waiting for you to&nbsp;explore!</em></p><p><em>Thank you for reading, and happy learning! üöÄ</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=60feec84e865\" width=\"1\" height=\"1\" alt=\"\">","contentLength":8711,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mastering Capacity Planning For Microsoft Fabric F64","url":"https://blog.devops.dev/mastering-capacity-planning-for-microsoft-fabric-f64-e7f9a9333318?source=rss----33f8b2d9a328---4","date":1739813897,"author":"Noel","guid":1969,"unread":true,"content":"<p>Capacity planning is a critical yet often overlooked aspect of managing cloud-based services. Organizations today are increasingly beginning to rely on platforms like  to power their operations, analytics, and decision-making. However, without a robust strategy for capacity planning, businesses risk overprovisioning‚Ää‚Äî‚Ääleading to unnecessary costs‚Ää‚Äî‚Ääor underprovisioning, which can result in system slowdowns, inefficiencies, and user dissatisfaction.</p><p>This article aims to provide a <strong>comprehensive guide to capacity planning for Microsoft Fabric F64</strong>, focusing on actionable insights, technical strategies, and industry best practices. Whether we‚Äôre migrating workloads, scaling operations, or optimizing costs, our Fabric specialists at <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Mastering+Capacity+Planning+For+Microsoft+Fabric%C2%A0F64\">Xinthe</a> have found these insights to be crucial in making informed decisions and maximizing the value of our investment in Fabric&nbsp;F64.</p><p>By the end of this article, you too will have gained a <strong>deep understanding of how to approach capacity planning for Microsoft Fabric F64</strong>, equipped with the tools and strategies to make data-driven decisions, optimize performance, and future-proof your cloud infrastructure.</p><h3><strong>What Is Microsoft Fabric&nbsp;F64?</strong></h3><ul><li><strong><em>Overview Of Microsoft Fabric</em></strong></li></ul><p>Microsoft Fabric is a  designed to simplify and enhance data management, analytics, and AI-driven solutions. Built on Microsoft‚Äôs robust cloud infrastructure, it integrates seamlessly with services like Azure and Power BI to offer  for data processing, storage, and visualization.</p><p>Key capabilities of Microsoft Fabric include&nbsp;-</p><p>: Support for diverse data sources, both on-premises and cloud-based, ensuring flexibility and scalability.</p><p>: Tools to process and analyze data, generate insights, and drive decision-making using machine learning&nbsp;models.</p><p>: Seamless integration with Power BI for visualization and reporting, enabling teams to work collaboratively on data-driven projects.</p><p>: Built-in optimizations for workloads ranging from real-time analytics to large-scale batch processing.</p><p>In essence, Microsoft Fabric serves as a <strong>foundation for modern data ecosystems</strong>, allowing organizations to harness the power of their data effectively while leveraging advanced AI and analytics capabilities.</p><ul><li><strong><em>Understanding The F64&nbsp;Tier</em></strong></li></ul><p>The  tier is a specific offering within Microsoft Fabric, designed to deliver <strong>enterprise-grade performance and scalability</strong>. It is part of the Fabric series that categorizes resources based on computational power and memory to meet varying workload&nbsp;demands.</p><p>Key specifications of the F64 tier include&nbsp;-</p><p>: Provides ample RAM to handle large datasets and complex computations, ensuring smooth performance even under heavy workloads.</p><p>: Equipped with sufficient CPU cores and processing capabilities to support compute-intensive operations, such as data transformations and machine learning workloads.</p><p>: Designed to support dynamic scaling, allowing organizations to seamlessly increase or decrease resources based on workload&nbsp;demands.</p><p><strong>Integration With Fabric Services</strong>: Full compatibility with other Microsoft services, including Azure Data Lake, Synapse Analytics, and Power&nbsp;BI.</p><p>: Typically offered under a pay-as-you-go or reserved capacity model, enabling flexibility and cost predictability.</p><p>With its powerful infrastructure, the F64 tier is tailored to handle <strong>high-performance workloads</strong> that demand reliability, speed, and efficiency.</p><p>The F64 capacity tier is ideal for <strong>enterprise-level scenarios</strong> and <strong>data-intensive applications</strong>. Here are some specific use cases where it shines&nbsp;-</p><p>Organizations with hundreds or thousands of users accessing shared resources will benefit from F64‚Äôs ability to manage high concurrent usage without performance degradation.</p><p>Businesses that rely on extensive data analysis, such as those in finance, healthcare, or retail, can leverage F64 to process large datasets efficiently and generate actionable insights.</p><p><strong>AI &amp; Machine Learning Workloads</strong>:</p><p>F64‚Äôs compute power is well-suited for training machine learning models, running predictive analytics, and deploying AI solutions at&nbsp;scale.</p><p><strong>Hybrid &amp; Multi-Cloud Environments</strong>:</p><p>Companies operating in hybrid setups, with data spread across on-premises and cloud platforms, can use F64 for seamless integration and consistent performance.</p><p><strong>Organizations Transitioning To Premium Capacities</strong>:</p><p>Businesses currently using lower-tier solutions (e.g., Pro licenses or smaller capacities) and experiencing performance bottlenecks can migrate to F64 to unlock advanced features and higher capacity thresholds.</p><h3><strong>Key Principles Of Capacity&nbsp;Planning</strong></h3><p>Effective capacity planning is a critical step in managing cloud resources, ensuring that your infrastructure is both efficient and scalable. This section explores the foundational principles of capacity planning, guiding organizations to make informed decisions that balance workload requirements, performance, cost, and future&nbsp;growth.</p><p>Workloads are the backbone of capacity planning. Each workload has unique requirements that influence the resources it demands. Categorizing workloads allows for precise allocation of resources, reducing waste and preventing bottlenecks.</p><p>Examples: Real-time analytics, machine learning model training, large-scale simulations.</p><p>Resource Needs: High CPU and GPU utilization with moderate memory&nbsp;usage.</p><p>Solution: Prioritize capacity tiers like F64 that provide powerful compute resources.</p><p>Examples: Archival data storage, data lakes, large-scale ETL operations.</p><p>Resource Needs: High disk space and I/O performance with moderate compute&nbsp;power.</p><p>Solution: Focus on storage optimization and integration with services like Azure Data&nbsp;Lake.</p><p>Examples: Data visualization with Power BI, hybrid cloud applications.</p><p>Resource Needs: Balanced requirements for CPU, memory, and&nbsp;storage.</p><p>Solution: Select scalable capacities like F64 to handle variable&nbsp;demands.</p><p>: Conduct a  to categorize and document each workload‚Äôs requirements before selecting a capacity&nbsp;tier.</p><p>Monitoring and understanding key performance metrics are essential for matching workload demands with capacity capabilities. These metrics help assess the current state of infrastructure and predict future&nbsp;needs.</p><p>Measures the percentage of compute resources being&nbsp;used.</p><p>High utilization indicates potential bottlenecks; underutilization suggests overprovisioning.</p><p>Tracks the amount of RAM consumed by active processes.</p><p>Memory shortages can lead to application crashes or degraded performance.</p><p>Measures the time it takes for a system to respond to a&nbsp;request.</p><p>Critical for real-time applications, where high latency impacts user experience.</p><p>Indicates the volume of data processed over&nbsp;time.</p><p>Useful for gauging how well the system handles high-demand scenarios.</p><p>: Use tools like Azure Monitor or Power BI to visualize and analyze these metrics in real time, ensuring optimal performance.</p><p>Capacity planning is not just about performance‚Ää‚Äî‚Ääit‚Äôs also about cost management. Striking the right balance between performance and budget constraints ensures that resources are used effectively without incurring unnecessary expenses.</p><p><strong>Pay-As-You-Go Vs. Reserved Capacity</strong>:</p><p>PAYG is ideal for fluctuating workloads, while reserved capacity offers predictable costs for steady usage patterns.</p><p><strong>Overprovisioning Vs. Underprovisioning</strong>:</p><p>Overprovisioning wastes resources and increases costs; underprovisioning can lead to performance degradation and user dissatisfaction.</p><p>Adjust resource allocation dynamically based on workload requirements. Use predictive analytics to forecast needs accurately.</p><p>: Continuously monitor cost vs. performance metrics and refine capacity allocation. Automate scaling where possible to optimize resource&nbsp;usage.</p><p>Modern infrastructure must be designed with scalability in mind. Capacity planning should not only address current needs but also anticipate future growth and unexpected demand&nbsp;surges.</p><p><strong>Key Elements Of Scalability</strong>:</p><p><strong>Vertical Scaling (Scaling&nbsp;Up)</strong>:</p><p>Adds more power (e.g., CPU, memory) to an existing instance.</p><p>Suitable for applications requiring intense but infrequent resource&nbsp;spikes.</p><p><strong>Horizontal Scaling (Scaling&nbsp;Out)</strong>:</p><p>Adds more instances to distribute the workload.</p><p>Ideal for scenarios with steady growth or consistent high&nbsp;demand.</p><p>Dynamically adjusts resources in real-time to match fluctuating demand.</p><p>Critical for unpredictable workloads, such as seasonal or event-driven applications.</p><p>: Use historical data to forecast future workload requirements.</p><p>: Simulate extreme demand to evaluate system resilience and prepare for peak&nbsp;loads.</p><p>: Build scalability into your capacity plan from the start. Ensure that your chosen tier, such as F64, supports seamless vertical and horizontal scaling.</p><h3><strong>Tools For Capacity&nbsp;Planning</strong></h3><p>Capacity planning for cloud-based platforms like Microsoft Fabric F64 requires a combination of estimation, simulation, monitoring, and automation. This section explores a suite of tools and techniques that simplify the process, offering actionable insights for estimating requirements, evaluating performance, and dynamically managing capacity.</p><ul><li><strong><em>Microsoft Fabric SKU Estimator</em></strong></li></ul><p>The <strong>Microsoft Fabric SKU Estimator</strong> is an invaluable tool designed to help organizations predict their capacity needs. By providing detailed insights into workload demands, it ensures that the selected SKU aligns with your operational requirements.</p><p>: Offers detailed analysis of resource consumption for various workloads.</p><p>: Helps identify the most cost-effective SKU without compromising performance.</p><p>: Provides projections for future needs based on historical data&nbsp;trends.</p><p><strong>How To Use The SKU Estimator</strong>:</p><p>: Visit Microsoft‚Äôs Fabric SKU Estimator portal.</p><p>: Enter workload specifics, such as concurrent user count, dataset sizes, and processing intensity.</p><p>: Review the suggested SKUs, including F64, along with cost and resource projections.</p><p>: Adjust inputs based on new data or planned changes in workload to refine the estimates.</p><p>: Pair the SKU Estimator with historical performance data to validate its recommendations.</p><p>Load testing is critical to understanding how a system performs under stress. Tools like the  simulate workloads to evaluate the resilience and efficiency of the selected capacity.</p><p><strong>Realistic Load Test Tool&nbsp;(RLT)</strong>:</p><p>Developed specifically for Microsoft Fabric and Power BI environments.</p><p>Allows users to simulate real-world workloads, such as concurrent report generation or heavy query&nbsp;loads.</p><p>Provides metrics like CPU usage, memory utilization, and response times under varying levels of&nbsp;stress.</p><p>:</p><p>: A versatile tool for simulating web and application loads, suitable for general-purpose testing.</p><p>: Lightweight and efficient, great for creating custom load-testing scenarios.</p><p><strong>How To Conduct Load&nbsp;Testing</strong>:</p><p>Define the test scenario, such as concurrent report usage or dataset querying.</p><p>Set up the tool to simulate real-world user behavior and load patterns.</p><p>Monitor key metrics during testing, including latency, throughput, and resource utilization.</p><p>Analyze the results to identify bottlenecks and evaluate the capacity‚Äôs suitability.</p><p>: Perform load tests on non-production environments to avoid service disruptions.</p><ul><li><strong><em>Monitoring &amp; Analytics Tools</em></strong></li></ul><p>Continuous monitoring is essential for effective capacity planning. Tools like Azure Monitor, Power BI, and third-party options provide real-time insights into resource utilization and system performance.</p><p>Tracks metrics such as CPU utilization, memory consumption, and network activity across your Fabric instances.</p><p>Provides alerts and notifications for anomalies or performance degradation.</p><p>Supports customizable dashboards for a unified view of resource&nbsp;usage.</p><p>Integrates seamlessly with Fabric to visualize capacity utilization and workload performance.</p><p>Enables the creation of detailed reports for decision-makers to track trends and patterns.</p><p>: Offers powerful visualization and analytics capabilities, integrating with Azure and other cloud services.</p><p>: Excellent for collecting and analyzing time-series data, particularly in dynamic environments.</p><p>: Set up proactive alerts in Azure Monitor to address issues before they impact performance.</p><p>Automation can significantly enhance capacity management by reducing manual effort and ensuring quick responses to changing workloads. Scripts and APIs are particularly useful for scaling, provisioning, and monitoring resources.</p><p><strong>Using Automation For Capacity Management</strong>:</p><p>: Use Azure APIs to automate scaling based on workload thresholds.</p><p>Example: Automatically increase capacity when CPU utilization exceeds&nbsp;80%.</p><p>: Leverage Python or PowerShell scripts to adjust resource allocations in real&nbsp;time.</p><p>: Automate the collection and analysis of performance metrics using tools like Azure Functions.</p><p><strong>Example Automation Workflow</strong>:</p><p>: A script runs periodically or is triggered by a performance anomaly.</p><p>: The script scales up or down resources based on predefined conditions.</p><p>: Results are logged and visualized in a dashboard for future analysis.</p><p>: Combine Azure Logic Apps with APIs to create robust, automated workflows for capacity adjustments.</p><h3><strong>Step-By-Step Guide To Capacity Planning For&nbsp;F64</strong></h3><p>Implementing capacity planning for Microsoft Fabric F64 requires a structured approach to ensure optimal resource allocation, cost efficiency, and scalability. This section outlines a step-by-step process to help you assess workloads, simulate demand, select the appropriate capacity, and monitor its performance.</p><ul><li><strong><em>Step 1‚Ää‚Äî‚ÄäAssess Current Workloads</em></strong></li></ul><p>Before selecting a capacity, it‚Äôs essential to understand the demands of your existing workloads. This foundational step provides the data necessary for informed decision-making.</p><p><strong>Gather Data On Usage Patterns</strong>:</p><p>Identify the primary workloads your organization runs, such as data visualizations, machine learning training, or batch processing.</p><p>Document the number of active users, query complexity, and data volumes to establish baseline requirements.</p><p><strong>Track Historical Performance Metrics</strong>:</p><p>Use tools like  or  to analyze metrics such as&nbsp;-</p><p>: Understand compute intensity.</p><p>: Evaluate data-heavy operations.</p><p>: Monitor response times under typical and peak&nbsp;loads.</p><p>: Measure the volume of data processed per unit of&nbsp;time.</p><p>Analyze trends over time to identify recurring bottlenecks or underutilized resources.</p><p>: Create a workload inventory spreadsheet detailing each workload‚Äôs resource consumption, user base, and performance expectations.</p><ul><li><strong><em>Step 2‚Ää‚Äî‚ÄäSimulate Workloads</em></strong></li></ul><p>Simulation is a critical step in validating the resource requirements of your workloads. By running realistic load tests, you can measure how your system behaves under different conditions.</p><p><strong>Run Load Tests Using Representative Datasets</strong>:</p><p>Use tools like the  or  to simulate real-world usage patterns.</p><p>Include scenarios such as concurrent user access, complex query execution, and high-volume data ingestion.</p><p><strong>Measure Performance Under Varying Conditions</strong>:</p><p>Test workloads under peak traffic to identify performance limits.</p><p>Evaluate edge cases, such as simultaneous large-scale reports or heavy ETL operations.</p><p>Gather detailed metrics on CPU, memory, latency, and error rates during the&nbsp;tests.</p><p><strong>Example Load Testing Scenario</strong>:</p><p>Simulate 500 concurrent users generating reports in Power&nbsp;BI.</p><p>Measure the impact on query response time and system stability.</p><p>: Use the insights from load testing to create a stress map, highlighting areas where resources are over- or under-utilized.</p><ul><li><strong><em>Step 3‚Ää‚Äî‚ÄäChoose The Right&nbsp;Capacity</em></strong></li></ul><p>With workload data and test results in hand, the next step is selecting the appropriate F64 capacity. This involves balancing current needs, budget constraints, and future growth expectations.</p><p><strong>Match Workload Requirements To F64 Specifications</strong>:</p><p>Evaluate how the F64 tier‚Äôs compute power, memory, and storage align with your workloads.</p><p>Consider whether your applications are compute-heavy, storage-heavy, or require a mix of resources.</p><p><strong>Address Overprovisioning Vs. Underprovisioning</strong>:</p><p>: Provides a buffer for unexpected demand but increases costs.</p><p>: Saves on costs but risks degraded performance during peak&nbsp;usage.</p><p>: Aim for a balanced configuration that meets peak demands without excessive idle capacity.</p><p><strong>Leverage Predictive Insights</strong>:</p><p>Use the <strong>Microsoft Fabric SKU Estimator</strong> to refine your capacity selection based on current and projected workloads.</p><p>: Start with a Pay-As-You-Go F64 capacity and monitor its performance before committing to a reserved capacity.</p><ul><li><strong><em>Step 4‚Ää‚Äî‚ÄäMonitor After Deployment</em></strong></li></ul><p>Capacity planning doesn‚Äôt end with deployment. Continuous monitoring is essential to ensure your chosen capacity meets performance expectations and adapts to changing&nbsp;needs.</p><p><strong>Track Metrics Post-Deployment</strong>:</p><p>Regularly monitor key metrics such as CPU and memory utilization, latency, and error rates using tools like Azure&nbsp;Monitor.</p><p>Set up alerts for anomalies or resource limits to proactively address&nbsp;issues.</p><p><strong>Identify Opportunities For Scaling Up Or&nbsp;Down</strong>:</p><p>Scale up if demand consistently approaches resource limits, causing latency or performance degradation.</p><p>Scale down if resources are underutilized, reducing unnecessary costs.</p><p>Use automated scaling tools to dynamically adjust capacity based on real-time demand.</p><p><strong>Example Monitoring Workflow</strong>:</p><p>Monitor performance dashboards daily to ensure optimal resource utilization.</p><p>Conduct weekly reviews to analyze trends and adjust capacity if&nbsp;needed.</p><p>: Use  to automate capacity scaling, triggered by specific thresholds like CPU usage exceeding 80%.</p><h3><strong>Best Practices For Capacity Optimization</strong></h3><p>Optimizing capacity usage is a critical step in ensuring that your Microsoft Fabric F64 deployment operates efficiently, scales seamlessly, and stays within budget. By adopting best practices in resource allocation, cost management, proactive scaling, and performance tuning, organizations can maximize the value of their infrastructure.</p><ul><li><strong><em>Efficient Resource Allocation</em></strong></li></ul><p>Resource allocation is central to achieving optimal performance and avoiding wastage. By strategically managing workloads and pooling resources, you can ensure that your capacity is used effectively.</p><p>Divide workloads based on their characteristics (e.g., compute-heavy, storage-intensive, or latency-sensitive).</p><p>Assign high-priority tasks to dedicated resources to minimize competition for shared capacity.</p><p>For example, allocate real-time analytics workloads separately from batch processing tasks to prevent bottlenecks.</p><p>Consolidate resources for workloads with similar demands to reduce overhead.</p><p>Create shared resource pools for non-peak operations, such as nightly ETL jobs or&nbsp;backups.</p><p>Use the F64 capacity‚Äôs scalability features to dynamically allocate resources between shared and dedicated tasks.</p><p>: Use tools like Azure Resource Manager to define and manage resource groups for better allocation control.</p><p>Balancing performance with cost efficiency is a critical goal of capacity optimization. Leveraging the right pricing models and usage strategies can significantly reduce expenses.</p><p><strong>Pay-As-You-Go (PAYG)&nbsp;Model</strong>:</p><p>Ideal for organizations with fluctuating workloads.</p><p>Scale resources dynamically to match real-time demands, avoiding overpayment for idle capacity.</p><p>Commit to a specific capacity level (e.g., F64) for a fixed term to gain significant cost savings compared to&nbsp;PAYG.</p><p>Best suited for predictable, steady-state workloads with consistent resource&nbsp;needs.</p><p><strong>Trial Periods &amp; Hybrid&nbsp;Models</strong>:</p><p>Utilize trial periods to evaluate workload compatibility with F64 before full commitment.</p><p>Combine reserved and PAYG models to handle base workloads and peak surges efficiently.</p><p>: Continuously monitor cost-performance metrics and re-evaluate capacity commitments annually to optimize expenses.</p><p>Proactive scaling ensures that your system can adapt seamlessly to changes in demand without disrupting performance or incurring excessive costs. Predictive analytics and automated tools play a key role in this strategy.</p><p><strong>Predictive Analytics For Demand Forecasting</strong>:</p><p>Analyze historical data to identify usage trends and forecast future capacity&nbsp;needs.</p><p>Use machine learning models or built-in Azure features to predict peak usage periods or seasonal&nbsp;surges.</p><p>Configure automated scaling rules based on thresholds, such as CPU usage exceeding 80% or memory usage nearing capacity&nbsp;limits.</p><p>For example, configure Azure Autoscale to increase resources during high-traffic periods and reduce them during off-peak&nbsp;hours.</p><p><strong>Simulate Growth Scenarios</strong>:</p><p>Run stress tests to simulate future demand scenarios and ensure your F64 capacity can handle projected growth.</p><p>: Combine Azure Monitor with Logic Apps to automate preemptive scaling workflows for both vertical and horizontal scaling.</p><p>Optimizing resource utilization ensures that your workloads perform efficiently, reducing the risk of bottlenecks and improving user experience.</p><p>Use distributed caching systems, such as Redis, to minimize repeated data fetches from&nbsp;storage.</p><p>Cache frequently accessed data or intermediate results to reduce query execution time.</p><p>Optimize SQL and other queries by indexing critical columns, reducing unnecessary joins, and minimizing data transfers.</p><p>Leverage Power BI‚Äôs DirectQuery mode for real-time data access without overloading capacity.</p><p><strong>Efficient Data Partitioning</strong>:</p><p>Partition large datasets based on access patterns (e.g., time-based or region-based) to improve query performance.</p><p>Continuously monitor resource utilization metrics to identify underperforming processes.</p><p>Adjust workloads, reconfigure parameters, or refactor applications to improve efficiency.</p><p>: Use Azure Data Studio to profile queries and identify areas for optimization in large-scale data operations.</p><h3><strong>Common Challenges &amp; How To Overcome&nbsp;Them</strong></h3><p>Capacity planning is a dynamic and complex process, often accompanied by challenges that can hinder performance, inflate costs, or complicate user management. This section highlights the most common issues organizations face when managing capacity for Microsoft Fabric F64 and provides actionable solutions to address them effectively.</p><p>Underutilization occurs when allocated resources are not fully used, leading to wasted expenditure. For example, reserving an F64 capacity tier but consuming only a fraction of its compute or storage capabilities results in unnecessary costs.</p><p><strong>Risks Of Underutilization</strong>:</p><p>: Paying for resources that are not used impacts budgets without adding&nbsp;value.</p><p><strong>Missed Optimization Opportunities</strong>: Idle resources could be better reallocated to other workloads or projects.</p><p><strong>Monitor Utilization Rates</strong>:</p><p>Use tools like Azure Monitor or Power BI to continuously track CPU, memory, and storage utilization.</p><p>Set up alerts for sustained underutilization, such as CPU usage consistently below&nbsp;40%.</p><p>Periodically review and adjust capacity to match actual workload&nbsp;demands.</p><p>Consider transitioning from reserved to pay-as-you-go models for workloads with fluctuating usage.</p><p>Combine smaller, underutilized workloads into a single capacity to maximize efficiency.</p><p>: Perform quarterly utilization audits to identify and address underutilized resources.</p><p>Exceeding the allocated capacity can lead to degraded performance, increased latency, and even service interruptions. This is particularly common during unexpected spikes in&nbsp;demand.</p><p>: Users experience delays and timeouts.</p><p>: Critical applications may fail under excessive load.</p><p>Use throttling mechanisms to control resource-intensive tasks during peak&nbsp;periods.</p><p>Example: Limit the number of concurrent queries in Power BI to prevent CPU overload.</p><p>Leverage Azure Autoscale to dynamically adjust capacity based on real-time metrics.</p><p>Configure scaling rules, such as adding more resources when CPU utilization exceeds&nbsp;80%.</p><p>Use load balancers to distribute workloads evenly across available resources.</p><p><strong>Simulate Stress Scenarios</strong>:</p><p>Regularly conduct load tests using tools like the Realistic Load Test Tool to identify capacity limits and prepare for high-demand events.</p><p>: Establish peak load thresholds and ensure adequate buffer capacity is available during these&nbsp;periods.</p><p>Migrating workloads to a new capacity tier, such as F64, often involves technical, operational, and logistical hurdles. Poorly managed transitions can disrupt services and result in data loss or downtime.</p><p>:</p><p>: Critical applications may experience downtime during migration.</p><p>: Differences in capacity settings can cause compatibility issues.</p><p><strong>Plan &amp; Test The Migration</strong>:</p><p>Develop a detailed migration plan that outlines timelines, roles, and responsibilities.</p><p>Use staging environments to test workloads on the new capacity tier before full migration.</p><p>Schedule migrations during non-peak hours to reduce the impact on&nbsp;users.</p><p>Use tools like Azure Data Factory to streamline data migration processes.</p><p>Move workloads in phases, starting with low-priority tasks to identify and resolve issues&nbsp;early.</p><p>: Document all configurations and processes before migration to ensure they can be replicated accurately in the new capacity.</p><p>Managing access for large user bases‚Ää‚Äî‚Ääoften in the hundreds or thousands‚Ää‚Äî‚Ääcan be complex and prone to errors. Ensuring security, while maintaining efficient collaboration, is critical in multi-user environments.</p><p><strong>Common User Management Issues</strong>:</p><p><strong>Access Control Complexity</strong>: Balancing user permissions without compromising security.</p><p>: Unrestricted access may lead to resource misuse or unintentional overloading.</p><p><strong>Implement Role-Based Access Control&nbsp;(RBAC)</strong>:</p><p>Assign permissions based on roles (e.g., admin, analyst, viewer) to simplify management.</p><p>Use Azure Active Directory (AAD) to centralize user authentication and role assignments.</p><p>Restrict access to specific workspaces, datasets, or reports based on user&nbsp;roles.</p><p>Monitor usage logs to detect and prevent unauthorized access.</p><p><strong>Automate User Provisioning</strong>:</p><p>Use scripts or APIs to automate the addition, removal, or modification of user&nbsp;roles.</p><p>Example: Automatically assign access permissions when new users are added to a specific AAD&nbsp;group.</p><p><strong>Provide Training &amp; Documentation</strong>:</p><p>Educate users on best practices for resource usage and access management.</p><p>: Regularly audit user permissions to ensure compliance with organizational policies and security standards.</p><p>Real-world scenarios and hypothetical examples provide valuable insights into how Microsoft Fabric F64 can address capacity planning challenges and optimize resource utilization. This section explores three illustrative case studies that highlight the impact of F64 on enterprise application migration, high user load handling, and iterative capacity refinement.</p><ul><li><strong><em>Example 1‚Ää‚Äî‚ÄäEnterprise Application Migration</em></strong></li></ul><p>A global retail chain managing customer insights and sales data relied on an on-premises infrastructure that struggled to handle its growing data demands. The organization decided to migrate its data analytics workloads to Microsoft Fabric F64 to modernize operations and reduce&nbsp;costs.</p><p>Scalability limitations in the on-premises system caused frequent delays in generating reports for business decisions.</p><p>High operational costs for maintaining aging hardware and infrastructure.</p><p>Risk of downtime during the migration process.</p><p>Migrated all data analytics workloads to F64, leveraging Azure Data Factory for seamless data transfer to&nbsp;Fabric.</p><p>Implemented Power BI to visualize data insights in real-time, improving collaboration across global&nbsp;teams.</p><p>Used incremental migration strategies to move datasets in phases, reducing the risk of downtime.</p><p>: Reduced operational costs by 30% due to the elimination of hardware maintenance expenses.</p><p>: Average report generation time decreased from 20 minutes to under 5&nbsp;minutes.</p><p>: The F64 capacity now handles seasonal spikes in data queries with ease, ensuring business continuity during peak sales&nbsp;periods.</p><p>Migrating to F64 not only enhanced the organization‚Äôs operational efficiency but also future-proofed its data infrastructure for&nbsp;growth.</p><ul><li><strong><em>Example 2‚Ää‚Äî‚ÄäHandling High User&nbsp;Loads</em></strong></li></ul><p>An educational technology company delivering digital learning platforms to over 500 institutions faced challenges managing concurrent user access during peak hours, such as exam periods. The company turned to F64 to ensure uninterrupted service for its&nbsp;users.</p><p>High concurrent user load caused frequent slowdowns in accessing learning materials.</p><p>Latency issues during critical periods, leading to user dissatisfaction.</p><p>Difficulty in predicting and handling peak traffic&nbsp;surges.</p><p>Deployed Microsoft Fabric F64 to centralize data processing and support high-performance workloads.</p><p>Enabled Azure Autoscale to dynamically adjust resources during peak usage&nbsp;periods.</p><p>Used caching strategies to pre-load frequently accessed content, reducing server load during real-time queries.</p><p>: Successfully supported over 500 concurrent users with less than 2% latency complaints during peak&nbsp;periods.</p><p>: Reduced average query latency from 1.5 seconds to 0.6&nbsp;seconds.</p><p>: Automated scaling ensured seamless handling of traffic surges, even during unplanned demand&nbsp;spikes.</p><p>The F64 capacity tier provided the scalability and performance needed to deliver a reliable, high-quality user experience under heavy load conditions.</p><ul><li><strong><em>Example 3‚Ää‚Äî‚ÄäTrial-&amp;-Error Learning</em></strong></li></ul><p>A financial services firm managing regulatory reporting and real-time analytics for its trading platforms opted to test Microsoft Fabric F64 through a trial-and-error approach to refine its capacity planning strategy.</p><p>Uncertainty about the required capacity to support complex analytics and regulatory compliance workflows.</p><p>Inconsistent workload patterns made it difficult to estimate resource needs accurately.</p><p>Concerns about overprovisioning and unnecessary costs.</p><p>Started with a Pay-As-You-Go F64 capacity to test workload performance under varying conditions.</p><p>Conducted load tests simulating peak reporting and analytics demand, identifying the optimal configuration for memory and compute&nbsp;power.</p><p>Iteratively refined capacity allocations based on real-time metrics collected through Azure&nbsp;Monitor.</p><p>: Reduced costs by 18% by identifying and eliminating underutilized resources.</p><p>: Achieved consistent performance for real-time analytics, even during market volatility.</p><p>: Insights from load testing enabled the firm to transition to a reserved F64 capacity with confidence, balancing performance and&nbsp;cost.</p><p>An iterative, data-driven approach to capacity planning allowed the firm to maximize F64‚Äôs capabilities while avoiding unnecessary expenses.</p><h3><strong>Future Trends In Capacity&nbsp;Planning</strong></h3><p>The field of capacity planning is evolving rapidly, driven by advancements in AI, hybrid cloud solutions, and emerging technologies. These trends are reshaping how organizations estimate, allocate, and manage resources, offering greater efficiency and scalability than ever before. In this section, we explore the key innovations shaping the future of capacity planning.</p><p>Artificial intelligence (AI) and machine learning (ML) are transforming capacity planning by introducing predictive and automated capabilities that reduce manual effort and improve accuracy.</p><p><strong>Predictive Capacity Estimation</strong>:</p><p>AI models analyze historical performance data to forecast future capacity requirements.</p><p>For example, machine learning algorithms can identify seasonal trends or recurring workload patterns, helping organizations prepare for spikes in&nbsp;demand.</p><p><strong>Real-Time Resource Allocation</strong>:</p><p>ML-powered systems dynamically adjust resource allocations based on real-time usage&nbsp;metrics.</p><p>For instance, Microsoft Fabric could automatically scale resources up or down during high or low traffic periods, ensuring optimal utilization and cost efficiency.</p><p>AI tools monitor performance metrics to detect unusual patterns, such as sudden increases in CPU or memory usage, and recommend corrective actions.</p><p>:</p><p>AI-powered insights enable decision-makers to explore multiple capacity scenarios and choose the most efficient configuration.</p><p>Azure‚Äôs AI-driven insights for workload optimization.</p><p>Integration of AI with tools like Microsoft Fabric SKU Estimator for intelligent capacity recommendations.</p><p>: Leverage Azure Machine Learning and AI-based analytics tools to implement predictive and real-time capacity planning strategies.</p><ul><li><strong><em>Integration With Hybrid Cloud Solutions</em></strong></li></ul><p>The increasing adoption of hybrid cloud models is driving new innovations in capacity planning, particularly for organizations operating across both on-premises and cloud environments.</p><p>Microsoft Fabric supports hybrid architectures by integrating on-premises data sources with cloud-based resources.</p><p>This enables organizations to retain control over sensitive data while benefiting from cloud scalability.</p><p><strong>Dynamic Scaling Across Environments</strong>:</p><p>Hybrid solutions allow workloads to shift dynamically between on-premises and cloud environments based on demand and resource availability.</p><p>For example, during peak usage, a portion of on-premises workloads could be offloaded to the cloud, leveraging the elasticity of Fabric&nbsp;F64.</p><p>Tools like Azure Arc provide a single control plane for managing resources across hybrid infrastructures, simplifying capacity planning.</p><p><strong>Cost Optimization In Hybrid&nbsp;Models</strong>:</p><p>Hybrid deployments can leverage pay-as-you-go cloud resources for burst capacity while relying on fixed-cost on-premises infrastructure for baseline workloads.</p><p>Organizations using Azure Data Factory to connect on-premises SQL databases with Fabric for real-time analytics.</p><p>Leveraging Azure Arc to manage hybrid workloads effectively.</p><p>: Use hybrid integration to handle unpredictable workloads while maintaining compliance with data residency and security requirements.</p><p>Emerging technologies are poised to revolutionize capacity planning by enhancing hardware capabilities, virtualization techniques, and orchestration tools.</p><p>:</p><p>The rise of high-performance computing (HPC) hardware, such as GPUs and TPUs, is enabling faster data processing and AI model training.</p><p>Energy-efficient processors are reducing operational costs while improving performance.</p><p>Modern virtualization technologies are optimizing resource utilization by allowing multiple workloads to share the same physical infrastructure without performance degradation.</p><p>For example, containerization with Kubernetes ensures precise resource allocation and scalability for microservices-based workloads.</p><p><strong>Enhanced Orchestration Tools</strong>:</p><p>Tools like Kubernetes and Docker Swarm are automating workload distribution and scaling, making capacity planning more dynamic and flexible.</p><p>Orchestration enables rapid provisioning of resources in response to workload changes, minimizing downtime and overprovisioning.</p><p>Though still in its infancy, quantum computing has the potential to revolutionize capacity planning by solving complex optimization problems in real&nbsp;time.</p><p>Quantum algorithms could enhance predictive analytics, offering even greater accuracy in capacity forecasts.</p><p>The use of GPUs for real-time fraud detection in financial services on Fabric&nbsp;F64.</p><p>Containerized applications dynamically scaled via Kubernetes to handle seasonal traffic surges in e-commerce.</p><p>: Stay informed about emerging technologies and evaluate their applicability to your capacity planning strategy, ensuring your organization remains competitive.</p><p>At <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Mastering+Capacity+Planning+For+Microsoft+Fabric%C2%A0F64\">Xinthe</a>, we‚Äôve found the F64 tier, designed for high-performance workloads, offering scalable compute, storage, and memory resources to be suitable for the diverse enterprise needs of our clientele. Capacity planning is no longer optional in the modern cloud landscape‚Ää‚Äî‚Ääit‚Äôs essential for achieving operational efficiency, cost savings, and scalability. We recommend getting started on your capacity planning journey right away and ensuring your infrastructure is prepared for potential challenges and opportunities lying&nbsp;ahead.</p><p>This appendix provides sample code examples that demonstrate how to implement workload simulation, monitor resource utilization, and automate capacity adjustments.</p><p><strong>1.Workload Simulation with&nbsp;Python</strong></p><p>This script uses Python to simulate workload demand on an F64 capacity&nbsp;-</p><pre>import timeimport random    for i in range(10):  # Simulate 10 tasks<p>        cpu_usage = random.uniform(40, 90)  # Random CPU usage %</p>        memory_usage = random.uniform(30, 80)  # Random memory usage %<p>        print(f\"Simulated Task {i+1}: CPU Usage={cpu_usage:.2f}%, Memory Usage={memory_usage:.2f}%\")</p>        time.sleep(1)  # Pause to simulate real-time task execution</pre><p><strong>2.Azure Monitor Alert Configuration</strong></p><p>An Azure Resource Manager template for setting up an alert for high CPU usage&nbsp;-</p><pre>{    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",<p>    \"contentVersion\": \"1.0.0.0\",</p>    \"resources\": [            \"type\": \"Microsoft.Insights/metricAlerts\",<p>            \"apiVersion\": \"2018-03-01\",</p>            \"name\": \"HighCpuUsageAlert\",            \"properties\": {                \"enabled\": true,                    \"allOf\": [                            \"threshold\": 80,<p>                            \"metricName\": \"Percentage CPU\",</p>                            \"operator\": \"GreaterThan\",<p>                            \"timeAggregation\": \"Average\"</p>                        }                },                \"scopes\": [\"/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}\"]        }}</pre><p><strong>3.Automated Scaling with Azure Logic&nbsp;Apps</strong></p><p>A script to scale up resources when CPU utilization exceeds a threshold -</p><pre>$ResourceGroupName = \"MyResourceGroup\"$CapacityName = \"MyFabricF64\"<p>$cpuUtilization = Get-AzureRmMetric -ResourceGroupName $ResourceGroupName -Name $CapacityName -MetricName \"Percentage CPU\" | Measure-Object -Property Average</p><p>if ($cpuUtilization.Average -gt $ScaleUpThreshold) {</p>    Write-Host \"Scaling up resources for $CapacityName\"<p>    Set-AzureRmFabricCapacity -ResourceGroupName $ResourceGroupName -Name $CapacityName -SkuName \"F128\"  # Example for scaling to F128</p>}</pre><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e7f9a9333318\" width=\"1\" height=\"1\" alt=\"\">","contentLength":37447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bootstrapping Argo for Entra ID OIDC","url":"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/","date":1739813751,"author":"/u/UberBoob","guid":1947,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks! I&#39;m trying to spin up an Argo-managed Cluster to use Azure AD credentials as the sole SSO provider.</p> <p>I have the secrets mounted on the Argo Server pods, provided from AWS Secrets Manager by AWS Secrets Store CSI driver and provider. client_id and client_secret are located at /mnt/secrets-store. My terrafrom modules are running a helm release install of Argo CD 7.7.7.</p> <p>Im trying to use env variables passed as helm values.yaml. Argo CD runs fine, I can login via initial Admin creds. The Entra ID button is in place for login, however response from Microsoft is that I must provide a client id in the request.</p> <p>Anyone else take this approach and have it working? We, can pass the values via Terraform, however the secret ends up in plan files and is not masked even when using the sensitive() in Terraform. This fails our scan audits and want to keep the secrets in AWS secrets manager as a permanent solution.</p> <p>The Argo Docs don&#39;t go into much detail around OIDC, other than setting the OIDC details in the ConfiMap.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/UberBoob\"> /u/UberBoob </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irpibv/bootstrapping_argo_for_entra_id_oidc/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Canonical Extends Kubernetes Distro Support to a Dozen Years","url":"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/","date":1739813703,"author":"/u/CrankyBear","guid":1997,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/\"> <img src=\"https://external-preview.redd.it/CrTnhrf0m844iLfAuNSRmJ3R8qO-sAQM4pZX5wPcg5c.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=74e86d0c279e67e1c32bd05a5d434e80aba9d3f0\" alt=\"Canonical Extends Kubernetes Distro Support to a Dozen Years\" title=\"Canonical Extends Kubernetes Distro Support to a Dozen Years\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/CrankyBear\"> /u/CrankyBear </a> <br/> <span><a href=\"https://thenewstack.io/canonical-extends-kubernetes-distro-support-to-a-dozen-years/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irphjr/canonical_extends_kubernetes_distro_support_to_a/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UI Unit Testing: Best Practices and Strategies","url":"https://blog.devops.dev/ui-unit-testing-best-practices-and-strategies-d5bb39306c76?source=rss----33f8b2d9a328---4","date":1739813583,"author":"Adem KORKMAZ","guid":1968,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Redis on Docker and Redis Insight: A Comprehensive Guide","url":"https://blog.devops.dev/redis-on-docker-and-redis-insight-a-comprehensive-guide-d5ca63de7705?source=rss----33f8b2d9a328---4","date":1739813574,"author":"Nithidol Vacharotayan","guid":1967,"unread":true,"content":"<div><p>Redis, a powerful in-memory data store, has become a go-to technology for caching, real-time analytics, and pub/sub messaging. Combining‚Ä¶</p></div>","contentLength":139,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Machine Learning and AI in Spring Boot Applications","url":"https://blog.devops.dev/leveraging-machine-learning-and-ai-in-spring-boot-applications-d5ada53967f9?source=rss----33f8b2d9a328---4","date":1739813522,"author":"Pradeep K","guid":1966,"unread":true,"content":"<p><em>Artificial Intelligence (AI) and Machine Learning (ML) are transforming industries by enabling applications to make data-driven decisions and predictions. As organizations increasingly adopt these technologies, integrating AI/ML models into web applications built with Spring Boot, a popular Java framework, is becoming a key&nbsp;trend.</em></p><p><em>In this blog, we will explore how you can integrate AI/ML into a Spring Boot application, the tools available, and a step-by-step approach to getting&nbsp;started.</em></p><h3>Why Integrate AI/ML in Spring&nbsp;Boot?</h3><p>Spring Boot is a powerful framework that simplifies the development of Java-based applications. It&nbsp;offers:</p><p><em> AI/ML services can be exposed as RESTful APIs.</em><em>Spring Boot applications can handle large-scale data processing.</em><em>Various ML libraries and services can be easily connected.</em><em>Well-suited for large-scale enterprise applications.</em></p><p>With the rise of AI-driven applications, businesses can leverage Spring Boot to deploy intelligent services such as recommendation engines, fraud detection, sentiment analysis, and&nbsp;more.</p><h3>Steps to Integrate Machine Learning in Spring&nbsp;Boot</h3><h4>Choosing the Right ML&nbsp;Model</h4><p>The first step is to identify the ML model based on the problem you are trying to solve. Common use cases&nbsp;include:</p><p><em>Spam detection, fraud detection</em><em>Sales forecasting, price prediction</em><strong>Natural Language Processing (NLP):</strong><em>Sentiment analysis, chatbot responses</em></p><p>You can build models using tools such as TensorFlow, Scikit-learn, or Apache Spark MLlib, and then deploy them within your Spring Boot&nbsp;app.</p><h4>2. Building and Exporting the ML&nbsp;Model</h4><p>If you‚Äôre using Python-based ML models (built using Scikit-learn or TensorFlow), export the model to a format that Java can consume, such&nbsp;as:</p><p><strong>PMML (Predictive Model Markup Language): </strong><em>Supported by libraries like JPMML in Java.</em><strong>ONNX (Open Neural Network Exchange): </strong><em>Interoperable across multiple frameworks.</em><em>Expose ML models as microservices using Python (Flask, FastAPI) and consume them in Spring&nbsp;Boot.</em></p><h4>3. Using Java AI/ML Libraries in Spring&nbsp;Boot</h4><p>If you‚Äôd prefer to build and serve models directly in Java, you can leverage the following libraries:</p><p><em>A high-level API for deploying deep learning models in Java.</em><em> Basic statistical and ML functions.</em><em>A fast and comprehensive machine learning framework in Java.</em><em>A collection of ML algorithms for data mining&nbsp;tasks.</em></p><p> Integrating DJL in Spring Boot. Add the following dependency to your&nbsp;pom.xml:</p><pre>&lt;dependency&gt;     &lt;groupId&gt;ai.djl&lt;/groupId&gt;<p>     &lt;artifactId&gt;api&lt;/artifactId&gt;</p>     &lt;version&gt;0.23.0&lt;/version&gt;</pre><p>Load and use a pre-trained model:</p><pre>import ai.djl.Model;import ai.djl.inference.Predictor;<p>import ai.djl.modality.Classifications;</p>import ai.djl.translate.TranslateException;<p>public class ImageClassifier {</p>  public static void main(String[] args) throws TranslateException {<p>    Model model = Model.newInstance(\"resnet50\");</p>    Predictor&lt;String, Classifications&gt; predictor = model.newPredictor();<p>    Classifications result = predictor.predict(\"image.jpg\");</p>    System.out.println(result);}</pre><h4><strong>4. Deploying the Model as a REST&nbsp;API</strong></h4><p>You can create a RESTful API endpoint in Spring Boot to serve ML predictions.</p><pre>@RestController@RequestMapping(\"/api/predict\")<p>public class PredictionController {</p> public ResponseEntity&lt;String&gt; predict(@RequestBody InputData inputData) {<p> // Call your AI/ML model here</p> String result = someMLService.predict(inputData);<p> return ResponseEntity.ok(result);</p> }</pre><p>This allows front-end applications or other services to request predictions via HTTP&nbsp;calls.</p><h4>5. Optimizing and Scaling the&nbsp;Solution</h4><p>Once your AI model is integrated and working, you should focus&nbsp;on:</p><p><em> Use caching mechanisms like Redis to store frequently requested results.</em><em> Implement logging and monitoring using tools like Spring Boot Actuator.</em><em>Deploy your Spring Boot app with Docker and Kubernetes for scalability.</em></p><h4>Challenges and Considerations</h4><p>While integrating AI/ML in Spring Boot applications, keep the following in&nbsp;mind:</p><p><em>Ensure your model is optimized for inference time to avoid slowing down the application.</em><em>Secure sensitive data and predictions, especially in financial or healthcare applications.</em><em>Ensure proper data pipelines and preprocessing mechanisms are in&nbsp;place.</em></p><p><em>Integrating machine learning into Spring Boot applications opens up endless possibilities for intelligent, data-driven applications. By leveraging Java-based ML frameworks or consuming Python models via REST APIs, businesses can enhance their applications with powerful predictive capabilities.</em></p><p><em>Start small by experimenting with open-source libraries and gradually scale your solution to meet enterprise needs.</em></p><p><em>Looking to implement AI in your Spring Boot application? Start experimenting today and unlock the full potential of intelligent web services!</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d5ada53967f9\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building an NBA Data Lake: From API to Analytics with AWS","url":"https://blog.devops.dev/building-an-nba-data-lake-from-api-to-analytics-with-aws-3dc390776246?source=rss----33f8b2d9a328---4","date":1739813510,"author":"Josina Jose","guid":1965,"unread":true,"content":"<p>Have you ever wondered how to store and analyze live NBA data efficiently? Imagine having a structured system where player stats, team rankings, and game details are automatically collected, stored, and ready for analysis‚Ää‚Äî‚Ääall in one&nbsp;place.</p><p>In this project, I built an  using <strong>AWS services (S3, Glue, and Athena)</strong> to create a scalable analytics pipeline. The data comes from , a popular sports API, and is processed in AWS for easy querying.</p><p>This guide will walk you through the entire setup‚Ää‚Äî‚Ääfrom fetching NBA data to running SQL queries on AWS. Let‚Äôs dive in!&nbsp;üöÄ</p><p>Before getting started, ensure you&nbsp;have:</p><ul><li> with permissions for S3, Glue, and&nbsp;Athena.</li><li> installed on your local&nbsp;machine.</li><li><strong>An API key from SportsData.io</strong> (Sign up for a free trial and select NBA as your&nbsp;sport).</li><li> configured with your credentials.</li><li> installed using:</li></ul><pre>pip install boto3 python-dotenv</pre><p><strong>AWS S3 (Simple Storage Service)</strong>üóÇÔ∏è A scalable object storage service used for storing and retrieving any amount of data at any time. It is commonly used for backups, data archiving, and serving static website&nbsp;content.</p><p>üßë‚ÄçüíªA fully managed ETL (Extract, Transform, Load) service that makes it easier to prepare and load data for analytics. It automates the process of discovering, cataloging, and transforming data, and can integrate with other AWS analytics services.</p><p>üîç An interactive query service that allows you to analyze data in Amazon S3 using standard SQL. It is serverless, meaning no infrastructure management is required, and it is commonly used for querying large datasets directly in&nbsp;S3.</p><p>üìà is a cloud-based business intelligence service that enables users to create and share interactive data visualizations and dashboards. It integrates with various data sources, including AWS services, SQL databases, and third-party tools, to provide insights through visual analytics.</p><h4>Step 1: Setting Up Your&nbsp;Project</h4><ol><li>Open  or your preferred editor.</li><li>Create a new project folder and navigate to&nbsp;it.</li><li>Inside the folder, create a new Python&nbsp;file:</li></ol><pre>echo.&gt; setup_nba_data_lake.py</pre><p>4. Create a&nbsp;.env file to store sensitive credentials:</p><h4>Step 2: Configure API and AWS Credentials</h4><p>Edit the&nbsp;.env file and add your  and :</p><pre>SPORTS_DATA_API_KEY=your_api_key_hereNBA_ENDPOINT=https://api.sportsdata.io/v3/nba/scores/json/Players</pre><h4>Step 3: Writing the&nbsp;Script</h4><p>Inside setup_nba_data_lake.py, write a script&nbsp;to:</p><ul><li>Create an  to store NBA&nbsp;data.</li><li>Fetch NBA player data from .</li><li>Upload the data to .</li><li>Create a  and external&nbsp;table.</li><li>Configure  for querying the&nbsp;data.</li></ul><h4>Step 4: Running the&nbsp;Script</h4><p>Once the script is ready, run the following command in the terminal:</p><pre>python setup_nba_data_lake.py</pre><p><em>You should see messages indicating successful creation of the AWS resources and data&nbsp;upload.</em></p><h4>Step 5: Verifying the&nbsp;Setup</h4><p>Log in to  ‚Üí Navigate to&nbsp;.</p><p>Click on  folder and verify that nba_player_data.json exists.</p><p><strong>2. Check AWS Glue Database and&nbsp;Table</strong></p><p>In the left sidebar, click on  under the &nbsp;section.</p><p>Verify that the database you created earlier (e.g., nba_data_lake) appears in the&nbsp;list.</p><p><strong>3. Query Data Using&nbsp;Athena</strong></p><ul><li>Navigate to  in AWS&nbsp;Console.</li><li>Configure Athena to Store Query Results in&nbsp;S3.</li><li>In the , click on Manage Settings-In the  section, click  to choose an S3 bucket for query&nbsp;results.</li></ul><ul><li>Run the following SQL&nbsp;query:</li></ul><pre>SELECT FirstName, LastName, Position, TeamFROM nba_players</pre><ul><li>Click  and check if results&nbsp;appear.</li></ul><p><strong>Export Athena Query Results to&nbsp;S3</strong>:</p><ul><li>After running the query in Athena, click on the &nbsp;button.</li><li>You can download the result file from the S3 bucket by clicking on the file name in the S3&nbsp;Console.</li><li>Choose  to save the results to your local&nbsp;machine.</li></ul><h4>Adding Insights with Amazon QuickSight</h4><ul><li>Go to  ‚Üí Click  ‚Üí Choose .</li><li>Select your  and upload&nbsp;it.</li><li>Click , then .</li></ul><ul><li>Click  to save and share&nbsp;insights</li></ul><h4>Step5: Deleting AWS Resources After&nbsp;Use</h4><p>To prevent unnecessary costs, follow these steps to safely remove all AWS resources associated with your NBA data&nbsp;lake.</p><pre>import boto3<p>def delete_iam_role(role_name):</p>    iam = boto3.client(\"iam\")        iam.delete_role(RoleName=role_name)<p>        print(f\"Deleted IAM Role: {role_name}\")</p>    except Exception as e:<p>        print(f\"Error deleting IAM Role {role_name}: {e}\")</p><p>def delete_lambda_function(function_name):</p>    lambda_client = boto3.client(\"lambda\")        lambda_client.delete_function(FunctionName=function_name)<p>        print(f\"Deleted Lambda Function: {function_name}\")</p>    except Exception as e:<p>        print(f\"Error deleting Lambda function {function_name}: {e}\")</p><p>def delete_cloudwatch_logs(log_group):</p>    logs = boto3.client(\"logs\")        logs.delete_log_group(logGroupName=log_group)<p>        print(f\"Deleted CloudWatch Log Group: {log_group}\")</p>    except Exception as e:<p>        print(f\"Error deleting CloudWatch logs {log_group}: {e}\")</p><p>def delete_quicksight_dataset(account_id, dataset_id):</p>    quicksight = boto3.client(\"quicksight\")        quicksight.delete_dataset(AwsAccountId=account_id, DataSetId=dataset_id)<p>        print(f\"Deleted QuickSight Dataset: {dataset_id}\")</p>    except Exception as e:<p>        print(f\"Error deleting QuickSight dataset {dataset_id}: {e}\")</p><p>def delete_athena_workgroup(workgroup):</p>    athena = boto3.client(\"athena\")        athena.delete_work_group(WorkGroup=workgroup)<p>        print(f\"Deleted Athena Workgroup: {workgroup}\")</p>    except Exception as e:<p>        print(f\"Error deleting Athena workgroup {workgroup}: {e}\")</p><p># Call these functions with your resource names</p>delete_iam_role(\"GlueServiceRole\")<p>delete_lambda_function(\"myLambdaFunction\")</p>delete_cloudwatch_logs(\"/aws/lambda/myLambdaFunction\")<p>delete_quicksight_dataset(\"&lt;your-account-id&gt;\", \"&lt;dataset-id&gt;\")</p>delete_athena_workgroup(\"my-workgroup\")</pre><ol><li>: The delete_iam_role function removes IAM roles like GlueServiceRole, which may have been used by AWS Glue or other services.</li><li>: The delete_lambda_function function deletes Lambda functions (e.g., myLambdaFunction) to prevent any unnecessary Lambda invocation charges.</li><li>: The delete_cloudwatch_logs function removes log groups created by Lambda functions or other services to free up&nbsp;space.</li><li><strong>QuickSight Dataset Cleanup</strong>: The delete_quicksight_dataset function removes QuickSight datasets that you no longer need for visualizations or analysis.</li><li>: The delete_athena_workgroup function deletes the Athena workgroups you created during data querying&nbsp;tasks.</li></ol><p>‚úÖ Built a scalable data lake using AWS services like S3, Glue, and&nbsp;Athena.</p><p>‚úÖ Integrated external data sources (e.g., SportsData.io API) for real-time data collection.</p><p>‚úÖ Leveraged Amazon QuickSight for data visualization and insights.</p><p>‚úÖ Learned how to manage AWS resources efficiently and perform cleanup to save&nbsp;costs.</p><p>This project demonstrated the power of AWS in managing large datasets, automating data workflows, and enabling interactive querying and visualization. By combining AWS tools, we built a scalable NBA Data Lake that supports real-time analysis, paving the way for future improvements and broader applications in sports analytics and&nbsp;beyond.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3dc390776246\" width=\"1\" height=\"1\" alt=\"\">","contentLength":6904,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unraveling Kubespray‚Äôs Incremental etcd Cluster Configuration","url":"https://blog.devops.dev/unraveling-kubesprays-incremental-etcd-cluster-configuration-39be0ec5e79a?source=rss----33f8b2d9a328---4","date":1739813497,"author":"Ali  Ansari","guid":1964,"unread":true,"content":"<p>If you‚Äôve ever worked with Kubernetes, you know that etcd is its heartbeat‚Ää‚Äî‚Ääkeeping all cluster state consistent and reliable. Recently, I noticed something odd while digging into Kubespray, the popular Kubernetes deployment tool. The way it configures the --initial-cluster flag in etcd.yaml wasn‚Äôt what I expected.</p><p>This kicked off a curiosity-driven investigation, and I finally got my answer. And honestly? The journey was just as interesting as the answer&nbsp;itself.</p><h3>The Curiosity: Something Looks&nbsp;Off</h3><p>It all started when I was checking out the etcd.yaml static pod manifest on my Kubernetes master nodes. You can find this file at /etc/kubernetes/manifests/, and it defines how the etcd static pods are configured. While going through it, I noticed something strange about the --initial-cluster flag.</p><p>Here‚Äôs what I found on each master&nbsp;node:</p><ul><li>--initial-cluster=lab-master1=https://192.168.122.131:2380</li><li>--initial-cluster=lab-master1=https://192.168.122.131:2380,lab-master2=https://192.168.122.132:2380</li><li>--initial-cluster=lab-master1=https://192.168.122.131:2380,lab-master2=https://192.168.122.132:2380,lab-master3=https://192.168.122.133:2380</li></ul><p>Wait a minute. This isn‚Äôt how the official etcd documentation suggests doing it. According to the <a href=\"https://etcd.io/docs/v3.4/op-guide/clustering/\">etcd clustering guide</a>, all nodes should have the  --initial-cluster flag, listing every member upfront. But here, Kubespray was  adding nodes.&nbsp;Why?</p><h3>The Investigation: A Trail of Questions</h3><p>Like any good mystery, this needed some serious sleuthing. I turned to the Kubernetes community for answers‚Ää‚Äî‚Ääposting on IRC, CNCF forums, and Slack channels.</p><p>Then, a Kubespray maintainer named  gave me a nudge in the right direction:</p><blockquote><em>‚ÄúAs in most cases when the question is ‚Äòwhy this way,‚Äô the answer is probably historical reasons. You‚Äôll have to dig in the git history to find out, I‚Äôm&nbsp;afraid.‚Äù</em></blockquote><p>Alright, challenge accepted. Time to dive into the Kubespray codebase.</p><p>I started by checking Kubespray‚Äôs Git history. Here‚Äôs&nbsp;how:</p><pre>git log --oneline | grep -i \"etcd\"</pre><p>Then, I zeroed in on commits related to --initial-cluster:</p><pre>git log --oneline | grep -i \"initial-cluster\"</pre><p>Eventually, this led me to a crucial configuration line inside etcd.env.j2:</p><pre>ETCD_INITIAL_CLUSTER={{ etcd_peer_addresses }}</pre><p>Following the breadcrumbs, I found that etcd_peer_addresses was defined in join_etcd_member.yml‚Äîa task handling how new etcd members are&nbsp;added.</p><p>And here‚Äôs the exact part of the Ansible playbook that defines&nbsp;it:</p><pre>- name: Join Member | Refresh etcd config  include_tasks: refresh_config.yml    etcd_peer_addresses: &gt;-<p>      {% for host in groups['etcd'] -%}</p>        {%- if hostvars[host]['etcd_member_in_cluster'].rc == 0 -%}<p>          {{ \"etcd\" + loop.index | string }}=https://{{ hostvars[host].etcd_access_address | default(hostvars[host].ip | default(fallback_ips[host])) }}:2380,</p>        {%- endif -%}          {{ etcd_member_name }}={{ etcd_peer_url }}      {%- endfor -%}</pre><blockquote><em>‚ÄúNow adding unjoined members to existing etcd cluster occurs one at a time so that the cluster does not lose&nbsp;quorum.‚Äù</em></blockquote><h3>The Answer: Stability Over Simplicity</h3><p>Kubespray configures the --initial-cluster flag incrementally <strong>to avoid losing quorum when&nbsp;scaling</strong>.</p><ul><li> etcd needs most nodes (quorum) to work. In a three-node cluster, at least two nodes must be online for it to function.</li><li><strong>Risk of Adding All at Once:</strong> If all nodes were added at once and something went wrong (network issues, misconfigurations, etc.), quorum could be lost, making the cluster unavailable.</li><li><strong>Kubespray‚Äôs Safer Approach:</strong> By adding nodes , it ensures that the cluster remains stable even if something goes&nbsp;wrong.</li></ul><blockquote><em>‚ÄúIf adding multiple members the best practice is to configure a single member at a time and verify it starts correctly before adding more new members.‚Äù</em></blockquote><p>This approach prioritizes ‚Ää‚Äî‚Ääa good tradeoff for reliability.</p><p>This whole experience was a reminder of some key&nbsp;lessons:</p><ol><li> If something looks odd, dig deeper. The answers aren‚Äôt always in the official&nbsp;docs.</li><li><strong>AI is Helpful, but Not Perfect:</strong> AI tools couldn‚Äôt give me deep insights. For tricky problems, hands-on research is&nbsp;better.</li><li> The Kubernetes community is filled with experts happy to help. Slack, forums, and GitHub are great places to&nbsp;learn.</li><li> Learning to check Git logs and commits is  when working with open-source projects.</li><li> Many choices in software are based on history. Looking back at past decisions helps understand why things are done a certain&nbsp;way.</li></ol><p>This deep dive into Kubespray‚Äôs etcd configuration wasn‚Äôt just about solving a puzzle‚Ää‚Äî‚Ääit was about learning how distributed systems evolve and why certain design choices&nbsp;exist.</p><p>Kubespray‚Äôs incremental approach might look unusual, but it‚Äôs actually a well-thought-out strategy to keep things stable. And that‚Äôs the beauty of open-source‚Ää‚Äî‚Ääthere‚Äôs always something new to discover.</p><p>So next time you see something weird in a config file, <strong>don‚Äôt just accept it‚Ää‚Äî‚Ääask why!</strong> You might learn something cool.</p><p><strong>Have you ever found a strange Kubernetes behavior? Share it in the comments!</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=39be0ec5e79a\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Streamline Development with New Amazon Q Developer Agents","url":"https://aws.amazon.com/blogs/devops/streamline-development-with-new-amazon-q-developer-agents/","date":1739811523,"author":"Ryan Yanchuleff","guid":1914,"unread":true,"content":"<p>As software development continues to evolve at a rapid pace, developers are constantly seeking tools that can streamline their workflow, improve code quality, and boost productivity. Amazon Web Services (AWS) has answered this call with the introduction of powerful new AI agents for <a href=\"https://aws.amazon.com/q/developer/\">Amazon Q Developer</a>.</p><p><a href=\"https://aws.amazon.com/what-is/ai-agents/\">AI-powered agents</a> transform the way developers approach documentation, unit testing, and code reviews. Agents are autonomous software programs that can interact with their environment, gather data, and use that data to perform tasks independently to achieve pre-defined goals. They are rational, making informed decisions based on their perceptions and data to optimize performance. AI agents can bring benefits like improved productivity, reduced costs, better decision-making, and enhanced customer experience. Amazon Q Developer has three new agents: , , and . In this post, we‚Äôll explore each of them in a bit more detail and talk about how you can incorporate them into your daily development workflow.</p><p>The first <a href=\"https://aws.amazon.com/blogs/devops/reinventing-the-amazon-q-developer-agent-for-software-development/\">agent released</a> for Amazon Q Developer was the software developer agent launched last year. The  agent allows you to generate new code or make code changes directly within your IDE to implement new features or fix issues in your projects. Simply provide a description of the task you want to accomplish, and Q Developer will analyze select context from your current codebase to generate the necessary code changes. Q Developer can help you build new AWS applications or update existing ones, providing a step-by-step summary of the changes it‚Äôs making and allowing you to easily accept or reject the proposed modifications. Q Developer has the ability to handle everything from creating new API endpoints to optimizing database queries, the /dev feature is a must-try tool for any developer working on anything from simple to complex tasks.</p><p>In the coming sections, let‚Äôs dive into how we can put these new agents to use with an application use case.</p><p>To begin using Amazon Q Developer, the following are required:</p><p> Amazon Q Developer supports additional IDEs such as Eclipse and Visual Studio Pro, but only the those listed above have agent support enabled.</p><p>We have learned that Q Developer‚Äôs new documentation, test, and security scanning agents can assist with the application software development lifecycle (SDLC). SDLC is a flywheel with several stages such as Planning and Research, Coding, Documentation, Testing, Build and Deploy, Operations and Maintenance.</p><p>With the new Q Developer agents, let‚Äôs see how we can put these into practice with the above application as part of the SDLC.</p><h2>Documentation Generation ()</h2><p>Creating and maintaining application documentation is often one of the most time-consuming and frequently overlooked aspects of software development. Detailed documentation empowers team members and stakeholders about code, design, and architecture. It enhances readability, facilitates rapid onboarding to accelerate new feature and functionality development and streamlines SDLC tasks. The introduction of the agent in Amazon Q Developer makes this process both easier and faster. Amazon Q Developer can now automatically generate new README files at any level of your project by analyzing the code in your selected folder or review code changes and recommend corresponding updates to existing documentation. Additionally, developers can leverage natural language to request specific modifications to their files to include custom summarizations, making the entire process more intuitive and user-friendly. These capabilities help reduce the burden on development teams while maintaining accurate and up-to-date project documentation as teams operate the applications.</p><p>The above application doesn‚Äôt have a detailed README and the following example shows how you can leverage  agent of Q Developer in Visual Studio Code (VS Code) IDE to help generate detailed documentation for the application codebase.</p><p><strong>Best practices when using Q Developer  agent:</strong></p><ul><li>For large repositories, consider requesting documentation by targeting specific directories for documentation.</li><li>Maintain well-commented and organized code with good naming conventions to improve the quality of generated documentation.</li><li>Be specific when describing desired changes to your README.</li></ul><p><em> The  feature supports various file types, including .template files, requirements.txt, package.json, tsconfig.json, Dockerfile, and more. It‚Äôs important to note that there are quotas in place, such as a maximum existing README size of 15 KB and a code project size limit of 200 MB uncompressed or 50 MB compressed.</em></p><h2>Unit Test Generation ()</h2><p>Test driven development is a common SDLC best practice. Part of it is Unit testing, which is a cornerstone for maintaining code quality. Unit testing helps catch bugs early on and minimize tech debt. However, the process of writing comprehensive tests has traditionally demanded significant time and effort from development teams. The new  agent in Amazon Q Developer is a groundbreaking solution that automates the testing process and enables developers to channel more energy into feature development and problem solving. The tool comes equipped with several powerful capabilities that streamline the testing workflow. It automatically identifies necessary test cases, generates appropriate mocks and stubs for isolated testing scenarios, and produces test code based on the identified cases. Currently supporting both Java and Python projects, the feature seamlessly integrates with popular development environments including VS Code and JetBrains IDEs, making it an invaluable asset for modern development teams looking to enhance their testing practices while maximizing productivity.</p><p>With the new Q Developer  agent, you can generate unit tests by specifying section of code such as class, function or method and also highlight code to generate tests within the IDE. In the following example, you can see the use of  agent to generate unit tests for open Python file (add_item.py) that adds new items to the DynamoDB table.</p><p><code>/test generate unit tests for the lambda_handler method that is adding items to dynamodb table</code></p><div><img aria-describedby=\"caption-attachment-21432\" loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/17/qdoc7.gif\" alt=\"Short animation of the /test agent generating a test file\" width=\"1674\" height=\"896\"><p>Using Amazon Q Developer  agent to generate unit tests</p></div><div><img aria-describedby=\"caption-attachment-21429\" loading=\"lazy\" src=\"https://d2908q01vomqb2.cloudfront.net/7719a1c782a1ba91c031a682a0a2f8658209adbf/2025/02/14/image-5-1024x838.png\" alt=\"/test invoked within the IDE\" width=\"1024\" height=\"838\"><p>Using Amazon Q Developer Generate Tests within IDE from code selection to develop unit tests</p></div><p><em> The Q Developer agent for unit tests supports Java and Python projects in VS Code and JetBrains IDEs. see <a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-language-ide-support.html#code-test-automation-language-support\">Language and framework support for unit test generation with </a>. The  feature handles various special cases, such as unsupported languages or frameworks, non-public methods in Java, and reaching monthly usage limits. It‚Äôs designed to provide a smooth user experience with helpful guidance throughout the process.</em></p><h2>Enhancing Code Quality and Security ()</h2><p>Code reviews have become an indispensable part of maintaining high standards in software development, and Amazon Q Developer‚Äôs new  feature is transforming this critical process with powerful AI-driven code analysis. This innovative tool brings comprehensive code analysis capabilities right to your fingertips, enabling development teams to identify and address potential issues before they evolve into serious problems. Users can benefit from continuous code scanning that works seamlessly as they write, while receiving detailed issue reports complete with clear explanations and actionable recommendations. The feature even goes a step further by offering in-place code fixes, making it easier than ever to implement necessary improvements directly within your code.</p><p>The  feature‚Äôs security detection capabilities cover a range of potential issues, ensuring thorough code analysis across multiple dimensions. Q Developer  feature performs static application security testing (<a href=\"https://docs.aws.amazon.com/wellarchitected/latest/devops-guidance/qa.st.4-enhance-sou[‚Ä¶]code-security-with-static-application-security-testing.html\">SAST</a>) to identify security vulnerabilities, implements secrets detection to prevent the exposure of sensitive information, and analyzes infrastructure as code (IaC) files for potential issues. This essentially shifting security further left in the SDLC enabling developers to detect security vulnerabilities early in the development cycle before code gets committed to the repository thus improving overall security posture and code quality. Additionally, the tool evaluates code quality factors that might affect maintainability and efficiency, assesses potential deployment risks, and conducts software composition analysis (SCA) for third-party code. This comprehensive approach to code review helps development teams maintain high-quality, secure, and efficient codebases while significantly streamlining the review process.</p><p>In the below example, you can see how you can leverage  feature to perform code scanning across the entire application workspace that includes both application python code and infrastructure terraform code and subsequently remediate it. Optionally, you can also choose to scan active opened files in the IDE.</p><p><em> The  feature maintains quotas to ensure optimal performance, with limits on input artifact size and source code size for both auto reviews and full project scans. More details can be found <a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/code-reviews.html\">here</a></em></p><p>The innovative agents in Amazon Q Developer ‚Äì , , and  ‚Äì directly address some of the most challenging and time-intensive aspects of the development lifecycle, from documentation management to testing and code review. By automating crucial tasks like README creation or maintenance, unit test generation, and comprehensive code analysis, development teams can significantly enhance their productivity while maintaining high standards of code quality and security. The ability to catch potential issues early and streamline development lifecycle empowers teams to work more efficiently and effectively than ever before. However, it‚Äôs crucial to remember that these AI-powered features are designed to complement rather than replace human expertise ‚Äì they serve as intelligent assistants that augment developers‚Äô capabilities while still relying on their professional judgment to ensure optimal project outcomes. These tools stand as prime examples of how AI can be leveraged to overcome traditional development challenges while enabling teams to focus on what matters most: creating exceptional software. Embrace these new features, and take your development process to the next level with Amazon Q Developer. To learn more about Amazon Q Developer and explore innovative ways of accelerating software development refer to the <a href=\"https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/what-is.html\">Q Developer documentation</a>. These agents are part of Q Developer‚Äôs expansive free tier of features, you can <a href=\"https://aws.amazon.com/q/developer/build/\">get started</a> with them today!</p>","contentLength":10533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Weekly Roundup: AWS Developer Day, Trust Center, Well-Architected for Enterprises, and more (Feb 17, 2025)","url":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-aws-developer-day-trust-center-well-architected-for-enterprises-and-more-feb-17-2025/","date":1739807351,"author":"Channy Yun (Ïú§ÏÑùÏ∞¨)","guid":1887,"unread":true,"content":"<p>Join us for the <a href=\"https://pages.awscloud.com/AWS-Developer-Day-2025.html\">AWS Developer Day</a> on February 20! This virtual event is designed to help developers and teams incorporate cutting-edge yet responsible <a href=\"https://aws.amazon.com/generative-ai/\">generative AI</a> across their development lifecycle to accelerate innovation.</p><p>In his keynote, <a href=\"https://www.linkedin.com/in/jeffbarr/\">Jeff Barr</a>, Vice President of AWS Evangelism, shares his thoughts on the next generation of software development based on generative AI, the skills needed to thrive in this changing environment, and how he sees it evolving in the future.</p><p> Here are some launches that got my attention:</p><p><a href=\"https://aws.amazon.com/blogs/aws/aws-cloudtrail-network-activity-events-for-vpc-endpoints-now-generally-available/\"><strong>AWS CloudTrail network activity events for VPC endpoint</strong></a> ‚Äì This feature provides you with a powerful tool to enhance your security posture, detect potential threats, and gain deeper insights into your VPC network traffic. This feature addresses your critical needs for comprehensive visibility and control over your AWS environments.</p><p> Here are some additional news items that you might find interesting:</p><p> ‚Äì <a href=\"https://www.linkedin.com/in/gregorywilson/\">Greg Wilson</a>, a lead of AWS Documentation, SDK, and CLI teams shared an insightful blog post about the progress, challenges, and what‚Äôs next for technical documentation for 200+ AWS services. It includes <a href=\"https://aws.amazon.com/getting-started/decision-guides/\">AWS Decision Guides</a> for choosing the right service for specific needs; optimizing documents for readability, such as doubled code samples; and improving usability, such as dark mode and auto-suggest with top global navigation controls. You can also learn about how we use generative AI to help create technical documents.</p><p> ‚Äì This is a new free digital course designed for technical professionals who architect, build, and operate AWS solutions at scale. This intermediate-level course will help you optimize your cloud architecture while aligning to your business goals. The course takes approximately 1 hour to complete and includes a knowledge check at the end to reinforce your learning.</p><p> ‚Äì The .NET team at AWS has been working on integrations for connecting your .NET applications to AWS resources. Learn about how to automatically deploy AWS application resources using <a href=\"https://www.nuget.org/packages/Aspire.Hosting.AWS\">Aspire.Hosting.AWS</a> NuGet package for NET Aspire, an open source framework building cloud-ready applications.</p><p> Here‚Äôs my personal favorites posts from <a href=\"https://community.aws/\">community.aws</a>:</p><p>Check your calendars and sign up for these upcoming AWS events:</p><p> ‚Äì Join a free online conference focusing on generative AI and data innovations. Available in multiple geographic regions: APJC and EMEA (March 6), North America (March 13), Greater China Region (March 14), and Latin America (April 8).</p><p> ‚Äì Join free online and in-person events that bring the cloud computing community together to connect, collaborate, and learn about AWS. Register in your nearest city: <a href=\"https://aws.amazon.com/events/summits/paris/\">Paris</a> (April 9), <a href=\"https://aws.amazon.com/events/summits/amsterdam/\">Amsterdam</a> (April 16), <a href=\"https://aws.amazon.com/events/summits/london/\">London</a> (April 30), and <a href=\"https://pages.awscloud.com/aws-summit-poland-2025-registration.html\">Poland</a> (May 5).</p><p> ‚Äì Mark your calendars for AWS re:Inforce (June 16‚Äì18) in Philadelphia, PA. AWS re:Inforce is a learning conference focused on AWS security solutions, cloud security, compliance, and identity. You can subscribe for event updates now!</p><p>That‚Äôs all for this week. Check back next Monday for another Weekly Roundup!</p><p><a href=\"https://aws.amazon.com/blogs/aws/tag/week-in-review/\"></a><em> series. Check back each week for a quick roundup of interesting news and announcements from AWS!</em></p>","contentLength":3138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self hosted kubernetes, how to make control plane easier....","url":"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/","date":1739800870,"author":"/u/TheBeardMD","guid":1865,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Very familiar with AWS EKS, where you don&#39;t really have to worry about the control plane at all. Thinking about starting a cluster from scratch, but find the control plane very complex. Are there any options to make managing the control plane easier so it&#39;s possible to create a cluster from scratch?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheBeardMD\"> /u/TheBeardMD </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkh90/self_hosted_kubernetes_how_to_make_control_plane/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The state of Kubernetes job market in 2024","url":"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/","date":1739800433,"author":"/u/danielepolencic","guid":1864,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/\"> <img src=\"https://external-preview.redd.it/BycxNH7ovolfHwotvNh-bJXbjRfA8Et5_h5nErx1JMA.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca3d7a63e70d5d842882f98b3db33b15e3165655\" alt=\"The state of Kubernetes job market in 2024\" title=\"The state of Kubernetes job market in 2024\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/danielepolencic\"> /u/danielepolencic </a> <br/> <span><a href=\"https://kube.careers/state-of-kubernetes-jobs-2024-q4\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irkbzd/the_state_of_kubernetes_job_market_in_2024/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deploying and Managing Application Configurations using AWS AppConfig","url":"https://aws.amazon.com/blogs/devops/deploying-and-managing-application-configurations-using-aws-appconfig/","date":1739800142,"author":"Aditya Ranjan","guid":1832,"unread":true,"content":"<p>The management of configurations across multiple environments and tenants poses a significant challenge in modern software development. Organizations must balance maintaining distinct settings for various environments while accommodating the unique needs of different tenants in multi-tenant architectures. This complexity is compounded by requirements for consistency, version control, security, and efficient troubleshooting.</p><p><a href=\"https://aws.amazon.com/systems-manager/features/appconfig/?whats-new-cards.sort-by=item.additionalFields.postDateTime&amp;whats-new-cards.sort-order=desc&amp;blog-posts-cards.sort-by=item.additionalFields.createdDate&amp;blog-posts-cards.sort-order=desc\">AWS AppConfig</a> offers a powerful solution to these challenges. AWS AppConfig centrally stores, manages, and deploys application configurations. It streamlines pushing changes without frequent code deployments. The service also enables automatic rollbacks, providing a safety net for configuration changes.</p><p>When integrated with a CI/CD pipeline, such as GitLab, AWS AppConfig becomes part of a streamlined, automated system for configuration management. This combination addresses the complexities of multi-environment and multi-tenant deployments, ensuring consistent, version-controlled, and secure configuration management across the entire application ecosystem.</p><h2><strong>Solution and Scenario Overview</strong></h2><p>The GitLab CI/CD pipeline in this blog focuses on the way application configurations are managed and deployed using AWS AppConfig. By automating the entire process from configuration updates to multi-environment deployment, it offers a streamlined approach to configuration management.</p><p>In this configuration management setup, we‚Äôre dealing with a multi-environment, multi-tenant application structure that leverages AWS AppConfig for configuration deployment.</p><p>It describes a multi-tenant configuration setup where each tenant has dedicated environments (dev and qa). Real-world examples of what these could represent:</p><ul><li>Development (dev): Where developers test new features and changes</li><li>Quality Assurance (qa): Where quality assurance teams validate changes before production</li></ul><p>The system supports multiple tenants (tenant1, tenant2), each with their own isolated environments. In real-world applications, these tenants could represent:</p><ul><li>Different customers: \n  <ul><li>A retail company (tenant1)</li><li>A healthcare provider (tenant2)</li></ul></li><li>Different business units: \n  <ul><li>North America division (tenant1)</li></ul></li></ul><p>Each tenant maintains separate configurations for their dev and qa environments, with three example configuration files:</p><ol></ol><p>The ‚Äòtemplate‚Äô directory provides base configuration files that can be inherited and customized by each tenant‚Äôs environment-specific configurations. This hierarchical structure ensures that tenants can maintain their unique configurations while adhering to a standardized template format.</p><p>Here‚Äôs an example of how the template YAML files might look:</p><pre><code># AllowList.yml\n\n# Network Access Controls\nip_allowlists:\n  internal_networks:\n    - \"10.0.0.0/8\"     # Internal corporate network\n    - \"172.16.0.0/12\"  # VPC network range\n    - \"192.168.1.0/24\" # Development network\n\n# Domain Allowlist\ndomain_allowlist:\n  api_consumers:\n    - \"api.partner1.com\"\n    - \"services.partner2.com\"\n    - \"*.trusted-client.com\"</code></pre><pre><code># FeatureFlags.yml\n\nfeatures:\n  new_search:\n    enabled: true\n    rollout_percentage: 76\n    description: \"Enhanced search functionality\"\n    \n  ai_recommendations:\n    enabled: true\n\n  chat_support:\n    enabled: false\n    description: \"In-app chat support\"</code></pre><pre><code>#ThrottlingLimits.yml\napi_limits:\n  global:\n    requests_per_second: 100\n    concurrent_requests: 50\n    max_retry_attempts: 3\n\nservice_specific:\n  user_service:\n      requests_per_second: 80\n      burst_limit: 100</code></pre><p>These templates serve as the starting point for all environment and tenant-specific configurations.</p><p>The folder structure reflects a sophisticated approach to organizing configurations across different environments and tenants.</p><div><div><pre><code>‚îú‚îÄ‚îÄ template\n‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ AllowList.yml\n‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ FeatureFlags.yml\n‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ ThrottlingLimits.yml\n\n‚îî‚îÄ‚îÄ tenants\n‚îú‚îÄ‚îÄ tenant1\n‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ dev\n‚îÇ&nbsp;&nbsp; ‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ AllowList.yml\n‚îÇ&nbsp;&nbsp; ‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ FeatureFlags.yml\n‚îÇ&nbsp;&nbsp; ‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ ThrottlingLimits.yml\n‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ qa\n‚îÇ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚îú‚îÄ‚îÄ AllowList.yml\n‚îÇ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚îú‚îÄ‚îÄ FeatureFlags.yml\n‚îÇ&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚îî‚îÄ‚îÄ ThrottlingLimits.yml\n‚îî‚îÄ‚îÄ tenant2\n‚îú‚îÄ‚îÄ dev\n‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ AllowList.yml\n‚îÇ&nbsp;&nbsp; ‚îú‚îÄ‚îÄ FeatureFlags.yml\n‚îÇ&nbsp;&nbsp; ‚îî‚îÄ‚îÄ ThrottlingLimits.yml\n‚îî‚îÄ‚îÄ qa\n‚îú‚îÄ‚îÄ AllowList.yml\n‚îú‚îÄ‚îÄ FeatureFlags.yml\n‚îî‚îÄ‚îÄ ThrottlingLimits.yml</code></pre></div><p>At the root level, we have two main directories:</p></div><ol><li>template: Houses the base configuration templates</li><li>tenants: Contains tenant-specific configurations</li></ol><p>The ‚Äòtenants‚Äô directory follows a hierarchical structure where each tenant (tenant1, tenant2) has their own directory. Within each tenant‚Äôs directory, there are ‚Äòdev‚Äô and ‚Äòqa‚Äô environment subdirectories. Each environment directory contains three configuration files: AllowList.yml, FeatureFlags.yml, and ThrottlingLimits.yml. These files represent different aspects of the application‚Äôs configuration and can override the base templates found in the ‚Äòtemplate‚Äô directory. This structure allows for environment-specific configurations while maintaining a clear separation between tenants and their respective environments.</p><p>This structure allows for:</p><ol><li><strong>Standardization through templates:</strong> The base templates in the ‚Äòtemplate‚Äô directory ensure consistency across all tenants, providing default configurations that can be selectively overridden by tenant-specific needs.</li><li><strong>Tenant-specific customization:</strong> Each tenant can maintain unique configurations in their dev and qa environments while inheriting from the base templates. This allows for customization without losing standardization benefits.</li><li> Clear separation between dev and qa environments within each tenant‚Äôs directory ensures that configuration changes in one environment don‚Äôt affect other</li><li><strong>Version control of configurations:</strong> By storing configurations in a Git repository, changes can be tracked, reviewed, and rolled back if necessary.</li><li><strong>AWS AppConfig integration:</strong></li></ol><ol><li><ol><li>Each tenant gets their own Application in AWS AppConfig</li><li>Configuration profiles map to different configuration types (AllowList, FeatureFlags, ThrottlingLimits)</li><li>Separate environments (dev/qa) within each tenant‚Äôs application</li></ol></li></ol><p>The GitLab CI/CD pipeline we‚Äôre setting up will need to:</p><ol><li>Generate environment and tenant-specific configurations based on these templates</li><li>Update the corresponding applications and configuration profiles in AWS AppConfig</li><li>Deploy the appropriate configurations to each tenant and environment</li></ol><pre><code>job_name:\ntags:\n- aws-runner&nbsp; # Tag of your specific runner</code></pre><h2><strong>Setting Up the Directory Structure:</strong></h2><ol><li>First, create the base directory structure using these commands:</li></ol><div><div><div><pre><code># Create directory structure and files\nmkdir -p template tenants/{tenant1,tenant2}/{dev,qa}</code></pre></div></div></div><ol start=\"2\"><li>Create all required YAML files:</li></ol><div><div><pre><code>for file in AllowList.yml FeatureFlags.yml ThrottlingLimits.yml;do\n  touch template/$file\n  touch tenants/tenant{1,2}/{dev,qa}/$file\ndone</code></pre></div></div><ol start=\"3\"><li>Populate the template files:</li></ol><div><pre><code>Copy the content of each YAML file (AllowList.yml, FeatureFlags.yml, ThrottlingLimits.yml) shown above into the corresponding files in the template directory.</code></pre></div><ol start=\"4\"><li>For tenant-specific configurations:</li></ol><div><pre><code>Start by copying the template files to each tenant's environment directory</code></pre></div><ol start=\"5\"><li>Verify the folder structure.</li></ol><h2><strong>Setting Up the GitLab CI/CD Pipeline</strong></h2><p>Code for the GitLab pipeline is in this <a href=\"https://github.com/aws-samples/Deploy-and-Manage-Application-Configurations-with-AWSAppConfig\">repo.</a></p><p>This phase begins with gaining a clear understanding of the pipeline‚Äôs structure and flow, which forms the foundation for all subsequent steps.</p><h3>Configuring .gitlab-ci.yml</h3><ol><li><ol><li>Creating the .gitlab-ci.yml file in your repository root</li><li>Defining the base image for the pipeline (e.g., alpine:latest)</li><li>Setting up pipeline stages: update-app-config, deploy-app-config</li><li>Configuring global variables and default settings \n    <ul><li>Locate these sections in the .gitlab-ci.yml file below and Replace them with your AWS account details</li></ul></li></ol></li></ol><pre><code>variables:\n  AWS_CREDS_TARGET_ROLE: arn:aws:iam::&lt;aws_account_ID&gt;:role/GitLab\n  AWS_DEFAULT_REGION: &lt;aws_region&gt;</code></pre><ul><li><ul><li><ul><li>&nbsp;Make sure to replace these variables in both stages (update-app-config and deploy-app-config) of the pipeline. The AWS role should have appropriate permissions to interact with AWS AppConfig service</li></ul></li></ul></li></ul><p>Here‚Äôs the complete .gitlab-ci.yml file:</p><pre><code>stages:\n  - update-app-config\n  - deploy-app-config\n\nupdate-app-config:\n  stage: update-app-config\n  image:\n    name: amazon/aws-cli:latest\n    entrypoint:\n      - '/usr/bin/env'\n  script:\n    - |\n      # Get list of all tenant\n      TENANTS=$(find tenants -mindepth 1 -maxdepth 1 -type d -exec basename {} \\;)\n      \n      for TENANT in $TENANTS; do\n        echo \"Processing tenant: $TENANT\"\n        \n        # Create/Get Application for tenant\n        APP_ID=$(aws appconfig list-applications --query \"Items[?Name=='$TENANT'].Id\" --output text)\n        if [ -z \"$APP_ID\" ]; then\n          echo \"Creating application for tenant '$TENANT'...\"\n          APP_ID=$(aws appconfig create-application --name $TENANT --query Id --output text)\n        fi\n        \n        # Process each configuration type\n        for CONFIG_TYPE in AllowList FeatureFlags ThrottlingLimits; do\n          echo \"Processing config type: $CONFIG_TYPE\"\n          \n          # Create/Get Configuration Profile\n          PROFILE_ID=$(aws appconfig list-configuration-profiles --application-id \"$APP_ID\" --query \"Items[?Name=='$CONFIG_TYPE'].Id\" --output text)\n          if [ -z \"$PROFILE_ID\" ]; then\n            echo \"Creating configuration profile '$CONFIG_TYPE' for tenant '$TENANT'...\"\n            PROFILE_ID=$(aws appconfig create-configuration-profile --application-id \"$APP_ID\" --name \"$CONFIG_TYPE\" --description \"Configuration profile for $CONFIG_TYPE\" --location-uri hosted --query Id --output text)\n          fi\n          \n          # Process each environment\n          for ENV in dev qa; do\n            echo \"Processing environment: $ENV\"\n            \n            # Priority: Use tenant-specific config if it exists, otherwise use template\n            if [ -f \"tenants/$TENANT/$ENV/$CONFIG_TYPE.yml\" ]; then\n              echo \"Using tenant-specific configuration for $ENV\"\n              CONFIG_CONTENT=$(cat \"tenants/$TENANT/$ENV/$CONFIG_TYPE.yml\" | base64)\n            else\n              echo \"Using template configuration for $ENV\"\n              CONFIG_CONTENT=$(cat \"template/$CONFIG_TYPE.yml\" | base64)\n            fi\n            \n            echo \"Creating new version for $CONFIG_TYPE configuration in $ENV...\"\n            aws appconfig create-hosted-configuration-version \\\n              --application-id \"$APP_ID\" \\\n              --configuration-profile-id \"$PROFILE_ID\" \\\n              --content \"$CONFIG_CONTENT\" \\\n              --content-type \"application/json\" \\\n              configuration_version_output\n          done\n        done\n      done\n  variables:\n    AWS_CREDS_TARGET_ROLE: arn:aws:iam::&lt;aws_account_ID&gt;:role/GitLab \n    AWS_DEFAULT_REGION: &lt;aws_region&gt;\n\ndeploy-app-config:\n  stage: deploy-app-config\n  image: \n    name: amazon/aws-cli:latest\n    entrypoint: \n      - '/usr/bin/env'\n  script:\n    - yum install -y jq\n    - |\n      TENANTS=$(find tenants -mindepth 1 -maxdepth 1 -type d -exec basename {} \\;)\n      \n      for TENANT in $TENANTS; do\n        echo \"Processing tenant: $TENANT\"\n        APP_ID=$(aws appconfig list-applications --query \"Items[?Name=='$TENANT'].Id\" --output text)\n        \n        # Process each environment\n        for ENV in dev qa; do\n          echo \"Processing environment: $ENV\"\n          \n          # Create/Get Environment\n          ENV_ID=$(aws appconfig list-environments --application-id \"$APP_ID\" --query \"Items[?Name=='$ENV'].Id\" --output text)\n          if [ -z \"$ENV_ID\" ]; then\n            echo \"Creating environment '$ENV' for tenant '$TENANT'...\"\n            ENV_ID=$(aws appconfig create-environment --application-id \"$APP_ID\" --name \"$ENV\" --description \"Environment for $ENV\" --query Id --output text)\n          fi\n          \n          # Process each configuration types\n          for CONFIG_TYPE in AllowList FeatureFlags ThrottlingLimits; do\n            echo \"Processing $CONFIG_TYPE for $TENANT/$ENV\"\n            \n            PROFILE_ID=$(aws appconfig list-configuration-profiles --application-id \"$APP_ID\" --query \"Items[?Name=='$CONFIG_TYPE'].Id\" --output text)\n\n            echo \" Profile ID $PROFILE_ID \"\n            # Get latest version for this specific profile\n            LATEST_VERSION=$(aws appconfig list-hosted-configuration-versions \\\n              --application-id \"$APP_ID\" \\\n              --configuration-profile-id \"$PROFILE_ID\" \\\n              --query \"Items[0].VersionNumber\" \\\n              --output text)\n            \n            # Get current deployment for this specific profile\n            CURRENT_DEPLOYMENT=$(aws appconfig list-deployments \\\n            --application-id \"$APP_ID\" \\\n            --environment-id \"$ENV_ID\" \\\n            --query \"Items[?ConfigurationName=='$CONFIG_TYPE'].ConfigurationVersion | [0]\" \\\n            --output text)\n\n\n            echo \"Current deployment $CURRENT_DEPLOYMENT\"\n              \n            CURRENT_VERSION=$(aws appconfig list-deployments \\\n            --application-id \"$APP_ID\" \\\n            --environment-id \"$ENV_ID\" \\\n            --query \"Items[?ConfigurationName=='$CONFIG_TYPE'].ConfigurationVersion | [0]\" \\\n            --output text)\n            \n            echo \"Latest Version: $LATEST_VERSION\"\n            echo \"Current Version: $CURRENT_VERSION\"\n            \n            if [[ \"$CURRENT_DEPLOYMENT\" == \"None\" ]] || [[ \"$LATEST_VERSION\" != \"$CURRENT_VERSION\" ]]; then\n              echo \"Starting deployment for $TENANT/$ENV/$CONFIG_TYPE...\"\n              DEPLOYMENT_RESPONSE=$(aws appconfig start-deployment \\\n                --application-id \"$APP_ID\" \\\n                --environment-id \"$ENV_ID\" \\\n                --deployment-strategy-id Linear50PercentEvery30Seconds \\\n                --configuration-profile-id \"$PROFILE_ID\" \\\n                --configuration-version \"$LATEST_VERSION\")\n              \n              DEPLOYMENT_ID=$(echo $DEPLOYMENT_RESPONSE | jq -r '.DeploymentNumber')\n              \n              # Monitor deployment\n              max_attempts=10\n              attempt=1\n              while [ $attempt -le $max_attempts ]; do\n                echo \"Checking deployment status (attempt $attempt of $max_attempts)...\"\n                status=$(aws appconfig get-deployment \\\n                  --application-id \"$APP_ID\" \\\n                  --environment-id \"$ENV_ID\" \\\n                  --deployment-number \"$DEPLOYMENT_ID\" \\\n                  --query \"State\" \\\n                  --output text)\n                \n                if [ \"$status\" = \"COMPLETE\" ]; then\n                  echo \"Deployment completed successfully!\"\n                  break\n                elif [ \"$status\" = \"FAILED\" ] || [ \"$status\" = \"ROLLED_BACK\" ]; then\n                  echo \"Deployment failed or was rolled back!\"\n                  exit 1\n                fi\n                \n                if [ $attempt -eq $max_attempts ]; then\n                  echo \"Deployment timed out after $max_attempts attempts\"\n                  exit 1\n                fi\n                \n                attempt=$((attempt + 1))\n                sleep 30\n              done\n            else\n              echo \"No changes detected for $TENANT/$ENV/$CONFIG_TYPE (Current: $CURRENT_VERSION, Latest: $LATEST_VERSION). Skipping deployment...\"\n            fi\n          done\n        done\n      done\n  dependencies:\n    - update-app-config\n  variables:\n    AWS_CREDS_TARGET_ROLE: arn:aws:iam::&lt;aws_account_ID&gt;:role/GitLab \n    AWS_DEFAULT_REGION: &lt;aws_region&gt;</code></pre><h3>Implementing Pipeline Stages</h3><ul><li>Creates/Updates AWS AppConfig Applications: \n  <ul><li>Creates one application per tenant (tenant1, tenant2)</li><li>Uses tenant ID as application name</li><li>Retrieves existing application if already present</li></ul></li><li>Manages Configuration Profiles: \n  <ul><li>Creates three profiles per tenant application (AllowList, FeatureFlags, ThrottlingLimits)</li><li>Each profile represents a distinct configuration type</li><li>Handles profile creation if not already existing</li></ul></li><li>Creates Hosted Configuration Versions: \n  <ul><li>Processes changes from both template and tenant directories</li><li>Prioritizes tenant-specific configurations over templates</li><li>Creates new versions only for modified configurations</li><li>Uploads properly encoded configurations to AWS AppConfig</li></ul></li></ul><ol><li><ul><li>Environment Deployment: \n    <ul><li>Manages dev and qa environments per tenant</li><li>Creates environments if not existing</li><li>Uses staged deployment strategy</li></ul></li><li>Tenant Configuration Process: \n    <ul><li>Deploys per tenant and configuration type</li><li>Checks current deployed version against latest version</li><li>Only deploys if either of the follows is true: \n      <ul><li>No existing deployment is found</li><li>Latest Hosted Configuration version differs from currently deployed version</li></ul></li><li>Maintains tenant-specific settings and version history</li><li>Provides clear deployment status messages, including cases where deployment is skipped</li></ul></li></ul></li></ol><ul><li><ul><li>Deployment Management: \n    <ul><li>Executes AWS AppConfig deployments</li><li>Monitors deployment status</li><li>Handles failures and rollbacks</li><li>Times out after 10 retries</li></ul></li></ul></li></ul><ol><li>Initiation: \n  <ul><li>Pipeline triggered by changes pushed to the repository</li></ul></li></ol><ol start=\"2\"><li>Update-App-Config Stage: \n  <ul><li>Creates or updates applications and configuration profiles</li><li>Generates new versions of hosted configurations</li></ul></li></ol><ol start=\"3\"><li>Deploy-App-Config Stage: \n  <ul><li>Iterates through each environment tenant and their environments</li><li>Checks current deployment status for each environment and tenant</li><li>Initiates new deployments only for changed configurations</li><li>Implements specified AWS AppConfig deployment strategy</li></ul></li></ol><p> Deployment Strategy used in this example is a fast one used for testing (Linear50PercentEvery30Seconds) but for real production workloads, the reader should use the slower, AWS-recommended Linear20PercentEvery6Minutes strategy. More details <a href=\"https://docs.aws.amazon.com/appconfig/latest/userguide/appconfig-creating-deployment-strategy.html\">here</a></p><p>This structured execution process ensures efficient and consistent deployment of configuration changes across the entire application ecosystem, maintaining synchronization between GitLab and AWS AppConfig.</p><p>To clean up all AWS AppConfig resources created by this solution, you can use the following cleanup script. Create a file named delete_appconfig_resources.sh with this content:</p><pre><code>#!/bin/bash\n\n# List all applications\nAPPS=$(aws appconfig list-applications --query 'Items[*].Id' --output text)\n\nfor APP_ID in $APPS\ndo\n  echo \"Processing application $APP_ID\"\n  \n  # List and delete all environments for this application\n  ENVS=$(aws appconfig list-environments --application-id $APP_ID --query 'Items[*].Id' --output text)\n  for ENV_ID in $ENVS\n  do\n    echo \"  Deleting environment $ENV_ID\"\n    aws appconfig delete-environment --application-id $APP_ID --environment-id $ENV_ID\n  done\n\n  # List and delete all configuration profiles for this application\n  PROFILES=$(aws appconfig list-configuration-profiles --application-id $APP_ID --query 'Items[*].Id' --output text)\n  for PROFILE_ID in $PROFILES\n  do\n    echo \"  Deleting configuration profile $PROFILE_ID\"\n    \n    # Delete all hosted configuration versions for this profile\n    VERSIONS=$(aws appconfig list-hosted-configuration-versions --application-id $APP_ID --configuration-profile-id $PROFILE_ID --query 'Items[*].VersionNumber' --output text)\n    for VERSION in $VERSIONS\n    do\n      echo \"    Deleting hosted configuration version $VERSION\"\n      aws appconfig delete-hosted-configuration-version --application-id $APP_ID --configuration-profile-id $PROFILE_ID --version-number $VERSION\n    done\n\n    # Delete the configuration profile\n    aws appconfig delete-configuration-profile --application-id $APP_ID --configuration-profile-id $PROFILE_ID\n  done\n\n  # Delete the application\n  echo \"  Deleting application $APP_ID\"\n  aws appconfig delete-application --application-id $APP_ID\ndone\n\necho \"All AppConfig resources have been deleted.\"</code></pre><p>The script is a comprehensive cleanup utility for AWS AppConfig resources.</p><p>To execute this script, you need to have the AWS CLI installed and configured with appropriate credentials that have permissions to delete AppConfig resources. Make the script delete_appconfig_resources.sh &nbsp;executable by running the command:</p><div><pre><code>chmod +x cleanup_appconfig.sh.</code></pre></div><p>Before running the script, ensure that you‚Äôre in the correct AWS account and region, as this script will delete ALL AppConfig resources in the configured account and region. To execute the script, simply run it from your terminal: &nbsp;./ delete_appconfig_resources.sh</p><p>It‚Äôs crucial to note that this script performs irreversible deletions. Use it with extreme caution, preferably in non-production environments or when you‚Äôre absolutely certain you want to remove all AppConfig resources.</p><p>This blog post has explored the powerful synergy between GitLab CI/CD and AWS AppConfig for managing application configurations in multi-tenant environments. We‚Äôve demonstrated how this integration automates and streamlines the process of updating, versioning, and deploying configuration changes, offering benefits such as scalability, version control, and the balance between consistency and flexibility. By adopting this approach, development teams can significantly reduce manual errors, save time, and focus more on building features, ultimately leading to faster development cycles and more reliable applications in our increasingly complex and distributed computing landscape.</p><p>Key resources for further reading:</p>","contentLength":21247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"kubegui.io - user friendly kubernetes desktop application","url":"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/","date":1739795999,"author":"/u/Live_Landscape_7570","guid":1809,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\"> <img src=\"https://a.thumbs.redditmedia.com/3eh_HCnUiUTn-Kn5n_tSv4JYVCz06lQL6-ycwAITBU8.jpg\" alt=\"kubegui.io - user friendly kubernetes desktop application\" title=\"kubegui.io - user friendly kubernetes desktop application\" /> </a> </td><td> <!-- SC_OFF --><div class=\"md\"><p>Who we are? SRE engineers! What do we want? </p> <p><a href=\"http://kubegui.io\">One more GUI-based Kubernetes management tool</a> for daily operations. </p> <p>I&#39;ve just created a golang/wails based client for any available Kubernetes cluster that&#39;s better then alternatives (based on exceptional research made within my family members) and much much cheaper. </p> <p><a href=\"https://preview.redd.it/veh4twld2pje1.png?width=1600&amp;format=png&amp;auto=webp&amp;s=33b972e13d958d3ff208aef70a2c6497fc4f9aea\">kubegui.io</a></p> <p>Some advantages: </p> <p>‚ö° Lightning Fast Performance: Built with Go official kubernetes client for maximum speed/cache usage + minimal resource usage </p> <p>üíª Zero Dependencies: No kubectl required (or any other tools) </p> <p>üîÑ Seamless Multi-cluster Management: Switch between clusters with last viewed resource state saved </p> <p>üí° AI provided suggestions: Realtime AI integrations for fix suggestions (for deployments/pods/events issues) </p> <p>üìä Advanced Monitoring: Real-time metrics out of the box (for pods/nodes for the last hour) </p> <p>üîí Enhanced Security: No external calls (except for AI fix suggestions if enabled) </p> <p>üì¶ Single Binary Distribution: No runtime dependencies required </p> <p>üìÑ Smart yaml viewer: Context-aware editor with indentation linter and error detection </p> <p>üìù Interactive Shell access: One click pod exec (xterm with copy-paste available) </p> <p>üéÆ Pod ports forwarding: One click inside pod details exposed ports (via default browser session). </p> <p>üõ°Ô∏è Network Policies: Visualize policy feature inside resource details </p> <p>üîç Enhanced log viewer: Built-in logs syntax highlighter </p> <p>üîë Custom resource generator: Unique custom resource example creation based on crd schema </p> <p>‚ûï Auto updates: self-updating application (via github public project repo background calls)</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Live_Landscape_7570\"> /u/Live_Landscape_7570 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irixbt/kubeguiio_user_friendly_kubernetes_desktop/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask r/kubernetes: What are you working on this week?","url":"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/","date":1739790030,"author":"/u/gctaylor","guid":1775,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>What are you up to with Kubernetes this week? Evaluating a new tool? In the process of adopting? Working on an open source project or contribution? Tell <a href=\"/r/kubernetes\">/r/kubernetes</a> what you&#39;re up to this week!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/gctaylor\"> /u/gctaylor </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irhcrh/ask_rkubernetes_what_are_you_working_on_this_week/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes Doesn‚Äôt Have to Be Complicated ‚Äì A Practical Guide to Simplifying Your Workflow","url":"https://www.reddit.com/r/kubernetes/comments/1irghmz/kubernetes_doesnt_have_to_be_complicated_a/","date":1739786358,"author":"/u/Sheishofert","guid":1743,"unread":true,"content":"&#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Sheishofert\"> /u/Sheishofert </a> <br/> <span><a href=\"https://rust-on-nails.com/blog/kubernetes-complicated/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irghmz/kubernetes_doesnt_have_to_be_complicated_a/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-126/","date":1739782821,"author":"Mike Vizard","guid":1674,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses","url":"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/","date":1739781956,"author":"/u/Wownever","guid":1679,"unread":true,"content":"<table> <tr><td> <a href=\"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/\"> <img src=\"https://external-preview.redd.it/htjVgpTbC6LEHHA_g3zSEEkmVxg_6sYEyx94ado_4FU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=f8858d3c38de489836abc3910c8502b1710b9834\" alt=\"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses\" title=\"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses\" /> </a> </td><td> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Wownever\"> /u/Wownever </a> <br/> <span><a href=\"https://www.wiz.io/blog/kubernetes-report-preview-2025\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/\">[comments]</a></span> </td></tr></table>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do you scale to zero and from zero?","url":"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/","date":1739779527,"author":"/u/Electronic_Role_5981","guid":1680,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://github.com/kubernetes/enhancements/issues/2021\">https://github.com/kubernetes/enhancements/issues/2021</a> is open. `HPAScaleToZero` is alpha in v1.16 and has no much updates. </p> <p>There are several known choices like</p> <ul> <li>Scale from zero(<strong>No native support</strong>), or known as activator. (Also some Faas platforms, like OpenFaas or Serverless apps supports)</li> </ul> <ol> <li>Knative: Activator. <a href=\"https://knative.dev/docs/serving/architecture/#diagram\">https://knative.dev/docs/serving/architecture/#diagram</a> <a href=\"https://github.com/knative/serving\">knative/serving</a></li> <li>KEDA <ol> <li>Example: OpenFunction: <a href=\"https://github.com/OpenFunction/OpenFunction/pull/483\">https://github.com/OpenFunction/OpenFunction/pull/483</a> &amp; <a href=\"https://github.com/OpenFunction/OpenFunction/blob/main/docs/proposals/20230726-integrate-keda-http-add-on.md\">https://github.com/OpenFunction/OpenFunction/blob/main/docs/proposals/20230726-integrate-keda-http-add-on.md</a></li> </ol></li> <li>A initial implementation using service, like kube-proxy: <a href=\"https://github.com/wzshiming/kube-activator\">https://github.com/wzshiming/kube-activator</a> <ul> <li>Scale to zero (natively alpha feature)</li> </ul></li> <li>HPA: HPAScaleToZero feature gate</li> <li><a href=\"https://knative.dev/docs/serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period\">https://knative.dev/docs/serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period</a></li> <li><a href=\"https://github.com/deislabs/osiris\">https://github.com/deislabs/osiris</a> archived as the hpa supports it</li> </ol> <p>The last discussion in reddit is <a href=\"https://www.reddit.com/r/kubernetes/comments/1de8qiz/scaling%5C_to%5C_zero/\">https://www.reddit.com/r/kubernetes/comments/1de8qiz/scaling\\_to\\_zero/</a>. </p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Set Up a Persistent Volume for MinIO on GKE Free Tier? Do I Get Any Free Storage?","url":"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/","date":1739775790,"author":"/u/blvck_viking","guid":1633,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m setting up a self-hosted MinIO instance on Google Kubernetes Engine (GKE) and need to configure a persistent volume for storage. I&#39;m currently using the GKE free tier and was wondering:</p> <ol> <li>Does GKE free tier include any free persistent storage, or will I need to pay for it?</li> <li>What&#39;s the best way to set up a Persistent Volume (PV) and Persistent Volume Claim (PVC) for MinIO in a GKE cluster?</li> <li>Any recommendations on storage classes and best practices?</li> </ol> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/blvck_viking\"> /u/blvck_viking </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingress Help","url":"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/","date":1739754260,"author":"/u/MeerkatMoe","guid":860,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I&#39;m trying to setup ingress using ingress nginx, but I can&#39;t figure out how to get routing to work...either my frontend breaks or my api is unreachable.</p> <p>I have an nginx service (not ingress nginx) that serves a frontend on port 80 and an express service that serves a backend API on port 5000.</p> <p>My first attempt was two separate ingresses (not sure about terminology):</p> <pre><code>--- metadata: name: api-ingress annotations: kubernetes.io/ingress.class: &quot;nginx&quot; spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} --- metadata: name: frontend-ingress namespace: {{ k3s_namespace }} annotations: kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: / pathType: Prefix backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre> <p>but that didn&#39;t work, and sometimes my API won&#39;t get routed correctly. I think it&#39;s because they get combined and I can&#39;t guarantee the order.</p> <p>My next try was to combine them:</p> <pre><code>kubernetes.io/ingress.class: &quot;nginx&quot; nginx.ingress.kubernetes.io/use-regex: &quot;true&quot; nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} - path: &quot;/(?!api).*&quot; pathType: ImplementationSpecific backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre> <p>(left some stuff out to save space)</p> <p>but that also didn&#39;t work.</p> <p>What is the best way to get this working? To summarize, I just need</p> <p>&quot;/api/*&quot; -&gt; api service port 5000 (it can route as /api/&lt;whatever&gt; or just &lt;whatever&gt;)</p> <p>&quot;/*&quot; -&gt; nginx port 80</p> <p>Thank you!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/MeerkatMoe\"> /u/MeerkatMoe </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event driven workloads on K8s - how do you handle them?","url":"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/","date":1739752293,"author":"/u/sniktasy","guid":846,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey folks! </p> <p>I have been working with <a href=\"https://github.com/numaproj/numaflow\">Numaflow</a>, an open source project that helps build event driven applications on K8s. It basically makes it easier to process streaming data (think events on kafka, pulsar, sqs etc). </p> <p>Some cool stuff - autoscaling based on pending events/ back pressure handling (scale to 0 if need be), source and sink connectors, multi-language support, can support real time data processing use cases with the pipeline semantics etc</p> <p>Curious, how are you handling event-driven workloads today? Would love to hear what&#39;s working for others?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/sniktasy\"> /u/sniktasy </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Issues with logrotate when logrotate failed to rotate the logs for container","url":"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/","date":1739739973,"author":"/u/barely_malted","guid":796,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I am using AWS EKS and using default kubelet logrotate parameters (maxsize = 10 Mi and maxfiles = 5)<br/> I am facing an issue where I believe these default values are not respected. The kubelet is failing with &#39;Failed to rotate log for container&#39; &#39;err=failed to compress log (container/pod log paths) nospace left on device&#39;<br/> At the same time one of my pods generated 200 GB logs in one single file. How is this possible ?<br/> I was not able to find out any documentation regarding this behaviour.<br/> Does this mean that since the kubelet was not able to rotate logs, it just kept on writing them to this one log file till it reached the diskspace limits of my worker nodes ?<br/> K8s/EKS version 1.27</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/barely_malted\"> /u/barely_malted </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do devs use kubernetes services locally via ingress on the likes of docker desktop","url":"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/","date":1739734137,"author":"/u/TheRandyOne","guid":746,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have recently started getting some toolkits running for my devs. I need to get them started on k8s as I am moving services over to k8s.</p> <p>I was explaining how this works to a friend and it dawned on me that to use a resource inside the cluster you need to enter via an ingress. The ingress is easy enough since we have the nginx ingress. </p> <p>The problem comes in with the dns records required to point to the defined resource to 127.0.0.1 in the /etc/hosts file. Since we have quite few services that need to hosted in k8s, it&#39;ll really suck to have the devs to add a bunch of records to the hosts file</p> <p>Basically I want something like a wild card record that always returns 127.0.0.1 outside the cluster. So they can pick whatever name they want and always have that delivered to the ingress.</p> <p>Am I doing this wrong? Is there some other way that I should be approaching this problem?<br/> Or can someone explain how they deal with this other than just editing hosts files.</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/TheRandyOne\"> /u/TheRandyOne </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pull Request testing on Kubernetes: working with GitHub Actions and GKE","url":"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/","date":1739730123,"author":"/u/nfrankel","guid":722,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I‚Äôm continuing my series on running the test suite for each Pull Request on Kubernetes. In the <a href=\"https://blog.frankel.ch/integration-test-kubernetes/1/\">previous post</a>, I laid the groundwork for our learning journey: I developed a basic JVM-based CRUD app, tested it locally using Testcontainers, and tested it in a GitHub workflow with a GitHub service container.</p> <p>This week, I will raise the ante to run the end-to-end test in the target Kubernetes environment. For this, I‚Äôve identified gaps that I‚Äôll implement in this blog post:</p> <ul> <li>Create and configure a Google Kubernetes Engine instance</li> <li>Create a Kubernetes manifest for the app, with Kustomize for customization</li> <li>Allow the GitHub workflow to use the GKE instance</li> <li>Build the Docker image and store it in the GitHub Docker repo</li> <li>Install the PostgreSQL Helm chart</li> <li>Apply our app manifest</li> <li>Finally, run the end-to-end test</li> </ul> <p>Stages 1, 2, and 3 are upstream, while the workflow executes the latter steps for each PR.</p> <p>As I had to choose a tech stack for the app, I had to select a Cloud provider for my infrastructure. I choose GKE because I‚Äôm more familiar with Google Cloud, but you can apply the same approach to any other provider. The concept will be the same, only the implementation will differ slightly.</p> <p><a href=\"https://blog.frankel.ch/pr-testing-kubernetes/2/\">Read more</a></p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/nfrankel\"> /u/nfrankel </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Managing a Talos cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/","date":1739727418,"author":"/u/simen64","guid":698,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>I have been looking into moving my homelab to Kubernetes and Talos seems great for the job. I use OpenTofu for deploying infra in my homelab like VM&#39;s in proxmox, but how do people integrate Talos into OpenTofu / Terraform? I have not gotten the talos terraform provider to work and it lacks basic functionality for stuff like updating. So how do people manage their talos clusters?</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/simen64\"> /u/simen64 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measure cpu utilization per deployment?","url":"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/","date":1739722689,"author":"/u/netcat23","guid":745,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hi guys, does measuring cpu utilization of a deployment brings any value?</p> <p>What is you opinion about it?</p> <p>Thanks!</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/netcat23\"> /u/netcat23 </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starting a Weekly Rancher Series ‚Äì From Zero to Hero!","url":"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/","date":1739717223,"author":"/u/abhimanyu_saharan","guid":650,"unread":true,"content":"<!-- SC_OFF --><div class=\"md\"><p>Hey everyone,</p> <p>I&#39;m kicking off a weekly YouTube series on Rancher, covering everything from getting started to advanced use cases. Whether you&#39;re new to Rancher or looking to level up your Kubernetes management skills, this series will walk you through step-by-step tutorials, hands-on demos, and real-world troubleshooting.</p> <p>I&#39;ve just uploaded the introductory video where I break down what Rancher is and why it matters: üì∫ <a href=\"https://youtu.be/_CRjSf8i7Vo?si=ZR6IcXaNOCCppFiG\">https://youtu.be/_CRjSf8i7Vo?si=ZR6IcXaNOCCppFiG</a></p> <p>I&#39;ll be posting new videos every week, so if you&#39;re interested in mastering Rancher, make sure to follow along. Would love to hear your feedback and any specific topics you&#39;d like to see covered!</p> <p>Let‚Äôs build and learn together! üöÄ</p> <h1>Kubernetes #Rancher #DevOps #Containers #SelfHosting #Homelab</h1> </div><!-- SC_ON --> &#32; submitted by &#32; <a href=\"https://www.reddit.com/user/abhimanyu_saharan\"> /u/abhimanyu_saharan </a> <br/> <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/\">[link]</a></span> &#32; <span><a href=\"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/\">[comments]</a></span>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digest #160: Heroku to AWS, GPU Twists, AMI Attacks, FinOps Tools and Solving Crimes with SQL","url":"https://www.devopsbulletin.com/p/digest-160-heroku-to-aws-gpu-twists","date":1739716709,"author":"Mohamed Labouardy","guid":628,"unread":true,"content":"<p><strong>Welcome to this week‚Äôs edition of the DevOps Bulletin!</strong></p><p>We start with a big move‚Äîfrom Heroku to AWS‚Äîand even a surprising twist about GPUs. There‚Äôs also news on a new attack that uses AWS AMI names, ideas on keeping AI safe, and a fun debate about trailing commas in SQL.</p><p>Next, our featured podcast shows how finance and engineering work hand in hand to manage cloud costs, using tools like AWS CUR and CloudWatch. </p><p>If you‚Äôre looking for hands-on advice, our tutorials are packed with clear guides‚Äîfrom Terraform production tips and using BigQuery to easy guides for Azure Kubernetes, understanding unit economics, and more.</p><p>We also highlight open-source devtools like a tool to spot Nginx issues, a fun game that uses SQL to solve mysteries, a smart tool for managing config files, and even a Rust-based ping tool that tracks network speed.</p><p>All this and more in this week‚Äôs DevOps Bulletin‚Äîdon‚Äôt miss out!</p><p>This podcast explores how finance and engineering teams collaborate in FinOps to manage cloud costs. Success comes from aligning goals, sharing insights, and using AWS CUR and CloudWatch.</p><ul><li><p> is a tool that analyzes Nginx configuration to prevent security misconfiguration and automate flaw detection.</p></li><li><p> is a game where you solve crimes with SQL queries and uncover evidence through data.</p></li><li><p> is a lightweight configuration management tool that updates local config files using key/value stores like etcd or Consul.</p></li><li><p> provides automated cost optimization for AWS and GCP. It includes a GCP Organization Recommender for cost-saving insights and an AWS Resource Cleanup tool to remove unused resources.</p></li></ul><ul><li><p> is a Rust-based Ping tool using the ICMP protocol, offering real-time latency tracking, visual charts, and concurrent pinging of multiple addresses.</p></li></ul><div><p>If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":1893,"flags":null,"enclosureUrl":"https://substackcdn.com/image/youtube/w_728,c_limit/_cjuQlc62uc","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}