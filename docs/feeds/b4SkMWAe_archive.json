{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":71,"items":[{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-25/","date":1762011154,"author":"","guid":324098,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-24/","date":1762011153,"author":"","guid":324097,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-23/","date":1762011147,"author":"","guid":324096,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-22/","date":1762011146,"author":"","guid":324095,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-21/","date":1762011143,"author":"","guid":324094,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-20/","date":1762011139,"author":"","guid":324093,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-19/","date":1762011138,"author":"","guid":324092,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-18/","date":1762011132,"author":"","guid":324091,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-17/","date":1762011131,"author":"","guid":324090,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Key not found in data","url":"https://devops.com/key-not-found-in-data-16/","date":1762011130,"author":"","guid":324089,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Story Of an App That Was Slow","url":"https://blog.devops.dev/the-story-of-an-app-that-was-slow-3844f1af9250?source=rss----33f8b2d9a328---4","date":1762006322,"author":"Amita Pal Singh","guid":324086,"unread":true,"content":"<p>Once upon a time, there was an app — , it had beautiful forms, users loved it. Soon hundreds of people started visiting it. But then it started struggling. With 1500 daily users… slow load times and outages started causing&nbsp;chaos!</p><p>Bandwidth is the maximum amount of data that can pass through the network at any given&nbsp;time.</p><p>Throughput is the average amount of data that actually passes through over a given period of&nbsp;time.</p><p>It is measured in bps = bits per&nbsp;second</p><p>Network latency is measured in milliseconds by calculating the time interval between the initiation of a send operation from a source system and the completion of the matching receive operation by the target&nbsp;system.</p><p>This measurement helps developers understand how quickly a webpage or application will load for&nbsp;users.</p><p>A ping rate of less than 100ms is considered acceptable but latency in the range of 30–40ms is desirable.</p><p>The amount of time it takes for a response to reach a client device after a client request. It is double the Latency, plus processing time at&nbsp;server.</p><p>Expected Daily users =&nbsp;1500</p><p>Expected Peak traffic = 100–200 form submissions per&nbsp;hour</p><p>Required Bandwidth ≥300 Mbps for 200–300 RPS (~1 MB&nbsp;each).</p><p>Throughput ≥250 Mbps (80% of Bandwidth) for smooth data&nbsp;flow.</p><p>Required Latency &lt;100ms for snappy form rendering.</p><p>RTT &lt;200ms for quick submissions.</p><p>Upgrade Bandwidth to 500 Mbps, enough for 200–300 simultaneous 1 MB form submissions</p><p>Use QoS rules to prioritize form traffic and upgrade load balancers</p><p>Deploy a CDN to cache form assets closer to users and optimize server&nbsp;code</p><p>Optimize Db Queries, Add&nbsp;Indexes</p><p>Want to create your own networking adventure? Track Bandwidth, Latency, Throughput, and RTT. Use observability tools, scale smart, and aim for these metrics to keep your users&nbsp;happy!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3844f1af9250\" width=\"1\" height=\"1\" alt=\"\">","contentLength":1788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digest #186: Inside the AWS Outage, Docker Compose in Production, F1 Hacks and 86,000 npm Packages Attacks","url":"https://www.devopsbulletin.com/p/digest-186-inside-the-aws-outage","date":1761989524,"author":"Mohamed Labouardy","guid":323960,"unread":true,"content":"<p>Welcome to this week’s edition of the DevOps Bulletin!</p><p>A recent 14-hour AWS us-east-1 outage took down 140 services after a DNS race condition in DynamoDB spiraled out of control. Palo Alto’s Unit42 uncovered a cloud-based gift card fraud campaign, and researchers exploited bugs in the FIA portal to access F1 driver data. Meanwhile, npm faced another supply-chain attack, with over 86,000 malicious packages downloaded.</p><p>Cloudflare detailed how it’s escaping the Linux networking stack, AWS quietly deprecated two dozen services, and Netflix revealed how Tudum supports 20M+ users using CQRS.</p><p>On the hands-on side: Docker Compose in production, ArgoCD for multi-cluster deployments, detecting bad images in S3 with Rekognition, and TDD with Terraform. Plus, why for some workloads, Postgres can beat Kafka.</p><p>Tools of the week: WhoDB (chat-based DB explorer), LME (CISA’s free SIEM), Grype (vulnerability scanner), Kanchi (Celery monitor), Bruin (data pipeline), and Nyno (multi-language workflow engine).</p><p>All this and more in this week’s DevOps Bulletin, don’t miss out!</p><div><p> Consider supporting it with a <a href=\"https://www.devopsbulletin.com/subscribe\">paid subscription</a>. You’ll keep the free Friday issues  get extras like bonus deep-dives, templates, and the full archive.</p></div><p>Ever feel like your cloud bill keeps growing, but you’re not sure  the money’s going? Start with an .</p><p><em>Listing all your resources — EC2 instances, S3 buckets, Lambdas, and more — often reveals idle or forgotten assets quietly adding to your bill. You can script it yourself with the AWS CLI or use tools like AWS Config or CloudQuery for a more automated setup.</em></p><p><em>If you want more hands-on tips like this, check out my latest book, “.</em></p><ul><li><p>A lightweight  - Postgres, MySQL, SQLite, MongoDB, Redis, MariaDB, Elastic Search, and Clickhouse with Chat interface.</p></li></ul><ul><li><p> is a no-cost, open-source platform that centralizes log collection, enhances threat detection, and enables real-time alerting.</p></li><li><p> is a vulnerability scanner for container images and filesystems.</p></li><li><p> is a real-time Celery task monitoring (and management) system with an enjoyable user interface.</p></li></ul><ul><li><p> is a data pipeline tool that brings together data ingestion, data transformation with SQL &amp; Python, and data quality into a single framework.</p></li><li><p> is an open-source multi-language workflow engine that lets you build, extend, and connect automation in the languages you already know.</p></li></ul><div><p> If you have feedback to share or are interested in <a href=\"https://www.devopsbulletin.com/p/sponsorships\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email. </p></div>","contentLength":2494,"flags":null,"enclosureUrl":"https://substackcdn.com/image/youtube/w_728,c_limit/2p_huDMN8XI","enclosureMime":"","commentsUrl":null},{"title":"How Data, Empathy and Visibility Are Redefining DevOps Maturity","url":"https://devops.com/how-data-empathy-and-visibility-are-redefining-devops-maturity/","date":1761923338,"author":"Alan Shimel","guid":323599,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Developer Discipline Matters More Than Ever in the AI Era","url":"https://devops.com/why-developer-discipline-matters-more-than-ever-in-the-ai-era/","date":1761922400,"author":"Mike Vizard","guid":323570,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Top 5 Open Source Tools to Automate Database Deployments in CI/CD Pipelines — Setup Guide","url":"https://blog.devops.dev/top-5-open-source-tools-to-automate-database-deployments-in-ci-cd-pipelines-setup-guide-4703395bde4c?source=rss----33f8b2d9a328---4","date":1761919861,"author":"Virinchi T","guid":323593,"unread":true,"content":"<h3>Top 5 Open Source Tools to Automate Database Deployments in CI/CD Pipelines — Setup&nbsp;Guide</h3><p>Database migrations are critical for modern application development, yet they remain one of the riskiest aspects of the deployment process. Whether you’re upgrading legacy systems, moving to cloud environments, or maintaining consistency across development, testing, and production, reliable database migration tools integrated into your CI/CD pipeline are essential.</p><p>In this comprehensive guide, we’ll explore the top 5 open-source database migration CI/CD tools, their key features, and step-by-step setup instructions to help you automate your database change management.</p><h3>Why Database Migration Tools&nbsp;Matter</h3><p>Before diving into the tools, let’s understand why they’re&nbsp;crucial:</p><ul><li>: Track database changes just like application code</li><li>: Ensure all environments have the same&nbsp;schema</li><li>: Eliminate manual errors and reduce deployment time</li><li>: Safely revert changes if something goes&nbsp;wrong</li><li>: Maintain compliance with detailed change&nbsp;logs</li><li>: Enable multiple developers to work on database changes simultaneously</li></ul><h3>1. Flyway — The Simple Yet Powerful&nbsp;Choice</h3><p>Flyway is a lightweight, open-source database migration tool that has gained massive popularity due to its simplicity and reliability. Created by Redgate, it treats database migrations like code using simple SQL or Java-based migration scripts.</p><ul><li><strong>Simple SQL-based migrations</strong>: Write migrations in plain SQL that are easy to understand</li><li><strong>Support for 50+ databases</strong>: Including PostgreSQL, MySQL, Oracle, SQL Server, MariaDB, and&nbsp;more</li><li><strong>Version control integration</strong>: Seamlessly integrates with Git and other&nbsp;VCS</li><li>: Support for views, stored procedures, and functions</li><li>: Easy integration into existing databases</li><li>: Prevents duplicate migrations</li><li>: CLI, Maven/Gradle plugins, and API&nbsp;support</li><li>: Minimal overhead and fast execution</li></ul><p>PostgreSQL, MySQL, MariaDB, Oracle, SQL Server, DB2, H2, SQLite, Redshift, CockroachDB, Firebird, and many&nbsp;more.</p><p>Flyway uses a migration naming convention and maintains a flyway_schema_history table to track which migrations have been&nbsp;applied:</p><p><strong>Migration Naming Convention:</strong></p><pre>V&lt;Version&gt;__&lt;Description&gt;.sql</pre><ul><li>V1__create_users_table.sql</li><li>V1.1__add_email_column.sql</li></ul><pre># Download and extractwget -qO- https://download.red-gate.com/maven/release/com/redgate/flyway/flyway-commandline/10.8.1/flyway-commandline-10.8.1-linux-x64.tar.gz | tar -xvz</pre><pre># Create symbolic linksudo ln -s `pwd`/flyway-10.8.1/flyway /usr/local/bin</pre><pre>docker pull flyway/flyway</pre><pre>&lt;dependency&gt;    &lt;groupId&gt;org.flywaydb&lt;/groupId&gt;<p>    &lt;artifactId&gt;flyway-core&lt;/artifactId&gt;</p>    &lt;version&gt;10.8.1&lt;/version&gt;</pre><h4>Step 2: Create Migration Files</h4><p>Create a directory structure:</p><pre>/project  /migrations    V2__add_tables.sql</pre><p>Example migration file (V1__create_users_table.sql):</p><pre>CREATE TABLE users (    id INT PRIMARY KEY AUTO_INCREMENT,<p>    username VARCHAR(255) NOT NULL,</p>    email VARCHAR(255) NOT NULL,<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><p>Create a flyway.conf file:</p><pre>flyway.url=jdbc:postgresql://localhost:5432/mydbflyway.user=dbuser<p>flyway.password=dbpassword</p>flyway.locations=filesystem:./migrations<p>flyway.baselineOnMigrate=true</p></pre><pre># Check migration statusflyway info</pre><pre># Run migrationsflyway migrate</pre><pre># Rollback (requires Flyway Teams)flyway undo</pre><p>Create&nbsp;.github/workflows/database-migration.yml:</p><pre>on:  push:</pre><pre>jobs:  test:      postgres:        env:<p>          POSTGRES_DB: ${{ secrets.DB_TEST }}</p>          POSTGRES_USER: ${{ secrets.USER_TEST }}<p>          POSTGRES_PASSWORD: ${{ secrets.PASSWORD_TEST }}</p>        options: &gt;-          --health-interval 10s          --health-retries 5      - uses: actions/checkout@v2<p>      - name: Run Flyway Migrations</p>        uses: joshuaavalon/flyway-action@v3.0.0          url: ${{ secrets.URL_TEST }}<p>          user: ${{ secrets.USER_TEST }}</p>          password: ${{ secrets.PASSWORD_TEST }}<p>          locations: filesystem:./migrations</p><p>      - run: echo 'Testing complete'</p></pre><pre>  deploy-to-prod:    needs: test      - uses: actions/checkout@v2<p>      - name: Deploy to Production</p>        uses: joshuaavalon/flyway-action@v3.0.0          url: ${{ secrets.URL_PROD }}<p>          user: ${{ secrets.USER_PROD }}</p>          password: ${{ secrets.PASSWORD_PROD }}<p>          locations: filesystem:./migrations</p><p>      - run: echo 'Deployment complete'</p></pre><pre>database_migration:  stage: migrate  script:<p>    - flyway -url=\"jdbc:postgresql://$DB_HOST:5432/$DB_NAME\" </p>            -user=$DB_USER             -locations=filesystem:./migrations   only:</pre><pre>pipeline {    agent any        FLYWAY_URL = credentials('flyway-url')<p>        FLYWAY_USER = credentials('flyway-user')</p>        FLYWAY_PASSWORD = credentials('flyway-password')        stage('Checkout') {                git 'https://github.com/yourorg/your-repo.git'        }<p>        stage('Flyway Migration') {</p>            steps {                    docker run --rm \\<p>                        -v $(pwd)/migrations:/flyway/sql \\</p>                        flyway/flyway \\                        -user=${FLYWAY_USER} \\<p>                        -password=${FLYWAY_PASSWORD} \\</p>                        migrate            }    }</pre><ol><li><strong>Never modify applied migrations</strong>: Always create new migration files</li><li>: Make migration purpose clear from&nbsp;filename</li><li>: One logical change per migration</li><li><strong>Test on non-production first</strong>: Always test migrations in&nbsp;staging</li><li><strong>Store credentials securely</strong>: Use secrets management tools</li><li><strong>Version control everything</strong>: Include migrations in your repository</li></ol><h3>2. Liquibase — The Enterprise-Grade Solution</h3><p>Liquibase is arguably the most well-known database migration tool, having been in the market since 2006. It’s a Java-based CLI tool that provides sophisticated tracking of database changes through XML, YAML, JSON, or SQL&nbsp;scripts.</p><ul><li><strong>Multiple change log formats</strong>: XML, YAML, JSON, and&nbsp;SQL</li><li><strong>Support for 60+ databases</strong>: Vendor-independent solution</li><li><strong>Advanced rollback capabilities</strong>: Targeted rollback for specific changesets</li><li>: Apply changes based on database&nbsp;state</li><li><strong>Diff and sync capabilities</strong>: Compare schemas across environments</li><li><strong>Automated drift detection</strong>: Identify untracked database&nbsp;changes</li><li>: Enforce compliance and governance</li><li><strong>Extensive CI/CD integrations</strong>: Jenkins, GitLab, GitHub Actions, Azure DevOps, and&nbsp;more</li></ul><p>Oracle, SQL Server, PostgreSQL, MySQL, MariaDB, MongoDB, Snowflake, DB2, Redshift, CockroachDB, and many&nbsp;others.</p><p>Liquibase organizes changes as  within . It maintains a DATABASECHANGELOG table to track applied&nbsp;changes.</p><ul><li>: Master file listing all changesets</li><li>: Unit of change with unique ID and&nbsp;author</li><li>: Create table, add column, insert data,&nbsp;etc.</li></ul><pre># Download from liquibase.orgwget https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gz<p>tar -xzf liquibase-4.24.0.tar.gz</p>sudo mv liquibase /usr/local/bin/</pre><pre>&lt;dependency&gt;    &lt;groupId&gt;org.liquibase&lt;/groupId&gt;<p>    &lt;artifactId&gt;liquibase-core&lt;/artifactId&gt;</p>    &lt;version&gt;4.24.0&lt;/version&gt;</pre><pre>&lt;plugin&gt;    &lt;groupId&gt;org.liquibase&lt;/groupId&gt;<p>    &lt;artifactId&gt;liquibase-maven-plugin&lt;/artifactId&gt;</p>    &lt;version&gt;4.24.0&lt;/version&gt;</pre><h4>Step 2: Create Changelog Files</h4><p>Create a master changelog (db.changelog-master.xml):</p><pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;databaseChangeLog<p>    xmlns=\"http://www.liquibase.org/xml/ns/dbchangelog\"</p>    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"<p>    xsi:schemaLocation=\"http://www.liquibase.org/xml/ns/dbchangelog</p>        http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.0.xsd\"&gt;</pre><pre>    &lt;include file=\"db/changelog/v1.0/create-users-table.xml\"/&gt;    &lt;include file=\"db/changelog/v1.1/add-email-column.xml\"/&gt;</pre><p>Create individual changeset (create-users-table.xml):</p><pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;databaseChangeLog<p>    xmlns=\"http://www.liquibase.org/xml/ns/dbchangelog\"</p>    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"<p>    xsi:schemaLocation=\"http://www.liquibase.org/xml/ns/dbchangelog</p>        http://www.liquibase.org/xml/ns/dbchangelog/dbchangelog-4.0.xsd\"&gt;</pre><pre>    &lt;changeSet id=\"1\" author=\"john.doe\"&gt;        &lt;createTable tableName=\"users\"&gt;<p>            &lt;column name=\"id\" type=\"int\" autoIncrement=\"true\"&gt;</p>                &lt;constraints primaryKey=\"true\"/&gt;            &lt;column name=\"username\" type=\"varchar(255)\"&gt;<p>                &lt;constraints nullable=\"false\"/&gt;</p>            &lt;/column&gt;<p>            &lt;column name=\"email\" type=\"varchar(255)\"&gt;</p>                &lt;constraints nullable=\"false\"/&gt;            &lt;column name=\"created_at\" type=\"timestamp\" defaultValueComputed=\"CURRENT_TIMESTAMP\"/&gt;    &lt;/changeSet&gt;</pre><pre>databaseChangeLog:  - changeSet:      author: john.doe        - createTable:            columns:                  name: id                  autoIncrement: true                    primaryKey: true                  name: username                  constraints:</pre><pre>--liquibase formatted sql</pre><pre>--changeset john.doe:1CREATE TABLE users (<p>    id INT PRIMARY KEY AUTO_INCREMENT,</p>    username VARCHAR(255) NOT NULL,<p>    email VARCHAR(255) NOT NULL,</p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><p>Create liquibase.properties:</p><pre>driver: org.postgresql.Driverurl: jdbc:postgresql://localhost:5432/mydbpassword: dbpassword<p>changeLogFile: db.changelog-master.xml</p>liquibase.hub.mode: off</pre><pre># Check statusliquibase status</pre><pre># Update databaseliquibase update</pre><pre># Rollback last changeliquibase rollback-count 1</pre><pre># Rollback to specific tagliquibase rollback tagName</pre><pre># Generate documentationliquibase dbDoc ./output</pre><pre># Diff two databasesliquibase diff</pre><h4>Jenkins Pipeline with&nbsp;Docker</h4><pre>pipeline {    agent {            image 'liquibase/liquibase:4.24.0'    }        DB_HOST = 'localhost'        DB_NAME = 'mydb'<p>        DB_USER = credentials('db-username')</p>        DB_PASS = credentials('db-password')<p>        CHANGELOG_FILE = 'db.changelog-master.xml'</p>        ROLLBACK_COUNT = 2        stage('Checkout') {                git 'https://github.com/yourorg/database-repo.git'        }            steps {                    liquibase status \\<p>                        --url=\"jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}\" \\</p>                        --changeLogFile=${CHANGELOG_FILE} \\                        --password=${DB_PASS}            }<p>        stage('Update Database') {</p>            steps {                    liquibase update \\<p>                        --url=\"jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}\" \\</p>                        --changeLogFile=${CHANGELOG_FILE} \\                        --password=${DB_PASS}            }                failure {                        liquibase rollback-count ${ROLLBACK_COUNT} \\<p>                            --url=\"jdbc:postgresql://${DB_HOST}:${DB_PORT}/${DB_NAME}\" \\</p>                            --changeLogFile=${CHANGELOG_FILE} \\                            --password=${DB_PASS}                }        }}</pre><p>Create&nbsp;.github/workflows/liquibase.yml:</p><pre>on:  push:  pull_request:</pre><pre>jobs:  database-update:      - name: Checkout code<p>        uses: actions/checkout@v2</p>        uses: actions/setup-java@v2          java-version: '11'<p>      - name: Install Liquibase</p>        run: |<a href=\"https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gz\">https://github.com/liquibase/liquibase/releases/download/v4.24.0/liquibase-4.24.0.tar.gz</a>          tar -xzf liquibase-4.24.0.tar.gz<p>          sudo mv liquibase /usr/local/bin/</p><p>      - name: Run Liquibase Update</p>        env:<p>          DB_URL: ${{ secrets.DB_URL }}</p>          DB_USER: ${{ secrets.DB_USER }}<p>          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}</p>        run: |<p>          liquibase --url=\"${DB_URL}\" \\</p>                    --username=\"${DB_USER}\" \\<p>                    --password=\"${DB_PASSWORD}\" \\</p>                    --changeLogFile=db.changelog-master.xml \\</pre><pre>stages:  - validate</pre><pre>variables:  LIQUIBASE_VERSION: \"4.24.0\"</pre><pre>validate:  stage: validate<p>  image: liquibase/liquibase:${LIQUIBASE_VERSION}</p>  script:<p>    - liquibase --changeLogFile=db.changelog-master.xml validate</p>  only:</pre><pre>deploy-staging:  stage: deploy<p>  image: liquibase/liquibase:${LIQUIBASE_VERSION}</p>  script:      liquibase --url=\"${STAGING_DB_URL}\" \\<p>                --username=\"${STAGING_DB_USER}\" \\</p>                --password=\"${STAGING_DB_PASSWORD}\" \\<p>                --changeLogFile=db.changelog-master.xml \\</p>                update    name: staging    - develop</pre><pre>deploy-production:  stage: deploy<p>  image: liquibase/liquibase:${LIQUIBASE_VERSION}</p>  script:      liquibase --url=\"${PROD_DB_URL}\" \\<p>                --username=\"${PROD_DB_USER}\" \\</p>                --password=\"${PROD_DB_PASSWORD}\" \\<p>                --changeLogFile=db.changelog-master.xml \\</p>                update    name: production  only:</pre><ol><li>: Combine author name with timestamp</li><li>: Apply different changes for different environments</li><li>: Ensure database is in expected&nbsp;state</li><li>: Enable easy rollback to known-good states</li><li>: Organize changesets logically</li><li><strong>Never modify deployed changesets</strong>: Liquibase tracks checksums</li></ol><h3>3. Atlas — The Modern Schema-as-Code Tool</h3><p>Atlas is a modern, open-source database CI/CD tool that promotes “database schema-as-code” philosophy. Built with Go, it draws inspiration from Terraform and provides a declarative approach to database schema management.</p><ul><li>: Define schemas in HCL, SQL, or via&nbsp;ORMs</li><li>: Plan, apply, and manage migrations</li><li><strong>Automated drift detection</strong>: Identify untracked schema&nbsp;changes</li><li>: Traditional migration files also supported</li><li>: Built-in quality&nbsp;checks</li><li><strong>Multiple database support</strong>: PostgreSQL, MySQL, MariaDB, SQLite, SQL Server, ClickHouse</li><li>: Atlas Cloud for team collaboration</li><li>: Enterprise-ready security</li></ul><p>PostgreSQL, MySQL, MariaDB, SQLite, SQL Server, ClickHouse, Redshift.</p><p>Atlas supports two primary workflows:</p><ol><li><strong>Declarative (Schema-as-Code)</strong>: Define desired state, Atlas calculates migration</li><li>: Traditional migration files with Atlas enhancements</li></ol><pre>curl -sSf https://atlasgo.sh | sh</pre><pre>docker pull arigaio/atlas</pre><pre>brew install ariga/tap/atlas</pre><h4>Step 2: Initialize Project</h4><pre># Create project directorymkdir my-database</pre><pre># Initialize Atlas projectatlas init</pre><h4>Step 3: Define Schema (Declarative Approach)</h4><pre>table \"users\" {  schema = schema.public    type = int  }    type = varchar(255)  }    type = varchar(255)  }    type = timestamp<p>    default = sql(\"CURRENT_TIMESTAMP\")</p>  }    columns = [column.id]    columns = [column.email]  }</pre><pre>table \"posts\" {  schema = schema.public    type = int  }    type = int  }    type = varchar(255)  }    type = text    columns = [column.id]    columns = [column.user_id]<p>    ref_columns = [table.users.column.id]</p>    on_delete = CASCADE}</pre><h4>Step 4: Generate and Apply Migrations</h4><p><strong>Inspect current database:</strong></p><pre>atlas schema inspect \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\</pre><p><strong>Plan migration (dry&nbsp;run):</strong></p><pre>atlas schema apply \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\<p>  --to \"file://schema.hcl\" \\</p>  --dry-run</pre><pre>atlas schema apply \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\</pre><h4>Step 5: Using Versioned Migrations</h4><p>Create migration directory:</p><p>Generate migration from&nbsp;schema:</p><pre>atlas migrate diff create_users \\  --dir \"file://migrations\" \\<p>  --to \"file://schema.hcl\" \\</p>  --dev-url \"docker://postgres/15/dev\"</pre><pre>atlas migrate apply \\  --url \"postgres://user:pass@localhost:5432/mydb?sslmode=disable\" \\<p>  --dir \"file://migrations\"</p></pre><p>Create&nbsp;.github/workflows/atlas.yml:</p><pre>on:  push:  pull_request:</pre><pre>jobs:  lint:    steps:<p>      - uses: actions/checkout@v3</p>        uses: ariga/setup-atlas@v0        run: |            --dir \"file://migrations\" \\<p>            --dev-url \"docker://postgres/15/dev\" \\</p>            --latest 1</pre><pre>  deploy:    needs: lint    if: github.ref == 'refs/heads/main'      - uses: actions/checkout@v3        uses: ariga/setup-atlas@v0        env:<p>          DATABASE_URL: ${{ secrets.DATABASE_URL }}</p>        run: |            --url \"${DATABASE_URL}\" \\<p>            --dir \"file://migrations\"</p></pre><pre>stages:  - validate</pre><pre>variables:  ATLAS_VERSION: \"latest\"</pre><pre>validate:  stage: validate<p>  image: arigaio/atlas:${ATLAS_VERSION}</p>  script:<p>    - atlas migrate lint --dir \"file://migrations\" --dev-url \"docker://postgres/15/dev\"</p>  only:</pre><pre>deploy-staging:  stage: deploy<p>  image: arigaio/atlas:${ATLAS_VERSION}</p>  script:      atlas migrate apply \\<p>        --url \"${STAGING_DB_URL}\" \\</p>        --dir \"file://migrations\"    name: staging    - develop</pre><pre>deploy-production:  stage: deploy<p>  image: arigaio/atlas:${ATLAS_VERSION}</p>  script:      atlas migrate apply \\        --dir \"file://migrations\"    name: production  only:</pre><ol><li>: Always specify dev-url for schema calculations</li><li>: Commit both HCL files and generated migrations</li><li>: Catch issues before production deployment</li><li>: Validate migrations in non-production first</li><li>: For team collaboration and enhanced&nbsp;features</li><li>: Regularly check for schema&nbsp;drift</li></ol><h3>4. Bytebase — The All-in-One Database DevOps&nbsp;Platform</h3><p>Bytebase is an all-in-one database DevOps and CI/CD solution that provides a GUI workspace for developers and DBAs to collaborate on database changes. It’s like GitHub/GitLab but specifically designed for database management.</p><ul><li>: User-friendly interface for managing database&nbsp;changes</li><li>: Database-as-code with Git integration</li><li>: Built-in review system for database&nbsp;changes</li><li>: Detect SQL anti-patterns automatically</li><li>: Protect sensitive data</li><li>: Complete audit trail of all database activities</li><li>: Fine-grained permission management</li><li>: Integrated SQL editor with data access&nbsp;control</li><li><strong>Multiple database support</strong>: MySQL, PostgreSQL, Oracle, SQL Server, MongoDB, Redis, and&nbsp;more</li><li>: Visual timeline of all database&nbsp;changes</li></ul><p>PostgreSQL, MySQL, Oracle, SQL Server, MongoDB, Redis, MariaDB, TiDB, ClickHouse, Snowflake, and&nbsp;more.</p><p>Bytebase uses a database-as-code approach&nbsp;with:</p><ul><li>: Organize databases by application or&nbsp;team</li><li>: Track database change&nbsp;requests</li><li>: Manage Dev, Staging, Production separately</li><li>: Sync database schemas from Git repositories</li><li>: Approval workflow for sensitive changes</li></ul><p><strong>Using Docker (Recommended):</strong></p><pre>docker run --init \\  --name bytebase \\  --add-host host.docker.internal:host-gateway \\  --volume ~/.bytebase/data:/var/opt/bytebase \\<p>  bytebase/bytebase:latest \\</p>  --data /var/opt/bytebase \\</pre><p>Create docker-compose.yml:</p><pre>services:  bytebase:<p>    image: bytebase/bytebase:latest</p>    container_name: bytebase    ports:    volumes:<p>      - ./bytebase-data:/var/opt/bytebase</p>    command:      - /var/opt/bytebase      - \"8080\"      - \"host.docker.internal:host-gateway\"</pre><p>Create bytebase-deployment.yaml:</p><pre>apiVersion: apps/v1kind: Deployment  name: bytebasespec:  selector:      app: bytebase    metadata:        app: bytebase      containers:        image: bytebase/bytebase:latest        - containerPort: 8080        - name: data<p>          mountPath: /var/opt/bytebase</p>        command:          - --data          - --port      volumes:        persistentVolumeClaim:---kind: Service  name: bytebasespec:  ports:    targetPort: 8080    app: bytebase</pre><h4>Step 3: Add Database Instances</h4><ol><li>Go to  → </li><li>Select database type (PostgreSQL, MySQL,&nbsp;etc.)</li><li>Enter connection details:</li></ol><ul></ul><ol><li>Go to  → </li><li>Enter project name and description</li></ol><ul><li>: Traditional approval&nbsp;workflow</li><li>: Sync from Git repository</li></ul><ol><li>Assign team members with&nbsp;roles</li></ol><h4>Step 5: Setup GitOps (Optional)</h4><ol><li>Connect Git provider (GitHub, GitLab, Bitbucket)</li><li>Configure repository and&nbsp;branch</li><li>Set up directory structure:</li></ol><ul><li>/bytebase /migrations /prod 001_create_users.sql 002_add_email_column.sql /staging 001_create_users.sql</li></ul><h4>Step 6: Create Database&nbsp;Change</h4><ol></ol><ol><li>Create SQL file in Git repository</li><li>Bytebase automatically detects and creates&nbsp;issue</li></ol><p>Bytebase provides native API for CI/CD integration.</p><pre># Get access tokencurl -X POST http://localhost:8080/v1/auth/login \\<p>  -H \"Content-Type: application/json\" \\</p>  -d '{<p>    \"email\": \"admin@example.com\",</p>    \"password\": \"yourpassword\"</pre><pre># Create issuecurl -X POST http://localhost:8080/v1/projects/project-1/issues \\<p>  -H \"Authorization: Bearer YOUR_TOKEN\" \\</p>  -H \"Content-Type: application/json\" \\    \"type\": \"DATABASE_CHANGE\",<p>    \"title\": \"Add users table\",</p>    \"description\": \"Create users table for authentication\",      \"statement\": \"CREATE TABLE users (id INT PRIMARY KEY, username VARCHAR(255));\"  }'</pre><p>Create&nbsp;.github/workflows/bytebase.yml:</p><pre>on:  push:      - 'migrations/**'</pre><pre>jobs:  deploy:      - uses: actions/checkout@v2<p>      - name: Authenticate with Bytebase</p>        id: auth          TOKEN=$(curl -X POST ${{ secrets.BYTEBASE_URL }}/v1/auth/login \\<p>            -H \"Content-Type: application/json\" \\</p>            -d '{<p>              \"email\": \"${{ secrets.BYTEBASE_EMAIL }}\",</p>              \"password\": \"${{ secrets.BYTEBASE_PASSWORD }}\"          echo \"::set-output name=token::${TOKEN}\"<p>      - name: Create Migration Issue</p>        run: |<p>          MIGRATION_SQL=$(cat migrations/latest.sql)</p>          curl -X POST ${{ secrets.BYTEBASE_URL }}/v1/projects/my-project/issues \\<p>            -H \"Authorization: Bearer ${{ steps.auth.outputs.token }}\" \\</p>            -H \"Content-Type: application/json\" \\              \\\"type\\\": \\\"DATABASE_CHANGE\\\",<p>              \\\"title\\\": \\\"Auto migration from CI\\\",</p>              \\\"payload\\\": {<p>                \\\"statement\\\": \\\"${MIGRATION_SQL}\\\"</p>              }</pre><ol><li>: Separate Dev, Staging, Production</li><li>: Require review for production changes</li><li>: Catch issues&nbsp;early</li><li>: Maintain single source of&nbsp;truth</li><li>: Track all database activities</li><li>: Protect sensitive data</li><li>: Integrate with Slack or&nbsp;email</li></ol><h3>5. Sqitch — The Git-like Database Change Management</h3><p>Sqitch is a purely open-source database change management system with no commercial offerings. Built with Perl, it has been on the market since 2012 and takes a unique, Git-inspired approach to managing database&nbsp;changes.</p><ul><li>: Works with Git, Mercurial, SVN, or no&nbsp;VCS</li><li>: Support for PostgreSQL, MySQL, Oracle, SQLite, Firebird, Vertica, Exasol, Snowflake</li><li>: Explicit dependency management</li><li><strong>No file naming conventions</strong>: Changes managed via sqitch&nbsp;plan</li><li>: Mark important milestones</li><li><strong>Powerful revert capability</strong>: Built-in rollback&nbsp;support</li><li>: Specify prerequisites for&nbsp;changes</li><li>: No GUI, fully command-line driven</li><li>: Deploy with all dependencies</li></ul><p>PostgreSQL, MySQL, Oracle, SQLite, Firebird, Vertica, Exasol, Snowflake, and&nbsp;more.</p><p>Unlike Liquibase and Flyway which use file naming conventions, Sqitch uses an explicit plan file (sqitch.plan) to specify the order and dependencies of&nbsp;changes:</p><ul><li>: Apply&nbsp;changes</li><li>: Undo&nbsp;changes</li><li>: Test that changes&nbsp;worked</li></ul><pre>docker pull sqitch/sqitch</pre><pre># Install dependenciessudo apt-get install libdbd-pg-perl libdbd-mysql-perl</pre><pre># Install from CPANcpan App::Sqitch</pre><h4>Step 2: Initialize Project</h4><pre># Create new Sqitch projectsqitch init myapp --uri https://github.com/myorg/myapp --engine pg</pre><pre># This creates:# - sqitch.conf (configuration)<p># - sqitch.plan (change plan)</p># - deploy/, revert/, verify/ directories</pre><p>Configuration file (sqitch.conf):</p><pre>[core]    engine = pg    plan_file = sqitch.plan</pre><pre>[engine \"pg\"]    target = db:pg://user:pass@localhost/mydb    client = psql</pre><pre># Add a new changesqitch add create_users -n \"Create users table\"</pre><pre># This creates three files:# - deploy/create_users.sql<p># - revert/create_users.sql</p># - verify/create_users.sql</pre><p>Edit deploy/create_users.sql:</p><pre>-- Deploy myapp:create_users to pg</pre><pre>CREATE TABLE users (    id SERIAL PRIMARY KEY,<p>    username VARCHAR(255) NOT NULL UNIQUE,</p>    email VARCHAR(255) NOT NULL UNIQUE,<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><p>Edit revert/create_users.sql:</p><pre>-- Revert myapp:create_users from pg</pre><p>Edit verify/create_users.sql:</p><pre>-- Verify myapp:create_users on pg</pre><pre>SELECT id, username, email, created_atFROM users</pre><h4>Step 4: Add Change with Dependencies</h4><pre># Add change that depends on create_userssqitch add add_posts --requires create_users -n \"Create posts table\"</pre><p>The sqitch.plan file will look&nbsp;like:</p><pre>%syntax-version=1.0.0%project=myapp<p>%uri=https://github.com/myorg/myapp</p></pre><pre>create_users 2025-01-15T10:30:00Z User Name &lt;user@example.com&gt; # Create users tableadd_posts [create_users] 2025-01-15T11:00:00Z User Name &lt;user@example.com&gt; # Create posts table</pre><pre># Check statussqitch status</pre><pre># Deploy all changessqitch deploy</pre><pre># Deploy to specific changesqitch deploy add_posts</pre><pre># Verify deploymentsqitch verify</pre><pre># Revert last changesqitch revert</pre><pre># Revert to specific changesqitch revert create_users</pre><pre># Revert all changessqitch revert --to @ROOT</pre><p>Create&nbsp;.github/workflows/sqitch.yml:</p><pre>name: Sqitch Database Migration</pre><pre>on:  push:    paths:      - 'revert/**'      - 'sqitch.plan'</pre><pre>jobs:  deploy:      postgres:        env:<p>          POSTGRES_PASSWORD: postgres</p>          POSTGRES_DB: testdb          --health-cmd pg_isready          --health-timeout 5s        ports:      - uses: actions/checkout@v2        run: |          sudo apt-get install -y sqitch libdbd-pg-perl postgresql-client<p>      - name: Run Sqitch Deploy</p>        env:<p>          SQITCH_TARGET: db:pg://postgres:postgres@localhost:5432/testdb</p>        run: |<p>      - name: Verify Deployment</p>        env:<p>          SQITCH_TARGET: db:pg://postgres:postgres@localhost:5432/testdb</p>        run: |</pre><pre>  deploy-production:    needs: deploy    if: github.ref == 'refs/heads/main'      - uses: actions/checkout@v2        run: |          sudo apt-get install -y sqitch libdbd-pg-perl<p>      - name: Deploy to Production</p>        env:<p>          SQITCH_TARGET: ${{ secrets.PROD_DATABASE_URL }}</p>        run: |          sqitch verify</pre><pre>variables:  POSTGRES_DB: testdb  POSTGRES_PASSWORD: postgres</pre><pre>test:  stage: test<p>  image: sqitch/sqitch:latest</p>  services:  variables:<p>    SQITCH_TARGET: \"db:pg://$POSTGRES_USER:$POSTGRES_PASSWORD@postgres/$POSTGRES_DB\"</p>  script:    - sqitch verify    - merge_requests</pre><pre>deploy-staging:  stage: deploy<p>  image: sqitch/sqitch:latest</p>  variables:<p>    SQITCH_TARGET: $STAGING_DATABASE_URL</p>  script:    - sqitch verify    name: staging    - develop</pre><pre>deploy-production:  stage: deploy<p>  image: sqitch/sqitch:latest</p>  variables:<p>    SQITCH_TARGET: $PROD_DATABASE_URL</p>  script:    - sqitch verify    name: production  only:</pre><pre>pipeline {    agent any        STAGING_DB = credentials('staging-db-url')<p>        PROD_DB = credentials('prod-db-url')</p>    }        stage('Checkout') {                git 'https://github.com/yourorg/database-repo.git'        }            steps {                    docker run --rm \\                        -w /repo \\                        sqitch deploy --target ${STAGING_DB}                        -v $(pwd):/repo \\                        sqitch/sqitch:latest \\<p>                        sqitch verify --target ${STAGING_DB}</p>                '''        }<p>        stage('Deploy to Production') {</p>            when {            }                input message: 'Deploy to production?', ok: 'Deploy'                    docker run --rm \\                        -w /repo \\                        sqitch deploy --target ${PROD_DB}                        -v $(pwd):/repo \\                        sqitch/sqitch:latest \\<p>                        sqitch verify --target ${PROD_DB}</p>                '''        }        failure {                docker run --rm \\                    -w /repo \\                    sqitch revert --target ${PROD_DB} -y        }}</pre><ol><li><strong>Always write verify scripts</strong>: Test that changes applied correctly</li><li><strong>Use explicit dependencies</strong>: Define prerequisites in sqitch.plan</li><li>: Mark important milestones</li><li>: Ensure rollback works before production</li><li>: Use sqitch bundle for offline deployments</li><li>: Write clear commit&nbsp;messages</li><li>: Define environments in sqitch.conf</li></ol><p>Feature Flyway Liquibase Atlas Bytebase Sqitch  Java Java Go Go/TypeScript Perl  Open Source + Commercial Open Source + Commercial Open Source + Cloud Open Source + Enterprise Pure Open Source  CLI, API, Maven/Gradle CLI, API, Maven/Gradle CLI GUI + CLI + API CLI  SQL, Java XML, YAML, JSON, SQL HCL, SQL SQL SQL  50+ 60+ 7+ 15+ 10+  Limited (Teams only) Yes Yes Yes Yes  Easy Moderate Moderate Easy Moderate  Via CI/CD Via CI/CD Native Native Native  No No (separate tool) No (Cloud has GUI) Yes No  Sequential Limited Yes Limited Explicit  Simple migrations, CI/CD Enterprise, Complex changes Modern teams, IaC fans Teams needing GUI, collaboration Git-like workflow, explicit&nbsp;control</p><ul><li>You want simplicity and ease of&nbsp;use</li><li>Your team prefers SQL-based migrations</li><li>You need fast integration into CI/CD pipelines</li><li>You’re working with standard relational databases</li><li>You don’t require complex rollback scenarios</li></ul><ul><li>You need enterprise-grade features</li><li>You require support for many database&nbsp;types</li><li>You need flexible change log&nbsp;formats</li><li>Compliance and audit trails are&nbsp;critical</li><li>You want precondition support</li></ul><ul><li>You embrace infrastructure-as-code principles</li><li>You like Terraform’s workflow</li><li>You want modern, declarative schema management</li><li>You’re building cloud-native applications</li></ul><ul><li>You want an all-in-one GUI&nbsp;solution</li><li>Your team includes non-technical stakeholders</li><li>You need built-in collaboration features</li><li>SQL review and approval workflows are important</li><li>You want integrated access control and data&nbsp;masking</li></ul><ul><li>You want Git-like change management</li><li>You need explicit dependency control</li><li>You prefer pure open-source tools</li><li>You’re comfortable with command-line tools</li><li>You want bundled offline deployments</li></ul><h3>General Best Practices for Database Migrations</h3><p>Regardless of which tool you choose, follow these universal best practices:</p><h3>1. Version Control Everything</h3><ul><li>Store all migration scripts in&nbsp;Git</li><li>Treat database changes like&nbsp;code</li><li>Use branches for feature development</li><li>Review changes via pull&nbsp;requests</li></ul><h3>2. Test in Non-Production First</h3><ul><li>Always test migrations in&nbsp;staging</li><li>Use production-like data&nbsp;volumes</li><li>Verify rollback procedures</li></ul><h3>3. Maintain Backward Compatibility</h3><ul><li>Make changes backward-compatible when&nbsp;possible</li><li>Use blue-green deployments for breaking&nbsp;changes</li><li>Keep old columns/tables temporarily</li><li>Coordinate with application deployments</li></ul><h3>4. Use Secure Credential Management</h3><ul><li>Never commit passwords to&nbsp;Git</li><li>Use secret management tools (HashiCorp Vault, AWS Secrets&nbsp;Manager)</li><li>Rotate credentials regularly</li><li>Use least-privilege database&nbsp;users</li></ul><h3>5. Implement Proper CI/CD Integration</h3><ul><li>Automate testing of migrations</li><li>Run migrations in deployment pipeline</li><li>Use separate databases for each environment</li><li>Implement approval gates for production</li></ul><ul><li>Log all migration activities</li><li>Set up alerts for&nbsp;failures</li><li>Track migration execution time</li><li>Maintain audit trails for compliance</li></ul><ul><li>Always write and test rollback&nbsp;scripts</li><li>Practice rollback procedures</li><li>Keep rollback windows&nbsp;minimal</li></ul><ul><li>Document migration purpose</li><li>Include context in commit&nbsp;messages</li><li>Maintain runbooks for complex migrations</li><li>Share knowledge with&nbsp;team</li></ul><p>Database migrations are a critical yet challenging aspect of modern software development. The right tool can make the difference between smooth, reliable deployments and risky, error-prone manual processes.</p><p>Each of the five tools we’ve covered has its strengths:</p><ul><li> offers simplicity and reliability for teams wanting straightforward SQL migrations</li><li> provides enterprise-grade features for complex, compliance-driven environments</li><li> brings modern infrastructure-as-code principles to database management</li><li> delivers an all-in-one GUI platform for team collaboration</li><li> offers Git-like control for teams wanting explicit change management</li></ul><p>The key is choosing the tool that best fits your team’s needs, technical stack, and workflow preferences. Start with one tool, integrate it into your CI/CD pipeline, and iterate based on your experience.</p><p>Remember: the best database migration tool is the one your team will actually use consistently. Pick one, set it up properly, and make database changes as reliable and traceable as your application code deployments.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4703395bde4c\" width=\"1\" height=\"1\" alt=\"\">","contentLength":30476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Doesn’t Have to Hurt","url":"https://www.docker.com/blog/security-shadow-it-collaboration/","date":1761915600,"author":"Simeon Ratliff","guid":323517,"unread":true,"content":"<p>Do you ever wish security would stop blocking the tools you need to do your job? Surprise: your security team wants the same.</p><p>There you are, just trying to get your work done, when…</p><ul><li>You need an AI to translate documentation, but all the AI services are blocked by a security web monitoring tool.</li><li>You finish coding and QA for a new software version just under the wire, but the release is late because security has not reviewed the open source software and libraries included.</li><li>Your new database works perfectly in dev/test, but it does not work in production because of a port configuration, and you do not have permissions. Changes to production permissions all require security approval</li></ul><p>Shadow IT is a spy-movie name for a phenomenon that is either a frustrating necessity or a game of whack-a-mole, depending on your responsibilities.</p><p>If you’re an engineer creating the next best product, shadow IT is a necessity.&nbsp;</p><p>Company-supplied information technology does not change fast enough to keep up with the market, let alone allow you to innovate. Despite that, your security team will come down hard on anyone who tries to go outside the allowed vendors and products. Data storage has to be squared away in encrypted, protected spaces, and you have to jump like a show pony to get access. And you have no flexibility in the tools you’re allowed to use, even if you could produce faster and better with other options.</p><p>So you stop playing by the rules, and you find tools and tech that work.</p><p>That is, until someone protests the cloud hosting bill, finds the wifi access point, or notices the unofficial software repository. Security takes away your tools or cuts off access. And then you are upset, your team feels attacked, and security is up in arms.</p><p>If you are on a security team, shadow IT is a game of whack-a-mole. Company-supplied information technology changes without review. You know they’re trying to enable innovation, but they’re negating all the IT compliance certifications that allow you to sell your services and products. You have to investigate, prove, and argue about policies and regulations just to stop people from storing client secrets in their personal cloud storage.</p><p>Whether you are a new hire in the Security Operations Center or the unlucky CISO who reports to the CTO, this is a familiar refrain.</p><p>Yet no one wants this. Not you, not your boss, and not security.</p><h2>If It Cannot Be Fixed, Break It</h2><p>It’s time we change the ground rules of security to focus on compromise rather than stringency.&nbsp;</p><p>Most security teams want to change their operations to concentrate on the capabilities they are trained for: threat intelligence, risk management, forensic analysis, and security engineering. I have never met a security professional who wants to spend their time arguing over a port configuration. It’s tiresome, and that friction inspires lasting antagonism on both sides.</p><p>Imagine working in a place where you can use innovative new tools, release products without a security delay, and change configurations so that your deployment works smoothly.</p><p>But there is a subtle change that must happen to enable this security-IT paradise: non-security teams would have to understand and implement all the requirements security departments would check. And everyone who is part of the change would need to understand the implications of their actions and take sole responsibility for the security outcomes.</p><p>My non-IT colleagues are shocked when I explain the scope of work for a security department in preparation for any release or product launch:</p><ul><li>Weaknesses and exploits for custom and third-party code</li><li>Scope and adequacy of vendor security</li><li>Data encryption, transmission, and storage, especially across borders</li><li>Compliance with regulation and data protection laws</li></ul><p>In many industries, we legally cannot remove security practices from IT processes. But we can change who takes responsibility for which parts of the work&nbsp;</p><p>Security requirements are not a secret. A developer with integrated code scanners can avoid OWASP Top 10 flaws and vulnerable libraries and remove hard-coded accounts. Infrastructure admins with access to network security tools can run tidy networks, servers, and containers with precise configurations.</p><p>The result? The security team can let go of their rigid deployment rules.</p><p>If developers use code security tools and incorporate good practices, security team approval should take hours rather than days or weeks. Security can also approve the standard container configuration rather than each separate container in an architecture. They can define the requirements, offer you tools to review your work, and help you integrate good practices into your workflow.</p><p>“Trust but verify” would become a daily pattern instead of lip service to good interdepartmental relationships. Security will continue to monitor the environment and the application after release. They will keep an eye on vendor assertions and audits, watching threat intelligence streams for notifications that demonstrate risk. Security teams will have time to do the job they signed up for, which is much more interesting than policing other departments.</p><p>This change would also require that the security team be allowed to let go. When trust is broken—if vendors are not properly assessed, or software is introduced but not reported—the fault should not lie with the security team. If insecure coding causes a compromise, the development team must be accountable, and if an inadequately configured network causes a data leak, the network and hosting team must be called on the carpet. If the requirements are in place but not met, the responsible parties must be those that agreed to them but neglected to enact them.</p><h2>Freedom to Choose Comes with a Catch</h2><p>This new freedom makes shadow IT unnecessary. Teams do not need to hide the choices they make. However, the freedom to choose comes with a catch: full responsibility for your choices.</p><p>Consider the company charge card: Finance teams create the policy for how to use company charge cards and provide the tools for reimbursement. They do not scrutinize every charge in real time, but they review usage and payments.</p><p>If the tool is abused and the agreed-upon care is ignored, the card user is held responsible. Any lack of knowledge does not exempt you from the consequences. For minor infractions, you may get a written notice. For severe infractions, you can expect to be terminated for cause.</p><p>The finance requirements, your agreement, regular review, and enacted consequences minimize fraud internally. More importantly, though, this combination protects the company against accusations of negligence.</p><p>Security responsibility could work the same. Security teams can set requirements that IT workers agree to individually. IT teams are then free to deploy and make changes as appropriate for their work. IT secures assets before they are put into production, and security continues with the best practice of reviewing assets continuously after the fact. Delays in getting the tools you need are reduced, and you control the deployment of your work with much more assurance. The incentive for shadow IT is much lower, and the personal risk of choosing it is higher.</p><p>That last bit is the catch, though—when you take control, you take responsibility for the result. Instead of committing to a patch, you back out insecure code and redeploy when it is corrected. When your department contracts with a squirrelly vendor, your manager’s budget takes the hit for breaking the contract. When the network is compromised, the CIO, not the CISO, gets fired.</p><p>Right now, the security team carries this responsibility and shoulders these risks. But the result is an enterprise held hostage by risk aversion, with no understanding or control over the outcomes.</p><p>So far, I’ve mostly addressed IT, but I also want to bring this argument back home: Security professionals, let’s stop taking control of everyone else’s work. When we make hard requirements that do not meet tech realities, our IT teams get better at hiding their tracks. You will make more progress if you invest in mutual success and reward people who step up to exceed your expectations.</p><h2>When Security and IT Make Peace, Shadow IT Becomes Unnecessary</h2><p>I once worked with a development team that wanted to store proprietary code in a hosted code repository. The repository was great for their needs: versioning automation, fine-grained access management, easy branching, access from anywhere, and centralized storage. Instead of waiting six months for the new vendor security investigation process, the developer team gathered the vendor’s audit certificates, data handling guarantees, and standard contract language about security and data mining. The devs proactively researched the third-party security scanning policies and asked for their incident response and notification policies.</p><p>Our security team would have struggled to locate this repository if the developers had simply chosen to use it. Instead, they circumvented our process in the best way—by providing every necessary answer to our security questions.</p><p>The reward was an instant yes from me, the security leader, without having to wait for my overworked team to schedule yet another vendor review.</p><p>My reward? No shadow IT plus a very happy IT team.</p><p>Security should go beyond allowing compromises like this: we should seek them out. Convince the CISO to work toward giving your IT teams both control and responsibility, find a compromise with the teams that will take security seriously—and save your energy for wrangling teams that don’t.</p><p>For admins and developers: Provide the ISO audit documents for that vendor you want to use. Be the first dev team to learn the org’s code scanning tool. Read the latest risk assessments from your cloud environment and don’t repeat vulnerable configurations. These small changes make your work faster, simpler, and less expensive than finding your own solutions.</p>","contentLength":9967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mr. Bones: A Pirate-Voiced Halloween Chatbot Powered by Docker Model Runner","url":"https://www.docker.com/blog/talking-skeleton-docker-llm/","date":1761913041,"author":"Mike Coleman","guid":323516,"unread":true,"content":"<p>My name is Mike Coleman, a staff solution architect at Docker. This year I decided to turn a Home Depot animatronic skeleton into an AI-powered,&nbsp; live, interactive Halloween chatbot. The result: kids walk up to Mr. Bones, a spooky skeleton in my yard, ask it questions, and it answers back — in full pirate voice — with actual conversational responses, thanks to a local LLM powered by <a href=\"https://docs.docker.com/ai/model-runner/\" rel=\"nofollow noopener\" target=\"_blank\">Docker Model Runner</a>.</p><p><a href=\"https://docs.docker.com/ai/model-runner/?utm_source=chatgpt.com\" rel=\"nofollow noopener\" target=\"_blank\"></a> is a tool from Docker that makes it dead simple to run open-source LLMs locally using standard Docker workflows. I pulled the model like I’d pull any image, and it exposed an OpenAI-compatible API I could call from my app. Under the hood, it handled model loading, inference, and optimization.</p><p>For this project, Docker Model Runner offered a few key benefits:</p><ul><li> for LLM inference — unlike OpenAI or Anthropic</li><li> because the model runs on local hardware</li><li> over model selection, prompts, and scaffolding</li><li><strong>API-compatible with OpenAI</strong> — switching providers is as simple as changing an environment variable and restarting the service</li></ul><p>That last point matters: if I ever needed to switch to OpenAI or Anthropic for a particular use case, the change would take seconds.</p><p>Figure 1: System overview of Mr. Bones answering questions in pirate language</p><ol><li> records audio</li><li> transcribes speech to text</li><li><strong>API call to a Windows gaming PC</strong> with an RTX 5070 GPU</li><li> runs a local LLaMA 3.1 8B (Q4 quant) model</li><li><strong>LLM returns a text response</strong></li><li> converts the text to speech (pirate voice)</li><li><strong>Pi sends audio to skeleton via Bluetooth</strong>, which moves the jaw in sync</li></ol><p>Figure 2: The controller box that holds the Raspberry Pi that drives the pirate</p><p>That Windows machine isn’t a dedicated inference server — it’s my gaming rig. Just a regular setup running a quantized model locally.</p><p>The biggest challenge with this project was balancing response quality (in character and age appropriate) and response time. With that in mind, there were four key areas that needed a little extra emphasis: model selection, how to do text to speech (TTS) processing efficiently, fault tolerance, and setting up guardrails.&nbsp;</p><h2><strong>Consideration 1: Model Choice and Local LLM Performance</strong></h2><p>I tested several open models and found <strong>LLaMA 3.1 8B (Q4 quantized)</strong> to be the best mix of performance, fluency, and personality. On my RTX 5070, it handled real-time inference fast enough for the interaction to feel responsive.</p><p>At one point I was struggling to keep Mr. Bones in character, so I&nbsp; tried OpenAI’s ChatGPT API, but response times averaged .</p><p>By revising the prompt and Docker Model Runner serving the right model, I got that down to . That’s a huge difference when a kid is standing there waiting for the skeleton to talk.</p><p>In the end, GPT-4 was only  at staying in character and avoiding inappropriate replies. With a solid prompt scaffold and some guardrails, the local model held up just fine.</p><h2><strong>Consideration 2: TTS Pipeline: Kokoro to ElevenLabs Flash</strong></h2><p>I first tried using , a local TTS engine. It worked, but the voices were too generic. I wanted something more pirate-y, without adding custom audio effects.</p><p>So I moved to , starting with their multilingual model. The voice quality was excellent, but latency was painful — especially when combined with LLM processing. Full responses could take up to , which is way too long.</p><p>Eventually I found , a much faster model. That helped a lot. I also changed the logic so that instead of waiting for the entire LLM response, I  the output and sent it to ElevenLabs in parts. Not true streaming, but it allowed the Pi to start playing the audio as each chunk came back.</p><p>This turned the skeleton from slow and laggy into something that felt snappy and responsive.</p><h2><strong>Consideration 3: Weak Points and Fallback Ideas</strong></h2><p>While the LLM runs locally, the system still depends on the internet for ElevenLabs. If the network goes down, the skeleton stops talking.</p><p>One fallback idea I’m exploring: creating a set of common Q&amp;A pairs (e.g., “What’s your name?”, “Are you a real skeleton?”), embedding them in a local , and having the Pi serve those in case the TTS call fails.</p><p>But the deeper truth is: this is a . If the Pi loses its connection to the Windows machine, the whole thing is toast. There’s no skeleton-on-a-chip mode yet.</p><h2><strong>Consideration 4: Guardrails and Prompt Engineering</strong></h2><p>Because kids will say anything, I put some safeguards in place via my system prompt.&nbsp;</p><div><pre>You are \"Mr. Bones,\" a friendly pirate who loves chatting with kids in a playful pirate voice.\n\nIMPORTANT RULES:\n- Never break character or speak as anyone but Mr. Bones\n- Never mention or repeat alcohol (rum, grog, drink), drugs, weapons (sword, cannon, gunpowder), violence (stab, destroy), or real-world safety/danger\n- If asked about forbidden topics, do not restate the topic; give a kind, playful redirection without naming it\n- Never discuss inappropriate content or give medical/legal advice\n- Always be kind, curious, and age-appropriate\n\nBEHAVIOR:\n- Speak in a warm, playful pirate voice using words like \"matey,\" \"arr,\" \"aye,\" \"shiver me timbers\"\n- Be imaginative and whimsical - talk about treasure, ships, islands, sea creatures, maps\n- Keep responses conversational and engaging for voice interaction\n- If interrupted or confused, ask for clarification in character\n- If asked about technology, identity, or training, stay fully in character; respond with whimsical pirate metaphors about maps/compasses instead of tech explanations\n\nFORMAT:\n- Target 30 words; must be 10-50 words. If you exceed 50 words, stop early\n- Use normal punctuation only (no emojis or asterisks)\n- Do not use contractions. Always write \"Mister\" (not \"Mr.\"), \"Do Not\" (not \"Don't\"), \"I Am\" (not \"I'm\")\n- End responses naturally to encourage continued conversation\n\n</pre></div><p>The prompt is designed to deal with a few different issues. First and foremost, keeping things appropriate for the intended audience. This includes not discussing sensitive topics, but also staying in character at all times.&nbsp; Next I added some instructions to deal with pesky parents trying to trick Mr. Bones into revealing his true identity. Finally, there is some guidance on response format to help keep things conversational – for instance, it turns out that some STT engines can have problems with things like contractions.&nbsp;</p><p>Instead of just refusing to respond, the prompt redirects sensitive or inappropriate inputs in-character. For example, if a kid says “I wanna drink rum with you,” the skeleton might respond, “Arr, matey, seems we have steered a bit off course. How about we sail to smoother waters?”</p><p>This approach keeps the interaction playful while subtly correcting the topic. So far, it’s been enough to keep Mr. Bones spooky-but-family-friendly.</p><p>Figure 3: Mr. Bones is powered by AI and talks to kids in pirate-speak with built-in safety guardrails.</p><p>This project started as a Halloween goof, but it’s turned into a surprisingly functional proof-of-concept for real-time, local voice assistants.</p><p>Using  for LLMs gave me speed, cost control, and flexibility. ElevenLabs Flash handled voice. A Pi 5 managed the input and playback. And a Home Depot skeleton brought it all to life.</p><p>Could you build a more robust version with better failover and smarter motion control? Absolutely. But even as he stands today, Mr. Bones has already made a bunch of kids smile — and probably a few grown-up engineers think, “Wait, I could build one of those.”&nbsp;</p><p>Figure 4: Aye aye! Ye can build a Mr. Bones too and bring smiles to all the young mateys in the neighborhood!</p>","contentLength":7471,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cursor 2.0 Brings Faster AI Coding and Multi-Agent Workflows","url":"https://devops.com/cursor-2-0-brings-faster-ai-coding-and-multi-agent-workflows/","date":1761901689,"author":"Tom Smith","guid":323431,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"8 Pipeline Caching Tricks That Cut CI Time in Half","url":"https://blog.devops.dev/8-pipeline-caching-tricks-that-cut-ci-time-in-half-3af50fae7db9?source=rss----33f8b2d9a328---4","date":1761833522,"author":"Obafemi","guid":322183,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aembit Introduces Identity and Access Management for Agentic AI","url":"https://devops.com/aembit-introduces-identity-and-access-management-for-agentic-ai/","date":1761825724,"author":"cybernewswire","guid":322090,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Integrate Quantum-Safe Security into Your DevOps Workflow","url":"https://devops.com/how-to-integrate-quantum-safe-security-into-your-devops-workflow/","date":1761825054,"author":"Carl Torrence","guid":322089,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"theCUBE Research economic validation of Docker’s development platform","url":"https://www.docker.com/blog/thecube-research-economic-validation-of-docker-development-platform/","date":1761824788,"author":"John Ayub","guid":322088,"unread":true,"content":"<h3><strong>Docker’s ROI and impact on agentic AI, security, and developer productivity</strong>.</h3><p> surveyed ~400 IT and AppDev professionals at leading global enterprises to investigate <strong>Docker’s ROI and impact on agentic AI development, software supply chain security, and developer productivity</strong>.&nbsp; The industry context is that enterprise developers face mounting pressure to rapidly ship features, build agentic AI applications, and maintain security, all while navigating a fragmented array of development tools and open source code that require engineering cycles and introduce security risks. Docker transformed software development through containers and DevSecOps workflows, and is now doing the same for agentic AI development and software supply chain security. <strong>&nbsp;theCUBE Research quantified Docker’s impact:</strong> teams build agentic AI apps faster, achieve near-zero CVEs, remediate vulnerabilities before exploits, ship modern cloud-native applications, save developer hours, and generate financial returns.</p><p> for key highlights and analysis.  theCUBE Research <a href=\"https://www.docker.com/resources/thecube-research-docker-economic-validation-analyst-study/\"></a>and <a href=\"https://www.docker.com/resources/thecube-research-docker-economic-validation-ebook/\"></a>to take a deep dive.</p><h3>Agentic AI development streamlined using familiar technologies</h3><p>Developers can build, run, and share agents and compose agentic systems using familiar Docker container workflows. To do this, developers can build agents safely using Docker MCP Gateway, Catalog, and Toolkit; run agents securely with Docker Sandboxes; and run models with Docker Model Runner. These capabilities align with theCUBE Research findings that <strong>87% of organizations reduced AI setup time by over 25%</strong> and <strong>80% report accelerating AI time-to-market by at least 26%</strong>.&nbsp; Using Docker’s modern and secure software delivery practices, development teams can implement AI feature experiments faster and in days test agentic AI capabilities that previously took months. <strong>Nearly 78% of developers experienced significant improvement in the standardization and streamlining of AI development workflows</strong>, enabling better testing and validation of AI models. Docker helps enterprises generate business advantages through deploying new customer experiences that leverage agentic AI applications. This is phenomenal, given the nascent stage of agentic AI development in enterprises.</p><h3>Software supply chain security and innovation can move in lockstep</h3><p>Security engineering and vulnerability remediation can slow development to a crawl. Furthermore, checkpoints or controls may be applied too late in the software development cycle, or after dangerous exploits, creating compounded friction between security teams seeking to mitigate vulnerabilities and developers seeking to rapidly ship features. Docker embeds security directly into development workflows through vulnerability analysis and continuously-patched certified container images. theCUBE Research analysis supports these Docker security capabilities: <strong>79% of organizations find Docker extremely or very effective at maintaining security &amp; compliance</strong>, while <strong>95% of respondents reported that Docker improved their ability to identify and remediate vulnerabilities.</strong> By making it very simple for developers to use secure images as a default, Docker enables engineering teams to plan, build, and deploy securely without sacrificing feature velocity or creating deployment bottlenecks. Security and innovation can move in lockstep because Docker concurrently secures software supply chains and eliminates vulnerabilities.</p><h3>Developer productivity becomes a competitive advantage</h3><p>Consistent container environments eliminate friction, accelerate software delivery cycles, and enable teams to focus on building features rather than overcoming infrastructure challenges. When developers spend less time on environment setup and troubleshooting, they ship more features. Application features that previously took months now reach customers in weeks. The research demonstrates Docker’s ability to increase developer productivity. <strong>72% of organizations reported significant productivity gains in development workflows</strong>, while <strong>75% have transformed or adopted DevOps practices when using Docker.</strong> Furthermore, when it comes to AI and supply chain security, the findings mentioned above further support how Docker unlocks developer productivity.</p><h3>Financial returns exceed expectations</h3><p>CFOs demand quantifiable returns for technology investments, and Docker delivers them.  reported substantial , <strong>with 43% reporting $50,000-$250,000 in cost reductions</strong> from infrastructure efficiency, reduced rework, and faster time-to-market. The ROI story is equally compelling: <strong>69% of organizations report ROI exceeding 101%, with many achieving ROI above 500%</strong>. When factoring in faster feature delivery, improved developer satisfaction, and reduced security incidents, the business case for Docker becomes even more tangible. <strong>The direct costs of a security breach can surpass $500 million</strong>, so mitigating even a fraction of this cost provides a compelling financial justification for enterprises to deploy Docker to every developer.</p><h3>Modernization and cloud native apps remain top of mind</h3><p>For enterprises who maintain extensive legacy systems, Docker serves as a proven catalyst for cloud-native transformation at scale. Results show that nearly <strong>nine in ten (88%) of organizations report Docker has enabled modernization of at least 10%</strong> of their applications, with half achieving modernization across 31-60% of workloads and another 20% modernizing over 60%. Docker accelerates the shift from monolithic architectures to modern containerized cloud-native environments while also delivering substantial business value.&nbsp; For example, <strong>37% of organizations report 26% to &gt;50% faster product time-to-market</strong>, and <strong>72% report annual cost savings ranging from $50,000 to over $1 million</strong>.</p><h3><strong>Learn more about Docker’s impact on enterprise software development</strong></h3><p>Docker has evolved from a containerization suite into a development platform for testing, building, securing, and deploying modern software, including agentic AI applications. Docker enables enterprises to apply proven containerization and DevSecOps practices to agentic AI development and software supply chain security.</p><p><strong>Download (below) the full report and the ebook from theCUBE Research analysis </strong>to learn Docker’s impact on developer productivity, software supply chain security, agentic AI application development, CI/CD and DevSecOps, modernization, cost savings, and ROI.&nbsp; Learn how enterprises leverage Docker to transform application development and win in markets where speed and innovation determine success.</p><p><strong>theCUBE Research economic validation of Docker’s development platform</strong></p>","contentLength":6581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anatomy of an Outage: Our AWS AutoScaling Group “Helping” Hand Pushed us off the Cliff","url":"https://devops.com/anatomy-of-an-outage-our-aws-autoscaling-group-helping-hand-pushed-us-off-the-cliff/","date":1761820328,"author":"Muhammad Yawar Malik","guid":322048,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Cloud Key Management Options","url":"https://devops.com/exploring-cloud-key-management-options/","date":1761818865,"author":"Alexander Williams","guid":322047,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GKE 10 Year Anniversary, with Gari Singh","url":"https://e780d51f-f115-44a6-8252-aed9216bb521.libsyn.com/gke-10-year-anniversary-with-gari-singh","date":1761780840,"author":"","guid":320999,"unread":true,"content":"<p dir=\"ltr\">GKE turned 10 in 2025! In this episode, we talk with GKE PM <a href=\"https://www.linkedin.com/in/garisingh/\">Gari Singh</a> about GKE's journey from early container orchestration to AI-driven ops. Discover Autopilot, IPPR, and a bold vision for the future of Kubernetes.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p>","contentLength":282,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/kpod262.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Deaf and Hard of Hearing WG Meeting - October 2025","url":"https://www.youtube.com/watch?v=Ngjrv07mdDI","date":1761768785,"author":"CNCF [Cloud Native Computing Foundation]","guid":320946,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":319,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Ngjrv07mdDI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Survey Surfaces Rising Tide of Vulnerabilities in Code Generated by AI","url":"https://devops.com/survey-surfaces-rising-tide-of-vulnerabilities-in-code-generated-by-ai/","date":1761762418,"author":"Mike Vizard","guid":320876,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AppOmni Open Sources Heisenberg Tool to Scan Pull Requests for Dependencies","url":"https://devops.com/appomni-open-sources-heisenberg-tool-to-scan-pull-requests-for-dependencies/","date":1761754105,"author":"Mike Vizard","guid":320812,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spring Boot Java App on Azure VM","url":"https://blog.devops.dev/spring-boot-java-app-on-azure-vm-e36f5ca7ce86?source=rss----33f8b2d9a328---4","date":1761746761,"author":"Amita Pal Singh","guid":320733,"unread":true,"content":"<p>The classic developer dilemma: Your code is a masterpiece, but it’s trapped on .</p><p>To make it accessible to the world, you need a robust deployment platform. For critical applications, like core banking systems or high-throughput financial services, deployment isn’t just a technical task; it’s a cornerstone of digital transformation. Large FinTech companies are increasingly turning to cloud infrastructure, even for their complex, legacy monolith applications, to accelerate deployment, ensure scalability, enhance disaster recovery, and deliver insights faster. Azure Virtual Machines (VMs) provide a powerful, flexible solution for hosting these applications with enterprise-grade reliability.</p><p>This guide serves as your roadmap to deploying a Spring Boot Java application on an Azure VM. We’ll walk through the essential steps to take your compiled application from local development to a production-ready cloud environment</p><h3>Prerequisites and Environment Setup</h3><h4>Sign in to the Azure&nbsp;portal</h4><p>Enter virtual machines in the search. Under Services, select Virtual machines.</p><ul><li>In the Basics tab, under the Project details, select an appropriate subscription. Then choose an existing resource group or create a new one to&nbsp;use.</li><li>Provide a VM name, select the region nearest to you, and fill in mandatory fields.</li><li>Select OS Image for&nbsp;Linux</li><li>Under Authentication type, select password, provide user name and password, and save the credentials somewhere. We would need the credentials to login to the&nbsp;VM.</li><li>Under Inbound port rules &gt; Public inbound ports, choose Allow selected ports and then select SSH (22) and HTTP (80) from the drop-down.</li></ul><ul><li>Select the Review + create button at the bottom of the&nbsp;page.</li><li>On the Create a virtual machine page, you can see the details about the VM you are about to create. Select&nbsp;Create.</li><li>When the deployment is finished, select Go to resource.</li><li>On the page for your new VM, select the public IP address and copy&nbsp;it.</li></ul><p>Select BashExecute&nbsp;command</p><pre>ssh &lt;username&gt;@&lt;vm public ip&gt;</pre><p>Enter the password to&nbsp;connect.</p><p>If you are deploying to an Azure Virtual Machine (VM) running a Linux distribution like Ubuntu, you may need to ensure the correct Java version is installed. The following commands are specific to Debian/Ubuntu-based systems to install .</p><p><strong>Step 1: Search for Available OpenJDK&nbsp;Packages</strong></p><p>This command verifies the existence of OpenJDK 17 packages in the repository cache.</p><pre>apt-cache search openjdk | grep openjdk-17</pre><p><strong>Step 2: Install OpenJDK 17&nbsp;JRE</strong></p><pre>sudo apt install openjdk-17-jre</pre><p>Confirm that the correct version is set as the default Java&nbsp;runtime.</p><p> The output should clearly display the installed Java 17 version&nbsp;details.</p><p>Tomcat server is used for deploying Java-based web applications.</p><p>Instructions for setup and configuration of Tomcat server are available in&nbsp;<a href=\"https://amitasinghtech.medium.com/tomcat-server-setup-dde5f42dd4c6\">article</a></p><p>In an Azure VM environment, you must also ensure the VM’s <strong>Network Security Group (NSG)</strong> has an inbound rule to allow traffic on the Tomcat port (default ) from your desired source IP or&nbsp;range.</p><p>Details on how to create a WAR file for Spring Boot application are available in the&nbsp;<a href=\"https://medium.com/@amitasinghtech/deploying-a-spring-boot-application-as-a-war-file-on-apache-tomcat-9a48cd04f3ae\">article</a></p><p>Navigate to the Tomcat Manager GUI -&gt; “WAR file to deploy” section, and&nbsp;deploy</p><p><strong>Check for Issues and Exceptions</strong></p><p>Deployment is rarely smooth on the first attempt. To troubleshoot, you must monitor the logs at /opt/tomcat/logs directory.</p><p><strong>Checking Logs using the Command&nbsp;Line:</strong></p><p>Log files are typically huge in size. To view the end of a log file in real-time or view a specific number of lines, use following Bash commands:</p><p>View the last 10 lines of the host-manager log</p><pre>/opt/tomcat/logs tail host-manager.*.log</pre><p>View the last N lines of any&nbsp;file</p><pre>tail -n &lt;number of lines&gt; &lt;filename&gt;</pre><p>Continuously stream the main application log (Ctrl+C to&nbsp;stop)</p><ol><li>The application will be accessible at:</li></ol><pre>http://&lt;public-IP&gt;:8080/your-war-file-name/</pre><p>Where your-war-file-name is the name of your WAR file without the&nbsp;.war extension (e.g. BasicLOS). This is also known as the Context&nbsp;Path.</p><pre>curl http://&lt;privateip&gt;:8080</pre><h3>Change the port to 80 (Optional)</h3><p>For improved user experience it is advisable to change the port to&nbsp;80.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e36f5ca7ce86\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum‑Ready Cloud DevOps – Getting ready for Quantum Computing Integration","url":"https://devops.com/quantum%e2%80%91ready-cloud-devops-getting-ready-for-quantum-computing-integration/","date":1761736788,"author":"Joydip Kanjilal","guid":320663,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Experience-Based Guide to Choosing the Right DevOps Provider in 2026","url":"https://devops.com/an-experience-based-guide-to-choosing-the-right-devops-provider-in-2026/","date":1761732552,"author":"Alex Vakulov","guid":320635,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build more accurate AI applications with Amazon Nova Web Grounding","url":"https://aws.amazon.com/blogs/aws/build-more-accurate-ai-applications-with-amazon-nova-web-grounding/","date":1761695957,"author":"Matheus Guimaraes","guid":319489,"unread":true,"content":"<p>Imagine building AI applications that deliver accurate, current information without the complexity of developing intricate data retrieval systems. Today, we’re excited to announce the general availability of Web Grounding, a new built-in tool for Nova models on <a href=\"https://aws.amazon.com/bedrock/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">Amazon Bedrock</a>.</p><p>Web Grounding provides developers with a turnkey <a href=\"https://aws.amazon.com/what-is/retrieval-augmented-generation/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">Retrieval Augmented Generation (RAG)</a> option that allows the Amazon Nova <a href=\"https://aws.amazon.com/what-is/foundation-models/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">foundation models</a> to intelligently decide when to retrieve and incorporate relevant up-to-date information based on the context of the prompt. This helps to ground the model output by incorporating cited public sources as context, aiming to reduce hallucinations and improve accuracy.</p><p>Developers should consider using Web Grounding when building applications that require access to current, factual information or need to provide well-cited responses. The capability is particularly valuable across a range of applications, from knowledge-based chat assistants providing up-to-date information about products and services, to content generation tools requiring fact-checking and source verification. It’s also ideal for research assistants that need to synthesize information from multiple current sources, as well as customer support applications where accuracy and verifiability are crucial.</p><p>Web Grounding is especially useful when you need to reduce hallucinations in your AI applications or when your use case requires transparent source attribution. Because it automatically handles the retrieval and integration of information, it’s an efficient solution for developers who want to focus on building their applications rather than managing complex RAG implementations.</p><p> Web Grounding seamlessly integrates with supported Amazon Nova models to handle information retrieval and processing during inference. This eliminates the need to build and maintain complex RAG pipelines, while also providing source attributions that verify the origin of information.</p><p>Let’s see an example of asking a question to Nova Premier using Python to call the Amazon Bedrock Converse API with Web Grounding enabled.</p><p>First, I created an Amazon Bedrock client using <a href=\"https://aws.amazon.com/sdk-for-python/?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">AWS SDK for Python (Boto3)</a> in the usual way. For good practice, I’m using a session, which helps to group configurations and make them reusable. I then create a BedrockRuntimeClient.</p><pre><code>try:\n    session = boto3.Session(region_name='us-east-1')\n    client = session.client(\n        'bedrock-runtime')</code></pre><p>I then prepare the Amazon Bedrock Converse API payload. It includes a “role” parameter set to “user”, indicating that the message comes from our application’s user (compared to “assistant” for AI-generated responses).</p><p>For this demo, I chose the question “What are the current AWS Regions and their locations?” This was selected intentionally because it requires current information, making it useful to demonstrate how Amazon Nova can automatically invoke searches using Web Grounding when it determines that up-to-date knowledge is needed.</p><pre><code># Prepare the conversation in the format expected by Bedrock\nquestion = \"What are the current AWS regions and their locations?\"\nconversation = [\n   {\n     \"role\": \"user\",  # Indicates this message is from the user\n     \"content\": [{\"text\": question}],  # The actual question text\n      }\n    ]</code></pre><p>First, let’s see what the output is without Web Grounding. I make a call to Amazon Bedrock Converse API.</p><pre><code># Make the API call to Bedrock \nmodel_id = \"us.amazon.nova-premier-v1:0\" \nresponse = client.converse( \n    modelId=model_id, # Which AI model to use \n    messages=conversation, # The conversation history (just our question in this case) \n    )\nprint(response['output']['message']['content'][0]['text'])</code></pre><p>I get a list of all the current <a href=\"https://docs.aws.amazon.com/glossary/latest/reference/glos-chap.html#region?trk=ac97e39c-d115-4d4a-b3fe-c695e0c9a7ee&amp;sc_channel=el\">AWS Regions</a> and their locations.</p><p>Now let’s use Web Grounding. I make a similar call to the Amazon Bedrock Converse API, but declare  as one of the tools available to the model.</p><pre><code>model_id = \"us.amazon.nova-premier-v1:0\" \nresponse = client.converse( \n    modelId=model_id, \n    messages=conversation, \n    toolConfig= {\n          \"tools\":[ \n              {\n                \"systemTool\": {\n                   \"name\": \"nova_grounding\" # Enables the model to search real-time information\n                 }\n              }\n          ]\n     }\n)</code></pre><p>After processing the response, I can see that the model used Web Grounding to access up-to-date information. The output includes reasoning traces that I can use to follow its thought process and see where it automatically queried external sources. The content of the responses from these external calls appear as  – a standard practice in AI systems that both protects sensitive information and helps manage output size.</p><p>Additionally, the output also includes  objects containing information about the sources queried by Web Grounding.</p><p>Finally, I can see the list of AWS Regions. It finishes with a message right at the end stating that “These are the most current and active AWS regions globally.”</p><p>Web Grounding represents a significant step forward in making AI applications more reliable and current with minimum effort. Whether you’re building customer service chat assistants that need to provide up-to-date accurate information, developing research applications that analyze and synthesize information from multiple sources, or creating travel applications that deliver the latest details about destinations and accommodations, Web Grounding can help you deliver more accurate and relevant responses to your users with a convenient turnkey solution that is straightforward to configure and use.</p><p>Amazon Nova Web Grounding is available now in US East (N. Virginia), US East (Ohio), and US West (Oregon).</p><p>Currently, you can only use Web Grounding with Nova Premier but support for other Nova models will be added soon.</p><p>10/30/25: Updated to all available regions. Original launch only in US East (N. Virginia).</p><a href=\"https://link.codingmatheus.com/linkedin\">Matheus Guimaraes | @codingmatheus</a>","contentLength":5921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TAP Developer Portals in Practice: A Deep Dive into Cloud-Native Productivity with Backstage","url":"https://blog.devops.dev/tap-developer-portals-in-practice-a-deep-dive-into-cloud-native-productivity-with-backstage-bdb6d20bee32?source=rss----33f8b2d9a328---4","date":1761681104,"author":"JIN","guid":319404,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CORS Busters: Quick Hacks for Local Dev (Chrome, Edge, Firefox, Safari)","url":"https://blog.devops.dev/cors-busters-quick-hacks-for-local-dev-chrome-edge-firefox-safari-bd06bd93dc3a?source=rss----33f8b2d9a328---4","date":1761681094,"author":"Raja Sekar Durairaj","guid":319403,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automating Azure Infrastructure and Deployment with Terraform & Ansible: Step-by-Step Guide","url":"https://blog.devops.dev/automating-azure-infrastructure-and-deployment-with-terraform-ansible-step-by-step-guide-88bbe020418c?source=rss----33f8b2d9a328---4","date":1761681085,"author":"Egwu Oko","guid":319402,"unread":true,"content":"<p>Automation is at the heart of modern DevOps. Manually creating cloud resources and configuring servers is time-consuming, error-prone, and non-repeatable.</p><p>In this project, I built — a complete automated pipeline to deploy a :</p><ul><li> provisions the Azure infrastructure.</li><li> installs and configures Nginx, builds the frontend app, deploys static files, and reloads the service automatically.</li></ul><p>This project demonstrates <strong>Infrastructure as Code (IaC)</strong>, , and real-world automation practices.</p><h3>Step 1: Prepare the Project&nbsp;Folder</h3><ul><li>Create the project root directory and change to&nbsp;it.</li></ul><pre>mkdir mini-finance &amp;&amp; cd mini-finance</pre><ul><li>Create the Terraform and Ansible directories</li></ul><ul><li>Change directory to the Terraform folder and create the Terraform files</li></ul><pre>cd terraformtouch main.tf variables.tf outputs.tf</pre><ul><li>Change to the ansible directory and create the ansible&nbsp;files</li></ul><pre>cd ansibletouch inventory.ini site.yml</pre><h3>Step 2: Provision Azure Infrastructure with Terraform</h3><p>Create a secure VM with networking and firewall rules, ready for deployment.</p><h3>Terraform Implementation Steps</h3><ol><li><strong>Define main.tf file for resources</strong></li></ol><pre>provider \"azurerm\" {  features {}resource \"azurerm_resource_group\" \"rg\" {<p>  name     = var.resource_group_name</p>  location = var.location<p>resource \"azurerm_virtual_network\" \"vnet\" {</p>  name                = \"${var.resource_group_name}-vnet\"<p>  address_space       = [\"10.0.0.0/16\"]</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>}<p>resource \"azurerm_subnet\" \"subnet\" {</p>  name                 = \"${var.resource_group_name}-subnet\"<p>  resource_group_name  = azurerm_resource_group.rg.name</p>  virtual_network_name = azurerm_virtual_network.vnet.name<p>  address_prefixes     = [\"10.0.1.0/24\"]</p>}<p># NSG with rules for SSH and HTTP only</p>resource \"azurerm_network_security_group\" \"nsg\" {<p>  name                = \"${var.resource_group_name}-nsg\"</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>}<p>resource \"azurerm_network_security_rule\" \"ssh\" {</p>  name                        = \"Allow-SSH\"<p>  priority                    = 1001</p>  direction                   = \"Inbound\"<p>  access                      = \"Allow\"</p>  protocol                    = \"Tcp\"<p>  source_port_range           = \"*\"</p>  destination_port_range      = \"22\"<p>  source_address_prefix       = \"*\"</p>  destination_address_prefix  = \"*\"<p>  resource_group_name         = azurerm_resource_group.rg.name</p>  network_security_group_name = azurerm_network_security_group.nsg.name<p>resource \"azurerm_network_security_rule\" \"http\" {</p>  name                        = \"Allow-HTTP\"<p>  priority                    = 1010</p>  direction                   = \"Inbound\"<p>  access                      = \"Allow\"</p>  protocol                    = \"Tcp\"<p>  source_port_range           = \"*\"</p>  destination_port_range      = \"80\"<p>  source_address_prefix       = \"*\"</p>  destination_address_prefix  = \"*\"<p>  resource_group_name         = azurerm_resource_group.rg.name</p>  network_security_group_name = azurerm_network_security_group.nsg.nameresource \"azurerm_public_ip\" \"pip\" {<p>  name                = \"${var.resource_group_name}-pip\"</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>  allocation_method   = \"Static\"<p>  #sku                 = \"Basic\"</p>}resource \"azurerm_network_interface\" \"nic\" {<p>  name                = \"${var.resource_group_name}-nic\"</p>  location            = azurerm_resource_group.rg.location<p>  resource_group_name = azurerm_resource_group.rg.name</p>    name                          = \"ipconfig1\"<p>    subnet_id                     = azurerm_subnet.subnet.id</p>    private_ip_address_allocation = \"Dynamic\"<p>    public_ip_address_id          = azurerm_public_ip.pip.id</p>  }<p>resource \"azurerm_network_interface_security_group_association\" \"nsg_association\" {</p>  network_interface_id       = azurerm_network_interface.nic.id<p>  network_security_group_id  = azurerm_network_security_group.nsg.id</p>}<p># Ubuntu VM (azurerm_linux_virtual_machine)</p>resource \"azurerm_linux_virtual_machine\" \"vm\" {<p>  name                = \"${var.resource_group_name}-vm\"</p>  resource_group_name = azurerm_resource_group.rg.name<p>  location            = azurerm_resource_group.rg.location</p>  size                = var.vm_size<p>  admin_username      = var.admin_username</p>  network_interface_ids = [azurerm_network_interface.nic.id]<p>  disable_password_authentication = true</p>    username   = var.admin_username<p>    public_key = file(var.ssh_public_key)</p>  }    caching              = \"ReadWrite\"<p>    storage_account_type = \"Standard_LRS\"</p>  }    publisher = \"Canonical\"<p>    offer     = \"0001-com-ubuntu-server-jammy\"</p>    sku       = \"22_04-lts\"  }    project = \"mini-finance\"}</pre><p>2. Define variables in variables.tf</p><pre>variable \"resource_group_name\" {  description = \"Name of the Azure Resource Group\"}  description = \"Azure region for resources\"}<p>variable \"admin_username\" {</p>  description = \"Admin username for Linux VMs\"}<p>variable \"ssh_public_key\" {</p>  description = \"Path to your SSH public key\"}  description = \"VM size\"}</pre><p>3. Define the output in outputs.tf</p><pre># Outputsoutput \"public_ip\" {<p>  value = azurerm_public_ip.pip.ip_address</p>}  value = var.admin_username  description = \"SSH commands to access each VM\"<p>  value = \"ssh ${var.admin_username}@${azurerm_public_ip.pip.ip_address}\"</p>}</pre><p>4. Define Terraform variables in terraform.tfvars</p><pre>location = \"eastus\"resource_group_name = \"rg-mini-finance\"<p>admin_username = \"azureuser\"</p>vm_size = \"Standard_B1s\"<p>ssh_public_key = \"~/.ssh/id_ed25519.pub\"</p></pre><p>5. Provision the Azure Infrastructure by running Terraform commands:</p><pre>terraform initterraform plan<p>terraform apply -auto-approve</p></pre><h3>Step 3: Configure Ansible Inventory</h3><ul><li>Paste the following code into the inventory.ini</li></ul><pre>[web]20.169.254.159 # Use the actual Public IP of your serveransible_user=azureuser<p>ansible_ssh_private_key_file=~/.ssh/id_ed25519</p></pre><ul><li>Test connection to the&nbsp;server</li></ul><p>- Change directory to the Ansible&nbsp;folder</p><p>- Run the following command to ping the&nbsp;server</p><pre>ansible all -i inventory -m ping</pre><h3>Step 4: Prepare the Ansible multi-Playbook</h3><p>Open the site.yml and add the different playbooks</p><ul><li>Install and configure nginx&nbsp;playbook</li></ul><pre>---- name: Install and configure Nginx  become: yes    nginx_conf: /etc/nginx/sites-available/default    - name: Update apt cache        update_cache: yes<p>    - name: Install required packages</p>      apt:          - nginx        state: present<p>    - name: Ensure Nginx is enabled and running</p>      service:        state: started</pre><ul><li>Installs required packages.</li><li>Ensures Nginx is enabled and&nbsp;running.</li></ul><p>Deploy the static mini finance website&nbsp;playbook</p><pre>- name: Deploy static mini finance website  hosts: web  vars:<p>    site_repo: \"https://github.com/vincegwu/mini-finance-app.git\"</p>    tmp_dir: \"/home/azureuser/mini-finance-app\"<p>    nginx_root: \"/var/www/html\"</p>    - name: Remove default Nginx index        path: \"{{ nginx_root }}/index.nginx-debian.html\"      ignore_errors: yes<p>    - name: Install Node.js and npm</p>      apt:          - nodejs        state: present<p>    - name: Clone static site into temporary directory (user-owned)</p>      git:        dest: \"{{ tmp_dir }}\"<p>    - name: Install npm dependencies</p>      npm:        production: no<p>    - name: Build the static app</p>      command: npm run build        chdir: \"{{ tmp_dir }}\"<p>    - name: Deploy built app to Nginx root</p>      copy:<p>        src: \"{{ tmp_dir }}/build/\"</p>        dest: \"{{ nginx_root }}/\"        group: www-data        remote_src: yes<p>    - name: Ensure proper ownership for Nginx root</p>      file:        owner: www-data        recurse: yes      service:        state: reloaded</pre><ul><li>Removes default nginx&nbsp;index</li><li>Clones static site into temporary directory</li><li>Installs npm dependencies</li><li>Deploys built app to Nginx&nbsp;root</li><li>Ensures proper ownership for Nginx&nbsp;root</li></ul><p>Verify Nginx and Deployment Playbook</p><pre>- name: Verify Nginx and deployment  hosts: web    - name: Check Nginx service status<p>    - name: Display Nginx status</p>      debug:<p>        msg: \"Nginx is {{ 'running' if services['nginx'].state == 'running' else 'not running' }}\"</p><p>    - name: Check homepage availability</p>      uri:        return_content: yes<p>    - name: Show homepage preview</p>      debug:<p>        msg: \"{{ homepage.content | truncate(200) }}\"</p></pre><ul><li>Checks the Nginx service&nbsp;status</li><li>Checks Homepage availability</li></ul><p>Run the automation from the control&nbsp;node</p><pre>ansible-playbook -i inventory.ini site.yml</pre><p>Visit http://&lt;VM Public_ip&gt; — the <strong>Mini-Finance App home page</strong> should render&nbsp;fully.</p><p>Pull down the infrastructure once done using Terraform</p><pre>cd terraformterraform destroy --auto-approve</pre><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=88bbe020418c\" width=\"1\" height=\"1\" alt=\"\">","contentLength":8375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We Lost Events in Production — Then I Discovered Kafka Transactions","url":"https://blog.devops.dev/we-lost-events-in-production-then-i-discovered-kafka-transactions-db4851f12684?source=rss----33f8b2d9a328---4","date":1761681082,"author":"Gaddam.Naveen","guid":319401,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Minimal Feast Tutorial: Turning the California Housing Dataset into Features","url":"https://blog.devops.dev/a-minimal-feast-tutorial-turning-the-california-housing-dataset-into-features-dbda10a0507b?source=rss----33f8b2d9a328---4","date":1761681080,"author":"Okan Yenigün","guid":319400,"unread":true,"content":"<h4>From Raw Data to Real-Time ML: A Hands-On Guide with&nbsp;Feast</h4><p>Feast (short for Feature Store) is an open-source feature store for machine learning, originally developed by Google Cloud and&nbsp;GO-JEK.</p><p>It acts as the bridge between models and&nbsp;data.</p><p>In this post, we’ll briefly explore what Feast is, and later, we’ll use it to build projects.</p><p><em>Before reading this post, you may want to check out my previous blog entry, where I introduced the concept of feature&nbsp;stores:</em></p><p>To ground the concepts of a feature store without getting lost in infrastructure, let’s build a tiny, local Feast project around the California housing&nbsp;dataset.</p><p>First, install the&nbsp;library:</p><p>Let’s pull in the California housing dataset from scikit-learn and take a quick inventory of its shape and&nbsp;columns.</p><pre>from sklearn.datasets import fetch_california_housing<p>df = fetch_california_housing(as_frame=True).frame</p><p>print(f\"Shape of the dataset: {df.shape}\")</p>print(f\"Columns in the dataset: {df.columns.tolist()}\")Shape of the dataset: (20640, 9)<p>Columns in the dataset: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'MedHouseVal']</p>\"\"\"</pre><p>Before we can register features in a Feast feature store, each record must be associated with a . Feast relies on these event timestamps to determine when a feature value was observed — crucial for maintaining point-in-time correctness during both training and&nbsp;serving.</p><p>To prepare our dataset for that, let’s separate predictors and targets, then attach synthetic timestamps to each&nbsp;record.</p><pre>import pandas as pd<p>df_predictors = df.drop(columns=[\"MedHouseVal\"])</p>df_target = df[\"MedHouseVal\"]<p>timestamps = pd.date_range(</p>    end=pd.Timestamp.now(),     freq=\"D\"<p>).to_frame(name=\"event_timestamp\", index=False)</p><p>df_predictors = pd.concat([timestamps, df_predictors], axis=1)</p>df_target = pd.concat([timestamps, df_target], axis=1)</pre><p>To make our dataset fully compatible with Feast, we need an , a column that uniquely identifies each record (or entity) in the feature&nbsp;store.</p><p>In a production setting, this could be something meaningful like a user_id, customer_id, or property_id. Since our California housing dataset doesn’t include a natural unique identifier, we can generate one ourselves using the DataFrame index.</p><pre>df_predictors.reset_index(drop=False, inplace=True)df_predictors.rename(columns={\"index\": \"id\"}, inplace=True)<p>df_target.reset_index(drop=False, inplace=True)</p>df_target.rename(columns={\"index\": \"id\"}, inplace=True)</pre><p>At this stage, we’re setting up the Feast feature repository, which will serve as the central configuration and management hub for all our features, entities, and data&nbsp;sources.</p><p>This command creates a brand-new Feast project directory structure — essentially scaffolding the environment where you’ll define your feature store’s schema and operational logic.</p><p>Feast automatically generates a folder (here named feature_repo) that contains the core components of a feature repository.</p><ol><li>feature_store.yaml: This is the global configuration file that tells Feast where to find your data and how to&nbsp;operate.</li><li>example_repo.py: A starter example containing a sample Entity, FeatureView, and DataSource.</li><li>data/ folder: A placeholder directory where you can store or point to your offline feature data (e.g., Parquet or CSV&nbsp;files).</li></ol><p>Feast’s design philosophy separates data engineering (how features are created and stored) from modeling (how features are consumed). The feature repository is where you define that boundary.</p><p>Now that our Feast repository has been initialized, the next step is to provide it with offline data sources — datasets that define the historical values of our features.</p><pre>df_predictors.to_parquet(\"./feature_repo/feature_repo/data/predictors.parquet\")df_target.to_parquet(\"./feature_repo/feature_repo/data/target.parquet\")</pre><p>Now that our data is organized and stored in Parquet format, the next step is defining feature metadata — the schema, sources, and entities that Feast uses to understand and serve our features.</p><p>Remove the code inside example_repo.py, and&nbsp;then:</p><pre># feature_repo/example_repo.py    Project,    ValueType,    Field,)<p>from feast.types import Float64, Int64</p>from datetime import timedelta<p>project = Project(name=\"feature_repo\",</p>                  description=\"A project for house prices prediction\")<p>house = Entity(name=\"id\", value_type=ValueType.INT64,</p>               description=\"house id\")<p>predictors_fv = FeatureView(</p>    name=\"predictors\",<p>    ttl=timedelta(seconds=3600 * 1),</p>    entities=[house],        Field(name=\"MedInc\", dtype=Float64),<p>        Field(name=\"HouseAge\", dtype=Float64),</p>        Field(name=\"AveRooms\", dtype=Float64),<p>        Field(name=\"AveBedrms\", dtype=Float64),</p>        Field(name=\"Population\", dtype=Int64),<p>        Field(name=\"AveOccup\", dtype=Float64),</p>        Field(name=\"Latitude\", dtype=Float64),<p>        Field(name=\"Longitude\", dtype=Float64),</p>    ],<p>    source=FileSource(path=r\"data/predictors.parquet\",</p>                      timestamp_field=\"event_timestamp\"),    tags={\"team\": \"house_price_prediction\"},    name=\"target\",<p>    ttl=timedelta(seconds=3600 * 1),</p>    entities=[house],        Field(name=\"MedHouseVal\", dtype=Float64),    source=FileSource(path=r\"data/target.parquet\",<p>                      timestamp_field=\"event_timestamp\"),</p>    online=True,<p>    tags={\"team\": \"house_price_prediction\"},</p>)</pre><p>First, we declared a Feast project — a logical grouping of entities, features, and&nbsp;sources.</p><pre>house = Entity(name=\"id\", value_type=ValueType.INT64, description=\"house id\")</pre><p>An Entity represents the unique key around which features are organized. In our case, each house record has a unique id (we created it earlier using reset_index()). Feast uses this to join feature tables during offline training and online&nbsp;serving.</p><p>A Feature View tells&nbsp;Feast:</p><ul><li>What data to treat as&nbsp;features</li><li>How to link it to&nbsp;entities</li><li>How long those features remain fresh&nbsp;TTL</li></ul><pre>predictors_fv = FeatureView(    name=\"predictors\",<p>    ttl=timedelta(seconds=3600 * 1),</p>    entities=[house],        Field(name=\"MedInc\", dtype=Float64),<p>        Field(name=\"HouseAge\", dtype=Float64),</p>        ...    source=FileSource(...),    tags={\"team\": \"house_price_prediction\"},</pre><ul><li>schema: explicitly lists each feature and its data&nbsp;type.</li><li>ttl: sets how long a feature remains&nbsp;valid.</li><li>online=True: ensures this view can be materialized to an online store for low-latency inference.</li><li>tags: metadata for discovery.</li></ul><p>Each FeatureView points to a FileSource, which specifies where Feast should read offline data&nbsp;from.</p><p>Similarly, we define a smaller view for the&nbsp;target.</p><p>After saving the file, apply the definitions to register them in&nbsp;Feast:</p><p>This command takes Python definitions — the Entity, FeatureView, and FileSource objects we declared in example_repo.py — and materializes them into a structured, version-controlled registry.</p><pre>Applying changes for project feature_repo Created project feature_repo Created feature view predictors <p>Created feature view target </p>Created sqlite table feature_repo_predictors <p>Created sqlite table feature_repo_target</p></pre><p>The message we saw means Feast&nbsp;has:</p><ul><li>Created a new project feature_repo. A logical namespace where all our entities, features, and sources&nbsp;live.</li><li>Registered the entity id. This entity now acts as the unique key that binds our predictors and targets together across data&nbsp;sources.</li><li>Created two FeatureViews, predictors and target. Each view has been added to Feast’s internal registry (stored under registry.db).</li><li>Initialized an SQLite online store. These tables will serve as our online store — a lightweight, fast-access database for serving features during real-time predictions. Since our feature_store.yaml specifies SQLite as the default online store, Feast automatically creates the necessary tables.</li></ul><p>Now, we’ve reached the practical stage in our Feast workflow: generating and saving a training dataset from our feature&nbsp;store.</p><p>Up until now, we’ve defined features, entities, and sources and registered them in the Feast registry.</p><pre>from feast import FeatureStorefrom feast.infra.offline_stores.file_source import SavedDatasetFileStorage<p>store = FeatureStore(repo_path=\"./feature_repo/feature_repo\")</p><p>entity_df = pd.read_parquet(path=\"./feature_repo/feature_repo/data/target.parquet\")</p><p>training_data = store.get_historical_features(</p>    entity_df=entity_df,        \"predictors:MedInc\",        \"predictors:AveRooms\",        \"predictors:Population\",        \"predictors:Latitude\",    ]<p>dataset = store.create_saved_dataset(</p>    from_=training_data,<p>    name=\"house_price_prediction_dataset\",</p>    storage=SavedDatasetFileStorage(path=\"./data/house_price_prediction_dataset.parquet\"),</pre><p>First, we initialize the FeatureStore object, which gives us access to both offline (historical data) and online (real-time) retrieval capabilities.</p><p>Feast needs an entity_df which acts as a query template that tells&nbsp;it:</p><ul><li>Which entities (via the id column) we want features&nbsp;for.</li><li>When (via the event_timestamp column) those features should be&nbsp;valid.</li></ul><p>Each row provides context for point-in-time feature&nbsp;lookup.</p><p>Feast will automatically ensure that no future data leakage occurs — it only pulls feature values that existed before the timestamp in each&nbsp;row.</p><pre>training_data = store.get_historical_features(    entity_df=entity_df,<p>    features=[ ... list of predictor features ... ]</p>)</pre><p>Our target.parquet includes entity key and timestamp, but also the target value. Fest doesn’t care that entity_df has extra columns. All it needs&nbsp;are:</p><ul><li>event_timestamp (the event&nbsp;time)</li></ul><p>So, using the target DataFrame as entity_df is totally valid — it’s a convenient shortcut.</p><p>What happens behind the&nbsp;scenes:</p><ul><li>Feast looks up the entity IDs id in the entity_df.</li><li>It finds all matching feature records in the predictors FeatureView.</li><li>It ensures each feature value’s event_timestamp is before the entity’s timestamp.</li><li>It returns a merged dataset ready for model training.</li></ul><p>The result training_data is a Feast retrieval job object, which you can turn into a DataFrame by calling&nbsp;.to_df().</p><p>Then, we convert the retrieval job result into a reusable saved dataset managed by&nbsp;Feast.</p><pre>dataset = store.create_saved_dataset(    from_=training_data,<p>    name=\"house_price_prediction_dataset\",</p>    storage=SavedDatasetFileStorage(path=\"./data/house_price_prediction_dataset.parquet\"),</pre><p>This stores it at the specified Parquet path. It also registers metadata about the dataset in the Feast registry.</p><p>Now, let’s train the&nbsp;model.</p><p>First, we will reopen the Feast project by pointing to our&nbsp;repo.</p><pre>store = FeatureStore(repo_path=\"./feature_repo/feature_repo\")<p>training_df = store.get_saved_dataset(\"house_price_prediction_dataset\").to_df()</p></pre><p>That dataset already contains:</p><ul></ul><p>We split the predictors and the target, and split again for evaluation.</p><pre>y = training_df[\"MedHouseVal\"]X = training_df.drop(columns=[\"MedHouseVal\", \"event_timestamp\", \"id\"])</pre><pre>from sklearn.model_selection import train_test_split<p>X_train, X_test, y_train, y_test = train_test_split(</p>    X, y, test_size=0.2, random_state=42</pre><pre>from sklearn.ensemble import RandomForestRegressor<p>model = RandomForestRegressor()</p>model.fit(X_train, y_train)<p>dump(model, \"./feature_repo/feature_repo/data/model.joblib\")</p></pre><p>Lastly, we serialized our model using joblib and saved&nbsp;it.</p><p>Let’s move from “features on disk” to “features ready for real-time use.” To serve features at low latency, we’ll materialize our offline data (the Parquet files) into Feast’s online store (SQLite in this project).</p><p>We’ll define a time window so Feast only loads rows whose event_timestamp falls within that&nbsp;range.</p><pre>from datetime import datetime<p>store = FeatureStore(repo_path=\"./feature_repo/feature_repo\")</p><p>store.materialize(start_date=datetime(2010, 1, 1), end_date=datetime.now())</p># Materializing 2 feature views from 2010-01-01 00:00:00+00:00 to 2025-10-21 19:57:54+00:00 into the sqlite online store.# predictors:</pre><p> is actually copying for&nbsp;serving.</p><p>Feast scans our sources (our two Parquet files), picks all rows whose event_timestamp is between start_date and end_date, and upserts them into the online store. After this, those features are available for low-latency lookup by&nbsp;id.</p><p>To wrap the workflow with real-time inference, let’s fetch the latest features for a specific house id from Feast’s online store and pass that feature vector to our trained&nbsp;model.</p><pre>feast_features = [    \"predictors:MedInc\",    \"predictors:AveRooms\",    \"predictors:Population\",    \"predictors:Latitude\",]<p>df_features = store.get_online_features(</p>    features=feast_features,<p>    entity_rows=[{\"id\": 20637}],</p>).to_df()<p>#       id  Population  MedInc  AveOccup  AveRooms  Longitude  Latitude  \\</p># 0  20637        1007     1.7  2.325635  5.205543    -121.22     39.43#    HouseAge  AveBedrms</pre><ul><li>entity_rows=[{\"id\": 20637}] asks Feast for the latest materialized features for that single&nbsp;entity.</li><li>.to_df() returns a tidy, single-row DataFrame that includes the id column plus all requested features.</li></ul><pre>from joblib import load<p>model = load(\"./feature_repo/feature_repo/data/model.joblib\")</p><p>X = df_features.reindex(columns=model.feature_names_in_)</p>print(preds)</pre><p>model.predict produces one scalar—the estimated median house value—for id=20637.</p><p>We built a complete mini feature store using Feast to manage features for a house price prediction model — from raw data to real-time predictions.</p><p>Starting with the California housing dataset, we split predictors and targets, added synthetic timestamps, and an id entity, and saved them as Parquet files. After initializing a Feast repo, we defined an Entity id and two Feature Views predictors and target, then applied them to register our features.</p><p>Using Feast’s offline store, we created a historical training dataset with point-in-time correctness, trained a Random Forest Regressor, and saved the&nbsp;model.</p><p>We then materialized the features into the online store and retrieved them by entity ID for real-time inference.</p><p>The key learnings: every record needs an entity and timestamp, Feast guarantees no data leakage, and the same feature definitions power both training and serving — ensuring consistent, reproducible ML workflows from offline to&nbsp;online.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dbda10a0507b\" width=\"1\" height=\"1\" alt=\"\">","contentLength":14055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VoxScribe: A platform to test Opensource Speech-to-Text models","url":"https://blog.devops.dev/voxscribe-a-platform-to-test-opensource-speech-to-text-models-70474a05c513?source=rss----33f8b2d9a328---4","date":1761681077,"author":"Fraser sequeira","guid":319399,"unread":true,"content":"<p><em>The views and opinions expressed in this blog post are my own and do not reflect the official position or views of Amazon Web Services&nbsp;(AWS)</em></p><p>As a Solutions Architect at AWS, I’ve worked with customers across healthcare, contact centers, and enterprise applications who all share a common challenge:- speech-to-text(STT) at scale is expensive. Whether it’s HealthScribe for medical documentation, analyzing millions of call center recordings, or building voice-enabled applications, proprietary STT solutions quickly become a significant line item in the&nbsp;budget.</p><p>I’ve watched customers hit cost ceilings with transcription services, because at scale, the per-minute pricing becomes prohibitive. A contact center processing 100,000 hours of calls monthly can easily spend $150,000+ on transcription alone.</p><p>The open-source STT landscape has matured significantly. Models like <a href=\"https://openai.com/index/whisper/\">Whisper</a>, <a href=\"https://mistral.ai/news/voxtral\">Voxtral</a>, <a href=\"https://developer.nvidia.com/blog/nvidia-speech-ai-models-deliver-industry-leading-accuracy-and-performance/\">Parakeet</a>, and <a href=\"https://huggingface.co/nvidia/canary-qwen-2.5b\">Canary-Qwen</a> now rival or exceed proprietary solutions in accuracy. But here’s the problem I kept seeing: customers wanted to evaluate these models for their specific use cases, but each engine has different dependencies, APIs, and setup requirements. Comparing models meant building custom infrastructure, managing version conflicts, and writing integration code, a weeks-long project before you even see&nbsp;results.</p><p>So here is a lightweight platform <a href=\"https://github.com/Fraser27/VoxScribe\"></a>😊 a platform to test out opensource STT models. It’s a FastAPI backend with a lightweight HTML/JS frontend that lets you test multiple open-source STT models through a single interface. Upload your audio, select models, and compare transcriptions side-by-side. The platform handles dependency conflicts (yes, even the transformers version nightmare between Voxtral and NeMo models), manages model caching, and provides a clean API for integration.</p><h3>The Problem: Fragmented STT Ecosystem</h3><p>If you’ve ever worked with multiple speech recognition models, you know the&nbsp;pain:</p><ul><li>: Different models require conflicting library versions. Mistral’s Voxtral works great with the latest transformers, but then you try to add Parakeet which depends on NeMo that relies of a lower transformers model. Or even the latest Canary-Qwen-2.5B for which you need to directly build NeMo-toolkit from their GIT&nbsp;repo.</li><li>: Each model has its own interface, making it difficult to compare results or switch between&nbsp;engines.</li><li>: Installing CUDA drivers, managing Python environments, and debugging version conflicts can take hours or days. I have combined notes from multiple sources to make this process easier😏.</li><li><strong>Limited Comparison Tools*</strong>: Evaluating which model works best for your use case means building custom testing infrastructure.</li></ul><p>This OpenSource STT platform solves these challenges by managing version conflicts, providing you a playground to test out &gt;5 STT models. Let get straight into deploying this solution. As they say the only way to learn something is to do it yourself. No worries if you aren’t a CUDA freak we’ve got you&nbsp;covered</p><ol><li>We started out with a G6.xlarge EC2 on AWS. This instance comprises of 4 VCPUs/16GB RAM and a single L4 Tensor core GPU with 16GB GPU memory. The latest pricing details for this instance can be found <a href=\"https://instances.vantage.sh/aws/ec2/g6.xlarge?currency=USD&amp;region=us-east-1\">here</a>. Select sufficient GP3 storage, in our case we went ahead with 100 GB GP3&nbsp;storage.</li></ol><p>2. This instance should be launched in a public subnet with a security groups that allows inbound access on port 8000 and SSH access on port&nbsp;22.</p><p>3. Once you’ve launched your instance and its in running state, lets SSH into the system using the below command or you could use <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-connect-methods.html\">EC2 Instance&nbsp;Connect</a>.</p><ul><li>The SSH approach: SSH from your local machine using the PEM key used while launching the EC2 instance.</li></ul><pre>ssh -i &lt;your-pem-file&gt; -o \"StrictHostKeyChecking=no\" ec2-user@&lt;your-ec2-public-ip&gt;</pre><ul><li>The EC2 Instance Connect approach: You could also login via the AWS console. However for the remainder of this blog we shall go ahead with the SSH approach as I’ve found the SSH approach to be more stable especially when our platform downloads larger models from the&nbsp;hub.</li></ul><h3>NVIDIA GRID DRIVERS Installation</h3><p>4. Once within the EC2, we need to install the GRID drivers that gives us access to CUDA. Here are the installation steps the details of which can be found in this <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/install-nvidia-driver.html#nvidia-GRID-driver\">AWS documentation</a> (Head to Option3).</p><pre>sudo dnf update -ysudo dnf install gcc make</pre><p>5. Once you hit reboot you will lose connectivity to your instance. Hence you would again need to SSH into the EC2 machine once its rebooted (around 30 seconds&nbsp;later).</p><p>6. Lets continue with the GRID installation process</p><pre>sudo dnf install -y kernel-devel kernel-modules-extraaws s3 cp --recursive s3://ec2-linux-nvidia-drivers/latest/ .<p>chmod +x NVIDIA-Linux-x86_64*.run</p>sudo /bin/sh ./NVIDIA-Linux-x86_64*.run</pre><p>7. Verify whether the driver is functional. Here you should see the CUDA version. We are on version&nbsp;13.</p><p>8. Conda is an open-source package and environment manager for Python and other software that helps users create isolated environments with different package versions to avoid conflicts. It comes pre-compiled with binaries such as <a href=\"https://pypi.org/project/cdifflib/\">cdifflib</a> required for <a href=\"https://github.com/NVIDIA-NeMo/NeMo\">Nemo</a> on which Parakeet-v2 and Canary-Qwen-2.5B rely. It just makes dependency management way easier compared to&nbsp;PIP.</p><p>9. Lets install CONDA through the following commands</p><pre>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.shbash Miniconda3-latest-Linux-x86_64.sh</pre><ul><li>Accept the license agreement (type&nbsp;`yes`)</li><li>Confirm installation location (default is&nbsp;fine)</li><li>Initialize Conda (type `yes` when prompted)</li></ul><p>10. Restart your shell(SSH Terminal, just as in Step 3) our source&nbsp;bashrc.</p><p>11. You should now be able to verify the conda installation by firing the below&nbsp;command</p><p>12. Now lets create a conda environment with python3.12. Conda will automatically install python3.12 for us. Lets also activate the environment</p><pre>conda create -n stthub python=3.12conda activate stthub</pre><p>13. Lets install ffmpeg in our conda environment. ffmpeg is implicitly used by STT libraries to convert our audio files into compatible audio formats for the STT engine. It can also efficiently extract the audio track from a video file, isolating it for STT processing.</p><h3>GIT CLONE the STT&nbsp;Platform</h3><p>15. git clone into our VoxScribe project</p><pre>git clone https://github.com/Fraser27/VoxScribe.gitcd VoxScribe</pre><p>16. Lets install the project dependencies</p><pre>pip install -r requirements.txt</pre><p>17. Now lets start the VoxScribe app</p><p>18. The app is now running on port 8000 and can be accessed from your public&nbsp;IP</p><pre>http://&lt;your-public-ip&gt;:8000</pre><p>The platform allows you to compare various STT models. The comparisons happen sequentially on the GPU-based instance.</p><ul><li>Solves our number one problem of cost. This self-hosted solution offers a handle on your costs which is crucial as your STT requirements scale.</li><li>You can test multiple models in minutes instead of&nbsp;days.</li><li>Make data-driven choices about which model to use in production.</li><li>One codebase to update, one set of dependencies to&nbsp;manage.</li><li>New models can be added without disrupting existing functionality.</li></ul><p>This is just the beginning. Future enhancements could&nbsp;include:</p><ul><li>Chunking large audio&nbsp;files</li><li>Real-time streaming transcription for live&nbsp;audio</li><li>Speaker diarization to identify different voices</li><li>Language detection and automatic model selection</li><li>Integration with cloud storage (S3, GCS) for seamless workflows</li></ul><p>The <a href=\"https://github.com/Fraser27/VoxScribe\">platform</a> is open source and ready to test. If you’re working with speech recognition models and want a cleaner way to evaluate them, give it a spin. I’m actively working on it and would value your feedback — especially around edge cases, performance bottlenecks, or additional models you’d like to see supported.</p><ul><li><strong>Interested in testing it?</strong> Clone the repo, follow the setup instructions, and let me know what breaks or what could be better. PRs and issues&nbsp;welcome.</li><li>Looking for beta testers and contributors. If you have audio samples that challenge STT models or ideas for improving the comparison workflow, I’d love to hear from&nbsp;you.*</li></ul><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=70474a05c513\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7914,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Complete Guide: GitHub, Git, and Jenkins Integration on Windows","url":"https://blog.devops.dev/complete-guide-github-git-and-jenkins-integration-on-windows-8822a83d4165?source=rss----33f8b2d9a328---4","date":1761681071,"author":"Aravindcsebe","guid":319398,"unread":true,"content":"<p>This comprehensive guide will walk you through setting up a complete CI/CD pipeline on Windows, from creating your first GitHub repository to automating builds with Jenkins. By the end of this tutorial, you’ll have a fully functional setup where pushing code to GitHub automatically triggers builds in&nbsp;Jenkins.</p><h3>Part 1: Installing Git on&nbsp;Windows</h3><ol><li>The download should start automatically. If not, click on the appropriate version (64-bit or&nbsp;32-bit)</li><li>Save the installer file to your&nbsp;computer</li></ol><ol><li>Run the downloaded&nbsp;.exe&nbsp;file</li><li>Click  through the license agreement</li><li>Choose the installation location (default is usually fine: C:\\Program Files\\Git)</li><li>Select components — keep the defaults selected:</li></ol><ul><li>Windows Explorer integration</li></ul><ol><li>Choose the default editor (select your preferred editor or leave as&nbsp;Vim)</li><li>Adjust your PATH environment — select <strong>“Git from the command line and also from 3rd-party software”</strong></li><li>Choose HTTPS transport backend — select <strong>“Use the OpenSSL&nbsp;library”</strong></li><li>Configure line ending conversions — select <strong>“Checkout Windows-style, commit Unix-style line&nbsp;endings”</strong></li><li>Configure terminal emulator — select </li><li>Choose default behavior of git pull - select <strong>\"Default (fast-forward or&nbsp;merge)\"</strong></li><li>Choose credential helper — select </li><li>Enable file system caching — check this&nbsp;option</li></ol><h3>Step 3: Verify Installation</h3><p>Open Command Prompt or PowerShell and&nbsp;type:</p><p>You should see output like: git version 2.x.x.windows.x</p><p>Set up your identity (this will be associated with your commits):</p><pre>git config --global user.name \"Your Name\"git config --global user.email \"your.email@example.com\"</pre><p>Verify your configuration:</p><h3>Part 2: Creating a GitHub&nbsp;Account</h3><ol><li>Click on  in the top right&nbsp;corner</li><li>Enter your email address and click&nbsp;</li><li>Create a strong password and click&nbsp;</li><li>Enter a username (this will be your GitHub handle) and click&nbsp;</li><li>Choose whether you want to receive updates and click&nbsp;</li><li>Solve the verification puzzle</li><li>Check your email for the verification code</li><li>Enter the 6-digit code sent to your&nbsp;email</li></ol><h3>Step 2: Personalize Your&nbsp;Account</h3><ol><li>Answer the questions about your experience (or&nbsp;skip)</li><li>Complete your profile by adding a profile picture and bio (optional)</li></ol><h3>Part 3: Creating Your First Repository</h3><h3>Step 1: Create a New Repository</h3><ol><li>Log in to your GitHub&nbsp;account</li><li>Click the  icon in the top right&nbsp;corner</li><li>Fill in the repository details:</li></ol><ul><li>: my-jenkins-project</li><li>: “Testing GitHub and Jenkins integration”</li><li>: Select  (this is important for Jenkins integration without authentication)</li><li> initialize with README,&nbsp;.gitignore, or license (we’ll add files&nbsp;later)</li></ul><h3>Step 2: Note Your Repository URL</h3><p>After creation, you’ll see a page with setup instructions. Copy the repository URL, which looks&nbsp;like:</p><pre>https://github.com/YOUR-USERNAME/my-jenkins-project.git</pre><h3>Part 4: Creating a&nbsp;Branch</h3><h3>Step 1: Clone the Repository Locally</h3><p>Open Command Prompt or PowerShell and navigate to where you want to store your&nbsp;project:</p><pre>cd C:\\Users\\YourUsername\\Documentsgit clone https://github.com/YOUR-USERNAME/my-jenkins-project.git</pre><h3>Step 2: Create a New&nbsp;Branch</h3><h3>Step 3: Switch to the New&nbsp;Branch</h3><p>Or create and switch in one&nbsp;command:</p><pre>git checkout -b development</pre><h3>Step 4: Verify Your Current&nbsp;Branch</h3><p>The active branch will be marked with an asterisk&nbsp;(*).</p><h3>Step 5: Push the Branch to&nbsp;GitHub</h3><pre>git push -u origin development</pre><p>This creates the branch on GitHub and sets up tracking.</p><h3>Part 5: Creating a Shell Script (.sh&nbsp;File)</h3><h3>Step 1: Create the Script&nbsp;File</h3><p>In your project directory, create a new file called build.sh:</p><p>Open build.sh with any text editor (Notepad, VS Code, etc.) and add the following content:</p><pre>echo \"==================================\"echo \"Starting Build Process\"<p>echo \"==================================\"</p>echo \"Build Date: $(date)\"<p>echo \"Build Number: ${BUILD_NUMBER:-Manual}\"</p>echo \"==================================\"<p>echo \"Hello from the build script!\"</p>echo \"This is version 1.0\"<p>echo \"==================================\"</p>echo \"Build completed successfully!\"<p>echo \"==================================\"</p></pre><p>: Even though Windows uses PowerShell/CMD, the&nbsp;.sh file will be executed by Jenkins, which can run bash&nbsp;scripts.</p><p>Save and close the&nbsp;editor.</p><h3>Part 6: Git Operations — Add, Commit, and&nbsp;Push</h3><p>See what files have&nbsp;changed:</p><p>You should see build.sh as an untracked file.</p><h3>Step 2: Add Files to&nbsp;Staging</h3><pre>git commit -m \"Add initial build script\"</pre><p>The -m flag adds a commit message describing your&nbsp;changes.</p><p>Push your changes to the remote repository:</p><pre>git push origin development</pre><p>This pushes the development branch to&nbsp;GitHub.</p><p>Go to your GitHub repository in a browser and switch to the development branch using the branch dropdown. You should see your build.sh&nbsp;file.</p><h3>Part 7: Git Pull Operations</h3><h3>Pulling Changes from&nbsp;Remote</h3><p>When others make changes or you make changes from another location, you’ll need to pull&nbsp;updates:</p><pre>git pull origin development</pre><p>This fetches and merges changes from the remote development branch.</p><p>Before making changes, always pull&nbsp;first:</p><pre>git pull origin development# Make your changesgit commit -m \"Your commit message\"<p>git push origin development</p></pre><h3>Part 8: Installing and Setting Up Jenkins on&nbsp;Windows</h3><h4>Using the Windows Installer:</h4><ol><li>Run the downloaded&nbsp;.msi&nbsp;file</li><li>Follow the installation wizard</li><li>Jenkins will install as a Windows&nbsp;service</li></ol><ol><li>The installer will show you a path to the initial admin password, typically:(C:\\Program Files\\Jenkins\\secrets\\initialAdminPassword)</li><li>Open this file with&nbsp;Notepad</li><li>Copy the password and paste it into the web interface</li></ol><ol><li>Choose <strong>Install suggested plugins</strong></li><li>Wait for the installation to complete (this may take several&nbsp;minutes)</li></ol><h3>Step 6: Create First Admin&nbsp;User</h3><ol><li>Fill in the form with your&nbsp;details:</li></ol><ul></ul><h3>Step 7: Configure Instance</h3><ol><li>Keep the default Jenkins URL (http://localhost:8080)</li></ol><h3>Part 9: Installing Required Jenkins&nbsp;Plugins</h3><h3>Step 1: Access Plugin&nbsp;Manager</h3><ol><li>From the Jenkins dashboard, click </li><li>Click the &nbsp;tab</li></ol><h3>Step 2: Install Git&nbsp;Plugin</h3><ol><li>In the search box, type </li><li>Check the box next to  (it might already be installed)</li><li>Also search for and&nbsp;install:</li></ol><ul></ul><ol><li>Click  (or <strong>Download now and install after&nbsp;restart</strong>)</li><li>Wait for installation to&nbsp;complete</li></ol><h3>Part 10: Configuring Git in&nbsp;Jenkins</h3><h3>Step 1: Configure Git&nbsp;Path</h3><ol><li>Go to  → <strong>Global Tool Configuration</strong></li><li>Path to Git executable: C:\\Program Files\\Git\\bin\\git.exe</li></ol><h3>Part 11: Linking GitHub and&nbsp;Jenkins</h3><p>Since your repository is , you don’t need to set up credentials. Jenkins can access public repositories directly.</p><ol><li>Make sure your repository is set to  on&nbsp;GitHub:</li></ol><ul><li>Under , verify visibility is&nbsp;“Public”</li></ul><h3>Part 12: Creating a Freestyle Job in&nbsp;Jenkins</h3><ol><li>From Jenkins dashboard, click </li><li>Enter job name: GitHub-Build-Job</li></ol><h3>Step 2: Configure Source Code Management</h3><ol><li>In the  section, select&nbsp;</li><li>In , paste your GitHub repository URL:</li></ol><pre>https://github.com/YOUR-USERNAME/my-jenkins-project.git</pre><p>: Leave as — (since the repo is&nbsp;public)</p><p>: Change from */master to */development</p><p><strong>Do NOT check any build triggers</strong> since you want to build manually or trigger manually.</p><h3>Step 4: Configure Build&nbsp;Steps</h3><ol><li>Scroll down to &nbsp;section</li><li>Select <strong>Execute Windows batch&nbsp;command</strong></li><li>In the command box,&nbsp;enter:</li></ol><pre>@echo offecho Running build script...echo.<p>echo ================================</p>echo Executing script contents:<p>echo ================================</p>bash build.sh</pre><p><strong>Alternative if Git Bash is in&nbsp;PATH:</strong></p><pre>\"C:\\Program Files\\Git\\bin\\bash.exe\" build.sh</pre><h3>Step 5: Save Configuration</h3><p>Click  at the bottom of the&nbsp;page.</p><h3>Part 13: Testing the&nbsp;Pipeline</h3><ol><li>On your job page, click </li><li>You’ll see a build appear in the </li><li>Click on the build number (e.g.,&nbsp;)</li><li>You should see the output from your build.sh&nbsp;script</li></ol><h3>Step 2: Make Changes and&nbsp;Push</h3><ol><li>Open build.sh on your local&nbsp;machine</li><li>Modify the script (e.g., change the version&nbsp;number):</li></ol><pre>echo \"==================================\"echo \"Starting Build Process\"<p>echo \"==================================\"</p>echo \"Build Date: $(date)\"<p>echo \"Build Number: ${BUILD_NUMBER:-Manual}\"</p>echo \"==================================\"<p>echo \"Hello from the build script!\"</p>echo \"This is version 2.0 - NEW UPDATE!\"<p>echo \"==================================\"</p>echo \"Build completed successfully!\"<p>echo \"==================================\"</p></pre><ol></ol><pre>git add build.shgit commit -m \"Update build script to version 2.0\"<p>git push origin development</p></pre><h3>Step 3: Trigger New Build in&nbsp;Jenkins</h3><ol><li>Click on your job </li><li>Click on the new build&nbsp;number</li><li><strong>You should see the NEW output</strong> with “version 2.0 — NEW&nbsp;UPDATE!”</li></ol><ol><li>Commit and push to&nbsp;GitHub</li></ol><p>The console output will reflect your latest&nbsp;changes!</p><h3>Part 14: Understanding the Console&nbsp;Output</h3><p>When you check the console output in Jenkins, you’ll&nbsp;see:</p><ol><li>: Jenkins pulling the latest code from your repository</li><li>: Your script&nbsp;running</li><li>: Everything echoed from your build.sh&nbsp;file</li><li>: Success or failure&nbsp;status</li></ol><pre>Started by user adminRunning as SYSTEM<p>Building in workspace C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job</p>The recommended git tool is: NONECloning the remote Git repository<p>Cloning repository https://github.com/YOUR-USERNAME/my-jenkins-project.git</p> &gt; git init C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job<p> &gt; git fetch --tags --force --progress -- https://github.com/YOUR-USERNAME/my-jenkins-project.git +refs/heads/*:refs/remotes/origin/*</p> &gt; git config remote.origin.url https://github.com/YOUR-USERNAME/my-jenkins-project.git<p> &gt; git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/*</p> &gt; git rev-parse refs/remotes/origin/development^{commit}<p>Checking out Revision abc123... (refs/remotes/origin/development)</p> &gt; git config core.sparsecheckout<p> &gt; git checkout -f abc123...</p>Commit message: \"Update build script to version 2.0\"<p>[GitHub-Build-Job] $ cmd /c call C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\jenkins123.bat</p></pre><pre>C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job&gt;@echo off</pre><pre>C:\\ProgramData\\Jenkins\\.jenkins\\workspace\\GitHub-Build-Job&gt;echo Running build script...Running build script...</pre><pre>==================================Starting Build Process<p>==================================</p>Build Date: Wed Oct 15 10:30:45 2025==================================<p>Hello from the build script!</p>This is version 2.0 - NEW UPDATE!<p>==================================</p>Build completed successfully!<p>==================================</p></pre><h3>Part 15: Common Workflows and&nbsp;Tips</h3><pre>cd my-jenkins-project # Edit your files</pre><ul><li>git add&nbsp;. git commit -m \"Descriptive message about&nbsp;changes\"</li></ul><ul><li>git push origin development</li></ul><p>: Click “Build Now” and check console&nbsp;output</p><h3>Switching Between&nbsp;Branches</h3><pre># View all branchesgit branch -a</pre><pre># Switch to a different branchgit checkout branch-name</pre><pre># Create and switch to new branchgit checkout -b new-feature-branch</pre><pre># Update your local repositorygit pull origin development</pre><pre>git log# or for a compact view</pre><p>: Git commands not recognized</p><ul><li>: Ensure Git is added to PATH during installation. Restart Command Prompt after installation.</li></ul><p>: Permission denied when&nbsp;pushing</p><ul><li>: Check your Git credentials. Run git config --list to&nbsp;verify.</li></ul><p>: Cannot access Jenkins at localhost:8080</p><ul><li>: Check if Jenkins service is running. Go to Services (services.msc) and look for&nbsp;Jenkins.</li></ul><p>: Build fails with “git command not&nbsp;found”</p><ul><li>: Configure Git path in Jenkins: Manage Jenkins → Global Tool Configuration →&nbsp;Git</li></ul><p>: Script doesn’t&nbsp;execute</p><ul><li>: Verify the bash path in your build step. Use full path: \"C:\\Program Files\\Git\\bin\\bash.exe\"</li></ul><p>: Cannot push to repository</p><ul><li>: Verify repository URL and that you have permissions. Use git remote -v to check configured remotes.</li></ul><p>Congratulations! You’ve successfully:</p><p>✅ Installed Git on Windows ✅ Created a GitHub account and repository<p> ✅ Created and managed branches</p> ✅ Created and edited shell scripts<p> ✅ Performed Git operations (add, commit, push, pull)</p> ✅ Installed and configured Jenkins<p> ✅ Linked GitHub with Jenkins</p> ✅ Created a Freestyle job without triggers<p> ✅ Built your project and viewed console&nbsp;output</p></p><p>Every time you push changes to your development branch and manually trigger a build in Jenkins, you'll see the updated output in the console. This forms the foundation of CI/CD practices!</p><p>To enhance your setup, consider:</p><ol><li>: Learn to merge your development branch to&nbsp;main</li><li>: Set up automatic builds when you push (requires webhook configuration)</li><li>: Explore Jenkins Pipeline for more complex workflows</li><li>: Configure email or Slack notifications for build&nbsp;results</li><li>: Add automated tests to your build&nbsp;process</li></ol><p>Happy coding and building! 🚀</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8822a83d4165\" width=\"1\" height=\"1\" alt=\"\">","contentLength":12069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Connect Spring Boot with PostgreSQL in Docker","url":"https://blog.devops.dev/how-to-connect-spring-boot-with-postgresql-in-docker-a654fffdd717?source=rss----33f8b2d9a328---4","date":1761681063,"author":"Aravindcsebe","guid":319397,"unread":true,"content":"<blockquote><em>Run PostgreSQL in Docker and integrate it seamlessly with a Spring Boot application</em></blockquote><p>PostgreSQL is one of the most popular open-source databases — and pairing it with Spring Boot is a common choice for modern backend applications.</p><p>In this guide, I’ll show you how&nbsp;to:</p><ul><li>Run PostgreSQL using&nbsp;Docker</li><li>Connect a Spring Boot application (built with Maven) to the containerized database</li><li>Set up everything for quick and clean local development</li></ul><h3>🐳 Step 1: Pull and Run PostgreSQL with&nbsp;Docker</h3><p>We’ll use the official PostgreSQL image from Docker Hub. If you don’t already have it locally, Docker will  it the first time you run the container.</p><p>🔧 One-liner to pull and&nbsp;run:</p><pre>docker run --name my-postgres -e POSTGRES_USER=testuser -e POSTGRES_PASSWORD=testpass -e POSTGRES_DB=testdb -p 5433:5432 -d postgres</pre><ul><li><strong>Image pulled automatically</strong>: If the postgres image isn’t found locally, Docker will pull the latest version from Docker&nbsp;Hub</li><li>: Named my-postgres</li><li>: With user testuser, password testpass, and database&nbsp;testdb</li><li>: Maps container’s 5432 to your machine's 5433</li></ul><p>You can verify the container is&nbsp;running:</p><p>And if you want to pull the image manually beforehand:</p><h3>⚙️ Step 2: Configure Spring Boot (application.properties)</h3><p>Add the following to your src/main/resources/application.properties:</p><pre>spring.application.name=PostgresDockerDemo<p>spring.datasource.url=jdbc:postgresql://localhost:5433/testdb</p>spring.datasource.username=testuser<p>spring.datasource.password=testpass</p>spring.datasource.driver-class-name=org.postgresql.Driver<p>spring.jpa.hibernate.ddl-auto=update</p>spring.jpa.show-sql=true<p>spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect</p></pre><p>This configures Spring Boot to connect to the PostgreSQL container using the same credentials.</p><h3>📦 Step 3: Add PostgreSQL Driver to&nbsp;Maven</h3><p>In your pom.xml,&nbsp;include:</p><pre> &lt;dependencies&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</p>   &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</p>   &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.postgresql&lt;/groupId&gt;</p>   &lt;artifactId&gt;postgresql&lt;/artifactId&gt;  &lt;/dependency&gt;   &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;<p>   &lt;artifactId&gt;lombok&lt;/artifactId&gt;</p>   &lt;optional&gt;true&lt;/optional&gt;  &lt;dependency&gt;<p>   &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;</p>   &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;  &lt;/dependency&gt;</pre><p>🧪 Step 4: Create a Sample Entity and Repository</p><pre>@Entity@Datapublic class User    @Id<p>    @GeneratedValue(strategy = GenerationType.IDENTITY)</p>    private Long id;    private String email;</pre><pre>public interface UserRepository extends JpaRepository&lt;User, Long&gt; {}</pre><p>🌐 Step 5: Simple REST Controller</p><pre>@RestController@RequestMapping(\"/users\")<p>public class UserController {</p>  private UserRepository userRepository;  public User create(@RequestBody User user) {<p>    return userRepository.save(user);</p>  }  public List&lt;User&gt; all() {<p>    return userRepository.findAll();</p>  }</pre><p>When you run the spring application you could see the below in&nbsp;console.</p><pre>Hibernate: create table users (id bigint generated by default as identity, email varchar(255), name varchar(255), primary key (id))</pre><p>Access PostgreSQL Inside the Docker Container (Optional)</p><p>After your PostgreSQL container is up and running, you can <strong>connect to the database shell</strong> to inspect or insert data manually.</p><p>🧩 Command to enter the container:</p><pre>docker exec -it my-postgres psql -U testuser -d testdb</pre><ul><li>docker exec -it runs a command inside your running container</li><li>my-postgres is the name of your container</li><li>psql is the PostgreSQL CLI</li><li>-U testuser connects with the user you set&nbsp;up</li><li>-d testdb connects to the database you&nbsp;created</li></ul><p>let’s hit the endpoint and&nbsp;check.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a654fffdd717\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitOps Observability: From “It Works on My Machine” to “We Can Prove It Works Everywhere”","url":"https://blog.devops.dev/gitops-observability-from-it-works-on-my-machine-to-we-can-prove-it-works-everywhere-a7896b8f0e08?source=rss----33f8b2d9a328---4","date":1761681007,"author":"Salwan Mohamed","guid":319396,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Host your Helm repo using GitHub","url":"https://blog.devops.dev/host-your-helm-repo-using-github-212ee44466cd?source=rss----33f8b2d9a328---4","date":1761681003,"author":"Dejanu Alex","guid":319395,"unread":true,"content":"<p>When you’re ready to share your charts, the preferred way to do so is by uploading them to a chart repository.</p><p><em>TL;DR: A Helm Repository is HTTP server where packaged charts can be stored and shared. The repository consists of packaged charts and a special file called </em><em>index.yaml (which contains an index of all of the charts in the repository).</em></p><ol><li>Create the repository structure in the root directory: mkdir {charts,packages}&nbsp;:</li></ol><pre>GIT_REPO/├── charts/      # For your source chart directories<p>└── packages/    # For packaged .tgz files and index.yaml</p></pre><p>3. Create a chart i.e. mychart: helm create charts/mychart (<a href=\"https://helm.sh/docs/helm/helm_create/\">helm create</a> creates a chart directory along with the common files and directories used in a&nbsp;chart).</p><p>4. Package the mychart into a versioned chart archive file (basically creating the&nbsp;.tgz file for the chart under packages directory): helm package charts/mychart -d&nbsp;packages</p><p>helm repo indexwill read the packagesdirectory, and generate an index file based on the charts found and write the result to index.yaml (that lists all the charts in your repository along with their metadata).</p><p>6. Configure your GitHub repo to serve static web pages (via <a href=\"https://docs.github.com/en/pages\">GitHubPages</a>) by configuring it to serve a particular branch (main in this&nbsp;case)</p><p>To verify everything works,&nbsp;simply:</p><pre># Add your repo (point to the location of index.yaml file)helm repo add helm_reponame https://your-username.github.io/gitrepo-name/packages<p># Update to fetch the latest index</p>helm repo updatehelm repo list<p># Search for charts in your repo</p>helm search repo helm_reponame</pre><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=212ee44466cd\" width=\"1\" height=\"1\" alt=\"\">","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub Adds Platform for Managing AI Agents Embedded in DevOps Workflows","url":"https://devops.com/github-adds-platform-for-managing-ai-agents-embedded-in-devops-workflows/","date":1761677781,"author":"Mike Vizard","guid":319344,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Securing the AI Era: How Development, Security, and Compliance Must Evolve","url":"https://devops.com/securing-the-ai-era-how-development-security-and-compliance-must-evolve/","date":1761676154,"author":"Sumeet Singh","guid":319343,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ASL pod","url":"https://www.youtube.com/watch?v=A8o99is_L-k","date":1761673770,"author":"CNCF [Cloud Native Computing Foundation]","guid":319313,"unread":true,"content":"<article>Here is the ASL sign for pod.</article>","contentLength":29,"flags":null,"enclosureUrl":"https://www.youtube.com/v/A8o99is_L-k?version=3","enclosureMime":"","commentsUrl":null},{"title":"Survey Surfaces Impact AI Coding Tools Are Having on DevOps Workflows","url":"https://devops.com/survey-surfaces-impact-ai-coding-tools-are-having-on-devops-workflows/","date":1761664864,"author":"Mike Vizard","guid":319237,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon Nova Multimodal Embeddings: State-of-the-art embedding model for agentic RAG and semantic search","url":"https://aws.amazon.com/blogs/aws/amazon-nova-multimodal-embeddings-now-available-in-amazon-bedrock/","date":1761664358,"author":"Danilo Poccia","guid":319230,"unread":true,"content":"<p>Embedding models convert textual, visual, and audio inputs into numerical representations called <a href=\"https://aws.amazon.com/what-is/embeddings-in-machine-learning/\">embeddings</a>. These embeddings capture the meaning of the input in a way that AI systems can compare, search, and analyze, powering use cases such as semantic search and RAG.</p><p>Organizations are increasingly seeking solutions to unlock insights from the growing volume of unstructured data that is spread across text, image, document, video, and audio content. For example, an organization might have product images, brochures that contain infographics and text, and user-uploaded video clips. Embedding models are able to unlock value from unstructured data, however traditional models are typically specialized to handle one content type. This limitation drives customers to either build complex crossmodal embedding solutions or restrict themselves to use cases focused on a single content type. The problem also applies to mixed-modality content types such as documents with interleaved text and images or video with visual, audio, and textual elements where existing models struggle to capture crossmodal relationships eﬀectively.</p><p>Nova Multimodal Embeddings supports a unified semantic space for text, documents, images, video, and audio for use cases such as crossmodal search across mixed-modality content, searching with a reference image, and retrieving visual documents.</p><p>We evaluated the model on a broad range of benchmarks, and it delivers leading accuracy out-of-the-box as described in the following table.</p><p>Nova Multimodal Embeddings supports a context length of up to 8K tokens, text in up to 200 languages, and accepts inputs via synchronous and asynchronous APIs. Additionally, it supports segmentation (also known as “chunking”) to partition long-form text, video, or audio content into manageable segments, generating embeddings for each portion. Lastly, the model oﬀers four output embedding dimensions, trained using <a href=\"https://arxiv.org/abs/2205.13147\">Matryoshka Representation Learning (MRL)</a> that enables low-latency end-to-end retrieval with minimal accuracy changes.</p><p>Let’s see how the new model can be used in practice.</p><p> Getting started with Nova Multimodal Embeddings follows the same pattern as <a href=\"https://aws.amazon.com/bedrock/model-choice/\">other models in Amazon Bedrock</a>. The model accepts text, documents, images, video, or audio as input and returns numerical embeddings that you can use for semantic search, similarity comparison, or RAG.</p><p>Here’s a practical example using the <a href=\"https://aws.amazon.com/sdk-for-python/\">AWS SDK for Python (Boto3)</a> that shows how to create embeddings from different content types and store them for later retrieval. For simplicity, I’ll use <a href=\"https://aws.amazon.com/s3/features/vectors/\">Amazon S3 Vectors</a>, a cost-optimized storage with native support for storing and querying vectors at any scale, to store and search the embeddings.</p><p>Let’s start with the fundamentals: converting text into embeddings. This example shows how to transform a simple text description into a numerical representation that captures its semantic meaning. These embeddings can later be compared with embeddings from documents, images, videos, or audio to find related content.</p><p>To make the code easy to follow, I’ll show a section of the script at a time. The full script is included at the end of this walkthrough.</p><pre><code>import json\nimport base64\nimport time\nimport boto3\n\nMODEL_ID = \"amazon.nova-2-multimodal-embeddings-v1:0\"\nEMBEDDING_DIMENSION = 3072\n\n# Initialize Amazon Bedrock Runtime client\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n\nprint(f\"Generating text embedding with {MODEL_ID} ...\")\n\n# Text to embed\ntext = \"Amazon Nova is a multimodal foundation model\"\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"text\": {\"truncationMode\": \"END\", \"value\": text},\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")</code></pre><p>Now we’ll process visual content using the same embedding space using a  file in the same folder as the script. This demonstrates the power of multimodality: Nova Multimodal Embeddings is able to capture both textual and visual context into a single embedding that provides enhanced understanding of the document.</p><p>Nova Multimodal Embeddings can generate embeddings that are optimized for how they are being used. When indexing for a search or retrieval use case,  can be set to . For the query step,  can be set depending on the type of item to be retrieved. For example, when retrieving documents,  can be set to .</p><pre><code># Read and encode image\nprint(f\"Generating image embedding with {MODEL_ID} ...\")\n\nwith open(\"photo.jpg\", \"rb\") as f:\n    image_bytes = base64.b64encode(f.read()).decode(\"utf-8\")\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"image\": {\n            \"format\": \"jpeg\",\n            \"source\": {\"bytes\": image_bytes}\n        },\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")</code></pre><p>To process video content, I use the asynchronous API. That’s a requirement for videos that are larger than 25MB when encoded as <a href=\"https://en.wikipedia.org/wiki/Base64\">Base64</a>. First, I upload a local video to an S3 bucket in the same <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Region</a>.</p><pre><code>aws s3 cp presentation.mp4 s3://my-video-bucket/videos/</code></pre><p>This example shows how to extract embeddings from both visual and audio components of a video file. The segmentation feature breaks longer videos into manageable chunks, making it practical to search through hours of content efficiently.</p><pre><code># Initialize Amazon S3 client\ns3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n\nprint(f\"Generating video embedding with {MODEL_ID} ...\")\n\n# Amazon S3 URIs\nS3_VIDEO_URI = \"s3://my-video-bucket/videos/presentation.mp4\"\nS3_EMBEDDING_DESTINATION_URI = \"s3://my-embedding-destination-bucket/embeddings-output/\"\n\n# Create async embedding job for video with audio\nmodel_input = {\n    \"taskType\": \"SEGMENTED_EMBEDDING\",\n    \"segmentedEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"video\": {\n            \"format\": \"mp4\",\n            \"embeddingMode\": \"AUDIO_VIDEO_COMBINED\",\n            \"source\": {\n                \"s3Location\": {\"uri\": S3_VIDEO_URI}\n            },\n            \"segmentationConfig\": {\n                \"durationSeconds\": 15  # Segment into 15-second chunks\n            },\n        },\n    },\n}\n\nresponse = bedrock_runtime.start_async_invoke(\n    modelId=MODEL_ID,\n    modelInput=model_input,\n    outputDataConfig={\n        \"s3OutputDataConfig\": {\n            \"s3Uri\": S3_EMBEDDING_DESTINATION_URI\n        }\n    },\n)\n\ninvocation_arn = response[\"invocationArn\"]\nprint(f\"Async job started: {invocation_arn}\")\n\n# Poll until job completes\nprint(\"\\nPolling for job completion...\")\nwhile True:\n    job = bedrock_runtime.get_async_invoke(invocationArn=invocation_arn)\n    status = job[\"status\"]\n    print(f\"Status: {status}\")\n\n    if status != \"InProgress\":\n        break\n    time.sleep(15)\n\n# Check if job completed successfully\nif status == \"Completed\":\n    output_s3_uri = job[\"outputDataConfig\"][\"s3OutputDataConfig\"][\"s3Uri\"]\n    print(f\"\\nSuccess! Embeddings at: {output_s3_uri}\")\n\n    # Parse S3 URI to get bucket and prefix\n    s3_uri_parts = output_s3_uri[5:].split(\"/\", 1)  # Remove \"s3://\" prefix\n    bucket = s3_uri_parts[0]\n    prefix = s3_uri_parts[1] if len(s3_uri_parts) &gt; 1 else \"\"\n\n    # AUDIO_VIDEO_COMBINED mode outputs to embedding-audio-video.jsonl\n    # The output_s3_uri already includes the job ID, so just append the filename\n    embeddings_key = f\"{prefix}/embedding-audio-video.jsonl\".lstrip(\"/\")\n\n    print(f\"Reading embeddings from: s3://{bucket}/{embeddings_key}\")\n\n    # Read and parse JSONL file\n    response = s3.get_object(Bucket=bucket, Key=embeddings_key)\n    content = response['Body'].read().decode('utf-8')\n\n    embeddings = []\n    for line in content.strip().split('\\n'):\n        if line:\n            embeddings.append(json.loads(line))\n\n    print(f\"\\nFound {len(embeddings)} video segments:\")\n    for i, segment in enumerate(embeddings):\n        print(f\"  Segment {i}: {segment.get('startTime', 0):.1f}s - {segment.get('endTime', 0):.1f}s\")\n        print(f\"    Embedding dimension: {len(segment.get('embedding', []))}\")\nelse:\n    print(f\"\\nJob failed: {job.get('failureMessage', 'Unknown error')}\")</code></pre><p>With our embeddings generated, we need a place to store and search them efficiently. This example demonstrates setting up a vector store using Amazon S3 Vectors, which provides the infrastructure needed for similarity search at scale. Think of this as creating a searchable index where semantically similar content naturally clusters together. When adding an embedding to the index, I use the metadata to specify the original format and the content being indexed.</p><pre><code># Initialize Amazon S3 Vectors client\ns3vectors = boto3.client(\"s3vectors\", region_name=\"us-east-1\")\n\n# Configuration\nVECTOR_BUCKET = \"my-vector-store\"\nINDEX_NAME = \"embeddings\"\n\n# Create vector bucket and index (if they don't exist)\ntry:\n    s3vectors.get_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Vector bucket {VECTOR_BUCKET} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Created vector bucket: {VECTOR_BUCKET}\")\n\ntry:\n    s3vectors.get_index(vectorBucketName=VECTOR_BUCKET, indexName=INDEX_NAME)\n    print(f\"Vector index {INDEX_NAME} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_index(\n        vectorBucketName=VECTOR_BUCKET,\n        indexName=INDEX_NAME,\n        dimension=EMBEDDING_DIMENSION,\n        dataType=\"float32\",\n        distanceMetric=\"cosine\"\n    )\n    print(f\"Created index: {INDEX_NAME}\")\n\ntexts = [\n    \"Machine learning on AWS\",\n    \"Amazon Bedrock provides foundation models\",\n    \"S3 Vectors enables semantic search\"\n]\n\nprint(f\"\\nGenerating embeddings for {len(texts)} texts...\")\n\n# Generate embeddings using Amazon Nova for each text\nvectors = []\nfor text in texts:\n    response = bedrock_runtime.invoke_model(\n        body=json.dumps({\n            \"taskType\": \"SINGLE_EMBEDDING\",\n            \"singleEmbeddingParams\": {\n                \"embeddingDimension\": EMBEDDING_DIMENSION,\n                \"text\": {\"truncationMode\": \"END\", \"value\": text}\n            }\n        }),\n        modelId=MODEL_ID,\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n\n    response_body = json.loads(response[\"body\"].read())\n    embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\n    vectors.append({\n        \"key\": f\"text:{text[:50]}\",  # Unique identifier\n        \"data\": {\"float32\": embedding},\n        \"metadata\": {\"type\": \"text\", \"content\": text}\n    })\n    print(f\"  ✓ Generated embedding for: {text}\")\n\n# Add all vectors to store in a single call\ns3vectors.put_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    vectors=vectors\n)\n\nprint(f\"\\nSuccessfully added {len(vectors)} vectors to the store in one put_vectors call!\")</code></pre><p>This final example demonstrates the capability of searching across different content types with a single query, finding the most similar content regardless of whether it originated from text, images, videos, or audio. The distance scores help you understand how closely related the results are to your original query.</p><pre><code># Text to query\nquery_text = \"foundation models\"  \n\nprint(f\"\\nGenerating embeddings for query '{query_text}' ...\")\n\n# Generate embeddings\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps({\n        \"taskType\": \"SINGLE_EMBEDDING\",\n        \"singleEmbeddingParams\": {\n            \"embeddingPurpose\": \"GENERIC_RETRIEVAL\",\n            \"embeddingDimension\": EMBEDDING_DIMENSION,\n            \"text\": {\"truncationMode\": \"END\", \"value\": query_text}\n        }\n    }),\n    modelId=MODEL_ID,\n    accept=\"application/json\",\n    contentType=\"application/json\"\n)\n\nresponse_body = json.loads(response[\"body\"].read())\nquery_embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Searching for similar embeddings...\\n\")\n\n# Search for top 5 most similar vectors\nresponse = s3vectors.query_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    queryVector={\"float32\": query_embedding},\n    topK=5,\n    returnDistance=True,\n    returnMetadata=True\n)\n\n# Display results\nprint(f\"Found {len(response['vectors'])} results:\\n\")\nfor i, result in enumerate(response[\"vectors\"], 1):\n    print(f\"{i}. {result['key']}\")\n    print(f\"   Distance: {result['distance']:.4f}\")\n    if result.get(\"metadata\"):\n        print(f\"   Metadata: {result['metadata']}\")\n    print()</code></pre><p>Crossmodal search is one of the key advantages of multimodal embeddings. With crossmodal search, you can query with text and find relevant images. You can also search for videos using text descriptions, find audio clips that match certain topics, or discover documents based on their visual and textual content. For your reference, the full script with all previous examples merged together is here:</p><pre><code>import json\nimport base64\nimport time\nimport boto3\n\nMODEL_ID = \"amazon.nova-2-multimodal-embeddings-v1:0\"\nEMBEDDING_DIMENSION = 3072\n\n# Initialize Amazon Bedrock Runtime client\nbedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n\nprint(f\"Generating text embedding with {MODEL_ID} ...\")\n\n# Text to embed\ntext = \"Amazon Nova is a multimodal foundation model\"\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"text\": {\"truncationMode\": \"END\", \"value\": text},\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")\n# Read and encode image\nprint(f\"Generating image embedding with {MODEL_ID} ...\")\n\nwith open(\"photo.jpg\", \"rb\") as f:\n    image_bytes = base64.b64encode(f.read()).decode(\"utf-8\")\n\n# Create embedding\nrequest_body = {\n    \"taskType\": \"SINGLE_EMBEDDING\",\n    \"singleEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"image\": {\n            \"format\": \"jpeg\",\n            \"source\": {\"bytes\": image_bytes}\n        },\n    },\n}\n\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps(request_body),\n    modelId=MODEL_ID,\n    contentType=\"application/json\",\n)\n\n# Extract embedding\nresponse_body = json.loads(response[\"body\"].read())\nembedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Generated embedding with {len(embedding)} dimensions\")\n# Initialize Amazon S3 client\ns3 = boto3.client(\"s3\", region_name=\"us-east-1\")\n\nprint(f\"Generating video embedding with {MODEL_ID} ...\")\n\n# Amazon S3 URIs\nS3_VIDEO_URI = \"s3://my-video-bucket/videos/presentation.mp4\"\n\n# Amazon S3 output bucket and location\nS3_EMBEDDING_DESTINATION_URI = \"s3://my-video-bucket/embeddings-output/\"\n\n# Create async embedding job for video with audio\nmodel_input = {\n    \"taskType\": \"SEGMENTED_EMBEDDING\",\n    \"segmentedEmbeddingParams\": {\n        \"embeddingPurpose\": \"GENERIC_INDEX\",\n        \"embeddingDimension\": EMBEDDING_DIMENSION,\n        \"video\": {\n            \"format\": \"mp4\",\n            \"embeddingMode\": \"AUDIO_VIDEO_COMBINED\",\n            \"source\": {\n                \"s3Location\": {\"uri\": S3_VIDEO_URI}\n            },\n            \"segmentationConfig\": {\n                \"durationSeconds\": 15  # Segment into 15-second chunks\n            },\n        },\n    },\n}\n\nresponse = bedrock_runtime.start_async_invoke(\n    modelId=MODEL_ID,\n    modelInput=model_input,\n    outputDataConfig={\n        \"s3OutputDataConfig\": {\n            \"s3Uri\": S3_EMBEDDING_DESTINATION_URI\n        }\n    },\n)\n\ninvocation_arn = response[\"invocationArn\"]\nprint(f\"Async job started: {invocation_arn}\")\n\n# Poll until job completes\nprint(\"\\nPolling for job completion...\")\nwhile True:\n    job = bedrock_runtime.get_async_invoke(invocationArn=invocation_arn)\n    status = job[\"status\"]\n    print(f\"Status: {status}\")\n\n    if status != \"InProgress\":\n        break\n    time.sleep(15)\n\n# Check if job completed successfully\nif status == \"Completed\":\n    output_s3_uri = job[\"outputDataConfig\"][\"s3OutputDataConfig\"][\"s3Uri\"]\n    print(f\"\\nSuccess! Embeddings at: {output_s3_uri}\")\n\n    # Parse S3 URI to get bucket and prefix\n    s3_uri_parts = output_s3_uri[5:].split(\"/\", 1)  # Remove \"s3://\" prefix\n    bucket = s3_uri_parts[0]\n    prefix = s3_uri_parts[1] if len(s3_uri_parts) &gt; 1 else \"\"\n\n    # AUDIO_VIDEO_COMBINED mode outputs to embedding-audio-video.jsonl\n    # The output_s3_uri already includes the job ID, so just append the filename\n    embeddings_key = f\"{prefix}/embedding-audio-video.jsonl\".lstrip(\"/\")\n\n    print(f\"Reading embeddings from: s3://{bucket}/{embeddings_key}\")\n\n    # Read and parse JSONL file\n    response = s3.get_object(Bucket=bucket, Key=embeddings_key)\n    content = response['Body'].read().decode('utf-8')\n\n    embeddings = []\n    for line in content.strip().split('\\n'):\n        if line:\n            embeddings.append(json.loads(line))\n\n    print(f\"\\nFound {len(embeddings)} video segments:\")\n    for i, segment in enumerate(embeddings):\n        print(f\"  Segment {i}: {segment.get('startTime', 0):.1f}s - {segment.get('endTime', 0):.1f}s\")\n        print(f\"    Embedding dimension: {len(segment.get('embedding', []))}\")\nelse:\n    print(f\"\\nJob failed: {job.get('failureMessage', 'Unknown error')}\")\n# Initialize Amazon S3 Vectors client\ns3vectors = boto3.client(\"s3vectors\", region_name=\"us-east-1\")\n\n# Configuration\nVECTOR_BUCKET = \"my-vector-store\"\nINDEX_NAME = \"embeddings\"\n\n# Create vector bucket and index (if they don't exist)\ntry:\n    s3vectors.get_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Vector bucket {VECTOR_BUCKET} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_vector_bucket(vectorBucketName=VECTOR_BUCKET)\n    print(f\"Created vector bucket: {VECTOR_BUCKET}\")\n\ntry:\n    s3vectors.get_index(vectorBucketName=VECTOR_BUCKET, indexName=INDEX_NAME)\n    print(f\"Vector index {INDEX_NAME} already exists\")\nexcept s3vectors.exceptions.NotFoundException:\n    s3vectors.create_index(\n        vectorBucketName=VECTOR_BUCKET,\n        indexName=INDEX_NAME,\n        dimension=EMBEDDING_DIMENSION,\n        dataType=\"float32\",\n        distanceMetric=\"cosine\"\n    )\n    print(f\"Created index: {INDEX_NAME}\")\n\ntexts = [\n    \"Machine learning on AWS\",\n    \"Amazon Bedrock provides foundation models\",\n    \"S3 Vectors enables semantic search\"\n]\n\nprint(f\"\\nGenerating embeddings for {len(texts)} texts...\")\n\n# Generate embeddings using Amazon Nova for each text\nvectors = []\nfor text in texts:\n    response = bedrock_runtime.invoke_model(\n        body=json.dumps({\n            \"taskType\": \"SINGLE_EMBEDDING\",\n            \"singleEmbeddingParams\": {\n                \"embeddingPurpose\": \"GENERIC_INDEX\",\n                \"embeddingDimension\": EMBEDDING_DIMENSION,\n                \"text\": {\"truncationMode\": \"END\", \"value\": text}\n            }\n        }),\n        modelId=MODEL_ID,\n        accept=\"application/json\",\n        contentType=\"application/json\"\n    )\n\n    response_body = json.loads(response[\"body\"].read())\n    embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\n    vectors.append({\n        \"key\": f\"text:{text[:50]}\",  # Unique identifier\n        \"data\": {\"float32\": embedding},\n        \"metadata\": {\"type\": \"text\", \"content\": text}\n    })\n    print(f\"  ✓ Generated embedding for: {text}\")\n\n# Add all vectors to store in a single call\ns3vectors.put_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    vectors=vectors\n)\n\nprint(f\"\\nSuccessfully added {len(vectors)} vectors to the store in one put_vectors call!\")\n# Text to query\nquery_text = \"foundation models\"  \n\nprint(f\"\\nGenerating embeddings for query '{query_text}' ...\")\n\n# Generate embeddings\nresponse = bedrock_runtime.invoke_model(\n    body=json.dumps({\n        \"taskType\": \"SINGLE_EMBEDDING\",\n        \"singleEmbeddingParams\": {\n            \"embeddingPurpose\": \"GENERIC_RETRIEVAL\",\n            \"embeddingDimension\": EMBEDDING_DIMENSION,\n            \"text\": {\"truncationMode\": \"END\", \"value\": query_text}\n        }\n    }),\n    modelId=MODEL_ID,\n    accept=\"application/json\",\n    contentType=\"application/json\"\n)\n\nresponse_body = json.loads(response[\"body\"].read())\nquery_embedding = response_body[\"embeddings\"][0][\"embedding\"]\n\nprint(f\"Searching for similar embeddings...\\n\")\n\n# Search for top 5 most similar vectors\nresponse = s3vectors.query_vectors(\n    vectorBucketName=VECTOR_BUCKET,\n    indexName=INDEX_NAME,\n    queryVector={\"float32\": query_embedding},\n    topK=5,\n    returnDistance=True,\n    returnMetadata=True\n)\n\n# Display results\nprint(f\"Found {len(response['vectors'])} results:\\n\")\nfor i, result in enumerate(response[\"vectors\"], 1):\n    print(f\"{i}. {result['key']}\")\n    print(f\"   Distance: {result['distance']:.4f}\")\n    if result.get(\"metadata\"):\n        print(f\"   Metadata: {result['metadata']}\")\n    print()</code></pre><p>For production applications, embeddings can be stored in any vector database. <a href=\"https://aws.amazon.com/opensearch-service/\">Amazon OpenSearch Service</a> offers native integration with Nova Multimodal Embeddings at launch, making it straightforward to build scalable search applications. As shown in the examples before, <a href=\"https://aws.amazon.com/s3/features/vectors/\">Amazon S3 Vectors</a> provides a simple way to store and query embeddings with your application data.</p><p> Nova Multimodal Embeddings offers four output dimension options: 3,072, 1,024, 384, and 256. Larger dimensions provide more detailed representations but require more storage and computation. Smaller dimensions offer a practical balance between retrieval performance and resource efficiency. This flexibility helps you optimize for your specific application and cost requirements.</p><p>The model handles substantial context lengths. For text inputs, it can process up to 8,192 tokens at once. Video and audio inputs support segments of up to 30 seconds, and the model can segment longer files. This segmentation capability is particularly useful when working with large media files—the model splits them into manageable pieces and creates embeddings for each segment.</p><p>The model includes responsible AI features built into Amazon Bedrock. Content submitted for embedding goes through Amazon Bedrock content safety filters, and the model includes fairness measures to reduce bias.</p><p>As described in the code examples, the model can be invoked through both synchronous and asynchronous APIs. The synchronous API works well for real-time applications where you need immediate responses, such as processing user queries in a search interface. The asynchronous API handles latency insensitive workloads more efficiently, making it suitable for processing large content such as videos.</p><p>If you’re using an AI–powered assistant for software development such as <a href=\"https://aws.amazon.com/q/developer/\">Amazon Q Developer</a> or <a href=\"https://kiro.dev/\">Kiro</a>, you can set up the <a href=\"https://awslabs.github.io/mcp/servers/aws-api-mcp-server\">AWS API MCP Server</a> to help the AI assistants interact with AWS services and resources and the <a href=\"https://awslabs.github.io/mcp/servers/aws-knowledge-mcp-server\">AWS Knowledge MCP Server</a> to provide up-to-date documentation, code samples, knowledge about the regional availability of AWS APIs and CloudFormation resources.</p><p>Start building multimodal AI-powered applications with Nova Multimodal Embeddings today, and share your feedback through <a href=\"https://repost.aws/tags/TAQeKlaPaNRQ2tWB6P7KrMag/amazon-bedrock\">AWS re:Post for Amazon Bedrock</a> or your usual AWS Support contacts.</p>","contentLength":24126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Before learning Kubernetes, understand what a service architecture is.","url":"https://blog.devops.dev/antes-de-aprender-kubernetes-entenda-o-que-e-uma-arquitetura-de-servicos-f7ad9160f4fc?source=rss----33f8b2d9a328---4","date":1761660738,"author":"Prata","guid":319224,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"7 Proven Benefits of DevOps Implementation in Modern Software Development","url":"https://devops.com/7-proven-benefits-of-devops-implementation-in-modern-software-development/","date":1761653342,"author":"Albert Hilton","guid":319133,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"End-to-End Visibility: The Role of Observability in Frontend and Backend Systems","url":"https://devops.com/end-to-end-visibility-observability-in-frontend-backend-systems/","date":1761587337,"author":"Neel Shah","guid":316905,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Weekly Roundup: AWS RTB Fabric, AWS Customer Carbon Footprint Tool, AWS Secret-West Region, and more (October 27, 2025)","url":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-aws-rtb-fabric-aws-customer-carbon-footprint-tool-aws-secret-west-region-and-more-october-27-2025/","date":1761583037,"author":"Micah Walter","guid":316875,"unread":true,"content":"<p>This week started with challenges for many using services in the the North Virginia (us-east-1) Region. On Monday, we experienced a service disruption affecting DynamoDB and several other services due to a DNS configuration problem. The issue has been fully resolved, and you can read the full details in our <a href=\"https://aws.amazon.com/message/101925/\">official summary</a>. As someone who works closely with developers, I know how disruptive these incidents can be to your applications and your users. The teams are learning valuable lessons from this event that will help improve our services going forward.</p><p>On a brighter note, I’m excited to share some launches and updates from this past week that I think you’ll find interesting.</p><p><a href=\"https://aws.amazon.com/blogs/aws/introducing-aws-rtb-fabric-for-real-time-advertising-technology-workloads/\">AWS RTB Fabric is now generally available</a> — If you’re working in advertising technology, you’ll be interested in AWS RTB Fabric, a fully managed service for real-time bidding workloads. It connects AdTech partners like SSPs, DSPs, and publishers through a private, high-performance network that delivers single-digit millisecond latency—critical for those split-second ad auctions. The service reduces networking costs by up to 80% compared to standard cloud solutions with no upfront commitments, and includes three built-in modules to optimize traffic, improve bid efficiency, and increase bid response rates. AWS RTB Fabric is available in US East (N. Virginia), US West (Oregon), Asia Pacific (Singapore and Tokyo), and Europe (Frankfurt and Ireland).</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/aws-customer-carbon-footprint-tool-scope-3-emissions-data/\">Customer Carbon Footprint Tool now includes Scope 3 emissions</a><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/aws-customer-carbon-footprint-tool-scope-3-emissions-data/\"> data</a> — Understanding the full environmental impact of your cloud usage just got more comprehensive. The AWS Customer Carbon Footprint Tool (CCFT) now covers all three industry-standard emission scopes as defined by the Greenhouse Gas Protocol. This update adds Scope 3 emissions—covering the lifecycle carbon impact from manufacturing servers, powering AWS facilities, and transporting equipment to data centers—plus Scope 1 natural gas and refrigerants. With historical data available back to January 2022, you can track your progress over time and make informed decisions about your cloud strategy to meet sustainability goals. Access the data through the CCFT dashboard or AWS Billing and Cost Management Data Exports.</p><p>I thought these projects, blog posts, and news items were also interesting:</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/aws-secret-west-region-is-now-available\">AWS Secret-West Region is now</a><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/aws-secret-west-region-is-now-available\"> available</a> — AWS launched its second Secret Region in the western United States, capable of handling mission-critical workloads at the Secret U.S. security classification level. This new region provides enhanced performance for latency-sensitive workloads and offers multi-region resiliency with geographic separation for Intelligence Community and Department of Defense missions. The infrastructure features data centers and network architecture designed, built, accredited, and operated for security compliance with Intelligence Community Directive requirements.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/amazon-cloudwatch-incident-report/\">Amazon CloudWatch now generates incident reports</a> — CloudWatch investigations can now automatically generate comprehensive incident reports that include executive summaries, timeline of events, impact assessments, and actionable recommendations. The feature collects and correlates telemetry data along with investigation actions to help teams identify patterns and implement preventive measures through structured post-incident analysis.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/amazon-connect-threaded-views-conversation-history/\">Amazon Connect introduces threaded email views</a> — Amazon Connect email now displays exchanges in a threaded format and automatically includes prior conversation context when agents compose responses. These enhancements make it easier for both agents and customers to maintain context and continuity across interactions, delivering a more natural and familiar email experience.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/amazon-ec2-i8g-instances-available-in-additional-aws/\">Amazon EC2 I8g instances expand to additional regions</a> — Storage Optimized I8g instances are now available in Europe (London), Asia Pacific (Singapore), and Asia Pacific (Tokyo). Powered by AWS Graviton4 processors and third-generation AWS Nitro SSDs, these instances deliver up to 60% better compute performance and 65% better real-time storage performance per TB compared to previous generation I4g instances, with storage I/O latency reduced by up to 50%.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/aws-location-services-new-map-styling-enchanced-customization/\">AWS Location Service adds enhanced map styling</a> — Developers can now incorporate terrain visualization, contour lines, real-time traffic overlays, and transportation-specific routing details through the GetStyleDescriptor API. The new styling parameters enable tailored maps for specific applications—from outdoor navigation to logistics planning.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/amazon-cloudwatch-synthetics-bundled-multi-check-canaries/\">CloudWatch Synthetics introduces multi-check canaries</a> — You can now bundle up to 10 different monitoring steps in a single canary using JSON configuration without custom scripts. The multi-check blueprints support HTTP endpoints with authentication, DNS validation, SSL certificate monitoring, and TCP port checks, making API monitoring more cost-effective.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/amazon-s3-generates-aws-cloudtrail-events/\">Amazon S3 Tables now generates CloudTrail events</a> — S3 Tables now logs AWS CloudTrail events for automatic maintenance operations, including compaction and snapshot expiration. This enables organizations to audit the maintenance activities that S3 Tables automatically performs to enhance query performance and reduce operational costs.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/10/aws-lambda-payload-size-256-kb-1-mb-invocations/\">AWS Lambda increases asynchronous invocation payload size to 1 MB</a> — Lambda has quadrupled the maximum payload size for asynchronous invocations from 256 KB to 1 MB across all AWS Commercial and GovCloud (US) Regions. This expansion streamlines architectures by allowing comprehensive data to be included in a single event, eliminating the need for complex data chunking or external storage solutions. Use cases now better supported include large language model prompts, detailed telemetry signals, complex ML output structures, and complete user profiles. The update applies to asynchronous invocations through the Lambda API or push-based events from services like S3, CloudWatch, SNS, EventBridge, and Step Functions. Pricing remains at 1 request charge for the first 256 KB, with 1 additional charge per 64 KB chunk thereafter.</p><p>Keep a look out and be sure to sign up for these upcoming events:</p><p><a href=\"https://reinvent.awsevents.com/\">AWS re:Invent 2025</a> (December 1-5, 2025, Las Vegas) — AWS flagship annual conference offering collaborative innovation through peer-to-peer learning, expert-led discussions, and invaluable networking opportunities. Registration is now open.</p><p>Join the <a href=\"https://builder.aws.com/\">AWS Builder Center</a> to learn, build, and connect with builders in the AWS community. Browse for upcoming in-person and virtual developer-focused events in your area.</p><p>That’s all for this week. Check back next Monday for another Weekly Roundup!</p>","contentLength":6621,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Major AWS Outage — Lessons Every DevOps Engineer Should Learn (Oct 20, 2025)","url":"https://blog.devops.dev/major-aws-outage-lessons-every-devops-engineer-should-learn-oct-20-2025-45dc4f72a166?source=rss----33f8b2d9a328---4","date":1761574311,"author":"Rajesh Thakur","guid":316833,"unread":true,"content":"<h3>🚨 Major AWS Outage — Lessons Every DevOps Engineer Should Learn (Oct 20,&nbsp;2025)</h3><p>On , the internet slowed down for many engineers around the world — and I was one of&nbsp;them.</p><p>That morning, I was in the middle of a <strong>major AWS migration project</strong>, moving workloads to DynamoDB as part of a new infrastructure design. Suddenly, my deployments began failing in ways that didn’t make sense. Lambda invocations were hanging, EC2 status checks were failing, and even IAM permissions seemed inconsistent.</p><p>At first, I suspected an issue in our Terraform configuration — but within minutes, the Slack channels and X (Twitter) feeds lit&nbsp;up:</p><p>The outage was eventually traced back to an issue within <strong>Amazon DynamoDB’s DNS resolution system</strong> in the <strong>US-EAST-1 (Northern Virginia)</strong> region — one of AWS’s most critical&nbsp;zones.</p><p>A  inside DynamoDB’s <strong>automated DNS management system</strong> caused the DNS record for the service endpoint to become&nbsp;.</p><p>That meant thousands of systems across the globe simply  — as if the service vanished off the internet.</p><p>This small DNS failure cascaded&nbsp;quickly:</p><ul><li>⚙️  instances began timing&nbsp;out.</li><li>🔐 , , and other internal services that depend on DynamoDB were disrupted.</li><li>🌍 Major global platforms — including , , and several SaaS providers — experienced multi-hour downtime.</li></ul><p>For a service as foundational as DynamoDB, even a short outage had massive ripple&nbsp;effects.</p><h3>💭 What It Felt Like in Real&nbsp;Time</h3><p>As a DevOps engineer, this kind of moment tests everything — your alerting setup, incident response plan, and patience.</p><p>In our case, autoscaling policies kicked in aggressively (since health checks failed), temporarily spinning up more EC2 instances than needed — but they couldn’t connect to the database layer, rendering them&nbsp;useless.</p><p>Meanwhile, our monitoring dashboards showed spikes everywhere — CPU, latency, error rates — but not a single clear root&nbsp;cause.</p><p>It was a humbling reminder that <strong>even the most robust cloud provider isn’t invincible.</strong></p><h3>💡 Key Takeaways for DevOps Engineers</h3><p>While AWS engineers worked around the clock to restore services, this incident gave all of us some critical lessons to revisit in our own&nbsp;systems:</p><h3>1️⃣ Implement Multi-Region Redundancy</h3><p>Always design your infrastructure for . Keep read replicas or secondary clusters in another region (e.g., US-WEST-2 or AP-SOUTHEAST-1). Don’t assume one region will always stay&nbsp;up.</p><h3>2️⃣ Decouple Critical Dependencies</h3><p>Avoid designing systems where multiple services depend on a . If DynamoDB (or any core component) fails, your app shouldn’t completely stop functioning.</p><h3>3️⃣ Proactive DNS Health Monitoring</h3><p>Set up tools that <strong>continuously monitor DNS record integrity and latency</strong>. Even small propagation delays or empty responses can be early warning&nbsp;signals.</p><h3>4️⃣ Circuit Breakers &amp; Graceful Fallbacks</h3><p>Your services should . If DynamoDB isn’t reachable, queue writes temporarily, serve cached reads, or degrade functionality intelligently instead of going completely offline.</p><h3>5️⃣ Chaos Engineering Drills</h3><p>Run  regularly — disable a region, break your DNS, or simulate latency spikes — to test how your systems (and your team)&nbsp;respond.</p><p>This AWS outage was more than a temporary disruption — it was a  for everyone building in the&nbsp;cloud.</p><p>Resilience isn’t something you build  incidents; it’s something you build &nbsp;them.</p><p>No platform, no matter how advanced, is immune to failure. But as DevOps engineers, our job is to make sure those failures don’t bring everything else down with&nbsp;them.</p><p>If you’re working on <strong>high-availability architecture</strong>, , or , I’d love to discuss ideas and best practices.</p><p>💬 Feel free to connect or message me — always happy to share insights, collaborate, or learn from your experience.</p><p>AWS DevOps CloudComputing InfrastructureAsCode DynamoDB SiteReliabilityEngineering Outage ResilienceEngineering Automation Observability</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=45dc4f72a166\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3943,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"10 CLI Tools That 10x Your Backend Productivity","url":"https://blog.devops.dev/10-cli-tools-that-10x-your-backend-productivity-ed365669ff58?source=rss----33f8b2d9a328---4","date":1761570693,"author":"Yash Batra","guid":316806,"unread":true,"content":"<div><p>I’ve spent the last few years building backend systems across multiple product-based companies — from scrappy startups to teams…</p></div>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fixing Blocked Subdomain DNS with Dnsmasq on Ubuntu","url":"https://blog.devops.dev/fixing-blocked-subdomain-dns-with-dnsmasq-on-ubuntu-b25cc19e9877?source=rss----33f8b2d9a328---4","date":1761570635,"author":"mobin shaterian","guid":316783,"unread":true,"content":"<p>When dealing with corporate networks, restrictive firewalls, or filtered DNS resolvers, sometimes a <strong>specific subdomain gets blocked</strong> — even though the main domain works&nbsp;fine.</p><p>In this post, I’ll show how I solved a real-world case using  on Ubuntu to override the DNS mapping for a single subdomain that was pointing to a blocked Cloudflare IP.</p><p>I had installed  locally on Ubuntu to manage and route DNS queries for different domains.</p><p>When testing connectivity, I found something strange:</p><pre>ping datadriveninvestor.comPING datadriveninvestor.com (104.26.0.23) 56(84) bytes of data.<p>ping medium.datadriveninvestor.com</p>PING medium.datadriveninvestor.com (162.159.153.4) 56(84) bytes of data.</pre><p>Both domains were behind Cloudflare, but the IP 162.159.153.4 was  on my network, while 104.26.0.23 worked&nbsp;fine.</p><p>So even though the root domain loaded perfectly, the medium.datadriveninvestor.com subdomain was inaccessible.</p><p>Redirect all DNS requests for medium.datadriveninvestor.com to 104.26.0.23 without affecting other DNS behavior or&nbsp;domains.</p><p>Dnsmasq acts as a lightweight DNS forwarder and cache. I already had it configured to forward different domains to custom DNS servers, something like&nbsp;this:</p><pre># Listen only on loopbackinterface=lolisten-address=127.0.0.1server=/google.com/10.131.57.151<p>server=/datadriveninvestor.com/76.76.2.11</p></pre><p>This setup meant that all subdomains of datadriveninvestor.com were being resolved through the DNS server at 76.76.2.11.</p><p>That includes the problematic subdomain medium.datadriveninvestor.com.</p><h3>🛠 The Solution: Override the IP&nbsp;Manually</h3><p>Dnsmasq allows you to <strong>force specific domains to resolve to a fixed IP address</strong> using the address= directive.</p><p>So I added this line to /etc/dnsmasq.conf:</p><pre>address=/medium.datadriveninvestor.com/104.26.0.23</pre><blockquote><em>medium.datadriveninvestor.com, return </em><em>104.26.0.23 directly — no upstream&nbsp;lookup.</em></blockquote><p>Here’s the relevant part of the final configuration:</p><pre># Listen only on loopbackinterface=lolisten-address=127.0.0.1<p># Override blocked subdomain</p>address=/medium.datadriveninvestor.com/104.26.0.23<p># Custom DNS servers for domains</p>server=/datadriveninvestor.com/76.76.2.11<p>server=/medium.com/76.76.2.11</p><p># Default and fallback DNS servers</p>server=9.9.9.9domain-neededno-resolvcache-size=1000</pre><p>After editing, restart the Dnsmasq&nbsp;service:</p><pre>sudo systemctl restart dnsmasq</pre><pre>dig medium.datadriveninvestor.com @127.0.0.1</pre><pre>;; ANSWER SECTION:medium.datadriveninvestor.com. 0 IN A 104.26.0.23</pre><p>And ping confirms the&nbsp;fix:</p><pre>ping medium.datadriveninvestor.comPING medium.datadriveninvestor.com (104.26.0.23) 56(84) bytes of data.</pre><ul><li>The server=/domain/ directive tells Dnsmasq  for that&nbsp;domain.</li><li>The address=/domain/ip directive  that process — Dnsmasq answers directly with the given&nbsp;IP.</li><li>address= rules take  over server= rules, so even if another DNS server is set for that domain, the override&nbsp;wins.</li></ul><p>When a subdomain resolves to a , you don’t have to modify your entire DNS setup. With Dnsmasq, a single address= line can override any domain mapping — giving you total control over your DNS resolution.</p><blockquote><em>🧭 Dnsmasq gives you local control when the internet (or your network admin)&nbsp;doesn’t.</em></blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b25cc19e9877\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When ECS Auto Scaling Fails at 2 AM — and What It Taught Me About Cloud Resilience","url":"https://blog.devops.dev/when-ecs-auto-scaling-fails-at-2-am-and-what-it-taught-me-about-cloud-resilience-0f3e74491b32?source=rss----33f8b2d9a328---4","date":1761570631,"author":"Rajesh Thakur","guid":316782,"unread":true,"content":"<h3>When ECS Auto Scaling Fails at 2 AM — and What It Taught Me About Cloud Resilience</h3><p>It was 2:07 AM when my phone buzzed. Our monitoring alert&nbsp;read:</p><blockquote><em>“Increased latency detected on production API.”</em></blockquote><p>Nothing unusual — until I noticed that  hadn’t scaled up, even though the service was clearly under load. Auto Scaling was configured. Targets were healthy.<p> So… why were customers still experiencing 5–10 second response&nbsp;times?</p></p><p>This story is about that night — and how a small oversight in  almost took down our production system.</p><h3>🧩 The Setup: ECS Fargate + ALB + Auto&nbsp;Scaling</h3><p>Our workload was a microservices-based API running on . Each service had an <strong>Application Load Balancer (ALB)</strong> in front and  configured with target tracking policies:</p><ul><li> CPU utilization</li></ul><p>Pretty standard stuff. Except… it didn’t work when traffic&nbsp;spiked.</p><h3>⚠️ The Problem: “CPU ≠ Traffic&nbsp;Reality”</h3><p>During traffic bursts, API latency spiked, but CPU usage barely moved — hovering around 40%. Since ECS scaling was tied to CPU, <strong>no scale-out event was triggered</strong>.</p><p>Meanwhile, the ALB’s “TargetResponseTime” metric was climbing rapidly. Requests were queuing up. Users were getting slower responses.<p> And our “auto-scaling” was sitting&nbsp;idle.</p></p><p>That’s when it hit us: We were scaling based on what , not what .</p><p>The application was , not CPU-bound. Most requests were waiting on downstream dependencies — like database queries and external APIs — rather than consuming heavy&nbsp;CPU.</p><p>So while our ECS tasks  idle (from a CPU perspective), the users were experiencing delays. Auto Scaling simply had no idea this was happening.</p><h3>💡 The Fix: Scaling on Real Experience</h3><p>We decided to shift from  to .</p><h3>1. Created Custom CloudWatch Metrics</h3><p>We pushed the following metrics from our app using the AWS SDK and CloudWatch Agent:</p><ul><li>PendingRequests (from ALB target&nbsp;group)</li></ul><p>Then we used <strong>CloudWatch Math expressions</strong> to&nbsp;derive:</p><pre>AverageLatency = sum(RequestLatency) / sum(RequestCount)</pre><p>This gave us a smooth latency trend line to scale&nbsp;against.</p><h3>2. Updated ECS Auto Scaling&nbsp;Policy</h3><p>Instead of scaling on CPUUtilization, we used <strong>Target Tracking Scaling Policies</strong> based on a :</p><pre>resource \"aws_appautoscaling_policy\" \"ecs_latency_scaling\" {  name               = \"ecs-latency-scaling\"<p>  policy_type        = \"TargetTrackingScaling\"</p>  resource_id        = \"service/${var.cluster_name}/${var.service_name}\"<p>  scalable_dimension = \"ecs:service:DesiredCount\"</p></pre><pre>  target_tracking_scaling_policy_configuration {    target_value = 200  # milliseconds<p>    customized_metric_specification {</p>      metric_name = \"AverageLatency\"<p>      namespace   = \"Custom/ECSService\"</p>      statistic   = \"Average\"  }</pre><p>This made scaling <strong>react to real-world latency</strong>, not just&nbsp;CPU.</p><h3>3. Introduced Dynamic Thresholds</h3><p>We also implemented a  to adjust target scaling thresholds dynamically. If latency stayed high for more than 10 minutes, the function automatically lowered the scale-out threshold, ensuring ECS stayed ahead of the curve during load&nbsp;surges.</p><p>After deploying the new scaling&nbsp;logic:</p><p>✅ Response times dropped by  ✅ ECS scaled , before user impact ✅ Cost increased only , well within&nbsp;budget</p><p>But the real win? Our system became , not just by configuration.</p><ol><li><strong>Don’t trust CPU metrics blindly.</strong> They’re often the last thing to&nbsp;spike.</li><li> (like latency, queue depth, or request count) for&nbsp;scaling.</li><li><strong>Test scaling logic under load</strong>, not just in&nbsp;theory.</li><li> — static rules rarely fit dynamic workloads.</li></ol><p>ECS Auto Scaling isn’t about throwing more tasks at a problem — it’s about understanding <strong>what truly drives your system’s bottlenecks</strong>.</p><p>So next time your scaling policy looks fine on paper but fails in production… Ask yourself: <em>“Am I scaling what matters to the&nbsp;user?”</em></p><p>What’s the most unusual metric <strong>you’ve ever used for auto-scaling</strong>? I’d love to hear your experience 👇</p><p>#DevOps #AWS #ECS #Fargate #CloudComputing #Automation #InfrastructureAsCode #Deploycrafts</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0f3e74491b32\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3933,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Amazon ECS: Newcomer’s Guide","url":"https://blog.devops.dev/understanding-amazon-ecs-newcomers-guide-cee18933a844?source=rss----33f8b2d9a328---4","date":1761570627,"author":"Guide Cloud DevOps","guid":316781,"unread":true,"content":"<p>Think of ECS as a smart conductor that manages a huge orchestra of containers (small units of application code packaged with everything it needs). You just tell ECS what containers to run, how many, and where, and it handles launching, scaling, networking, and health-checking those containers.</p><p>Amazon ECS is built on a highly scalable and reliable architecture that abstracts container orchestration complexities. Key components include:</p><ul><li>Clusters: Logical grouping of container instances (EC2) or serverless resources (Fargate). Clusters provide isolation and resource management boundaries.</li><li>Task Definitions: Blueprint defining container images, CPU, memory, port mappings, environment variables, and IAM roles at task&nbsp;level.</li><li>Tasks and Services: Tasks are instantiations of task definitions, while services ensure desired task count, manage deployment strategies (rolling updates, blue/green), and integrate with Elastic Load Balancing.</li><li>Schedulers: ECS schedulers decide where tasks run based on resource availability, task placement strategies, and constraints.</li><li>Networking: ECS supports multiple networking modes; awsvpc gives tasks individual ENIs for fine-grained network security while bridge and host modes provide legacy compatibility.</li></ul><h3>Features Often Missed by DevOps Engineers</h3><ol><li>Task Placement Strategies &amp; Constraints: Many focus on deploying containers but miss how to optimize placement of tasks across clusters, reducing costs and increasing availability using strategies like binpack, spread, and custom constraints.</li><li>Service Auto Scaling with Target Tracking: Auto scaling is often seen as basic, but ECS integrates deeply with CloudWatch metrics enabling fine-grained scaling of services based on custom metrics, not just CPU or&nbsp;memory.</li><li>ECS Capacity Providers: Allow dynamic switching between EC2 and Fargate resources, giving flexibility to optimize based on workload or&nbsp;cost.</li><li>Task Networking (awsvpc Mode): Offers each task its own Elastic Network Interface (ENI), improving security and network control, often overlooked compared to older bridge&nbsp;mode.</li><li>Integration with CloudWatch Logs and AWS X-Ray: Enables advanced monitoring, tracing, and debugging within ECS environments, improving observability.</li></ol><ul><li>Use Fargate for serverless container deployments where managing infrastructure is not required.</li><li>Define task resource limits (CPU and memory) precisely to avoid over-provisioning or throttling.</li><li>Implement CI/CD pipelines integrating ECS with CodePipeline/CodeBuild or Jenkins for automated container updates.</li><li>Use official AWS CLI &amp; SDKs to manage ECS programmatically and automate deployments.</li><li>Enable task IAM roles to grant minimum permissions required for the containers.</li><li>Configure health checks and circuit breakers to keep services resilient.</li><li>Use Service Discovery with Route 53 for dynamic container endpoint resolution within a microservice architecture.</li></ul><h3>Case Study: Expedia Group — Travel&nbsp;Industry</h3><p>Expedia Group, a global leader in travel technology, leverages Amazon ECS to run its large-scale microservices architecture that powers various travel booking platforms.</p><h3>Key Benefits and Outcomes:</h3><ul><li>Simplified Container Management: Expedia migrated hundreds of microservices to ECS, enabling consistent deployment and management across multiple AWS&nbsp;regions.</li><li>Cost Efficiency: By using a combination of EC2 and Fargate launch types with Spot Instances, Expedia optimized infrastructure cost while maintaining high availability.</li><li>Scalability: ECS enabled seamless scaling during peak travel seasons, handling sudden spikes in user traffic without downtime.</li><li>Improved Developer Productivity: Developers could focus on building features instead of managing infrastructure, thanks to ECS integrations with CI/CD pipelines.</li><li>Security and Compliance: Task IAM roles and network isolation with awsvpc mode maintained strong security posture compliant with industry regulations.</li></ul><p>Mastering Amazon ECS opens a gateway to efficient, scalable, and secure containerized application deployment on the cloud. Beyond simplifying container orchestration, ECS empowers teams to innovate faster by removing infrastructure headaches and enhancing operational agility. By leveraging its advanced features — like dynamic task placement, fine-grained security controls, and flexible compute options — organizations across industries can tailor their cloud strategies to meet evolving demands. Real-world successes, from fintech pioneers to global travel platforms, demonstrate ECS’s versatility in solving complex challenges while optimizing cost and performance. As cloud-native technologies continue to evolve, gaining proficiency in ECS will be an invaluable asset for anyone looking to build resilient and future-ready applications in the AWS ecosystem.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cee18933a844\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4746,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Global State Management in React: From Basics to Advanced","url":"https://blog.devops.dev/global-state-management-in-react-from-basics-to-advanced-3fb46559330e?source=rss----33f8b2d9a328---4","date":1761570580,"author":"Adem KORKMAZ","guid":316780,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightweight Fail2Ban IP Monitoring for Prometheus Using Node Exporter","url":"https://blog.devops.dev/lightweight-fail2ban-ip-monitoring-for-prometheus-using-node-exporter-666209a52fe2?source=rss----33f8b2d9a328---4","date":1761570573,"author":"Zameel Amjed","guid":316779,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Create and Deploy a Neo4J Database with a GraphQL REST API on OpenShift","url":"https://blog.devops.dev/how-to-create-and-deploy-a-neo4j-database-with-a-graphql-rest-api-on-openshift-f0c31b24ec80?source=rss----33f8b2d9a328---4","date":1761570560,"author":"Alessandro","guid":316778,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenTelemetry Collector: The Backbone of Modern Observability Pipelines","url":"https://blog.devops.dev/opentelemetry-collector-the-backbone-of-modern-observability-pipelines-da47b68bb89e?source=rss----33f8b2d9a328---4","date":1761570462,"author":"Scaibu","guid":316777,"unread":true,"content":"<h4>Learn how the OpenTelemetry Collector centralises telemetry data — traces, metrics, and logs — into a unified, vendor-neutral pipeline for scalable observability.</h4><ol><li><strong><em>What is OpenTelemetry Collector?</em></strong></li><li><strong><em>Architecture &amp; Key Components</em></strong></li></ol><ul></ul><p><strong><em>4. Service &amp; Pipeline Configuration</em></strong></p><p><strong><em>5. Docker Deployment Considerations</em></strong></p><p><strong><em>6. Modular Pipeline Recommendations</em></strong></p><p><strong><em>7. Debugging, Health, and Performance Tips</em></strong></p><p><strong><em>8. Hands-On: Run a Complete OpenTelemetry Stack&nbsp;Locally</em></strong></p><ul><li><strong><em>Docker Compose &amp; Service Definitions</em></strong></li><li><strong><em>Collector, Prometheus, Grafana, Loki, Python App&nbsp;Configs</em></strong></li></ul><p><strong><em>9. Final Tips &amp; Next&nbsp;Steps</em></strong></p><h4>1) What is OpenTelemetry Collector?</h4><p>The  is a <strong>centralized, vendor-neutral telemetry pipeline</strong> designed to handle <strong>traces, metrics, and logs</strong> from distributed systems at scale. It acts as an intermediary between applications generating telemetry and the backends that store or visualize that&nbsp;data.</p><p>Modern systems, especially microservices architectures, can generate <strong>millions of telemetry events per second</strong>. The collector ensures these are <strong>processed, filtered, enriched, and exported efficiently</strong>.</p><p>The architecture consists of : receivers, processors, and exporters. Receivers ingest telemetry from various sources (applications, Kubernetes, databases, cloud services). Processors can <strong>batch, sample, filter, or enrich data</strong>, which drastically reduces overhead and network traffic. Exporters then forward data to multiple backends, enabling <strong>multi-cloud observability strategies</strong>.</p><p>The collector’s  allows organizations to switch or add observability backends without modifying application code. This is critical when migrating from one backend to another, or when <strong>running hybrid environments</strong> with multiple monitoring tools.</p><p> A company like Uber or Netflix can handle <strong>tens of millions of traces per day</strong>, and without a collector, the operational overhead of sending raw data directly from every microservice would overwhelm both the network and backend&nbsp;systems.</p><h4>Root couse investigation flow</h4><pre>START: SYSTEM FAILURE DETECTED│<p>├── PRIMARY SYMPTOM CLASSIFICATION</p>│   ├── COMPLETE OUTAGE (5xx errors, timeouts)<p>│   │   ├── Check: Load Balancer Health</p>│   │   ├── Check: Database Connection Pool<p>│   │   ├── Check: External Dependency Status</p>│   │   └── Check: Infrastructure Capacity│   ├── PERFORMANCE DEGRADATION (High latency, slow responses)<p>│   │   ├── Check: Application Metrics (CPU, Memory, GC)</p>│   │   ├── Check: Database Query Performance  <p>│   │   ├── Check: Cache Hit Ratios</p>│   │   └── Check: Network Latency Between Services│   └── DATA INCONSISTENCY (Wrong results, stale data)<p>│       ├── Check: Database Replication Lag</p>│       ├── Check: Cache Invalidation Patterns<p>│       ├── Check: Message Queue Backlogs</p>│       └── Check: Concurrent Modification Issues├── OBSERVABILITY DATA QUALITY ASSESSMENT<p>│   ├── ARE METRICS STREAMING?</p>│   │   ├── Yes → Check Metric Anomalies<p>│   │   │   ├── Sudden Spikes/Dips in Traffic</p>│   │   │   ├── Error Rate Increases<p>│   │   │   ├── Resource Utilization Changes</p>│   │   │   └── Business Metric Deviations│   │   └── No → Investigate Telemetry Pipeline<p>│   │       ├── Collector Health</p>│   │       ├── Network Connectivity<p>│   │       ├── Application Instrumentation</p>│   │       └── Backend Storage Issues│   ├── ARE TRACES AVAILABLE?<p>│   │   ├── Yes → Analyze Distributed Traces</p>│   │   │   ├── Identify Slowest Service in Call Chain<p>│   │   │   ├── Check for Cascading Timeouts</p>│   │   │   ├── Analyze Database Query Times in Spans<p>│   │   │   └── Look for Repeated Error Patterns</p>│   │   │<p>│   │   └── No → Trace Collection Issues</p>│   │       ├── Sampling Rate Too Aggressive<p>│   │       ├── Trace Context Propagation Broken</p>│   │       ├── Collector Processing Bottlenecks<p>│   │       └── Storage Backend Overloaded</p>│   │<p>│   └── ARE LOGS BEING CAPTURED?</p>│       ├── Yes → Search for Error Patterns<p>│       │   ├── Exception Stack Traces</p>│       │   ├── Warning Message Frequency<p>│       │   ├── Custom Error Log Entries</p>│       │   └── Authentication/Authorization Failures│       └── No → Log Pipeline Failure<p>│           ├── Log Agent Crashes</p>│           ├── Log Rotation Issues<p>│           ├── Storage Quota Exceeded</p>│           └── Network Partition to Log Backend├── CORRELATION ANALYSIS<p>│   ├── TEMPORAL CORRELATION</p>│   │   ├── What Changed at Failure Time?<p>│   │   │   ├── Deployment Timeline</p>│   │   │   ├── Configuration Changes<p>│   │   │   ├── Traffic Pattern Shifts</p>│   │   │   └── External Event Correlation│   │   └── Rate of Change Analysis<p>│   │       ├── Gradual Degradation vs Sudden Drop</p>│   │       ├── Periodic Patterns (Time-based)<p>│   │       ├── User Segment Specific Issues</p>│   │       └── Geographic Impact Patterns│   └── DEPENDENCY CORRELATION  <p>│       ├── Service Dependency Mapping</p>│       │   ├── Direct Dependencies Failure Impact<p>│       │   ├── Transitive Dependencies Analysis</p>│       │   ├── Shared Resource Contention<p>│       │   └── Database/Storage Layer Issues</p>│       │<p>│       └── Infrastructure Correlation</p>│           ├── Host/Container Level Resource Issues<p>│           ├── Network Partition/Saturation</p>│           ├── Storage I/O Bottlenecks<p>│           └── Memory Pressure/Leak Patterns</p>│<p>└── ANOMALY PATTERN RECOGNITION</p>    ├── KNOWN ANOMALY PATTERNS<p>    │   ├── CASCADING FAILURE</p>    │   │   ├── Symptoms: Sequential service failures<p>    │   │   ├── Root Cause: Missing circuit breakers</p>    │   │   ├── Detection: Trace waterfall analysis<p>    │   │   └── Resolution: Implement bulkheads + timeouts</p>    │   │<p>    │   ├── RESOURCE EXHAUSTION</p>    │   │   ├── Symptoms: Gradual performance decline<p>    │   │   ├── Root Cause: Memory leaks, connection pool exhaustion</p>    │   │   ├── Detection: Resource utilization trends<p>    │   │   └── Resolution: Restart + fix resource management</p>    │   │    │   │   ├── Symptoms: Intermittent failures, hard to reproduce<p>    │   │   ├── Root Cause: Concurrent access patterns</p>    │   │   ├── Detection: Log timing analysis, trace concurrency<p>    │   │   └── Resolution: Add locking/coordination</p>    │   │    │       ├── Symptoms: Wrong results, calculation errors<p>    │       ├── Root Cause: Bug in business logic, cache issues</p>    │       ├── Detection: Business metric validation<p>    │       └── Resolution: Data repair + logic fix</p>    │<p>    └── UNKNOWN ANOMALY PATTERNS</p>        ├── STATISTICAL OUTLIER DETECTION<p>        │   ├── Z-score analysis on key metrics</p>        │   ├── Seasonal decomposition of time series<p>        │   ├── Cluster analysis of error patterns</p>        │   └── Change point detection in traffic        └── MACHINE LEARNING APPROACHES<p>            ├── Anomaly detection on multi-dimensional metrics</p>            ├── Pattern matching against historical incidents<p>            ├── Predictive failure scoring</p>            └── Root cause suggestion engines</pre><h4>Performace anomaly investigatio</h4><pre>START: P95 LATENCY INCREASED BY 300%│<p>├── DATA COLLECTION VERIFICATION</p>│   ├── Verify metric collection is working<p>│   ├── Check for data gaps or sampling issues</p>│   └── Confirm time synchronization across systems├── TRAFFIC PATTERN ANALYSIS│   │   ├── Total request rate increase?<p>│   │   ├── Specific endpoint traffic spikes?</p>│   │   └── User segment concentration?│   ├── Request Characteristics<p>│   │   ├── Payload size changes?</p>│   │   ├── New user agent patterns?<p>│   │   ├── Geographic source shifts?</p>│   │   └── API version adoption changes?│   └── Workload Patterns<p>│       ├── Read/Write ratio changes?</p>│       ├── Complex query frequency?<p>│       ├── Batch job interference?</p>│       └── Background task scheduling?├── INFRASTRUCTURE DEPTH ANALYSIS<p>│   ├── Compute Resources</p>│   │   ├── CPU utilization correlation<p>│   │   ├── Memory pressure indicators</p>│   │   ├── Garbage collection behavior<p>│   │   └── I/O wait times</p>│   ││   │   ├── Network latency between services<p>│   │   ├── Packet loss rates</p>│   │   ├── DNS resolution times<p>│   │   └── Connection pool exhaustion</p>│   ││       ├── Database query performance<p>│       ├── Cache hit ratio degradation</p>│       ├── Disk I/O saturation<p>│       └── Lock contention patterns</p>│<p>├── APPLICATION CODE ANALYSIS</p>│   ├── Recent Deployment Correlation<p>│   │   ├── Code changes in critical paths</p>│   │   ├── Library/dependency updates<p>│   │   ├── Configuration changes</p>│   │   └── Feature flag activations│   ├── Code-Level Performance<p>│   │   ├── Profiler data analysis</p>│   │   ├── Method-level timing changes<p>│   │   ├── Algorithm complexity issues</p>│   │   └── Memory allocation patterns│   └── Concurrency Issues<p>│       ├── Thread pool exhaustion</p>│       ├── Deadlock detection<p>│       ├── Race condition symptoms</p>│       └── Resource contention patterns└── EXTERNAL DEPENDENCY ANALYSIS    │   ├── API response time changes<p>    │   ├── Rate limiting encounters</p>    │   ├── Authentication service delays<p>    │   └── Payment processor latency</p>    │    │   ├── Message queue backlogs<p>    │   ├── Service discovery delays</p>    │   ├── Configuration service latency<p>    │   └── Secret management performance</p>    │<p>    └── Environmental Factors</p>        ├── Data center network issues<p>        ├── Cloud provider degradation</p>        ├── CDN performance changes<p>        └── Geographic routing problems</p></pre><h4>Error rate spike investigation</h4><pre>START: ERROR RATE INCREASED FROM 0.1% TO 5%│<p>├── ERROR PATTERN CLASSIFICATION</p>│   ├── Error Type Distribution<p>│   │   ├── New error types emerged?</p>│   │   ├── Existing error frequency increase?<p>│   │   ├── Stack trace pattern analysis?</p>│   │   └── Error message clustering?│   ├── Temporal Patterns<p>│   │   ├── Continuous vs intermittent?</p>│   │   ├── Time-based correlation?<p>│   │   ├── Gradual increase vs sudden spike?</p>│   │   └── Recovery pattern analysis?│   └── User Impact Patterns<p>│       ├── Specific user segments affected?</p>│       ├── Geographic concentration?<p>│       ├── Device/platform specific?</p>│       └── Transaction type correlation?├── DEPLOYMENT CORRELATION ANALYSIS<p>│   ├── Direct Deployment Impact</p>│   │   ├── Timeline alignment with deployment<p>│   │   ├── Canary deployment failure rate</p>│   │   ├── Feature flag correlation<p>│   │   └── Configuration change impact</p>│   │<p>│   ├── Dependency Chain Impact</p>│   │   ├── Upstream service deployments<p>│   │   ├── Shared library updates</p>│   │   ├── Infrastructure changes<p>│   │   └── Database schema migrations</p>│   │<p>│   └── Environmental Changes</p>│       ├── Certificate expirations<p>│       ├── Quota/limit changes</p>│       ├── Security policy updates<p>│       └── Network configuration changes</p>││   ├── Input Validation Issues<p>│   │   ├── Malformed request patterns</p>│   │   ├── Schema validation failures<p>│   │   ├── Data type conversion errors</p>│   │   └── Payload size limit breaches│   ├── Data Corruption Patterns<p>│   │   ├── Database constraint violations</p>│   │   ├── Cache consistency issues<p>│   │   ├── Message format mismatches</p>│   │   └── Serialization/deserialization failures│   └── State Management Issues<p>│       ├── Session data corruption</p>│       ├── Concurrent modification conflicts<p>│       ├── Cache invalidation problems</p>│       └── Distributed state inconsistencies└── EXTERNAL INTEGRATION ANALYSIS<p>    ├── API Dependency Failures</p>    │   ├── Third-party service degradation<p>    │   ├── Authentication/authorization issues</p>    │   ├── Rate limiting responses<p>    │   └── Protocol version mismatches</p>    │<p>    ├── Infrastructure Integration</p>    │   ├── Service discovery failures<p>    │   ├── Load balancer health check issues</p>    │   ├── Storage connectivity problems<p>    │   └── Message queue delivery failures</p>    │<p>    └── Security-Related Errors</p>        ├── Certificate validation failures<p>        ├── Token expiration issues</p>        ├── Permission/privilege changes<p>        └── Firewall/network policy blocks</p></pre><h4>Observability anomaly pattern&nbsp;library</h4><h4>PATTERN 1: Cascading failer&nbsp;anomaly</h4><pre>DETECTION SIGNATURES:├── Primary Indicators<p>│   ├── Sequential service failure timeline</p>│   ├── Increasing timeout errors in call chain<p>│   ├── Retry storm patterns in metrics</p>│   └── Resource exhaustion spreading between services├── Trace Analysis Patterns<p>│   ├── Span duration increasing upstream to downstream</p>│   ├── Error propagation following service dependencies<p>│   ├── Repeated calls to failing dependencies</p>│   └── Circuit breaker tripping events└── Resolution Steps<p>    ├── IMMEDIATE: Break dependency chain (degrade functionality)</p>    ├── SHORT-TERM: Implement/configure circuit breakers<p>    ├── MEDIUM-TERM: Add bulkheading and timeout configurations</p>    └── LONG-TERM: Design for graceful degradation</pre><h4>PATTERN 2: Memory leak&nbsp;anomaly</h4><pre>DETECTION SIGNATURES:├── Primary Indicators<p>│   ├── Gradual memory consumption increase</p>│   ├── Frequent garbage collection cycles<p>│   ├── System becomes unresponsive over time</p>│   └── Restarts temporarily resolve issues├── Deep Analysis Patterns<p>│   ├── Heap dump analysis showing retained objects</p>│   ├-&gt; Memory allocation trends by object type<p>│   ├── GC pause duration increases</p>│   └── Off-heap memory growth patterns└── Investigation Path<p>    ├── Check for unclosed resources (connections, files, streams)</p>    ├── Analyze cache growth without eviction<p>    ├── Review static collection usage patterns</p>    └── Examine third-party library memory management</pre><h3>PATTERN 3: Database connection anomaly</h3><pre>DETECTION SIGNATURES:├── Primary Indicators<p>│   ├── High database CPU with low application CPU</p>│   ├── Increasing query latency without traffic increase<p>│   ├── Lock wait time metrics showing spikes</p>│   └── Deadlock detection in database logs├── Deep Analysis Patterns<p>│   ├── Query execution plan changes</p>│   ├── Index usage statistics degradation<p>│   ├── Table lock contention patterns</p>│   └── Connection pool exhaustion timelines└── Resolution Tree<p>    ├── IMMEDIATE: Kill blocking queries, increase timeouts</p>    ├── SHORT-TERM: Add missing indexes, optimize queries<p>    ├── MEDIUM-TERM: Implement read replicas, connection pooling</p>    └── LONG-TERM: Database sharding, application-level caching</pre><h4>PATTERN 4: Race condition anomaly</h4><pre>DETECTION SIGNATURES:├── Primary Indicators<p>│   ├── Intermittent failures with no clear pattern</p>│   ├── Time-dependent bug manifestations<p>│   ├── Concurrent user actions causing inconsistencies</p>│   └── Hard to reproduce in development environments├── Investigation Techniques<p>│   ├── Log correlation across concurrent requests</p>│   ├── Distributed trace analysis for timing issues<p>│   ├-&gt; Load testing with concurrent user simulation</p>│   └── Code review for thread-unsafe patterns└── Resolution Approaches<p>    ├── Add synchronization mechanisms carefully</p>    ├── Implement optimistic/pessimistic locking<p>    ├── Use atomic operations where possible</p>    └── Design for idempotency and eventual consistency</pre><h4>Multi-Dimesional correlation matrix</h4><pre>TIME-BASED CORRELATION├── Event Timeline Reconstruction<p>│   ├── Deployment events → Metric changes</p>│   ├── Configuration changes → Error patterns  <p>│   ├── Traffic spikes → Performance degradation</p>│   └── External events → System behavior changes├── Lag Correlation Analysis<p>│   ├── Delayed impact detection (changes that take time to manifest)</p>│   ├── Cumulative effect analysis (small changes building up)<p>│   ├── Periodic pattern recognition (daily/weekly cycles)</p>│   └── Seasonality impact assessment (holidays, events)└── Rate of Change Analysis<p>    ├── Acceleration/deceleration patterns in metrics</p>    ├── Step function changes vs gradual trends<p>    ├── Recovery time analysis after incidents</p>    └── Baseline deviation significance calculation<p>DEPENDENCY-BASED CORRELATION  </p>├── Service Dependency Mapping<p>│   ├── Direct dependency failure impact quantification</p>│   ├── Transitive dependency risk assessment<p>│   ├── Shared resource contention identification</p>│   └── Circular dependency detection├── Infrastructure Dependency Analysis<p>│   ├── Host/container resource sharing impact</p>│   ├── Network topology failure propagation<p>│   ├── Storage subsystem performance coupling</p>│   └── Load balancer routing effect analysis└── Data Flow Dependency Tracking<p>    ├── Data transformation pipeline issues</p>    ├── Cache dependency invalidation chains<p>    ├── Message queue consumer lag impact</p>    └── Database replication lag effects├── User Experience Impact Analysis<p>│   ├── Conversion funnel degradation correlation</p>│   ├── User segment-specific issue identification<p>│   ├── Geographic performance impact mapping</p>│   └── Device/platform-specific problem detection├── Business Metric Correlation<p>│   ├── Technical metrics → Business KPI impact</p>│   ├── Error rates → Revenue loss quantification<p>│   ├── Performance degradation → User engagement correlation</p>│   └── Availability issues → Customer satisfaction impact└── Workflow Analysis<p>    ├── Multi-step transaction failure patterns</p>    ├── User navigation path performance issues<p>    ├── Feature usage correlation with system health</p>    └── A/B test variant performance differences</pre><h4>Anomaly root cause probability matrix</h4><pre>SYMPTOM: HIGH LATENCY + INCREASED ERROR RATE│<p>├── HIGH PROBABILITY (60-80%)</p>│   ├── Database performance degradation<p>│   ├── External API dependency slowdown</p>│   ├── Resource exhaustion (memory/CPU)<p>│   └── Network connectivity issues</p>│<p>├── MEDIUM PROBABILITY (30-60%)  </p>│   ├── Recent deployment introducing bugs<p>│   ├── Configuration changes causing issues</p>│   ├── Cache invalidation problems<p>│   └── Load balancer misconfiguration</p>│<p>└── LOW PROBABILITY (10-30%)</p>    ├── Security attack (DDoS, exploitation)    ├── Data corruption issues<p>    └── Race conditions in code</p><p>SYMPTOM: INTERMITTENT FAILURES + DATA INCONSISTENCIES</p>│<p>├── HIGH PROBABILITY (60-80%)</p>│   ├── Race conditions in application code<p>│   ├── Cache coherence issues</p>│   ├── Concurrent modification problems<p>│   └── Session state management bugs</p>│<p>├── MEDIUM PROBABILITY (30-60%)</p>│   ├── Database replication lag issues<p>│   ├── Message queue duplication/delays</p>│   ├── Clock synchronization problems<p>│   └── Distributed lock management issues</p>│<p>└── LOW PROBABILITY (10-30%)</p>    ├── Memory corruption    ├── Compiler/runtime bugs<p>    └── Cosmic ray impacts (in specific environments)</p></pre><h4>2) Why is OpenTelemetry Collector Important?</h4><p><strong>Massive Telemetry Management</strong></p><ul><li>Modern distributed systems generate <strong>huge volumes of telemetry</strong>. A single large-scale e-commerce platform can generate <strong>over 100 million metrics and 10 million traces per day</strong> across thousands of microservices. Without a centralized pipeline, managing, storing, and querying this data becomes <strong>inefficient and error-prone</strong>.</li></ul><ul><li>Without OpenTelemetry, switching backends would require <strong>instrumentation rewrites in potentially hundreds of services</strong>, each generating data in a proprietary format. The collector standardizes telemetry in the <strong>OpenTelemetry Protocol (OTLP)</strong> format, making backend migration . For instance, Netflix and Shopify rely on collectors to avoid being tied to one observability vendor while maintaining <strong>global monitoring consistency</strong>.</li></ul><p><strong>Optimizes Throughput and Resource&nbsp;Usage</strong></p><p>OpenTelemetry Collector reduces overhead through <strong>sampling, batching, filtering, and aggregation</strong>:</p><ul><li>Sampling can reduce telemetry volume by  without losing critical observability.</li><li>Batching multiple events before sending them to backends reduces , lowering latency and cloud egress&nbsp;costs.</li><li>Preprocessing logs and metrics can prevent storage overload in backends like Elasticsearch or Prometheus, which may struggle with <strong>millions of inserts per&nbsp;minute</strong>.</li></ul><p><strong>Supports Multi-Backend Strategies</strong></p><p>Many organizations use different tools for different observability needs:</p><ul><li> in Jaeger or Datadog for debugging and root cause analysis.</li><li> in Prometheus or VictoriaMetrics for real-time monitoring.</li><li> in Loki or Elasticsearch for auditing and troubleshooting.</li></ul><p>OpenTelemetry Collector allows the same data to flow <strong>to multiple systems simultaneously</strong>, supporting redundancy, disaster recovery, and cross-functional analytics.</p><p><strong>Enables Uniform Observability Across Microservices</strong></p><p>Large distributed systems often have hundreds or thousands of services, each with slightly different instrumentation. The collector <strong>enforces uniform tags, context propagation, and metadata enrichment</strong>, making cross-service queries, dependency graphs, and SLA monitoring reliable.</p><p><strong>Scalability and Reliability</strong></p><p>Collectors can be deployed&nbsp;as:</p><ul><li> on each host, handling local telemetry and sending it upstream.</li><li> in central locations, aggregating telemetry from multiple&nbsp;sources.</li></ul><p>They can handle <strong>millions of telemetry events per second</strong> with <strong>low latency (&lt;5ms in many setups)</strong>. This ensures critical observability data is delivered <strong>without impacting production performance</strong>.</p><p>By filtering, aggregating, and sampling telemetry at the collector, organisations can save substantial cloud&nbsp;costs:</p><ul><li>Filtering 80% of noisy log events can save <strong>thousands of dollars per month</strong> for mid-to-large deployments.</li><li>Reducing redundant traces and metrics prevents unnecessary storage in time-series databases, which can <strong>scale costs linearly with&nbsp;volume</strong>.</li></ul><ul><li><strong><em>CNCF (Cloud Native Computing Foundation) project</em></strong><em>, backed by major cloud providers like AWS, Azure, and Google&nbsp;Cloud.</em></li><li><em>OpenTelemetry Collector is </em><strong><em>over 4,500 GitHub stars and contributions from 1,200+ developers</em></strong></li><li><em>Widely adopted by enterprises like </em><strong><em>Netflix, Uber, Shopify, and Microsoft</em></strong><em>, proving its ability to handle </em><strong><em>hyper-scale distributed telemetry</em></strong></li></ul><pre>START: PRODUCTION INCIDENT DECLARED│<p>├── IMMEDIATE ACTIONS (First 5 minutes)</p>│   ├── Assemble incident response team<p>│   ├── Establish communication channels</p>│   ├── Begin user impact assessment<p>│   └── Start observability data collection</p>│<p>├── TRIAGE PHASE (Minutes 5-15)</p>│   ├── Symptom Classification<p>│   │   ├── Complete outage vs degraded performance?</p>│   │   ├── User impact scope and severity?<p>│   │   ├── Business process disruption level?</p>│   │   └── Revenue/customer impact estimation?│   ├── Initial Observability Check<p>│   │   ├── Are metrics/traces/logs flowing?</p>│   │   ├── Key health check status?<p>│   │   ├── Error rate and latency baselines?</p>│   │   └── Recent deployment correlation?│   └── Communication Setup<p>│       ├── Status page updates</p>│       ├── Internal stakeholder notifications<p>│       ├── Customer communication plan</p>│       └── Executive briefing preparation├── INVESTIGATION PHASE (Minutes 15-60)<p>│   ├── Data Collection and Analysis</p>│   │   ├── Gather all relevant observability data<p>│   │   ├── Correlate across metrics, traces, logs</p>│   │   ├── Identify anomaly patterns and timelines<p>│   │   └── Map service dependencies and impacts</p>│   │<p>│   ├── Hypothesis Generation</p>│   │   ├── Brainstorm potential root causes<p>│   │   ├── Prioritize by probability and impact</p>│   │   ├── Design verification tests for each hypothesis<p>│   │   └── Assign investigation tasks to team members</p>│   │<p>│   └── Deep Dive Analysis</p>│       ├── Trace analysis for performance bottlenecks<p>│       ├── Log analysis for error patterns</p>│       ├── Metric analysis for resource issues<p>│       └── Configuration and deployment history review</p>│<p>└── RESOLUTION PHASE (Ongoing)</p>    ├── Solution Implementation<p>    │   ├── Deploy fixes or workarounds</p>    │   ├── Verify resolution effectiveness<p>    │   ├── Monitor recovery progress</p>    │   └── Communicate resolution status    ├── Recovery Validation<p>    │   ├── Confirm user impact resolved</p>    │   ├── Verify system stability<p>    │   ├── Monitor for regression signs</p>    │   └── Conduct post-mortem preparation    └── Incident Closure<p>        ├── Document root cause and timeline</p>        ├── Identify improvement actions<p>        ├── Update runbooks and monitoring</p>        └── Share learnings across organization</pre><h4>3) Architecture &amp; Key Components</h4><p>OpenTelemetry Collector is : data flows through pipelines composed of <strong>receivers → processors → exporters</strong>, with optional .</p><h4>Receivers: The Ingestion Layer</h4><p> are the collector’s first point of contact with telemetry. They accept data in multiple protocols, normalise it, and forward it to processors.</p><ul><li>Receivers handle <strong>traces, metrics, and logs</strong> simultaneously.</li><li>They support a wide variety of protocols, including <strong>OTLP (gRPC and HTTP), Jaeger, Zipkin, Prometheus, FluentForward, and&nbsp;Kafka</strong>.</li><li>High-throughput environments may see <strong>millions of events per second</strong>. For example, a global e-commerce platform could generate <strong>over 50 million traces and 200 million metrics daily</strong>. Receivers must ingest these reliably.</li></ul><p><strong>Why receivers are critical:</strong></p><ol><li>They <strong>normalise diverse protocols</strong> into a common format for downstream processing.</li><li>They  during network spikes or backend outages by queuing or buffering telemetry.</li><li>They act as the <strong>first line of defense against malformed or malicious telemetry</strong>, ensuring pipeline stability.</li></ol><p><strong>Challenges and considerations:</strong></p><ul><li>How to <strong>scale receivers horizontally</strong> to handle sudden spikes in telemetry without dropping&nbsp;events.</li><li>How to <strong>monitor receiver performance</strong> itself — high latency or backlog in receivers can propagate problems downstream.</li><li>How to ensure <strong>end-to-end ordering and consistency</strong> of telemetry from multiple microservices.</li></ul><p>Configuration example&nbsp;(OTLP):</p><pre>receivers:  otlp:      grpc:      http:</pre><h4><strong>Processors: Transforming and Optimising Data</strong></h4><p> transform telemetry for efficiency, reliability, and consistency. They <strong>filter, batch, sample, enrich, or limit memory&nbsp;usage</strong>.</p><ul><li>Common processors: <strong>batch, memory_limiter, attributes, filter, sampling</strong>.</li><li>They reduce volume before sending data to backends, preventing <strong>storage and network overload</strong>.</li><li>Example: Netflix reports using sampling to <strong>drop 50–70% of non-critical traces</strong> while preserving key insights.</li></ul><ol><li> Without batching, millions of small telemetry events can overwhelm backends like Prometheus or Elasticsearch.</li><li> memory_limiter prevents the collector from consuming excessive RAM, which could crash Kubernetes pods under high&nbsp;load</li><li> attributes processors standardize tags like service, environment, and deployment, enabling cross-service correlation.</li></ol><p><strong>Questions operators face:</strong></p><ul><li>How do you balance  to reduce load but preserve alerting fidelity?</li><li>What is the impact of  on latency-sensitive telemetry?</li><li>How do processors handle  during events like Black Friday traffic or sudden microservice failures?</li></ul><pre>processors:  batch:    check_interval: 5s  attributes:      - key: \"deployment\"        action: insert</pre><h4>Exporters: Delivering Data to&nbsp;Backends</h4><p> are responsible for <strong>sending processed telemetry to storage and visualization backends</strong>. This is where data becomes actionable.</p><ul><li>Exporters support multiple destinations simultaneously: <strong>OTLP, Prometheus, Jaeger, Zipkin, Loki, and logging&nbsp;sinks</strong>.</li><li>Multi-backend strategies allow organisations to <strong>use the right tool for the right type of&nbsp;data</strong>.</li><li>They implement <strong>retry, backpressure handling, and timeout mechanisms</strong> to maintain reliability.</li></ul><ol><li>Telemetry is useless if it , so exporters must be <strong>highly reliable and fault-tolerant</strong>.</li><li>Multi-backend delivery requires  to prevent duplication or data&nbsp;loss.</li><li>Exporters must operate efficiently under load. A collector sending millions of spans per second cannot afford to be blocked by a slow&nbsp;backend.</li></ol><p><strong>Questions operators face:</strong></p><ul><li>How do exporters handle  without dropping critical&nbsp;data?</li><li>How do they <strong>balance speed and reliability</strong>?</li><li>How to  to detect silent failures?</li></ul><pre>exporters:  otlp:<p>    endpoint: \"otel-collector:4317\"</p>  prometheus:    namespace: \"myapp\"    logLevel: debug</pre><h4>Extensions: Supporting Observability of the Collector</h4><p>Extensions provide <strong>diagnostics, profiling, and health monitoring</strong> for the collector itself.</p><p><strong>health_check, pprof,&nbsp;zpages</strong>.</p><ul><li>Observability pipelines are software systems themselves — they can <strong>fail under load or misconfiguration</strong>.</li><li>Extensions allow operators to <strong>identify memory leaks, CPU spikes, or pipeline bottlenecks</strong> before they impact production telemetry.</li><li>They are vital in  where collectors are processing millions of events per&nbsp;second.</li></ul><p><strong>Questions operators face:</strong></p><ul><li>How do you monitor collector health without impacting performance?</li><li>Can extensions scale horizontally in a multi-node deployment?</li><li>How do you debug complex pipelines spanning thousands of services?</li></ul><pre>extensions:  health_check:<p>    endpoint: \"0.0.0.0:13133\"</p>  pprof:  zpages:<p>    endpoint: \"0.0.0.0:55679\"</p></pre><h4>4) Service &amp; Pipeline Configuration</h4><ul><li>Define pipelines per signal type (traces, metrics,&nbsp;logs).</li><li>Multiple pipelines can share receivers/exporters.</li></ul><pre>service:  extensions: [health_check, pprof, zpages]    traces:      processors: [batch, memory_limiter]    metrics:      processors: [batch, memory_limiter]</pre><h4>Why This Architecture is Hard to&nbsp;Solve</h4><ol><li>: Modern systems can generate <strong>millions of traces, metrics, and logs per second</strong>. Any pipeline must handle this without dropping&nbsp;data.</li><li><strong>Diverse formats and protocols</strong>: Normalising telemetry from multiple sources into a single pipeline requires careful engineering.</li><li>: Batching reduces backend load but increases delivery delay; sampling reduces volume but risks losing critical&nbsp;signals.</li><li><strong>Reliability and fault tolerance</strong>: Each stage must handle spikes, network failures, or backend downtime gracefully.</li><li>: The collector must remain lightweight in memory and CPU even under peak telemetry traffic.</li></ol><h4>5) Docker Deployment Considerations</h4><p>Deploying the OpenTelemetry Collector via Docker is the , but it requires careful planning to ensure reliability and performance.</p><ul><li>Use  such as otel/opentelemetry-collector-contrib:latest to ensure <strong>compatibility, security, and support for all receivers, processors, and exporters</strong>. Using custom or outdated images risks <strong>missing important bug fixes or telemetry features</strong>.</li><li>Mount your  into the container (-v&nbsp;./otel-collector-config.yaml:/etc/otelcol/config.yaml) so the collector can read pipelines at startup. This allows <strong>centralized versioned configurations</strong> and simplifies CI/CD deployments.</li><li>Use  to reload configuration without restarting the container. This avoids , which is critical in production environments where missing even a few minutes of data can lead to <strong>gaps in SLA monitoring or alerting</strong>.</li><li>Allocate . Telemetry pipelines are resource-intensive, especially when processing <strong>millions of events per second</strong>. Under-provisioning can cause <strong>queue backlogs, dropped telemetry, or collector crashes</strong>.</li><li>For high-throughput or large-scale deployments, run <strong>multiple Collector instances</strong> with load-balancing and redundancy to  and ensure .</li></ul><ul><li>Managing  without duplicating telemetry requires careful coordination of pipelines and sampling&nbsp;rules.</li><li>Containers are ephemeral; persistent state like queues or buffers must either be  or designed to tolerate transient loss.</li><li>Docker networks must allow <strong>connectivity to multiple backends</strong>, often across cloud providers, without causing latency&nbsp;spikes.</li></ul><h4>6) Modular Pipeline Recommendations</h4><p>Structuring pipelines correctly is critical for <strong>performance, maintainability, and observability accuracy</strong>.</p><ul><li><strong>Separate pipelines by signal</strong>: create distinct pipelines for <strong>traces, metrics, and logs</strong>. This prevents , allows tailored processing, and ensures scaling independently per telemetry type.</li><li><strong>Multiple receivers per pipeline</strong>: in fan-in scenarios, multiple receivers can feed a single pipeline. This is important when services use different protocols (e.g., OTLP, Jaeger, Zipkin) or when multi-cluster telemetry is aggregated.</li><li><strong>Fan-out to multiple exporters</strong>: allow a single pipeline to export telemetry to <strong>multiple backends simultaneously</strong>. For instance, traces might go to Jaeger for debugging and Datadog for dashboards. This <strong>reduces duplication of processing logic</strong> and ensures all systems have consistent telemetry.</li></ul><ul><li>Without proper separation, pipelines may , mixing high-frequency metrics with sparse logs or&nbsp;traces.</li><li>Fan-in/fan-out must be  to avoid <strong>overloading a single collector or&nbsp;backend</strong>.</li></ul><ul><li>Managing : if one exporter is slow, it can delay other telemetry in the same pipeline.</li><li>Configuring <strong>signal-specific processors</strong>: batch intervals and memory limits may need to differ for traces vs. metrics vs.&nbsp;logs.</li><li>Ensuring <strong>reliability across distributed collectors</strong> in a multi-instance deployment.</li></ul><h4>7) Debugging, Health, and Performance Tips</h4><p>A production-grade collector requires <strong>active monitoring and diagnostic tooling</strong> to maintain reliability.</p><ul><li>Enable <strong>health_check, pprof, and zpages extensions</strong>. These provide <strong>live metrics, profiling data, and debugging endpoints</strong>, which are essential to detect and resolve bottlenecks, memory leaks, or misconfigurations.</li><li>Use the <strong>Logging exporter during development</strong> to verify that telemetry flows correctly. This helps catch <strong>pipeline misconfigurations before hitting production backends</strong>.</li><li>Monitor collector  at the designated port (commonly 8888) to track throughput, batch sizes, memory usage, dropped telemetry, and latency. These metrics are critical for <strong>scaling decisions and troubleshooting</strong>.</li><li>Include <strong>memory_limiter and batch processors</strong> in pipelines to prevent the collector from exhausting system resources under high load. Proper configuration ensures <strong>stability even under sudden spikes</strong>, like Black Friday traffic or system-wide incidents.</li></ul><p>Validate configurations using:</p><pre>otelcol validate --config=&lt;file&gt;</pre><p>This catches <strong>syntax errors, missing fields, and invalid references</strong> before deployment, reducing downtime&nbsp;risk.</p><p><strong>Why this is hard to&nbsp;solve:</strong></p><ul><li>Telemetry pipelines themselves generate <strong>millions of events per second</strong>, so diagnosing slow pipelines or resource exhaustion can be difficult without these&nbsp;tools.</li><li>Memory, CPU, and network consumption must be balanced dynamically; misconfigured batch sizes or memory limits can <strong>cascade into dropped telemetry or backend overload</strong>.</li><li>Debugging in production often requires correlating <strong>collector metrics with application telemetry</strong>, which adds complexity in distributed environments.</li></ul><ul><li>Observability of the collector is as important as observability of your applications. If the collector fails silently, you lose visibility across your entire&nbsp;system.</li><li>Proper configuration of <strong>processors, extensions, and exporters</strong> ensures <strong>high reliability, low latency, and minimal resource&nbsp;usage</strong>.</li><li>Continuous monitoring and proactive tuning allow <strong>collectors to scale horizontally</strong> without introducing telemetry gaps or performance bottlenecks.</li></ul><h4>8) Hands-On: Run a Complete OpenTelemetry Stack&nbsp;Locally</h4><p>Below is a <strong>compact, practical, all-in-one setup</strong> for traces, metrics, and logs using Docker&nbsp;Compose.</p><ul><li><strong>OpenTelemetry Collector (contrib)</strong></li><li> (emits traces, metrics,&nbsp;logs)</li></ul><pre>START: Application Telemetry Collection├── DATA TYPE DECISION<p>│   ├── TRACES → OTLP Receiver → Attributes Processor → Batch → Jaeger + Logging</p>│   ├── METRICS → Prometheus Receiver → Batch → Prometheus Exporter<p>│   └── LOGS → OTLP Receiver → Attributes Processor → Batch → Loki + Logging</p>├── PROCESSING DECISION<p>│   ├── Memory Pressure? → Memory Limiter (700MiB limit)</p>│   ├── Batch Optimization? → Batch Processor (5s timeout)<p>│   └── Security/Sanitization? → Attributes Processor</p>└── EXPORT DECISION<p>    ├── Tracing Backend → Jaeger (14250)</p>    ├── Metrics Backend → Prometheus (8888)<p>    ├── Logging Backend → Loki (3100)</p>    └── Debug Output → Logging Exporter</pre><pre>START: Telemetry Generation│<p>├── CRITICAL PATH DECISION</p>│   ├── LATENCY SENSITIVE? <p>│   │   ├── YES → Fast Path: Direct OTLP → Regional Collector → Local Storage</p>│   │   └── NO → Reliable Path: OTLP → Kafka → Global Processing│   ├── DATA CRITICALITY<p>│   │   ├── MISSION-CRITICAL → Dual Write + Synchronous ACK</p>│   │   ├── BUSINESS-CRITICAL → Kafka with min.insync.replicas=3<p>│   │   └── DEBUG → Fire-and-Forget with Sampling</p>│   │<p>│   └── RETENTION REQUIREMENTS</p>│       ├── REAL-TIME (&lt;1h) → Memory-Optimized Pipeline<p>│       ├── SHORT-TERM (30d) → Cost-Optimized Pipeline  </p>│       └── LONG-TERM (&gt;30d) → Cold Storage Pipeline├── PROCESSING STRATEGY DECISION<p>│   ├── VOLUME PREDICTION</p>│   │   ├── &lt;10K EPS → Single Collector per Region<p>│   │   ├── 10K-100K EPS → Collector Pool with Load Balancer</p>│   │   └── &gt;100K EPS → Sharded Collectors by Service/Team│   ├── DATA ENRICHMENT NEEDED?<p>│   │   ├── YES → Attribute Processor + Custom Business Logic</p>│   │   └── NO → Minimal Processing Path│   └── COMPLIANCE REQUIREMENTS<p>│       ├── PII DETECTION → Automated PII Scrubbing Processor</p>│       ├── GDPR → Right-to-Erasure Pipeline<p>│       └── HIPAA → Audit Trail + Immutable Logging</p>│<p>└── STORAGE &amp; QUERY STRATEGY</p>    ├── QUERY PATTERNS<p>    │   ├── AD-HOC EXPLORATION → Columnar Storage (ClickHouse)</p>    │   ├── METRICS ALERTING → Time-Series Optimized (Prometheus/M3)<p>    │   └── LOG AGGREGATION → Full-Text Indexed (Elasticsearch/Loki)</p>    │        ├── HOT STORAGE (0-7 days) → SSD/High-Performance<p>        ├── WARM STORAGE (7-30 days) → Standard Storage</p>        └── COLD STORAGE (&gt;30 days) → Object Storage + Compression</pre><pre>otel-minimal/├─ docker-compose.yml│  └─ otel-collector-config.yaml│  └─ prometheus.yml│  ├─ provisioning/<p>│  │  ├─ datasources/datasource.yml</p>│  │  └─ dashboards/dashboard.yml<p>│  └─ dashboards/otel-dashboard.json</p>├─ python-app/│  ├─ Dockerfile├─ loki/<p>│  └─ (no extra config needed)</p>└─ README.md</pre><pre>version: \"3.8\"services:    image: otel/opentelemetry-collector-contrib:latest<p>    container_name: otel-collector</p>    volumes:<p>      - ./otel-collector/otel-collector-config.yaml:/etc/otelcol/config.yaml:ro</p>    command: [\"--config=/etc/otelcol/config.yaml\"]      - \"4317:4317\"   # OTLP gRPC receiver<p>      - \"4318:4318\"   # OTLP http receiver</p>      - \"8888:8888\"   # Collector prometheus exporter<p>      - \"13133:13133\" # health_check</p>      - \"55679:55679\" # zpages      - otel-net    image: jaegertracing/all-in-one:1.41    ports:      - \"14250:14250\"      - otel-net    image: prom/prometheus:latest<p>    container_name: prometheus</p>    volumes:<p>      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro</p>    ports:    networks:  loki:<p>    image: grafana/loki:2.7.3</p>    container_name: loki      - \"3100:3100\"<p>    command: -config.file=/etc/loki/local-config.yaml</p>    networks:  grafana:<p>    image: grafana/grafana:9.5.2</p>    container_name: grafana      - GF_SECURITY_ADMIN_USER=admin<p>      - GF_SECURITY_ADMIN_PASSWORD=admin</p>      - GF_USERS_ALLOW_SIGN_UP=false      - ./grafana/provisioning:/etc/grafana/provisioning:ro<p>      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro</p>    ports:    depends_on:      - loki    networks:  python-app:    container_name: python-app      - \"8000:8000\"      - OTEL_EXPORTER_OTLP_ENDPOINT=http://collector:4317<p>      - OTEL_EXPORTER_OTLP_INSECURE=true</p>    depends_on:    networks:networks:    driver: bridge</pre><p>2) Collector Config — otel-collector/otel-collector-config.yaml</p><pre>receivers:  otlp:      grpc:      http:  prometheus:      scrape_configs:          static_configs:<p>            - targets: ['python-app:8000']</p>  batch:    send_batch_size: 1024    check_interval: 5s    spike_limit_mib: 64    actions:        action: insert      - key: http.request.header.authorization  jaeger:    tls:  prometheus:    namespace: \"collector\"    endpoint: \"http://loki:3100/loki/api/v1/push\"    logLevel: info  extensions: [health_check, zpages]    traces:      processors: [memory_limiter, batch, attributes]<p>      exporters: [jaeger, logging]</p>    metrics:<p>      receivers: [prometheus, otlp]</p>      processors: [memory_limiter, batch]    logs:      processors: [memory_limiter, batch, attributes]<p>      exporters: [loki, logging]</p></pre><h4>3) Prometheus Config — prometheus/prometheus.yml</h4><pre>global:  scrape_interval: 15sscrape_configs:<p>  - job_name: 'otel-collector'</p>    static_configs:<p>      - targets: ['collector:8888']</p>  - job_name: 'python-app'      - targets: ['python-app:8000']</pre><p> prometheus/alert.rules.yml</p><pre>groups:  - name: example.rules      - alert: HighErrorRate<p>        expr: rate(http_requests_total{status=~\"5..\"}[2m]) &gt; 0.1</p>        for: 1m          severity: critical          summary: \"High HTTP 5xx error rate\"</pre><h4>4) Grafana Provisioning &amp; Dashboard</h4><p><strong>grafana/provisioning/datasources/datasource.yml</strong></p><pre>apiVersion: 1deleteDatasources: false  - name: Prometheus    access: proxy<p>    url: http://prometheus:9090</p>    isDefault: true    type: loki    url: http://loki:3100    type: jaeger    url: http://jaeger:16686</pre><p><strong>grafana/provisioning/dashboards/dashboard.yml</strong></p><pre>apiVersion: 1providers:<p>  - name: 'otel-dashboards'</p>    orgId: 1    type: file    editable: true      path: /var/lib/grafana/dashboards</pre><p><strong>grafana/dashboards/otel-dashboard.json</strong></p><pre>{  \"annotations\": {\"list\": []},    {      \"title\": \"HTTP requests (rate)\",<p>      \"gridPos\": {\"h\": 8,\"w\": 12,\"x\": 0,\"y\": 0},</p>      \"targets\": [{\"expr\": \"rate(http_requests_total[1m])\",\"refId\": \"A\",\"datasource\": \"Prometheus\"}]    {      \"title\": \"Recent logs (Loki)\",<p>      \"gridPos\": {\"h\": 8,\"w\": 12,\"x\": 12,\"y\": 0},</p>      \"targets\": [{\"expr\": \"{job=\\\"python-app\\\"}\",\"refId\": \"B\",\"datasource\": \"Loki\"}]    {      \"title\": \"Traces (Jaeger)\",<p>      \"gridPos\": {\"h\": 4,\"w\": 24,\"x\": 0,\"y\": 8},</p>      \"options\": {\"content\":\"Open Jaeger UI: http://localhost:16686\"}  ],  \"title\": \"OTel Minimal Overview\",}</pre><p>5) Python App — python-app/app.py</p><pre>from flask import Flaskimport logging, random, os<p>from prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST</p>from opentelemetry import trace<p>from opentelemetry.sdk.resources import Resource</p>from opentelemetry.sdk.trace import TracerProvider<p>from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter</p>from opentelemetry.sdk.trace.export import BatchSpanProcessorREQUESTS = Counter('http_requests_total', 'HTTP requests total', ['service', 'status'])logging.basicConfig(level=logging.INFO)<p>logger = logging.getLogger(\"python-app\")</p>resource = Resource.create({\"service.name\": \"python-demo-app\"})<p>tp = TracerProvider(resource=resource)</p>otlp_endpoint = os.environ.get(\"OTEL_EXPORTER_OTLP_ENDPOINT\", \"http://collector:4317\")<p>otlp_insecure = os.environ.get(\"OTEL_EXPORTER_OTLP_INSECURE\", \"true\").lower() == \"true\"</p>otlp_exporter = OTLPSpanExporter(endpoint=otlp_endpoint, insecure=otlp_insecure)<p>tp.add_span_processor(BatchSpanProcessor(otlp_exporter))</p>trace.set_tracer_provider(tp)<p>tracer = trace.get_tracer(__name__)</p>app = Flask(__name__)def index():<p>    with tracer.start_as_current_span(\"handle_request\"):</p>        if random.random() &lt; 0.1:            REQUESTS.labels(\"python-demo-app\", status).inc()<p>            logger.error(\"simulated error\")</p>            return \"error\", 500            status = \"200\"<p>            REQUESTS.labels(\"python-demo-app\", status).inc()</p>            logger.info(\"ok request\")@app.route(\"/metrics\")    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}<p>if __name__ == \"__main__\":</p>    app.run(host=\"0.0.0.0\", port=8000)</pre><p><strong>python-app/requirements.txt</strong></p><pre>flaskprometheus_clientopentelemetry-sdk<p>opentelemetry-exporter-otlp</p></pre><p>Dockerfile — python-app/Dockerfile</p><pre>FROM python:3.10-slimWORKDIR /appRUN pip install --no-cache-dir -r requirements.txtEXPOSE 8000</pre><pre>for i in {1..100}; do curl -s http://localhost:8000/ &gt;/dev/null; sleep 0.1; done</pre><p>Explore Grafana dashboards, check logs in Loki, traces in Jaeger, and metrics in Prometheus.</p><pre>APPLICATION LAYER (python-app:8000)    ↓<p>OTLP PROTOCOL (gRPC/HTTP @ 4317/4318)</p>    ↓<p>COLLECTOR CORE PROCESSING</p>    ├── RECEIVERS: Multi-protocol ingestion    │   ├── MEMORY_LIMITER: 700MiB cap + 64MiB spike    │   │   ├── INSERT: environment=demo<p>    │   │   └── DELETE: http.request.header.authorization</p>    │   └── BATCH: 5s timeout, 1024 batch size<p>    └── EXPORTERS: Multi-backend routing</p>        ├── TRACES → Jaeger:14250<p>        ├── METRICS → Prometheus:8888  </p>        ├── LOGS → Loki:3100<p>        └── DEBUG → Console logging</p></pre><pre>┌─────────────────┐    OTLP gRPC/HTTP    ┌──────────────────┐    Export Protocols    ┌─────────────────┐│   PYTHON-APP    │ ────────────────────►│   COLLECTOR      │ ────────────────────►  │    BACKENDS     │<p>│                 │     4317/4318        │                  │                        │                 │</p>│ • Instrumented  │                      │ • Receivers:     │ ────TRACES───────►     │ • JAEGER        │<p>│ • OTLP enabled  │ ◄────────────────────│   - OTLP         │                        │   (16686 UI)    │</p>│ • Port: 8000    │   Scrape Metrics     │   - Prometheus   │ ────METRICS──────►     │ • PROMETHEUS    │<p>└─────────────────┘                      │ • Processors:    │   (8888)               │   (9090 UI)     │</p>                                         │   - Batch        │                        │ • LOKI          │<p>┌─────────────────┐                      │   - MemoryLimit  │ ────LOGS─────────►     │   (3100)        │</p>│   ADMIN/UI      │                      │   - Attributes   │                        │ • GRAFANA       │<p>│                 │   Monitoring Ports   │ • Exporters:     │                        │   (3000 UI)     │</p>│ • Grafana:3000  │ ◄────────────────────│   - Jaeger       │                        └─────────────────┘<p>│ • Jaeger:16686  │   8888/13133/55679   │   - Prometheus   │</p>│ • Prom:9090     │                      │   - Loki         │<p>└─────────────────┘                      │   - Logging      │</p>                                         └──────────────────┘</pre><h4>Advanced networking topology for global&nbsp;scale</h4><pre>┌─────────────────────────────────────────────────────────────────────────────────┐│                              GLOBAL CONTROL PLANE                               │<p>├─────────────────┬─────────────────┬─────────────────┬───────────────────────────┤</p>│   CONFIG MGMT   │  SERVICE MESH   │    GLOBAL LB    │     CERTIFICATE AUTH      │<p>│   (Consul)      │   (Istio)       │    (Envoy)      │        (Vault)            │</p>└─────────────────┴─────────────────┴─────────────────┴───────────────────────────┘┌─────────────────────────────────────────────────────────────────────────────────┐<p>│                            REGIONAL DATA PLANES (10+)                           │</p>├──────────────┬──────────────┬──────────────┬──────────────┬──────────────────────┤<p>│   US-EAST    │   US-WEST    │    EU-CENT   │   AP-SOUTH   │       ...            │</p>└──────────────┴──────────────┴──────────────┴──────────────┴──────────────────────┘<p>              ↓                  ↓                  ↓                  ↓</p>    ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐<p>    │   APP CLUSTER   │  │   APP CLUSTER   │  │   APP CLUSTER   │  │   APP CLUSTER   │</p>    │ • Service A     │  │ • Service A     │  │ • Service A     │  │ • Service A     │<p>    │ • Service B     │  │ • Service B     │  │ • Service B     │  │ • Service B     │</p>    │ • OTLP Agents   │  │ • OTLP Agents   │  │ • OTLP Agents   │  │ • OTLP Agents   │<p>    └─────────────────┘  └─────────────────┘  └─────────────────┘  └─────────────────┘</p>              ↓                  ↓                  ↓                  ↓<p>    ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐</p>    │ REGIONAL KAFKA  │  │ REGIONAL KAFKA  │  │ REGIONAL KAFKA  │  │ REGIONAL KAFKA  │<p>    │ • traces-{shard}│  │ • traces-{shard}│  │ • traces-{shard}│  │ • traces-{shard}│</p>    │ • metrics-{shard}│ │ • metrics-{shard}│ │ • metrics-{shard}│ │ • metrics-{shard}│<p>    │ • logs-{shard}  │  │ • logs-{shard}  │  │ • logs-{shard}  │  │ • logs-{shard}  │</p>    └─────────────────┘  └─────────────────┘  └─────────────────┘  └─────────────────┘<p>              ↓                  ↓                  ↓                  ↓</p>    ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐<p>    │  COLLECTOR FLEET│  │  COLLECTOR FLEET│  │  COLLECTOR FLEET│  │  COLLECTOR FLEET│</p>    │ • 50+ instances │  │ • 50+ instances │  │ • 50+ instances │  │ • 50+ instances │<p>    │ • Auto-scaling  │  │ • Auto-scaling  │  │ • Auto-scaling  │  │ • Auto-scaling  │</p>    │ • Multi-tenant  │  │ • Multi-tenant  │  │ • Multi-tenant  │  │ • Multi-tenant  │<p>    └─────────────────┘  └─────────────────┘  └─────────────────┘  └─────────────────┘</p>              ↓                  ↓                  ↓                  ↓<p>    ┌─────────────────────────────────────────────────────────────────────────────────┐</p>    │                          GLOBAL AGGREGATION LAYER                              │<p>    ├──────────────┬──────────────┬──────────────┬──────────────┬──────────────────────┤</p>    │  CENTRALIZED │   METRICS    │    TRACES    │     LOGS     │    COMPLIANCE       │<p>    │    KAFKA     │  AGGREGATION │  AGGREGATION │  AGGREGATION │      AUDIT          │</p>    │              │   (M3DB)     │  (Jaeger)    │ (Elastic)    │     (Immuta)        │<p>    └──────────────┴──────────────┴──────────────┴──────────────┴──────────────────────┘</p></pre><pre># YEAR 1-2: EARLY GROWTH PHASE (1-10M USERS)PredictedFailures:    Scenario: \"Black Friday Traffic Spike\"<p>    Impact: 15% data loss during peak hours</p>    RootCause: Collector memory limits too aggressive<p>    BusinessCost: $2.3M in lost transaction visibility</p>    Scenario: \"Regional Network Outage\"<p>    Impact: Cross-region replication backlog of 4 hours</p>    RootCause: Synchronous cross-DC writes<p>    RecoveryTime: 6 hours to clear backlog</p><p># YEAR 3-5: HYPERGROWTH PHASE (10-100M USERS)  </p>CriticalBottlenecks:    Scenario: \"Service Mesh Adoption\"<p>    Impact: Topic partition count exceeds Kafka limits (2000/broker)</p>    Symptoms: Producer backpressure, increased latency<p>    Solution: Multi-cluster Kafka with federation</p>    Scenario: \"Microservices Proliferation (500+ services)\"<p>    Impact: Storage costs grow 300% quarter-over-quarter  </p>    DataVolume: 15TB/day telemetry data<p>    Solution: Aggressive sampling + tiered storage</p><p># YEAR 5+: PLANET SCALE (100M-1B+ USERS)</p>ExistentialThreats:    Scenario: \"Multi-Cloud Deployment (AWS + GCP + Azure)\"<p>    Impact: Inconsistent query results across clouds</p>    LatencyVariance: 200ms-2s query times<p>    Solution: Global query federation layer</p>    Scenario: \"GDPR + CCPA + PIPL Compliance\"<p>    Impact: Data sovereignty requires 40+ regional storage silos</p>    Complexity: Different retention policies per jurisdiction<p>    Solution: Automated data governance engine</p></pre><h4>2. Security Vulnerability</h4><pre># IMMEDIATE THREATS (Months 0-6)CriticalVulnerabilities:    AttackVector: \"Direct data injection via public endpoints\"<p>    Risk: Data poisoning + metric manipulation</p>    BusinessImpact: False alerts causing engineer fatigue<p>    RealWorldCase: \"Coinbase observability breach 2023\"</p>    Weakness: \"Self-signed certificates in production\"<p>    Risk: Man-in-the-middle attacks on telemetry data</p>    Compliance: Fails SOC2, HIPAA, PCI-DSS requirements<p># STRATEGIC THREATS (Years 1-3)</p>AdvancedPersistentThreats:    Vector: \"Malicious OpenTelemetry exporters\"<p>    Impact: Exfiltration of business metrics to attackers</p>    Detection: Nearly impossible without runtime attestation    Scenario: \"SaaS offering with 1000+ tenants\"<p>    Risk: Cross-tenant data leakage via collector misconfiguration</p>    RealWorldCase: \"Datadog tenant isolation breach 2024\"<p># EXISTENTIAL SECURITY RISKS (Years 3+)</p>NationStateLevelThreats:    Threat: \"DDoS via telemetry data injection\"<p>    Method: Malicious clients sending 1TB/day garbage data</p>    Impact: $500K/month cloud cost explosion<p>    Defense: AI-based anomaly detection + rate limiting</p>    Scenario: \"Multi-jurisdictional data sovereignty violations\"<p>    Fines: Up to 4% global revenue (GDPR)</p>    Solution: Automated data localization + deletion workflows</pre><pre># TACTICAL GAPS (Immediate Impact)ImmediateBlindSpots:    Impact: \"We don't know when we're not collecting\"<p>    RealWorldFailure: \"8-hour outage undetected because collectors were down\"</p>    BusinessCost: $1.2M in lost revenue    Missing: End-to-end data lineage tracking<p>    Symptom: \"Why are my metrics delayed by 30 minutes?\"</p>    RootCause: Kafka consumer lag undetected for weeks<p># STRATEGIC GAPS (Year 1-2)</p>ArchitecturalBlindSpots:<p>  BusinessContextCorrelation:</p>    Gap: \"Cannot correlate infrastructure metrics to business KPI\"<p>    Example: \"Why did 5% increase in latency decrease revenue by 2%?\"</p>    Solution: Business context injection pipeline<p>  CostPerQueryOptimization:</p>    Problem: \"Exploratory queries cost $5000 in cloud spend\"<p>    RealWorld: \"Uber's $1.2M monthly observability bill\"</p>    Solution: Query budget enforcement + optimization<p># EXISTENTIAL GAPS (Years 3+)</p>DigitalTransformationBlockers:    Gap: \"Data quality insufficient for machine learning\"<p>    Impact: Cannot implement predictive alerting</p>    Requirement: 99.9% data completeness + accuracy<p>  MultiModalQueryFederation:</p>    Challenge: \"Unified query across traces, metrics, logs\"<p>    CurrentState: 3 different query languages + UIs</p>    FutureNeed: Natural language query interface</pre><h4>Tuning &amp; Dynamic Config&nbsp;Notes</h4><ul><li>YAML supports ${env:VAR} for dynamic endpoints/secrets.</li><li>Send SIGHUP to reload Collector config: docker kill -s SIGHUP otel-collector.</li><li>Production considerations: split pipelines, scale Collector, secure endpoints, use persistent backends.</li></ul><h4>9) Final Tips &amp; Next&nbsp;Steps</h4><h4>Data pipeline resilience path</h4><pre># PHASE 1: Foundation (Months 0-12)App → Regional Collector → Kafka → Processing → Backends<p>    └── Dead Letter Queue (S3) for 1% failure tolerance</p><p># PHASE 2: Global Scale (Years 1-2)  </p>App → Service Mesh Sidecar → Regional Kafka → Collector Fleet → <p>    ├── Hot Path: Real-time processing (&lt;1s latency)</p>    ├── Warm Path: Batch processing (&lt;5m latency) <p>    └── Cold Path: Archive processing (&lt;1h latency)</p><p># PHASE 3: Planet Scale (Years 3-5)</p>App → Edge Processing → Global Event Bus → AI-Driven Routing →<p>    ├── Critical Data: Multi-region synchronous replication</p>    ├── Important Data: Eventually consistent global aggregation<p>    └── Debug Data: Regional storage with cross-query capability</p><p># PHASE 4: Autonomous Operations (Years 5+)</p>Self-Healing Pipeline with:<p>    • Predictive auto-scaling</p>    • Automated data quality enforcement  <p>    • Cost-aware routing decisions</p>    • Zero-touch compliance management</pre><h4>2. Security posture enhancement</h4><pre># LAYER 1: NETWORK SECURITYZeroTrustNetwork:<p>  - Service-to-service mTLS with automatic certificate rotation</p>  - Network policies: Default deny, explicit allow rules<p>  - Segmentation: Separate planes for control, data, management</p>  - Egress filtering: All outbound traffic proxied and inspected<p># LAYER 2: IDENTITY &amp; ACCESS MANAGEMENT</p>IdentityFederation:<p>  - OAuth2.0 + OIDC for user authentication</p>  - SPIFFE/SPIRE for workload identity<p>  - RBAC with attribute-based access control (ABAC)</p>  - Just-in-time access with time-bound credentialsEndToEndEncryption:<p>  - Field-level encryption for PII data</p>  - Key management: HSM-backed with quarterly rotation<p>  - Tokenization for sensitive business metrics</p>  - Automated data classification and labeling<p># LAYER 4: COMPLIANCE &amp; GOVERNANCE</p>AutomatedCompliance:<p>  - Policy-as-code with OPA/Rego</p>  - Continuous compliance monitoring<p>  - Automated audit trail generation</p>  - Data sovereignty enforcement engine</pre><pre>SCALING TRIGGERS &amp; RESPONSES:├── MEMORY PRESSURE<p>│   ├── &gt;60% utilization → Scale up 25% more resources</p>│   ├── &gt;80% utilization → Scale out +50% instances<p>│   └── &gt;90% utilization → Emergency scale out +200% + traffic shedding</p>││   ├── P95 &gt; 1s → Optimize batch sizes + parallelize processing<p>│   ├── P95 &gt; 5s → Add read replicas + caching layers</p>│   └── P95 &gt; 10s → Implement circuit breakers + load shedding├── COST OPTIMIZATION<p>│   ├── Storage growth &gt; 20%/month → Implement tiered storage</p>│   ├── Query cost &gt; $1000/day → Add query budget controls<p>│   └── Cross-region traffic &gt; $50K/month → Implement regional aggregation</p>│<p>└── RELIABILITY INDICATORS</p>    ├── Data loss &gt; 0.1% → Enable synchronous replication<p>    ├── Pipeline lag &gt; 5min → Scale consumers + optimize partitions</p>    └── Error rate &gt; 1% → Enable dead letter queues + retry with backoff</pre><ul><li>This minimal repo is fully functional yet&nbsp;small.</li><li>You can extend it with Alertmanager, richer dashboards, or multiple Collector instances.</li><li>Validate configs and monitor internal metrics to ensure pipeline&nbsp;health.</li></ul><p>This setup is perfect for <strong>development, learning, or demos</strong>, giving full visibility into <strong>traces, metrics, and logs</strong> with a single&nbsp;command.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=da47b68bb89e\" width=\"1\" height=\"1\" alt=\"\">","contentLength":62324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Production-Grade Machine Learning Through MLOps","url":"https://blog.devops.dev/production-grade-machine-learning-through-mlops-cc4286c7d37d?source=rss----33f8b2d9a328---4","date":1761570442,"author":"Caamanno","guid":316776,"unread":true,"content":"<p>Machine learning has evolved from experimental notebooks to mission-critical business infrastructure. Yet a staggering reality persists: only 13% of machine learning projects successfully reach production. The gap between training a model and deploying it reliably at scale represents one of the most significant challenges facing modern enterprises.</p><p>This isn’t a technical problem — it’s an operational one. The models themselves often work. What fails is the infrastructure, processes, and culture needed to maintain them in dynamic production environments where data shifts, requirements evolve, and business stakes escalate&nbsp;daily.</p><h3>Why Traditional Software Practices Fall Short for Machine&nbsp;Learning</h3><p>Traditional software deployment follows a relatively predictable pattern: write code, test it, deploy it, monitor for errors. Machine learning systems introduce fundamental complexity that breaks this&nbsp;model.</p><p>Unlike traditional software where the behavior is explicitly programmed, ML systems learn their behavior from data. This creates three critical axes of change that traditional DevOps wasn’t designed to handle: the  implementing the model, the  training it, and the  itself — each capable of changing independently and creating cascading effects throughout the&nbsp;system.</p><p>Consider a fraud detection model deployed by a financial institution. The code might remain unchanged, but shifts in fraudster behavior alter the data distribution. The model’s performance degrades silently, potentially costing millions before detection. Traditional monitoring catches code errors but misses this  — a phenomenon unique to ML systems where the statistical properties underlying predictions change over&nbsp;time.</p><p>This is where <strong>Machine Learning Operations (MLOps)</strong> emerges as a discipline distinct from DevOps, addressing the unique challenges of productionizing intelligent systems.</p><h3>Understanding MLOps: Beyond DevOps for Intelligent Systems</h3><p>MLOps represents the convergence of machine learning, software engineering, and operations, creating a systematic approach to deploying and maintaining ML models in production environments. But what distinguishes it from traditional operational practices?</p><h3>The MLOps Lifecycle: More Than Just Deployment</h3><p>The MLOps lifecycle extends far beyond the traditional deploy-and-forget model:</p><p><strong>Data Management and Engineering:</strong> Unlike traditional software, ML systems are data-dependent artifacts. Data quality, versioning, and lineage tracking become first-class concerns. Organizations must implement robust data pipelines that ensure consistency, handle drift, and maintain auditability.</p><p><strong>Model Development and Experimentation:</strong> Data scientists require frameworks for systematic experimentation, including experiment tracking, hyperparameter optimization, and reproducibility. Tools like MLflow and Weights &amp; Biases have emerged to address these needs, enabling teams to track thousands of experiments and their associated metrics.</p><p><strong>Continuous Integration for ML:</strong> Traditional CI focuses on code integration and testing. ML CI extends this to include , , and  across the entire pipeline. This means validating not just that code runs, but that models maintain expected performance characteristics and handle edge cases appropriately.</p><p><strong>Continuous Delivery and Deployment:</strong> CD in MLOps automates the path from trained model to production inference service. This involves packaging models with their dependencies, orchestrating multi-stage deployments, implementing canary releases, and managing rollbacks when performance degrades.</p><p><strong>Monitoring and Observability:</strong> Production ML requires monitoring dimensions that don’t exist in traditional software: , , , and <strong>feature distribution shifts</strong>. These metrics determine whether a model remains valid for its operational context.</p><p><strong>Model Governance and Compliance:</strong> Regulated industries require comprehensive model governance frameworks that document model lineage, validate fairness and bias, ensure explainability, and maintain audit trails for regulatory scrutiny.</p><h3>MLOps Maturity: A Framework for Assessment</h3><p>Organizations exist at different stages of MLOps maturity, from ad-hoc manual processes to fully automated, self-healing systems.</p><p><strong>Level 0 — Manual Process:</strong> Data scientists work in isolation, manually training models in notebooks. Deployment is a handoff to engineering teams who manually integrate models into production systems. No standardized processes exist for versioning, monitoring, or retraining. Model updates take weeks or&nbsp;months.</p><p><strong>Level 1 — Pipeline Automation:</strong> Teams implement automated training pipelines with experiment tracking. Continuous training enables more frequent model updates. However, deployment remains largely manual, and monitoring is limited to basic performance metrics.</p><p> Full automation of the ML lifecycle from data validation through deployment. Automated testing validates models before production release. Comprehensive monitoring detects drift and triggers retraining. Model registries track versions and lineage. Governance frameworks ensure compliance and auditability.</p><p>The journey from Level 0 to Level 2 typically takes organizations 12–24 months and requires investment in infrastructure, tooling, and organizational change.</p><h3>Building the Foundation: Core MLOps Components</h3><p>Effective MLOps requires assembling a technology stack that addresses each stage of the ML lifecycle. While specific tools vary by organizational context, certain capabilities are non-negotiable.</p><h3>Data Versioning and Feature&nbsp;Stores</h3><p>Just as code requires version control, ML systems demand . Tools like DVC (Data Version Control) and LakeFS enable teams to track dataset versions, reproduce experiments, and trace model predictions back to specific training&nbsp;data.</p><p> have emerged as critical infrastructure for production ML, providing a centralized repository for feature definitions and serving. They solve several problems simultaneously:</p><ul><li> Features computed identically for training and inference, eliminating training-serving skew</li><li> Features shared across models, reducing duplication</li><li> Real-time feature computation for low-latency predictions</li><li><strong>Point-in-time correctness:</strong> Historical feature values for accurate&nbsp;training</li></ul><p>Organizations like Uber and Airbnb credit feature stores with accelerating model development by 3–5x while reducing feature-related bugs by over&nbsp;80%.</p><h3>Experiment Tracking and Model&nbsp;Registry</h3><p>Data scientists typically experiment with dozens or hundreds of model variations before selecting one for production. <strong>Experiment tracking platforms</strong> capture hyperparameters, metrics, artifacts, and code versions for each run, enabling teams to compare results and reproduce successful experiments.</p><p>The  extends this concept to production, serving as the source of truth for deployed models. It&nbsp;tracks:</p><ul><li>Model versions and&nbsp;lineage</li><li>Performance metrics and validation results</li><li>Deployment status and approval workflows</li><li>Associated artifacts (model files, preprocessing code, dependencies)</li></ul><p>Netflix’s model registry manages over 5,000 production models, processing 10 million predictions per second at peak load. Without this centralized tracking, managing models at scale becomes untenable.</p><h3>CI/CD Pipelines for Machine&nbsp;Learning</h3><p>Continuous integration and deployment for ML extends traditional software practices to accommodate the unique characteristics of intelligent systems.</p><p> for ML encompasses:</p><ul><li> Unit tests for data processing, feature engineering, and model&nbsp;code</li><li> Schema validation, distribution checks, and anomaly detection</li><li> Performance benchmarks, bias detection, and robustness checks</li><li> End-to-end pipeline validation from raw data to predictions</li></ul><p> automates the path to production through multi-stage pipelines:</p><ol><li> Model deployed to staging environment resembling production</li><li> Model generates predictions alongside production model without affecting users</li><li> Gradual rollout to increasing traffic percentages</li><li> Complete traffic migration after validation</li><li> Automated rollback if performance thresholds violated</li></ol><p>Google’s TFX (TensorFlow Extended) platform exemplifies this approach, enabling thousands of models to be deployed daily across Google’s products through automated validation and progressive rollout strategies.</p><h3>Model Monitoring and Observability</h3><p>Production ML systems require monitoring dimensions beyond traditional application metrics:</p><p> Statistical tests detect when input feature distributions diverge from training data. The Kolmogorov-Smirnov test, Population Stability Index (PSI), and Jensen-Shannon divergence quantify distribution shifts.</p><p> Changes in model output distributions may indicate model degradation even when inputs appear&nbsp;stable.</p><p> Ground truth labels often arrive delayed in production. Proxy metrics (confidence scores, prediction latency) provide early warning&nbsp;signals.</p><p> The relationship between features and target variable evolves over time, requiring model retraining. Detecting this without labeled data requires sophisticated techniques including confidence-based drift detection and adversarial validation.</p><p>Amazon’s SageMaker Model Monitor exemplifies comprehensive monitoring, automatically detecting data drift, concept drift, bias drift, and feature attribution drift in real-time, providing alerts that trigger automated retraining workflows.</p><h3>Real-World MLOps: Case Studies from Industry&nbsp;Leaders</h3><p>Theory illuminates, but practice convinces. Examining how leading technology companies implement MLOps reveals patterns worth emulating and pitfalls worth avoiding.</p><h3>Netflix: Metaflow and the Quest for Data Scientist Productivity</h3><p>Netflix deploys machine learning across virtually every aspect of its platform, from personalized recommendations to content thumbnail optimization to streaming quality adjustment.</p><p> Netflix data scientists were spending 80% of their time on operational concerns — managing infrastructure, debugging pipelines, and coordinating deployments — rather than improving models.</p><p> Netflix built Metaflow, an open-source MLOps framework designed to maximize data scientist productivity. Metaflow provides:</p><ul><li> ML pipelines defined as directed acyclic graphs (DAGs) with automatic dependency management</li><li> Seamless scaling from laptop to AWS cloud infrastructure</li><li> Automatic tracking of code, data, and results for every experiment</li><li> One-click deployment to production through integration with Netflix’s Meson scheduling engine</li></ul><p> Model development velocity increased by 10x, with models progressing from idea to production in days rather than months. Netflix now manages thousands of production models with a relatively small ML engineering team.</p><p> Investing in developer experience yields exponential returns in model iteration speed and&nbsp;quality.</p><h3>Uber: Michelangelo and the Platform&nbsp;Approach</h3><p>Uber’s business fundamentally depends on real-time ML predictions — dynamic pricing, fraud detection, route optimization, and ETA prediction all rely on models making millions of decisions per&nbsp;second.</p><p> Uber needed to scale from dozens to thousands of models while maintaining low latency and high reliability. Teams were building redundant infrastructure and struggling with consistency.</p><p> Uber developed Michelangelo, an end-to-end ML platform that standardizes the ML lifecycle:</p><ul><li> Centralized feature computation and serving with consistency guarantees</li><li> Distributed training across CPUs and GPUs with automatic resource allocation</li><li> Containerized serving with auto-scaling and load balancing</li><li> Real-time performance tracking with automated alerting</li></ul><p> Michelangelo supports over 5,000 models in production, handling 10 million predictions per second. Time from model idea to deployment decreased by 90%, and model reliability improved dramatically.</p><p> Platform thinking — building reusable infrastructure rather than solving problems repeatedly — enables scaling ML across organizations.</p><h3>Google: TFX and the Industrial ML&nbsp;Pipeline</h3><p>Google pioneered production ML at scale, deploying thousands of models across search, ads, Gmail, and&nbsp;YouTube.</p><p> Maintaining thousands of models across diverse products while ensuring quality, compliance, and efficiency.</p><p> Google developed TensorFlow Extended (TFX), a production-scale ML platform:</p><ul><li> Ingests training data from diverse sources with validation</li><li><strong>StatisticsGen and SchemaGen:</strong> Analyzes data and generates schemas automatically</li><li> Feature engineering with consistency between training and&nbsp;serving</li><li> Distributed model training with hyperparameter tuning</li><li> Validates model quality before deployment</li><li> Deploys validated models to serving infrastructure</li></ul><p> TFX enables Google to deploy models with unprecedented reliability and efficiency. The platform handles billions of predictions daily while maintaining strict quality and latency requirements.</p><p> End-to-end automation of the ML pipeline, from data validation to deployment, is essential for operating ML at&nbsp;scale.</p><h3>Airbnb: Data Quality as the Foundation</h3><p>Airbnb’s recommendation and pricing models directly impact billions in revenue, making data quality paramount.</p><p> Poor data quality was causing model failures and eroding trust in ML&nbsp;systems.</p><p> Airbnb invested heavily in data infrastructure:</p><ul><li><strong>Automated data validation:</strong> Airflow pipelines validate data quality at ingestion</li><li> Comprehensive metadata and lineage tracking for all&nbsp;datasets</li><li><strong>Near real-time pipelines:</strong> Processing over 50 GB of data daily on AWS&nbsp;EMR</li></ul><p> Recommendation match rates improved significantly, dynamic pricing optimized occupancy rates, and model reliability increased dramatically.</p><p> Data quality infrastructure is not a nice-to-have — it’s the foundation upon which reliable ML systems are&nbsp;built.</p><h3>MLOps Metrics and Monitoring: Beyond Model&nbsp;Accuracy</h3><p>Model accuracy, while important, tells an incomplete story in production environments. Comprehensive MLOps requires tracking metrics across multiple dimensions.</p><h3>Model Performance Metrics</h3><p> Traditional metrics (accuracy, precision, recall, F1, AUC-ROC) remain relevant but must be monitored continuously rather than computed&nbsp;once.</p><p> Are predicted probabilities accurate? A model predicting 70% probability should be correct 70% of the time. Calibration plots and Brier scores quantify&nbsp;this.</p><p> Do model predictions exhibit bias across demographic groups? Metrics like demographic parity, equal opportunity, and disparate impact quantify fairness.</p><p> Percentage of required features present. Missing data can silently degrade model performance.</p><p> Do features follow expected patterns? Schema validation catches type mismatches and constraint violations.</p><p> Population Stability Index (PSI), Kullback-Leibler divergence, and Characteristic Stability Index (CSI) quantify changes in feature distributions.</p><p> Time from request to prediction. P50, P95, and P99 latencies characterize system performance.</p><p> Predictions per second. Capacity planning requires understanding throughput under various load patterns.</p><p> CPU, memory, and GPU usage. Optimization opportunities emerge from utilization patterns.</p><p> Prediction failures, timeouts, and exceptions. Error budgets define acceptable failure&nbsp;rates.</p><p>Ultimately, ML systems exist to drive business value. Tracking business KPIs alongside technical metrics connects model performance to outcomes:</p><ul><li> For recommendation systems</li><li> For churn prediction</li><li> For fraud detection systems</li><li> For optimization systems</li></ul><p>Netflix tracks over 100 metrics for each model, ranging from prediction latency to customer engagement, creating comprehensive visibility into model health and business&nbsp;impact.</p><h3>Building Your MLOps Stack: Tools and Platforms</h3><p>The MLOps ecosystem has exploded with hundreds of tools addressing different aspects of the ML lifecycle. Selecting the right combination requires understanding your organization’s specific needs, constraints, and maturity&nbsp;level.</p><p><strong>Vertex AI (Google Cloud):</strong> Comprehensive platform for training, deploying, and managing models on Google Cloud infrastructure. Strong integration with BigQuery for data analytics. Best for organizations already invested in&nbsp;GCP.</p><p> AWS’s managed platform offering feature stores, model registry, pipelines, and deployment capabilities. Extensive ecosystem integration. Ideal for AWS-centric organizations.</p><p> Microsoft’s platform with strong enterprise features, including AutoML, MLOps capabilities, and compliance tools. Best fit for organizations using Microsoft cloud infrastructure.</p><p> Unified analytics platform combining data engineering and ML. Excellent for organizations with complex data processing requirements.</p><h3>Model Development and Experimentation</h3><p> Open-source platform for experiment tracking, model packaging, and deployment. Framework-agnostic and widely&nbsp;adopted.</p><p> Experiment tracking with advanced visualization, hyperparameter optimization, and collaboration features. Popular with research&nbsp;teams.</p><p> ML metadata store with comprehensive tracking, versioning, and collaboration capabilities.</p><p> Open-source feature store with real-time and batch serving capabilities.</p><p> Enterprise feature platform built by Uber’s Michelangelo team. Strong governance and operational features.</p><p> Open-source feature store with Python-first interface and comprehensive versioning.</p><p> High-performance serving system for TensorFlow models with version management and batching.</p><p> Model serving framework for PyTorch models with multi-model serving and&nbsp;metrics.</p><p> Framework-agnostic model serving with easy deployment to various platforms.</p><p> Kubernetes-native platform for deploying, scaling, and monitoring models.</p><h3>Monitoring and Observability</h3><p> Open-source library for monitoring data drift, model drift, and data&nbsp;quality.</p><p> Comprehensive ML observability platform with automated drift detection and explainability.</p><p> Model performance monitoring with explainability and fairness capabilities.</p><p> Data and ML monitoring with privacy-preserving techniques.</p><p> Kubernetes-native platform for orchestrating ML workflows with extensive component ecosystem.</p><p> General-purpose workflow orchestration tool widely used for ML pipelines.</p><p> Modern workflow orchestration with dynamic workflows and failure handling.</p><p>The optimal stack depends on organizational factors: cloud provider, team skills, budget, compliance requirements, and scale. Most successful implementations start with minimal tooling and incrementally add capabilities as needs&nbsp;emerge.</p><h3>Model Governance and Compliance: Navigating the Regulatory Landscape</h3><p>As ML systems make increasingly consequential decisions — loan approvals, medical diagnoses, hiring recommendations — regulatory scrutiny intensifies. Organizations must implement governance frameworks that ensure compliance while maintaining agility.</p><p> The Federal Reserve’s SR 11–7 guidance establishes model risk management requirements for financial institutions. Models must undergo independent validation, continuous monitoring, and regular review. Documentation, testing, and governance processes must be comprehensive and auditable.</p><p> HIPAA requires safeguarding patient data used in ML systems. FDA guidance addresses AI/ML-based medical devices, requiring continuous validation and monitoring. The EU Medical Device Regulation imposes additional requirements.</p><p> The EU AI Act classifies AI systems by risk level and imposes requirements for high-risk systems, including transparency, human oversight, and robustness. California’s proposed AI regulations would require impact assessments for high-risk systems.</p><h3>Governance Framework Components</h3><p> Comprehensive catalog of all models in development and production, including metadata on purpose, risk level, and compliance requirements.</p><p> Systematic evaluation of model risks considering usage, complexity, and potential impact. High-risk models require enhanced controls and validation.</p><p> Independent review of model development, implementation, and performance by qualified personnel separate from development teams.</p><p> Ongoing surveillance of model performance, data quality, and compliance with established thresholds. Automated alerts trigger review when thresholds violated.</p><p><strong>Documentation and Audit Trails:</strong> Comprehensive documentation of model development decisions, validation findings, and operational performance. Audit trails enable reconstruction of any prediction.</p><p><strong>Bias and Fairness Testing:</strong> Systematic evaluation of model outputs across demographic groups. Mitigation strategies when disparities detected.</p><h3>Implementing Governance Without Killing Innovation</h3><p>The challenge is implementing governance that ensures compliance without paralyzing innovation. Leading organizations balance these concerns&nbsp;through:</p><p> Governance rigor proportional to model risk. Experimental models have lighter requirements than high-stakes production systems.</p><p> Embedding compliance checks into automated pipelines rather than manual review processes.</p><p> Standardized documentation capturing model characteristics, training data, performance metrics, and limitations. Facilitates review and approval.</p><p> Automated monitoring replaces periodic manual reviews, detecting issues faster and reducing&nbsp;burden.</p><p>Organizations successfully implementing governance report that it <strong>improves rather than hinders</strong> model development by catching issues earlier and providing clear quality standards.</p><h3>The Path Forward: Implementing MLOps in Your Organization</h3><p>Transitioning from ad-hoc ML processes to mature MLOps practices requires systematic planning and incremental execution. Organizations rarely succeed by attempting to implement everything simultaneously.</p><h3>Phase 1: Assessment and Foundation (Months&nbsp;1–3)</h3><p> Map existing ML workflows, identifying bottlenecks, failure modes, and improvement opportunities. Use maturity frameworks to establish baseline.</p><p><strong>Build Leadership Support:</strong> MLOps requires investment and organizational change. Secure executive sponsorship by connecting MLOps capabilities to business outcomes.</p><p> Assemble cross-functional team including data scientists, ML engineers, and infrastructure specialists. This team champions MLOps adoption.</p><p><strong>Select Initial Use Cases:</strong> Choose 2–3 models for pilot implementation. Ideal candidates have business value, manageable complexity, and supportive stakeholders.</p><p><strong>Implement Basic Infrastructure:</strong> Establish experiment tracking, model registry, and basic monitoring. Focus on solving immediate pain points rather than perfection.</p><h3>Phase 2: Automation and Standardization (Months&nbsp;4–9)</h3><p> Automate training, validation, and deployment for pilot models. Start with simple pipelines and incrementally add sophistication.</p><p> Deploy data drift detection, performance monitoring, and alerting for production models.</p><p><strong>Establish Retraining Processes:</strong> Create workflows for updating models when drift detected or new data available.</p><p> Capture learnings from pilot projects in runbooks, templates, and guidelines accessible to broader&nbsp;team.</p><p> Add capabilities based on actual needs discovered during pilot. Resist temptation to adopt every tool proactively.</p><h3>Phase 3: Scale and Maturity (Months&nbsp;10–24)</h3><p> Build or adopt platforms that codify best practices and reduce friction for new&nbsp;models.</p><p><strong>Governance Implementation:</strong> Establish model governance framework with appropriate controls for different risk&nbsp;levels.</p><p> Train broader organization on MLOps practices and tooling. Create communities of practice.</p><p> Implement feature stores, A/B testing frameworks, automated retraining, and sophisticated monitoring.</p><p> Regularly assess MLOps maturity, gather feedback, and evolve practices based on lessons&nbsp;learned.</p><h3>Common Pitfalls and How to Avoid&nbsp;Them</h3><p>Organizations implementing MLOps repeatedly encounter similar challenges. Anticipating these pitfalls enables proactive mitigation.</p><h3>Overengineering Too&nbsp;Early</h3><p> Teams attempt to build Netflix-scale infrastructure before deploying their first&nbsp;model.</p><p> Start simple. Manual processes with documentation beat over-engineered automation. Add complexity only when manual processes become bottlenecks.</p><p> Focusing on model algorithms while ignoring data quality issues that undermine performance.</p><p> Invest in data infrastructure first. Automated data validation, profiling, and monitoring prevent garbage-in-garbage-out scenarios.</p><p> Deploying models without comprehensive monitoring, discovering failures only through user complaints.</p><p> Implement monitoring before deployment. Start with basic metrics and progressively add sophistication.</p><p> Proliferating models without inventory, documentation, or oversight.​</p><p> Establish lightweight governance from the start. Model registry, basic documentation, and approval processes prevent&nbsp;chaos.</p><h3>Underestimating Cultural&nbsp;Change</h3><p> Treating MLOps as purely technical challenge while ignoring organizational dynamics.</p><p> Invest in change management, training, and communication. MLOps succeeds when people adopt new practices, not just when tools are deployed.</p><h3>The Future of MLOps: Emerging Trends and Opportunities</h3><p>MLOps continues evolving rapidly, with several trends shaping its future trajectory.</p><h3>LLMOps: Operationalizing Large Language&nbsp;Models</h3><p>Large language models like GPT-4 introduce unique operational challenges requiring specialized practices — collectively termed LLMOps. These&nbsp;include:</p><ul><li><strong>Prompt engineering and versioning:</strong> Managing prompts as code with A/B testing and optimization</li><li> Handling long context windows and retrieval-augmented generation (RAG)</li><li> Managing inference costs that can exceed training&nbsp;costs</li><li> Ensuring outputs align with guidelines and detecting harmful&nbsp;content</li></ul><p>LLMOps builds on MLOps foundations while addressing these novel challenges.</p><p>Deploying models to edge devices — smartphones, IoT sensors, vehicles — creates new operational requirements:</p><ul><li> Quantization, pruning, and distillation to fit resource constraints</li><li> Training on distributed data without centralization</li><li> Safely updating models on deployed&nbsp;devices</li><li> Balancing model accuracy with power consumption</li></ul><p>Automation continues expanding to more aspects of the ML lifecycle:</p><ul><li><strong>Automated feature engineering:</strong> Discovering useful features from raw&nbsp;data</li><li><strong>Neural architecture search:</strong> Optimizing model architectures automatically</li><li><strong>Automated hyperparameter tuning:</strong> Self-optimizing model&nbsp;training</li><li> Detecting and recovering from failures automatically</li></ul><p>Growing awareness of ML’s environmental impact drives focus on sustainability:</p><ul><li> Scheduling training when renewable energy available</li><li> Designing models that achieve better performance-per-watt</li><li> Leveraging transfer learning and pre-trained models</li><li> Tracking and optimizing energy consumption</li></ul><h3>From Experiments to Enterprise Infrastructure</h3><p>The journey from experimental machine learning to production-grade AI infrastructure represents one of the most significant transformations organizations can undertake. MLOps provides the frameworks, practices, and tooling to navigate this journey successfully.</p><p>The evidence is compelling: organizations implementing mature MLOps practices deploy models 10x faster, experience 80% fewer production failures, and achieve significantly better business outcomes than those relying on ad-hoc processes.</p><p>Yet success requires more than adopting tools. It demands cultural transformation, executive commitment, cross-functional collaboration, and patient persistence through inevitable challenges. Organizations must resist the temptation to copy-paste Netflix’s architecture or Google’s processes, instead building MLOps capabilities appropriate for their context, maturity, and constraints.</p><p>The path forward begins with small steps: tracking experiments systematically, automating one deployment pipeline, implementing basic monitoring. These foundations compound over time, enabling progressively sophisticated capabilities that transform ML from isolated projects into enterprise infrastructure generating sustained competitive advantage.</p><p>The question is no longer whether to invest in MLOps, but how quickly your organization can make the transition. Every day spent with manual, error-prone ML processes represents opportunity cost — models that could have been deployed, insights that could have been generated, value that could have been captured.</p><p>The tools exist. The practices are proven. The case studies demonstrate success. What remains is execution: building the discipline, infrastructure, and culture to turn machine learning’s promise into production reality.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=cc4286c7d37d\" width=\"1\" height=\"1\" alt=\"\">","contentLength":28101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Running NGINX as a Windows Service","url":"https://blog.devops.dev/running-nginx-as-a-windows-service-a368570f9c6b?source=rss----33f8b2d9a328---4","date":1761570413,"author":"Alfonso Baqueiro Bernal","guid":316775,"unread":true,"content":"<p>nginx is a great tool that works as a web server, and also as a web proxy that allows to make useful things like protect a site using TLS, you know, put it under HTTPS protocol. Other useful thing possible to acomplish is to protect a site with credentials, or generating access logs, or load balance traffic between several servers (in this aspect nginx is the product behind comercial solutions like BigIP&nbsp;F5).</p><p>After you download just unzip it to a folder like&nbsp;c:\\nginx</p><p>There is only an issue for setting the nginx as a service, and it is that its executable does not support the WindowsNT service API, for this reason we need a wrapper to handle its interaction with Windows Services.</p><p>To run it as a service there are several solutions, older ones like the Windows Server 2003 Resource Kit using SrvAny, that it is to old to recommend in 2023, also the 2014 misfortune named NSSM (non-sucking service manager).</p><p>Luckily I stumbled across a simple, open source based solution built with&nbsp;.net, that allows to run any executable as a service<a href=\"https://github.com/winsw/winsw\">https://github.com/winsw/winsw</a></p><p>This wrapper allows to use any&nbsp;.exe as a Windows Service. It handles the configuration and installation of the service using&nbsp;XML.</p><p>Once you download, copy the file to where you setup your nginx dir in this example c:\\nginx and rename it to nginx-service.exe.</p><p>Next you need to create a nginx-service.xml for describe the&nbsp;service:</p><pre>&lt;service&gt; &lt;id&gt;nginx&lt;/id&gt;<p> &lt;executable&gt;c:\\nginx\\nginx.exe&lt;/executable&gt;</p> &lt;name&gt;nginx&lt;/name&gt;<p> &lt;description&gt;nxing web server&lt;/description&gt;</p> &lt;workingdirectory&gt;c:\\nginx&lt;/workingdirectory&gt;<p> &lt;priority&gt;Normal&lt;/priority&gt;</p> &lt;stoptimeout&gt;15&lt;/stoptimeout&gt;<p> &lt;stopexecutable&gt;c:\\nginx\\nginx.exe&lt;/stopexecutable&gt;</p> &lt;stoparguments&gt;-s stop&lt;/stoparguments&gt;<p> &lt;startmode&gt;Automatic&lt;/startmode&gt;</p>&lt;/service&gt;</pre><p>Finally, to install as a service, run a command line (the old ms-dos command line works better) with administrative privileges and&nbsp;run:</p><pre>cd c:\\nginxnginx-service.exe install</pre><p>You will get a confirmation for the service installation, and also if you open the windows services UI you will find the new service&nbsp;entry.</p><p>From that UI you can start or stop the service, also change parameters as the Startup Type, change to Manual, Automatic or Disable, also you can change the User the service runs as, in the LogOn&nbsp;tab.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a368570f9c6b\" width=\"1\" height=\"1\" alt=\"\">","contentLength":2278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning osquery","url":"https://blog.devops.dev/learning-some-osquery-6155ffd9d013?source=rss----33f8b2d9a328---4","date":1761570409,"author":"Alfonso Baqueiro Bernal","guid":316774,"unread":true,"content":"<p>osquery allows us to query system information using SQL and a common schema, asking the system things&nbsp;like:</p><ul><li>how much memory is being&nbsp;used</li><li>how much I/O is taking&nbsp;place</li><li>how much network bandwith is being&nbsp;used</li></ul><p>For setting up in linux you can do something like:</p><pre>sudo su -wget https://pkg.osquery.io/linux/osquery-5.19.0_1.linux_x86_64.tar.gz<p>tar -zxvf osquery-5.19.0_1.linux_x86_64.tar.gz -C /</p>exit</pre><p>After installing to run just run osqueryi like in the screenshot</p><p>The first thing we need to investigate is the tables that are available to acknoweledge what information can be&nbsp;queried.</p><p>As you can see from the computer platform icons, not all tables are available in all platforms.</p><pre>osquery&gt; select * from cpu_info;+-----------+------------------------------------------+----------------------+----------------+------------+-----------------+--------------------+---------------+---------------------+-----------------+--------------------+<p>| device_id | model                                    | manufacturer         | processor_type | cpu_status | number_of_cores | logical_processors | address_width | current_clock_speed | max_clock_speed | socket_designation |</p>+-----------+------------------------------------------+----------------------+----------------+------------+-----------------+--------------------+---------------+---------------------+-----------------+--------------------+<p>| CPU0      | Intel(R) Core(TM) i7-8650U CPU @ 1.90GHz | Intel(R) Corporation | 3              | 65         | 4               | 8                  | 64            | 1900                | 2100            | U3E1               |</p>+-----------+------------------------------------------+----------------------+----------------+------------+-----------------+--------------------+---------------+---------------------+-----------------+--------------------+</pre><pre>osquery&gt; select * from uptime;+------+-------+---------+---------+---------------+<p>| days | hours | minutes | seconds | total_seconds |</p>+------+-------+---------+---------+---------------+<p>| 41   | 5     | 9       | 21      | 3560961       |</p>+------+-------+---------+---------+---------------+</pre><pre>osquery&gt; select * from cpu_time;+------+----------+------+---------+-----------+---------+-----+---------+-------+-------+------------+<p>| core | user     | nice | system  | idle      | iowait  | irq | softirq | steal | guest | guest_nice |</p>+------+----------+------+---------+-----------+---------+-----+---------+-------+-------+------------+<p>| 0    | 26073492 | 147  | 6575946 | 316347271 | 2102819 | 0   | 65135   | 0     | 0     | 0          |</p>| 1    | 25622030 | 151  | 6853736 | 316385640 | 1984979 | 0   | 574971  | 0     | 0     | 0          |<p>| 2    | 27039406 | 237  | 6520588 | 317378993 | 1531449 | 0   | 34681   | 0     | 0     | 0          |</p>| 3    | 25120227 | 231  | 6311577 | 318945761 | 1609154 | 0   | 330230  | 0     | 0     | 0          |<p>| 4    | 24648711 | 280  | 6026712 | 319468865 | 2192053 | 0   | 238526  | 0     | 0     | 0          |</p>| 5    | 25338855 | 231  | 6157254 | 319166411 | 2002363 | 0   | 26415   | 0     | 0     | 0          |<p>| 6    | 22975209 | 87   | 6297103 | 319630357 | 1753644 | 0   | 245471  | 0     | 0     | 0          |</p>| 7    | 25498600 | 668  | 6477953 | 318333089 | 2175651 | 0   | 52074   | 0     | 0     | 0          |<p>+------+----------+------+---------+-----------+---------+-----+---------+-------+-------+------------+</p></pre><pre>osquery&gt; .all load_average+--------+----------++--------+----------+| 5m     | 0.594238 |+--------+----------+</pre><pre>select * from authorized_keys;</pre><pre>select * from block_devices;</pre><pre>select * from certificates;</pre><pre>osquery&gt; select name, version, source from deb_packages where name like 'z%';+----------------+-------------------------+-----------+<p>| name           | version                 | source    |</p>+----------------+-------------------------+-----------+<p>| zenity         | 3.44.2-1+wilma          |           |</p>| zenity-common  | 3.44.2-1+wilma          | zenity    |<p>| zfs-zed        | 2.2.2-0ubuntu9          | zfs-linux |</p>| zfsutils-linux | 2.2.2-0ubuntu9          | zfs-linux |<p>| zip            | 3.0-13build1            |           |</p>| zlib1g         | 1:1.3.dfsg-3.1ubuntu2.1 | zlib      |<p>| zlib1g-dev     | 1:1.3.dfsg-3.1ubuntu2.1 | zlib      |</p>| zstd           | 1.5.5+dfsg2-2build1     | libzstd   |<p>+----------------+-------------------------+-----------+</p></pre><pre>osquery&gt; .all memory_info+--------------+-------------+------------------+---------+------------+-------------+------------+------------+-------------+------------+<p>| memory_total | memory_free | memory_available | buffers | cached     | swap_cached | active     | inactive   | swap_total  | swap_free  |</p>+--------------+-------------+------------------+---------+------------+-------------+------------+------------+-------------+------------+<p>| 16503930880  | 421617664   | 2546647040       | 12288   | 3308470272 | 46780416    | 8136720384 | 6236536832 | 17179865088 | 5330509824 |</p>+--------------+-------------+------------------+---------+------------+-------------+------------+------------+-------------+------------+</pre><pre>select device, substr(path,0,20) trunc_path, type, blocks_size, blocks, blocks_free, blocks_available from mounts;+---------------------------+---------------------+-----------------+-------------+----------+-------------+------------------+<p>| device                    | trunc_path          | type            | blocks_size | blocks   | blocks_free | blocks_available |</p>+---------------------------+---------------------+-----------------+-------------+----------+-------------+------------------+<p>| sysfs                     | /sys                | sysfs           | 4096        | 0        | 0           | 0                |</p>| proc                      | /proc               | proc            | 4096        | 0        | 0           | 0                |<p>| udev                      | /dev                | devtmpfs        | 4096        | 2004863  | 2004863     | 2004863          |</p>| devpts                    | /dev/pts            | devpts          | 4096        | 0        | 0           | 0                |<p>| tmpfs                     | /run                | tmpfs           | 4096        | 402928   | 402228      | 402228           |</p>| efivarfs                  | /sys/firmware/efi/e | efivarfs        | 1           | 157596   | 67944       | 62824            |<p>| /dev/mapper/vg0-mint_mate | /                   | btrfs           | 4096        | 13107200 | 1040229     | 874649           |</p>| securityfs                | /sys/kernel/securit | securityfs      | 4096        | 0        | 0           | 0                |<p>| tmpfs                     | /dev/shm            | tmpfs           | 4096        | 2014640  | 1807628     | 1807628          |</p>| tmpfs                     | /run/lock           | tmpfs           | 4096        | 1280     | 1278        | 1278             |<p>| cgroup2                   | /sys/fs/cgroup      | cgroup2         | 4096        | 0        | 0           | 0                |</p>| pstore                    | /sys/fs/pstore      | pstore          | 4096        | 0        | 0           | 0                |<p>| bpf                       | /sys/fs/bpf         | bpf             | 4096        | 0        | 0           | 0                |</p>| systemd-1                 | /proc/sys/fs/binfmt | autofs          | 0           | 0        | 0           | 0                |<p>| mqueue                    | /dev/mqueue         | mqueue          | 4096        | 0        | 0           | 0                |</p>| debugfs                   | /sys/kernel/debug   | debugfs         | 4096        | 0        | 0           | 0                |<p>| hugetlbfs                 | /dev/hugepages      | hugetlbfs       | 2097152     | 0        | 0           | 0                |</p>| tracefs                   | /sys/kernel/tracing | tracefs         | 4096        | 0        | 0           | 0                |<p>| fusectl                   | /sys/fs/fuse/connec | fusectl         | 4096        | 0        | 0           | 0                |</p>| configfs                  | /sys/kernel/config  | configfs        | 4096        | 0        | 0           | 0                |<p>| /dev/sda1                 | /boot/efi           | vfat            | 4096        | 130811   | 129240      | 129240           |</p>| /dev/mapper/vg0-mint_mate | /home               | btrfs           | 4096        | 13107200 | 1040229     | 874649           |<p>| binfmt_misc               | /proc/sys/fs/binfmt | binfmt_misc     | 4096        | 0        | 0           | 0                |</p>| tmpfs                     | /run/user/1000      | tmpfs           | 4096        | 402928   | 402851      | 402851           |<p>| gvfsd-fuse                | /run/user/1000/gvfs | fuse.gvfsd-fuse | 0           | 0        | 0           | 0                |</p>| portal                    | /run/user/1000/doc  | fuse.portal     | 0           | 0        | 0           | 0                |<p>| overlay                   | /var/lib/docker/ove | overlay         | 4096        | 13107200 | 1040229     | 874649           |</p>| nsfs                      | /run/docker/netns/3 | nsfs            | 4096        | 0        | 0           | 0                |<p>| overlay                   | /var/lib/docker/ove | overlay         | 4096        | 13107200 | 1040229     | 874649           |</p>| nsfs                      | /run/docker/netns/d | nsfs            | 4096        | 0        | 0           | 0                |<p>| nsfs                      | /run/docker/netns/i | nsfs            | 4096        | 0        | 0           | 0                |</p>| nsfs                      | /run/docker/netns/1 | nsfs            | 4096        | 0        | 0           | 0                |<p>| overlay                   | /var/lib/docker/ove | overlay         | 4096        | 13107200 | 1040229     | 874649           |</p>| nsfs                      | /run/docker/netns/1 | nsfs            | 4096        | 0        | 0           | 0                |<p>| user@server:/runtime      | /home/jack/runtime  | fuse.sshfs      | 0           | 0        | 0           | 0                |</p>+---------------------------+---------------------+-----------------+-------------+----------+-------------+------------------+</pre><pre>osquery&gt; .all os_version+------------+------------+-------+-------+-------+-------+-----------+---------------+----------+--------+<p>| name       | version    | major | minor | patch | build | platform  | platform_like | codename | arch   |</p>+------------+------------+-------+-------+-------+-------+-----------+---------------+----------+--------+<p>| Linux Mint | 22 (Wilma) | 22    | 0     | 0     |       | linuxmint | ubuntu debian | noble    | x86_64 |</p>+------------+------------+-------+-------+-------+-------+-----------+---------------+----------+--------+</pre><p>There are some fields that are not implemented and are not displayed with the&nbsp;.all TABLE neither with select * from TABLE, the only way to check the field exists but is empty is with an explicit&nbsp;query:</p><pre>osquery&gt; select name, version, install_date from os_version;+------------+------------+--------------+<p>| name       | version    | install_date |</p>+------------+------------+--------------+<p>| Linux Mint | 22 (Wilma) |              |</p>+------------+------------+--------------+</pre><h4>Getting hardware information</h4><pre>select pci_class, model from pci_devices;+------------------------------+-----------------------------------------------------------------------------------------+<p>| pci_class                    | model                                                                                   |</p>+------------------------------+-----------------------------------------------------------------------------------------+<p>| Bridge                       | Xeon E3-1200 v6/7th Gen Core Processor Host Bridge/DRAM Registers                       |</p>| Display controller           | UHD Graphics 620                                                                        |<p>| Signal processing controller | Xeon E3-1200 v5/E3-1500 v5/6th Gen Core Processor Thermal Subsystem                     |</p>| Generic system peripheral    | Xeon E3-1200 v5/v6 / E3-1500 v5 / 6th/7th/8th Gen Core Processor Gaussian Mixture Model |<p>| Unclassified device          | Sunrise Point-LP Integrated Sensor Hub                                                  |</p>| Serial bus controller        | Sunrise Point-LP USB 3.0 xHCI Controller                                                |<p>| Signal processing controller | Sunrise Point-LP Thermal subsystem                                                      |</p>| Signal processing controller | Sunrise Point-LP Serial IO I2C Controller                                               |<p>| Communication controller     | Sunrise Point-LP CSME HECI                                                              |</p>| Communication controller     | Sunrise Point-LP Active Management Technology - SOL                                     |<p>| Mass storage controller      | Sunrise Point-LP SATA Controller [AHCI mode]                                            |</p>| Bridge                       | Sunrise Point-LP PCI Express Root Port                                                  |<p>| Network controller           | Wireless 8265 / 8275                                                                    |</p>| Bridge                       | Sunrise Point-LP PCI Express Root Port                                                  |<p>| Bridge                       | JHL6540 Thunderbolt 3 Bridge (C step) [Alpine Ridge 4C 2016]                            |</p>| Bridge                       | JHL6540 Thunderbolt 3 Bridge (C step) [Alpine Ridge 4C 2016]                            |<p>| Generic system peripheral    | JHL6540 Thunderbolt 3 NHI (C step) [Alpine Ridge 4C 2016]                               |</p>| Bridge                       | JHL6540 Thunderbolt 3 Bridge (C step) [Alpine Ridge 4C 2016]                            |<p>| Bridge                       | JHL6540 Thunderbolt 3 Bridge (C step) [Alpine Ridge 4C 2016]                            |</p>| Serial bus controller        | JHL6540 Thunderbolt 3 USB Controller (C step) [Alpine Ridge 4C 2016]                    |<p>| Bridge                       | JHL6540 Thunderbolt 3 Bridge (C step) [Alpine Ridge 4C 2016]                            |</p>| Bridge                       | Sunrise Point LPC/eSPI Controller                                                       |<p>| Memory controller            | Sunrise Point-LP PMC                                                                    |</p>| Multimedia controller        | Sunrise Point-LP HD Audio                                                               |<p>| Serial bus controller        | Sunrise Point-LP SMBus                                                                  |</p>| Network controller           | Ethernet Connection (4) I219-LM                                                         |<p>+------------------------------+-----------------------------------------------------------------------------------------+</p></pre><pre>osquery&gt; .all platform_info+--------+------------------+------------+----------+-------+---------------+---------+----------+-------------+<p>| vendor | version          | date       | revision | extra | firmware_type | address | size     | volume_size |</p>+--------+------------------+------------+----------+-------+---------------+---------+----------+-------------+<p>| LENOVO | N25(1.15 ).      | 01/01/2015 | 1.15     |       | uefi          | 0xe000  | 16041834 | 0           |</p>+--------+------------------+------------+----------+-------+---------------+---------+----------+-------------+</pre><h4>Getting some metadata about osquery&nbsp;itself</h4><pre>osquery&gt; .all osquery_info+---------+--------------------------------------+--------------------------------------+---------+-------------+--------------+------------+----------------+--------------+------------+---------+---------------+<p>| pid     | uuid                                 | instance_id                          | version | config_hash | config_valid | extensions | build_platform | build_distro | start_time | watcher | platform_mask |</p>+---------+--------------------------------------+--------------------------------------+---------+-------------+--------------+------------+----------------+--------------+------------+---------+---------------+<p>| 1999703 | fe0ba52f-c741-453e-af05-44571756653d | 0eb6e1a1-14fa-4741-8f6e-96427e0ab536 | 5.19.0  |             | 0            | active     | linux          | centos7      | 1760401878 | -1      | 9             |</p>+---------+--------------------------------------+--------------------------------------+---------+-------------+--------------+------------+----------------+--------------+------------+---------+---------------+</pre><pre>osquery&gt; .all osquery_extensions+------+------+---------+-------------+-------------------------+------+<p>| uuid | name | version | sdk_version | path                    | type |</p>+------+------+---------+-------------+-------------------------+------+<p>| 0    | core | 5.19.0  | 0.0.0       | /root/.osquery/shell.em | core |</p>+------+------+---------+-------------+-------------------------+------+</pre><p>In the documentation of the schema, there are some tables that are tagged as evented table. What does this&nbsp;mean?</p><p>“Evented” tables are tables that are triggered by an event happening on the system, rather than by osquery asking “what’s the current&nbsp;state?</p><p>Most osquery tables are snapshot tables. When you run a query against them, osquery scans the current state of the system at that exact moment and returns the&nbsp;results.</p><p>Example: SELECT * FROM processes;</p><p>Evented tables are designed to monitor for specific  over time. Instead of polling for state, they sit and wait for the operating system to tell them that something has happened.</p><p>Example: SELECT * FROM process_events;</p><p>This will give you information of the&nbsp;kind:</p><p>“process with PID 1234 (named ‘notepad.exe’) was launched by user Alice at time&nbsp;T”.</p><p>This table doesn’t show all running processes. It shows a history of processes that have started or exited since you started listening for&nbsp;events.</p><p>Publishers/Subscribers: Under the hood, osquery uses a publish-subscribe model.</p><ol><li>A publisher (e.g., the Windows ETW subsystem, macOS EndpointSecurity, Linux auditd) detects an event (a file was written, a process started).</li><li>Osquery, acting as a subscriber, receives that event and adds it to the appropriate table’s internal&nbsp;buffer.</li><li>Your SQL query reads from this&nbsp;buffer.</li></ol><p>Requires a Daemon/Service: To continuously collect events, you must run osquery as a daemon (osqueryd), not just the interactive shell (osqueryi). The daemon runs in the background, collecting events into its&nbsp;buffer.</p><p>Because evented tables are a continuous stream, osqueryd stores these events in a finite-sized buffer in memory. If you don’t query the table frequently enough, old events will be removed from the buffer to make room for new&nbsp;ones.</p><p>There are queries that return a table with many columns and its very wide, this makes a mess of the rendered output, for this cases the line mode is handy, to activate&nbsp;use:</p><pre>osquery&gt; .mode lineosquery&gt; select * from processes limit 1;              name = systemd<p>              path = /usr/lib/systemd/systemd</p>           cmdline = /sbin/init splash               cwd = /               uid = 0              euid = 0              suid = 0           on_disk = 1     resident_size = 8617984         user_time = 90030   disk_bytes_read = 72458566656<p>disk_bytes_written = 125935652864</p>        start_time = 1756833280            pgroup = 1              nice = 0<p>       cgroup_path = /init.scope</p></pre><pre>osquery&gt; .mode csvosquery&gt; select * from processes limit 1;<p>pid,name,path,cmdline,state,cwd,root,uid,gid,euid,egid,suid,sgid,on_disk,wired_size,resident_size,total_size,user_time,system_time,disk_bytes_read,disk_bytes_written,start_time,parent,pgroup,threads,nice,cgroup_path</p>1,systemd,/usr/lib/systemd/systemd,\"/sbin/init splash\",S,/,/,0,0,0,0,0,0,1,0,8617984,23695360,90030,77130,72458566656,125935652864,1756833280,0,1,1,0,/init.scope</pre><pre>osquery&gt; .mode listosquery&gt; .separator |<p>osquery&gt; select * from processes limit 1;</p>pid|name|path|cmdline|state|cwd|root|uid|gid|euid|egid|suid|sgid|on_disk|wired_size|resident_size|total_size|user_time|system_time|disk_bytes_read|disk_bytes_written|start_time|parent|pgroup|threads|nice|cgroup_path<p>1|systemd|/usr/lib/systemd/systemd|/sbin/init splash|S|/|/|0|0|0|0|0|0|1|0|8617984|23695360|90040|77130|72458566656|125935652864|1756833280|0|1|1|0|/init.scope</p></pre><pre>osquery&gt; select * from routes limit 16;+-----------------+---------+---------------+---------------+-------+-----------------+-----+--------+-----------+----------+<p>| destination     | netmask | gateway       | source        | flags | interface       | mtu | metric | type      | hopcount |</p>+-----------------+---------+---------------+---------------+-------+-----------------+-----+--------+-----------+----------+<p>| 0.0.0.0         | 0       | 192.168.1.254 | 192.168.1.17  | 0     | enx6c           | 0   | 100    | gateway   | 0        |</p>| 172.17.0.0      | 16      |               | 172.17.0.1    | 0     | docker0         | 0   | 0      | gateway   | 0        |<p>| 172.18.0.0      | 16      |               | 172.18.0.1    | 0     | br-fecb         | 0   | 0      | gateway   | 0        |</p>| 172.19.0.0      | 16      |               | 172.19.0.1    | 0     | docker_gwbridge | 0   | 0      | gateway   | 0        |<p>| 192.168.1.0     | 24      |               | 192.168.1.17  | 0     | enx6c           | 0   | 100    | gateway   | 0        |</p>| 127.0.0.0       | 8       |               | 127.0.0.1     | 0     | lo              | 0   | 0      | local     | 0        |<p>| 127.0.0.1       | 0       |               | 127.0.0.1     | 0     | lo              | 0   | 0      | local     | 0        |</p>| 127.255.255.255 | 0       |               | 127.0.0.1     | 0     | lo              | 0   | 0      | broadcast | 0        |<p>| 172.17.0.1      | 0       |               | 172.17.0.1    | 0     | docker0         | 0   | 0      | local     | 0        |</p>| 172.17.255.255  | 0       |               | 172.17.0.1    | 0     | docker0         | 0   | 0      | broadcast | 0        |<p>| 172.18.0.1      | 0       |               | 172.18.0.1    | 0     | br-fecb         | 0   | 0      | local     | 0        |</p>| 172.18.255.255  | 0       |               | 172.18.0.1    | 0     | br-fecb         | 0   | 0      | broadcast | 0        |<p>| 172.19.0.1      | 0       |               | 172.19.0.1    | 0     | docker_gwbridge | 0   | 0      | local     | 0        |</p>| 172.19.255.255  | 0       |               | 172.19.0.1    | 0     | docker_gwbridge | 0   | 0      | broadcast | 0        |<p>| 192.168.1.17    | 0       |               | 192.168.1.17  | 0     | enx6c           | 0   | 0      | local     | 0        |</p>| 192.168.1.255   | 0       |               | 192.168.1.17  | 0     | enx6c           | 0   | 0      | broadcast | 0        |<p>+-----------------+---------+---------------+---------------+-------+-----------------+-----+--------+-----------+----------+</p></pre><pre>osquery&gt; select * from shell_history limit 10;+-----+------+----------------------------+---------------------+<p>| uid | time | command                    | history_file        |</p>+-----+------+----------------------------+---------------------+<p>| 0   | 0    | exit                       | /root/.bash_history |</p>| 0   | 0    | vim /etc/hosts             | /root/.bash_history |<p>| 0   | 0    | exit                       | /root/.bash_history |</p>| 0   | 0    | apt update                 | /root/.bash_history |<p>| 0   | 0    | apt-get update             | /root/.bash_history |</p>| 0   | 0    | ping 8.8.8.8               | /root/.bash_history |<p>| 0   | 0    | cd /etc/apt/               | /root/.bash_history |</p>+-----+------+----------------------------+---------------------+</pre><pre>osquery&gt; .mode lineosquery&gt; select * from system_info;              uuid = D1BCA554-7710-4B16-9C75-43C54534F029       cpu_subtype = 142<p>         cpu_brand = Intel(R) Core(TM) i7-8650U CPU @ 1.90GHz</p>cpu_physical_cores = 4       cpu_sockets = 1   physical_memory = 16503930880    hardware_model = ThinkPad X1 Yoga 3rd   hardware_serial = x14MQP       board_model = 20LE     computer_name = thinkpad<p>    local_hostname = thinkpad</p></pre><pre>osquery&gt; .all ulimit_info+------------+------------+------------+<p>| type       | soft_limit | hard_limit |</p>+------------+------------+------------+<p>| as         | unlimited  | unlimited  |</p>| core       | 0          | 0          |<p>| cpu        | unlimited  | unlimited  |</p>| data       | unlimited  | unlimited  |<p>| fsize      | unlimited  | unlimited  |</p>| locks      | unlimited  | unlimited  |<p>| memlock    | 2062991360 | 2062991360 |</p>| msgqueue   | 819200     | 819200     |<p>| nice       | 0          | 0          |</p>| nofile     | 1024       | 1048576    |<p>| nproc      | 62651      | 62651      |</p>| rss        | unlimited  | unlimited  |<p>| rtprio     | 0          | 0          |</p>| sigpending | 62651      | 62651      |<p>| stack      | 8388608    | unlimited  |</p>+------------+------------+------------+</pre><pre>osquery&gt; select usb_address, usb_port, vendor, model, removable from usb_devices order by 1, 2;+-------------+----------+-----------------------------+----------------------------------------+-----------+<p>| usb_address | usb_port | vendor                      | model                                  | removable |</p>+-------------+----------+-----------------------------+----------------------------------------+-----------+<p>| 1           | 0        | Synaptics, Inc.             | Metallica MIS Touch Fingerprint Reader | 1         |</p>| 1           | 1        | Linux Foundation            | 2.0 root hub                           | -1        |<p>| 1           | 3        | Genesys Logic, Inc.         | Hub                                    | 1         |</p>| 1           | 6        | Genesys Logic, Inc.         | Hub                                    | -1        |<p>| 1           | 7        | Bison Electronics Inc.      | Integrated_Camera                      | 1         |</p>| 1           | 8        | Fresco Logic                | Generic_Billboard_Device               | -1        |<p>| 1           | 9        | Wacom Co., Ltd              | Pen_and_multitouch_sensor              | 1         |</p>| 1           | 10       | Apple, Inc.                 | USB2.0_Hub                             | 1         |<p>| 1           | 11       | Apple, Inc.                 | HDMI Adapter                           | 1         |</p>| 1           | 14       | Logitech, Inc.              | Unifying Receiver                      | -1        |<p>| 1           | 15       | Intel Corp.                 | Bluetooth wireless interface           | 1         |</p>| 2           | 1        | Linux Foundation            | 3.0 root hub                           | -1        |<p>| 2           | 2        | Realtek Semiconductor Corp. | USB3.0-CRW                             | 1         |</p>| 3           | 1        | Linux Foundation            | 2.0 root hub                           | -1        |<p>| 4           | 1        | Linux Foundation            | 3.0 root hub                           | -1        |</p>| 4           | 3        | Genesys Logic, Inc.         | Hub                                    | 1         |<p>| 4           | 4        | Genesys Logic, Inc.         | SD Card Reader and Writer              | -1        |</p>| 4           | 6        | Apple, Inc.                 | USB3.1_Hub                             | 1         |<p>+-------------+----------+-----------------------------+----------------------------------------+-----------+</p></pre><pre>osquery&gt; select * from users limit 5;+-----+-------+------------+------------+----------+-------------+-----------+-------------------+------+<p>| uid | gid   | uid_signed | gid_signed | username | description | directory | shell             | uuid |</p>+-----+-------+------------+------------+----------+-------------+-----------+-------------------+------+<p>| 0   | 0     | 0          | 0          | root     | root        | /root     | /bin/bash         |      |</p>| 1   | 1     | 1          | 1          | daemon   | daemon      | /usr/sbin | /usr/sbin/nologin |      |<p>| 2   | 2     | 2          | 2          | bin      | bin         | /bin      | /usr/sbin/nologin |      |</p>| 3   | 3     | 3          | 3          | sys      | sys         | /dev      | /usr/sbin/nologin |      |<p>| 4   | 65534 | 4          | 65534      | sync     | sync        | /bin      | /bin/sync         |      |</p>+-----+-------+------------+------------+----------+-------------+-----------+-------------------+------+</pre><pre>osquery&gt; select pid, name, cmdline, state, cwd, root, uid, resident_size, total_size, elapsed_time, user_time, system_time, disk_bytes_read read_b, disk_bytes_written write_b, start_time, parent, threads from processes where pid&gt;999 order by pid limit 30;+------+----------------+-----------------------------------------------------------------------------------------------------------+-------+-------+-------+------+---------------+------------+--------------+-----------+-------------+-------------+------------+------------+--------+---------+<p>| pid  | name           | cmdline                                                                                                   | state | cwd   | root  | uid  | resident_size | total_size | elapsed_time | user_time | system_time | read_b      | write_b    | start_time | parent | threads |</p>+------+----------------+-----------------------------------------------------------------------------------------------------------+-------+-------+-------+------+---------------+------------+--------------+-----------+-------------+-------------+------------+------------+--------+---------+<p>| 1015 | avahi-daemon   | avahi-daemon: chroot helper                                                                               | S     | /     | /     | 111  | 1200128       | 8626176    |              | 0         | 0           | 0           | 0          | 1756833295 | 912    | 1       |</p>| 1030 | NetworkManager | /usr/sbin/NetworkManager --no-daemon                                                                      | S     | /     | /     | 0    | 11087872      | 424189952  |              | 652890    | 261880      | 194084864   | 12357632   | 1756833295 | 1      | 4       |<p>| 1059 | wpa_supplicant | /usr/sbin/wpa_supplicant -u -s -O DIR=/run/wpa_supplicant GROUP=netdev                                    | S     | /     | /     | 0    | 4063232       | 18694144   |              | 121730    | 54730       | 69058560    | 0          | 1756833295 | 1      | 1       |</p>| 1117 | rsyslogd       | /usr/sbin/rsyslogd -n -iNONE                                                                              | S     | /     | /     | 102  | 3145728       | 227848192  |              | 39170     | 62930       | 30457856    | 441729024  | 1756833295 | 1      | 4       |<p>| 1157 | ModemManager   | /usr/sbin/ModemManager                                                                                    | S     | /     | /     | 0    | 4784128       | 401506304  |              | 270       | 260         | 51200000    | 0          | 1756833295 | 1      | 4       |</p>| 1188 | boltd          | /usr/libexec/boltd                                                                                        | S     | /     | /     | 0    | 5558272       | 323719168  |              | 4470      | 5940        | 11456512    | 16384      | 1756833295 | 1      | 4       |<p>| 1193 | upowerd        | /usr/libexec/upowerd                                                                                      | S     | /     | /     | 0    | 25952256      | 387678208  |              | 175120    | 320740      | 653115392   | 81731584   | 1756833295 | 1      | 4       |</p>| 1321 | containerd     | /usr/bin/containerd                                                                                       | S     | /     | /     | 0    | 16760832      | 2148417536 |              | 3264730   | 2601010     | 1896722432  | 53444608   | 1756833297 | 1      | 12      |<p>| 1324 | lightdm        | /usr/sbin/lightdm                                                                                         | S     | /     | /     | 0    | 4898816       | 389210112  |              | 1040      | 330         | 27279360    | 73728      | 1756833297 | 1      | 4       |</p>| 1330 | colord         | /usr/libexec/colord                                                                                       | S     | /     | /     | 120  | 6365184       | 330620928  |              | 640       | 400         | 175255552   | 0          | 1756833297 | 1      | 4       |<p>| 1337 | Xorg           | /usr/lib/xorg/Xorg -core :0 -seat seat0 -auth /var/run/lightdm/root/:0 -nolisten tcp vt7 -novtswitch      | S     | /     | /     | 0    | 35241984      | 2418151424 |              | 101901260 | 54124580    | 14806507520 | 21577728   | 1756833297 | 1324   | 8       |</p>| 1339 | agetty         | /sbin/agetty -o -p -- \\u --noclear - linux                                                                | S     | /     | /     | 0    | 1966080       | 11501568   |              | 0         | 0           | 77824       | 4096       | 1756833297 | 1      | 1       |<p>| 1442 | rtkit-daemon   | /usr/libexec/rtkit-daemon                                                                                 | S     | /proc | /proc | 105  | 3014656       | 23490560   |              | 13530     | 44690       | 65536       | 0          | 1756833302 | 1      | 3       |</p>| 1491 | krfcommd       |                                                                                                           | S     | /     | /     | 0    |               |            |              | 10        | 170         | 0           | 0          | 1756833303 | 2      | 1       |<p>| 1526 | dockerd        | /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock                                    | S     | /     | /     | 0    | 47538176      | 2711597056 |              | 11905830  | 8008820     | 3818344448  | 3974979584 | 1756833303 | 1      | 18      |</p>| 1541 | kerneloops     | /usr/sbin/kerneloops --test                                                                               | S     | /     | /     | 106  | 2265088       | 13049856   |              | 4190      | 405960      | 221184      | 0          | 1756833303 | 1      | 1       |<p>| 1546 | kerneloops     | /usr/sbin/kerneloops                                                                                      | S     | /     | /     | 106  | 2236416       | 13049856   |              | 4110      | 405630      | 151552      | 0          | 1756833303 | 1      | 1       |</p>| 1559 | nginx          | nginx: master process /usr/sbin/nginx -g daemon on; master_process on;                                    | S     | /     | /     | 0    | 1310720       | 11472896   |              | 0         | 10          | 8728576     | 0          | 1756833303 | 1      | 1       |<p>| 1560 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2801664       | 13340672   |              | 10        | 40          | 10485760    | 28672      | 1756833303 | 1559   | 1       |</p>| 1561 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2428928       | 13189120   |              | 0         | 0           | 933888      | 0          | 1756833303 | 1559   | 1       |<p>| 1562 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2691072       | 13189120   |              | 0         | 10          | 901120      | 0          | 1756833303 | 1559   | 1       |</p>| 1563 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2166784       | 13189120   |              | 0         | 0           | 937984      | 0          | 1756833303 | 1559   | 1       |<p>| 1564 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2719744       | 13336576   |              | 0         | 20          | 3702784     | 8192       | 1756833303 | 1559   | 1       |</p>| 1565 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2428928       | 13660160   |              | 0         | 30          | 2502656     | 12288      | 1756833303 | 1559   | 1       |<p>| 1566 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2560000       | 13189120   |              | 0         | 10          | 2420736     | 4096       | 1756833303 | 1559   | 1       |</p>| 1567 | nginx          | nginx: worker process                                                                                     | S     | /     | /     | 33   | 2560000       | 13189120   |              | 0         | 0           | 827392      | 4096       | 1756833303 | 1559   | 1       |<p>| 1571 | tor            | /usr/bin/tor --defaults-torrc /usr/share/tor/tor-service-defaults-torrc -f /etc/tor/torrc --RunAsDaemon 0 | S     | /     | /     | 123  | 4468736       | 679882752  |              | 1300      | 460         | 244092928   | 0          | 1756833303 | 1      | 9       |</p>| 1607 | lightdm        | lightdm --session-child 13 20                                                                             | S     | /     | /     | 0    | 5165056       | 242155520  |              | 110       | 90          | 4530176     | 73728      | 1756833303 | 1324   | 4       |<p>| 1840 | systemd        | /usr/lib/systemd/systemd --user                                                                           | S     | /     | /     | 1000 | 5242880       | 21356544   |              | 5590      | 1740        | 1695055872  | 999424     | 1756833310 | 1      | 1       |</p>| 1841 | (sd-pam)       | (sd-pam)                                                                                                  | S     | /     | /     | 1000 | 2097152       | 23543808   |              | 0         | 0           | 0           | 0          | 1756833310 | 1840   | 1       |<p>+------+----------------+-----------------------------------------------------------------------------------------------------------+-------+-------+-------+------+---------------+------------+--------------+-----------+-------------+-------------+------------+------------+--------+---------+</p></pre><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6155ffd9d013\" width=\"1\" height=\"1\" alt=\"\"><p><a href=\"https://blog.devops.dev/learning-some-osquery-6155ffd9d013\">Learning osquery</a> was originally published in <a href=\"https://blog.devops.dev\">DevOps.dev</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>","contentLength":38697,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to add MCP Servers to Claude Desktop with Docker MCP Toolkit","url":"https://www.docker.com/blog/connect-mcp-servers-to-claude-desktop-with-mcp-toolkit/","date":1761570000,"author":"Ajeet Singh Raina","guid":316964,"unread":true,"content":"<p>What if you could turn Claude from a conversational assistant into a development partner that actually  things—safely, securely, and without touching your local machine?</p><p>If you’ve been exploring Claude Desktop and wondering how to connect it with real developer tools, <a href=\"https://www.docker.com/products/mcp-catalog-and-toolkit/\">Docker MCP Toolkit</a> is the missing piece you’ve been looking for.</p><p>Here’s the reality: Today’s AI assistants are brilliant at reasoning and explaining concepts. But when it comes to  on that knowledge? They’re stuck. They can’t deploy containers, manage repos, or analyze data without trusted tools to bridge that gap.</p><p>That’s where Docker MCP Toolkit comes in.</p><ul><li> is the reasoning engine—the brain that understands what you want</li><li> is the translator, turning your requests into actionable commands</li><li> is the hand that safely executes those actions in isolated containers</li></ul><p>Together, they transform Claude from something that just talks to something that —all through Docker’s trusted, security-hardened workflow.</p><p>In this guide, I’ll walk you through setting up Claude Desktop as an MCP client, connecting it to MCP servers running in Docker Desktop, and automating real developer tasks securely and reproducibly.&nbsp;</p><p><strong>What if you could turn 4 hours of work into 10 minutes?</strong> Imagine taking a screenshot of any app UI—a Stripe checkout page, a dashboard component, anything—and watching Claude recreate it as a fully-tested, production-ready React component, complete with Jest tests, GitHub repo, and deployment. That’s exactly what we’ll build together</p><h2>What is Model Context Protocol (MCP)?</h2><p>Before we dive into the setup, let’s clarify what MCP actually is.</p><p><a href=\"https://modelcontextprotocol.io/\" rel=\"nofollow noopener\" target=\"_blank\">Model Context Protocol</a> (MCP) is the standardized way AI agents like Claude connect to tools, APIs, and services. It’s what lets Claude go beyond conversation and perform real-world actions—like deploying containers, analyzing datasets, or managing GitHub repositories.</p><p> MCP is the bridge between Claude’s reasoning and your developer stack. And Docker? Docker provides the guardrails that make it safe.</p><h2>Why use Docker MCP Toolkit with Claude Desktop?</h2><p>I’ve been working with AI tools for a while now, and this Docker MCP setup is one of the most impressive things I’ve seen. Docker MCP Toolkit bridges Claude’s intelligence with Docker’s trusted developer workflow.</p><p>Docker MCP Toolkit doesn’t just connect Claude to your tools—it creates a <em>secure, containerized environment</em> where Claude can work without any risk to your local machine. Every action happens in an isolated container that gets torn down when you’re done. No mess, no security risks, complete reproducibility.</p><p>Once connected, Claude Desktop can tap into Docker’s curated <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">MCP servers</a> to automate developer tasks that used to eat up hours of your day. Here’s what that looks like in practice:</p><ul><li>: Want to create a repo, push some code, or review a pull request? Claude’s got you covered. No more switching between tabs constantly.</li><li>: If you’re into K8s (and let’s be honest, who isn’t these days?), Claude can deploy pods, manage services, and even handle Helm charts.</li><li>: Upload a CSV file and watch Claude analyze it, create visualizations, and give you insights you didn’t even know you needed.</li><li>: With Firecrawl integration, Claude can go fetch data from websites and bring it back to you, all formatted and ready to use.</li></ul><p>Each of these MCP servers can be enabled directly through the MCP Toolkit in Docker Desktop. The setup is straightforward, the execution is secure, and the results are reproducible every single time.</p><h2><strong>Setting up Claude Desktop with Docker MCP Toolkit</strong></h2><p>Before you begin, make sure you have:</p><h4>Step 1: Install and sign in to Claude Desktop</h4><p>Head over to <a href=\"https://claude.ai/desktop?ref=ajeetraina.com\" rel=\"nofollow noopener\" target=\"_blank\">claude.ai/desktop</a> and download  if you haven’t already. The installation is pretty straightforward – just follow the prompts. Once it’s installed, sign in with your Anthropic account.</p><h4>Step 2: Enable Docker MCP Toolkit</h4><p>Open Docker Desktop and enable the MCP Toolkit from the Settings menu.</p><h4>Step 3. Connect Claude Desktop as MCP Client</h4><p>Click “MCP Toolkit” in the left sidebar of Docker Desktop and click on “Connect” under Claude Desktop.</p><h4>Step 4. Verify the MCP_DOCKER connection&nbsp;</h4><p>Restart Claude Desktop and verify that the MCP_DOCKER connection appears under the Claude Desktop menu.</p><h4>Step 5. View the Claude Configuration</h4><p>You can view the Claude configuration file claude_desktop_config.json accessible via Claude &gt; Settings &gt; Developers &gt; Edit Config.</p><p>Click “Edit Config” and open it via Visual Studio Code or your preferred IDE.</p><p>When you connect Claude Desktop to Docker MCP Toolkit, this configuration gets added to Claude Desktop’s claude_desktop_config.json file:</p><p>Here’s what each part does:</p><ul><li>: The parent object that defines all MCP server connections available to Claude Desktop</li><li>: The name identifier for this connection. This is what you’ll see referenced in Claude Desktop’s settings and logs</li><li>: Tells Claude Desktop to use the Docker CLI as the execution command</li><li><strong>“args”: [“mcp”, “gateway”, “run”]</strong>: The arguments passed to Docker, which translate to running:</li></ul><p>When Claude Desktop needs to access MCP tools, it executes docker mcp gateway run, which:</p><ol><li>Starts the Docker MCP Gateway – This acts as the central router/bridge</li><li>Connects Claude Desktop (MCP client) to all enabled MCP servers in Docker Desktop</li><li>Routes tool requests from Claude → Gateway → Appropriate MCP Server → Back to Claude</li><li>Runs everything in isolated containers for security and reproducibility</li></ol><ul><li>Claude Desktop = Your AI assistant (the client)</li><li>Docker MCP Gateway = The switchboard operator (the router)</li><li>MCP Servers = The actual tools (GitHub, Firecrawl, Node.js Sandbox, etc.)</li></ul><p>This one configuration line is what makes all the magic happen—it’s the handshake that lets Claude safely access and execute all those containerized developer tools.</p><ul><li>: ~/Library/Application Support/Claude/claude_desktop_config.json</li><li>: %APPDATA%\\Claude\\claude_desktop_config.json</li></ul><div><p> When you click “Connect” in Docker Desktop’s MCP Client settings, this configuration gets added automatically—you don’t need to edit it manually.</p></div><h2>Real-World Demo: From Screenshot to Deployed App (With Tests!)</h2><p>Now that you’ve connected Claude Desktop to <a href=\"https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/\" rel=\"nofollow noopener\" target=\"_blank\">Docker MCP Toolkit</a>, let’s see it in action with a practical example. What if you could take a screenshot of any app, and have Claude recreate it with working code AND full test coverage in minutes?</p><p>Right now, the typical workflow looks like this:</p><ul><li>Developers manually rebuild them in code</li></ul><p>It’s slow, manual, and riddled with back-and-forth communication gaps.</p><p>Claude Desktop + <a href=\"https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/\" rel=\"nofollow noopener\" target=\"_blank\">Docker MCP Toolkit</a> automates all three steps. You provide a screenshot, and Claude handles the rest—analysis, code generation, testing, debugging, and deployment.</p><p>A functional React component from a screenshot, complete with:</p><ul><li>A GitHub repo (basic structure)</li><li>Configuration files (package.json, <a href=\"http://babelrc.js\" rel=\"nofollow noopener\" target=\"_blank\">babelrc.js</a>, gitignore)</li></ul><div><p>Note: This is a code foundation, not production deployment. Does NOT include: CI/CD, live deployment, Playwright tests, or a11y validation.</p></div><p>For this demonstration, we’ll use five MCP servers:</p><ul><li><a href=\"https://hub.docker.com/mcp/server/node-code-sandbox/overview\" rel=\"nofollow noopener\" target=\"_blank\"></a> – Runs tests, installs dependencies, validates code (in isolated containers)</li><li><a href=\"https://hub.docker.com/mcp/server/github-official/overview\" rel=\"nofollow noopener\" target=\"_blank\"></a> – Handles version control and deployment</li><li><a href=\"https://hub.docker.com/mcp/server/context7/overview\" rel=\"nofollow noopener\" target=\"_blank\"></a>– Provides code documentation for LLMs and AI code editors</li></ul><h3>The Walkthrough (Step-by-Step)</h3><div><table><tbody><tr><td><p>Phase 1: Screenshot to Component</p></td><td><p>– You provide a screenshot, could be a Stripe checkout page, a dashboard card, whatever UI you need.</p><p>– Claude analyzes the layout, identifies components, measures spacing, captures colors, and generates a React/Next.js component that matches the design.</p></td></tr><tr><td><p>Phase 2: Generate Test Suite</p></td><td><p>– Jest unit tests for component logic (41 tests covering rendering, interactions, filters and state management).</p></td></tr><tr><td><p>Phase 3: Run &amp; Debug with Node.js Sandbox</p></td><td><p><strong><em>This is where Docker really shines</em></strong></p><p>– The Node.js Sandbox MCP spins up an isolated container, installs all dependencies safely, and runs your test suite. </p><p>– Everything happens in a sandboxes environment; nothing touches your local machine.</p><p>– If tests fail? Sequential Thinking kicks in. Claude iterates, fixes the code, spins up a  container, and runs the tests again. It repeats this loop until everything passes.</p></td></tr><tr><td><p>Phase 4: Deploy to GitHub</p></td><td><p>– Claude creates a GitHub repo with proper structure</p></td></tr></tbody></table></div><ul><li> A screenshot of a UI</li><li> A fully tested, deployed, production-ready component</li></ul><p> What used to take 4 hours now takes 10 minutes.</p><ol><li> Code runs in isolated containers, not on your machine. No risk of malicious code affecting your system. Every execution has an audit trail.</li><li> Same Docker image = same results, every time. Works on your machine, your teammate’s machine, and in CI/CD. No more “works on my machine” excuses.</li><li> Test potentially dangerous AI-generated code without fear. Iterate quickly in isolated environments. Roll back instantly if something breaks.</li><li> See exactly what’s running and where. Monitor resource usage in Docker Desktop. Access full logs and debugging capabilities.</li></ol><p>Assuming that Docker MCP Toolkit is already configured with Claude Desktop, follow the below steps to configure and add MCP servers.&nbsp;</p><p>Click “MCP Toolkit” and select “Catalog” to search for the following MCP servers and add them one by one.</p><ul><li>Firecrawl (web scraping, screenshots)</li><li>GitHub Official (repo management, PRs, issues, commits)</li><li>Sequential Thinking (systematic problem-solving)</li><li>Context7 (up-to-date code documentation for LLMs and code editors)</li></ul><div><p>Note:&nbsp;<p>The Node.js Sandbox MCP server implements Docker-out-of-Docker (DooD) pattern by mounting /var/run/docker.sock, giving the sandbox container full Docker daemon API access. This allows it to spawn ephemeral sibling containers for code execution – when Claude requests JavaScript execution, the sandbox container makes Docker API calls to create temporary Node.js containers (node:lts-slim, Playwright, etc.) with resource limits (512MB RAM, 0.75 CPU cores), executes the code in isolation, and auto-removes the container.</p><p>The Docker socket mount is a privilege escalation vector (effectively granting root-level host access) but enables dynamic container orchestration without the overhead of true Docker-in-Docker. Files are persisted via volume mount to ~/Desktop/sandbox-output, mapped to /root inside execution containers. This architecture trades security for flexibility – acceptable for local development but requires Docker Scout vulnerability scanning and careful consideration for production use. Hence, you’ll require a separate entry for </p><a href=\"http://node.js\" rel=\"nofollow noopener\" target=\"_blank\">Node.js</a> Sandbox MCP server in&nbsp; the Claude configuration file. You’ll see how to configure later in this article.</p></div><h4>Configure Firecrawl MCP Server</h4><ol><li>Open  →  → </li><li>Find in the results</li><li>Add Firecrawl API Keys (firecrawl.api_key) that you created earlier</li><li>Leave all the other entries blank</li></ol><p>The Firecrawl MCP server should now appear under “My Servers” in Docker MCP Toolkit.</p><ul><li> including:\n<ul><li>firecrawl_check_crawl_status – check the status of a crawl job.</li><li>firecrawl_crawl – Starts a crawl job on a website and extracts content from all pages.</li><li>firecrawl_map – Map a website to discover all indexed URLs on the site</li><li>firecrawl_extract – Extract structured information from web pages using LLM capabilties.</li><li>firecrawl_scrape -Scrape content from a single URL with advanced options.</li><li>firecrawl_search – Search the web and optionally extract content from search results.</li></ul></li></ul><h4>Configure GitHub Official MCP Server</h4><p>The <a href=\"https://hub.docker.com/mcp/server/github-official/overview\" rel=\"nofollow noopener\" target=\"_blank\">GitHub MCP</a> enables Claude Desktop to create issues, PRs, and manage repositories on your behalf.</p><p><strong>Option 1: OAuth Authentication (Recommended – Easiest)</strong></p><ol><li>In  → , search </li><li>Go to the  tab in Docker Desktop</li><li>Your browser opens GitHub’s authorization page</li><li>Click  on GitHub</li><li>You’re redirected back to Docker Desktop</li><li>Return to  tab, find </li></ol><p> No manual token creation. Authorization happens through GitHub’s secure OAuth flow with automatic token refresh.</p><p><strong>Option 2: Personal Access Token (For Granular Control)</strong></p><p>If you prefer manual control or need specific scopes:</p><p><strong>Step 1: Create GitHub Personal Access Token</strong></p><ol><li>Click your  → </li><li>Scroll to  in the left sidebar</li><li>Click  → </li><li>Click  → <strong>“Generate new token (classic)”</strong></li><li>Name it: <strong>“Docker MCP Browser Testing”</strong></li><li><ul><li>repo (Full control of repositories)</li><li>workflow (Update GitHub Actions workflows)</li></ul></li><li><strong>Copy the token immediately</strong> (you won’t see it again!)</li></ol><p><strong>Step 2: Configure in Docker Desktop</strong></p><ol><li>In  → , find </li><li>Click  (if not already added)</li><li>Select  as the authentication method</li></ol><div><pre>docker mcp secret set GITHUB.PERSONAL_ACCESS_TOKEN=github_pat_YOUR_TOKEN_HERE\n</pre></div><h4>Configure Sequential Thinking MCP Server</h4><p>The <a href=\"https://hub.docker.com/mcp/server/sequentialthinking/overview\" rel=\"nofollow noopener\" target=\"_blank\">Sequential Thinking MCP server</a> gives Claude Desktop the ability for dynamic and reflective problem-solving through thought sequences. Adding the Sequential Thinking MCP server is straightforward –&nbsp; it doesn’t require any API key. Just search for Sequential Thinking in the Catalog and get it to your MCP server list.</p><ol><li>Open  →  → </li><li>Search for </li><li>Find in the results</li><li>Click “Add MCP Server” to add without any configuration</li></ol><p>The Sequential Thinking MCP MCP server should now appear under “My Servers” in Docker MCP Toolkit.</p><ul><li><strong>A single Sequential Thinking tool that</strong> includes:\n<ul><li>sequentialthinking – A detailed tool for dynamic and reflective problem-solving through thoughts. This tool helps analyze problems through a flexible thinking process that can adapt and evolve. Each thought can build on, question, or revise previous insights as understanding deepens.</li></ul></li></ul><h4>Configure Node.js Sandbox MCP Server</h4><p>The <a href=\"http://node.js\" rel=\"nofollow noopener\" target=\"_blank\">Node.js</a> Sandbox MCP enables Claude Desktop to spin up disposable Docker containers to execute arbitrary JavaScript. To get it added to the Claude Desktop, replace the contents of the Claude Desktop configuration file with the following JSON structure. This configuration tells Claude Desktop to start the <a href=\"http://node.js\" rel=\"nofollow noopener\" target=\"_blank\">Node.js</a> Sandbox with access to specific directories:</p><div><pre>{\n  \"mcpServers\": {\n    \"MCP_DOCKER\": {\n      \"command\": \"docker\",\n      \"args\": [\"mcp\", \"gateway\", \"run\"]\n    },\n    \"node-code-sandbox\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"-v\",\n        \"/var/run/docker.sock:/var/run/docker.sock\",\n        \"-v\",\n        \"/Users/YOUR_USERNAME/Desktop/sandbox-output:/root\",\n        \"mcp/node-code-sandbox\"\n      ],\n      \"env\": {\n        \"FILES_DIR\": \"/root\",\n        \"SANDBOX_MEMORY_LIMIT\": \"512m\",\n        \"SANDBOX_CPU_LIMIT\": \"0.75\"\n      }\n    }\n  }\n}\n\n\n</pre></div><p>Before you restart your Claude Desktop, make sure that the /Users/YOUR_USERNAME/Desktop/sandout-output directory exists on your local system and this directory is made available to containers via Docker Desktop &gt; Settings &gt; Resources &gt; File Sharing.</p><ul><li><a href=\"http://node.js\" rel=\"nofollow noopener\" target=\"_blank\"></a> including:\n<ul><li>get_dependency_types – Given an array of npm package names (and optional versions), fetch whether each package ships its own TypeScript definitions or has a corresponding @types/… package, and return the raw .d.ts text. Useful whenwhen you’re about to run a Node.js script against an unfamiliar dependency and want to inspect what APIs and types it exposes.</li><li>run_js- Install npm dependencies and run JavaScript code inside a running sandbox container.</li><li>run_js_ephermeral – Run a JavaScript snippet in a temporary disposable container with optional npm dependencies, then automatically clean up.</li><li>sandbox_exe – Execute one or more shell commands inside a running sandbox container. Requires a sandbox initialized beforehand.</li><li>sandbox_initialize – Start a new isolated Docker container running Node.js. Used to set up a sandbox session for multiple commands and scripts.</li><li>sandbox_stop – Terminate and remove a running sandbox container. Should be called after finishing work in a sandbox initialized with sandbox_initialize.</li><li>search_npm_packages – Search for npm packages by a search term and get their name, description, and a README snippet.</li></ul></li></ul><h4>Configure Context7 MCP Server</h4><p>The <a href=\"https://hub.docker.com/mcp/server/context7/overview\" rel=\"nofollow noopener\" target=\"_blank\">Context7 MCP</a> enables Claude Desktop to access the latest and up-to-date code documentation for LLMs and AI code editors. Adding Context7 MCP server is straightforward. It doesn’t require any API key. Just search for Context7 in the Catalog and get it added to the MCP server lists.</p><ol><li>Open  →  → </li><li>Find in the results</li><li>Click “Add MCP Server” to add without any configuration</li></ol><p>The Context7 MCP server should now appear under “My Servers” in Docker MCP Toolkit</p><ul><li> including:\n<ul><li>get-library-docs – Fetches up-to-date documentation for a library.</li><li>resolve-library-id – Resolves a package/product name to a Context7-compatible library ID and returns a list of matching libraries.&nbsp;</li></ul></li></ul><h3>Verify the available tools under Claude Desktop</h3><p>Once you have added all the MCP servers, click “Disconnect” and “Connect” so as to see the various MCP tools under MCP_DOCKER.</p><p>That’s it. It’s time to start interacting with your MCP servers and tools.</p><div><pre>I'm going to upload a screenshot of a UI component. Please execute this complete workflow using all available MCP tools: \n\nPHASE 0: STRATEGIC PLANNING Sequential Thinking to: \n\n1. Analyze what type of component this appears to be \n2. Determine what research and documentation we'll need \n3. Plan the component architecture and structure \n4. Identify testing requirements and edge cases \n5. Create a step-by-step implementation strategy \n6. Estimate the number of tests needed for full coverage \n\nPHASE 1: DESIGN RESEARCH \n\nUse Firecrawl to analyze the screenshot and extract: \n- Complete color palette (hex values) \n- All spacing and padding measurements \n- Typography specifications (font family, sizes, weights) - Layout structure (grid, flexbox patterns) \n- Component boundaries and hierarchy \n- Interactive elements (buttons, inputs, dropdowns) \n\nPHASE 2: DOCUMENTATION RESEARCH \n\nUse Context7 to fetch the latest documentation for: \n- React (latest hooks patterns and best practices) \n- Next.js (if applicable \n- Jest (current testing patterns and matchers) \n- React Testing Library (current query methods and best practices) \n- Lucide React or any icon library needed \n- Any other libraries identified in the planning phase \n\nPHASE 3: CODE GENERATION \n\nBased on the research from Phases 0-2, generate: \n1. A React/Next.js component that: \n   - Uses exact colors and spacing from Firecrawl analysis \n   - Follows current best practices from Context7 docs \n   - Implements the architecture from Sequential Thinking plan \n   - Includes proper TypeScript types \n   - Has clean, readable, production-ready code '\n\n2. Complete project structure: \n- package.json with correct dependencies \n- babel.config.js for Jest \n- .gitignore \n- README.md with setup instructions \n\nPHASE 4: COMPREHENSIVE TESTING \n\nGenerate Jest + React Testing Library tests covering: \n- Component rendering (all variants) \n- User interactions (clicks, inputs, selections) \n- State management and updates \n- Edge cases (empty states, loading, errors) \n- Accessibility (ARIA roles, keyboard navigation) \n- Props validation and defaults \n\nFollow current best practices from Context7 documentation. Aim for the test count estimated in Phase 0. \n\nPHASE 5: VALIDATION &amp; DEBUGGING \n\nUse Node.js Sandbox to: \n1. Install all dependencies in isolated container \n2. Run the complete test suite \n3. Capture all output and results \n\nIf ANY tests fail: \n- Use Sequential Thinking to: \n   * Analyze the error messages \n   * Generate 3 hypotheses for root cause \n   * Rank hypotheses by likelihood \n   * Test hypothesis 1 with a fix \n   * Re-run in Node.js Sandbox \n   * If still failing, move to hypothesis #2 \n   * Continue until all tests pass \n\nIf tests need clarification on errors: \n  - Use Context7 to look up specific error messages or API changes \n\n</pre></div><p>Ensure that you <a href=\"https://www.docker.com/resources/?tags=docker-mcp\">upload</a> the screenshot</p><p>Here’s how Claude Desktop responds.</p><div><pre>I'll execute this comprehensive workflow step-by-step using all available MCP tools. Let me start by analyzing the uploaded screenshot and planning our approach.\nPHASE 0: STRATEGIC PLANNING\nLet me use Sequential Thinking to analyze this component and create a comphrenesive implementation strategy.\nPerfect! I've completed the strategic planning. Now let's move to Phase 1.\nPHASE 1: DESIGN RESEARCH\n\nFor Phase 1, I'll use Firecrawl to scrape the uploaded image and extract visual specifications. However, Firecrawl is designed for web scraping, not image analysis. I should use the image directly for visual analysis and describe what I see.\n..\n\nPHASE 2: DOCUMENTATION RESEARCH\nNow let me fetch the latest documentation using Context7:\n...\nNow let me fetch React Testing Library documentation:...\nI'll use /websites/testing-library_com-docs as it has the most code snippets (565) and comprehensive documentation.\nPerfect! I have enough documentation. Now let's proceed to PHASE 3: CODE GENERATION\n...\nPerfect! Now let's move to PHASE 6: DEPLOYMENT. Let me create a GitHub repository\n..\n\nNow let's proceed with PHASE 4: COMPREHENSIVE TESTING. I'll create comprehensive test files for all components:\n...\nNow I'll create comprehensive tests for the resource library component based on the screenshot. Let me generate a complete test suite.\n...\nExcellent progress! CSS issue resolved - 35 of 41 tests now passing! Let me analyze the remaining 6 failures:\n\nBUG IDENTIFIED: Type Filter Mismatch\nProblem: Type filter keys don't match resource.type values:\nFilter keys: 'Infographics', 'Videos', 'White Papers' (plural/different)\nResource types: 'Infographic', 'Video', 'White Paper' (singular/different)\nFixing the component:\nSUCCESS! ALL 41 TESTS PASSING!\nTest Suites: 1 passed, 1 total\nTests:       41 passed, 41 total\nTime:        3.046 s\n..\nDue to character limits, let me push the updated files more efficiently. Since all tests pass locally, let me commit the fixes...\n\n</pre></div><p>Within minutes, Claude will:&nbsp;</p><ul><li>Write comprehensive tests&nbsp;</li><li>Run them in an isolated Docker container&nbsp;</li><li>Push the final code to GitHub with a live repo link</li></ul><p>You should be able to verify if <a href=\"http://node.js\" rel=\"nofollow noopener\" target=\"_blank\">Node.js</a> Sandbox container is running or not using Docker Dashboard. The logs show the MCP (Model Context Protocol) server successfully initializing inside a Docker container. The container starts a scavenger process that will automatically clean up after 1 hour of inactivity (checking every 60 seconds). It then performs a handshake announcing it supports MCP Protocol 2025-06-18 and server version 1.3.0.</p><p>The server registers 7 tools for JavaScript execution:&nbsp;</p><ul><li> (create persistent containers),&nbsp;</li><li> (run shell commands),&nbsp;</li><li> (run JS in persistent container),&nbsp;</li><li> (one-shot JS with auto-cleanup),&nbsp;</li><li> (fetch TypeScript definitions), and&nbsp;</li><li> (search npm registry).&nbsp;</li></ul><p>All tools that execute JavaScript require ESModules syntax and use the ./files directory for file persistence. The server is fully operational and ready to execute code!&nbsp;</p><p>You’ve just witnessed how Docker MCP Toolkit transforms Claude Desktop from a chat assistant into a genuine development partner.</p><p>What used to require hours of manual work—analyzing designs, writing code, creating tests, debugging failures, setting up repos—now happens in minutes with a single prompt.</p><p>This is the new paradigm for AI-assisted development. You’re not just using AI anymore. You’re  with it in a way that’s secure, reproducible, and production-ready.</p><p>Ready to try it? <a href=\"https://hub.docker.com/open-desktop?url=https://open.docker.com/dashboard/mcp\" rel=\"nofollow noopener\" target=\"_blank\">Open Docker Desktop</a> to get started with MCP Toolkit (<em>requires v4.48 or newer to launch automatically</em>).</p>","contentLength":23103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Modern Approach to Multi-Signal Optimization","url":"https://devops.com/a-modern-approach-to-multi-signal-optimization/","date":1761565493,"author":"Nikhil Kurup","guid":316731,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Patch Management is Essential for Securing DevOps","url":"https://devops.com/patch-management-is-essential-for-securing-devops/","date":1761564553,"author":"Alexander Williams","guid":316730,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Observability Improves User Experience and Digital Performance","url":"https://devops.com/how-observability-improves-user-experience-and-digital-performance/","date":1761563635,"author":"Neel Shah","guid":316729,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Claude Introduces Agent Skills for Custom AI Workflows","url":"https://devops.com/claude-introduces-agent-skills-for-custom-ai-workflows/","date":1761557013,"author":"Tom Smith","guid":316654,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Visual Studio Copilot Gets Planning Mode for Complex Tasks","url":"https://devops.com/visual-studio-copilot-gets-planning-mode-for-complex-tasks/","date":1761556483,"author":"Tom Smith","guid":316653,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-161/","date":1761555905,"author":"Mike Vizard","guid":316638,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mastering Linux Networking","url":"https://blog.devops.dev/mastering-linux-networking-7ff0a7af9a97?source=rss----33f8b2d9a328---4","date":1761490337,"author":"bektiaw","guid":315389,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}