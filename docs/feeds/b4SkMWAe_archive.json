{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":32,"items":[{"title":"Busting Myths About Cisco 700-150 ICS!","url":"https://www.reddit.com/r/kubernetes/comments/1iq3umq/busting_myths_about_cisco_700150_ics/","date":1739633610,"author":"/u/lucina_scott","guid":339,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Career transition in to Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/","date":1739626865,"author":"/u/Similar-Secretary-86","guid":340,"unread":true,"content":"<p>\"I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated </p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My new blog post comparing networking in EKS vs. GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/","date":1739617569,"author":"/u/jumiker","guid":341,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Dive into VPA Recommender","url":"https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/","date":1739615164,"author":"/u/erik_zilinsky","guid":342,"unread":true,"content":"<p>I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.</p><p>Based on my findings, I wrote a <a href=\"https://erikzilinsky.com/posts/vpa1.html\">blog post</a> about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.</p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container Networking - Kubernetes with Calico","url":"https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/","date":1739604355,"author":"/u/tkr_2020","guid":338,"unread":true,"content":"<ul><li>: VLAN 10</li><li>: VLAN 20</li></ul><p>When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:</p><p>The inner IP header reflects:</p><p>The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.</p><p>Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?</p>","contentLength":502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tech Leaders Reveal New Approaches to Corporate Sustainability","url":"https://devops.com/executive-strategies-driving-corporate-sustainability/","date":1739598887,"author":"Bonnie Schneider","guid":274,"unread":true,"content":"<article>Over the past two years, I‚Äôve interviewed more than 100 executives on tech innovation. Key insights emerged. But one stood out: sustainability is no longer a ‚Äúnice to have.‚Äù It‚Äôs now a core business strategy. That‚Äôs the focus of my inaugural, exclusive report: Decisions That Define: Executive Strategies Driving Corporate Sustainability. Why 2025 is a [‚Ä¶]</article>","contentLength":368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"For those managing or working with multiple clusters, do you use a combined kubeconfig file or separate by cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1ipg99n/for_those_managing_or_working_with_multiple/","date":1739555053,"author":"/u/trouphaz","guid":343,"unread":true,"content":"<p>I wonder if I'm in the minority. I have been keeping my kubeconfigs separate per cluster for years while I know others that combine everything to a single file. I started doing this because I didn't fully grasp yaml when I started and when I had an issue with the kubeconfig, I didn't have any idea on how to repair it. So I'd have to fully recreate it.</p><p>So, each cluster has its own kubeconfig file named for the cluster's name and I have a function that'll set my KUBECONFIG variable to the file using the cluster name.</p><pre><code>sc() { CLUSTER_NAME=\"${1}\" export KUBECONFIG=\"~/.kube/${CLUSTER_NAME}\" } </code></pre>","contentLength":592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DataRobot Acquires Agnostic to Gain Distributed Covalent Platform for AI Apps","url":"https://devops.com/datarobot-acquires-agnostic-to-gain-distributed-covalent-platform-for-ai-apps/","date":1739546554,"author":"Mike Vizard","guid":273,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11","url":"https://www.youtube.com/watch?v=eHa6GhK7L0I","date":1739541570,"author":"CNCF [Cloud Native Computing Foundation]","guid":318,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eHa6GhK7L0I?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 46 (Dragonfly)","url":"https://www.youtube.com/watch?v=gd6HRgr8KcA","date":1739512616,"author":"CNCF [Cloud Native Computing Foundation]","guid":317,"unread":true,"content":"<article>Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. \n\nThis CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.</article>","contentLength":576,"flags":null,"enclosureUrl":"https://www.youtube.com/v/gd6HRgr8KcA?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1739491200,"author":"","guid":433,"unread":true,"content":"<p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the \nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial  is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><div>sequenceDiagram\nautonumber\nrect rgb(191, 223, 255)\nKubelet-&gt;&gt;+Kube-apiserver: Create Node\nNote over Kubelet: Taint: node.cloudprovider.kubernetes.io\nKube-apiserver-&gt;&gt;-Kubelet: Node Created\nend\nNote over Kube-apiserver: Node is Not Ready<p> Tainted, Missing Node Addresses*, ...\nNote over Kube-apiserver: Send Updates\nrect rgb(200, 150, 255)\nKube-apiserver-&gt;&gt;+Cloud-controller-manager: Watch: New Node Created\nNote over Cloud-controller-manager: Initialize Node:</p>Cloud Provider Labels, Node Addresses, ...\nCloud-controller-manager-&gt;&gt;-Kube-apiserver: Update Node\nend\nNote over Kube-apiserver: Node is Ready\n</div><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a  resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a  resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one ‚Äúcorrect way‚Äù to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting ‚ÄúhostNetwork‚Äù to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider‚Äôs instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure‚Äôs node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.\n<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider‚Äôs documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nname: cloud-controller-manager\nnamespace: kube-system\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nannotations:\nkubernetes.io/description: Cloud controller manager for my infrastructure\nspec:\ncontainers: # the container details will depend on your specific cloud controller manager\n- name: cloud-controller-manager\ncommand:\n- /bin/my-infrastructure-cloud-controller-manager\n- --leader-elect=true\n- -v=1\nimage: registry/my-infrastructure-cloud-controller-manager@latest\nresources:\nrequests:\ncpu: 200m\nmemory: 50Mi\nhostNetwork: true # these Pods are part of the control plane\nnodeSelector:\nnode-role.kubernetes.io/control-plane: \"\"\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- topologyKey: \"kubernetes.io/hostname\"\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\ntolerations:\n- effect: NoSchedule\nkey: node-role.kubernetes.io/master\noperator: Exists\n- effect: NoExecute\nkey: node.kubernetes.io/unreachable\noperator: Exists\ntolerationSeconds: 120\n- effect: NoExecute\nkey: node.kubernetes.io/not-ready\noperator: Exists\ntolerationSeconds: 120\n- effect: NoSchedule\nkey: node.cloudprovider.kubernetes.io/uninitialized\noperator: Exists\n- effect: NoSchedule\nkey: node.kubernetes.io/not-ready\noperator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Terraform Architecture Explained , Terraform Core, State, and Plugins: How Terraform Works Under‚Ä¶","url":"https://blog.devops.dev/terraform-architecture-explained-terraform-core-state-and-plugins-how-terraform-works-under-a19e4d4dbb09?source=rss----33f8b2d9a328---4","date":1739465498,"author":"Kuseh Simon Wewoliamo","guid":455,"unread":true,"content":"<h3>Terraform Architecture Explained&nbsp;, Terraform Core, State, and Plugins: How Terraform Works Under the&nbsp;Hood.</h3><p><em>1. Introduction 2. Terraform Architecture4. Terraform Best Practices6. References</em></p><p>Infrastructure as Code (IaC), is an approach to managing and provisioning infrastructure by writing code instead of the manual processes&nbsp;, ‚ÄúClickOps‚Äù. IaC can be described as a mindset where you treat all aspects of operations (servers, databases, networks) as software. When you define your infrastructure using code&nbsp;, it enables you to automate and use all the best practices of software development. IaC eliminates human errors&nbsp;, speeds up infrastructure deployments and ensures infrastructure is version-controlled, just like software&nbsp;code.</p><p>Terraform is an open-source tool developed by HashiCorp and the most popular and widely used IaC tool used by DevOps, SREs and cloud architects. Terraform is widely used because of it‚Äôs declarative syntax, platform agnostic and its simplicity. Understanding how terraform works behind the hood will go along way to help you in write better terraform code.</p><p>In this article, we will explore Terraform architecture, its core components, and how it orchestrates infrastructure provisioning efficiently.</p><h3><em>2. Terraform Architecture</em></h3><p>Terraform follows a standard architecture to fulfill the necessary IaC tasks. Terraform architecture mainly consists of the following components:<em> 1 Terraform core 2 Plugins (Providers and Provisioners) </em></p><p>Terraform core is the engine/brain behind how terraform works. It is responsible for reading configurations files&nbsp;, building the dependency graphs from resources and data sources, managing state and applying changes. Terraform Core does not directly interact with cloud providers but communicates with plugins via remote procedure calls (RPCs) and the plugins in turn communicates with their corresponding platforms via&nbsp;HTTPs.</p><h4>Plugins (Providers and Provisioners)</h4><p>Terraform ability is enhance by plugins, which enable terraform to interact with cloud services and configure resources dynamically. Plugins acts as connectors or the glue between terraform and external APIs such as AWS, Azure, GCP, Kubernetes, Docker etc. Each plugin is written in the Go programming language and implements a specific interface. Terraform core knows how to install and execute plugins. Provisioners in Terraform are used to execute scripts or commands on a resource after it has been created or modified.</p><p>State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources it created. It is a costumed JSON file that terraform uses to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By default, state is stored in a local file named ‚Äúterraform.tfstate‚Äù. You can read more about terraform state <a href=\"http://State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources  it created. It is a customed  JSON file that terraform uses to  map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By  default, state is stored  in a local file named &quot;terraform.tfstate&quot;. You can read more about terraform state [here](https://developer.hashicorp.com/terraform/language/state)    There are two ways to manage state:  - 1. Local State:  Local State refers to the default way by which Terraform stores state files (terraform.tfstate).  It is suitable for small-scale projects or development environments and single person is managing Terraform.       - 2. Remote State: Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. [Remote State](https://developer.hashicorp.com/terraform/language/state/remote)\">here</a><p> There are two ways to manage state:</p> Local State refers to the default way by which Terraform stores state files (terraform.tfstate). It is suitable for small-scale projects or development environments and single person managing Terraform.</p><p>Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. More on <a href=\"https://developer.hashicorp.com/terraform/language/state/remote\">Remote&nbsp;State</a></p><p>Terraform follows a structured execution flow to provision, update, and manage infrastructure. This process ensures that infrastructure is deployed in a controlled and predictable manner. Terraform workflow consist of mainly three&nbsp;steps:</p><p> The first step is to write your terraform configuration just like any other code using any editor of your&nbsp;choice.</p><p> This is the step where you review your configurations. Terraform plan will define the infrastructure to be created, modified, or destroyed depending on the current configuration and infrastructure.</p><p> The final step in the workflow is Apply, where you are ready to provision real infrastructure. Once you approve of the changes&nbsp;,terraform will go ahead perform the desired actions as defined execution.</p><h3>4. Terraform Best Practices</h3><p>1. You should never edit the Terraform state files by hand or write code that reads them directly. If for some reason you need to manipulate the state file which should be a relatively rare occurrence, use the terraform import or terraform state commands.</p><p>2. It‚Äôs a good practice to store your work in a version controlled repository even when you‚Äôre just operating as an individual.</p><p>3. When working as a team, it‚Äôs important to delegate ownership of infrastructure across these teams and empower them to work in parallel without conflicts.</p><p>4. Never Store your state file in a version controlled repository.</p><p>5. Always use state locking on your state files to prevent data loss, conflicts and state file corruption.</p><p>6. Integrate Terraform to your CI/CD pipelines to make your DevOps pipeline efficient.</p><p>Well well, we have come to the end of this deep dive into terraform Architecture. To learn more about Terraform visit the <a href=\"https://developer.hashicorp.com\">official Terraform page</a>. Don‚Äôt forget to add your comments&nbsp;, till then keep&nbsp;coding.</p><p>6. Terraform:Up &amp; Running&nbsp;, Third Edition by Yevgeniy&nbsp;Brikman</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a19e4d4dbb09\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Micro Frontend Revolution","url":"https://blog.devops.dev/the-micro-frontend-revolution-29b6eedc8783?source=rss----33f8b2d9a328---4","date":1739465492,"author":"Adem KORKMAZ","guid":454,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Free AI models: Running Local LLMS with Llama 3.3,","url":"https://blog.devops.dev/running-local-llms-with-llama-3-3-deepseek-r1-and-other-large-language-models-using-ollama-5d0dc2d09358?source=rss----33f8b2d9a328---4","date":1739465463,"author":"Joel Wembo","guid":453,"unread":true,"content":"<h4>Part 4 of 10 Part series on DeepSeek&nbsp;MLOps</h4><h3>Free AI models: Running Local LLMS with Llama 3.3, DeepSeek-R1, and other Large Language Models using&nbsp;Ollama</h3><h4>Step-by-Step Guide: Installing a Web UI for Local LLMs on&nbsp;Windows</h4><p>With the rise of powerful open-source large language models (LLMs) like , , Phi-4, and Gemma 2, many users want to run these models locally for privacy, performance, and customization. However, interacting with these models via the command line can be limiting. <strong><em>The solution? A web-based user interface (UI) that allows easy interaction with your local&nbsp;LLMs.</em></strong></p><p>In this article, we will explore the best web UIs for running LLMs locally on Windows and guide you through the installation process.</p><p> is a lightweight, high-performance framework designed for running large language models (LLMs) locally with optimized execution. It works by leveraging <strong>GGUF (GGML Unified Format)</strong>, an efficient model storage format that supports quantization, allowing models to run smoothly even on consumer hardware.</p><h3>Why Use a Web UI for Local&nbsp;LLMs?</h3><p>Using a web UI for local LLMs offers several advantages:</p><ul><li>: No need to work with command-line tools.</li><li>: Manage multiple models in one&nbsp;place.</li><li>: Chat history, prompt engineering, and adjustable settings.</li><li>: Access your models remotely via a web&nbsp;browser</li></ul><h4>Step 1&nbsp;: Download and Install&nbsp;Ollama</h4><p>Download Ollama from <a href=\"https://ollama.com/download/windows\">https://ollama.com/download/windows</a>, then right click on the downloaded OllamaSetup.exe file and run the installer as administrator. Once the installation is complete, Ollama is ready to use on your Windows system. An Ollama icon will be added to the tray area at the bottom of the&nbsp;desktop.</p><p>To run Ollama and start utilizing its AI models, you‚Äôll need to use a terminal on Windows. We‚Äôll skip it here and let‚Äôs see how to install WebUI for a better experience.</p><p>Now open the browser and type localhost:11434 to check is Ollama is up and&nbsp;running</p><p>Also, Check in your system&nbsp;Tray</p><p>Next, Open your CMD to pull some free AI&nbsp;models</p><h4>Step 2‚Ää‚Äî‚ÄäInstall Ollama&nbsp;WebUI</h4><p>Run the below docker command to deploy ollama-webui docker container on your local machine. If Ollama is on your computer, use this&nbsp;command:</p><pre>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</pre><p>To connect to Ollama on another server, change the OLLAMA_BASE_URL to the server‚Äôs URL. So if Ollama is on a Different Server, use this&nbsp;command:</p><pre>docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</pre><p>Next, Open your browser and type localhost:3000</p><p>Ollama utilizes <strong>Metal on macOS and CUDA on Windows/Linux</strong> for hardware acceleration, enabling faster inference by directly leveraging GPU tensor operations. It runs a <strong>persistent server in the background</strong>, managing requests via an  that communicates with models using optimized token streaming.</p><p>Internally, it uses <strong>low-level memory-efficient inference kernels</strong>, minimizing VRAM and RAM usage while maintaining performance. It also supports <strong>LoRA (Low-Rank Adaptation) fine-tuning</strong>, allowing users to personalize models on their local machine with minimal compute overhead.</p><p>Run the following command:</p><pre>ollama run deepseek-r1:671b</pre><h3>Choosing the Right Web UI for Your&nbsp;Needs</h3><ul><li>: LM Studio (Simple setup, user-friendly UI)</li><li>: Oobabooga (More features, customization options)</li><li>: Gradio (Custom interface, lightweight solution)</li><li>: Open WebUI (Accessible over the internet)</li></ul><p>Setting up a web UI for local LLMs on Windows significantly enhances your experience, making it easier to interact with AI models without complex command-line operations. Whether you‚Äôre a beginner or an advanced user, the right UI can streamline your workflow and unlock new possibilities with local AI&nbsp;models.</p><p>Start today with one of these web UIs and bring AI power to your local machine!&nbsp;üöÄ</p><p>Thank you for Reading&nbsp;!! üôåüèª, don‚Äôt forget to subscribe and give it a&nbsp;CLAP</p><p><em>, cloud Solutions architect, Back-end developer, and AWS Community Builder, currently working at prodxcloud as a DevOps &amp; Cloud Architect. I bring a powerful combination of expertise in cloud architecture, DevOps practices, and a deep understanding of high availability (HA) principles. For more information about the author ( </em><a href=\"https://joelwembo.com/\"></a><a href=\"https://www.linkedin.com/in/joelotepawembo/\"></a><a href=\"https://github.com/joelwembo\"></a><a href=\"https://twitter.com/joelwembo1\"></a><a href=\"http://joelwembo.github.io/\"></a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d0dc2d09358\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Container Orchestration (AWS ECS, AWS EKS & Kubernetes)","url":"https://blog.devops.dev/understanding-container-orchestration-aws-ecs-aws-eks-kubernetes-baee401db009?source=rss----33f8b2d9a328---4","date":1739465455,"author":"Althaf Hussain","guid":452,"unread":true,"content":"<h3>Why Do We Need Container Orchestration?</h3><p>1Ô∏è‚É£ <strong>We use Docker to create and run containers</strong></p><ul><li>Docker  using a Dockerfile with :</li><li>: Packages and compiles the&nbsp;app.</li><li>: Runs the app and exposes&nbsp;ports.</li></ul><pre>docker run -p 80:80 my-app</pre><ul><li>Now the app is running inside a . üéâ</li></ul><p>2Ô∏è‚É£ <strong>But what if the app crashes due to high&nbsp;traffic?</strong></p><ul><li><strong>Docker cannot restart or scale the&nbsp;app</strong>.</li><li>If there‚Äôs high traffic (e.g., festive season sales), .</li></ul><p>3Ô∏è‚É£ <strong>Solution? We need a tool to manage containers automatically!</strong></p><ul><li>This is where <strong>Container Orchestration Tools</strong> come&nbsp;in!</li><li>Examples: <strong>Kubernetes, AWS ECS, AWS EKS, Azure AKS, Google GKE, OpenShift</strong>.</li></ul><h3>üöÄ Kubernetes‚Ää‚Äî‚ÄäFull Control but Complex&nbsp;Setup</h3><p>‚úÖ  When you want  over your cluster.‚úÖ </p><ul><li><strong>Manages multiple containers</strong> (Docker is just for one container).</li><li> (if traffic increases, it adds more containers).</li><li> (if an app crashes, Kubernetes restarts&nbsp;it).</li></ul><h3>What is Kubernetes (Self-Managed)?</h3><p>If you want  over your cluster, you can <strong>set up Kubernetes manually</strong>.</p><h4>How Kubernetes Works (Practical Steps)</h4><p>1Ô∏è‚É£ Create a <strong>VM or server (EC2, Azure VM, GCP VM, on-premise server, etc.).</strong>2Ô∏è‚É£ Install <strong>Kubernetes, kubeadm, kubectl, networking, storage, etc.</strong>3Ô∏è‚É£ Set up a  and .4Ô∏è‚É£ Deploy your app using a .5Ô∏è‚É£ Manage <strong>scaling, auto-healing, networking, etc.</strong> manually.</p><h4>üõ†Ô∏è Steps to Deploy an App Using Kubernetes:</h4><p>1Ô∏è‚É£ <strong>Set up a server (EC2 instance or&nbsp;VM)</strong></p><pre>sudo apt update &amp;&amp; sudo apt install -y curl apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -<p>echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list</p>sudo apt update<p>sudo apt install -y kubelet kubeadm kubectl</p></pre><p>2Ô∏è‚É£ <strong>Initialize Kubernetes cluster</strong></p><pre>mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<p>sudo chown $(id -u):$(id -g) $HOME/.kube/config</p></pre><pre>apiVersion: apps/v1kind: Deployment  name: my-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-app<p>        image: my-docker-image:latest</p>        ports:</pre><pre>kubectl apply -f deployment.yaml</pre><pre>apiVersion: v1kind: Service  name: my-app-service  type: LoadBalancer    app: my-app    - protocol: TCP      targetPort: 80</pre><pre>kubectl apply -f service.yaml</pre><p>üéØ <strong>Your app is now running inside Kubernetes!</strong> üöÄ</p><h4>‚úÖ Advantages of Self-Managed Kubernetes</h4><p>‚úî  ‚Üí You can configure every part of the cluster.‚úî  ‚Üí On-premise, AWS, Azure, GCP, or hybrid cloud.‚úî  ‚Üí You‚Äôre not tied to AWS, Azure, or any provider.‚úî  ‚Üí Most companies use  for flexibility.</p><h4>‚ùå Disadvantages of Self-Managed Kubernetes</h4><p>‚úñ  ‚Üí You need to manually configure <strong>networking, storage, security, etc.</strong>‚úñ  ‚Üí You have to <strong>patch, upgrade, and secure</strong> the cluster yourself.‚úñ  ‚Üí Setting up and managing Kubernetes is .</p><h3>üöÄ AWS ECS‚Ää‚Äî‚ÄäAWS Manages Everything</h3><p>‚úÖ  Running containers <strong>without managing Kubernetes</strong>.‚úÖ </p><ul><li>You <strong>don‚Äôt need to set up Kubernetes</strong>.</li><li>Just  what app you want to run, and it does everything.</li><li> over cluster management.</li></ul><h3>üõ†Ô∏è Steps to Deploy an App in AWS&nbsp;ECS</h3><p>1Ô∏è‚É£  in the AWS Console.2Ô∏è‚É£  (Choose Fargate or EC2).3Ô∏è‚É£ :</p><ul><li>Go to <strong>ECS &gt; Task Definitions &gt; Create new task definition</strong>.</li><li>Choose  (serverless) or  (self-managed).</li><li>Define  (Docker image, ports, CPU, memory).4Ô∏è‚É£ :</li><li>Go to <strong>ECS &gt; Services &gt; Create&nbsp;Service</strong>.</li><li>Choose the <strong>cluster and task definition</strong> you&nbsp;created.</li><li>Define  (number of tasks).5Ô∏è‚É£ &nbsp;üéâ</li></ul><p>üéØ <strong>Your app is running inside AWS ECS without managing infrastructure!</strong> üöÄ</p><p>‚úî  ‚Üí AWS takes care of the infrastructure.‚úî  ‚Üí No need to set up Kubernetes manually.‚úî <strong>Tightly integrated with AWS</strong> ‚Üí Works great with AWS services like ALB, IAM, CloudWatch, etc.‚úî <strong>Less operational overhead</strong> ‚Üí No need to worry about maintaining a&nbsp;cluster.</p><h4>‚ùå Disadvantages of AWS&nbsp;ECS</h4><p>‚úñ  ‚Üí If you want to move your app from AWS to , or , you have to <strong>set up everything from scratch</strong>.‚úñ  ‚Üí You don‚Äôt have full control over how the cluster is managed.‚úñ  ‚Üí Most companies prefer  over ECS for multi-cloud strategies.</p><h3>üöÄ AWS EKS‚Ää‚Äî‚ÄäAWS Manages Kubernetes for&nbsp;You</h3><p>‚úÖ  When you want <strong>Kubernetes but don‚Äôt want to install it manually</strong>.‚úÖ </p><ul><li>AWS  (no need to install manually).</li><li>You  to deploy&nbsp;apps.</li><li> than ECS but  than DIY Kubernetes.</li></ul><h3>üõ†Ô∏è Steps to Deploy an App in AWS&nbsp;EKS</h3><p>1Ô∏è‚É£  in the AWS Console.2Ô∏è‚É£ :</p><ul><li>Set <strong>Cluster name, VPC, IAM&nbsp;role</strong>.</li><li>AWS will create &amp; configure the Kubernetes control plane.3Ô∏è‚É£ </li></ul><pre>aws eks update-kubeconfig --region your-region --name your-cluster-name</pre><p>4Ô∏è‚É£  (same as Kubernetes DIY)</p><pre>kubectl apply -f deployment.yaml</pre><p>5Ô∏è‚É£ <strong>Expose the app using a Kubernetes service</strong> (same as&nbsp;before).</p><p>üéØ <strong>Your app is running in AWS EKS with Kubernetes, but AWS helps with the setup!</strong>&nbsp;üöÄ</p><p>‚úî  ‚Üí Works exactly like Kubernetes, so it‚Äôs <strong>easy to move to another cloud</strong> (Azure AKS, Google GKE, etc.).‚úî <strong>Fully managed control plane</strong> ‚Üí AWS handles the  (setting up Kubernetes).‚úî  than ECS ‚Üí You can tweak networking, security, and scaling.‚úî  ‚Üí You can run Kubernetes anywhere (AWS, Azure, GCP, or on-premise).</p><h4>‚ùå Disadvantages of AWS&nbsp;EKS</h4><p>‚úñ  ‚Üí You still need to understand Kubernetes concepts.‚úñ <strong>More operational overhead</strong> ‚Üí Though AWS sets up Kubernetes, you still .‚úñ  ‚Üí You  for the Kubernetes control&nbsp;plane.</p><h3>üéØ Real-World Example of How These Work&nbsp;Together</h3><p>Imagine you‚Äôre running an :1Ô∏è‚É£ You  to package your app into a container.2Ô∏è‚É£ You deploy it to <strong>Kubernetes (DIY) if you want full control</strong>.3Ô∏è‚É£ If you <strong>don‚Äôt want to manage Kubernetes</strong>, you use  (simplest).4Ô∏è‚É£ If you <strong>need Kubernetes but don‚Äôt want manual setup</strong>, you use .</p><p>üìå <strong>Think of Kubernetes as a powerful machine where you control everything.</strong>üìå <strong>Think of AWS ECS as a service where AWS does the heavy lifting for you.</strong>üìå <strong>Think of AWS EKS as Kubernetes, but AWS helps with&nbsp;setup.</strong></p><h3>Conclusion: Which One Should You&nbsp;Use?</h3><p>üëâ  if you  and don‚Äôt care about moving to another cloud.üëâ  if you  but don‚Äôt want to set it up manually.üëâ <strong>Use Self-Managed Kubernetes</strong> if you  and <strong>plan to run across multiple clouds (AWS, Azure, GCP, on-premise, etc.).</strong></p><p>üí° If you‚Äôre , start with .If you‚Äôre building , go for .If you want , use .</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=baee401db009\" width=\"1\" height=\"1\" alt=\"\">","contentLength":6091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding APIs: A Developer‚Äôs Guide to Building and Using APIs","url":"https://blog.devops.dev/understanding-apis-a-developers-guide-to-building-and-using-apis-4253418d18ba?source=rss----33f8b2d9a328---4","date":1739465421,"author":"Subbareddysangham","guid":451,"unread":true,"content":"<p>An Application Programming Interface (API) acts as a bridge between different software applications, allowing them to communicate with each other. Think of an API like a waiter in a restaurant‚Ää‚Äî‚Ääcustomers (the client application) don‚Äôt need to know how the kitchen (the server) prepares their food; they need to know how to place their order through the waiter (the&nbsp;API).</p><p>I designed and developed an e-commerce web application with <strong>HTML, CSS, and JavaScript</strong> for the front end,  for the back end, and  for the database. I will use this as an example to explain the core concepts of&nbsp;APIs.</p><h3>E-commerce Web Application API Flow&nbsp;Chart:</h3><p><strong><em>To check the complete source&nbsp;code:</em></strong></p><h3>APIs in the E-Commerce Application</h3><p><strong>This E-Commerce application</strong> consists of the following API endpoints:</p><h4>1. Authentication API (auth_routes)</h4><ul><li>: /api/auth/login (POST) ‚Üí Authenticates users and starts a&nbsp;session.</li><li>: /api/auth/logout (POST) ‚Üí Clears session and logs out&nbsp;users.</li></ul><h4>2. Product API (product_routes)</h4><ul><li>: /api/products (GET) ‚Üí Returns a list of products.</li><li>: /api/products/&lt;int:product_id&gt; (GET) ‚Üí Fetches details of a specific&nbsp;product.</li></ul><h4>3. Cart API (cart_routes)</h4><ul><li>: /api/cart (GET) ‚Üí Returns the current user's&nbsp;cart.</li><li>: /api/cart (POST) ‚Üí Adds a product to the&nbsp;cart.</li><li>: /api/cart/&lt;int:item_id&gt; (DELETE) ‚Üí Removes an item from the&nbsp;cart.</li></ul><h4>4. Order API (order_routes)</h4><ul><li>: /api/orders (POST) ‚Üí Places an order with the items in the&nbsp;cart.</li><li>: /api/orders/&lt;int:order_id&gt; (GET) ‚Üí Fetches details of a specific&nbsp;order.</li></ul><ul><li>: /api/health (GET) ‚Üí Provides API uptime, session data, and frontend&nbsp;path.</li></ul><ul><li>: /&lt;path:filename&gt; (GET) ‚Üí Serves frontend&nbsp;files.</li><li>: / (GET) ‚Üí Serves index.html or API running&nbsp;message.</li></ul><ul><li>: Handles missing resources.</li><li><strong>500 Internal Server Error</strong>: Handles unexpected issues.</li><li>: Handles invalid requests.</li></ul><p>An API consists of several key components that work together:</p><ol><li>These are the URLs where the  can be accessed. Similar to a , each endpoint serves a specific purpose. For example,üìå <strong>https://api.ecommerce.com/products</strong> ‚Üí Retrieves a list of available products.üìå <strong>https://api.ecommerce.com/cart</strong> ‚Üí Fetches the current user's shopping cart details.üìå <strong>https://api.ecommerce.com/orders</strong> ‚Üí Handles order-related operations.</li></ol><p>These actions can be performed on the allowed endpoints. They‚Äôre like verbs telling the API what to do with the&nbsp;data.</p><ul><li> ‚Üí Read data (<strong>View products, orders, cart&nbsp;items</strong>).</li><li> ‚Üí Create new data (<strong>Add product, register user, place&nbsp;order</strong>).</li><li> ‚Üí Update existing data (<strong>Update profile, modify cart quantity</strong>).</li><li> ‚Üí Remove data (<strong>Delete cart item, cancel&nbsp;order</strong>).</li></ul><p> Additional data is sent to fine-tune the API request, such as specifying which page of results you want to&nbsp;see.</p><ul><li>: Parameters are extra details added to an API request to filter or refine the&nbsp;results.</li><li>Fetch  of products:</li></ul><pre>GET /api/products?category=laptops</pre><p>Security measures ensure that only authorized users can access the&nbsp;API.</p><ul><li>: Ensures that only authorized users can access the&nbsp;API.</li></ul><p>When a user logs in, the API gives a&nbsp;:</p><pre>{  \"message\": \"Login successful\",<p>  \"token\": \"eyJhbGciOiJIUz...\"</p>}</pre><p>To <strong>add a product to the cart</strong>, the request must include this&nbsp;:</p><pre>POST /api/cart/addAuthorization: Bearer eyJhbGciOiJIUz...</pre><p> Prevents unauthorized access and protects user&nbsp;data.</p><p> The structure of the data returned by the API, commonly in formats like JSON or&nbsp;XML.</p><ul><li>: The structure of the data sent back by the&nbsp;API.</li><li> (because it‚Äôs easy to read and&nbsp;use).</li></ul><pre>{  \"id\": 1,  \"price\": 799.99,}</pre><p> Frontend uses this data to display products to&nbsp;users.</p><p>APIs are classified according to their usage patterns and architectures.</p><h3>API Types According to Purposes of&nbsp;Use</h3><p>üîπ ‚Ää‚Äî‚ÄäUsed within a company, hidden from public access. Helps teams share data securely.</p><p>üîπ ‚Ää‚Äî‚ÄäAvailable to everyone, can be free or paid. Example: Google Maps&nbsp;API.</p><p>üîπ ‚Ää‚Äî‚ÄäUsed between business partners for secure data exchange. Example: E-commerce &amp; shipping company integration.</p><p>üîπ ‚Ää‚Äî‚ÄäCombines multiple APIs into one request for efficiency. Example: Fetching account balance + transaction history in one&nbsp;call.</p><h3>API Types According to Architectural Structure:</h3><h3>1. Web APIs (HTTP/HTTPS APIs)</h3><p>These are the most common APIs, operating over the internet using HTTP protocols. They come in several varieties:</p><h3>1.1. REST (Representational State Transfer):</h3><p>The most popular type of web API today. REST APIs follow these principles:</p><ul><li>Stateless: Each request contains all the information needed</li><li>Resource-based: Everything is treated as a resource with a unique&nbsp;URL</li><li>Uses standard HTTP methods (GET, POST, PUT,&nbsp;DELETE)</li><li>Supports multiple data formats (usually&nbsp;JSON)</li></ul><h3><strong><em>Example REST API Request in an E-Commerce Web Application:</em></strong></h3><p>This request <strong>fetches all available products</strong> from the online&nbsp;store.</p><p>‚úÖ <strong>Request (Client ‚Üí&nbsp;Server)</strong></p><pre>GET /api/products HTTP/1.1Host: api.ecommerce.com<p>Authorization: Bearer &lt;User_Token&gt;</p>Content-Type: application/json</pre><p>‚úÖ <strong>Response (Server ‚Üí&nbsp;Client)</strong></p><pre>[    {\"id\": 1, \"name\": \"iPhone 15\", \"price\": 999.99, \"stock\": 20},<p>    {\"id\": 2, \"name\": \"Samsung Galaxy S24\", \"price\": 899.99, \"stock\": 15}</p>]</pre><h3><strong><em>catalog.html Fetches and Displays&nbsp;Products</em></strong></h3><h4>1Ô∏è‚É£ User Visits catalog.html</h4><ul><li>The user opens the  page in their browser (http://52.90.222.178:5000/catalog.html).</li><li>The browser  to fetch product&nbsp;data.</li></ul><h4>2Ô∏è‚É£ Frontend (JavaScript) Sends an API&nbsp;Request</h4><ul><li>JavaScript code in catalog.html makes a GET request to the  /api/products.</li></ul><h4>3Ô∏è‚É£ Backend API (GET /api/products) Fetches&nbsp;Data</h4><ul><li>The get_all_products() function runs when the frontend calls /api/products.</li></ul><h4>4Ô∏è‚É£ Database Retrieves Product Information</h4><ul><li>The backend queries the  in the&nbsp;database</li><li>Example database response:</li></ul><pre>[    {\"id\": 1, \"name\": \"Laptop\", \"price\": 799.99},<p>    {\"id\": 2, \"name\": \"Smartphone\", \"price\": 499.99}</p>]</pre><h4>5Ô∏è‚É£ Frontend Renders Product Data in catalog.html</h4><ul><li>The JavaScript loops through the  and dynamically  to display products.</li></ul><pre>&lt;div class=\"product-card\"&gt;  &lt;h3&gt;Laptop&lt;/h3&gt;  &lt;button onclick=\"addToCart(1)\"&gt;Add to Cart&lt;/button&gt;&lt;div class=\"product-card\"&gt;  &lt;p&gt;Price: $499.99&lt;/p&gt;<p>  &lt;button onclick=\"addToCart(2)\"&gt;Add to Cart&lt;/button&gt;</p>&lt;/div&gt;</pre><h3>1.2. SOAP (Simple Object Access Protocol):</h3><p>SOAP has strict rules and rigid messaging standards that can make it more secure than protocols such as REST. These types of APIs are frequently used in enterprise applications, particularly for payment processing and customer management, as they are highly safe in&nbsp;nature.</p><p>A more rigid, protocol-specific API style used in enterprise environments:</p><ul><li>Highly structured messaging</li></ul><pre>&lt;soap:Envelope&gt;  &lt;soap:Header&gt;<p>    &lt;Authorization&gt;Bearer abc123&lt;/Authorization&gt;</p>  &lt;/soap:Header&gt;    &lt;GetUser&gt;    &lt;/GetUser&gt;&lt;/soap:Envelope&gt;</pre><p>A modern API query language that gives clients more&nbsp;control:</p><ul><li>Clients specify precisely what data they&nbsp;need</li><li>Single endpoint for all&nbsp;requests</li><li>Reduces over-fetching and under-fetching of&nbsp;data</li></ul><p>Facebook initially developed GraphQL to simplify endpoint management for REST-based APIs. Instead of maintaining multiple endpoints with small amounts of disjointed data, GraphQL provides a single endpoint that inputs complex queries and outputs only as much information as is needed for the&nbsp;query.</p><pre>query {  user(id: \"123\") {    email      title  }</pre><p>These are programming interfaces provided by software libraries or frameworks:</p><ul><li>Used directly in your&nbsp;code</li><li>No network requests are&nbsp;needed</li><li>Usually specific to a programming language</li></ul><p>Example using a Python library&nbsp;API:</p><pre>import pandas as pd<p># Using pandas API to read a CSV file</p>df = pd.read_csv('data.csv')</pre><p>These allow applications to interact with the operating system:</p><ul></ul><p>Example using Python‚Äôs OS&nbsp;API:</p><pre>import os<p># Using OS API to create a directory</p>os.mkdir('new_folder')</pre><p>The foundation of web APIs, using well-defined methods and status&nbsp;codes:</p><ul><li>PUT: Update existing&nbsp;data</li><li>PATCH: Partially update&nbsp;data</li></ul><ul><li>2xx: Success (e.g., 200&nbsp;OK)</li></ul><p>Enables real-time, two-way communication:</p><ul><li>Ideal for chat apps and live&nbsp;updates</li></ul><p><strong><em>Example WebSocket connection:</em></strong></p><pre>const ws = new WebSocket('wss://api.example.com/chat');ws.onmessage = (event) =&gt; {<p>    console.log('Received:', event.data);</p>};</pre><p><strong>gRPC (Google Remote Procedure Call)</strong> is a  framework for <strong>inter-service communication</strong> in <strong>microservices architecture</strong>. Unlike REST APIs that use , gRPC uses <strong>Protocol Buffers (Protobuf)</strong>, making it <strong>faster and more efficient</strong>.</p><p>Google‚Äôs high-performance RPC framework:</p><ul><li>Excellent for microservices</li></ul><p><strong><em>Example Protocol Buffer definition:</em></strong></p><h3>üìå How gRPC Works in a Web Application</h3><p>In an , gRPC can be used for <strong>fast communication between microservices</strong>.</p><p>A  must fetch a  from the backend .</p><h4>1Ô∏è‚É£ Defining gRPC Service (product.proto)</h4><p>gRPC services use <strong>Protocol Buffers (Protobuf)</strong> to define API contracts.</p><ul><li>GetAllProducts(): Returns a list of products.</li><li>GetProductById(): Fetches a single product by&nbsp;ID.</li><li>Product: Defines the product structure.</li></ul><h4>2Ô∏è‚É£ Implementing gRPC Server (product_server.py)</h4><p>The gRPC server <strong>implements the service&nbsp;logic</strong>.</p><ul><li>Implements ProductService methods (GetAllProducts, GetProductById).</li></ul><h4>3Ô∏è‚É£ Implementing gRPC Client (product_client.py)</h4><p>The client  to fetch&nbsp;data.</p><ul><li>Calls GetAllProducts() to fetch all products.</li><li>Calls GetProductById() to fetch a single&nbsp;product.</li></ul><p>4Ô∏è‚É£ Running gRPC Server &amp;&nbsp;Client</p><pre># 1. Generate Python code from Protobufpython -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. product.protopython product_server.pypython product_client.py</pre><pre>ProductService gRPC Server is running on port 50051...Product List: products {  name: \"Laptop\"}  id: 2  price: 499.99</pre><h3>1. GET: Used to retrieve&nbsp;data</h3><p>It is a request used to retrieve data. Never used to delete, update or insert&nbsp;data.</p><ul></ul><h4>Product API in E-Commerce Web Application</h4><pre>curl -X GET http://52.90.222.178:5000/api/products</pre><pre>[   {\"id\": 1, \"name\": \"iPhone 15\", \"price\": 999.99},<p>   {\"id\": 2, \"name\": \"Samsung Galaxy S24\", \"price\": 899.99}</p>]</pre><p>If the API returns JSON, you can format the response using&nbsp;</p><pre>curl -X GET http://52.90.222.178:5000/api/products | jq</pre><h3>Debugging &amp; Troubleshooting</h3><p>If you‚Äôre not getting the expected response, check:</p><p><strong>Is the Flask server&nbsp;running?</strong></p><pre>curl -X GET http://52.90.222.178:5000/api/health</pre><h3>How the /api/health Endpoint Works in&nbsp;Flask</h3><p>The endpoint is a  that provides the current <strong>status of the application</strong>, including . It helps in monitoring the system and ensuring that the API is .</p><p>I have configured this /api/health endpoint in my E-Commerce Web Application.</p><pre>@app.route(\"/api/health\", methods=[\"GET\"])def health_check():<p>    uptime = time.time() - start_time</p>    return jsonify({        \"uptime\": f\"{uptime:.2f} seconds\",<p>        \"session_active\": \"username\" in session</p>    }), 200</pre><h3>2. POST: Creates new resources</h3><p>The  method is a request used to insert data. Posted data type‚Ää‚Äî‚ÄäJSON.</p><ul></ul><p>‚úÖ Authentication Endpoints</p><p>This is handled in auth_routes.py, prefixed with /api/auth.</p><p> ‚Üí The frontend sends a request to the backend API (POST /api/auth/login). This is handled in auth_routes.py, prefixed with /api/auth.</p><ol><li>User sends a  with username &amp; password.</li><li>If credentials are&nbsp;valid:</li></ol><ul><li>The user session is&nbsp;stored.</li><li>API returns a success&nbsp;message.</li></ul><p>3. If credentials are&nbsp;invalid:</p><ul><li>API returns </li></ul><h3>3. PUT&nbsp;: Updates existing resources</h3><p>PUT method is used to create or update (replace) a resource. Useful for syncing&nbsp;data.</p><ul></ul><p>Ex: We can <strong>add a ‚ÄúChange Password‚Äù feature</strong> for an existing user using a <strong>PUT /api/auth/change-password</strong> API endpoint.</p><h3>Steps to Implement ‚ÄúChange Password‚Äù API</h3><ol><li><strong>PUT request with their old and new password.</strong></li></ol><pre>curl -X PUT http://localhost:5000/api/auth/change-password      -H \"Content-Type: app<p>     -d '{\"old_password\": \"currentPass123\", \"new_password\": \"newPass456\"}'</p></pre><p><strong>2. API verifies the old password</strong>:</p><ul><li>If incorrect, return an error (401 Unauthorized).</li></ul><p><strong>3. If correct, update the password in the database</strong>.</p><p><strong>4. Save the new password (after hashing it for security).</strong></p><p><strong>5. Return a success&nbsp;message.</strong></p><pre>{\"message\": \"Password changed successfully\"}</pre><h3><strong><em>In an e-commerce application like yours, a </em></strong><strong><em>PUT method would typically be used&nbsp;in:</em></strong></h3><ol><li> ‚Üí PUT /api/auth/update-profile</li><li><strong>Updating Product Information (Admin)</strong> ‚Üí PUT /api/products/&lt;product_id&gt;</li><li> ‚Üí PUT /api/cart/&lt;cart_id&gt;</li><li> ‚Üí PUT /api/orders/&lt;order_id&gt;</li></ol><h3><strong>4. DELETE: Removes resources</strong></h3><p>The DELETE method deletes the specified resource.</p><ul></ul><h4><strong><em>/api/cart API to Remove a Product from the&nbsp;Cart</em></strong></h4><p>1Ô∏è‚É£ Endpoint Definition (Flask&nbsp;API):</p><pre>@cart_bp.route('/cart', methods=['DELETE'])def remove_from_cart():    Remove a product from the cart for the logged-in user.<p>    Expects JSON payload: { product_id }.</p>    \"\"\"        user_id = session.get('user_id')            return jsonify({\"message\": \"User not authenticated\"}), 401        product_id = data.get('product_id')            return jsonify({\"message\": \"'product_id' is required\"}), 400<p>        connection = get_db_connection()</p>        cursor = connection.cursor()<p>        delete_query = \"DELETE FROM cart_items WHERE user_id = %s AND product_id = %s\"</p>        cursor.execute(delete_query, (user_id, product_id))            return jsonify({\"message\": \"Product not found in cart\"}), 404        return jsonify({\"message\": \"Product removed from cart successfully\"}), 200        return jsonify({\"message\": \"Failed to remove product from cart\", \"error\": str(e)}), 500        close_db_connection(connection)</pre><ol><li><strong>User sends a DELETE request</strong> with the product_id they want to&nbsp;remove.</li><li><strong>API verifies if the user is logged in</strong> (checks session['user_id']).</li><li> (if missing, returns 400 Bad Request).</li><li> to remove the product from the cart_items table.</li><li><strong>If the product does not exist</strong>, it returns 404 Not&nbsp;Found.</li><li>, it commits the transaction and returns a success message (200&nbsp;OK).</li><li><strong>Handles database errors and ensures the connection is&nbsp;closed.</strong></li></ol><p>3Ô∏è‚É£ Example API Request &amp; Response:</p><p>Now User wanted to delete iPhone 15 Pro from the&nbsp;cart:</p><ul><li>Click the  button under ‚ÄúiPhone 15&nbsp;Pro‚Äù.</li><li>The item should disappear, and the cart total should&nbsp;update.</li></ul><p>Run this command in the terminal:</p><pre>curl -X DELETE http://52.90.222.178:5000/api/cart      -H \"Content-Type: application/json\" <p>     -H \"Cookie: session=00068d4c-4b41-4e3e-8884-7389cabbb9b0\"</p>     -d '{\"product_id\": 4}'</pre><pre>{    \"message\": \"Product removed from cart successfully\"</pre><p>After deletion of that&nbsp;item:</p><h3>5. PATCH: Partially updates resources</h3><p>PATCH method is to request used to update data. Only passed data will be updated. You don‚Äôt need to provide all the data&nbsp;set.</p><ul></ul><p>The  method is used to  a resource. Instead of sending the entire data set, we <strong>only send the fields that need to be&nbsp;updated</strong>.</p><h4>Use Case: Updating a User‚Äôs Profile (PATCH /api/auth/update-profile)</h4><p>Imagine a user wants to update  or  without changing their username.</p><p>1Ô∏è‚É£ PATCH Endpoint: PATCH /api/auth/update-profile</p><p>2Ô∏è‚É£ Sending a PATCH&nbsp;Request</p><p>If the user wants to update </p><pre>curl -X PATCH http://52.90.222.178:5000/api/auth/update-profile \\     -H \"Content-Type: application/json\" \\<p>     -H \"Cookie: session=your_valid_session_id\" \\</p>     -d '{\"email\": \"newemail@example.com\"}'</pre><p>üîπ Only the  field will be&nbsp;updated.</p><p>‚úÖ </p><pre>{    \"message\": \"Profile updated successfully\"</pre><p>‚ùå <strong>If no fields are provided:</strong></p><pre>{    \"message\": \"No valid fields provided for update\"</pre><pre>{    \"message\": \"User not authenticated\"</pre><h4>4Ô∏è‚É£ Why Use PATCH Instead of&nbsp;PUT?</h4><h3>Conclusion: Understanding APIs, Endpoints, and Methods in Web Development</h3><p>APIs (Application Programming Interfaces) allow different systems to  with each other. They define how requests and responses are exchanged between a client (browser, app) and a&nbsp;server.</p><ul><li>An  acts as a bridge between two applications, enabling data exchange.</li><li>Example: A shopping website uses an API to fetch product details from a database.</li></ul><ul><li>An  is a URL that clients use to request or send&nbsp;data.</li><li>Example: GET /api/products retrieves all products.</li></ul><ul><li> ‚Üí Uses HTTP methods (GET, POST, PUT, DELETE) to manage&nbsp;data.</li><li> ‚Üí Lets clients request specific data fields, reducing unnecessary data transfer.</li><li> ‚Üí Uses XML messaging, mainly in enterprise applications.</li><li> ‚Üí Maintains a continuous connection for real-time updates (e.g., live&nbsp;chat).</li></ul><ul><li>Use  (JWT, API Keys, OAuth) to restrict&nbsp;access.</li><li>Protect sensitive data with .</li><li>Implement  to prevent&nbsp;abuse.</li></ul><p>APIs are the backbone of modern applications, enabling data sharing between different services. Developers create smooth and efficient digital experiences by designing well-structured and secure&nbsp;APIs.</p><p><em>I‚Äôd love to hear what you think about this article‚Ää‚Äî‚Ääfeel free to share your opinions in the comments below (or above, depending on your device!). If you found this helpful or enjoyable, a clap, a comment, or a highlight of your favourite sections would mean a&nbsp;lot.</em></p><p><em>For more insights into the world of technology and data, visit </em><a href=\"http://www.subbutechops.com/\"></a><em> There‚Äôs plenty of exciting content waiting for you to&nbsp;explore!</em></p><p><em>Thank you for reading, and happy learning! üöÄ</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4253418d18ba\" width=\"1\" height=\"1\" alt=\"\">","contentLength":16408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Most Developers Get This Wrong in Docker Networking!","url":"https://blog.devops.dev/most-developers-get-this-wrong-in-docker-networking-359dbb3eac16?source=rss----33f8b2d9a328---4","date":1739465415,"author":"Gaddam.Naveen","guid":450,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jenkins in Kubernetes: Deployment and Persistent Storage(volume) Setup","url":"https://blog.devops.dev/jenkins-in-kubernetes-deployment-and-persistent-storage-volume-setup-a70fe0579ac8?source=rss----33f8b2d9a328---4","date":1739465410,"author":"th@n@n","guid":449,"unread":true,"content":"<p>Jenkins, a popular automation server, becomes even more powerful when deployed in Kubernetes. Ensuring its availability and data persistence is crucial for uninterrupted CI/CD pipelines. In this guide, we‚Äôll walk through deploying Jenkins in Kubernetes, configuring its resources, and setting up persistent storage to safeguard critical&nbsp;data.</p><p>In this configuration, we have a Deployment resource for deploying Jenkins in Kubernetes, along with associated PersistentVolumeClaim (PVC), PersistentVolume (PV), Service, and StorageClass resources. Let‚Äôs break down each&nbsp;part</p><pre>kind: StorageClassapiVersion: storage.k8s.io/v1  name: localstorage<p>provisioner: kubernetes.io/no-provisioner</p>volumeBindingMode: WaitForFirstConsumer</pre><ul><li>This StorageClass resource defines storage provisioning and management policies.</li><li>Since provisioner is set to kubernetes.io/no-provisioner, it indicates that no dynamic provisioning is performed by Kubernetes.</li><li>volumeBindingMode: WaitForFirstConsumer ensures that volume binding waits for the first Pod using the PersistentVolumeClaim to be&nbsp;created.</li></ul><h3>PersistentVolumeClaim (PVC) Resource:</h3><pre>apiVersion: v1kind: PersistentVolumeClaim  name: pvc-jenkinsspec:<p>  storageClassName: localstorage</p>  accessModes:  resources:      storage: 2Gi</pre><ul><li>This PVC resource requests storage from a PersistentVolume using the localstorage StorageClass.</li><li>It requests 2Gi of storage with access mode ReadWriteOnce, meaning it can be mounted as read-write by a single&nbsp;node.</li></ul><h3>PersistentVolume (PV) Resource:</h3><pre>apiVersion: v1kind: PersistentVolume  name: pv-jenkins    type: local  claimRef:    namespace: jenkins    storage: 3Gi    - ReadWriteOnce    path: /mnt<p>  storageClassName: localstorage</p></pre><ul><li>This PV resource represents the actual storage volume available for use by the&nbsp;PVC.</li><li>It is bound to the PVC pvc-jenkins within the jenkins namespace.</li><li>The PV has a capacity of 3Gi and is accessible in ReadWriteOnce mode.</li><li>The storage is provided by a hostPath /mnt on the host machine, with storage class localstorage.</li></ul><pre>apiVersion: apps/v1kind: Deployment  name: jenkins-deployment    name: jenkinsspec:    matchLabels:  replicas: 1    metadata:      labels:    spec:        - name: deployment-jenkins<p>          image: jenkins/jenkins:lts</p>          resources:              memory: \"0.5Gi\"            requests:              cpu: \"125m\"            - name: http-port            - name: jnlp-port          livenessProbe:              path: \"/login\"            initialDelaySeconds: 60            timeoutSeconds: 5          readinessProbe:              path: \"/login\"            initialDelaySeconds: 60            timeoutSeconds: 5          volumeMounts:              mountPath: /var/jenkins_home        - name: data-jenkins            claimName: pvc-jenkins        runAsUser: 0        fsGroup: 0</pre><ul><li>This Deployment resource defines how Jenkins is deployed.</li><li>It specifies a single replica (replicas: 1) of the Jenkins container.</li><li>The container is based on the jenkins/jenkins:lts image.</li><li>Resource limits and requests for CPU and memory are set to ensure resource allocation.</li><li>Ports 8080 and 50000 are exposed for HTTP and JNLP respectively.</li><li>Liveness and readiness probes are configured to check the health of the container.</li><li>The Jenkins home directory (/var/jenkins_home) is mounted to a PersistentVolumeClaim (pvc-jenkins) named data-jenkins.</li><li>SecurityContext ensures that Jenkins runs with the appropriate user and group permissions.</li></ul><pre> securityContext:        runAsUser: 0        fsGroup: 0</pre><p>When you deploy this deployment instance in kubernetes cluster, make sure the user have the right privileges to read and write the host volume For this demo purpose, I am using the root user to do this task, but this is not encouraged to do in real environment.</p><pre>apiVersion: v1kind: Service  name: jenkins-servicespec:    app: jenkins-pod  ports:      port: 8080      nodePort: 32000</pre><ul><li>This Service resource exposes the Jenkins deployment internally within the jenkins namespace.</li><li>It selects pods with the label app: jenkins-pod.</li><li>The service type is NodePort, making the service accessible from outside the cluster on each node's IP at a static port (nodePort: 32000).</li><li>Port 8080 is mapped to the targetPort 8080 where Jenkins is listening.</li></ul><p>Once you execute all the manifiest file in kubernetes cluster.</p><p>Check the host volume path ls -al&nbsp;/mnt</p><p>Execute the below command to see the whether the same files are present in the jenkins container</p><pre>kubectl exec -it POD_NAME /bin/bash -n jenkinsls -al /var/jenkins_home</pre><p>This configuration sets up Jenkins deployment in Kubernetes with persistence using a PersistentVolume and PersistentVolumeClaim. It ensures that Jenkins data stored in /var/jenkins_home persists across container restarts and pod rescheduling. Additionally, the Service resource exposes Jenkins for external access within the Kubernetes cluster.</p><p>For now, that‚Äôs it guys, If you like this article don‚Äôt forget to give a clap.&nbsp;üëè</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a70fe0579ac8\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Build and Deploy a Simple Frontend App with Python Backend","url":"https://blog.devops.dev/how-to-build-and-deploy-a-simple-frontend-app-with-python-backend-108b505be2be?source=rss----33f8b2d9a328---4","date":1739465406,"author":"krth1k","guid":448,"unread":true,"content":"<p>Building a full-stack web application might seem daunting, especially if you‚Äôre primarily a backend developer. However, with the right approach, you can create a simple frontend and connect it to a Python backend with&nbsp;ease.</p><p>In this guide, we‚Äôll walk through the process&nbsp;of:</p><ul><li>Setting up a basic Python backend with&nbsp;Flask</li><li>Creating a simple frontend with HTML, CSS, and JavaScript</li><li>Connecting the frontend to the backend using REST&nbsp;API</li><li>Deploying the app on a local Kubernetes cluster</li></ul><h3>1. Setting Up the Python&nbsp;Backend</h3><p>We‚Äôll use , a lightweight Python web framework, to create a REST API that serves data to the frontend.</p><p>Ensure you have Python installed, then install&nbsp;Flask:</p><p>Create a new file called&nbsp;app.py:</p><pre>from flask import Flask, jsonify</pre><pre>@app.route('/api/message')def get_message():<p>    return jsonify({\"message\": \"Hello from the Python backend!\"})</p></pre><pre>if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)</pre><p>This API exposes a single endpoint /api/message that returns a JSON response.</p><p>For the frontend, we‚Äôll use <strong>HTML, CSS, and JavaScript</strong> to display the data from our&nbsp;backend.</p><h3>Create an HTML File (index.html)</h3><pre>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;    &lt;meta charset=\"UTF-8\"&gt;<p>    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;</p>    &lt;title&gt;Frontend App&lt;/title&gt;        body {<p>            font-family: Arial, sans-serif;</p>            text-align: center;        }            padding: 10px 20px;        }&lt;/head&gt;    &lt;h1&gt;Simple Frontend App&lt;/h1&gt;<p>    &lt;button onclick=\"fetchMessage()\"&gt;Get Message&lt;/button&gt;</p>    &lt;p id=\"message\"&gt;&lt;/p&gt;        function fetchMessage() {<p>            fetch('http://127.0.0.1:5000/api/message')</p>                .then(response =&gt; response.json())                    document.getElementById(\"message\").innerText = data.message;                .catch(error =&gt; console.error('Error:', error));    &lt;/script&gt;&lt;/html&gt;</pre><p>This page has a button that fetches and displays a message from the Flask&nbsp;backend.</p><h3>3. Connecting the Frontend to the&nbsp;Backend</h3><p>Now, let‚Äôs serve the frontend using  itself so that both frontend and backend are accessible from the same&nbsp;origin.</p><h3>Update app.py to Serve&nbsp;HTML</h3><p>Modify app.py to serve the index.html file:</p><pre>from flask import Flask, jsonify, send_from_directory</pre><pre>app = Flask(__name__, static_folder='static')</pre><pre>@app.route('/api/message')def get_message():<p>    return jsonify({\"message\": \"Hello from the Python backend!\"})</p></pre><pre>@app.route('/')def serve_frontend():<p>    return send_from_directory('static', 'index.html')</p></pre><pre>if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)</pre><h3>Move index.html to a static&nbsp;Folder</h3><p>Your project structure should now look like&nbsp;this:</p><pre>/project-folder‚îÇ-- app.py‚îÇ   ‚îî‚îÄ‚îÄ index.html</pre><p>Now, visit http://127.0.0.1:5000/ in your browser, and your frontend will be&nbsp;served!</p><p>Now, let‚Äôs deploy this app using .</p><pre># Use Python base imageFROM python:3.9</pre><pre># Set the working directoryWORKDIR /app</pre><pre># Copy application filesCOPY . .</pre><pre># Install dependenciesRUN pip install flask</pre><pre># Expose port 5000EXPOSE 5000</pre><pre># Run the appCMD [\"python\", \"app.py\"]</pre><h3>Build and Run the Docker Container</h3><pre>docker build -t myapp .docker run -p 5000:5000 myapp</pre><ol><li><strong>Create a Kubernetes Deployment YAML (</strong></li></ol><pre>apiVersion: apps/v1kind: Deployment  name: myapp  replicas: 1    matchLabels:  template:      labels:    spec:        - name: myapp          ports:---kind: Service  name: myapp-service  selector:  ports:      port: 80  type: NodePort</pre><pre>kubectl apply -f deployment.yaml</pre><pre>minikube service myapp-service --url</pre><p>Visit the displayed URL in your&nbsp;browser!</p><p>In this tutorial, we covered: ‚úÖ Creating a Flask backend<p> ‚úÖ Building a simple HTML/JavaScript frontend</p> ‚úÖ Connecting the frontend to the backend<p> ‚úÖ Deploying the app with Docker and Kubernetes</p></p><p>This is a basic example, but you can expand it&nbsp;by:</p><ul><li>Adding user authentication</li><li>Using React or Vue.js for a modern&nbsp;frontend</li><li>Storing and retrieving data from a&nbsp;database</li></ul><p>If you found this helpful, let me know in the comments! üöÄ</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=108b505be2be\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Practices For Database Authorization In Multi-Tenant Systems","url":"https://blog.devops.dev/best-practices-for-database-authorization-in-multi-tenant-systems-001a1bcf2568?source=rss----33f8b2d9a328---4","date":1739465383,"author":"Noel","guid":447,"unread":true,"content":"<p>Multi-tenant databases allow multiple companies or organizations (tenants) to securely share the same database infrastructure while ensuring data isolation and integrity. However, this shared structure introduces complexities in managing access and authorization. A robust authorization strategy is essential to ensure that users can only access resources belonging to their tenant without compromising scalability or performance.</p><p>This article explores the best practices and technical solutions that we adopt at <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Best+Practices+For+Database+Authorization+In+Multi-Tenant+Systems\">Xinthe</a>, for implementing efficient authorization mechanisms in multi-tenant systems, with a focus on nested resource structures, such as companies, clients, projects, and tasks. After having read this entire article, you will be armed with actionable insights to build secure, efficient, and future-proof authorization strategies for multi-tenant applications.</p><h3><strong>Understanding Multi-Tenant Database Authorization</strong></h3><ul><li><strong><em>Definition Of Multi-Tenancy</em></strong></li></ul><p>Multi-tenancy refers to an architectural pattern where a single instance of a software application and its database serves multiple tenants (e.g., companies, organizations, or users). Each tenant‚Äôs data remains logically isolated, ensuring that no tenant can access another‚Äôs data, while sharing underlying resources for efficiency.</p><p><strong>Key Multi-Tenancy Models:</strong></p><p>Each tenant has a dedicated database or&nbsp;schema.</p><p>Offers strong isolation and security.</p><p>Higher costs and maintenance complexity due to multiple instances.</p><p>Multiple tenants share the same database.</p><p>Logical separation is maintained through identifiers (e.g., tenant_id or company_id).</p><p>Cost-effective and scalable but requires robust authorization mechanisms.</p><ul><li><strong><em>Challenges Of Authorization</em></strong></li></ul><p>Implementing authorization in multi-tenant systems is a non-trivial task, especially as the scale and complexity of resources grow. Common challenges include&nbsp;-</p><p><strong>Cross-Tenant Data&nbsp;Leakage:</strong></p><p>Risk: Improper queries or configurations can expose data to unauthorized tenants.</p><p>Example: A user from Company A inadvertently accessing tasks belonging to Company B due to a missing or incorrect WHERE&nbsp;clause.</p><p>Deeply nested resource structures often require joins across multiple&nbsp;tables.</p><p>Queries with extensive joins can degrade performance as data volume increases.</p><p><strong>Scalability &amp; Maintainability:</strong></p><p>The need to balance fast access controls with a maintainable schema.</p><p>Adding new authorization rules or resource types without overhauling the&nbsp;system.</p><p><strong>Data Localization &amp; Compliance:</strong></p><p>For multi-tenant systems spanning regions, ensuring that tenant data complies with regulations like GDPR can complicate authorization logic.</p><ul><li><strong><em>Importance Of Nested Resource Structures</em></strong></li></ul><p>In many applications, resources are interconnected in a hierarchical fashion. Consider the following nested structure -</p><p><strong>Company ‚Üí Client ‚Üí Project ‚Üí&nbsp;Task</strong></p><p>A  has multiple&nbsp;.</p><p>Each  manages several .</p><p>Each  contains multiple&nbsp;.</p><p><strong>Why Nested Structures Matter:</strong></p><p><strong>Access Control Complexity:</strong> Permissions must flow through the hierarchy (e.g., a user‚Äôs access to a task must be verified against their company).</p><p> Hierarchical access often necessitates multiple joins, impacting query efficiency.</p><p> Hierarchical structures reflect real-world use cases like SaaS platforms, where users must operate within their organization‚Äôs boundaries.</p><p>A user from Company A should only edit tasks within their projects. Authorization must ensure that the task ‚Üí project ‚Üí client ‚Üí company linkage is maintained without exposing data from Company&nbsp;B.</p><h3><strong>Comparing Authorization Approaches</strong></h3><p>When implementing authorization in a multi-tenant database, there are three common strategies to choose from: the , the , and <strong>Tenant-Specific Databases or Tables</strong>. Each comes with its own set of benefits and trade-offs. Let‚Äôs break them down&nbsp;-</p><ul><li><strong><em>1. Flat Model (Adding Tenant&nbsp;IDs)</em></strong></li></ul><p>In this approach, a tenant_id or company_id is added to every resource table (e.g., tasks, projects, clients), enabling direct filtering for authorization.</p><p> Queries can directly filter by tenant_id without traversing the hierarchy.</p><pre>SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;</pre><p> Reduces query complexity by avoiding multiple table joins to enforce&nbsp;access.</p><p> Straightforward implementation makes it easy to debug and maintain.</p><p> tenant_id is replicated across multiple tables, introducing redundancy.</p><p> Adding tenant_id and other metadata can lead to bloated schemas, especially as the number of attributes grows.</p><p> Schema updates (e.g., adding new relationships) might require extensive changes across multiple&nbsp;tables.</p><p>Ideal for systems where performance is critical and the schema is relatively stable, such as SaaS platforms with many small&nbsp;tenants.</p><ul><li><strong><em>2. Hierarchical Model (Enforcing Relationships)</em></strong></li></ul><p>In this approach, the relationships between resources (e.g., task ‚Üí project ‚Üí client ‚Üí company) are strictly enforced through foreign keys. Authorization is achieved by traversing the hierarchy.</p><p> Avoids redundant fields by relying on inherent relationships.</p><pre>CREATE TABLE tasks (    id SERIAL PRIMARY KEY,<p>    project_id INT REFERENCES projects(id),</p>    ...    id SERIAL PRIMARY KEY,<p>    client_id INT REFERENCES clients(id),</p>    ...    id SERIAL PRIMARY KEY,<p>    company_id INT REFERENCES companies(id),</p>    ...</pre><p> Reduces duplication of metadata like tenant_id.</p><p><strong>Relationship-Centric Queries:</strong> Makes it easier to enforce hierarchical constraints and maintain referential integrity.</p><p> Queries require multiple joins to verify access, which can impact performance.</p><pre>SELECT t.*FROM tasks t<p>JOIN projects p ON t.project_id = p.id</p>JOIN clients c ON p.client_id = c.id<p>WHERE c.company_id = :tenant_id AND t.id = :task_id;</p></pre><p> Deep hierarchies with large datasets can significantly increase query execution time.</p><p> As the hierarchy grows, maintaining performance becomes challenging.</p><p>Suitable for applications where maintaining strict relationships between resources is essential, such as ERP systems or large enterprise applications.</p><ul><li><strong><em>3. Tenant-Specific Databases Or&nbsp;Tables</em></strong></li></ul><p>This approach creates separate databases or tables for each tenant, isolating their data entirely.</p><p> Each tenant‚Äôs data can be managed independently, making it easier to scale horizontally by distributing databases across&nbsp;servers.</p><p> Ensures complete data isolation, reducing the risk of cross-tenant data&nbsp;leakage.</p><p> Simplifies adherence to regulations like GDPR by enabling tenant-specific backups, retention policies, and deletions.</p><p> Managing multiple databases or schemas requires sophisticated deployment and CI/CD pipelines.</p><p> Schema updates need to be applied consistently across all tenant databases.</p><p> For tenants with small datasets, the resource consumption of separate databases might be inefficient.</p><p>Best for large organizations with high regulatory or security requirements, or when dealing with tenants that require dedicated resources (e.g., enterprise customers).</p><ul><li><strong><em>Summary Table‚Ää‚Äî‚ÄäComparing Approaches</em></strong></li></ul><h3><strong>Criteria For Choosing An Authorization Model</strong></h3><p>Selecting the right authorization model for a multi-tenant database is critical for ensuring scalability, performance, and compliance. The decision hinges on a combination of technical, regulatory, and operational factors. Below are the primary criteria to consider&nbsp;-</p><p>The level of traffic and query complexity your application handles directly impacts the choice of an authorization model.</p><p><strong>High-Traffic Applications</strong>:</p><p>Benefit from simpler and faster queries, such as those enabled by the .</p><pre>SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;</pre><p>Minimal joins mean lower query latency, ensuring the system performs well under heavy&nbsp;loads.</p><p>Suitable for SaaS platforms or e-commerce systems with a high volume of tenant interactions.</p><p>:</p><p>Can afford the  with more joins, as performance trade-offs are less significant.</p><p>Allows for cleaner schema designs and strict relational integrity.</p><p>Suitable for internal enterprise tools or smaller-scale applications.</p><ul><li><strong><em>2. Regulatory Requirements</em></strong></li></ul><p>Compliance with data protection and privacy regulations often dictates how data is stored and accessed.</p><p>Using <strong>Tenant-Specific Databases or Tables</strong> simplifies compliance for regulations like GDPR or&nbsp;HIPAA.</p><p>Tenant isolation reduces the risk of data leakage and ensures tenant-specific data retention and deletion policies.</p><p>An enterprise customer requires dedicated storage with separate backups and audit&nbsp;logs.</p><p>A  can still meet compliance needs with appropriate access controls and audit mechanisms.</p><p>Challenges arise in managing and enforcing tenant-specific data governance policies within shared infrastructure.</p><p>The ability to handle growth in the number of tenants and data volumes is a critical&nbsp;factor.</p><p><strong>Planning For Tenant&nbsp;Growth</strong>:</p><p>For a rapidly scaling user base, <strong>Tenant-Specific Databases or Tables</strong> provide the most flexibility -</p><p>Each tenant can be distributed across servers to balance&nbsp;load.</p><p>Tenant databases can be independently scaled based on specific&nbsp;needs.</p><p>A B2B SaaS platform serving both small businesses and large enterprises can allocate resources dynamically based on tenant&nbsp;size.</p><p>The  can handle larger datasets more efficiently as indexes on tenant_id make filtering faster.</p><p>The  may struggle as table sizes grow, requiring optimization for complex&nbsp;joins.</p><p>Ease of schema management and updates is essential for long-term maintainability.</p><p><strong>Simplified Schema&nbsp;Updates</strong>:</p><p>The  simplifies schema updates by centralizing data attributes like tenant_id.</p><p>However, redundant fields may increase the risk of errors during&nbsp;updates.</p><p>The  enforces relational integrity, ensuring data consistency.</p><p>Complex queries for nested structures may require more effort to maintain and optimize.</p><p><strong>Automated CI/CD Pipelines</strong>:</p><p>For <strong>Tenant-Specific Databases</strong>, CI/CD automation becomes critical to manage schema changes across multiple databases.</p><p>Tools like Octopus Deploy or Liquibase can help automate schema migrations and ensure consistency.</p><ul><li><strong><em>Key Considerations Summary</em></strong></li></ul><h3><strong>Designing Authorization Strategies For Multi-Tenancy</strong></h3><p>Designing robust authorization strategies for multi-tenant systems requires careful consideration of schema design, indexing, and data partitioning to ensure scalability, security, and performance. This section outlines best practices for implementing these strategies effectively.</p><p>The foundation of a successful multi-tenant authorization system lies in a well-thought-out schema.</p><p>Add a tenant_id column to all relevant tables (e.g., clients, projects, tasks) for direct tenant filtering.</p><pre>CREATE TABLE tasks (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    FOREIGN KEY (project_id) REFERENCES projects(id)</p>);</pre><p>Ensure tenant_id is a mandatory field in all write operations to enforce multi-tenancy constraints.</p><p><strong>Defining Relationships In Hierarchical Structures</strong>:</p><p>Maintain strict referential integrity between hierarchical entities.</p><p>Example for hierarchical relationships -</p><pre>CREATE TABLE projects (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    client_id BIGINT NOT NULL,    FOREIGN KEY (client_id) REFERENCES clients(id)</pre><p>Flat schema enables quick lookups for tenant-specific data.</p><p>Hierarchical relationships ensure data consistency and logical separation.</p><ul><li><strong><em>2. Indexing Best Practices</em></strong></li></ul><p>Indexes are essential for optimizing queries in multi-tenant systems. However, improper indexing can lead to inefficiencies.</p><p><strong>Compound Indexes For Tenant-Specific Queries</strong>:</p><p>Use composite indexes combining tenant_id with frequently queried&nbsp;columns.</p><pre>CREATE INDEX idx_tasks_tenant_projectON tasks (tenant_id, project_id, status);</pre><p>This enables efficient filtering and sorting within a tenant‚Äôs&nbsp;scope.</p><p><strong>Balancing Indexing Depth &amp; Query&nbsp;Speed</strong>:</p><p>Avoid over-indexing, which can slow down write operations.</p><p>Prioritize indexing columns involved in filtering, joining, and sorting operations.</p><p>Regularly analyze query performance using tools like  in PostgreSQL or  in&nbsp;MySQL.</p><p>Partitioning improves scalability by dividing data into smaller, more manageable segments, reducing query times for tenant-specific operations.</p><p><strong>Horizontal Partitioning By&nbsp;Tenants</strong>:</p><p>Partition data within a single database based on tenant_id.</p><pre>CREATE TABLE tasks_1 PARTITION OF tasksFOR VALUES IN (1); -- Partition for tenant_id 1</pre><p>Faster tenant-specific queries as partitions reduce the search&nbsp;space.</p><p>Simplifies maintenance for large datasets.</p><p><strong>Database Sharding For High-Scale Systems</strong>:</p><p>Distribute tenant data across multiple databases (shards).</p><p>Example Sharding Strategy&nbsp;-</p><p>Use tenant_id % shard_count to assign tenants to&nbsp;shards.</p><p>Tools like  or  can manage sharding in distributed database&nbsp;systems.</p><p>Eliminates contention in single-database systems.</p><p>Enhances fault isolation and scalability.</p><p><strong>Example Use Case‚Ää‚Äî‚ÄäApplying These Strategies</strong></p><p>An application manages 100,000 tenants, each with thousands of projects and&nbsp;tasks.</p><p>Add tenant_id to all&nbsp;tables.</p><p>Use foreign keys to link tasks ‚Üí projects ‚Üí&nbsp;clients.</p><p>Create a compound index on tasks (tenant_id, project_id) for common queries like&nbsp;-</p><pre>SELECT * FROM tasks WHERE tenant_id = 123 AND project_id = 456;</pre><p>For smaller tenants, use horizontal partitioning -</p><pre>CREATE TABLE tasks_tenant_123 PARTITION OF tasks FOR VALUES IN (123);</pre><p>For larger tenants, shard data across multiple databases to&nbsp;scale.</p><p>A well-designed schema with tenant_id simplifies multi-tenant data filtering.</p><p>Proper indexing ensures efficient queries, even at&nbsp;scale.</p><p>Partitioning and sharding prepare the system for growth, reducing query times and enhancing reliability.</p><p>This section provides concrete examples of implementing different authorization models for multi-tenant systems, including schemas, queries, and tooling. Each approach demonstrates how to enforce tenant-specific access effectively.</p><ul><li><strong><em>1. Flat Model Implementation</em></strong></li></ul><p>The flat model relies on adding a tenant_id column to all relevant tables, ensuring that queries are scoped to the tenant directly.</p><pre>CREATE TABLE tasks (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,</p>    FOREIGN KEY (project_id) REFERENCES projects(id)    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    client_id BIGINT NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,<p>    FOREIGN KEY (client_id) REFERENCES clients(id)</p>);</pre><p>: Access tasks for a user‚Äôs company&nbsp;-</p><pre>SELECT * FROM tasks <p>WHERE tenant_id = 123 AND status = 'in_progress';</p></pre><p>Simplifies authorization logic with direct&nbsp;lookups.</p><p>Reduces query complexity by avoiding&nbsp;joins.</p><p>Potential schema bloat with additional tenant_id columns.</p><ul><li><strong><em>2. Hierarchical Model Implementation</em></strong></li></ul><p>In this model, tenant authorization is enforced by traversing relationships between resources (e.g., Company ‚Üí Client ‚Üí Project ‚Üí&nbsp;Task).</p><pre>CREATE TABLE companies (    id BIGINT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT PRIMARY KEY,<p>    company_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    FOREIGN KEY (company_id) REFERENCES companies(id)</p>);    id BIGINT PRIMARY KEY,<p>    client_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    FOREIGN KEY (client_id) REFERENCES clients(id)</p>);    id BIGINT PRIMARY KEY,<p>    project_id BIGINT NOT NULL,</p>    description TEXT,    FOREIGN KEY (project_id) REFERENCES projects(id)</pre><p>: Check task access by traversing relationships -</p><pre>SELECT t.* FROM tasks t<p>INNER JOIN projects p ON t.project_id = p.id</p>INNER JOIN clients c ON p.client_id = c.id<p>INNER JOIN companies co ON c.company_id = co.id</p>WHERE co.id = 123 AND t.status = 'in_progress';</pre><p>Maintains normalized relationships.</p><p>Avoids redundant tenant_id columns.</p><p>Complex joins increase query&nbsp;costs.</p><p>Requires optimized indexes to maintain performance.</p><ul><li><strong><em>3. Tenant-Specific Database/Table Implementation</em></strong></li></ul><p>For scenarios requiring strict isolation, separate databases or tables for each tenant can be&nbsp;used.</p><p>Create a separate database or schema for each tenant&nbsp;-</p><pre>CREATE DATABASE company_123;CREATE TABLE company_123.tasks (    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><p>: Access tasks for a specific tenant&nbsp;-</p><pre>USE company_123;SELECT * WHERE status = 'in_progress';</pre><p>: Use CI/CD tools like  to manage multi-tenant databases:</p><p>Automate schema changes across databases.</p><p>Track versioning for each tenant‚Äôs database.</p><p>Complete tenant isolation for security and compliance (e.g.,&nbsp;GDPR).</p><p>Simplifies data archival and backup for individual tenants.</p><p>Deployment complexity increases with the number of&nbsp;tenants.</p><p>Resource-intensive for systems with many small&nbsp;tenants.</p><ul><li><strong><em>Choosing The Right Implementation</em></strong></li></ul><p>Use the flat model for simplicity in high-traffic environments.</p><p>Use the hierarchical model when data relationships must be preserved and redundancy minimized.</p><p>Opt for tenant-specific databases for strict isolation and compliance requirements.</p><p>Each implementation can be tailored based on application needs, tenant size, and regulatory requirements. Balancing performance, scalability, and maintainability is key to successful multi-tenant authorization systems.</p><h3><strong>Security Best Practices For Authorization</strong></h3><p>Ensuring robust security in multi-tenant systems is essential to prevent data breaches, maintain compliance, and build user trust. This section outlines key practices for implementing secure and reliable authorization mechanisms.</p><ul><li><strong><em>1. Strict Access&nbsp;Controls</em></strong></li></ul><p>Implementing strong access controls ensures that only authorized users can access or modify resources.</p><p><strong>Role-Based Access Control&nbsp;(RBAC)</strong>:</p><p>Assign roles (e.g., Admin, Manager, User) to users based on their responsibilities.</p><p>Enforce role-specific permissions at the application and database&nbsp;layers.</p><p>Example: Use database roles to restrict access to tenant-specific tables.</p><pre>CREATE ROLE company_admin;GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO company_admin;<p>REVOKE ALL ON ALL TABLES FROM PUBLIC; -- Restrict public access</p></pre><p>:</p><p>Enforce tenant-level data segregation directly at the database&nbsp;layer.</p><p>RLS ensures that queries automatically filter data based on the user‚Äôs&nbsp;tenant.</p><p><strong>PostgreSQL Example For&nbsp;RLS</strong>:</p><pre>CREATE POLICY tenant_policyON tasks<p>USING (tenant_id = current_setting('app.current_tenant')::BIGINT);</p><p>ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;</p>SET app.current_tenant = '123'; -- Simulate tenant context<p>SELECT * FROM tasks; -- Only tasks with tenant_id = 123 will be visible</p></pre><ul><li><strong><em>2. Preventing Cross-Tenant Data&nbsp;Leaks</em></strong></li></ul><p>Preventing accidental or intentional cross-tenant data leaks is critical in multi-tenant architectures.</p><p><strong>Multi-Layer Access&nbsp;Checks</strong>:</p><p>Enforce tenant isolation at both the database and application layers.</p><p>Validate all queries to ensure they are scoped to the user‚Äôs&nbsp;tenant.</p><p>Always include a tenant_id check in database&nbsp;queries.</p><p>Use database views or abstractions to simplify tenant-specific filtering.</p><p><strong>Application-Layer Validation</strong>:</p><p>Add additional validation at the application level as a guardrail.</p><p>Ensure that APIs restrict data access to the authenticated tenant&nbsp;context.</p><pre>def get_user_tasks(user):    if user.tenant_id != request.tenant_id:<p>        raise PermissionDenied(\"Cross-tenant access is not allowed.\")</p>    return db.query(Tasks).filter(Tasks.tenant_id == user.tenant_id).all()</pre><p>Audit logs are essential for monitoring, compliance, and debugging. They provide visibility into access patterns and help detect unauthorized access attempts.</p><p>User ID and tenant ID for all&nbsp;queries.</p><p>Access attempts (successful and&nbsp;failed).</p><p>Data modification operations (insert, update,&nbsp;delete).</p><p>Timestamps and IP addresses for requests.</p><p><strong>SQL Example For Logging&nbsp;Queries</strong>:</p><pre>CREATE TABLE audit_logs (    id SERIAL PRIMARY KEY,    tenant_id BIGINT,    table_name VARCHAR(255),    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP<p>INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)</p>VALUES (123, 456, 'SELECT', 'tasks', 'SELECT * FROM tasks WHERE tenant_id = 456');</pre><p><strong>Integrating Logging&nbsp;Tools</strong>:</p><p>Use database triggers to log operations automatically.</p><p>Combine with external tools like  (Elasticsearch, Logstash, Kibana) or  for advanced monitoring.</p><pre>CREATE OR REPLACE FUNCTION log_task_changes()RETURNS TRIGGER AS $$    INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)<p>    VALUES (current_user_id(), NEW.tenant_id, TG_OP, TG_TABLE_NAME, current_query());</p>    RETURN NEW;$$ LANGUAGE plpgsql;<p>CREATE TRIGGER audit_task_changes</p>AFTER INSERT OR UPDATE OR DELETE ON tasks<p>FOR EACH ROW EXECUTE FUNCTION log_task_changes();</p></pre><p>Combine RBAC, RLS, and application-level validation for comprehensive protection.</p><p>Use multi-layered access checks and robust query scoping to ensure tenant isolation.</p><p>Maintain detailed audit logs to track access and modifications for accountability and compliance.</p><p>These practices create a secure foundation for multi-tenant authorization systems, ensuring that each tenant‚Äôs data is isolated, protected, and auditable.</p><p>Designing a robust multi-tenant authorization system involves navigating a set of challenges and trade-offs. Each approach has its own set of complexities that must be carefully managed to ensure scalability, performance, and maintainability.</p><ul><li><strong><em>1. Balancing Performance &amp; Flexibility</em></strong></li></ul><p>Choosing between speed and schema cleanliness can significantly impact your database design and performance.</p><p><strong>Prioritizing Performance (Flat&nbsp;Model)</strong>:</p><p>Direct lookups using a tenant_id column ensure fast query execution.</p><p>Reduced join complexity leads to quicker response&nbsp;times.</p><p>: May result in data redundancy (e.g., repeating tenant IDs across multiple&nbsp;tables).</p><p><strong>SQL Example For Optimized Query</strong>:</p><pre>SELECT * FROM tasks <p>WHERE tenant_id = 123 AND status = 'pending';</p></pre><p><strong>Prioritizing Schema Cleanliness (Hierarchical Model)</strong>:</p><p>Using a normalized schema ensures a clean and consistent database structure.</p><p>: Requires more complex joins and increased query times, especially for deeply nested relationships.</p><p><strong>Hierarchical Query&nbsp;Example</strong>:</p><pre>SELECT tasks.*FROM tasks<p>JOIN projects ON tasks.project_id = projects.id</p>JOIN clients ON projects.client_id = clients.id<p>WHERE clients.tenant_id = 123;</p></pre><p>Managing tenant-specific setups adds complexity, particularly as the number of tenants&nbsp;grows.</p><p><strong>Tenant-Specific Databases</strong>:</p><p>Each tenant has its own database, simplifying compliance and data isolation.</p><p>: Maintaining consistency across databases for schema&nbsp;changes.</p><p>: Use automation tools like  or  to manage schema migrations across&nbsp;tenants.</p><pre># Liquibase command to apply migrations to multiple tenant databasesliquibase --url=\"jdbc:mysql://db_host/tenant1\" update<p>liquibase --url=\"jdbc:mysql://db_host/tenant2\" update</p></pre><p><strong>Single Multi-Tenant Database</strong>:</p><p>Shared schema reduces maintenance but requires more sophisticated query scoping and indexing.</p><p>: Tracking and isolating tenant data effectively without introducing query overhead.</p><ul><li><strong><em>3. Handling Schema Updates In Multi-Tenant Databases</em></strong></li></ul><p>Ensuring all tenants have consistent schemas while minimizing downtime is one of the most significant challenges in multi-tenant systems.</p><p>Use versioned migrations to apply incremental updates across all&nbsp;tenants.</p><p>Maintain backward compatibility to prevent disruptions during&nbsp;updates.</p><pre>CREATE TABLE schema_versions (    tenant_id BIGINT,    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><p>Apply updates to a subset of tenants, validate, and then roll out to the&nbsp;rest.</p><p>Use feature flags to selectively enable new schema features.</p><p><strong>Code Example For Rolling&nbsp;Updates</strong>:</p><pre>tenants = get_tenant_list()for tenant in tenants:<p>    apply_schema_update(tenant_id=tenant.id)</p></pre><p><strong>Testing &amp; CI/CD For Multi-Tenant Systems</strong>:</p><p>Test migrations on a staging environment with realistic tenant data before deploying.</p><p>Use CI/CD tools like  to automate and track updates across tenant databases.</p><p><strong>Performance Vs. Flexibility</strong>:</p><p>Use a flat model for high-speed queries or hierarchical models for cleaner schemas but expect performance trade-offs.</p><p>Tenant-specific databases simplify compliance but require robust automation for schema management.</p><p>Implement version control and rolling updates to ensure seamless schema changes across all&nbsp;tenants.</p><p>Addressing these challenges with well-defined strategies ensures a scalable and maintainable multi-tenant authorization system, capable of adapting to evolving application needs.</p><p>Real-world applications of multi-tenant database authorization vary depending on the complexity of the resource structure, performance requirements, and compliance needs. Below are three illustrative scenarios demonstrating how different authorization models can be applied effectively.</p><ul><li><strong><em>Scenario 1‚Ää‚Äî‚ÄäFlat Model For A SaaS CRM&nbsp;App</em></strong></li></ul><p>A SaaS customer relationship management (CRM) application needs to store and manage customer interactions for multiple companies, ensuring users can only access data associated with their organization.</p><p>: Each company has its own sales team, and users need quick access to customer records and sales&nbsp;data.</p><p>: Use a flat model by adding tenant_id to every table, such as customers, leads, and&nbsp;sales.</p><pre>CREATE TABLE customers (    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    name VARCHAR(255),    phone VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    customer_id BIGINT,    status VARCHAR(50),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><pre>SELECT * FROM customers </pre><p>Simple queries without joins for tenant-specific data.</p><p>High performance due to direct&nbsp;lookups.</p><p>Wider tables due to the inclusion of tenant_id.</p><p>Potential redundancy if relationships between entities are not properly normalized.</p><ul><li><strong><em>Scenario 2‚Ää‚Äî‚ÄäHierarchical Model For A Project Management Tool</em></strong></li></ul><p>A project management tool with a nested structure: Company ‚Üí Client ‚Üí Project ‚Üí Task. Users need to manage projects while maintaining strict access control based on their organization.</p><p>: Each company has multiple clients, each with its own projects and tasks. Users must only access tasks related to their&nbsp;company.</p><p>: Use a hierarchical model to enforce relationships and control access through&nbsp;joins.</p><pre>CREATE TABLE companies (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    company_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    client_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    project_id BIGINT NOT NULL,</p>    description TEXT,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><pre>SELECT tasks.*FROM tasks<p>JOIN projects ON tasks.project_id = projects.id</p>JOIN clients ON projects.client_id = clients.id<p>WHERE clients.company_id = 123;</p></pre><p>Clean, normalized schema without redundant data.</p><p>Access naturally follows the hierarchy.</p><p>Complex queries due to multi-level joins.</p><p>Slower query performance for deep hierarchies.</p><ul><li><strong><em>Scenario 3‚Ää‚Äî‚ÄäSingle-Tenant Databases For An Enterprise App</em></strong></li></ul><p>An enterprise application handles sensitive data, requiring strict data isolation for compliance with GDPR and HIPAA regulations.</p><p>: Each tenant‚Äôs data must be completely isolated to ensure compliance and scalability.</p><p>: Use a single-tenant database model where each company has its own dedicated database.</p><pre>Database Names:  tenant_1_db  tenant_3_db</pre><p>: Use  to manage database updates across multiple&nbsp;tenants.</p><pre>deploy:  steps:<p>    - name: Update Tenant Databases</p>      script: |<p>        for db in $(list_databases); do</p>          apply_migrations $db</pre><p>Complete isolation ensures compliance with regulatory requirements.</p><p>Scalability: Large tenants can have dedicated resources (e.g., separate hardware).</p><p>Higher operational complexity in managing multiple databases.</p><p>Requires robust CI/CD pipelines for schema&nbsp;updates.</p><p>Each use case demonstrates how careful consideration of application requirements, data relationships, and compliance needs can guide the choice of the best authorization model for a multi-tenant database.</p><p>Efficient, scalable, and secure authorization in multi-tenant databases as we‚Äôve found often at <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Best+Practices+For+Database+Authorization+In+Multi-Tenant+Systems\">Xinthe</a>, requires a well-thought-out approach tailored to the application‚Äôs needs.</p><p>Authorization in multi-tenant databases isn‚Äôt a one-size-fits-all challenge. Developers and database architects must carefully evaluate their application‚Äôs structure, expected growth, and regulatory needs to select the most effective approach. Armed with the insights and strategies outlined in this article, you can design multi-tenant database systems that are secure, scalable, and efficient, ensuring both developer productivity and a seamless user experience.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=001a1bcf2568\" width=\"1\" height=\"1\" alt=\"\">","contentLength":28256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker, Kubernetes, and NATS ‚Äî The Backbone of Cloud-Native Apps","url":"https://blog.devops.dev/docker-kubernetes-and-nats-the-backbone-of-cloud-native-apps-af724f41c17d?source=rss----33f8b2d9a328---4","date":1739465289,"author":"Cristhian Ferrufino","guid":446,"unread":true,"content":"<h3><strong>Docker, Kubernetes, and NATS‚Ää‚Äî‚ÄäThe Backbone of Cloud-Native Apps</strong></h3><p>Welcome back to the ! In the <a href=\"https://medium.com/devops-dev/decoding-the-message-broker-kafka-vs-rabbitmq-vs-nats-a-tale-of-three-titans-a4f47127256b\">last article</a>, we explored the world of message brokers and why NATS is a standout choice for modern microservices. Now, it‚Äôs time to dive into the backbone of cloud-native applications:  and . If microservices are the chefs in our restaurant analogy, containers are the kitchen tools that keep everything running smoothly. And Kubernetes? That‚Äôs the head chef, making sure everyone works in&nbsp;harmony.</p><p>In this article, we‚Äôll break down  and , explore how they work together, and even touch on how  fits into the mix. By the end, you‚Äôll have a solid understanding of how to containerize your applications and orchestrate them like a pro. Let‚Äôs get&nbsp;cooking!</p><h3>What Is Containerization, and Why Is It Important?</h3><p>Imagine you‚Äôre shipping a fragile package across the world. You‚Äôd want to pack it in a sturdy container, right? That‚Äôs exactly what containerization does for your applications. It packages your app and all its dependencies (libraries, frameworks, etc.) into a lightweight, portable unit called a . This ensures that your app runs consistently across different environments‚Ää‚Äî‚Ääwhether it‚Äôs your laptop, a testing server, or a production cluster.</p><p><strong>Why developers love containers:</strong>‚úÖ : ‚ÄúWorks on my machine‚Äù becomes ‚ÄúWorks everywhere.‚Äù‚úÖ : No more dependency hell‚Ää‚Äî‚Ääeach app lives in its own bubble.‚úÖ : Deploy to AWS, Azure, or your grandma‚Äôs PC (if she‚Äôs cool with Kubernetes).‚úÖ : 10x lighter than VMs. Think EVs vs. a gas-guzzling truck</p><h3>Introduction to Docker: Building, Running, and Managing Containers</h3><p>Docker is the most popular tool for containerization, and for good reason. It‚Äôs simple, powerful, and widely supported. Let‚Äôs break it&nbsp;down:</p><p>To create a container, you start with a ‚Ää‚Äî‚Ääa text file that defines the steps to build your app‚Äôs environment. Here‚Äôs a simple&nbsp;example:</p><pre># Use a lightweight Python image (because nobody likes bloat)FROM python:3.9-slim<p># Set the stage for your app</p>WORKDIR /app<p># Install dependencies (Pro tip: Skip the cache to shrink your image)</p>COPY requirements.txt .<p>RUN pip install --no-cache-dir -r requirements.txt</p><p># Copy the rest of the code</p>COPY . .<p># Open the app‚Äôs ‚Äúfront door‚Äù</p>EXPOSE 8080CMD [\"python\", \"app.py\"]</pre><p>With this Dockerfile, you can build a container image using the ``&nbsp;command:</p><pre>üöÄ Run this: docker build -t my-python-app .</pre><p>Once you‚Äôve built your image, you can run it as a container:</p><pre>üéØ Pro tip: Map ports like a pirate mapping treasure.  docker run -p 8080:8080 my-python-app</pre><p>This command starts your app and maps port 8080 on your host to port 8080 in the container. Easy,&nbsp;right?</p><p>Docker also provides tools to manage your containers:</p><ul><li>: List running containers.</li><li><em>docker logs &lt;container_id&gt;</em>: View logs for a specific container</li><li><em>docker stop &lt;container_id&gt;</em>: Stop a running container.</li></ul><h3>Docker vs. Podman: A Detailed Comparison</h3><p>While Docker is the most popular containerization tool, it‚Äôs not the only one.  is a rising star in the container world, and it‚Äôs worth understanding how it compares to&nbsp;Docker.</p><pre>+----------------------+-------------------------------------+-----------------------------------+| Feature              | Docker üê≥                           | Podman üì¶                         |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Daemon Requirement   | Requires a daemon (dockerd)         | Daemonless (runs containers       |<p>|                      |                                     | directly)                         |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Root vs. Rootless    | Runs as root by default              | Supports rootless containers out  |</p>|                      |                                     | of the box                        |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Compatibility        | Uses Docker CLI and Dockerfiles     | Fully compatible with Docker CLI  |<p>|                      |                                     | and Dockerfiles                   |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Security             | Requires root privileges, which can | Rootless mode reduces attack      |</p>|                      | be a security risk                  | surface                           |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Orchestration        | Requires Docker Swarm for           | Integrates with Kubernetes        |<p>|                      | orchestration                       | natively                          |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Community Support    | Larger community and ecosystem      | Growing community, backed by Red  |</p>|                      |                                     | Hat                               |<p>+----------------------+-------------------------------------+-----------------------------------+</p></pre><ul><li>You need a mature, widely supported tool with a large ecosystem.</li><li>You‚Äôre already using Docker Swarm for orchestration.</li><li>You‚Äôre okay with running containers as&nbsp;root.</li></ul><ul><li>You want a daemonless, more secure alternative to&nbsp;Docker.</li><li>You‚Äôre working in environments where root privileges are restricted.</li><li>You‚Äôre already using Kubernetes and want tighter integration.</li></ul><p>Both tools are excellent choices, so pick the one that best fits your&nbsp;needs.</p><h3>Kubernetes Overview: Orchestration, Scaling, and Self-Healing</h3><p>While Docker is great for running containers, managing them at scale can get tricky. Enter  (or K8s for short), the de facto standard for container orchestration. Think of Kubernetes as the conductor of an orchestra‚Ää‚Äî‚Ääit ensures all your containers play in&nbsp;harmony.</p><p><strong>Key Features of Kubernetes</strong></p><ul><li>: Automates deployment, scaling, and management of containers.</li><li>: Automatically adjusts the number of running containers based on&nbsp;demand.</li><li>: Restarts failed containers and replaces unhealthy ones.</li><li>: Automatically assigns IP addresses and DNS names to containers.</li></ul><p>Kubernetes organizes containers into , which are the smallest deployable units. A pod can contain one or more containers that share resources like storage and networking. Here‚Äôs a simple Kubernetes deployment file:</p><pre>apiVersion: apps/v1kind: Deployment  name: my-python-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-python-app<p>        image: my-python-app:latest</p>        ports:</pre><p>his file tells Kubernetes to run three replicas of your app and expose it on port 8080. You can apply it&nbsp;using:</p><pre>üî• Run this: kubectl apply -f deployment.yaml</pre><h3>How NATS Shines in a Kubernetes Environment</h3><p>Now, let‚Äôs talk about . As a lightweight, high-performance messaging system, NATS plays well with Kubernetes. Here‚Äôs how it stands&nbsp;out:</p><p><strong>Use Cases for NATS in Kubernetes</strong></p><ol><li>Service-to-Service Communication: NATS excels at enabling fast, reliable communication between microservices. Its lightweight design makes it perfect for Kubernetes‚Äô dynamic environment.</li><li>Event-Driven Architectures: NATS‚Äôs pub/sub, request-reply or streams patterns make it ideal for event-driven systems, where services need to react to events in real&nbsp;time.</li><li>Scalability: NATS can handle millions of messages per second, making it a great fit for high-throughput applications running on Kubernetes.</li><li>Resilience: NATS‚Äôs built-in fault tolerance ensures that your messaging system remains reliable, even in the face of node failures.</li></ol><p><strong>Deploying NATS on Kubernetes</strong></p><p>Deploying NATS on Kubernetes is straightforward. Here‚Äôs a basic NATS deployment file:</p><pre>apiVersion: apps/v1kind: Deployment  name: nats  replicas: 1    matchLabels:  template:      labels:    spec:      - name: nats        ports:<p>        - containerPort: 4222 # The messaging highway üõ£Ô∏è</p></pre><p>Once deployed, NATS can be used by your microservices for seamless communication.</p><h3>Best Practices for Containerizing Microservices</h3><p>To wrap things up, here are some best practices for containerizing your microservices:</p><ol><li><strong>Keep Containers Lightweight</strong>: Use minimal base images (e.g.,  or  versions) to reduce size and improve performance.</li><li>: Separate the build and runtime environments to keep production images&nbsp;small.</li><li><strong>Leverage Kubernetes Features</strong>: Use ConfigMaps and Secrets to manage configuration and sensitive data.</li><li>: Integrate tools like Prometheus and Fluentd for monitoring and&nbsp;logging.</li><li>: Use CI/CD pipelines to automate building, testing, and deploying containers.</li></ol><p>In the next article, we‚Äôll explore ‚Äú<strong>NATS as a Service Mesh‚Ää‚Äî‚ÄäThe Lightweight Superhero Your Microservices Deserve</strong>‚Äù and how it simplifies communication between microservices. Spoiler alert: it‚Äôs like giving your microservices a supercharged walkie-talkie. Stay&nbsp;tuned!</p><p>Until then, feel free to drop a comment or share your thoughts. What‚Äôs your experience with Docker and Kubernetes? Any tips or tricks you‚Äôd like to share? Let‚Äôs keep the conversation going.</p><p> üí¨ <em>What‚Äôs your #1 Kubernetes struggle? Scaling? Debugging? Share&nbsp;below!</em></p><p> ‚ù§Ô∏è </p><p>Happy containerizing, and stay tuned for the next chapter in the !</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=af724f41c17d\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Coding Assistants are Not the Solution You Think","url":"https://devops.com/ai-coding-assistants-are-not-the-solution-you-think/","date":1739449895,"author":"Anish Dhar","guid":272,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes History Inspector, with Kakeru Ishii","url":"http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii","date":1739445780,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":366,"unread":true,"content":"<p dir=\"ltr\">Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week ","contentLength":341,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Sandbox environments: Creating efficient and isolated testing realms","url":"https://www.youtube.com/watch?v=fh7-lQVmX-o","date":1739426433,"author":"CNCF [Cloud Native Computing Foundation]","guid":316,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fh7-lQVmX-o?version=3","enclosureMime":"","commentsUrl":null},{"title":"KitOps: AI Model Packaging Standards","url":"https://www.youtube.com/watch?v=1TD-e_wVe4Q","date":1739426400,"author":"CNCF [Cloud Native Computing Foundation]","guid":315,"unread":true,"content":"<article>Chat with us on Discord:  https://discord.gg/Tapeh8agYy\n\nCheck out our repos:\nKitOps      https://github.com/jozu-ai/kitops\nPyKitOps Python Library  https://github.com/jozu-ai/pykitops\nKitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin</article>","contentLength":253,"flags":null,"enclosureUrl":"https://www.youtube.com/v/1TD-e_wVe4Q?version=3","enclosureMime":"","commentsUrl":null},{"title":"Training IT Teams for Multi-Cloud DevOps Environments","url":"https://devops.com/training-it-teams-for-multi-cloud-devops-environments/","date":1739362266,"author":"Anne Fernandez","guid":271,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StackGen‚Äôs New Migration Engine: A DevOps Game-Changer for Multi-Cloud Transitions","url":"https://devops.com/stackgens-new-migration-engine-a-devops-game-changer-for-multi-cloud-transitions/","date":1739269023,"author":"Tom Smith","guid":270,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gcore Radar report reveals 56% year-on-year increase in DDoS attacks","url":"https://devops.com/gcore-radar-report-reveals-56-year-on-year-increase-in-ddos-attacks/","date":1739257299,"author":"cybernewswire","guid":269,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harness Merges with Traceable to Provide Integrated DevSecOps Platform","url":"https://devops.com/harness-merges-with-traceable-to-provide-integrated-devsecops-platform/","date":1739216172,"author":"Mike Vizard","guid":268,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing Kyverno policy enforcement performance for large clusters","url":"https://www.youtube.com/watch?v=DWmCAUCs3bc","date":1739211188,"author":"CNCF [Cloud Native Computing Foundation]","guid":314,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DWmCAUCs3bc?version=3","enclosureMime":"","commentsUrl":null},{"title":"AWS Extends AI Agent Reach into the Realm of Testing Code","url":"https://devops.com/aws-extends-ai-agent-reach-into-the-realm-of-testing-code/","date":1739195146,"author":"Mike Vizard","guid":267,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-125/","date":1739179101,"author":"Mike Vizard","guid":266,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}