{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":51,"items":[{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-126/","date":1739782821,"author":"Mike Vizard","guid":1674,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Fast Are Kubernetes Clusters Attacked? Security Report Reveals Key Trends and Defenses","url":"https://www.wiz.io/blog/kubernetes-report-preview-2025","date":1739781956,"author":"/u/Wownever","guid":1679,"unread":true,"content":"<p>In the ever-evolving world of cloud-native technologies, Kubernetes continues to reign supreme - and with great power comes great responsibility. Our latest Kubernetes Security Report Refresh is coming soon and will unveil a landscape of both peril and progress.&nbsp;</p><p>As a special sneak preview, let's explore the key findings that are shaping the future of container security.&nbsp;</p><p>AKS clusters face probing attempts a mere 18 minutes after deployment.</p><p>Picture this: Your freshly deployed public <a rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://www.wiz.io/academy/kubernetes-clusters-a-security-review\"></a>, barely out of its digital infancy, already under siege. Our research reveals a startling reality where malicious actors operate at breakneck speeds, probing for weaknesses before the digital ink has even dried on your configuration files. This finding serves as a stark reminder: in the world of Kubernetes, security can never be an afterthought.&nbsp;</p><ul><li><p>As of October 2024, Kubernetes 1.29 now leads the pack, dethroning last year's 1.24&nbsp;</p></li><li><p>End of Support versions down to 46% from 58% last year among the managed clusters&nbsp;</p></li></ul><p>Here's a reason to celebrate: Kubernetes operators are leveling up their game. We're witnessing a sea change in version management practices, with teams swiftly adopting the latest releases and bidding farewell to outdated versions. This proactive stance isn't just about chasing the newest features - it's a robust defense against lurking vulnerabilities.&nbsp;</p><ul><li><p>Severe vulnerabilities in exposed pods slashed by 50%&nbsp;</p></li><li><p>Significant drop in high-privilege pod counts&nbsp;</p></li></ul><p>The data paints a picture of leaner, meaner workloads. Security teams are tightening the screws on <a rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://www.wiz.io/academy/what-is-vulnerability-management\"></a>, resulting in a dramatic reduction of critical flaws in exposed containers. Moreover, the principle of least privilege is gaining traction, with fewer pods wielding unnecessary powers. It's a testament to the growing sophistication of Kubernetes security practices.&nbsp;</p><p>While these highlights offer a glimpse into the state of Kubernetes security, they're just the tip of the iceberg. To truly navigate the complexities of <a rel=\"noreferrer noopener\" target=\"_blank\" href=\"https://www.wiz.io/academy/cloud-native-security\"></a> in 2025, you need the full picture.&nbsp;</p><p>While you await the full report (coming soon), check out some of our other Kubernetes content, including:&nbsp;</p>","contentLength":2131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1irfhce/how_fast_are_kubernetes_clusters_attacked/"},{"title":"How do you scale to zero and from zero?","url":"https://www.reddit.com/r/kubernetes/comments/1irexg4/how_do_you_scale_to_zero_and_from_zero/","date":1739779527,"author":"/u/Electronic_Role_5981","guid":1680,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/Electronic_Role_5981\"> /u/Electronic_Role_5981 </a>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Set Up a Persistent Volume for MinIO on GKE Free Tier? Do I Get Any Free Storage?","url":"https://www.reddit.com/r/kubernetes/comments/1ire2it/how_to_set_up_a_persistent_volume_for_minio_on/","date":1739775790,"author":"/u/blvck_viking","guid":1633,"unread":true,"content":"<div><p>I'm setting up a self-hosted MinIO instance on Google Kubernetes Engine (GKE) and need to configure a persistent volume for storage. I'm currently using the GKE free tier and was wondering:</p><ol><li>Does GKE free tier include any free persistent storage, or will I need to pay for it?</li><li>What's the best way to set up a Persistent Volume (PV) and Persistent Volume Claim (PVC) for MinIO in a GKE cluster?</li><li>Any recommendations on storage classes and best practices?</li></ol></div>   submitted by   <a href=\"https://www.reddit.com/user/blvck_viking\"> /u/blvck_viking </a>","contentLength":483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingress Help","url":"https://www.reddit.com/r/kubernetes/comments/1ir7s2b/ingress_help/","date":1739754260,"author":"/u/MeerkatMoe","guid":860,"unread":true,"content":"<p>I'm trying to setup ingress using ingress nginx, but I can't figure out how to get routing to work...either my frontend breaks or my api is unreachable.</p><p>I have an nginx service (not ingress nginx) that serves a frontend on port 80 and an express service that serves a backend API on port 5000.</p><p>My first attempt was two separate ingresses (not sure about terminology):</p><pre><code>--- metadata: name: api-ingress annotations: kubernetes.io/ingress.class: \"nginx\" spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} --- metadata: name: frontend-ingress namespace: {{ k3s_namespace }} annotations: kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/rewrite-target: / spec: ingressClassName: nginx rules: - host: {{ domain_name }} http: paths: - path: / pathType: Prefix backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre><p>but that didn't work, and sometimes my API won't get routed correctly. I think it's because they get combined and I can't guarantee the order.</p><p>My next try was to combine them:</p><pre><code>kubernetes.io/ingress.class: \"nginx\" nginx.ingress.kubernetes.io/use-regex: \"true\" nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: - host: {{ domain_name }} http: paths: - path: /api pathType: Prefix backend: service: name: {{ api_service_name }} port: number: {{ api_port }} - path: \"/(?!api).*\" pathType: ImplementationSpecific backend: service: name: {{ nginx_service_name }} port: number: {{ http_application_entry_port }} </code></pre><p>(left some stuff out to save space)</p><p>but that also didn't work.</p><p>What is the best way to get this working? To summarize, I just need</p><p>\"/api/*\" -&gt; api service port 5000 (it can route as /api/&lt;whatever&gt; or just &lt;whatever&gt;)</p>","contentLength":1819,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event driven workloads on K8s - how do you handle them?","url":"https://www.reddit.com/r/kubernetes/comments/1ir74bk/event_driven_workloads_on_k8s_how_do_you_handle/","date":1739752293,"author":"/u/sniktasy","guid":846,"unread":true,"content":"<p>I have been working with <a href=\"https://github.com/numaproj/numaflow\">Numaflow</a>, an open source project that helps build event driven applications on K8s. It basically makes it easier to process streaming data (think events on kafka, pulsar, sqs etc). </p><p>Some cool stuff - autoscaling based on pending events/ back pressure handling (scale to 0 if need be), source and sink connectors, multi-language support, can support real time data processing use cases with the pipeline semantics etc</p><p>Curious, how are you handling event-driven workloads today? Would love to hear what's working for others?</p>","contentLength":545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Issues with logrotate when logrotate failed to rotate the logs for container","url":"https://www.reddit.com/r/kubernetes/comments/1ir2lhs/issues_with_logrotate_when_logrotate_failed_to/","date":1739739973,"author":"/u/barely_malted","guid":796,"unread":true,"content":"<p>I am using AWS EKS and using default kubelet logrotate parameters (maxsize = 10 Mi and maxfiles = 5) I am facing an issue where I believe these default values are not respected. The kubelet is failing with 'Failed to rotate log for container' 'err=failed to compress log (container/pod log paths) nospace left on device'<p> At the same time one of my pods generated 200 GB logs in one single file. How is this possible ?</p> I was not able to find out any documentation regarding this behaviour.<p> Does this mean that since the kubelet was not able to rotate logs, it just kept on writing them to this one log file till it reached the diskspace limits of my worker nodes ?</p> K8s/EKS version 1.27</p>","contentLength":684,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do devs use kubernetes services locally via ingress on the likes of docker desktop","url":"https://www.reddit.com/r/kubernetes/comments/1ir0af3/how_do_devs_use_kubernetes_services_locally_via/","date":1739734137,"author":"/u/TheRandyOne","guid":746,"unread":true,"content":"<p>I have recently started getting some toolkits running for my devs. I need to get them started on k8s as I am moving services over to k8s.</p><p>I was explaining how this works to a friend and it dawned on me that to use a resource inside the cluster you need to enter via an ingress. The ingress is easy enough since we have the nginx ingress. </p><p>The problem comes in with the dns records required to point to the defined resource to 127.0.0.1 in the /etc/hosts file. Since we have quite few services that need to hosted in k8s, it'll really suck to have the devs to add a bunch of records to the hosts file</p><p>Basically I want something like a wild card record that always returns 127.0.0.1 outside the cluster. So they can pick whatever name they want and always have that delivered to the ingress.</p><p>Am I doing this wrong? Is there some other way that I should be approaching this problem? Or can someone explain how they deal with this other than just editing hosts files.</p>","contentLength":959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pull Request testing on Kubernetes: working with GitHub Actions and GKE","url":"https://www.reddit.com/r/kubernetes/comments/1iqyo15/pull_request_testing_on_kubernetes_working_with/","date":1739730123,"author":"/u/nfrankel","guid":722,"unread":true,"content":"<p>I’m continuing my series on running the test suite for each Pull Request on Kubernetes. In the <a href=\"https://blog.frankel.ch/integration-test-kubernetes/1/\">previous post</a>, I laid the groundwork for our learning journey: I developed a basic JVM-based CRUD app, tested it locally using Testcontainers, and tested it in a GitHub workflow with a GitHub service container.</p><p>This week, I will raise the ante to run the end-to-end test in the target Kubernetes environment. For this, I’ve identified gaps that I’ll implement in this blog post:</p><ul><li>Create and configure a Google Kubernetes Engine instance</li><li>Create a Kubernetes manifest for the app, with Kustomize for customization</li><li>Allow the GitHub workflow to use the GKE instance</li><li>Build the Docker image and store it in the GitHub Docker repo</li><li>Install the PostgreSQL Helm chart</li><li>Finally, run the end-to-end test</li></ul><p>Stages 1, 2, and 3 are upstream, while the workflow executes the latter steps for each PR.</p><p>As I had to choose a tech stack for the app, I had to select a Cloud provider for my infrastructure. I choose GKE because I’m more familiar with Google Cloud, but you can apply the same approach to any other provider. The concept will be the same, only the implementation will differ slightly.</p>","contentLength":1168,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Managing a Talos cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1iqxkjl/managing_a_talos_cluster/","date":1739727418,"author":"/u/simen64","guid":698,"unread":true,"content":"<p>I have been looking into moving my homelab to Kubernetes and Talos seems great for the job. I use OpenTofu for deploying infra in my homelab like VM's in proxmox, but how do people integrate Talos into OpenTofu / Terraform? I have not gotten the talos terraform provider to work and it lacks basic functionality for stuff like updating. So how do people manage their talos clusters?</p>","contentLength":382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measure cpu utilization per deployment?","url":"https://www.reddit.com/r/kubernetes/comments/1iqvp2x/measure_cpu_utilization_per_deployment/","date":1739722689,"author":"/u/netcat23","guid":745,"unread":true,"content":"<p>Hi guys, does measuring cpu utilization of a deployment brings any value?</p><p>What is you opinion about it?</p>","contentLength":102,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Starting a Weekly Rancher Series – From Zero to Hero!","url":"https://www.reddit.com/r/kubernetes/comments/1iqtpjc/starting_a_weekly_rancher_series_from_zero_to_hero/","date":1739717223,"author":"/u/abhimanyu_saharan","guid":650,"unread":true,"content":"<p>I'm kicking off a weekly YouTube series on Rancher, covering everything from getting started to advanced use cases. Whether you're new to Rancher or looking to level up your Kubernetes management skills, this series will walk you through step-by-step tutorials, hands-on demos, and real-world troubleshooting.</p><p>I'll be posting new videos every week, so if you're interested in mastering Rancher, make sure to follow along. Would love to hear your feedback and any specific topics you'd like to see covered!</p><p>Let’s build and learn together! 🚀</p>","contentLength":542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digest #160: Heroku to AWS, GPU Twists, AMI Attacks, FinOps Tools and Solving Crimes with SQL","url":"https://www.devopsbulletin.com/p/digest-160-heroku-to-aws-gpu-twists","date":1739716709,"author":"Mohamed Labouardy","guid":628,"unread":true,"content":"<p><strong>Welcome to this week’s edition of the DevOps Bulletin!</strong></p><p>We start with a big move—from Heroku to AWS—and even a surprising twist about GPUs. There’s also news on a new attack that uses AWS AMI names, ideas on keeping AI safe, and a fun debate about trailing commas in SQL.</p><p>Next, our featured podcast shows how finance and engineering work hand in hand to manage cloud costs, using tools like AWS CUR and CloudWatch. </p><p>If you’re looking for hands-on advice, our tutorials are packed with clear guides—from Terraform production tips and using BigQuery to easy guides for Azure Kubernetes, understanding unit economics, and more.</p><p>We also highlight open-source devtools like a tool to spot Nginx issues, a fun game that uses SQL to solve mysteries, a smart tool for managing config files, and even a Rust-based ping tool that tracks network speed.</p><p>All this and more in this week’s DevOps Bulletin—don’t miss out!</p><p>This podcast explores how finance and engineering teams collaborate in FinOps to manage cloud costs. Success comes from aligning goals, sharing insights, and using AWS CUR and CloudWatch.</p><ul><li><p> is a tool that analyzes Nginx configuration to prevent security misconfiguration and automate flaw detection.</p></li><li><p> is a game where you solve crimes with SQL queries and uncover evidence through data.</p></li><li><p> is a lightweight configuration management tool that updates local config files using key/value stores like etcd or Consul.</p></li><li><p> provides automated cost optimization for AWS and GCP. It includes a GCP Organization Recommender for cost-saving insights and an AWS Resource Cleanup tool to remove unused resources.</p></li></ul><ul><li><p> is a Rust-based Ping tool using the ICMP protocol, offering real-time latency tracking, visual charts, and concurrent pinging of multiple addresses.</p></li></ul><div><p>If you have feedback to share or are interested in <a href=\"https://www.passionfroot.me/devopsbulletin\">sponsoring</a> this newsletter, feel free to reach out via , or simply reply to this email.</p></div>","contentLength":1893,"flags":null,"enclosureUrl":"https://substackcdn.com/image/youtube/w_728,c_limit/_cjuQlc62uc","enclosureMime":"","commentsUrl":null},{"title":"Help with k3s setup on wsl","url":"https://www.reddit.com/r/kubernetes/comments/1iqqly6/help_with_k3s_setup_on_wsl/","date":1739706589,"author":"/u/watterbottle800","guid":526,"unread":true,"content":"<p>I'm trying to install a mern stack application consisting of 11 microservices some which have init containers that depend response from some of the other containers, I have a k3s cluster installed on wsl2, with single node and the external IP of the node is the eth0 ip of the wsl which is in 192.168 range. My pods are in 10.42.0.0/24 and svc in 10.43.0.0/24. All the pods are in default subnet, one of the pods is exposed on port 15672, behind a nodeport svc (say my-svc) with nodeport 30760. One of the init container completed only after a 200 response to curl http:my-svc:15762, but the connectivity is failing with \"failed to connect to &lt;svc cluster ip&gt; port 15672 : couldn't connect to server\" after sometime. </p><p>This specific initcontainer doesn't have nslookup utility doesn't have nslookup or curl utility hence I tried both curl and nslookup from a test pod in the same namespace. Curl failed while nslookup resolved to correct service name and ip), I'm assuming the traffic is going till the svc but not beyond that. I tried with other pods for example call nginx test pod at port 80 from another test pod it failed as well. </p><p>The same setup works fine in k3s cluster in my ec2 and my personal pc, this is my work pc. It would be really helpful if someone could advice on how to troubleshoot this. Thanks</p>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes In-Place Pod Vertical Scaling","url":"https://scaleops.com/blog/kubernetes-in-place-pod-vertical-scaling/","date":1739703019,"author":"/u/Wownever","guid":527,"unread":true,"content":"<p>Kubernetes continues to evolve, offering features that enhance efficiency and adaptability for developers and operators. Among these are <a href=\"https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/\" target=\"_blank\" rel=\"noreferrer noopener\">Resize CPU and Memory Resources assigned to Containers</a>, introduced in Kubernetes version 1.27. This feature allows for adjusting the CPU and memory resources of running pods without  them, helping to minimize downtime and optimize resource usage. This blog post explores how this feature works, its practical applications, limitations, and cloud provider support. Understanding this functionality is vital for effectively managing containerized workloads and maintaining system reliability.</p><h2>What Is In-Place Pod Vertical Scaling?</h2><p>Traditionally, modifying the resource allocation for a Kubernetes pod required a restart, potentially disrupting applications and causing downtime. In-place scaling changes this by enabling real-time CPU and memory adjustments while the pod continues running. This is particularly useful for workloads with a very low tolerance for pod evictions.</p><p>What’s behind the feature gate?</p><p>The new  spec element allows you to specify how a pod reacts to a patch command that changes its resource requests, enabling changing resource requests without rescheduling the pod.</p><p>The result of the change attempt is communicated as part of the pods’ status in a field called&nbsp;  (for more information on the new fields, check out the Kubernetes API <a href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#container-v1-core\" target=\"_blank\" rel=\"noreferrer noopener\">documentation</a>.)<p>Additionally, this feature introduces the </p> in the spec element for containers, allowing fine-grained control over resizing behavior and allowing the developer to choose if CPU change or Memory change should lead to rescheduling the pod.</p><div><ul><li>Dynamic Scaling: Modify CPU and memory allocations while pods run.</li><li>No Restarts: Avoid downtime caused by pod restarts.</li><li>Granular Control: Enable precise resource tuning for better efficiency.</li></ul></div><p>The <code>InPlacePodVerticalScaling</code> feature integrates seamlessly into Kubernetes to provide a more dynamic approach to resource allocation. Here’s a detailed breakdown of how it operates:</p><div><ol><li> Activating the <code>InPlacePodVerticalScaling</code> feature gate in your cluster configuration is required to enable this functionality. This allows the kubelet on each node to detect and process resource updates dynamically.</li><li><strong>Dynamic Resource Updates via Kube API:</strong> With the feature enabled, the kubelet directly applies resource changes to running pods without requiring restarts. Supported container runtimes (e.g., <a href=\"https://github.com/containerd/containerd/releases/tag/v1.6.9\" target=\"_blank\" rel=\"noreferrer noopener\">containerd v1.6.9</a> or later) ensure these updates are applied efficiently. If constraints like insufficient free memory or CPU prevent the changes, the pod follows the regular flow: it is recreated and rescheduled.</li><li> The  field dictates how CPU and memory adjustments are handled. For instance, you can set  for live updates without restarts or  to force a restart when a specific resource is modified.</li></ol></div><h2>Limitations and Considerations</h2><p>While In-Place Pod Vertical Scaling offers significant benefits, it has limitations:</p><p>1. Cloud Provider Support</p><div><ul><li>AWS: Not supported by Amazon Elastic Kubernetes Service (EKS) as there is no way to activate the needed feature gate.</li><li>GCP: Google Kubernetes Engine (GKE) supports this feature as an alpha capability, starting with Kubernetes version 1.27. It must be enabled during cluster creation and requires disabling auto-repair and auto-upgrade. See the <a href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/alpha-clusters\" target=\"_blank\" rel=\"noreferrer noopener\">GKE alpha clusters documentation</a>.</li></ul></div><p>Several Kubernetes policies and mechanisms govern resource scaling. These include:</p><div><ul><li>Resource quotas limit the total CPU and memory usage for a namespace. If an <code>InPlacePodVerticalScaling</code> operation exceeds these limits, the scaling request will fail. For example:</li></ul></div><pre><code>apiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: compute-quota\n  namespace: example-namespace\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"32Gi\"\n</code></pre><div><ul><li>Limit ranges enforce minimum and maximum resource constraints for individual pods or containers within a namespace. The pod will be denied the resource adjustment if a scaling operation exceeds these bounds. Example configuration:</li></ul></div><pre><code>apiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limits\n  namespace: example-namespace\nspec:\n  limits:\n  - type: Container\n    max:\n      cpu: \"2\"\n      memory: \"4Gi\"\n    min:\n      cpu: \"100m\"\n      memory: \"128Mi\"\n</code></pre><div><ul><li>Admission controllers, such as <a href=\"https://kubernetes.io/docs/concepts/security/pod-security-admission/\" target=\"_blank\" rel=\"noreferrer noopener\">Pod Security Admission</a> or custom webhook controllers, can deny scaling operations if they conflict with security or operational policies. For example, a controller may restrict pods from exceeding certain CPU limits.</li></ul></div><div><ul><li>Not all applications can dynamically consume additional resources or adjust to reduced allocations. Examples include:<div><ul><li>Thread Pool Bound applications, like Gunicorn or Unicorn, rely on predefined worker counts.</li><li>Memory-Bound Applications: Applications like Java with fixed Xmx parameters.</li></ul></div></li></ul></div><div><ul><li>In cases where the <a href=\"https://scaleops.com/blog/kubernetes-hpa/\">HPA</a> is based on the resource being patched, this can cause an erratic horizontal scaling behavior. For example:<div><ul><li>HPA scaling behavior is based on CPU average utilization</li></ul></div></li></ul></div><div><ol><li>A pod is changing from 1 core to 2 cores; this can cause a scale-down in pods and affect the bottom-line performance of the application.</li><li>A pod changes from 2 cores to 1; this can cause a scale-up in pods, creating a waste of resources or potential downstream pressure due to the additional and unexpected pods created.</li></ol></div><div><ul><li> Dynamically allocate resources during training and inference phases.</li><li> Combine Horizontal Pod Autoscaler (HPA) with In-Place Pod Vertical Scaling for efficient surge handling.</li><li> Reduce waste by allocating the right amount of resources to each pod in real-time.</li><li> Some applications require significantly higher CPU and memory resources during startup compared to their runtime needs. Google’s example, <a href=\"https://cloud.google.com/blog/products/containers-kubernetes/understanding-kubernetes-dynamic-resource-scaling-and-cpu-boost\" target=\"_blank\" rel=\"noreferrer noopener\">Startup CPU Boost</a>, demonstrates how dynamic resource scaling can address such scenarios effectively.</li></ul></div><h3>1. Enable the Feature Gate</h3><p>Add the following configuration to enable the <code>InPlacePodVerticalScaling</code> feature:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\napiServer:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\ncontrollerManager:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\nscheduler:\n  extraArgs:\n    feature-gates: InPlacePodVerticalScaling=true\n</code></pre><p>For GKE, create a cluster with alpha features enabled:</p><pre><code>gcloud container clusters create poc \\\n    --enable-kubernetes-alpha \\\n    --no-enable-autorepair \\\n    --no-enable-autoupgrade\n</code></pre><p>Define a deployment with initial CPU and memory requests and limits:</p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        resources:\n          limits:\n            memory: \"128Mi\"\n            cpu: \"500m\"\n          requests:\n            memory: \"64Mi\"\n            cpu: \"250m\"\n        resizePolicy:\n        - resourceName: cpu\n          restartPolicy: NotRequired\n        - resourceName: memory\n          restartPolicy: NotRequired\n</code></pre><p>Once deployed, you can check the <code>cpu.weight, cpu.max, memory.max, memory.min</code> from within the container to see the initial values that the container starts with.</p><pre><code>kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max \n</code></pre><p>Adjust resource allocations for a running pod dynamically:</p><pre><code>kubectl patch pod $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -p '{\"spec\":{\"containers\":[{\"name\":\"nginx\",\"resources\":{\"requests\":{\"cpu\":\"750m\"}}}]}}'\n</code></pre><p>Confirm updated resource settings:</p><pre><code>kubectl describe pod -l app=app\n</code></pre><p>Additionally, you can connect to the container and see the change in <code>cpu.weight, cpu.max, memory.max, memory.min</code> from within the container.</p><pre><code>kubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.weight\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/cpu.max\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.min\n\nkubectl exec -it $(kubectl get pods -l app=app -o jsonpath='{.items[*].metadata.name}') -- cat /sys/fs/cgroup/memory.max\n</code></pre><p>In-Place Pod Vertical Scaling is a powerful tool for managing dynamic workloads in Kubernetes, reducing downtime, and <a href=\"https://scaleops.com/blog/optimizing-kubernetes-resources/\">optimizing resource usage</a>. While its adoption depends on cloud provider support and application compatibility, this feature offers significant efficiency and cost-saving benefits. As Kubernetes evolves, such features will become essential for effective container orchestration.</p><p>While Google’s <a href=\"https://github.com/google/kube-startup-cpu-boost\" target=\"_blank\" rel=\"noreferrer noopener\">Kube Startup CPU Boost</a> example is just a specific use case scenario, <a href=\"https://try.scaleops.com\">ScaleOps</a> provides an all in one resource management solution to address all needed scenarios related to Kubernetes resource management.</p>","contentLength":9121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/kubernetes/comments/1iqps4m/kubernetes_inplace_pod_vertical_scaling/"},{"title":"rke2 and DNS","url":"https://www.reddit.com/r/kubernetes/comments/1iqdela/rke2_and_dns/","date":1739658902,"author":"/u/Affectionate_Horse86","guid":515,"unread":true,"content":"<p>I'm going crazy trying to get coredns to talk to my DNS server for names in my domain (I'm using a pihole server that is updated by terraform for VM addresses and by external-dns for k8s services)</p><p>I'm using lablabs ansible role, but a pure rke2 answer is fine, I can figure out the rest. I have</p><pre><code> dest: /var/lib/rancher/rke2/server/manifests/rke2-coredns-config.yaml content: | apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: rke2-coredns namespace: kube-system spec: valuesContent: |- nodelocal: enabled: true ipvs: true zoneFiles: - filename: my-domain.com.conf domain: my-domain.com contents: | my-domain.com:53 { errors cache 30 forward . 10.0.200.1 # my Pihole DNS server } extraConfig: import: parameters: /etc/coredns/my-domain.com.conf when: rke2_type == \"server\" </code></pre><p>and this should have the effect of instructing coredns to use my DNS server for everyting in 'my-domain.com', but although this part lands in the appropriate config map, it doesn't seem to do any good.</p><p>I can replace coredns completely with kubelet flags, but then I lose the resolution of cluster addresses and I don;t get too far in bringing the cluster up.</p>","contentLength":1146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Networking in K8s","url":"https://www.reddit.com/r/kubernetes/comments/1iq9mqp/networking_in_k8s/","date":1739648990,"author":"/u/I-Ad-7","guid":270,"unread":true,"content":"<p>Background: Never used k8s before 4 months ago. I would say I’m pretty good at picking up new stuff and already have lots of knowledge and hands on experience (mostly from doing stuff on my own and reading lots of Oreilly books) for someone like me (age 23). Have a CS background. Doing an internship. </p><p>I was put into a position where I had to use K8s for everyday work and don’t get me wrong I’m ecstatic about being an intern but already having the opportunity to work with deployments etc. </p><p>What I did was read The kubernetes book by Nigel Poulton and got myself 3 cheap PCs and bootstrapped myself a K3s cluster and installed Longorn as the storage and Nginx as the ingress controller.</p><p>Right now I can pretty much do most stuff and have some cool projects running on my cluster.</p><p>I’m also learning new stuff every day. </p><p>But where I find myself lacking is Networking. Not just in Kubernetes but also generally. </p><p>There are two examples of me getting frustrated because of my lacking networking knowledge:</p><ul><li><p>I wanted to let a GitHub actions step access my cluster through the tailscale K8s operator which runs on my cluster but failed</p></li><li><p>Was wondering why I can’t see the real IPs of people that are accessing my api which is on a pod on my cluster and got intimidated by stuff like Layer 2 Networking and why you need a load balancer for that etc.</p></li></ul><p>Do I really have to be as competent as a network engineer to be a good dev ops engineer / data engineer / cloud engineer or anything in ops?</p><p>I don’t mind it but I’m struggling to learn Networking and it’s not that I don’t have the basics but I don’t have the advanced knowledge needed yet, so how do I actually get there?</p>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How are you monitoring your cluster?","url":"https://www.reddit.com/r/kubernetes/comments/1iq94yg/how_are_you_monitoring_your_cluster/","date":1739647696,"author":"/u/psavva","guid":269,"unread":true,"content":"<p>I have a 3 node bare metal cluster and installed Kube Prometheus Stack helm chart.</p><p>I'm having a very hard time getting the service monitors working correctly. I have any 30% of the 150 or so service monitors failing.</p><p>CPU and networking are always displaying 'No Data'</p><p>I fixed the bind addresses for etdc, scheduler, Kube proxy, controller manager from 127.0.0.1 to bind to 0.0.0.0</p><p>That fixes the alerts on a fresh install of the stack. </p><p>1) CPU Metrics 2) Network Metrics 3) Resource Dashboards are all not working properly (Namespace and pods are always empty,) 4) Service Monitors failing.</p><p>I'm using the latest version of the stack on bare metal cluster 1.31, running calico as a CNI.</p><p>Any advice would be appreciated.</p><p>If anyone has a fully working example of the helm chart values that fully work, that would be awesome.</p>","contentLength":813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Questions around LoadBalancer","url":"https://www.reddit.com/r/kubernetes/comments/1iq7y2v/questions_around_loadbalancer/","date":1739644561,"author":"/u/HahaHarmonica","guid":525,"unread":true,"content":"<p>New to k8s. I’ve deployed rke2 and i’ve got several questions. </p><p>Main Question) So i’m trying to install rancher UI on it. When you go to install with helm it asks for a “hostname” and the hostname should be the name of your load balancer…i enabled the load balancer of rke2 but I have no clue how to operate with it…how do I change the configuration to point to rancher? The instructions aren’t very clear on the rke2 site on how to use it other than setting the enable-loadbalancer flag. </p><p>2) During my debugging, i ran the command “kubectl get pods -A -o wide. I have a server node and an agent node. In the column of IP it showed the two IPs of the sever and agent. What was odd was that it showed pods running that were running on the agent node that shouldn’t have been running since I stopped the agent service on the agent node and I ran the kill all script. So how in the world can the containers supposedly running on the agent node…actually be running.</p><p>3) I had some problems with ports not opened initially. Forgot to apply the reload command to make sure the ports were open. I then ran systemctl restart rke2-server on the sever and then systemctl restart rke2-agent on the agent and it was still broken. I finally after 30 min of thinking that wasn’t the problem completely resetting the services by running the killall scripts on both of them before it works…so why in the world won’t k8s actually respect systemctl and restart properly without literally shutting everything down. </p>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Career transition in to Kubernetes","url":"https://www.reddit.com/r/kubernetes/comments/1iq1ka1/career_transition_in_to_kubernetes/","date":1739626865,"author":"/u/Similar-Secretary-86","guid":272,"unread":true,"content":"<p>\"I've spent the last six months working with Docker and Kubernetes to deploy my application on Kubernetes, and I've successfully achieved that. Now, I'm looking to transition into a Devops Gonna purchase kode cloud pro for an year is worth for money ? Start from scratch like linux then docker followed by kubernetes then do some certification Any guidance here would be appreciated </p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My new blog post comparing networking in EKS vs. GKE","url":"https://www.reddit.com/r/kubernetes/comments/1ipz55k/my_new_blog_post_comparing_networking_in_eks_vs/","date":1739617569,"author":"/u/jumiker","guid":271,"unread":true,"content":"   submitted by   <a href=\"https://www.reddit.com/user/jumiker\"> /u/jumiker </a>","contentLength":30,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Dive into VPA Recommender","url":"https://www.reddit.com/r/kubernetes/comments/1ipylpu/deep_dive_into_vpa_recommender/","date":1739615164,"author":"/u/erik_zilinsky","guid":273,"unread":true,"content":"<p>I wanted to understand how the Recommender component of the VPA (Vertical Pod Autoscaler) works - specifically, how it aggregates CPU/Memory samples and calculates recommendations. So, I checked its source code and ran some debugging sessions.</p><p>Based on my findings, I wrote a <a href=\"https://erikzilinsky.com/posts/vpa1.html\">blog post</a> about it, which might be helpful if you're interested in how the Recommender's main loop works under the hood.</p>","contentLength":395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container Networking - Kubernetes with Calico","url":"https://www.reddit.com/r/kubernetes/comments/1ipw9bu/container_networking_kubernetes_with_calico/","date":1739604355,"author":"/u/tkr_2020","guid":268,"unread":true,"content":"<ul><li>: VLAN 10</li><li>: VLAN 20</li></ul><p>When traffic flows from VLAN 10 to VLAN 20, the outer IP header shows:</p><p>The inner IP header reflects:</p><p>The firewall administrator notices that both the source and destination ports appear as , indicating they are set to . This prevents the creation of granular security policies, as all ports must be permitted.</p><p>Could you please advise on how to set specific source and destination ports at the outer IP layer to allow the firewall administrator to apply more granular and secure policies?</p>","contentLength":502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tech Leaders Reveal New Approaches to Corporate Sustainability","url":"https://devops.com/executive-strategies-and-innovations-driving-corporate-sustainability/","date":1739598887,"author":"Bonnie Schneider","guid":242,"unread":true,"content":"<article>Over the past two years, I’ve interviewed more than 100 executives on tech innovation. Key insights emerged. But one stood out: sustainability is no longer a “nice to have.” It’s now a core business strategy. That’s the focus of my inaugural, exclusive report: Decisions That Define: Executive Strategies and Innovations Driving Innovation Corporate Sustainability. Why […]</article>","contentLength":385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DataRobot Acquires Agnostic to Gain Distributed Covalent Platform for AI Apps","url":"https://devops.com/datarobot-acquires-agnostic-to-gain-distributed-covalent-platform-for-ai-apps/","date":1739546554,"author":"Mike Vizard","guid":241,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Black, Indigenous, and People of Color (BIPOC) Initiative Meeting - 2025-02-11","url":"https://www.youtube.com/watch?v=eHa6GhK7L0I","date":1739541570,"author":"CNCF [Cloud Native Computing Foundation]","guid":334,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eHa6GhK7L0I?version=3","enclosureMime":"","commentsUrl":null},{"title":"ChatLoopBackOff Episode 46 (Dragonfly)","url":"https://www.youtube.com/watch?v=gd6HRgr8KcA","date":1739512616,"author":"CNCF [Cloud Native Computing Foundation]","guid":333,"unread":true,"content":"<article>Dragonfly, a CNCF Incubating project, is an open-source, cloud-native image and file distribution system optimized for large-scale data delivery. It is designed to enhance the efficiency, speed, and reliability of distributing container images and other data files across distributed systems. \n\nThis CNCF project is for organizations looking to improve the speed, efficiency, and reliability of artifact distribution in cloud-native environments. Join CNCF Ambassador Nitish Kumar as he explores how it works, Kubernetes integration, as well as its simplified setup and usage.</article>","contentLength":576,"flags":null,"enclosureUrl":"https://www.youtube.com/v/gd6HRgr8KcA?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1739491200,"author":"","guid":404,"unread":true,"content":"<p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><figure><img src=\"https://kubernetes.io/images/docs/components-of-kubernetes.svg\" alt=\"Components of Kubernetes\"></figure><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the Node\nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial Node is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><figure><img src=\"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/ccm-chicken-egg-problem-sequence-diagram.svg\" alt=\"Chicken and egg problem sequence diagram\"><figcaption><p>Chicken and egg problem sequence diagram</p></figcaption></figure><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a Node resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a Node resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one “correct way” to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting “hostNetwork” to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider’s instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure’s node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.\n<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider’s documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nname: cloud-controller-manager\nnamespace: kube-system\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nannotations:\nkubernetes.io/description: Cloud controller manager for my infrastructure\nspec:\ncontainers: # the container details will depend on your specific cloud controller manager\n- name: cloud-controller-manager\ncommand:\n- /bin/my-infrastructure-cloud-controller-manager\n- --leader-elect=true\n- -v=1\nimage: registry/my-infrastructure-cloud-controller-manager@latest\nresources:\nrequests:\ncpu: 200m\nmemory: 50Mi\nhostNetwork: true # these Pods are part of the control plane\nnodeSelector:\nnode-role.kubernetes.io/control-plane: \"\"\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- topologyKey: \"kubernetes.io/hostname\"\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\ntolerations:\n- effect: NoSchedule\nkey: node-role.kubernetes.io/master\noperator: Exists\n- effect: NoExecute\nkey: node.kubernetes.io/unreachable\noperator: Exists\ntolerationSeconds: 120\n- effect: NoExecute\nkey: node.kubernetes.io/not-ready\noperator: Exists\ntolerationSeconds: 120\n- effect: NoSchedule\nkey: node.cloudprovider.kubernetes.io/uninitialized\noperator: Exists\n- effect: NoSchedule\nkey: node.kubernetes.io/not-ready\noperator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":10702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS CloudTrail network activity events for VPC endpoints now generally available","url":"https://aws.amazon.com/blogs/aws/aws-cloudtrail-network-activity-events-for-vpc-endpoints-now-generally-available/","date":1739488674,"author":"Esra Kayabali","guid":538,"unread":true,"content":"<p>Today, I’m happy to announce the general availability of network activity events for <a href=\"https://aws.amazon.com/vpc/\">Amazon Virtual Private Cloud (Amazon VPC)</a> endpoints in <a href=\"https://aws.amazon.com/cloudtrail/\">AWS CloudTrail</a>. This feature helps you to record and monitor AWS API activity traversing your VPC endpoints, helping you strengthen your data perimeter and implement better detective controls.</p><p>Previously, it was hard to detect potential data exfiltration attempts and unauthorized access to the resources within your network through VPC endpoints. While VPC endpoint policies could be configured to prevent access from external accounts, there was no built-in mechanism to log denied actions or detect when external credentials were used at a VPC endpoint. This often required you to build custom solutions to inspect and analyze TLS traffic, which could be operationally costly and negate the benefits of encrypted communications.</p><p>With this new capability, you can now opt in to log all AWS API activity passing through your VPC endpoints. CloudTrail records these events as a new event type called network activity events, which capture both control plane and data plane actions passing through a VPC endpoint.</p><p>Network activity events in CloudTrail provide several key benefits:</p><ul><li> – Log all API activity traversing VPC endpoints, regardless of the AWS account initiating the action.</li><li><strong>External credential detection</strong> – Identify when credentials from outside your organization are accessing your VPC endpoint.</li><li><strong>Data exfiltration prevention</strong> – Detect and investigate potential unauthorized data movement attempts.</li><li><strong>Enhanced security monitoring</strong> – Gain insights into all AWS API activity at your VPC endpoints without the need to decrypt TLS traffic.</li><li><strong>Visibility for regulatory compliance</strong> – Improve your ability to meet regulatory requirements by tracking all API activity passing through.</li></ul><p>To enable network activity events, I go to the <a href=\"https://console.aws.amazon.com/cloudtrailv2\">AWS CloudTrail</a> console and choose  in the navigation pane. I choose  to create a new one. I enter a name in the  field and choose an <a href=\"https://aws.amazon.com/s3/\">Amazon Simple Storage Service (Amazon S3)</a> bucket to store the event logs. When I create a trail in CloudTrail, I can specify an existing Amazon S3 bucket or create a new bucket to store my trail’s event logs.</p><p>If you set <strong>Log file SSE-KMS encryption</strong> to , you have two options: Choose  to create a new <a href=\"https://aws.amazon.com/kms/\">AWS Key Management Service (AWS KMS)</a> key or choose  to choose an existing KMS key. If you chose , you need to type an alias in the  field. CloudTrail encrypts your log files with this KMS key and adds the policy for you. The KMS key and Amazon S3 must be in the same AWS Region. For this example, I use an existing KMS key. I enter the alias in the  field and leave the rest as default for this demo. I choose for the next step.</p><p>In the  step, I choose  under . I choose the event source from the list of AWS services, such as , , , , and <code>secretsmanager.amazonaws.com</code>. I add two network activity event sources for this demo. For the first source, I select  option. For , I can use templates for common use cases or create fine-grained filters for specific scenarios. For example, to log all API activities traversing the VPC endpoint, I can choose the  template. I choose <strong>Log network activity access denied events</strong> template to log only access denied events. Optionally, I can enter a name in the  field to identify the log selector template, such as <strong>Include network activity events for Amazon EC2</strong>.</p><p>As a second example, I choose  to create custom filters on multiple fields, such as  and . I can specify specific VPC endpoint IDs or filter the results to include only the VPC endpoints that match specific criteria. For <strong>Advanced event selectors,</strong> I choose  from the  dropdown, choose  as , and enter the VPC endpoint ID. When I expand the JSON view, I can see my event selectors as a JSON block. I choose  and after reviewing the selections, I choose .</p><p>After it’s configured, CloudTrail will begin logging network activity events for my VPC endpoints, helping me analyze and act on this data. To analyze AWS CloudTrail network activity events, you can use the CloudTrail console, <a href=\"https://aws.amazon.com/cli/\">AWS Command Line Interface (AWS CLI)</a>, and <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDK</a> to retrieve relevant logs. You can also use <a href=\"https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-lake.html\">CloudTrail Lake</a> to capture, store and analyze your network activity events. If you are using Trails, you can use <a href=\"https://aws.amazon.com/athena\">Amazon Athena</a> to query and filter these events based on speciﬁc criteria. Regular analysis of these events can help you maintain security, comply with regulations, and optimize your network infrastructure in AWS.</p><p>CloudTrail network activity events for VPC endpoint logging provide you with a powerful tool to enhance your security posture, detect potential threats, and gain deeper insights into your VPC network traffic. This feature addresses your critical needs for comprehensive visibility and control over your AWS environments.</p><p>Network activity events for VPC endpoints are available in all commercial AWS Regions.</p><a href=\"https://www.linkedin.com/in/esrakayabali/\">— Esra</a>","contentLength":4912,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Terraform Architecture Explained , Terraform Core, State, and Plugins: How Terraform Works Under…","url":"https://blog.devops.dev/terraform-architecture-explained-terraform-core-state-and-plugins-how-terraform-works-under-a19e4d4dbb09?source=rss----33f8b2d9a328---4","date":1739465498,"author":"Kuseh Simon Wewoliamo","guid":430,"unread":true,"content":"<h3>Terraform Architecture Explained&nbsp;, Terraform Core, State, and Plugins: How Terraform Works Under the&nbsp;Hood.</h3><p><em>1. Introduction 2. Terraform Architecture4. Terraform Best Practices6. References</em></p><p>Infrastructure as Code (IaC), is an approach to managing and provisioning infrastructure by writing code instead of the manual processes&nbsp;, “ClickOps”. IaC can be described as a mindset where you treat all aspects of operations (servers, databases, networks) as software. When you define your infrastructure using code&nbsp;, it enables you to automate and use all the best practices of software development. IaC eliminates human errors&nbsp;, speeds up infrastructure deployments and ensures infrastructure is version-controlled, just like software&nbsp;code.</p><p>Terraform is an open-source tool developed by HashiCorp and the most popular and widely used IaC tool used by DevOps, SREs and cloud architects. Terraform is widely used because of it’s declarative syntax, platform agnostic and its simplicity. Understanding how terraform works behind the hood will go along way to help you in write better terraform code.</p><p>In this article, we will explore Terraform architecture, its core components, and how it orchestrates infrastructure provisioning efficiently.</p><h3><em>2. Terraform Architecture</em></h3><p>Terraform follows a standard architecture to fulfill the necessary IaC tasks. Terraform architecture mainly consists of the following components:<em> 1 Terraform core 2 Plugins (Providers and Provisioners) </em></p><p>Terraform core is the engine/brain behind how terraform works. It is responsible for reading configurations files&nbsp;, building the dependency graphs from resources and data sources, managing state and applying changes. Terraform Core does not directly interact with cloud providers but communicates with plugins via remote procedure calls (RPCs) and the plugins in turn communicates with their corresponding platforms via&nbsp;HTTPs.</p><h4>Plugins (Providers and Provisioners)</h4><p>Terraform ability is enhance by plugins, which enable terraform to interact with cloud services and configure resources dynamically. Plugins acts as connectors or the glue between terraform and external APIs such as AWS, Azure, GCP, Kubernetes, Docker etc. Each plugin is written in the Go programming language and implements a specific interface. Terraform core knows how to install and execute plugins. Provisioners in Terraform are used to execute scripts or commands on a resource after it has been created or modified.</p><p>State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources it created. It is a costumed JSON file that terraform uses to map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By default, state is stored in a local file named “terraform.tfstate”. You can read more about terraform state <a href=\"http://State is one of the most important core components of Terraform. Terraform state is a record about all the infrastructure and resources  it created. It is a customed  JSON file that terraform uses to  map real world resources to your configuration, keep track of metadata, and to improve performance for large infrastructures. By  default, state is stored  in a local file named &quot;terraform.tfstate&quot;. You can read more about terraform state [here](https://developer.hashicorp.com/terraform/language/state)    There are two ways to manage state:  - 1. Local State:  Local State refers to the default way by which Terraform stores state files (terraform.tfstate).  It is suitable for small-scale projects or development environments and single person is managing Terraform.       - 2. Remote State: Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. [Remote State](https://developer.hashicorp.com/terraform/language/state/remote)\">here</a><p> There are two ways to manage state:</p> Local State refers to the default way by which Terraform stores state files (terraform.tfstate). It is suitable for small-scale projects or development environments and single person managing Terraform.</p><p>Remote State refers to storing the Terraform state file (terraform.tfstate) in a remote backend rather than locally on your machine. This enables collaboration, prevents state loss, and supports features like state locking and versioning. Some common remote backends include AWS S3,Terraform Cloud, Azure Blob Storage etc. More on <a href=\"https://developer.hashicorp.com/terraform/language/state/remote\">Remote&nbsp;State</a></p><p>Terraform follows a structured execution flow to provision, update, and manage infrastructure. This process ensures that infrastructure is deployed in a controlled and predictable manner. Terraform workflow consist of mainly three&nbsp;steps:</p><p> The first step is to write your terraform configuration just like any other code using any editor of your&nbsp;choice.</p><p> This is the step where you review your configurations. Terraform plan will define the infrastructure to be created, modified, or destroyed depending on the current configuration and infrastructure.</p><p> The final step in the workflow is Apply, where you are ready to provision real infrastructure. Once you approve of the changes&nbsp;,terraform will go ahead perform the desired actions as defined execution.</p><h3>4. Terraform Best Practices</h3><p>1. You should never edit the Terraform state files by hand or write code that reads them directly. If for some reason you need to manipulate the state file which should be a relatively rare occurrence, use the terraform import or terraform state commands.</p><p>2. It’s a good practice to store your work in a version controlled repository even when you’re just operating as an individual.</p><p>3. When working as a team, it’s important to delegate ownership of infrastructure across these teams and empower them to work in parallel without conflicts.</p><p>4. Never Store your state file in a version controlled repository.</p><p>5. Always use state locking on your state files to prevent data loss, conflicts and state file corruption.</p><p>6. Integrate Terraform to your CI/CD pipelines to make your DevOps pipeline efficient.</p><p>Well well, we have come to the end of this deep dive into terraform Architecture. To learn more about Terraform visit the <a href=\"https://developer.hashicorp.com\">official Terraform page</a>. Don’t forget to add your comments&nbsp;, till then keep&nbsp;coding.</p><p>6. Terraform:Up &amp; Running&nbsp;, Third Edition by Yevgeniy&nbsp;Brikman</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a19e4d4dbb09\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Micro Frontend Revolution","url":"https://blog.devops.dev/the-micro-frontend-revolution-29b6eedc8783?source=rss----33f8b2d9a328---4","date":1739465492,"author":"Adem KORKMAZ","guid":429,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Free AI models: Running Local LLMS with Llama 3.3,","url":"https://blog.devops.dev/running-local-llms-with-llama-3-3-deepseek-r1-and-other-large-language-models-using-ollama-5d0dc2d09358?source=rss----33f8b2d9a328---4","date":1739465463,"author":"Joel Wembo","guid":428,"unread":true,"content":"<h4>Part 4 of 10 Part series on DeepSeek&nbsp;MLOps</h4><h3>Free AI models: Running Local LLMS with Llama 3.3, DeepSeek-R1, and other Large Language Models using&nbsp;Ollama</h3><h4>Step-by-Step Guide: Installing a Web UI for Local LLMs on&nbsp;Windows</h4><p>With the rise of powerful open-source large language models (LLMs) like , , Phi-4, and Gemma 2, many users want to run these models locally for privacy, performance, and customization. However, interacting with these models via the command line can be limiting. <strong><em>The solution? A web-based user interface (UI) that allows easy interaction with your local&nbsp;LLMs.</em></strong></p><p>In this article, we will explore the best web UIs for running LLMs locally on Windows and guide you through the installation process.</p><p> is a lightweight, high-performance framework designed for running large language models (LLMs) locally with optimized execution. It works by leveraging <strong>GGUF (GGML Unified Format)</strong>, an efficient model storage format that supports quantization, allowing models to run smoothly even on consumer hardware.</p><h3>Why Use a Web UI for Local&nbsp;LLMs?</h3><p>Using a web UI for local LLMs offers several advantages:</p><ul><li>: No need to work with command-line tools.</li><li>: Manage multiple models in one&nbsp;place.</li><li>: Chat history, prompt engineering, and adjustable settings.</li><li>: Access your models remotely via a web&nbsp;browser</li></ul><h4>Step 1&nbsp;: Download and Install&nbsp;Ollama</h4><p>Download Ollama from <a href=\"https://ollama.com/download/windows\">https://ollama.com/download/windows</a>, then right click on the downloaded OllamaSetup.exe file and run the installer as administrator. Once the installation is complete, Ollama is ready to use on your Windows system. An Ollama icon will be added to the tray area at the bottom of the&nbsp;desktop.</p><p>To run Ollama and start utilizing its AI models, you’ll need to use a terminal on Windows. We’ll skip it here and let’s see how to install WebUI for a better experience.</p><p>Now open the browser and type localhost:11434 to check is Ollama is up and&nbsp;running</p><p>Also, Check in your system&nbsp;Tray</p><p>Next, Open your CMD to pull some free AI&nbsp;models</p><h4>Step 2 — Install Ollama&nbsp;WebUI</h4><p>Run the below docker command to deploy ollama-webui docker container on your local machine. If Ollama is on your computer, use this&nbsp;command:</p><pre>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</pre><p>To connect to Ollama on another server, change the OLLAMA_BASE_URL to the server’s URL. So if Ollama is on a Different Server, use this&nbsp;command:</p><pre>docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main</pre><p>Next, Open your browser and type localhost:3000</p><p>Ollama utilizes <strong>Metal on macOS and CUDA on Windows/Linux</strong> for hardware acceleration, enabling faster inference by directly leveraging GPU tensor operations. It runs a <strong>persistent server in the background</strong>, managing requests via an  that communicates with models using optimized token streaming.</p><p>Internally, it uses <strong>low-level memory-efficient inference kernels</strong>, minimizing VRAM and RAM usage while maintaining performance. It also supports <strong>LoRA (Low-Rank Adaptation) fine-tuning</strong>, allowing users to personalize models on their local machine with minimal compute overhead.</p><p>Run the following command:</p><pre>ollama run deepseek-r1:671b</pre><h3>Choosing the Right Web UI for Your&nbsp;Needs</h3><ul><li>: LM Studio (Simple setup, user-friendly UI)</li><li>: Oobabooga (More features, customization options)</li><li>: Gradio (Custom interface, lightweight solution)</li><li>: Open WebUI (Accessible over the internet)</li></ul><p>Setting up a web UI for local LLMs on Windows significantly enhances your experience, making it easier to interact with AI models without complex command-line operations. Whether you’re a beginner or an advanced user, the right UI can streamline your workflow and unlock new possibilities with local AI&nbsp;models.</p><p>Start today with one of these web UIs and bring AI power to your local machine!&nbsp;🚀</p><p>Thank you for Reading&nbsp;!! 🙌🏻, don’t forget to subscribe and give it a&nbsp;CLAP</p><p><em>, cloud Solutions architect, Back-end developer, and AWS Community Builder, currently working at prodxcloud as a DevOps &amp; Cloud Architect. I bring a powerful combination of expertise in cloud architecture, DevOps practices, and a deep understanding of high availability (HA) principles. For more information about the author ( </em><a href=\"https://joelwembo.com/\"></a><a href=\"https://www.linkedin.com/in/joelotepawembo/\"></a><a href=\"https://github.com/joelwembo\"></a><a href=\"https://twitter.com/joelwembo1\"></a><a href=\"http://joelwembo.github.io/\"></a></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5d0dc2d09358\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Container Orchestration (AWS ECS, AWS EKS & Kubernetes)","url":"https://blog.devops.dev/understanding-container-orchestration-aws-ecs-aws-eks-kubernetes-baee401db009?source=rss----33f8b2d9a328---4","date":1739465455,"author":"Althaf Hussain","guid":427,"unread":true,"content":"<h3>Why Do We Need Container Orchestration?</h3><p>1️⃣ <strong>We use Docker to create and run containers</strong></p><ul><li>Docker  using a Dockerfile with :</li><li>: Packages and compiles the&nbsp;app.</li><li>: Runs the app and exposes&nbsp;ports.</li></ul><pre>docker run -p 80:80 my-app</pre><ul><li>Now the app is running inside a . 🎉</li></ul><p>2️⃣ <strong>But what if the app crashes due to high&nbsp;traffic?</strong></p><ul><li><strong>Docker cannot restart or scale the&nbsp;app</strong>.</li><li>If there’s high traffic (e.g., festive season sales), .</li></ul><p>3️⃣ <strong>Solution? We need a tool to manage containers automatically!</strong></p><ul><li>This is where <strong>Container Orchestration Tools</strong> come&nbsp;in!</li><li>Examples: <strong>Kubernetes, AWS ECS, AWS EKS, Azure AKS, Google GKE, OpenShift</strong>.</li></ul><h3>🚀 Kubernetes — Full Control but Complex&nbsp;Setup</h3><p>✅  When you want  over your cluster.✅ </p><ul><li><strong>Manages multiple containers</strong> (Docker is just for one container).</li><li> (if traffic increases, it adds more containers).</li><li> (if an app crashes, Kubernetes restarts&nbsp;it).</li></ul><h3>What is Kubernetes (Self-Managed)?</h3><p>If you want  over your cluster, you can <strong>set up Kubernetes manually</strong>.</p><h4>How Kubernetes Works (Practical Steps)</h4><p>1️⃣ Create a <strong>VM or server (EC2, Azure VM, GCP VM, on-premise server, etc.).</strong>2️⃣ Install <strong>Kubernetes, kubeadm, kubectl, networking, storage, etc.</strong>3️⃣ Set up a  and .4️⃣ Deploy your app using a .5️⃣ Manage <strong>scaling, auto-healing, networking, etc.</strong> manually.</p><h4>🛠️ Steps to Deploy an App Using Kubernetes:</h4><p>1️⃣ <strong>Set up a server (EC2 instance or&nbsp;VM)</strong></p><pre>sudo apt update &amp;&amp; sudo apt install -y curl apt-transport-httpscurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -<p>echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list</p>sudo apt update<p>sudo apt install -y kubelet kubeadm kubectl</p></pre><p>2️⃣ <strong>Initialize Kubernetes cluster</strong></p><pre>mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<p>sudo chown $(id -u):$(id -g) $HOME/.kube/config</p></pre><pre>apiVersion: apps/v1kind: Deployment  name: my-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-app<p>        image: my-docker-image:latest</p>        ports:</pre><pre>kubectl apply -f deployment.yaml</pre><pre>apiVersion: v1kind: Service  name: my-app-service  type: LoadBalancer    app: my-app    - protocol: TCP      targetPort: 80</pre><pre>kubectl apply -f service.yaml</pre><p>🎯 <strong>Your app is now running inside Kubernetes!</strong> 🚀</p><h4>✅ Advantages of Self-Managed Kubernetes</h4><p>✔  → You can configure every part of the cluster.✔  → On-premise, AWS, Azure, GCP, or hybrid cloud.✔  → You’re not tied to AWS, Azure, or any provider.✔  → Most companies use  for flexibility.</p><h4>❌ Disadvantages of Self-Managed Kubernetes</h4><p>✖  → You need to manually configure <strong>networking, storage, security, etc.</strong>✖  → You have to <strong>patch, upgrade, and secure</strong> the cluster yourself.✖  → Setting up and managing Kubernetes is .</p><h3>🚀 AWS ECS — AWS Manages Everything</h3><p>✅  Running containers <strong>without managing Kubernetes</strong>.✅ </p><ul><li>You <strong>don’t need to set up Kubernetes</strong>.</li><li>Just  what app you want to run, and it does everything.</li><li> over cluster management.</li></ul><h3>🛠️ Steps to Deploy an App in AWS&nbsp;ECS</h3><p>1️⃣  in the AWS Console.2️⃣  (Choose Fargate or EC2).3️⃣ :</p><ul><li>Go to <strong>ECS &gt; Task Definitions &gt; Create new task definition</strong>.</li><li>Choose  (serverless) or  (self-managed).</li><li>Define  (Docker image, ports, CPU, memory).4️⃣ :</li><li>Go to <strong>ECS &gt; Services &gt; Create&nbsp;Service</strong>.</li><li>Choose the <strong>cluster and task definition</strong> you&nbsp;created.</li><li>Define  (number of tasks).5️⃣ &nbsp;🎉</li></ul><p>🎯 <strong>Your app is running inside AWS ECS without managing infrastructure!</strong> 🚀</p><p>✔  → AWS takes care of the infrastructure.✔  → No need to set up Kubernetes manually.✔ <strong>Tightly integrated with AWS</strong> → Works great with AWS services like ALB, IAM, CloudWatch, etc.✔ <strong>Less operational overhead</strong> → No need to worry about maintaining a&nbsp;cluster.</p><h4>❌ Disadvantages of AWS&nbsp;ECS</h4><p>✖  → If you want to move your app from AWS to , or , you have to <strong>set up everything from scratch</strong>.✖  → You don’t have full control over how the cluster is managed.✖  → Most companies prefer  over ECS for multi-cloud strategies.</p><h3>🚀 AWS EKS — AWS Manages Kubernetes for&nbsp;You</h3><p>✅  When you want <strong>Kubernetes but don’t want to install it manually</strong>.✅ </p><ul><li>AWS  (no need to install manually).</li><li>You  to deploy&nbsp;apps.</li><li> than ECS but  than DIY Kubernetes.</li></ul><h3>🛠️ Steps to Deploy an App in AWS&nbsp;EKS</h3><p>1️⃣  in the AWS Console.2️⃣ :</p><ul><li>Set <strong>Cluster name, VPC, IAM&nbsp;role</strong>.</li><li>AWS will create &amp; configure the Kubernetes control plane.3️⃣ </li></ul><pre>aws eks update-kubeconfig --region your-region --name your-cluster-name</pre><p>4️⃣  (same as Kubernetes DIY)</p><pre>kubectl apply -f deployment.yaml</pre><p>5️⃣ <strong>Expose the app using a Kubernetes service</strong> (same as&nbsp;before).</p><p>🎯 <strong>Your app is running in AWS EKS with Kubernetes, but AWS helps with the setup!</strong>&nbsp;🚀</p><p>✔  → Works exactly like Kubernetes, so it’s <strong>easy to move to another cloud</strong> (Azure AKS, Google GKE, etc.).✔ <strong>Fully managed control plane</strong> → AWS handles the  (setting up Kubernetes).✔  than ECS → You can tweak networking, security, and scaling.✔  → You can run Kubernetes anywhere (AWS, Azure, GCP, or on-premise).</p><h4>❌ Disadvantages of AWS&nbsp;EKS</h4><p>✖  → You still need to understand Kubernetes concepts.✖ <strong>More operational overhead</strong> → Though AWS sets up Kubernetes, you still .✖  → You  for the Kubernetes control&nbsp;plane.</p><h3>🎯 Real-World Example of How These Work&nbsp;Together</h3><p>Imagine you’re running an :1️⃣ You  to package your app into a container.2️⃣ You deploy it to <strong>Kubernetes (DIY) if you want full control</strong>.3️⃣ If you <strong>don’t want to manage Kubernetes</strong>, you use  (simplest).4️⃣ If you <strong>need Kubernetes but don’t want manual setup</strong>, you use .</p><p>📌 <strong>Think of Kubernetes as a powerful machine where you control everything.</strong>📌 <strong>Think of AWS ECS as a service where AWS does the heavy lifting for you.</strong>📌 <strong>Think of AWS EKS as Kubernetes, but AWS helps with&nbsp;setup.</strong></p><h3>Conclusion: Which One Should You&nbsp;Use?</h3><p>👉  if you  and don’t care about moving to another cloud.👉  if you  but don’t want to set it up manually.👉 <strong>Use Self-Managed Kubernetes</strong> if you  and <strong>plan to run across multiple clouds (AWS, Azure, GCP, on-premise, etc.).</strong></p><p>💡 If you’re , start with .If you’re building , go for .If you want , use .</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=baee401db009\" width=\"1\" height=\"1\" alt=\"\">","contentLength":6091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding APIs: A Developer’s Guide to Building and Using APIs","url":"https://blog.devops.dev/understanding-apis-a-developers-guide-to-building-and-using-apis-4253418d18ba?source=rss----33f8b2d9a328---4","date":1739465421,"author":"Subbareddysangham","guid":426,"unread":true,"content":"<p>An Application Programming Interface (API) acts as a bridge between different software applications, allowing them to communicate with each other. Think of an API like a waiter in a restaurant — customers (the client application) don’t need to know how the kitchen (the server) prepares their food; they need to know how to place their order through the waiter (the&nbsp;API).</p><p>I designed and developed an e-commerce web application with <strong>HTML, CSS, and JavaScript</strong> for the front end,  for the back end, and  for the database. I will use this as an example to explain the core concepts of&nbsp;APIs.</p><h3>E-commerce Web Application API Flow&nbsp;Chart:</h3><p><strong><em>To check the complete source&nbsp;code:</em></strong></p><h3>APIs in the E-Commerce Application</h3><p><strong>This E-Commerce application</strong> consists of the following API endpoints:</p><h4>1. Authentication API (auth_routes)</h4><ul><li>: /api/auth/login (POST) → Authenticates users and starts a&nbsp;session.</li><li>: /api/auth/logout (POST) → Clears session and logs out&nbsp;users.</li></ul><h4>2. Product API (product_routes)</h4><ul><li>: /api/products (GET) → Returns a list of products.</li><li>: /api/products/&lt;int:product_id&gt; (GET) → Fetches details of a specific&nbsp;product.</li></ul><h4>3. Cart API (cart_routes)</h4><ul><li>: /api/cart (GET) → Returns the current user's&nbsp;cart.</li><li>: /api/cart (POST) → Adds a product to the&nbsp;cart.</li><li>: /api/cart/&lt;int:item_id&gt; (DELETE) → Removes an item from the&nbsp;cart.</li></ul><h4>4. Order API (order_routes)</h4><ul><li>: /api/orders (POST) → Places an order with the items in the&nbsp;cart.</li><li>: /api/orders/&lt;int:order_id&gt; (GET) → Fetches details of a specific&nbsp;order.</li></ul><ul><li>: /api/health (GET) → Provides API uptime, session data, and frontend&nbsp;path.</li></ul><ul><li>: /&lt;path:filename&gt; (GET) → Serves frontend&nbsp;files.</li><li>: / (GET) → Serves index.html or API running&nbsp;message.</li></ul><ul><li>: Handles missing resources.</li><li><strong>500 Internal Server Error</strong>: Handles unexpected issues.</li><li>: Handles invalid requests.</li></ul><p>An API consists of several key components that work together:</p><ol><li>These are the URLs where the  can be accessed. Similar to a , each endpoint serves a specific purpose. For example,📌 <strong>https://api.ecommerce.com/products</strong> → Retrieves a list of available products.📌 <strong>https://api.ecommerce.com/cart</strong> → Fetches the current user's shopping cart details.📌 <strong>https://api.ecommerce.com/orders</strong> → Handles order-related operations.</li></ol><p>These actions can be performed on the allowed endpoints. They’re like verbs telling the API what to do with the&nbsp;data.</p><ul><li> → Read data (<strong>View products, orders, cart&nbsp;items</strong>).</li><li> → Create new data (<strong>Add product, register user, place&nbsp;order</strong>).</li><li> → Update existing data (<strong>Update profile, modify cart quantity</strong>).</li><li> → Remove data (<strong>Delete cart item, cancel&nbsp;order</strong>).</li></ul><p> Additional data is sent to fine-tune the API request, such as specifying which page of results you want to&nbsp;see.</p><ul><li>: Parameters are extra details added to an API request to filter or refine the&nbsp;results.</li><li>Fetch  of products:</li></ul><pre>GET /api/products?category=laptops</pre><p>Security measures ensure that only authorized users can access the&nbsp;API.</p><ul><li>: Ensures that only authorized users can access the&nbsp;API.</li></ul><p>When a user logs in, the API gives a&nbsp;:</p><pre>{  \"message\": \"Login successful\",<p>  \"token\": \"eyJhbGciOiJIUz...\"</p>}</pre><p>To <strong>add a product to the cart</strong>, the request must include this&nbsp;:</p><pre>POST /api/cart/addAuthorization: Bearer eyJhbGciOiJIUz...</pre><p> Prevents unauthorized access and protects user&nbsp;data.</p><p> The structure of the data returned by the API, commonly in formats like JSON or&nbsp;XML.</p><ul><li>: The structure of the data sent back by the&nbsp;API.</li><li> (because it’s easy to read and&nbsp;use).</li></ul><pre>{  \"id\": 1,  \"price\": 799.99,}</pre><p> Frontend uses this data to display products to&nbsp;users.</p><p>APIs are classified according to their usage patterns and architectures.</p><h3>API Types According to Purposes of&nbsp;Use</h3><p>🔹  — Used within a company, hidden from public access. Helps teams share data securely.</p><p>🔹  — Available to everyone, can be free or paid. Example: Google Maps&nbsp;API.</p><p>🔹  — Used between business partners for secure data exchange. Example: E-commerce &amp; shipping company integration.</p><p>🔹  — Combines multiple APIs into one request for efficiency. Example: Fetching account balance + transaction history in one&nbsp;call.</p><h3>API Types According to Architectural Structure:</h3><h3>1. Web APIs (HTTP/HTTPS APIs)</h3><p>These are the most common APIs, operating over the internet using HTTP protocols. They come in several varieties:</p><h3>1.1. REST (Representational State Transfer):</h3><p>The most popular type of web API today. REST APIs follow these principles:</p><ul><li>Stateless: Each request contains all the information needed</li><li>Resource-based: Everything is treated as a resource with a unique&nbsp;URL</li><li>Uses standard HTTP methods (GET, POST, PUT,&nbsp;DELETE)</li><li>Supports multiple data formats (usually&nbsp;JSON)</li></ul><h3><strong><em>Example REST API Request in an E-Commerce Web Application:</em></strong></h3><p>This request <strong>fetches all available products</strong> from the online&nbsp;store.</p><p>✅ <strong>Request (Client →&nbsp;Server)</strong></p><pre>GET /api/products HTTP/1.1Host: api.ecommerce.com<p>Authorization: Bearer &lt;User_Token&gt;</p>Content-Type: application/json</pre><p>✅ <strong>Response (Server →&nbsp;Client)</strong></p><pre>[    {\"id\": 1, \"name\": \"iPhone 15\", \"price\": 999.99, \"stock\": 20},<p>    {\"id\": 2, \"name\": \"Samsung Galaxy S24\", \"price\": 899.99, \"stock\": 15}</p>]</pre><h3><strong><em>catalog.html Fetches and Displays&nbsp;Products</em></strong></h3><h4>1️⃣ User Visits catalog.html</h4><ul><li>The user opens the  page in their browser (http://52.90.222.178:5000/catalog.html).</li><li>The browser  to fetch product&nbsp;data.</li></ul><h4>2️⃣ Frontend (JavaScript) Sends an API&nbsp;Request</h4><ul><li>JavaScript code in catalog.html makes a GET request to the  /api/products.</li></ul><h4>3️⃣ Backend API (GET /api/products) Fetches&nbsp;Data</h4><ul><li>The get_all_products() function runs when the frontend calls /api/products.</li></ul><h4>4️⃣ Database Retrieves Product Information</h4><ul><li>The backend queries the  in the&nbsp;database</li><li>Example database response:</li></ul><pre>[    {\"id\": 1, \"name\": \"Laptop\", \"price\": 799.99},<p>    {\"id\": 2, \"name\": \"Smartphone\", \"price\": 499.99}</p>]</pre><h4>5️⃣ Frontend Renders Product Data in catalog.html</h4><ul><li>The JavaScript loops through the  and dynamically  to display products.</li></ul><pre>&lt;div class=\"product-card\"&gt;  &lt;h3&gt;Laptop&lt;/h3&gt;  &lt;button onclick=\"addToCart(1)\"&gt;Add to Cart&lt;/button&gt;&lt;div class=\"product-card\"&gt;  &lt;p&gt;Price: $499.99&lt;/p&gt;<p>  &lt;button onclick=\"addToCart(2)\"&gt;Add to Cart&lt;/button&gt;</p>&lt;/div&gt;</pre><h3>1.2. SOAP (Simple Object Access Protocol):</h3><p>SOAP has strict rules and rigid messaging standards that can make it more secure than protocols such as REST. These types of APIs are frequently used in enterprise applications, particularly for payment processing and customer management, as they are highly safe in&nbsp;nature.</p><p>A more rigid, protocol-specific API style used in enterprise environments:</p><ul><li>Highly structured messaging</li></ul><pre>&lt;soap:Envelope&gt;  &lt;soap:Header&gt;<p>    &lt;Authorization&gt;Bearer abc123&lt;/Authorization&gt;</p>  &lt;/soap:Header&gt;    &lt;GetUser&gt;    &lt;/GetUser&gt;&lt;/soap:Envelope&gt;</pre><p>A modern API query language that gives clients more&nbsp;control:</p><ul><li>Clients specify precisely what data they&nbsp;need</li><li>Single endpoint for all&nbsp;requests</li><li>Reduces over-fetching and under-fetching of&nbsp;data</li></ul><p>Facebook initially developed GraphQL to simplify endpoint management for REST-based APIs. Instead of maintaining multiple endpoints with small amounts of disjointed data, GraphQL provides a single endpoint that inputs complex queries and outputs only as much information as is needed for the&nbsp;query.</p><pre>query {  user(id: \"123\") {    email      title  }</pre><p>These are programming interfaces provided by software libraries or frameworks:</p><ul><li>Used directly in your&nbsp;code</li><li>No network requests are&nbsp;needed</li><li>Usually specific to a programming language</li></ul><p>Example using a Python library&nbsp;API:</p><pre>import pandas as pd<p># Using pandas API to read a CSV file</p>df = pd.read_csv('data.csv')</pre><p>These allow applications to interact with the operating system:</p><ul></ul><p>Example using Python’s OS&nbsp;API:</p><pre>import os<p># Using OS API to create a directory</p>os.mkdir('new_folder')</pre><p>The foundation of web APIs, using well-defined methods and status&nbsp;codes:</p><ul><li>PUT: Update existing&nbsp;data</li><li>PATCH: Partially update&nbsp;data</li></ul><ul><li>2xx: Success (e.g., 200&nbsp;OK)</li></ul><p>Enables real-time, two-way communication:</p><ul><li>Ideal for chat apps and live&nbsp;updates</li></ul><p><strong><em>Example WebSocket connection:</em></strong></p><pre>const ws = new WebSocket('wss://api.example.com/chat');ws.onmessage = (event) =&gt; {<p>    console.log('Received:', event.data);</p>};</pre><p><strong>gRPC (Google Remote Procedure Call)</strong> is a  framework for <strong>inter-service communication</strong> in <strong>microservices architecture</strong>. Unlike REST APIs that use , gRPC uses <strong>Protocol Buffers (Protobuf)</strong>, making it <strong>faster and more efficient</strong>.</p><p>Google’s high-performance RPC framework:</p><ul><li>Excellent for microservices</li></ul><p><strong><em>Example Protocol Buffer definition:</em></strong></p><h3>📌 How gRPC Works in a Web Application</h3><p>In an , gRPC can be used for <strong>fast communication between microservices</strong>.</p><p>A  must fetch a  from the backend .</p><h4>1️⃣ Defining gRPC Service (product.proto)</h4><p>gRPC services use <strong>Protocol Buffers (Protobuf)</strong> to define API contracts.</p><ul><li>GetAllProducts(): Returns a list of products.</li><li>GetProductById(): Fetches a single product by&nbsp;ID.</li><li>Product: Defines the product structure.</li></ul><h4>2️⃣ Implementing gRPC Server (product_server.py)</h4><p>The gRPC server <strong>implements the service&nbsp;logic</strong>.</p><ul><li>Implements ProductService methods (GetAllProducts, GetProductById).</li></ul><h4>3️⃣ Implementing gRPC Client (product_client.py)</h4><p>The client  to fetch&nbsp;data.</p><ul><li>Calls GetAllProducts() to fetch all products.</li><li>Calls GetProductById() to fetch a single&nbsp;product.</li></ul><p>4️⃣ Running gRPC Server &amp;&nbsp;Client</p><pre># 1. Generate Python code from Protobufpython -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. product.protopython product_server.pypython product_client.py</pre><pre>ProductService gRPC Server is running on port 50051...Product List: products {  name: \"Laptop\"}  id: 2  price: 499.99</pre><h3>1. GET: Used to retrieve&nbsp;data</h3><p>It is a request used to retrieve data. Never used to delete, update or insert&nbsp;data.</p><ul></ul><h4>Product API in E-Commerce Web Application</h4><pre>curl -X GET http://52.90.222.178:5000/api/products</pre><pre>[   {\"id\": 1, \"name\": \"iPhone 15\", \"price\": 999.99},<p>   {\"id\": 2, \"name\": \"Samsung Galaxy S24\", \"price\": 899.99}</p>]</pre><p>If the API returns JSON, you can format the response using&nbsp;</p><pre>curl -X GET http://52.90.222.178:5000/api/products | jq</pre><h3>Debugging &amp; Troubleshooting</h3><p>If you’re not getting the expected response, check:</p><p><strong>Is the Flask server&nbsp;running?</strong></p><pre>curl -X GET http://52.90.222.178:5000/api/health</pre><h3>How the /api/health Endpoint Works in&nbsp;Flask</h3><p>The endpoint is a  that provides the current <strong>status of the application</strong>, including . It helps in monitoring the system and ensuring that the API is .</p><p>I have configured this /api/health endpoint in my E-Commerce Web Application.</p><pre>@app.route(\"/api/health\", methods=[\"GET\"])def health_check():<p>    uptime = time.time() - start_time</p>    return jsonify({        \"uptime\": f\"{uptime:.2f} seconds\",<p>        \"session_active\": \"username\" in session</p>    }), 200</pre><h3>2. POST: Creates new resources</h3><p>The  method is a request used to insert data. Posted data type — JSON.</p><ul></ul><p>✅ Authentication Endpoints</p><p>This is handled in auth_routes.py, prefixed with /api/auth.</p><p> → The frontend sends a request to the backend API (POST /api/auth/login). This is handled in auth_routes.py, prefixed with /api/auth.</p><ol><li>User sends a  with username &amp; password.</li><li>If credentials are&nbsp;valid:</li></ol><ul><li>The user session is&nbsp;stored.</li><li>API returns a success&nbsp;message.</li></ul><p>3. If credentials are&nbsp;invalid:</p><ul><li>API returns </li></ul><h3>3. PUT&nbsp;: Updates existing resources</h3><p>PUT method is used to create or update (replace) a resource. Useful for syncing&nbsp;data.</p><ul></ul><p>Ex: We can <strong>add a “Change Password” feature</strong> for an existing user using a <strong>PUT /api/auth/change-password</strong> API endpoint.</p><h3>Steps to Implement “Change Password” API</h3><ol><li><strong>PUT request with their old and new password.</strong></li></ol><pre>curl -X PUT http://localhost:5000/api/auth/change-password      -H \"Content-Type: app<p>     -d '{\"old_password\": \"currentPass123\", \"new_password\": \"newPass456\"}'</p></pre><p><strong>2. API verifies the old password</strong>:</p><ul><li>If incorrect, return an error (401 Unauthorized).</li></ul><p><strong>3. If correct, update the password in the database</strong>.</p><p><strong>4. Save the new password (after hashing it for security).</strong></p><p><strong>5. Return a success&nbsp;message.</strong></p><pre>{\"message\": \"Password changed successfully\"}</pre><h3><strong><em>In an e-commerce application like yours, a </em></strong><strong><em>PUT method would typically be used&nbsp;in:</em></strong></h3><ol><li> → PUT /api/auth/update-profile</li><li><strong>Updating Product Information (Admin)</strong> → PUT /api/products/&lt;product_id&gt;</li><li> → PUT /api/cart/&lt;cart_id&gt;</li><li> → PUT /api/orders/&lt;order_id&gt;</li></ol><h3><strong>4. DELETE: Removes resources</strong></h3><p>The DELETE method deletes the specified resource.</p><ul></ul><h4><strong><em>/api/cart API to Remove a Product from the&nbsp;Cart</em></strong></h4><p>1️⃣ Endpoint Definition (Flask&nbsp;API):</p><pre>@cart_bp.route('/cart', methods=['DELETE'])def remove_from_cart():    Remove a product from the cart for the logged-in user.<p>    Expects JSON payload: { product_id }.</p>    \"\"\"        user_id = session.get('user_id')            return jsonify({\"message\": \"User not authenticated\"}), 401        product_id = data.get('product_id')            return jsonify({\"message\": \"'product_id' is required\"}), 400<p>        connection = get_db_connection()</p>        cursor = connection.cursor()<p>        delete_query = \"DELETE FROM cart_items WHERE user_id = %s AND product_id = %s\"</p>        cursor.execute(delete_query, (user_id, product_id))            return jsonify({\"message\": \"Product not found in cart\"}), 404        return jsonify({\"message\": \"Product removed from cart successfully\"}), 200        return jsonify({\"message\": \"Failed to remove product from cart\", \"error\": str(e)}), 500        close_db_connection(connection)</pre><ol><li><strong>User sends a DELETE request</strong> with the product_id they want to&nbsp;remove.</li><li><strong>API verifies if the user is logged in</strong> (checks session['user_id']).</li><li> (if missing, returns 400 Bad Request).</li><li> to remove the product from the cart_items table.</li><li><strong>If the product does not exist</strong>, it returns 404 Not&nbsp;Found.</li><li>, it commits the transaction and returns a success message (200&nbsp;OK).</li><li><strong>Handles database errors and ensures the connection is&nbsp;closed.</strong></li></ol><p>3️⃣ Example API Request &amp; Response:</p><p>Now User wanted to delete iPhone 15 Pro from the&nbsp;cart:</p><ul><li>Click the  button under “iPhone 15&nbsp;Pro”.</li><li>The item should disappear, and the cart total should&nbsp;update.</li></ul><p>Run this command in the terminal:</p><pre>curl -X DELETE http://52.90.222.178:5000/api/cart      -H \"Content-Type: application/json\" <p>     -H \"Cookie: session=00068d4c-4b41-4e3e-8884-7389cabbb9b0\"</p>     -d '{\"product_id\": 4}'</pre><pre>{    \"message\": \"Product removed from cart successfully\"</pre><p>After deletion of that&nbsp;item:</p><h3>5. PATCH: Partially updates resources</h3><p>PATCH method is to request used to update data. Only passed data will be updated. You don’t need to provide all the data&nbsp;set.</p><ul></ul><p>The  method is used to  a resource. Instead of sending the entire data set, we <strong>only send the fields that need to be&nbsp;updated</strong>.</p><h4>Use Case: Updating a User’s Profile (PATCH /api/auth/update-profile)</h4><p>Imagine a user wants to update  or  without changing their username.</p><p>1️⃣ PATCH Endpoint: PATCH /api/auth/update-profile</p><p>2️⃣ Sending a PATCH&nbsp;Request</p><p>If the user wants to update </p><pre>curl -X PATCH http://52.90.222.178:5000/api/auth/update-profile \\     -H \"Content-Type: application/json\" \\<p>     -H \"Cookie: session=your_valid_session_id\" \\</p>     -d '{\"email\": \"newemail@example.com\"}'</pre><p>🔹 Only the  field will be&nbsp;updated.</p><p>✅ </p><pre>{    \"message\": \"Profile updated successfully\"</pre><p>❌ <strong>If no fields are provided:</strong></p><pre>{    \"message\": \"No valid fields provided for update\"</pre><pre>{    \"message\": \"User not authenticated\"</pre><h4>4️⃣ Why Use PATCH Instead of&nbsp;PUT?</h4><h3>Conclusion: Understanding APIs, Endpoints, and Methods in Web Development</h3><p>APIs (Application Programming Interfaces) allow different systems to  with each other. They define how requests and responses are exchanged between a client (browser, app) and a&nbsp;server.</p><ul><li>An  acts as a bridge between two applications, enabling data exchange.</li><li>Example: A shopping website uses an API to fetch product details from a database.</li></ul><ul><li>An  is a URL that clients use to request or send&nbsp;data.</li><li>Example: GET /api/products retrieves all products.</li></ul><ul><li> → Uses HTTP methods (GET, POST, PUT, DELETE) to manage&nbsp;data.</li><li> → Lets clients request specific data fields, reducing unnecessary data transfer.</li><li> → Uses XML messaging, mainly in enterprise applications.</li><li> → Maintains a continuous connection for real-time updates (e.g., live&nbsp;chat).</li></ul><ul><li>Use  (JWT, API Keys, OAuth) to restrict&nbsp;access.</li><li>Protect sensitive data with .</li><li>Implement  to prevent&nbsp;abuse.</li></ul><p>APIs are the backbone of modern applications, enabling data sharing between different services. Developers create smooth and efficient digital experiences by designing well-structured and secure&nbsp;APIs.</p><p><em>I’d love to hear what you think about this article — feel free to share your opinions in the comments below (or above, depending on your device!). If you found this helpful or enjoyable, a clap, a comment, or a highlight of your favourite sections would mean a&nbsp;lot.</em></p><p><em>For more insights into the world of technology and data, visit </em><a href=\"http://www.subbutechops.com/\"></a><em> There’s plenty of exciting content waiting for you to&nbsp;explore!</em></p><p><em>Thank you for reading, and happy learning! 🚀</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4253418d18ba\" width=\"1\" height=\"1\" alt=\"\">","contentLength":16408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Most Developers Get This Wrong in Docker Networking!","url":"https://blog.devops.dev/most-developers-get-this-wrong-in-docker-networking-359dbb3eac16?source=rss----33f8b2d9a328---4","date":1739465415,"author":"Gaddam.Naveen","guid":425,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jenkins in Kubernetes: Deployment and Persistent Storage(volume) Setup","url":"https://blog.devops.dev/jenkins-in-kubernetes-deployment-and-persistent-storage-volume-setup-a70fe0579ac8?source=rss----33f8b2d9a328---4","date":1739465410,"author":"th@n@n","guid":424,"unread":true,"content":"<p>Jenkins, a popular automation server, becomes even more powerful when deployed in Kubernetes. Ensuring its availability and data persistence is crucial for uninterrupted CI/CD pipelines. In this guide, we’ll walk through deploying Jenkins in Kubernetes, configuring its resources, and setting up persistent storage to safeguard critical&nbsp;data.</p><p>In this configuration, we have a Deployment resource for deploying Jenkins in Kubernetes, along with associated PersistentVolumeClaim (PVC), PersistentVolume (PV), Service, and StorageClass resources. Let’s break down each&nbsp;part</p><pre>kind: StorageClassapiVersion: storage.k8s.io/v1  name: localstorage<p>provisioner: kubernetes.io/no-provisioner</p>volumeBindingMode: WaitForFirstConsumer</pre><ul><li>This StorageClass resource defines storage provisioning and management policies.</li><li>Since provisioner is set to kubernetes.io/no-provisioner, it indicates that no dynamic provisioning is performed by Kubernetes.</li><li>volumeBindingMode: WaitForFirstConsumer ensures that volume binding waits for the first Pod using the PersistentVolumeClaim to be&nbsp;created.</li></ul><h3>PersistentVolumeClaim (PVC) Resource:</h3><pre>apiVersion: v1kind: PersistentVolumeClaim  name: pvc-jenkinsspec:<p>  storageClassName: localstorage</p>  accessModes:  resources:      storage: 2Gi</pre><ul><li>This PVC resource requests storage from a PersistentVolume using the localstorage StorageClass.</li><li>It requests 2Gi of storage with access mode ReadWriteOnce, meaning it can be mounted as read-write by a single&nbsp;node.</li></ul><h3>PersistentVolume (PV) Resource:</h3><pre>apiVersion: v1kind: PersistentVolume  name: pv-jenkins    type: local  claimRef:    namespace: jenkins    storage: 3Gi    - ReadWriteOnce    path: /mnt<p>  storageClassName: localstorage</p></pre><ul><li>This PV resource represents the actual storage volume available for use by the&nbsp;PVC.</li><li>It is bound to the PVC pvc-jenkins within the jenkins namespace.</li><li>The PV has a capacity of 3Gi and is accessible in ReadWriteOnce mode.</li><li>The storage is provided by a hostPath /mnt on the host machine, with storage class localstorage.</li></ul><pre>apiVersion: apps/v1kind: Deployment  name: jenkins-deployment    name: jenkinsspec:    matchLabels:  replicas: 1    metadata:      labels:    spec:        - name: deployment-jenkins<p>          image: jenkins/jenkins:lts</p>          resources:              memory: \"0.5Gi\"            requests:              cpu: \"125m\"            - name: http-port            - name: jnlp-port          livenessProbe:              path: \"/login\"            initialDelaySeconds: 60            timeoutSeconds: 5          readinessProbe:              path: \"/login\"            initialDelaySeconds: 60            timeoutSeconds: 5          volumeMounts:              mountPath: /var/jenkins_home        - name: data-jenkins            claimName: pvc-jenkins        runAsUser: 0        fsGroup: 0</pre><ul><li>This Deployment resource defines how Jenkins is deployed.</li><li>It specifies a single replica (replicas: 1) of the Jenkins container.</li><li>The container is based on the jenkins/jenkins:lts image.</li><li>Resource limits and requests for CPU and memory are set to ensure resource allocation.</li><li>Ports 8080 and 50000 are exposed for HTTP and JNLP respectively.</li><li>Liveness and readiness probes are configured to check the health of the container.</li><li>The Jenkins home directory (/var/jenkins_home) is mounted to a PersistentVolumeClaim (pvc-jenkins) named data-jenkins.</li><li>SecurityContext ensures that Jenkins runs with the appropriate user and group permissions.</li></ul><pre> securityContext:        runAsUser: 0        fsGroup: 0</pre><p>When you deploy this deployment instance in kubernetes cluster, make sure the user have the right privileges to read and write the host volume For this demo purpose, I am using the root user to do this task, but this is not encouraged to do in real environment.</p><pre>apiVersion: v1kind: Service  name: jenkins-servicespec:    app: jenkins-pod  ports:      port: 8080      nodePort: 32000</pre><ul><li>This Service resource exposes the Jenkins deployment internally within the jenkins namespace.</li><li>It selects pods with the label app: jenkins-pod.</li><li>The service type is NodePort, making the service accessible from outside the cluster on each node's IP at a static port (nodePort: 32000).</li><li>Port 8080 is mapped to the targetPort 8080 where Jenkins is listening.</li></ul><p>Once you execute all the manifiest file in kubernetes cluster.</p><p>Check the host volume path ls -al&nbsp;/mnt</p><p>Execute the below command to see the whether the same files are present in the jenkins container</p><pre>kubectl exec -it POD_NAME /bin/bash -n jenkinsls -al /var/jenkins_home</pre><p>This configuration sets up Jenkins deployment in Kubernetes with persistence using a PersistentVolume and PersistentVolumeClaim. It ensures that Jenkins data stored in /var/jenkins_home persists across container restarts and pod rescheduling. Additionally, the Service resource exposes Jenkins for external access within the Kubernetes cluster.</p><p>For now, that’s it guys, If you like this article don’t forget to give a clap.&nbsp;👏</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a70fe0579ac8\" width=\"1\" height=\"1\" alt=\"\">","contentLength":4859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Build and Deploy a Simple Frontend App with Python Backend","url":"https://blog.devops.dev/how-to-build-and-deploy-a-simple-frontend-app-with-python-backend-108b505be2be?source=rss----33f8b2d9a328---4","date":1739465406,"author":"krth1k","guid":423,"unread":true,"content":"<p>Building a full-stack web application might seem daunting, especially if you’re primarily a backend developer. However, with the right approach, you can create a simple frontend and connect it to a Python backend with&nbsp;ease.</p><p>In this guide, we’ll walk through the process&nbsp;of:</p><ul><li>Setting up a basic Python backend with&nbsp;Flask</li><li>Creating a simple frontend with HTML, CSS, and JavaScript</li><li>Connecting the frontend to the backend using REST&nbsp;API</li><li>Deploying the app on a local Kubernetes cluster</li></ul><h3>1. Setting Up the Python&nbsp;Backend</h3><p>We’ll use , a lightweight Python web framework, to create a REST API that serves data to the frontend.</p><p>Ensure you have Python installed, then install&nbsp;Flask:</p><p>Create a new file called&nbsp;app.py:</p><pre>from flask import Flask, jsonify</pre><pre>@app.route('/api/message')def get_message():<p>    return jsonify({\"message\": \"Hello from the Python backend!\"})</p></pre><pre>if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)</pre><p>This API exposes a single endpoint /api/message that returns a JSON response.</p><p>For the frontend, we’ll use <strong>HTML, CSS, and JavaScript</strong> to display the data from our&nbsp;backend.</p><h3>Create an HTML File (index.html)</h3><pre>&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;    &lt;meta charset=\"UTF-8\"&gt;<p>    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;</p>    &lt;title&gt;Frontend App&lt;/title&gt;        body {<p>            font-family: Arial, sans-serif;</p>            text-align: center;        }            padding: 10px 20px;        }&lt;/head&gt;    &lt;h1&gt;Simple Frontend App&lt;/h1&gt;<p>    &lt;button onclick=\"fetchMessage()\"&gt;Get Message&lt;/button&gt;</p>    &lt;p id=\"message\"&gt;&lt;/p&gt;        function fetchMessage() {<p>            fetch('http://127.0.0.1:5000/api/message')</p>                .then(response =&gt; response.json())                    document.getElementById(\"message\").innerText = data.message;                .catch(error =&gt; console.error('Error:', error));    &lt;/script&gt;&lt;/html&gt;</pre><p>This page has a button that fetches and displays a message from the Flask&nbsp;backend.</p><h3>3. Connecting the Frontend to the&nbsp;Backend</h3><p>Now, let’s serve the frontend using  itself so that both frontend and backend are accessible from the same&nbsp;origin.</p><h3>Update app.py to Serve&nbsp;HTML</h3><p>Modify app.py to serve the index.html file:</p><pre>from flask import Flask, jsonify, send_from_directory</pre><pre>app = Flask(__name__, static_folder='static')</pre><pre>@app.route('/api/message')def get_message():<p>    return jsonify({\"message\": \"Hello from the Python backend!\"})</p></pre><pre>@app.route('/')def serve_frontend():<p>    return send_from_directory('static', 'index.html')</p></pre><pre>if __name__ == '__main__':    app.run(host='0.0.0.0', port=5000, debug=True)</pre><h3>Move index.html to a static&nbsp;Folder</h3><p>Your project structure should now look like&nbsp;this:</p><pre>/project-folder│-- app.py│   └── index.html</pre><p>Now, visit http://127.0.0.1:5000/ in your browser, and your frontend will be&nbsp;served!</p><p>Now, let’s deploy this app using .</p><pre># Use Python base imageFROM python:3.9</pre><pre># Set the working directoryWORKDIR /app</pre><pre># Copy application filesCOPY . .</pre><pre># Install dependenciesRUN pip install flask</pre><pre># Expose port 5000EXPOSE 5000</pre><pre># Run the appCMD [\"python\", \"app.py\"]</pre><h3>Build and Run the Docker Container</h3><pre>docker build -t myapp .docker run -p 5000:5000 myapp</pre><ol><li><strong>Create a Kubernetes Deployment YAML (</strong></li></ol><pre>apiVersion: apps/v1kind: Deployment  name: myapp  replicas: 1    matchLabels:  template:      labels:    spec:        - name: myapp          ports:---kind: Service  name: myapp-service  selector:  ports:      port: 80  type: NodePort</pre><pre>kubectl apply -f deployment.yaml</pre><pre>minikube service myapp-service --url</pre><p>Visit the displayed URL in your&nbsp;browser!</p><p>In this tutorial, we covered: ✅ Creating a Flask backend<p> ✅ Building a simple HTML/JavaScript frontend</p> ✅ Connecting the frontend to the backend<p> ✅ Deploying the app with Docker and Kubernetes</p></p><p>This is a basic example, but you can expand it&nbsp;by:</p><ul><li>Adding user authentication</li><li>Using React or Vue.js for a modern&nbsp;frontend</li><li>Storing and retrieving data from a&nbsp;database</li></ul><p>If you found this helpful, let me know in the comments! 🚀</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=108b505be2be\" width=\"1\" height=\"1\" alt=\"\">","contentLength":3895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Best Practices For Database Authorization In Multi-Tenant Systems","url":"https://blog.devops.dev/best-practices-for-database-authorization-in-multi-tenant-systems-001a1bcf2568?source=rss----33f8b2d9a328---4","date":1739465383,"author":"Noel","guid":422,"unread":true,"content":"<p>Multi-tenant databases allow multiple companies or organizations (tenants) to securely share the same database infrastructure while ensuring data isolation and integrity. However, this shared structure introduces complexities in managing access and authorization. A robust authorization strategy is essential to ensure that users can only access resources belonging to their tenant without compromising scalability or performance.</p><p>This article explores the best practices and technical solutions that we adopt at <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Best+Practices+For+Database+Authorization+In+Multi-Tenant+Systems\">Xinthe</a>, for implementing efficient authorization mechanisms in multi-tenant systems, with a focus on nested resource structures, such as companies, clients, projects, and tasks. After having read this entire article, you will be armed with actionable insights to build secure, efficient, and future-proof authorization strategies for multi-tenant applications.</p><h3><strong>Understanding Multi-Tenant Database Authorization</strong></h3><ul><li><strong><em>Definition Of Multi-Tenancy</em></strong></li></ul><p>Multi-tenancy refers to an architectural pattern where a single instance of a software application and its database serves multiple tenants (e.g., companies, organizations, or users). Each tenant’s data remains logically isolated, ensuring that no tenant can access another’s data, while sharing underlying resources for efficiency.</p><p><strong>Key Multi-Tenancy Models:</strong></p><p>Each tenant has a dedicated database or&nbsp;schema.</p><p>Offers strong isolation and security.</p><p>Higher costs and maintenance complexity due to multiple instances.</p><p>Multiple tenants share the same database.</p><p>Logical separation is maintained through identifiers (e.g., tenant_id or company_id).</p><p>Cost-effective and scalable but requires robust authorization mechanisms.</p><ul><li><strong><em>Challenges Of Authorization</em></strong></li></ul><p>Implementing authorization in multi-tenant systems is a non-trivial task, especially as the scale and complexity of resources grow. Common challenges include&nbsp;-</p><p><strong>Cross-Tenant Data&nbsp;Leakage:</strong></p><p>Risk: Improper queries or configurations can expose data to unauthorized tenants.</p><p>Example: A user from Company A inadvertently accessing tasks belonging to Company B due to a missing or incorrect WHERE&nbsp;clause.</p><p>Deeply nested resource structures often require joins across multiple&nbsp;tables.</p><p>Queries with extensive joins can degrade performance as data volume increases.</p><p><strong>Scalability &amp; Maintainability:</strong></p><p>The need to balance fast access controls with a maintainable schema.</p><p>Adding new authorization rules or resource types without overhauling the&nbsp;system.</p><p><strong>Data Localization &amp; Compliance:</strong></p><p>For multi-tenant systems spanning regions, ensuring that tenant data complies with regulations like GDPR can complicate authorization logic.</p><ul><li><strong><em>Importance Of Nested Resource Structures</em></strong></li></ul><p>In many applications, resources are interconnected in a hierarchical fashion. Consider the following nested structure -</p><p><strong>Company → Client → Project →&nbsp;Task</strong></p><p>A  has multiple&nbsp;.</p><p>Each  manages several .</p><p>Each  contains multiple&nbsp;.</p><p><strong>Why Nested Structures Matter:</strong></p><p><strong>Access Control Complexity:</strong> Permissions must flow through the hierarchy (e.g., a user’s access to a task must be verified against their company).</p><p> Hierarchical access often necessitates multiple joins, impacting query efficiency.</p><p> Hierarchical structures reflect real-world use cases like SaaS platforms, where users must operate within their organization’s boundaries.</p><p>A user from Company A should only edit tasks within their projects. Authorization must ensure that the task → project → client → company linkage is maintained without exposing data from Company&nbsp;B.</p><h3><strong>Comparing Authorization Approaches</strong></h3><p>When implementing authorization in a multi-tenant database, there are three common strategies to choose from: the , the , and <strong>Tenant-Specific Databases or Tables</strong>. Each comes with its own set of benefits and trade-offs. Let’s break them down&nbsp;-</p><ul><li><strong><em>1. Flat Model (Adding Tenant&nbsp;IDs)</em></strong></li></ul><p>In this approach, a tenant_id or company_id is added to every resource table (e.g., tasks, projects, clients), enabling direct filtering for authorization.</p><p> Queries can directly filter by tenant_id without traversing the hierarchy.</p><pre>SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;</pre><p> Reduces query complexity by avoiding multiple table joins to enforce&nbsp;access.</p><p> Straightforward implementation makes it easy to debug and maintain.</p><p> tenant_id is replicated across multiple tables, introducing redundancy.</p><p> Adding tenant_id and other metadata can lead to bloated schemas, especially as the number of attributes grows.</p><p> Schema updates (e.g., adding new relationships) might require extensive changes across multiple&nbsp;tables.</p><p>Ideal for systems where performance is critical and the schema is relatively stable, such as SaaS platforms with many small&nbsp;tenants.</p><ul><li><strong><em>2. Hierarchical Model (Enforcing Relationships)</em></strong></li></ul><p>In this approach, the relationships between resources (e.g., task → project → client → company) are strictly enforced through foreign keys. Authorization is achieved by traversing the hierarchy.</p><p> Avoids redundant fields by relying on inherent relationships.</p><pre>CREATE TABLE tasks (    id SERIAL PRIMARY KEY,<p>    project_id INT REFERENCES projects(id),</p>    ...    id SERIAL PRIMARY KEY,<p>    client_id INT REFERENCES clients(id),</p>    ...    id SERIAL PRIMARY KEY,<p>    company_id INT REFERENCES companies(id),</p>    ...</pre><p> Reduces duplication of metadata like tenant_id.</p><p><strong>Relationship-Centric Queries:</strong> Makes it easier to enforce hierarchical constraints and maintain referential integrity.</p><p> Queries require multiple joins to verify access, which can impact performance.</p><pre>SELECT t.*FROM tasks t<p>JOIN projects p ON t.project_id = p.id</p>JOIN clients c ON p.client_id = c.id<p>WHERE c.company_id = :tenant_id AND t.id = :task_id;</p></pre><p> Deep hierarchies with large datasets can significantly increase query execution time.</p><p> As the hierarchy grows, maintaining performance becomes challenging.</p><p>Suitable for applications where maintaining strict relationships between resources is essential, such as ERP systems or large enterprise applications.</p><ul><li><strong><em>3. Tenant-Specific Databases Or&nbsp;Tables</em></strong></li></ul><p>This approach creates separate databases or tables for each tenant, isolating their data entirely.</p><p> Each tenant’s data can be managed independently, making it easier to scale horizontally by distributing databases across&nbsp;servers.</p><p> Ensures complete data isolation, reducing the risk of cross-tenant data&nbsp;leakage.</p><p> Simplifies adherence to regulations like GDPR by enabling tenant-specific backups, retention policies, and deletions.</p><p> Managing multiple databases or schemas requires sophisticated deployment and CI/CD pipelines.</p><p> Schema updates need to be applied consistently across all tenant databases.</p><p> For tenants with small datasets, the resource consumption of separate databases might be inefficient.</p><p>Best for large organizations with high regulatory or security requirements, or when dealing with tenants that require dedicated resources (e.g., enterprise customers).</p><ul><li><strong><em>Summary Table — Comparing Approaches</em></strong></li></ul><h3><strong>Criteria For Choosing An Authorization Model</strong></h3><p>Selecting the right authorization model for a multi-tenant database is critical for ensuring scalability, performance, and compliance. The decision hinges on a combination of technical, regulatory, and operational factors. Below are the primary criteria to consider&nbsp;-</p><p>The level of traffic and query complexity your application handles directly impacts the choice of an authorization model.</p><p><strong>High-Traffic Applications</strong>:</p><p>Benefit from simpler and faster queries, such as those enabled by the .</p><pre>SELECT * FROM tasks WHERE tenant_id = :tenant_id AND id = :task_id;</pre><p>Minimal joins mean lower query latency, ensuring the system performs well under heavy&nbsp;loads.</p><p>Suitable for SaaS platforms or e-commerce systems with a high volume of tenant interactions.</p><p>:</p><p>Can afford the  with more joins, as performance trade-offs are less significant.</p><p>Allows for cleaner schema designs and strict relational integrity.</p><p>Suitable for internal enterprise tools or smaller-scale applications.</p><ul><li><strong><em>2. Regulatory Requirements</em></strong></li></ul><p>Compliance with data protection and privacy regulations often dictates how data is stored and accessed.</p><p>Using <strong>Tenant-Specific Databases or Tables</strong> simplifies compliance for regulations like GDPR or&nbsp;HIPAA.</p><p>Tenant isolation reduces the risk of data leakage and ensures tenant-specific data retention and deletion policies.</p><p>An enterprise customer requires dedicated storage with separate backups and audit&nbsp;logs.</p><p>A  can still meet compliance needs with appropriate access controls and audit mechanisms.</p><p>Challenges arise in managing and enforcing tenant-specific data governance policies within shared infrastructure.</p><p>The ability to handle growth in the number of tenants and data volumes is a critical&nbsp;factor.</p><p><strong>Planning For Tenant&nbsp;Growth</strong>:</p><p>For a rapidly scaling user base, <strong>Tenant-Specific Databases or Tables</strong> provide the most flexibility -</p><p>Each tenant can be distributed across servers to balance&nbsp;load.</p><p>Tenant databases can be independently scaled based on specific&nbsp;needs.</p><p>A B2B SaaS platform serving both small businesses and large enterprises can allocate resources dynamically based on tenant&nbsp;size.</p><p>The  can handle larger datasets more efficiently as indexes on tenant_id make filtering faster.</p><p>The  may struggle as table sizes grow, requiring optimization for complex&nbsp;joins.</p><p>Ease of schema management and updates is essential for long-term maintainability.</p><p><strong>Simplified Schema&nbsp;Updates</strong>:</p><p>The  simplifies schema updates by centralizing data attributes like tenant_id.</p><p>However, redundant fields may increase the risk of errors during&nbsp;updates.</p><p>The  enforces relational integrity, ensuring data consistency.</p><p>Complex queries for nested structures may require more effort to maintain and optimize.</p><p><strong>Automated CI/CD Pipelines</strong>:</p><p>For <strong>Tenant-Specific Databases</strong>, CI/CD automation becomes critical to manage schema changes across multiple databases.</p><p>Tools like Octopus Deploy or Liquibase can help automate schema migrations and ensure consistency.</p><ul><li><strong><em>Key Considerations Summary</em></strong></li></ul><h3><strong>Designing Authorization Strategies For Multi-Tenancy</strong></h3><p>Designing robust authorization strategies for multi-tenant systems requires careful consideration of schema design, indexing, and data partitioning to ensure scalability, security, and performance. This section outlines best practices for implementing these strategies effectively.</p><p>The foundation of a successful multi-tenant authorization system lies in a well-thought-out schema.</p><p>Add a tenant_id column to all relevant tables (e.g., clients, projects, tasks) for direct tenant filtering.</p><pre>CREATE TABLE tasks (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    FOREIGN KEY (project_id) REFERENCES projects(id)</p>);</pre><p>Ensure tenant_id is a mandatory field in all write operations to enforce multi-tenancy constraints.</p><p><strong>Defining Relationships In Hierarchical Structures</strong>:</p><p>Maintain strict referential integrity between hierarchical entities.</p><p>Example for hierarchical relationships -</p><pre>CREATE TABLE projects (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    client_id BIGINT NOT NULL,    FOREIGN KEY (client_id) REFERENCES clients(id)</pre><p>Flat schema enables quick lookups for tenant-specific data.</p><p>Hierarchical relationships ensure data consistency and logical separation.</p><ul><li><strong><em>2. Indexing Best Practices</em></strong></li></ul><p>Indexes are essential for optimizing queries in multi-tenant systems. However, improper indexing can lead to inefficiencies.</p><p><strong>Compound Indexes For Tenant-Specific Queries</strong>:</p><p>Use composite indexes combining tenant_id with frequently queried&nbsp;columns.</p><pre>CREATE INDEX idx_tasks_tenant_projectON tasks (tenant_id, project_id, status);</pre><p>This enables efficient filtering and sorting within a tenant’s&nbsp;scope.</p><p><strong>Balancing Indexing Depth &amp; Query&nbsp;Speed</strong>:</p><p>Avoid over-indexing, which can slow down write operations.</p><p>Prioritize indexing columns involved in filtering, joining, and sorting operations.</p><p>Regularly analyze query performance using tools like  in PostgreSQL or  in&nbsp;MySQL.</p><p>Partitioning improves scalability by dividing data into smaller, more manageable segments, reducing query times for tenant-specific operations.</p><p><strong>Horizontal Partitioning By&nbsp;Tenants</strong>:</p><p>Partition data within a single database based on tenant_id.</p><pre>CREATE TABLE tasks_1 PARTITION OF tasksFOR VALUES IN (1); -- Partition for tenant_id 1</pre><p>Faster tenant-specific queries as partitions reduce the search&nbsp;space.</p><p>Simplifies maintenance for large datasets.</p><p><strong>Database Sharding For High-Scale Systems</strong>:</p><p>Distribute tenant data across multiple databases (shards).</p><p>Example Sharding Strategy&nbsp;-</p><p>Use tenant_id % shard_count to assign tenants to&nbsp;shards.</p><p>Tools like  or  can manage sharding in distributed database&nbsp;systems.</p><p>Eliminates contention in single-database systems.</p><p>Enhances fault isolation and scalability.</p><p><strong>Example Use Case — Applying These Strategies</strong></p><p>An application manages 100,000 tenants, each with thousands of projects and&nbsp;tasks.</p><p>Add tenant_id to all&nbsp;tables.</p><p>Use foreign keys to link tasks → projects →&nbsp;clients.</p><p>Create a compound index on tasks (tenant_id, project_id) for common queries like&nbsp;-</p><pre>SELECT * FROM tasks WHERE tenant_id = 123 AND project_id = 456;</pre><p>For smaller tenants, use horizontal partitioning -</p><pre>CREATE TABLE tasks_tenant_123 PARTITION OF tasks FOR VALUES IN (123);</pre><p>For larger tenants, shard data across multiple databases to&nbsp;scale.</p><p>A well-designed schema with tenant_id simplifies multi-tenant data filtering.</p><p>Proper indexing ensures efficient queries, even at&nbsp;scale.</p><p>Partitioning and sharding prepare the system for growth, reducing query times and enhancing reliability.</p><p>This section provides concrete examples of implementing different authorization models for multi-tenant systems, including schemas, queries, and tooling. Each approach demonstrates how to enforce tenant-specific access effectively.</p><ul><li><strong><em>1. Flat Model Implementation</em></strong></li></ul><p>The flat model relies on adding a tenant_id column to all relevant tables, ensuring that queries are scoped to the tenant directly.</p><pre>CREATE TABLE tasks (    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,</p>    FOREIGN KEY (project_id) REFERENCES projects(id)    id BIGINT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    client_id BIGINT NOT NULL,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,<p>    FOREIGN KEY (client_id) REFERENCES clients(id)</p>);</pre><p>: Access tasks for a user’s company&nbsp;-</p><pre>SELECT * FROM tasks <p>WHERE tenant_id = 123 AND status = 'in_progress';</p></pre><p>Simplifies authorization logic with direct&nbsp;lookups.</p><p>Reduces query complexity by avoiding&nbsp;joins.</p><p>Potential schema bloat with additional tenant_id columns.</p><ul><li><strong><em>2. Hierarchical Model Implementation</em></strong></li></ul><p>In this model, tenant authorization is enforced by traversing relationships between resources (e.g., Company → Client → Project →&nbsp;Task).</p><pre>CREATE TABLE companies (    id BIGINT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT PRIMARY KEY,<p>    company_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    FOREIGN KEY (company_id) REFERENCES companies(id)</p>);    id BIGINT PRIMARY KEY,<p>    client_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    FOREIGN KEY (client_id) REFERENCES clients(id)</p>);    id BIGINT PRIMARY KEY,<p>    project_id BIGINT NOT NULL,</p>    description TEXT,    FOREIGN KEY (project_id) REFERENCES projects(id)</pre><p>: Check task access by traversing relationships -</p><pre>SELECT t.* FROM tasks t<p>INNER JOIN projects p ON t.project_id = p.id</p>INNER JOIN clients c ON p.client_id = c.id<p>INNER JOIN companies co ON c.company_id = co.id</p>WHERE co.id = 123 AND t.status = 'in_progress';</pre><p>Maintains normalized relationships.</p><p>Avoids redundant tenant_id columns.</p><p>Complex joins increase query&nbsp;costs.</p><p>Requires optimized indexes to maintain performance.</p><ul><li><strong><em>3. Tenant-Specific Database/Table Implementation</em></strong></li></ul><p>For scenarios requiring strict isolation, separate databases or tables for each tenant can be&nbsp;used.</p><p>Create a separate database or schema for each tenant&nbsp;-</p><pre>CREATE DATABASE company_123;CREATE TABLE company_123.tasks (    project_id BIGINT NOT NULL,    status VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><p>: Access tasks for a specific tenant&nbsp;-</p><pre>USE company_123;SELECT * WHERE status = 'in_progress';</pre><p>: Use CI/CD tools like  to manage multi-tenant databases:</p><p>Automate schema changes across databases.</p><p>Track versioning for each tenant’s database.</p><p>Complete tenant isolation for security and compliance (e.g.,&nbsp;GDPR).</p><p>Simplifies data archival and backup for individual tenants.</p><p>Deployment complexity increases with the number of&nbsp;tenants.</p><p>Resource-intensive for systems with many small&nbsp;tenants.</p><ul><li><strong><em>Choosing The Right Implementation</em></strong></li></ul><p>Use the flat model for simplicity in high-traffic environments.</p><p>Use the hierarchical model when data relationships must be preserved and redundancy minimized.</p><p>Opt for tenant-specific databases for strict isolation and compliance requirements.</p><p>Each implementation can be tailored based on application needs, tenant size, and regulatory requirements. Balancing performance, scalability, and maintainability is key to successful multi-tenant authorization systems.</p><h3><strong>Security Best Practices For Authorization</strong></h3><p>Ensuring robust security in multi-tenant systems is essential to prevent data breaches, maintain compliance, and build user trust. This section outlines key practices for implementing secure and reliable authorization mechanisms.</p><ul><li><strong><em>1. Strict Access&nbsp;Controls</em></strong></li></ul><p>Implementing strong access controls ensures that only authorized users can access or modify resources.</p><p><strong>Role-Based Access Control&nbsp;(RBAC)</strong>:</p><p>Assign roles (e.g., Admin, Manager, User) to users based on their responsibilities.</p><p>Enforce role-specific permissions at the application and database&nbsp;layers.</p><p>Example: Use database roles to restrict access to tenant-specific tables.</p><pre>CREATE ROLE company_admin;GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO company_admin;<p>REVOKE ALL ON ALL TABLES FROM PUBLIC; -- Restrict public access</p></pre><p>:</p><p>Enforce tenant-level data segregation directly at the database&nbsp;layer.</p><p>RLS ensures that queries automatically filter data based on the user’s&nbsp;tenant.</p><p><strong>PostgreSQL Example For&nbsp;RLS</strong>:</p><pre>CREATE POLICY tenant_policyON tasks<p>USING (tenant_id = current_setting('app.current_tenant')::BIGINT);</p><p>ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;</p>SET app.current_tenant = '123'; -- Simulate tenant context<p>SELECT * FROM tasks; -- Only tasks with tenant_id = 123 will be visible</p></pre><ul><li><strong><em>2. Preventing Cross-Tenant Data&nbsp;Leaks</em></strong></li></ul><p>Preventing accidental or intentional cross-tenant data leaks is critical in multi-tenant architectures.</p><p><strong>Multi-Layer Access&nbsp;Checks</strong>:</p><p>Enforce tenant isolation at both the database and application layers.</p><p>Validate all queries to ensure they are scoped to the user’s&nbsp;tenant.</p><p>Always include a tenant_id check in database&nbsp;queries.</p><p>Use database views or abstractions to simplify tenant-specific filtering.</p><p><strong>Application-Layer Validation</strong>:</p><p>Add additional validation at the application level as a guardrail.</p><p>Ensure that APIs restrict data access to the authenticated tenant&nbsp;context.</p><pre>def get_user_tasks(user):    if user.tenant_id != request.tenant_id:<p>        raise PermissionDenied(\"Cross-tenant access is not allowed.\")</p>    return db.query(Tasks).filter(Tasks.tenant_id == user.tenant_id).all()</pre><p>Audit logs are essential for monitoring, compliance, and debugging. They provide visibility into access patterns and help detect unauthorized access attempts.</p><p>User ID and tenant ID for all&nbsp;queries.</p><p>Access attempts (successful and&nbsp;failed).</p><p>Data modification operations (insert, update,&nbsp;delete).</p><p>Timestamps and IP addresses for requests.</p><p><strong>SQL Example For Logging&nbsp;Queries</strong>:</p><pre>CREATE TABLE audit_logs (    id SERIAL PRIMARY KEY,    tenant_id BIGINT,    table_name VARCHAR(255),    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP<p>INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)</p>VALUES (123, 456, 'SELECT', 'tasks', 'SELECT * FROM tasks WHERE tenant_id = 456');</pre><p><strong>Integrating Logging&nbsp;Tools</strong>:</p><p>Use database triggers to log operations automatically.</p><p>Combine with external tools like  (Elasticsearch, Logstash, Kibana) or  for advanced monitoring.</p><pre>CREATE OR REPLACE FUNCTION log_task_changes()RETURNS TRIGGER AS $$    INSERT INTO audit_logs (user_id, tenant_id, action, table_name, query)<p>    VALUES (current_user_id(), NEW.tenant_id, TG_OP, TG_TABLE_NAME, current_query());</p>    RETURN NEW;$$ LANGUAGE plpgsql;<p>CREATE TRIGGER audit_task_changes</p>AFTER INSERT OR UPDATE OR DELETE ON tasks<p>FOR EACH ROW EXECUTE FUNCTION log_task_changes();</p></pre><p>Combine RBAC, RLS, and application-level validation for comprehensive protection.</p><p>Use multi-layered access checks and robust query scoping to ensure tenant isolation.</p><p>Maintain detailed audit logs to track access and modifications for accountability and compliance.</p><p>These practices create a secure foundation for multi-tenant authorization systems, ensuring that each tenant’s data is isolated, protected, and auditable.</p><p>Designing a robust multi-tenant authorization system involves navigating a set of challenges and trade-offs. Each approach has its own set of complexities that must be carefully managed to ensure scalability, performance, and maintainability.</p><ul><li><strong><em>1. Balancing Performance &amp; Flexibility</em></strong></li></ul><p>Choosing between speed and schema cleanliness can significantly impact your database design and performance.</p><p><strong>Prioritizing Performance (Flat&nbsp;Model)</strong>:</p><p>Direct lookups using a tenant_id column ensure fast query execution.</p><p>Reduced join complexity leads to quicker response&nbsp;times.</p><p>: May result in data redundancy (e.g., repeating tenant IDs across multiple&nbsp;tables).</p><p><strong>SQL Example For Optimized Query</strong>:</p><pre>SELECT * FROM tasks <p>WHERE tenant_id = 123 AND status = 'pending';</p></pre><p><strong>Prioritizing Schema Cleanliness (Hierarchical Model)</strong>:</p><p>Using a normalized schema ensures a clean and consistent database structure.</p><p>: Requires more complex joins and increased query times, especially for deeply nested relationships.</p><p><strong>Hierarchical Query&nbsp;Example</strong>:</p><pre>SELECT tasks.*FROM tasks<p>JOIN projects ON tasks.project_id = projects.id</p>JOIN clients ON projects.client_id = clients.id<p>WHERE clients.tenant_id = 123;</p></pre><p>Managing tenant-specific setups adds complexity, particularly as the number of tenants&nbsp;grows.</p><p><strong>Tenant-Specific Databases</strong>:</p><p>Each tenant has its own database, simplifying compliance and data isolation.</p><p>: Maintaining consistency across databases for schema&nbsp;changes.</p><p>: Use automation tools like  or  to manage schema migrations across&nbsp;tenants.</p><pre># Liquibase command to apply migrations to multiple tenant databasesliquibase --url=\"jdbc:mysql://db_host/tenant1\" update<p>liquibase --url=\"jdbc:mysql://db_host/tenant2\" update</p></pre><p><strong>Single Multi-Tenant Database</strong>:</p><p>Shared schema reduces maintenance but requires more sophisticated query scoping and indexing.</p><p>: Tracking and isolating tenant data effectively without introducing query overhead.</p><ul><li><strong><em>3. Handling Schema Updates In Multi-Tenant Databases</em></strong></li></ul><p>Ensuring all tenants have consistent schemas while minimizing downtime is one of the most significant challenges in multi-tenant systems.</p><p>Use versioned migrations to apply incremental updates across all&nbsp;tenants.</p><p>Maintain backward compatibility to prevent disruptions during&nbsp;updates.</p><pre>CREATE TABLE schema_versions (    tenant_id BIGINT,    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><p>Apply updates to a subset of tenants, validate, and then roll out to the&nbsp;rest.</p><p>Use feature flags to selectively enable new schema features.</p><p><strong>Code Example For Rolling&nbsp;Updates</strong>:</p><pre>tenants = get_tenant_list()for tenant in tenants:<p>    apply_schema_update(tenant_id=tenant.id)</p></pre><p><strong>Testing &amp; CI/CD For Multi-Tenant Systems</strong>:</p><p>Test migrations on a staging environment with realistic tenant data before deploying.</p><p>Use CI/CD tools like  to automate and track updates across tenant databases.</p><p><strong>Performance Vs. Flexibility</strong>:</p><p>Use a flat model for high-speed queries or hierarchical models for cleaner schemas but expect performance trade-offs.</p><p>Tenant-specific databases simplify compliance but require robust automation for schema management.</p><p>Implement version control and rolling updates to ensure seamless schema changes across all&nbsp;tenants.</p><p>Addressing these challenges with well-defined strategies ensures a scalable and maintainable multi-tenant authorization system, capable of adapting to evolving application needs.</p><p>Real-world applications of multi-tenant database authorization vary depending on the complexity of the resource structure, performance requirements, and compliance needs. Below are three illustrative scenarios demonstrating how different authorization models can be applied effectively.</p><ul><li><strong><em>Scenario 1 — Flat Model For A SaaS CRM&nbsp;App</em></strong></li></ul><p>A SaaS customer relationship management (CRM) application needs to store and manage customer interactions for multiple companies, ensuring users can only access data associated with their organization.</p><p>: Each company has its own sales team, and users need quick access to customer records and sales&nbsp;data.</p><p>: Use a flat model by adding tenant_id to every table, such as customers, leads, and&nbsp;sales.</p><pre>CREATE TABLE customers (    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    name VARCHAR(255),    phone VARCHAR(20),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    tenant_id BIGINT NOT NULL,</p>    customer_id BIGINT,    status VARCHAR(50),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);</pre><pre>SELECT * FROM customers </pre><p>Simple queries without joins for tenant-specific data.</p><p>High performance due to direct&nbsp;lookups.</p><p>Wider tables due to the inclusion of tenant_id.</p><p>Potential redundancy if relationships between entities are not properly normalized.</p><ul><li><strong><em>Scenario 2 — Hierarchical Model For A Project Management Tool</em></strong></li></ul><p>A project management tool with a nested structure: Company → Client → Project → Task. Users need to manage projects while maintaining strict access control based on their organization.</p><p>: Each company has multiple clients, each with its own projects and tasks. Users must only access tasks related to their&nbsp;company.</p><p>: Use a hierarchical model to enforce relationships and control access through&nbsp;joins.</p><pre>CREATE TABLE companies (    id BIGINT AUTO_INCREMENT PRIMARY KEY,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    company_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    client_id BIGINT NOT NULL,</p>    name VARCHAR(255),<p>    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</p>);    id BIGINT AUTO_INCREMENT PRIMARY KEY,<p>    project_id BIGINT NOT NULL,</p>    description TEXT,    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP</pre><pre>SELECT tasks.*FROM tasks<p>JOIN projects ON tasks.project_id = projects.id</p>JOIN clients ON projects.client_id = clients.id<p>WHERE clients.company_id = 123;</p></pre><p>Clean, normalized schema without redundant data.</p><p>Access naturally follows the hierarchy.</p><p>Complex queries due to multi-level joins.</p><p>Slower query performance for deep hierarchies.</p><ul><li><strong><em>Scenario 3 — Single-Tenant Databases For An Enterprise App</em></strong></li></ul><p>An enterprise application handles sensitive data, requiring strict data isolation for compliance with GDPR and HIPAA regulations.</p><p>: Each tenant’s data must be completely isolated to ensure compliance and scalability.</p><p>: Use a single-tenant database model where each company has its own dedicated database.</p><pre>Database Names:  tenant_1_db  tenant_3_db</pre><p>: Use  to manage database updates across multiple&nbsp;tenants.</p><pre>deploy:  steps:<p>    - name: Update Tenant Databases</p>      script: |<p>        for db in $(list_databases); do</p>          apply_migrations $db</pre><p>Complete isolation ensures compliance with regulatory requirements.</p><p>Scalability: Large tenants can have dedicated resources (e.g., separate hardware).</p><p>Higher operational complexity in managing multiple databases.</p><p>Requires robust CI/CD pipelines for schema&nbsp;updates.</p><p>Each use case demonstrates how careful consideration of application requirements, data relationships, and compliance needs can guide the choice of the best authorization model for a multi-tenant database.</p><p>Efficient, scalable, and secure authorization in multi-tenant databases as we’ve found often at <a href=\"https://xinthe.com/?utm_source=medium&amp;utm_medium=blog&amp;utm_campaign=Best+Practices+For+Database+Authorization+In+Multi-Tenant+Systems\">Xinthe</a>, requires a well-thought-out approach tailored to the application’s needs.</p><p>Authorization in multi-tenant databases isn’t a one-size-fits-all challenge. Developers and database architects must carefully evaluate their application’s structure, expected growth, and regulatory needs to select the most effective approach. Armed with the insights and strategies outlined in this article, you can design multi-tenant database systems that are secure, scalable, and efficient, ensuring both developer productivity and a seamless user experience.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=001a1bcf2568\" width=\"1\" height=\"1\" alt=\"\">","contentLength":28256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker, Kubernetes, and NATS — The Backbone of Cloud-Native Apps","url":"https://blog.devops.dev/docker-kubernetes-and-nats-the-backbone-of-cloud-native-apps-af724f41c17d?source=rss----33f8b2d9a328---4","date":1739465289,"author":"Cristhian Ferrufino","guid":421,"unread":true,"content":"<h3><strong>Docker, Kubernetes, and NATS — The Backbone of Cloud-Native Apps</strong></h3><p>Welcome back to the ! In the <a href=\"https://medium.com/devops-dev/decoding-the-message-broker-kafka-vs-rabbitmq-vs-nats-a-tale-of-three-titans-a4f47127256b\">last article</a>, we explored the world of message brokers and why NATS is a standout choice for modern microservices. Now, it’s time to dive into the backbone of cloud-native applications:  and . If microservices are the chefs in our restaurant analogy, containers are the kitchen tools that keep everything running smoothly. And Kubernetes? That’s the head chef, making sure everyone works in&nbsp;harmony.</p><p>In this article, we’ll break down  and , explore how they work together, and even touch on how  fits into the mix. By the end, you’ll have a solid understanding of how to containerize your applications and orchestrate them like a pro. Let’s get&nbsp;cooking!</p><h3>What Is Containerization, and Why Is It Important?</h3><p>Imagine you’re shipping a fragile package across the world. You’d want to pack it in a sturdy container, right? That’s exactly what containerization does for your applications. It packages your app and all its dependencies (libraries, frameworks, etc.) into a lightweight, portable unit called a . This ensures that your app runs consistently across different environments — whether it’s your laptop, a testing server, or a production cluster.</p><p><strong>Why developers love containers:</strong>✅ : “Works on my machine” becomes “Works everywhere.”✅ : No more dependency hell — each app lives in its own bubble.✅ : Deploy to AWS, Azure, or your grandma’s PC (if she’s cool with Kubernetes).✅ : 10x lighter than VMs. Think EVs vs. a gas-guzzling truck</p><h3>Introduction to Docker: Building, Running, and Managing Containers</h3><p>Docker is the most popular tool for containerization, and for good reason. It’s simple, powerful, and widely supported. Let’s break it&nbsp;down:</p><p>To create a container, you start with a  — a text file that defines the steps to build your app’s environment. Here’s a simple&nbsp;example:</p><pre># Use a lightweight Python image (because nobody likes bloat)FROM python:3.9-slim<p># Set the stage for your app</p>WORKDIR /app<p># Install dependencies (Pro tip: Skip the cache to shrink your image)</p>COPY requirements.txt .<p>RUN pip install --no-cache-dir -r requirements.txt</p><p># Copy the rest of the code</p>COPY . .<p># Open the app’s “front door”</p>EXPOSE 8080CMD [\"python\", \"app.py\"]</pre><p>With this Dockerfile, you can build a container image using the ``&nbsp;command:</p><pre>🚀 Run this: docker build -t my-python-app .</pre><p>Once you’ve built your image, you can run it as a container:</p><pre>🎯 Pro tip: Map ports like a pirate mapping treasure.  docker run -p 8080:8080 my-python-app</pre><p>This command starts your app and maps port 8080 on your host to port 8080 in the container. Easy,&nbsp;right?</p><p>Docker also provides tools to manage your containers:</p><ul><li>: List running containers.</li><li><em>docker logs &lt;container_id&gt;</em>: View logs for a specific container</li><li><em>docker stop &lt;container_id&gt;</em>: Stop a running container.</li></ul><h3>Docker vs. Podman: A Detailed Comparison</h3><p>While Docker is the most popular containerization tool, it’s not the only one.  is a rising star in the container world, and it’s worth understanding how it compares to&nbsp;Docker.</p><pre>+----------------------+-------------------------------------+-----------------------------------+| Feature              | Docker 🐳                           | Podman 📦                         |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Daemon Requirement   | Requires a daemon (dockerd)         | Daemonless (runs containers       |<p>|                      |                                     | directly)                         |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Root vs. Rootless    | Runs as root by default              | Supports rootless containers out  |</p>|                      |                                     | of the box                        |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Compatibility        | Uses Docker CLI and Dockerfiles     | Fully compatible with Docker CLI  |<p>|                      |                                     | and Dockerfiles                   |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Security             | Requires root privileges, which can | Rootless mode reduces attack      |</p>|                      | be a security risk                  | surface                           |<p>+----------------------+-------------------------------------+-----------------------------------+</p>| Orchestration        | Requires Docker Swarm for           | Integrates with Kubernetes        |<p>|                      | orchestration                       | natively                          |</p>+----------------------+-------------------------------------+-----------------------------------+<p>| Community Support    | Larger community and ecosystem      | Growing community, backed by Red  |</p>|                      |                                     | Hat                               |<p>+----------------------+-------------------------------------+-----------------------------------+</p></pre><ul><li>You need a mature, widely supported tool with a large ecosystem.</li><li>You’re already using Docker Swarm for orchestration.</li><li>You’re okay with running containers as&nbsp;root.</li></ul><ul><li>You want a daemonless, more secure alternative to&nbsp;Docker.</li><li>You’re working in environments where root privileges are restricted.</li><li>You’re already using Kubernetes and want tighter integration.</li></ul><p>Both tools are excellent choices, so pick the one that best fits your&nbsp;needs.</p><h3>Kubernetes Overview: Orchestration, Scaling, and Self-Healing</h3><p>While Docker is great for running containers, managing them at scale can get tricky. Enter  (or K8s for short), the de facto standard for container orchestration. Think of Kubernetes as the conductor of an orchestra — it ensures all your containers play in&nbsp;harmony.</p><p><strong>Key Features of Kubernetes</strong></p><ul><li>: Automates deployment, scaling, and management of containers.</li><li>: Automatically adjusts the number of running containers based on&nbsp;demand.</li><li>: Restarts failed containers and replaces unhealthy ones.</li><li>: Automatically assigns IP addresses and DNS names to containers.</li></ul><p>Kubernetes organizes containers into , which are the smallest deployable units. A pod can contain one or more containers that share resources like storage and networking. Here’s a simple Kubernetes deployment file:</p><pre>apiVersion: apps/v1kind: Deployment  name: my-python-app  replicas: 3    matchLabels:  template:      labels:    spec:      - name: my-python-app<p>        image: my-python-app:latest</p>        ports:</pre><p>his file tells Kubernetes to run three replicas of your app and expose it on port 8080. You can apply it&nbsp;using:</p><pre>🔥 Run this: kubectl apply -f deployment.yaml</pre><h3>How NATS Shines in a Kubernetes Environment</h3><p>Now, let’s talk about . As a lightweight, high-performance messaging system, NATS plays well with Kubernetes. Here’s how it stands&nbsp;out:</p><p><strong>Use Cases for NATS in Kubernetes</strong></p><ol><li>Service-to-Service Communication: NATS excels at enabling fast, reliable communication between microservices. Its lightweight design makes it perfect for Kubernetes’ dynamic environment.</li><li>Event-Driven Architectures: NATS’s pub/sub, request-reply or streams patterns make it ideal for event-driven systems, where services need to react to events in real&nbsp;time.</li><li>Scalability: NATS can handle millions of messages per second, making it a great fit for high-throughput applications running on Kubernetes.</li><li>Resilience: NATS’s built-in fault tolerance ensures that your messaging system remains reliable, even in the face of node failures.</li></ol><p><strong>Deploying NATS on Kubernetes</strong></p><p>Deploying NATS on Kubernetes is straightforward. Here’s a basic NATS deployment file:</p><pre>apiVersion: apps/v1kind: Deployment  name: nats  replicas: 1    matchLabels:  template:      labels:    spec:      - name: nats        ports:<p>        - containerPort: 4222 # The messaging highway 🛣️</p></pre><p>Once deployed, NATS can be used by your microservices for seamless communication.</p><h3>Best Practices for Containerizing Microservices</h3><p>To wrap things up, here are some best practices for containerizing your microservices:</p><ol><li><strong>Keep Containers Lightweight</strong>: Use minimal base images (e.g.,  or  versions) to reduce size and improve performance.</li><li>: Separate the build and runtime environments to keep production images&nbsp;small.</li><li><strong>Leverage Kubernetes Features</strong>: Use ConfigMaps and Secrets to manage configuration and sensitive data.</li><li>: Integrate tools like Prometheus and Fluentd for monitoring and&nbsp;logging.</li><li>: Use CI/CD pipelines to automate building, testing, and deploying containers.</li></ol><p>In the next article, we’ll explore “<strong>NATS as a Service Mesh — The Lightweight Superhero Your Microservices Deserve</strong>” and how it simplifies communication between microservices. Spoiler alert: it’s like giving your microservices a supercharged walkie-talkie. Stay&nbsp;tuned!</p><p>Until then, feel free to drop a comment or share your thoughts. What’s your experience with Docker and Kubernetes? Any tips or tricks you’d like to share? Let’s keep the conversation going.</p><p> 💬 <em>What’s your #1 Kubernetes struggle? Scaling? Debugging? Share&nbsp;below!</em></p><p> ❤️ </p><p>Happy containerizing, and stay tuned for the next chapter in the !</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=af724f41c17d\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Coding Assistants are Not the Solution You Think","url":"https://devops.com/ai-coding-assistants-are-not-the-solution-you-think/","date":1739449895,"author":"Anish Dhar","guid":240,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kubernetes History Inspector, with Kakeru Ishii","url":"http://sites.libsyn.com/419861/kubernetes-history-inspector-with-kakeru-ishii","date":1739445780,"author":"gdevs.podcast@gmail.com (gdevs.podcast@gmail.com)","guid":312,"unread":true,"content":"<p dir=\"ltr\">Kakeru is the initiator of the Kubernetes History Inspector or KHI. An open source tool that allows you to visualise Kubernetes Logs and troubleshoot issues. We discussed what the tool does, how it's built and what was the motivation behind Open sourcing it.</p><p dir=\"ltr\">Do you have something cool to share? Some questions? Let us know:</p> News of the week ","contentLength":341,"flags":null,"enclosureUrl":"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD247.mp3?dest-id=3486674","enclosureMime":"","commentsUrl":null},{"title":"Sandbox environments: Creating efficient and isolated testing realms","url":"https://www.youtube.com/watch?v=fh7-lQVmX-o","date":1739426433,"author":"CNCF [Cloud Native Computing Foundation]","guid":332,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fh7-lQVmX-o?version=3","enclosureMime":"","commentsUrl":null},{"title":"KitOps: AI Model Packaging Standards","url":"https://www.youtube.com/watch?v=1TD-e_wVe4Q","date":1739426400,"author":"CNCF [Cloud Native Computing Foundation]","guid":331,"unread":true,"content":"<article>Chat with us on Discord:  https://discord.gg/Tapeh8agYy\n\nCheck out our repos:\nKitOps      https://github.com/jozu-ai/kitops\nPyKitOps Python Library  https://github.com/jozu-ai/pykitops\nKitOps MLFlow Plugin   https://github.com/jozu-ai/mlflow-jozu-plugin</article>","contentLength":253,"flags":null,"enclosureUrl":"https://www.youtube.com/v/1TD-e_wVe4Q?version=3","enclosureMime":"","commentsUrl":null},{"title":"Training IT Teams for Multi-Cloud DevOps Environments","url":"https://devops.com/training-it-teams-for-multi-cloud-devops-environments/","date":1739362266,"author":"Anne Fernandez","guid":239,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StackGen’s New Migration Engine: A DevOps Game-Changer for Multi-Cloud Transitions","url":"https://devops.com/stackgens-new-migration-engine-a-devops-game-changer-for-multi-cloud-transitions/","date":1739269023,"author":"Tom Smith","guid":238,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gcore Radar report reveals 56% year-on-year increase in DDoS attacks","url":"https://devops.com/gcore-radar-report-reveals-56-year-on-year-increase-in-ddos-attacks/","date":1739257299,"author":"cybernewswire","guid":237,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harness Merges with Traceable to Provide Integrated DevSecOps Platform","url":"https://devops.com/harness-merges-with-traceable-to-provide-integrated-devsecops-platform/","date":1739216172,"author":"Mike Vizard","guid":236,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Weekly Roundup: AWS Step Functions, AWS CloudFormation, Amazon Q Developer, and more (February 10, 2024)","url":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-aws-step-functions-aws-cloudformation-amazon-q-developer-and-more-february-10-2024/","date":1739215669,"author":"Matheus Guimaraes","guid":537,"unread":true,"content":"<p>We are well settled into 2025 by now, but many people are still catching up with all the exciting new releases and announcements that came out of re:Invent last year. There have been hundreds of re:Invent recap events around the world since the beginning of the year, including in-person all-day official AWS events with multiple tracks to help you discover and dive deeper into the releases you care about, as well as community and virtual events.</p><p>Last month, I was lucky to be a co-host for AWS EMEA re:Invent re:Cap which was a nearly 4-hour livestream with experts featuring demos, whiteboard sessions, and a live Q&amp;A. The good news is that you can now <a href=\"https://aws.amazon.com/events/reinvent-recaps-emea/virtual-emea/\">watch it on-demand</a>! We had a great team and thousands of people enjoyed learning through the virtual experience. I recommend you check it out or share it with colleagues who have not been able to attend any re:Invent re:Cap events.</p><p>The Korean team also did an amazing job hosting their own virtual re:Invent re:Cap event, and it’s also now <a href=\"https://pages.awscloud.com/aws-reinvent-recap-korea-reg.html\">available on-demand</a>. So if you speak Korean I do recommend you check it out.</p><p>If you’re more of a reader, then we have a treat for you. You can download the full official re:Invent re:Cap deck with all the slides covering releases across all areas by visiting <a href=\"https://community.aws/recaps\">community.aws</a>! While there, you can also check all the upcoming in-person <a href=\"https://community.aws/recaps\">re:Invent re:Cap community events</a> remaining across the globe for a chance to still attend one of those in a city near you.</p><p>But as we know, new releases, announcements, and updates don’t stop at re:Invent. Every week there are even more, and this is why we have this Weekly Roundup series that you can read every Monday to get the AWS news highlights from the week before.</p><p>So here’s what caught my attention last week.</p><p>Here are some other releases that caught my attention this week from a variety of other AWS services:</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/02/reshape-aws-cloudformation-stack-refactoring/\">AWS CloudFormation introduces stack refactoring</a> – You can now split your CloudFormation stacks, move resources from one stack to another, and change the logical name of resources within the same stack. This adds a lot of flexibility enabling you to keep up with changes within your organization and architectures, such as streamlining resource lifecycle management for existing stacks, keeping up with naming convention changes, and other cases. You can refactor your stacks by using the <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html\">AWS command line interface (CLI)</a> or <a href=\"https://aws.amazon.com/developer/tools/\">AWS SDK</a>.</p><p><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/02/aws-config-4-new-resource-types/\">AWS Config now supports 4 new release types</a> – <a href=\"https://aws.amazon.com/config/getting-started/\">AWS Config</a> is great for monitoring resources across your AWS environment and help you towards ensuring alignment with your company and security policies as well as compliance requirements. It now has four new types of resources enabling you to monitor <a href=\"https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html\">Amazon VPC</a> block public access settings, any exceptions made within those settings, as well as monitor <a href=\"https://aws.amazon.com/s3/storage-classes/express-one-zone/\">S3 Express One Zone</a> bucket policies and directory bucket settings.</p><p><a href=\"https://aws.amazon.com/blogs/security/announcing-upcoming-changes-to-the-aws-security-token-service-global-endpoint/\">Upcoming changes to the AWS Security Token Service (AWS STS) global endpoint</a> – To help improve the resiliency and performance of your applications, we are making changes to the AWS STS global endpoint (https://sts.amazonaws.com), with no action required from customers. Starting in early 2025, requests to the STS global endpoint will be automatically served in the same Region as your AWS deployed workloads. For example, if your application calls  from the US West (Oregon) Region, your calls will be served locally in the US West (Oregon) Region instead of being served by the US East (N. Virginia) Region. These changes will be released in the coming weeks and we will gradually roll it out to AWS Regions that are enabled by default by mid-2025.</p><p>Looking for some reading recommendations? At the beginning of every year Dr. Werner Vogles, VP and CTO of Amazon, publishes <a href=\"https://www.allthingsdistributed.com/2025/02/thinking-like-a-fox-a-reading-list-for-the-future.html\">a list of recommended books</a> that he believes should have your attention. This year’s list is looking particularly good in my opinion!</p><p><a href=\"https://aws.amazon.com/events/public-sector-days/london/\">AWS Public Sector Day London</a>, February 27 — Join public sector leaders and innovators to explore how AWS is enabling digital transformation in government, education, and healthcare.</p><p><a href=\"https://aws.amazon.com/events/aws-innovate/\">AWS Innovate GenAI + Data Edition</a> — A free online conference focusing on generative AI and data innovations. Available in multiple Regions: APJC and EMEA (March 6), North America (March 13), Greater China Region (March 14), and Latin America (April 8).</p><p>That’s it for this week! See you next time :)</p><a href=\"https://www.linkedin.com/in/codingmatheus/\">Matheus Guimaraes | @codingmatheus</a>","contentLength":4383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CNL: Optimizing Kyverno policy enforcement performance for large clusters","url":"https://www.youtube.com/watch?v=DWmCAUCs3bc","date":1739211188,"author":"CNCF [Cloud Native Computing Foundation]","guid":330,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":317,"flags":null,"enclosureUrl":"https://www.youtube.com/v/DWmCAUCs3bc?version=3","enclosureMime":"","commentsUrl":null},{"title":"AWS Extends AI Agent Reach into the Realm of Testing Code","url":"https://devops.com/aws-extends-ai-agent-reach-into-the-realm-of-testing-code/","date":1739195146,"author":"Mike Vizard","guid":235,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-125/","date":1739179101,"author":"Mike Vizard","guid":234,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}