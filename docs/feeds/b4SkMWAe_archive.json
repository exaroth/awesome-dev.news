{"id":"b4SkMWAe","title":"DevOps","displayTitle":"DevOps","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":103,"items":[{"title":"Platform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platfor... Puja Abbassi","url":"https://www.youtube.com/watch?v=Xr0Eb-ybvck","date":1751401964,"author":"CNCF [Cloud Native Computing Foundation]","guid":179271,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPlatform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platforms - Puja Abbassi, Giant Swarm\n\nEveryone is talking about platform engineering. You see smooth demos of golden paths and self-service platforms. However, there’s a significant area of challenges that is less talked about and thus often neglected when designing developer platforms.\n\nIn this talk, we’ll explore the often-overlooked day 2 challenges that platform teams face. We’ll dissect the area of day 2 into the many sub-areas and challenges they pose. Drawing on real-world experiences, including notable migrations that many in this community have faced, we'll shed light on the pain behind developer platforms and discuss solutions to these issues. Among others, we’ll delve into practical strategies for managing versioning and rollouts, and highlight the significant hurdles encountered, such as dependencies on end user teams or GitOps.\n\nJoin us for insights, strategies, and stories from the trenches that will help you navigate the complexities of service iteration in developer platforms.</article>","contentLength":1476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Xr0Eb-ybvck?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=hERRANApN5c","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179127,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: OpenTelemetry And The Future of Open Source Observability - Austin Parker, Honeycomb</article>","contentLength":476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/hERRANApN5c?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Con... Gordon Radlein","url":"https://www.youtube.com/watch?v=dlDTX-aDNzg","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179128,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Why Semantic Conventions are OpenTelemetry’s Most Important Contribution - Gordon Radlein, Datadog\n\nOpenTelemetry has accelerated the commoditization of instrumentation. Telemetry generation is becoming a solved problem, an implementation detail. But this has created a new challenge: a wealth of standardized signals with no standard meaning. Different systems instrumented with different semantics generating telemetry in their own unique language. And while signal correlation connects specific workloads, it fails when we need to understand our systems at a macro scale by joining disparate datasets.\nThat is, until we all agreed to speak the same language.\n\nJust as English as a lingua franca fueled progress across the internet, OpenTelemetry Semantic Conventions are providing a shared language for our systems. In this talk we’ll discuss why semantic interoperability is the real connective tissue, how it’s fueling deeper insights into our production environments, and the key role it plays in enabling the AI systems that are rapidly ushering in the next revolution of our industry.</article>","contentLength":1500,"flags":null,"enclosureUrl":"https://www.youtube.com/v/dlDTX-aDNzg?version=3","enclosureMime":"","commentsUrl":null},{"title":"Welcome + Opening Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=_rqgWHaEvgc","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179129,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nWelcome + Opening Remarks - Austin Parker, Honeycomb</article>","contentLength":435,"flags":null,"enclosureUrl":"https://www.youtube.com/v/_rqgWHaEvgc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere","url":"https://www.youtube.com/watch?v=Z4umnlRdLtA","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179130,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Manage Logging Costs While Preserving Value - Alok Bhide, Chronosphere\n\nLogs can get very expensive and often how useful all those logs are is unknown, some are but many are not. It is very difficult to know which logs are useful and how exactly they are used. With Chronosphere's Control plane for logs users can now get a comprehensive analysis of value and usage patterns, along with sophisticated recommendations and control actions that allow some or most of the value derived from those logs to be preserved. In order to achieve our goals we have enhanced Fluent Bit to be more flexible in which logs are actioned upon and will share useful future additions to it.</article>","contentLength":1072,"flags":null,"enclosureUrl":"https://www.youtube.com/v/Z4umnlRdLtA?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson","url":"https://www.youtube.com/watch?v=J_hHiwa_3QU","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179131,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Hybrid Cloud Architecture: Making Big Bets on Open Standards - Margaret Dawson, Chronosphere\n\nHybrid cloud isn’t a stepping stone—it’s a destination. With 39% of CNCF survey respondents already operating in hybrid environments, this model is here to stay. But as teams pursue cloud-native architectures, many skip a critical step: developing a clear cloud strategy and an observability approach to match.\nThe result is predictable— widening visibility gaps, redundant tooling and data, and spiraling costs as teams try to stitch together disconnected, vendor-specific systems never meant to work in concert. Hybrid environments expose these issues quickly, especially when workloads span multiple platforms without a unified way to observe and understand them.\nModernization efforts demand open observability from the start—not as an add-on. Technologies like OpenTelemetry, Fluent Bit, and Prometheus act as connective tissue across clouds, clusters, and on-prem infrastructure, enabling standardization where it’s needed most.\nThis talk outlines how to center open observability in your modernization journey: where to standardize architectural layers, how to maintain a more open approach, and why these decisions have long-term payoff. \nHybrid complexity is inevitable. Leading with open observability is how you stay in control—now and in the future.</article>","contentLength":1761,"flags":null,"enclosureUrl":"https://www.youtube.com/v/J_hHiwa_3QU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Sponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data I... Dotan Horovits","url":"https://www.youtube.com/watch?v=C5Y3qnEJSY8","date":1751391324,"author":"CNCF [Cloud Native Computing Foundation]","guid":179132,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSponsored Keynote: Foundation-Led Innovation: OpenSearch's Impact on Modern Data Insights - Dotan Horovits, AWS OpenSearch</article>","contentLength":505,"flags":null,"enclosureUrl":"https://www.youtube.com/v/C5Y3qnEJSY8?version=3","enclosureMime":"","commentsUrl":null},{"title":"Building Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Per... Denton Krietz","url":"https://www.youtube.com/watch?v=zgnY8szpKUw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179118,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nBuilding Resilient Telemetry Pipelines: Mastering the OpenTelemetry Collector's Persistent Queue - Denton Krietz, Bindplane\n\nThe OpenTelemetry Collector’s persistent queue provides a robust mechanism for handling data bursts, destination outages, and processing delays, ensuring no telemetry data is lost—but from experience, it’s consistently one of the collector's least understood features.\n\nIn this talk, we’ll explore the inner workings of the OTel Collector’s persistent queue, including how it buffers data, ensures durability, and enables replay after failures. Attendees will learn how to configure persistent queues for their unique workloads, optimize their telemetry pipeline performance, and troubleshoot common pitfalls.\n\nWhether you’re a site reliability engineer, developer, or observability enthusiast, this talk will equip you with the knowledge to deeply understand persistent queues to optimize your telemetry pipeline in production.</article>","contentLength":1348,"flags":null,"enclosureUrl":"https://www.youtube.com/v/zgnY8szpKUw?version=3","enclosureMime":"","commentsUrl":null},{"title":"Introducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner & Ray Jenkins, Streamfold","url":"https://www.youtube.com/watch?v=xeQnP8Ct7qY","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179119,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nIntroducing a Lightweight Rust OpenTelemetry Collector - Mike Heffner &amp; Ray Jenkins, Streamfold\n\nIn this talk, we'll introduce Rotel—an open-source OpenTelemetry collector built in Rust. Rotel is lightweight and resource-efficient, integrating seamlessly into your development workflow. Its compact design lets you package it with your Python or NodeJS projects, so telemetry collection runs alongside your code without needing additional sidecars.\n\nWe'll explore how rethinking telemetry collection at the edge can empower developers right from the early stages of development, paving the way for broader OpenTelemetry adoption. You’ll learn how Rust’s low-overhead FFI enables native extensions for telemetry filtering, transformation, and enrichment using Python and Typescript.\n\nBy leveraging Rust’s performance strengths, Rotel avoids the overhead of garbage collection, resulting in lower memory usage and reduced latency. Its quick cold start times make it a natural fit for modern cloud-native, serverless, and edge computing environments. Join us to discover how moving telemetry collection closer to the source can help you analyze high-volume, high-fidelity signals more effectively.</article>","contentLength":1585,"flags":null,"enclosureUrl":"https://www.youtube.com/v/xeQnP8Ct7qY?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTele... Diana Todea","url":"https://www.youtube.com/watch?v=wWON2NT41lE","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179120,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: From Zero To Developer: My One Year Serendipity Journey With OpenTelemetry - Diana Todea, Aircall\n\nBecoming a contributor to an open-source project is a transformative step in any developer's career. This session explores the journey from first-time contributor to active developer, covering best practices for navigating project communities, understanding codebases, and making meaningful contributions. Learn strategies for selecting the right project, mastering collaboration tools, and embracing the culture of open-source development. The audience will be inspired about my one year journey with the open source project OpenTelemetry and how I have built a proof of concept for it and achieved developer status for this project. By the end of this talk, the public will gain insights into the tools to become a better developer and how to build more engagement with the community.</article>","contentLength":1284,"flags":null,"enclosureUrl":"https://www.youtube.com/v/wWON2NT41lE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Telemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchma... Henrik Rexed","url":"https://www.youtube.com/watch?v=tZho5W9L_Z8","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179121,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nTelemetry Showdown: Fluent Bit Vs. OpenTelemetry Collector - A Comprehensive Benchmark Analysis - Henrik Rexed, Dynatrace\n\nIn a push to standardize observability practices, the cloud-native community has embraced OpenTelemetry, offering a unified framework for metrics, logs, and traces. Prior to this, log processing relied on agents like fluent, evolving into fluentbit. With fluentbit's recent expansion to support additional signals and the OpenTelemetry Collector's emergence, a pertinent question arises: Which is the superior choice for performance?\n\nThis session delves into:\n- Unveiling the distinctions between Fluent Bit and the OpenTelemetry Collector.\n- Sharing the findings derived from a series of benchmark tests.\n- Providing valuable insights to empower the community in selecting the most fitting agent for their cloud-native environments.</article>","contentLength":1240,"flags":null,"enclosureUrl":"https://www.youtube.com/v/tZho5W9L_Z8?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Spec-tacular Game Show - Liudmila Molkova, Ted Young, Tyler Helmuth, Jamie Danielson, Alex Boten","url":"https://www.youtube.com/watch?v=ipFVu0dl5Bw","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179122,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPanel: The Spec-tacular Game Show - Liudmila Molkova, Microsoft; Ted Young, Grafana Labs; Tyler Helmuth, Jamie Danielson &amp; Alex Boten, Honeycomb\n\nFrom OTLP to OTTL, engineers are excited about a lot of things. But there is one thing that excites them above all else and that is correcting people. Welcome to “The Spec-tacular Game Show”.\n\nIn this fun game show our panelists will be given incorrect statements about the OpenTelemetry Specification or Semantic Convention. The panelists will buzz in, identify what’s wrong, and state the correction. If none of the panelists know the answer the audience will get a chance to answer to steal the point. The panelist (or audience) with the most points wins!\n\nAfter each question we’ll spend a time explaining why the Spec and Semconv is the way it is and highlight how it produces the production-quality telemetry you know and love. Join us for a fun, relaxing, (snarky) panel about everyone’s favorite part of Otel!</article>","contentLength":1356,"flags":null,"enclosureUrl":"https://www.youtube.com/v/ipFVu0dl5Bw?version=3","enclosureMime":"","commentsUrl":null},{"title":"How To Think About Instrumentation Overhead - Jason Plumb, Splunk","url":"https://www.youtube.com/watch?v=fvmzAX_ZyvM","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179123,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nHow To Think About Instrumentation Overhead - Jason Plumb, Splunk\n\nNovice observability practitioners are often overly obsessed with performance. They might approach instrumentation with skepticism and have concerns about latency degradation or resource consumption. This talk is a primer on the topic of instrumentation overhead, and it will teach you how to think about overhead in an observability context. We will cover the causes of overhead and why overhead is so hard to measure and even harder to predict reliably. Lastly, we will present some practical techniques for understanding overhead in your environment and some strategies for coping with it.</article>","contentLength":1042,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fvmzAX_ZyvM?version=3","enclosureMime":"","commentsUrl":null},{"title":"No Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft","url":"https://www.youtube.com/watch?v=fU6jsw0yaVU","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179124,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNo Dependencies. No Plugins. Just Native OpenTelemetry - Liudmila Molkova, Microsoft\n\nThe best telemetry starts at the source—inside the client libraries.\nBut in most cases, that means taking a dependency on the OpenTelemetry API from your library. And while it’s stable, minimal, reliable, and safely no-op unless configured—transitive dependencies are still the bane of any library developer’s existence, and most of us try to avoid them.\n\nTo work around this, people reach for abstractions, plugins, bridges, or even OTel forks that break context propagation. The result? A poor user experience. Users must find the right plugin, install it, wire it up—and still hit the diamond dependency problem, now it just affects a subset of users.\n\nBut what if you could take a truly optional dependency? If OpenTelemetry is on the classpath, instrumentation kicks in. If it’s not, no harm done.\nHow hard is that to pull off? How reliable? How performant?\n\nLet’s explore that—through the lens of the next generation of Azure SDKs for Java. Spoiler: it’s easy and fast, and as a side-bonus, we can fall back to logs-based tracing if OTel is not found.</article>","contentLength":1544,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fU6jsw0yaVU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Closing Remarks - Austin Parker, Honeycomb","url":"https://www.youtube.com/watch?v=eDbQfZ9eoNI","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179125,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nClosing Remarks - Austin Parker, Honeycomb</article>","contentLength":425,"flags":null,"enclosureUrl":"https://www.youtube.com/v/eDbQfZ9eoNI?version=3","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace","url":"https://www.youtube.com/watch?v=di5nhYvUh6w","date":1751391275,"author":"CNCF [Cloud Native Computing Foundation]","guid":179126,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: Beyond Good Enough: Why We Want a Kotlin API and SDK - Hanson Ho, Embrace\n\nThe OTel Java API, SDK, and ecosystem are perfectly adequate for Android developer to get OTel instrumentation into their apps. But for a host of reasons, the match is not perfect, especially for developers who only write in Kotlin, which is the recommended development language for Android by Google, not the least of which is the emergence of Kotlin Multiple Platform (KMP) as a means to share code between Android, iOS, and many other platforms.\n\nThis session will outline the reasons why we at Embrace is trying to kick-start the development of a pure Kotlin ecosystem for OTel, starting with an API and SDK implementation, and how we are doing it in a way where mobile developers can get value incrementally without having to wait until every aspect is fully built out.\n\nWe want OTel to feel natural and idiomatic for Android developers, and this is the first step towards that end.</article>","contentLength":1361,"flags":null,"enclosureUrl":"https://www.youtube.com/v/di5nhYvUh6w?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Docker MCP Catalog: the Secure Way to Discover and Run MCP Servers","url":"https://www.docker.com/blog/docker-mcp-catalog-secure-way-to-discover-and-run-mcp-servers/","date":1751375060,"author":"Nuno Coracao","guid":178895,"unread":true,"content":"<p>The Model Context Protocol (MCP) ecosystem is exploding. In just weeks, our Docker MCP Catalog has surpassed , validating that developers are hungry for a <a href=\"https://www.docker.com/products/mcp-catalog-and-toolkit/\">secure way to run MCP servers</a>. Today, we’re excited to share major updates to the Docker MCP Catalog, including enhanced discovery features and our new open submission process. With hundreds of developers already requesting to publish their MCP servers through Docker, we’re accelerating our mission to make <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">containerized MCP servers</a> the standard for secure AI tool distribution.</p><p>The rapid adoption of MCP servers also highlights a critical problem — the current practice of running them via npx or uvx commands exposes systems to unverified code with full host access, not to mention dependency management friction. In this post, we’ll explain why Docker is investing in the MCP ecosystem, showcase the new catalog capabilities, and share how you can contribute to building a more secure foundation for AI applications.</p><p><strong>Figure 1: The new Docker MCP Catalog, built for easier discovery.</strong></p><h2>Why Docker is building the MCP Catalog</h2><h3>The security issues in MCP distribution</h3><p>Every time a developer runs npx -y @untrusted/mcp-server or uvx some-mcp-tool, they’re making a dangerous trade-off: convenience over security. These commands execute arbitrary code directly on the host system with full access to:</p><ul><li>Environment variables and secrets</li></ul><p>Some MCP clients limit environment variable access, but even that is not a universal practice. This isn’t sustainable. As MCP moves from experimentation to production, we need a fundamentally different approach.</p><p>Docker has spent over a decade solving exactly these problems for cloud-native applications. We’ve built the infrastructure, tools, and trust that developers rely on to run billions of containers in production. Now, we’re applying these same principles to the MCP ecosystem.</p><p>When you run an MCP server from our Catalog, you get:</p><ul><li> verifying the image hasn’t been tampered with</li><li><strong>Software Bill of Materials (SBOMs)</strong> documenting every component</li><li> from your host system</li><li> to only what the server actually needs</li></ul><p>This isn’t about making life harder for developers—it’s about making security the path of least resistance.</p><h2>Introducing the enhanced MCP Catalog</h2><p>We’ve reimagined the MCP Catalog to make it more accessible and easier to navigate. You can still access the MCP Catalog from Docker Hub and the MCP Toolkit in Docker Desktop just like before, or <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">go straight to the MCP catalog</a>. We’ve gone beyond generic container image listings by building features that help you quickly find the right MCP servers for your AI applications.&nbsp;&nbsp;</p><p>: MCP servers are organized by what they actually do:</p><ul><li>Data Integration (databases, APIs, file systems)</li><li>Development Tools (IDEs, code analysis, testing)</li><li>Communication (email, Slack, messaging platforms)</li><li>Productivity (task management, calendars, note-taking)</li><li>Analytics (data processing, visualization, reporting)</li></ul><p>: Find servers by capability, tools, GitHub tags, and categories — not just by name.</p><p>: Every catalog entry clearly shows whether it’s Docker-built (with transparent build signing and verification) or community-built (containerized and maintained by the publisher).</p><p><strong>Figure 2: Discover MCP servers by use cases.</strong></p><h3>How we classify MCP Servers: Built by Docker vs. community-built</h3><p>: When you see “Built by Docker,” you’re getting our complete security treatment. We control the entire build pipeline, providing cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning.</p><p>: These servers are packaged as Docker images by their developers. While we don’t control their build process, they still benefit from container isolation, which is a massive security improvement over direct execution.</p><p><strong>Tiers serve important roles</strong>: Docker-built servers demonstrate the gold standard for security, while community-built servers ensure we can scale rapidly to meet developer demand. Developers can change their mind after submitting a community-built server and opt to resubmit it as a Docker-built server.</p><p><strong>Figure 3: An example of Built by Docker MCP Server.</strong></p><h2>Open for MCP server submission: Join the secure MCP movement</h2><p>Starting today, we’re opening our submission process to the community. Whether you’re an individual developer or an enterprise team, you can feature your MCP servers on the Docker MCP Catalog. By publishing through our catalog, you’re not just distributing your MCP server — you’re helping establish a new security standard for the entire ecosystem while getting your MCP tools available to millions of developers already using Docker via Docker Hub and Docker Desktop. Your containerized server becomes part of the solution, demonstrating that production-ready AI tools don’t require compromising on security.&nbsp;</p><h3>How to submit your MCP server</h3><ol><li> – Package your MCP server as a Docker image</li><li> – Opt for Docker-built (we handle the build) or community-built (you build and maintain it)</li></ol><p>We’re committed to a fast, transparent review process. Quality MCP servers that follow our security guidelines will be published quickly, helping you reach Docker’s 20+ million developer community.</p><p>ClickHouse is one of the first companies to take advantage of Docker’s MCP Catalog, and they opted for the Docker-built tier to ensure maximum security. Here’s why they chose to partner with Docker:</p><p><a href=\"https://clickhouse.com/\" rel=\"nofollow noopener\" target=\"_blank\"></a><em>, we deliver the fastest analytics database – open-source, and designed for real-time data processing and analytics at scale. As agentic AI becomes more embedded in modern applications, developers are using the ClickHouse MCP server to support intelligent, data-driven workflows that demand low latency, high concurrency, and cost efficiency.</em><em>To make it easier for developers to deploy these workloads, we’re featuring </em><a href=\"https://hub.docker.com/mcp/server/clickhouse/overview\" rel=\"nofollow noopener\" target=\"_blank\"></a><em> on Docker’s MCP Catalog, which provides </em><strong><em>a powerful way to reach 20M+ developers</em></strong><em> and makes it easier for Docker users to discover and use our solution. </em><strong><em>We opted for “Built by Docker” with the highest security standard</em></strong><em>, including cryptographic signatures, SBOMs, provenance attestations, and continuous vulnerability scanning. Together with Docker, developers can run ClickHouse MCP Server with confidence, knowing it’s secured, verified, and ready for their agentic applications.” – </em>Tanya Bragin, VP of Product and Marketing Clickhouse</p><p>We’re preparing for the future of cloud-native AI applications. Remote MCP servers will enable:</p><ul><li>Managed MCP services that scale automatically</li><li>Shared capabilities across teams without distributing code</li><li>Stricter security boundaries for sensitive operations</li></ul><h3>Integration with the official MCP registry</h3><p>We’re actively collaborating with the MCP community on the upcoming official registry. Our vision is complementary:</p><ul><li>The official registry provides centralized discovery – the “yellow pages” of available MCP servers</li><li>Docker provides the secure runtime and distribution for those listings</li><li>Together, we create a complete ecosystem where discovery and security work hand-in-hand</li></ul><p>The explosive growth of our MCP Catalog, 1 million pulls and hundreds of publisher requests, tells us developers are ready for change. They want the power of MCP, but they need it delivered securely.</p><p>By establishing containers as the standard for MCP server distribution, we’re not trying to own the ecosystem — we’re trying to secure it. Every MCP server that moves from npx execution to containerized deployment is a win for the entire community.</p><ul><li><strong>Explore the enhanced MCP Catalog</strong>: <a href=\"https://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\">Visit the MCP Catalog</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to discover MCP servers that solve your specific needs securely.</li><li><strong>Use and test hundreds of MCP Servers</strong>: <a href=\"https://www.docker.com/products/docker-desktop/\">Download Docker Desktop</a><a href=\"http://hub.docker.com/mcp\" rel=\"nofollow noopener\" target=\"_blank\"></a>to download and use any MCP server in our catalog with your favorite clients: Gordon, Claude, Cursor, VSCode, etc</li><li>: Star our repository and watch for updates on the MCP Gateway release and remote server capabilities.</li></ul><p>Together, we’re building more than a catalog — we’re establishing the secure foundation that the MCP ecosystem needs to grow from experimental tool to production-ready platform. Because when it comes to AI applications, security isn’t optional. It’s fundamental.</p>","contentLength":8137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Build the highest resilience apps with multi-Region strong consistency in Amazon DynamoDB global tables","url":"https://aws.amazon.com/blogs/aws/build-the-highest-resilience-apps-with-multi-region-strong-consistency-in-amazon-dynamodb-global-tables/","date":1751315448,"author":"Donnie Prakoso","guid":176903,"unread":true,"content":"<p>While tens of thousands of customers are successfully using <a href=\"https://aws.amazon.com/dynamodb/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon DynamoDB</a><a href=\"https://aws.amazon.com/dynamodb/global-tables/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">global tables</a> with eventual consistency, we’re seeing emerging needs for even stronger resilience. Many organizations find that the DynamoDB multi-Availability Zone architecture and eventually consistent global tables meet their requirements, but critical applications like payment processing systems and financial services demand more.</p><p>For these applications, customers require a zero Recovery Point Objective (RPO) during rare Region-wide events, meaning you can direct your app to read the latest data from any Region. Your multi-Region applications always need to access the same data regardless of location.</p><p>Starting today, you can use a new Amazon DynamoDB global tables capability that provides multi-Region strong consistency (MRSC), enabling zero RPO. This capability, first announced as a <a href=\"https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-dynamodb-global-tables-previews-multi-region-strong-consistency/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">preview at AWS re:Invent 2024</a>, simplifies building highly resilient global applications.</p><p>Here’s how you can enable MRSC starting with an existing empty DynamoDB table:</p><p>With MRSC, DynamoDB now offers you the highest level of resilience for your applications. If application processing is interrupted in a Region, you can redirect traffic to another Region containing a MRSC replica and know that you’ll be processing the latest data.</p><p>Let me walk you through how you can use this new capability.</p><p>To start using MRSC, I need to create a global table from an existing DynamoDB table that doesn’t contain any data. I navigate to my existing table, select the  tab and select .</p><p>The MRSC availability architecture requires three AWS Regions. I can configure MRSC with either three full replicas or with two replicas and a witness. A witness contains only replicated change data to provide the required availability without maintaining a full copy of my table data.</p><p>The following screenshot shows how I configure MRSC with two replicas and a witness. To configure MRSC with three full replicas instead, I can clear <strong>Configure Region 2 as Witness</strong>.</p><p>If I need to update my existing table programmatically, I can use the <a href=\"https://aws.amazon.com/developer/learning/q-developer-cli/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon Q CLI</a> to generate the required command with the following prompt:</p><p><code>&gt; Hey Q! Update my existing DynamoDB table called \"demo-mrsc\" in us-east-1 with multi-Region strong consistency across us-east-2 with witness in us-west-2 Regions</code></p><p>Shortly after, Q CLI will respond back with following command:</p><pre><code>&gt; To update your DynamoDB table with multi-Region strong consistency, you'll need to use the update-table command with the appropriate parameters. Here's how you would do it:\n\naws dynamodb update-table \\ \n    --table-name demo-mrsc \\ \n    --replica-updates '[{\"Create\": {\"RegionName\": \"us-east-2\"}}]' \\ \n    --global-table-witness-updates '[{\"Create\": {\"RegionName\": \"us-west-2\"}}]' \\ \n    --multi-region-consistency STRONG \\ \n    --region us-east-1\n</code></pre><p>After it’s finished processing, I can check the status of my MRSC global table. I can see I have a  configured for my DynamoDB global table. A witness reduces costs while still providing the resilience benefits of multi-Region strong consistency.</p><p>Then, in my application, I can use  to read data with strong consistency. Here’s a Python example:</p><pre><code>import boto3\n\n# Configure the DynamoDB client for your region\ndynamodb = boto3.resource('dynamodb', region_name='us-east-2')\ntable = dynamodb.Table('demo-mrsc')\n\npk_id = \"demo#test123\"\n\n# Read with strong consistency across regions\nresponse = table.get_item(\n    Key={\n        'PK': pk_id\n    },\n    ConsistentRead=True\n)\n\nprint(response)\n</code></pre><p>For operations that require the strongest resilience, I can use . For less critical operations where eventual consistency is acceptable, I can omit this parameter to improve performance and reduce costs.</p><p>Here are a couple of things to note:</p><ul><li> – The Amazon DynamoDB multi-Region strong consistency capability is available in following AWS Regions: US East (Ohio, N. Virginia), US West (Oregon), Asia Pacific (Osaka, Seoul, Tokyo), and Europe (Frankfurt, Ireland, London, Paris)</li></ul><p>Learn more about how you can achieve the highest level of application resilience, enable your applications to be always available and always read the latest data regardless of the Region by visiting <a href=\"https://aws.amazon.com/dynamodb/global-tables/?trk=c4ea046f-18ad-4d23-a1ac-cdd1267f942c&amp;sc_channel=el\">Amazon DynamoDB global tables</a>.</p>","contentLength":4218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Survey: Pace of Increased Adoption of GitOps Varies Widely","url":"https://devops.com/survey-pace-of-increased-adoption-of-gitops-varies-widely/?utm_source=rss&utm_medium=rss&utm_campaign=survey-pace-of-increased-adoption-of-gitops-varies-widely","date":1751313103,"author":"Mike Vizard","guid":176880,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kusari Adds AI Security Tool to Inspect Code as Pull Requests Are Made","url":"https://devops.com/kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made/?utm_source=rss&utm_medium=rss&utm_campaign=kusari-adds-ai-security-tool-to-inspect-code-as-pull-requests-are-made","date":1751310779,"author":"Mike Vizard","guid":176879,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond a RHEL Clone: How Rocky Linux Is Evolving Into Something More","url":"https://devops.com/beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more/?utm_source=rss&utm_medium=rss&utm_campaign=beyond-a-rhel-clone-how-rocky-linux-is-evolving-into-something-more","date":1751308370,"author":"Nathan Blackham","guid":176852,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Amazon EC2 C8gn instances powered by AWS Graviton4 offering up to 600Gbps network bandwidth","url":"https://aws.amazon.com/blogs/aws/new-amazon-ec2-c8gn-instances-powered-by-aws-graviton4-offering-up-to-600gbps-network-bandwidth/","date":1751306492,"author":"Channy Yun (윤석찬)","guid":176811,"unread":true,"content":"<p>You can use C8gn instances to run the most demanding network intensive workloads, such as security and network virtual appliances (virtual ﬁrewalls, routers, load balancers, proxy servers, DDoS appliances), data analytics, and tightly-coupled cluster computing jobs.</p><p><strong><u>EC2 C8gn instances specifications</u></strong> C8gn instances provide up to 192 vCPUs and 384 GiB memory, and offer up to 30 percent higher compute performance compared Graviton3-based <a href=\"https://aws.amazon.com/blogs/aws/new-amazon-ec2-c7gn-instances-graviton3e-processors-and-up-to-200-gbps-network-bandwidth/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">EC2 C7gn instances</a>.</p><p>Here are the specs for C8gn instances:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>If you’re using C7gn instances now, you will have straightforward experience migrating network intensive workloads to C8gn instances because the new instances offer similar vCPU and memory ratios. To learn more, check out the collection of <a href=\"https://aws.amazon.com/ec2/graviton/resources/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Graviton resources</a> to help you start migrating your applications to Graviton instance types.</p>","contentLength":831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Weekly Roundup: Project Rainier, Amazon CloudWatch investigations, AWS MCP servers, and more (June 30, 2025)","url":"https://aws.amazon.com/blogs/aws/aws-weekly-roundup-project-rainier-amazon-cloudwatch-investigations-aws-mcp-servers-and-more-june-30-2025/","date":1751301557,"author":"Channy Yun (윤석찬)","guid":176779,"unread":true,"content":"<p>Every time I visit Seattle, the first thing that greets me at the airport is <a href=\"https://en.wikipedia.org/wiki/Mount_Rainier\">Mount Rainier</a>. Did you know that the most innovative project at <a href=\"https://aws.amazon.com/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Web Services (AWS)</a> is named after this mountain?</p><p>Project Rainier is a new project to create what is expected to be the world’s most powerful computer for training AI models across multiple data centers in the United Stages. Anthropic will develop the advanced versions of its <a href=\"https://aws.amazon.com/bedrock/anthropic/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Claude models</a> with five times more computing power than its current largest training cluster.</p><p>The key technology powering Project Rainier is <a href=\"https://aws.amazon.com/ai/machine-learning/trainium/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS custom-designed Trainium2 chips</a>, which are specialized for the immense data processing required to train complex AI models. Thousands of these Trainium2 chips will be connected in a new type of <a href=\"https://aws.amazon.com/ec2/ultraservers/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon EC2 UltraServer</a> and <a href=\"https://aws.amazon.com/ec2/ultraclusters/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">EC2 UltraCluster</a> architecture that allows ultra-fast communication and data sharing across the massive system.</p><p>Learn about the <a href=\"https://www.aboutamazon.com/news/aws/aws-project-rainier-ai-trainium-chips-compute-cluster\">AWS vertical integration of Project Rainer</a>, where it designs every component of the technology stack from chips to software, allows it to optimize the entire system for maximum efficiency and reliability.</p><p> Here are some launches that got my attention:</p><ul><li><a href=\"https://aws.amazon.com/blogs/aws/amazon-fsx-for-openzfs-now-supports-amazon-s3-access-without-any-data-movement/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon S3 access for Amazon FSx for OpenZFS</a> – You can access and analyze your FSx for OpenZFS file data through Amazon S3 Access Points, enabling seamless integration with AWS AI/ML, and analytics services without moving your data out of the file system. You can treat your FSx for OpenZFS data as if it were stored in S3, making it accessible through the S3 API for various applications including Amazon Bedrock, Amazon SageMaker, AWS Glue, and other S3 based cloud-native applications.</li><li><a href=\"https://aws.amazon.com/blogs/aws/new-improve-apache-iceberg-query-performance-in-amazon-s3-with-sort-and-z-order-compaction/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon S3 with sort and z-order compaction for Apache Iceberg tables</a> – You can optimize query performance and reduce costs with new sort and z-order compaction. With S3 Tables, sort compaction automatically organizes data files based on defined column orders, while z-order compaction can be enabled through the maintenance API for efficient multicolumn queries.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/ga-accelerate-troubleshooting-amazon-cloudwatch-investigations/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon CloudWatch investigations</a> – You can accelerate your operational troubleshooting in AWS environments using the Amazon CloudWatch AI-powered investigation feature, which helps identify anomalies, surface related signals, and suggest remediation steps. This capability can be initiated through CloudWatch data widgets, multiple AWS consoles, CloudWatch alarm actions, or Amazon Q chat and enables team collaboration and integration with Slack and Microsoft Teams.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-bedrock-guardrails-tiers-content-filters-denied-topics/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Bedrock Guardrails Standard tier</a> – You can enhance your AI content safety measures using the new Standard tier. It offers improved content filtering and topic denial capabilities across up to 60 languages, better detection of variations including typos, and stronger protection against prompt attacks. This feature lets you configure safeguards to block harmful content, prevent model hallucinations, redact personally identifiable information (PII), and verify factual claims through automated reasoning checks.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-route-53-resolver-endpoints-dns-delegation-private-hosted-zones/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Route 53 Resolver endpoints for private hosted zone</a> – You can simplify DNS management across AWS and on-premises infrastructure using the new Route 53 DNS delegation feature for private hosted zone subdomains, which works with both inbound and outbound Resolver endpoints. You can delegate subdomain authority between your on-premises infrastructure and Route 53 Resolver cloud service using name server records, eliminating the need for complex conditional forwarding rules.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/amazon-q-developer-java-upgrade-transformation-cli-generally-available/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">Amazon Q Developer CLI for Java transformation</a> – You can automate and scale Java application upgrades using the new Amazon Q Developer Java transformation command line interface (CLI). This feature perform upgrades from Java versions 8, 11, 17, or 21 to versions 17 or 21 directly from the command line. This tool offers selective transformation options so you can choose specific steps from transformation plans and customize library upgrades.</li><li><a href=\"https://aws.amazon.com/about-aws/whats-new/2025/06/managed-integrations-aws-iot-device-management/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">New AWS IoT Device Management managed integrations</a> – You can simplify Internet of Things (IoT) device management across multiple manufacturers and protocols using the new managed integrations feature, which provides a unified interface for controlling devices whether they connect directly, through hubs or third-party clouds. The feature includes pre-built cloud-to-cloud (C2C) connectors, device data model templates, and SDKs that support ZigBee, Z-Wave, and Wi-Fi protocols, while you can still create custom connectors and data models.</li></ul><p> Check your calendars and sign up for these upcoming AWS events:</p><ul><li><a href=\"https://reinvent.awsevents.com/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS re:Invent</a> – Register now to get a head start on choosing your best learning path, booking travel and accommodations, and bringing your team to learn, connect, and have fun. If you’re an early-career professional, you can apply to the <a href=\"https://reinvent.awsevents.com/all-builders-welcome/\">All Builders Welcome Grant program</a>, which is designed to remove financial barriers and create diverse pathways into cloud technology.</li><li><a href=\"https://aws.amazon.com/events/builders-online-series/?trk=769a1a2b-8c19-4976-9c45-b6b1226c7d20&amp;sc_channel=el\">AWS Builders Online Series</a> – If you’re based in one of the Asia Pacific time zones, join and learn fundamental AWS concepts, architectural best practices, and hands-on demonstrations to help you build, migrate, and deploy your workloads on AWS.</li></ul><p>That’s all for this week. Check back next Monday for another Weekly Roundup!</p>","contentLength":5246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast Code, Real Risks: Guardrails for AI-Generated Software","url":"https://devops.com/fast-code-real-risks-guardrails-for-ai-generated-software/?utm_source=rss&utm_medium=rss&utm_campaign=fast-code-real-risks-guardrails-for-ai-generated-software","date":1751298670,"author":"Mike Vizard","guid":176743,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Next Version of Grok Includes Advanced Coding Assistance: Reports","url":"https://devops.com/next-version-of-grok-includes-advanced-coding-assistance-reports/?utm_source=rss&utm_medium=rss&utm_campaign=next-version-of-grok-includes-advanced-coding-assistance-reports","date":1751298224,"author":"Jon Swartz","guid":176742,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tool Calling with Local LLMs: A Practical Evaluation","url":"https://www.docker.com/blog/local-llm-tool-calling-a-practical-evaluation/","date":1751291322,"author":"Ignasi Lopez Luna","guid":176646,"unread":true,"content":"<h2>Which local model should I use for tool calling?</h2><p>When building GenAI and agentic applications, one of the most pressing and persistent questions is: <em>“Which local model should I use for tool calling?”</em>&nbsp; We kept hearing again and again, from colleagues within Docker and the developer community, ever since we started working on <a href=\"https://www.docker.com/blog/introducing-docker-model-runner/\"></a>, a local inference engine that helps developers run and experiment with local models.&nbsp;</p><p>It’s a deceptively simple question with a surprisingly nuanced answer. Even when we tried to answer it for a very specific case: <em>“What if I just expose 5 simple tools to the model?”</em>We realized we had no definite answer forthat. <a href=\"https://www.docker.com/blog/run-llms-locally/\">Local LLM models</a> offer control, cost-efficiency, and privacy, but when it comes to structured tool use, deciding when and how to act, they can behave very differently. We decided to dig deep and test this properly. We started with manual experimentation, then built a framework to scale our testing. This blog documents that journey and shares which models ranked highest on our tool-calling leaderboard.</p><h2>The first attempt: Manual testing</h2><p>Our first instinct was to build something quickly and try it out manually.</p><p>So we created<a href=\"https://github.com/ilopezluna/chat2cart\" rel=\"nofollow noopener\" target=\"_blank\"></a>, an AI-powered shopping assistant that lets users interact via chat to build, modify, and check out a shopping cart. Through a natural conversation, users can discover products, add or remove items, and complete or cancel their purchase, all from the chat interface.</p><p>To support testing across different LLMs, we added a model selector that makes it easy to switch between local models (via Docker Model Runner or Ollama) and hosted models using the OpenAI API.</p><p>OpenAI’s GPT-4 or GPT-3.5 worked as expected, and the experience was fairly smooth.&nbsp;</p><ul><li>Called tools when they were needed</li><li>Avoided unnecessary tool usage</li><li>Handled tool responses naturally</li></ul><p>But the local models? That’s where the challenges started to surface.</p><h2>What went wrong with local models</h2><p>We started experimenting with some of the local models listed on the<a href=\"https://huggingface.co/spaces/gorilla-llm/berkeley-function-calling-leaderboard\" rel=\"nofollow noopener\" target=\"_blank\"> Berkeley Function-Calling Leaderboard</a>. Our goal was to find smaller models, ideally with fewer than 10 billion parameters, so we tested xLAM-2-8b-fc-r and watt-tool-8B. We quickly ran into several recurring issues:</p><ul><li>: Tools were being called even for greeting messages like “Hi there!”</li><li>: The model would search when it should have added, or tried to remove when the cart was empty</li><li>: Parameters like product_name or quantity were missing or malformed</li><li>: The model often failed to respond to tool output, leading to awkward or incomplete conversations</li></ul><p>At this point, it was clear that manual testing wouldn’t scale. Different models failed in different ways, some struggled with invocation logic, while others mishandled tool arguments or responses.&nbsp; Testing was not only slow, but also unreliable. Because these models are non-deterministic, we had to run each scenario multiple times just to get a reliable read on behavior.</p><p>We needed a testing setup that was repeatable, measurable, and fast.</p><h2>Our second attempt: A scalable testing tool</h2><p>Our goal wasn’t academic rigor.It was: <em>“Give us good-enough answers in 2–3 days, not weeks.”</em></p><p>In a couple of days, we created<a href=\"https://github.com/docker/model-test\" rel=\"nofollow noopener\" target=\"_blank\"></a>, This is a flexible project with the following capabilities</p><ul><li>Define real-world  with multiple valid tool call sequences</li><li>Run them against many models (local &amp; hosted)</li><li>Track , , and </li><li>Log  for analysis (or eventual fine-tuning)</li></ul><p>The core idea behind model-test is simple: simulate realistic tool-using conversations, give the model room to reason and act, and check whether its behavior makes sense.</p><ul><li>A  (e.g. “Add iPhone to cart”)</li><li>The  (optional)</li><li>One or more , because there’s often more than one right answer</li></ul><div><pre>{\n&nbsp;&nbsp;\"prompt\": \"Add iPhone to cart\",\n&nbsp;&nbsp;\"expected_tools_variants\": [\n&nbsp;&nbsp;&nbsp;&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\": \"direct_add\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"tools\": [{ \"name\": \"add_to_cart\", \"arguments\": { \"product_name\": \"iPhone\" } }]\n&nbsp;&nbsp;&nbsp;&nbsp;},\n&nbsp;&nbsp;&nbsp;&nbsp;{\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\": \"search_then_add\",\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"tools\": [\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{ \"name\": \"search_products\", \"arguments\": { \"query\": \"iPhone\" } },\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{ \"name\": \"add_to_cart\", \"arguments\": { \"product_name\": \"iPhone 15\" } }\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]\n&nbsp;&nbsp;&nbsp;&nbsp;}\n&nbsp;&nbsp;]\n}\n</pre></div><p>In this case, we consider both  and <strong>“search first, then add the result”</strong> as acceptable. Even though “iPhone” isn’t a real product name, we’re fine with it. We weren’t aiming for overly strict precision, just realistic behavior.</p><p>Each test case belongs to a test suite. We provide two built-in suites. However, you can run an entire suite, individual test cases, or a selection of multiple test cases. Additionally, you can create your own custom suites to group tests as needed.&nbsp;</p><ul><li>: Greetings, single-step actions</li><li>: Multi-step reasoning and tool chaining</li></ul><p>To make tests feel closer to how real agents behave, we simulate an agent loop up to .</p><p>User: </p><ol><li>Model: <em>“Let me search for iPhone 5…”</em><ol><li>Tool: </li></ol></li><li>Model: <em>“Adding product X to cart…”</em></li><li>Model:  → Great, test passed!</li></ol><p>But if the model still wants to keep going after round 5?</p><p>That’s it, my friend,&nbsp; . Time’s up.</p><p>We deliberately avoided designing tests that require perfect predictions.</p><ul><li>We didn’t demand that the model always know the exact product name.</li><li>What mattered was: <strong>did the tool sequence make sense</strong> for the intent?</li></ul><p>This helped us focus on the kind of reasoning and behavior we actually want in agents, not just perfect token matches.</p><p>Our test outputs distilled down to a final F1 score, encapsulating three core dimensions:</p><div><table><tbody><tr><td><p>Did the model realize a tool was needed?</p></td></tr><tr><td><p>Did it choose the right tool(s) and use them correctly?</p></td></tr><tr><td><p>Whether the tool call arguments were correct?</p></td></tr></tbody></table></div><p>The F1 score is the harmonic mean of two things: precision (how often the model made valid tool calls) and recall (how often it made the tool calls it was supposed to).</p><p>We also tracked latency, the average runtime in seconds, but that wasn’t part of the F1 calculation; it simply helped us evaluate speed and user experience.</p><h2>21 models and 3,570 tests later: Which models nailed tool calling?</h2><p>We tested 21 models across  using 210 batch runs.</p><h3>Overall Rankings (by Tool Selection F1):</h3><div><table><tbody><tr></tr><tr><td><p>claude-3-5-sonnet-20241022</p></td></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>Among all models, OpenAI’s GPT-4 came out on top with a tool selection F1 score of 0.974, completing responses in just under 5 seconds on average. While hosted and not the focus of our local model exploration, it served as a reliable benchmark and provided some ground truths.</p><p>On the local side, Qwen 3 (14B) delivered outstanding results, nearly matching GPT-4 with a 0.971 F1 score, though with significantly higher latency (~142 seconds per interaction).</p><p>If you’re looking for something faster, Qwen 3 (8B) also achieved an F1 score of 0.933, while cutting latency nearly in half (~84 seconds), making it a compelling balance between speed and tool-use accuracy.</p><p>Hosted models like Claude 3 Haiku also performed very well, hitting 0.933 F1 with exceptional speed (3.56 seconds average latency), further illustrating the high bar set by cloud-based offerings.</p><p>Not all models handled tool calling well. The quantized Watt 8B model struggled with parameter accuracy and ended up with a tool selection F1 score of just 0.484. Similarly, the LLaMA-based XLam 8B variant often missed the correct tool path altogether, finishing with an F1 score of 0.570. These models may be suitable for other tasks, but for our structured tool use test, they underdeliver.</p><p>We also experimented with both  and  variants for some models, and in all cases observed <strong>no significant difference</strong> in tool-calling behavior or performance. This suggests that quantization is beneficial for reducing resource usage without negatively impacting accuracy or reasoning quality, at least for the models and scenarios we tested.</p><p>If your goal is maximum tool-calling accuracy, then Qwen 3 (14B) or Qwen 3 (8B) are your best bets, both local, both precise, with the 8B variant being notably faster.</p><p>For a good trade-off between speed and performance, Qwen 2.5 stood out as a solid option. It’s fast enough to support real-time experiences, while still maintaining decent tool selection accuracy.</p><p>If you need something more lightweight, especially for resource-constrained environments, the <a href=\"https://groq.com/introducing-llama-3-groq-tool-use-models/\" rel=\"nofollow noopener\" target=\"_blank\">LLaMA 3 Groq 7B</a> variant offers modest performance at a much lower compute footprint.</p><h2>What we learned and why this matters</h2><p>Our testing confirmed that the Qwen family of models leads the pack among open-source options for tool calling. But as always, there’s a trade-off; you’ll need to balance between accuracy and latency when designing your application</p><ul><li>: Even the 8B version of Qwen3 outperformed any other local model</li><li>: Higher-accuracy models take longer, often significantly.</li></ul><p>Tool calling is core to almost every real-world GenAI application. Whether you’re building agents or creating agentic workflows, your LLM must know when to act and how. Thanks to this simple framework, “We don’t know which model to pick” became “We’ve narrowed it down to three great options, each with clear pros and cons.”</p>","contentLength":8955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Still Running Vulnerable Log4j Instances?","url":"https://devops.com/still-running-vulnerable-log4j-instances/?utm_source=rss&utm_medium=rss&utm_campaign=still-running-vulnerable-log4j-instances","date":1751281674,"author":"Ofer Regev","guid":176565,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serverless CI/CD: Redefining Continuous Delivery in the Modern DevOps Era","url":"https://devops.com/serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era/?utm_source=rss&utm_medium=rss&utm_campaign=serverless-ci-cd-redefining-continuous-delivery-in-the-modern-devops-era","date":1751280883,"author":"Harikrishna Kundariya","guid":176527,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How DevOps Services Improve Software Delivery and Quality","url":"https://devops.com/how-devops-services-improve-software-delivery-and-quality/?utm_source=rss&utm_medium=rss&utm_campaign=how-devops-services-improve-software-delivery-and-quality","date":1751277346,"author":"Vinay Pasilkar","guid":176468,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revolutionizing CI/CD: A Framework for Integrating Generative AI Across the Software Delivery Lifecycle","url":"https://devops.com/revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle/?utm_source=rss&utm_medium=rss&utm_campaign=revolutionizing-ci-cd-a-framework-for-integrating-generative-ai-across-the-software-delivery-lifecycle","date":1751275960,"author":"Anirban Biswas","guid":176467,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Software Migrations Fail: It’s Not the Code","url":"https://devops.com/why-software-migrations-fail-its-not-the-code/?utm_source=rss&utm_medium=rss&utm_campaign=why-software-migrations-fail-its-not-the-code","date":1751273613,"author":"Nishil Macwan","guid":176426,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harnessing AI and Automation for the Future of Innovation in DevOps","url":"https://devops.com/harnessing-ai-and-automation-for-the-future-of-innovation-in-devops/?utm_source=rss&utm_medium=rss&utm_campaign=harnessing-ai-and-automation-for-the-future-of-innovation-in-devops","date":1751268502,"author":"Tony Barbagallo","guid":176391,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Self-Driving Help Desk: Agentic AI’s Role in the Next DevOps Era","url":"https://devops.com/the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era/?utm_source=rss&utm_medium=rss&utm_campaign=the-self-driving-help-desk-agentic-ais-role-in-the-next-devops-era","date":1751267594,"author":"Venkat Thiruvengadam","guid":176390,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Agile Methodologies and DevOps Practices Shape Custom Software Product Development","url":"https://devops.com/how-agile-methodologies-and-devops-practices-shape-custom-software-product-development/?utm_source=rss&utm_medium=rss&utm_campaign=how-agile-methodologies-and-devops-practices-shape-custom-software-product-development","date":1751265805,"author":"Roman Davydov","guid":176357,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google Adds Gemini CLI to AI Coding Portfolio","url":"https://devops.com/gemini-cli-the-open-source-ai-agent-thats-revolutionizing-terminal-workflows/?utm_source=rss&utm_medium=rss&utm_campaign=gemini-cli-the-open-source-ai-agent-thats-revolutionizing-terminal-workflows","date":1751264732,"author":"Tom Smith","guid":176356,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Five Great DevOps Job Opportunities","url":"https://devops.com/five-great-devops-job-opportunities-145/?utm_source=rss&utm_medium=rss&utm_campaign=five-great-devops-job-opportunities-145","date":1751263232,"author":"Mike Vizard","guid":176319,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Level Up your bash skill : Mastering Sed command","url":"https://blog.devops.dev/level-up-your-bash-skill-mastering-sed-command-3b6f28ea0e60?source=rss----33f8b2d9a328---4","date":1751133811,"author":"bektiaw","guid":174650,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Install Prometheus & Grafana in AKS Using Helm","url":"https://blog.devops.dev/how-to-install-prometheus-grafana-in-aks-using-helm-d9c46c9d5170?source=rss----33f8b2d9a328---4","date":1751133668,"author":"Nirmal Chandrasiri","guid":174619,"unread":true,"content":"<p>In this article, I’ll walk you through how to install  and  on <strong>Azure Kubernetes Service (AKS)</strong> using  — in a beginner-friendly and straightforward way. This is perfect and easy way to setup prometheus in AKS or EKS using Helm… So let’s dive in&nbsp;:)</p><p>Before anything, make sure you have an  up and&nbsp;running.</p><p>In my case, I created my AKS cluster using . You can find my Terraform code in this repo: Of course you can change the values as you&nbsp;like.</p><p>❗Please export your Azure subscription ID&nbsp;first:</p><p><strong>For Bash (Linux/macOS/WSL):</strong></p><p>export TF_VAR_subscription_id=&lt;your_subscription_id&gt;</p><p><strong>For PowerShell (Windows):</strong></p><p>$env:TF_VAR_subscription_id=&lt;your_subscription_id&gt;</p><p>In order to setup AKS cluster using Terraform use the below commands:</p><p>terraform plan -var-file terraform.tfvars</p><p>terraform apply -var-file terraform.tfvars</p><p>After you’ve created the AKS cluster, connect to it&nbsp;using:</p><pre>az aks get-credentials --resource-group central-monitoring-dev-rg --name central-monitoring-dev-aks</pre><p>Replace central-monitoring-dev-rg and central-monitoring-dev-aks with your own  and (Optional).</p><h3>Add the Prometheus Helm&nbsp;Chart</h3><p>We’ll use the official <strong>Prometheus Community Kubernetes Helm Chart</strong>. Run the command below to add&nbsp;it:</p><pre>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</pre><p>Then update your Helm&nbsp;repos:</p><h3>Install Prometheus and&nbsp;Grafana</h3><p>Now let’s install everything:</p><pre>helm install prometheus-stack prometheus-community/kube-prometheus-stack --namespace=prometheus-stack --create-namespace</pre><p>This will install Prometheus, Grafana, and related components into the prometheus-stack namespace.</p><h3>🌐 Expose Grafana via LoadBalancer</h3><p>By default, Grafana is exposed as a , So let’s change it to a  type so Azure will create an external IP for&nbsp;us:</p><ul><li>First, check the services:</li></ul><pre>kubectl get svc -n prometheus-stack</pre><ul><li>Then edit the Grafana&nbsp;service:</li></ul><pre>kubectl edit svc prometheus-stack-grafana -n prometheus-stack</pre><ul><li>In the editor that opens,&nbsp;change:</li></ul><p>Once the LoadBalancer is ready, grab the  from:</p><pre>kubectl get svc prometheus-stack-grafana -n prometheus-stack</pre><p>Then open it in your browser. copy the external ip address into a new tab and login page will appear to&nbsp;you.</p><ul></ul><blockquote>how do I know the password?<em> You can check the default values using&nbsp;this:</em></blockquote><pre>helm show values prometheus-community/kube-prometheus-stack &gt; prometheus-default-values.yaml</pre><p>Look for adminPassword under Grafana settings — it’s usually set to prom-operator.</p><p>Now you can explore dashboards, set up alerts, and start <strong>monitoring your AKS cluster like a&nbsp;pro</strong>.</p><p>If you navigate to your AKS portal and check the  section, you’ll see the external IP created for Grafana. Just click and log&nbsp;in.</p><p>That’s all! Hope this helps you get started with  on . If you found this helpful, feel free to reach drop a comment if you have any&nbsp;issues.</p><p>See you in the next one!&nbsp;✌️</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=d9c46c9d5170\" width=\"1\" height=\"1\" alt=\"\">","contentLength":2802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mastering Kubernetes Operators: Automating Beyond YAML","url":"https://blog.devops.dev/mastering-kubernetes-operators-automating-beyond-yaml-09a47b5ae4b5?source=rss----33f8b2d9a328---4","date":1751133575,"author":"Siddharth Singh","guid":174618,"unread":true,"content":"<div><p>In modern Kubernetes-based environments, we often find ourselves writing tons of YAML files — Deployments, Services, ConfigMaps, and more…</p></div>","contentLength":146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Set Up Prometheus, InfluxDB, and Grafana for Full-Stack Monitoring with Docker Compose","url":"https://blog.devops.dev/monitor-docker-containers-system-metrics-using-grafana-prometheus-7cbc015e2044?source=rss----33f8b2d9a328---4","date":1751133568,"author":"David Lam","guid":174617,"unread":true,"content":"<div><p>Monitoring your infrastructure is critical to keeping systems healthy, optimizing performance, and planning for growth. In this guide…</p></div>","contentLength":136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ansible for DevOps: The Complete Guide to Automation That Actually Works","url":"https://blog.devops.dev/ansible-for-devops-the-complete-guide-to-automation-that-actually-works-ae0c70d6fb56?source=rss----33f8b2d9a328---4","date":1751133563,"author":"Dhruv Patel","guid":174616,"unread":true,"content":"<div><p>Tired of repetitive deployment tasks eating up your sprint time? Here’s how Ansible transforms DevOps workflows with real examples you can…</p></div>","contentLength":143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden “Verbosity Tax” in AI: Why Per-Token Pricing Isn’t What It Seems","url":"https://blog.devops.dev/the-hidden-verbosity-tax-in-ai-why-per-token-pricing-isnt-what-it-seems-a3d446499102?source=rss----33f8b2d9a328---4","date":1751133557,"author":"Simardeep Singh","guid":174615,"unread":true,"content":"<p>As a senior DevOps engineer who’s spent years optimizing cloud costs and capacity planning, I thought I understood pricing models. Then I came across post that made me question everything I knew about AI service economics.</p><p>The post was from Kevin Jiang (for link see references), a developer building at otherhalf.ai, who shared a frustrating discovery: despite switching to Google’s Gemini 2.5 Pro for its lower per-token costs, his team’s AI bills had mysteriously spiked. The culprit? Outputs that were becoming dramatically longer for identical prompts.</p><p>This revelation triggered my DevOps instincts. In infrastructure management, unexpected cost increases usually signal either a misconfiguration or a fundamental shift in service behavior. Kevin’s observation suggested the latter, and I decided to investigate.</p><p>Kevin’s team noticed that Gemini 2.5 Pro, despite advertising lower per-token rates than competitors like Claude Sonnet, was generating responses that were consistently 2x longer than necessary:</p><ul><li>Five lines of comments before writing a single line of&nbsp;code</li><li>50% more whitespace and formatting padding</li><li>Unsolicited debug logs and error&nbsp;handling</li><li>Verbose explanations for simple&nbsp;requests</li></ul><p>From a cost perspective, this creates a hidden multiplier effect:</p><pre>Advertised: Gemini ($1.25/1M tokens) vs Claude ($3.00/1M tokens) = 58% savingsReality: Gemini (2x verbosity) = $2.50 effective rate = 17% more expensive</pre><p>My first step was checking if this was an isolated incident or part of a broader pattern. Recent academic research suggests the&nbsp;latter.</p><p>The paper’s key insight resonates with any infrastructure engineer who’s dealt with cloud pricing optimization:</p><blockquote><em>“Unlike conventional billing, where the quantity and quality of services are verifiable, today’s LLM platforms operate under structural opacity: users are charged based on reported token and API usage, but have no means to confirm that these metrics reflect real or necessary work.”</em></blockquote><h3>The Smoking Gun in Vendor Documentation</h3><p>Digging into Google’s documentation revealed the most compelling evidence. Their <a href=\"https://developers.googleblog.com/en/updated-gemini-models-reduced-15-pro-pricing-increased-rate-limits-and-more/\">September 2024 release notes</a> explicitly mention tuning models for different verbosity levels:</p><blockquote><em>“For use cases like summarization, question answering, and extraction, the default output length of the updated models is ~5–20% shorter than previous models. For chat-based products where users might prefer longer responses by default, you can read our prompting strategies guide to learn more about how to make the models more verbose and conversational.”</em></blockquote><p>Even more telling, Google’s latest <a href=\"https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025/\">Gemini 2.5 updates</a> tout efficiency improvements, claiming the new Flash model uses “20–30% fewer tokens in our evaluations.” This explicitly acknowledges that previous versions were generating unnecessary tokens.</p><h3>The DevOps Perspective: Infrastructure Implications</h3><p>From an infrastructure management standpoint, this phenomenon breaks fundamental assumptions about cost predictability that we rely on for capacity planning.</p><h3>Traditional vs. LLM Pricing&nbsp;Models</h3><p><strong>Traditional cloud&nbsp;pricing:</strong></p><ul><li>Pay $X for Y resources (compute hours, storage GB, network bandwidth)</li><li>Resource consumption is directly measurable and predictable</li><li>Cost scales linearly with actual&nbsp;usage</li></ul><ul><li>Pay $X per token, but the provider controls token generation patterns</li><li>Token consumption can vary dramatically for identical inputs</li><li>Algorithmic changes on the provider side directly impact your&nbsp;costs</li></ul><p>This introduces a new category of cost drift that’s invisible until you analyze usage patterns over time. Unlike compute or storage costs that scale predictably with load, LLM costs can inflate through unilateral provider&nbsp;changes.</p><p>Traditional infrastructure monitoring focuses on resource utilization metrics: CPU, memory, network, and storage. LLM cost optimization requires tracking entirely different metrics:</p><ul><li><strong>Tokens per request trends</strong> over&nbsp;time</li><li><strong>Output length distribution</strong> for similar prompt&nbsp;types</li><li><strong>Provider efficiency ratios</strong> across different models</li><li> rather than cost per&nbsp;token</li></ul><p>This is analogous to the shift from monitoring raw server metrics to application performance metrics, but with an added layer of complexity: the “application” behavior is controlled by an external provider.</p><h3>Broader Patterns in Cloud Service Evolution</h3><p>This verbosity inflation mirrors other evolutionary patterns I’ve observed in cloud services&nbsp;pricing:</p><h3>The Burstable Instance Precedent</h3><p>Remember when AWS introduced “burstable” EC2 instances (t2, t3, t4g families)? They appeared cheaper than standard instances but could spike costs when applications exceeded baseline CPU credits. The advertised low hourly rate became meaningless without understanding usage patterns.</p><h3>The API Call Proliferation</h3><p>Storage providers like AWS S3 initially charged primarily for storage and bandwidth. Over time, they added charges for API calls, lifecycle transitions, and various request types. What started as simple per-GB pricing became a complex matrix of&nbsp;fees.</p><p>Cloud providers have progressively increased data egress charges while keeping ingress free. This creates vendor lock-in effects where the true cost of a service isn’t apparent until you try to&nbsp;leave.</p><p>LLM verbosity inflation follows the same playbook: advertise low headline rates, then optimize for revenue through usage pattern manipulation that’s difficult for customers to predict or&nbsp;control.</p><h3>Practical Implications for Infrastructure Teams</h3><p>For teams evaluating LLM integration or optimizing existing AI costs, this research suggests several critical considerations:</p><h3>1. Redefine Cost Comparison Metrics</h3><p>Per-token price comparisons are becoming as meaningless as comparing cloud instances purely on CPU count without considering memory, storage, or network performance.</p><p>Instead, benchmark on :</p><pre>Total monthly cost ÷ Successful task completions = True cost per outcome</pre><h3>2. Implement Comprehensive Monitoring</h3><p>Traditional cost monitoring isn’t sufficient. Infrastructure teams need to&nbsp;track:</p><ul><li>Average tokens per request by prompt&nbsp;type</li><li>Output length trends over&nbsp;time</li><li>Provider efficiency ratios</li><li>Token utilization patterns</li></ul><h3>3. Build Provider Diversification Strategy</h3><p>Just as we avoid single-region deployments for availability, avoiding single-provider dependencies becomes crucial for cost predictability. Maintaining the ability to switch providers limits exposure to arbitrary pricing&nbsp;changes.</p><h3>4. Prompt Engineering as Cost Engineering</h3><p>In traditional infrastructure, we optimize resource allocation for cost efficiency. With LLMs, prompt engineering becomes a cost optimization discipline:</p><ul><li>Explicit brevity instructions</li><li>Output format specifications</li></ul><h3>The Research That Validates the&nbsp;Concern</h3><p>The academic community is taking this seriously. Beyond the Max Planck study, <a href=\"https://epoch.ai/data-insights/llm-inference-price-trends\">Epoch AI’s analysis</a> of LLM pricing trends shows that while per-token costs are dropping rapidly, the variation in actual usage costs is increasing due to differences in model efficiency and output patterns.</p><p>Another study, <a href=\"https://arxiv.org/html/2505.18471v1\">“Invisible Tokens, Visible Bills”</a>, proposes auditing frameworks to detect quantity inflation, suggesting this is becoming a recognized industry problem requiring systematic solutions.</p><h3>What This Means Moving&nbsp;Forward</h3><p>The AI market is maturing from a technology race to a business model optimization phase. Providers are discovering that algorithmic control over token generation provides more pricing flexibility than simple rate adjustments.</p><p>For infrastructure teams, this&nbsp;means:</p><p><strong>Cost predictability requires deeper analysis.</strong> Traditional cost modeling based on advertised rates isn’t sufficient. Teams need to test real workloads and monitor actual consumption patterns.</p><p><strong>Vendor evaluation must include efficiency testing.</strong> Beyond accuracy benchmarks, teams need to measure tokens-per-useful-output across providers with representative workloads.</p><p><strong>Monitoring and alerting systems need updates.</strong> Cost anomaly detection should include token efficiency trends, not just absolute spending&nbsp;changes.</p><p><strong>Contract negotiations should address efficiency.</strong> SLAs might need to include token efficiency guarantees or caps on output length for specified prompt&nbsp;types.</p><h3>The Broader Industry&nbsp;Question</h3><p>This investigation raises a fundamental question about AI service pricing: Should providers have unlimited control over consumption patterns when users are charged per unit of consumption?</p><p>Traditional utility models have regulations around metering accuracy and consumption reporting. As AI services become infrastructure-critical, similar transparency requirements might be necessary.</p><h3>Conclusion: The New&nbsp;Normal</h3><p>My observation about Gemini’s verbosity inflation isn’t just an isolated pricing quirk — it’s an early indicator of how AI service economics will evolve. As these services become more central to application infrastructure, understanding and monitoring true efficiency becomes as critical as traditional performance metrics.</p><p>For DevOps engineers, this means adding a new dimension to cost optimization playbooks. The era of simple per-unit pricing in AI is ending, replaced by a more complex landscape where provider algorithms directly impact your&nbsp;costs.</p><p>The key is building monitoring and evaluation frameworks that look beyond advertised rates to measure actual value delivery. Because in the emerging AI infrastructure landscape, the cheapest per-token rate rarely translates to the lowest monthly&nbsp;bill.</p><p><em>This investigation was inspired by and builds upon Kevin Jiang’s original observation. The research methodology and infrastructure perspective represent my own analysis as a senior DevOps engineer.</em></p><h3>Kevin Jiang’s Original&nbsp;Post</h3><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a3d446499102\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stuck in Kubernetes Hell? Fix These 8 Common Scenarios!","url":"https://blog.devops.dev/stuck-in-kubernetes-hell-fix-these-8-common-scenarios-0a42376b0ac6?source=rss----33f8b2d9a328---4","date":1751133550,"author":"Devops Diaries","guid":174614,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Handling PostgreSQL Connection Pooling","url":"https://blog.devops.dev/handling-postgresql-connection-pooling-dee3849d0299?source=rss----33f8b2d9a328---4","date":1751133543,"author":"M Mahdi Ramadhan, M. Si","guid":174613,"unread":true,"content":"<blockquote>PostgreSQL: connection limit exceeded.</blockquote><p>PostgreSQL’s powerful features and reliability make it a preferred choice for modern applications. However, in serverless and microservice environments, managing PostgreSQL connections and resources is critical. This article explains:</p><ul><li>What PostgreSQL connection pooling&nbsp;is</li><li>How to configure it (with code examples)</li><li>How to calculate resource (RAM &amp; CPU)&nbsp;needs</li><li>Best practices and alternatives</li></ul><h3>🔍 What is PostgreSQL Connection Pooling?</h3><p>PostgreSQL uses a  model:</p><pre>max_connections = 100  # default</pre><p>In high-concurrency environments (e.g., AWS Lambda), hitting this limit leads&nbsp;to:</p><pre>FATAL: sorry, too many clients already</pre><h3>🚧 Problem Example: Without&nbsp;Pooling</h3><pre>import psycopg2<p>def handler(event, context):</p>    conn = psycopg2.connect(        user=\"user\",<p>        password=\"pass\", // dont use password like this, just sample</p>        host=\"mydb.host\",<p>        port=5432 // sometime for security purposes, dont use default port</p>    )    cur.execute(\"SELECT * FROM users LIMIT 1;\")    conn.close()</pre><blockquote><em>💥 High concurrent Lambda invocations will crash PostgreSQL.</em></blockquote><h3>✅ Solution: PgBouncer Connection Pooling</h3><p>PgBouncer acts as a middleware that reuses backend connections.</p><h3>PgBouncer pgbouncer.ini Configuration</h3><pre>[databases]mydb = host=127.0.0.1 port=5432 dbname=mydblisten_port = 6432<p>pool_mode = transaction  # ensures each client gets a backend connection only during a transaction;</p># other modes include 'session' (holds backend connection for entire client session) and <p># 'statement' (allocates backend per individual SQL statement, least supported but highly efficient)</p>default_pool_size = 20  # number of server-side connections kept in the pool per database<p>max_client_conn = 2000  # max number of client connections PgBouncer will accept</p>reserve_pool_size = 5  # number of additional connections to handle sudden spikes</pre><p>For high availability and better isolation, based on my experienced, you have two main architectural choices:</p><ol><li><strong>Per-microservice PgBouncer</strong>: Each microservice instance deploys its own PgBouncer, which connects to your PostgreSQL cluster (primary and replicas). This offers better fault isolation and connection control per&nbsp;service.</li><li>: Multiple clients or microservices share a centralized PgBouncer instance, which then manages pooled connections to the PostgreSQL primary and replicas. This approach reduces infrastructure overhead but can become a single point of&nbsp;failure.</li></ol><p>In both setups, ensure your PgBouncer is configured with failover awareness or integrated with a load balancer that understands your PostgreSQL master/replica roles to maintain high availability.</p><h3>Application Code (Updated)</h3><p>The application now connects to PgBouncer instead of directly to PostgreSQL. This allows the connection to be managed through a pool, improving scalability and efficiency, especially under high concurrency scenarios.</p><pre>conn = psycopg2.connect(    dbname=\"mydb\",    password=\"pass\",    port=6432</pre><h3>🚡 Understanding Pool Size vs. max_connections</h3><ul><li>max_connections = maximum backend processes PostgreSQL accepts.</li><li>pool_size = active backend connections PgBouncer keeps.</li></ul><pre>max_connections ≥ Σ (PgBouncer_instances × pool_size) + admin buffer</pre><p>based on my experienced, always prepare admin buffer connection for your analytics and monitoring purposes such as health&nbsp;check.</p><ul><li>3 services using PgBouncer</li></ul><pre>max_connections = (3 × 20) + 10 = 70</pre><p>You might wonder: why use PgBouncer instead of a message queue like RabbitMQ?</p><p>The key difference lies in <strong>what problem each one&nbsp;solves</strong>:</p><ul><li> solves the problem of <strong>too many database connections</strong> by reusing a smaller pool of persistent connections to PostgreSQL. It’s ideal when your application (especially serverless or microservices) has many short-lived or bursty&nbsp;queries.</li><li><strong>Message queues (e.g., RabbitMQ, Kafka)</strong> decouple the processing layer from the database entirely. They’re better suited for  like background processing, batch jobs, or high-throughput pipelines.</li></ul><ul><li>Use  when you’re doing direct, real-time database reads/writes and need to scale connections efficiently.</li><li>Use a  when you want to absorb spikes, delay execution, or build resilient processing pipelines that aren’t tightly coupled to the database.</li></ul><p>Based on my experienced, I used both: PgBouncer for interactive queries and RabbitMQ for background jobs.</p><h3>📊 Calculating RAM and CPU Requirements</h3><pre>Total_RAM ≈ shared_buffers + (max_connections × work_mem) + overhead</pre><h3>Example for 200 connections:</h3><pre>work_mem = 4MBshared_buffers = 2GB<p>work_mem_total = 200 × 4MB = 800MB</p>Total_RAM = 2GB + 800MB + ~600MB overhead ≈ 3.4GB</pre><p>✅ Recommended: 4GB RAM&nbsp;minimum</p><h3>CPU Formula (Little’s Law-based):</h3><pre>Required vCPU = max_connections / concurrency_factor</pre><p>Workload Factor OLTP 10 Mixed 6 OLAP&nbsp;4</p><pre>max_connections = 200 (OLTP) ⇒ vCPU ≈ 20</pre><ol><li><strong>Handling ID Locking via PgBouncer</strong>: Since PgBouncer in transaction mode does not support prepared statements or session-level advisory locks, avoid using SELECT MAX(id) + 1 (<strong>this causes race conditions</strong>). Instead:</li></ol><ul><li>Or use SELECT nextval('your_seq') to fetch the next&nbsp;ID</li></ul><pre>INSERT INTO orders (id, customer_id) VALUES (nextval('orders_id_seq'), 123);</pre><p><strong>2. Set Timeout in PostgreSQL for Long-Running Queries</strong>: Use the statement_timeout parameter to prevent long query execution.</p><pre>SET statement_timeout = '5s';  -- timeout after 5 seconds</pre><pre>statement_timeout = 5000  # in milliseconds</pre><p><strong>3. Handling Query Timeouts and Root Cause Analysis</strong>: If queries frequently timeout:</p><ul><li>Check indexes using EXPLAIN&nbsp;ANALYZE</li><li>Monitor for locking/blocking:</li></ul><pre>SELECT * FROM pg_stat_activity WHERE wait_event IS NOT NULL;</pre><p>D. Long-running transactions</p><p>These tools help optimize queries and configurations for pooling environments.</p><p>PostgreSQL connection management is a critical skill in building scalable and resilient cloud-native applications. Based on my experience, many issues in production environments stem from misconfigured connection pools, either from overcommitting memory or underestimating traffic bursts. By correctly calculating pool_size, tuning max_connections, and understanding the implications of RAM and CPU allocations, you can avoid common pitfalls such as backend process exhaustion and connection timeouts.</p><p>As one system architect once said: <em>“Connection pooling is not an optimization — it’s survival.”</em> Especially in environments like serverless and microservices, where concurrency patterns are unpredictable, intelligent pooling and resource planning isn’t optional — it’s foundational.</p><blockquote>Let PostgreSQL scale  use the right tools, the right math, and the right architecture for the right&nbsp;job.</blockquote><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=dee3849d0299\" width=\"1\" height=\"1\" alt=\"\">","contentLength":6506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Testing AWS EventBridge for Cross-Region Event Replication","url":"https://blog.devops.dev/testing-aws-eventbridge-for-cross-region-event-replication-36f5c71b89cf?source=rss----33f8b2d9a328---4","date":1751133538,"author":"Carlos Biagolini-Jr.","guid":174612,"unread":true,"content":"<p>One of the great advantages of building on AWS is the ability to design systems that operate across geographically distant regions. Whether for disaster recovery, latency optimization, or compliance, multi-region architectures are a powerful feature of the AWS ecosystem. In this proof-of-concept (PoC), we explore how AWS EventBridge — a native event bus service — can be used to deliver event data from one region to another, effectively enabling decoupled, cross-region communication between serverless components.</p><p>The following diagram illustrates the high-level architecture we will implement:</p><ol><li><strong>Client Request Initiation:</strong>External users initiate the flow by sending an HTTP POST request to an Amazon API Gateway endpoint exposed in the N. Virginia region (us-east-1).</li><li><strong>Source Lambda Invocation and Event Creation:</strong>API Gateway triggers the source-lambda function. This function generates a unique identifier (UUID), captures the current UTC timestamp, and merges this metadata with the user's original message payload. The enriched event is then published to a custom EventBridge event bus named poc-eventbridge-cross-region-source-bus within the same region (us-east-1). The Lambda returns the generated ID immediately to the caller to confirm successful receipt.</li><li><strong>Source EventBridge Routing Rule:</strong>A rule configured on the poc-eventbridge-cross-region-source-bus evaluates incoming events. When an event matches the defined pattern (e.g., source: poc.eventbridge.crossregion, detail-type: message-event), the rule routes the event to a target EventBridge bus located in the destination region (us-west-2). This cross-region routing is enabled via the PutEvents API and the ARN of the destination event&nbsp;bus.</li><li><strong>Destination EventBridge Bus and Rule:</strong>In the Oregon region (us-west-2), the custom event bus poc-eventbridge-cross-region-destination-bus receives the forwarded event. A rule on this bus matches the same pattern and is configured to trigger the destination-lambda function whenever such an event is received.</li><li><strong>Destination Lambda Execution:</strong>The destination-lambda function appends a second UTC timestamp to indicate the time of receipt. It then stores the full payload—including the original metadata and both timestamps—into a DynamoDB table named destination_table.</li><li><strong>End-to-End Flow Complete:</strong>At this point, the event has successfully traveled from an external HTTP request in us-east-1, through EventBridge, across regions, and been persisted in a DynamoDB table in us-west-2, completing the cross-region asynchronous processing workflow.</li></ol><p>To follow along, you will need access to an AWS account with sufficient permissions to create and manage the following services:</p><ul></ul><p>This tutorial assumes a basic familiarity with the AWS&nbsp;Console.</p><h3>🔵 Part 1. Create resources in destination region</h3><p>At this first part of the tutorial, we will create resources for the POC in the destination region (Oregon — us-west-2)</p><h4>Step 1: Create a DynamoDB Table in the Destination Region</h4><p>Next, we need a DynamoDB table in the destination region to store the processed messages.</p><p>1.1. In the us-west-2 region, open the <a href=\"https://console.aws.amazon.com/dynamodb/home\">DynamoDB Console</a>.1.2. Click .1.3. Provide the following settings:</p><ul><li>: destination_table</li><li>: id (type:&nbsp;String)</li></ul><p>1.4. Leave all other settings at their default values to enable on-demand (serverless) capacity.1.5. Click .</p><p>This table will receive and store the full payload, including both the source and destination timestamps.</p><h4>Step 2: Create Source and Destination Lambda Functions</h4><p>To start, we need to set up two AWS Lambda functions in different regions: one for sending events (source) and one for receiving and processing them (destination).</p><p>2.1. Navigate to the <a href=\"https://console.aws.amazon.com/lambda/home\">AWS Lambda Console</a>.2.2. Click .2.3. Choose  and configure the function:</p><h4>Step 3: Create the EventBridge Event Bus in the Destination Region</h4><p>In this step, you will configure a custom EventBridge event bus in the destination region (us-west-2). This bus will serve as the receiving endpoint for events routed from the source region, enabling cross-region event processing.</p><p>3.1. Open the <a href=\"https://console.aws.amazon.com/events/home\">Amazon EventBridge Console</a> in the us-west-2 region.3.2. In the left navigation go to , click at .</p><p>3.3. Enter the following name: poc-eventbridge-cross-region-destination-bus</p><p>3.4. Define , as the following policy:</p><pre>{  \"Version\": \"2012-10-17\",    {<p>      \"Sid\": \"AllowAccountToPutEvents\",</p>      \"Effect\": \"Allow\",        \"AWS\": \"arn:aws:iam::123456789012:root\"      \"Action\": \"events:PutEvents\",<p>      \"Resource\": \"arn:aws:events:us-west-2:123456789012:event-bus/poc-eventbridge-cross-region-destination-bus\"</p>    }}</pre><blockquote><em>123456789012 with your actual AWS account ID and adjust the bus name if you are using a different naming convention.</em></blockquote><p>3.6. After pasting the policy, click  to finish creating the Event&nbsp;Bus.</p><p>Your destination Event Bus is now ready to receive events from the source region via cross-region routing.</p><h4>Step 4: Create an EventBridge Rule to Trigger the Destination Lambda</h4><p>In this step, we will configure an EventBridge rule that listens to custom events published on the source event bus and invokes the destination Lambda function directly when those events match a defined&nbsp;pattern.</p><p>4.1. In the  created in the previous step (poc-eventbridge-cross-region-destination-bus), go to the  tab and click .</p><p>4.2. Enter the following details:</p><ul><li>: poc-eventbridge-cross-region-destination-rule</li><li>: Confirm that poc-eventbridge-cross-region-destination-bus is&nbsp;selected</li><li>: Rule with an event&nbsp;pattern</li></ul><p>4.3. Under , configure the following:</p><ul><li>: Select . This is required for custom events sent using the PutEvents API.</li><li>: Choose <strong>Custom pattern (JSON&nbsp;editor)</strong>.</li></ul><pre>{  \"source\": [\"poc.eventbridge.crossregion\"],<p>  \"detail-type\": [\"message-event\"]</p>}</pre><p>This pattern matches only the custom events emitted by the source Lambda function.</p><p>4.4. Under , configure the following:</p><ul><li>: Select </li><li>: Select </li><li>: Choose </li><li>: Select the destination-lambda function deployed in the destination region (us-west-2)</li></ul><p>4.5. Under , allow EventBridge to create a new execution role with permissions to invoke the&nbsp;Lambda.</p><p>4.6. Leave other settings as default and click  to complete the&nbsp;setup.</p><p>This rule is now ready to listen for events on the source bus and invoke the destination Lambda in another region when the event pattern&nbsp;matches.</p><h3>🔴 Part 2. Create Resources in Source&nbsp;Region</h3><p>At this Second part of the tutorial, we will create resources for the POC in the source region (N. Virginia — us-east-1)</p><h4>Step 5: Create the EventBridge Event Bus in the Source&nbsp;Region</h4><p>In this step, you will create a custom EventBridge Event Bus in the . This bus will explicitly receive events from the source Lambda function.</p><p>5.1. Open the <a href=\"https://console.aws.amazon.com/events/home\">Amazon EventBridge Console</a> in the us-east-1 region.5.2. In the left navigation pane, click .5.3. Choose .5.4. Enter the following name: poc-eventbridge-cross-region-source-bus<p>5.5. Leave the resource-based policy section empty — it is not needed since the Lambda is publishing within the same account and region.</p>5.6. Click  to finish creating the source Event&nbsp;Bus.</p><p>This Event Bus will now be used by the source Lambda to explicitly publish&nbsp;events.</p><h4>Step 6: Create an EventBridge Rule to Route from Source to Destination Bus</h4><p>Now that the source Event Bus is ready, we will create a rule in us-east-1 to match custom events and forward them to the destination Event Bus in us-west-2.</p><p>6.1. In the  created in the previous step (poc-eventbridge-cross-region-source-bus), go to the  tab and click .6.2. Enter the following configuration:</p><ul><li>: poc-eventbridge-cross-region-source-rule</li><li>: poc-eventbridge-cross-region-source-bus</li><li>: Rule with an event&nbsp;pattern</li></ul><p>6.3. Under :</p><ul><li>: Select&nbsp;</li><li>: Custom pattern (JSON&nbsp;editor)</li></ul><pre>{  \"source\": [\"poc.eventbridge.crossregion\"],<p>  \"detail-type\": [\"message-event\"]</p>}</pre><p>6.4. Under , choose <strong>Event bus in another account and&nbsp;region</strong>.</p><ul><li>: arn:aws:events:us-west-2:123456789012:event-bus/poc-eventbridge-cross-region-destination-bus</li><li>Replace 123456789012 with your actual AWS account&nbsp;ID.</li></ul><p>6.5. Leave all other settings as default and click .</p><p>This rule now listens to events on the custom source Event Bus and forwards them across regions to the destination Event&nbsp;Bus.</p><h4>Step 7: Create Source Lambda&nbsp;Function</h4><p>7.1. Navigate to the <a href=\"https://console.aws.amazon.com/lambda/home\">AWS Lambda Console</a>.7.2. Click .7.3. Choose  and configure the function:</p><ul><li>: source-lambda</li><li>: Python 3.x (or your preferred runtime)</li></ul><h4>Step 8: Create an API Gateway to Trigger the Source&nbsp;Lambda</h4><p>To expose the source Lambda via HTTP, we will use Amazon API&nbsp;Gateway.</p><p>8.1. In the us-east-1 region, navigate to the <a href=\"https://console.aws.amazon.com/apigateway\">API Gateway Console</a>.8.2. Choose  and click .8.3. Under , click at .8.4. Select POST as method type, choose  and select the source-lambda function.8.5. Use  button to deploy your API, define a desired stage.8.6. Under Stage tab, find the , you will need this url to test our implementation latter.</p><h4>Step 9: Test the End-to-End Flow</h4><p>Once all components are deployed:</p><p>9.1. Send a POST request to the API Gateway endpoint created in Step&nbsp;2.</p><pre>{  \"message\": \"Hello from the East!\"</pre><p>9.2. Check the DynamoDB table poc-eventbridge-cross-region-destination_table in us-west-2 to verify the message was&nbsp;stored.</p><p>This proof-of-concept validated Amazon EventBridge as a fast and reliable option for cross-region message replication. In our test, a message successfully traversed from us-east-1 to us-west-2 in approximately 634 milliseconds, highlighting the platform's low-latency performance.</p><p>While we implemented a basic event pattern, EventBridge supports more advanced rules that enable rich, event-driven architectures. This includes content-based filtering and complex routing logic, allowing for greater flexibility within each event&nbsp;bus.</p><p>Overall, this setup proves EventBridge’s capability to support scalable, asynchronous workflows across AWS regions using minimal, serverless infrastructure.</p><p>If you found this guide helpful, stay connected for more insights on <strong>AI, cloud security, and AWS automation</strong>:</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=36f5c71b89cf\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cilium Egress Gateway API: Implementing as an in-built Security through Kernel","url":"https://blog.devops.dev/cilium-egress-gateway-api-implementing-as-an-in-built-security-through-kernel-46d2670c2a7e?source=rss----33f8b2d9a328---4","date":1751133531,"author":"Aryan Parashar","guid":174611,"unread":true,"content":"<p>Kubernetes changed networking over pods and namespaces which was just limited to routing and security between the application which was controlled by pod networks using Network Policies.</p><p>Cilium implements egress control at Layer 3/4, directly in the Linux kernel using&nbsp;eBPF</p><p>With Many solutions implemented, external connectivity tries to communicate with workloads living outside the Kubernetes Cluster which gets subjected to connectivity constraints and security enforcements.</p><p>Traditional Firewalling usually relies on static IP Addresses, although in case of Kubernetes and Cloud workload we might also need to manage connectivity &amp; Traffic management with Dynamic IPs, which can be managed by Cilium Egress Gateway API and combination of some other Spacemaker solutions or Cloud-Native Solution Projects.</p><p>These traditional firewalling methods can make it really difficult to integrate a Kubernetes Cluster which has a varying and at the same time a dynamic number of nodes into such a&nbsp;network.</p><h3>Cilium Egress Gateway&nbsp;API</h3><p>Cilium Egress Gateway API solve these all problems by specifying which nodes should be used by any particular pod in order to reach outside&nbsp;world.</p><p>Traffic from these pods will be Source NATed to IP address of the node and will reach the external firewall with a predictable IP, enabling firewall to enforce right policy on a specific&nbsp;pod.</p><p>In contrast to other Egress or Gateway API solutions, a Cilium Egress Gateway&nbsp;API:</p><ul><li>Requires Cilium kube-proxy replacement</li><li>Enables connectivity between Kubernetes Cluster and traditional Firewall.</li></ul><h3>Benefits of using Cilium Egress Gateway&nbsp;API</h3><p>Cilium implements egress control at Layer 3/4, directly in the Linux kernel using eBPF, offering:</p><ol><li>Policy-based egress routing: You can define egress policies that route traffic based on pod labels, destination CIDRs, and even IP&nbsp;pools.</li><li>IP Pooling: Assigns dynamic or static public egress IPs per node or pod&nbsp;group.</li><li>Node-aware policies: You can ensure traffic is routed through a specific node with a public&nbsp;IP.</li><li>Failover support: If a node becomes unavailable, Cilium can reroute via another gateway dynamically.</li><li>No sidecar, no proxy overhead: Much lower latency and CPU&nbsp;usage.</li><li>Integration with Gateway API (in progress/alpha): Brings declarative Kubernetes-native way of managing egress at CNI&nbsp;level.</li></ol><h3>Implementing Cilium Egress Gateway&nbsp;API</h3><p>Here’s an example which, we can also find from the official example given by the Cilium and Isovalent.</p><p>Lets say we had 4 nodes as kind-worker, kind-worker2, kind-worker3, kind-worker4 with the 3rd and 4th node been planned to be deployed as Egress GatewayAPI. Now to handle the Egress Gateway functioning, we will carry out the following practices:</p><ul><li>Now as discussed for choosing which node to get deployed/bound with a pod for handling traffic, we’ll taint them from mistakenly scheduling any general-purpose pods</li></ul><pre>kubectl taint node kind-worker3 egress-gw:NoSchedule   kubectl taint node kind-worker4 egress-gw:NoSchedule</pre><ul><li>Label them for better monitoring and observability.</li></ul><pre>kubectl label nodes kind-worker3 egress-gw=true   kubectl label nodes kind-worker4 egress-gw=true</pre><ul><li>All the kind nodes are attached to Docker network called kind (Kubernetes in Docker) which use 172.18.0.0/16 IPV4&nbsp;CIDR.</li></ul><pre>docker network inspect -f '{{range.IPAM.Config}}{{.Subnet}}, {{end}}' kind</pre><p>CIDR is Classless Inter Domain Routing Method for allocating IP address on the internet, designed to improve efficiency and manage growth of internet’s routing&nbsp;table.</p><ul><li>Add a new Dummy interface eth0 to both the kind-workers 3 &amp; 4 with new address 172.18.0.42/16 and then add it to kind-worker 3 and 4 who’s IPs will be used by&nbsp;Cilium.</li></ul><pre>docker exec kind-worker3 ip link add net0 type dummy   docker exec kind-worker3 ip a add 172.18.0.42/16 dev net0   docker exec kind-worker3 ip link set net0 updocker exec kind-worker4 ip link add net0 type dummy   docker exec kind-worker4 ip a add 172.18.0.43/16 dev net0   docker exec kind-worker4 ip link set net0 up</pre><ul><li>Add Cilium and Egress Gateway API, disable Layer 7 proxy as its incomplete with Egress Gateway and attach two network interfaces to Egress Nodes called eth0 and&nbsp;net0.</li></ul><pre>cilium install \\     --version 1.17.1 \\     --set kubeProxyReplacement=true \\     --set egressGateway.enabled=true \\     --set bpf.masquerade=true \\     --set l7Proxy=false \\     --set devices=\"{eth+,net+}\"</pre><ul><li>Verify also that Cilium was started with the Egress Gateway&nbsp;feature:</li></ul><pre>cilium config view | grep egress-gateway</pre><h3>Setting up Egress Server and Egress&nbsp;Policies</h3><p>Needs to be attached to kind network, and we will pass the allowed source IP addresses as environment variables:</p><pre>docker run -d \\  --name remote-outpost \\  -e ALLOWED_IP=172.18.0.42,172.18.0.43 \\<p>   quay.io/isovalent-dev/egressgw-whatismyip:latest</p></pre><p>Retrieve the container’s IP in a variable:</p><pre>OUTPOST=$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' remote-outpost)echo $OUTPOS</pre><ul><li>Now lets deploy two more pods in our external outpost and see if they responds according to the labels we attached to&nbsp;them:</li></ul><pre>kubectl run tiefighter \\     --labels \"org=empire,class=tiefighter\" \\     --image docker.io/tgraf/netperf   kubectl run xwing \\     --labels \"org=alliance,class=xwing\" \\     --image docker.io/tgraf/netperf</pre><ul><li>Now we will add a Gateway Policies egress-gw-policy.yaml to route traffic to the external workload running in the&nbsp;network.</li><li>Resource to specify Cilium which Egress needs to be used for the traffci we’ll use: CiliumEgressGatewayPolicy</li><li>This will allow traffic to flow through one of the two Egress Nodes i.e. kind-worker3 or kind-worker4&nbsp;.</li></ul><ul><li>Here’s the egress-gw-policy.yaml file&nbsp;defined:</li></ul><pre>apiVersion: cilium.io/v2kind: CiliumEgressGatewayPolicy  name: outpost  destinationCIDRs:  selectors:      matchLabels:  egressGateway:      matchLabels:    interface: net0</pre><p>Then deploy the definition file it to the workload:</p><pre>kubectl apply -f egress-gw-policy.yaml</pre><p>If we try to deploy another pod with a new IP address with the org=alliance, then it will be accepted as it accepts the definition file&nbsp;policies</p><pre>kubectl run ywing \\  --labels \"org=alliance,class=ywing\" \\<p>  --image docker.io/tgraf/netperf</p></pre><h3>Accessing the external outpost server from the pods&nbsp;deployed</h3><pre>kubectl exec -ti xwing -- \\  curl --max-time 2 http://172.18.0.7:8000<p>kubectl exec -ti tiefighter -- \\</p>  curl --max-time 2 http://172.18.0.7:8000<p>kubectl exec -ti ywing -- \\</p>  curl --max-time 2 http://172.18.0.7:8000</pre><ol><li>Here since the traffic exits through one of the first two IP addresses been listed thatswhy, this command will give a success result for one amongst the first 2 of the IP&nbsp;address.</li><li>It will also give a success message with the 3rd IP address because it has org=alliance which matches with the policies been listed in the definition file for the Egress GatewayAPI.</li></ol><p>Access the outpost from the X-Wing a few times in a&nbsp;loop:</p><pre>for i in $(seq 1 10); do  kubectl exec -ti xwing -- \\<p>    curl --max-time 2 http://172.18.0.7:8000</p>done</pre><p>Note that traffic always exits the cluster from the same IP address, which means it always uses the same exit&nbsp;node.</p><p>In Cilium OSS, Egress Gateway Policies are used to select a node for a traffic, and that node will always be used for that given&nbsp;traffic.</p><p>For Managing the IP through which our traffic can exit, we will need to use High Availability type while defining our Cilium Egress Gateway&nbsp;API.</p><p>Soon, I will be uploading another Blog on High Availability in Cilium Egress Gateway API and will also integrate other CloudNative Solutions like kube-vip, etc. Till then stay&nbsp;excited:</p><p>If you like my Article then please react to it and connect with me on Linkedin &amp; Twitter if you are also a tech enthusiast. I would love to collaborate with people and share the experience of tech😄😄.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=46d2670c2a7e\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Complete CI/CD Pipeline Guide: Jenkins, Docker & GitHub Automation (2025)","url":"https://blog.devops.dev/the-complete-ci-cd-pipeline-guide-jenkins-docker-github-automation-2025-89479420e7fa?source=rss----33f8b2d9a328---4","date":1751133525,"author":"Ashish Singh","guid":174610,"unread":true,"content":"<div><p>An automated deployment system using Jenkins, Docker, Docker Compose, Git, and GitHub creates a powerful and flexible CI/CD pipeline. This…</p></div>","contentLength":141,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DevSecOps : Pipeline in Jenkins with Email Notification","url":"https://blog.devops.dev/devsecops-pipeline-in-jenkins-with-email-notification-0c7acfd0ce8f?source=rss----33f8b2d9a328---4","date":1751128514,"author":"Saisamarth Udikeri","guid":174571,"unread":true,"content":"<h3>DevSecOps&nbsp;: Pipeline in Jenkins with Email Notification</h3><p>In this article we’ll walk through a complete CI/CD pipeline that integrates security scanning at every stage. We’ll use Jenkins for orchestration, OWASP Dependency-Check and Trivy for vulnerability detection, SonarQube for code quality and security rules, Docker for packaging, and ArgoCD for continuous deployment to Kubernetes.</p><p>DevSecOps is the practice of integrating security “left” into every phase of the software delivery lifecycle — rather than bolting it on at the end. By embedding automated security checks into your CI/CD pipeline, you&nbsp;can:</p><ul><li><strong>Catch vulnerabilities early</strong> (in code and dependencies)</li><li> before merge or&nbsp;deploy</li><li><strong>Automate remediation feedback</strong> to developers</li><li> and compliance overhead</li></ul><h4>2. Overview of the&nbsp;Pipeline</h4><p>Below is a generic Jenkinsfile that implements:</p><ol><li><strong>Update Kubernetes manifests</strong></li></ol><pre>pipeline {  agent any    nodejs 'NodeJS'    SONAR_PROJECT_KEY   = 'MyAppProject'<p>    SONAR_SCANNER_HOME  = tool 'SonarScanner'</p>    SONAR_TOKEN         = credentials('sonar-token')<p>    OWASP_TOOL_HOME     = tool 'OWASP-CLI'</p>    DOCKER_REPO         = 'myorg/myapp'    stage('Checkout') {        git branch: 'main',<p>            url:    'https://github.com/myorg/myapp.git'</p>      }<p>    stage('Install &amp; Build') {</p>      steps {        sh 'npm run build'    }<p>    stage('OWASP Dependency-Check') {</p>      steps {<p>        catchError(buildResult: 'SUCCESS', stageResult: 'SUCCESS') {</p>          sh \"\"\"<p>            ${OWASP_TOOL_HOME}/bin/dependency-check.sh \\</p>              --project \"${SONAR_PROJECT_KEY}\" \\              --format XML \\              --out reports/dependency-check          dependencyCheckPublisher(<p>            pattern: 'reports/dependency-check/dependency-check-report.xml',</p>            stopBuild: false        }    }<p>    stage('SonarQube Analysis') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        withSonarQubeEnv('SonarQube') {            ${SONAR_SCANNER_HOME}/bin/sonar-scanner \\<p>              -Dsonar.projectKey=${SONAR_PROJECT_KEY} \\</p>              -Dsonar.sources=src \\<p>              -Dsonar.host.url=${env.SONAR_HOST_URL} \\</p>              -Dsonar.login=${SONAR_TOKEN}        }      post {          waitForQualityGate abortPipeline: true      }<p>    stage('Build Docker Image') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        script {<p>          def tag = \"${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}\"</p>          docker.build(tag)      }<p>    stage('Trivy Container Scan') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        script {<p>          def tag = \"${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}\"</p>          sh \"\"\"              --severity HIGH,CRITICAL \\              --no-progress \\<p>              ${tag} &gt; reports/trivy-report.txt</p>          \"\"\"      }        always {<p>          archiveArtifacts artifacts: 'reports/trivy-report.txt', fingerprint: true</p>        }    }<p>    stage('Push to Registry') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        script {<p>          docker.withRegistry('', 'docker-creds') {</p>            docker.image(\"${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}\").push()        }    }<p>    stage('Update K8s Manifests') {</p>      when { expression { currentBuild.result == null || currentBuild.result == 'SUCCESS' } }        withCredentials([usernamePassword(credentialsId: 'git-creds', usernameVariable: 'GIT_USER', passwordVariable: 'GIT_TOKEN')]) {            checkout([              branches: [[name: '*/main']],                url: 'https://github.com/myorg/k8s-manifests.git',<p>                credentialsId: 'git-creds'</p>              ]]            sh \"\"\"<p>              sed -i 's|image:.*|image: ${DOCKER_REPO}:1.0.${env.BUILD_NUMBER}|' deployment.yaml</p>              git config user.email \"ci@myorg.com\"<p>              git config user.name  \"CI Bot\"</p>              git add deployment.yaml<p>              git commit -m \"Update image to 1.0.${env.BUILD_NUMBER}\"</p>              git push origin main          }      }      steps {<p>        sh \"docker rmi ${DOCKER_REPO}:1.0.${env.BUILD_NUMBER} || true\"</p>      }  }    success {        attachLog: true,<p>        attachmentsPattern: 'dependency-check-report/*.html, dependency-check-report/*.xml, trivy-scan-report.txt',</p>        from:    name@gmail.com',<p>        to:      name@gmail.com',</p>        subject: \"✅ Build #${env.BUILD_NUMBER} of ${env.JOB_NAME} Succeeded\",        body: \"\"\"            &lt;body&gt;<p>              &lt;h2 style=\"color: green;\"&gt;Build Succeeded!&lt;/h2&gt;</p>              &lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; ${env.JOB_NAME}&lt;/p&gt;<p>              &lt;p&gt;&lt;strong&gt;Build Number:&lt;/strong&gt; ${env.BUILD_NUMBER}&lt;/p&gt;</p>              &lt;p&gt;&lt;strong&gt;Build URL:&lt;/strong&gt; &lt;a href=\"${env.BUILD_URL}\"&gt;${env.BUILD_URL}&lt;/a&gt;&lt;/p&gt;<p>              &lt;p&gt;&lt;strong&gt;Preview Site:&lt;/strong&gt; &lt;a href=\"site\"&gt;site/&lt;/a&gt;&lt;/p&gt;</p>            &lt;/body&gt;        \"\"\"    }      emailext(        attachmentsPattern: 'dependency-check-report/*.html, dependency-check-report/*.xml, trivy-scan-report.txt',<p>        from:    name@gmail.com',</p>        to:      name@gmail.com',<p>        subject: \"❌ Build #${env.BUILD_NUMBER} of ${env.JOB_NAME} Failed\",</p>        mimeType: 'text/html',          &lt;html&gt;              &lt;h2 style=\"color: red;\"&gt;Build Failed!&lt;/h2&gt;<p>              &lt;p&gt;&lt;strong&gt;Project:&lt;/strong&gt; ${env.JOB_NAME}&lt;/p&gt;</p>              &lt;p&gt;&lt;strong&gt;Build Number:&lt;/strong&gt; ${env.BUILD_NUMBER}&lt;/p&gt;<p>              &lt;p&gt;&lt;strong&gt;Build URL:&lt;/strong&gt; &lt;a href=\"${env.BUILD_URL}\"&gt;${env.BUILD_URL}&lt;/a&gt;&lt;/p&gt;</p>              &lt;p&gt;Please review the console output and attached reports for details.&lt;/p&gt;          &lt;/html&gt;      )  }</pre><p>Use Docker Compose to stand up Jenkins with Docker‐in‐Docker support:</p><pre>version: '3.8'services:    image: jenkins/jenkins:lts    ports:      - \"50000:50000\"      - DOCKER_GID=${DOCKER_GID}      - /var/run/docker.sock:/var/run/docker.sock<p>      - /usr/bin/docker:/usr/bin/docker</p></pre><p> on host and export DOCKER_GID.</p><p> with docker-compose up&nbsp;-d.</p><ul></ul><ul><li> (e.g. “NodeJS&nbsp;14.x”)</li></ul><ul><li>Git (username/password or&nbsp;PAT)</li></ul><p>Deploy a SonarQube + PostgreSQL stack via Docker&nbsp;Compose:</p><pre>version: '3'services:    image: sonarqube:lts-community      SONAR_JDBC_URL: jdbc:postgresql://db:5432/sonar<p>      SONAR_JDBC_USERNAME: sonar</p>      SONAR_JDBC_PASSWORD: sonar      - \"9001:9000\"      - ./conf:/opt/sonarqube/conf<p>      - ./data:/opt/sonarqube/data</p>      - ./extensions:/opt/sonarqube/extensions<p>      - ./logs:/opt/sonarqube/logs</p>      - ./temp:/opt/sonarqube/temp    image: postgres:13      POSTGRES_USER: sonar      POSTGRES_DB: sonar      - ./postgresql_data:/var/lib/postgresql/data</pre><ul><li> and chown 1000:1000.</li><li> with docker-compose up&nbsp;-d.</li><li> at http://&lt;host&gt;:9001 (admin/admin).</li><li> and ; add to Jenkins as&nbsp;secret.</li></ul><h4>5. OWASP Dependency-Check in&nbsp;Jenkins</h4><ol><li> the OWASP Dependency-Check plugin.</li><li> the CLI tool and add it in <strong>Manage Jenkins → Global Tool Configuration</strong>.</li><li> the tool by name (OWASP-CLI) in your Jenkinsfile.</li><li> the XML report with dependencyCheckPublisher.</li></ol><h4>6. Trivy Container Scanning</h4><p><a href=\"https://github.com/aquasecurity/trivy\">Trivy</a> is a simple vulnerability scanner for containers:</p><pre>trivy image --severity HIGH,CRITICAL --exit-code 1 myorg/myapp:latest</pre><ul><li> Trivy on your Jenkins agent (apt, brew or binary download).</li><li> the plain-text report for&nbsp;audit.</li></ul><h4>7. Continuous Deployment with&nbsp;ArgoCD</h4><p>ArgoCD lets you declaratively push Kubernetes manifests:</p><pre>kubectl create namespace argocdkubectl apply -n argocd \\<p>  -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</p>kubectl patch svc argocd-server \\  -p '{\"spec\": {\"type\": \"LoadBalancer\"}}'</pre><ul><li> for sync and rollbacks.</li><li> (argocd login &lt;server&gt;) and  your Git&nbsp;repo.</li><li> an App that points at myorg/k8s-manifests.</li></ul><p>By combining Jenkins, SonarQube, OWASP Dependency-Check, Trivy and ArgoCD, you get a fully automated, policy-enforced DevSecOps pipeline. Shift security left, automate compliance, and deliver faster with confidence. Feel free to fork the sample repos and adapt the placeholders (MyAppProject, myorg/myapp, etc.) to kick off your own secure delivery process&nbsp;today!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=0c7acfd0ce8f\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7936,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop Using Loops in JavaScript: 6 Smarter, Cleaner Alternatives You Should Know","url":"https://blog.devops.dev/stop-using-loops-in-javascript-6-smarter-cleaner-alternatives-you-should-know-1b3984e24694?source=rss----33f8b2d9a328---4","date":1751128509,"author":"David Lam","guid":174570,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Practical Guide to Common Podman Commands — Part 2","url":"https://blog.devops.dev/a-practical-guide-to-common-podman-commands-part-2-8a3e6633308a?source=rss----33f8b2d9a328---4","date":1751128505,"author":"Vijayasekhar Deepak","guid":174569,"unread":true,"content":"<div><p>In this blog, I will share the list of mostly used podman commands for your development. This blog is part of the Podman series on how to…</p></div>","contentLength":140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How To — Open Firewall Ports in RHEL & CentOS Linux","url":"https://blog.devops.dev/how-to-open-firewall-ports-in-rhel-centos-linux-a433fd268de2?source=rss----33f8b2d9a328---4","date":1751128489,"author":"Kiranms","guid":174568,"unread":true,"content":"<h3>How To — Open Firewall Ports in RHEL &amp; CentOS&nbsp;Linux</h3><p>To Open any ports — for me specific  and  ports, you also need to open  port on all the RHEL7 and RHEL8 servers to connect your OC to CM and jenkins agent, I have demonstrated this in Linux server for my <strong>Cloudbees Jenkins Migration Series</strong>, please follow below procedure.</p><p><strong><em>Note:- 50000/tcp is agent port that you will be connecting to your OC and&nbsp;CM.</em></strong></p><p>if you see below URL for Jenkins OC for RHEL7 is not working, once we add firewall it will start&nbsp;opening.</p><p>Cloudbees Jenkins OC RHEL7&nbsp;:-</p><p>Cloudbees Jenkins CM RHEL7&nbsp;:-</p><ol><li>List already opened&nbsp;Ports</li></ol><p><strong># firewall-cmd — list-all</strong>you will see there are by-default below services are open and no ports are seems to be&nbsp;open.</p><p>heck the Service that we want to use and is configured already.</p><p><strong>#firewall-cmd — get-services</strong></p><p>3. Check the Zones&nbsp;, real-time there will be many zones on the firewall but we are interested in Public only for this&nbsp;case.</p><p><strong>#firewall-cmd — get-zones</strong></p><p>4. Open Port service that we want to open for public. for my case I am opening traffic for http on 8888 &amp; https on 8443&nbsp;port.</p><p><strong>#firewall-cmd — zone=public — permanent — add-service=http#firewall-cmd — zone=public — permanent — add-port&nbsp;8888/tcp</strong></p><p><strong>#firewall-cmd — zone=public — permanent — add-service=https#firewall-cmd — zone=public — permanent — add-port&nbsp;8443/tcp</strong></p><p><strong>#firewall-cmd — zone=public — permanent — add-service=jenkins#firewall-cmd — zone=public — permanent — add-port 50000/tcp</strong></p><p>6. Confirm if port are&nbsp;opened.</p><p>now you can see in the above screen-shot&nbsp;, you see http/8888&nbsp;, https/8443 and jenkins/50000 ports are open and our jenkins URL will work&nbsp;now.</p><p>Now you can see URL for both Operation Center and Controller started working after opening the firewall&nbsp;ports.</p><p>Cloudbees Jenkins OC&nbsp;RHEL7:-</p><p><strong>Happy Learning &amp; GOOD LUCK&nbsp;!!!!!</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a433fd268de2\" width=\"1\" height=\"1\" alt=\"\">","contentLength":1921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How To — Applying License on Cloudbees Jenkins Operation Center","url":"https://blog.devops.dev/how-to-applying-license-on-cloudbees-jenkins-operation-center-1d60ef76ab77?source=rss----33f8b2d9a328---4","date":1751128481,"author":"Kiranms","guid":174567,"unread":true,"content":"<h3>How To — Applying License on Cloudbees Jenkins Operation Center</h3><p>There are two way to apply licenses on Cloudbees Jenkins Operation Center&nbsp;, Lets us look at them one by&nbsp;one.</p><h4><strong>Way 1&nbsp;:- License Applying from Initial Jenkins&nbsp;Screen..</strong></h4><ol><li>Login to Jenkins URL and enter yourfrom the specified location and click continue.</li></ol><p>2. Then click on buttonand enter your license key.<strong><em>Note&nbsp;:- In this case I have requested Cloudbees Trial License and performing this migration task.</em></strong></p><p>3. You will be prompted to enter the License Key. Here you will see two section License Key &amp; License Certificate</p><p>4. then enter your License and Certificate details and accept the license and click on Submit&nbsp;button.</p><p>5. then you will be prompted for suggested plugins installation, if you want to go with “Install suggested plugins” or “Select plugins to install” based on your&nbsp;need.</p><p>6. then Jenkins URL configuration screen appears&nbsp;, name your URL that you need to access your Jenkins URL &amp; save and&nbsp;finish.</p><p>7. Click on the <strong>“Start Using Cloudbees Jenkins Operation Center”</strong> and you will be entered to Jenkins home&nbsp;page.</p><p>8. after clicking <strong>“Start Using Cloudbees Jenkins Operation Center” </strong>you will be landed on Jenkins Home UI and That's it we have applied Jenkins Enterprise License successfully&nbsp;…&nbsp;🤩🤩🤩.</p><h4><strong>Way 2&nbsp;:- Applying License from “Manage Jenkins”&nbsp;Section.</strong></h4><ol><li>From the jenkins UI, Please click on </li></ol><p>2. Then Click on “Manage&nbsp;License”</p><p>3. Enter you License details in  and and Click on Save and Yes that’s it we have successfully applied the Cloudbees Jenkins Enterprise License on Operation Center.</p><h3><strong>Happy Learning &amp; GOOD LUCK&nbsp;!!!!!</strong></h3><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1d60ef76ab77\" width=\"1\" height=\"1\" alt=\"\">","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How To — Understanding Jenkins Parameter Configuration","url":"https://blog.devops.dev/how-to-understanding-jenkins-parameter-configuration-1d995887614d?source=rss----33f8b2d9a328---4","date":1751128465,"author":"Kiranms","guid":174566,"unread":true,"content":"<h3>How To — Understanding Jenkins Parameter Configuration</h3><p>Why Cloudbees Jenkins Parameter Configuration is important, the parameter configuration in Jenkins configuration files allows us to define and customize various settings and behaviors of the cloudbees Jenkins server. These configuration files are stored in the Jenkins home directory or in Linux Configuration directory called /etc/ and all the jenkins configuration are placed in jenkins home directory or in /etc/ directory</p><p>for Jenkins Operation Center /etc/sysconfig/jenkins-oc or /etc/sysconfig/cloudbees-core-oc and for Controller called /etc/sysconfig/cloudbees-core-cm depends on the version you have installed.</p><p>Here Just trying to quickly explain Jenkins configuration file which is very important for all your parameter configuration of&nbsp;jenkins.</p><ul><li> This directory stores configuration and working files. default JENKINS_HOME=/var/lib/jenkins-oc.</li><li> Customize to run jenkins with your Specifies Java executable.</li><li> Customize with your Unix service user account that runs your Jenkins daemon. in our case i have create </li><li> This is where we specifies the options to pass to Java when running Jenkins. In this case just to demo we have included options such as , which runs Jenkins in headless mode, and additional options of ”<strong>Dhudson.TcpSlaveAgentListener.hostName=jmasterr8\"</strong> “<strong>Dhudson.TcpSlaveAgentListener.port=50000\" </strong>which means TCP slave agent listener hostname (jmasterr8) and port number&nbsp;(50000).</li><li>This specifies the http port on which Jenkins is listening for HTTP requests. by-default it is on&nbsp;8888.</li><li> This specifies https port on which our Jenkins listening for HTTPS requests by default it is disabled in our case we will be using&nbsp;8443.</li><li> This option allows us passing arbitrary arguments to the Jenkins process like ,  and .</li></ul><p>There are and will be many configurational parameter setting as per organizational setup, but will not be going in details it will be too lengthy, I have just tried to show which are most important and which will help us to setup our Cloudbees Jenkins Migration Series&nbsp;Demo.</p><p>I am putting down screenshot of my Jenkins configuration file from Operation Center and from Controller it will give you more clear idea on&nbsp;it.</p><p><strong>jmasterr8&nbsp;:- /etc/sysconfig/jenkins-oc</strong></p><p><strong>jslaver8&nbsp;:- /etc/sysconfig/cloudbees-core-cm</strong></p><p>When we will head over to SSL configuration in my <strong>“Demystify — Cloudbees Jenkins Migration Series”</strong> will show more about and </p><p>That's it on Understanding of Jenkins Parameter Configuration.</p><p>Hope you are clear on <strong>“How To — Understanding Jenkins Parameter Configuration”</strong>.</p><p>if you like this blog please share, comment that motivates me to writes another great technological blogs.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=1d995887614d\" width=\"1\" height=\"1\" alt=\"\">","contentLength":2682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"87% of GenAI Failures Were Preventable: This MLOps Framework Shows How","url":"https://blog.devops.dev/87-of-genai-failures-were-preventable-this-mlops-framework-shows-how-f968505990c1?source=rss----33f8b2d9a328---4","date":1751128413,"author":"R. Thompson (PhD)","guid":174565,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Istio on Large GKE Clusters","url":"https://blog.devops.dev/istio-on-large-gke-clusters-b8bbf528e3b9?source=rss----33f8b2d9a328---4","date":1751128397,"author":"Mohamed Rasvi","guid":174564,"unread":true,"content":"<h3><strong>ASM was deprecated last year for&nbsp;CSM</strong></h3><h3>Installation, Optimization, and Namespace-Scoped Traffic Management</h3><p>Deploying and operating Istio at scale on a Google Kubernetes Engine (GKE) cluster with 36 nodes and 2000 applications requires careful planning and optimization. The primary concerns typically revolve around the resource footprint of the Istio control plane (istiod) and the efficient management of traffic&nbsp;rules.</p><h3>Istio Installation on&nbsp;GKE</h3><p>For large GKE clusters, the recommended approach is to use <strong>Anthos Service Mesh (ASM)</strong>, which is Google’s managed Istio offering. ASM provides a hardened, supported, and automatically updated Istio distribution, significantly reducing operational overhead compared to a self-managed Istio installation.</p><p><strong>Why ASM for Large Clusters?</strong></p><ul><li> Google manages the Istio control plane (istiod), including scaling, updates, and patching. This is crucial for stability and resource efficiency in large environments.</li><li> ASM is optimized for Google Cloud environments and is designed to scale with large clusters.</li><li> Deep integration with Google Cloud operations suite (Cloud Monitoring, Cloud Logging, Cloud Trace) for enhanced observability.</li><li> Enterprise-grade support from&nbsp;Google.</li></ul><p><strong>Installation Steps (High-Level for&nbsp;ASM):</strong></p><p> Ensure necessary Google Cloud APIs (e.g., GKE, Anthos Service Mesh, Cloud Monitoring) are enabled for your&nbsp;project.</p><p><strong>Meet Cluster Requirements:</strong> Your GKE cluster must meet specific requirements (e.g., GKE version, Workload Identity enabled, sufficient node pool&nbsp;size).</p><p> Register your GKE cluster to a Google Cloud&nbsp;Fleet.</p><p> Use the gcloud container fleet mesh enable command or the Google Cloud Console to install ASM onto your cluster. This process automatically sets up istiod and related components.</p><p><strong>Enable Sidecar Injection:</strong> Label namespaces where you want to deploy applications with the istio-injection=enabled label (or istio.io/rev=asm-managed for ASM) to automatically inject Envoy sidecars.</p><p>For self-managed Istio (less recommended for this scale but an&nbsp;option):</p><p> Get the Istio command-line tool.</p><p> Use an IstioOperator custom resource or istioctl install with --set flags to configure istiod resources and features.</p><p> istioctl install -f &lt;your-config.yaml&gt; --set profile=default (or a more customized profile).</p><h3>Memory and Resource Optimization for Large&nbsp;Clusters</h3><p>The key to handling a large number of nodes and applications is to optimize both the Istio control plane (istiod) and the Envoy sidecars.</p><h4>Istio Control Plane (istiod)&nbsp;Sizing</h4><p>istiod is the heart of the Istio control plane, responsible for configuration, certificate management, and proxy injection. Its resource consumption scales with the number of services, configurations (VirtualServices, DestinationRules), and especially the number of Envoy proxies it needs to&nbsp;manage.</p><ul><li> With ASM, Google takes on much of the burden of scaling istiod. They will dynamically adjust the resources allocated to the managed control plane based on your cluster's demands.</li><li> Explicitly set higher CPU and memory limits for the istiod deployment. Start with recommended values and monitor carefully. For 2000 apps, you will likely need several cores and tens of GBs of&nbsp;RAM.</li><li><strong>Horizontal Pod Autoscaler (HPA):</strong> Configure HPA for istiod to allow it to scale horizontally based on CPU utilization or custom&nbsp;metrics.</li><li> During installation or with an IstioOperator manifest, disable features you don't immediately need (e.g., Mixer, unused telemetry exporters, policy checks). This significantly reduces istiod's&nbsp;load.</li></ul><pre># Example IstioOperator snippet for disabling features (for self-managed)apiVersion: install.istio.io/v1alpha1spec:    enableTracing: true # Only enable if you need tracing<p>    disablePolicyChecks: true # Disable if you don't use Istio's policy engine</p>    # ... other mesh config settings    pilot: # Renamed to istiod in newer Istio versions        resources:            cpu: 2000m # 2 CPU cores          limits:            memory: 8Gi # 8 GB RAM</pre><h4>Envoy Sidecar Resource&nbsp;Limits</h4><p>Each application pod will have an Envoy proxy sidecar injected. While individual sidecars are lightweight, 2000 applications mean 2000 Envoy proxies, which can collectively consume significant resources.</p><ul><li> Istio injects sidecars with default CPU and memory limits. For large clusters, you might need to adjust&nbsp;these.</li><li> Monitor the actual resource usage of your Envoy sidecars in different application types. Some applications (e.g., high-throughput APIs) might need more resources, while others (e.g., simple internal services) can operate with&nbsp;less.</li><li> You can override the default sidecar resource requests and limits globally or per namespace/pod.</li></ul><pre># Global sidecar configuration via IstioOperator (for self-managed)apiVersion: install.istio.io/v1alpha1spec:    defaultConfig:      proxy_requests:        memory: 20Mi # 20 MiB        cpu: 100m # 100 millicores<p># Per-pod override (via annotation in Deployment YAML)</p>apiVersion: apps/v1metadata:spec:    metadata:        sidecar.istio.io/proxyCPU: \"50m\"<p>        sidecar.istio.io/proxyMemory: \"128Mi\"</p>    spec:      - name: my-app-container</pre><h3>Dividing Istio Control Traffic Rules Per Namespace</h3><p>This is a crucial strategy for managing complexity and preventing a single, monolithic set of traffic rules from becoming unwieldy and impacting performance. Istio’s traffic management resources are inherently designed with namespace scoping in&nbsp;mind.</p><ul><li><strong>VirtualService and DestinationRule Scoping:</strong></li><li>VirtualService and DestinationRule resources are . This means you define these rules within the specific Kubernetes namespace where the target services&nbsp;reside.</li><li> Define VirtualServices and DestinationRules directly in the namespaces where your applications are deployed. For example, if you have service-a in namespace-a, define its VirtualService and DestinationRule in namespace-a.</li><li>Gateway resources are  by default or can be defined in a specific \"gateway\" namespace. They represent a load balancer or ingress point for traffic entering the&nbsp;mesh.</li><li>Typically, you define your Gateway once (e.g., in an istio-system or ingress-gateway namespace) and then attach VirtualServices from other namespaces to&nbsp;it.</li></ul><p><strong>How Namespace Scoping&nbsp;Helps:</strong></p><p><strong>Reduced Configuration Load:</strong> istiod processes configurations more efficiently when they are distributed across namespaces. It only needs to push relevant configurations to sidecars in a specific namespace.</p><p><strong>Isolation and Blast Radius:</strong> Changes to traffic rules in one namespace are less likely to impact services in other, unrelated namespaces.</p><p> Different teams can manage their own Istio traffic rules within their designated namespaces without interfering with&nbsp;others.</p><p><strong>Clarity and Organization:</strong> It becomes much easier to understand and troubleshoot traffic flow when rules are logically grouped by application or team namespace.</p><p><strong>Example of Namespace-Scoped Rule Management:</strong></p><p>Let’s say you have two applications, app-a in namespace-a and app-b in namespace-b.</p><p><strong>Gateway Definition (typically in a central namespace like </strong></p><pre># istio-system/my-ingress-gateway.yamlapiVersion: networking.istio.io/v1beta1metadata:  namespace: istio-system # Or a dedicated ingress namespace  selector:<p>    istio: ingressgateway # Selects the Istio ingress gateway pod</p>  servers:      number: 80      protocol: HTTP    - \"app-a.example.com\"</pre><pre># namespace-a/app-a-virtualservice.yamlapiVersion: networking.istio.io/v1beta1metadata:  namespace: namespace-a # Defined in app-a's namespace  hosts:  gateways:<p>  - istio-system/my-ingress-gateway # Reference the central gateway</p>  http:    - uri:    route:        host: app-a-service.namespace-a.svc.cluster.local          number: 80</pre><pre># namespace-a/app-a-destinationrule.yamlapiVersion: networking.istio.io/v1beta1metadata:  namespace: namespace-a # Defined in app-a's namespace  host: app-a-service.namespace-a.svc.cluster.local  - name: v1      version: v1    labels:</pre><p>You would then repeat similar VirtualService and DestinationRule definitions in namespace-b for&nbsp;app-b.</p><p><strong>Exporting Resources (less common for basic traffic&nbsp;rules):</strong></p><p>While most traffic rules stay within their namespace, VirtualServices and DestinationRules have an exportTo field. If you define a rule in namespace-a and explicitly want it to be usable by services in namespace-b (e.g., for direct service-to-service communication across namespaces that's more complex than a simple ServiceEntry), you can use exportTo: [\"namespace-b\"] or exportTo: [\"*\"] (for all namespaces). However, for basic ingress and internal service routing, keeping rules localized is generally preferred.</p><h3>Monitoring Istio Components</h3><p>Comprehensive monitoring is essential to identify bottlenecks and ensure stability in a large Istio deployment.</p><ul><li><strong>Google Cloud Operations Suite (Stackdriver):</strong> For ASM, this is your primary monitoring tool. It provides dashboards, metrics, and logs for Istio components and sidecars.</li><li><strong>Prometheus and Grafana (for self-managed Istio):</strong> Istio natively integrates with Prometheus for metrics collection and Grafana for visualization. Deploy these tools and set up dashboards to&nbsp;monitor:</li><li>istiod CPU, memory, and networking usage.</li><li>Envoy sidecar CPU, memory, request rates, latency, and error rates per&nbsp;service.</li><li>Control plane health and configuration pushes.</li><li> Use this command regularly to check your Istio configuration for potential issues.</li><li> Useful for checking the connection status and configuration synchronization between istiod and individual Envoy&nbsp;proxies.</li></ul><p>By combining the power of managed Istio (ASM) for control plane scalability with careful namespace-based organization of traffic rules and robust monitoring, you can effectively manage a large GKE cluster with 2000 applications under&nbsp;Istio.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b8bbf528e3b9\" width=\"1\" height=\"1\" alt=\"\">","contentLength":9593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prometheus and Grafana: A Match Made in Cloud-Native Heaven","url":"https://blog.devops.dev/prometheus-and-grafana-a-match-made-in-cloud-native-heaven-eebad162cc1c?source=rss----33f8b2d9a328---4","date":1751128391,"author":"ThreadSafe Diaries","guid":174563,"unread":true,"content":"<div><p>In the world of cloud-native applications, where the stakes are high and every millisecond matters, monitoring isn’t just a luxury; it’s a…</p></div>","contentLength":145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Migrating 5 Petabytes from AWS S3 to GCP Cloud Storage Archive","url":"https://blog.devops.dev/migrating-5-petabytes-from-aws-s3-to-gcp-cloud-storage-archive-a107634969eb?source=rss----33f8b2d9a328---4","date":1751128383,"author":"Mohamed Rasvi","guid":174562,"unread":true,"content":"<p>Migrating 5 petabytes of data from an Amazon Web Services (AWS) S3 bucket to Google Cloud Platform (GCP) Cloud Storage Archive is a complex task that requires careful planning, execution, and cost management. This blog outlines a step-by-step process using Google Cloud’s Storage Transfer Service, addresses key considerations including checksum validation, and provides cost estimates for AWS egress and GCP&nbsp;storage.</p><p>Before starting, ensure you&nbsp;have:</p><ul><li>: Access to the S3 bucket with 5 petabytes of data and AWS credentials (Access Key ID and Secret Access Key) with read permissions.</li><li>: A Google Cloud project with billing enabled and the Cloud Storage API activated.</li><li>AWS: An IAM role or user with AmazonS3ReadOnlyAccess for the source&nbsp;bucket.</li><li>GCP: A service account with Storage Object Admin and Storage Transfer Admin&nbsp;roles.</li><li>: Sufficient bandwidth for large-scale data transfer or a dedicated interconnect for reduced egress&nbsp;costs.</li><li>: Install awscli and gsutil for checksum generation and validation.</li></ul><h3>Step-by-Step Migration Process</h3><h3>Step 1: Create a GCP Cloud Storage&nbsp;Bucket</h3><ol><li>Navigate to  and click .</li></ol><ul><li>: Choose a globally unique name (e.g., my-archive-bucket-5pb).</li><li>: Select a region (e.g., us-central1) to optimize costs and&nbsp;latency.</li><li>: Choose  for long-term, infrequently accessed&nbsp;data.</li><li>: Use uniform access control for simplicity.</li><li>: Use Google-managed keys or a custom key for security.</li></ul><p>Create the bucket and note its&nbsp;name.</p><h3>Step 2: Configure AWS Credentials</h3><ol><li>In the AWS Management Console, go to  or&nbsp;.</li><li>Create or use an existing user/role with AmazonS3ReadOnlyAccess permissions.</li><li>Generate an  and  (save securely).</li></ol><ul><li>Alternatively, use an IAM role ARN for identity federation if preferred.</li></ul><h3>Step 3: Set Up Storage Transfer&nbsp;Service</h3><ol><li>In the Google Cloud Console, navigate to <strong>Data Transfers &gt; Storage Transfer&nbsp;Service</strong>.</li><li>Click .</li></ol><ul><li>Select  as the data&nbsp;source.</li><li>Enter the  (e.g., my-s3-bucket-5pb).</li><li>Provide the  and  from Step&nbsp;2.</li></ul><p><strong>Configure the destination:</strong></p><ul><li>Select the GCP bucket created in Step 1 (e.g., my-archive-bucket-5pb).</li><li>Optionally, specify a prefix to filter specific&nbsp;objects.</li></ul><ul><li>Choose <strong>Overwrite objects if they already exist</strong> if&nbsp;needed.</li><li>Select <strong>Delete objects from source after transfer</strong> only if you’re sure the source data can be&nbsp;deleted.</li><li>For large datasets, enable  to sync incremental changes.</li></ul><p>Select  for a one-time migration or set a recurring schedule for ongoing&nbsp;sync.</p><p>For 5 petabytes, a one-time transfer is recommended initially, followed by incremental syncs if&nbsp;needed.</p><p>Review and start the transfer&nbsp;job.</p><h3>Step 4: Monitor the&nbsp;Transfer</h3><ol><li>In the Storage Transfer Service console, track the job’s progress.</li><li>Check logs for errors (e.g., permission issues or network interruptions).</li><li>Use GCP’s monitoring tools to verify data integrity post-transfer.</li><li>For large datasets, consider running the job in batches (e.g., by prefix) to manage errors and optimize performance.</li></ol><p>Compare object counts and&nbsp;sizes:</p><ul><li>Use aws s3 ls --summarize --recursive s3://my-s3-bucket-5pb/ to get object counts and total size in&nbsp;AWS.</li><li>Use gsutil ls -l gs://my-archive-bucket-5pb/ to get the same in&nbsp;GCP.</li></ul><p>Generate and compare checksums for data integrity:</p><ul><li>: Use aws s3api list-objects-v2 --bucket my-s3-bucket-5pb --query 'Contents[].{Key:Key,ETag:ETag}' to retrieve object ETags (MD5 checksums for non-multipart uploads). For multipart uploads, use a tool like s3md5 or compute MD5 locally if&nbsp;needed.</li><li>: Use gsutil hash -m gs://my-archive-bucket-5pb/** to compute MD5 checksums for objects in the GCP&nbsp;bucket.</li><li>Script a comparison (e.g., using&nbsp;Python):</li></ul><pre>import boto3import google.cloud.storages3 = boto3.client('s3', aws_access_key_id='YOUR_KEY', aws_secret_access_key='YOUR_SECRET')<p>paginator = s3.get_paginator('list_objects_v2')</p>s3_checksums = {}<p>for page in paginator.paginate(Bucket='my-s3-bucket-5pb'):</p>    for obj in page.get('Contents', []):<p>        s3_checksums[obj['Key']] = obj['ETag'].strip('\"')</p>gcp_client = google.cloud.storage.Client()<p>bucket = gcp_client.bucket('my-archive-bucket-5pb')</p>gcp_checksums = {blob.name: blob.md5_hash for blob in bucket.list_blobs()}mismatches = [(key, s3_checksums[key], gcp_checksums.get(key)) for key in s3_checksums if key in gcp_checksums and s3_checksums[key] != gcp_checksums[key]]    print(\"Checksum mismatches:\", mismatches)    print(\"All checksums match!\")</pre><p>For 5 petabytes, process checksums in batches (e.g., by prefix) to manage memory and performance.</p><p>Test application access to the GCP bucket to confirm functionality.</p><ol><li>Once validated, delete the source S3 bucket if no longer needed to avoid ongoing storage&nbsp;costs.</li><li>Remove temporary AWS credentials or roles used for the transfer.</li><li>Archive or delete the transfer job in GCP if it’s a one-time migration.</li></ol><h4>AWS Egress Costs: AWS Egress Pricing (Data Out to Internet)</h4><p>AWS S3 egress pricing is tiered. The exact rates depend on the AWS region, but generally follow this structure:</p><ul><li>Next 9.9 TB/month (up to 10 TB total): $0.09 per&nbsp;GB</li><li>Next 40 TB/month (up to 50 TB total): $0.085 — $0.09 per&nbsp;GB</li><li>Next 100 TB/month (up to 150 TB total): $0.07 per&nbsp;GB</li><li>Greater than 150 TB/month: $0.05 per&nbsp;GB</li></ul><p> As of March 2024, AWS announced a waiver of egress fees for customers transferring data <em>out of their cloud platform</em> for good, if you contact AWS support and are moving data to another cloud provider or on-premises data center. However, this waiver <em>does not apply to regular business data egress</em>. The context of your question implies a one-time, large-scale migration, which  qualify for this waiver, but it requires direct engagement with AWS and meeting their criteria. For general calculation, we’ll assume standard egress&nbsp;applies.</p><p>Let’s do a  calculation for 5 PB (5,242,880 GB) as if it were transferred purely over the internet, using a common US East region (prices are approximate and can vary slightly):</p><ul><li> 0 GB remaining</li><li><strong>Next 9,900 GB (10 TB total):</strong> $0.09/GB * 9,900 GB =&nbsp;$891</li><li><strong>Next 40,000 GB (50 TB total):</strong> $0.09/GB * 40,000 GB =&nbsp;$3,600</li><li><strong>Next 100,000 GB (150 TB total):</strong> $0.07/GB * 100,000 GB =&nbsp;$7,000</li><li><strong>Remaining 5,092,880 GB (5,242,880 GB — 150,000 GB — 100&nbsp;GB):</strong></li><li>5,092,880 GB * $0.05/GB =&nbsp;$254,644</li></ul><p><strong>Total Estimated AWS Egress Cost for 5 PB (Internet Transfer): Approximately $266,135</strong></p><p><strong>This is a very high cost and is why physical appliances are preferred.</strong></p><h3>How Physical Appliances Mitigate AWS Egress&nbsp;Costs</h3><p>When using a Google Cloud Data Transfer Appliance (or even AWS Snowball/Snowmobile as an intermediate step), the egress cost from AWS is significantly reduced because you’re moving data from S3 to a device  your network, rather than directly to the public internet or another cloud provider’s network.</p><p><strong>AWS Egress for Snowball/Snowmobile (Relevant for Google Appliance Loading):</strong></p><p>When moving data from S3 to an AWS Snowball or Snowmobile, there’s a charge for data transfer out, but it’s typically lower than general internet egress. For instance, data transferred out from S3 to a Snowball device might be priced around , depending on the region and specific&nbsp;service.</p><p>If you are directly loading the Google Cloud Data Transfer Appliance from S3 (which is the recommended approach), you would essentially be incurring AWS egress charges for data moving from your S3 bucket to your on-premises network where the Google appliance is connected. This would still fall under the standard internet egress tiers if not specifically covered by the AWS egress waiver program for moving data&nbsp;out.</p><p><strong>However, the critical difference is:</strong></p><ul><li><strong>Google Cloud Data Transfer Appliance:</strong> You pay for the appliance service (job fee, daily fee after a certain period, shipping). There are <strong>no per-GB egress charges from AWS to GCP for the appliance transfer itself</strong><em>once the data is loaded onto the appliance and shipped to Google</em>. The AWS egress cost is only for getting the data <em>out of AWS S3 and onto your network where the appliance is</em>. This is usually a much smaller cost if your data is “local” to your network or if you have a direct connect/VPN. If you’re copying from S3 over the internet to your datacenter to load the appliance, you still face the internet egress charges from&nbsp;AWS.</li><li> Often, people use AWS Snowball or Snowmobile to get data <em>out of AWS S3 with reduced egress fees</em>, then transfer that data from the Snow device to the Google Transfer Appliance on-premises. This effectively swaps high S3 egress to internet for lower S3 egress to a Snow&nbsp;device.</li><li><strong>AWS Snowmobile (for 5 PB):</strong> For 5 PB, AWS Snowmobile would be the equivalent AWS service. Pricing is typically , plus a daily fee and shipping. This is for . For moving data <em>out of AWS using Snowmobile</em>, you generally pay a per-GB transfer out fee, which is significantly reduced compared to internet&nbsp;egress.</li></ul><p><strong>Example Snowball Edge Storage Optimized (210TB):</strong></p><ul><li>On-Demand Job Fee (up to 100TB, includes 15 days):&nbsp;$1,800</li><li>On-Demand Job Fee (101TB to 210TB, includes 15 days):&nbsp;$3,200</li><li>On-Demand Per Day Fee (beyond 15 days):&nbsp;$250</li><li>Data Transfer OUT (from S3 to Snowball): $0.03 — $0.05 per GB (depending on&nbsp;region).</li><li>To transfer 5 PB, you would need many Snowball Edge devices or a Snowmobile. A Snowmobile can hold up to 100&nbsp;PB.</li><li><strong>AWS Snowmobile pricing is complex and requires a custom quote.</strong> It’s typically priced per GB transferred with an upfront cost. A general figure found online is around  + service fees. For a 5 PB transfer, this could still be very substantial, but likely less than direct internet&nbsp;egress.</li></ul><h3>GCP Cloud Storage Archive&nbsp;Costs</h3><p>GCP’s Archive storage class is designed for long-term, infrequently accessed&nbsp;data.</p><p><strong>Pricing (as of 2025, us-central1 region)</strong>:</p><ul><li>Storage: $0.0012/GB/month</li><li>Data retrieval: $0.05/GB (minimal for Archive class if rarely accessed)</li><li>Operations (Class A, e.g., PUT): $0.004/10,000 operations</li><li>Operations (Class B, e.g., GET): $0.0004/10,000 operations</li></ul><p><strong>Calculation for 5 Petabytes</strong>:</p><ul><li>Storage: 5,242,880 GB × $0.0012/GB/month = $6,291.46/month</li><li>Retrieval: Assume minimal access (e.g., 1% of data retrieved annually):</li><li>52,428.8 GB × $0.05/GB = $2,621.44/year</li><li>Operations: Assume 1 billion objects (rough estimate for 5&nbsp;PB):</li><li>Class A (PUT for initial transfer): 1,000,000,000 × $0.004/10,000 =&nbsp;$400</li><li>Class B (GET, minimal access): 1,000,000 × $0.0004/10,000 =&nbsp;$0.04</li></ul><p>: $6,291.46 × 12 + $2,621.44 + $400 = $78,519.96</p><ul><li>: Costs are low for storage but increase with frequent retrievals. Ensure data is truly archival.</li></ul><ul><li>: Use MD5 checksums (via aws s3api and gsutil hash) to verify data integrity. For multipart uploads in S3, ETags may not match MD5; compute MD5 locally if needed. Batch processing is critical for 5 petabytes to avoid memory&nbsp;issues.</li><li>: 5 petabytes requires significant network capacity. A dedicated interconnect (e.g., Google Cloud Interconnect or AWS Direct Connect) can improve speed and reduce&nbsp;costs.</li><li>: Beyond checksums, use Storage Transfer Service’s built-in validation and compare metadata (e.g., object sizes, timestamps) to ensure completeness.</li><li>: Plan for minimal disruption by using event-driven transfers for incremental updates.</li><li>: Encrypt data in transit (Storage Transfer Service uses HTTPS) and at rest (use GCP’s encryption options).</li><li>: Run a pilot with a small dataset (e.g., 1 TB) and validate checksums to confirm the process before&nbsp;scaling.</li></ul><h3>Recommendation for 5 PB AWS to&nbsp;GCP</h3><p><strong>The most cost-effective and practical solution for 5 PB from AWS to GCP will focus on minimizing AWS egress&nbsp;charges.</strong></p><p><strong>Primary Transfer: Google Cloud Data Transfer Appliance (or AWS Snowmobile/Snowball as a&nbsp;bridge):</strong></p><p> Request multiple Google Cloud Data Transfer Appliances. The AWS egress cost will be for downloading data from S3 to your on-premises network where you load the Google appliance. This cost is still the tiered internet egress if you’re pulling over the public internet, but it’s typically a one-time operation per appliance.</p><p><strong>Bridged Approach (often preferred for very large AWS&nbsp;data):</strong></p><ul><li>Use AWS Snowball Edge devices (or a Snowmobile for 5 PB) to move data from your AWS S3 bucket with  (e.g., $0.03-$0.05/GB) to your on-premises data&nbsp;center.</li><li>Once the data is on the AWS Snow device on-premises, transfer it from the Snow device to the <strong>Google Cloud Data Transfer Appliance</strong>. This would be a local, high-speed transfer on your network, incurring no further AWS or GCP data transfer&nbsp;fees.</li><li>Then, ship the Google Cloud Data Transfer Appliance back to&nbsp;Google.</li><li> The substantial cost savings come from avoiding the full AWS internet egress rates for 5 PB by using a physical transfer mechanism.</li></ul><p><strong>Ongoing Sync: Google Cloud Storage Transfer&nbsp;Service:</strong></p><ul><li>Once the bulk data is moved, use the <strong>Google Cloud Storage Transfer Service</strong> to manage incremental changes from your AWS S3 bucket to your new GCP Cloud Storage&nbsp;bucket.</li><li>This service is <em>free on the Google Cloud side</em> for transfers from S3, but you will still incur AWS egress charges for the smaller volume of ongoing changed&nbsp;data.</li></ul><p><strong>To get an accurate cost estimate:</strong></p><ul><li> Discuss their current programs for large-scale data migration out of AWS. They sometimes offer waivers or special pricing for large migrations to other clouds, but these are typically negotiated.</li><li><strong>Contact Google Cloud Sales:</strong> Get a quote for the Google Cloud Data Transfer Appliance for 5 PB, including all job fees and shipping.</li></ul><p>Without engaging with both cloud providers for specific quotes and potential waivers, any calculation for 5 PB will be a theoretical estimate, but the general principle holds: <strong>physical data transfer is vastly more cost-effective for petabyte-scale migrations to avoid exorbitant internet egress&nbsp;fees.</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a107634969eb\" width=\"1\" height=\"1\" alt=\"\">","contentLength":13354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"“DevOps is Dead? Long Live DevOps-Powered Platforms”","url":"https://devops.com/devops-is-dead-long-live-devops-powered-platforms/?utm_source=rss&utm_medium=rss&utm_campaign=devops-is-dead-long-live-devops-powered-platforms","date":1751115891,"author":"Alan Shimel","guid":174392,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightning Talk: Optimizing Web Applications by Offloading Heavy Processing To Kuberne... Asami Okina","url":"https://www.youtube.com/watch?v=tdW_q4ufH4k","date":1751068443,"author":"CNCF [Cloud Native Computing Foundation]","guid":174043,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nLightning Talk: Optimizing Web Applications by Offloading Heavy Processing To Kubernetes Jobs - Asami Okina, Craftsman Software, Inc.\n\nWhile typical web applications do not require large amounts of resources constantly, there are cases where specific processes consume significant CPU and memory.\n\nIn this session, we will introduce an architecture that offloads such resource-intensive processes to Kubernetes Jobs.\n\nWe will explain specific methods for Job management, how to integrate web applications (Next.js, @kubernetes/client-node) with the Kubernetes API, methods for data integration between Jobs and web applications, and real-time tracking of Job progress in the UI, all while sharing practical examples. Furthermore, we will provide a detailed introduction to a pattern where Kubernetes Job definitions generated from applications are managed using ConfigMaps, enabling quick configuration switching between environments, and offer hints to optimize your applications in terms of cost, performance, and management.</article>","contentLength":1410,"flags":null,"enclosureUrl":"https://www.youtube.com/v/tdW_q4ufH4k?version=3","enclosureMime":"","commentsUrl":null},{"title":"Spacelift Adds On-Premises Edition of Infrastructure Management Platform","url":"https://devops.com/spacelift-adds-on-premises-edition-of-infrastructure-management-platform/?utm_source=rss&utm_medium=rss&utm_campaign=spacelift-adds-on-premises-edition-of-infrastructure-management-platform","date":1751051018,"author":"Mike Vizard","guid":173884,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DataOps and Automation: The Future of Database Management","url":"https://devops.com/dataops-and-automation-the-future-of-database-management/?utm_source=rss&utm_medium=rss&utm_campaign=dataops-and-automation-the-future-of-database-management","date":1751049962,"author":"Vignyanand Penumatcha","guid":173883,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explanation First, Then Code Conversion: A Practical Guide to Mainframe Optimization","url":"https://devops.com/explanation-first-then-code-conversion-a-practical-guide-to-mainframe-optimization/?utm_source=rss&utm_medium=rss&utm_campaign=explanation-first-then-code-conversion-a-practical-guide-to-mainframe-optimization","date":1751041000,"author":"Mark Schettenhelm","guid":173814,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building an Easy Private AI Assistant with Goose and Docker Model Runner","url":"https://www.docker.com/blog/building-an-ai-assistant-with-goose-and-docker-model-runner/","date":1751040000,"author":"Oleg Selajev","guid":173844,"unread":true,"content":"<p><a href=\"https://block.github.io/goose/\" rel=\"nofollow noopener\" target=\"_blank\">Goose</a> is an innovative CLI assistant designed to automate development tasks using AI models. Docker Model Runner simplifies deploying AI models locally with Docker. Combining these technologies creates a powerful local environment with advanced AI assistance, ideal for coding and automation.</p><p>Looking for a seamless way to run AI-powered development tasks locally without compromising on privacy or flexibility? Look no further. By combining the power of<a href=\"https://github.com/block/goose\" rel=\"nofollow noopener\" target=\"_blank\"> Goose</a>, a CLI-based AI assistant, with Docker Model Runner, you get a streamlined, developer-friendly setup for running large language models right on your machine.</p><p>Docker Model Runner makes it easy to run open-source AI models with Docker, no cloud APIs or external dependencies required. And the best part? It works out of the box with tools like Goose that expect an OpenAI-compatible interface. That means you can spin up advanced local assistants that not only chat intelligently but also automate tasks, run code, and interact with your system, without sending your data anywhere else.</p><p>In this guide, you’ll learn how to build your own AI assistant with these innovative tools. We’ll walk you through how to install Goose, configure it to work with Docker Model Runner, and unleash a private, scriptable AI assistant capable of powering real developer workflows. Whether you want to run one-off commands or schedule recurring automations, this local-first approach keeps you in control and gets things done faster.</p><h2><strong>Install Goose CLI on macOS</strong></h2><p>Goose is <a href=\"https://block.github.io/goose/docs/getting-started/installation\" rel=\"nofollow noopener\" target=\"_blank\">available on Windows, macOS, and Linux</a> as a command-line tool, and also has a desktop application for macOS if that’s what you prefer. In this article, we’ll configure and show the CLI version on macOS.&nbsp;</p><p>To install Goose on you can use this handy curl2sudo oneliner technique:</p><div><pre>curl -fsSL https://github.com/block/goose/releases/download/stable/download_cli.sh | bash\n\n</pre></div><h2><strong>Enable Docker Model Runner</strong></h2><p>First, ensure you have <a href=\"https://docs.docker.com/get-docker/\" rel=\"nofollow noopener\" target=\"_blank\">Docker Desktop</a> installed. Then, configure Docker Model Runner with your model of choice. Go to  and check the checkboxes for Docker Model Runner.</p><p>By default, it’s not wired to be available from your host machine, as a security precaution, but we want to simplify the setup and enable the TCP support as well. The default port for that would be 12434, so the base URL for the connection would be: <a href=\"http://localhost:12434\" rel=\"nofollow noopener\" target=\"_blank\"></a></p><p><em>Figure 1: Docker Desktop beta features settings showing how to enable port 12434</em></p><p>Now we can pull the models from Docker Hub: <a href=\"http://hub.docker.com/u/ai\" rel=\"nofollow noopener\" target=\"_blank\">hub.docker.com/u/ai</a> and run the models. For this article, we’ll use  because it gives a good balance of world knowledge and intelligence at just 3B active parameters:&nbsp;</p><div><pre>docker model pull ai/qwen3:30B-A3B-Q4_K_M\ndocker model run ai/qwen3:30B-A3B-Q4_K_M\n</pre></div><p>This command starts the interactive chat with the model.</p><h2><strong>Configure Goose for Docker Model Runner</strong></h2><p>Edit your Goose config at <code>~/.config/goose/config.yaml</code>:</p><div><pre>GOOSE_MODEL: ai/qwen3:30B-A3B-Q4_K_M\nGOOSE_PROVIDER: openai\nextensions:\n  developer:\n    display_name: null\n    enabled: true\n    name: developer\n    timeout: null\n    type: builtin\nGOOSE_MODE: auto\nGOOSE_CLI_MIN_PRIORITY: 0.8\nOPENAI_API_KEY: irrelevant\nOPENAI_BASE_PATH: /engines/llama.cpp/v1/chat/completions\nOPENAI_HOST: http://localhost:12434\n\n</pre></div><p>The  is irrelevant as Docker Model Runner does not require authentication because the model is run locally and privately on your machine.</p><p>We provide the base path for the OpenAI compatible API, and choose the model <code>GOOSE_MODEL: ai/qwen3:30B-A3B-Q4_K_M</code> that we have pulled before.</p><p>Try Goose CLI by running goose in the terminal. You can see that it automatically connects to the correct model, and when you ask for something, you’ll see the GPU spike as well.</p><p><em>Figure 2: Goose CLI running in terminal, showing example of response to local prompts</em></p><p>Now, we also configure Goose to have the Developer extension enabled. It allows it to run various commands on your behalf, and makes it a much more powerful assistant with access to your machine than just a chat application.</p><p>You can additionally configure the custom hints to Goose to tweak its behaviour using the <a href=\"https://block.github.io/goose/docs/guides/using-goosehints//\" rel=\"nofollow noopener\" target=\"_blank\">.goosehints</a> file.</p><p>And what’s even better, you can script Goose to run tasks on your behalf with a simple one-liner:</p><p><code>goose run -t \"your instructions here\"</code> or <code>goose run -i instructions.md</code>where  is the file with what to do.</p><p>On macOS you have access to crontab for scheduling recurrent scripts, so you can automate Goose with Docker Model Runner to activate repeatedly and act on your behalf. For example, , will open the editor for the commands you want to run, and a line like the one below should do the trick:</p><div><pre>5 8 * * 1-5 goose run -i fetch_and_summarize_news.md\n</pre></div><p>Will make Goose run at 8:05 am every workday and follow the instructions in the <code>fetch_and_summarize_news.md</code> file. For example, to skim the internet and prioritize news based on what you like.</p><p>All in all, integrating Goose with Docker Model Runner creates a simple but powerful setup for using local AI for your workflows.You can make it run custom instructions for you or easily script it to perform repetitive actions intelligently.<p>It is all powered by a local model running in Docker Model Runner, so you don’t compromise on privacy either.</p></p>","contentLength":5162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Far Are We From Truly Sustainable DevOps","url":"https://devops.com/how-far-are-we-from-truly-sustainable-devops/?utm_source=rss&utm_medium=rss&utm_campaign=how-far-are-we-from-truly-sustainable-devops","date":1751016014,"author":"Alexander Williams","guid":173556,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"App Migration: How to Build a Winning Risk Management Plan","url":"https://devops.com/app-migration-how-to-build-a-winning-risk-management-plan/?utm_source=rss&utm_medium=rss&utm_campaign=app-migration-how-to-build-a-winning-risk-management-plan","date":1751013933,"author":"Roman Davydov","guid":173528,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No More Disruption: PlayStation Network’s Approaches To Avoid Outa... Tomoyuki Ehira & Shuhei Nagata","url":"https://www.youtube.com/watch?v=xxSPRdwjuqE","date":1750970957,"author":"CNCF [Cloud Native Computing Foundation]","guid":172477,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNo More Disruption: PlayStation Network’s Approaches To Avoid Outages on Kubernetes Platform - Tomoyuki Ehira &amp; Shuhei Nagata, Sony Interactive Entertainment\n\nAt PlayStation Network, our Kubernetes platform with 50+ clusters handles massive amounts of user traffic every day, and the platform team consists of engineers in several global locations with different technological and cultural backgrounds.\n\nDespite such scale and organizational complexity, we achieved remarkable stability in FY2024 so far, maintaining a notable 99.995% uptime for our platform.\n\nIn this session, we will share the key practices behind this success, including a controlled deployment strategy, robust scaling techniques, minimized manual intervention, and 24/7 operations spanning global regions. While these approaches may not be special individually, their consistent and disciplined application has been the foundation of our platform's stability.\n\nThose who strive to achieve stable platform operation and organizations looking to expand or consolidate their platforms will leave with actionable strategies to enhance the reliability of their platform.</article>","contentLength":1522,"flags":null,"enclosureUrl":"https://www.youtube.com/v/xxSPRdwjuqE?version=3","enclosureMime":"","commentsUrl":null},{"title":"New Cache Hierarchy for Container Images and OCI Artifact in Kub... Toru Komatsu & Hidehito Yabuuchi","url":"https://www.youtube.com/watch?v=trFILyK6mPw","date":1750970957,"author":"CNCF [Cloud Native Computing Foundation]","guid":172478,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNew Cache Hierarchy for Container Images and OCI Artifact in Kubernetes Clusters Using Containerd - Toru Komatsu &amp; Hidehito Yabuuchi, Preferred Networks, Inc.\n\nOne of the key bottlenecks in Kubernetes pod startup is the time taken to pull container images and OCI artifacts. It’s also costly to fetch large container images from the registry often. To tackle this problem, we developed a cache system with the following features:\n\n* New Cache Hierarchy: Images pulled by pods are shared across the entire cluster, enabling cluster-wide optimization, not only cluster-local cache.\n* Ninja: Users experience faster container image pulls without any changes on their part. Just like a ninja, the system stealthily enhances performance.\n* Preheating: It supports pushing images to preheat the cache for subsequent pulls.\n\nDeployed in a production cluster, the cache system has achieved a cache hit rate of around 95%, significantly reducing pod startup times and network communication with registries. Attendees will learn practical insights into leveraging cache and CRI to optimize image and OCI artifact pulls, ultimately enhancing cluster efficiency.</article>","contentLength":1535,"flags":null,"enclosureUrl":"https://www.youtube.com/v/trFILyK6mPw?version=3","enclosureMime":"","commentsUrl":null},{"title":"2-Node Kubernetes: A Reliable and Compatible Solution - Xin Zhang & Guang Hu, Microsoft","url":"https://www.youtube.com/watch?v=l-SlSp7Y0wE","date":1750970957,"author":"CNCF [Cloud Native Computing Foundation]","guid":172479,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\n2-Node Kubernetes: A Reliable and Compatible Solution - Xin Zhang &amp; Guang Hu, Microsoft\n\nHigh availability in Kubernetes typically requires a 3-node setup to support etcd's Raft algorithm. But what if you could achieve HA with only 2 nodes, slashing infrastructure costs by over 30% without sacrificing reliability? This is a game-changer, especially for deployments in retail and manufacturing scaling across hundreds or thousands of locations.\n\nJoin us to explore a groundbreaking 2-node HA Kubernetes solution, built by evolving the Raft algorithm for etcd. Unlike alternatives that compromise on compatibility, our approach delivers etcd-based HA, which can tolerate both node failures and network partitioning like a traditional 3-node cluster. You can seamlessly transit between 3-node and 2-node cluster utilizing standard tools like kubeadm or CAPI. This approach requires only a simple shared storage witness.\n\nIn this session, we will unpack the mechanics of this innovation, demonstrate 2-node cluster provisioning, and showcase its resilience under real-world failure scenarios.</article>","contentLength":1473,"flags":null,"enclosureUrl":"https://www.youtube.com/v/l-SlSp7Y0wE?version=3","enclosureMime":"","commentsUrl":null},{"title":"Add Single-sign-on To Your Applications With Keycloak and Learn... Takashi Norimatsu & Marek Posolda","url":"https://www.youtube.com/watch?v=jr6dV6s0fog","date":1750970957,"author":"CNCF [Cloud Native Computing Foundation]","guid":172480,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nAdd Single-sign-on To Your Applications With Keycloak and Learn About Its Latest Features - Takashi Norimatsu, Hitachi &amp; Marek Posolda, Red Hat\n\nKeycloak is an Identity and Access Management (IAM) open-source software, and CNCF incubating project.\nUse it to add single-sign-on and authentication to your applications and secure your services with minimum effort.\n\nIn the first part of this talk, we will introduce Keycloak and tell what Keycloak can do, how you can use Keycloak, where Keycloak is used and how Keycloak can resolve the issue that developers encounter when using IAM. We'll look behind the scenes how Keycloak is managed by maintainers, how you can contribute to Keycloak, which community activities are going on, and how you can participate in the activities.\n\nIn the second part of this talk, we will explain the latest updates of Keycloak and introduce new features, enhancements of existing features. We will also describe planned features and enhancements for the future.</article>","contentLength":1375,"flags":null,"enclosureUrl":"https://www.youtube.com/v/jr6dV6s0fog?version=3","enclosureMime":"","commentsUrl":null},{"title":"Breaking Limits: Highly-Isolated and Low-Overhead Wasm Container - Soichiro Ueda & Ai Nozaki","url":"https://www.youtube.com/watch?v=fV_SDqzRAG4","date":1750970957,"author":"CNCF [Cloud Native Computing Foundation]","guid":172481,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nBreaking Limits: Highly-Isolated and Low-Overhead Wasm Container - Soichiro Ueda, Kyoto University &amp; Ai Nozaki, The University of Tokyo\n\nWasm is touted as the next generation of containers, offering a smaller, more secure, and more portable application format. However, challenges remain, particularly in achieving enough isolation for public clouds where multi-tenancy exists. This is because Wasm shares the host kernel between workloads like containers. To take full advantage of Wasm, there is still insufficient discussion on this problem.\n\nTo address the issue, we've developed a new Wasm runtime, Mewz. It runs a single Wasm module within a dedicated VM while also having a lightweight and specialized kernel (unikernel). This revolutionary execution model enables more secure and low-overhead Wasm containers. We've open-sourced the implementation, and Mewz is listed in the CNCF cloud native landscape! In this session, we'll explain the architecture of Mewz and why it's more isolated and low-overhead than ordinary Wasm runtimes. Building on this presentation, let’s discuss the future of cloud workloads powered by Wasm!</article>","contentLength":1517,"flags":null,"enclosureUrl":"https://www.youtube.com/v/fV_SDqzRAG4?version=3","enclosureMime":"","commentsUrl":null},{"title":"Streamlined Baremetal Deployment: A Journey of Custom Controll... Mitsuhiro Tanino & Masanori Kuroha","url":"https://www.youtube.com/watch?v=ToXzudYQ8Es","date":1750970957,"author":"CNCF [Cloud Native Computing Foundation]","guid":172482,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nStreamlined Baremetal Deployment: A Journey of Custom Controllers Integrated With OpenStack - Mitsuhiro Tanino &amp; Masanori Kuroha, LY Corporation\n\nAs LY Corporation transitioned to developing a new private cloud infrastructure, we confronted significant challenges in managing over 6,000 baremetal servers through Kubernetes integrated with OpenStack, resulting in increased complexity, extended deployment times, and excessive resource consumption.\n\nThis session delivers how our custom controllers enhanced our approach, automating complex configurations and addressing disruptions in ArgoCD and Ansible, Kubernetes resource shortages, and scalability constraints.\n\nWe will focus on:\n\n- Challenges in Existing Baremetal Provisioning: Explore the operational complexities and inefficiencies caused by scale, including deployment delays.\n\n- Implementation of Custom Controllers: How we automated configurations and leading to faster, more reliable deployments with Helm.\n\n- Enhancements in Resource Management: Techniques that streamlined processes and enhanced OpenStack integration, ultimately boosting operational simplicity and efficiency.</article>","contentLength":1525,"flags":null,"enclosureUrl":"https://www.youtube.com/v/ToXzudYQ8Es?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Grand Adventure of Production Apps: Build, Break, and Survive! ~ A Kawaii Manga... Aoi Takahashi","url":"https://www.youtube.com/watch?v=MU_oEAuLGxQ","date":1750970957,"author":"CNCF [Cloud Native Computing Foundation]","guid":172483,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nThe Grand Adventure of Production Apps: Build, Break, and Survive! ~ A Kawaii Manga Journey Through - Aoi Takahashi, Independent\n\n\"I've started to understand the basics of Kubernetes, but when it comes to running it in production, I can't quite imagine what kind of issues might arise...\"\n\nTo ease these concerns, this session will use original characters, illustrations, and animations in a “cute manga” style to visually demonstrate how production applications can break and how to troubleshoot them.\nIn our story, the main application as a character takes center stage as the hero, venturing out on a grand journey—only to be \"suddenly attacked by a monster\" at the most inopportune moment. By following this storyline, you will learn both how applications fail and how to fix them. Additionally, just as an adventurer equips better armor to prepare for future battles, we will explore common issues and explain how to prevent them from happening in the first place.\n\nJoin us on an exciting adventure in \"manga-style troubleshooting\" and gain the confidence to tackle production Kubernetes challenges head-on!</article>","contentLength":1501,"flags":null,"enclosureUrl":"https://www.youtube.com/v/MU_oEAuLGxQ?version=3","enclosureMime":"","commentsUrl":null},{"title":"Never Underestimate Memory Architecture - Bryan Boreham, Grafana Labs","url":"https://www.youtube.com/watch?v=C6aBa1vnYT4","date":1750970956,"author":"CNCF [Cloud Native Computing Foundation]","guid":172476,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nNever Underestimate Memory Architecture - Bryan Boreham, Grafana Labs\n\nModern cloud servers are built on NUMA (Non-Uniform Memory Access) and SMT (Symmetric Multi-Threading, aka Hyperthreading) — but few engineers realize how much these technologies impact application performance.\n\nNUMA means that your cloud server’s memory might be significantly slower to access, because it’s connected to a different CPU, while Hyperthreading makes a single CPU core pretend to be two, but not at twice the speed.\n\nThis talk will:\n• Demystify NUMA &amp; Hyperthreading — what they are, how they work, and why they matter.\n• Explore Kubernetes integration—the (limited) ways Kubernetes interacts with NUMA.\n• Show real-world performance impact — illustrated with measurements on AWS and Google Cloud.\n• Give you visibility — how to use Prometheus metrics and Linux commands to view your servers’ NUMA and SMT configurations.\n\nBy the end of the session, you'll have an understanding of the issues, and the tools to measure and their impact on the performance of your workloads.</article>","contentLength":1466,"flags":null,"enclosureUrl":"https://www.youtube.com/v/C6aBa1vnYT4?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Expanding Cloud Native Ecosystem From Japan - Yuichi Nakamura","url":"https://www.youtube.com/watch?v=xqKJwkZtZRQ","date":1750970913,"author":"CNCF [Cloud Native Computing Foundation]","guid":172469,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Expanding Cloud Native Ecosystem From Japan - Yuichi Nakamura, Linux Foundation Japan Evangelist, Governing board of CNCF\n\nFinally, the first KubeCon Japan is here, but it is an only start point. Japanese communities and companies are collaborating in CNCF Japan Chapter “Cloud Native Community Japan (CNCF)” to accelerate cloud native momentum in Japan and expand collaboration with other communities like FinOps Foundation and LF AI. In the talk, achievements of CNCJ since KubeDay Japan and forecast will be introduced.</article>","contentLength":918,"flags":null,"enclosureUrl":"https://www.youtube.com/v/xqKJwkZtZRQ?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Unleashing AI Infrastructure and Platforms: Accelerating Innovatio... Sunyanan Choochotkaew","url":"https://www.youtube.com/watch?v=rd69fmAEJXo","date":1750970913,"author":"CNCF [Cloud Native Computing Foundation]","guid":172470,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Unleashing AI Infrastructure and Platforms: Accelerating Innovation Through the Open Source Ecosystem - Sunyanan Choochotkaew, Staff Research Scientist, IBM Research\n\nThis session presents how open-source ecosystems can accelerate research and innovation in AI, highlighting the use of an AI-optimized, cloud-native supercomputing platform designed for the development and training of large-scale generative models. By leveraging cloud-native infrastructure and open-source technologies, the platform supports flexible model deployment, efficient orchestration, and rapid iteration—enabling continuous optimization. Within our research department, it plays a central role in several transformative AI initiatives, including the development of open-source large language models, and contributes to a range of open-source projects such as InstructLab, Data Prep Kit, MLBatch, Kqueue, PyTorch, vLLM, Kepler, SusQL, and Multi-NIC CNI.\n\nWith cloud-native technologies essential to maximizing these benefits, this highlights how active engagement with open source—not just as consumers, but as contributors—cultivates collaboration, unleashes AI infrastructure and platforms, drives sustainable innovation, and helps strengthen the broader community.</article>","contentLength":1642,"flags":null,"enclosureUrl":"https://www.youtube.com/v/rd69fmAEJXo?version=3","enclosureMime":"","commentsUrl":null},{"title":"Safeguarding Your Applications - Achieving Zero Downtime During Kube... Kazuki Uchima & Kakeru Ishii","url":"https://www.youtube.com/watch?v=piyovtmfMWI","date":1750970913,"author":"CNCF [Cloud Native Computing Foundation]","guid":172471,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nSafeguarding Your Applications - Achieving Zero Downtime During Kubernetes Upgrades - Kazuki Uchima &amp; Kakeru Ishii, Google Cloud\n\nKubernetes cluster upgrades are frequent and can lead to unforeseen problems, including application downtime. Therefore, it is essential to understand the roles of each component that makes up a Kubernetes cluster and how they behave during an upgrade in order to safely upgrade the Kubernetes cluster and achieve zero downtime for your applications.\n\nThis session will explain the basics and practical practices to improve safety, and ensure zero downtime for your applications. In 30 minutes, we will provide an easy-to-understand explanation of everything from the roles of the main Kubernetes components to points to note during upgrades and recommended configurations to minimize or eliminate application downtime during the upgrade process. This session is recommended for those who are going to operate Kubernetes in earnest and those who are troubled by upgrades and want to learn how to prevent application downtime during Kubernetes cluster upgrades.</article>","contentLength":1473,"flags":null,"enclosureUrl":"https://www.youtube.com/v/piyovtmfMWI?version=3","enclosureMime":"","commentsUrl":null},{"title":"The Future of Prometheus Exposition Format - Arthur Sens, Grafana Labs","url":"https://www.youtube.com/watch?v=pRFUFJIAluk","date":1750970913,"author":"CNCF [Cloud Native Computing Foundation]","guid":172472,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nThe Future of Prometheus Exposition Format - Arthur Sens, Grafana Labs\n\nOpenMetrics (OM) had a wild journey: it started as a project to standardize the Prometheus exposition format, and it became an entirely separate CNCF Incubating project. Even though the project had high maturity, it struggled for years to find tools to comply with the first version of the spec. Finally, in 2025, it was incorporated back into the Prometheus Github organization so Prometheus developers could lead the efforts for OM 2.0.\n\nIn this talk, Arthur, a Prometheus maintainer and OpenMetrics contributor, will walk you through the main challenges that tools like Prometheus and OpenTelemetry face when trying to comply with OpenMetrics 1.0 and how the community plans to address these challenges in OM 2.0.\n\nThe audience will also learn how changing an exposition format can make Prometheus and OpenTelemetry-Collector more memory-efficient while making their specifications easier to translate into each other!</article>","contentLength":1376,"flags":null,"enclosureUrl":"https://www.youtube.com/v/pRFUFJIAluk?version=3","enclosureMime":"","commentsUrl":null},{"title":"Platform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platfor... Puja Abbassi","url":"https://www.youtube.com/watch?v=p-8hkNDZYF4","date":1750970913,"author":"CNCF [Cloud Native Computing Foundation]","guid":172473,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nPlatform Engineering Day 2: Why Service Iterations Are the Crux of Developer Platforms - Puja Abbassi, Giant Swarm\n\nEveryone is talking about platform engineering. You see smooth demos of golden paths and self-service platforms. However, there’s a significant area of challenges that is less talked about and thus often neglected when designing developer platforms.\n\nIn this talk, we’ll explore the often-overlooked day 2 challenges that platform teams face. We’ll dissect the area of day 2 into the many sub-areas and challenges they pose. Drawing on real-world experiences, including notable migrations that many in this community have faced, we'll shed light on the pain behind developer platforms and discuss solutions to these issues. Among others, we’ll delve into practical strategies for managing versioning and rollouts, and highlight the significant hurdles encountered, such as dependencies on end user teams or GitOps.\n\nJoin us for insights, strategies, and stories from the trenches that will help you navigate the complexities of service iteration in developer platforms.</article>","contentLength":1476,"flags":null,"enclosureUrl":"https://www.youtube.com/v/p-8hkNDZYF4?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Welcome Back + Opening Remarks","url":"https://www.youtube.com/watch?v=kM3DkcrhaqA","date":1750970913,"author":"CNCF [Cloud Native Computing Foundation]","guid":172474,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Welcome Back + Opening Remarks</article>","contentLength":422,"flags":null,"enclosureUrl":"https://www.youtube.com/v/kM3DkcrhaqA?version=3","enclosureMime":"","commentsUrl":null},{"title":"Keynote: Make Cloud-Native Ubiquitous: KubeEdge's Graduation Journey wit... Yue Bao & Hongbing Zhang","url":"https://www.youtube.com/watch?v=gbPFjRMpAEs","date":1750970913,"author":"CNCF [Cloud Native Computing Foundation]","guid":172475,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io\n\nKeynote: Make Cloud-Native Ubiquitous: KubeEdge's Graduation Journey with Innovation and Collaborative - Yue Bao, Huawei Cloud Computing Technology Co., Ltd. &amp; Hongbing Zhang, DaoCloud\n\nSustainable growth of open-source projects requires both technological advancement and collaborative growth within a diverse community. How to create a community with vendor diversity that can collectively drive technological progress is a hot topic of concern today. It is increasingly clear that establishing the right governance structure and technical roadmap is critical during a project’s evolution.\n\nKubeEdge, the industry’s first cloud-native edge computing open-source project, has grown from its launch in 2018 to achieving CNCF graduation this year. Over the past few years, KubeEdge has worked alongside multi-partners to implement practical applications in various fields, including satellite and smart vehicles, while fostering the robust development of multiple SIGs.\n\nIn this session, we will discuss the KubeEdge graduation journey, focusing on technical roadmap, community governance, and project maintenance. We will explore how to unite a diverse array of vendors within the community to advance our technological initiatives. Additionally, we will highlight the latest developments of KubeEdge in the fields of AI and robotics, aligning with current trends. Join us to explore how to build a mature, diverse, and technologically leading open-source community.</article>","contentLength":1853,"flags":null,"enclosureUrl":"https://www.youtube.com/v/gbPFjRMpAEs?version=3","enclosureMime":"","commentsUrl":null},{"title":"Secure Code Warrior Defines Security Rules for AI Coding","url":"https://devops.com/secure-code-warrior-defines-security-rules-for-ai-coding/?utm_source=rss&utm_medium=rss&utm_campaign=secure-code-warrior-defines-security-rules-for-ai-coding","date":1750965434,"author":"Mike Vizard","guid":172426,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Demystifying— Cloudbees Jenkins Migration — Part 4","url":"https://blog.devops.dev/demystifying-cloudbees-jenkins-migration-part-4-ac40b83485fa?source=rss----33f8b2d9a328---4","date":1750957261,"author":"Kiranms","guid":172347,"unread":true,"content":"<h3>Demystifying— Cloudbees Jenkins Migration — Part&nbsp;4</h3><p>So we have understand in our previous blog <strong>“Demystifying — Cloudbees Jenkins Migration — Part 3” </strong>about <strong>Configure Cloudbees Jenkins Service to run with Service&nbsp;Account.</strong></p><p>So in this we are going to talk about <strong>“Part 4&nbsp;:- Joining Cloudbees Jenkins Agent to CM &amp; CM to Operation Center.” </strong>Lets us start with <strong>Joining Cloudbees Jenkins Agent to CM &amp; CM to Operation Center. </strong>lets get into some technical action then……&nbsp;🏁🏁</p><p><em>Before we start with Joining Cloudbees Jenkins Agent to CM &amp; CM to Operation Center. You should know few things on Jenkins URL by-default firewall rules are not open to access&nbsp;, to do that you need to open the following ports to establish HTTP (8888)&nbsp;, HTTPS (8443) and Agent port (50000) connection to do so go through my HOW-TO blog at </em><a href=\"https://blog.devops.dev/how-to-open-firewall-ports-in-rhel-centos-linux-a433fd268de2\"><strong><em>“How To — Open Firewall Ports in RHEL &amp; CentOS&nbsp;Linux”</em></strong></a></p><p>Let us start with <strong>Joining Cloudbees Jenkins CM to Operation Center &amp; then Jenkins agent to Jenkins&nbsp;CM.</strong></p><p>Before that lets us understand why do we need to join Controller to Operation Center. joining a Jenkins controller to a Jenkins Operation Center brings centralized management, scalability, high availability, global configuration management, reporting, and collaboration benefits. It provides us robust framework for managing and scaling Jenkins environments.</p><ul><li>Jenkins Operation Center acts as a central management point for multiple Jenkins instances. By joining a Jenkins controller to OC&nbsp;, we can centrally manage and monitor multiple Jenkins controllers and their associated jobs, plugins, configurations, and nodes from a single interface.</li><li> OC allows us to scale our Jenkins environment efficiently. By joining a Jenkins controller to OC, we can easily add more Jenkins controllers and distribute the workload across. OC provides features like load balancing and distribution of jobs to ensure efficient resource utilization.</li><li> OC enables high availability and fault tolerance for Jenkins controllers. we can set up fail-over mechanisms and automatic backups. If a Jenkins controller fails, OC can redirect the jobs to other available controllers and ensures continuous operation and minimize downtime.</li><li><strong>Global Configuration and Policy Enforcement: </strong>OC allows you to define global configurations and policies that can be enforced across multiple Jenkins controllers.</li><li> OC provides advanced reporting and analytics capabilities for your Jenkins environment. We can access comprehensive data and metrics related to job execution, resource usage, build trends, and so&nbsp;on.</li></ul><p>Now lets us jump into to actual hands on by joining Cloudbees Operations Center to Jenkins Controller.</p><p>There are two ways you can join your OC to Controller or from Controller to&nbsp;OC.</p><h4><strong>Way 1&nbsp;: — Join from Jenkins Operation Center to Jenkins Controller</strong></h4><p>2. After clicking on “New Item” Enter item name of your controller that you want to connect to, in my case  and then click on  and then click on&nbsp;</p><p>3. After clicking on you will getting into the next page to enter some details enter your  and click on and <strong><em>Note:- In this case we will just go over default&nbsp;setting.</em></strong></p><p>4. Then you will get into the next page for Pushing the connection details to controller that we want to connect. Please enter your URL of Controller and click on </p><p>5. Then you will be getting on the next page of <strong><em>“Join Operation Center Cluster”</em></strong> here you can see OC URL&nbsp;, Agent Address&nbsp;, Agent Port and Agent Status to online, then Click on </p><p>6. Now you can see Controller is connected with Operation Center successfully in below screenshot.</p><h4><strong>Way 2: — Join from Jenkins Controller to Jenkins Operation Center.</strong></h4><ol><li>Open the Controller URL&nbsp;, In my case “<a href=\"http://jslaver8.cnl.com:8888/\">http://jslaver8.cnl.com:8888/</a>” and you will see screenPrompt there you will need click on <strong>“Join Cloudbees Operation Center”</strong></li></ol><p>2. Once you click on <strong>“Join Cloudbees Operation Center” </strong>you will need to enter the connection details form your Operation Center.</p><p>3. Go to Operation Center and Click on “New Item” Enter item name of your controller that you want to connect to, in my case  and then click on  and then click on&nbsp;</p><p>4. After clicking on you will getting into the next page to enter some details enter your  and click on and <strong><em>Note:- In this case we will just going over default&nbsp;setting.</em></strong></p><p>5. Then you will get into the next page of Pushing the connection details and Connection Details from Operation Center. Copy those  and paste in the Controller and click on&nbsp;</p><p>6. Here you will see we have join the Jenkins Controller to Operation Center successfully in other&nbsp;way.</p><h4><strong>Configure Jenkins Agent with Controller:-</strong></h4><p>Before we go ahead with configuring jenkins agent, let us understand what is Jenkins&nbsp;Agent.</p><p>Jenkins Agents aka Jenkins Slave are worker nodes that perform tasks on behalf of the Jenkins Controller aka CM. Agents are responsible for executing build and deployment process, running test, and performing automated tasks. By distributing the workload to multiple agents, Jenkins enables parallel and distributed builds, improving efficiency and scalability. That is were Jenkins Agent comes into the picture.To configure Jenkins Agent with Controller you need to follow following steps.</p><ol><li>Login to controller in my case <a href=\"http://jslaver8.cnl.com:8888/\">http://jslaver8.cnl.com:8888/</a> and go to  and then click on  and search for <strong>“SSH Agent Plugin” and then click on&nbsp;Install.</strong></li></ol><p>2. Once Installed click on <strong>“Restart Jenkins when Installation is complete and no job are running”</strong> or once the plugins installed then enter URL like below add restart to restart the jenkins controller as below screenshot and click on <a href=\"http://jslaver8.cnl.com:8888/\">http://jslaver8.cnl.com:8888/</a>restart</p><p>3. Once after Controller restart&nbsp;, login back to controller and click on  or </p><p>4. Click on “New Node” and then enter “Node Name” in my case  and click on radio button and click  to proceed&nbsp;next.</p><p>5. Then enter all the details accordingly with<strong> “Agent Name”, “Remote root directory”&nbsp;, “Labels” </strong>and select<strong> “Launch Method with Launch agent via&nbsp;SSH”.</strong></p><p>6. Then click on  the credential of agent to connect with then click on “Kind” and select <strong>“SSH Username with Private Key”</strong> to connect as permanent agent. Then click on  and enter private key in the dialog box with out space at the end and click on&nbsp;Add</p><p>8. Then in the main  screen you can see Jenkins Agent is connected successfully in the below screen&nbsp;shot.</p><p>You can see that we have successfully connected jagentr8 to Jenkins controller in the&nbsp;picture.</p><p>In the Left pane of  you can see jagentr8 is connected and in idle&nbsp;state.</p><p>You can see with same steps and process we have also joined and configure jagentr7 to Controller.</p><p>Hope you are clear on How to join Cloudbees Jenkins Controller to Operation Center and Agent to Controller.</p><p>if you like this blog please share, comment that motivates me to writes another great technological blogs.</p><p>That’s all for this blog and will see you in next <strong>“Part 5:- Installing and Configure Maven, Ant and Gradle on both Cloudbees Jenkins&nbsp;Agents”</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ac40b83485fa\" width=\"1\" height=\"1\" alt=\"\">","contentLength":6986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Demystifying — Cloudbees Jenkins Migration — Part 5","url":"https://blog.devops.dev/demystifying-cloudbees-jenkins-migration-part-5-36f78b69968e?source=rss----33f8b2d9a328---4","date":1750957252,"author":"Kiranms","guid":172346,"unread":true,"content":"<h3>Demystifying — Cloudbees Jenkins Migration — Part&nbsp;5</h3><p>So we have understand in our previous blog <strong>“Demystifying — Cloudbees Jenkins Migration — Part 4” </strong>about Attaching Cloudbees Jenkins Agent to Controller and Controller to Cloudbees Jenkins Operation Center.</p><p>So in this we are going to talk about <strong>“Part 5&nbsp;:- Installing and Configure Maven, Ant and Gradle on Cloudbees Jenkins Agent &amp; setup them on Cloudbees Jenkins Controller Web UI.</strong>Lets us start with Installing and Configure Maven, Ant and Gradle on both Cloudbees Jenkins Agentslets get into some technical action then……&nbsp;🏁🏁</p><p><em>In this blog we are only going to talk about Maven, Ant, Gradle and Ansible there are many thing to configure on actual production grade Server, To avoid the length of the blog we are just talking about Maven, Ant&nbsp;, Gradle and&nbsp;Ansible.</em></p><p>We are going to see all installation and configuration setup on RHEL8 Jenkins agent server only and Integration with Cloudbees Jenkins Controller.<strong><em>Note:- I am going to install all this tools in /opt for this demo series&nbsp;, you can install wherever you want or according to your organizational setup</em></strong></p><h4><strong>Install Maven and configure in Jenkins Controller WebUI</strong></h4><ol><li>Please copy/wget Maven package on the jagentr8 server for installation. In my case I have downloaded to my local system and then copy them over to&nbsp;agent.</li></ol><p>2. Then you need unzip Maven packages.<strong>#sudo unzip apache-maven-3.9.3-bin.zip</strong></p><p>unzip will create following directory with&nbsp;name.</p><p>3. Move maven directory to maven.<strong>#sudo mv apache-maven-3.9.3 maven</strong></p><p>4. Create maven.sh in /etc/profile.d to setup environmental variables for Maven.<strong>#sudo vi /etc/profile.d/maven.sh</strong></p><p>and enter the following variable details in the file and&nbsp;save.</p><p>Once done then you need to load the environment variables in the current shell using the following command.<strong>#source /etc/profile.d/maven.sh</strong></p><p>5. Verify Maven installation.</p><p><strong>Let us integrate maven with Jenkins Controller to execute maven job on jenkins&nbsp;agent.</strong></p><ol><li>Go to “Manage Plugins” and Install “Maven Integration plugin”.</li></ol><p>3. Then Click on “Manage Jenkins” and then Click on “Global Tool Configuration”</p><p>4. Then go to “Maven” and Click on “Add Maven” then enter required details like “Name” and “MAVEN_HOME” which installed on jenkins agents and Click on Apply/Save.</p><h4>Install Ant and configure in Jenkins Controller WebUI</h4><ol><li>Unzip Ant package in /opt.<strong>#sudo unzip apache-ant-1.10.13-bin.zip</strong></li></ol><p>2. Move apache-ant-1.10.13 to ant<strong>#sudo mv apache-ant-1.10.13 ant</strong></p><p>3. Create ant.sh in /etc/profile.d to setup environmental variables for Ant.<strong>#sudo vim /etc/profile.d/ant.sh</strong></p><p>and enter the following variable details in the file and&nbsp;save.</p><p>Once done then you need to load the environment variables in the current shell using the following command.</p><p>4. Verify Ant installation.</p><p><strong>Let us integrate ant with Jenkins Controller to execute ant on jenkins&nbsp;agent.</strong></p><ol><li>Go to “Manage Plugins” and Install “Ant plugin” and Click on&nbsp;Install.</li></ol><p>2. Then Click on “Manage Jenkins” and then Click on “Global Tool Configuration”</p><p>3. Then go to “Ant” and Click on “Add Ant” then enter required details like “Name” and “ANT_HOME” which installed on jenkins agents and Click on Apply and&nbsp;Save.</p><h4>Install Gradle and configure in Jenkins Controller WebUI</h4><ol><li>Unzip Gradle package in /opt.<strong>#sudo unzip gradle-6.3-bin.zip</strong></li></ol><p>2. Move gradle-6.3 to gradle<strong>#sudo mv gradle-6.3 gradle</strong></p><p>3. Create gradle.sh in /etc/profile.d to setup environmental variables for gradle.<strong>#sudo vim /etc/profile.d/gradle.sh</strong></p><p>Once done then you need to load the environment variables in the current shell using the following command.</p><p>4. Verify gradle installation.</p><p><strong>Let us integrate Gradle with Jenkins Controller to execute Gradle jobs from jenkins&nbsp;agent.</strong></p><ol><li>Go to “Manage Plugins” and Install “Gradle plugin” and Click on&nbsp;Install.</li></ol><p>2. Then Click on “Manage Jenkins” and then Click on “Global Tool Configuration”</p><p>3. Then go to “Gradle” and Click on “Add Gradle” then enter required details like “Name” and “GRADLE_HOME” which installed on jenkins agents and Click on Apply and&nbsp;Save.</p><h4>Install Ansible and configure in Jenkins&nbsp;WebUI</h4><p>Before you start installing Ansible you should have following requirement install on Ansible Master or Controller — Whatever you like to call 😉&nbsp;😉</p><p><strong>Ansible Controller/Master&nbsp;:- </strong>This is the machine that runs Ansible, you can use nearly any UNIX-like machine which run Python 3.9 or&nbsp;newer.</p><p>This is theachine that Ansible is managing and does not require Ansible to be installed, but requires Python 2.7, or Python 3.5–3.11 to run Ansible library code on the Ansible&nbsp;targets.</p><ol><li>Install Ansible.</li></ol><p>2. Verify Ansible Installation.</p><p><strong>Let us integrate Ansible with Jenkins Controller to execute Ansible Playbooks from jenkins&nbsp;agent.</strong></p><ol><li>Login in to <a href=\"http://jslaver8.cnl.com:8888/\">http://jslaver8.cnl.com:8888/</a> and go to “Manage Jenkins” and “Manage Plugins” search for Ansible and click on “Install without&nbsp;restart”</li></ol><p>2. Then go to “Manage Jenkins” and “Global Tool Configuration” you will see Ansible then provide the following details and “Save”. Note:- for “Path to ansible executable directory” you should provide the executable directory path where you have installed Ansible in my case I have installed on jagentr8 with /usr/bin/.</p><p>We have configured and integrated Ansible with Jenkins Controller.</p><p>Hope you are clear on How to I<strong>nstalling and Configure Maven, Ant&nbsp;, Gradle and Ansible on Cloudbees Jenkins Agent &amp; setup them on Cloudbees Jenkins Controller Web&nbsp;UI</strong>.</p><p>if you like this blog please share, comment that motivates me to writes another great technological blogs.</p><p>That’s all for this blog and will see you in next <strong>“Part 6:- SSL Configuration on Cloudbees Jenkins Operation Center and Client-Master.”</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=36f78b69968e\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How To — OpenSSL Overview","url":"https://blog.devops.dev/how-to-openssl-overview-795d07407be9?source=rss----33f8b2d9a328---4","date":1750957242,"author":"Kiranms","guid":172345,"unread":true,"content":"<h3>How To — OpenSSL&nbsp;Overview</h3><p><strong>Secure Sockets Layer (SSL) </strong>or often referred as<strong> Transport Layer Security (TLS)</strong>. It is a standard technology used to establish a secure and encrypted connection between a web browser and a web server. SSL ensures that the data transferred between the browser and server remains private and cannot be Compromised or tampered with by hackers. It provides three fundamental layers of protection for our websites.</p><ol><li> SSL encrypts the data being exchanged, so even if intercepted, it appears as unreadable gibberish to anyone without the decryption key. This protects sensitive information like login credentials, credit card details, and personal&nbsp;data.</li><li> SSL certificates verify the identity of the website. Which helps users trust that they are connecting to the intended website and not to malicious things on the internet. Valid SSL certificate shows that the website owner’s identity has authenticated by a trusted certificate authority (CA) which can be seen from the lock button from the URL&nbsp;path.</li><li> SSL ensures that the data being transferred remains intact and un-compromised during transmission.</li></ol><p>Let me give you simple example why SSL is important when you are browsing secure and important <strong>Personal Information (PI)</strong>&nbsp;data.</p><p> Let’s say Kiran wants to buy a product from an online site, and he is worried about the security of his PI data. Kiran wants to make sure that his Personal information does not expose and harm to his PI information.</p><p>Let’s see what happens without SSL and with SSL online store&nbsp;sites.</p><ol><li>Kiran visit the online store’s website, and then he notices that the URL starts with (e.g., <a href=\"http://www.examplestore.com/\">http://www.mystore.com</a>).</li><li>He add the product to cart and proceed to the checkout&nbsp;page.</li><li>When he enters his credit card details and click “Submit,” the information is sent to the store’s server in plain text. Hackers or Malicious attackers read his PI data while it is transit and can compromise the information.</li></ol><ol><li>Kiran visit the online store’s website, and he notices that the URL now starts with (e.g., <a href=\"https://www.examplestore.com/\">https://www.mystore.com</a>).</li><li>He add the product to cart and proceed to the checkout&nbsp;page.</li><li>When he enters his credit card details and click “Submit,” the information is encrypted before it is getting sent to the store’s server. This means that even if Hackers or Malicious attackers trying to exchange or compromise the data, they will only see a garbage characters. It is like sending his data in a locked box and only store’s server can unlock with is private&nbsp;key.</li><li>He also notice a padlock icon in his browser’s address bar, indicating that the connection is secure. He can click on this icon to view the SSL certificate information, which includes details about the website’s identity and the organization that issued the certificate.</li></ol><p>OpenSSL is a widely used open-source software library that provides tools and libraries for Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. It primarily focuses on cryptographic functions and secure communication over computer networks and offers various purpose to manage SSL and TLS certificates.</p><p><strong>Here are several essential functions provided by OpenSSL&nbsp;tools</strong></p><ul><li>Certificate Signing&nbsp;Request</li><li>X.509 Certificate Operations</li><li>Public Key Infrastructure (PKI) Operations</li><li>Encryption and Decryption</li><li>Digital Signature Management</li></ul><p><strong>Types of SSL Certificate&nbsp;?</strong></p><p>SSL certificates come in various types and with different extensions. Lets looks at the most common types of SSL certificates and their associated file extensions.</p><ol><li>Domain Validated (DV) Certificate:</li></ol><ul><li>Description: DV certificates are the most basic SSL certificates. They only validate the ownership of the domain and are typically the easiest and quickest to obtain. DV certificates are suitable for simple websites and&nbsp;blogs.</li></ul><p>2. Organization Validated (OV) Certificate:</p><ul><li>Description: OV certificates provide a higher level of validation than DV certificates. They verify the domain ownership and some basic organization information. These certificates are often used by small to medium-sized businesses.</li></ul><p>3. Extended Validation (EV) Certificate:</p><ul><li>Description: EV certificates offer the highest level of validation. They not only validate domain ownership and organization information but also require a more thorough verification process. Websites using EV certificates display a green address bar in most web browsers, signifying a higher level of&nbsp;trust.</li></ul><ul><li>Description: Wildcard certificates secure a domain and its subdomains with a single certificate. For example, a wildcard certificate for can be used to secure <strong>mystore.com&nbsp;, sub.mystore.com,</strong> and any other subdomains. This simplifies certificate management for websites with many subdomains.</li></ul><p>5. Multi-Domain (SAN) Certificate:</p><ul><li>Description: Multi-Domain (Subject Alternative Name or SAN) certificates allow you to secure multiple domain names with a single certificate. This is useful for websites with multiple domains or subdomains. The certificate can include various domains or subdomains within a single certificate.</li></ul><p>6. Code Signing Certificate:</p><ul><li>Description: Code signing certificates are used to digitally sign software and applications to ensure their authenticity and integrity. These certificates are crucial for developers and software publishers.</li></ul><p>7. Email (S/MIME) Certificate:</p><ul><li>Description: Email certificates are used for securing email communication, specifically for digitally signing and encrypting emails. These certificates are essential for ensuring email authenticity and&nbsp;privacy.</li></ul><p>8. SSL/TLS Certificate Chain:</p><ul><li>Description: SSL/TLS certificate chains consist of multiple certificates, including the end-entity certificate, intermediate certificates, and root certificates. These are used to establish trust in the certificate hierarchy. This will not be a separate certificate type, they play a critical role in the SSL/TLS handshake process.</li></ul><ol><li><strong>Generate the private Key (RSA)</strong><strong># openssl genrsa -out myprivatekey.pem 2048</strong></li></ol><p><strong>2. View Certificate Details</strong><strong>#openssl rsa -text -in myprivatekey.pem -noout</strong></p><p><strong>3. Extract Public Key from Private Key.</strong><strong> #openssl rsa -in myprivatekey.key -pubout -out mydomain_public.key</strong></p><p><strong>4. Creating CSR # openssl req -new -key myprivatekey.key -out mydomain.csr</strong></p><p><strong>5. Verify CSR Information# openssl req -text -in mydomain.csr -noout&nbsp;-verify</strong></p><p>6. After sending your&nbsp;.csr to Certificate Authority&nbsp;, you will receive certificate&nbsp;.crt and run the following command to verify and match with our private key with x509 command.<strong># openssl x509 -text -in mydomain.crt -noout</strong></p><p>Note: I will be creating self sign certificate for my OC and Master configuration.</p><p>This is one of the important task after you receive your key from Certificate Authority and validate the Private and Public key match with the hash&nbsp;output.</p><p><strong>#openssl pkey -pubout -in myprivatekey.key | openssl sha256#openssl req -pubkey -in mydomain.csr -noout | openssl sha256<p>#openssl x509 -pubkey -in mydomain.crt -noout | openssl&nbsp;sha256</p></strong></p><p> The above commands should be entered one by one to generate three separate&nbsp;outputs.</p><p>8. Convert Certificate formats.</p><p>#openssl pkcs12 -export -name “mydomain-key-(expiration date)” \\<p>-out mydomain.pfx -inkey mydomain.key -in mydomain.crt</p></p><ul><li><strong>Extract the private key from a PKCS#12 (.pfx) file and convert it into a PEM#</strong>openssl pkcs12 -in mydomain.pfx -nocerts -out mydomain.key -nodes</li><li><strong>Extract the certificate from a PKCS#12 (.pfx) file and convert it into a PEM</strong> #openssl pkcs12 -in mydomain.pfx -nokeys -clcerts -out mydomain.crt</li></ul><p>Hope you are clear on OpenSSL and some basic of Certificate Management part&nbsp;now.</p><p>if you like this blog please share, comment that motivates me to writes another great technological blogs.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=795d07407be9\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Demystifying — Cloudbees Jenkins Migration — Part 6","url":"https://blog.devops.dev/demystifying-cloudbees-jenkins-migration-part-6-eb2ea98366e2?source=rss----33f8b2d9a328---4","date":1750957235,"author":"Kiranms","guid":172344,"unread":true,"content":"<h3>Demystifying — Cloudbees Jenkins Migration — Part&nbsp;6</h3><p>So we have understand in our previous blog <strong>“Demystifying — Cloudbees Jenkins Migration — Part 5” </strong>about Installing and Configure Maven, Ant&nbsp;, Gradle and Ansible on Cloudbees Jenkins Agent &amp; setup them on Cloudbees Jenkins Controller Web&nbsp;UI.</p><p>So in this we are going to talk about <strong>“Part 6:- SSL Configuration on Cloudbees Jenkins Operation Center and Client-Master”</strong>Lets us start with SSL Configuration on Cloudbees Jenkins Operation Center and Client-Master or controller&nbsp;,lets get into some technical action then……&nbsp;🏁🏁</p><p>Before we start SSL configuration lets understand why do we need SSL configuration on Web&nbsp;sites</p><p><strong>Secure Sockets Layer (SSL) </strong>or often referred as<strong> Transport Layer Security (TLS)</strong>. It is a standard technology used to establish a secure and encrypted connection between a web browser and a web server. SSL ensures that the data transferred between the browser and server remains private and cannot be Compromised or tampered with by hackers. It provides three fundamental layers of protection for our websites.</p><ol><li> SSL encrypts the data being exchanged, so even if intercepted, it appears as unreadable gibberish to anyone without the decryption key. This protects sensitive information like login credentials, credit card details, and personal&nbsp;data.</li><li> SSL certificates verify the identity of the website. Which helps users trust that they are connecting to the intended website and not to malicious things on the internet. Valid SSL certificate shows that the website owner’s identity has authenticated by a trusted certificate authority (CA) which can be seen from the lock button from the URL&nbsp;path.</li><li> SSL ensures that the data being transferred remains intact and un-compromised during transmission.</li></ol><p>Lets start now with Jenkins SSL certificate configuration step by&nbsp;step.</p><p> In this blog I will show you how to create self-signed SSL certificate and configure with self-signed certificate. You should have openssl installed in your Linux/Windows server to achieve SSL certificate tasks.</p><p><strong>Create a Private Key and a Self-Signed SSL Certificate</strong></p><ol><li>Generate a Server password key.<strong>#openssl genrsa -des3 -passout pass:changeit -out server.passwd.key 2048</strong></li></ol><p>2. Generate a private key server.key<strong> # openssl rsa -passin pass:changeit -in server.passwd.key -out server.key</strong></p><p>3. Delete the password server key</p><p>Note:- This steps is not mandatory</p><p>4. Generate a server certificate and follow the prompt accordingly.<strong># openssl req -new -key server.key -out server.csr</strong></p><p>5. Generate the SSL certificate server.crt<strong>#openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt</strong></p><p>6. Create folder where you can keep all your generated certificate. For Demo purpose I am creating it under <strong>/var/lib/jenkins-oc/cert# mkdir -p /var/lib/jenkins-oc/cert<p># chown jenkins-user:jenkins cert</p></strong></p><p><strong># mv /home/jenkins-user/server.* /var/lib/jenkins-oc/cert/</strong></p><h3>Configure Operations Center to use&nbsp;SSL</h3><ol><li>Go to you OC configuration path and edit it to add JENKINS_ARGS.</li></ol><p>Edit your jenkins configuration and update following argument “JENKINS_HTTPS_PORT” and “JENKINS_ARGS”<strong>Note: we have already open the 8443 port in the filrewall&nbsp;, if you have not done this&nbsp;, do&nbsp;so.</strong></p><pre>JENKINS_HTTPS_PORT=\"8443\"JENKINS_ARGS=\"--httpsCertificate=/var/lib/jenkins-oc/cert/server.crt --httpsPrivateKey=/var/lib/jenkins-oc/cert/server.key --httpsPort=8443 --httpPort=-1\"</pre><p>2. Once you have update the configuration for SSL&nbsp;, Please restart the Jenkins-oc service.<strong>#systemctl restart jenkins-oc.service</strong></p><p>3. Please open the <a href=\"http://jmasterr8.cnl.com:8888/\">https://jmasterr8.cnl.com:8443/</a> and see you will see your jenkins oc page is not secure. means we have configured SSL successfully just we need to import certificate details in Java cacert files. you can observ the https and 8443&nbsp;port.</p><p>4. Now we need to add our OC self-signed certificate to our controller keystore with keytool command.<strong>Note: We have to convert our&nbsp;.CRT to&nbsp;.CER certificate to import in cacerts.# mv server.crt server.cer</strong></p><p><strong># keytool -import -alias server -keystore /usr/lib/jvm/jdk-11-oracle-x64/lib/security/cacerts -file server.cer</strong></p><p>5. Add the following arguments to our OC JENKINS_JAVA_OPTIONS to<strong>-Djavax.net.ssl.keyStore=/usr/lib/jvm/jdk-11-oracle-x64/lib/security/cacerts -Djavax.net.ssl.keyStorePassword=changeit</strong></p><p>6. Restart your Jenkins OC to take in effects.<strong># systemctl restart jenkins-oc.service</strong></p><p>7. Validate your SSL certificate for jmasterr8 has been configured successfully.</p><p>So we have completed <strong>jmasterr8 Operation Centre</strong> configuration with SSL lets configure jslaver8 (<a href=\"http://jslaver8.cnl.com:8888/\">http://jslaver8.cnl.com:8888/</a> ) Slave or CM to configure with&nbsp;SSL.</p><ol><li>Create <strong>cert directory to place the certificate at one location&nbsp;. you can create as you want.#mkdir -p /var/lib/cloudbees-core-cm/cert</strong></li><li>Generate a Server password key.<strong>#openssl genrsa -des3 -passout pass:changeit -out server.passwd.key 2048</strong></li></ol><p>3. Generate a private key server.key<strong> # openssl rsa -passin pass:changeit -in server.passwd.key -out server.key</strong></p><p>4. Delete the password server key</p><p>Note:- This steps is not mandatory</p><p>5. Generate a server certificate and follow the prompt accordingly.<strong># openssl req -new -key server.key -out server.csr</strong></p><p>6. Generate the SSL certificate server.crt<strong>#openssl x509 -req -days 365 -in server.csr -signkey server.key -out server.crt</strong></p><h3>Configure Client controllers to use&nbsp;SSL</h3><ol><li>Go to your CM configuration path and edit it to add following JENKINS_ARGS.</li></ol><pre>JENKINS_HTTPS_PORT=\"8443\"JENKINS_ARGS=\"--httpsCertificate=/var/lib/cloudbees-core-cm/cert/server.crt --httpsPrivateKey=/var/lib/cloudbees-core-cm/cert/server.key --httpsPort=8443 --httpPort=-1\"</pre><p>2. Once you have update the configuration for SSL&nbsp;, Please restart the Jenkins-cm service.<strong>#systemctl restart jenkins-oc.service</strong></p><p>3. Please open the <a href=\"http://jmasterr8.cnl.com:8888/\">https://jslaver8.cnl.com:8443/</a> and see you will see your jenkins oc page is not secure. means we have configured SSL successfully just we need to import certificate details in Java cacert files. you can observe the https and 8443&nbsp;port.</p><p>4. Now we need to add our CM self-signed certificate to our Slave keystore with keytool command.<strong># mv server.crt server.cer</strong></p><p><strong># keytool -import -alias server -keystore /usr/lib/jvm/jdk-11-oracle-x64/lib/security/cacerts -file server.cer</strong></p><p>5. Add the following arguments to our OC JENKINS_JAVA_OPTIONS to<strong>-Djavax.net.ssl.keyStore=/usr/lib/jvm/jdk-11-oracle-x64/lib/security/cacerts -Djavax.net.ssl.keyStorePassword=changeit</strong></p><p>6. Restart your Jenkins CM to take in effects.<strong># systemctl restart cloudbees-core-cm.service</strong></p><p>7. Validate you SSL certificate for jslaver8 has been configured successfully.</p><p><strong>Note — We are not going to cover SSL configuration for jmasterr7 and jslaver7 as it will be same&nbsp;process.</strong></p><p>We have successfully configured SSL configuration on Jmasterr8 OC and Jslaver8&nbsp;CM.</p><p>Hope you are clear on How to <strong>SSL Configuration on Cloudbees Jenkins Operation Center and Client-Master</strong></p><p>if you like this blog please share, comment that motivates me to writes another great technological blogs.</p><p>That’s all for this blog and will see you in next <strong>“Part 7:- Data Migrate from Jenkins jmasterr7 to jmasterr8.”</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=eb2ea98366e2\" width=\"1\" height=\"1\" alt=\"\">","contentLength":7057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Demystifying — Cloudbees Jenkins Migration — Part 7","url":"https://blog.devops.dev/demystifying-cloudbees-jenkins-migration-part-7-96834954a215?source=rss----33f8b2d9a328---4","date":1750957222,"author":"Kiranms","guid":172343,"unread":true,"content":"<h3>Demystifying — Cloudbees Jenkins Migration — Part&nbsp;7</h3><p>So we have understand in our previous blog <strong>“Demystifying — Cloudbees Jenkins Migration — Part 6” </strong>about <strong>SSL Configuration on Cloudbees Jenkins Operation Center and Client-Master</strong>.</p><p><strong>Note&nbsp;: My Cloudbees trial license has expired hence I am going to configure Data Migration and HA configuration on Jenkins Open-Source software.</strong></p><p>So in this we are going to talk about <strong>“Part 7&nbsp;:- Data Migration from Jenkins jmasterr7 to jmasterr8”</strong></p><p>Before we start let us understand what are the important aspect during the migration of Jenkins RHEL7 to Jenkins RHEL8. In this data migration process we need to copy necessary data from the RHEL7 existing Jenkins instance to the target RHEL8 Jenkins environment. It includes job configurations, user accounts, credentials, and plugin settings. make sure that all the file permissions and ownership are configure correctly to transferred data.</p><p>Lets look at what are the files and directories from $JENKINS_HOME are important during Data Migration and what we need them on Target so that our migration will be successful&nbsp;?</p><p>So lets us not wait and get started with Data Migration from RHEL7 Jenkins to RHEL8 Jenkinslets get into some technical action then……&nbsp;🏁🏁</p><p>Lets understand and see what are the directory and files that are there on Jenkins and what is their&nbsp;use.</p><p>These are the below files and directory on Jenkins (there will be many files and directory based on your plugins and configuration that you may need to copy over to target&nbsp;, this is just the standard migration approach).</p><p> This file “nectar-rbac.xml” is created by Jenkins Nectar Plugins and used to define RBAC (Role-Based Access Control) settings. its has all RBAC related configuration saved in this file.  config.xml file is a important configuration file that stores the configuration details of jenkins job or project. this is the XML file that stores information about job, including its name, description, SCM (Source Code Management) settings, build steps, triggers, post-build actions, and so. config.xmlstores in $JENKINS_HOME directory in the jobs folder. This files contents very sensitive information which used to store the credential as name suggest. thisfile is a configuration file that stores the credentials used by Jenkins for various purposes, such as authentication, accessing external systems, or securely storing sensitive information like usernames, passwords, API tokens, SSH keys, and certificates.This is the directory on Jenkins and “Users directory” refers to the location for user-related information such as user configurations, credentials, access control settings, and other user-specific data. This directory is very important for managing users, permissions, and security of Jenkins. This is the directory on Jenkins and “plugins directory” refers to the location where Jenkins stores all its plugins. Plugins are extensions that add functionality to Jenkins, enabling features such as source code management, build triggers, build steps, notifications, and various integrations with external tools and services.This is the directory on Jenkins&nbsp;, “jobs directory” refers to the location where Jenkins stores configuration and build data for projects. Each job in Jenkins represents a specific task, such as compiling code, running tests, or deploying applications. The job configuration includes settings like the build steps, source code management details, triggers, and post-build actions.This directory is used to store sensitive information securely, it includes passwords&nbsp;, API tokens, SSH keys and&nbsp;so.</p><p> I have already configured some sample pipelines for an example for data migration.</p><p>In the below Dashboard you can see I have already configured Sample Jobs like&nbsp;, Maven Build, Python Job and some Sample Shell script&nbsp;output.</p><p>You can see we have Workspace&nbsp;, Users&nbsp;, plugins and so on in the  directory.</p><p>You can see on RHEL8 Jenkins Dashboard we do not have any Jenkins jobs and details on the new Target RHEL8&nbsp;servers.</p><p>Lets start Data copy or Data migration from <strong>Jenkins RHEL7 to Jenkins RHEL8&nbsp;</strong>server.</p><p>Stop Jenkins Services on RHEL8 servers, where we are copying data&nbsp;over.</p><p><strong># systemctl stop jenkins# systemctl status&nbsp;jenkins</strong></p><p>From RHEL7 server start copying one by one each important directory that we have discussed.</p><p>As you can see we have copied all the the important and required Jenkins directory from RHEL7 to&nbsp;RHEL8</p><p>Lets bring up the Jenkins services on jmasterr8</p><p>Now go to Jenkins UI and check the Dashboard if you have successfully migrated data from jmasterr7 to jmasterr8.</p><p>Whoooooooo Hooooooooooo, we have migrated Jenkins from RHEL7 to RHEL8 successfully.Lets try running some of our build from jmasterr8 server to validate the migrations.</p><p>I have ran the all the jobs and which ran successfully.</p><p>We have successfully migrated data from from jmasterr7 to jmasterr8 jenkins opensource software&nbsp;version.</p><p>Hope you are clear on How to <strong>Data Migrate from Jenkins jmasterr7 to jmasterr8.</strong></p><p>if you like this blog please share, comment that motivates me to writes another great technological blogs.</p><p>That’s all for this blog and will see you in next and last technical blog of this series <strong>“Part 8:- Jenkins HA configuration.”</strong></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=96834954a215\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Demystifying — Cloudbees Jenkins Migration — Part 8","url":"https://blog.devops.dev/demystifying-cloudbees-jenkins-migration-part-8-fc2ea79e6a63?source=rss----33f8b2d9a328---4","date":1750957197,"author":"Kiranms","guid":172342,"unread":true,"content":"<h3>Demystifying — Cloudbees Jenkins Migration — Part&nbsp;8</h3><p>So we have understand in our previous blog <strong>“Demystifying — Cloudbees Jenkins Migration — Part 7” </strong>about <strong>Data Migration from Jenkins jmasterr7 to jmasterr8</strong>.</p><p><strong>Note&nbsp;: My Cloudbees trial license has expired hence I am going to configure HA configuration on Jenkins Open-Source software.</strong></p><p>So in this we are going to talk about <strong>“Part 8:- Jenkins HA configuration.”</strong></p><p>High availability (HA) is important part of application availability to ensure that application is critical component of every organization and priority is to remains available and operational even if there is any failures. Here are few reasons why do we need HA, I have listed few reasons&nbsp;, if you have more please comment in the&nbsp;section</p><ul></ul><p>In this article we are going to talk about Jenkins HA configuration and simulating the failure of the primary Jenkins node and verifying that the secondary node takes over seamlessly.</p><p>Jenkins high availability (HA) configurations done with more then one Jenkins servers are configured for redundancy and load distribution, in this typical setup we are going to store shared data which JENKINS_HOME on a network-attached storage (NAS) share (in my case will configure on jhaproxy server as NFS share) to ensure consistency and accessibility across all Jenkins instances.</p><p>OK lets start and understand how the Jenkins architecture that will talk throughout this&nbsp;blog.</p><p>This is my HAProxy Server and also act as NFS share for  directory for jenkinsha1 and jenkinsha2 instances.</p><p>This will be my Active jenkins instance which will act as&nbsp;primary.</p><p>This will be my passive jenkins instance which will act as secondary and fail-over in the event of failure of active or Primary Jenkins Instance which is jenkinsha1.</p><p>I have setup this in my virtual lab you can see in the screenshot below.</p><p>We will do NFS configuration and HAProxy installation on  server as it is acting as .</p><p>Lets start HAProxy installation on jhaproxy server, you can do it by multiple ways whichever you like. I have used my favorite yum package&nbsp;manager</p><p><strong>#yum install haproxy#dnf install&nbsp;haproxy</strong></p><p>Once you have installed HAProxy navigate to /etc/haproxy and open the haproxy.cfg and edit the following parameters to update your server&nbsp;names.</p><p>Following the  section above in the screen shot, there is a  section named which has , which is binding to port and  to the  backend&nbsp;, is logging all the system log on 127.0.0.1 at locally. The parameter has  header to the incoming requests with a value of , which is very important for applications to identify that which protocol is being&nbsp;used.</p><p>In last para, there is the  section named , that defines two  servers (jenkinsha1 and jenkinsha2) with their IP addresses and ports on my lab system. C keyword indicates that HAProxy should perform health checks on these servers, and  defines that  as a backup server, meaning it will only receive traffic if  is unavailable or not reachable.</p><p>Once all the above configuration is completed now we need to open the port 80 on the  to access the single fronted URL for jenkins and HA will act as per our configuration.</p><ol><li>Check if the port 80 is already&nbsp;open</li></ol><p><strong># firewall-cmd — list-all</strong></p><p>2. Add port 80 and then reload the firewall configuration and after that list all the&nbsp;ports.</p><p><strong># firewall-cmd — zone=public — add-port=80/tcp — permanent# firewall-cmd — reload<p># firewall-cmd — list-all</p></strong></p><p>Now lets start the HAproxy service and test the HA functionality.</p><p><strong># systemctl start haproxy# systemctl status&nbsp;haproxy</strong></p><p>Here in this configuration jhaproxy will become frontend URL for our jenkins to act as HA&nbsp;setup.</p><p>lets open jhaproxy IP address and our traffic will redirect it to active jenkins server as per our&nbsp;setup.</p><p>In this case we have jenkinsha1 acting as Active and jenkinsha2 as passive&nbsp;node.</p><p>Lets validate from the haproxy URL which host is active and passive from the jenkins script console and create some sample job or run our sample pipeline that we have and test if the passive server get updated with latest changes like build, user configuration, workspace plugins etc. and&nbsp;likewise</p><p>Let have quick check which server is primary facing to the jhaproxy HTTP&nbsp;traffic.</p><p>To check that login to your <a href=\"http://192.168.1.5:80\"></a>in my case<a href=\"http://192.168.1.5:80\"></a><strong> and navigate to “Manage Jenkins → Script Console” a</strong>nd run the following groovy command to check what is current host name for Jenkins instance that is primary facing to HAProxy&nbsp;server.</p><p><strong>println InetAddress.localHost.hostName</strong></p><p>Lets bring down the jenkins service on jenkinsha1 and see if that fail overs to our secondary passive node named jenkinsha2.</p><p>Run again the same command to check the Jenkins instance that is primary facing to HAProxy server after failover.</p><p>Now you can see in the below screenshot jhaproxy has failed over to jenkinsha2.</p><p>Now lets run some jobs here and see if those are populating to jenkinsha1 when jenkinsha2 has brought down in case of any event and all the jobs and configuration in sync with both the nodes accordingly.</p><p>Now lets bring up jenkinsha1 and bring down jenkinsha2 and lets see if our jobs and configuration are&nbsp;synced.</p><p>Let navigate to jhaproxy URL and see which is the backend jenkins server also see the updates and changes of our jenkins jobs are getting synced with jenkinsha1 and we can see all the status&nbsp;.</p><p>Wooooo Hoooo&nbsp;, we have completed testing and syncing the Jenkins jobs from Active jenkins nodes to passive nodes and wise&nbsp;versa.</p><p>That is it&nbsp;, we have successfully configured HAproxy with NFS share and tested HA from jhaproxy to jenkinsha1 and jenkinsha2 servers accordingly.</p><p>So this how we have completed our “Demystifying — Cloudbees Jenkins Migration Series”&nbsp;, Please comment and share your feedback if you&nbsp;like.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=fc2ea79e6a63\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5727,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How To — NFS Configuration","url":"https://blog.devops.dev/how-to-nfs-configuration-f284aba509cd?source=rss----33f8b2d9a328---4","date":1750957191,"author":"Kiranms","guid":172341,"unread":true,"content":"<h3>How To — NFS Configuration</h3><p>In this article we are going to configure NFS on jhaproxy server to share the JENKINS_HOME directory to be share on jenkinsha1 and jenkinsha2 servers.</p><ol><li>Install NFS utilities on jhaproxy&nbsp;server</li></ol><p>2. Create JENKINS_HOME directory</p><p><strong>#mkdir -p /var/lib/jenkins</strong></p><p>Open  and mention the following entry in the exports file to share across the&nbsp;network.</p><p><strong>#vim /etc/exports/var/lib/jenkins *(rw,sync,no_root_squash)</strong></p><p>This line allows any client (*) to mount  with read-write access.</p><p> Instead of * we can also specify specific IPs of server to get mounted NFS on it directly like below. I have showcase this with simplicity to make in&nbsp;easy.</p><p><strong>/var/lib/jenkins jenkinsha1(rw,sync,no_root_squash)/var/lib/jenkins jenkinsha2(rw,sync,no_root_squash)</strong></p><p>4. Enable and Start NFS server service.<strong>#systemctl enable nfs-server#systemctl start nfs-server</strong></p><p>5. Export NFS share and check the status of share.# exportsfs -a</p><p>6. Open NFS Services &amp; Firewall Ports on jhaproxy server.<strong># firewall-cmd — permanent — zone=public — add-service=nfs# firewall-cmd — permanent — zone=public — add-service=mountd<p># firewall-cmd — permanent — zone=public — add-service=rpc-bind</p># firewall-cmd — reload</strong></p><p>and then restart the NFS server and RPC service.<strong># systemctl enable nfs-server.service rpcbind.service# systemctl start nfs-server.service rpcbind.service</strong></p><p>Now we need to go to our NFS client which is jenkinsha1 and jenkinsha2 servers.</p><ol><li>Install NFS package on jenkinsha1 and jenkinsha2 servers.</li></ol><p>2. Create Mount point and Mount NFS share on jenkinsha1 and jenkinsha2 servers.</p><p>Check the Mount points from jenkinsha1 and jenkinsha2 servers. if they show as&nbsp;exports.</p><p># mkdir -p /var/lib/jenkins</p><p># Mount NFS share and add the entry to fstab file to make the mount point permanent.</p><p><strong># mount jhaproxy:/var/lib/jenkins /var/lib/jenkins# df -T /var/lib/jenkins/</strong></p><p><strong># mount jhaproxy:/var/lib/jenkins /var/lib/jenkins# df -T /var/lib/jenkins/</strong></p><p>You can see in above screenshot we have mounted /var/lib/jenkins from jhaproxy to jenkinsha1 and jenkinsha2 server and can see mounted as NFS4 mount&nbsp;points.</p><p>Now lets make this mount point permanent by adding them into the  files.</p><p>Open the /etc/fstab file and add the following entry to make them permanent even after reboot it will be mounted as it&nbsp;is.</p><p><strong># vim /etc/fstabjhaproxy:/var/lib/jenkins /var/lib/jenkins nfs defaults 0&nbsp;0</strong></p><p>then run the mount -a and command to see if the mount updated in&nbsp;memory.</p><p><strong>#mount -a # mount | grep -i&nbsp;jhaproxy</strong></p><p>You can just check the NFS mount point by creating sample txt or any way that would like to test NFS share on both&nbsp;server.</p><p>I am just going to create the&nbsp;.txt file on jenkinsha1 in JENKINS_HOME and will check on jenkinsha2 if that get&nbsp;created.</p><p>That's it&nbsp;, we have successfully configured NFS share on jhaproxy and mounted them on jenkinsha1 and jenkinsha2 server.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f284aba509cd\" width=\"1\" height=\"1\" alt=\"\">","contentLength":2850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop Guessing! These 15 Dockerfile Commands Are All You Need","url":"https://blog.devops.dev/stop-guessing-these-15-dockerfile-commands-are-all-you-need-4148688d3233?source=rss----33f8b2d9a328---4","date":1750957171,"author":"Devops Diaries","guid":172340,"unread":true,"content":"<div><p>Ever felt lost while writing a Dockerfile?\n&nbsp;You’re not alone. With tens of instructions, it’s very common to miss out any one of these…</p></div>","contentLength":142,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Handle Race Conditions in Ruby on Rails Applications","url":"https://blog.devops.dev/how-to-handle-race-conditions-in-ruby-on-rails-applications-755b9c66c398?source=rss----33f8b2d9a328---4","date":1750957160,"author":"Bhavesh Saluja","guid":172339,"unread":true,"content":"<div><p>Race conditions occur in Ruby on Rails applications when multiple processes or threads access shared resources concurrently, leading to…</p></div>","contentLength":138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating a Secure and Modular Terraform Pipeline Using GitHub Actions","url":"https://blog.devops.dev/creating-a-secure-and-modular-terraform-pipeline-using-github-actions-43abfa2723cf?source=rss----33f8b2d9a328---4","date":1750957144,"author":"Carlos Biagolini-Jr.","guid":172338,"unread":true,"content":"<p>This tutorial guides you through the process of building a secure and modular CI/CD pipeline using Terraform and GitHub Actions to provision AWS infrastructure. The pipeline uses OpenID Connect (OIDC) to authenticate GitHub Actions with AWS, avoiding long-lived credentials.</p><ul><li>An AWS account with permissions to manage IAM roles, S3 buckets, and DynamoDB&nbsp;tables.</li><li>A GitHub repository to host your Terraform code.</li><li>Basic knowledge of Terraform, AWS IAM, and GitHub&nbsp;Actions.</li></ul><pre>TerraformGitActionsDemo/├── module/                        # Reusable Terraform module│   └── variables.tf├── environments/                 # Environment-specific configurations│       ├── main.tf               # Uses the module and sets variables<p>│       ├── variables.tf          # Declares expected input variables</p>│       ├── terraform.tfvars      # Environment-specific values<p>│       └── providers.tf          # Backend (S3 + DynamoDB) and AWS provider config</p>│└── workflows/<p>    └── terraform.yml             # GitHub Actions CI/CD pipeline</p></pre><h3>Step 1: Create an IAM Identity Provider for&nbsp;GitHub</h3><p>In the AWS Management Console:</p><p>This allows GitHub Actions to securely assume roles in your AWS&nbsp;account.</p><h3>Step 2: Create an IAM Role for GitHub&nbsp;Actions</h3><p>In IAM &gt; Roles &gt; Create&nbsp;Role:</p><ul><li>Trusted entity type: Web&nbsp;identity</li><li>Identity provider: token.actions.githubusercontent.com</li><li>Audience: sts.amazonaws.com</li><li>Add condition: sub = repo:&lt;your-org&gt;/&lt;your-repo&gt;:ref:refs/heads/main</li></ul><p>Assign a role name such as github-pipelines-terraform-deploy. Attach a policy with permissions for S3 and DynamoDB. Example:</p><pre>{  \"Version\": \"2012-10-17\",    {      \"Action\": [        \"dynamodb:*\"      \"Resource\": \"*\"  ]</pre><p>Note the IAM Role ARN for later&nbsp;use.</p><h3>Step 3: Create Backend Resources</h3><p>Before running terraform init, manually create the S3 bucket and DynamoDB table that Terraform will use for remote state and&nbsp;locking.</p><pre># Create the S3 bucket for state storageaws s3api create-bucket \\<p>  --bucket tutorial-terraform-tfstate \\</p>  --region us-east-1 \\<p>  --create-bucket-configuration LocationConstraint=us-east-1</p><p># Create the DynamoDB table for state locking</p>aws dynamodb create-table \\<p>  --table-name terraform-lock-table \\</p>  --attribute-definitions AttributeName=LockID,AttributeType=S \\<p>  --key-schema AttributeName=LockID,KeyType=HASH \\</p>  --billing-mode PAY_PER_REQUEST \\</pre><h3>Step 4: Store Secrets in&nbsp;GitHub</h3><p>Go to <strong>Settings &gt; Secrets and Variables &gt; Actions</strong> in your GitHub repository and&nbsp;define:</p><ul><li>AWS_ROLE_ARN: IAM Role&nbsp;ARN</li><li>AWS_REGION: Example: us-east-1</li></ul><h3>Step 5: Write Terraform Configuration</h3><p>Example terraform.tfvars (in environments/dev/):</p><pre>bucket_name = \"example-dev-bucket\"</pre><p>In providers.tf, define the&nbsp;backend:</p><pre># Specify the required Terraform version and configure the remote state backendterraform {<p>  required_version = \"&gt;= 1.5.0\"</p>    bucket         = \"tutorial-terraform-tfstate\" # S3 bucket used for remote state storage<p>    key            = \"terraform/dev/s3.tfstate\"   # Path within the bucket to store the state</p>    region         = \"us-east-1\"                  # AWS region of the state bucket<p>    dynamodb_table = \"terraform-lock-table\"       # DynamoDB table used for locking</p>    encrypt        = true                         # Enable encryption for the state bucket}<p># AWS provider configuration with common default tags</p>provider \"aws\" {    tags = {      project     = \"git-actions-demo\"  }</pre><h3>Step 6: Configure GitHub Actions&nbsp;Workflow</h3><p>In&nbsp;.github/workflows/terraform.yml:</p><pre>name: 'Terraform Deploy [Dev]'  push:  deploy:<p>    name: 'Deploy Terraform to Dev'</p>    runs-on: ubuntu-latest      id-token: write      run:<p>        working-directory: environments/dev </p>      - name: 'Checkout Code'<p>        uses: actions/checkout@v3</p><p>      - name: 'Configure AWS Credentials'</p>        uses: aws-actions/configure-aws-credentials@v3          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}<p>          aws-region: ${{ secrets.AWS_REGION }}</p><p>      - name: 'Install Terraform'</p>        uses: hashicorp/setup-terraform@v3          terraform_version: 1.5.0        run: terraform init<p>      - name: 'Terraform Validate'</p>        run: terraform validate        run: terraform plan -var-file=terraform.tfvars<p>      - name: 'Terraform Apply'</p>        run: terraform apply -auto-approve -var-file=terraform.tfvars</pre><p>To run Terraform locally in the development environment:</p><pre>cd environments/devterraform initterraform plan -var-file=terraform.tfvars<p>terraform apply -auto-approve -var-file=terraform.tfvars</p></pre><p>Ensure AWS credentials are configured locally with appropriate access.</p><p>This tutorial demonstrates how to create a secure and modular Terraform pipeline using GitHub Actions and AWS OIDC integration. By separating infrastructure code, environment configurations, and state management, you achieve a scalable and maintainable IaC workflow.</p><p>If you found this guide helpful and want to learn more about , infrastructure as code, automation pipelines, AI, and cloud security, follow the author for future content and tutorials:</p><p>Stay tuned for more practical walkthroughs and solutions for cloud infrastructure and DevOps challenges.</p><p>Happy shipping with Terraform! 🚀</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=43abfa2723cf\" width=\"1\" height=\"1\" alt=\"\">","contentLength":5083,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-Driven Drift Detection in AWS: Terraform Meets Intelligence","url":"https://devops.com/ai-driven-drift-detection-in-aws-terraform-meets-intelligence/?utm_source=rss&utm_medium=rss&utm_campaign=ai-driven-drift-detection-in-aws-terraform-meets-intelligence","date":1750940338,"author":"Manvitha Potluri","guid":172145,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Technical Leadership in Modern Software Infrastructure: Architecting for Performance and Scale","url":"https://devops.com/technical-leadership-in-modern-software-infrastructure-architecting-for-performance-and-scale/?utm_source=rss&utm_medium=rss&utm_campaign=technical-leadership-in-modern-software-infrastructure-architecting-for-performance-and-scale","date":1750938996,"author":"Stephen Romain","guid":172105,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why DevSecOps Isn’t a Thing Yet","url":"https://devops.com/why-devsecops-isnt-a-thing-yet/?utm_source=rss&utm_medium=rss&utm_campaign=why-devsecops-isnt-a-thing-yet","date":1750937931,"author":"Naveen Reddy Katam","guid":172104,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Future of AI-Augmented Infrastructure: Letting AI Handle the Terraform Tax","url":"https://devops.com/the-future-of-ai-augmented-infrastructure-letting-ai-handle-the-terraform-tax/?utm_source=rss&utm_medium=rss&utm_campaign=the-future-of-ai-augmented-infrastructure-letting-ai-handle-the-terraform-tax","date":1750935085,"author":"Carlos Feliciano","guid":172067,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Change-Driven Architectures - A New Cloud-Native Model with Drasi","url":"https://www.youtube.com/watch?v=PsY1OilC3A0","date":1750921222,"author":"CNCF [Cloud Native Computing Foundation]","guid":171928,"unread":true,"content":"<article>Building change-driven solutions that respond to specific changes in distributed data is challenging.  This talk introduces Drasi, a CNCF Sandbox project that simplifies the design and implementation of change-driven architectures by codifying the continuous query and reaction patterns, removing the need to write custom code.</article>","contentLength":327,"flags":null,"enclosureUrl":"https://www.youtube.com/v/PsY1OilC3A0?version=3","enclosureMime":"","commentsUrl":null},{"title":"CNL: Integrating MCP metadata in your internal developer platform","url":"https://www.youtube.com/watch?v=MhyMLykUMUc","date":1750913422,"author":"CNCF [Cloud Native Computing Foundation]","guid":171840,"unread":true,"content":"<article>MCP, Model Context Protocol, is clearly the hot topic of 2025 and while we are seeing more and more interesting use cases around this, no one has really yet focused on all the metadata that MCP brings to the table: Tools description, Tool parameters description, prompt description. All of this is really useful information that can be used by the developer building AI Infused applications.\n\nIt’s also totally aligned with the Platform Engineering vision which tries to streamline the service catalogs to its platform user.\n\nJoin me in the mainly live coding session to see how to integrate MCP Metadata into your Platform Engineering strategy.</article>","contentLength":647,"flags":null,"enclosureUrl":"https://www.youtube.com/v/MhyMLykUMUc?version=3","enclosureMime":"","commentsUrl":null},{"title":"Amazon FSx for OpenZFS now supports Amazon S3 access without any data movement","url":"https://aws.amazon.com/blogs/aws/amazon-fsx-for-openzfs-now-supports-amazon-s3-access-without-any-data-movement/","date":1750884743,"author":"Elizabeth Fuentes","guid":170975,"unread":true,"content":"<p>Organizations store hundreds of exabytes of file data on premises and want to move this data to AWS for greater agility, reliability, security, scalability, and reduced costs. Once their file data is in AWS, organizations often want to do even more with it. For example, they want to use their enterprise data to augment <a href=\"https://aws.amazon.com/ai/generative-ai/\">generative AI</a> applications and build and train machine learning models with the broad spectrum of <a href=\"https://aws.amazon.com/ai/services/\">AWS generative AI and machine learning services</a>. They also want the flexibility to use their file data with new AWS applications. However, many AWS data analytics services and applications are built to work with data stored in Amazon S3 as data lakes. After migration, they can use tools that work with Amazon S3 as their data source. Previously, this required data pipelines to copy data between Amazon FSx for OpenZFS file systems and Amazon S3 buckets.</p><p>Amazon S3 Access Points attached to FSx for OpenZFS file systems remove data movement and copying requirements by maintaining unified access through both file protocols and <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/Type_API_Reference.html\">Amazon S3 API</a> operations. You can read and write file data using S3 object operations including <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_GetObject.html\">GetObject</a>, <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutObject.html\">PutObject</a>, and <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html\">ListObjectsV2</a>. You can attach hundreds of access points to a file system, with each S3 access point configured with application-specific permissions. These access points support the same granular permissions controls as S3 access points that attach to S3 buckets, including <a href=\"https://aws.amazon.com/iam/\">AWS Identity and Access Management (IAM)</a><a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-policies.html\">access point policies</a>, <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html\">Block Public Access</a>, and network origin controls such as restricting access to your <a href=\"https://aws.amazon.com/vpc/\">Virtual Private Cloud (VPC)</a>. Because your data continues to reside in your FSx for OpenZFS file system, you continue to access your data using <a href=\"https://aws.amazon.com/compare/the-difference-between-nfs-smb/\">Network File System</a> (NFS) and benefit from existing data management capabilities.</p><p>To monitor the creation progress, you can go to the Amazon FSx console.</p><p>Once available, choose the name of the new S3 access point and review the access point summary. This summary includes an automatically generated alias that works anywhere you would normally use S3 bucket names.</p><p>Using the bucket-style alias, you can access the FSx data directly through S3 API operations.</p><ul><li>List objects using the ListObjectsV2 API</li></ul><ul><li>Get files using the GetObject API</li></ul><ul><li>Write data using the PutObject API</li></ul><p>The data continues to be accessible via NFS.</p><p>Once the knowledge base is synchronized, I can see all documents and the  as S3.</p><p>Finally, I ran queries against the knowledge base and verified that it successfully used the file data from my Amazon FSx for OpenZFS file system to provide contextual answers, demonstrating seamless integration without data movement.</p><p><strong>Integration and access control</strong> – Amazon S3 Access Points for Amazon FSx for OpenZFS file systems support standard S3 API operations (such as GetObject, ListObjectsV2, PutObject) through the S3 endpoint, with granular access controls through AWS Identity and Access Management (IAM) permissions and file system user authentication. Your S3 Access Point includes an automatically generated access point alias for data access using S3 bucket names, and public access is blocked by default for Amazon FSx resources.</p><p> – Your data stays in your Amazon FSx for OpenZFS file system while becoming accessible as if it were in Amazon S3, eliminating the need for data movement or copies, with file data remaining accessible through NFS file protocols.</p><p> – Amazon S3 Access Points for Amazon FSx for OpenZFS file systems deliver first-byte latency in the tens of milliseconds range, consistent with S3 bucket access. Performance scales with your Amazon FSx file system’s provisioned throughput, with maximum throughput determined by your underlying FSx file system configuration.</p><p> – You’re billed by Amazon S3 for the requests and data transfer costs through your S3 Access Point, in addition to your standard Amazon FSx charges. Learn more on the <a href=\"https://aws.amazon.com/fsx/openzfs/pricing/\">Amazon FSx for OpenZFS pricing</a> page.</p><p>You can get started today using the Amazon FSx console, AWS CLI, or AWS SDK to attach Amazon S3 Access Points to your Amazon FSx for OpenZFS file systems. The feature is available in the following <a href=\"https://aws.amazon.com/about-aws/global-infrastructure/regions_az/\">AWS Regions</a>:&nbsp;US East (N. Virginia, Ohio), US West (Oregon), Europe (Frankfurt, Ireland, Stockholm), and Asia Pacific (Hong Kong, Singapore, Sydney, Tokyo).</p>","contentLength":4287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Docker State of App Dev: AI","url":"https://www.docker.com/blog/docker-state-of-app-dev-ai/","date":1750862522,"author":"Olga Diachkova","guid":170727,"unread":true,"content":"<p><strong>AI is changing software development — but not how you think</strong></p><p><em>The hype is real, but so are the challenges. Here’s what developers, teams, and tech leaders need to know about AI’s uneven, evolving role in software.</em></p><p>Rumors of AI’s pervasiveness in software development have been greatly exaggerated. A look under the hood shows <strong>adoption is far from uniform</strong>. While some dev teams are embedding AI into daily workflows, others are still kicking the tires or sitting it out entirely. Real-world usage reveals a nuanced picture shaped by industry, role, and data readiness.</p><p>Here are six key insights into AI tools and development from Docker’s second annual <strong><em>State of Application Development Survey</em></strong>, based on responses from over 4,500 industry professionals.</p><p><strong>1. How are people using AI?</strong></p><p>Right off the bat, we saw a split between two classes of respondents:&nbsp;</p><ul><li>Those who use AI tools like ChatGPT and GitHub Copilot for everyday work-related tasks such as writing, documentation, and research&nbsp;</li><li>Those who  applications with AI/ML functionality</li></ul><p><strong>IT leads the way in AI tool usage and app development&nbsp;</strong></p><p>Only about 1 in 4 respondents () report using AI tools for work. But there’s a <strong>huge spread across industries — from</strong>. Among the top AI users areIT/SaaS folks (). And because we surveyed over three times more users this year than for last year’s report, the snapshot covers a broader spectrum of industries beyond just those focused on IT.</p><p>Underscoring tech’s embrace of AI: <strong>34% of IT/SaaS respondents say they develop AI/ML apps</strong>, compared to just  outside that bubble.</p><p>And <strong>strategy reflects this gulf</strong>. Only  of companies outside IT report having a real AI strategy. Within tech, the number soars to . Translation: AI is gaining traction, but it’s concentrated in certain industries — at least for now.</p><p><strong>3. AI tools are overhyped — and incredibly useful</strong></p><p>Here’s the paradox: <strong>64% of users say AI tools make work easier</strong>, yet almost as many ()<strong> think AI tools are overhyped</strong>. The hype may be loud, but utility is speaking louder, especially for those who’ve stuck with it. In fact, <strong>65% of current users say they’re using AI more than they did a year ago</strong>, and that same percentage use it every day.</p><p>This tracks roughly with findings in our 2024 report, in which 61% of respondents agreed AI made their job easier, even as 45% reported feeling AI was overhyped. And 65% agreed that AI was a positive option.</p><p><strong>4. AI tool usage is up — and ChatGPT leads the pack</strong></p><p>No surprises here. The most-used AI-powered tools are the same as in our 2024 survey — ChatGPT (especially among full-stack developers), GitHub Copilot, and Google Gemini.&nbsp;</p><p>But usage this year far outstrips what users reported last year, with (versus 46% in our 2024 report), (versus 30%), and (versus 19%).</p><p><strong>5. Developers don’t use AI the same way</strong></p><p>The top overall use case is coding. Beyond that, it depends.</p><ul><li> turn to AI to write documentation and tests but use it sparingly.&nbsp;</li><li> use it for CLI help and writing docs.</li><li> tap AI to write tests and do research.</li></ul><p>And not all devs lean on AI equally. Seasoned devs are the least reliant, most often rating themselves as not at all dependent (0/10), while DevOps engineers rate their dependence at 7/10. Software devs are somewhere in the middle, usually landing at a 5/10 on the dependence scale. For comparison, the overall average dependence on AI in our 2024 survey was about 4 out of 10 (all users).</p><p>Looking ahead, it will be interesting to see how dependence on AI shifts and becomes further integrated by role.&nbsp;</p><p><strong>6. Data is the bottleneck no one talks about</strong></p><p>The use of AI/ML in app development is a new and rapidly growing phenomenon that, not surprisingly, brings new pain points. For teams building AI/ML apps, one headache stands out: data prep. A full <strong>of AI builders say they’re not confident</strong> in how to identify or prepare the right datasets.</p><p>Even with the right intent and tools, teams hit friction where it hurts productivity most — upfront.</p><p>We’re in the early stages of the next tech revolution — complex, fast-evolving, and rife of challenges. Developers are meeting it head-on, quickly ramping up on new tools and architectures, and driving innovation at every layer of the stack. And Docker is right there with them, empowering innovation every step of the way.</p>","contentLength":4275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deaf and Hard of Hearing WG Meeting - 2025-06-24","url":"https://www.youtube.com/watch?v=l5D5oWhczYU","date":1750860839,"author":"CNCF [Cloud Native Computing Foundation]","guid":170732,"unread":true,"content":"<article>Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon India in Hyderabad (August 6-7), and KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io</article>","contentLength":381,"flags":null,"enclosureUrl":"https://www.youtube.com/v/l5D5oWhczYU?version=3","enclosureMime":"","commentsUrl":null},{"title":"Breaking Through – Beating Mainframe Delivery Bottlenecks","url":"https://devops.com/breaking-through-beating-mainframe-delivery-bottlenecks/?utm_source=rss&utm_medium=rss&utm_campaign=breaking-through-beating-mainframe-delivery-bottlenecks","date":1750849259,"author":"Gary Thornhill","guid":170573,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Report Shows Overinflated Opinion of Infrastructure Automation Excellence","url":"https://devops.com/report-shows-overinflated-opinion-of-infrastructure-automation-excellence/?utm_source=rss&utm_medium=rss&utm_campaign=report-shows-overinflated-opinion-of-infrastructure-automation-excellence","date":1750848024,"author":"Pawel Hytry","guid":170572,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI’s Impact on Secure DevOps and the Future of Secure Software Development","url":"https://devops.com/ais-impact-on-secure-devops-and-the-future-of-secure-software-development/?utm_source=rss&utm_medium=rss&utm_campaign=ais-impact-on-secure-devops-and-the-future-of-secure-software-development","date":1750847130,"author":"Pankit Desai","guid":170571,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["devops"]}