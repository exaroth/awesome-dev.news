{"id":"6W9","title":"HN","displayTitle":"HN","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":121,"items":[{"title":"Introducing tmux-rs","url":"https://richardscollin.github.io/tmux-rs/","date":1751554994,"author":"Jtsummers","guid":183208,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44455787"},{"title":"Tools: Code Is All You Need","url":"https://lucumr.pocoo.org/2025/7/3/tools/","date":1751539878,"author":"Bogdanp","guid":183199,"unread":true,"content":"<p>If you've been following me on Twitter, you know I'm not a big fan of MCP\n(<a href=\"https://en.wikipedia.org/wiki/Model_Context_Protocol\">Model Context Protocol</a>)\nright now.  It's not that I dislike the idea; I just haven't found it to work as\nadvertised.  In my view, MCP suffers from two major flaws:</p><ol><li><strong>It isn’t truly composable.</strong>  Most composition happens through inference.</li><li><strong>It demands too much context.</strong>  You must supply significant upfront input, and\nevery tool invocation consumes even more context than simply writing and\nrunning code.</li></ol><p>A quick experiment makes this clear: try completing a GitHub task with the\nGitHub MCP, then repeat it with the  CLI tool.  You'll almost certainly\nfind the latter uses context far more efficiently and you get to your intended\nresults quicker.</p><p>I want to address some of the feedback I've received on my stance on this.  I\nevaluated MCP extensively in the context of agentic coding, where its\nlimitations were easiest to observe.  One piece of feedback is that MCP might\nnot make a ton of sense for general code generation, because models are already\nvery good at that but they make a lot of sense for end-user applications, like,\nsay, automating a domain-specific task in a financial company.  Another one is\nthat I need to look at the world of the future, where models will be able to\nreach many more tools and handle much more complex tasks.</p><p>My current take is that my data indicates that current MCP will always be harder\nto use than writing code, primarily due to the reliance on inference.  If you\nlook at the approaches today for pushing towards higher tool counts, the\nproposals all include a layer of filtering.  You pass all your tools to an LLM\nand ask it to filter it down based on the task at hand.  So far, there hasn't\nbeen much better approaches proposed.</p><p>The main reason I believe this will most likely also hold true — that you\nshouldn't be using MCP in its current form even for non-programming,\ndomain-specific tasks — is that even in those cases code generation just is the\nbetter choice because of the ability to compose.</p><h2>Replace Yourself With A Shellscript</h2><p>The way to think about this problem is that when you don't have an AI, and\nyou're solving a problem as a software engineer, your tool of choice is code.\nPerhaps as a non-software engineer, code is out of reach.  Many many tasks\npeople do by hand are actually automatable through software.  The challenge is\nfinding someone to write that software.  If you're working in a niche\nenvironment and you're not a programmer yourself, you might not pick up a\nprogramming book to learn how to code, and you might not find a developer\nwilling to provide you with a custom piece of software to solve your specific\nproblem.  And yes, maybe your task requires some inference, but many do need\nthem all the time.</p><p>There is a reason we say “to replace oneself with a shell script”, it's because\nthat has been happening for a long time.  With LLMs and programming, the idea is\nthat rather than replacing yourself with a shell script, you're replacing\nyourself with an LLM.  But you run into three problems: cost, speed, and general\nreliability.  All these problems are what we need to deal with <em>before we can\neven think of tool usage</em> or MCP.  We need to figure out how to ensure that our\nautomated task actually works correctly at scale.</p><p>The key to automation is really to automate things that will happen over and\nover.  You're not going to automate a one-shot change that will never recur.\nYou're going to start automating the things where the machine can truly give you\na productivity boost because you're going to do it once or twice, figure out how\nto make it work, and then have the machine repeat it a thousand times.  For that\nrepetition, there's a very strong argument to be made for always using code.\nThat's because if we instruct the machine to use inference to do it, it might\nwork, particularly for small tasks, but it requires validation which can take\nalmost the same time as doing it in the first place.  Getting an LLM to\ncalculate for you sort of works, but it's much better for the LLM to write the\nPython code to do the calculation.  Why?  First, you can review the formula, not\nthe calculation.  We can it ourselves or we can use the LLM as a judge to figure\nout if the  is correct.  Don't really have to validate that Python\ncalculates correct, you can rely on that.  So, by opting for code generation for\ntask solving, we get a little closer to being able to verify and validate the\nprocess ourselves, rather than hoping the LLM inferred correctly.</p><p>This obviously goes way beyond calculation.  Take, for instance, this blog.  I\nconverted this entire blob from reStructuredText to Markdown recently.  I put\nthis conversion off for a really long time, partly because I was a little too\nlazy.  But also, when I was lazy enough to consider deploying an LLM for it, I\njust didn't trust it to do the conversion itself without regressing somewhere.\nI was worried that if it ran out of context, it might start hallucinating text\nor change wording slightly.  It's just that I worried about subtle regressions\ntoo much.</p><p>I still used an LLM for it, but I asked it to do that transformation in a\ndifferent way: through code.</p><ol><li><p>I asked the LLM to perform the core transformation from reStructuredText to\nMarkdown but I also asked it to do this in a way that uses the underlying AST\n(Abstract Syntax Tree).  So, I instructed it to parse the reStructuredText\ninto an actual reStructuredText AST, then convert that to a Markdown AST, and\nfinally render it to HTML, just like it did before. This gave me an intermediate\ntransformation step and a comparable end result.</p></li><li><p>Then, I asked it to write a script that compares the old HTML with the new HTML,\nperforms the diffing after some basic cleanup it deemed necessary for\ncomparison.  I asked it to consider what kind of conversion errors were\nactually acceptable.  So, it read through its own scripts to see where it might\nnot match the original output due to known technical limitations (e.g.,\nfootnotes render differently between the Markdown library I'm using and the\nreStructuredText library, so even if the syntax matches correctly, the HTML\nwould look different).  I asked it to compensate for this in that script.</p></li><li><p>After that was done, I asked it to create a third script, which I could run\nover the output of hundreds of files to analyze the differece to go back into\nthe agentic loop for another iteration tep.</p></li></ol><p>Then I kicked off off this in a loop.  I did not provide all the posts, I\nstarted with 10 until differences were low and then had it do it for all.  It\ndid this for maybe 30 minutes or so until I came back to it and found it in a\npretty acceptable state.</p><p>What's key about this transformation is not so much that the LLM was capable of\npulling it off, but that I actually trusted this process at the end because I\ncould review the approach.  Not only that, I also tried to ask another LLM what\nit thinks of the code that another LLM wrote, and the changes.  It gave me much\nhigher confidence that what was going on would not lose data.  It felt right to\nme.  It felt like a mechanical process that was fundamentally correct, and I was\nable to observe it and do spot checks.  At worst, the regressions were minor\nMarkdown syntax errors, but the text itself wouldn't have been corrupted.</p><p>Another key here is also that because the inference is rather constant, the cost\nof inference in this process scales with the number of iteration steps and the\nsample size, but it doesn't depend on how many documents I'm wanting to convert\noverall.  Eventually, I just had it run over all documents all the time but\nrunning it over 15 docs vs 150 docs is more or less the same effort, because the\nfinal LLM based analysis step did not have that many more things to review (it\nalready skipped over all minor differences in the files).</p><p>This is a long-winded way of saying that this entire transformation went through\ncode.  It's a pipeline that starts with human input, produces code, does an LLM\nas a judge step and iterates.  And you can take this transformation and apply it\nto a general task as well.</p><p>To give an example, one MCP you might be using is Playwright.  I find it very\nhard to replace Playwright with a code approach  because what\nyou're essentially doing is remotely controlling your browser.  The task you're\ngiving it largely involves reading the page, understanding what's on it, and\nclicking the next button.  That's the kind of scenario where it's very hard to\neliminate inference at each step.</p><p>However, if you already know what the page is — for instance, if you're\nnavigating your own app you're working on — then you can actually start telling\nit to write a Playwright Python script instead and run that.  This script can\nperform many of those steps sequentially without any inference.  I've noticed\nthat this approach is significantly quicker, and because it understands your\ncode, it still generally produces correct results.  It doesn't need to navigate,\nread page contents, find a button, or press an input in real-time.  Instead, it\nwill write a single Python script that automates the entire process in one go,\nrequiring very little context by comparison. </p><p>This process is repeatable.  Once the script is written, I can execute it 100,\n200, or even 300 times without requiring any further inference.  This is a\nsignificant advantage that an MCP typically cannot offer.  It's incredibly\nchallenging to get an LLM to understand generic, abstract MCP tool calls.  I\nwish I could, for example, embed an MCP client directly into a shell script,\nallowing me to run remote MCP services efficiently via code generation, but\nactually doing that is incredibly hard because the tools are not written with\nnon inference based automation in mind.</p><p>Also, as ironic as it is: I'm a human, not an MCP client.  I can run and debug a\nscript, I cannot even figure out how to reliably do MCP calls.  It's always a\ngamble and incredibly hard to debug.  I love using the little tools that Claude\nCode generates while generating code.  Some of those I had it convert into long\nterm additions to my development process.</p><p>I don't know.  But it's an interesting moment to think what we could potentially\ndo to make code generation for purposeful agentic coding better.  The weird\nthing is that MCP is actually pretty great when it works.  But it feels in the\ncurrent form too much like a dead end that cannot be scaled up, particularly to\nautomation at scale because it relies on inference too much.</p><p>So maybe we need to look at ways to find a better abstraction for what MCP is\ngreat at, and code generation.  For that that we might need to build better\nsandboxes and maybe start looking at how we can expose APIs in ways that allow\nan agent to do some sort of fan out / fan in for inference.  Effectively we want\nto do as much in generated code as we can, but then use the magic of LLMs after\nbulk code execution to judge what we did.</p><p>I can also imagine that it might be quite interesting to do code generation in a\nway that also provides enough context for an LLM to explain in human language to\na non programmer what the script is doing.  That might enable these flows to be\nused by human users that are not developers themselves.</p><p>In any case I can only encourage people to bypass MCP and to explore what else\nis possible.  LLMs can do so much more if you give them the power to write code.</p><p>Here are some more posts you might want to read or videos you might want to\nwatch:</p><ul><li>Drew Breunig's post “<a href=\"https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html\">How to fix your context</a>”\nwhich covers some attempts to improve MCP tool selection if you cannot avoid\nit.</li><li>Manuel Odendahl's excellent “<a href=\"https://www.youtube.com/watch?v=J3oJqan2Gv8\">MCPs are Boring</a>”\ntalk from AI Engineer that was one of the first to point to the challenges\nwith MCP.</li></ul>","contentLength":11719,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44453688"},{"title":"I scanned all of GitHub's \"oops commits\" for leaked secrets","url":"https://trufflesecurity.com/blog/guest-post-how-i-scanned-all-of-github-s-oops-commits-for-leaked-secrets","date":1751525059,"author":"elza_1111","guid":182819,"unread":true,"content":"<ol><li data-preset-tag=\"p\"><p>GitHub Archive logs every public commit, even the ones developers try to delete. Force pushes often cover up mistakes like leaked credentials by rewriting Git history. GitHub keeps these dangling commits, from what we can tell, forever. In the archive, they show up as “zero-commit” .&nbsp;</p></li><li data-preset-tag=\"p\"><p>I scanned every force push event since 2020 and uncovered secrets worth .&nbsp;</p></li><li data-preset-tag=\"p\"><p>Together with Truffle Security, we're open sourcing a new tool to scan your own GitHub organization for these hidden commits (<a href=\"https://github.com/trufflesecurity/force-push-scanner\" target=\"_blank\" rel=\"noopener\">try it here</a>).</p></li></ol><p><a href=\"https://github.com/trufflesecurity/force-push-scanner\" target=\"_blank\" rel=\"noopener\"></a><em><strong>tool identifies secrets in dangling commits.</strong></em></p><p><em>This guest post by Sharon Brizinov, a white-hat hacker, was developed through </em><a href=\"https://trufflesecurity.com/cfp\" rel=\"noopener\"><em>Truffle Security’s Research CFP program</em></a><em>. We first connected with Sharon after his widely shared write-up, </em><a href=\"https://medium.com/@sharon.brizinov/how-i-made-64k-from-deleted-files-a-bug-bounty-story-c5bd3a6f5f9b\" rel=\"noopener\"><em>How I Made 64k From Deleted Files</em></a><em>, where he used TruffleHog to uncover high-value secrets in public GitHub repositories. In this follow-up, Sharon expanded his research to access 100% of deleted commits on GitHub. He takes a deeper dive into one of our favorite areas: secrets hidden in deleted GitHub commits.</em></p><ul><li data-preset-tag=\"p\"><p>What Does it Mean to Delete a Commit?</p></li><li data-preset-tag=\"p\"><p>Finding all Deleted Commits</p></li><li data-preset-tag=\"p\"><p>Hunting for Impactful Secrets</p></li><li data-preset-tag=\"p\"><p>Case Study - Preventing a Massive Supply-Chain Compromise</p></li></ul><p>My name is <a href=\"https://www.linkedin.com/in/sharonbrizinov/\" rel=\"noopener\">Sharon Brizinov</a>, and while I usually focus on low-level vulnerability and exploitation research in OT/IoT devices, I occasionally dive into bug bounty hunting.</p><p>I recently published a <a href=\"https://medium.com/@sharon.brizinov/how-i-made-64k-from-deleted-files-a-bug-bounty-story-c5bd3a6f5f9b\" rel=\"noopener\">blog post</a> about uncovering secrets hidden in dangling blobs within GitHub repositories, which sparked quite a lively discussion. After the post, I had several conversations with various people including Dylan, the CEO of Truffle Security, who gave me some intriguing ideas for continuing to explore new methods for large-scale secret hunting. I decided to create a mind map with everything I know related to this topic and try to come up with a new idea.&nbsp;</p><p>I’ll spare you my messy sketch, but here’s a roundup of the projects, blogs, ideas, and resources I zeroed in on (highly recommended):</p><ul><li data-preset-tag=\"p\"><p><a href=\"https://neodyme.io/en/blog/github_secrets/\" rel=\"noopener\">Hidden GitHub Commits and How to Reveal Them by Neodyme.io</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github\" rel=\"noopener\">Anyone can Access Deleted and Private Repository Data on GitHub by TruffleHog</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://trufflesecurity.com/blog/trufflehog-now-finds-all-deleted-and-private-commits-on-github\" rel=\"noopener\">TruffleHog now finds all Deleted &amp; Private Commits on GitHub by TruffleHog</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://trufflesecurity.com/blog/trufflehog-scans-deleted-git-branches\" rel=\"noopener\">TruffleHog Scans Deleted Git Branches by TruffleHog</a></p></li><li data-preset-tag=\"p\"><p><a href=\"https://www.aquasec.com/blog/undetected-hard-code-secrets-expose-corporations/\" rel=\"noopener\">Phantom Secrets: Undetected Secrets Expose Major Corporations by Aqua Security</a></p></li></ul><p>Eventually, I came up with a simple idea - I will use the Github Event API alongside the <a href=\"https://www.gharchive.org/\" rel=\"noopener\">GitHub Archive</a> project to scan all Zero-Commit Push-Events (deleted commits) for secrets. Everything was known, I just glued it together and built automation at scale that hunted for secrets.</p><p>In this blog, I will describe my journey from understanding why you can never really delete a commit in GitHub to how to find all of them and build automation around it.</p><h2>What Does it Mean to Delete a Commit?</h2><p>In my previous <a href=\"https://medium.com/@sharon.brizinov/how-i-made-64k-from-deleted-files-a-bug-bounty-story-c5bd3a6f5f9b\" rel=\"noopener\">blog post</a>, I discussed how I discovered supposedly deleted files within GitHub repositories. Specifically, I was able to reconstruct dangling blobs - objects that had been deleted and were no longer referenced by any commit or tree… Or so I thought. After chatting with the Truffle folks, it turns out these orphaned blobs actually had orphaned commits that went along with them. And with a little research, I was able to uncover 100% of those orphaned commits at scale, across all of GitHub.&nbsp;</p><p>Suppose you’ve accidentally committed and pushed a secret to your repository. What’s the next step? Typically, you’d want to reset the HEAD to the previous commit and force-push the changes, effectively removing the current commit and making it unreferenced - essentially deleting it. Here’s how you do it:</p><p>But as <a href=\"https://neodyme.io/en/blog/github_secrets/\" target=\"_blank\" rel=\"noopener\">neodyme </a>and <a href=\"https://trufflesecurity.com/blog/anyone-can-access-deleted-and-private-repo-data-github\" target=\"_blank\" rel=\"noopener\">TruffleHog</a> discovered, even when a commit is deleted from a repository, GitHub never forgets. If you know the full commit hash, you can access the supposedly deleted content. Moreover, you don't even need the full commit has, as TruffleHog <a href=\"https://trufflesecurity.com/blog/trufflehog-now-finds-all-deleted-and-private-commits-on-github\" target=\"_blank\" rel=\"noopener\">discovered </a>- it's enough to brute-force just the first four hex-digits.</p><h3>Force Pushing: A Tutorial</h3><p>Let’s see this in action using my own repository: <a href=\"https://github.com/SharonBrizinov/test-oops-commit\" rel=\"noopener\">test-oops-commit</a>. Try to locate the deleted commit - <a href=\"https://github.com/SharonBrizinov/test-oops-commit/commit/9eedfa00983b7269a75d76ec5e008565c2eff2ef\" rel=\"noopener\">9eedfa00983b7269a75d76ec5e008565c2eff2ef</a>.&nbsp;</p><p>To help visualize our commits, I prepared a simple bash script that shows the <a href=\"https://git-scm.com/book/en/v2/Git-Internals-Git-Objects\" target=\"_blank\" rel=\"noopener\">commit-tree-blob</a> objects, :</p><div><div><div><div><div><div><pre translate=\"no\"><code>- -- | - - | - -</code></pre></div></div></div></div></div></div><p>We start by creating a simple repository with a single commit (a  file):</p><p>Next, we create a new file named  containing our secret . We accidentally commit and push our secret to GitHub.</p><p>We look at the commit tree to see that we have a new commit … which is associated with a new tree and a new blob for the file . We see the same when we run , , or when we access it from the web on GitHub.</p><p>Oops! We discover our mistake and delete the commit by moving the HEAD of the branch to the previous commit and force-push it using:</p><p>Let's remove our local version of the repo, clone the repository again, and check the commit tree. Phew, no secrets; the commit was really deleted!</p><p>But we remember the commit hash, we we check online on GitHub and the commit can still be accessed - <a href=\"https://github.com/SharonBrizinov/test-oops-commit/commit/9eedfa00983b7269a75d76ec5e008565c2eff2ef\" rel=\"noopener\">9eedfa00983b7269a75d76ec5e008565c2eff</a> (even accessing using four hex digits is enough <a href=\"https://github.com/SharonBrizinov/test-oops-commit/commit/9eed\" target=\"_blank\" rel=\"noopener\">9eef</a>). However, this time we get a message saying that the commit is deleted or doesn't belong to any branch on this repository. </p><p>When you force-push after resetting (aka  followed by ), you remove Git’s reference to that commit from your branch, effectively making it unreachable through normal Git navigation (like ). However, the commit is still accessible on GitHub because GitHub stores these reflogs.&nbsp;</p><p>Why? I don’t know for sure, but GitHub does <a href=\"https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/what-happens-to-forks-when-a-repository-is-deleted-or-changes-visibility\" rel=\"noopener\">give some hints</a>. As I see it, GitHub is a much more complex beast than just a git server. It has many layers, including pull-requests, forks, private-public settings, and more.&nbsp;</p><p>My guess is that to support all of these features, GitHub stores all commits and never deletes them. Here are some cases to consider:</p><ol><li data-preset-tag=\"p\"><p>What are pull requests? These are just temporary branches, as <a href=\"https://www.aquasec.com/blog/undetected-hard-code-secrets-expose-corporations/\" rel=\"noopener\">Aqua Security</a> wrote about, and can be retrieved by fetching all refs using -</p><ol><li data-preset-tag=\"p\"><p><code>git -c \"remote.origin.fetch=+refs/*:refs/remotes/origin/*\" fetch origin</code></p></li></ol></li><li data-preset-tag=\"p\"><p>How does the GitHub fork network work? What happens when you “fork” a repository? All the data is replicated, including commits you might delete.</p></li></ol><p>For these cases, and probably many others too (auditing? monitoring?) Github stores all the commits and won’t delete them, even if you force-push the head and “delete” the commit.</p><p>OK, so commits are not really deleted. Fine. But you’d still need to know the full commit hash, or at least the first four hex-digits ignoring collisions (). As it turns out, TruffleHog <a href=\"https://trufflesecurity.com/blog/trufflehog-now-finds-all-deleted-and-private-commits-on-github\" rel=\"noopener\">has a tool </a>to do just that, but it’s very slow, as you can imagine, going through all those. It doesn’t scale well beyond taking a day or two on a single repo.</p><p>But there’s another way. A faster way, I’m now happy to share with you. The GitHub Event API is part of GitHub's REST API, which allows users to retrieve information about events that occur within GitHub. Events represent various activities in GitHub, such as:</p><ul><li data-preset-tag=\"p\"><p>Opening or closing issues or pull requests</p></li></ul><ul><li data-preset-tag=\"p\"><p>No API token or auth is needed!</p></li><li data-preset-tag=\"p\"><p>You can see all the events that GitHub supports <a href=\"https://docs.github.com/en/rest/using-the-rest-api/github-event-types?apiVersion=2022-11-28\" rel=\"noopener\">here</a>.</p></li><li data-preset-tag=\"p\"><p>Events are recorded in near-real-time, but may be delayed by a few seconds.</p></li><li data-preset-tag=\"p\"><p>It’s only for public repositories.</p></li></ul><p>So, we could monitor commit data for all GitHub public repositories and store all the hashes. No more guessing commit hashes! Yeah, but it’s way too much. We are talking about millions of events per hour, and what about past events? Are they lost?</p><p>Luckily for us a great developer named <a href=\"https://github.com/igrigorik\" rel=\"noopener\">Ilya Grigorik</a> decided many years ago to start a project that listens to GitHub’s event stream and systematically archives it. The project is open-source and called <a href=\"https://github.com/igrigorik/gharchive.org\" rel=\"noopener\">GH Archive</a> and the website is <a href=\"http://gharchive.org/\" rel=\"noopener\">gharchive.org</a>. So, if we want, for example, to get the entire GitHub public activity around Jan 1st at 3pm UTC we just download this from here: <a href=\"https://data.gharchive.org/2015-01-01-15.json.gz\" rel=\"noopener\">https://data.gharchive.org/2015-01-01-15.json.gz</a>.</p><p>Here is a random sample of a  from that  archive:</p><h3>Finding Force Push Deleted Commits</h3><p>To identify only the deleted commits from force push events, we can look for push events that contain zero commits. Why would a Git push event have no commits? It indicates a force push that resets the branch - essentially just moving the HEAD without adding any new commits! I call this an  or a .</p><p>Let’s see a quick example. We will download a random archive and search for such an event.</p><p>If we randomly select one of the target event types, we will see that the  array is empty (zero commits). And if we look at the  commit - the one that was “deleted” (the HEAD before moving to HEAD^1, which is the “after”) - we see that Github still holds a record of it 10 years later!</p><p>Here it is - <a href=\"https://github.com/grapefruit623/gcloud-python/commit/e9c3d31212847723aec86ef96aba0a77f9387493\" rel=\"noopener\">https://github.com/grapefruit623/gcloud-python/commit/e9c3d31212847723aec86ef96aba0a77f9387493</a></p><p>And it’s not necessarily just the  commit that was deleted. Sometimes a force push overwrites many commits at once.&nbsp;</p><p>Given a Github organization (or user), repo name, and commit hash, it’s quite easy to scan the content of the “deleted” commit(s) for secrets using Git access:</p><ul><li data-preset-tag=\"p\"><p>Clones a repo in a minimal way.</p><ul><li data-preset-tag=\"p\"><p>: Omits file contents (blobs), only history/trees/commits.</p></li><li data-preset-tag=\"p\"><p>: Doesn't check out the working directory (no files appear yet).</p></li></ul></li><li data-preset-tag=\"p\"><p>Fetches a specific commit ().</p></li><li data-preset-tag=\"p\"><p>Scans for secrets using TruffleHog.</p><ul><li data-preset-tag=\"p\"><p>TruffleHog will automatically pull down the file contents (blobs) that need to be scanned.&nbsp;</p></li><li data-preset-tag=\"p\"><p>This command will search for secrets in all commits, starting with the  commit and working backward until the start of that branch. This ensures that all data from a force push overwriting more than one commit gets scanned; however, it will scan some non-dangling commits. The <a href=\"https://github.com/trufflesecurity/force-push-scanner\" rel=\"noopener\">open-source tool</a> we’ve released is a bit more efficient and only scans the actual dangling (dereferenced) commits.</p></li></ul></li></ul><p>GitHub doesn't specify an exact rate limit for Git operations, but excessive cloning or fetching of repositories may trigger delaying or rate limiting (see <a href=\"https://github.com/orgs/community/discussions/44515\" rel=\"noopener\">here</a>).</p><p>In addition, we can use other methods to query a specific deleted/dangling commit with the GitHub API or simply with the Github web UI.</p><p>Query for the commit patch using&nbsp; GitHub’s REST API:&nbsp;</p><p><code><strong>https://api.github.com/repos/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commits/&lt;HASH&gt;</strong></code></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://api.github.com/repos/github/gitignore/commits/e9552d855c356b062ed82b83fcaacd230821a6eb\" rel=\"noopener\">https://api.github.com/repos/github/gitignore/commits/e9552d855c356b062ed82b83fcaacd230821a6eb</a></p></li></ul><p>Note: There’s a <a href=\"https://docs.github.com/en/rest/using-the-rest-api/rate-limits-for-the-rest-api?apiVersion=2022-11-28\" rel=\"noopener\">strict rate-limit</a> of 5,000 queries per hour for registered users and merely 60 for unregistered users. The server response header  indicates how many API calls users have left.</p><h4>Direct Web Access via <a href=\"https://github.com/\" rel=\"noopener\">Github.com</a></h4><p>You can also access the commit details directly from <a href=\"https://github.com/\" rel=\"noopener\">GitHub.com</a>.                                             </p><p>Here are three different examples of how to access any commit via the GitHub website:</p><p><strong>https://github.com/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commit/&lt;HASH&gt;</strong></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb\" target=\"_blank\" rel=\"noopener\">https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb</a></p></li></ul><p><strong>https://github.com/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commit/&lt;HASH&gt;.patch</strong></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.patch\" target=\"_blank\" rel=\"noopener\">https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.patch</a></p></li></ul><p><strong>https://github.com/&lt;ORG&gt;/&lt;REPO-NAME&gt;/commit/&lt;HASH&gt;.diff</strong></p><ul><li data-preset-tag=\"p\"><p><a href=\"https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.diff\" rel=\"noopener\">https://github.com/github/gitignore/commit/e9552d855c356b062ed82b83fcaacd230821a6eb.diff</a></p></li></ul><p>Although there is no documented rate limit, access is not guaranteed under heavy usage, and their WAF may block requests at any time without notice.</p><p>So we have all the ingredients - we can get all GitHub event data, search for all events, fetch the “deleted” commit (the  hash), and then scan for active secrets using TruffleHog. Let’s do this.&nbsp;</p><p>You know what? No need to build it, because together with Truffle Security’s Research team, we’re<a href=\"https://github.com/trufflesecurity/force-push-scanner\" rel=\"noopener\"> open-sourcing a new tool</a> to search the entire GH Archive for “Oops Commits” made by your GitHub organization or user account. Since the entire GH Archive is available as a <a href=\"https://www.gharchive.org/#bigquery\" rel=\"noopener\">Google Big Query public dataset</a>, this tool scans GHArchive PushEvent data for zero-commit events, fetches the corresponding commits, and scans them for secrets using TruffleHog.&nbsp;</p><p>: We are releasing this tool to help blue teamers assess their potential exposure. Please use it responsibly.</p><p>Here’s a command to get started:</p><div><div><div><div><div><div><pre translate=\"no\"><code>. --- ///. -- &lt;/</code></pre></div></div></div></div></div></div><p>For this research, I used a custom version of our open-source tool to scan all of GitHub's  since 2020. And wow. There were lots of secrets!</p><h2>Hunting for Impactful Secrets</h2><p>After running the automation, I found thousands of active secrets. But how can I identify the most interesting secrets tied to the most impactful organizations? My three-step formula for success: manual search, a vibe-coded triage tool, and AI.</p><p>First, I manually explored and manipulated the data - essentially, got my hands dirty. The automation I built stores each newly discovered secret in a well-structured JSON file. Here's an example of what one of those files looks like:</p><p>During this stage, I manually looked over the files for interesting secrets. For example, I filtered out all commits made by authors with generic email addresses (e.g. <a href=\"https://gmail.com/\" rel=\"noopener\">gmail.com</a>, <a href=\"https://outlook.com/\" rel=\"noopener\">outlook.com</a>, <a href=\"http://mail.ru/\" rel=\"noopener\">mail.ru</a>, etc)&nbsp; and focused on commits pushed by authors with a corporate email. While not perfect, it was a good start, and I found some really impactful keys.</p><p>To understand the impact of specific tokens, I tried to figure out who owns the key and what access it has using open-source tools (e.g. <a href=\"https://github.com/NikhilPanwar/secrets-ninja\" rel=\"noopener\">secrets-ninja</a>) and a few custom scripts. During my research, I learned that the Truffle Security team launched an open-source tool to do just that - <a href=\"https://github.com/trufflesecurity/trufflehog?tab=readme-ov-file#mag-analyze\" rel=\"noopener\">TruffleHog Analyze</a>. It’s built into TruffleHog; you just have to run .&nbsp;</p><p>Note: I only did this additional secret enumeration when it was in-scope for specific Bug Bounty or Vulnerability Disclosure programs.</p><p>Once I found something relevant or interesting, I reported it via a bug-bounty program or directly via email.</p><h3>Vibe Coding for Secret Triage</h3><p>After a couple hundred manual checks, I had enough and decided to scale-up my secrets review. I used vercel v0 to vibe-code a whole platform for triaging these “Oops Commit” secrets.&nbsp;</p><p>The platform was very simple. It was a front-end-only interface (no backend at all) that received a .zip file with JSON files created by the scanner. It then presented them in a very easy-to-use table so I could quickly review them and mark what I had already reviewed. This method proved very efficient, and I used a combination of filters to quickly find the hidden gems!</p><p>I also added some graphs and pie charts because why not? Looking at these graphs immediately revealed a few insights.</p><p>First, if you look at the time-series graph below, there’s clearly a direct correlation between the year and amount of  secrets - most likely because older secrets have already been revoked or expired - as they should!&nbsp;</p><p>Second, MongoDB secrets leaked the most. Based on my review of the data, this is because a lot of junior developers and CS students leaked mostly non-interesting side-project MongoDB credentials. The most interesting leaked secrets were GitHub PAT tokens and AWS credentials. These also generated the highest bounties!</p><p>Finally, I plotted the frequency of files leaking valid credentials, ahd the results are clear - your file needs extra protection!</p><p>Besides .env the most leaking filenames are: , , , , , , , , , , , , , , , , , , , , , , , ,, , , , , , , , , , , , , , , </p><p>I was quite satisfied with my vibe-coded secrets review platform. However, reviewing secrets is still a manual task. Ideally, the process should automatically resolve all secrets to extract basic information about the associated accounts wherever possible. This data could then be passed to a LLAMA-based agent that analyzes and identifies potentially valuable secrets. In essence, the goal is to build an offline agent capable of determining which secrets hold significance from a bug bounty or impact-driven perspective.</p><p>With the help of my friend <a href=\"https://il.linkedin.com/in/moti-harmats\" rel=\"noopener\">Moti Harmats</a>, I started working on it, but there’s still a lot more work to do, so I won’t release it at this time. But here’s a preview of what I started building:</p><h2>Case Study - Preventing a Massive Supply-Chain Compromise</h2><p>One of the secrets I found in a deleted commit was a <a href=\"https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens\" rel=\"noopener\">GitHub Personal Access Token</a> (PAT) belonging to a developer. The developer accidentally leaked this secret when they committed their hidden configuration files (dot files). I analyzed this token and found it had admin access to ALL of <a href=\"https://github.com/istio/istio\" rel=\"noopener\">Istio</a> repositories.</p><p>Istio is an open-source service mesh that provides a transparent and language-independent way to flexibly and easily automate application network functions. It is designed to manage the communication between microservices in a distributed application, offering features such as traffic management, security, and observability without requiring changes to the application code.</p><p>The main <a href=\"https://github.com/istio/istio\" rel=\"noopener\">Istio</a> project has  stars and  forks. Istio is used by a wide range of organizations and teams that run complex, distributed applications, especially those adopting microservices architectures. This includes giant corporations like Google, IBM, Red Hat and many others.</p><p>And I had ADMIN level access to ALL of Itsio repositories (<a href=\"https://github.com/istio/\" rel=\"noopener\">there are many of them</a>). I could have read environment variables, changed pipelines, pushed code, created new releases, or even deleted the entire project. The potential for a mass supply-chain attack here was scary.&nbsp;</p><p>Fortunately, Istio has a well-maintained <a href=\"https://istio.io/latest/docs/releases/security-vulnerabilities/\" rel=\"noopener\">report page</a>, and the team acted quickly to revoke the GitHub PATs as soon as the issue was reported. Thank you!</p><p>This was a really fun project. I glued together some known discoveries and was able to create a reliable automation that scanned and found thousands of active secrets, even some that were buried for years. I also got the chance to vibe code a secret hunting platform with some nice features that allowed me to find needles in a haystack and earn approximately $25k of bounties and deep-thanks through the process.</p><p>The common assumption that deleting a commit is secure must change - once a secret is committed it should be considered compromised and must be revoked ASAP. It’s true for git blobs, git commits, and anything else that goes online.</p>","contentLength":18035,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44452317"},{"title":"Astronomers discover 3I/ATLAS – Third interstellar object to visit Solar System","url":"https://www.abc.net.au/news/science/2025-07-03/3i-atlas-a11pl3z-interstellar-object-in-our-solar-system/105489180","date":1751512764,"author":"gammarator","guid":182735,"unread":true,"content":"<p>For only the third time in history, astronomers have discovered a new interstellar object that originated from outside our Solar System.</p><p>The object, known as 3I/ATLAS, is likely a comet and is much faster than any other interstellar object found before.&nbsp;</p><p>The object appears to be hurtling towards our Sun at about 60 kilometres a second.&nbsp;</p><p>\"This thing is coming in at such an incredible speed that absolutely nothing in the Solar System could have caused this,\" Jonti Horner, an astronomer at the University of Southern Queensland, said.&nbsp;</p><div data-component=\"EmphasisedText\"><blockquote>\"Of the three interstellar objects we have seen, this is by far the fastest.\"</blockquote></div><p>There are only two other interstellar objects that have previously beentracked entering our Solar System — <a href=\"https://www.abc.net.au/news/2023-03-23/scientists-offer-non-alien-explanation-for-oumuamua/102138318\" data-component=\"Link\">'Oumuamua</a> and <a href=\"https://www.abc.net.au/news/science/2019-11-16/interstellar-comet-2i-borisov-excites-astronomers/11691940\" data-component=\"Link\">Comet 2I/Borisov.</a></p><p>\"It's so exciting,\" Professor Horner added.</p><p>Rumblings of the potential interstellar object started in astronomy groups&nbsp;when the object was first detected early this week.</p><p>\"It has been picked up so early — relatively speaking — that we've got at least eight months [during which]we'll be able to observe it,\" he said.</p><p>The object was first detected by the ATLAS telescope in Chile on 1 July.</p><p>Follow-up observations confirmed the orbit was extremely unusual — almost unaffected by the Sun's gravity, Professor Horner said.&nbsp;</p><div data-component=\"EmphasisedText\"><blockquote>\"Plotting the orbit of this thing [shows] it barely bends as it goes past the Sun.\"</blockquote></div><figure data-print=\"inline-media\" data-component=\"Figure\" data-uri=\"coremedia://imageproxy/105489798\"><figcaption><p data-component=\"Typography\">The trajectory of interstellar comet 3I/ATLAS as it passes through the Solar System.<cite>()</cite></p></figcaption></figure><p>But it wasn't until yesterday that scientists at the <a href=\"https://minorplanetcenter.net/mpec/K25/K25N12.html\" data-component=\"Link\">Minor Planet Centre</a> in the USconfirmed the object was an interstellar object.</p><p>They also suggested the object was likely a comet, due to images that showed it had a short tail.</p><p>More observations will need to be done to confirm this, and get more detail about the object.</p><p>However, because 3I/ATLAS has been found so early, astronomers will have plenty of time to track it as it moves through the Solar System.</p><h2 data-component=\"Heading\">When will 3I/ATLAS be closest to the Sun?</h2><p>Currently, estimates suggest it will be closest to the Sun at the end of October, before returning out past Jupiter and into the outer Solar System by March next year.</p><p>Unfortunately, Earth will be on the other side of the Solar System when 3I/ATLAS is closest to the Sun and at its brightest, making it harder for us to see.</p><p>\"If we were on Mars, we'd have a fairly good view of it,\" Professor Horner said.</p><p>\"It's not going to be hugely close to Mars, but it's going to be a lot closer to Mars than it will to the Earth.\"</p><p>Because 3I/ATLAS might currently be going through an outburst — a sudden brightening caused by dust and gas being released by the object — it's difficult to track its size.</p><figure data-print=\"inline-media\" data-component=\"Figure\" data-uri=\"coremedia://imageproxy/105489848\"><figcaption><p data-component=\"Typography\">'Oumuamua was in a \"cigar\" shape, making it much less bright.&nbsp;<cite>()</cite></p></figcaption></figure><p>'Oumuamua was quite a small object, and estimates on the size of 2I/Borisov ranged from about 1 kilometre to more than 16km in diameter.</p><p>\"I would say this is probably more along the lines of a few hundred metres to a kilometre across, maybe a bit bigger than that,\" Professor Horner said.&nbsp;</p><p>\"Which is big, but not exceptional.\"</p><h2 data-component=\"Heading\">Will we see more interstellar objects?&nbsp;</h2><p>Interstellar objects have been extremely rare so far, but with better telescopes like the Rubin Observatory, we're likely to catch many more of these objects when they arrive.&nbsp;</p><p>\"We've had three [interstellar objects] in less than a decade with our current technology,\" Professor Horner said.&nbsp;</p><p>\"The Rubin Observatory is probably an order of magnitude better at finding things … so that would suggest we'll find a few of these per year.\"</p><p>Within its first 10 hours of operation the observatory detected more than 2,000 previously unknown asteroids in the Solar System.&nbsp;</p><div data-component=\"EmphasisedText\"><blockquote>\"It's kind of a sneak peek into the future.\"</blockquote></div>","contentLength":3679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44451329"},{"title":"Trans-Taiga Road (2004)","url":"https://www.jamesbayroad.com/ttr/index.html","date":1751504824,"author":"jason_pomerleau","guid":182591,"unread":true,"content":"<p align=\"left\">The Trans-Taiga Road is a gravel road that runs 666 km east \nfrom near the top end of the <a href=\"https://www.jamesbayroad.com/jbr/index.html\"> James Bay Road</a>. It \nwas built to access the various dams and generating stations that extend upriver \nalong the La Grande River.</p><p align=\"left\">This is an  road, leading 666 km \neast - almost to Labrador - with no settlements or towns aside from Hydro \nQuebec's settlements for workers (not open to the public). At the far end you \nwill be 745 km from the nearest town! This is the farthest you can get from a \ntown on a road in North America! (Not counting the private Hydro Quebec towns \nthat are not open to the public). Along this road is also the\n<a href=\"https://www.jamesbayroad.com/ttr/virtualtour/ttrvirtualtour08.html#km542\">farthest north</a> point you \ncan travel on a road in eastern Canada.</p><p align=\"left\">The road from <a href=\"https://www.jamesbayroad.com/ttr/brisay/index.html\">Brisay</a> (km 582) \nto <a href=\"https://www.jamesbayroad.com/ttr/caniapiscau/index.html\">Caniapiscau</a> is rougher and a 4-wheel \ndrive vehicle is recommended by Hydro Quebec. The main reason for this is the \nvery coarse gravel used for this road - there's large rocks littering the road \nsurface. However, I have heard from people who have driven this road in ordinary \npassenger cars and they say it is fine. You do have to keep your eyes open for \nthe larger rocks though.</p><p align=\"left\">You should definitely travel this road only in a reliable \nvehicle with good tires. It is not an overly rough road; passenger vehicles can \ndrive it, but it is gravel. Vehicle breakdowns here can be very costly. Flat \ntires can be a serious (and expensive) incident if your tires are damaged. You \ncould be looking at having tires flown in on a non-scheduled flight - there are \nno convenient \"tire stores\" up here! Please read\n<a href=\"https://www.jamesbayroad.com/ttr/ttrdriving.html\">Driving the Trans-Taiga Road</a> first.</p><p align=\"left\">Although there are no towns, there are a couple of outfitters \nalong the way that sell fuel and offer meals and lodging. Cell phones do not \nwork here.</p><p align=\"left\">Generally the scenery is fairly level, but this road is \ndefinitely more interesting than the <a href=\"https://www.jamesbayroad.com/jbr/index.html\">James Bay Road</a>.&nbsp;For \nmost of the length it runs through taiga: spruce and jack pine forest, bogs, \nrocks, and low hills.&nbsp;This is about all you'll see apart from birds and some \nwildlife, the occasional cabin a short distance off the road, and Hydro Quebec \ninstallations. I once saw a couple of wolves playing in the middle of this road.</p><table border=\"0\" width=\"100%\"><tbody><tr><td><ul></ul></td></tr></tbody></table>","contentLength":2156,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44450575"},{"title":"Whole-genome ancestry of an Old Kingdom Egyptian","url":"https://www.nature.com/articles/s41586-025-09195-5","date":1751502286,"author":"A_D_E_P_T","guid":182734,"unread":true,"content":"<li data-counter=\"1.\"><p>Garstang, J. in <i>The Burial Customs of Ancient Egypt</i> (ed. Garstang, J.) 26–30 (Archibald Constable, 1907).</p></li><li data-counter=\"3.\"><p>Skourtanioti, E. et al. Genomic history of Neolithic to Bronze Age Anatolia, Northern Levant, and Southern Caucasus. , 1158–1175.e28 (2020).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.cell.2020.04.044\" data-track-item_id=\"10.1016/j.cell.2020.04.044\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.cell.2020.04.044\" aria-label=\"Article reference 3\" data-doi=\"10.1016/j.cell.2020.04.044\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXhtVKnurbN\" aria-label=\"CAS reference 3\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32470401\" aria-label=\"PubMed reference 3\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 3\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Genomic%20history%20of%20Neolithic%20to%20Bronze%20Age%20Anatolia%2C%20Northern%20Levant%2C%20and%20Southern%20Caucasus&amp;journal=Cell&amp;doi=10.1016%2Fj.cell.2020.04.044&amp;volume=181&amp;pages=1158-1175.e28&amp;publication_year=2020&amp;author=Skourtanioti%2CE\">\n                    Google Scholar</a></p></li><li data-counter=\"4.\"><p>Haber, M. et al. Continuity and admixture in the last five millennia of Levantine history from ancient Canaanite and present-day Lebanese genome sequences. , 274–282 (2017).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.ajhg.2017.06.013\" data-track-item_id=\"10.1016/j.ajhg.2017.06.013\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.ajhg.2017.06.013\" aria-label=\"Article reference 4\" data-doi=\"10.1016/j.ajhg.2017.06.013\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXht1CksrzJ\" aria-label=\"CAS reference 4\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28757201\" aria-label=\"PubMed reference 4\">PubMed</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5544389\" aria-label=\"PubMed Central reference 4\">PubMed Central</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 4\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Continuity%20and%20admixture%20in%20the%20last%20five%20millennia%20of%20Levantine%20history%20from%20ancient%20Canaanite%20and%20present-day%20Lebanese%20genome%20sequences&amp;journal=Am.%20J.%20Hum.%20Genet.&amp;doi=10.1016%2Fj.ajhg.2017.06.013&amp;volume=101&amp;pages=274-282&amp;publication_year=2017&amp;author=Haber%2CM\">\n                    Google Scholar</a></p></li><li data-counter=\"6.\"><p>Salvatori, S. &amp; Usai, D. The neolithic and ‘pastoralism’ along the Nile: a dissenting view. , 251–285 (2019).</p></li><li data-counter=\"7.\"><p>Wengrow, D. <i>The Archaeology of Early Egypt: Social Transformations in North-East Africa, 10,000 to 2,650 BC</i> (Cambridge Univ. Press, 2006).</p></li><li data-counter=\"8.\"><p>Bard, K. A. in <i>The Oxford History of Ancient Egypt</i> (ed. Shaw, I.) 61–88 (Oxford Univ. Press, 2000).</p></li><li data-counter=\"9.\"><p>Stevenson, A. in  (ed. Crawford, H.) 620–636 (Routledge, 2013).</p></li><li data-counter=\"10.\"><p>Malek, J. in <i>The Oxford History of Ancient Egypt</i> (ed. Shaw, I.) 89–117 (Oxford Univ. Press, 2000).</p></li><li data-counter=\"11.\"><p>Doherty, S. K. <i>The Origins and Use of the Potter’s Wheel in Ancient Egypt</i> (Archaeopress, 2015).</p></li><li data-counter=\"12.\"><p>Keita, S. O. Y. Further studies of crania from ancient northern Africa: an analysis of crania from First Dynasty Egyptian tombs. , 245–254 (1992).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.1330870302\" data-track-item_id=\"10.1002/ajpa.1330870302\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.1330870302\" aria-label=\"Article reference 12\" data-doi=\"10.1002/ajpa.1330870302\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK383it1Slsw%3D%3D\" aria-label=\"CAS reference 12\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=1562056\" aria-label=\"PubMed reference 12\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 12\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Further%20studies%20of%20crania%20from%20ancient%20northern%20Africa%3A%20an%20analysis%20of%20crania%20from%20First%20Dynasty%20Egyptian%20tombs&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.1330870302&amp;volume=87&amp;pages=245-254&amp;publication_year=1992&amp;author=Keita%2CSOY\">\n                    Google Scholar</a></p></li><li data-counter=\"13.\"><p>Prowse, T. L. &amp; Lovell, N. C. Concordance of cranial and dental morphological traits and evidence for endogamy in ancient Egypt. , 237–246 (1996).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/(SICI)1096-8644(199610)101:2<237::AID-AJPA8>3.0.CO;2-Z\" data-track-item_id=\"10.1002/(SICI)1096-8644(199610)101:2<237::AID-AJPA8>3.0.CO;2-Z\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2F%28SICI%291096-8644%28199610%29101%3A2%3C237%3A%3AAID-AJPA8%3E3.0.CO%3B2-Z\" aria-label=\"Article reference 13\" data-doi=\"10.1002/(SICI)1096-8644(199610)101:2<237::AID-AJPA8>3.0.CO;2-Z\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DyaK2s%2FkvVamsw%3D%3D\" aria-label=\"CAS reference 13\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=8893087\" aria-label=\"PubMed reference 13\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 13\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Concordance%20of%20cranial%20and%20dental%20morphological%20traits%20and%20evidence%20for%20endogamy%20in%20ancient%20Egypt&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2F%28SICI%291096-8644%28199610%29101%3A2%3C237%3A%3AAID-AJPA8%3E3.0.CO%3B2-Z&amp;volume=101&amp;pages=237-246&amp;publication_year=1996&amp;author=Prowse%2CTL&amp;author=Lovell%2CNC\">\n                    Google Scholar</a></p></li><li data-counter=\"14.\"><p>Irish, J. D. Who were the ancient Egyptians? Dental affinities among Neolithic through postdynastic peoples. , 529–543 (2006).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.20261\" data-track-item_id=\"10.1002/ajpa.20261\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.20261\" aria-label=\"Article reference 14\" data-doi=\"10.1002/ajpa.20261\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=16331657\" aria-label=\"PubMed reference 14\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 14\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Who%20were%20the%20ancient%20Egyptians%3F%20Dental%20affinities%20among%20Neolithic%20through%20postdynastic%20peoples&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.20261&amp;volume=129&amp;pages=529-543&amp;publication_year=2006&amp;author=Irish%2CJD\">\n                    Google Scholar</a></p></li><li data-counter=\"15.\"><p>Zakrzewski, S. R. in <i>Egyptian Bioarchaeology: Humans, Animals, and the Environment</i> (eds Ikram, S. et al.) 157–167 (Sidestone, 2015).</p></li><li data-counter=\"16.\"><p>Dicke-Toupin, C. R. <i>Population Continuity or Replacement at Ancient Lachish?</i> (Fairbanks, 2012).</p></li><li data-counter=\"17.\"><p>Irish, J. D. Diachronic and synchronic dental trait affinities of late and post-Pleistocene peoples from North Africa. , 138–155 (1998).</p></li><li data-counter=\"18.\"><p>Maaranen, N., Zakrzewski, S. &amp; Schutkowski, H. Who were the Hyksos? , 66–69 (2022).</p></li><li data-counter=\"22.\"><p>De Meyer, M. et al. in <i>Under the Potter’s Tree: Studies on Ancient Egypt Presented to Janine Bourriau</i> Vol. 204 (eds Aston, D. et al.) 679–702 (Peeters, 2011).</p></li><li data-counter=\"23.\"><p>Power, R. K. &amp; Tristant, Y. From refuse to rebirth: repositioning the pot burial in the Egyptian archaeological record. , 1474–1488 (2016).</p></li><li data-counter=\"25.\"><p>Buikstra, J. E. &amp; Ubelaker, D. U. <i>Standards for Data Collection from Human Skeletal Remains</i> (Arkansas Archeological Survey, 1994).</p></li><li data-counter=\"26.\"><p>Raxter, M. H. et al. Stature estimation in ancient Egyptians: a new technique based on anatomical reconstruction of stature. , 147–155 (2008).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.20790\" data-track-item_id=\"10.1002/ajpa.20790\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.20790\" aria-label=\"Article reference 26\" data-doi=\"10.1002/ajpa.20790\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=18257013\" aria-label=\"PubMed reference 26\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 26\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Stature%20estimation%20in%20ancient%20Egyptians%3A%20a%20new%20technique%20based%20on%20anatomical%20reconstruction%20of%20stature&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.20790&amp;volume=136&amp;pages=147-155&amp;publication_year=2008&amp;author=Raxter%2CMH\">\n                    Google Scholar</a></p></li><li data-counter=\"27.\"><p>Işcan, M. Y., Loth, S. R. &amp; Wright, R. K. Age estimation from the rib by phase analysis: white males. , 1094–1104 (1984).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1520/JFS11776J\" data-track-item_id=\"10.1520/JFS11776J\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1520%2FJFS11776J\" aria-label=\"Article reference 27\" data-doi=\"10.1520/JFS11776J\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=6502109\" aria-label=\"PubMed reference 27\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 27\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Age%20estimation%20from%20the%20rib%20by%20phase%20analysis%3A%20white%20males&amp;journal=J.%20Forensic%20Sci.&amp;doi=10.1520%2FJFS11776J&amp;volume=29&amp;pages=1094-1104&amp;publication_year=1984&amp;author=I%C5%9Fcan%2CMY&amp;author=Loth%2CSR&amp;author=Wright%2CRK\">\n                    Google Scholar</a></p></li><li data-counter=\"28.\"><p>Kennedy, K. A. R. in <i>Reconstruction of Life from the Skeleton</i> (eds Kennedy, K. A. R. &amp; Işcan, M. Y.) 129–160 (Alan R. Liss, 1989).</p></li><li data-counter=\"29.\"><p>Capasso, L., Kenney, K. A. R. &amp; Wilczak, C. A. <i>Atlas of Occupational Markers on Human Remains</i> (Edigratifal, 1998).</p></li><li data-counter=\"30.\"><p>Buzon, M. R. &amp; Simonetti, A. Strontium isotope (Sr/Sr) variability in the Nile Valley: identifying residential mobility during ancient Egyptian and Nubian sociopolitical changes in the New Kingdom and Napatan periods. , 1–9 (2013).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.22235\" data-track-item_id=\"10.1002/ajpa.22235\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.22235\" aria-label=\"Article reference 30\" data-doi=\"10.1002/ajpa.22235\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23440634\" aria-label=\"PubMed reference 30\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 30\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Strontium%20isotope%20%2887Sr%2F86Sr%29%20variability%20in%20the%20Nile%20Valley%3A%20identifying%20residential%20mobility%20during%20ancient%20Egyptian%20and%20Nubian%20sociopolitical%20changes%20in%20the%20New%20Kingdom%20and%20Napatan%20periods&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.22235&amp;volume=151&amp;pages=1-9&amp;publication_year=2013&amp;author=Buzon%2CMR&amp;author=Simonetti%2CA\">\n                    Google Scholar</a></p></li><li data-counter=\"31.\"><p>Stantis, C., Nowell, G. M., Prell, S. &amp; Schutkowski, H. Animal proxies to characterize the strontium biosphere in the northeastern Nile Delta. <i>Bioarchaeology of the Near East</i>, 1–13 (2019).</p></li><li data-counter=\"32.\"><p>Touzeau, A. et al. Egyptian mummies record increasing aridity in the Nile Valley from 5500 to 1500 yr before present. , 92–100 (2013).</p></li><li data-counter=\"33.\"><p>Richards, M. P. in <i>Archaeological Science: An Introduction</i> (eds Richards, M. P. &amp; Britton, K.) 125–144 (Cambridge Univ. Press, 2019).</p></li><li data-counter=\"34.\"><p>Touzeau, A. et al. Diet of ancient Egyptians inferred from stable isotope systematics. , 114–124 (2014).</p></li><li data-counter=\"35.\"><p>Macko, S. A. et al. Documenting the diet in ancient human populations through stable isotope analysis of hair. <i>Philos. Trans. R. Soc. London, B: Biol. Sci.</i>, 65–76 (1999).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1098/rstb.1999.0360\" data-track-item_id=\"10.1098/rstb.1999.0360\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1098%2Frstb.1999.0360\" aria-label=\"Article reference 35\" data-doi=\"10.1098/rstb.1999.0360\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DyaK1MXhtlygtr4%3D\" aria-label=\"CAS reference 35\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=10091248\" aria-label=\"PubMed reference 35\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 35\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Documenting%20the%20diet%20in%20ancient%20human%20populations%20through%20stable%20isotope%20analysis%20of%20hair&amp;journal=Philos.%20Trans.%20R.%20Soc.%20London%2C%20B%3A%20Biol.%20Sci.&amp;doi=10.1098%2Frstb.1999.0360&amp;volume=354&amp;pages=65-76&amp;publication_year=1999&amp;author=Macko%2CSA\">\n                    Google Scholar</a></p></li><li data-counter=\"36.\"><p>Thompson, A. H., Richards, M. P., Shortland, A. &amp; Zakrzewski, S. R. Isotopic palaeodiet studies of ancient Egyptian fauna and humans. , 451–463 (2005).</p></li><li data-counter=\"37.\"><p>Thompson, A. H., Chaix, L. &amp; Richards, M. P. Stable isotopes and diet at ancient Kerma, Upper Nubia (Sudan). , 376–387 (2008).</p></li><li data-counter=\"38.\"><p>Poulallion, E. et al. High δN values in Predynastic Egyptian archeological remains: a potential indicator for localised soil fertilisation practices in extreme conditions. Preprint at <a href=\"https://doi.org/10.1101/2024.11.18.624066\" data-track=\"click_references\" data-track-action=\"external reference\" data-track-value=\"external reference\" data-track-label=\"10.1101/2024.11.18.624066\">https://doi.org/10.1101/2024.11.18.624066</a> (2024).</p></li><li data-counter=\"39.\"><p>Gansauge, M.-T. &amp; Meyer, M. Single-stranded DNA library preparation for the sequencing of ancient or damaged DNA. , 737–748 (2013).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/nprot.2013.038\" data-track-item_id=\"10.1038/nprot.2013.038\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fnprot.2013.038\" aria-label=\"Article reference 39\" data-doi=\"10.1038/nprot.2013.038\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23493070\" aria-label=\"PubMed reference 39\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 39\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Single-stranded%20DNA%20library%20preparation%20for%20the%20sequencing%20of%20ancient%20or%20damaged%20DNA&amp;journal=Nat.%20Protoc.&amp;doi=10.1038%2Fnprot.2013.038&amp;volume=8&amp;pages=737-748&amp;publication_year=2013&amp;author=Gansauge%2CM-T&amp;author=Meyer%2CM\">\n                    Google Scholar</a></p></li><li data-counter=\"48.\"><p>Zvelebil, M. &amp; Lillie, M. in  (ed Price, T. D.) 57–92 (Cambridge Univ. Press, 2000).</p></li><li data-counter=\"49.\"><p>Pinhasi, R. &amp; Stock, J. T. <i>Human Bioarchaeology of the Transition to Agriculture</i> (Wiley, 2011).</p></li><li data-counter=\"50.\"><p>Martin, N. et al. From hunter-gatherers to food producers: new dental insights into the Nile Valley population history (Late Paleolithic-Neolithic). , e24948 (2024).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.24948\" data-track-item_id=\"10.1002/ajpa.24948\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.24948\" aria-label=\"Article reference 50\" data-doi=\"10.1002/ajpa.24948\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=38733278\" aria-label=\"PubMed reference 50\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 50\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=From%20hunter-gatherers%20to%20food%20producers%3A%20new%20dental%20insights%20into%20the%20Nile%20Valley%20population%20history%20%28Late%20Paleolithic-Neolithic%29&amp;journal=Am.%20J.%20Biol.%20Anthropol.&amp;doi=10.1002%2Fajpa.24948&amp;volume=184&amp;publication_year=2024&amp;author=Martin%2CN\">\n                    Google Scholar</a></p></li><li data-counter=\"51.\"><p>Stevenson, A. The Egyptian Predynastic and state formation. , 421–468 (2016).</p></li><li data-counter=\"52.\"><p>Redford, D. B. <i>Egypt, Canaan, and Israel in Ancient Times</i>&nbsp;(Princeton Univ. Press, 1992).</p></li><li data-counter=\"53.\"><p>Llorente, M. G. et al. Ancient Ethiopian genome reveals extensive Eurasian admixture in Eastern Africa. , 820–822 (2015).</p></li><li data-counter=\"56.\"><p>Bourriau, J. in <i>The Oxford History of Ancient Egypt</i> (ed. Shaw, I.) 172–206 (Oxford Univ. Press, 2000).</p></li><li data-counter=\"57.\"><p>Ryholt, K. S. B. &amp; Bülow-Jacobsen, A. <i>The Political Situation in Egypt During the Second Intermediate Period, C. 1800-1550 B.C.</i> (Museum Tusculanum, 1997).</p></li><li data-counter=\"58.\"><p>Weiss, B. The decline of Late Bronze Age civilization as a possible response to climatic change. , 173–198 (1982).</p></li><li data-counter=\"59.\"><p>Ward, W. A., Joukowsky, M. S. &amp; Åström, P. <i>The Crisis Years: The 12th Century B.C.: From Beyond the Danube to the Tigris</i> (Kendall Hunt, 1992).</p></li><li data-counter=\"63.\"><p>Dabney, J. et al. Complete mitochondrial genome sequence of a Middle Pleistocene cave bear reconstructed from ultrashort DNA fragments. <i>Proc. Natl Acad. Sci. USA</i>, 15758–15763 (2013).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1073/pnas.1314445110\" data-track-item_id=\"10.1073/pnas.1314445110\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1073%2Fpnas.1314445110\" aria-label=\"Article reference 63\" data-doi=\"10.1073/pnas.1314445110\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC3sXhs1WlurnO\" aria-label=\"CAS reference 63\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24019490\" aria-label=\"PubMed reference 63\">PubMed</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3785785\" aria-label=\"PubMed Central reference 63\">PubMed Central</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 63\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Complete%20mitochondrial%20genome%20sequence%20of%20a%20Middle%20Pleistocene%20cave%20bear%20reconstructed%20from%20ultrashort%20DNA%20fragments&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1314445110&amp;volume=110&amp;pages=15758-15763&amp;publication_year=2013&amp;author=Dabney%2CJ\">\n                    Google Scholar</a></p></li><li data-counter=\"64.\"><p>Kircher, M., Sawyer, S. &amp; Meyer, M. Double indexing overcomes inaccuracies in multiplex sequencing on the Illumina platform. , e3 (2012).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1093/nar/gkr771\" data-track-item_id=\"10.1093/nar/gkr771\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1093%2Fnar%2Fgkr771\" aria-label=\"Article reference 64\" data-doi=\"10.1093/nar/gkr771\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38Xit1GmtQ%3D%3D\" aria-label=\"CAS reference 64\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22021376\" aria-label=\"PubMed reference 64\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 64\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Double%20indexing%20overcomes%20inaccuracies%20in%20multiplex%20sequencing%20on%20the%20Illumina%20platform&amp;journal=Nucleic%20Acids%20Res.&amp;doi=10.1093%2Fnar%2Fgkr771&amp;volume=40&amp;publication_year=2012&amp;author=Kircher%2CM&amp;author=Sawyer%2CS&amp;author=Meyer%2CM\">\n                    Google Scholar</a></p></li><li data-counter=\"65.\"><p>Gansauge, M.-T., Aximu-Petri, A., Nagel, S. &amp; Meyer, M. Manual and automated preparation of single-stranded DNA libraries for the sequencing of DNA from ancient biological remains and other sources of highly degraded DNA. , 2279–2300 (2020).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41596-020-0338-0\" data-track-item_id=\"10.1038/s41596-020-0338-0\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41596-020-0338-0\" aria-label=\"Article reference 65\" data-doi=\"10.1038/s41596-020-0338-0\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXhtlSht7vF\" aria-label=\"CAS reference 65\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32612278\" aria-label=\"PubMed reference 65\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 65\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Manual%20and%20automated%20preparation%20of%20single-stranded%20DNA%20libraries%20for%20the%20sequencing%20of%20DNA%20from%20ancient%20biological%20remains%20and%20other%20sources%20of%20highly%20degraded%20DNA&amp;journal=Nat.%20Protoc.&amp;doi=10.1038%2Fs41596-020-0338-0&amp;volume=15&amp;pages=2279-2300&amp;publication_year=2020&amp;author=Gansauge%2CM-T&amp;author=Aximu-Petri%2CA&amp;author=Nagel%2CS&amp;author=Meyer%2CM\">\n                    Google Scholar</a></p></li><li data-counter=\"66.\"><p>Dee, M. C14 data pottery coffin burial excavated by Garstang in Nuwayrat (World Museum, Liverpool, UK, 2016).</p></li><li data-counter=\"67.\"><p>Vanthuyne, B. <i>Early Old Kingdom Rock Circle Cemeteries in the 15th and 16th Nomes of Upper Egypt. A Socio-archaeological Investigation of the Cemeteries in Dayr al-Barshā, Dayr Abū Ḥinnis, Benī Ḥasan al-Shurūq and Nuwayrāt</i>. PhD thesis, KU Leuven (2017).</p></li><li data-counter=\"69.\"><p>Reimer, P. J. et al. The IntCal20 Northern Hemisphere radiocarbon age calibration curve (0–55 cal kBP). , 725–757 (2020).</p></li><li data-counter=\"71.\"><p>Bayliss, A. &amp; Marshall, P. <i>Radiocarbon Dating and Chronological Modelling: Guidelines and Best Practice</i> (Historical Association, 2022).</p></li><li data-counter=\"72.\"><p>Brown, T. A., Nelson, D. E., Vogel, J. S. &amp; Southon, J. R. Improved collagen extraction by modified Longin method. , 171–177 (1988).</p></li><li data-counter=\"75.\"><p>Coplen, T. B. Normalization of oxygen and hydrogen isotope data. , 293–297 (1988).</p></li><li data-counter=\"76.\"><p>Chenery, C. A., Pashley, V., Lamb, A. L., Sloane, H. J. &amp; Evans, J. A. The oxygen isotope relationship between the phosphate and structural carbonate fractions of human bioapatite. <i>Rapid Commun. Mass Spectrom.</i>, 309–319 (2012).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/rcm.5331\" data-track-item_id=\"10.1002/rcm.5331\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Frcm.5331\" aria-label=\"Article reference 76\" data-doi=\"10.1002/rcm.5331\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC38XjtFagtQ%3D%3D\" aria-label=\"CAS reference 76\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22223318\" aria-label=\"PubMed reference 76\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 76\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20oxygen%20isotope%20relationship%20between%20the%20phosphate%20and%20structural%20carbonate%20fractions%20of%20human%20bioapatite&amp;journal=Rapid%20Commun.%20Mass%20Spectrom.&amp;doi=10.1002%2Frcm.5331&amp;volume=26&amp;pages=309-319&amp;publication_year=2012&amp;author=Chenery%2CCA&amp;author=Pashley%2CV&amp;author=Lamb%2CAL&amp;author=Sloane%2CHJ&amp;author=Evans%2CJA\">\n                    Google Scholar</a></p></li><li data-counter=\"77.\"><p>Font, L., Nowell, G. M., Graham Pearson, D., Ottley, C. J. &amp; Willis, S. G. Sr isotope analysis of bird feathers by TIMS: a tool to trace bird migration paths and breeding sites. , 513 (2007).</p></li><li data-counter=\"78.\"><p>Nier, A. O. The isotopic constitution of strontium, barium, bismuth, thallium and mercury. , 275–278 (1938).</p></li><li data-counter=\"79.\"><p>Avanzinelli, R., Conticelli, S. &amp; Francalanci, L. High precision Sr, Nd, and Pb isotopic analyses using the new generation Thermal Ionisation Mass Spectrometer ThermoFinnigan Triton-Ti®. , 147–166 (2015).</p></li><li data-counter=\"80.\"><p>Işcan, M. Y., Loth, S. R. &amp; Wright, R. K. Age estimation from the rib by phase analysis: white females. , 853–863 (1985).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1520/JFS11018J\" data-track-item_id=\"10.1520/JFS11018J\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1520%2FJFS11018J\" aria-label=\"Article reference 80\" data-doi=\"10.1520/JFS11018J\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=4031812\" aria-label=\"PubMed reference 80\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 80\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Age%20estimation%20from%20the%20rib%20by%20phase%20analysis%3A%20white%20females&amp;journal=J.%20Forensic%20Sci.&amp;doi=10.1520%2FJFS11018J&amp;volume=30&amp;pages=853-863&amp;publication_year=1985&amp;author=I%C5%9Fcan%2CMY&amp;author=Loth%2CSR&amp;author=Wright%2CRK\">\n                    Google Scholar</a></p></li><li data-counter=\"81.\"><p>Işcan, M. Y. &amp; Loth, S. R. Determination of age from the sternal rib in white males: a test of the phase method. , 122–132 (1986).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1520/JFS11866J\" data-track-item_id=\"10.1520/JFS11866J\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1520%2FJFS11866J\" aria-label=\"Article reference 81\" data-doi=\"10.1520/JFS11866J\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=3944556\" aria-label=\"PubMed reference 81\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 81\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Determination%20of%20age%20from%20the%20sternal%20rib%20in%20white%20males%3A%20a%20test%20of%20the%20phase%20method&amp;journal=J.%20Forensic%20Sci.&amp;doi=10.1520%2FJFS11866J&amp;volume=31&amp;pages=122-132&amp;publication_year=1986&amp;author=I%C5%9Fcan%2CMY&amp;author=Loth%2CSR\">\n                    Google Scholar</a></p></li><li data-counter=\"82.\"><p>Meindl, R. S. &amp; Lovejoy, C. O. Ectocranial suture closure: a revised method for the determination of skeletal age at death based on the lateral-anterior sutures. , 57–66 (1985).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.1330680106\" data-track-item_id=\"10.1002/ajpa.1330680106\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.1330680106\" aria-label=\"Article reference 82\" data-doi=\"10.1002/ajpa.1330680106\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DyaL28%2FkvVWisg%3D%3D\" aria-label=\"CAS reference 82\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=4061602\" aria-label=\"PubMed reference 82\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 82\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Ectocranial%20suture%20closure%3A%20a%20revised%20method%20for%20the%20determination%20of%20skeletal%20age%20at%20death%20based%20on%20the%20lateral-anterior%20sutures&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.1330680106&amp;volume=68&amp;pages=57-66&amp;publication_year=1985&amp;author=Meindl%2CRS&amp;author=Lovejoy%2CCO\">\n                    Google Scholar</a></p></li><li data-counter=\"83.\"><p>Lovejoy, C. O., Meindl, R. S., Pryzbeck, T. R. &amp; Mensforth, R. P. Chronological metamorphosis of the auricular surface of the ilium: a new method for the determination of adult skeletal age at death. , 15–28 (1985).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.1330680103\" data-track-item_id=\"10.1002/ajpa.1330680103\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.1330680103\" aria-label=\"Article reference 83\" data-doi=\"10.1002/ajpa.1330680103\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DyaL28%2FkvVWjtQ%3D%3D\" aria-label=\"CAS reference 83\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=4061599\" aria-label=\"PubMed reference 83\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 83\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Chronological%20metamorphosis%20of%20the%20auricular%20surface%20of%20the%20ilium%3A%20a%20new%20method%20for%20the%20determination%20of%20adult%20skeletal%20age%20at%20death&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.1330680103&amp;volume=68&amp;pages=15-28&amp;publication_year=1985&amp;author=Lovejoy%2CCO&amp;author=Meindl%2CRS&amp;author=Pryzbeck%2CTR&amp;author=Mensforth%2CRP\">\n                    Google Scholar</a></p></li><li data-counter=\"84.\"><p>Brooks, S. &amp; Suchey, J. M. Skeletal age determination based on the os pubis: a comparison of the Acsádi-Nemeskéri and Suchey-Brooks methods. , 227–238 (1990).</p></li><li data-counter=\"85.\"><p>Trotter, M. &amp; Gleser, G. C. Estimation of stature from long bones of American whites and Negroes. , 463–514 (1952).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.1330100407\" data-track-item_id=\"10.1002/ajpa.1330100407\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.1330100407\" aria-label=\"Article reference 85\" data-doi=\"10.1002/ajpa.1330100407\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DyaG3s%2Fjt1arsQ%3D%3D\" aria-label=\"CAS reference 85\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=13007782\" aria-label=\"PubMed reference 85\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 85\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Estimation%20of%20stature%20from%20long%20bones%20of%20American%20whites%20and%20Negroes&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.1330100407&amp;volume=10&amp;pages=463-514&amp;publication_year=1952&amp;author=Trotter%2CM&amp;author=Gleser%2CGC\">\n                    Google Scholar</a></p></li><li data-counter=\"86.\"><p>Robins, G. &amp; Shute, C. C. D. Predynastic Egyptian stature and physical proportions. , 313–324 (1986).</p></li><li data-counter=\"87.\"><p>Bass, W. M. <i>Human Osteology: A Laboratory and Field Manual</i> (Missouri Archaeological Society, 2006).</p></li><li data-counter=\"88.\"><p>Richard Scott, G. &amp; Irish, J. D. <i>Human Tooth Crown and Root Morphology</i> (Cambridge Univ. Press, 2017).</p></li><li data-counter=\"89.\"><p>Howells, W. W. <i>Skull Shapes and the Map: Craniometric Analyses in the Dispersion of Modern Homo</i>, Vol. 79 (Harvard Univ. Press, 1989) .</p></li><li data-counter=\"90.\"><p>Scott, G. R. et al. rASUDAS: a new web-based application for estimating ancestry from tooth morphology. , 18–31 (2018).</p></li><li data-counter=\"92.\"><p>Ortner, D. J. &amp; Putschar, W. <i>Identification of Paleopathological Conditions in Human Skeletal Remains</i> (Smithsonian Institution, 1985).</p></li><li data-counter=\"93.\"><p>Aufderheide, A. C. &amp; Rodríguez-Martín, C. <i>The Cambridge Encyclopedia of Human Paleopathology</i> (Cambridge Univ. Press, 1998).</p></li><li data-counter=\"94.\"><p>Hawkey, D. E. &amp; Merbs, C. F. Activity‐induced musculoskeletal stress markers (MSM) and subsistence strategy changes among ancient Hudson Bay Eskimos. , 324–338 (1995).</p></li><li data-counter=\"95.\"><p>Alves-Cardoso, F. &amp; Assis, S. Exploring ‘wear and tear’ of joints and ‘muscle function’ assumptions in skeletons with known occupation at death. , 689–700 (2021).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.24334\" data-track-item_id=\"10.1002/ajpa.24334\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.24334\" aria-label=\"Article reference 95\" data-doi=\"10.1002/ajpa.24334\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=34013541\" aria-label=\"PubMed reference 95\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 95\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Exploring%20%E2%80%98wear%20and%20tear%E2%80%99%20of%20joints%20and%20%E2%80%98muscle%20function%E2%80%99%20assumptions%20in%20skeletons%20with%20known%20occupation%20at%20death&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.24334&amp;volume=175&amp;pages=689-700&amp;publication_year=2021&amp;author=Alves-Cardoso%2CF&amp;author=Assis%2CS\">\n                    Google Scholar</a></p></li><li data-counter=\"96.\"><p>Wallace, I. J. et al. Experimental evidence that physical activity inhibits osteoarthritis: implications for inferring activity patterns from osteoarthritis in archeological human skeletons. , 223–231 (2022).</p></li><li data-counter=\"97.\"><p>Wilkinson, C. M. &amp; Mahoney, G. in <i>Craniofacial Identification</i> (eds Wilkinson, C. M. &amp; Rynn, C.) 222–237 (Cambridge Univ. Press, 2012).</p></li><li data-counter=\"98.\"><p>El-Mehallawi, I. H. &amp; Soliman, E. M. Ultrasonic assessment of facial soft tissue thicknesses in adult Egyptians. , 99–107 (2001).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/S0379-0738(00)00453-9\" data-track-item_id=\"10.1016/S0379-0738(00)00453-9\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2FS0379-0738%2800%2900453-9\" aria-label=\"Article reference 98\" data-doi=\"10.1016/S0379-0738(00)00453-9\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:STN:280:DC%2BD3M3js1SltQ%3D%3D\" aria-label=\"CAS reference 98\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=11230951\" aria-label=\"PubMed reference 98\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 98\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Ultrasonic%20assessment%20of%20facial%20soft%20tissue%20thicknesses%20in%20adult%20Egyptians&amp;journal=Forensic%20Sci.%20Int.&amp;doi=10.1016%2FS0379-0738%2800%2900453-9&amp;volume=117&amp;pages=99-107&amp;publication_year=2001&amp;author=El-Mehallawi%2CIH&amp;author=Soliman%2CEM\">\n                    Google Scholar</a></p></li><li data-counter=\"100.\"><p>Rynn, C., Balueva, T. &amp; Veselovskaya, E. in <i>Craniofacial Identification</i> (eds Wilkinson, C. M. &amp; Rynn, C.) 193–202 (Cambridge Univ. Press, 2012).</p></li><li data-counter=\"101.\"><p>Wilkinson, C. M. Cognitive bias and facial depiction from skeletal remains. , 1–14 (2021).</p></li><li data-counter=\"108.\"><p>Jónsson, H., Ginolhac, A., Schubert, M., Johnson, P. L. F. &amp; Orlando, L. mapDamage2.0: fast approximate Bayesian estimates of ancient DNA damage parameters. , 1682–1684 (2013).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1093/bioinformatics/btt193\" data-track-item_id=\"10.1093/bioinformatics/btt193\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1093%2Fbioinformatics%2Fbtt193\" aria-label=\"Article reference 108\" data-doi=\"10.1093/bioinformatics/btt193\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=23613487\" aria-label=\"PubMed reference 108\">PubMed</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3694634\" aria-label=\"PubMed Central reference 108\">PubMed Central</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 108\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=mapDamage2.0%3A%20fast%20approximate%20Bayesian%20estimates%20of%20ancient%20DNA%20damage%20parameters&amp;journal=Bioinformatics&amp;doi=10.1093%2Fbioinformatics%2Fbtt193&amp;volume=29&amp;pages=1682-1684&amp;publication_year=2013&amp;author=J%C3%B3nsson%2CH&amp;author=Ginolhac%2CA&amp;author=Schubert%2CM&amp;author=Johnson%2CPLF&amp;author=Orlando%2CL\">\n                    Google Scholar</a></p></li><li data-counter=\"109.\"><p>Meyer, M. et al. Nuclear DNA sequences from the Middle Pleistocene Sima de los Huesos hominins. , 504–507 (2016).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/nature17405\" data-track-item_id=\"10.1038/nature17405\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fnature17405\" aria-label=\"Article reference 109\" data-doi=\"10.1038/nature17405\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XksVKlu7Y%3D\" aria-label=\"CAS reference 109\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26976447\" aria-label=\"PubMed reference 109\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 109\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Nuclear%20DNA%20sequences%20from%20the%20Middle%20Pleistocene%20Sima%20de%20los%20Huesos%20hominins&amp;journal=Nature&amp;doi=10.1038%2Fnature17405&amp;volume=531&amp;pages=504-507&amp;publication_year=2016&amp;author=Meyer%2CM\">\n                    Google Scholar</a></p></li><li data-counter=\"111.\"><p>Renaud, G., Slon, V., Duggan, A. T. &amp; Kelso, J. Schmutzi: estimation of contamination and endogenous mitochondrial consensus calling for ancient DNA. , 224 (2015).</p><p><a data-track=\"click_references\" rel=\"noopener\" data-track-label=\"10.1186/s13059-015-0776-0\" data-track-item_id=\"10.1186/s13059-015-0776-0\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://link.springer.com/doi/10.1186/s13059-015-0776-0\" aria-label=\"Article reference 111\" data-doi=\"10.1186/s13059-015-0776-0\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26458810\" aria-label=\"PubMed reference 111\">PubMed</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4601135\" aria-label=\"PubMed Central reference 111\">PubMed Central</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 111\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Schmutzi%3A%20estimation%20of%20contamination%20and%20endogenous%20mitochondrial%20consensus%20calling%20for%20ancient%20DNA&amp;journal=Genome%20Biol.&amp;doi=10.1186%2Fs13059-015-0776-0&amp;volume=16&amp;publication_year=2015&amp;author=Renaud%2CG&amp;author=Slon%2CV&amp;author=Duggan%2CAT&amp;author=Kelso%2CJ\">\n                    Google Scholar</a></p></li><li data-counter=\"112.\"><p>Korneliussen, T. S., Albrechtsen, A. &amp; Nielsen, R. ANGSD: analysis of next generation sequencing data. , 356 (2014).</p></li><li data-counter=\"113.\"><p>Skoglund, P., Storå, J., Götherström, A. &amp; Jakobsson, M. Accurate sex identification of ancient human remains using DNA shotgun sequencing. , 4477–4482 (2013).</p></li><li data-counter=\"116.\"><p>Briggs, A. W. et al. Removal of deaminated cytosines and detection of in vivo methylation in ancient DNA. , e87–e87 (2010).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1093/nar/gkp1163\" data-track-item_id=\"10.1093/nar/gkp1163\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1093%2Fnar%2Fgkp1163\" aria-label=\"Article reference 116\" data-doi=\"10.1093/nar/gkp1163\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=20028723\" aria-label=\"PubMed reference 116\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 116\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Removal%20of%20deaminated%20cytosines%20and%20detection%20of%20in%20vivo%20methylation%20in%20ancient%20DNA&amp;journal=Nucleic%20Acids%20Res.&amp;doi=10.1093%2Fnar%2Fgkp1163&amp;volume=38&amp;pages=e87-e87&amp;publication_year=2010&amp;author=Briggs%2CAW\">\n                    Google Scholar</a></p></li><li data-counter=\"117.\"><p>Schönherr, S., Weissensteiner, H., Kronenberg, F. &amp; Forer, L. Haplogrep3—an interactive haplogroup classification and analysis platform. , W263–W268 (2023).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1093/nar/gkad284\" data-track-item_id=\"10.1093/nar/gkad284\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1093%2Fnar%2Fgkad284\" aria-label=\"Article reference 117\" data-doi=\"10.1093/nar/gkad284\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37070190\" aria-label=\"PubMed reference 117\">PubMed</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10320078\" aria-label=\"PubMed Central reference 117\">PubMed Central</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 117\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Haplogrep3%E2%80%94an%20interactive%20haplogroup%20classification%20and%20analysis%20platform&amp;journal=Nucleic%20Acids%20Res.&amp;doi=10.1093%2Fnar%2Fgkad284&amp;volume=51&amp;pages=W263-W268&amp;publication_year=2023&amp;author=Sch%C3%B6nherr%2CS&amp;author=Weissensteiner%2CH&amp;author=Kronenberg%2CF&amp;author=Forer%2CL\">\n                    Google Scholar</a></p></li><li data-counter=\"127.\"><p>Fregel, R. et al. Ancient genomes from North Africa evidence prehistoric migrations to the Maghreb from both the Levant and Europe. <i>Proc. Natl Acad. Sci. USA</i>, 6774–6779 (2018).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1073/pnas.1800851115\" data-track-item_id=\"10.1073/pnas.1800851115\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1073%2Fpnas.1800851115\" aria-label=\"Article reference 127\" data-doi=\"10.1073/pnas.1800851115\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29895688\" aria-label=\"PubMed reference 127\">PubMed</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed central reference\" data-track-action=\"pubmed central reference\" href=\"http://www.ncbi.nlm.nih.gov/pmc/articles/PMC6042094\" aria-label=\"PubMed Central reference 127\">PubMed Central</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 127\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Ancient%20genomes%20from%20North%20Africa%20evidence%20prehistoric%20migrations%20to%20the%20Maghreb%20from%20both%20the%20Levant%20and%20Europe&amp;journal=Proc.%20Natl%20Acad.%20Sci.%20USA&amp;doi=10.1073%2Fpnas.1800851115&amp;volume=115&amp;pages=6774-6779&amp;publication_year=2018&amp;author=Fregel%2CR\">\n                    Google Scholar</a></p></li><li data-counter=\"147.\"><p>Omrak, A. et al. Genomic evidence establishes Anatolia as the source of the European Neolithic gene pool. , 270–275 (2016).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.cub.2015.12.019\" data-track-item_id=\"10.1016/j.cub.2015.12.019\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.cub.2015.12.019\" aria-label=\"Article reference 147\" data-doi=\"10.1016/j.cub.2015.12.019\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC28XhvFarsw%3D%3D\" aria-label=\"CAS reference 147\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=26748850\" aria-label=\"PubMed reference 147\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 147\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Genomic%20evidence%20establishes%20Anatolia%20as%20the%20source%20of%20the%20European%20Neolithic%20gene%20pool&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2015.12.019&amp;volume=26&amp;pages=270-275&amp;publication_year=2016&amp;author=Omrak%2CA\">\n                    Google Scholar</a></p></li><li data-counter=\"149.\"><p>Raghavan, M. et al. Upper Palaeolithic Siberian genome reveals dual ancestry of native Americans. , 87–91 (2014).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/nature12736\" data-track-item_id=\"10.1038/nature12736\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fnature12736\" aria-label=\"Article reference 149\" data-doi=\"10.1038/nature12736\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24256729\" aria-label=\"PubMed reference 149\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 149\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Upper%20Palaeolithic%20Siberian%20genome%20reveals%20dual%20ancestry%20of%20native%20Americans&amp;journal=Nature&amp;doi=10.1038%2Fnature12736&amp;volume=505&amp;pages=87-91&amp;publication_year=2014&amp;author=Raghavan%2CM\">\n                    Google Scholar</a></p></li><li data-counter=\"150.\"><p>Rodríguez-Varela, R. et al. Genomic analyses of pre-European conquest human remains from the Canary Islands reveal close affinity to modern North Africans. , 1677–1679 (2018).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.cub.2018.04.083\" data-track-item_id=\"10.1016/j.cub.2018.04.083\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.cub.2018.04.083\" aria-label=\"Article reference 150\" data-doi=\"10.1016/j.cub.2018.04.083\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29787714\" aria-label=\"PubMed reference 150\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 150\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Genomic%20analyses%20of%20pre-European%20conquest%20human%20remains%20from%20the%20Canary%20Islands%20reveal%20close%20affinity%20to%20modern%20North%20Africans&amp;journal=Curr.%20Biol.&amp;doi=10.1016%2Fj.cub.2018.04.083&amp;volume=28&amp;pages=1677-1679&amp;publication_year=2018&amp;author=Rodr%C3%ADguez-Varela%2CR\">\n                    Google Scholar</a></p></li><li data-counter=\"151.\"><p>Schlebusch, C. M. et al. Southern African ancient genomes estimate modern human divergence to 350,000 to 260,000 years ago. , 652–655 (2017).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1126/science.aao6266\" data-track-item_id=\"10.1126/science.aao6266\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1126%2Fscience.aao6266\" aria-label=\"Article reference 151\" data-doi=\"10.1126/science.aao6266\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2sXhslCitb3F\" aria-label=\"CAS reference 151\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28971970\" aria-label=\"PubMed reference 151\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 151\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Southern%20African%20ancient%20genomes%20estimate%20modern%20human%20divergence%20to%20350%2C000%20to%20260%2C000%20years%20ago&amp;journal=Science&amp;doi=10.1126%2Fscience.aao6266&amp;volume=358&amp;pages=652-655&amp;publication_year=2017&amp;author=Schlebusch%2CCM\">\n                    Google Scholar</a></p></li><li data-counter=\"152.\"><p>Seguin-Orlando, A. et al. Genomic structure in Europeans dating back at least 36,200 years. , 1113–1118 (2014).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1126/science.aaa0114\" data-track-item_id=\"10.1126/science.aaa0114\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1126%2Fscience.aaa0114\" aria-label=\"Article reference 152\" data-doi=\"10.1126/science.aaa0114\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXhvFKqtLfK\" aria-label=\"CAS reference 152\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=25378462\" aria-label=\"PubMed reference 152\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 152\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Genomic%20structure%20in%20Europeans%20dating%20back%20at%20least%2036%2C200%20years&amp;journal=Science&amp;doi=10.1126%2Fscience.aaa0114&amp;volume=346&amp;pages=1113-1118&amp;publication_year=2014&amp;author=Seguin-Orlando%2CA\">\n                    Google Scholar</a></p></li><li data-counter=\"154.\"><p>van de Loosdrecht, M. et al. Pleistocene North African genomes link Near Eastern and sub-Saharan African human populations. , 548–552 (2018).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1126/science.aar8380\" data-track-item_id=\"10.1126/science.aar8380\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1126%2Fscience.aar8380\" aria-label=\"Article reference 154\" data-doi=\"10.1126/science.aar8380\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29545507\" aria-label=\"PubMed reference 154\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 154\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Pleistocene%20North%20African%20genomes%20link%20Near%20Eastern%20and%20sub-Saharan%20African%20human%20populations&amp;journal=Science&amp;doi=10.1126%2Fscience.aar8380&amp;volume=360&amp;pages=548-552&amp;publication_year=2018&amp;author=Loosdrecht%2CM\">\n                    Google Scholar</a></p></li><li data-counter=\"157.\"><p>Wang, C.-C. et al. Ancient human genome-wide data from a 3000-year interval in the Caucasus corresponds with eco-geographic regions. , 1–13 (2019).</p></li><li data-counter=\"159.\"><p>Yang, M. A. et al. Ancient DNA indicates human population shifts and admixture in northern and southern China. , 282–288 (2020).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1126/science.aba0909\" data-track-item_id=\"10.1126/science.aba0909\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1126%2Fscience.aba0909\" aria-label=\"Article reference 159\" data-doi=\"10.1126/science.aba0909\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3cXhsVSjtLbN\" aria-label=\"CAS reference 159\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=32409524\" aria-label=\"PubMed reference 159\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 159\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Ancient%20DNA%20indicates%20human%20population%20shifts%20and%20admixture%20in%20northern%20and%20southern%20China&amp;journal=Science&amp;doi=10.1126%2Fscience.aba0909&amp;volume=369&amp;pages=282-288&amp;publication_year=2020&amp;author=Yang%2CMA\">\n                    Google Scholar</a></p></li><li data-counter=\"167.\"><p>Vyas, D. N., Al-Meeri, A. &amp; Mulligan, C. J. Testing support for the northern and southern dispersal routes out of Africa: an analysis of Levantine and southern Arabian populations. , 736–749 (2017).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1002/ajpa.23312\" data-track-item_id=\"10.1002/ajpa.23312\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1002%2Fajpa.23312\" aria-label=\"Article reference 167\" data-doi=\"10.1002/ajpa.23312\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28913852\" aria-label=\"PubMed reference 167\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 167\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Testing%20support%20for%20the%20northern%20and%20southern%20dispersal%20routes%20out%20of%20Africa%3A%20an%20analysis%20of%20Levantine%20and%20southern%20Arabian%20populations&amp;journal=Am.%20J.%20Phys.%20Anthropol.&amp;doi=10.1002%2Fajpa.23312&amp;volume=164&amp;pages=736-749&amp;publication_year=2017&amp;author=Vyas%2CDN&amp;author=Al-Meeri%2CA&amp;author=Mulligan%2CCJ\">\n                    Google Scholar</a></p></li><li data-counter=\"170.\"><p>Mallick, S. et al. The Allen Ancient DNA Resource (AADR) a curated compendium of ancient human genomes. , 1–10 (2024).</p></li><li data-counter=\"174.\"><p>Rubinacci, S., Ribeiro, D. M., Hofmeister, R. J. &amp; Delaneau, O. Efficient phasing and imputation of low-coverage sequencing data using large reference panels. , 120–126 (2021).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1038/s41588-020-00756-0\" data-track-item_id=\"10.1038/s41588-020-00756-0\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1038%2Fs41588-020-00756-0\" aria-label=\"Article reference 174\" data-doi=\"10.1038/s41588-020-00756-0\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BB3MXosVOluw%3D%3D\" aria-label=\"CAS reference 174\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=33414550\" aria-label=\"PubMed reference 174\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 174\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Efficient%20phasing%20and%20imputation%20of%20low-coverage%20sequencing%20data%20using%20large%20reference%20panels&amp;journal=Nat.%20Genet.&amp;doi=10.1038%2Fs41588-020-00756-0&amp;volume=53&amp;pages=120-126&amp;publication_year=2021&amp;author=Rubinacci%2CS&amp;author=Ribeiro%2CDM&amp;author=Hofmeister%2CRJ&amp;author=Delaneau%2CO\">\n                    Google Scholar</a></p></li><li data-counter=\"175.\"><p>The 1000 Genomes Project Consortium. A global reference for human genetic variation. , 68–74 (2015).</p></li><li data-counter=\"176.\"><p>Chaitanya, L. et al. The HIrisPlex-S system for eye, hair and skin colour prediction from DNA: introduction and forensic developmental validation. <i>Forensic Sci. Int. Genet.</i>, 123–135 (2018).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.fsigen.2018.04.004\" data-track-item_id=\"10.1016/j.fsigen.2018.04.004\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.fsigen.2018.04.004\" aria-label=\"Article reference 176\" data-doi=\"10.1016/j.fsigen.2018.04.004\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC1cXpt1egur4%3D\" aria-label=\"CAS reference 176\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=29753263\" aria-label=\"PubMed reference 176\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 176\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20HIrisPlex-S%20system%20for%20eye%2C%20hair%20and%20skin%20colour%20prediction%20from%20DNA%3A%20introduction%20and%20forensic%20developmental%20validation&amp;journal=Forensic%20Sci.%20Int.%20Genet.&amp;doi=10.1016%2Fj.fsigen.2018.04.004&amp;volume=35&amp;pages=123-135&amp;publication_year=2018&amp;author=Chaitanya%2CL\">\n                    Google Scholar</a></p></li><li data-counter=\"177.\"><p>Walsh, S. et al. Developmental validation of the HIrisPlex system: DNA-based eye and hair colour prediction for forensic and anthropological usage. <i>Forensic Sci. Int. Genet.</i>, 150–161 (2014).</p><p><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"10.1016/j.fsigen.2013.12.006\" data-track-item_id=\"10.1016/j.fsigen.2013.12.006\" data-track-value=\"article reference\" data-track-action=\"article reference\" href=\"https://doi.org/10.1016%2Fj.fsigen.2013.12.006\" aria-label=\"Article reference 177\" data-doi=\"10.1016/j.fsigen.2013.12.006\">Article</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"cas reference\" data-track-action=\"cas reference\" href=\"https://www.nature.com/articles/cas-redirect/1:CAS:528:DC%2BC2cXisFCitLg%3D\" aria-label=\"CAS reference 177\">CAS</a><a data-track=\"click_references\" rel=\"nofollow noopener\" data-track-label=\"link\" data-track-item_id=\"link\" data-track-value=\"pubmed reference\" data-track-action=\"pubmed reference\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=24528593\" aria-label=\"PubMed reference 177\">PubMed</a><a data-track=\"click_references\" data-track-action=\"google scholar reference\" data-track-value=\"google scholar reference\" data-track-label=\"link\" data-track-item_id=\"link\" rel=\"nofollow noopener\" aria-label=\"Google Scholar reference 177\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Developmental%20validation%20of%20the%20HIrisPlex%20system%3A%20DNA-based%20eye%20and%20hair%20colour%20prediction%20for%20forensic%20and%20anthropological%20usage&amp;journal=Forensic%20Sci.%20Int.%20Genet.&amp;doi=10.1016%2Fj.fsigen.2013.12.006&amp;volume=9&amp;pages=150-161&amp;publication_year=2014&amp;author=Walsh%2CS\">\n                    Google Scholar</a></p></li>","contentLength":15895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44450304"},{"title":"Gmailtail – Command-line tool to monitor Gmail messages and output them as JSON","url":"https://github.com/c4pt0r/gmailtail","date":1751501176,"author":"c4pt0r","guid":183198,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44450182"},{"title":"What to build instead of AI agents","url":"https://decodingml.substack.com/p/stop-building-ai-agents","date":1751500963,"author":"giuliomagnifico","guid":181701,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44450160"},{"title":"Websites hosting major US climate reports taken down","url":"https://apnews.com/article/climate-change-national-assessment-nasa-white-house-057cec699caef90832d8b10f21a6ffe8","date":1751490613,"author":"geox","guid":181355,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44448868"},{"title":"Vitamin C Boosts Epidermal Growth via DNA Demethylation","url":"https://www.jidonline.org/article/S0022-202X(25)00416-6/fulltext","date":1751488119,"author":"gnabgib","guid":182733,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44448462"},{"title":"A Higgs-Bugson in the Linux Kernel","url":"https://blog.janestreet.com/a-higgs-bugson-in-the-linux-kernel/","date":1751481282,"author":"Ne02ptzero","guid":182732,"unread":true,"content":"<p>We recently ran across a strange higgs-bugson that manifested itself in a critical system that stores and distributes the firm’s trading activity data, called Gord. (A <a href=\"https://en.wikipedia.org/wiki/Heisenbug#Related_terms\">higgs-bugson</a> is a bug that is reported in practice but difficult to reproduce, named for the <a href=\"https://en.wikipedia.org/wiki/Search_for_the_Higgs_boson\">Higgs boson</a>, a particle which was theorized in the 1960s but only found in 2013.) In this post I’ll walk you through the process I took to debug it. I tried to write down relevant details as they came up, so see if you can guess what the bug is while reading along.</p><p>The NFS (“Network File System”) protocol is designed to access a regular POSIX filesystem over the network. The default security story of NFSv3, which is what we’re using here, is roughly “no security” on an untrusted network: the server only checks whether or not the client is connected from a ”privileged” port number (i.e. less than 1024). If the client says it’s connecting on behalf of a particular user, the server just trusts the client. What could go wrong?</p><p>The other security option for NFS is Kerberos. When used with NFS, Kerberos cryptographically verifies the identity of the user accessing the file.</p><p>Gord often does large file copies to ship data around. These copies would very rarely fail with <code>-EACCES (Permission denied)</code> despite the permissions being correctly set on the filesystem. Although retries were possible, it would be sad to lose progress copying these files. Also, strange errors in data storage are scary! It’s possible that spurious errors could indicate a larger issue.</p><p>There was no obvious pattern in these copies failing. Even identical jobs running simultaneously didn’t necessarily fail together. We did have one clue: if we switched Kerberos off in the dev environment (because the error sounded auth related), the copies never failed.</p><p>So, maybe something was wrong with the Kerberos credentials?</p><h2>How does the kernel get your Kerberos credentials?</h2><p>In a typical userspace program making use of Kerberos, libkrb5 will parse some environment variables or a config file to find the location of a Kerberos credentials cache. However, applications using NFS don’t need to link libkrb5 or otherwise know anything about Kerberos. They just do normal file I/O syscalls (open, read, write, etc) as if they were accessing a local filesystem. So what’s going on?</p><p>It turns out the kernel gets credentials via a root userspace daemon called . When your application does its first I/O syscall to a file on NFS, the kernel writes to a file on a special mountpoint to communicate with . (Fun fact: this mountpoint, , is an entirely separate filesystem implementation in the Linux kernel, just like  or  itself.)</p><p>Making some simplifications, the rpc_gssd program grabs the user’s credentials, constructs the right Kerberos service ticket, and writes to the rpc_pipefs again with the result. This involves an API called GSSAPI (“Generic Security Services API”), which you’ll see mentioned throughout this post.</p><p>Looking at the rpc_gssd logs around the time of the bug, I noticed that the kernel hadn’t requested credentials for a while. The most recently requested credential should also have been fresh for another few hours. So, this was a dead end.</p><h2>Trying to reproduce the bug</h2><p>I decided to try my luck by running a slow trickle of writes over the weekend. It seemed like the issue would be key-related somehow, so having a long-running process would force key expiry and plausibly reproduce the bug.</p><p>Checking back in on Monday, none of the dozen boxes I ran this on failed. This wasn’t too surprising, because the issue was pretty rare in production.</p><p>I then generated some large (~200GB) random files, put them on a test NFS mount, and started copying them to another test NFS mount in a loop on even more boxes. Once again, none of these copies failed.</p><p>At this point I was surprised I hadn’t seen the issue. To make sure I wasn’t just getting extremely unlucky, I decided to scale up the number of copies running in parallel.</p><p>I was worried about wasting bandwidth and impacting other users, so I thought a bit about how to make the test a bit more lightweight. One easy win would be to copy from a local disk to NFS instead of from NFS to NFS. However, I had already requested boxes with tiny disks. Fortunately, I had a cool trick in mind.</p><p>I decided to create a filesystem that contains large random files, but fits entirely in memory on small test machines. Here’s the idea: Instead of storing actual file content, I’d use a noise function (or hash function) to generate consistent random bytes on demand.</p><p>This turns out to be fairly straightforward. There’s a Rust crate called  that provides a nicely typed implementation of FUSE (“Filesystem in USErspace”). Around 20 minutes later (with some assistance from Claude), I had a fake filesystem that “contained” some large files to copy from.</p><p>This ultimately did take less time than it would have taken to use larger boxes, but felt slightly yak-shavy before I was sure this would work. I don’t think I would have attempted this trick if I didn’t have an AI assistant to write most of the filesystem for me.</p><div><pre><code></code></pre></div><h2>Inserting arbitrary code into the Linux kernel at runtime</h2><p>The next thing I wanted to do was collect some debug information. If the reproducer  work, I would want to see the kernel stack traces for the syscall that was returning EACCES. I needed to be prepared for this beforehand, because I expected the bug to take a while to show up.</p><p>There’s a Linux kernel subsystem called “eBPF”, which stands for “extended Berkeley Packet Filter”. As you might imagine, it’s supposed to let you filter network packets. However, it has since eaten the world and now lets you insert ~any code you want at the start or end of basically any function in the Linux kernel at runtime. This is fine. Everything’s going to be ok. Don’t worry about it!</p><p>There’s a handy tool called  that can quickly print arguments and return values of kernel functions (among other things). I wrote a bpftrace script that instrumented a few interesting-looking functions, something like this:</p><div><pre><code></code></pre></div><p>The example above looks at the gss_cred_init function, and prints out the kernel stack trace if it returns an EACCES error. This is a very simple example, but definitely check out the <a href=\"https://github.com/bpftrace/bpftrace/blob/master/man/adoc/bpftrace.adoc\">bpftrace manual</a> for other functionality.</p><h2>Back to reproducing the issue</h2><p>The test setup was as follows:</p><ul><li>Some jobs that run rsync processes to copy from the FUSE filesystem to a test NFS server.</li><li>A bpftrace script that watches for  being returned from relevant kernel functions.</li><li>A way to take a packet capture (PCAP) of just the time surrounding a returned .</li></ul><p>And… It worked! ! Weirdly,  of my test boxes failed at the same time? That never happened in production. Usually only one or two Gord jobs would fail at a time. One bpftrace message stood out: “<code>gss_validate returned -13 (GSS_S_BAD_SIG)</code>”.</p><p>Bad signature??? What? Of all the things that would make sense, this made sense the least. Was the server returning a bad signature? Was the client failing to verify it correctly? Was there memory corruption somewhere? Keep in mind all of this software is written in C, so almost anything is possible. Even <a href=\"https://en.wikipedia.org/wiki/Undefined_behavior\">nasal demons</a>. If this  memory corruption, maybe I found a security vulnerability?</p><p>I peeked at the packet capture of the bug in Wireshark and did not see any obvious signs of corruption. Other interesting things I noticed were:</p><ul><li>There were a lot of retransmissions at the NFS level. The test NFS server I was using was small and probably got overloaded.</li><li>TCP frames were being split up and reassembled.</li><li>Again: A third of my jobs failed together, which was unexpected given what I saw in production.</li></ul><p>I didn’t have any good guesses based on the above. Maybe I could try to generate the signature myself to compare it with what’s in the packet? I knew Wireshark could decrypt Kerberos requests in network packets given the user’s Kerberos password, which was enough to grab the signature key (GSS token). All I needed to do was write a program to compute the signature given that token. Seems simple enough in theory, but how exactly do you do that?</p><p>An NFS request looks something like this. Some interesting things to call out here are:</p><ul><li>There’s an XID, which matches responses to requests. A client can have multiple requests in flight, and the server can respond to them out of order, so an ID is necessary.</li><li>The credentials field specifies which GSS context the RPC request is associated with, and includes an incrementing sequence number (“GSS sequence number”). Note that this is a separate sequence number from the XID.</li></ul><p>In the request, the checksum is the HMAC of roughly all the data in the request header, using the shared GSS key. In the response, the checksum is the HMAC of the GSS sequence number from the request.</p><p>An HMAC is a “Hash-based Message Authentication Code” – it allows someone with knowledge of the key to verify that someone else with the same key created the checksum.</p><h2>Writing a Wireshark plugin</h2><p>The next thing I did was write a Wireshark plugin to compute the checksums of replies.</p><p>While writing the Wireshark plugin I ran into a problem: there were retransmissions in my PCAP, so how do I figure out which of the retransmitted requests corresponds to a response? This was throwaway code for debugging, so I decided to make a big shared-mutable hashmap containing a map from XIDs to GSS sequence numbers. I updated the hashmap whenever Wireshark processed a frame containing an NFS request, assuming it would process them in order.</p><p>Then, I loaded up my packet capture and browsed to the response with an XID that failed verification.</p><p>Okay. So the checksum in the packet is correct. Why did the kernel think it wasn’t? I clicked back to the request in the PCAP to take a look. Annoyingly, there were two requests with the same XID, meaning that a retransmission was involved. I then clicked back to the response.</p><p>Huh. Was my Wireshark plugin buggy?</p><p>(At this point I think you should have all the information you need to guess what the bug is. It might be fun to think through this. When you’re ready, read on.)</p><p>Remember how I wasn’t sure which request to use to get the GSS sequence number from? It turns out the kernel has the exact same bug!</p><p>SunRPC matches responses to requests via their XIDs, so if the server is overloaded and takes a while to respond, the NFS client might retransmit the request. The checksum field in the response is an HMAC over the  GSS sequence number. Note that this is  the XID, and is  included in the response. When the kernel retransmits a request with the same XID, it uses a new sequence number and updates the GSS sequence number it has recorded. If the kernel then receives the response that was associated with the old GSS sequence number, checksum validation fails. If this happens 3x in a row,  is returned to userspace.</p><p>This is  self-fulfilling because each failure creates another retry. It is not guaranteed, however: you can still get lucky with timing and avoid the bug.</p><p>Basically, the only reason I was able to reproduce the bug is because I was using a tiny test NFS server, causing latencies in the hundreds of seconds. If I had kept going with low-load testing, I probably would have had to use another method to find the bug.</p><p>A quick read of some kernel source code confirmed that what I thought was happening  happen, but to be sure, I decided to write a lightweight reproducer that works by delaying packets.</p><p>There’s a kernel facility called NFQUEUE which allows you to use a userspace process for packet filtering. This is probably intended for security use cases, but what I did was hook it up to a Python script where I can individually look at packets and press  to let them through after enough time has passed to trigger the bug. Basically, I could manually simulate high latency by being a very very slow human firewall.</p><p>Then it was a matter of writing a little more glue code, and I had a fully automatic reproduction script.</p><p>At this point I reported my findings to my team, who quickly noticed that the RFC actually does mention this case.</p><blockquote><p>“Then when it receives a response with a matching RPC transaction identifier, it can compute the checksum of each sequence number in the cache to try to match the checksum in the reply’s verifier.” - RFC2203 5.3.3.1. (Page 13)</p></blockquote><p>The Linux kernel does not actually implement this cache as suggested by the RFC, so I wrote a kernel patch to add this functionality and mailed it off upstream. I also learned that the FreeBSD kernel actually already implements this, so this is new-to-Linux but not new-to-NFS.</p><p>More importantly, though, all that this cache does is increase the amount of retries needed to hit a bad interleaving. The fundamental problem is that a sequence number mismatch should not cause an immediate retransmission, which makes the problem self-fulfilling. So, I wrote a second kernel patch to not retransmit if a bad checksum is seen.</p><p>This feels principled, since a checksum mismatch suggests network tampering, so it makes sense to treat it as if we didn’t receive a message at all. The normal timeout logic can take care of retransmission in the unlikely case that one is needed. As final verification, I applied these patches and made sure that the test copy jobs and the Python reproducer no longer failed.</p><p>Both of these patches are now upstream and will be available in Linux 6.16.</p>","contentLength":13425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44447259"},{"title":"Couchers is officially out of beta","url":"https://couchers.org/blog/2025/07/01/releasing-couchers-v1","date":1751479520,"author":"laurentlb","guid":181177,"unread":true,"content":"<p><em>Quick summary: we are out of Beta and into version 1, we're releasing a new strategy around safe &amp; active community instead of bashing our competitors, a fancy redesigned landing page, and a bunch of new features to make core couch surfing functionality better! Share the platform with your friends and let's grow the community together!</em></p><p>We are super excited to share that Couchers is today finally out of the Beta phase with our version 1 (v1) launch. After five years of building and developing the platform and community―first through the Alpha stage and a very long Beta stage―we are finally ready for our big launch! This means a brand new strategy, a spiffy new landing page, and a bunch of new features.</p><p>Today we are launching <strong>our new strategy centered around a commitment on being the safest, healthiest, and most active couch surfing community</strong>. This is an evolution of our <a href=\"https://couchers.org/plan\">original plan</a>, but adapted for the next stage of the community, where we clearly define what we stand for instead of living the shadow of another platform.</p><p>In addition, we are <strong>launching a brand new landing page</strong> to more clearly communicate our values and what sets us apart, and to explain what our community is all about to newbies and veteran couch surfers alike.</p><p>Finally, we have combined a <strong>number of exciting new features</strong> into this release. These have been gradually rolled out over the past few months to make sure they are ready for you today (and hopefully mostly free of bugs)! Read on to find out what these are.</p><h2>What does the v1 launch mean?</h2><p>Although the platform has been usable for a long time, until now, even some of the core functionality has been somewhat buggy and not as polished as we would like, in order to call it \"complete\". The v1 launch signals the completion of the platform in the sense that core functionality has been cleaned up and we believe it is fully ready for all to use! A lot of this cleanup effort was also done in the <a href=\"https://couchers.org/blog/2025/05/11/v0.9.9-release\">v0.9.9 release a few months ago</a>.</p><p>This launch means we can now focus on growing the platform and coming up with new features that set us apart and help you connect with other members better.</p><p>The features we decided to work on for v1 were picked to best further our goal of being a safe and active community, as well as tighten up the core functionality of the platform to make the experience super smooth and easy.</p><p>We are really excited about our <a href=\"https://couchers.org\">new landing page</a>, which clearly communicates what Couchers is, what we stand for, and what you can find on the platform once you sign up. We hope this will help us gain more of the right kinds of members!</p><p>A huge thanks to <a href=\"https://couchers.org/user/unsettleddown\">Nicole</a> who built the majority of the landing page; <a href=\"https://couchers.org/user/pcolt86\">Pablo</a> and <a href=\"https://couchers.org/user/aapeli\">Aapeli</a> who also helped build it; <a href=\"https://couchers.org/user/marta\">Marta</a> and <a href=\"https://couchers.org/user/karottensaft1\">Charlotte</a> who designed it; <a href=\"https://couchers.org/user/jesse\">Jesse</a> and <a href=\"https://couchers.org/user/chrisk\">Chris</a> who helped test it; and countless others who helped write or brainstorm our marketing message, or provided their input in other ways!</p><p><em>The new landing page features an anonymized map layer: pins are randomized to be at a random point 2-10 km (1.2-6.2 mi) away from their true location and are not tied to any user info. The map is further restricted to not zoom very far in.</em></p><p>We completely overhauled functionality for leaving a reference with two key updates:</p><ul><li><p>you can now indicate whether you did not stay with or host someone (and we won't remind you to leave references); and</p></li><li><p>you can now send our Safety Team private feedback (that's not shown on the public reference) about your interactions through the reference flow. You can use this functionality to let us know if something felt off, even if there's no cause for bigger concern. More information is always better and helps the Safety Team understand situations better and keep the platform safe!</p></li></ul><p>Thanks to <a href=\"https://couchers.org/user/aapeli\">Aapeli</a>, <a href=\"https://couchers.org/user/unsettleddown\">Nicole</a>, and <a href=\"https://couchers.org/user/jasonwvh\">Jason</a> for these updates. <a href=\"https://couchers.org/user/unsettleddown\">Nicole</a> also added the number of references to the references tab on profiles to draw more attention to it.</p><h3>Find members faster: a new and redesigned map</h3><p><a href=\"https://couchers.org/user/unsettleddown\">Nicole</a> spent months re-architecting and rebuilding a brand new Map Search page! The old one was creaking with age and very difficult to update and add features to. Nicole rewrote it from scratch! <a href=\"https://couchers.org/user/aapeli\">Aapeli</a> helped build some new backend functionality to significantly speed up searching, and <a href=\"https://couchers.org/user/jesse\">Jesse</a> among many others helped test it!</p><p>We put a lot of thought into making the map as usable as possible, trying to think about what should be shown, what should be hidden, and how to best display all this data. We hope you find it useful!</p><h3>Don't miss out on news: new notification feed</h3><p>We've been working on notifications for quite a few releases now. In this release we introduced a notification feed, rolled out push notifications for desktop and mobile, as well as added a bunch of new notification types for things like nested replies on discussions, and pending host requests.</p><p>Don't want so many notifications? No problem. Each notification type can be adjusted for a particular item or for a whole class of notifications. Find this under Notification Settings (accessible from Account Settings). We hope this helps members see what they care about, without having to waste time searching the platform; a core value of not maximizing empty engagement.</p><h3>Speaking your language: a new language selector &amp; translations</h3><p>We implemented a language selector and are in the active process of translating the platform to languages other than English! Thanks to <a href=\"https://couchers.org/user/ellebee\">Laura</a>, <a href=\"https://couchers.org/user/unsettleddown\">Nicole</a> and <a href=\"https://couchers.org/user/aapeli\">Aapeli</a> for their work on the functionality, as well as the many folks helping out on translation!</p><p>If you'd like to see the platform be translated to your own language, please apply to volunteer for our <a href=\"https://couchers.org/volunteer/translator\">Translator Position</a>! You can contribute as much or as little as you want. Every bit helps!</p><p>We have rolled out significant new functionality to make the platform safer:</p><ul><li><a href=\"https://couchers.org/user/unsettleddown\">Nicole</a> built the frontend functionality to block (and unblock) users, while <a href=\"https://couchers.org/user/spreeni\">Yannic</a> helped fix up some lingering backend functionality for that;</li><li><a href=\"https://couchers.org/user/colleen\">Colleen</a> added report flags to event cards; and</li><li>under the hood, <a href=\"https://couchers.org/user/aapeli\">Aapeli</a>, <a href=\"https://couchers.org/user/jesse\">Jesse</a> and <a href=\"https://couchers.org/user/rafael_ferreira\">Rafael</a> continue to work on many different moderation and admin features to help combat abuse and make the platform safer for everyone!</li></ul><p>We also updated our policies and now forbid <a href=\"https://help.couchers.org/hc/couchersorg-help-center/articles/1746572895-why-is-nudism-naturism-nudity-clothing_optional-not-allowed-on-couchers-org\">nudism</a> and <a href=\"https://help.couchers.org/hc/couchersorg-help-center/articles/1748816842-why-are-shared-beds-sleeping-surfaces-not-allowed\">shared sleeping surfaces</a> (such as a shared bed). The change was prompted by the significant moderation burden caused by regular issues with this subgroup of the community. This was a decision made by the Board in order to promote safety on the platform and to set clear expectations for everyone. Thanks to the volunteers who spearheaded this difficult change!</p><p>While we actively work on many features and improvements to the platform, we aren't always great at making those changes easy to see and notice. Sometimes people even criticize us for not making enough progress.</p><p>To help communicate what we are doing and when, <a href=\"https://couchers.org/user/chrisk\">Chris</a> has spearheaded an effort to produce and maintain <a href=\"https://couchers.org/roadmap\">a public roadmap</a>. This is a great resource to learn about what we're working on and what to look forward to!</p><p><a href=\"https://couchers.org/user/aapeli\">Aapeli</a> with assistance from <a href=\"https://couchers.org/user/unsettleddown\">Nicole</a>, built out an  system: if you have set your status to hosting but haven't logged in in many months, we will occasionally send you a notification to ask if you are still interested in hosting.</p><p>This helps us get a better idea of whether you would respond to a prospective surfer coming into town. In the next few weeks we'll be gradually rolling out this new functionality. We hope it helps reduce the number of well-thought out requests sent by surfers to hosts that never read their notification in the first place.</p><p>In order to execute on our new strategy and marketing plans including this huge launch, we assembled a new team that we call . This new team is in charge of marketing &amp; branding (incl. social media, the blog, landing pages), safety &amp; support (moderation, policies, verification, etc), community building (community creation, engagement, feedback, etc), volunteer recruitment and general operations within Couchers. They did a phenomenal job brainstorming, writing, planning, coordinating, and executing this launch.</p><h2>Our Engineering and Product team</h2><p>Our engineers, UI/UX designers and other software-focused volunteers continue to work hard on our Engineering and Product team.</p><h2>Our rehauled branding &amp; marketing strategy</h2><p><strong>Our new strategy is centered around a commitment to being the safest, healthiest, and most active couch surfing community.</strong></p><p>From our founding, we have always been defined by our competition: being the non-profit and open-source alternative to CouchSurfing.com™ that takes our community seriously and is here to fix the structural problems with other platforms. This has been an incredibly effective strategy, and it has helped us seed the community with the right crowd: a lot of veteran couch surfers with the couch surfing spirit.</p><p>As our community continues to grow and mature, it's time to stop living in the shadow of another platform and start living up to our own identity and values. This is something the volunteer team and Board have been thinking about a lot, and this has steered our priorities since the start of the year. When you look back at our <a href=\"https://couchers.org/plan\">original plan</a>, it's clear that everything we identified as the issues then were different aspects of community health and safety.</p><p>In the coming months we are going to work on updating our messaging across the board and further building out our public landing pages to help new users learn about the platform and community.</p><p>Over the next weeks the volunteer teams and Board will be working together to define our concrete priorities and roadmap for the rest of the year. We have brainstormed many ideas and have a rough path forward, but will work on refining it further. We will certainly concentrate a large part of our efforts on building a native mobile app, as well as allocating engineering resources to help our CouchOps team with their marketing and social media efforts.</p><p>We have plans to build infrastructure to better measure the impact of what we do, taking a more metric-driven approach to features. The community is still at a size where it's hard to make strong statements about the impact of individual changes, but we believe that getting better visibility into what the experience is like for users is key.</p><p>Couchers.org is only possible due to our amazing and dedicated volunteer team. We're so appreciate of our volunteers who believe in the vision of a non-profit, open-source, safe and active couch surfing community and donate their free time to make it a reality.</p><p>To everyone mentioned in this post, to all our past volunteers who got us where we are today, and to all our future volunteers to come―thank you!</p><p>Finally we also want to thank everyone in our community for sticking with us through many years of hard work to get to this point!</p><p>Want to help Couchers be even more amazing?!</p><p>Want to hang with other motivated travelers and help our community thrive? We can always use more help!</p><p>Specifically we could really use help with:</p><h3>Hype us up on social media</h3><p>Follow us on the socials, and like+share religiously 🙏</p><p>We have set a fundraising goal of raising $5000 this year. Help us reach this goal and keep the servers running by <a href=\"https://couchers.org/donate\">donating to our non-profit</a>. You'll also get a fun badge!</p><p><em>Written by <a href=\"https://couchers.org/user/aapeli\">Aapeli</a>. Published on 2025/07/01</em></p>","contentLength":11188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44446917"},{"title":"AI note takers are flooding Zoom calls as workers opt to skip meetings","url":"https://www.washingtonpost.com/technology/2025/07/02/ai-note-takers-meetings-bots/","date":1751479517,"author":"tysone","guid":181700,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44446916"},{"title":"Stop Killing Games","url":"https://www.stopkillinggames.com/","date":1751474732,"author":"MYEUHD","guid":180866,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44445880"},{"title":"Features of D That I Love","url":"https://bradley.chatha.dev/blog/dlang-propaganda/features-of-d-that-i-love/","date":1751474728,"author":"vips7L","guid":181699,"unread":true,"content":"<p>This is a beginner-friendly post exploring some of my favourite parts of the <a href=\"https://dlang.org/\">D programming language</a>, ranging from smaller quality of life stuff, to more major features.</p><p>I  talk much about D’s metaprogramming in this post as that topic basically requires its own dedicated feature list, but I still want to mention that D’s metaprogramming is world class - allowing a level of flexibility &amp; modelling power that few statically compiled languages are able to rival.</p><p>I’ll be providing some minimal code snippets to demonstrate each feature, but this is by no means an in depth technical post, but more of an easy to read “huh, that’s neat/absolutely abhorrent!” sort of deal.</p><h2>Feature - Automatic constructors</h2><p>If you define a struct (by-value object) without an explicit constructor, the compiler will automatically generate one for you based on the lexical order of the struct’s fields.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Very handy for Plain Old Data types, especially with the semi-recent support for <a href=\"https://dlang.org/spec/expression.html#argument-parameter-matching\">named parameters</a>.</p><h2>Feature - Design by contract</h2><ul><li>“in” assertions to confirm that the function’s parameters are valid.</li><li>“out” assertions to confirm that the function’s return value is in a valid state.</li></ul><p>Additionally you can attach “invariants” onto structs and classes. Invariants are functions that run at the start and end of every  member function, and can be used to ensure that the type is always in a valid state.</p><p>Let’s start off with a contrived example of invariants:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Now let’s rewrite the above type to use “in” contracts instead, with an extra function to show off “out” contracts:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This can allow for an easy self-descriptive validation pattern for consumers/readers of your code, as well as an easy to implement self-checking mechanism for types that have complex internals.</p><p>Anecdotally I find this to be an underutilised feature of D, and it’s one I like to make use of a lot in my own code.</p><h2>Syntax - The dollar operator</h2><p>A lot of languages do not provide a shorthand syntax for referencing the length of an array, which can sometimes lead to awkward looking code when e.g. slicing arrays (any Go enjoyers here?).</p><p>D provides the dollar operator, which is a shorthand syntax for referencing the length of something.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Structs and classes can even <a href=\"https://dlang.org/spec/operatoroverloading.html#dollar\">overload</a> this operator.</p><p>D compilers provide an interpreter for the language which allows a very large amount of D code to be ran at compile time, as-is, without any special marking or other weirdness to go with it.</p><p>Generally, anywhere where the language requires a compile-time constant is a place where CTFE will transparently come into play.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This feature has a lot of different practical applications, and can allow for much cleaner, robust code than hardcoding precomputed values.</p><p>Since a lot of use cases relate to metaprogramming I’ll leave the topic here, but CTFE is an extremely instant example of D’s unusual feature set.</p><h2>Feature - Built-in unittests</h2><p>D has direct support for defining unittests, and even allows you to override the built-in test runner for something more robust (such as with the <a href=\"https://code.dlang.org/packages/unit-threaded\">unit-threaded</a> library).</p><p>D code usually bundles unittests and normal code within the same file, rather than splitting them out into separate files as with most other languages:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This extremely low-friction barrier for writing tests is a godsend for motivating people to write even the most minimal of tests.</p><p>Of course if you have more complex needs then the option to have a proper testing framework + structure is still available to you, but the vast majority of D code I’ve seen simply uses  blocks, optionally with a library that provides a better test runner.</p><h2>Feature - Exhaustive switch statements</h2><p>D provides a  statement which has an autogenerated  case that will immediately crash the program if its taken.</p><p>This allows you to define a switch that will always alert you if a new value needs to be added, or if an invalid value was somehow passed into it.</p><p>Additionally, if you use a  with an  value, then a compile-time check is triggered to ensure that every value within the  type has been declared, making it impossible to forget to add a new case when the enum is modified.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><h2>Syntax - Parenthesis omission</h2><p>D allows you to omit parentheses when calling functions in multiple contexts.</p><p>When calling a function with no parameters, you can omit them:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>(Marginally related) When calling a function with 1 parameter, you may use assignment syntax instead:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>When passing a single template parameter which consists of only 1 lexical token, you may omit the parenthesis:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>This can do wonders for readability.</p><p>UFCS allows call chains to be “inverted” by allowing freestanding functions to be used as if they were a member of their first parameter.</p><p>In other words:  can be rewritten as .</p><p>The two following snippets are completely equivalent in function, except the second snippet uses UFCS to provide a more clean look.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><h2>Feature - Scoped &amp; Selective Imports</h2><p>D supports limiting imports to a specific scope, whether that be a singular if-statement, an entire function, an entire struct/class, etc.</p><p>D will also allow you to selectively import symbols from other modules, instead of polluting your lookup scope with a ton of unrelated stuff - also helps increase comprehension of the codebase.</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>While it may seem like clutter and extra effort, in the long run this allows for:</p><ol><li>Making it easy for newcomers to understand where certain functions are coming from.</li><li>Allows for code to become “portable” between files since the code can carry most of its external dependencies inside of itself, making refactoring a bit easier.</li></ol><h2>Feature - Built-in documentation generator</h2><p>Finally, D has a built-in documentation generator with a relative standard, easy to read format.</p><p>There’s also a handful of documentation tools that are detached from the built-in one since the default generated output is a bit lacklustre ( I’m plugging my <a href=\"https://github.com/Juptune/marmos\">custom tool</a> here).</p><p>Here’s a relatively extreme example from one of my personal projects, to get an idea of the basic format:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>Here’s an example from the standard library, which has minor usage of documentation macros:</p><div><figure><pre data-language=\"d\"><code></code></pre></figure></div><p>I tried to focus more on the more simpler day-to-day features, with only a splattering of the bigger more complicated stuff.</p><p>Hopefully this provides some insight on the wacky-yet-wonderful feature set that D provides.</p>","contentLength":6276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44445877"},{"title":"Sony's Mark Cerny Has Worked on \"Big Chunks of RDNA 5\" with AMD","url":"https://overclock3d.net/news/gpu-displays/sonys-mark-cerny-has-worked-on-big-chunks-of-rdna-5-with-amd/","date":1751472646,"author":"ZenithExtreme","guid":182590,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44445458"},{"title":"Show HN: CSS generator for a high-def glass effect","url":"https://glass3d.dev/","date":1751471463,"author":"kris-kay","guid":180978,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44445238"},{"title":"ICEBlock climbs to the top of the App Store charts after officials slam it","url":"https://www.engadget.com/social-media/iceblock-climbs-to-the-top-of-the-app-store-charts-after-officials-slam-it-004319963.html","date":1751471252,"author":"doener","guid":180758,"unread":true,"content":"<p>US government officials have condemned ICEBlock and <a data-i13n=\"cpos:1;pos:1\" href=\"https://edition.cnn.com/2025/06/30/tech/iceblock-app-trump-immigration-crackdown\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:CNN's recent&nbsp;coverage;cpos:1;pos:1;elm:context_link;itc:0;sec:content-canvas\">recentcoverage</a> of it, leading to more people hearing about its existence and downloading it from the App Store. Now the application, which allows users to add a pin on a map to show where ICE agents have recently been spotted, has <a data-i13n=\"cpos:2;pos:1\" href=\"https://bsky.app/profile/bengoggin.bsky.social/post/3lswgll3m4k2k\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:climbed;cpos:2;pos:1;elm:context_link;itc:0;sec:content-canvas\">climbed</a> to the to the top of Apple's App Store charts. It's currently the number one free social networking app in the US and the third most downloaded free app overall.</p><p> piece talked about how the app's developer, Joshua Aaron, launched it in early April after seeing the Trump administration crack down on immigration. When the piece went live, Aaron said the app had 20,000 users, many of whom live in Los Angeles, where ICE has been raiding neighborhoods. In addition to letting users pin ICE agent locations on a map, the app also gives them a way to add notes, such as what the agents are wearing or what car they're driving. Any user within a five mile radius of the sighting will get an alert.</p><p>But White House press secretary Karoline Leavitt <a data-i13n=\"elm:affiliate_link;sellerN:The Independent;elmt:;cpos:3;pos:1\" href=\"https://shopping.yahoo.com/rdlw?merchantId=9b10ec1f-8e62-4410-8957-c54f7db63bfc&amp;siteId=us-engadget&amp;pageId=1p-autolink&amp;contentUuid=d556e816-ea47-4f16-8970-bd0fc1e5368f&amp;featureId=text-link&amp;merchantName=The+Independent&amp;linkText=suggested&amp;custData=eyJzb3VyY2VOYW1lIjoiV2ViLURlc2t0b3AtVmVyaXpvbiIsImxhbmRpbmdVcmwiOiJodHRwczovL3d3dy5pbmRlcGVuZGVudC5jby51ay9uZXdzL3dvcmxkL2FtZXJpY2FzL3VzLXBvbGl0aWNzL2ljZWJsb2NrLWFwcC10cmFja2VyLWNubi10cnVtcC1iMjc4MDY4NC5odG1sIiwiY29udGVudFV1aWQiOiJkNTU2ZTgxNi1lYTQ3LTRmMTYtODk3MC1iZDBmYzFlNTM2OGYiLCJvcmlnaW5hbFVybCI6Imh0dHBzOi8vd3d3LmluZGVwZW5kZW50LmNvLnVrL25ld3Mvd29ybGQvYW1lcmljYXMvdXMtcG9saXRpY3MvaWNlYmxvY2stYXBwLXRyYWNrZXItY25uLXRydW1wLWIyNzgwNjg0Lmh0bWwifQ&amp;signature=AQAAAa5qGiHdA4KS_JjwPdcDC_fPro3patrcHxPwboNCgaAs&amp;gcReferrer=https%3A%2F%2Fwww.independent.co.uk%2Fnews%2Fworld%2Famericas%2Fus-politics%2Ficeblock-app-tracker-cnn-trump-b2780684.html&amp;source=engadget_article_commerce_ctrl&amp;refurl=https%3A%2F%2Fwww.engadget.com%2Fsocial-media%2Ficeblock-climbs-to-the-top-of-the-app-store-charts-after-officials-slam-it-004319963.html\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:suggested;elm:affiliate_link;sellerN:The Independent;elmt:;cpos:3;pos:1;itc:0;sec:content-canvas\">suggested</a> that the  piece was \"an incitement of further violence against... ICE officers\" when asked to respond to the report on the podium. She said that there's been a 500 percent increase against ICE agents who are just \"trying to do their jobs and remove public safety threats from... communities.\" ICE acting Director Todd M. Lyons also <a data-i13n=\"cpos:4;pos:1\" href=\"https://www.ice.gov/news/releases/statement-ice-acting-director-todd-m-lyons-news-coverage-ice-spotting-app\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:issued a statement;cpos:4;pos:1;elm:context_link;itc:0;sec:content-canvas\">issued a statement</a>, saying that the app paints a target on federal law enforcement officers' backs. \" is willfully endangering the lives of officers who put their lives on the line every day and enabling dangerous criminal aliens to evade US law,\" he continued. \"Is this simply reckless 'journalism' or overt activism?\"</p><p>Meanwhile, US Homeland Security Secretary Kristi Noem and US Attorney General Pam Bondi both said the government is going after Aaron. \"He's giving a message to criminals where our federal officers are,\" <a data-i13n=\"cpos:5;pos:1\" href=\"https://www.foxnews.com/media/attorney-general-pam-bondi-warns-iceblock-app-developer-watch-out-says-doj-looking-him\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:Bondi said;cpos:5;pos:1;elm:context_link;itc:0;sec:content-canvas\">Bondi said</a>. \"...we are looking at it, we are looking at him, and he better watch out, because that's not a protected speech. That is threatening the lives of our law enforcement officers throughout this country.\"'</p><p>Aaron told  that ICEBlock doesn't collect personal data, such as device IDs and IP addresses, which <a data-i13n=\"cpos:6;pos:1\" href=\"https://techcrunch.com/2025/07/01/iceblock-an-app-for-anonymously-reporting-ice-sightings-goes-viral-overnight-after-bondi-criticism/\" rel=\"nofollow noopener\" target=\"_blank\" data-ylk=\"slk:TechCrunch;cpos:6;pos:1;elm:context_link;itc:0;sec:content-canvas\"></a> has confirmed in a test. The app is only available on iOS, because it would have to collect information on Android that could put people at risk.</p>","contentLength":2356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44445180"},{"title":"Firefox 120 to Firefox 141 Web Browser Benchmarks","url":"https://www.phoronix.com/review/firefox-benchmarks-120-141","date":1751469087,"author":"mikece","guid":180865,"unread":true,"content":"<p>For those curious about the direction of Mozilla Firefox web browser performance over the past year and a half, here are web browser benchmarks for every Firefox release from Firefox 120 in November 2023 through the newest Firefox 140 stable and Firefox 140 beta releases from a few days ago. Every major Firefox release was benchmarked on the same Ubuntu Linux system with AMD Ryzen 9 9950X for evaluating the performance and memory usage of this open-source web browser.</p><p>Following the recent <a href=\"https://www.phoronix.com/review/firefox-141-linux-ram\">Firefox 141 beta benchmarks looking at the lower RAM usage</a> on Linux, I got carried away and decided to benchmark every Firefox release going back to Firefox 120 that debuted in November of 2023. Firefox 120 was the breaking point since Firefox 119 and prior ended up running into issues with Selenium / Gecko web driver for automating the benchmarks. So due to that breakage, Firefox 120 was the old cut-off but still a useful exercise in seeing the performance of Firefox on Linux over roughly the past nearly two years.</p><p>The release builds of every major Firefox release from Firefox 120 through Firefox 141 Beta were tested. In the case of Firefox 125, Firefox 125.0.1 was used since Firefox 125.0 binaries were removed due to problems. In addition to looking at the Firefox performance across a variety of web browser benchmarks, the RAM usage was also monitored for reference.</p><p>The same AMD Ryzen 9 9950X desktop system running Ubuntu 25.04 was used for collecting all of these fresh Mozilla Firefox web browser benchmarks. The testing is very straight-forward so let's get right to it.</p>","contentLength":1580,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44444722"},{"title":"Gene therapy restored hearing in deaf patients","url":"https://news.ki.se/gene-therapy-restored-hearing-in-deaf-patients","date":1751468589,"author":"justacrow","guid":180864,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44444626"},{"title":"Exploiting the IKKO Activebuds “AI powered” earbuds (2024)","url":"https://blog.mgdproductions.com/ikko-activebuds/","date":1751465203,"author":"ajdude","guid":180583,"unread":true,"content":"<p>So my journey with these earbuds started after I saw them on <a href=\"https://www.youtube.com/clip/UgkxPHxlV8Uo2L2k_v_RNloYS80CGql6CWH8?ref=blog.mgdproductions.com\" rel=\"noreferrer\">this</a> Mrwhosetheboss video about pointless tech. This device seems to be also popular on TikTok. My suspicions were confirmed, this runs android. So of course i went ahead and bought them.</p><p>245 euros later... and they finally arrived!</p><p>Before we dive further into this, unlike with <a href=\"https://x.com/MarcelD505/status/1785346490635878837?ref=blog.mgdproductions.com\" rel=\"noreferrer\">rabbit</a>, this issue has been properly reported and patched. This is also my first real blog post/disclosure so feedback is appreciated.</p><p>I like how they strapped a USB-c cable to the outside of the box while there is also a smaller one inside the box. They ran out of box space it seems...</p><p>I also wonder if they are legally allowed to use this OpenAI logo (probably not lol)</p><p>Anyways, we aren't here for fancy boxes, lets get to the main point. The device itself boots up to a screen with the time and ChatGPT front and center.</p><p>There are some other AI features available too like translations. But this isn't a review of the device, you can watch <a href=\"https://www.youtube.com/watch?v=p83t0qj9SFM&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">other YouTube videos</a> about that. The ChatGPT animation looks way too similar to the actual app and OpenAI could probably get them in legal trouble for stealing their brand identity. I will also mention that the audio quality is absolute shit if you use their EQ profiles but can be upped to a usable level by tweaking the EQ curves yourself.</p><p>There are also some apps available in the IKKO store, the reason that there is no google play store available is because these apps are modified specifically for the screen on the ActiveBuds, at least, that is what the CEO says about them. We will check that out in a bit. These apps include some music apps like Spotify, but also some gaming apps like, oh god, SUBWAY SURFERS BAYBEEEEE</p><p>Of course all of them unbearable to navigate due to the small screen. However we can now confirm that it most definitely runs android.</p><p>There is sadly no browser available to directly download other apps. And while you can open the native android settings app, clicking the build number 7 times does not enable developer mode. So i couldn't enable adb it seems. Is it locked that well? heh nope.</p><p>Let's just plug it into a pc and see what happens....</p><p>What the fuck, they left ADB enabled. Well, this makes it a lot easier. </p><p>After sideloading the obligatory DOOM, i began checking out how the ChatGPT integration works on the backend. I first started HTTP inspecting the device, however since i couldn't enable the proper system certificates without rooting the device, i couldn't see exactly to what URL it communicated. Fortunately that wasn't really needed.</p><p>Holy shit, holy shit, holy shit, it communicates DIRECTLY TO OPENAI. This means that a ChatGPT key must be present on the device!</p><p>I know that this device can be rooted to get the proper certificates installed because <a href=\"https://www.hovatek.com/forum/thread-32287.html?ref=blog.mgdproductions.com\" rel=\"noreferrer\">a tool</a> exists on all Spreadtrum/Unisoc devices which can be used to unlock the bootloader as long as companies use the default signing keys. This was indeed the case here too. However, i couldn't get past the confirmation screen as the device does not have a volume up key to confirm the unlock. I think you are able to sign your own partitions to make it flash them without an unlocked bootloader but that's a bit too advanced for my own liking.</p><p>So, i went back to the drawing board and just dumped all of the apps from it with an APK extractor tool. After popping the launcher app into JADX, things immediately became concerning.</p><p>The device can communicate to either of these domains.</p><ul><li>api.openai.com\nObvious, the OpenAI API</li><li>chat1.chat.iamjoy.cn\nSeems to be the API for the entire device, including features not related to ChatGPT like the app store. Loading it up in a browser gives a login page.</li><li>chat2.chat.iamjoy.cn\nSame thing as chat1, possibly a backup server?</li><li>openspeech.bytedance.com\nNo idea, might be a speech recogniser backup instead of whisper, haven't seen communication to this from the device.</li><li>www.airdimple.cn\nSeems like an OpenAI API mirror or proxy?</li></ul><p>Knowing this i went hunting for api endpoints and keys. I found a file called SecurityStringsAPI which contained encrypted endpoints and authentication keys. </p><p>You might think, hey that's just base64 idiot, the most basic encoding known to mankind. And well, yeah, it is.</p><p>However, there is a second stage which is handled by a native library which is obfuscated to hell. I am not going to even try to read that. Fortunately i didn't have to. I just sideloaded the app on a different device which was rooted, and well, <a href=\"https://x.com/MarcelD505/status/1785346490635878837?ref=blog.mgdproductions.com\" rel=\"noreferrer\">just like the rabbit apk</a>, it just works!</p><p>Yup, that's an OpenAI key.</p><p>Now, while having this access, we can also expose their (pretty funny) system prompt.</p><p>The device also has another few modes, which are Angry Dan and In-Love Dan. For the angry one you need to confirm you are 18+ because it actually swears a lot.</p><p>The system prompts for these are a bit more boring.</p><p>I also noticed that it logs the chat to another endpoint on the chat1 domain. This is probably just to keep a log of messages since the ChatGPT API does not allow that. Possibly for some Chinese espionage? Well, possibly but not entirely, we will get to that.</p><p>The headers for this request include the message, model, response and the device IMEI as the device id.</p><p>I also sideloaded the store app and found out that the apps seem to be mostly ripped straight from apkpure.com</p><p>After discovering this information, i sent an email to the security department of IKKObuds.</p><p>While waiting for their response i started to investigate their companion app. Wait i forgot to tell you about that? Yeah, these earbuds have a companion app with which you can also directly interface with ChatGPT and see your previous chats from the device. So that's what the logging endpoint is used for! You bind the app by <a href=\"https://youtu.be/IfyIV2oE-tE?feature=shared&amp;t=38&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">scanning a QR code</a> from the device in the \"Membership\" menu.</p><p>So, let's HTTP inspect this app and check out where it gets this information from.</p><p>Alright so it queries this API with your account token and your device id and returns all the chats you have ever had with the device. However, after removing the account token, the request still worked? So this api has no authentication apart from the device id. I feared the worst.</p><p>I found a frame in <a href=\"https://www.youtube.com/watch?v=IfyIV2oE-tE&amp;t=38s&amp;ref=blog.mgdproductions.com\" rel=\"noreferrer\">the tutorial video</a> in which the device id wasn't properly blurred and plugged that into the api.</p><p>YUP, i now had their entire demo device chat history. And as the IMEI has a certain range, you would be able to figure out the chat history of all customers, which may include sensitive details.</p><p>I also added this new discovery to the email chain.</p><p>While that email was waiting for a reply i checked if i could fabricate a linking QR code from a known IMEI to bind the device. (The QR code is not the IMEI itself but something encrypted) I found the API endpoint by looking at the same SecurityStringsAPI, which was less secure than i initially thought because the variable names literally expose the encrypted api endpoints (lol)</p><p>Plugging in the getBindDevQrCode api in postman, i could fabricate a base64 image of the QR code with any IMEI.</p><p>However, using this QR code to try and bind the device to my app resulted in an error, saying that the device has already been bound to someone else. So that has been the only good security implementation up until now.</p><p>However, i lied, this is still a security/privacy issue. Why, you may ask? This exposes the username you set when creating the account for the app. However, there is no username field when creating your account. Only first and last name.</p><p>I created an account with the first name as \"Cheese2\" and the second name as \"Delight2\". Turns out that the username is equal to First name + Last name. When trying to bind that device to an app after it has already been bound to another app, the response includes the name \"Cheese2Delight2\". Great. Doxed.</p><p>So what we can do now is guess IMEI -&gt; generate QR code -&gt; Bind the device if not bound already, or get your full name when the device is already bound. -&gt; Get all your chat history either way if the device is bound or not.</p><p>There is an unbind_dev endpoint????</p><p>Unfortunately that one actually checks account token and does not allow to unbind a random device IMEI. Phew.</p><p>Hey, do you remember that logging endpoint that actually sent your chats you made with ChatGPT to their servers? This one?</p><p>Yeah, that also only used the device id as authentication, so we can send arbitrary text to the companion app of anyone....</p><p>I tried to send some HTML and JS through it to try and exploit the companion app, fortunately they use vue for their app and that has default HTML and JS injection security <a href=\"https://vuejs.org/guide/best-practices/security?ref=blog.mgdproductions.com#html-injection\" rel=\"noreferrer\">built in</a>. But we can still send scams or something to any user.</p><p>Oh hey a reply to my email!</p><p>First of all, from a gmail address? Come on, actually try to have at least some professionalism. Second, OK they are actually doing something about it. (The YouTube channel mentioned is because i said that i will be making a video about this. I have all the footage for it but i hate my voice with a passion so here we are on this blog post :))</p><p>Shortly after this email, they locked down the app and put out an announcement stating that the app will be in maintenance for a week.</p><p>They also wanted to become a sponsor of my empty YouTube channel? What? I don't think that they understood that i would be talking about their horrible security. Anyways.</p><p>The API was now non functional and displayed a maintenance message. After the service period they put out both an app update and a device update. What changed? The endpoint to get the chat history now needs a \"signature\" header. Which is composed of your account token, your device id, language and the current time encoded with a public/private key + a password. </p><p>Anyways, it is now impossible to fetch the chats without having a valid account token. Still doesn't fix the fact that i can generate a QR code with the guessable IMEI and bind the device to an app if it hasn't been bound already. That circumvents this all. </p><p>The device update broke the ChatGPT functionality from functioning on a device which is not the IkkoBuds itself. The keys remain on device and have not been rotated. So if anyone is able to figure out the broken app on another device or the key encryption system, you can still get your very own free OpenAI API key.</p><p>However i just gave up at this moment, also because they never replied with anything after my last email criticizing them for leaving the keys on device. This is now a month and a half ago.</p><p>So, that is it. You can still inject messages into apps of others, link devices that are not already bound to another companion app, thus leaking chat history. And leak first and last names of devices which are bound.</p><p>I am giving up, but if anyone else wants this company to fix this, be my guest.Also if you liked this deep dive, consider supporting me so i will be able to convince myself that buying more strange android devices is worth it lol<a href=\"https://ko-fi.com/mgdproductions?ref=blog.mgdproductions.com\">https://ko-fi.com/mgdproductions</a></p><p>I got this device rooted with help from <a href=\"https://x.com/haro7z?ref=blog.mgdproductions.com\">@haro7z</a></p><p>They are now checking the device's imei before it is able to use the chatgpt integration and are now using a proxy api instead of calling directly to openai. However this proxy api doesn't require any auth and only requires the User-Agent to be set to okhttp/4.9.0 LOL</p><p>They have also FINALLY rotated their old chatgpt api key!</p>","contentLength":11239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44443919"},{"title":"Azure API vulnerability and roles misconfiguration compromise corporate networks","url":"https://www.token.security/blog/azures-role-roulette-how-over-privileged-roles-and-api-vulnerabilities-expose-enterprise-networks","date":1751464795,"author":"ArielSimon","guid":181176,"unread":true,"content":"<p>Token Security researchers have discovered several Azure built-in roles that are misconfigured to be over-privileged - they grant more permissions than intended by Azure.In addition, we discovered another vulnerability in the Azure API that allows attackers to .</p><p>Combined, these two issues create a new attack chain that lets a <strong>weak user gain access to both internal cloud assets and on-premises networks.</strong></p><p>In this report, we detail the research process that led to the discoveries, their implications, and what organizations can do to stay safe against these threats and other identity-driven attacks.</p><p>Before jumping in, let’s discuss some basics.Azure’s permissions model, Azure RBAC (Role-Based Access Control), is, as the name states, based on roles.<p>Roles are basically groups of permissions that can be assigned to principals (users, service principals, groups, etc). When granting a role to a principal, you create a </p>Every  contains three main components:</p><ol role=\"list\"><li> - Who is given the permissions?</li><li> - Which role is assigned? What permissions does it grant? This section states the name of the role and the  and  (allow or deny) that this role is granting.</li><li> - What resources is the principal given access to? The scope can be vast, such as an entire management group or subscription, or more specific, like a resource group or a single resource (e.g., a specific VM or storage account).</li></ol><p>In Azure, there are more than 400 built-in roles, which can be divided into 2 categories:</p><ol role=\"list\"><li> - Roles that grant permissions that apply across all resources and all Azure services in the given scope (e.g., <strong>, , ,</strong> etc.</li><li> - Roles that grant permissions for a specific service or function in the given scope (e.g., <strong><em>Virtual Machine Contributor</em></strong>).</li></ol><p>If you assign the  role over a subscription scope, it will grant permissions to perform actions over all of the resources in this subscription, regardless if they’re storage accounts, virtual machines, or any other resource. But if you assign the <strong><em>Virtual Machine Contributor</em></strong>, it will grant these permissions only to perform actions over virtual machines in the subscription.So we can see the tradeoff here: using service-specific roles is the more secure approach, since you are granting fewer permissions, but the generic roles are easier to use since you need to manage fewer roles assignments.</p><p>Let’s take a look at few roles and their permissions. See if you can tell where things go wrong.</p><p>One of Azure’s built-in roles, called  is a generic role. As you’d expect, it gives read-only permissions over the resources in the chosen scope. Let’s examine its :</p><div><figure><pre><code>\n{\n  \"assignableScopes\": [\n    \"/\"\n  ],\n  \"description\": \"View all resources, but does not allow you to make any changes.\",\n  \"id\": \"/providers/Microsoft.Authorization/roleDefinitions/acdd72a7-3385-48ef-bd42-f606fba81ae7\",\n  \"name\": \"acdd72a7-3385-48ef-bd42-f606fba81ae7\",\n  \"permissions\": [\n    {\n      \"actions\": [\n        \"*/read\"\n      ],\n      \"notActions\": [],\n      \"dataActions\": [],\n      \"notDataActions\": []\n    }\n  ],\n  \"roleName\": \"Reader\",\n  \"roleType\": \"BuiltInRole\",\n  \"type\": \"Microsoft.Authorization/roleDefinitions\"\n}\n</code></pre></figure></div><p>As we can see in the  property, the permission given is , which means it lets you perform any read action in the given scope.</p><p>*Note that this does not include data actions (reading data objects like files in storage accounts, key vault secrets, etc). Those require different, service-specific sensitive permissions.</p><p>Okay, so a generic role giving generic read permissions, that makes sense.What about the permissions of service-specific roles?</p><div><figure><pre><code>\n{\n  \"assignableScopes\": [\n    \"/\"\n  ],\n  \"description\": \"Can read workbooks.\",\n  \"id\": \"/providers/Microsoft.Authorization/roleDefinitions/b279062a-9be3-42a0-92ae-8b3cf002ec4d\",\n  \"name\": \"b279062a-9be3-42a0-92ae-8b3cf002ec4d\",\n  \"permissions\": [\n    {\n      \"actions\": [\n        \"microsoft.insights/workbooks/read\",\n        \"microsoft.insights/workbooks/revisions/read\",\n        \"microsoft.insights/workbooktemplates/read\"\n      ],\n      \"notActions\": [],\n      \"dataActions\": [],\n      \"notDataActions\": []\n    }\n  ],\n  \"roleName\": \"Workbook Reader\",\n  \"roleType\": \"BuiltInRole\",\n  \"type\": \"Microsoft.Authorization/roleDefinitions\"\n}\n</code></pre><figcaption>Workbook Reader role definition</figcaption></figure></div><p>If we analyze the  property, we can see that as the description states, it grants read permissions to a few workbook-related objects. So a service-specific role that grants access to a specific service! So far so good.</p><h3><strong>Managed Applications Reader</strong></h3><p>Now, let’s check the <strong><em>Managed Applications Reader</em></strong> role, which its description is <strong>“Lets you read resources in a managed app and request JIT access.”</strong></p><div><pre><code>\n{\n  \"assignableScopes\": [\n    \"/\"\n  ],\n  \"description\": \"Lets you read resources in a managed app and request JIT access.\",\n  \"id\": \"/providers/Microsoft.Authorization/roleDefinitions/b9331d33-8a36-4f8c-b097-4f54124fdb44\",\n  \"name\": \"b9331d33-8a36-4f8c-b097-4f54124fdb44\",\n  \"permissions\": [\n    {\n      \"actions\": [\n        \"Microsoft.Resources/deployments/*\",\n        \"Microsoft.Solutions/jitRequests/*\",\n        \"*/read\"\n      ],\n      \"notActions\": [],\n      \"dataActions\": [],\n      \"notDataActions\": []\n    }\n  ],\n  \"roleName\": \"Managed Applications Reader\",\n  \"roleType\": \"BuiltInRole\",\n  \"type\": \"Microsoft.Authorization/roleDefinitions\"\n}\n</code></pre></div><p>We can see it has access to deployments, jitRequests, and… So this role, which is supposed to grant access to read managed apps and JIT access, actually also allows the user to read every Azure resource?This is clearly not what the description says, and certainly not what someone assigning the <strong><em>Managed Applications Reader</em></strong> role would expect.</p><p><strong>Essentially, the role’s name and description are misleading the user into thinking the role grants specific permissions, when in fact it grants generic permissions to every resource.</strong></p><p>I saw this and thought to myself, okay, this is just a read permission... how bad can it be?Well, I was seriously wrong.</p><p>Let’s dive into what we can actually do with this permission and how can it be useful for an attacker.</p><p>Since there are so many actions possible here, I divided them into three categories and gave some examples for each.</p><ul role=\"list\"><li><strong>Automation Accounts, Deployment scripts, Web applications</strong> - This one really surprised me: this permission actually allows you to read source code and environment variables of scripts and applications. The common thing among the three services I listed here, is that they all interact with your environment, <strong>which makes them very likely to contain credentials and secrets!</strong></li></ul><p>:</p><ul role=\"list\"><li><strong>Storage accounts, container registries, databases</strong> - Enumerating all instances and their metadata to find sensitive data spots</li><li> - Helps identify resources that are considered critical or sensitive</li><li> - Find DB exports, storage account backups and more</li></ul><ul role=\"list\"><li> - Know about who can access what. Useful for planning privilege escalation paths</li><li><strong>Diagnostics settings, alerts, and log analytics workspaces</strong> - Know what is being logged, and where, and view security alerts. Useful for OpSec and detection avoidance</li><li><strong>Network configurations, network security groups, public IPs, virtual network gateways</strong> - Further plan attack paths and network advancements</li><li> - Listing all vaults and their metadata</li></ul><p>So you might think to yourself - Okay that’s cool… but I don’t use the <strong>Managed Applications Reader</strong> role, so I am safe, right?Well, think again.</p><p>After analyzing all Azure built-in roles, I found that this problem (having an un-needed  permission, basically including the  role) recurs in </p><div><div><table><tbody><tr><td><div>\nMicrosoft.OperationalInsights/workspaces/analytics/query/action <p>\nMicrosoft.OperationalInsights/workspaces/search/action</p>\nMicrosoft.Support/*\n</div></td></tr><tr><td>Log Analytics Contributor</td><td><div>\nMicrosoft.ClassicCompute/virtualMachines/extensions/*<p>\nMicrosoft.ClassicStorage/storageAccounts/listKeys/action</p>\nMicrosoft.Compute/virtualMachines/extensions/*</div><div>\nMicrosoft.HybridCompute/machines/extensions/write\nMicrosoft.Insights/alertRules/*<p>\nMicrosoft.Insights/diagnosticSettings/*</p>\nMicrosoft.OperationalInsights/*<p>\nMicrosoft.OperationsManagement/*</p>\nMicrosoft.Resources/deployments/*<p>\nMicrosoft.Resources/subscriptions/resourcegroups/deployments/*</p>\nMicrosoft.Storage/storageAccounts/listKeys/action</div></td></tr><tr><td>App Compliance Automation Administrator</td><td><div>\nMicrosoft.AppComplianceAutomation/*<p>\nMicrosoft.Storage/storageAccounts/blobServices/write</p>\nMicrosoft.Storage/storageAccounts/fileservices/write</div><div>\nMicrosoft.Storage/storageAccounts/listKeys/action\nMicrosoft.Storage/storageAccounts/write<p>\nMicrosoft.Storage/storageAccounts/blobServices/generateUserDelegationKey/action</p>\nMicrosoft.Storage/storageAccounts/read<p>\nMicrosoft.Storage/storageAccounts/blobServices/containers/read</p>\nMicrosoft.Storage/storageAccounts/blobServices/containers/write<p>\nMicrosoft.Storage/storageAccounts/blobServices/read</p>\nMicrosoft.PolicyInsights/policyStates/queryResults/action<p>\nMicrosoft.PolicyInsights/policyStates/triggerEvaluation/action</p>\nMicrosoft.Resources/resources/read<p>\nMicrosoft.Resources/subscriptions/read</p>\nMicrosoft.Resources/subscriptions/resourceGroups/read<p>\nMicrosoft.Resources/subscriptions/resourceGroups/resources/read</p>\nMicrosoft.Resources/subscriptions/resources/read<p>\nMicrosoft.Resources/subscriptions/resourceGroups/delete</p>\nMicrosoft.Resources/subscriptions/resourceGroups/write<p>\nMicrosoft.Resources/tags/read</p>\nMicrosoft.Resources/deployments/validate/action<p>\nMicrosoft.Security/automations/read</p>\nMicrosoft.Resources/deployments/write<p>\nMicrosoft.Security/automations/delete</p>\nMicrosoft.Security/automations/write<p>\nMicrosoft.Security/register/action</p>\nMicrosoft.Security/unregister/action</div></td></tr><tr><td>App Compliance Automation Reader</td></tr><tr><td>Managed Application Contributor Role</td><td><div>\nMicrosoft.Solutions/applications/*<p>\nMicrosoft.Solutions/register/action</p>\nMicrosoft.Resources/subscriptions/resourceGroups/*</div><div>\nMicrosoft.Resources/deployments/*</div></td></tr><tr><td>Managed Application Operator Role</td><td><div>\nMicrosoft.Solutions/applications/read<p>\nMicrosoft.Solutions/*/action</p></div></td></tr><tr><td>Managed Application Operator Role</td><td><div>\nMicrosoft.Solutions/applications/read<p>\nMicrosoft.Solutions/*/action</p></div></td></tr><tr><td>Managed Applications Reader</td><td><div>\nMicrosoft.Resources/deployments/*<p>\nMicrosoft.Solutions/jitRequests/*</p></div></td></tr><tr><td><div>\nMicrosoft.AlertsManagement/alerts/*<p>\nMicrosoft.AlertsManagement/alertsSummary/*</p>\nMicrosoft.Insights/actiongroups/*</div><div>\nMicrosoft.Insights/activityLogAlerts/*\nMicrosoft.Insights/AlertRules/*<p>\nMicrosoft.Insights/components/*</p>\nMicrosoft.Insights/createNotifications/*<p>\nMicrosoft.Insights/dataCollectionEndpoints/*</p>\nMicrosoft.Insights/dataCollectionRules/*<p>\nMicrosoft.Insights/dataCollectionRuleAssociations/*</p>\nMicrosoft.Insights/DiagnosticSettings/*<p>\nMicrosoft.Insights/eventtypes/*</p>\nMicrosoft.Insights/LogDefinitions/*<p>\nMicrosoft.Insights/metricalerts/*</p>\nMicrosoft.Insights/MetricDefinitions/*<p>\nMicrosoft.Insights/Metrics/*</p>\nMicrosoft.Insights/notificationStatus/*<p>\nMicrosoft.Insights/Register/Action</p>\nMicrosoft.Insights/scheduledqueryrules/*<p>\nMicrosoft.Insights/webtests/*</p>\nMicrosoft.Insights/workbooks/*<p>\nMicrosoft.Insights/workbooktemplates/*</p>\nMicrosoft.Insights/privateLinkScopes/*<p>\nMicrosoft.Insights/privateLinkScopeOperationStatuses/*</p>\nMicrosoft.Monitor/accounts/*<p>\nMicrosoft.OperationalInsights/workspaces/write</p>\nMicrosoft.OperationalInsights/workspaces/intelligencepacks/*<p>\nMicrosoft.OperationalInsights/workspaces/savedSearches/*</p>\nMicrosoft.OperationalInsights/workspaces/search/action<p>\nMicrosoft.OperationalInsights/workspaces/sharedKeys/action</p>\nMicrosoft.OperationalInsights/workspaces/storageinsightconfigs/*\nMicrosoft.AlertsManagement/smartDetectorAlertRules/*<p>\nMicrosoft.AlertsManagement/actionRules/*</p>\nMicrosoft.AlertsManagement/smartGroups/*<p>\nMicrosoft.AlertsManagement/migrateFromSmartDetection/*</p>\nMicrosoft.AlertsManagement/investigations/*<p>\nMicrosoft.AlertsManagement/prometheusRuleGroups/*</p>\nMicrosoft.Monitor/investigations/*</div></td></tr><tr><td><div>\nMicrosoft.OperationalInsights/workspaces/search/action</div></td></tr><tr><td>Resource Policy Contributor</td><td><div>\nMicrosoft.Authorization/policyassignments/*<p>\nMicrosoft.Authorization/policydefinitions/*</p>\nMicrosoft.Authorization/policyexemptions/*</div><div>\nMicrosoft.Authorization/policysetdefinitions/*\nMicrosoft.PolicyInsights/*<p>\nMicrosoft.Resources/deployments/*</p>\nMicrosoft.Support/*\n</div></td></tr></tbody></table></div></div><p><strong>This risk exists in every user, service principal, managed identity, or group members that are assigned one of these seemingly innocent service-specific roles.</strong></p><p>As we can see in the table, some of the roles have more specific read permissions in addition to the , like the <code>Microsoft.Solutions/applications/read</code> permission in <strong><em>Managed Application Operator Role</em>,</strong> which is already included in the * expression. This shows that one of the permissions is redundant, and may point to it being added by mistake or by laziness.The case of <strong><em>App Compliance Automation Reader</em></strong> is even more absurd, since it only has the * permission, <strong>and is essentially identical to the all mighty and powerful generic  role</strong>.</p><p>So this is pretty bad… but can we exploit it even further?</p><p>I wanted to make this attack scenario even stronger, and find more quirks that are possible using the  permission.So I took a look its documentation:</p><p>So we understand that identities with read permissions should not be allowed to read secrets (which makes sense because those secrets can then be used to elevate privileges to more than read-only, access more resources, etc.)Using <a href=\"https://www.azadvertizer.net/\">this</a> really nice website, I saw that there are  (!) Azure actions that are included in the  expression (every operation that ends with ‘/read’).</p><p>If I can find one action, out of those 9,618, that will allow me to leak a secret, I will have a serious vulnerability here!</p><p>After going through many permissions in the list, I found one that piqued my interest:</p><p>What is a VPN link? What is a connection? What is a shared key? I don’t know, but it has the word ‘key’ in it, so it must be interesting :)</p><p>So we have an API call that retrieves some sort of a secret even if I only have read permissions. But why is that happening? What is the mistake that the Azure developer made here?</p><p>In Azure, API calls are implemented with different HTTP methods. For example, the <a href=\"https://learn.microsoft.com/en-us/rest/api/compute/virtual-machines/list\" target=\"_blank\">Virtual Machines - List</a> API call is implemented with , and <a href=\"https://learn.microsoft.com/en-us/rest/api/compute/virtual-machines/install-patches\" target=\"_blank\">Virtual Machines - Install Patches</a> is implemented with . That makes sense, because the  API requires data from the user (a body in the request), while the  only retrieves data from the server, and does not require any data from the user.But what I found out through some blackbox research, documentation reading, and a lot of trial and error, is that Azure chose to enforce permissions by varying the HTTP methods used in the API requests. Let me explain:<p>It seems that users with read permissions alone (like *</p>can issue  requests to the API, but will get denied access if they attempt to issue  requests.Regular read operations, like <a href=\"https://learn.microsoft.com/en-us/rest/api/compute/virtual-machines/list?view=rest-compute-2024-07-01&amp;tabs=HTTP\" target=\"_blank\"></a> are implemented with a  as we’ve seen, while operations that read sensitive values, such as<a href=\"https://learn.microsoft.com/en-us/rest/api/storagerp/storage-accounts/list-keys\" target=\"_blank\"><strong>Storage Accounts - List Keys</strong></a> or <a href=\"https://learn.microsoft.com/en-us/rest/api/cosmos-db-resource-provider/database-accounts/list-connection-strings\" target=\"_blank\"><strong>Database Accounts - List Connection Strings</strong></a>, are implemented with  requests - <strong>even though the request body is empty</strong>. This is to make sure that permissions enforcement is in place, and identities with read permissions alone would not be able to access those sensitive APIs.</p><p>To prove this, I performed an API call to a URL that doesn’t exist, using a read-only identity. When I issue a GET request, I get  error. But when I issue a POST request to the same non-existent URL, I get &nbsp; error. This further proves that permission enforcement is determined by the HTTP method, and not by the checking specific API call and whether or not I have access to it.</p><p>So why is that problematic? Because when you choose to design your software in a way that doesn’t make sense and isn’t intuitive, your developers are bound to make some critical mistakes…My assumption is that at some point, some Azure developer must have accidentally implemented an API call that retrieves a secret with the method that makes the most sense, which is GET, because there is no body needed in the request. And by doing so, they created a vulnerability…</p><p>If we take a look at the API call that we found earlier, we see that our theory was right! It was accidentally implemented with a GET, allowing read-only users to fetch the key!</p><p>So now we have a vulnerability. The only thing that is left is to find out what that shared key is…</p><p>VPN Gateway is an Azure service that acts as a VPN, allowing customers to connect networks over the internet. Organizations mainly use this service to support hybrid environments (connecting cloud and on-premise), as well as connecting between on-premise sites (aka Site-to-Site, or S2S). Individual users can also connect to it, for scenarios such as working remotely (aka Point-to-Site, or P2S).</p><p>P2S connection types require additional authentication (certificate/radius server/Entra ID login).But a Site-to-Site (S2S) connection type <strong>requires only the pre-shared key (PSK)</strong>, which is a password that is shared between the VPN devices of each site, and the Azure VPN Gateway service. And yes, that’s the key we can fetch using our vulnerability!</p><ol role=\"list\"><li>The attacker compromises a weak identity (identity with read permissions or one that is assigned one of the many over-privileged roles we listed earlier).</li><li>The attacker fetches the VPN Gateway pre-shared key.</li><li>Using the key, the attacker connects to the S2S connection and accesses internal networks, including VPCs and on-premise networks that are connected to the same Azure VPN Gateway.</li></ol><p>The video demonstrates that when a principal with no privileges is assigned the  role, which is only supposed to grant access to read logs but as we know by now is over-privileged, it has permissions to fetch the VPN pre-shared key, which is then used to connect to the VPN.</p><p>With this access, attackers can create a “rogue site”, essentially granting them access to cloud resources, other sites, and secure networks of the target organization.</p><p>Depending on the configuration of the VPN Gateway, this may work only when the connection attempt is originating from the IP address configured in the Azure VPN gateway.</p><p>In that case, an attacker that has some on-premise foothold can use this trick to access sensitive cloud infrastructure and data.</p><p>There are even more sensitive values you can access using the Reader privileges. <a href=\"https://binarysecurity.no/posts/2024/11/apim-privesc\" target=\"_blank\">Excellent research published by Binary Security</a> shows how you can escalate your Reader privileges in Azure API Management service by fetching subscription keys and SSO tokens, effectively resulting in a full takeover of the service.</p><p>After reporting this issue to Microsoft, their response was that this is a ‘low severity’ security issue and they decided to not fix it. I later noticed some major documentation changes based on my report: All 10 over-privileged roles’ documentations were added the following sentence:</p><p>So they chose to fix the documentation instead of fixing the actual issue, which still endangers customers.</p><p>This was acknowledged by Microsoft as an ‘Important’ severity vulnerability, and the issue was fixed. They also awarded me with a US$7,500 bounty award.</p><p>I was happy to find out that the fix was not changing the API HTTP method to POST.In this newly created <a href=\"https://learn.microsoft.com/en-us/azure/vpn-gateway/roles-permissions\" target=\"_blank\">documentation page</a> that explains Azure VPN permissions, it is stated that to fetch/update the PSK, you now need to have the <code>Microsoft.Network/connections/sharedKey/action</code> permission:</p><h2>Mitigations and recommendations</h2><h3>Audit the use of the problematic roles</h3><p>As we saw, the over-privileged roles issue is not going to be fixed. Refrain from using those roles in your environment, and use alternatives.</p><h3><strong>Use particular and limited scopes</strong></h3><p>Instead of assigning roles on a wide scope, such as an entire management group or subscription level, limit them only to the specific resource that the principal needs access to, or to a resource group if access to multiple resources is needed.</p><p>Instead of using built-in roles that are not managed by you, use custom roles and grant only the needed permissions. Replace your current role assignments (at least of the problematic roles mentioned here) with custom roles that have fine-grained permissions.</p><p>Securing cloud environments and their identities is not easy.</p><p>The shared responsibility model, which is adopted and presented by all major cloud providers, clearly states which security tasks are the cloud provider’s responsibility, and which are the customer’s. But the issues we discussed here are in the gray area: when the cloud provider is giving you a service that is supposed to help you with identities and permissions management, but in fact misleads you into making dangerous decisions, who is to blame? Is it the cloud provider who caused you to create the security issue, or is it you, who actually created it?</p><p>There is no easy answer here, but one thing is clear - don’t fully and blindly trust the services that are given to you. Always double check, and be proactive and vigilant about the security of your organization.</p><p>When you have many identities and big infrastructure, this becomes a real challenge. When you have to question every role, permission, and API call, things can get complicated. But luckily, there is a solution.</p><p>To learn more about Token Security and how we can help secure your Azure environment (and many more), book a demo <a href=\"https://www.token.security/book-a-demo\" target=\"_blank\">here</a>.</p><p>I’m Ariel Simon, a security researcher from the Token research team, primarily focused on uncovering vulnerabilities and finding new attack techniques in cloud environments.Feel free to contact me on LinkedIn or via email: <a href=\"mailto:ariels@token.security\">ariels@token.security</a>.</p>","contentLength":21253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44443832"},{"title":"Private sector lost 33k jobs, badly missing expectations of 100k increase","url":"https://www.cnbc.com/2025/07/02/adp-jobs-report-june-2025.html","date":1751463633,"author":"ceejayoz","guid":180582,"unread":true,"content":"<div data-test=\"InlineImage\"><div><div><div>People visit booths set up by the City of Sunrise and their police department at the Mega JobNewsUSA South Florida Job Fair at the Amerant Bank Arena on April 30, 2025 in Sunrise, Florida.</div><div>Joe Raedle | Getty Images</div></div></div></div><div><p>Private sector hiring unexpectedly contracted in June, payrolls processing firm ADP said Wednesday, in a possible sign that the economy may not be as sturdy as investors believe as they bid the  back up to record territory to end the month.</p><p>Private payrolls lost 33,000 jobs in June, the ADP report showed, the first decrease since March 2023. Economists polled by Dow Jones forecast an increase of 100,000 for the month. The May job growth figure was revised even lower to just 29,000 jobs added from 37,000.</p><p>\"Though layoffs continue to be rare, a hesitancy to hire and a reluctance to replace departing workers led to job losses last month,\" Nela Richardson, ADP's chief economist, said in a press release published Wednesday morning.</p><p>To be sure, the ADP report has a spotty track record on predicting the subsequent government jobs report, which investors tend to weigh more heavily. May's soft ADP data ended up differing significantly from the <a href=\"https://www.cnbc.com/2025/06/06/jobs-report-may-2025.html\">monthly jobs report figures</a> that came later in the week.</p><p>This week, the government's nonfarm payrolls report will be out on Thursday with economists expecting a healthy 110,000 increase for June, per Dow Jones estimates. Economists are expecting the unemployment rate to tick higher to 4.3% from 4.2%. Some economists could revise down their jobs reports estimates following ADP's data.</p><p>Weekly jobless claims data is also due Thursday, with economists penciling in 240,000. This string of labor stats comes during a shortened trading week, with the market closing early on Thursday and remaining dark on Friday in honor of the July Fourth holiday.</p></div><h2>Service roles hit hardest</h2><div><p>The bulk of job losses came in service roles tied to professional and business services and health and education, according to ADP. Professional/business services notched a decline of 56,000, while health/education saw a net loss of 52,000.</p><p>Financial activity roles also contributed to the month's decline with a drop of 14,000 on balance.</p><p>But the contraction was capped by payroll expansions in goods-producing roles across industries such as manufacturing and mining. All together, goods-producing positions grew by 32,000 in the month, while payrolls for service roles overall fell by 66,000.</p><p>The Midwest and Western U.S. saw the strongest contractions in June, declining by 24,000 and 20,000, respectively. Meanwhile, the Northeast shed 3,000 roles. The Southern U.S. was the sole region tracked by the ADP to see payrolls expand on net in the month, recording an increase of 13,000 positions.</p><p>The smallest firms tended to see more job losses in the month than their larger counterparts. In fact, businesses with more than 500 employees saw the biggest payroll growth in the month with an increase of 30,000, per ADP. By comparison, businesses with fewer than 20 employees accounted for 29,000 lost roles on net.</p><p>Annual income growth decreased modestly from May for both job stayers and hoppers. The rate of pay increase for those staying in their jobs ticked down to 4.4% from 4.5%, while those getting new roles slid to 6.8% from 7%.</p><p>The S&amp;P 500 is up more than 4% for the year, posting a stunning comeback in the second quarter after worries about President <a href=\"https://www.cnbc.com/donald-trump/\">Donald Trump</a>'s tariff fights nearly sent the benchmark into a bear market.</p><p><em>Clarification: The ADP report issued Wednesday referred to June data. That was not clear in an earlier version.</em></p></div>","contentLength":3573,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44443622"},{"title":"What I learned gathering nootropic ratings (2022)","url":"https://troof.blog/posts/nootropics/","date":1751462977,"author":"julianh65","guid":181175,"unread":true,"content":"<p>All data and code can be found <a href=\"https://github.com/LeoGrin/nootroflix\">here</a>.</p><h3>\nHow to interpret these ratings\n<a href=\"https://troof.blog/posts/nootropics/#how-to-interpret-these-ratings\">#</a></h3><ul><li><p>0 means a substance was totally useless, or had so many side effects you couldn’t continue taking it.</p></li><li><p>1 - 4 means subtle effects, maybe placebo but still useful.</p></li><li><p>5 - 9 means strong effects, definitely not placebo.</p></li></ul><p>In order of importance (I think):</p><ul><li><strong>Lack of random allocation</strong></li></ul><p>The people who entered their rating on my recommender system were not randomly assigned to try specific nootropics. Thus we can expect a (usually positive) correlation between “I’m likely to try nootropic A” and “Nootropic A might work on me”.</p><p>This means that my estimations don’t represent the ratings a random person would get on average (which would usually be lower); they’re instead a prediction of the rating that a person would give to a nootropic if they decided to take it organically. Take this into account when transferring the results to yourself.</p><p>You take a pill. It makes you feel good. You go on a website which asks you how good the pill is. You say it’s awesome. Little did you know that it was, in fact, merely good.</p><p>Take this into account when reading these ratings, especially if improving your mood is not your main goal.</p><ul><li><strong>Lack of control and blinding</strong></li></ul><p>All these biases sure seem inconvenient to estimate some “true rating”, but maybe they are not a problem if we just want to compare nootropics? Perhaps in some cases, but not always: lack of random allocation probably creates more bias for medications like SSRIs, which are usually prescribed to people with depression, and self-reported ratings inflation or placebo effect may be especially relevant for substances like Psilocybin, which can produce visual effects, or for hyped-up nootropics.</p><p>I’m mostly going to ignore these issues in the rest of this article, but keep them in mind and use your best judgment when comparing ratings. This also means that you shouldn’t use this data for anything more serious than reducing your nootropics search space.</p><h3>\nMean rating for each nootropic\n<a href=\"https://troof.blog/posts/nootropics/#mean-rating-for-each-nootropic\">#</a></h3><p>Here are the results (<strong>click to see all nootropics</strong>):</p><h3>\nProbability of positive effect\n<a href=\"https://troof.blog/posts/nootropics/#probability-of-positive-effect\">#</a></h3><p>Given the scale I used, the  is not so easy to interpret. Another metric I estimated was the <em>probablity that the effect of a nootropic on a user was positive</em>. For my scale, 0 corresponds to a neutral or negative effect, and higher ratings correspond to more-or-less confidence in a positive effect. </p><p>Here are the results (<strong>click to see all nootropics</strong>):</p><h3>\nProbability of life-changing effect\n<a href=\"https://troof.blog/posts/nootropics/#probability-of-life-changing-effect\">#</a></h3><p>Here are the results (<strong>click to see all nootropics</strong>):</p><h3>\nUsefulness for different usages\n<a href=\"https://troof.blog/posts/nootropics/#usefulness-for-different-usages\">#</a></h3><p>With my data, identifying the usefulness of a nootropic for different use cases seems hard. I’ll set this aside for future investigation (if it’s possible at all).</p><p>The risks of prescribed medications such as <a href=\"https://slatestarcodex.com/2017/12/28/adderall-risks-much-more-than-you-wanted-to-know/\">Adderall</a>, <a href=\"https://slatestarcodex.com/2014/07/07/ssris-much-more-than-you-wanted-to-know/\">SSRIs</a> and <a href=\"https://www.gwern.net/Modafinil#side-effects\">Modafinil</a> are quite well-documented, but information on the risks of weirder nootropics is scarce.</p><p>For instance, what would you guess is the probability of becoming tolerant to Phenylpiracetam (apparently between 10 and 20%)? Of becoming addicted to Kratom (apparently between 15 and 25%)?</p><p><strong>Click to see all nootropics</strong>:</p><p><strong>Click to see all nootropics</strong>:</p><p><strong>Click to see all nootropics</strong>:</p><p><strong>Click to see all nootropics</strong>:</p><p><em>EDIT: As pointed out by a commenter, the estimated probabilities of long term side effects are somewhat surprising. I’m not completely sure what people had in mind when entering “long term side effects”, and thus I’m not completely sure how to interpret these probabilities.</em></p><p><strong>Click to see all nootropics</strong>:</p><h3>\nLifestyle is a strong nootropic\n<a href=\"https://troof.blog/posts/nootropics/#lifestyle-is-a-strong-nootropic\">#</a></h3><p>Among the different sport categories, weightlifting is noticeably better rated (+ 0.5 points on adjusted mean) and is actually among the very best nootropics in my database. Furthermore, very few issues are reported. This is very impressive, and <a href=\"https://www.reddit.com/r/slatestarcodex/comments/9h2jbi/you_should_probably_lift_weights/\">maybe you should consider trying it</a>. You may be wondering why you should trust this self-reported data if you don’t trust your gym bros friends who can’t stop talking about how weightlifting changed their life. I’m just saying that there are a lot of people saying the same things in my data. Maybe they’re all the same gym bros, but maybe it means that you should start taking them seriously.</p><p>If sport and sleep are the nootropic low-hanging fruits, diets are the fruits you can maybe reach on tiptoes. For instance, the Paleo diet, with a mean rating of 5.4, is in the top-20, and Intermittent Fasting, with a mean rating of 5, is in the top-30.</p><p>Low carbs diets (Keto, Carnivore, Paleo) are rated much higher than Vegetarian or Vegan diets, though the Vegan diet is first for <em>probability of changing your life</em>, with around 5%[3-8% 95%] probability, similar to the Paleo diet.</p><p>Comparing issues reported, people often stop the Carnivore and Keto diets because of side effects, with a probability between 10 and 20%, much higher than the Paleo diet (between 2 and 10%), and somewhat higher than the Vegetarian and Vegan diets (between 7 and 17%).</p><p>The Paleo diet seems to be the winner here, though I’m wondering how its particular branding might be skewing results.</p><h4>\nOther lifestyle interventions\n<a href=\"https://troof.blog/posts/nootropics/#other-lifestyle-interventions\">#</a></h4><p>Meditation (mean rating = 5.8), bright lights in the morning (5), cold shower (4.7), and masturbation abstinence (4.1) also got impressive to pretty good ratings.</p><h3>\nMost famous nootropics aren’t that good\n<a href=\"https://troof.blog/posts/nootropics/#most-famous-nootropics-arent-that-good\">#</a></h3><p>What are the first things that come to mind when you think of nootropics? Piracetam? Ashwagandha? Ginseng? Theanine? Most of these common nootropics actually got relatively poor ratings. This is compared to potent prescription-only medications like Adderall, but also to sport (see above) and to a lot of lesser-known nootropics.</p><p>The plots below shows all the common-but-mediocre nootropics (red rectangle) and the uncommon-but-great nootropics (green rectangle):\n<img src=\"https://troof.blog/posts/nootropics/ggrepel_mean_ratings.jpeg\" alt=\"ggrepel_mean_ratings\"><img src=\"https://troof.blog/posts/nootropics/ggrepel_life_changing_ratings.jpeg\" alt=\"ggrepel_life_changing_ratings\"></p><p><a href=\"https://en.wikipedia.org/wiki/Selank\">Selank</a>, <a href=\"https://en.wikipedia.org/wiki/Semax\">Semax</a>, <a href=\"https://en.wikipedia.org/wiki/Cerebrolysin\">Cerebrolysin</a>, <a href=\"https://en.wikipedia.org/wiki/BPC-157\">BPC-157</a> are all peptides, and they are all in the green “uncommon-but-great” rectangle above. Their mean ratings are excellent, but their probabilities of changing your life are especially impressive: between 5 and 20% for Cerebrolysin (which matches <a href=\"https://www.reddit.com/r/Nootropics/comments/bg7vup/a_threeweek_review_of_the_strongest_nootropic/\">anecdotal</a><a href=\"https://www.reddit.com/r/Nootropics/comments/30l3hz/extreme_success_with_cerebrolysin/\">reports</a>), between 2 and 13% for BPC-157, and between 3 and 7% for Semax.</p><p>So why are they so unpopular? It may be because they’re really scary. Take Cerebrolysin:</p><p>Perhaps relatedly, these substances are mostly used in Russia and the former USSR and have an unclear legal status in other countries. This may also explain their unpopularity.</p><p>But are peptides really that dangerous? The plots above show that the addiction probabilities are tiny and that the tolerance and long-term side effect probabilities are below 5%. Some of these substances, like Cerebrolysin, have small sample sizes, so I quickly checked the literature. A Cochrane review from 2020 found that “Moderate‐quality evidence also indicates a potential increase in non‐fatal serious adverse events with Cerebrolysin use.” with a seemingly dose-dependent effect, while the <a href=\"https://link.springer.com/article/10.1007/s10072-017-3214-0\">other</a><a href=\"https://www.mdpi.com/1424-8247/14/12/1297\">meta-analyses</a> I found reported that “Safety aspects were comparable to placebo.” So it is a bit unclear, but I would be cautious, as I trust the Cochrane review more.\nFor other peptides, we’re not lucky enough to have a Cochrane review, but the few studies I can find tell me they’re safe: <a href=\"https://www.semanticscholar.org/paper/7410dace64d32b7b42740aec059b3feea31ff757\">“Semax is well-tolerated with few side-effects.\"</a>, <a href=\"https://doi.org/10.2174/092986712803414015\">“BPC-157 is free of side effects.\"</a>, <a href=\"https://doi.org/10.1007/s10787-006-1531-7\">“BPC-157 is a safe therapeutic agent.\"</a>, <a href=\"https://doi.org/10.2174/138161211796196954\">“BPC-157 is a safe anti-ulcer peptidergic agent.\"</a> We need more data on the safety of peptides but we can already say: “Probably less dangerous than they look.”</p><p>Bonus: Reddit user OilofOrigano <a href=\"https://www.reddit.com/r/Cerebrolysin/comments/jdlpcd/cerebrolysin_poll/\">has been collecting data on Cerebrolysin</a>, you can check the results <a href=\"https://docs.google.com/spreadsheets/d/1XFKiWEHPJZ0DG-e0k7SoXgTNNG_52vU9gxqbMAegbxM/edit#gid=36862191\">here</a>. Results are quite positive (see below), though I think it was mostly collected on a subreddit dedicated to Cerebrolysin, so I would expect the results to be overly positive.\n</p><h3>\nZembrin maybe isn’t interesting?\n<a href=\"https://troof.blog/posts/nootropics/#zembrin-maybe-isnt-interesting\">#</a></h3><blockquote><p>Of 37 kanna users, 20 used Zembrin and 17 used something else. The subgroup who used Zembrin reported a mean effectiveness of 6.88, which beats out modafinil to make it highest on the list. After ad hoc Bayesian adjustment, it was 6.72, second only to modafinil as the second most effective nootropic on the list. This really excites me - I’ve felt like Zembrin was special for a while, and this is the only case of a newer nootropic on the survey beating the mainstays. And it’s a really unexpected victory. The top eight substances in the list are all either stimulants, addictive, illegal in the US, or all three. Zembrin is none of those, and it beats them all.</p></blockquote><p>How can we explain the difference? Maybe the surveyed population is a bit different in my case? But the simplest answer is surely sample noise. SA’s ratings are based on 20 people who tried Zembrin and 17 people who tried non-Zembrin Kanna. My ratings are based on 45 people who tried Zembrin and 49 people who tried non-Zembrin Kanna. Where it gets more complicated is that SA has subsequent data:</p><blockquote><p>Based on these preliminary results, I wrote up a short page about Zembrin on my professional website, Lorien Psychiatry, and I asked anyone who planned to try it to preregister with me so I could ask them how it worked later. 29 people preregistered, of whom I was able to follow up with and get data from 22 after a few months. Of those 22, 16 (73%) said it seemed to help, 3 (14%) said it didn’t help, and another 3 (14%) couldn’t tell because they had to stop taking it due to side effects (two headaches, one case of “psychedelic closed-eye visuals”). Only 13 of the 22 people were willing to give it a score from 1-10 (people hate giving 1-10 scores!), and those averaged 5.9 (6.3 if we don’t count people who stopped it immediately due to side effects). That’s a little lower than on the survey, but this was a different population - for example, many of them in their answers specifically compared it to prescription antidepressants they’d taken, whereas the survey-takers were comparing it to nootropics. Although these findings are not very useful without a placebo control, they confirm that most people who take Zembrin at least subjectively find it helpful.</p></blockquote><p>Do you trust the (a bit) bigger sample size, or the preregistration? Your choice!</p><p>What is the best stimulant to take if you have trouble focusing? Adderall and Dexedrine have the best ratings. The latter is perhaps more likely to change your life, and both are far above Ritalin and Modafinil. I was surprised about Dexedrine, which I didn’t know about. Still, it does have higher patient ratings than Adderall on Drugs.com and the like, and there seems to be a debate in the literature on which is more effective for ADHD. This is all from <a href=\"https://astralcodexten.substack.com/p/know-your-amphetamines?s=r\">this article by Scott Alexander</a>, which is a great explainer on the different amphetamines used to treat ADHD (and a reminder that most of these substances are quite well-studied in the literature, and that you shouldn’t base any decision on my data!).</p><figcaption align=\"center\"><i>Guess where Methylphenidate is the only easily available stimulant?</i></figcaption><p>In the plot below, I’ve included most of the medications recommended for depression <a href=\"https://lorienpsych.com/2021/06/05/depression/#23_What_kind_of_medications_help_with_depression\">here</a>, and most of the supplements recommended <a href=\"https://lorienpsych.com/2021/06/05/depression/#24_What_kind_of_supplements_help_with_depression\">here</a>.\n</p><p>More surprising, using bright light in the morning was ranked second. Interpreting these ratings is hard, as they are not specifically about depression, but this is still impressive.</p><p>Most supplements users report few issues (except SAM-e and St John Wort, which have a high probability of side effects), while people using prescription medications report  of side effects. For instance, for Bupropion and SSRIs, I estimate a 30 to 40% probability of side-effects making you stop taking them. For SSRIs, there’s a 20 to 30% probability of having long-term side effects. This is quite scary.\nTianeptine users report way fewer side effects, but as a counterpart, they’re more likely to get addicted/tolerant (I wonder how much of this is explained by people getting Tianeptine over-the-counter).</p><p>All racetams got almost identical pretty low ratings, except Phenyracetam, which was rated much higher (but still way below something like Modafinil).</p><p>Racetams seem safe: they all got pretty much the same (low) probabilities for all issues, except for Phenylracetam, which has a tolerance probability between 10 and 20%.</p><h3>\nHow dangerous is Phenibut?\n<a href=\"https://troof.blog/posts/nootropics/#how-dangerous-is-phenibut\">#</a></h3><blockquote><p>Only 3% of users got addicted to phenibut. This came as a big surprise to me given the caution most people show about this substance. Both of the two people who reported major addictions were using it daily at doses &gt; 2g. The four people who reported minor addictions were less consistent, and some people gave confusing answers like that they had never used it more than once a month but still considered themselves “addicted”. People were more likely to report tolerance with more frequent use; of those who used it monthly or less, only 6% developed tolerance; of those who used it several times per month, 13%; of those who used it several times per week, 18%; of those who used it daily, 36%.</p></blockquote><p>The figures I have are somewhat worse, but still better than I expected: I estimate a 5.5%[3-9%] chance of becoming addicted, a 6%[4-10%] chance of having long-term side effects, and a 20%[15-26%] chance of becoming tolerant.</p><p>Microdosing psychedelics was quite highly rated, especially for Psilocybin: I estimate a mean rating of 5.6 [5.3-5.8] for LSD and 6[5.8-6.3] for Psilocybin, and a probability of changing your life of 6[4-8]% for LSD and 8[6-11]% for Psilocybin.</p><p>But be careful: this may be an example of the limit of self-reported, unblinded, data. Indeed, <a href=\"https://www.gwern.net/LSD-microdosing#microdosing\">Gwern has been gathering RCTs on the subject (in addition to his N=1 experiment)</a>, and most show little to no effect\n, except on things like visual intensity or time perception, and sometimes some effect on self-reported mood (how can you be sad when the colors are INTENSE?). People seem to be able to <a href=\"https://www.gwern.net/docs/psychedelic/2020-olson.pdf\">trip on a placebo</a>, so I guess this is an area where we should be careful.</p><p>While I got a lot of boring results (this is reassuring!), I was really surprised by a few things:</p><ul><li>Sport (especially Weightlifting) and sleep were really highly rated. More generally all “lifestyle interventions” were rated way higher than most famous nootropics like Piracetam or Rhodiola Rosea.</li><li>Peptides like Semax or Cerebrolysin were really highly rated, but seem poorly known outside of Russia.</li><li>Tianeptine was rated much higher than any other antidepressant. What is going on here?</li><li>Zembrin maybe isn’t better than normal Kanna, as suggested in ACX 2020 Nootropics survey, which would be disappointing.</li></ul><p>I’m sure there are a lot of interesting things I missed in my data. Feel free to <a href=\"https://github.com/LeoGrin/nootroflix\">explore them</a>.</p><p><strong>Subscribe to see new posts</strong>:</p>","contentLength":14589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44443492"},{"title":"Cloudflare Introduces Default Blocking of A.I. Data Scrapers","url":"https://www.nytimes.com/2025/07/01/technology/cloudflare-ai-data.html","date":1751462936,"author":"stephendause","guid":180581,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44443480"},{"title":"Microsoft to Cut 9k Workers in Second Wave of Major Layoffs","url":"https://www.bloomberg.com/news/articles/2025-07-02/microsoft-to-cut-9-000-workers-in-second-wave-of-major-layoffs","date":1751462804,"author":"htrp","guid":180580,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44443452"},{"title":"I'm dialing back my LLM usage","url":"https://zed.dev/blog/dialing-back-my-llm-usage-with-alberto-fortin","date":1751460502,"author":"sagacity","guid":180579,"unread":true,"content":"<p>We invited <a href=\"https://x.com/a7fort\">Alberto Fortin</a>, a seasoned software engineer with 15 years of experience, to share his candid journey with AI. Alberto initially embraced LLMs with genuine enthusiasm, hoping they would revolutionize his development workflow. However, after encountering significant challenges while rebuilding his infrastructure with Go and ClickHouse, he wrote a <a href=\"https://albertofortin.com/writing/coding-with-ai\">thoughtful blog post</a> reflecting on the gap between AI hype and reality.\nFor this conversation, Alberto also prepared a <a href=\"https://kings-kick-n9g.craft.me/alberto-fortin-zed\">detailed follow-up analysis</a> testing newer models like Claude Opus 4, examining whether recent improvements have addressed the core issues he encountered.</p><p>His experience provides practical lessons for engineers evaluating LLMs in production environments—balancing realistic expectations with an understanding of where these tools genuinely add value versus where they still fall short.</p><blockquote><p>\"I was really shocked at the poor quality of some things, and it was not just about bugs and features not working. I think as a developer who wants to maintain this codebase for the next few years, I also care about it being neat enough.\"</p></blockquote><blockquote><p>\"I feel like I'm a week away from fixing this, but actually a new small error would come up and then that will take another two weeks to fix.\"</p></blockquote><blockquote><p>\"I will give my error output to the LLM and then it will spit out something new that will kind of fix it, but also make things a bit more messed up—and break something else in the process.\"</p></blockquote><blockquote><p>\"I think everyone just got a little bit overexcited about it because the first iteration, the first little feature, the first autocomplete is like, 'Oh my God, this is amazing. This is like reading my mind.' So you kind of get duped into it a little bit.\"</p></blockquote><blockquote><p>\"I think we've gotten to a level where we can do probably 10 times as much coding. So we kind of expect that to happen and we require that from the LLMs, but I think everyone just gets a little bit overexcited about it.\"</p></blockquote><blockquote><p>\"I think this is the biggest difference, like a mental shift... I am the software engineer, the senior software engineer, I am the architect. The LLM is the assistant. The assistant responds to me; I make the plan.\"</p></blockquote><blockquote><p>\"I lost all my trust in LLMs, so I wouldn't give them a big feature again. I'll do very small things like refactoring or a very small-scoped feature.\"</p></blockquote><blockquote><p>\"I started fixing the bugs myself. Because as soon as you understand this—you have a hundred percent understanding of your codebase and what everything is doing—it's so much easier and quicker for you to go in and fix something.\"</p></blockquote><blockquote><p>\"If you are confident enough in your skills—you know, a senior developer—and this is not working for you, there's nothing wrong with you. Just try to do the things that you always did and use AI to leverage your knowledge a little bit.\"</p></blockquote><blockquote><p>\"We've gone up a level, it's great. But also, let's be mindful we're not there yet at the next level... We are offloading some of the programming work, but we still need to do architectural abstractions and make the decisions for the product.\"</p></blockquote><blockquote><p>\"Let's just try to calm down all this hype and find a balanced approach towards AI. Use it, because I think it's such an amazing revolution in technology, but we're not there yet.\"</p></blockquote>","contentLength":3170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44443109"},{"title":"Don’t use “click here” as link text (2001)","url":"https://www.w3.org/QA/Tips/noClickHere","date":1751456352,"author":"theandrewbailey","guid":180470,"unread":true,"content":"<h2>Don't use \"click here\" as link text</h2><p>When calling the user to action, use brief but meaningful link text\nthat:</p><ul><li>provides some information when read out of context</li><li>explains what the link offers</li><li>doesn't talk about mechanics</li></ul><p>For instance, avoid the following sentence on your page:</p><blockquote><p> To download W3C's editor/browser Amaya, <a href=\"https://www.w3.org/Amaya/\">click\nhere</a>.</p></blockquote><blockquote><p>\n  To download Amaya, go to the <a href=\"https://www.w3.org/Amaya/\">Amaya Website</a> and get\n  the necessary software.</p></blockquote><p>Both of these sentences divulge too much of the mechanics of getting the\nAmaya software. If you want to call your reader to action, use something\nlike:</p><p>Note that \"get\" is left out of the hypertext; we do not recommend putting\nverb phrases in link text. Thus, rather than:</p><blockquote><p>  Tell me more about <a href=\"https://www.w3.org/Amaya/\">Amaya</a>: W3C's free\n  editor/browser that lets you create <a href=\"https://www.w3.org/TR/html401\">HTML</a>,<a href=\"https://www.w3.org/TR/SVG/\"> SVG</a>, and <a href=\"https://www.w3.org/TR/MathML2/\">MathML</a>\n  documents.</p></blockquote><p>The <a href=\"https://www.w3.org/QA/Tips\">W3C QA Tips</a> are short documents explaining useful\nbits of knowledge for Web developers or designers, hosted and produced by the Quality Assurance \nInterest Group at W3C.</p><p>While the tips are carefully reviewed by the participants of the group, they should not be seen\nas anything else than informative bits of wisdom, and especially, they are \nnormative W3C technical specifications.</p><p>Learn more about the Tips, how to submit your own pearls of wisdom, and find all the other QA \ntips in the <a href=\"https://www.w3.org/QA/Tips/\">Tips Index</a>.</p>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44442473"},{"title":"Math.Pow(-1, 2) == -1 in Windows 11 Insider build","url":"https://github.com/dotnet/runtime/issues/117233","date":1751454285,"author":"jai_","guid":180469,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44442219"},{"title":"They tried Made in the USA – it was too expensive for their customers","url":"https://www.reuters.com/business/they-tried-made-usa-it-was-too-expensive-their-customers-2025-07-02/","date":1751453476,"author":"petethomas","guid":180578,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44442134"},{"title":"How large are large language models?","url":"https://gist.github.com/rain-1/cf0419958250d15893d8873682492c3e","date":1751452778,"author":"rain1","guid":180468,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44442072"},{"title":"Jack Welch, the Man Who Broke Capitalism (2022)","url":"https://www.forbes.com/sites/kylewestaway/2022/05/31/jack-welch-the-man-who-broke-capitalism/","date":1751452297,"author":"throw0101b","guid":180467,"unread":true,"content":"<p>David Gelles has been reporting on American CEOs for years at The New York Times. But there’s one CEO who stands heads and shoulders above his peers. He’s been retired for more than two decades, but his impact is still felt. Revered by some and reviled by others, his name is Jack Welch and he served as the CEO for General Electric (GE) from 1981 to 2001.</p><p>In Gelles’ new book, The Man Who Broke Capitalism: <a href=\"https://amzn.to/3a9BxWB\" target=\"_blank\" title=\"https://amzn.to/3a9BxWB\" rel=\"nofollow noopener noreferrer\" data-ga-track=\"ExternalLink:https://amzn.to/3a9BxWB\" aria-label=\"How Jack Welch Gutted the Heartland and Crushed the Soul of Corporate America―and How to Undo His Legacy\">How Jack Welch Gutted the Heartland and Crushed the Soul of Corporate America―and How to Undo His Legacy</a>, he chronicles how Welch’s laser focus on maximizing shareholder value by any means necessary - including layoffs, outsourcing, offshoring, acquisitions, and buybacks - became the new playbook in American business. The book demonstrates how this shareholder maximizing version of capitalism has led to the greatest socioeconomic inequality since the Great Depression and harmed many of the very companies that have embraced it.</p><p>I recently discussed the book with Mr. Gelles. The interview was condensed and edited for clarity. If you’re interested in viewing a video of the entire discussion, <a href=\"https://youtu.be/RUHTZawNCSY\" target=\"_blank\" title=\"https://youtu.be/RUHTZawNCSY\" rel=\"nofollow noopener noreferrer\" data-ga-track=\"ExternalLink:https://youtu.be/RUHTZawNCSY\" aria-label=\"click here\"></a>.</p><figure role=\"presentation\"><figcaption></figcaption></figure><h3><strong>Why do Americans revere our CEOs so much? Why does Jack Welch stand out among the crowd?</strong></h3><p>To get at the root of why this society seems to put our bosses up on pedestals, look back well over 100 years to the way we celebrated some of the early industrialists who rose to such great heights. We have the same veneration for modern-day technologists and entrepreneurs who are able to create amazing new breakthroughs and products.</p><p>In a big, diverse country that never had a monarchy or a unified religion, and is increasingly polarized and fractured, we look to our business leaders as some of the most important and perhaps some of the last sort of cultural touchstones who can be relevant to society at large. This helps explain some of the reasons we have some of the problems we do today.</p><p>I argue in the book that Jack Welch was a celebrity CEO. He was trying to marry this American reverence of CEOs with the modern media ecosystem, and he used it to disastrous effect. It was only through our collective veneration of Welch that he was able to be so influential over such a long period.</p><h3><strong>In the book, you draw a line from Jack Welch to the 737 Max issues at Boeing. Can you walk me through that? </strong></h3><p>Starting in 2019, I was one of the reporters at the  who started digging into Boeing after the second crash of the 737 Max. The plane’s technical problem was very clear early on. There was a bad piece of software that relied on one flimsy sensor on the fuselage of the plane. So, we understood, in theory, what caused the planes to crash.</p><p>What we started understanding as we dug deeper, though, was that there was a cultural story. Over 25 years, something fundamental had shifted inside Boeing—the company’s priorities and what made it tick. When we started trying to understand that cultural change, it was a story of Jack Welch.</p><p>Starting in 1997, three successive CEOs who studied at Jack's knee at GE, took over Boeing. They deliberately, explicitly tried to make Boeing more like GE. And in doing so, they transformed one of the great American manufacturers, a company that for nearly 100 years had been focused on aeronautical engineering, into one that was motivated by financial engineering.</p><p>Records from congressional inquiries revealed messages between mid-level Boeing employees. These records showed that engineers and test pilots were thinking about the stock price when making decisions about safety. The awareness of the company's stock price percolated all the way down to the level of people who should be focused on the quality and safety of the plane, not Wall Street.</p><p>In 2011, Boeing faced this critical juncture; it was faced with the loss of a major order from American Airlines. The company had been a Boeing customer for decades, so they gave the Boeing CEO, Jim McNerney, a courtesy call to let him know that they were about to place a big order from Airbus instead of Boeing. McNerney asked for a week or two to make a counter-offer. In those few days, Boeing decided to redesign the 737 one more time. Rather than design a whole new plane that was suitable for the 21st century, they tried to re-engineer and tinker with the 737, which had been introduced in the 1960s. And it was that decision that set in motion this cascade of decisions and design changes that required this flawed piece of software to be put in the plane in the first place.</p><h3><strong>In 2009, Welch said that maximizing shareholder value is “the dumbest idea in the world.” Do you believe that he meant that? If not, why did he say it, when it’s against his entire cannon?</strong></h3><p>Jack Welch was a master of reading the room. And I think he understood in that moment, right in the year after the financial crisis, a year during which it did become clear what that kind of management led to, that he was espousing something antithetical to his actions over 20 years as a CEO.</p><p>There was no great conversion moment.</p><p>I've spoken with enough CEOs over the years to recognize that many of them are experts at telling themselves a story where they are not the bad guys. So was there a certain amount of rationalizing going on? Or was he saying that in all his decisions that he made to maximize shareholder value at GE, he was actually motivated by something else? I don't know. I can't get inside his head.</p><p>But you just need to look at his public statements during his time as CEO, and in the aftermath of his retirement, to understand that he was on record as saying the purpose of the business is to increase its profits. When he was asked by the Wall Street Journal about what he believes his greatest legacy was, was making GE the most valuable company on Earth.</p><h3><strong>How did Welch contribute to income inequality?</strong></h3><p>I'm not an academic who has studied inequality in a deep way. But those who have, including most famously Thomas Piketty, draw a direct line between executive compensation and its absolutely relentless upward trajectory over the last decades, and the widening gap between the haves and have-nots.</p><p>Welch's own enormous executive compensation was immense. He was on the Forbes list of the 400 richest Americans simply for being a people manager. He didn't invent anything. He didn't own the company. He was hired help. And yet, he became something close to a billionaire. By doing so, he set a precedent for hundreds of other managers over the past several years to do the exact same thing. Now we don't even blink when a CEO is rewarded with a $20- or $50 million-a-year pay package.</p><p>As all that is happening, what's happening to his workers? They're getting laid off en masse. He's outsourcing them to contractors who don't pay nearly as good of wages as GE once did. He's sending jobs overseas in search of low wages and taxes. At the same time, look at what's happened to the American minimum wage: It's stuck at $7.25 an hour. If it had just kept pace with inflation over the last 20 years, it would be closer to $25. But we live in this world that was shaped by Jack Welch's priorities. And we're still trying to dig out of that hole.</p><p>I wrote the column for the last five years for the  and I got to interview hundreds of CEOs. It was a real privilege. And I got an insight into what makes CEOs tick. After a couple of years, I realized that one name kept coming up: Jack Welch. Some people would bring him up as a cautionary tale, and others look to him as guidance for how they ought to comport themselves. Either way, he was clearly living rent-free in the minds of CEOs today. And that just bugged me. It was just a question mark more than anything else. He hasn’t been a CEO for almost 20 years. Why is he still so influential?</p><p>When the Boeing story landed, and I realized that it was really a Jack Welch story, it clicked for me. He’s the guy that explains why we are in such a messed-up world today.</p><h3><strong>What’s the antithesis to Welch’s shareholder maximizing capitalism? What are the results? </strong></h3><p>There's a temptation to imagine that something as simple and squishy as stakeholder capitalism represents that antithesis to shareholder capitalism, but I believe that it's really just the very first steps. It's the opening awkward remarks in a conversation about what an equitable economy is actually going to look like.</p><p>The book covers 80 years—from the moments right after World War II and the way companies were behaving back then. This was the “golden age of capitalism” all the way to the highly unequal society we live in today. So, I recognize that shareholder capitalism has been a generational project. Jack Welch instituted the priorities of Milton Friedman and Friedrich Hayek—a relentless prioritization of shareholder value above everything else.</p><p>And in the same way, it's going to be a generational project to rebalance things. We’re seeing the start of that as stakeholder capitalism and ESG [environmental, social and governance] are becoming a part of the mainstream conversation. Maybe we’re at that moment in a pendulum's arc where it pauses and starts to begin its trajectory back in the other direction. I hope we're there because we need to reset.</p><p>I include some practical suggestions at the end of the book. We need to take better care of our workers. We need to give them better wages and better benefits. We need to offer them equity. The distribution of corporate profits over the last 50 years has gotten wildly out of whack. There's no law that says that shareholders and executives are entitled to this enormous slice of the pie. These are choices that people—mostly older white men—make about how wealth in this society is allocated. And we have the opportunity to change that.</p><p>So let's start talking about what is fair, what is equitable and what actually is healthy for the economy in the long term. We're starting to see that Welch’s philosophy has led us to a moment where cities in the middle of the country are hollowed out, communities across the country are starved for resources, and the tax base is unable to fund things like education and infrastructure.</p><p>These are choices we've made. We can make different choices that create a different kind of economy.</p><h3><strong>For founders launching companies, what lesson do you believe they should take away from Jack Welch?</strong></h3><p>The first thing that comes to mind is to avoid stack ranking. Stack ranking, also known as rank-and-yank, is a popular talent management system that was popularized by Welch himself at GE in the ‘80s. Managers are forced to sort their people into A, B and C players. The top 20% performers are A players. The middle 70% performers are B players. The bottom 10% performers are C players. Every year, Welch required all C players to be fired. And what was so astonishing is that not only did it take root at other big companies like Microsoft, but it continues to this day to show up in companies like Uber. The employees who experienced stack ranking at both of those companies talk about the absolutely corrosive effect it had on culture. It gets to the point where your job essentially becomes finding a colleague who you could make look bad to your boss in order to gain more job security. It is just the . It's terrible.</p><p>Additionally, Jack was just a brutally uncompassionate manager. He was crass. He was rude. He was argumentative. He had a serious strain of alpha male machismo, and often made sexist, derogatory remarks. When he wanted to fire someone, he referred to it as “shooting people.” And that’s hard to even talk about during a week like this—after the school shootings. It's that kind of violent rhetoric that caused loyalty inside GE and the culture under Jack Welch to crumble.</p><a href=\"https://www.amazon.com/Man-Who-Broke-Capitalism-America_and/dp/198217644X\" target=\"_blank\" aria-label=\"The Man Who Broke Capitalism: How Jack Welch Gutted the Heartland and Crushed the Soul of Corporate America―and How to Undo His Legacy\" rel=\"nofollow noopener noreferrer\" data-ga-track=\"ExternalLink:https://www.amazon.com/Man-Who-Broke-Capitalism-America_and/dp/198217644X\"></a>","contentLength":11718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44442022"},{"title":"More assorted notes on Liquid Glass","url":"https://morrick.me/archives/10068","date":1751444302,"author":"freediver","guid":181354,"unread":true,"content":"<p>Over the past couple of weeks, I’ve been trying to make sense of Apple’s latest user-interface redesign — Apple calls it  — that will affect all their platforms in the next iteration of their respective OS versions. But it’s hard to make sense of it when, after checking Apple’s own guidance, I’m mostly left with the feeling that at Apple they’re making things up as they&nbsp;go.</p><p>If you’ve been following me on Mastodon, you’ll be already familiar with a lot of what follows. I just wanted to gather my posts there in a more organic piece&nbsp;here.</p><p>In the  section, we find this figure:</p><blockquote><p>Key navigation elements like tab bars and sidebars float in this Liquid Glass layer to help people focus on the underlying content.</p></blockquote><p>Now take a look at the area I’ve highlighted in the image. Why would you want to “focus on the underlying content” here? Tab bars and toolbars still cover the underlying content, and the more transparent/translucent they are, the worse. When something fades to the background, it literally ceases to be in the foreground, so there’s no point in focusing on it. This is like proposing an interface that helps you focus your sight on your peripheral vision.</p><p>Below the figure, in the paragraph starting with <em>Establish a clear navigation hierarchy</em>, developers are advised to:</p><blockquote><p>Ensure that you clearly separate your content from navigation elements, like tab bars and sidebars, to establish a distinct functional layer above the content layer.</p></blockquote><p>Which is in direct contrast to what you’ve just shown on the image above. First you propose to blur the lines between controls and content, then you advise to “clearly separate your content from navigation elements”. Which is it? If you stop and think, it’s ironic that <em>Ensure that you clearly separate your content from navigation elements, like tab bars and sidebars, to establish a distinct functional layer above the content layer</em> is the exact description of what’s happening in the ‘Before’ image!</p><p>Moving on, we get to this figure related to the <em>Extend content beneath sidebars and inspectors</em> paragraph:</p><p>In other words, create the illusion of an image that extends under a sidebar, and while you won’t actually be able to see the part of the image under the sidebar, on the other hand the transparency effect applied to the sidebar will make the text on it less legible overall. A great lose-lose situation, visually, don’t you think? Also, this might be just a matter of personal perception, but to my eyes, the blank area below the image you can see behind the sidebar looks weird, as if there’s something missing.</p><p>In  we find&nbsp;this:</p><blockquote><p>To give content room to breathe, organizational components like lists, tables, and forms have a larger row height and padding. Sections have an increased corner radius to match the curvature of controls across the system.</p></blockquote><p>Which is largely unnecessary. It reduces the amount of information displayed on screen, and you’ll have to scroll more as a consequence. Look at the Before and After layouts: the Before layout doesn’t need solutions to increase its clarity. You’re just injecting white space everywhere. It’s also ironic that where more space and ‘breathing room’ are actually necessary, the header (“Single Table Row” in the figure) is pushed even nearer to the status bar.</p><p>And don’t get me started on those redesigned, stretched-out switches. They’re the essence of ‘change for change’s sake’.</p><p>Let’s start with : “Elevate and distinguish the content beneath them”: is this really the role of controls and interface elements? Should content and controls even occupy the same space? Should the lines be blurred between them?</p><p>In my opinion, the best way both controls and content can shine is by having each their own space: controls are out of content’s way, letting it shine and helping the user focus on it. And in their own space, controls can be clear, neatly organised, ready to be accessed in order to manipulate the content.</p><blockquote><p>Align with the concentric design of the hardware and software&nbsp;[…]</p></blockquote><p>No, seriously, how does one  in a  context? Is that a matter of picking a circle, an arc, a shape? All snark aside, this just sounds poorly worded to me. I get what Apple means here: in your app design, you should pick shapes that resemble the contours of the hardware — the shape of a MacBook’s display and bezel, for example — and the typical shapes that you find in the system’s UI. Pretty obvious stuff that’s wrapped in ‘pretentious designer vocabulary’.</p><p>Last but not least, :</p><blockquote><p>[…] to maintain a consistent design that continuously adapts&nbsp;[…]</p></blockquote><p>The definition of  is something that is “unchanging in nature, standard, or effect over time”. So, how does a  design continuously ?</p><p>This paragraph should have read something like: <em>Adopt platform conventions to create a design that remains visually and functionally consistent across window sizes and displays.</em></p><p>In the  section of the guidelines for <a href=\"https://developer.apple.com/design/human-interface-guidelines/app-icons\">App icons</a>, we find&nbsp;this:</p><blockquote><p>Find a concept or element that captures the essence of your app or game, make it the core idea of your icon, and express it in a simple, unique way with a minimal number of shapes. Prefer a simple background, such as a solid color or gradient&nbsp;[…]</p></blockquote><p>Not only is this the recipe for blandness, it’s also borderline contradictory. Like, <em>Make a unique dish using a minimal number of simple ingredients.</em> While it’s possible to make a few different dishes using just two or three things, you touch the ceiling of uniqueness and variety pretty damn&nbsp;soon.</p><p>Another thing that irks me about this obsession with icon simplification is that when you abstract things this much, you dilute their meaning instead of distilling it. Take the progressive degradation of the Dictionary icon, for example. In its subsequent iterations (as soon as it loses the ‘book’ shape), it could just be the icon for a font managing app. Because it ends up losing a lot (if not all) of its uniqueness.</p><p>This image is taken by <a href=\"https://basicappleguy.com/basicappleblog/macos-icon-history\">this post on the history of some of Mac OS icons</a> by Basic Apple Guy. Go take a look at that post and you’ll see a pattern emerge with application icons: they get progressively abstracted to the point that they barely represent what they should represent: the icon for Stickies goes from being an actual depiction of a few yellow sticky notes to being some small vague rounded rectangles inside a clear rounded rectangle. The icon for Notes goes from representing an actual notepad to being a flat square with two lines and a coloured top area. The icon for Calculator, same thing: from depicting a calculator to being what looks more like a security keypad. Game Centre: from an icon representing different types of games, to… a group of colourful bubbles.</p><p>The most recent iteration of Migration Assistant’s icon is yet another example:</p><div><img data-recalc-dims=\"1\" loading=\"lazy\" decoding=\"async\" aria-describedby=\"caption-attachment-10062\" src=\"https://i0.wp.com/morrick.me/wp-content/uploads/2025/06/06-Migration-Assistant-icon-e1751026783747.png?resize=748%2C242&amp;ssl=1\" alt=\"\" width=\"748\" height=\"242\" srcset=\"https://i0.wp.com/morrick.me/wp-content/uploads/2025/06/06-Migration-Assistant-icon-e1751026783747.png?w=748&amp;ssl=1 748w, https://i0.wp.com/morrick.me/wp-content/uploads/2025/06/06-Migration-Assistant-icon-e1751026783747.png?resize=260%2C84&amp;ssl=1 260w, https://i0.wp.com/morrick.me/wp-content/uploads/2025/06/06-Migration-Assistant-icon-e1751026783747.png?resize=640%2C207&amp;ssl=1 640w\" sizes=\"auto, (max-width: 748px) 100vw, 748px\"><p>Migration Assistant icon in Mac OS 15 Sequoia (left) and how it appears in Mac OS 26 Tahoe Beta 2 (right)</p></div><p>Look at it. It’s utterly meaningless. Maybe it can work in an airport to mark an emergency exit or something. The old one is so simple and clear. From an ‘old, now inactive’ system to a ‘fresh new one’. Migration, indeed. Right there. All while preserving the Mac identity. This once again feels like changing things for change’s sake and nothing else.</p><p>I’m pretty sure that if you were to interview one of the designers at Apple responsible for this icon devolution, they would say something about reducing icons to their . To me, this looks more like <em>squeezing all life out of them.</em> Icons in Mac OS X used to be inventive, well crafted, distinctive, with a touch of fun and personality. Mac OS X’s user interface was sober, utilitarian, intuitive, peppered with descriptive icons that made the user experience fun without signalling ‘this is a kid’s&nbsp;toy’.</p><p>Same for NeXTSTEP, from which Mac OS X originates. Here, some icons have a more 3D effect, others are flatter; some are logos (like the icon for the Webster’s Dictionary), others are descriptive to a fault (the user’s Home folder is an illustration of a tiny house), but they’re instantly memorable. They do what icons are supposed to do and they take full advantage of the high resolution monitors NeXT sold for their workstations (also remember that some of those monitors were greyscale, so icons had to work even with limited palettes).</p><p>In recent years, the reverse has happened: Apple has been infantilising and dumbing down Mac OS’s user interface in order to be more similar to simpler mobile devices and to their UIs, while transforming the icons into something bland and ‘corporate’.</p><p>In the iOS 5 days, the HIG for icons weren’t too restrictive, apart from some basic requirements and guidance. This gave developers plenty of freedom, and the results (if you exclude the usual trash apps) were tasteful and varied; some opted for a rich, skeuomorphic look; others for flatter designs; others for something in between. Apps were instantly recognisable.</p><p>Now Apple gives you the option of removing colour and depth to all icons. To make everything look samey and nondescript…</p><p>…So that you can “complement your wallpaper.”</p><p>On my main Mac I’ve left the default Ventura wallpaper because the only time I see it is when I wake the Mac mini from sleep and I’m presented with the Login screen. People who actually  with computers and mobile devices don’t stare at wallpapers and matching icons.</p><p>But it’s not just that, it’s that these ‘Icon Appearances’ also remove colour, depth, and personality from  too. This further dictates (and interferes with) what kind of design a third-party developer may choose for their apps. All this after recommending employing “a minimal number of shapes” and “prefer[ring] a simple background”.</p><blockquote><p>I’ve said this before, but Apple is forcing third party devs to be in service of Apple. The guidelines and rules are meant to sublimate the brands of the third party, and replace it with&nbsp;Apple.</p></blockquote><blockquote><p>Apple has effectively infinite resources and operates on their own timeline, but everyone else does not have this kind of luxury. Springing big changes like this all at once forces so many independent developers, entire companies, and the industry as a whole to freeze their own development schedules to accommodate Apple’s design system.</p><p>It’s asking a lot. For almost nothing in return. <strong>I keep looking at all the changes Liquid Glass brings, and I cannot find one instance where it has markedly improved the experience in any&nbsp;way.</strong></p><p>Everything that got rounder—except for the things that didn’t — why? Everything that got inset that wasn’t before — why? Everything that is now blurry — why? I don’t think it’s a secret that the content area of some apps decreased. The margins and padding increased — except where it didn’t.</p><p>In some ways, there’s almost more UI variance than there was before, which doesn’t make any sense. But in other ways, everything feels far more restrictive than it once was. Which I admit, also doesn’t make much sense. <strong>App icons weren’t just more expressive on OS X, they could be a much wider-range of materials than merely glass.</strong></p><p>I know I can still draw anything I want within that square, and that the glass appearance on objects inside of it is purely optional. <strong>But the edge of every icon now has a glass appearance I can’t do anything about.</strong> If my icon is paper, wood, metal, or—god forbid—leather? It has a glass specular highlight. On macOS, it’s currently locked at a 45° angle. Which is not something I agreed to.</p><p>Swinging for the fences like this comes with substantial risk. Especially for matured products like macOS. This product is almost 25 years old, and I would hope there would be a little more caution when expecting effort from and forcing changes upon a developer community you’ve largely lost your goodwill with. <strong>These kinds of decisions have long-lasting effects</strong> and I’m sure many developers would’ve appreciated their time being considered <strong>before asking them to incorporate a design they did not sign up&nbsp;for.</strong></p></blockquote><p>And in the paragraph just preceding this section I’ve quoted, Mantia writes (emphasis his):</p><blockquote><p>In a way, one could say Liquid Glass is like a new version of Aqua. It has reflective properties reminiscent of that. One could also say it’s an evolution of whatever iOS 7 was, leaning into the frosted panels and bright accent colors. But whatever Liquid Glass  it  what many of us were hoping for.</p></blockquote><p>Mantia’s piece is so good it’s difficult to extract a few quick quotes. Please take your time and go read it in&nbsp;full.</p><p>In  there are a few passages that unequivocally convey the message that Apple is in control of  app’s appearance (or part of it). Take for example this, in the  section:</p><blockquote><p>Any custom backgrounds and appearances you use in these elements might overlay or interfere with Liquid Glass or other effects that the system provides, such as the scroll edge effect. […] Prefer to remove custom effects and let the system determine the background appearance&nbsp;[…]</p></blockquote><blockquote><p>Let the system handle applying masking, blurring, and other visual effects, rather than factoring them into your design.</p></blockquote><p>Compare and contrast this with the language used in the 2010 iOS Human Interface Guidelines under :</p><blockquote><p>Try to balance eye appeal and clarity of meaning in your icon so that it’s rich and beautiful and clearly conveys the essence of your application’s purpose. Also, it’s a good idea to investigate how your choice of image and color might be interpreted by people from different cultures.</p></blockquote><p>After recommending to create different sizes of your application icon for different devices, the guidelines note&nbsp;that</p><blockquote><p>When it’s displayed on an iPhone Home screen, iOS adds rounded corners, a drop shadow, and a reflected shine.</p></blockquote><blockquote><p>You can prevent iOS from adding the shine to your application icon. To do this, you need to add the  key to your application’s  file&nbsp;[…]</p></blockquote><p>Sure, even back then there were visual requirements for icons, but I wouldn’t define this short list as particularly restrictive:</p><blockquote><p>Ensure your icon is eligible for the visual enhancements iOS provides. You should produce an image&nbsp;that:</p><ul><li>Does not have any shine or gloss (unless you’ve chosen to prevent the addition of the reflective shine)</li><li>Does not use alpha transparency</li></ul></blockquote><p>The language in these guidelines from 2010 strikes me as , like in this passage:</p><blockquote><p><strong>Create a 512×512 pixel version of your application icon for display in the App Store.</strong> Although it’s important that this version be instantly recognizable as your application icon, it can be subtly richer and more detailed. There are no visual effects added to this version of your application icon.</p></blockquote><p>The language in the  document is overall more prescriptive and impersonal, and as I was reading all the various recommendations, I couldn’t help but feel the underlying message, <em>We created this beautiful look based on glass effects, don’t you dare ruin it with your custom designs, effects, materials, brand identity.</em></p><p>The language in the <a href=\"https://developer.apple.com/design/human-interface-guidelines/app-icons\">current guidelines for app icons</a> isn’t much different. It also reflects Apple’s current philosophy of ‘keeping it simple’ which, out of context, could be valid design advice — you’re designing icons with small-ish dimensions, not full-page detailed illustrations for a book, so striving for simplicity isn’t a bad&nbsp;thing.</p><p>And yet — and I might be wrong here — I keep reading between the lines and feel that these guidelines are more concerned with ensuring that developers maintain the same level of blandness and unimaginativeness of Apple’s own redesigned app&nbsp;icons:</p><blockquote><p>Embrace simplicity in your icon design. Simple icons tend to be easiest for people to understand and recognize. An icon with fine visual features might look busy when rendered with system-provided shadows and highlights, and details may be hard to discern at smaller sizes. Find a concept or element that captures the essence of your app or game, make it the core idea of your icon, and express it in a simple, unique way with a minimal number of shapes. Prefer a simple background, such as a solid color or gradient, that puts the emphasis on your primary design — you don’t need to fill the entire icon canvas with content.</p></blockquote><p>Going back to the Mac OS X Human Interface Guidelines from 2009 is like entering a different dimension. The chapter dedicated to icon design starts off like&nbsp;this:</p><blockquote><p>Aqua offers a photo-illustrative icon style — it approaches the realism of photography but uses the features of illustrations to convey a lot of information in a small space. Icons can be represented in 512×512 pixels to allow ample room for detail. Anti-aliasing makes curves and nonrectilinear lines possible. Alpha channels and translucency allow for complex shading and dimensionality. All of these qualities allow you to create lush, vibrant icons that capture the user’s attention.&nbsp;[…]</p><p>Icon genres help communicate what users can do with an application before they open it. Applications are classified by role — user applications, software utilities, and so on — and each category, or genre, has its own icon style. Creating icons that express this differentiation helps users distinguish between types of icons in the&nbsp;Dock.</p><p>For example, the icons for user applications are colorful and inviting, whereas icons for utilities have a more serious appearance. Figure 11–2 shows user application icons in the top row and utility icons in the bottom row.</p></blockquote><p>You may argue that these are simply different icon design guidelines from different eras reflecting different tastes and aesthetic sense, and that it’s not a matter of one being better than the other, or a matter of right versus wrong, and I’ll concede that. But the older guidelines were informed in such a thoughtful way as to give third-party developers a lot of room for creativity and a wide range of choices while remaining within the required system-wide aesthetics of the time. If you look at the Figure 11–2 above, you could have very illustrative icons like the ones for Disk Utility (the hard disk with a stethoscope) or Front Row (the theatre armchair), but also more minimalistic designs such as the icon for the Terminal and AirPort Utility applications.</p><p>Tangentially, I found this bit ironic given where we are&nbsp;now:</p><blockquote><p>Use transparency only when it is convincing and when it helps complete the story the icon is telling. You would never see a transparent sneaker, for example, so don’t use one in your&nbsp;icon.</p></blockquote><p>This piece of advice is reiterated in the 2013 edition of Mac OS X’s Human Interface Guidelines:</p><blockquote><p><strong>Use transparency when it makes sense.</strong> Transparency in an icon can help depict glass or plastic, but it can be tricky to use convincingly. You would never see a transparent tree, for example, so don’t use one in your icon. The Preview and Pages app icons incorporate transparency effectively.</p></blockquote><p>Also, since the introduction of retina (high-resolution, high-density) displays in 2012, this part was added in the&nbsp;HIG:</p><blockquote><p><strong>Take Advantage of High-Resolution Display</strong></p><p>Retina display allows you to show high-resolution versions of your art and icons. If you merely scale up your existing artwork, you miss out on the opportunity to provide the beautiful, captivating images users expect. Instead, you should rework your existing image resources to create large, higher-quality versions that&nbsp;are:</p><ul></ul></blockquote><p>The aesthetics for icon design may have changed dramatically in the intervening years, but I just find it sad that, with the gorgeous displays we have today, Apple recommends simple designs made out of a few boring shapes, and everything is now in service of a ‘liquid glass’ effect the system superimposes on every aspect of the user interface — as if this surface gimmick is more important than the elements it distorts.</p><p>I’m sorry to sound like a broken record by now, but this is, once again, form before function, looks before workings. And don’t bother deviating from this new norm, because your app will be assimilated.</p>","contentLength":20024,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44441279"},{"title":"Huawei releases an open weight model trained on Huawei Ascend GPUs","url":"https://arxiv.org/abs/2505.21411","date":1751441801,"author":"buyucu","guid":180369,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44441089"},{"title":"Hilbert's sixth problem: derivation of fluid equations via Boltzmann's theory","url":"https://arxiv.org/abs/2503.01800","date":1751416308,"author":"nsoonhui","guid":180368,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44439242"},{"title":"Australians to face age checks from search engines","url":"https://ia.acs.org.au/article/2025/australians-to-face-age-checks-from-search-engines.html","date":1751414366,"author":"stubish","guid":180367,"unread":true,"content":"<p>Australians using search engines while logged in to accounts from the likes of Google and Microsoft will have their age checked by the end of 2025, under a new online safety code co-developed by technology companies and registered by the eSafety Commissioner.</p><p>Search engines operating in Australia will need to implement age assurance technologies for logged-in users in \"no later than six months”, under <a href=\"https://www.esafety.gov.au/sites/default/files/2025-06/Schedule-3-Internet-Search-Engine-Services-Online-Safety-Code-%28Class-1C-and-Class-2-Material%29.pdf\">new rules</a> published on Monday.</p><p>While only logged-in users will be required to have their age checked, many Australians typically surf the web while logged into accounts from Google, which <a href=\"https://ia.acs.org.au/article/2024/google-claims-it-doesn-t-dominate-search-in-australia.html\">dominates Australia’s search market</a> and also runs Gmail and YouTube; and Microsoft, which runs the Bing search engine and email platform Outlook.</p><p>If a search engine’s age assurance systems believe a signed-in user is “likely to be an Australian child” under the age of 18, they will need to set safety tools such as “safe search” functions at their highest setting by default to filter out pornography and high impact violence, including in advertising.</p><p>Currently, Australians must be at least 13 years of age to manage their own Google or Microsoft account.</p><p>Age assurance methods can include age verification systems, which use government documents or ID; age estimation systems, which typically use biometrics; and age inference systems, which use data about online activity or accounts to infer age.</p><p>Search engines will not be required to implement age assurance measures for users who are not logged in to their services, according to the new rules.</p><p>“Internet search engine services are designed for general public use, with or without an account,” the code states.  </p><p>However, users who are not logged in should also expect “default blurring of images of online pornography and high-impact violence material detected in search results”.</p><p>Other compliance measures in the code which search providers must abide by include improving search and age assurance technologies over time, preventing autocomplete predictions “that are sexually explicit or violent”, and responding to searches about eating disorders or self-harm with crisis prevention information.</p><p>Google and Microsoft were contacted for comment.</p><p>Earlier this year Google said it would begin <a href=\"https://ia.acs.org.au/article/2025/google-will-use-ai-to-estimate-a-user-s-age.html\">using artificial intelligence to estimate users' ages</a>, beginning with tests in the United States, while Microsoft previously stated it had explored age assurance methods while considering potential impacts for user safety and privacy.</p><h4><b>Changes ‘designed to protect’ Australian kids</b></h4><p>The new rules for search engine operators were “designed to protect\" Australian children, according to the code.</p><p>Drafting of the code was co-led by Digital Industry Group Inc. (DIGI), which was contacted for comment as it counts Google, Microsoft, and Yahoo among its members.</p><p>eSafety Commissioner Julie Inman Grant said she had registered three new codes submitted by the online industry, which covered harmful content on search engines, enterprise hosting services, and internet carriage services such as telecommunication firms.</p><p>The codes had been in the works since July 2024 and failure to comply with them could result in civil penalties of up to $49.5 million per breach, her office said.</p><p>The Commissioner said she had sought extra safety commitments from the industry on six outstanding codes, which covered the likes of app stores, device manufacturers, social media, and messaging services.</p><p>“It's critical to ensure the layered safety approach which also places responsibility and accountability at critical chokepoints in the tech stack including the app stores and at the device level, the physical gateways to the internet where kids sign-up and first declare their ages,” Inman Grant said.</p><h4><b>Push to protect children who use AI chatbots</b></h4><p>Members of the technology industry had also been asked to use the remaining six codes to strengthen their protections against generative AI chatbots engaging in harmful behaviours with children, Inman Grant said.</p><p>“We are already receiving anecdotal reports from school nurses, that kids as young as 10 are spending up to five hours a day with AI chatbots, at times engaging in sexualised conversations and being directed by the chatbots to engage in harmful sexual acts or behaviours,” she said.</p><p>Inman Grant said she would consider the changes proposed by the industry and would aim to make her final determination on the six outstanding codes by the end of July.</p><p>\"If I am not satisfied these industry codes meet appropriate community safeguards, I will move to developing mandatory standards,” she said.</p>","contentLength":4599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44439058"},{"title":"Using Sun Ray thin clients in 2025","url":"https://catstret.ch/202506/sun-ray-shenanigans/","date":1751412635,"author":"todsacerdoti","guid":179510,"unread":true,"content":"<p>i’ve used thin clients at home for quite a while - both for their  use (remotely accessing a desktop of another system); and in the sense of “modern thin clients are x86 boxes that are wildly overpowered for what they run, so they make good mini servers.”</p><p>recently, i saw a bulk lot of Sun Ray thin clients pop up on Trade Me (NZ’s eBay-like auction site) - and with very little idea of how many clients were actually included in this lot, i jumped on it. after a 9 hour round-trip drive (on some of the worst roads i’ve seen!), i returned home with the back of my car completely packed with Sun Rays. time for some interesting shenanigans!</p><p>when picking all of these up from the seller, i had guesstimated there was maybe 30 clients in total. turns out i was off by quite a bit.</p><p>i ended up bringing home:</p><ul><li>3x <a href=\"https://dogemicrosystems.ca/pub/Sun/System_Handbook/Sun_syshbk_V3.4/Systems/SunRay270/SunRay270.html\">Sun Ray 270</a> - 17” (1280x1024) LCD screens with integrated Sun Ray clients</li><li>4x <a href=\"https://web.archive.org/web/20170218173523/https://incarta.com.au/uvo\">Incarta Uvo</a> - 24” 1080p LCD screens with integrated clients\n    <ul><li>i can’t find any info about these other than the linked page on the Wayback Machine - if you know more about these, please send me an email!</li></ul></li><li>about 40 smart cards, for authentication/hotdesking</li><li>a small pile of Sun Type 7 USB keyboards, and some Sun-branded optical mice</li></ul><p>so that’s  clients all up!</p><p>a few days prior to picking all this up, i rented a storage unit in a local facility, and put some garage shelving units in there - and boy howdy i’m glad i did!</p><h2>setting up the Sun Ray Server Software</h2><p>looking at the Oracle (eugh.) documentation for the Sun Ray Server Software, it appeared there were two options: run it on ancient Linux, or run it on ancient Solaris. Oracle dropped support for the Sun Rays in 2014, as part of extinguishing everything Sun Microsystems stood for after the 2010 acquisition. i didn’t  want to have a RHEL 6 box kicking around, nor did i want to deal with trying to make Solaris 10 work in a VM on my home Proxmox cluster, so i did some digging.</p><p>enter  - well, in my case, OpenIndiana. illumos is, essentially, a fork of the pre-Oracle-acquisition OpenSolaris codebase. OpenIndiana is one of many illumos  (in a very similar sense to Linux distributions), and OpenIndiana is more suited for desktop use than most other illumos distributions. the OpenIndiana documentation has <a href=\"https://docs.openindiana.org/handbook/sunray/\">a section on setting up the Sun Ray Server Software on OpenIndiana</a>, but even with that in hand there was a lot of pieces to figure out on my own!</p><p>this is mostly a copy of the docs from the OpenIndiana handbook, with some adjustments to fix things i ran into. i did this on top of a text-only install - <code>OpenIndiana Hipster 2025.04 Text Install DVD (64-bit x86)</code> was the install media i used (from <a href=\"https://www.openindiana.org/downloads/\">https://www.openindiana.org/downloads/</a>).</p><p>to get the desktop environment installed:</p><div><div><pre><code># pkg install mate_install\n</code></pre></div></div><p>unlocking the dependencies for SRSS:</p><div><div><pre><code># pkg change-facet facet.version-lock.gnome/gnome-session=false\n# pkg change-facet facet.version-lock.gnome/gnome-settings-daemon=false\n# pkg change-facet facet.version-lock.system/display-manager/gdm=false\n# pkg change-facet facet.version-lock.library/gnome/libgnomekbd=false\n# pkg change-facet facet.version-lock.gnome/window-manager/metacity=false\n# pkg change-facet facet.version-lock.library/desktop/gnome-desktop=false\n# pkg change-facet facet.version-lock.cde/cde-runtime=false\n# pkg change-facet facet.version-lock.library/motif=false\n# pkg change-facet facet.version-lock.library/tooltalk=false\n# pkg change-facet facet.version-lock.compatibility/packages/SUNWxwplt=false\n</code></pre></div></div><p>setting up the package source, and installing the SRSS dependencies:</p><div><div><pre><code># pkg set-publisher --search-before=openindiana.org -g http://pkg.toc.de/sunray sunray\n# pkg set-publisher --non-sticky openindiana.org\n# pkg install sunray-essential\n</code></pre></div></div><p>after unpacking the Sun Ray Server Software installers (both the Solaris and Linux versions) into , i ran the  script from the OI Handbook, then tried to install SRSS, which bombed out spectacularly with package manager rejections of the <code>This version is excluded by installed incorporation consolidation/userland/userland-incorporation@...</code> sort. so here’s the correct (read: “worked for me!”) steps:</p><div><div><pre><code># /root/update_dhcp_dependency /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/\n# pkg set-publisher -g /root/srs_5.4.0.0-Solaris_11plus.i386/IPS.i386/ sunray\n# pkg uninstall entire userland-incorporation\n# pkg install SUNWut-srss SUNWut-srwc SUNWuti\n</code></pre></div></div><p>to make SRSS happy with isc-dhcp:</p><div><div><pre><code># rpm2cpio /root/srs_5.4.0.0-Linux.i386/Components/10-SRSS/Content/Sun_Ray_Core_Services_4.5/Linux/Packages/SUNWuto-4.5-44.i386.rpm | bsdtar -C /root -xf - ./opt/SUNWut/lib/dhcp/\n# sed 's#$UTDHCPDIR | sort#$UTDHCPDIR | gsort#g' -i.bak /root/opt/SUNWut/lib/dhcp/isc/dhcp_config_linux \n# cp -R /root/opt/SUNWut/lib/dhcp/isc /opt/SUNWut/lib/dhcp/\n# cp /opt/SUNWut/lib/dhcp/isc/dhcp_config_linux /opt/SUNWut/lib/dhcp/isc/dhcp_config_solaris\n# ln -s /opt/SUNWut/lib/dhcp/isc /etc/opt/SUNWut/dhcp\n</code></pre></div></div><p>then apply the needed patch to :</p><p>now, get the ancient JRE in place:</p><div><div><pre><code># cd /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Java_Runtime_Environment/Solaris\n# ./jre-6u41-solaris-i586.sh\n# mv ./jre1.6.0_41 /opt/\n# ln -s /opt/jre1.6.0_41 /etc/opt/SUNWut/jre\n</code></pre></div></div><p>and, since i wanted the web administration tools to work too:</p><div><div><pre><code># bsdtar -C /opt -xf /root/srs_5.4.0.0-Solaris_11plus.i386/Supplemental/Apache_Tomcat/apache-tomcat-5.5.36.tar.gz\n# ln -s /opt/apache-tomcat /opt/apache-tomcat-5.5.36\n</code></pre></div></div><p>i then configured the Sun Ray server:</p><div><div><pre><code># /opt/SUNWut/sbin/utconfig\n# /opt/SUNWut/sbin/utpolicy -a -z both -g -M\n# /opt/SUNWut/sbin/utadm -L on\n# /opt/SUNWut/sbin/utstart -c\n</code></pre></div></div><h3>getting the Sun Ray firmware in place</h3><p>since i was using version 5.4.x of the Sun Ray Server Software, the client firmware wasn’t part of the install - from version 5.3 onwards, you had to have an Oracle support contract to get firmware updates. sigh.</p><p>thankfully, getting a 5.2.x release (with the firmware included!) wasn’t hard. i grabbed a 5.2.x release for Linux, found the RPM with the firmware in it (), and extracted that with .</p><p>the Solaris version of SRSS wants to find the firmware in a different place than the Linux version it seems - the Linux versions put it in , but on Solaris/OpenIndiana, it needs to be in <code>/opt/SUNWutdfw/lib/firmware</code>. easy enough.</p><p>once in place, this was all it took to set up the TFTP server, and make SRSS populate the right places with the firmware:</p><div><div><pre><code># mkdir /tftpboot\n# cd /tftpboot\n# ln -f -s . tftpboot\n# /opt/SUNWut/sbin/utfwadm -AaV -G force\n</code></pre></div></div><p>i wanted to use some of the integrated-into-screens Sun Rays to replace some of the Raspberry Pis (and old iMacs) around the house showing Home Assistant dashboards. i also wanted to set up the Sun Ray server so that when i inserted a particular smart card into a client, it would bring up an RDP session to my existing “desktop” (a Fedora VM running Xrdp).</p><p>these both turned out to be… interesting to get working.</p><p>the Sun Ray Server Software has a built-in method for connecting to Microsoft RDP servers - the Sun Ray Windows Connector, also known as .\nas you might have guessed, it’s broken as fuck on OpenIndiana, even putting aside the fact that the newest RDP server it knows how to handle would be in the Windows Server 2003 era.</p><p>so, let’s hack something together with XFreeRDP!</p><p>i wanted to be able to specify what RDP server each token would connect to. this was a fairly common use case back in the day, and some people wrote helpers to allow things like that - one of which being <a href=\"https://web.archive.org/web/20131212042126/https://blogs.oracle.com/danielc/entry/meta_kiosk_how_to_run\">Daniel Cifuentes’ meta-kiosk</a>, which i borrowed some ideas from.</p><p>after much trial and error, i got something working!</p><div><figure><pre><code data-lang=\"shell\">/freerdp\n</code></pre></figure></div><div><figure><pre><code data-lang=\"shell\">\n\nopenbox  &amp;\n/opt/SUNWut/bin/utscreenresize  all  &amp;\n\n/opt/SUNWut/sbin/utuser  |  | zenity 1\nxterm  xfreerdp /cert:tofu /f  /dynamic-resolution /gfx +gfx-thin-client /smartcard /bpp:24 </code></pre></figure></div><p>after throwing those in place, install the dependencies and configure the session:</p><div><div><pre><code># pkg install openbox freerdp\n# printf \"KIOSK_SESSION=freerdp\\n\" | /opt/SUNWut/sbin/utkiosk -i FreeRDP\n</code></pre></div></div><p>then it’s just a matter of adding the needed data to each token, and assigning the tokens to the FreeRDP session:</p><div><div><pre><code># /opt/SUNWut/sbin/utkioskoverride -s kiosk -r OpenPlatform.47905167523905788499 -c FreeRDP\n</code></pre></div></div><p>upon inserting that token into a client…</p><p>with much the same setup as the RDP sessions, it’s pretty easy to start a kiosk-mode Firefox, pulling the URL to open from the token data:</p><div><figure><pre><code data-lang=\"shell\">/kiosk-browser\n</code></pre></figure></div><div><figure><pre><code data-lang=\"shell\">\n\nopenbox  &amp;\n/opt/SUNWut/bin/utscreenresize  all  &amp;\n\n\nxset s off\nxset s noblank\nxset /opt/SUNWut/sbin/utuser  |  | zenity 1\nfirefox </code></pre></figure></div><p>a problem, though. Firefox would show its first-run “Welcome to Firefox” popup… every time. Sun Ray kiosk sessions run as a random user named   (where  is a number), and after the kiosk session ends the home directory of the kiosk user gets fully deleted, so the user can be recycled for other sessions. given i wanted to use this with some always-on Sun Rays, with no input devices attached…</p><p>thankfully, Firefox policies allow turning that off! throwing this hunk of JSON into <code>/etc/firefox/policies/policies.json</code> fixed that:</p><div><figure><pre><code data-lang=\"json\"></code></pre></figure></div><p>and with that, i could create a token for an individual client (the tokens for this are , where the MAC is all lower-case), set that token’s “Other Info” field to the URL to show, and assign the kiosk session to that pseudo-token the same way as with smart card tokens.</p><p>this was a lot of fun to get working. i need to take a break from reading the Sun Ray Administration Guide though, so here’s my thinking for a potential part 2:</p><ul><li>i want to see how well the multi-head stuff works in SRSS - which joins multiple physical clients together into one desktop session, using the peripherals connected to the “primary” client. unfortunately the Xinerama support is weird (Xinerama and xrandr are mutually exclusive…), but if i can make it play ball it could be a neat thing to use.</li><li>i want to try and find a newer firmware package too, but that might be a little bit of a lost cause, given i refuse to give Oracle a bunch of money.</li><li>maybe i’ll set up another OpenIndiana VM and configure the HA failover in SRSS?</li></ul><p>for now, though… that’s all.</p>","contentLength":10036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44438900"},{"title":"Building a Personal AI Factory","url":"https://www.john-rush.com/posts/ai-20250701.html","date":1751404469,"author":"derek","guid":179509,"unread":true,"content":"<p>I keep several claude code windows open, each on its own git-worktree. o3 and sonnet 4 create plans, sonnet 3.7 or sonnet 4 execute the plan, and o3 checks the results against the original ask. Any issues found are fed back into the plan template and the code is regenerated. The factory improves itself.</p><p>Read on to see what might be useful for you.</p><h2>Guiding Principle – Fix Inputs, Not Outputs</h2><p>When something goes wrong, I don’t hand-patch the generated code. I don’t argue with claude. Instead, I adjust the plan, the prompts, or the agent mix so the next run is correct by construction.</p><p>If you know <a href=\"https://factorio.com/\">Factorio</a> you know it’s all about building a factory that can produce itself. If not, picture a top-down sandbox where conveyor belts and machines endlessly craft parts because the factory must grow. Do the same thing with AI agents: build a factory of agents that can produce code, verify it, and improve themselves over time.</p><h2>Basic day to day workflow - building the factory</h2><p>My main interface is <a href=\"https://claude.ai/code\">claude code</a>. It’s my computer now. I also have a local mcp which runs <a href=\"https://github.com/block/goose\">Goose</a> and o3. Goose only because I’ve already got it setup to use the models hosted in our Azure OpenAI subscription. Looking to improve this at some point, but it works for now.</p><p>I’ll give a high level task to claude code, which calls over to o3 to generate a plan. o3 is a good planner and can ask a bunch of good questions to clarify the job to be done. I then have it write out a  file with both my original ask and an implementation plan.</p><p>First, sonnet 4 reads the plan, verifies it, and turns it into a task list. Next claude code execute the plan, either with sonnet 3.7 or sonnet 4 depending on the complexity of the task. Because most of my day-to-day is in clojure I tend to use sonnet 4 to get the parens right.\nOne important instruction is to have claude write commits as it goes for each task step. This way either claude or I can revert to a previous state if something goes wrong.</p><h3>Step 3: Verification → Feedback into Inputs</h3><p>Once the code is generated, I have sonnet 4 verify the code against the original plan. Then I have o3 verify the code against the original plan and original ask. o3 is uncompromising. Claude wants to please, so will keep unnecessary backwards compatibility code in place. o3 will call that out and ask for it to be removed. Claude also tends to add “lint ignore flags” to the code which o3 will also call out. Having both models verify the code catches issues and saves me back and forth with claude.</p><p>Any issue sonnet 4 or o3 finds gets baked back into the plan template, not fixed inline.</p><p>Git worktrees let me open concurrent claude code instances and build multiple features at once. I still merge manually, but I’m no longer babysitting a single agent.</p><ul><li>Outputs are disposable; plans and prompts compound.</li><li>Debugging at the source scales across every future task.</li><li>It transforms agents from code printers into self-improving colleagues.</li></ul><p>Example: an agent once wrote code that would load an entire CSV into memory. I made it switch to streaming and had the agent write instructions to the plan to always use streaming for CSVs. Now, my plan checker flags any code that doesn’t use streaming for CSVs, and I don’t have to remember this in every PR review. The factory improves itself.</p><p>I’ve started to encode more complex workflows, where I have specific agents (behind mcps) for building specific tasks.</p><p>One MCP will sweep all the clojure code generated and then apply our local style rules. These rules are part of the instructions for the original plan and agent but often the generated code will have style issues. Especially once claude gets in the lint/test/debug cycle. This focused agent means we have tighter behavior and can apply our style rules consistently.</p><p>I’ve started doing this for internal libraries as well. It’s good at looking at generated code and replacing things like retries and  with our retry library.</p><p>I’m also building out a collection of these small agents. Each one can take a small specific task, and by composing them together I can build more complex workflows. For example, I can take an api doc, and a set of internally defined business cases and have a composition of agents build integrations, tests, and documentation for the api. This is a powerful way to build out features and integrations without having to do all the work by hand.</p><p>You don’t get there in one big step. Here’s the secret sauce: </p><p>It’s essentially free to fire off a dozen attempts at a task - so I do. All agents run in parallel. When one fails, stalls, or lacks context, I feed that lesson into the next iteration. I resist the urge to fix outputs, instead I fix the inputs.</p><p>That loop is the factory: the code itself is disposable; the instructions and agents are the real asset.</p><p>I’m working on a few things to improve the factory:</p><ul><li>Better overall coordination of the agents. I tend to kick things off manually, but I want to have a more automated way to manage the workflow and dependencies between agents.</li><li>Aligning our business docs with the agents. Changing the information we capture to be at a higher level of abstraction so that the agents can use it more effectively. This means moving away from low level implementation details and focusing on use cases.</li><li>More complex workflows. I’ve been able to build some pretty complex workflows with the current setup, but I want to push it further. This means more agents, more coordination, and more complex interactions between them.</li><li>Maximize token usage across providers. I’m pretty limited by bedrock’s token limits especially for sonnet 4. Going to need to be able to switch between the claude max plan and bedrock w/out interruption.</li></ul><p>That’s where my factory sits today: good enough to ship code while I refill my coffee, not yet good enough to bump me off the payroll. Constraints will shift, but the core principle remains: .</p>","contentLength":5911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44438065"},{"title":"Effectiveness of trees in reducing temperature, outdoor heat exposure in Vegas","url":"https://iopscience.iop.org/article/10.1088/2752-5295/ade17d","date":1751403545,"author":"PaulHoule","guid":179508,"unread":true,"content":"<h2>We apologize for the inconvenience...</h2><p>To ensure we keep this website safe, please can you confirm you are a human by ticking the box below. </p><p>If you are unable to complete the above request please contact us using the below link, providing a screenshot of your experience.</p>","contentLength":269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437948"},{"title":"The Roman Roads Research Association","url":"https://www.romanroads.org/","date":1751401972,"author":"bjourne","guid":180366,"unread":true,"content":"<p><a href=\"https://www.eventbrite.co.uk/e/road-construction-and-transport-infrastructure-in-the-roman-empire-registration-823562416357\" target=\"_blank\">Booking</a></p><p><a href=\"https://www.eventbrite.co.uk/e/788098332327?aff=oddtdtcreator\" target=\"_blank\">Booking</a></p><p><a href=\"https://youtu.be/DLc8lQvVcvM\" target=\"_blank\">https://youtu.be/DLc8lQvVcvM</a></p><p><a href=\"http://www.romanroads.org/gazetteer/cheshire/cheshire.html\" target=\"_blank\">Roman Roads in Cheshire</a></p>","contentLength":65,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437758"},{"title":"Fakespot shuts down today after 9 years of detecting fake product reviews","url":"https://blog.truestar.pro/fakespot-shuts-down/","date":1751401595,"author":"doppio19","guid":179507,"unread":true,"content":"<p>Today marks the end of an era. After nearly a decade of helping millions of shoppers navigate the murky waters of online reviews, <a href=\"https://blog.mozilla.org/en/mozilla/building-whats-next/?ref=blog.truestar.pro\">Fakespot has officially closed its doors</a>. If you tried to check a product listing this morning and found Fakespot not working, you're not alone. The service has permanently shut down.</p><p> Fakespot, the popular fake review detection tool acquired by Mozilla in 2023, shut down today, July 1, 2025. Founded by Saoud Khalifah in 2016, it helped millions identify unreliable Amazon reviews with 90% accuracy before Mozilla discontinued it due to sustainability challenges.</p><p>Back in 2016, Saoud Khalifah bought a product on Amazon, trusting the glowing reviews, only to discover he'd been duped by fake feedback. Instead of just leaving his own angry review, Khalifah took a more proactive approach: he built Fakespot.</p><p>What started as one person's frustration with deceptive sellers became a tool that analyzed millions of reviews across Amazon and other major retailers like eBay and Walmart. The premise was simple but powerful: use AI to spot patterns that human shoppers might miss, like suspiciously similar language or reviewer profiles that didn't quite add up.</p><h2>The magnitude of the deception</h2><p>Fakespot's technology revealed some eye-opening statistics. About 43% of the best-selling Amazon products had reviews that were unreliable or fabricated, according to a <a href=\"https://getcircuit.com/route-planner/blog/amazon-fake-review-analysis?ref=blog.truestar.pro\">study by app company Circuit</a>. The problem was even worse in certain categories. Clothing and jewelry led the pack with a staggering 88% of reviews deemed unreliable.</p><p>These numbers painted a sobering picture of the online shopping landscape. Most of us rely on product reviews as a major factor when deciding what to buy, but nearly half of the feedback you read might not be genuine.</p><p>Three years later, <a href=\"https://blog.mozilla.org/en/mozilla/fakespot-joins-mozilla-firefox-shopping-announcement/?ref=blog.truestar.pro\">Mozilla acquired Fakespot</a>, bringing the startup's 13-person team into the Firefox family. Mozilla integrated Fakespot's technology directly into Firefox as the \"Mozilla Review Checker\" feature, making it easier than ever for users to verify product reviews without installing separate extensions.</p><p>For many users, this felt like a perfect match. Mozilla's reputation for privacy and transparency aligned beautifully with Fakespot's mission to bring honesty to online shopping.</p><p>But as Mozilla announced in May, not all acquisitions fit into a sustainable long-term model. The company made the difficult decision to discontinue both Pocket and Fakespot as part of a strategic refocus on Firefox's core features and AI-powered innovations.</p><p>The reasons were practical, if devastating for users. A flood of reviews lamenting the closure have appeared on Fakespot's extension page on the Chrome Web Store:</p><p>Fakespot's mission resonated strongly with consumers, but <a href=\"https://www.distractify.com/p/why-is-fakespot-shutting-down?ref=blog.truestar.pro\">Mozilla couldn't find a sustainable model</a> to keep it running. Resources that once supported the service would now flow toward Firefox features like vertical tabs, smart search, and additional AI-powered features.</p><p>As we say goodbye to Fakespot, it's worth reflecting on what it accomplished. For nine years, it served as a defender against fraud in an increasingly deceptive marketplace. It gave shoppers a fighting chance against promotional reviewers and bot farms that undermine trust in online shopping.</p><p>For those of us who came to rely on Fakespot's review analysis before making purchases, its absence leaves us less confident in our buying decisions. The need for trustworthy review analysis hasn't gone away. If anything, it's more critical than ever.</p><p>I know I'm not alone in feeling this gap, which is why I've begun building a tool that aims to be the spiritual successor to Fakespot. <a href=\"https://truestar.pro/?ref=blog.truestar.pro\">TrueStar</a> will use modern AI, streamlined analysis techniques, and sustainable economics to keep costs manageable while maintaining the accuracy shoppers need.</p><div data-layout=\"minimal\"><div><div><a href=\"https://truestar.pro/?ref=blog.truestar.pro\">\n                            Get notified\n                        </a></div></div></div><h2>Quick answers about Fakespot's closure</h2><p><strong>When did Fakespot shut down?</strong>Fakespot officially closed on July 1, 2025, with the Mozilla Review Checker feature in Firefox having ended on June 10, 2025.</p><p><strong>Why did Fakespot shut down?</strong>Mozilla couldn't find a sustainable business model for Fakespot despite its popularity, choosing to redirect resources to core Firefox features and AI-powered browser tools.</p><p><strong>What happened to Fakespot?</strong>Mozilla acquired Fakespot in 2023 but announced in May 2025 that both Fakespot and Pocket would be discontinued as part of a strategic refocus on Firefox development.</p><p><strong>What are the best Fakespot alternatives?</strong>While several options exist including ReviewMeta, The Review Index, and emerging tools like <a href=\"https://truestar.pro/?ref=blog.truestar.pro\">TrueStar</a>, the market is still developing sustainable solutions that balance accuracy with affordability.</p><p>As Fakespot's servers go dark, let's raise a glass to the tool that made online shopping so much more trustworthy for nearly a decade. Thanks to Saoud Khalifah and his team for showing us what's possible when technology serves truth over profit.</p><p>Rest in peace, Fakespot. You fought the good fight. 🥂</p><p><em>If you found this article helpful, consider sharing it with others who might be wondering why their favorite review checker stopped working today. Let's keep the conversation about online authenticity going.</em></p>","contentLength":5181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437712"},{"title":"Figma files for proposed IPO","url":"https://www.figma.com/blog/s1-public/","date":1751398754,"author":"kualto","guid":179307,"unread":true,"content":"<p>Figma, Inc. (“Figma”) today announced that it has filed a registration statement on Form S-1 with the U.S. Securities and Exchange Commission (“SEC”) relating to a proposed initial public offering of its Class A common stock. Figma has applied to list its Class A common stock on the New York Stock Exchange under the symbol “.”</p><p>The number of shares to be offered and the price range for the proposed offering have not yet been determined. The offering is subject to market conditions, and there can be no assurance as to whether or when the offering may be completed, or as to the actual size or terms of the offering.</p><p><em>Morgan Stanley, Goldman Sachs &amp; Co. LLC, Allen &amp; Company LLC, and J.P. Morgan will act as joint lead book-running managers for the proposed offering. BofA Securities, Wells Fargo Securities, and RBC Capital Markets will act as book-running managers for the proposed offering. William Blair and Wolfe | Nomura Alliance will act as co-managers for the proposed offering.</em></p><p><em>The proposed offering will be made available only by means of a prospectus. Copies of the preliminary prospectus, when available, may be obtained from Morgan Stanley &amp; Co. LLC, Attention: Prospectus Department, 180 Varick Street, 2nd Floor, New York, New York 10014, or by email at prospectus@morganstanley.com; Goldman Sachs &amp; Co. LLC, Attention: Prospectus Department, 200 West Street, New York, New York 10282, by telephone at (866) 471-2526, or by email at prospectus-ny@ny.email.gs.com; Allen &amp; Company LLC, Attention: Prospectus Department, 711 Fifth Avenue, New York, New York 10022, by telephone at (212) 339-2220, or by email at allenprospectus@allenco.com; or J.P. Morgan Securities LLC, c/o Broadridge Financial Solutions, 1155 Long Island Avenue, Edgewood, New York 11717 or by email at prospectus-eq_fi@jpmchase.com and postsalemanualrequests@broadridge.com.</em></p><p><em>A registration statement on Form S-1 relating to these securities has been filed with the SEC but has not yet become effective. These securities may not be sold, nor may offers to buy be accepted, prior to the time the registration statement becomes effective. This press release shall not constitute an offer to sell or the solicitation of an offer to buy these securities, nor shall there be any sale of these securities in any state or jurisdiction in which such offer, solicitation, or sale would be unlawful prior to registration or qualification under the securities laws of any such state or jurisdiction.</em></p><p>Figma is where teams come together to turn ideas into the world’s best digital products and experiences. Founded in 2012, Figma has evolved from a design tool to a connected, AI-powered platform that helps teams go from idea to shipped product. Whether you’re ideating, designing, building, or shipping, Figma makes the entire design and product development process more collaborative, efficient, and fun––while keeping everyone on the same page.</p>","contentLength":2935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44437316"},{"title":"Sam Altman Slams Meta’s AI Talent Poaching: 'Missionaries Will Beat Mercenaries'","url":"https://www.wired.com/story/sam-altman-meta-ai-talent-poaching-spree-leaked-messages/","date":1751393318,"author":"spenvo","guid":180365,"unread":true,"content":"<p> Altman is hitting back at Meta CEO Mark Zuckerberg’s recent AI talent-poaching spree. In a full-throated response sent to OpenAI researchers Monday evening and obtained by WIRED, Altman made his pitch for why staying at OpenAI is the only answer for those looking to build artificial general intelligence, hinting that the company is evaluating compensation for the entire research organization.</p><p>He also dismissed Meta’s recruiting efforts, saying what the company is doing could lead to deep cultural problems down the road.</p><p>“We have gone from some nerds in the corner to the most interesting people in the tech industry (at least),” he wrote on Slack. “AI Twitter is toxic; Meta is acting in a way that feels somewhat distasteful; I assume things will get even crazier in the future. After I got fired and came back I said that was not the craziest thing that would happen in OpenAl history; certainly neither is this.”</p><p>The news comes on the heels of a major announcement from Zuckerberg. On Monday, the Meta CEO <a href=\"https://www.wired.com/story/mark-zuckerberg-welcomes-superintelligence-team/\">sent a memo</a> to staff introducing the company’s new superintelligence team, which will be helmed by Alexandr Wang, formerly of Scale AI, and Nat Friedman, who previously led GitHub. The list of new hires also included a number of <a href=\"https://www.wired.com/story/four-openai-researchers-leave-meta/\">people from OpenAI</a>, including Shengjia Zhao, Shuchao Bi, Jiahui Yu, and Hongyu Ren. OpenAI’s chief research officer, Mark Chen, <a href=\"https://www.wired.com/story/openai-meta-leadership-talent-rivalry/\">told staff</a> that it felt like “someone has broken into our home and stolen something.”</p><p>Altman struck a different tone about the departures in his note on Monday.</p><p>“Meta has gotten a few great people for sure, but on the whole, it is hard to overstate how much they didn't get their top people and had to go quite far down their list; they have been trying to recruit people for a super long time, and I've lost track of how many people from here they've tried to get to be their Chief Scientist,” he wrote. “I am proud of how mission-oriented our industry is as a whole; of course there will always be some mercenaries.”</p><p>He added that “Missionaries will beat mercenaries” and noted that OpenAI is assessing compensation for the entire research organization. “I believe there is much, much more upside to OpenAl stock than Meta stock,” he wrote. “But I think it's important that huge upside comes after huge success; what Meta is doing will, in my opinion, lead to very deep cultural problems. We will have more to share about this soon but it's very important to me we do it fairly and not just for people who Meta happened to target.”</p><p>Altman then made his pitch for people to remain at OpenAI. “I have never been more confident in our research roadmap,” he wrote. “We are making an unprecedented bet on compute, but I love that we are doing it and I'm confident we will make good use of it. Most importantly of all, I think we have the most special team and culture in the world. We have work to do to improve our culture for sure; we have been through insane hypergrowth. But we have the core right in a way that I don't think anyone else quite does, and I'm confident we can fix the problems.”</p><p>“And maybe more importantly than that, we actually care about building AGI in a good way,” he added. “Other companies care more about this as an instrumental goal to some other mission. But this is our top thing, and always will be. Long after Meta has moved on to their next flavor of the week, or defending their social moat, we will be here, day after day, year after year, figuring out how to do what we do better than anyone else. A lot of other efforts will rise and fall too.”</p><p>A number of high-ranking employees who’ve worked at Meta followed up in Slack with their own stories about why OpenAI’s culture is superior. “[T]hey constantly rotate their top focus,” wrote one. Another said: “Yes we’re quirky and weird, but that’s what makes this place a magical cradle of innovation,” wrote one. “OpenAI is weird in the most magical way. We contain multitudes.”</p>","contentLength":4000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44436579"},{"title":"Show HN: Arch-Router – 1.5B model for LLM routing by preferences, not benchmarks","url":"https://news.ycombinator.com/item?id=44436031","date":1751389991,"author":"adilhafeez","guid":180472,"unread":true,"content":"Hi HN — we're the team behind Arch (<a href=\"https://github.com/katanemo/archgw\">https://github.com/katanemo/archgw</a>), an open-source proxy for LLMs written in Rust. Today we're releasing Arch-Router (<a href=\"https://huggingface.co/katanemo/Arch-Router-1.5B\" rel=\"nofollow\">https://huggingface.co/katanemo/Arch-Router-1.5B</a>), a 1.5B router model for preference-based routing, now integrated into the proxy. As teams integrate multiple LLMs - each with different strengths, styles, or cost/latency profiles — routing the right prompt to the right model becomes a critical part of the application design. But it's still an open problem. Most routing systems fall into two camps:<p>- Embedding-based routers use intent classifiers — label a prompt as “support,” “SQL,” or “math,” then route to a matching model. This works for simple tasks but breaks down in real conversations. Users shift topics mid-conversation, task boundaries blur, and product changes require retraining classifiers.</p><p>- Performance-based routers pick models based on benchmarks like MMLU or MT-Bench, or based on latency or cost curves. But benchmarks often miss what matters in production: domain-specific quality or subjective preferences like “Will legal accept this clause?”</p><p>Arch-Router takes a different approach: route by preferences written in plain language. You write rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini Flash.” The router maps the prompt (and conversation context) to those rules using a lightweight 1.5B autoregressive model. No retraining, no fragile if/else chains. We built this with input from teams at Twilio and Atlassian. It handles intent drift, supports multi-turn conversations, and lets you swap in or out models with a one-line change to the routing policy. Full details are in our paper (<a href=\"https://arxiv.org/abs/2506.16655\" rel=\"nofollow\">https://arxiv.org/abs/2506.16655</a>), but here's a snapshot:</p><p>- 1.5B params — runs on a single GPU (or CPU for testing)</p><p>- No retraining needed — point it at any mix of LLMs</p><p>- Cost and latency aware — route heavy tasks to expensive models, light tasks to faster/cheaper ones</p><p>- Outperforms larger closed models on our conversational routing benchmarks (details in the paper)</p>","contentLength":2094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44436031"},{"title":"Code-GUI bidirectional editing via LSP","url":"https://jamesbvaughan.com/bidirectional-editing/","date":1751388199,"author":"jamesbvaughan","guid":179506,"unread":true,"content":"<p>I built a small proof-of-concept for a system that enables real-time\nbidirectional editing between any modern code editor and a GUI, enabled by an\nLSP server.</p><p>I like working on small projects at home that benefit from CAD. I’m also a\nprogrammer with a personal development environment that I’ve spent years making\nas cozy as possible. Naturally I’ve been interested in finding code-based CAD\nsystem to use for my projects that allows me to use that cozy development\nenvironment.</p><blockquote>For example: One idea I’m exploring is “bidirectional editing”, so geometry can\nbe manipulated using either:<ul><li>a purpose-built graphical UI, or</li><li>the textual codeCAD language</li></ul><p>If you graphically drag a point around, the coordinates in the source code\nshould automatically update.\nIf you edit the source code, the graphical UI should automatically update.</p><p>A simple way to test this idea is to throw a  in the UI that\ndisplays the corresponding source code.\nBut to me, that feels terrible because I never want to be coding in some janky,\nin-browser  — I want to be working with source code in Emacs, with\nall of my familiar key bindings, color schemes, autocomplete, and decades of\ncozy practice.</p><p><strong>That’s the core appeal of a textual programming language.</strong></p><p>But doing this properly is an absolute boatload of work:</p><ul><li>How does the system rewrite source code? Is it mediated by files on disk with\nreload on save? How do the editor and UI stay in sync and avoid clobbering\neach other’s unsaved changes? <strong>Maybe we need an LSP server?</strong></li><li>The language interpreter needs to preserve comments and flow them through,\neven when the UI makes edits to the code.</li><li>What about whitespace / pretty-printing?</li></ul><p>How much of this needs to be built to evaluate whether bidirectional editing\n“fits nicely in the hand”?</p></blockquote><blockquote><p>Maybe we need an LSP server?</p></blockquote><p>I’ve been a happy user of LSP servers since they became commonplace in Neovim\nsetups, but I have almost no experience with language server internals.\nI had certainly never considered that they could facilitate bidirectional\nediting with a GUI.</p><p>That line from Kevin’s post was a proper nerd-snipe because a few hours later I\nhad built this proof-of-concept:</p><p>What you’re seeing here is a text editor next to a GUI, and data live-updating\nboth ways between them, made possible by a small server that uses LSP to\ncommunicate with the text editor and WebSockets to communicate with a web app.</p><p>I’ve shared more technical details and the code for this demo <a href=\"https://github.com/jamesbvaughan/bidirectional-number-editor\">here on\nGitHub</a>.</p><p>Bidirectional editing isn’t new.\nWhat’s new, as far as I’m aware, is real-time bidirectional editing <em>that works\nwith your favorite text editor.</em></p><p>I’ve tried out a handful of code-based CAD systems, but so far I haven’t found\nany that achieve more than two out of these three features:</p><ul><li>Real-time-ish updates in the GUI from changes made in the code</li><li>Real-time-ish updates in the code from changes made in the GUI</li><li>Works well with my preferred code editor</li></ul><p><a href=\"https://www.autodesk.com/products/fusion-360/overview#top\">Fusion 360</a> has\ndecent bidirectional editing for parameters, but it’s not fully code-based and\nit certainly doesn’t let me use my own editor.</p><p><a href=\"https://openscad.org/\">OpenSCAD</a> doesn’t require the use of its own text\neditor, and it’s possible to trigger reloads in the GUI via file watching\nwhen you save source files in external editors, but it only goes one way.</p><p><a href=\"https://zoo.dev/design-studio\">Zoo</a> has some bidirectional editing, but only\nwith its built-in editor.</p><p><a href=\"https://www.arcol.io/\">Arcol</a>, the tool that I help build at my day job, is\ninnovating in CAD interface design in some exciting ways, but we’re building for\narchitects, not programmers.</p><p>This is just a toy demo, but it’s enough to excite me about the possibility of a\nsystem that achieves  of those points!</p><p>I don’t plan to develop this demo further, at least not anytime soon, but I hope\nit inspires people to find more creative uses (abuses?) of LSP servers.</p><p>One of the best code-CAD environments I’ve worked in is OpenSCAD + Neovim with\nthe <a href=\"https://github.com/Leathong/openscad-LSP\">OpenSCAD LSP server</a>, only using\nthe OpenSCAD GUI for the viewer, not the built-in text editor.\nOpenSCAD is fundamentally not built for GUI editing, but since it’s open source\nand has a nice language server already, it could be a good place to develop a\nmore interesting demo of this concept.</p><p>Like Kevin’s post said, doing this properly will be a boatload of work.\nHandling conflict resolution, incremental edits, and the more complex general\nLSP server internals are all serious tasks, let alone creating a whole new\nlanguage for CAD.</p><p>I’m looking forward to seeing what Kevin comes up with for codeCAD!</p>","contentLength":4463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435716"},{"title":"Show HN: Core – open source memory graph for LLMs – shareable, user owned","url":"https://github.com/RedPlanetHQ/core","date":1751387064,"author":"Manik_agg","guid":179512,"unread":true,"content":"<p>I keep running in the same problem of each AI app “remembers” me in its own silo. ChatGPT knows my project details, Cursor forgets them, Claude starts from zero… so I end up re-explaining myself dozens of times a day across these apps.</p><p>1. Not portable – context is vendor-locked; nothing travels across tools.</p><p>2. Not relational – most memory systems store only the latest fact (“sticky notes”) with no history or provenance.</p><p>3. Not yours – your AI memory is sensitive first-party data, yet you have no control over where it lives or how it’s queried.</p><p>- CORE (Context Oriented Relational Engine): An open source, shareable knowledge graph (your memory vault) that lets any LLM (ChatGPT, Cursor, Claude, SOL, etc.) share and query the same persistent context.</p><p>- Temporal + relational: Every fact gets a full version history (who, when, why), and nothing is wiped out when you change it—just timestamped and retired.</p><p>- Local-first or hosted: Run it offline in Docker, or use our hosted instance. You choose which memories sync and which stay private.</p>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435500"},{"title":"The Fed says this is a cube of $1M. They're off by half a million","url":"https://calvin.sh/blog/fed-lie/","date":1751386975,"author":"c249709","guid":179072,"unread":true,"content":"<p>At the Federal Reserve Bank of Chicago’s Money Museum, there’s a big transparent cube on display. It’s filled with tightly packed stacks of  bills, claiming to contain .</p><p>The plaque proudly declares:</p><blockquote><p>Have you ever wondered what one million dollars looks like?\nYou don’t have to wonder anymore because you can see it right in front of you!</p></blockquote><p>But I don’t trust signs. I trust counting.</p><p>I first tried counting the stacks right there in the room. The cube was tall, so I had to step back to see the whole thing, squinting at the stacks, trying to follow each row. I lost track almost immediately.</p><p>Also, people were starting to look at me funny. Apparently, staring intensely at a pile of cash while muttering numbers isn’t normal museum behavior.</p><p>Then, I tried with a photo. I zoomed all the way in on my phone, dragging my finger across the screen, mentally tallying as I went.</p><p>Still couldn’t keep count.</p><p>All I wanted was a way to click on things in a photo and have the number go up.</p><p>You’d think this would already exist, a browser based tool for counting things.</p><p>Turns out it… doesn’t. At least, not as a web app I can find on Google.</p><p>There are some clunky old Windows programs, niche scientific tools, and image analysis software that assumes you’re trying to count cells under a microscope, not people, penguins, or stacks of $1 bills in a Federal Reserve cube.</p><p>It’s stupidly simple: upload an image, click to drop a dot, and it tells you how many you’ve placed. That’s it. But somehow, nothing like it existed.</p><p>I originally made it to investigate this very cube, but I figured other people might need to count stuff in pictures.</p><p>Count your enemies. Count your blessings. Count your stacks of cash.</p><p>Because when someone tells you it’s a million dollars, you might want to double check.</p><a href=\"https://calvin.sh/blog/fed-lie/cube-labeled.png\" data-v-3b6c5e00=\"\"></a><p>Assuming each bundle contains  bills*, that’s</p><p>So yeah. They’re off by .</p><p>That’s  in extra cash.</p><blockquote><p>“Hey so… we’re $550,400 over budget on the million-dollar cube project.”</p></blockquote><p>If you knock  from each dimension (basically pealing away the outermost layer of money bundles), the math actually gets kinda close</p><p>but since dollar bills are much wider than they’re tall, it wouldn’t look like a cube anymore.</p><p>Maybe the Fed is playing the long game.</p><p>At the Fed’s  inflation target, this cube will be worth  million in today’s dollars in:</p><p>Can’t wait to come back in 2047 and say: “Nice. Nailed it.”</p><p>Sure, it does technically contain .</p><p>And also  of bonus money.</p><p>Which is kind of like ordering a burger and getting three.</p><p>I mean, sure, free stuff. But it’s not what you asked for.</p><p>You can only see the outer stacks. For all we know, the middle is just air and crumpled-up old newspaper.</p><p>A money shell. A decorative cube. A fiscal illusion. The world’s most expensive piñata (but don’t hit it, security is watching).</p><p>And get this: just the outermost layer is already worth:</p><p>You’d only need a 3-layer-thick shell to blow past a million:</p><h2>How  you make a million dollar cube?</h2><p>Turns out U.S. dollars are extremely non-cube-friendly. Each bill is  wide by  tall, a nice and even aspect ratio of:</p><p>Each 100-bill bundle is  inches thick.</p><ul><li> stacks</li></ul><p>Which gives you a lovely almost-cube:</p><ul><li> wide</li><li> deep</li><li> tall</li></ul><p>Not perfect. Not terrible. At least it’s honest, unlike that other cube.</p><p>Maybe it’s  million.</p><p>Maybe it’s an empty box with a money shell.</p><p>Most likely it’s  million.</p><p>All I know is I built a tool, did the math, and triple-checked the stacks.</p><p>The sign says you don’t have to wonder.\nBut I did anyway.</p><p>And now… you don’t have to either.</p>","contentLength":3525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435484"},{"title":"Show HN: HackerNewt – Breadth-first exploring HN client for iOS","url":"https://apps.apple.com/us/app/hackernewt-for-hacker-news/id6448201970","date":1751385443,"author":"hnand","guid":180471,"unread":true,"content":"<p dir=\"false\" data-test-bidi=\"\">App doesn’t work anymore. Nothing loads at all and spinner just spins. Tried clearing cache and uninstalling and reinstalling app. Disappointed I paid for this.<p>Update 6/5/24 - Developer quickly fixed the issue with the app and responded with the feedback. Kudos for listening to feedback and updating. Updating to 5. Thanks!</p></p>","contentLength":326,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44435211"},{"title":"HN Slop: AI startup ideas generated from Hacker News","url":"https://www.josh.ing/hn-slop","date":1751383905,"author":"coloneltcb","guid":180364,"unread":true,"content":"<div>© 2025 Just Joshing, LLC. All rights reserved.</div>","contentLength":47,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434938"},{"title":"Ask HN: Who is hiring? (July 2025)","url":"https://news.ycombinator.com/item?id=44434576","date":1751382096,"author":"whoishiring","guid":179306,"unread":true,"content":"Please state the location and include REMOTE for remote work, REMOTE (US)\nor similar if the country is restricted, and ONSITE when remote work is  an option.<p>Please only post if you personally are part of the hiring company—no\nrecruiting firms or job boards. One post per company. If it isn't a household name,\nexplain what your company does.</p><p>Please only post if you are actively filling a position and are committed\nto responding to applicants.</p><p>Commenters: please don't reply to job posts to complain about\nsomething. It's off topic here.</p><p>Readers: please only email if you are personally interested in the job.</p><p>Don't miss these other fine threads:</p>","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434576"},{"title":"Ask HN: Who wants to be hired? (July 2025)","url":"https://news.ycombinator.com/item?id=44434574","date":1751382096,"author":"whoishiring","guid":180363,"unread":true,"content":"Share your information if you are looking for work. Please use this format:<pre><code>  Location:\n  Remote:\n  Willing to relocate:\n  Technologies:\n  Résumé/CV:\n  Email:\n</code></pre>\nPlease only post if you are personally looking for work. Agencies, recruiters, job boards,\nand so on, are off topic here.<p>Readers: please only email these addresses to discuss work opportunities.</p>","contentLength":355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434574"},{"title":"Conversations with a hit man","url":"https://magazine.atavist.com/confessions-of-a-hit-man-larry-thompson-jim-leslie-george-dartois-louisiana-shreveport-cold-case/","date":1751380172,"author":"gmays","guid":183197,"unread":true,"content":"<p>a night for Jim Leslie to savor. On July 8, 1976, the Louisiana State Senate passed what was known as the Right to Work Bill. One of the most fiercely debated pieces of legislation in decades, the law did away with mandatory union membership and allowed businesses to hire nonunion workers.</p><p>Given the interests involved, this was a staggering achievement. Labor had a muscular presence in Louisiana, largely because it was controlled by organized crime. Developers were ordered to kick back as much as 25 percent of the cost of construction, and those who wouldn’t play along sometimes faced dire consequences. Plants blew up and buildings burned. In January 1976, a gang of more than 75 men commandeered a forklift to smash through the gate of a Jupiter Chemical construction site, where the company was building a new facility using non-AFL-CIO crews. The mob fired hundreds of rounds, killing one man and injuring five others.</p><p>Hired by the Louisiana Association of Business and Industry, Leslie inserted himself into the debate by creating a savvy ad campaign intended to build support for the Right to Work Bill. He was known for creating some of Louisiana’s first sophisticated TV spots. His reputation was as a brilliant influencer, and he was in high demand. He had run about 60 political campaigns by then, winning more than 80 percent of them. He had already opened three satellite offices—in Dallas, Baton Rouge, and Monroe, Louisiana—and seemed destined for the national stage; people speculated that he would spearhead the Southern arm of a presidential campaign, maybe as soon as 1980.</p><p>No one in Shreveport was surprised by his success. Leslie was a soft-spoken jokester and spellbinding storyteller with a dimpled chin, a Paul McCartney mop of hair, and a prodigious work ethic. At 20, he’d dropped out of college to become a reporter at the , where he latched on to daring stories. After wrangling an invitation to a Ku Klux Klan meeting, he wrote a piece describing 60 robed members conducting a cross burning. He exposed after-hours liquor sales and served as the prosecution’s star witness when the city shut down two well-known club operators. Once, while covering a shootout between police and a fugitive, Leslie was so excited that he yelled “Shoot! Shoot!” at the staff photographer behind him, only to look back and see his colleague waving a firearm. “No, no! The ” he shouted.</p><p>Leslie became a public relations executive in 1964, and when his employer announced a move to Houston in 1967, he remained in Shreveport and opened his own agency. In 1972, he scored a chance to lead J. Bennett Johnston’s long-shot campaign for U.S. Senate; after his man won, Leslie’s phone never stopped ringing. In 1975, he elevated Anthony Guarisco from little-known attorney to state senator by filming a commercial featuring an actor dressed in 18th-century garb, quoting poetry. For the Right to Work campaign, he crafted a TV spot that showed labor leaders dangling state legislators on marionette strings.&nbsp;He was a Dixie Don Draper.</p><p> That July night, after the successful legislative vote, Leslie headed to Baton Rouge’s Camelot Club for an after-party, and then to the Sheraton Inn for drinks with colleagues. But he was not in a celebratory mood, according to Stonecipher. Leslie was married, but he slept around and feared he was facing divorce papers back home. “Jim knew what was waiting on him, family-wise, as soon as he got back to Shreveport,” Stonecipher says. “He was depressed.”</p><p>Eventually, Leslie said goodnight and made the short drive to the Prince Murat Inn, where he was staying. As he pulled into a parking spot, fireworks detonated close by, vestiges perhaps of the summer’s raucous celebration of the U.S. Bicentennial. It was about 1:50 a.m. when he exited his car. He was still wearing his work clothes: brown checked suit, yellow dress shirt, tie.</p><p>He’d taken about a dozen steps when a shotgun blast exploded into his upper back, tearing through his heart and lungs. Leslie staggered sideways and fell face down onto the asphalt, dead before he landed.</p><h2><strong>Curtis saw a white 1976 Oldsmobile Cutlass pull onto the property: Jim Leslie’s car. Moments later he heard the blast.</strong></h2><p><strong>John Curtis thought the sound he heard </strong>was a cherry bomb. Partiers had been setting off fireworks on the other side of the fence bordering the Prince Murat, in an adjacent shopping center with a grocery store and a club called the Cahoots Lounge. Curtis, the hotel’s assistant manager, had just come outside to see whether the revelry was spreading into the Prince Murat’s parking lot when he saw a white 1976 Oldsmobile Cutlass pull onto the property: Jim Leslie’s car.</p><p>Moments later he heard the blast. He quickly walked 100 feet to the corner of the hotel, where he could peer into the back lot, and saw a body in a widening pool of blood. Curtis summoned the hotel’s security guard, who checked on the body and told Curtis to dial 911.</p><p>Baton Rouge Police detective Chris Schroeder happened to be nearby at the time, checking out a report of possible shots fired. He entered the Prince Murat’s lot “simply to drive around the building to check,” he’d later testify. But then he saw a man “waving frantically, pointing around the corner.” It was Curtis. Schroeder accelerated, then had to slam on the brakes to avoid running over the body.</p><p>Once Leslie was confirmed dead, Schroeder radioed a patrolman dealing with fireworks complaints and told him to look for a man with a shotgun instead. Curtis didn’t see or hear a car leave the hotel, so police thought the shooter might be on foot. They fanned out, searching a large warehouse on the back side of the neighboring shopping center, but came up empty. Meanwhile, officers began processing the scene. Finding a loose board in a wooden fence near where Leslie had parked, they surmised that the killer had pushed a gun barrel between two slats to fire the shot.</p><p>The murder became national news. covered it, and radio legend Paul Harvey commented on it during his noon broadcast. Rumors churned that Leslie was targeted in retaliation for the Right to Work legislation. But Stonecipher and many others in Shreveport immediately thought someone else was responsible: George D’Artois. Having been present for the call between Leslie and D’Artois, Stonecipher concluded that Leslie had prophesied his own death.</p><p>As police looked for ties to the disgraced commissioner, there was a startling development: Before they could solve Leslie’s killing, a second murder took place.</p><p><strong>Rusty Griffith likely felt </strong>some anxiety about the meeting on the evening of October 15, 1976. Griffith, 34, was a 400-pound Shreveport man with a mustache and dense curly hair. He had many livelihoods, some of which were legitimate: He ran a seafood business; several nightclubs, including one called Big Daddy’s Lounge; and a limousine service that ferried people from Dallas and Houston to the Louisiana Downs racetrack in Bossier City. His links to organized crime were well-known. He shared an office with Don Gardner, 39, a large, bearded former wrestler from Shreveport. Both consorted with a group of dodgy characters loosely known in law-enforcement circles as the Dixie Mafia. Among them were a trio of men from Baton Rouge: Steve Simoneaux, Clayton Kimble, and Jules Ron Kimbel. The latter two were siblings but spelled their surnames differently.</p><p>That night, as dusk approached, Griffith drove his brown Cadillac into the Three Rivers Wildlife Management Area, a dense forest nestled between the Mississippi and Red Rivers in Concordia Parish, about 200 miles southeast of Shreveport. Jules Ron Kimbel had asked him to meet there. What Griffith expected remains unclear, but it may have been trouble. About three weeks earlier, he and Kimbel had been indicted in Mississippi for interstate transportation of a stolen bulldozer, part of a Dixie Mafia scheme in which heavy equipment was swiped and taken to Mexico to be fenced.</p><p>Griffith met Kimbel as planned, on one of Three Rivers’ dirt roads. While they talked, another vehicle pulled up. The driver rolled down his window, and someone fired a shotgun into Griffith’s face. He died at the scene. Griffith’s address book, found in his car, included contact information for numerous organized-crime figures—and for George D’Artois.</p><p>As police investigated the killing, various Dixie Mafia members claimed that Griffith possessed recordings of conversations about the machinery thefts, illegal gambling activities, and the Leslie murder. These sources said there was concern that Griffith might use the recordings to reduce a potential sentence in the heavy-equipment case. The theory of his murder quickly became that someone had silenced him and sent a warning to others, in case there were copies of the tapes.</p><p>By November 7, Jules Ron Kimbel had been booked as a material witness in the Griffith murder, and so began what the  would later describe as one of “countless investigations by parish and city law enforcement from Baton Rouge to Shreveport” into the deaths of Griffith and Leslie. Soon, Gardner and six others were arrested for first-degree murder in the Griffith case. </p><p>As for the Leslie murder, it was difficult to piece together a working theory.  By April 1977, the state had added Clayton Kimble and Steve Simoneaux to its list of witnesses, but their stories were ever changing and influenced by the promise of immunity in exchange for testimony, the possibility of a get-out-of-jail-free card for parole violations, and a $35,000 reward in the Leslie case offered by the Louisiana Association of Business and Industry. The best version of events investigators could piece together was this: Two weeks before the Leslie murder, Griffith and Gardner asked Clayton Kimble if he could find someone to kill Leslie for D’Artois. At a second meeting, Kimble was offered $30,000 to do the job himself but declined; killing wasn’t his kind of work. The three men agreed that the murder should happen in Baton Rouge to insinuate a link to organized labor and deflect attention from D’Artois. Gardner then gave Kimble $5,000 as a deposit while he looked for someone to take the contract.</p><p>Five days before Leslie was killed, Kimble returned the money, saying that he couldn’t find any takers. Griffith then said he and Gardner would do it themselves. They decided that Griffith would hide behind the hotel fence with a shotgun while Gardner waited in a car parked in the space closest to Leslie’s room. When Leslie arrived, Gardner would leave, prompting Leslie to pull into his spot, and Griffith would do the deed.</p><p>Kimble told police that several hours after the murder, the conspirators met at a Denny’s, where Griffith and Gardner reported that the plan had worked. Seconds after Griffith killed Leslie, Gardner had retrieved him from behind the fence. The two men then drove to a bridge over the Mississippi River and threw the shotgun Griffith used into the water below.</p><p>Police obtained an arrest warrant for first-degree murder for Gardner, who denied involvement and said he was in Shreveport the night Leslie was killed. (He also denied having anything to do with Griffith’s death.) Investigators brought the same charge against D’Artois, which only added to his legal headaches. Over the previous year, a grand jury had indicted him for tampering with evidence seized in gambling raids; for the theft of more than $32,000 in city funds; for intimidating grand jury witnesses; and for malfeasance of office. He had resigned from office the previous July.</p><p>On April 19, 1977, Caddo Parish sheriff’s deputies showed up at D’Artois’s home to arrest him. He quickly locked himself in his attic with a .357 Magnum. Officers persuaded him to come out—only for D’Artois to barricade himself in a bathroom. After an eight-hour siege, three deputies crashed through the bathroom door. One of them told the  that D’Artois “was grabbing for the gun when we pinned him to the floor.”</p><p> for the Leslie and Griffith cases to begin to fall apart. The police had no physical evidence linking any of the purported participants to either murder; they were relying entirely on Kimble, Kimbel, and Simoneaux and their shifting, conflicting accounts: While the three men sometimes agreed that Griffith had shot Leslie, in other moments Simoneaux claimed Clayton Kimble had pulled the trigger, or that “a man from Dallas” had done it.</p><p>One by one, charges against the defendants in the Griffith case were dropped for lack of evidence. After the prosecutor in the Leslie case refused to grant Simoneaux, Kimble, and Kimbel immunity, the men took the stand at a hearing and pleaded the Fifth. The murder charges against D’Artois and Gardner were quickly dismissed. But D’Artois didn’t have long to enjoy his emancipation. On June 8, he underwent heart surgery in Houston; three days later, at the age of 51, he died from complications, taking countless secrets with him.</p><p>D’Artois’s death, along with the collapse of the two cases, left people wondering whether the murders would ever be solved. Then, in 1981, a new federal grand jury was empaneled. The state rounded up the same cast of characters and pegged the late D’Artois as the ringleader in both crimes—directing men to take out Leslie because he was talking to law enforcement, and to murder Griffith because he seemed like he was about to squeal. But after reviewing the evidence, the grand jury issued an indictment that only brought charges in Griffith’s slaying. </p><p>In 1982, five people were put on trial: Clayton Kimble, Jules Ron Kimbel, Don Gardner, and two peripheral Dixie Mafia players, Kenneth Brouillette and Benny O’Quinn. They were charged with violating the Racketeer Influenced and Corrupt Organizations, or RICO, statute; conspiracy to violate RICO; conspiracy to violate Griffith’s civil rights by murdering him; and obstruction of justice. Steve Simoneaux, who was in prison at the time for a series of robberies of expensive watches in Dallas, took a plea deal and served as star witness; his testimony would reduce his potential life sentence to 20 years. After more than three weeks of proceedings, Kimble and Kimbel were convicted. Brouillete and O’Quinn were acquitted, and a mistrial was declared for Gardner; prosecutors said it would be too expensive to retry him.</p><p>Leslie’s murder was a point of discussion during the Griffith case, given how intertwined the two men’s killings seemed to be. And U.S. Attorney Don Beckner assured the press that his office, along with the FBI, was still pursuing indictments for Leslie’s death. But Beckner never did bring charges against anyone; he wasn’t able to build the case he’d hoped for.</p><p>That result was unsatisfying but perhaps predictable, given the lack of physical evidence and the dependence on unreliable narrators. When Kimble and Kimbel appealed the verdict in the Griffith case, a panel of three judges noted: “Simoneaux acknowledged that he was dishonorable and that he had repeatedly lied, including lies under oath, to save his own skin.” (They upheld the brothers’ convictions anyway.)</p><p>No one found the situation more unpalatable than Elliott Stonecipher. He grew up in Shreveport, in a tiny house in the wrong-side-of-the-tracks Caddo Heights neighborhood. A stellar student, he was also a skilled public speaker, and by the time he returned from college he was touted as the city’s future. “I was supposed to be the mayor of Shreveport,” Stonecipher says.</p><p>But he was never willing to traffic in the kind of machinations necessary to reach the mayor’s office. Instead, he worked as a pollster, public speaker, and political strategist. In the 2010s, Stonecipher launched an effort to finally untangle Leslie’s unsolved murder, thinking he could lean on his access to Shreveport’s levers of power. He dug through records in the National Archives and at Louisiana State University Shreveport, and attempted to track down people rumored to be involved. But in his telling, the wealthy and powerful didn’t want the matter revisited. Longtime friends suggested that he let it lie and chastised him for reopening old wounds. Potential clients said their boards of directors weren’t comfortable hiring him. “I was simply viewed as a turncoat,” he says. “And those are not tolerated by the wealthy leadership of Shreveport.”</p><p>Stonecipher has gone as far as he can with his search for answers. He’s eager for help. And in an interview, he makes a provocative claim: He believes that authorities identified the man who shot Leslie, and that it wasn’t Rusty Griffith. It was the guy from Dallas Simoneaux had once mentioned, a shadowy mob-connected assassin named Rick Roberts. Stonecipher has never been able to track him down—but maybe we can.</p><h2><strong>Murdering someone with a shotgun in the middle of a major city without being spotted seemed well beyond the Dixie Mafia’s skill set.</strong></h2><p>Fuller and I find of Rick Roberts is in the 1981 grand jury indictment. It’s odd, however. He was one of nine people named as conspirators in Griffith’s murder, but he was never charged, and he wasn’t included in the indictment’s description of the conspiracy. When it covered the indictment, the  ran a front-page grid with a photo or sketch of everyone named by the grand jury—except Roberts. We also find a story in the  about a taped interview in which Simoneaux told investigators that Roberts took part in planning Leslie’s murder, then served as the getaway driver—a role that hardly seemed to require a seasoned hit man.</p><p>We find more references to Roberts at the LSU Shreveport library. The archive has a robust section on George D’Artois, and at some point pages of testimony from the grand jury proceedings were deposited there. (By law that testimony is sealed, but someone apparently wanted it in public view.) The documents include testimony that put Roberts in Griffith’s orbit and at the scene of Leslie’s murder: A witness said Roberts worked at one of Griffith’s establishments, the Inside Out Club, but left for Dallas after Griffith unloaded it. Clayton Kimble recalled that the afternoon before the Leslie murder, various conspirators gathered at a Holiday Inn, where Griffith had a “buddy … which we found out later was Mr. Rick Roberts.” Kimble further claimed that he saw Roberts in the back seat of Griffith’s Cadillac, the supposed getaway car, shortly before the murder, and that Simoneaux told him after Leslie’s death that “Rick Roberts killed a man.” But Kimble also said that he never saw Roberts after murder. “I took for granted that he possibly drove the red and white [Cadillac] wherever it had to go,” Kimble testified, referring to the vehicle he’d previously placed Roberts in.</p><p>Roberts was called to testify before the grand jury too, but his testimony isn’t in the library’s files. There’s only a cover page and a second sheet noting where the proceedings took place and who was present.</p><p>Was it possible that Griffith, working on behalf of D’Artois, contracted Roberts to shoot Leslie? On the surface, the idea of a professional assassin made sense. Murdering someone with a shotgun in the middle of a major city without being spotted seemed well beyond the Dixie Mafia’s skill set. As Fuller and I zoomed out, questions mounted. How could Griffith have wedged a shotgun into a narrow space in a fence, fired, extracted the weapon, shimmied his 400-pound bulk out of the tight space between the fence and a loading dock, then driven away with Gardner—all within seconds? How could a car have sped away from the scene without the Prince Murat’s assistant manager, who was standing outside the hotel when the murder happened, seeing or hearing it?</p><p>And also: Where the hell was Rick Roberts?</p><p><strong>With questions about the Leslie murder </strong>rattling around in our heads,Fuller and I return to Wade in March 2022, curious to follow up on the richly embroidered account of Thompson’s life as a hired gun and career criminal in one of his fictionalized memoirs, titled  His character is a consummate professional: no drinking or drugs to impede clear and quick thinking. (“Never trust a drunk” was one of his mantras.) He is unfailingly polite, avoids conversation, and keeps his face and head covered, though he sometimes wears only a bandage on his cheek: If someone is asked to describe him later, that’s the detail they’ll focus on. By then he’ll have long since removed it.</p><p>Thompson tells us that, as a hit man, he adhered to a set of rigidly compartmentalized behaviors. His killings usually lacked any direct connection to him. He used pseudonyms and avoided anyone involved with a hit—including the person who hired him. He limited contact to an intermediary who shared his adherence to the Mafia’s code of silence. “Everybody that I’ve done business with all my life will tell you that I’m very secretive about what I do,” he says.</p><p>Fuller and I are eager to learn how this approach informed some of his most notorious hits, including the Maria Marshall murder, which Thompson breaks down for us in detail. But we also want to glean more about the Leslie murder. I tell Thompson that there’s something I want to ask him about from our first meeting. We noticed how readily he recalled the detail about the murder’s location.</p><p>“You answered so quickly,” I venture, “it sounded as if you were there.”</p><p>He smiles and says, “I probably was.”</p><p><strong>By the time we arrive at Wade</strong> for our third visit, Fuller and I have become fairly obsessed with the Leslie murder. A known hit man who seemingly admits to involvement in a notorious unsolved crime will do that. We’ve copied or photographed thousands of pages of files kept at the LSU Shreveport archive—old newspaper stories, police reports, transcripts, George D’Artois campaign materials—and pored over them. Fuller has plunged into his early FBI days, exploring possible links to one of his assets in New York City: a former New Orleans–based Marcello-family mobster. He has reconnected with former colleagues from the Shreveport office and cold-called law enforcement who were active in the seventies. We hope that Thompson can provide some additional intel.</p><p>We greet him by asking about his recent bout with COVID and how his job in the prison kitchen is going; during wild boar season, he tells us, local hunters sometimes drop off cuts of meat. After catching up, I hand him a summary of the East Baton Rouge Parish sheriff’s investigation of Leslie’s murder. I point out something that’s already obvious: Although the probe seemed exhaustive, Thompson’s name didn’t come up, which seems to undercut his claim that he was there that night.</p><p>It’s a gambit. Fuller and I have now talked to Thompson for a total of 15 or so hours. We have a pretty good idea of how he communicates. If we draw inferences or make intuitive leaps that are accurate, he’ll confirm them in his own way: “Maybe I was” or “I may have.” Our understanding of his code has allowed us to develop a kind of Larry Thompson operating manual. He doesn’t respond well to overly broad questions. “Tell me what happened that night” goes nowhere. We steer him onto a subject and then sit quietly, asking questions but also leaving conversational space to fill.</p><p>Thompson reads the investigation summary slowly, looking as if he’s just chewed tinfoil. “I don’t know where they get all this shit right here,” he says when he finishes.</p><p>I ask how he—or whoever was in the parking lot that night—knew when Leslie would return to the hotel. Thompson says that he had a spy, someone who reported back when Leslie had left the party. “I was by a phone,” he says, “waiting for that call.”</p><p>We nudge him on. Was the plan to shoot through the hotel fence, as police believed the killer did? Thompson shakes his head. He accurately recalls the makeup of the fence, but says, “I didn’t have to go through no fence.”</p><p>At this point the room ionizes. It’s happening, I think. Thompson is not only telling us that he was there, but he’s confessing, in his own elliptical way, to having pulled the trigger. Not wanting to break the spell, I resist the urge to look over at Fuller. Somewhere outside a door slams. The conversation unfolds with an eerie calm.</p><p>“You were not behind the fence?” I ask.</p><p>“So you were just in the parking lot?”</p><p>He nods. “Behind an automobile.”</p><p>He knew Leslie’s room number, he says, and “it don’t take a rocket scientist to figure out where he’s gonna park at.” When Leslie pulled in, Thompson tells us, he slipped from behind an adjacent vehicle and fired his gun.</p><p>He says he knew that the blast would draw attention, so he’d planned a non-getaway getaway: He’d checked into the Prince Murat under a pseudonym the previous day. After killing Leslie, he hustled straight through the back entrance of the hotel and headed to his room, where he stashed the shotgun under his bed. “How many of the police are gonna think the man who shot that man was in the hotel?” he says. At about 6 a.m., Thompson quietly got into a car he’d parked elsewhere in the lot, drove to Shreveport, and disposed of the gun.</p><p>We ask him: Why did he do it? Did D’Artois order the hit, as was widely believed?</p><p>Speaking deliberately, pausing for long intervals, Thompson says the hit was  connected to labor and thus to organized crime—no surprise, given the contentious Right to Work vote. Leslie was “talking to the wrong people,” Thompson says. “You didn’t talk to the wrong people back in the day, ’cause there was a lot of money floating around back then, and they didn’t mind spending it.”</p><p>Some of that money, Thompson says, took the form of bribes—specifically, his buddy Carlos Marcello had tried to influence the state senate’s vote. Leslie became aware of this and was sharing the sensitive information with people in the capital. Why take that risk? Thompson believes Leslie became so successful so fast that he felt untouchable.</p><p>“We used to call that too big for your britches,” Fuller says.</p><p>“That’s exactly what we got there,” Thompson replies. “He thought he was somebody he wasn’t.”</p><p>When Thompson is done telling his story, we sit quietly for a while. “You said quite a bit there,” I say.</p><p>Thompson gazes at us and nods. “I shouldn’t have,” he replies.</p><h2><strong>Scott almost seems impressed by the planning that went into the crime. “That has a lot of Larry Thompson in it right there,” he says. </strong></h2><p> when Fuller and I emerge from Wade Correctional. I feel both dazed and wired. We walk silently to the car, letting the enormity of what we’ve heard sink in. We climb into the rental and Fuller looks at me with raised eyebrows. “Well,” he says. I can see how energized he is. He’s not just back in the middle of an investigation; he finally has the chance to get to the bottom of something in Shreveport.</p><p>Is there a chance Thompson merely told us what we wanted to hear? It’s possible. He could be looking to burnish his legend or simply keep us entertained and coming back. Then again, Thompson has a full visitation list; he has no shortage of company. And his version of events feels utterly plausible: a known hit man pulling off a professional job, a crime no one was ever able to solve.</p><p>More than that, his story explained what  made sense as we dug into the police investigation—for instance, how, in the short time it took the hotel’s assistant manager to see Leslie’s body, the killer had supposedly maneuvered from behind the fence and vanished in a getaway car without being seen or heard. If Thompson was telling the truth, the answer was now as clear as it was obvious: He didn’t.</p><p>To check our instincts, we run the story past law enforcement who know Thompson. When we talk to Larry Scott, a former Shreveport cop acquainted with Thompson since the 1980s, he almost seems impressed by the planning that supposedly went into the crime. “That has a lot of Larry Thompson in it right there,” he says. Scott also believes that organized crime was likely responsible for the hit. “The union was 100 percent behind killing Jim Leslie,” he says. “They just let [the police] go after D’Artois.”</p><p>But how was it that Thompson was never linked to the Leslie murder? “My first thought is that Larry is very good at what he does,” Ford McWilliams, a former Caddo Parish prosecutor, tells us. “He killed all these people and robbed all these banks and night deposits, but he knew to keep his mouth shut. If he had a loose end, he tied it up, and he was just very professional. He didn’t make mistakes.”</p><p>Robert “Robbo” Davidson is a former chief detective for the DeSoto Parish Sheriff’s Department. He has met with Thompson many times in prison and investigated one of the cold cases Thompson confessed to in an effort to reduce his son’s prison time; arguably no one in law enforcement knows the hit man better. Davidson not only thinks Thompson’s description of the Leslie murder is believable, but he also finds it strange that Thompson’s name  come up during the investigation. “He was so close to D’Artois,” Robertson says. “I think the first person [D’Artois would] go to was Larry Thompson.” But the case was mostly built in Baton Rouge, not Shreveport, and before Thompson had achieved the peak of his notoriety. If his name did come up, it could have slipped through the cracks.</p><p>What about the motive—did the bribes Thompson mentioned actually occur? I look up people Leslie worked for; most of them are gone, but Anthony Guarisco, whom he helped get elected to the state senate, is still alive. When I search his name online, something astonishing pops up: Guarisco was mentioned at the end of an Associated Press story about Leslie’s murder. Citing a New Orleans newspaper, the piece says that Guarisco told federal authorities he declined an offer of $10,000 to help clear a campaign debt if he opposed the Right to Work Bill.</p><p>I connect with Guarisco via email, and he sends me a document titled “The Bribe.” It’s only an introduction to the story I want to know—a 326-word passage that begins with a detailed description of Leslie heading to work on the day of the vote. I reply by asking Guarisco to elaborate over the phone, but when I later mention that I’d like to loop in a retired FBI agent when we talk, he decides he has nothing to share about the Leslie case.</p><p>Fuller reaches out to Mike Barnett, a retired colonel from the East Baton Rouge Sheriff’s Department, which investigated the Leslie killing. Barnett agrees that Leslie was murdered because he inserted himself into an extremely charged political issue. If the mob couldn’t stop the Right to Work Bill with bribes, maybe a reminder of the cost of going against them would help their interests in the future. “I think they believed it was going to intimidate a lot of legislators,” Barnett says.</p><p>We are building consensus around the idea that Thompson might have pulled the trigger, but if that’s true, a key question remains: What about the elusive Rick Roberts? Was he involved?</p><p>Fuller, who once worked in the FBI’s Dallas office, canvasses his law-enforcement contacts throughout Louisiana and Texas: longtime members of sheriff’s offices, mob investigators, veteran agents, people who knew criminal heavyweights of all kinds. None had heard of a hit man named Rick Roberts.</p><p>Elliott Stonecipher says he was equally baffled when he tried to find him. “I tried every way I could to get a lead on Rick Roberts,” he says. “I couldn’t find cops that knew him, couldn’t find anybody.”</p><h2><strong>“Imagine that. Nobody’s ever seen him. Don’t know what he looks like, don’t know what he sounds like. And nobody’s ever met him.”</strong></h2><p><strong>When Warden Goodwin ushers</strong> us in for a fourth prison visit, Thompson seems glad to see us. A certain camaraderie has developed. After all those years as rivals, Fuller and Thompson have become, if not exactly friends, at least not  friends.</p><p>This time we’ve brought pointed questions about the Leslie case, based on details drawn from police and forensic reports. Even after so much time has passed, the hit man’s answers largely match what we know to be true. He estimates that he was 30 feet from Leslie when he fired the shotgun, echoing the distance cited in the initial police report (31 feet). The description of him slipping into his hotel room fits the timeline of how long it took the assistant manager to see Leslie’s body.</p><p>I have another question for Thompson, one that’s been percolating since our most recent conversation with Stonecipher, about the purported Dallas hit man. I remind Thompson that in his fictional memoirs, his character uses a pseudonym while working as a contract killer. It’s part of that character’s modus operandi to leave no trace. Then I ask: Did he ever use the name Rick Roberts?</p><p>Without hesitation he chuckles. “I don’t know, I might have,” he says.</p><p>We’ve learned to pay close attention to Thompson’s reactions. Fuller has occasionally thrown in questions about murders he knows Thompson didn’t commit. When Thompson wasn’t involved, he says so immediately. It’s like introducing a control into an experiment. Now his response to the question of Roberts’s identity feels unpremeditated.</p><p>“Rick?” Fuller says. “Can we call you Rick?”</p><p>Thompson laughs deeply. “That don’t mean I’d like to answer to it,” he says. “I would use a lot of names over the years, but nobody’d ever know who it really was.”</p><p>The admission is such that I have to force myself to stay plugged into the moment. But there’s something in the room that makes it easy. Maybe Thompson is energized by the chance to confess after all these years, and I’m feeling it—Fuller is, too. We are three people who have little in common but are emotionally joined, like the survivors of a plane crash or an infantry unit under fire.</p><p>We explain that we’ve done extensive research in archives and throughout the law-enforcement community for information about a hit man named Rick Roberts.</p><p>Thompson nods. “They can’t find him, can they?”</p><p>“Amazing how someone could just slip by like that.”</p><p>“Allegedly,” Fuller says, “he might be in Dallas.”</p><p>“Son of a bitch,” Thompson replies. “Imagine that. Nobody’s ever seen him. Don’t know what he looks like, don’t know what he sounds like. And nobody’s ever met him.”</p><p>Two things come to me. One is that Thompson was never considered a suspect in the investigation in part because none of the other men charged in connection with the crime ever mentioned his name. If Thompson is now telling the truth about using a pseudonym, that was by design. Keeping his identity a secret from the Dixie Mafia guys who knew Leslie was going to be murdered would have been as much a part of the planning as where to dump the gun afterward.</p><p>The second thought is a wisp of a memory, something Thompson told us the first time we visited, when Fuller initially asked about the murder. If we’d understood more then, we might have divined its meaning: , he told us then, <em>my name never came up in that deal</em>.</p><p> with the theory we’re working out in our interview: Investigators and prosecutors did talk to someone named Rick Roberts. We know this because of the bare-bones documents we found at the library, pointing to his grand jury testimony in 1981. Finding other evidence of his existence will require a trip west.</p><p>In early 2023, Fuller and I drive to the Fort Worth, Texas, branch of the National Archives, which holds transcripts and court filings from the 1982 trial. The staff brings out three boxes and we dig in; there are more than 10,000 pages. They’re divided into binders, each containing a single day’s testimony. Every binder begins with a table of contents naming the day’s witnesses. I churn through them all, scanning the contents. When I open the binder for June 9, 1982, my pulse thumps in my forehead. There he is, on page 58.</p><p>The transcript takes up a mere ten pages, double-spaced. It reveals little. Roberts knew Rusty Griffith; he’d worked at Griffith’s lounge in Shreveport; they’d talked about a development project in Costa Rica; before that, Roberts sold advertising for Holiday Inn. Roberts said that he only ever spoken to Don Gardner once in his life, and never met the four other defendants charged with conspiring to kill Griffith. When asked about the Leslie murder, Roberts replied, “The only thing I know is that he was killed, and that I have given testimony in front of two grand juries already concerning the fact that I knew nothing of that situation at all.”</p><p>“Have you ever been a hit man?” he was asked.</p><p>“Do you know what that term means?”</p><p>“I believe so. I don’t even own a gun.”</p><p>Prodded one last time for any knowledge of either killing, Roberts said, “I know nothing about anything. I was just a victim of circumstances, being at the wrong time in the wrong place.” And with that he was dismissed.</p><p>I show the pages to Fuller, my mind churning. Rick Roberts  exist, and if he played a role in Leslie’s killing, he seems to have artfully avoided any blame. Does that mean Thompson was lying about using a pseudonym? Could Thompson be protecting Roberts? Or, more improbably, could Thompson have been interviewed by the police or appeared in front of a grand jury ?</p><p>Fortunately, Wade Correctional is our next stop.</p><h2><strong>Fuller and I sit, thunderstruck. This, it occurs to me, is literally how you get away with murder.</strong></h2><p><strong>This visit, we don’t spend </strong>much time on preliminaries. Once we’ve settled into the meeting room, I hand over the first page of the Roberts testimony. Thompson is unfazed; he says he sort of knew Roberts. We watch for a reaction, but Thompson reveals nothing. I begin to wonder if he even remembers our previous conversation.</p><p>We let it slide for the moment and move on to other subjects, but later I return to the topic. Since Roberts turned out to be a real person, maybe there was some kind of miscommunication before, I suggest. Is Thompson certain he used the name as an alias? “Yeah, I’ve used it,” he says.</p><p>“So it’s just a coincidence that this guy’s name is also Rick Roberts?”</p><p>Thompson allows one of his trademark long pauses. “I guess,” he finally says. “I used a lot of names in different places. I can walk into a restaurant up there in Arkansas and they’ll call me Mr. Morrow.”</p><p>But, Fuller says, “You wouldn’t appear before a grand jury and raise your right hand and say your name is Rick Roberts and ‘I live at XYZ address,’ would you?”</p><p>Perusing the testimony again, Thompson finds a way to answer. “I’ll tell you one thing, I never worked for Rusty Griffith.”</p><p>As we mull over the riddle, Thompson grins, head tilted back. “When he smiles like that,” Fuller says to me, “there’s more behind the story.”</p><p>Then it strikes me: What if Thompson used Roberts’s name on the Leslie job because he wanted their identities to blend? What if the  was to be confused with Rick Roberts, to deflect attention in the ensuing investigation?</p><p>When I put these questions to Thompson, he nods. “That could be what happened.”</p><p>The genius of this washes over me. If the police were given the name of a nonexistent person, that would raise suspicion. So Thompson could have used an alias that didn’t require him to invent a new identity. He could have borrowed the name of a real person—someone he knew would be quickly dismissed as a suspect, and whose name was commonplace to boot. When I advance this idea, Thompson, arms folded, says: “Might’ve happened just like that.”</p><p>Fuller and I sit, thunderstruck. This, it occurs to me, is literally how you get away with murder.</p><p>Fuller and I then try to get a deeper understanding of why the hit was ordered. Thompson claims that Leslie was given information by someone associated with Carlos Marcello about the mob’s attempts to infiltrate the state firefighters’ union. “Organized crime wasn’t controlling that union here [in Louisiana] at the time—not like they did up north—and they were trying to get that established here,” Thompson says.</p><p>These efforts may or may not have included the bribe attempts he mentioned earlier, like the one that former state senator Anthony Guarisco declined. But it wasn’t the bribes that led to Leslie’s demise; it was his loose talk about them. Thompson tells us that Leslie scheduled a meeting with law enforcement to discuss the payoffs on July 9, only hours after he was murdered. “You can’t do what he was doing and get away with it, not back then,” Thompson says. “Or do what he was doing to the person he tried to do it to.”</p><p>Leslie’s stature made him a real threat. “He had the power of the media,” Fuller says.</p><p>“Right, that was his power right there,” Thompson replies. “He had a lot of ears, and he was using some of those ears in the wrong way, and it was getting back to the right people.”</p><p>We later confirm a key element of this account: According to an investigative series in Baton Rouge’s, Leslie was due in a meeting with U.S. Attorney Douglas Gonzales Sr. the morning after he was killed. The timing of the hit worked in another way, too: By waiting until after the Right to Work vote, Marcello’s people could pin the hit on D’Artois—who was in ever deeper legal trouble.</p><p>Outside the prison, Fuller and I linger in the humid air for a while, somewhat dazed and in disbelief. It took five trips to Louisiana and nearly 40 hours of conversation. But after three years circling a case that had gone ice cold—47 years after Jim Leslie was shot dead—everything seems to fit together.</p><p> to Shreveport, in March 2025, a source tells us that Louisiana has created a cold-case task force to try to pin Jim Leslie’s murder on Larry Thompson. The unit’s existence promises something we weren’t sure we’d obtain on our own: closure.</p><p>It’s unlikely that filing charges will accomplish more than that. Wade’s new warden, Michele Dauzat, tells me that Thompson, now 81, has been moved to another prison. He isn’t responding to our communications like he used to. When Thompson was at Wade, he answered emails within hours; now he doesn’t reply for days, if at all. Extending his sentence would be a symbolic gesture, given the term he’s currently serving, his advanced age, and some kidney-related health problems he mentioned to us. He might agree to a deal to shorten his son’s prison term, but it’s unclear whether he’d be offered one.</p><p>Still, there are plenty of people that resolution would mean a lot to—most prominently Elliott Stonecipher. “Everything that happened to Jim is insane,” he says. “Everything organized labor got away with, not to mention organized crime, is insane.”</p><p>Stonecipher has told us countless hours’ worth of stories over the years about the ways Shreveport broke his heart. The racial schisms. The corruption. But one is particularly personal: Jim Leslie and George D’Artois were both buried in the city’s venerable Forest Park Cemetery, less than a year apart. It bothered Stonecipher that the crowd for D’Artois’s funeral rivaled that of Leslie’s. “They couldn’t get in all of the cars that wanted to be in the funeral,” he recalls. “And Forest Park’s road goes on forever.”</p><p>For Stonecipher, it was a reminder that Shreveport had learned nothing from the senseless tragedy of Leslie’s death. “I never got over that, and frankly, it changed everything that I ever felt about my hometown,” Stonecipher says. “It never came back.”</p><p>As for Myron Fuller, Shreveport was the one place during his 31 years in the FBI where he struggled to find a gear. Clandestine forces conspired against him, investigations blinked out, people died. He came back looking for a reckoning. He needed to know it wasn’t all on him.</p><p>He comes away not only with a deeper understanding of his time in Shreveport, but with something more: He may have cracked open one of the most famous unsolved murders in Louisiana history. “When you’re in the middle of the forest, you can’t see the trees,” he says. “When you get out of there and look down from a bird’s-eye view? Holy shit. So that’s what this has done for me.”</p><p>Our last night in Shreveport, we have no plan. We drive for a bit, get hungry, and spontaneously land at Ernest’s Orleans, a place long known as an organized-crime hangout. It still has an unreconstructed mafioso vibe, as if it were lifted straight from a 1970s Scorsese film. A guy wearing a gigantic cowboy hat sits at a table near ours with an ostentatiously dressed woman. The waiter, a portly, uniformed man who also seems from another era, confirms the mob history: Yes, Carlos Marcello came here.</p><p>Fuller orders a whiskey on the rocks, looks around. When he talks about Shreveport, the edge is gone. The ghosts have vanished. But only for him. For the city, they remain.</p>","contentLength":45194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434245"},{"title":"Feasibility study of a mission to Sedna – Nuclear propulsion and solar sailing","url":"https://arxiv.org/abs/2506.17732","date":1751378891,"author":"speckx","guid":179071,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44434062"},{"title":"Fei-Fei Li: Spatial intelligence is the next frontier in AI [video]","url":"https://www.youtube.com/watch?v=_PioN-CpOP0","date":1751378433,"author":"sandslash","guid":182818,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433996"},{"title":"Grammarly acquires Superhuman","url":"https://www.reuters.com/business/grammarly-acquires-email-startup-superhuman-ai-platform-push-2025-07-01/","date":1751378430,"author":"thm","guid":179011,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433994"},{"title":"Converting a large mathematical software package written in C++ to C++20 modules","url":"https://arxiv.org/abs/2506.21654","date":1751377616,"author":"vblanco","guid":179305,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433899"},{"title":"Show HN: I built the tool I wished existed for moving Stripe between countries","url":"https://www.stripemove.com/","date":1751374370,"author":"felphos","guid":179224,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433429"},{"title":"Show HN: Spegel, a Terminal Browser That Uses LLMs to Rewrite Webpages","url":"https://simedw.com/2025/06/23/introducing-spegel/","date":1751374182,"author":"simedw","guid":178963,"unread":true,"content":"<p>TL;DR Spegel is a proof-of-concept terminal web browser that feeds HTML through an LLM and renders the result as markdown directly in your terminal.</p><p>Two weekends ago, after my family had gone to sleep, I found myself unsupervised with a laptop and an itch to build something interesting. A couple of hours later, I had a minimal web browser running in my terminal (no JavaScript, GET requests only) that transformed web content based on my custom prompts.</p><p>Then, a few days later, Google released Gemini 2.5 Pro Lite, significantly faster inference speed, suddenly my little weekend hack became a tad more practical.</p><p>Adapting content to suit individual needs isn’t a new idea, think about translating books or summarising lengthy articles. However, this used to be slow and expensive. LLMs have changed this dramatically, making these transformations quick and easy.</p><p>Spegel (\"mirror\" in Swedish) lets you explore web content through personalized views using your own prompts. A single page can have multiple views, maybe one simplifying everything down to ELI5 or another highlighting key actions. It's entirely up to you and your prompting skills. </p><p>Sometimes you don't want to read through someone's life story just to get to a recipe.\n<img alt=\"Recipe Example\" src=\"https://simedw.com/2025/06/23/introducing-spegel/images/recipe_example.png\"><small>A previous version of this screenshot showed an incorrect recipe on the right. That was due to a bug where large websites got truncated. Thanks to everyone who pointed it out!</small></p><div><pre><code></code></pre></div><p>The pipeline is straightforward.</p><p>Spegel fetches HTML content, processes it through an LLM using prompts stored in a config file (~/.spegel.toml), and outputs markdown rendered via Textual. Prompts and views can be adjusted live during a browsing session.</p><p>This was my first experience using Textual for a TUI, and it's been delightful, possibly too delightful, as I found myself adding a few unnecessary interface elements just because it was easy.</p><p>One gotcha was ensuring only completed lines (ending in newline characters) were streamed; otherwise, the markdown renderer would parse incomplete markdown and fail to recover formatting</p><div><pre><code></code></pre></div><p>There are a lot of great terminal browsers out there, Lynx and Links2 are close to my heart. There are also modern attempts like Browsh that can even render graphs using half-block Unicode characters (▄█). </p><p>Spegel isn’t meant to replace these, it’s more of an exploration or proof-of-concept. It currently doesn't support POST requests (though I have some ideas on handling  elements by creating on-the-fly UIs).</p><p>But most modern websites aren’t designed with terminal browsing in mind. They rely on CSS and JS, making them cumbersome in small terminal windows, full of clutter and noise. Spegel tries to clear away distractions, providing content tailored more closely to your needs.</p><p>Spegel is still in the early stages, so expect some rough edges, but it’s usable and kind of fun to play with.</p><p>Then just run it with a URL:</p><div><pre><code>spegelsimedw.com</code></pre></div><p>Don't forget to configure your own , (<a href=\"https://github.com/simedw/spegel/blob/main/example_config.toml\">example</a>)</p><p>Want to check out the source or contribute? It’s all on GitHub:</p>","contentLength":2997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433409"},{"title":"Show HN: Jobs by Referral: Find jobs in your LinkedIn network","url":"https://jobsbyreferral.com/","date":1751374026,"author":"nicksergeant","guid":179223,"unread":true,"content":"<div><p data-slot=\"text\">JobsByReferral analyzes your professional network to find job openings at companies where you have connections. Get referred by people you already know and dramatically increase your chances of landing interviews.</p></div>","contentLength":213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44433386"},{"title":"ASCIIMoon: The moon's phase live in ASCII art","url":"https://asciimoon.com/","date":1751367201,"author":"zayat","guid":181353,"unread":true,"content":"<div data-astro-cid-z5apybui=\"\"><p data-astro-cid-z5apybui=\"\">\nView &amp; cycle through The Moon's phases day to day - rendered in ASCII\n        art.\n</p></div><section data-astro-cid-z5apybui=\"\"></section>","contentLength":84,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432571"},{"title":"Scientists identify culprit behind biggest-ever U.S. honey bee die-off","url":"https://www.science.org/content/article/scientists-identify-culprit-behind-biggest-ever-u-s-honeybee-die","date":1751366123,"author":"pseudolus","guid":179010,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432467"},{"title":"Cloudflare to introduce pay-per-crawl for AI bots","url":"https://blog.cloudflare.com/introducing-pay-per-crawl/","date":1751365227,"author":"scotchmi_st","guid":178870,"unread":true,"content":"<div><h2>A changing landscape of consumption&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#a-changing-landscape-of-consumption\" aria-hidden=\"true\"></a></div><p>Many publishers, content creators and website owners currently feel like they have a binary choice — either leave the front door wide open for AI to consume everything they create, or create their own walled garden. But what if there was another way?</p><p>At Cloudflare, we started from a simple principle: we wanted content creators to have control over who accesses their work. If a creator wants to block all AI crawlers from their content, they should be able to do so. If a creator wants to allow some or all AI crawlers full access to their content for free, they should be able to do that, too. Creators should be in the driver’s seat.</p><p>After hundreds of conversations with news organizations, publishers, and large-scale social media platforms, we heard a consistent desire for a third path: They’d like to allow AI crawlers to access their content, but they’d like to get compensated. Currently, that requires knowing the right individual and striking a one-off deal, which is an insurmountable challenge if you don’t have scale and leverage.&nbsp;</p><div><h2>What if I could charge a crawler?&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#what-if-i-could-charge-a-crawler\" aria-hidden=\"true\"></a></div><p>We believe your choice need not be binary — there should be a third, more nuanced option: <b>You can charge for access.</b> Instead of a blanket block or uncompensated open access, we want to empower content owners to monetize their content at Internet scale.</p><div><h2>Introducing pay per crawl</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#introducing-pay-per-crawl\" aria-hidden=\"true\"></a></div><p><a href=\"https://www.cloudflare.com/paypercrawl-signup/\">Pay per crawl</a>, in private beta, is our first experiment in this area.&nbsp;</p><p>Pay per crawl integrates with existing web infrastructure, leveraging HTTP status codes and established authentication mechanisms to create a framework for paid content access.&nbsp;</p><p>Each time an AI crawler requests content, they either present payment intent via request headers for successful access (<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Status/200\"></a>), or receive a  response with pricing. Cloudflare acts as the Merchant of Record for pay per crawl and also provides the underlying technical infrastructure.</p><div><h3>Publisher controls and pricing</h3><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#publisher-controls-and-pricing\" aria-hidden=\"true\"></a></div><p>Pay per crawl grants domain owners full control over their monetization strategy. They can define a flat, per-request price across their entire site. Publishers will then have three distinct options for a crawler:</p><ul><li><p> Grant the crawler free access to content.</p></li><li><p> Require payment at the configured, domain-wide price.</p></li><li><p> Deny access entirely, with no option to pay.</p></li></ul><p>An important mechanism here is that even if a crawler doesn’t have a billing relationship with Cloudflare, and thus couldn’t be charged for access, a publisher can still choose to ‘charge’ them. This is the functional equivalent of a network level block (an HTTP  response where no content is returned) — but with the added benefit of telling the crawler there could be a relationship in the future.&nbsp;</p><p>While publishers currently can define a flat price across their entire site, they retain the flexibility to bypass charges for specific crawlers as needed. This is particularly helpful if you want to allow a certain crawler through for free, or if you want to negotiate and execute a content partnership outside the pay per crawl feature.&nbsp;</p><p>To ensure integration with each publisher’s existing security posture, Cloudflare enforces Allow or Charge decisions via a rules engine that operates only after existing WAF policies and bot management or bot blocking features have been applied.</p><div><h3>Payment headers and access</h3><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#payment-headers-and-access\" aria-hidden=\"true\"></a></div><p>As we were building the system, we knew we had to solve an incredibly important technical challenge: ensuring we could charge a specific crawler, but prevent anyone from spoofing that crawler. Thankfully, there’s a way to do this using <a href=\"https://developers.cloudflare.com/bots/concepts/bot/verified-bots/web-bot-auth/\"></a> proposals.</p><ul><li><p>Generating an Ed25519 key pair, and making the <a href=\"https://datatracker.ietf.org/doc/html/rfc7517\"></a>-formatted public key available in a hosted directory</p></li><li><p>Registering with Cloudflare to provide the URL of your key directory and user agent information.</p></li></ul><p>Once registration is accepted, crawler requests should always include , , and  headers to identify your crawler and discover paid resources.</p><pre><code>GET /example.html\nSignature-Agent: \"https://signature-agent.example.com\"\nSignature-Input: sig2=(\"@authority\" \"signature-agent\")\n ;created=1735689600\n ;keyid=\"poqkLGiymh_W0uP6PZFw-dvez3QJT5SolqXBCW38r0U\"\n ;alg=\"ed25519\"\n ;expires=1735693200\n;nonce=\"e8N7S2MFd/qrd6T2R3tdfAuuANngKI7LFtKYI/vowzk4lAZYadIX6wW25MwG7DCT9RUKAJ0qVkU0mEeLElW1qg==\"\n ;tag=\"web-bot-auth\"\nSignature: sig2=:jdq0SqOwHdyHr9+r5jw3iYZH6aNGKijYp/EstF4RQTQdi5N5YYKrD+mCT1HA1nZDsi6nJKuHxUi/5Syp3rLWBA==:</code></pre><p>Once a crawler is set up, determination of whether content requires payment can happen via two flows:</p><h4>Reactive (discovery-first)</h4><p>Should a crawler request a paid URL, Cloudflare returns an <code>HTTP 402 Payment Required</code> response, accompanied by a  header. This signals that payment is required for the requested resource.</p><pre><code>HTTP 402 Payment Required\ncrawler-price: USD XX.XX</code></pre><p>&nbsp;The crawler can then decide to retry the request, this time including a  header to indicate agreement to pay the configured price.</p><pre><code>GET /example.html\ncrawler-exact-price: USD XX.XX </code></pre><p>Alternatively, a crawler can preemptively include a  header in its initial request.</p><pre><code>GET /example.html\ncrawler-max-price: USD XX.XX</code></pre><p>If the price configured for a resource is equal to or below this specified limit, the request proceeds, and the content is served with a successful  response, confirming the charge:</p><pre><code>HTTP 200 OK\ncrawler-charged: USD XX.XX \nserver: cloudflare</code></pre><p>If the amount in a  request is greater than the content owner’s configured price, only the configured price is charged. However, if the resource’s configured price exceeds the maximum price offered by the crawler, an  response is returned, indicating the specified cost. &nbsp;Only a single price declaration header,  or , may be used per request.</p><p>The  or  headers explicitly declare the crawler's willingness to pay. If all checks pass, the content is served, and the crawl event is logged. If any aspect of the request is invalid, the edge returns an <code>HTTP 402 Payment Required</code> response.</p><p>Crawler operators and content owners must configure pay per crawl payment details in their Cloudflare account. Billing events are recorded each time a crawler makes an authenticated request with payment intent and receives an HTTP 200-level response with a  header. Cloudflare then aggregates all the events, charges the crawler, and distributes the earnings to the publisher.</p><div><h2>Content for crawlers today, agents tomorrow&nbsp;</h2><a href=\"https://blog.cloudflare.com/introducing-pay-per-crawl/#content-for-crawlers-today-agents-tomorrow\" aria-hidden=\"true\"></a></div><p>At its core, pay per crawl begins a technical shift in how content is controlled online. By providing creators with a robust, programmatic mechanism for valuing and controlling their digital assets, we empower them to continue creating the rich, diverse content that makes the Internet invaluable.&nbsp;</p><p>We expect pay per crawl to evolve significantly. It’s very early: we believe many different types of interactions and marketplaces can and should develop simultaneously. We are excited to support these various efforts and open standards.</p><p>For example, a publisher or new organization might want to charge different rates for different paths or content types. How do you introduce dynamic pricing based not only upon demand, but also how many users your AI application has? How do you introduce granular licenses at internet scale, whether for training, inference, search, or something entirely new?</p><p>The true potential of pay per crawl may emerge in an agentic world. What if an agentic paywall could operate entirely programmatically? Imagine asking your favorite deep research program to help you synthesize the latest cancer research or a legal brief, or just help you find the best restaurant in Soho — and then giving that agent a budget to spend to acquire the best and most relevant content. By anchoring our first solution on , we enable a future where intelligent agents can programmatically negotiate access to digital resources.&nbsp;</p><p>Pay per crawl is currently in private beta. We’d love to hear from you if you’re either a crawler interested in paying to access content or a content creator interested in charging for access. You can reach out to us at <a href=\"https://www.cloudflare.com/paypercrawl-signup/\"><u>http://www.cloudflare.com/paypercrawl-signup/</u></a> or contact your Account Executive if you’re an existing Enterprise customer.</p>","contentLength":8099,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432385"},{"title":"Show HN: ToplingDB - A Persistent Key-Value Store for External Storage","url":"https://github.com/topling/toplingdb","date":1751364479,"author":"rockeetterark","guid":179511,"unread":true,"content":"<p>As the creator of TerarkDB (acquired by ByteDance in 2019), I have developed ToplingDB in recent years.</p><p>ToplingDB is forked from RocksDB,   where   we have replaced almost all components with more efficient alternatives(db_bench shows ToplingDB is about ~8x faster than RocksDB):</p><p>* MemTable: SkipList is replaced by CSPP(Crash Safe Parallel Patricia trie), which is 8x faster.</p><p>* SST: BlockBasedTable is replaced by ToplingZipTable, implemented by searchable compression algo, it is very small and fast, typically less than 1μs per lookup:</p><pre><code>  * Keys/Indexes are compressed   using NestLoudsTrie(a multi-layer nesting LOUDS succinct trie).\n\n  * Values in a SST are compressed   together with better zip ratio than zstd, and can unzip by a single value at 1GB/sec.\n\n  * BlockCache is no longer needed, double caching(BlockCache &amp; PageCache) is avoided\n</code></pre>\nOther hotspots are also improved:<p>* Flush MemTable to L0 is omited, greatly reducing write amp and is very friendly for large(GB) MemTable</p><pre><code>  * MemTable   serves as the index of Key to \"value position in WAL log\"\n\n  * Since WAL file content almost always in page cache, thus value content can be efficiently accessed by mmap\n\n  * When Flush happens, MemTable is dumpped as an SST and WAL is treated as a blob file\n\n    * CSPP MemTable use integer index instead of physical pointers, thus in-memory format is exactly same with in-file format\n</code></pre>\n* Prefix cache for searching candidate SSTs and prefix cache for scanning by iterators<pre><code>  * Caching fixed len key prefix into an array, binary search it as an uint array\n</code></pre>\n* Distributed compaction(superior replacement to rocksdb remote compaction)<pre><code>  * Gracefully support MergeOperator, CompactionFilter, PropertiesCollector...\n\n  * Out of the box, development efforts are significantly reduced\n\n  * Very easy to share compaction service on spot instances for many DB nodes\n</code></pre>\nUseful Bonus Feature:<p>* Config by json/yaml: can config almost all features</p><p>* Optional embeded WebView: show db structures in web browser, refreshing pages like animation</p><p>* Online update db configs by http</p><p>MySQL integration, ToplingDB has integrated into MySQL by MyTopling, which is forked from MyRocks with great improvements, like improvements of ToplingDB on RocksDB:</p><p>* WBWI(WriteBatchWithIndex): like MemTable, SkipList is replace with CSPP, 20x faster(speedup is more than MemTable).</p><p>* LockManager &amp; LockTracker: 10x faster</p><p>* Encoding &amp; Decoding: 5x faster</p><p>MyRocks has many disadvantages compared to InnoDB, while MyTopling outperforms InnoDB at almost all aspect - excluding feature differences.</p><p>We have create ~100 PRs for RocksDB, in which ~40 were accepted. Our PRs are mostly \"small\" changes, since big changes are not likely accepted.</p><p>ToplingDB has been deployed in numerous production environments.</p>","contentLength":2756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44432322"},{"title":"Evidence of a 12,800-year-old shallow airburst depression in Louisiana","url":"https://www.scienceopen.com/hosted-document?doi=10.14293/ACI.2025.0004","date":1751356553,"author":"keepamovin","guid":181698,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44431638"},{"title":"OpenFLOW – Quickly make beautiful infrastructure diagrams local to your machine","url":"https://github.com/stan-smith/OpenFLOW","date":1751351397,"author":"x0z","guid":178962,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44431178"},{"title":"About AI Evals","url":"https://hamel.dev/blog/posts/evals-faq/","date":1751338096,"author":"TheIronYuppie","guid":183207,"unread":true,"content":"<p>This post curates the most common questions Shreya and I have recieved from <a href=\"https://bit.ly/evals-ai\" target=\"_blank\">teaching 700+</a> engineers &amp; PMs in AI Evals. <em>Warning: These are sharp opinions about what works in most cases. They are not universal truths. Use your judgment.</em></p><div><p><em> (we have to get back to building). Here is a <a href=\"https://bit.ly/evals-ai\" target=\"_blank\">35% discount code</a> for readers of this post.</em> 👈</p></div><section><p>Question: Should I avoid using RAG for my AI application after reading that <a href=\"https://pashpashpash.substack.com/p/why-i-no-longer-recommend-rag-for\">“RAG is dead”</a> for coding agents?</p><blockquote><p>Many developers are confused about when and how to use RAG after reading articles claiming “RAG is dead.” Understanding what RAG actually means versus the narrow marketing definitions will help you make better architectural decisions for your AI applications.</p></blockquote><p>The viral article claiming RAG is dead specifically argues against using <em>naive vector database retrieval</em> for autonomous coding agents, not RAG as a whole. This is a crucial distinction that many developers miss due to misleading marketing.</p><p>RAG simply means Retrieval-Augmented Generation - using retrieval to provide relevant context that improves your model’s output. The core principle remains essential: your LLM needs the right context to generate accurate answers. The question isn’t whether to use retrieval, but how to retrieve effectively.</p><p>For coding applications, naive vector similarity search often fails because code relationships are complex and contextual. Instead of abandoning retrieval entirely, modern coding assistants like Claude Code <a href=\"https://x.com/pashmerepat/status/1926717705660375463?s=46\">still uses retrieval</a> —they just employ agentic search instead of relying solely on vector databases.similar to how human developers work.</p><p>You have multiple retrieval strategies available, ranging from simple keyword matching to embedding similarity to LLM-powered relevance filtering. The optimal approach depends on your specific use case, data characteristics, and performance requirements. Many production systems combine multiple strategies or use multi-hop retrieval guided by LLM agents.</p><p>Unforunately, “RAG” has become a buzzword with no shared definition. Some people use it to mean any retrieval system, others restrict it to vector databases. Focus on the fundamental goal: getting your LLM the context it needs to succeed. Whether that’s through vector search, agentic exploration, or hybrid approaches is a product and engineering decision that requires understanding your users’ failure modes and usage patterns.</p><p>Rather than following categorical advice to avoid or embrace RAG, experiment with different retrieval approaches and measure what works best for your application.</p></section><section><h2 data-anchor-id=\"q-can-i-use-the-same-model-for-both-the-main-task-and-evaluation\">Q: Can I use the same model for both the main task and evaluation?</h2><p>For LLM-as-Judge selection, using the same model is usually fine because the judge is doing a different task than your main LLM pipeline. The judges we recommend building do <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales\">scoped binary classification tasks</a>. Focus on achieving high True Positive Rate (TPR) and True Negative Rate (TNR) with your judge on a held out labeled test set rather than avoiding the same model family. You can use these metrics on the test set to understand how well your judge is doing.</p><p>When selecting judge models, start with the most capable models available to establish strong alignment with human judgments. You can optimize for cost later once you’ve established reliable evaluation criteria. We do not recommend using the same model for open ended preferences or response quality (but we don’t recommend building judges this way in the first place!).</p></section><section><h2 data-anchor-id=\"q-how-much-time-should-i-spend-on-model-selection\">Q: How much time should I spend on model selection?</h2><p>Many developers fixate on model selection as the primary way to improve their LLM applications. Start with error analysis to understand your failure modes before considering model switching. As Hamel noted in office hours, “I suggest not thinking of switching model as the main axes of how to improve your system off the bat without evidence. Does error analysis suggest that your model is the problem?”</p></section><section><h2 data-anchor-id=\"q-why-do-you-recommend-binary-passfail-evaluations-instead-of-1-5-ratings-likert-scales\">Q: Why do you recommend binary (pass/fail) evaluations instead of 1-5 ratings (Likert scales)?</h2><blockquote><p>Engineers often believe that Likert scales (1-5 ratings) provide more information than binary evaluations, allowing them to track gradual improvements. However, this added complexity often creates more problems than it solves in practice.</p></blockquote><p>Binary evaluations force clearer thinking and more consistent labeling. Likert scales introduce significant challenges: the difference between adjacent points (like 3 vs 4) is subjective and inconsistent across annotators, detecting statistical differences requires larger sample sizes, and annotators often default to middle values to avoid making hard decisions.</p><p>Having binary options forces people to make a decision rather than hiding uncertainty in middle values. Binary decisions are also faster to make during error analysis - you don’t waste time debating whether something is a 3 or 4.</p><p>For tracking gradual improvements, consider measuring specific sub-components with their own binary checks rather than using a scale. For example, instead of rating factual accuracy 1-5, you could track “4 out of 5 expected facts included” as separate binary checks. This preserves the ability to measure progress while maintaining clear, objective criteria.</p><p>Start with binary labels to understand what ‘bad’ looks like. Numeric labels are advanced and usually not necessary.</p></section><section><h2 data-anchor-id=\"q-how-do-i-debug-multi-turn-conversation-traces\">Q: How do I debug multi-turn conversation traces?</h2><p>Start simple. Check if the whole conversation met the user’s goal with a pass/fail judgment. Look at the entire trace and focus on the first upstream failure. Read the user-visible parts first to understand if something went wrong. Only then dig into the technical details like tool calls and intermediate steps.</p><p>When you find a failure, reproduce it with the simplest possible test case. Here’s a example: suppose a shopping bot gives the wrong return policy on turn 4 of a conversation. Before diving into the full multi-turn complexity, simplify it to a single turn: “What is the return window for product X1000?” If it still fails, you’ve proven the error isn’t about conversation context - it’s likely a basic retrieval or knowledge issue you can debug more easily.</p><p>For generating test cases, you have two main approaches. First, you can simulate users with another LLM to create realistic multi-turn conversations. Second, use “N-1 testing” where you provide the first N-1 turns of a real conversation and test what happens next. The N-1 approach often works better since it uses actual conversation prefixes rather than fully synthetic interactions (but is less flexible and doesn’t test the full conversation). User simulation is getting better as models improve. Keep an eye on this space.</p><p>The key is balancing thoroughness with efficiency. Not every multi-turn failure requires multi-turn analysis.</p></section><section><h2 data-anchor-id=\"q-should-i-build-automated-evaluators-for-every-failure-mode-i-find\">Q: Should I build automated evaluators for every failure mode I find?</h2><p>Focus automated evaluators on failures that persist after fixing your prompts. Many teams discover their LLM doesn’t meet preferences they never actually specified - like wanting short responses, specific formatting, or step-by-step reasoning. Fix these obvious gaps first before building complex evaluation infrastructure.</p><p>Consider the cost hierarchy of different evaluator types. Simple assertions and reference-based checks (comparing against known correct answers) are cheap to build and maintain. LLM-as-Judge evaluators require 100+ labeled examples, ongoing weekly maintenance, and coordination between developers, PMs, and domain experts. This cost difference should shape your evaluation strategy.</p><p>Only build expensive evaluators for problems you’ll iterate on repeatedly. Since LLM-as-Judge comes with significant overhead, save it for persistent generalization failures - not issues you can fix trivially. Start with cheap code-based checks where possible: regex patterns, structural validation, or execution tests. Reserve complex evaluation for subjective qualities that can’t be captured by simple rules.</p></section><section><h2 data-anchor-id=\"q-how-many-people-should-annotate-my-llm-outputs\">Q: How many people should annotate my LLM outputs?</h2><p>For most small to medium-sized companies, appointing a single domain expert as a “benevolent dictator” is the most effective approach. This person—whether it’s a psychologist for a mental health chatbot, a lawyer for legal document analysis, or a customer service director for support automation—becomes the definitive voice on quality standards.</p><p>A single expert eliminates annotation conflicts and prevents the paralysis that comes from “too many cooks in the kitchen”. The benevolent dictator can incorporate input and feedback from others, but they drive the process. If you feel like you need five subject matter experts to judge a single interaction, it’s a sign your product scope might be too broad.</p><p>However, larger organizations or those operating across multiple domains (like a multinational company with different cultural contexts) may need multiple annotators. When you do use multiple people, you’ll need to measure their agreement using metrics like Cohen’s Kappa, which accounts for agreement beyond chance. However, use your judgment. Even in larger companies, a single expert is often enough.</p><p>Start with a benevolent dictator whenever feasible. Only add complexity when your domain demands it.</p></section><section><h2 data-anchor-id=\"q-what-gaps-in-eval-tooling-should-i-be-prepared-to-fill-myself\">Q: What gaps in eval tooling should I be prepared to fill myself?</h2><p>Most eval tools handle the basics well: logging complete traces, tracking metrics, prompt playgrounds, and annotation queues. These are table stakes. Here are four areas where you’ll likely need to supplement existing tools.</p><p>Watch for vendors addressing these gaps—it’s a strong signal they understand practitioner needs.</p><section><h4 data-anchor-id=\"error-analysis-and-pattern-discovery\">1. Error Analysis and Pattern Discovery</h4><p>After reviewing traces where your AI fails, can your tooling automatically cluster similar issues? For instance, if multiple traces show the assistant using casual language for luxury clients, you need something that recognizes this broader “persona-tone mismatch” pattern. We recommend building capabilities that use AI to suggest groupings, rewrite your observations into clearer failure taxonomies, help find similar cases through semantic search, etc.</p></section><section><h4 data-anchor-id=\"ai-powered-assistance-throughout-the-workflow\">2. AI-Powered Assistance Throughout the Workflow</h4><p>The most effective workflows use AI to accelerate every stage of evaluation. During error analysis, you want an LLM helping categorize your open-ended observations into coherent failure modes. For example, you might annotate several traces with notes like “wrong tone for investor,” “too casual for luxury buyer,” etc. Your tooling should recognize these as the same underlying pattern and suggest a unified “persona-tone mismatch” category.</p><p>You’ll also want AI assistance in proposing fixes. After identifying 20 cases where your assistant omits pet policies from property summaries, can your workflow analyze these failures and suggest specific prompt modifications? Can it draft refinements to your SQL generation instructions when it notices patterns of missing WHERE clauses?</p><p>Additionally, good workflows help you conduct data analysis of your annotations and traces. I like using notebooks with AI in-the-loop like <a href=\"https://julius.ai/\">Julius</a>,<a href=\"https://hex.tech\">Hex</a> or <a href=\"https://solveit.fast.ai/\">SolveIt</a>. These help me discover insights like “location ambiguity errors spike 3x when users mention neighborhood names” or “tone mismatches occur 80% more often in email generation than other modalities.”</p></section><section><h4 data-anchor-id=\"custom-evaluators-over-generic-metrics\">3. Custom Evaluators Over Generic Metrics</h4><p>Be prepared to build most of your evaluators from scratch. Generic metrics like “hallucination score” or “helpfulness rating” rarely capture what actually matters for your application—like proposing unavailable showing times or omitting budget constraints from emails. In our experience, successful teams spend most of their effort on application-specific metrics.</p></section><section><h4 data-anchor-id=\"apis-that-support-custom-annotation-apps\">4. APIs That Support Custom Annotation Apps</h4><p>Custom annotation interfaces <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf\">work best for most teams</a>. This requires observability platforms with thoughtful APIs. I often have to build my own libraries and abstractions just to make bulk data export manageable. You shouldn’t have to paginate through thousands of requests or handle timeout-prone endpoints just to get your data. Look for platforms that provide true bulk export capabilities and, crucially, APIs that let you write annotations back efficiently.</p></section></section><section><h2 data-anchor-id=\"q-what-is-the-best-approach-for-generating-synthetic-data\">Q: What is the best approach for generating synthetic data?</h2><p>A common mistake is prompting an LLM to  without structure, resulting in generic, repetitive outputs. A structured approach using dimensions produces far better synthetic data for testing LLM applications.</p><p><strong>Start by defining dimensions</strong>: categories that describe different aspects of user queries. Each dimension captures one type of variation in user behavior. For example:</p><ul><li>For a recipe app, dimensions might include Dietary Restriction (, , ), Cuisine Type (, , ), and Query Complexity (, , ).</li><li>For a customer support bot, dimensions could be Issue Type (, , ), Customer Mood (, , ), and Prior Context (, , ).</li></ul><p><strong>Choose dimensions that target likely failure modes.</strong> If you suspect your recipe app struggles with scaling ingredients for large groups or your support bot mishandles angry customers, make those dimensions. Use your application first—you need hypotheses about where failures occur. Without this, you’ll generate useless test data.</p><p><strong>Once you have dimensions, create tuples:</strong> specific combinations selecting one value from each dimension. A tuple like (, , ) represents a particular use case. Write 20 tuples manually to understand your problem space, then use an LLM to scale up.</p><p>The two-step generation process is important. First, have the LLM generate structured tuples. Then, in a separate prompt, convert each tuple to a natural language query. This separation prevents repetitive phrasing. For the vegan Italian tuple above, you might get <code>\"I need a dairy-free lasagna recipe that I can prep the day before.\"</code></p><p><strong>Don’t generate synthetic data for problems you can fix immediately.</strong> If your prompt never mentions handling dietary restrictions, fix the prompt rather than generating hundreds of specialized queries. Save synthetic data for complex issues requiring iteration—like an LLM consistently failing at ingredient scaling math or misinterpreting ambiguous requests.</p><p>After iterating on your tuples and prompts, <strong>run these synthetic queries through your actual system to capture full traces</strong>. Sample 100 traces for error analysis. This number provides enough traces to manually review and identify failure patterns without being overwhelming. Rather than generating thousands of similar queries, ensure your 100 traces cover diverse combinations across your dimensions—this variety will reveal more failure modes than sheer volume.</p></section><section><h2 data-anchor-id=\"q-how-do-i-approach-evaluation-when-my-system-handles-diverse-user-queries\">Q: How do I approach evaluation when my system handles diverse user queries?</h2><blockquote><p>Complex applications often support vastly different query patterns—from “What’s the return policy?” to “Compare pricing trends across regions for products matching these criteria.” Each query type exercises different system capabilities, leading to confusion on how to design eval criteria.</p></blockquote><p> Your evaluation strategy should emerge from observed failure patterns (e.g.&nbsp;error analysis), not predetermined query classifications. Rather than creating a massive evaluation matrix covering every query type you can imagine, let your system’s actual behavior guide where you invest evaluation effort.</p><p>During error analysis, you’ll likely discover that certain query categories share failure patterns. For instance, all queries requiring temporal reasoning might struggle regardless of whether they’re simple lookups or complex aggregations. Similarly, queries that need to combine information from multiple sources might fail in consistent ways. These patterns discovered through error analysis should drive your evaluation priorities. It could be that query category is a fine way to group failures, but you don’t know that until you’ve analyzed your data.</p><div><p><em> (we have to get back to building). Here is a <a href=\"https://bit.ly/evals-ai\">35% discount code</a> for readers of this post.</em> 👈</p></div></section><section><h2 data-anchor-id=\"q-how-do-i-choose-the-right-chunk-size-for-my-document-processing-tasks\">Q: How do I choose the right chunk size for my document processing tasks?</h2><p>Unlike RAG, where chunks are optimized for retrieval, document processing assumes the model will see every chunk. The goal is to split text so the model can reason effectively without being overwhelmed. Even if a document fits within the context window, it might be better to break it up. Long inputs can degrade performance due to attention bottlenecks, especially in the middle of the context. Two task types require different strategies:</p><section><h3 data-anchor-id=\"fixed-output-tasks-large-chunks\">1. Fixed-Output Tasks → Large Chunks</h3><p>These are tasks where the output length doesn’t grow with input: extracting a number, answering a specific question, classifying a section. For example:</p><ul><li>“What’s the penalty clause in this contract?”</li><li>“What was the CEO’s salary in 2023?”</li></ul><p>Use the largest chunk (with caveats) that likely contains the answer. This reduces the number of queries and avoids context fragmentation. However, avoid adding irrelevant text. Models are sensitive to distraction, especially with large inputs. The middle parts of a long input might be under-attended. Furthermore, if cost and latency are a bottleneck, you should consider preprocessing or filtering the document (via keyword search or a lightweight retriever) to isolate relevant sections before feeding a huge chunk.</p></section><section><h3 data-anchor-id=\"expansive-output-tasks-smaller-chunks\">2. Expansive-Output Tasks → Smaller Chunks</h3><p>These include summarization, exhaustive extraction, or any task where output grows with input. For example:</p><ul><li>“List all customer complaints”</li></ul><p>In these cases, smaller chunks help preserve reasoning quality and output completeness. The standard approach is to process each chunk independently, then aggregate results (e.g., map-reduce). When sizing your chunks, try to respect content boundaries like paragraphs, sections, or chapters. Chunking also helps mitigate output limits. By breaking the task into pieces, each piece’s output can stay within limits.</p></section><section><p>It’s important to recognize <strong>why chunk size affects results</strong>. A larger chunk means the model has to reason over more information in one go – essentially, a heavier cognitive load. LLMs have limited capacity to <strong>retain and correlate details across a long text</strong>. If too much is packed in, the model might prioritize certain parts (commonly the beginning or end) and overlook or “forget” details in the middle. This can lead to overly coarse summaries or missed facts. In contrast, a smaller chunk bounds the problem: the model can pay full attention to that section. You are trading off <strong>global context for local focus</strong>.</p><p>No rule of thumb can perfectly determine the best chunk size for your use case – <strong>you should validate with experiments</strong>. The optimal chunk size can vary by domain and model. I treat chunk size as a hyper parameter to tune.</p></section></section><section><h2 data-anchor-id=\"q-how-should-i-approach-evaluating-my-rag-system\">Q: How should I approach evaluating my RAG system?</h2><p>RAG systems have two distinct components that require different evaluation approaches: retrieval and generation.</p><p>The retrieval component is a search problem. Evaluate it using traditional information retrieval (IR) metrics. Common examples include Recall@k (of all relevant documents, how many did you retrieve in the top k?), Precision@k (of the k documents retrieved, how many were relevant?), or MRR (how high up was the first relevant document?). The specific metrics you choose depend on your use case. These metrics are pure search metrics that measure whether you’re finding the right documents (more on this below).</p><p>To evaluate retrieval, create a dataset of queries paired with their relevant documents. Generate this synthetically by taking documents from your corpus, extracting key facts, then generating questions those facts would answer. This reverse process gives you query-document pairs for measuring retrieval performance without manual annotation.</p><p>For the generation component—how well the LLM uses retrieved context, whether it hallucinates, whether it answers the question—use the same evaluation procedures covered throughout this course: error analysis to identify failure modes, collecting human labels, building LLM-as-judge evaluators, and validating those judges against human annotations.</p><p>Jason Liu’s <a href=\"https://jxnl.co/writing/2025/05/19/there-are-only-6-rag-evals/\">“There Are Only 6 RAG Evals”</a> provides a framework that maps well to this separation. His Tier 1 covers traditional IR metrics for retrieval. Tiers 2 and 3 evaluate relationships between Question, Context, and Answer—like whether the context is relevant (C|Q), whether the answer is faithful to context (A|C), and whether the answer addresses the question (A|Q).</p><p>In addition to Jason’s six evals, error analysis on your specific data may reveal domain-specific failure modes that warrant their own metrics. For example, a medical RAG system might consistently fail to distinguish between drug dosages for adults versus children, or a legal RAG might confuse jurisdictional boundaries. These patterns emerge only through systematic review of actual failures. Once identified, you can create targeted evaluators for these specific issues beyond the general framework.</p><p>Finally, when implementing Jason’s Tier 2 and 3 metrics, don’t just use prompts off the shelf. The standard LLM-as-judge process requires several steps: error analysis, prompt iteration, creating labeled examples, and measuring your judge’s accuracy against human labels. Once you know your judge’s True Positive and True Negative rates, you can correct its estimates to determine the actual failure rate in your system. Skip this validation and your judges may not reflect your actual quality criteria.</p><p>In summary, debug retrieval first using IR metrics, then tackle generation quality using properly validated LLM judges.</p></section><section><h2 data-anchor-id=\"q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs\">Q: What makes a good custom interface for reviewing LLM outputs?</h2><p>Great interfaces make human review fast, clear, and motivating. We recommend <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf\">building your own annotation tool</a> customized to your domain. The following features are possible enhancements we’ve seen work well, but you don’t need all of them. The screenshots shown are illustrative examples to clarify concepts. In practice, I rarely implement all these features in a single app. It’s ultimately a judgment call based on your specific needs and constraints.</p><p><strong>1. Render Traces Intelligently, Not Generically</strong>: Present the trace in a way that’s intuitive for the domain. If you’re evaluating generated emails, render them to look like emails. If the output is code, use syntax highlighting. Allow the reviewer to see the full trace (user input, tool calls, and LLM reasoning), but keep less important details in collapsed sections that can be expanded. Here is an example of a custom annotation tool for reviewing real estate assistant emails:</p><div><figure><figcaption>A custom interface for reviewing emails for a real estate assistant.</figcaption></figure></div><p><strong>2. Show Progress and Support Keyboard Navigation</strong>: Keep reviewers in a state of flow by minimizing friction and motivating completion. Include progress indicators (e.g., “Trace 45 of 100”) to keep the review session bounded and encourage completion. Enable hotkeys for navigating between traces (e.g., N for next), applying labels, and saving notes quickly. Below is an illustration of these features:</p><div><figure><figcaption>An annotation interface with a progress bar and hotkey guide</figcaption></figure></div><p><strong>4. Trace navigation through clustering, filtering, and search</strong>: Allow reviewers to filter traces by metadata or search by keywords. Semantic search helps find conceptually similar problems. Clustering similar traces (like grouping by user persona) lets reviewers spot recurring issues and explore hypotheses. Below is an illustration of these features:</p><div><figure><figcaption>Cluster view showing groups of emails, such as property-focused or client-focused examples. Reviewers can drill into a group to see individual traces.</figcaption></figure></div><p><strong>5. Prioritize labeling traces you think might be problematic</strong>: Surface traces flagged by guardrails, CI failures, or automated evaluators for review. Provide buttons to take actions like adding to datasets, filing bugs, or re-running pipeline tests. Display relevant context (pipeline version, eval scores, reviewer info) directly in the interface to minimize context switching. Below is an illustration of these ideas:</p><div><figure><figcaption>A trace view that allows you to quickly see auto-evaluator verdict, add traces to dataset or open issues. Also shows metadata like pipeline version, reviewer info, and more.</figcaption></figure></div><section><h3 data-anchor-id=\"general-principle-keep-it-minimal\">General Principle: Keep it minimal</h3><p>Keep your annotation interface minimal. Only incorporate these ideas if they provide a benefit that outweighs the additional complexity and maintenance overhead.</p></section></section><section><h2 data-anchor-id=\"q-how-much-of-my-development-budget-should-i-allocate-to-evals\">Q: How much of my development budget should I allocate to evals?</h2><p>It’s important to recognize that evaluation is part of the development process rather than a distinct line item, similar to how debugging is part of software development.</p><p>You should always be doing <a href=\"https://www.youtube.com/watch?v=qH1dZ8JLLdU\">error analysis</a>. When you discover issues through error analysis, many will be straightforward bugs you’ll fix immediately. These fixes don’t require separate evaluation infrastructure as they’re just part of development.</p><p>The decision to build automated evaluators comes down to <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-should-i-build-automated-evaluators-for-every-failure-mode-i-find\">cost-benefit analysis</a>. If you can catch an error with a simple assertion or regex check, the cost is minimal and probably worth it. But if you need to align an LLM-as-judge evaluator, consider whether the failure mode warrants that investment.</p><p>In the projects we’ve worked on, <strong>we’ve spent 60-80% of our development time on error analysis and evaluation</strong>. Expect most of your effort to go toward understanding failures (i.e.&nbsp;looking at data) rather than building automated checks.</p><p>Be <a href=\"https://ai-execs.com/2_intro.html#a-case-study-in-misleading-ai-advice\">wary of optimizing for high eval pass rates</a>. If you’re passing 100% of your evals, you’re likely not challenging your system enough. A 70% pass rate might indicate a more meaningful evaluation that’s actually stress-testing your application. Focus on evals that help you catch real issues, not ones that make your metrics look good.</p></section><section><h2 data-anchor-id=\"q-whats-the-difference-between-guardrails-evaluators\">Q: What’s the difference between guardrails &amp; evaluators?</h2><p>Guardrails are  that sit directly in the request/response path. They validate inputs or outputs  anything reaches a user, so they typically are:</p><ul><li> – typically a few milliseconds of latency budget.</li><li> – regexes, keyword block-lists, schema or type validators, lightweight classifiers.</li><li><strong>Targeted at clear-cut, high-impact failures</strong> – PII leaks, profanity, disallowed instructions, SQL injection, malformed JSON, invalid code syntax, etc.</li></ul><p>If a guardrail triggers, the system can redact, refuse, or regenerate the response. Because these checks are user-visible when they fire, false positives are treated as production bugs; teams version guardrail rules, log every trigger, and monitor rates to keep them conservative.</p><p>On the other hand, evaluators typically run  a response is produced. Evaluators measure qualities that simple rules cannot, such as factual correctness, completeness, etc. Their verdicts feed dashboards, regression tests, and model-improvement loops, but they do not block the original answer.</p><p>Evaluators are usually run asynchronously or in batch to afford heavier computation such as a <a href=\"https://hamel.dev/blog/posts/llm-judge/\">LLM-as-a-Judge</a>. Inline use of an LLM-as-Judge is possible  when the latency budget and reliability targets allow it. Slow LLM judges might be feasible in a cascade that runs on the minority of borderline cases.</p><p>Apply guardrails for immediate protection against objective failures requiring intervention. Use evaluators for monitoring and improving subjective or nuanced criteria. Together, they create layered protection.</p><p>Word of caution: Do not use llm guardrails off the shelf blindly. Always <a href=\"https://hamel.dev/blog/posts/prompt/\">look at the prompt</a>.</p></section><section><h2 data-anchor-id=\"q-whats-a-minimum-viable-evaluation-setup\">Q: What’s a minimum viable evaluation setup?</h2><p>Start with <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed\">error analysis</a>, not infrastructure. Spend 30 minutes manually reviewing 20-50 LLM outputs whenever you make significant changes. Use one <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs\">domain expert</a> who understands your users as your quality decision maker (a “<a href=\"https://hamel.dev/blog/posts/evals-faq/#q-how-many-people-should-annotate-my-llm-outputs\">benevolent dictator</a>”).</p><p>If possible,  to help you review traces and analyze data. In our opinion, this is the single most effective tool for evals because you can write arbitrary code, visualize data, and iterate quickly. You can even build your own <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-what-makes-a-good-custom-interface-for-reviewing-llm-outputs\">custom annotation interface</a> right inside notebooks, as shown in this <a href=\"https://youtu.be/aqKUwPKBkB0?si=5KDmMQnRzO_Ce9xH\">video</a>.</p></section><section><h2 data-anchor-id=\"q-how-do-i-evaluate-agentic-workflows\">Q: How do I evaluate agentic workflows?</h2><p>We recommend evaluating agentic workflows in two phases:</p><p><strong>1. End-to-end task success.</strong> Treat the agent as a black box and ask “did we meet the user’s goal?”. Define a precise success rule per task (exact answer, correct side-effect, etc.) and measure with human or <a href=\"https://hamel.dev/blog/posts/llm-judge/\">aligned LLM judges</a>. Take note of the first upstream failure when conducting <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-why-is-error-analysis-so-important-in-llm-evals-and-how-is-it-performed\">error analysis</a>.</p><p>Once error analysis reveals which workflows fail most often, move to step-level diagnostics to understand why they’re failing.</p><p><strong>2. Step-level diagnostics.</strong> Assuming that you have sufficiently <a href=\"https://hamel.dev/blog/posts/evals/#logging-traces\">instrumented your system</a> with details of tool calls and responses, you can score individual components such as: - : was the selected tool appropriate? - : were inputs complete and well-formed? - : did the agent recover from empty results or API failures? - : did it preserve earlier constraints? - : how many steps, seconds, and tokens were spent? - : for long workflows verify key milestones.</p><p>Example: “Find Berkeley homes under $1M and schedule viewings” breaks into: parameters extracted correctly, relevant listings retrieved, availability checked, and calendar invites sent. Each checkpoint can pass or fail independently, making debugging tractable.</p><p><strong>Use transition failure matrices to understand error patterns.</strong> Create a matrix where rows represent the last successful state and columns represent where the first failure occurred. This is a great way to understand where the most failures occur.</p><div><figure><figcaption>Transition failure matrix showing hotspots in text-to-SQL agent workflow</figcaption></figure></div><p>Transition matrices transform overwhelming agent complexity into actionable insights. Instead of drowning in individual trace reviews, you can immediately see that GenSQL → ExecSQL transitions cause 12 failures while DecideTool → PlanCal causes only 2. This data-driven approach guides where to invest debugging effort. Here is another <a href=\"https://www.figma.com/deck/nwRlh5renu4s4olaCsf9lG/Failure-is-a-Funnel?node-id=2009-927&amp;t=GJlTtxQ8bLJaQ92A-1\">example</a> from Bryan Bischof, that is also a text-to-SQL agent:</p><div><figure><figcaption>Bischof, Bryan “Failure is A Funnel - Data Council, 2025”</figcaption></figure></div><p>In this example, Bryan shows variation in transition matrices across experiments. How you organize your transition matrix depends on the specifics of your application. For example, Bryan’s text-to-SQL agent has an inherent sequential workflow which he exploits for further analytical insight. You can watch his <a href=\"https://youtu.be/R_HnI9oTv3c?si=hRRhDiydHU5k6ikc\">full talk</a> for more details.</p><p><strong>Creating Test Cases for Agent Failures</strong></p><p>Creating test cases for agent failures follows the same principles as our previous FAQ on <a href=\"https://hamel.dev/blog/posts/evals-faq/#q-how-do-i-debug-multi-turn-conversation-traces\">debugging multi-turn conversation traces</a> (i.e.&nbsp;try to reproduce the error in the simplest way possible, only use multi-turn tests when the failure actually requires conversation context, etc.).</p></section><section><h2 data-anchor-id=\"q-seriously-hamel.-stop-the-bullshit.-whats-your-favorite-eval-vendor\">Q: Seriously Hamel. Stop the bullshit. What’s your favorite eval vendor?</h2><p>Eval tools are in an intensely competitive space. It would be futile to compare their features. If I tried to do such an analysis, it would be invalidated in a week! Vendors I encounter the most organically in my work are: <a href=\"https://www.langchain.com/langsmith\">Langsmith</a>, <a href=\"https://arize.com/\">Arize</a> and <a href=\"https://www.braintrust.dev/\">Braintrust</a>.</p><p>When I help clients with vendor selection, the decision weighs heavily towards who can offer the best support, as opposed to purely features. This changes depending on size of client, use case, etc. Yes - its mainly the human factor that matters, and dare I say, vibes.</p><p>I have no favorite vendor. At the core, their features are very similar - and I often build <a href=\"https://hamel.dev/blog/posts/evals/#q-should-i-build-a-custom-annotation-tool-or-use-something-off-the-shelf\">custom tools</a> on top of them to fit my needs.</p><p>My suggestion is to explore the vendors and see which one you like the most.</p></section><section><h2 data-anchor-id=\"q-how-are-evaluations-used-differently-in-cicd-vs.-monitoring-production\">Q: How are evaluations used differently in CI/CD vs.&nbsp;monitoring production?</h2><p>The most important difference between CI vs.&nbsp;production evaluation is the data used for testing.</p><p>Test datasets for CI are small (in many cases 100+ examples) and purpose-built. Examples cover core features, regression tests for past bugs, and known edge cases. Since CI tests are run frequently, the cost of each test has to be carefully considered (that’s why you carefully curate the dataset). Favor assertions or other deterministic checks over LLM-as-judge evaluators.</p><p>For evaluating production traffic, you can sample live traces and run evaluators against them asynchronously. Since you usually lack reference outputs on production data, you might rely more on on more expensive reference-free evaluators like LLM-as-judge. Additionally, track confidence intervals for production metrics. If the lower bound crosses your threshold, investigate further.</p><p>These two systems are complementary: when production monitoring reveals new failure patterns through error analysis and evals, add representative examples to your CI dataset. This mitigates regressions on new issues.</p><div><p><em> (we have to get back to building). Here is a <a href=\"https://bit.ly/evals-ai\">35% discount code</a> for readers of this post.</em> 👈</p></div></section>","contentLength":32880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44430117"},{"title":"Writing Code Was Never the Bottleneck","url":"https://ordep.dev/posts/writing-code-was-never-the-bottleneck","date":1751334237,"author":"phire","guid":183196,"unread":true,"content":"<p>For years, I’ve felt that writing lines of code  the bottleneck in software engineering.</p><p>The actual bottlenecks were, and still are, ,  through mentoring and pairing, , , and the human overhead of <strong>coordination and communication</strong>. All of this wrapped inside the labyrinth of tickets, planning meetings, and agile rituals.</p><p>These processes, meant to drive quality, often slow us down more than the act of writing code itself because they require thought, shared understanding, and sound judgment.</p><p>Now, with LLMs making it easy to generate working code faster than ever, a new narrative has emerged: that writing code  the bottleneck, and we’ve finally cracked it.</p><p>But that’s .</p><p>The marginal cost of adding new software is approaching , especially with LLMs. But what is the price of , , and  that code? .</p><h2>LLMs shift the workload — they don’t remove it</h2><p>Tools like Claude can speed up initial implementation. Still, the result is often more code flowing through systems and more pressure on the people responsible for reviewing, integrating, and maintaining it.</p><p>This becomes especially clear when:</p><ul><li>It’s unclear whether the author fully understands what they submitted.</li><li>The generated code introduces unfamiliar patterns or breaks established conventions.</li><li>Edge cases and unintended side effects aren’t obvious.</li></ul><p>We end up in a situation where code is more straightforward to produce but more complex to verify, which doesn’t necessarily make teams move faster overall.</p><p>It’s not a new challenge. Developers have long joked about , but the velocity and scale that LLMs enable have <strong>amplified those copy-paste habits</strong>.</p><h2>Understanding code is still the hard part</h2><blockquote><p><em>“The biggest cost of code is understanding it — not writing it.”</em></p></blockquote><p>LLMs reduce the time it takes to produce code, but they haven’t changed the amount of effort required to reason about behavior, identify subtle bugs, or ensure long-term maintainability. That work can be even more challenging when reviewers struggle to distinguish between generated and handwritten code or understand why a particular solution was chosen.</p><h2>Teams still rely on trust and shared context</h2><p>Software engineering has always been collaborative. It depends on , , and . However, when code is generated faster than it can be discussed or reviewed, teams risk falling into a mode where <strong>quality is assumed rather than ensured</strong>. That creates stress on reviewers and mentors, potentially slowing things down in more subtle ways.</p><h2>LLMs are powerful — but they don’t fix the fundamentals</h2><p>There’s real value in faster prototyping, scaffolding, and automation. But LLMs don’t remove the need for , , and . If anything, those become even more important as more code gets generated.</p><p>Yes, the cost of writing code has indeed dropped. But the cost of making sense of it together as a team .</p><p><strong>That’s still the bottleneck. Let’s not pretend it isn’t.</strong></p>","contentLength":2868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429789"},{"title":"Rust CLI with Clap","url":"https://tucson-josh.com/posts/rust-clap-cli/","date":1751333107,"author":"rajman187","guid":178724,"unread":true,"content":"<p>Types are important. In fact, I'd guess that the expressive type system in rust\nis the single biggest reason why so many developers love the language. Types\nallow us to have a contract between parts of the system about our data and how\nto interact with it. All programming languages have the concept of types, but\nthese exist along several dimensions. Strongly typed vs weakly typed as well as\nstatic vs dynamic typing. Rust stakes out its place as a statically,\nstrongly typed language.</p><p>Many languages that are go-to solutions for creating custom command line tools\nfall in the opposite quadrant with weak, dynamic typing. Whether looking at\ncurrently popular tooling like python and node.js or more traditional solutions\nlike awk and perl, they tend to favor a loose approach to types. Perhaps this\nis the result of an iterative approach to designing CLI tools that might favor\nflexibility. Or it could just be that those languages are already popular,\nleading to an abundance of such programs. Regardless of the reasons, I feel that\nthere is tremendous value for both the developer and user which can arise from\ninteracting with the command line via the sort of strict contract that rust's\ntype system enables.</p><div><p>I assume that if you're already a rust developer, or at least rust-curious, then\nI don't need to convince you of the general value of strong, static typing.\nRather, this is a call to use this same approach for interacting with a command\nline user as you would when developing a library or service API.</p></div><p>At the very lowest level rust exposes command line arguments through the\n function that returns an  struct, an  for the\n arguments passed to start the program. This is illustrated in the Rust\nBook's section on\n<a href=\"https://doc.rust-lang.org/book/ch12-01-accepting-command-line-arguments.html\">accepting command line arguments</a>:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The naive approach seen above obviously lacks robustness as it relies entirely\non argument positioning and also makes a number of other assumptions about the\nresults. Perhaps for very simple tools this solution can work but as the number\nand types of arguments increases, it seems unlikely that a developer would want\nto try and rely on just argument position for the interface to their program.</p><p>A more flexible approach would be to examine all of the arguments passed in and\nparse these for patterns that would allow customary  and  style\noptions. Doing this by hand for every CLI tool would be error-prone and tedious,\nbut fortunately some awesome folks have already done that for you with the\nexcellent <a href=\"https://github.com/clap-rs/clap\">clap</a> crate.</p><h3>The Sound of One Hand Clapping</h3><p>The Command Line Argument Parser for Rust, or clap, is one of the most\nwidely-used crates in the rust ecosystem. GitHub shows that there are over 445k\nrepos which depend on clap at the time of writing. Adding clap to your project\nwill allow you to avoid writing your own parsing logic to interact with the\ncommand line:</p><p>Out of the box clap offers a builder pattern approach that can be used to\nget arguments from the command line without the hassle of parsing an \nof  values:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>Your users can now invoke the above  program from the command line\nand pass in the main argument and optionally enable your x long mode:</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>Clap offers a lot more than just parsing arguments, though. It can also reject\noptions and arguments that are not specified by the programmer and it provides\nbuilt-in help:</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>Okay, so I think we can all agree that clap has some nice features and is far\nmore robust than trying to roll your own command line argument parser, but this\npost started off talking about rust's type system and how that can be used as an\ninterface with the command line user. And that is where clap's  feature\ncomes in.</p><h3>Defining Your CLI Interactions with </h3><p>Clap offers a much more ergonomic way to specify your program's arguments than\nthe builder method shown above, but first you need to include the \nfeature in your dependencies:</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>You can now define rust types in your source which will be translated into an\ninterface contract for your program when called from the command line:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The above program behaves identically to the builder version from the previous\nsection, with a  help option and all the other features that clap offers.\nThe key difference is that we are now using the type system to define the\ninterface rather than imperative calls to a builder. Note that the doc\ncomments for the  struct are used to build the  help subcommand for\nthe resulting application.</p><p>Clap isn't limited to simple structs for the definition of the interface either.\nAs shown above,  works just as you would expect. To build up more\ncomplex command line interactions you can use enums to define subcommand syntax\nwith configuration options for each different subcommand via associated values\n(think  or  subcommands). This offers an elegant solution for managing\nthe complexity that your tool might need to expose to the user.</p><p>There are tons of other great features in clap that can be found in the\n<a href=\"https://docs.rs/clap/latest/clap/index.html\">docs</a>, but rather than get into\nthe specifics of this crate, I want to discuss how type-driven design\ncan elevate command line interfaces to be on equal footing with published\nlibraries and service APIs.</p><p>What can be gained from specifying your software's command line interactions via\nthe rust type system?</p><h3>Advantage 1: Code Maintainability and Readability</h3><p>Perhaps the most obvious benefit of using explicit rust types to define your\ncommand line interface is that it provides a clear, concise definition of what\ninput the program accepts. If you peel away the clap macro calls which annotate\nthe type, it looks just like any other data structure that you would expect to\npass between portions of the program. Because clap builds help from the doc\ncomments, the developer documentation for the type also transcends the command\nline boundary to help users understand how to properly use your software. There\nare no<a href=\"https://tucson-josh.com/posts/rust-clap-cli/#good-for-the-environment-too\">**</a> hidden inputs that will affect your\nprogram. This helps new developers on a project to understand a codebase and\nalso assists maintainers down the road when they need to add new features, as\nthere is a single entry point from which they can start designing their changes.</p><p>Alternative approaches such as using the builder pattern or a custom parsing of\n don't offer this same clarity. At best, these solutions would\nbe contained in one or more functions that abstract away the interface logic. At\nworst these could be scattered across the codebase as each portion of the\nprogram tries to interact directly with the arguments passed in.</p><p>As software grows in complexity the case grows stronger for type-driven CLI\nspecification. Imagine that we are creating a tool which will interact with a\nkey-value store and allow the user to add, remove and list the entries of the\nstore, all of which also require an access token to validate the user. We could\nuse the following to model the interface:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The  type that we've outlined above allows us to clearly express that a\ntoken is always required for all actions, but the  argument is only needed,\nand indeed only allowed, when the user is either adding or removing entries. The\ntype that we have created is concise and removes the complexity one would have\nto deal with if command line arguments were being handled imperatively.</p><h3>Advantage 2: Reduced Test Surface Area and Mock Support</h3><p>Using a crate like clap can eliminate huge swathes of imperative code that would\notherwise be necessary to parse, validate and consume arguments from the command\nline. Every line of code that you don't write saves time on tests that don't\nneed to be created as well. Moving your interaction with the command line from\nimperative functions to a declarative description of possible states moves the\ntesting burden upstream to the maintainers of the clap crate, which is widely\nused and well supported.</p><p>Type-driven command line interaction does more for us than just reducing the\nsurface area, though. It also provides a foundation for more complete unit tests\nby providing the simplest possible mock for an actual command line interaction.\nImagine that our key-value client above delegates each of the top-level actions\n(add, remove, list) to one function each, where more complex operations are\norchestrated. Something like the following:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>Some obvious tests of the above method might involve asserting that \nwould return an  wrapping a <code>KVStoreError::InvalidRequest</code> if we call the\nfunction with , for instance. We could also verify that the key\nreturned by the server matches the key we requested to add:</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>The above test is simplistic, but it is representative of the way data must be\nstructured from an actual user because of the strict typing. This approach gives\nus a high fidelity mock of a command line interaction.</p><h3>Advantage 3: Semantic Versioning: Not Just for Libraries</h3><p>Semantic versioning, or SemVer, is a widely-used framework for determining how\nsoftware creators should version their releases so that downstream users of that\ncode can confidently know what versions are safe to upgrade to from other\nversions. It leads to the familiar three-part version number consisting of\n where each component conveys different levels of change and\npotential upgrade risk. The rust core team follows SemVer for rust releases and\neven have an extensive\n<a href=\"https://doc.rust-lang.org/cargo/reference/semver.html\">section</a> about the topic\nin the Cargo Book.</p><p>Library maintainers generally follow SemVer so that other developers who depend\non their crate can understand when it is safe to upgrade without needing to\ndelve into the release notes of every single release. Authors of binary tools,\nhowever, have been less likely to strictly follow SemVer, as illustrated by the\n<a href=\"https://blog.rust-lang.org/2025/03/04/Rustup-1.28.1/\">rustup 1.28.0</a> adventure,\nwherein a minor release ended up breaking CI for many rust projects.</p><p>Perhaps the reason why authors of binary CLI tools are less likely to follow\nSemVer is because they have an image in their head of the user being a person\nwho can adapt to changes between versions. The reality, however, is that any\nsufficiently useful CLI tool will eventually be integrated into an automated\ntoolchain that expects input and output to be consistent across versions. Good\nCLI tools end up operating very similarly to a library. Unlike libraries,\nthough, upgrading a binary version doesn't get a chance to throw compiler\nerrors. Worse yet, CLI tools are often integrated in parts of the stack where\nobservability is poor and errors are only discovered when catastrophic failure\nhas already occurred.</p><p>So, how can a strictly-typed approach to command line arguments help us to\nbetter follow SemVer with CLI applications? The answer to this is through\ntooling that already exists,\n<a href=\"https://github.com/obi1kenobi/cargo-semver-checks\">cargo-semver-checks</a>. This\ncargo tool examines your source code and compares it against a prior release\nin order to determine if your changes constitute major, minor or merely\npatch level changes. Importantly, though, you should begin to think of your\ncommand line program more like a library in order to help cargo-semver-checks\nto analyze the importance of changes. Your CLI argument types should be made\n even if this level isn't required for your program to run properly.\nThey are, after all, truly the most public part of the software. A similar\napproach is also reasonable with the types that might represent your program's\noutput, whether they are used to write back to the shell, to files or some other\nform of output. Once you've done this, start versioning your binaries\naccordingly. If cargo-semver-checks warns you that a change is major and you\nonly thought that it was a patch, that's a big warning. Did you really intend to\nmake a major, breaking change? If you did, then don't hesitate to change the\nmajor version number.</p><p>Merely knowing about a tool like cargo-semver-checks and having it installed\nis nice, but we all know that things like this are best when they become an\nautomated part of our workflow. It's easy to add a GitHub Action to run a SemVer\ncheck automatically:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Now, even if you forget to run your SemVer check manually, you probably won't\npush out a binary release that breaks some dependency in a completely hidden\nway.</p><h3>Good for the Environment Too</h3><p>There is a loose end that may have been nagging at some readers going over the\nprevious sections: <em>What about environment variables?</em> After all, many command\nline programs can also look at the shell's environment variables as a source of\ninput. We see this particularly around secrets or omnipresent settings.\nFortunately clap has us covered here too with the crate feature  that lets\nyou specify an environment variable which will be queried when a given argument\nwas not specified as part of the command invocation.</p><p>Let's use this to flesh out the code from our key-value store client example in\nthe <a href=\"https://tucson-josh.com/posts/rust-clap-cli/#advantage-1-code-maintainability-and-readability\">maintainability</a> section\nabove. In that example, it would make a lot of sense to make  an argument\nwhich can be stored in an environment variable as well as be overridden from the\ncommand line.</p><div><pre tabindex=\"0\"><code data-lang=\"rust\"></code></pre></div><p>All that was required (aside from adding the  feature to our dependencies)\nwas to add  on line 7. The user can now either pass in the\ntoken via  or by setting the environment variable . The\ngenerated help will automatically pick this up and educate the user about this\noption (line 13 below):</p><div><pre tabindex=\"0\"><code data-lang=\"fish\"></code></pre></div><p>We are now able to have a fully type-driven specification of our command line\ninterface that seamlessly incorporates both the arguments passed in as well as\nenvironment variables from the shell. What's not to love?</p><p>\n    If you want to discuss  this post\n     or any other, please feel free to drop me a message on\n    <a href=\"https://www.instagram.com/tucson.josh/\">Instagram</a>\n    or over at\n    <a href=\"https://bsky.app/profile/tucson-josh.com\">Bluesky</a>.\n</p>","contentLength":13399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429695"},{"title":"The Email Startup Graveyard: Why 80%+ of Email Companies Fail","url":"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail","date":1751330464,"author":"skeptrune","guid":178666,"unread":true,"content":"<img loading=\"lazy\" src=\"https://forwardemail.net/img/articles/email-startup-graveyard-fa0072188b.webp\" alt=\"\"><p>While many email startups have invested millions in solving perceived problems, we at <a href=\"https://forwardemail.net\">Forward Email</a> have focused on building reliable email infrastructure from scratch since 2017. This analysis explores the patterns behind email startup outcomes and the fundamental challenges of email infrastructure.</p><div><p>: Most email startups don't build actual email infrastructure from scratch. Many build on top of existing solutions like Amazon SES or open-source systems like Postfix. The core protocols work well - the challenge is in the implementation.</p></div><div><p>: For comprehensive details on our approach, architecture, and security implementation, see our <a href=\"https://forwardemail.net/technical-whitepaper.pdf\">Forward Email Technical Whitepaper</a> and <a href=\"https://forwardemail.net/en/about\">About page</a> which documents our complete development timeline since 2017.</p></div><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-email-startup-failure-matrix\" role=\"button\" aria-label=\"The Email Startup Failure Matrix\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-email-startup-failure-matrix\" data-target=\"#collapse-the-email-startup-failure-matrix\"></a>The Email Startup Failure Matrix</h2><p>Here's every major email startup failure we could find, organized by accelerator, funding, and outcome:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-infrastructure-reality-check\" role=\"button\" aria-label=\"The Infrastructure Reality Check\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-infrastructure-reality-check\" data-target=\"#collapse-the-infrastructure-reality-check\"></a>The Infrastructure Reality Check</h2><div><p>: Every single \"email startup\" is just building UI on top of existing infrastructure. They're not building actual email servers - they're building apps that connect to real email infrastructure.</p></div><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-email-startups-actually-build\" role=\"button\" aria-label=\"Go to top\"></a>What \"Email Startups\" Actually Build</h3><div><p><strong>Key Pattern for Email Success</strong>: The companies that actually succeed in email don't try to reinvent the wheel. Instead, they build <strong>infrastructure and tools that enhance</strong> existing email workflows. <a href=\"https://sendgrid.com/\" target=\"_blank\" rel=\"noopener noreferrer\">SendGrid</a>, <a href=\"https://www.mailgun.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Mailgun</a>, and <a href=\"https://postmarkapp.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Postmark</a> became billion-dollar companies by providing reliable SMTP APIs and delivery services - they work  email protocols, not against them. This is the same approach we take at Forward Email.</p></div><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-most-email-startups-fail\" role=\"button\" aria-label=\"Why Most Email Startups Fail\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-why-most-email-startups-fail\" data-target=\"#collapse-why-most-email-startups-fail\"></a>Why Most Email Startups Fail</h2><div><p>: Email  startups typically fail because they try to replace working protocols, while email  companies can succeed by enhancing existing workflows. The key is understanding what users actually need versus what entrepreneurs think they need.</p></div><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#1-email-protocols-work-implementation-often-doesnt\" role=\"button\" aria-label=\"Go to top\"></a>1. Email Protocols Work, Implementation Often Doesn't</h3><p>The core email protocols are solid, but implementation quality varies widely:</p><p>: Better implementation of existing protocols, not protocol replacement.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#2-network-effects-are-unbreakable\" role=\"button\" aria-label=\"Go to top\"></a>2. Network Effects Are Unbreakable</h3><p>Email's network effect is absolute:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#3-they-often-target-the-wrong-problems\" role=\"button\" aria-label=\"Go to top\"></a>3. They Often Target the Wrong Problems</h3><p>Many email startups focus on perceived issues rather than real pain points:</p><p><strong>Real problems worth solving</strong>: Infrastructure reliability, deliverability, spam filtering, and developer tools.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#4-technical-debt-is-massive\" role=\"button\" aria-label=\"Go to top\"></a>4. Technical Debt Is Massive</h3><p>Building real email infrastructure requires:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#5-the-infrastructure-already-exists\" role=\"button\" aria-label=\"Go to top\"></a>5. The Infrastructure Already Exists</h3><p>Why reinvent when you can use:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#case-studies-when-email-startups-fail\" role=\"button\" aria-label=\"Case Studies: When Email Startups Fail\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-case-studies-when-email-startups-fail\" data-target=\"#collapse-case-studies-when-email-startups-fail\"></a>Case Studies: When Email Startups Fail</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#case-study-the-skiff-disaster\" role=\"button\" aria-label=\"Go to top\"></a>Case Study: The Skiff Disaster</h3><p>Skiff perfectly exemplifies everything wrong with email startups.</p><ul><li>: \"Privacy-first email and productivity platform\"</li><li>: Better email through privacy and encryption</li></ul><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#y-combinator-the-email-app-factory\" role=\"button\" aria-label=\"Go to top\"></a>Y Combinator: The Email App Factory</h4><p><a href=\"https://www.ycombinator.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Y Combinator</a> has funded dozens of email startups. Here's the pattern:</p><p>: Mixed results with some notable exits. Several companies achieved successful acquisitions (reMail to Google, Rapportive to LinkedIn), while others pivoted away from email or were acqui-hired for talent.</p><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#techstars-the-email-graveyard\" role=\"button\" aria-label=\"Go to top\"></a>Techstars: The Email Graveyard</h4><p>: Vague value propositions, no real technical innovation, quick failures.</p><div><p>: VCs love email startups because they sound simple but are actually impossible. The fundamental assumptions that attract investment are exactly what guarantee failure.</p></div><p>VCs love email startups because they sound simple but are actually impossible:</p><p>: None of these assumptions hold true for email.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-technical-reality-modern-email-stacks\" role=\"button\" aria-label=\"The Technical Reality: Modern Email Stacks\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-technical-reality-modern-email-stacks\" data-target=\"#collapse-the-technical-reality-modern-email-stacks\"></a>The Technical Reality: Modern Email Stacks</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-actually-powers-email-startups\" role=\"button\" aria-label=\"Go to top\"></a>What Actually Powers \"Email Startups\"</h3><p>Let's look at what these companies actually run:</p><p>: Most email apps are Electron-based web apps that consume massive amounts of RAM:</p><div><p><strong>Electron Performance Crisis</strong>: Modern email clients built with Electron and React Native suffer from severe memory bloat and performance issues. These cross-platform frameworks, while convenient for developers, create resource-heavy applications that consume hundreds of megabytes to gigabytes of RAM for basic email functionality.</p></div><p>: Constant syncing and inefficient code:</p><ul><li>Background processes that never sleep</li><li>Unnecessary API calls every few seconds</li><li>Poor connection management</li><li>No third-party dependencies except those absolutely required for core functionality</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-acquisition-patterns-success-vs-shutdown\" role=\"button\" aria-label=\"The Acquisition Patterns: Success vs. Shutdown\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-acquisition-patterns-success-vs-shutdown\" data-target=\"#collapse-the-acquisition-patterns-success-vs-shutdown\"></a>The Acquisition Patterns: Success vs. Shutdown</h2><p><strong>Client App Pattern (Usually Fails)</strong>:</p><p><strong>Infrastructure Pattern (Often Succeeds)</strong>:</p><p>:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#industry-evolution-and-consolidation\" role=\"button\" aria-label=\"Industry Evolution and Consolidation\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-industry-evolution-and-consolidation\" data-target=\"#collapse-industry-evolution-and-consolidation\"></a>Industry Evolution and Consolidation</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#natural-industry-progression\" role=\"button\" aria-label=\"Go to top\"></a>Natural Industry Progression</h3><p>The email industry has naturally evolved toward consolidation, with larger companies acquiring smaller ones to integrate features or eliminate competition. This isn't necessarily negative - it's how most mature industries develop.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#post-acquisition-transitions\" role=\"button\" aria-label=\"Go to top\"></a>Post-Acquisition Transitions</h3><p>When email companies are acquired, users often face:</p><ul><li>: Moving to new platforms</li><li>: Loss of specialized functionality</li><li>: Different subscription models</li><li>: Temporary service disruptions</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#user-considerations-during-transitions\" role=\"button\" aria-label=\"Go to top\"></a>User Considerations During Transitions</h3><p>During industry consolidation, users benefit from:</p><ul><li>: Multiple providers offer similar services</li><li><strong>Understanding migration paths</strong>: Most services provide export tools</li><li><strong>Considering long-term stability</strong>: Established providers often offer more continuity</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-hacker-news-reality-check\" role=\"button\" aria-label=\"The Hacker News Reality Check\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-hacker-news-reality-check\" data-target=\"#collapse-the-hacker-news-reality-check\"></a>The Hacker News Reality Check</h2><p>Every email startup gets the same comments on <a href=\"https://news.ycombinator.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Hacker News</a>:</p><p>. These comments appear on every email startup launch because the fundamental problems are always the same.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-modern-ai-email-grift\" role=\"button\" aria-label=\"The Modern AI Email Grift\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-modern-ai-email-grift\" data-target=\"#collapse-the-modern-ai-email-grift\"></a>The Modern AI Email Grift</h2><p>2024 brought a new wave of \"AI-powered email\" startups, with the first major successful exit already happening:</p><p>Adding \"AI\" doesn't solve the fundamental challenges:</p><p>: AI features require significant infrastructure investment while addressing relatively minor pain points.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-actually-works-the-real-email-success-stories\" role=\"button\" aria-label=\"What Actually Works: The Real Email Success Stories\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-what-actually-works-the-real-email-success-stories\" data-target=\"#collapse-what-actually-works-the-real-email-success-stories\"></a>What Actually Works: The Real Email Success Stories</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#infrastructure-companies-the-winners\" role=\"button\" aria-label=\"Go to top\"></a>Infrastructure Companies (The Winners)</h3><p>: They build infrastructure, not apps.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-providers-the-survivors\" role=\"button\" aria-label=\"Go to top\"></a>Email Providers (The Survivors)</h3><div><p><strong>The JMAP Investment Question</strong>: While Fastmail invests resources in <a href=\"https://jmap.io/\" target=\"_blank\" rel=\"noopener noreferrer\">JMAP</a>, a protocol that's <a href=\"https://github.com/zone-eu/wildduck/issues/2#issuecomment-1765190790\" target=\"_blank\" rel=\"noopener noreferrer\">10+ years old with limited adoption</a>, they simultaneously <a href=\"https://www.fastmail.com/blog/why-we-dont-offer-pgp/\" target=\"_blank\" rel=\"noopener noreferrer\">refuse to implement PGP encryption</a> that many users request. This represents a strategic choice to prioritize protocol innovation over user-requested features. Whether JMAP will gain broader adoption remains to be seen, but the current email client ecosystem continues to rely primarily on IMAP/SMTP.</p></div><div><p>: Forward Email powers <a href=\"https://forwardemail.net/en/blog/docs/alumni-email-forwarding-university-case-study\">alumni email solutions for top universities</a>, including the University of Cambridge with 30,000 alumni addresses, delivering $87,000 in annual cost savings compared to traditional solutions.</p></div><p>: They enhance email, don't replace it.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-exception-xobnis-success-story\" role=\"button\" aria-label=\"Go to top\"></a>The Exception: Xobni's Success Story</h3><p><a href=\"https://en.wikipedia.org/wiki/Xobni\" target=\"_blank\" rel=\"noopener noreferrer\">Xobni</a> stands out as one of the few email-related startups that actually succeeded by taking the right approach.</p><ul><li>: Built on top of Outlook instead of replacing it</li><li>: Contact management and email search</li><li>: Worked with existing workflows</li><li>: Targeted business users with real pain points</li></ul><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-xobni-succeeded-where-others-failed\" role=\"button\" aria-label=\"Go to top\"></a>Why Xobni Succeeded Where Others Failed</h4><ol><li><strong>Built on proven infrastructure</strong>: Used Outlook's existing email handling</li><li>: Contact management was genuinely broken</li><li>: Businesses pay for productivity tools</li><li>: Enhanced rather than replaced existing workflows</li></ol><h4><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-founders-continued-success\" role=\"button\" aria-label=\"Go to top\"></a>The Founders' Continued Success</h4><ul><li>: Became an active <a href=\"https://mercury.com/investor-database/matt-brezina\" target=\"_blank\" rel=\"noopener noreferrer\">angel investor</a> with investments in Dropbox, Mailbox, and others</li><li>: Continued building successful companies in the productivity space</li><li>: Demonstrated that email success comes from enhancement, not replacement</li></ul><p>Companies succeed in email when they:</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#has-anyone-successfully-reinvented-email\" role=\"button\" aria-label=\"Has Anyone Successfully Reinvented Email?\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-has-anyone-successfully-reinvented-email\" data-target=\"#collapse-has-anyone-successfully-reinvented-email\"></a>Has Anyone Successfully Reinvented Email?</h2><p>This is a crucial question that gets to the heart of email innovation. The short answer is: <strong>no one has successfully replaced email, but some have successfully enhanced it</strong>.</p><p>Looking at email innovations over the past 20 years:</p><p>: All successful innovations  existing email protocols rather than replacing them.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#new-tools-complement-email-but-dont-replace-it\" role=\"button\" aria-label=\"Go to top\"></a>New Tools Complement Email (But Don't Replace It)</h3><ul><li>: Great for team chat, but still sends email notifications</li><li>: Excellent for communities, but uses email for account management</li><li>: Perfect for messaging, but businesses still use email</li><li>: Essential for video calls, but meeting invites come via email</li></ul><div><p>: HEY's founder <a href=\"https://dhh.dk/\" target=\"_blank\" rel=\"noopener noreferrer\">DHH</a> actually uses our service at Forward Email for his personal domain  and has for several years, demonstrating that even email innovators rely on proven infrastructure.</p></div><p><a href=\"https://hey.com/\" target=\"_blank\" rel=\"noopener noreferrer\">HEY</a> by <a href=\"https://basecamp.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Basecamp</a> represents the most serious recent attempt to \"reinvent\" email:</p><ul><li>: Completely new email paradigm with screening, bundling, and workflows</li><li>: Mixed - some love it, most stick with existing email</li><li>: It's still email (SMTP/IMAP) with a different interface</li></ul><p>The most successful email innovations have been:</p><ol><li>: Faster servers, better spam filtering, improved deliverability</li><li>: APIs for sending email, webhooks for tracking</li><li>: CRM integration, marketing automation, transactional email</li></ol><p><strong>None of these replaced email - they made it better.</strong></p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#building-modern-infrastructure-for-existing-email-protocols-our-approach\" role=\"button\" aria-label=\"Building Modern Infrastructure for Existing Email Protocols: Our Approach\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-building-modern-infrastructure-for-existing-email-protocols-our-approach\" data-target=\"#collapse-building-modern-infrastructure-for-existing-email-protocols-our-approach\"></a>Building Modern Infrastructure for Existing Email Protocols: Our Approach</h2><p>Before diving into the failures, it's important to understand what actually works in email. The challenge isn't that email is broken - it's that most companies try to \"fix\" something that already works perfectly.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-email-innovation-spectrum\" role=\"button\" aria-label=\"Go to top\"></a>The Email Innovation Spectrum</h3><p>Email innovation falls into three categories:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-we-focus-on-infrastructure\" role=\"button\" aria-label=\"Go to top\"></a>Why We Focus on Infrastructure</h3><p>We chose to build modern email infrastructure because:</p><ul><li><strong>The problem is implementation</strong>: Most email services use outdated software stacks</li><li>: Not new features that break existing workflows</li><li>: Better APIs and management interfaces</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#what-actually-works-in-email\" role=\"button\" aria-label=\"Go to top\"></a>What Actually Works in Email</h3><p>The successful pattern is simple: <strong>enhance existing email workflows instead of replacing them</strong>. This means:</p><ul><li>Building faster, more reliable SMTP servers</li><li>Creating better spam filtering without breaking legitimate email</li><li>Providing developer-friendly APIs for existing protocols</li><li>Improving deliverability through proper infrastructure</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#our-approach-why-were-different\" role=\"button\" aria-label=\"Our Approach: Why We're Different\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-our-approach-why-were-different\" data-target=\"#collapse-our-approach-why-were-different\"></a>Our Approach: Why We're Different</h2><ul><li><strong>Build actual infrastructure</strong>: Custom SMTP/IMAP servers from scratch</li><li><strong>Enhance existing workflows</strong>: Work with all email clients</li><li>: APIs and tools that actually work</li></ul><ul><li>Build \"revolutionary\" email clients</li><li>Try to replace existing email protocols</li><li>Add unnecessary AI features</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#how-we-build-email-infrastructure-that-actually-works\" role=\"button\" aria-label=\"How We Build Email Infrastructure That Actually Works\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-how-we-build-email-infrastructure-that-actually-works\" data-target=\"#collapse-how-we-build-email-infrastructure-that-actually-works\"></a>How We Build Email Infrastructure That Actually Works</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#our-anti-startup-approach\" role=\"button\" aria-label=\"Go to top\"></a>Our Anti-Startup Approach</h3><p>While other companies burn millions trying to reinvent email, we focus on building reliable infrastructure:</p><ul><li>: We've been building email infrastructure for 7+ years</li><li>: We're building for the long term</li><li><strong>No \"revolutionary\" claims</strong>: We just make email work better</li></ul><div><p><strong>Government-Grade Compliance</strong>: Forward Email is <a href=\"https://forwardemail.net/en/blog/docs/federal-government-email-service-section-889-compliant\">Section 889 compliant</a> and serves organizations like the US Naval Academy, demonstrating our commitment to meeting stringent federal security requirements.</p></div><div><p><strong>OpenPGP and OpenWKD Implementation</strong>: Unlike Fastmail, which <a href=\"https://www.fastmail.com/blog/why-we-dont-offer-pgp/\" target=\"_blank\" rel=\"noopener noreferrer\">refuses to implement PGP</a> citing complexity concerns, Forward Email provides full OpenPGP support with OpenWKD (Web Key Directory) compliance, giving users the encryption they actually want without forcing them to use experimental protocols like JMAP.</p></div><p><strong>Technical Stack Comparison</strong>:</p><ul><li>= <a href=\"https://blog.apnic.net/2024/10/04/smtp-downgrade-attacks-and-mta-sts/#:~:text=Logs%20indicate%20that%20Proton%20Mail%20uses%C2%A0postfix%2Dmta%2Dsts%2Dresolver%2C%20hinting%20that%20they%20run%20a%20Postfix%20stack\" target=\"_blank\" rel=\"noopener noreferrer\">APNIC blog post</a> confirms Proton uses postfix-mta-sts-resolver, indicating they run a Postfix stack</li></ul><ul><li>: JavaScript across the entire stack vs. 1980s C code</li><li>: Single language eliminates integration complexity</li><li>: Built for modern web development from the ground up</li><li>: Any web developer can understand and contribute</li><li>: Clean, modern codebase without decades of patches</li></ul><div><p>: Our <a href=\"https://forwardemail.net/en/privacy\">privacy policy</a> ensures we don't store forwarded emails to disk storage or databases, don't store metadata about emails, and don't store logs or IP addresses - operating in-memory only for email forwarding services.</p></div><p>: For comprehensive details on our approach, architecture, and security implementation, see our <a href=\"https://forwardemail.net/technical-whitepaper.pdf\">technical whitepaper</a> and extensive technical documentation.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-service-provider-comparison-growth-through-proven-protocols\" role=\"button\" aria-label=\"Go to top\"></a>Email Service Provider Comparison: Growth Through Proven Protocols</h3><div><p>: While other providers chase experimental protocols, Forward Email focuses on what users actually want - reliable IMAP, POP3, SMTP, CalDAV, and CardDAV that works across all devices. Our growth demonstrates the value of this approach.</p></div><table><tbody><tr></tr><tr></tr><tr><td><code>in1-smtp.messagingengine.com</code></td></tr><tr></tr><tr></tr><tr></tr></tbody></table><ul><li> shows strong growth (+21.1%) with over 500K domains using our MX records</li><li><strong>Proven infrastructure wins</strong>: Services with reliable IMAP/SMTP show consistent domain adoption</li><li>: Fastmail's JMAP investment shows slower growth (+14%) compared to providers focusing on standard protocols</li><li>: The defunct startup lost 55.2% of domains, demonstrating the failure of \"revolutionary\" email approaches</li><li>: Domain count growth reflects real user adoption, not marketing metrics</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-we-succeed-where-others-fail\" role=\"button\" aria-label=\"Go to top\"></a>Why We Succeed Where Others Fail</h3><ol><li><strong>We build infrastructure, not apps</strong>: Focus on servers and protocols</li><li><strong>We enhance, don't replace</strong>: Work with existing email clients</li><li>: No VC pressure to \"grow fast and break things\"</li><li>: 7+ years of deep technical experience</li><li>: APIs and tools that actually solve problems</li></ol><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#security-challenges-in-email-infrastructure\" role=\"button\" aria-label=\"Security Challenges in Email Infrastructure\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-security-challenges-in-email-infrastructure\" data-target=\"#collapse-security-challenges-in-email-infrastructure\"></a>Security Challenges in Email Infrastructure</h2><p>Email security is a complex challenge that affects all providers in the industry. Rather than highlighting individual incidents, it's more valuable to understand the common security considerations that all email infrastructure providers must address.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#common-security-considerations\" role=\"button\" aria-label=\"Go to top\"></a>Common Security Considerations</h3><p>All email providers face similar security challenges:</p><ul><li>: Securing user data and communications</li><li>: Managing authentication and authorization</li><li>: Protecting servers and databases</li><li>: Meeting various regulatory requirements like <a href=\"https://gdpr.eu/\" target=\"_blank\" rel=\"noopener noreferrer\">GDPR</a> and <a href=\"https://oag.ca.gov/privacy/ccpa\" target=\"_blank\" rel=\"noopener noreferrer\">CCPA</a></li></ul><div><p>: Our <a href=\"https://forwardemail.net/en/security\">security practices</a> include ChaCha20-Poly1305 encryption for mailboxes, full disk encryption with LUKS v2, and comprehensive protection with encryption-at-rest, encryption-in-memory, and encryption-in-transit.</p></div><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-value-of-transparency\" role=\"button\" aria-label=\"Go to top\"></a>The Value of Transparency</h3><p>When security incidents occur, the most valuable response is transparency and quick action. Companies that:</p><ul><li><strong>Disclose incidents promptly</strong>: Help users make informed decisions</li><li><strong>Provide detailed timelines</strong>: Show they understand the scope of issues</li><li>: Demonstrate technical competence</li><li>: Contribute to industry-wide security improvements</li></ul><p>These responses benefit the entire email ecosystem by promoting best practices and encouraging other providers to maintain high security standards.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#ongoing-security-challenges\" role=\"button\" aria-label=\"Go to top\"></a>Ongoing Security Challenges</h3><p>The email industry continues to evolve its security practices:</p><p>These challenges require ongoing investment and expertise from all providers in the space.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#conclusion-focus-on-infrastructure-not-apps\" role=\"button\" aria-label=\"Conclusion: Focus on Infrastructure, Not Apps\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-conclusion-focus-on-infrastructure-not-apps\" data-target=\"#collapse-conclusion-focus-on-infrastructure-not-apps\"></a>Conclusion: Focus on Infrastructure, Not Apps</h2><p>After analyzing hundreds of email startups:</p><ul><li>: Most email startups fail completely (this figure is likely WAY higher than 80%; we're being nice)</li><li>: Being acquired usually means death for email clients</li><li><strong>Infrastructure can succeed</strong>: Companies building SMTP/API services often thrive</li><li><strong>VC funding creates pressure</strong>: Venture capital creates unrealistic growth expectations</li><li><strong>Technical debt accumulates</strong>: Building email infrastructure is harder than it looks</li></ul><p>Email has been \"dying\" for 20+ years according to startups:</p><ul><li>: \"Social networks will replace email\"</li><li>: \"Mobile messaging will kill email\"</li><li>: \"<a href=\"https://slack.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Slack</a> will replace email\"</li><li>: \"AI will revolutionize email\"</li><li>: \"Remote work needs new communication tools\"</li><li>: \"AI will finally fix email\"</li></ul><p>. It's still growing. It's still essential.</p><p>The lesson isn't that email can't be improved. It's about choosing the right approach:</p><ol><li>: Reliability and performance beat flashy features</li><li><strong>Enhancement beats replacement</strong>: Work with email, don't fight it</li><li><strong>Sustainability beats growth</strong>: Profitable businesses outlast VC-funded ones</li><li>: Tools and APIs create more value than end-user apps</li></ol><p>: Better implementation of proven protocols, not protocol replacement.</p><div><p><strong>Comprehensive Email Service Analysis</strong>: For an in-depth comparison of 79 email services in 2025, including detailed reviews, screenshots, and technical analysis, see our comprehensive guide: <a href=\"https://forwardemail.net/en/blog/best-email-service\">79 Best Email Services</a>. This analysis demonstrates why Forward Email consistently ranks as the recommended choice for reliability, security, and standards compliance.</p></div><p>If you're thinking about building an email startup, consider building email infrastructure instead. The world needs better email servers, not more email apps.</p><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-extended-email-graveyard-more-failures-and-shutdowns\" role=\"button\" aria-label=\"The Extended Email Graveyard: More Failures and Shutdowns\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-extended-email-graveyard-more-failures-and-shutdowns\" data-target=\"#collapse-the-extended-email-graveyard-more-failures-and-shutdowns\"></a>The Extended Email Graveyard: More Failures and Shutdowns</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#googles-email-experiments-gone-wrong\" role=\"button\" aria-label=\"Go to top\"></a>Google's Email Experiments Gone Wrong</h3><p>Google, despite owning <a href=\"https://gmail.com/\" target=\"_blank\" rel=\"noopener noreferrer\">Gmail</a>, has killed multiple email projects:</p><ul><li> (2009-2012): \"Email killer\" that nobody understood</li><li> (2010-2011): Social email integration disaster</li><li> email features (2011-2019): Social network email integration</li></ul><p>: Even Google can't successfully reinvent email.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-serial-failure-newton-mails-three-deaths\" role=\"button\" aria-label=\"Go to top\"></a>The Serial Failure: Newton Mail's Three Deaths</h3><ol><li> (2013-2016): Email client acquired by Newton</li><li> (2016-2018): Rebranded, subscription model failed</li></ol><p>: Email clients can't sustain subscription models.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-apps-that-never-launched\" role=\"button\" aria-label=\"Go to top\"></a>The Apps That Never Launched</h3><p>Many email startups died before launching:</p><ul><li> (2014): Calendar-email integration, shut down pre-launch</li><li> (2011): Email management tool, acquired before release</li><li> (2013): Email client, development stopped</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-acquisition-to-shutdown-pattern\" role=\"button\" aria-label=\"Go to top\"></a>The Acquisition-to-Shutdown Pattern</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-infrastructure-consolidation\" role=\"button\" aria-label=\"Go to top\"></a>Email Infrastructure Consolidation</h3><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-open-source-email-graveyard-when-free-isnt-sustainable\" role=\"button\" aria-label=\"The Open-Source Email Graveyard: When &quot;Free&quot; Isn't Sustainable\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-open-source-email-graveyard-when-free-isnt-sustainable\" data-target=\"#collapse-the-open-source-email-graveyard-when-free-isnt-sustainable\"></a>The Open-Source Email Graveyard: When \"Free\" Isn't Sustainable</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#nylas-mail--mailspring-the-fork-that-couldnt\" role=\"button\" aria-label=\"Go to top\"></a>Nylas Mail → Mailspring: The Fork That Couldn't</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#eudora-the-18-year-death-march\" role=\"button\" aria-label=\"Go to top\"></a>Eudora: The 18-Year Death March</h3><ul><li>: Dominant email client for Mac/Windows</li><li>: Open-sourced as \"Eudora OSE\"</li><li>: Even successful email clients eventually die</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#fairemail-killed-by-google-play-politics\" role=\"button\" aria-label=\"Go to top\"></a>FairEmail: Killed by Google Play Politics</h3><p>Open-source email projects fail because:</p><ul><li>: Email protocols are complex to implement correctly</li><li>: Constant security updates required</li><li>: Must work with all email providers</li><li>: Volunteer developers burnout</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-ai-email-startup-surge-history-repeating-with-intelligence\" role=\"button\" aria-label=\"The AI Email Startup Surge: History Repeating with &quot;Intelligence&quot;\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-ai-email-startup-surge-history-repeating-with-intelligence\" data-target=\"#collapse-the-ai-email-startup-surge-history-repeating-with-intelligence\"></a>The AI Email Startup Surge: History Repeating with \"Intelligence\"</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-current-ai-email-gold-rush\" role=\"button\" aria-label=\"Go to top\"></a>The Current AI Email Gold Rush</h3><p>2024's AI email startups:</p><p>VCs are throwing money at \"AI + Email\":</p><ul><li>: \"Revolutionary email experience\"</li><li>: Building on top of existing infrastructure</li><li>: Most will fail within 3 years</li></ul><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#why-theyll-all-fail-again\" role=\"button\" aria-label=\"Go to top\"></a>Why They'll All Fail (Again)</h3><ol><li><strong>AI doesn't solve email's non-problems</strong>: Email works fine</li><li>: AI requires reading all your emails</li><li>: AI processing is expensive, email is commodity</li><li>: Can't break Gmail/Outlook dominance</li></ol><ul><li>: Most remaining AI email startups will pivot or shut down</li><li>: Survivors will be acquired, with mixed outcomes</li><li>: \"Blockchain email\" or the next trend will emerge</li></ul><h2><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-consolidation-catastrophe-when-survivors-become-disasters\" role=\"button\" aria-label=\"The Consolidation Catastrophe: When &quot;Survivors&quot; Become Disasters\" data-toggle=\"collapse\" aria-expanded=\"false\" aria-controls=\"collapse-the-consolidation-catastrophe-when-survivors-become-disasters\" data-target=\"#collapse-the-consolidation-catastrophe-when-survivors-become-disasters\"></a>The Consolidation Catastrophe: When \"Survivors\" Become Disasters</h2><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-great-email-service-consolidation\" role=\"button\" aria-label=\"Go to top\"></a>The Great Email Service Consolidation</h3><p>The email industry has consolidated dramatically:</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#outlook-the-survivor-that-cant-stop-breaking\" role=\"button\" aria-label=\"Go to top\"></a>Outlook: The \"Survivor\" That Can't Stop Breaking</h3><p><strong>Our Real-World Experience</strong>: We regularly help customers whose Outlook setups break our perfectly compliant IMAP implementation.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-postmark-infrastructure-problem\" role=\"button\" aria-label=\"Go to top\"></a>The Postmark Infrastructure Problem</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#recent-email-client-casualties-2024-2025\" role=\"button\" aria-label=\"Go to top\"></a>Recent Email Client Casualties (2024-2025)</h3><p>: Users increasingly report poor experience with the email client.</p><p>: Windows users face licensing issues and subscription confusion.</p><p>: The Mac/iOS email client, based on the failed Sparrow codebase, continues to receive <a href=\"https://airmailapp.com/\" target=\"_blank\" rel=\"noopener noreferrer\">poor reviews</a> for reliability issues.</p><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#email-extension-and-service-acquisitions\" role=\"button\" aria-label=\"Go to top\"></a>Email Extension and Service Acquisitions</h3><h3><a href=\"https://forwardemail.net/en/blog/docs/email-startup-graveyard-why-80-percent-email-companies-fail#the-survivors-email-companies-that-actually-work\" role=\"button\" aria-label=\"Go to top\"></a>The Survivors: Email Companies That Actually Work</h3><p>Not all email companies fail. Here are the ones that actually work:</p><p>: Bootstrap success story generating <a href=\"https://www.indiehackers.com/product/gmass\" target=\"_blank\" rel=\"noopener noreferrer\">$140K/month</a> as a Gmail extension for email marketing.</p><p>: These companies succeed because they <strong>enhance existing email workflows</strong> rather than trying to replace email entirely. They build tools that work  email infrastructure, not against it.</p>","contentLength":18695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429457"},{"title":"Claude Code now supports hooks","url":"https://docs.anthropic.com/en/docs/claude-code/hooks","date":1751328075,"author":"ramoz","guid":177020,"unread":true,"content":"<p>Claude Code hooks are user-defined shell commands that execute at various points\nin Claude Code’s lifecycle. Hooks provide deterministic control over Claude\nCode’s behavior, ensuring certain actions always happen rather than relying on\nthe LLM to choose to run them.</p><p>Example use cases include:</p><ul><li>: Customize how you get notified when Claude Code is awaiting\nyour input or permission to run something.</li><li>: Run  on .ts files,  on .go files,\netc. after every file edit.</li><li>: Track and count all executed commands for compliance or\ndebugging.</li><li>: Provide automated feedback when Claude Code produces code that\ndoes not follow your codebase conventions.</li><li>: Block modifications to production files or sensitive\ndirectories.</li></ul><p>By encoding these rules as hooks rather than prompting instructions, you turn\nsuggestions into app-level code that executes every time it is expected to run.</p><p>In this quickstart, you’ll add a hook that logs the shell commands that Claude\nCode runs.</p><p>Quickstart Prerequisite: Install  for JSON processing in the command line.</p><h3></h3><p> hooks run before tool calls and can block them while providing\nClaude feedback on what to do differently.</p><p>Select  to run your hook only on Bash tool calls.</p><p>Select  and enter this command:</p><h3></h3><p>For storage location, select  since you’re logging to your home\ndirectory. This hook will then apply to all projects, not just your current\nproject.</p><p>Then press Esc until you return to the REPL. Your hook is now registered!</p><h3></h3><p>Run  again or check  to see your configuration:</p><ul><li> - User settings</li><li> - Project settings</li><li><code>.claude/settings.local.json</code> - Local project settings (not committed)</li><li>Enterprise managed policy settings</li></ul><p>Hooks are organized by matchers, where each matcher can have multiple hooks:</p><ul><li>: Pattern to match tool names (only applicable for  and\n)<ul><li>Simple strings match exactly:  matches only the Write tool</li><li>Supports regex:  or </li><li>If omitted or empty string, hooks run for all matching events</li></ul></li><li>: Array of commands to execute when the pattern matches<ul><li>: Currently only  is supported</li><li>: The bash command to execute</li><li>: (Optional) How long a command should run, in seconds, before\ncanceling all in-progress hooks.</li></ul></li></ul><p>Runs after Claude creates tool parameters and before processing the tool call.</p><ul><li> - File pattern matching</li><li>,  - File editing</li><li>,  - Web operations</li></ul><p>Runs immediately after a tool completes successfully.</p><p>Recognizes the same matcher values as PreToolUse.</p><p>Runs when Claude Code sends notifications.</p><p>Runs when the main Claude Code agent has finished responding.</p><p>Runs when a Claude Code subagent (Task tool call) has finished responding.</p><p>Hooks receive JSON data via stdin containing session information and\nevent-specific data:</p><p>The exact schema for  depends on the tool.</p><p>The exact schema for  and  depends on the tool.</p><h3></h3><p> is true when Claude Code is already continuing as a result of\na stop hook. Check this value or process the transcript to prevent Claude Code\nfrom running indefinitely.</p><p>There are two ways for hooks to return output back to Claude Code. The output\ncommunicates whether to block and any feedback that should be shown to Claude\nand the user.</p><p>Hooks communicate status through exit codes, stdout, and stderr:</p><ul><li>: Success.  is shown to the user in transcript mode\n(CTRL-R).</li><li>: Blocking error.  is fed back to Claude to process\nautomatically. See per-hook-event behavior below.</li><li>: Non-blocking error.  is shown to the user and\nexecution continues.</li></ul><table><tbody><tr><td>Blocks the tool call, shows error to Claude</td></tr><tr><td>Shows error to Claude (tool already ran)</td></tr><tr><td>N/A, shows stderr to user only</td></tr><tr><td>Blocks stoppage, shows error to Claude</td></tr><tr><td>Blocks stoppage, shows error to Claude subagent</td></tr></tbody></table><p>Hooks can return structured JSON in  for more sophisticated control:</p><p>All hook types can include these optional fields:</p><p>If  is false, Claude stops processing after the hooks run.</p><ul><li>For , this is different from , which only\nblocks a specific tool call and provides automatic feedback to Claude.</li><li>For , this is different from , which\nprovides automated feedback to Claude.</li><li>For  and , this takes precedence over any\n output.</li><li>In all cases,  takes precedence over any\n output.</li></ul><p> accompanies  with a reason shown to the user, not shown\nto Claude.</p><p> hooks can control whether a tool call proceeds.</p><ul><li>“approve” bypasses the permission system.  is shown to the user but\nnot to Claude.</li><li>“block” prevents the tool call from executing.  is shown to Claude.</li><li> leads to the existing permission flow.  is ignored.</li></ul><h4></h4><p> hooks can control whether a tool call proceeds.</p><ul><li>“block” automatically prompts Claude with .</li><li> does nothing.  is ignored.</li></ul><h4></h4><p> and  hooks can control whether Claude must continue.</p><ul><li>“block” prevents Claude from stopping. You must populate  for Claude\nto know how to proceed.</li><li> allows Claude to stop.  is ignored.</li></ul><p>Claude Code hooks work seamlessly with\n<a href=\"https://docs.anthropic.com/en/docs/claude-code/mcp\">Model Context Protocol (MCP) tools</a>. When MCP servers\nprovide tools, they appear with a special naming pattern that you can match in\nyour hooks.</p><p>MCP tools follow the pattern , for example:</p><ul><li><code>mcp__memory__create_entities</code> - Memory server’s create entities tool</li><li><code>mcp__filesystem__read_file</code> - Filesystem server’s read file tool</li><li><code>mcp__github__search_repositories</code> - GitHub server’s search tool</li></ul><p>You can target specific MCP tools or entire MCP servers:</p><p>Automatically format code after file modifications:</p><p>Customize the notification that is sent when Claude Code requests permission or\nwhen the prompt input has become idle.</p><p>: Claude Code hooks execute arbitrary shell commands on\nyour system automatically. By using hooks, you acknowledge that:</p><ul><li>You are solely responsible for the commands you configure</li><li>Hooks can modify, delete, or access any files your user account can access</li><li>Malicious or poorly written hooks can cause data loss or system damage</li><li>Anthropic provides no warranty and assumes no liability for any damages\nresulting from hook usage</li><li>You should thoroughly test hooks in a safe environment before production use</li></ul><p>Always review and understand any hook commands before adding them to your\nconfiguration.</p><p>Here are some key practices for writing more secure hooks:</p><ol><li><strong>Validate and sanitize inputs</strong> - Never trust input data blindly</li><li><strong>Always quote shell variables</strong> - Use  not </li><li> - Check for  in file paths</li><li> - Specify full paths for scripts</li><li> - Avoid , , keys, etc.</li></ol><p>Direct edits to hooks in settings files don’t take effect immediately. Claude\nCode:</p><ol><li>Captures a snapshot of hooks at startup</li><li>Uses this snapshot throughout the session</li><li>Warns if hooks are modified externally</li><li>Requires review in  menu for changes to apply</li></ol><p>This prevents malicious hook modifications from affecting your current session.</p><ul><li>: 60-second execution limit by default, configurable per command.<ul><li>If any individual command times out, all in-progress hooks are cancelled.</li></ul></li><li>: All matching hooks run in parallel</li><li>: Runs in current directory with Claude Code’s environment</li><li>:<ul><li>PreToolUse/PostToolUse/Stop: Progress shown in transcript (Ctrl-R)</li><li>Notification: Logged to debug only ()</li></ul></li></ul><ol><li>Check if  menu displays your configuration</li><li>Review stdout and stderr format expectations</li><li>Ensure proper quote escaping</li><li>Use  to debug your hooks. The output of a successful hook\nappears like below.</li></ol><p>Progress messages appear in transcript mode (Ctrl-R) showing:</p><ul></ul>","contentLength":7006,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429225"},{"title":"Melbourne man discovers extensive model train network underneath house","url":"https://www.sbs.com.au/news/article/i-was-shocked-melbourne-mans-unbelievable-find-after-buying-house/m4sksfer8","date":1751327622,"author":"cfcfcf","guid":178665,"unread":true,"content":"<div><ul><li>After finalising the purchase of a home in Melbourne's northern suburbs, a Melbourne man found something unexpected.</li><li>There had been no mention of the expansive model train network beneath the home's floors.</li><li>Coincidentally, new owner Daniel Xu is a keen train enthusiast and engineer.</li></ul></div><div><a target=\"_self\" data-clickevent=\"{&quot;clickURL&quot;:&quot;/news/article/how-australias-biggest-cities-rank-for-public-transport-access/iy0wrwm4k&quot;,&quot;elementText&quot;:&quot;rail lines&quot;,&quot;clickType&quot;:&quot;clickSource&quot;}\" data-testid=\"internal-link\" href=\"https://www.sbs.com.au/news/article/how-australias-biggest-cities-rank-for-public-transport-access/iy0wrwm4k\"></a></div>","contentLength":281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44429182"},{"title":"Show HN: A continuation of IRS Direct File that can be self-hosted","url":"https://github.com/openfiletax/openfile","date":1751321339,"author":"elijahwright_","guid":178791,"unread":true,"content":"<p>the IRS recently open sourced most of Direct File, a tax tool it has been working on for a few years now. unfortunately, due to recent events, the IRS isn't working on it anymore. I decided to pick up where they left off and I'm trying to get it ready for next tax season</p>","contentLength":271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44428438"},{"title":"The new skill in AI is not prompting, it's context engineering","url":"https://www.philschmid.de/context-engineering","date":1751316835,"author":"robotswantdata","guid":176933,"unread":true,"content":"<p>Context Engineering is new term gaining traction in the AI world. The conversation is shifting from \"prompt engineering\" to a broader, more powerful concept: . <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://x.com/tobi/status/1935533422589399127\">Tobi Lutke</a> describes it as \"the art of providing all the context for the task to be plausibly solvable by the LLM.” and he is right.</p><p>With the rise of Agents it becomes more important what information we load into the “limited working memory”. We are seeing that the main thing that determines whether an Agents succeeds or fails is the quality of the context you give it. Most agent failures are not model failures anyemore, they are context failures.</p><p>To understand context engineering, we must first expand our definition of \"context.\" It isn't just the single prompt you send to an LLM. Think of it as everything the model sees before it generates a response.</p><ul><li><strong>Instructions / System Prompt:</strong> An initial set of instructions that define the behavior of the model during a conversation, can/should include examples, rules ….</li><li> Immediate task or question from the user.</li><li><strong>State / History (short-term Memory):</strong> The current conversation, including user and model responses that have led to this moment.</li><li> Persistent knowledge base, gathered across many prior conversations, containing learned user preferences, summaries of past projects, or facts it has been told to remember for future use.</li><li><strong>Retrieved Information (RAG):</strong> External, up-to-date knowledge, relevant information from documents, databases, or APIs to answer specific questions.</li><li> Definitions of all the functions or built-in tools it can call (e.g., check_inventory, send_email).</li><li> Definitions on the format of the model's response, e.g. a JSON object.</li></ul><h2><a href=\"https://www.philschmid.de/context-engineering#why-it-matters-from-cheap-demo-to-magical-product\" aria-hidden=\"true\" tabindex=\"-1\"></a>Why It Matters: From Cheap Demo to Magical Product</h2><p>The secret to building truly effective AI agents has less to do with the complexity of the code you write, and everything to do with the quality of the context you provide.</p><p>Building Agents is less about the code you write or framework you use. The difference between a cheap demo and a “magical” agent is about the quality of the context you provide. Imagine an AI assistant is asked to schedule a meeting based on a simple email:</p><blockquote><p>Hey, just checking if you’re around for a quick sync tomorrow.</p></blockquote><p> has poor context. It sees only the user's request and nothing else. Its code might be perfectly functional—it calls an LLM and gets a response—but the output is unhelpful and robotic:</p><blockquote><p>Thank you for your message. Tomorrow works for me. May I ask what time you had in mind?</p></blockquote><p> is powered by rich context. The code's primary job isn't to figure out  to respond, but to  the LLM needs to full fill its goal. Before calling the LLM, you would extend the context to include</p><ul><li>Your calendar information (which shows you're fully booked).</li><li>Your past emails with this person (to determine the appropriate informal tone).</li><li>Your contact list (to identify them as a key partner).</li><li>Tools for send_invite or send_email.</li></ul><p>Then you can generate a response.</p><blockquote><p>Hey Jim! Tomorrow’s packed on my end, back-to-back all day. Thursday AM free if that works for you? Sent an invite, lmk if it works.</p></blockquote><p>The magic isn't in a smarter model or a more clever algorithm. It’s in about providing the right context for the right task. This is why context engineering will matter. Agent failures aren't only model failures; they are context failures.</p><h2><a href=\"https://www.philschmid.de/context-engineering#from-prompt-to-context-engineering\" aria-hidden=\"true\" tabindex=\"-1\"></a>From Prompt to Context Engineering</h2><p>What is context engineering? While \"prompt engineering\" focuses on crafting the perfect set of instructions in a single text string, context engineering is a far broader. Let's put it simply:</p><blockquote><p>Context Engineering is the discipline of designing and building dynamic systems that provides the right information and tools, in the right format, at the right time, to give a LLM everything it needs to accomplish a task.</p></blockquote><ul><li> Context isn't just a static prompt template. It’s the output of a  that runs  the main LLM call.</li><li> Created on the fly, tailored to the immediate task. For one request this could be the calendar data for another the emails or a web search.</li><li><strong>About the right information, tools at the right time:</strong> The core job is to ensure the model isn’t missing crucial details (\"Garbage In, Garbage Out\"). This means providing both knowledge (information) and capabilities (tools) only when required and helpful.</li><li><strong>where the format matters:</strong> How you present information matters. A concise summary is better than a raw data dump. A clear tool schema is better than a vague instruction.</li></ul><p>Building powerful and reliable AI Agents is becoming less about finding a magic prompt or model updates. It is about the engineering of context and providing the right information and tools, in the right format, at the right time. It’s a cross-functional challenge that involves understanding your business use case, defining your  outputs, and structuring all the necessary information so that an LLM can “accomplish the task.\"</p><p>This overview was created with the help of deep and manual research, drawing inspiration and information from several excellent resources, including:</p><p>Thanks for reading! If you have any questions or feedback, please let me know on <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://twitter.com/_philschmid\">Twitter</a> or <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://www.linkedin.com/in/philipp-schmid-a6a2bb196/\">LinkedIn</a>.</p>","contentLength":5105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44427757"},{"title":"Next month, saved passwords will no longer be in Microsoft’s Authenticator app","url":"https://www.cnet.com/tech/microsoft-will-delete-your-passwords-in-one-month-do-this-asap/","date":1751311892,"author":"ColinWright","guid":182817,"unread":true,"content":"<p>Starting this month, you'll no longer be able to use Microsoft Authenticator's autofill password function, a move the company is making to transition from passwords to passkeys. Last month, Microsoft stopped letting you save new passwords in the app.</p><p>Next month is the biggest change, all your saved passwords will no longer be in the Authenticator app. You'll have to use passkeys instead -- such as a PIN, fingerprint or facial recognition.&nbsp;</p><p>Attila Tomaschek, CNET's software senior writer and digital security expert, believes passkeys are a safer alternative to the risky password habits that 49% of US adults have, according to a <a href=\"https://www.cnet.com/tech/services-and-software/password-survey-2025/#link=%7B%22role%22:%22standard%22,%22href%22:%22https://www.cnet.com/tech/services-and-software/password-survey-2025/%22,%22target%22:%22%22,%22absolute%22:%22%22,%22linkText%22:%22recent%20survey%22%7D\" target=\"_self\">recent CNET survey</a>.&nbsp;</p><p>\"Passwords can be cracked, whereas passkeys need both the public and the locally stored private key to authenticate users, which can help mitigate risks like falling victim to phishing and brute-force or credential-stuffing attacks,\" said Tomaschek.&nbsp;</p><p>Using the same password for several accounts or adding personal hints can be a convenient way to remember your login. But it's a big risk for scammers, identity theft and fraud. Here's more on Microsoft's plan for eliminating passwords and how to make the switch to passkeys before August.&nbsp;</p><h2>Microsoft Authenticator will stop supporting passwords</h2><p>Microsoft Authenticator houses your passwords and lets you sign into all of your Microsoft accounts using a PIN, facial recognition such as Windows Hello, or other biometric data, like a fingerprint. Authenticator can be used in other ways, such as verifying you're logging in if you forgot your password, or using two-factor authentication as an extra layer of security for your Microsoft accounts.In June, Microsoft stopped letting users add passwords to Authenticator, but here's a timeline of other changes you can expect, according to <a href=\"https://support.microsoft.com/en-us/account-billing/changes-to-microsoft-authenticator-autofill-09fd75df-dc04-4477-9619-811510805ab6\" target=\"_self\">Microsoft</a>.</p><ul><li>&nbsp;You won't be able to use the autofill password function.</li><li>&nbsp;You'll no longer be able to use saved passwords.</li></ul><p>If you still want to use passwords instead of passkeys, you can store them in Microsoft Edge. However, CNET experts recommend adopting passkeys during this transition.&nbsp;</p><p>\"Passkeys use public key cryptography to authenticate users, rather than relying on users themselves creating their own (often weak or reused) passwords to access their online accounts,\" said Tomaschek.</p><h2>Why passkeys are a better alternative to passwords</h2><p>So what exactly is a passkey? It's a credential created by the Fast Identity Online Alliance that uses biometric data or a PIN to verify your identity and access your account. Think about using your fingerprint or Face ID to log into your account. That's generally safer than using a password that is easy to guess or susceptible to a phishing attack.</p><p>Passkeys aren't stored on servers like passwords. Instead, they're stored only on your personal device. More conveniently, this takes the guesswork out of remembering your passwords and the need for a .</p><h2>How to set up a passkey in Microsoft Authenticator</h2><p>Microsoft said in a <a href=\"https://www.microsoft.com/en-us/security/blog/2025/05/01/pushing-passkeys-forward-microsofts-latest-updates-for-simpler-safer-sign-ins/\" target=\"_self\">May 1 blog post</a> that it will automatically detect the best passkey to set up and make that your default sign-in option. \"If you have a password and 'one-time code' set up on your account, we'll prompt you to sign in with your one-time code instead of your password. After you're signed in, you'll be prompted to enroll a passkey. Then the next time you sign in, you'll be prompted to sign in with your passkey,\" according to the blog post.</p><p>To set up a new passkey, open your Authenticator app on your phone. Tap on your account and select \"Set up a passkey.\" You'll be prompted to log in with your existing credentials. After you're logged in, you can set up the passkey.</p>","contentLength":3623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426985"},{"title":"The original LZEXE (A.K.A. Kosinski) compressor source code has been released","url":"https://clownacy.wordpress.com/2025/05/24/the-original-lzexe-a-k-a-kosinski-compressor-source-code-has-been-released/","date":1751311192,"author":"elvis70","guid":178664,"unread":true,"content":"<p><a href=\"https://forums.sonicretro.org/index.php?posts/1078212/\">Last year</a>, I discovered that the Kosinski compression format is actually LZEXE, which was used for compressing DOS executables back in the 90s and the late 80s. Its developer catalogues three versions on <a href=\"https://bellard.org/lzexe/\" target=\"_blank\" rel=\"noreferrer noopener\">his website</a>: v0.90, v0.91, and v0.91e. While only binaries of v0.91 and v0.91e can be found on the website, v0.90 can be found mirrored on various <a href=\"http://justsolve.archiveteam.org/wiki/LZEXE\" target=\"_blank\" rel=\"noreferrer noopener\">other websites</a>.<p>I got in touch with LZEXE’s developer, Fabrice Bellard, and he was able to release </p><a href=\"https://bellard.org/lzexe/\" target=\"_blank\" rel=\"noreferrer noopener\">LZEXE’s source code</a>, untouched since 1990! It is released under the terms of the MIT licence, allowing it to be freely used in other projects. To maximise performance, the compression logic was written in x86 assembly, while its frontend was written in Pascal. This particular source code appears to be for v0.91.<a href=\"https://forums.sonicretro.org/index.php?threads/accurate-kosinski-compressor.40558/\">my own Kosinski compressor</a> which produced identical data to what could be found in the Mega Drive Sonic games. At the time, I noticed that it did not accurately reproduce the Mega CD BIOS’s compressed Sub-CPU payload data. The inaccuracies were so extensive that it appeared that the BIOS’s data was compressed with a different tool to the Sonic games. Notably, the compressor which was used for the Sonic games suffered from a number of bugs and shortcomings, causing the compressed data to less efficient than it should have been. The Mega CD BIOS developers may have used a different version of the compressor, which lacked these bugs, or which had additional bugs.<p>With this in mind, the source code which has been released may not be for the exact compressor which was used by the Sonic games, though it could be modified to function identically to it. Since the compression logic was written in assembly, it should be simple enough to disassemble the compressor executables and compare them to the source code. Devon did the heavy-lifting of extracting and unpacking the core logic, which </p><a href=\"https://forums.sonicretro.org/index.php?posts/1079853/\">can be found here</a>.<p>With that, we now have the source code of two of the four ‘KENS’ format compressors – Kosinski </p><a href=\"https://forums.sonicretro.org/index.php?threads/i-found-the-saxman-compressor-source-code.39261/\">and Saxman</a>! Unfortunately, I do not have much hope of ever finding the original compressors for, let alone the source code of, the remaining two formats – <a href=\"https://segaretro.org/Enigma_compression\">Enigma</a> and <a href=\"https://segaretro.org/Nemesis_compression\">Nemesis</a> – due to them evidently being custom formats which were designed specifically for the Mega Drive, likely meaning that the compressors and their source code never left the hands of Sega (Enigma encodes plane map data, operating on 16-bit words and specifically acknowledging the separation of bits of the tile’s index from its X/Y flip, palette line, and priority; meanwhile Nemesis encodes tiles, operating on nibbles and bunching data into groups of 32 bytes (8 x 8 4-bit nibbles).</p>","contentLength":2645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426864"},{"title":"End of an Era","url":"https://www.erasmatazz.com/personal/self/end-of-an-era.html","date":1751311031,"author":"marcusestes","guid":177019,"unread":true,"content":"<p>I recall saying to one of my colleagues at Atari way back in 1982 that I wanted to make a game that would be genuine art. A year later I built a game that was my first experiment in that direction: Gossip. It was a ridiculously simple game in which a player attempted to win favor in a group by calling people and telling them how much you liked or disliked some third party. The underlying concept was that “people like people who like people they like.” For some reason, many players had problems absorbing this simple concept.&nbsp;</p><p>I later incorporated many of the ideas in Gossip into Excalibur, my first Arthurian game. But Atari collapsed soon after that, and I had to make a living as a freelance game designer. That kept me busy for the rest of the 1980s, but after I published my last game, Balance of the Planet, in 1990, I resolved to come back to the computer-games-as-art goal. I wanted to make another Arthurian game with much greater emphasis on the interpersonal interactions. I knew that my ambitions would take several years to realize, but I felt that the future lay in this direction, so I went ahead. I started building the game, and it went well. I began approaching publishers, and struck out everywhere. They greatly respected my reputation as one of the top game designers in the world, but they had decided that the only games that would sell had to be variations on Doom or Myst.&nbsp;</p><p>Then the Markle Foundation came to the rescue with an offer that was to bail me out in the short run and send me down the wrong path in the long run. They offered me $350,000 to build a tool for other people to create interactive storyworlds. The idea appealed to me, so I accepted the offer and went to work. Now, most of the money went to various contractors, even though I designed and wrote all the code. The result was the Erasmatron. I don’t have a screenshot of the Erasmatron user interface. My primary goal was to build software that would permit a storyteller to implement many of the dramatic processes that take place in a story. For example, there was a component that permitted one character to spy on two other characters conversing without being seen. There was an extensive system for managing how information traveled through a group of people and how secrets were kept or broken.</p><p>I won’t dwell on Erasmatron, as it evolved into Storytron, but I will note that Erasmatron had no takers. Other than Laura Mixon, nobody ever built anything with Erasmatron.</p><p>This was the culmination of my effort to build a software development environment for interactive storytelling. It is best understood as Erasmatron with a much superior user interface and considerably better support features.</p><p>The central component of Storytron was Deikto, a technology for creating a toy language specific to an interactive storyworld. “Toy language” is my term for a tiny language containing only the words necessary to permit interaction between characters in the storyworld. Toy languages do not need anywhere near as large a vocabulary as a full language. A storyworld for children will have no verbs for sex, higher education, jobs, finance, marriage, alcohol, and many other things. Similarly, a storyworld about corporate politics can dispense with scientific verbs, most economic and financial verbs, verbs for family interactions, and not much about food. A good storyworld designer can, in my estimate, build an adequate toy language for most storyworlds with only a few hundred verbs.&nbsp;</p><p>The verbs are the core of the system; the player can build sentences out of the verbs and all the other words in the system. There are words for actors, props, and stages.</p><p>This description is growing too large. Storytron technology had many other wonderful features that I won’t describe here. The important point I want to make is that nobody was interested in Storytron. I spread the word about it as well as I could, but I’m no salesman. I spent about ten years on Storytron and a great deal of my money hiring contractors to do some of the work that I couldn’t do. And it was all for nothing. Storytron was just too complicated for the audience. I don’t think that was because it was intrinsically too complicated for anybody to understand. My impression is that there just weren’t any people willing to make the big commitment required to learn how to use Storytron. It was easier to learn than professional programming systems like Eclipse or the Microsoft suite of software development applications. But it demanded more of its users than they were willing to invest.</p><p>I made one last effort to make Storytron work using Siboot, a concept that I had developed in 1987. I poured my energy into Siboot, and a number of good people helped me, but after I had expended several years on it, I realized that it was crap. The story felt too mechanical. I realized that it needed a boost in the form of the encounter technology that I had developed for the 1987 version, but at that point I was so discouraged that I just couldn’t go on. I gave up on Siboot.</p><p>I gave up on Storytron around 2018. It was painful to accept that all the energy, all the creativity, all the sweat I had committed to the project was for naught, but I had no choice. I rested for some months, then in 2020, for my 70th birthday, I realized that I was growing old and would not be able to handle a tough technical challenge for much longer. I therefore decided that the time had come for me to make one last effort, and that effort had to an Arthurian game. I re-read the many Arthurian books I had accumulated during previous efforts, girded my loins, and set to work. I made many changes along the way; the final version of Le Morte d’Arthur was quite unlike the original. But it worked. I knew that, after all these years, I had finally achieved my goal of making genuine interactive art. I was proud, tired, and gratified. Not many people played the storyworld, but I didn’t care. That was the world’s failure, not mine.&nbsp;</p><p>I continued to fiddle around with interactive storytelling, discussing issues with a small group of people devoted to the problem. I even made a few attempts to make the technology I used for Le Morte d’Arthur available to others, but, once again, nobody was interested.</p><p>Late in 2024 I happened upon a mention of Narrascope, an annual conference for interactive fiction held once every June. I knew of the conference from previous references, and it occurred to me that I had one last shot at making interactive storytelling technology available to the world. The attendees of Narrascope were not the techie types I had previously dealt with. These were mostly storytellers, weaker on the technology but stronger on the storytelling side. I decided to make my technology available to them, but to do so I would have to strip away all the technical complexity. I set to work building a web page that could edit storyworlds, using HTML, CSS, Javascript, and JSON. My programming powers were fading fast. Time and time again I would send my friend Dave Walker an email declaring that Javascript (or something else) was utterly broken, incapable of executing the simplest program without errors. Dave would ask to see the source code and I would present it to him with detailed notes proving that my code was perfect and Javascript was broken. He’d call me, we’d discuss it, and eventually he’d say something like, “Where did you terminate the loop beginning at line 563?” There would be a long silence, followed by the tiniest  from me. I’d thank him for his help and hang up. A week later, I’d be fuming again about another fundamental flaw in Javascript.&nbsp;</p><p>Narrascope had accepted my lecture proposal, as well as my request to deliver a workshop on my technology. I spent dozens of hours working on the lecture; my lectures have always been top-notch and I wasn’t about to scrimp on this one. I made scores of nifty-keen images to illustrate my points. When will people learn that text doesn’t belong on a slide???</p><p>Meanwhile, I struggled with the program. I didn’t quite get it finished, but it was workable and users could readily see that it was close to completion.&nbsp;</p><p>On the big day I arrived at the airport at 5:00 AM to catch the early flight. We sat on the tarmac for an hour because of a mechanical problem, at which point I realized that I could not possibly make a crucial connection. I had to abort the trip to Narrascope and deliver the lectures via video, which turned out to be disastrous.&nbsp;</p><p>This was my last-gasp effort to stimulate progress in interactive storytelling. \"Once more, into the breach!” I had told myself. Now, more than a week after I delivered my spiel, not one person has answered my call for emails expressing some interest in my technology. Once again, my efforts were in vain.</p><p>And so it is time for me to admit that, after all those decades of work, I have failed, with the single exception of Le Morte d’Arthur. When I designed for myself, I succeeded. When I designed for others, I failed. It’s time to throw in the towel and leave interactive storytelling to others. I don’t think that the world is ready. I feel like Charles Babbage, who invented the programmable computer in 1850. It used gears, levers, and cams and was brilliant. But the world had no need for programmable computers in 1850, so he never got the funding to build his invention. I’m nowhere near as smart as Charles Babbage, but my life dimly echoed his.&nbsp;</p><p>I realized that my opus magnus, Le Morte d’Arthur, is a metaphorical autobiography of sorts. At the least, it expresses my experience working on interactive storytelling. Here is Merlin’s final conversation with Arthur:</p><p>The time has come to close this chapter of my life. Perhaps I shall write a book summarizing my findings. Perhaps I shall not.</p>","contentLength":9850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426845"},{"title":"Xfinity using WiFi signals in your house to detect motion","url":"https://www.xfinity.com/support/articles/wifi-motion","date":1751310230,"author":"bearsyankees","guid":176932,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426726"},{"title":"The hidden JTAG in a Qualcomm/Snapdragon device’s USB port","url":"https://www.linaro.org/blog/hidden-jtag-qualcomm-snapdragon-usb/","date":1751308467,"author":"denysvitali","guid":177018,"unread":true,"content":"<p>EUD stands for Embedded USB Debug: essentially, this is a debug interface built right into almost every Qualcomm SoC since ~2018. Internally it hooks deep into the SoC, providing debug facilities for not just the CPUs but also the myriad of Hexagon co-processor/DSPs; many of the exciting details can be found in <a href=\"https://patents.google.com/patent/US20160124822A1/en\">this patent from way back in 2014</a>.</p><p>In practise, for a non-production device (like a dev board, though some production devices seem to work too), EUD can be enabled by writing a few registers and then starting up the USB phy (though the details vary by generation). Instead of whatever typical gadget you might expect, what appears on your PC is a 7-port USB hub, with 1 port populated by the “EUD control interface”.</p><p>With the right USB commands, a second device will appear, this one exposes an SWD interface! Yes! SWD right over the USB cable, no external tools, no soldering, and no expensive debuggers. Closed case debug that (almost) puts Google’s Suzy-Q to shame!</p><p>For those unfamiliar: JTAG and SWD are both mechanisms for debugging the CPU cores inside a device, just like you can use GDB to debug programs on your computer (or your IDEs integrated debugger). They let you set breakpoints, halt execution, inspect the registers, single step instructions and all sorts of other useful things.</p><p>For quite a while there has been a tantalising <a href=\"https://git.codelinaro.org/clo/la/openocd-org/openocd/-/commits/qcom_changes/?ref_type=heads\">fork of openOCD published by Qualcomm on CodeLinaro</a>, promising EUD integration. However, it relied on an at-the-time proprietary EUD library, which was only available to Qualcomm employees and their OEM partners.</p><p>The device-side part of this (enabling the EUD interface so it shows up on your PC) has been somewhat supported in upstream Linux for a while. Back in August last year there was an attempt to extend this support for some newer platforms which have additional requirements. This <a href=\"https://lore.kernel.org/all/622c0fd6-e4e2-6597-d0a2-ff449d7d2f59@quicinc.com/\">sparked some discussion</a> over the kernel policy: is it acceptable to have drivers in Linux that are only usable by some internal software, gatekept for Qualcomm and their paying partners? The answer appeared to be no, and this seemed to be enough to push Qualcomm in the right direction as after 8 months of silence, here we are!</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>Let’s be fair, it almost definitely builds just fine on Ubuntu 20.10 with Qualcomm’s GCC 8.x toolchain. But that’s not what most people are using, we have to fix this!</p><p>It turns out to be not too bad, just some minor stuff. Somehow they have  and  enabled though, and there is no way we’re gonna get that all passing just yet.</p><p>With everything building, the necessary fixes (and a shiny new ) have been submitted to Qualcomm’s repo <a href=\"https://github.com/quic/eud/pull/2\">here</a>.</p><p>Now we have EUD building, we can try it with OpenOCD. It looks like they based their changes on the latest OpenOCD release 0.12.0, very nice. But wait, this release came out in 2023, and OpenOCD is still in active development… So there’s 2 years worth of changes, and</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>Almost 11k commits! It would really be nice to get this upstream eventually, so maybe let’s just rebase it real quick, we need to point it at the cleaned up EUD fork anyways.</p><p>Among Qualcomm’s changes to support EUD, there are also patches adding Hexagon debugging support (and seemingly some improvements for LLDB as well). These got lost along the way but are almost certainly worth looking into at some point.</p><p>So here we are, a fun day of fixing up and rebasing some codebases, and a very tasty reward!</p><pre tabindex=\"0\" data-language=\"plaintext\"><code></code></pre><p>You can find the rebased OpenOCD patches <a href=\"https://github.com/linux-msm/openocd\">over on the linux-msm GitHub</a> along with some quickstart instructions in the README. So far this has been tested on the Snapdragon 845, it should work similarly for the 855 and 865 where we can get away with just poking the enable register and then using Linux or U-Boot to start a USB gadget. Newer SoCs however will probably require additional changes <a href=\"https://lore.kernel.org/all/20240730222439.3469-1-quic_eserrao@quicinc.com/\">like these for SM8450</a>. Let’s hope these old patch series get refreshed now that the tooling side of the story is in better shape!</p><p>Torvalds himself famously doesn’t support the use of debuggers with the kernel (though that certainly hasn’t stopped the wonderful work on kgdb), he <a href=\"https://lkml.org/lkml/2000/9/6/65\">wrote</a> (all the way back in 2000):</p><blockquote><p><em>I don’t like debuggers. Never have, probably never will. I do not condone single-stepping through code to find the bug.</em></p></blockquote><p>So of course, how practically useful JTAG support is really depends on your workflow. In the Qualcomm Landing Team at Linaro, debuggers have never been a staple of our work for all the typical reasons you’d expect (cost and complexity being the main ones), however with more focus being spent on non-kernel things like U-Boot and the secure world this dynamic is shifting.</p><p>U-Boot is an obvious example for us, since it doesn’t currently provide stack traces when it crashes, diagnosis can sometimes be an arduous process which is made infinitely simpler with a .</p><p>We are particularly interested in the possibilities that EUD opens up for debugging a vertically integrated BSP, especially when TF-A, OP-TEE and U-Boot are in the mix via the <a href=\"https://gitlab.com/Linaro/trustedsubstrate/meta-ts\">Trusted Substrate layer for OpenEmbedded</a>. If this is something you’d like to explore with us then don’t hesitate to <a href=\"https://www.linaro.org/contact\">get in touch</a>.</p><p>In addition to the SWD peripheral, there is also a COM (UART) peripheral, and a trace peripheral. These haven’t yet been explored (and aren’t integrated into OpenOCD) but they should allow for a bidirectional serial port and MMIO tracing respectively. These do open up some more interesting use cases around Closed Cased Debugging in production - this appears to have been intentional on Qualcomm’s behalf with EUD being disabled as part of the production signing process, but with the ability to be re-enabled with a (cryptographically validated) “debug policy”.</p><p>Some different SoCs use different addresses for the debug base and CTI base registers, as well as the additional changes required to enable EUD. If you’re able to make this work on your board/SoC, please do <a href=\"https://github.com/linux-msm/openocd/issues/new\">open an issue on the linux-msm fork</a> and let us know what worked for you.</p><p>Additionally, there is a strange quirk where the sticky reset bit of the PRSR register is always set, perhaps relating to SMP. For now the sticky reset behaviour of OpenOCD is <a href=\"https://github.com/linux-msm/openocd/commit/8d154f2270358e3aa35f338c8b6211a5638f22a6\">stubbed out</a> but it would be good to figure out what’s going on.</p><p>SMP support in general is also currently lacking. The config file has <a href=\"https://github.com/linux-msm/openocd/commit/d1a4f90cb70009191da177d80f98332240c68d32\">been updated</a> (using rcar as a reference) to define multiple CPU cores, but this doesn’t seem to behave correctly in Linux. For now it’s recommended to boot with  if you want to actually debug your kernel.</p><p>Whether or not EUD is available on your device seems to depend on a variety of options: there are fuses to configure what debug functionality is allowed, as well as support for an OEM signed “debug policy” which can override this behaviour. On at least one production device (the OnePlus 6) EUD appears to be disabled via fuse, and yet it just works anyway. This device also has “crashdump mode” enabled which is not typical, this suggests that maybe OnePlus shipped the device with a loose debug policy, perhaps by mistake.</p><p>Lastly, while it is of course extremely useful to have proper JTAG for debugging the kernel (especially when it’s so effortless!). The obvious question is: can this be used to gain control of higher execution levels? And unfortunately the answer appears to be no. If you do manage to halt execution in EL2, all registers will read as 0, and not much seems to be possible, at least on a production device. If your board behaves differently do let us know!</p><p>EUD gives us a huge new surface to explore, and offers the potential to greatly improve the experience of low level debugging on Qualcomm boards. We are extremely excited that this is now published and freely available to use, and we very much hope it will become a seamless experience as the tooling and drivers are better integrated.</p><p>It is awesome to see Qualcomm’s commitment to improving the developer experience and making their platforms more open is continuing to be demonstrated in their actions, EUD has the potential to save huge amounts of money on expensive debugging equipment, drastically reduce set-up times and make remote debugging easier too (no doubt it will eventually be integrated into our existing <a href=\"https://github.com/linux-msm/cdba\">remote debugging</a> tooling). Quite simply this raises the foundations for anyone working on Qualcomm platforms, and we can’t wait to see what’s next.</p>","contentLength":8338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426428"},{"title":"Datadog's $65M/year customer mystery solved","url":"https://blog.pragmaticengineer.com/datadog-65m-year-customer-mystery/","date":1751308269,"author":"thunderbong","guid":176931,"unread":true,"content":"<p><em>The internet has been speculating the past few days on which crypto company spent $65M on Datadog in 2022. I confirmed it was Coinbase, and here are the details of what happened. Originally published on 11 May 2023.</em></p><p><a href=\"https://twitter.com/gergelyorosz?ref=blog.pragmaticengineer.com\"></a><em> with a bonus, free issue of the Pragmatic Engineer Newsletter. We cover one out of six topics in today’s subscriber-only <a href=\"https://newsletter.pragmaticengineer.com/p/the-scoop-47?ref=blog.pragmaticengineer.com\">The Scoop issue</a>. To get full newsletters twice a week, </em><a href=\"https://newsletter.pragmaticengineer.com/about?ref=blog.pragmaticengineer.com\"></a></p><p>Datadog is a leading observability tooling provider which went public in 2019, with a current market cap of $28B. The company made $1.67B revenue in 2022, circa $140M per month. On an earnings call a week ago, on 4 May, the CFO mentioned a “large upfront bill that did not recur,” saying:</p><blockquote>“Billings were $511 million, up 15% year-over-year. We had a large upfront bill for a client in Q1 2022 that did not recur at the same level or timing in Q1 2023. Pro forma for this client, billings growth was in the low 30s percent year-over-year.”</blockquote><p>If you’re like me, you’d probably skim over this detail, as it’s 15% here, 30% there. However, analysts attend these calls whose bread and butter is crunching the numbers and figuring out what a company might be trying to hide. A JP Morgan stock analyst did just this, quickly crunching numbers and asking the question:</p><blockquote>“David, looking at the math on this large upfront bill that did not recur, it seems to be about $65 million, if I'm running that correctly. Can you possibly shed a little more light?“</blockquote><p>Datadog’s CFO, David Obstler gave more details:</p><blockquote>“That was a crypto company which continues to be a customer of ours. But that was an early optimizer. We had always talked about some of the industries that were most affected and optimized.“</blockquote><p>So, who is this mysterious crypto company? Investor Turner Novak <a href=\"https://twitter.com/TurnerNovak/status/1654577231937544192?s=20&amp;ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">speculated</a> that it’s Coinbase:</p><p>He added he doesn’t know for certain that it is Coinbase, as other crypto companies have also raised silly amounts of money in the past several years.</p><p>So, did Coinbase spend $65M on Datadog in 2022? Online there’s no shortage of theories, or people pretending to be Coinbase employees, such as this anonymous commenter on Hacker News, <a href=\"https://news.ycombinator.com/item?id=35866061&amp;ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">claiming</a> that the $65M was for a 3-year upfront payment (which information I could not verify). I wanted to find the truth, so I tracked down software engineers at the company. And I got my answer:</p><p><strong>Yes. Coinbase spent $65M with Datadog in 2021, and this was their due bill for that year. </strong>I can confirm this, having talked with both current and former software engineers at Coinbase who shared details of what happened.</p><p>Here’s how Datadog’s CEO explained, on the earnings call on what happened:</p><blockquote>“This is one of those situations where this customer was in an industry that got pretty much decimated over the past year. And their own business was cut in three or four, in terms of the revenue. And when that's the case, we really work with customers to restructure their contracts with us. We want to be part of the solution for them, not part of the problem (...) We restructure their contract, so we keep them as a happy customer for many more years and do a deal that works for everyone with their business profile.”</blockquote><p>And here’s what actually happened, as I understand from talking with engineers at Coinbase.</p><p>Coinbase had an incredible 2021 and did not have to care about costs. The company went public in June that year, and was valued at an eye-popping $86B. In comparison, nearly two years later the company is valued around $14B, a 75% decline.</p><p>During the boom, trading volumes were surging, beating record after record, and Coinbase could barely keep up. Here’s how Coinbase CEO Brian Amstrong summarized it:</p><blockquote>“So, obviously 2021 was just an incredible year for Coinbase, the kind of thing that you see very rarely in your lifetime, in a business career (...) We hit an all time high in our monthly transacting users of 11.4 million, which is 4x year-over-year, 400% pretty incredible.”</blockquote><p>Following the IPO in summer 2021, nobody at the company cared about infra costs; the only focus was growth. The company racked up huge bills for the likes of AWS, Snowflake, and also Datadog. And so, the $65M bill was for Datadog, for 2021. Coinbase settled the bill in Q1 2022.</p><p><strong>In early 2022 Coinbase suddenly needed to cut back infra spending. </strong>The crypto industry hit a sudden chill, affecting Coinbase’s business. As revenue dried up, the company turned its attention to reducing its overly high costs.</p><p>For observability, Coinbase spun up a dedicated team with the goal of moving off of Datadog, and onto a Grafana/Prometheus/Clickhouse stack. A quick summary of these technologies:</p><p><a href=\"https://prometheus.io/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\"></a><strong>: a time series database. </strong>A very popular open-source solution for systems and services monitoring. Prometheus collects metrics from configured targets (services) at given intervals. It evaluates rules and can trigger alerts. It’s mostly written in Go, with some Java, Python and Ruby parts. Prometheus stores time series in-memory and on storage (HDD or SSD), using an efficient and custom format, and supports <a href=\"https://en.wikipedia.org/wiki/Shard_(database_architecture)?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">sharding</a> and <a href=\"https://en.wikipedia.org/wiki/Federated_database_system?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">federation</a>.</p><p>Prometheus <a href=\"https://www.cncf.io/projects/prometheus/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">is part</a> of the Cloud Native Foundation, membership of which indicates that it’s safe to build on top of Prometheus, as it’s actively maintained and will continue to be.</p><p>Prometheus can be self-hosted, but several cloud providers also offer managed Prometheus services: both Google Cloud and AWS have this service in production, while Azure has it in preview.</p><p><a href=\"https://grafana.com/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\"></a><strong>: the frontend for visualizing metrics. </strong>Grafana is a popular source analytics and monitoring visualization solution. If you need to display or dive into metrics or alerts, it’s the go-to tool, and widely used across tech companies. When I was at Uber, Grafana powered many of our graphs. Here’s an example of Grafana dashboards you can <a href=\"https://play.grafana.org/d/000000012/grafana-play-home?orgId=1&amp;ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">try out</a>:</p><p><a href=\"https://clickhouse.com/?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\"></a>: log management. A fast and open-source column-oriented database management system, which is a popular choice for log management. Clickhouse is written predominantly in C++, and is widely used across the industry. For example, Cloudflare uses Clickhouse to store all its DNS and HTTP logs – which is more than 10M rows per second! – and Uber uses Clickhouse as its central logging platform.</p><p>Coinbase spun up its in-house approach without the main goal of saving costs, but to have full control and ownership of observability. Observability and reliability is a major differentiator for Coinbase, as it gives a competitive advantage over rivals.</p><p>However, with the crypto market cooling, costs became a major focus, and it was clear the in-house Grafana/Prometheus solution was much cheaper. The Coinbase team had been double-writing the new stack for months, confirming everything worked well, and ironing out any issues.</p><p>So Coinbase was ready to pull the plug on Datadog, but Datadog saved its customer relationship at the last minute by making Coinbase a very appealing deal it could not refuse. In future, the bill for Datadog would be nowhere near the $65M of 2021. As Brian Amstrong said of the crypto market during 2021, a $65M bill is the kind of thing you see very rarely in a business career.</p><p>I asked an engineer at Coinbase who used the in-house stack and Datadog how they felt about the decision to stay on Datadog. They said it was ultimately the right decision, considering the reasonable costs, and the superior Datadog development experience.</p><p>Coinbase could  have engineered a similar experience in-house. However, to provide a similarly seamless developer experience, would have likely taken tens of engineering years.</p><p><strong>“Expensive” in observability tooling is relative. </strong>Let’s assume that today Coinbase “only” spends, say, $10M per year on Datadog. Is this too much? Looking at the headline number, it’s tempting to think so.</p><p>However, let’s look a level deeper. A platform like Datadog helps prevent outages, detects them instantly, and mitigates them faster. In 2022, Coinbase had 17 outages, totalling about 12 hours of downtime. The company’s daily average revenue is around $9M/day, based on their 2022 earnings.</p><p>Assume that Datadog cuts the number of outages by half, by preventing them with early monitoring. That would mean that without Datadog, we’d look at 24 hours’ worth of downtime, not 12. Let’s also assume that using Datadog results in mitigating outages 50% faster than without - thanks to being able to connect health metrics with logs, debug faster, pinpoint the root cause and mitigate faster. In that case, without Datadog, we could be looking at 36 hours worth of total downtime, versus the 12 hours with Datadog. To put it in numbers: the company would make around $9M in revenue it would otherwise lose, Now that $10M/year fee practically pays for itself!</p><p><strong>What can we learn from Coinbase’s cost reduction exercise? </strong>Vendors are tight-lipped about their customers reducing spend, and it is a lucky coincidence that Datadog gave enough hints to find out who their big “early optimizer” customer was, and find out more details. But is the story of Coinbase a one-off?</p><p>I’m not sure that it is. Three months ago, I covered the trend that <a href=\"https://newsletter.pragmaticengineer.com/p/vendor-spend-cuts?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">Tech companies are aggressively cutting back on vendor spend</a> - and two months later, The Wall Street Journal also <a href=\"https://www.wsj.com/articles/corporate-technology-under-new-scrutiny-amid-recession-fears-a69c6583?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">reported</a> on the same topic. Coinbase, to me, seems to have been early to the cost optimizing trend. However, look closely at the <a href=\"https://newsletter.pragmaticengineer.com/p/vendor-spend-cuts?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">responses</a> I gathered, and “AWS” and “Datadog” are the two most mentioned vendors as targets for cost savings. This is simply because infra and observability costs tend to be the highest and AWS is the leader for cloud infra, and Datadog the leader for observability.</p><p>Datadog CEO Olivier Pomel confirmed that this type of optimization is happening across all of their customers, saying:</p><blockquote>“When we look at our data, when we look at what we hear from the hyperscalers also, we also listen carefully to their commentary on what they foresee in the near future, we don't see anything that gives us confidence that we can call an end to optimization in the next quarter or the quarter after that. So as far as our guidance goes and our plan for the year, we assume that this is going to continue at a similar level for the rest of the year.”</blockquote><p>I have since confirmed several large companies with thousands of engineers building their own Grafana/Prometheus stack, planning to migrate off of their current observability vendor and operate the observability stack themselves. But why is this?</p><p><strong>Above $2-5M/year annual spend is where bringing a vendor in-house tends to come up. </strong>And this is because it is around this number where the cost of hiring a whole team to do what a vendor is doing can  make sense.</p><p>As a rule of thumb, you can get infra costs much lower than what vendors charge. This is because both the vendor, and you are probably using the same Cloud infrastructure provider, which is usually AWS, GCP or Azure. However, you would need to hire and staff a dedicated engineering team to build and run that infra.</p><p>So, from a cost perspective, this is the math problem you need to solve. At what point does is this equation become true:</p><p>$infra_cost + $platform_team_cost &lt; $current_vendor_costs</p><p>In this question, $platform_team_costs will be above $1M, and sometimes above $2M. This is because you need to have a team of 4-5 engineers, plus a manager, and their average total compensation will be somewhere between $150-400K/year, depending on your cost basis.</p><p>So when you have a bill that is above $2-3M/year, it can start to look tempting to build, rather than buy. The economics of this decision start to get down to how high of a margin is the vendor charging on top of raw infra? The curious question with Coinbase is: did they consider building, when talking about such a huge projected cost that could justify having a team?</p><p>In the case of Coinbase, building in-house following a $65M bill was a clear no-brainer. They could hire a team of 10 senior and staff-level engineers in the Bay Area, and still have this team cost less than $5M/year. And they then only need to budget for the infra costs, which they can presumably bring down to low double digits per year.</p><p>Coinbase planned to move off Datadog, but ended up staying. However, it is not the only larger tech company thinking about bringing observability in house. I have another exclusive which even Datadog might not be aware of, yet. This report is about Shopify and its plan to move off Datadog. But could recent layoffs change things? I cover details on this topic in <a href=\"https://newsletter.pragmaticengineer.com/p/the-scoop-47?ref=blog.pragmaticengineer.com\" rel=\"noopener noreferrer nofollow\">the full The Scoop</a>.</p><p><em>This was one out of the five topics covered in this week’s The Scoop. A lot of what I share in The Scoop is exclusive to this publication, meaning it’s not been covered in any other media outlet before and you’re the first to read about it.</em></p><p><em>The full The Scoop edition additionally covers:</em></p><ol><li><strong>Will Shopify migrate onto an in-house observability tool? </strong>Shopify decided to build its own observability platform and migrate off Datadog. This plan looked certain until Shopify cut the very engineering teams that built its new platform. What happens next?.</li><li><strong>Microsoft cuts its compensation targets</strong>. Almost exactly a year ago, Microsoft employees received a welcome surprise: they could expect higher-than-usual compensation increases. Yesterday, another unexpected email came, but its contents were the opposite of last year’s. I talked with managers and engineers at the tech giant for their reaction to disappointing compensation news. .</li><li><strong>Shopify letting go most staff in Germany. </strong>As part of cutting 20% of staff, most people in Germany were made redundant. These layoffs happened a week before a Works Council election in Germany. Is this unlucky timing, or is there more behind the move?.</li><li><strong>Senior compensation trending down in Ukraine. </strong>Ukraine is one of the few countries for which we have access to nationwide data, through job site Djinni. Data for the first part of this year are in, and they point to something not seen recently: senior engineers are making less. Is this a local trend, or could we see it happening in other countries?.</li><li><strong>A follow-up to this week’s public tech company compensation article</strong>. Why was Netflix lower down the list than many software engineers expected? Plus, new details about Roblox and why Jack Dorsey’s total compensation is $2.75. </li></ol>","contentLength":14225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426399"},{"title":"Ask HN: What's the 2025 stack for a self-hosted photo library with local AI?","url":"https://news.ycombinator.com/item?id=44426233","date":1751307050,"author":"jamesxv7","guid":176905,"unread":true,"content":"First of all, this is purely a personal learning project for me, aiming to combine three of my passions: photography, software engineering, and my family memories. I have a large collection of family photos and want to build an interactive experience to explore them, ala Google or Apple Photo features.<p>My goal is to create a system with smart search capabilities, and one of the most important requirements is that it must run entirely on my local hardware. Privacy is key, but the main driver is the challenge and joy of building it myself (an obviously learn).</p><p>The key features I'm aiming for are:</p><p>Automatic identification and tagging of family members (local face recognition).</p><p>Generation of descriptive captions for each photo.</p><p>Natural language search (e.g., \"Show me photos of us at the beach in Luquillo from last summer\").</p><p>I've already prompted AI tools for a high-level project plan, and they provided a solid blueprint (eg, Ollama with LLaVA, a vector DB like ChromaDB, you know it). Now, I'm highly interested in the real-world human experience. I'm looking for advice, learning stories, and the little details that only come from building something similar.</p><p>What tools, models, and best practices would you recommend for a project like this in 2025? Specifically, I'm curious about combining structured metadata (EXIF), face recognition data, and semantic vector search into a single, cohesive application.</p><p>Any and all advice would be deeply appreciated. Thanks!</p>","contentLength":1467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426233"},{"title":"Sony DTC-700 audio DAT player/recorder","url":"https://kevinboone.me/dtc-700.html","date":1751306606,"author":"naves","guid":178663,"unread":true,"content":"<p>Don’t let anyone tell you otherwise: DAT players were fantastic. They\noffered all the advantages of an audio cassette, but with the sound\nquality of a CD. The compact audio cassette was a marvellous invention,\nin its own way; but this technology struggled to provide audio fidelity\nthat would satisfy discerning listeners. Its frequency response was\nlimited, and the unavoidable background hiss was very obvious in quiet\nenvironments. Still, in the 1970s audio cassettes were  way\nmost people listened to music, and I still have a stack of them.</p><p>One thing that made cassettes so popular was that you could record on\nthem. Setting aside the legal issues, you could record from FM radio, or\nfrom vinyl records, or even from microphones. It was easy to make ‘mix\ntapes’ of you favourite tracks, and share them with friends. Cassettes\nwere everywhere – from portable players like the Walkman, to serious\nhardware in hi-fi racks; they were even in cars.\nThere were shops that sold nothing but cassettes, and they sold by the\nmillion.</p><p>Serious hi-fi enthusiasts, however, listened to vinyl records or FM\nradio. There  good-quality cassette decks, but the\n`audiophile’ crowd embraced them with reluctance, if at all. Still, even\nthe most ardent hi-fi junkie couldn’t deny the usefulness of cassettes.\nWhat we needed was something that could record high-quality sources,\nwith no loss of fidelity.</p><p>That’s where DAT, ‘digital audio tape’ comes in. DAT offered digital\nrecording, in a range of qualities, the highest of which exceeded that\nof CD. If you wanted to record from a CD, you could just connect the CD\ntransport’s digital output to the DAT’s digital input, and away you go.\nWell, maybe – more this subject later. Of course, most DAT units could\nrecord from analog sources like radio as well.</p><p>DAT entered the market at about the same time as CD, but was much\nless successful. For all its notional advantages, DAT never really\ncaught on in the domestic market, although it was somewhat more popular\nin professional applications. A companion data storage technology, DDS,\nused the same hardware, and was somewhat more successful although,\nagain, in professional rather than domestic applications.\nSony pulled out of the market in 2005, although I think it was clear\nlong before then that the format was moribund.</p><p>The DTC-700, introduced in 1990, was Sony’s ‘budget’ hi-fi DAT\nplayer/recorder. The more expensive DTC-55ES and DTC-60ES models had\nfancy (and probably snake oil) features like a copper chassis. Yes,\ncopper is a better electrical conductor than steel, but a great chunk of\nsteel like the DTC-700 chassis is a pretty good conductor already. I’ve\nnot been able to find how much a new DTC-700 cost but, even as the\nintroductory model in the range, I imagine it was well into\nsell-a-kidney territory. In 1995, even a five-year-old, second-hand unit\nwas eye-wateringly expensive. These days, you can pick up a refurbished\nunit for about three hundred quid. It’s well worth the money – if you\ncan find tapes. There are lots more digital DDS tapes in circulation\nthat audio tapes; these are not guaranteed to be compatible with audio\nplayers, but early DDS tapes often are.</p><p>The DTC-700 had a flight-deck of controls, because it offered a stack\nof functionality. It had two different digital inputs and an analog\ninput; there was a headphone amplifier with its own volume control; you\ncould skip to specific tracks by their number, or to a particular time;\nand, of course, you could insert the meta-data that made this possible\nwhen you recorded. And, like all serious hi-fi equipment, it had a\nvaccuum-flourescent display, available in different colours. For that\nreal 70s look, you could buy it with mock-walnut case sides.</p><p>Compared to cassettes, DAT recordings sounded fantastic. It wasn’t\nnecessary for the rest of your equipment – amplifier, speakers,\nheadphones – to be of top quality to realize this: the difference\nbetween DAT and cassette was just that striking. In principle, DAT\noffered better-than-CD quality, with its 48kHz sampling rate. In fact,\nDAT set the standard here: 48kHz remains a common sampling rate to this\nday. Folklore has it that Sony was encouraged to adopt 48kHz to make it\nharder to record commercial CDs, which used (and still use) 44.1kHz.\nBack in the 90s, technology hardly existed to resample these different\nformats on-the-fly; eventually, Sony and others started selling DAT\nunits that supported 44.1kHz directly. This wasn’t an entirely welcome\nmove, as I’ll explain later.</p><p>High cost was one of the reasons – perhaps the main reason – why DAT\ndidn’t catch on in the domestic market; but it certainly wasn’t the only\none. Another problem was the lack of original material: recording\nstudios didn’t seem to want to release commercial recordings on DAT.\nTheir reluctance isn’t hard to understand: DAT tapes could be copied an\nunlimited number of times, with no loss of quality. In the the late 80s\nit wasn’t easy to copy a CD onto DAT, because of the different sampling\nrates. But there would have been no such limitation with a DAT-to-DAT\ncopy.</p><p>Representatives of the recording industry were so worried about\nillegal copying that, in the USA and elsewhere, they bullied legislators\ninto placing legal restrictions on the capabilities and sale of DAT\nrecorders. The USA also introducted taxation on the sales of DAT\ndevices, which was supposed to offset the loss in tax revenue that\nillegal copying would create. This made expensive DAT players even more\nexpensive. Sony tamed the objections of the recording industry, to some\nextent, by the simple expedient of buying CBS Records, one of the main\nobjectors. Nevertheless, the DTC-700 still suffers from the anti-copying\nparanoia of the 80s; it will record a CD, but it will write meta-data\nonto the recording to indicate that it’s a copy. The DTC-700, and other\nDAT units of the same vintage, won’t record from another DAT unit, if\nthe meta-data indicates that the source is a copy. There are ways around\nthis limitation, but they’re fiddly.</p><p>Whether illegal copying was a genuine risk or not, there never really\nwas a large selection of original music on DAT. As I recall, there\nwasn’t even a “killer album” for DAT, like Dire Straits’  – an album so popular that people bought CD players just to\nhear it at its best.</p><p>DAT units also tended to have problems with reliability;\nunderstanding why requires a basic understanding of how DAT technology\nworks.</p><p>From a technological perspective, DAT was implemented in an\ninteresting way. “Interesting” in this context means, of course, “weird\nand unreliable”. The DAT tape itself is only 4mm wide – the same as an\naudio cassette. To get sufficient data bandwidth, the tape couldn’t be\nscanned lengthwise, as all previous tape formats were. At the speeds\nthat would have been required, the tape length would have been\nunmanageable. Instead, DAT works in a similar way to a VHS video\nrecorder: the magnetic head is on a rotating drum, aligned at an angle\nto the direction of tape movement. This arrangement allows the whole\nwidth of the tape to be used, not just a couple of narrow strips in the\nmiddle.</p><p>Naturally, the scanning mechanism required close-tolerance alignment\nto operate reliably. Even when adjusted perfectly, the high rate of\nrotation led to mechanical stresses. This was true of VHS as well, but\nVHS players rapidly became throw-away items – eventually nobody really\ncared if they only lasted a year or two. But if you’d just paid the\nprice of a new car for a DAT player, you’d expect a better service life.\nAnd Sony didn’t help itself: the DTC-700 contained a huge number of\nlow-cost, plastic parts in critical locations. A plastic cog might cost\nonly pennies to replace, but stripping the machine down to get to it\ncost a lot more.</p><p>In the end, though, I don’t think it was the price, or the lack of\ncommercial releases, or the questionnable reliability, or the legal\ncomplications that killed off DAT – although all these factors played a\npart. Rather, I think it was just that old bugbear of the consumer\nelectronics industry: market saturation.</p><p>By about 1992, everybody who was ever likely to want a home DAT\nplayer already had one. The format couldn’t readily be improved, because\nit already offered audio fidelity beyond the limits of human hearing. So\nthere wasn’t a “DAT Mark 2” that manufacturers could have sold to eager\ncustomers. If DAT players could have been made more cheaply, this might\nhave expanded the customer base a little. But I doubt that DAT units\ncould ever have become as cheap as cassette players, and certainly not\nas portable, because the electromechanical design was so complex and\nfussy.</p><p>It’s not as if any alternative technology has really presented\nitself. These days, it’s trivially easy to record from digital or analog\nsources, onto hard disk or solid-state storage. Any desktop computer\nwith a soundcard can do this. A number of manufacturers, including Sony,\ndid release self-contained hard-disk audio recorders, but they seem to\nhave enjoyed even less success than DAT. And these days, of course,\nthere’s even less need for such a device than there was in the 90s. If I\nwant to listen to a radio broadcast more than once, I can probably just\nget it from the broadcaster’s website. Some modern radio tuners even\nhave built-in digital recording capabilities. No: if there were any\ndemand for a modern alternative to the DAT recorder, somebody would be\nselling one.</p><p>Many of the audio technologies from my youth have undergone a\nrevivial recently: vinyl records are the obvious example, but even\ncassettes are starting to sell again. Are we likely to see renewed\ninterest in DAT? On the whole, I think probably not. Plenty of people\nlook back with fondness on vinyl and cassette, even on CD; I don’t think\nDAT gives anybody a warm glow.</p>","contentLength":9915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426171"},{"title":"Proton joins suit against Apple for practices that harm developers and consumers","url":"https://proton.me/blog/apple-lawsuit","date":1751306318,"author":"moose44","guid":178662,"unread":true,"content":"<p>Earlier today, Proton filed court papers in the US District Court for the Northern District of California to join an existing class-action lawsuit against Apple. Proton is a plaintiff in the case, but we are representing and suing on behalf of a class of similarly situated developers. Challenging one of the most powerful corporations in the history of capitalism is not a decision we make lightly, but Proton has long championed online freedom, privacy, and security, and we believe this action is necessary to ensure the internet of the future lives up to its potential.</p><h2>Why are we doing this now?</h2><p>We believe that Apple’s conduct, as detailed in the complaint we filed, constitutes further violations of US antitrust law. Without this case, Apple could get away with behavior in the US that is already outlawed in the European Union. If this were to happen, American consumers, and developers focused on the American market, would have to pay higher prices for fewer choices, and be left at a disadvantage.</p><p>There is also urgency to act now because of a <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.courtlistener.com/docket/70356851/korean-publishers-association-v-apple-inc/\">parallel class-action suit</a> by app developers against Apple on May 23, and any settlement there could be binding on all other developers. By joining that lawsuit, we can ensure that this suit will not only be about monetary damages to compensate app developers for the harm caused by Apple’s conduct, but also changes to App Store policies that will improve the state of the internet. We are seeking to permanently end anti-competitive behavior on the App Store, and we are joining this lawsuit to ensure that any future settlement enforces real changes to Apple’s practices and policies to benefit all consumers, developers, and competition, and not just cosmetic changes.</p><p>While the suit does seek monetary damages on behalf of all developers who have been harmed in order to deter future anti-competitive behavior and provide compensation to class members harmed by Apple’s anti-competitive conduct, Proton will donate any money we receive from the lawsuit to organizations fighting for democracy and human rights so that some portion of Apple’s profits made from countries with authoritarian regimes are redirected to freedom. These donations will be coordinated through the nonprofit Proton Foundation, which oversees Proton and ensures that our work always prioritizes the public good over financial gain.</p><p>Apple’s monopoly control of software distribution on iOS devices presents a myriad of problems for consumers, businesses, and society as a whole. Anti-monopoly laws exist because the power gifted by monopoly status inevitably leads to abuse. In the case of oligarchic tech giants, these abuses have wide implications for society, and it’s vital to the future of the internet that they be addressed now.</p><h3>The App Store policies hurt privacy</h3><p>Apple’s App Store policies disproportionately favor the surveillance capitalism business model employed by companies like Meta and Google and therefore entrench an online business model that routinely violates consumers’ personal privacy. All developers are required to pay Apple an annual fee of $99 to be in the App Store, but Apple also takes a 30% cut from payments made through iOS apps, which are forced to use Apple’s payment system.</p><p>Companies that monetize user data in exchange for “free” services that abuse your privacy aren’t affected by this, as they don’t process payments through the App Store. However, privacy-first companies that monetize through subscriptions are disproportionately hit by this fee, putting a major barrier toward the adoption of privacy-first business models. Naturally, these are also the very companies Apple is directly competing with through its disingenuous privacy marketing campaigns. This is a significant driver behind the internet’s descent into widespread surveillance capitalism.</p><h3>Apple’s policies undermine freedom and democracy</h3><p>Apple’s complete control of the App Store has given it a dangerous level of control over app distribution, giving it the power to decide which apps can and cannot be distributed in different markets. Apple argues this control is necessary for security reasons. But the reality is that this has made Apple the single point of failure for free speech and a tool of dictatorships. There have been numerous incidents where Apple has removed or censored apps at the behest of authoritarian governments, in order to continue profiting from those markets.</p><p>For example, the advocacy group <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://en.greatfire.org/\">GreatFire.org</a> publishes important information about the state of censorship in the App Store through its <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://applecensorship.com/news/new-report-unveils-app-censorship-in-chinas-apple-app-store-amid-recent-developments\">AppleCensorship program</a>, which highlights some striking statistics. Sixty-six of the 100 most popular apps worldwide are unavailable to iOS users in China. Additionally, all 240 VPN apps that the group tested were also unavailable to Chinese users. Overall, 27% of apps are missing from the Chinese App Store, more than double the global average of 13%. Many of those missing apps are news apps (including the likes of The New York Times, BBC News, and Reuters) or social networking or messaging apps, strongly implying that this is a matter of censorship, not security. Apple has also been caught removing apps to help suppress protests, such as the <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.theguardian.com/world/2019/oct/10/hong-kong-protests-apple-pulls-tracking-app-after-china-criticism\">2019 case of HKmap.Live</a>, which was removed at the height of the pro-democracy protests in Hong Kong.</p><p><a href=\"https://proton.me/blog/apple-app-store-antitrust\">Proton itself has also been victim of Apple’s censorship</a>. In 2020, Apple threatened to take Proton VPN out of the App Store unless we removed language from our App Store description that said the app could be used to “unblock censored websites.” We don’t question Apple’s right to act on behalf of authoritarians for the sake of profit, but Apple’s monopoly over iOS app distribution means it can enforce this perverse policy on all app developers, forcing them to also be complicit. We believe it is critical for the future of the internet to end the monopoly on app distribution, so that developers and companies who are prepared to fight for democracy can do so.</p><h3>App Store policies lead to a worse user experience</h3><p>Apple’s approach to subscriptions management is designed to ensure it maintains complete control over the relationship between users and developers. To guarantee it gets its 30% cut of subscription revenue, it has imposed ironclad rules that dictate what developers can and cannot say to their users, which has a detrimental impact on the user experience. One basic example of this is that developers cannot tell users that other pricing options or discounts may be available if users upgrade via a website instead of inside the app. Not supporting Apple’s payment system is also considered a violation, which can lead to threats to remove your app, as <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://www.theverge.com/2020/10/8/21506995/apple-forced-in-app-purchase-protonmail-ceo-wordpress-iap\">happened to Proton</a>.</p><p>But this controlling behavior goes even further. Developers are prohibited from linking to their websites at all. Proton cannot even link to FAQ or customer support pages from its apps, as Apple believes it’s possible that users will then navigate from the support page to a pricing page and upgrade their accounts without paying Apple its fee. This has a direct, negative impact on customer experience.</p><p>It’s also impossible for users to manage their subscriptions from multiple devices, as this would necessitate stepping outside Apple’s walled garden and weakening its control over the user. For example, users who upgraded their accounts on the web and then wish to upgrade or downgrade their subscription are not allowed to do so from their iOS devices. It is similarly impossible for users who purchased a subscription on iOS to change the subscription on the web. In a world where most users are accessing their apps and services over multiple devices, this is an unacceptably poor customer experience.</p><p>Apple, however, goes even further in a bid to maintain its monopoly and trap users within the Apple ecosystem. Apple intentionally cripples third-party apps that compete with Apple services by making functionality that is available to Apple apps unavailable to other apps. For instance, there is no way to set Proton Calendar as the default calendar app on iOS. Furthermore, in a bid to prevent data portability, competing cloud storage services like Proton Drive are unable to seamlessly do background processing, while no such restrictions are known to exist for iCloud.</p><p>These examples of coercive behavior illustrate time and time again that Apple is willing to inflict a worse experience and higher prices on consumers out of corporate greed, and it leverages its monopoly control over the App Store to do so.</p><h3>App Store tariffs cause price inflation</h3><p>Apple’s 30% fees act as an artificial and arbitrary tax on internet commerce, which, much like a tariff, serves to raise prices, as part or all of this fee is inevitably passed on to the customer. Apple claims this fee is necessary to pay for the maintenance of the App Store, but evidence presented in the  case indicated that Apple makes a 78% profit on App Store fees, raising the question of whether these fees are really necessary or a clear example of the company profiting from its illegal monopoly.</p><p>The only reason Apple can get away with this behavior is because there’s no competition in iOS app distribution or iOS in-app payments. If you want to provide an app or service to iOS users, you have to go through Apple’s systems, and you have to use Apple’s system for collecting payments. Breaking this monopoly and ending this punitive tax on the internet would allow companies like Proton to collect payments via less expensive methods, enabling the option to pass these savings on to you, and ultimately reducing the prices you pay.</p><p>The remedies we are seeking would address many of the social ills mentioned above, ensuring that the internet of the future can continue to protect privacy and democracy. Mobile apps are now the dominant platform of the internet and the way the bulk of the world interacts with one another and with the web. Even if app stores started out as niche markets, today they are a critical component of the internet and fundamental to democracy. It is more essential than ever that we fight to create mobile ecosystems that are truly free, competitive, and not beholden to whichever dictator corporate leaders are currently bowing down to.</p><p>This is also why we enter this fight not just representing ourselves, but as a class representative, to ensure that the outcome of this litigation will benefit all app developers and users of apps in this market. We expect this to be a difficult fight that could take many years, but our mission to build an internet that serves the interest of all of society affords us no other choice. By bringing this case, we hope to set an important precedent that free people, not monopolies, will dictate the future of the internet.</p><p><em>Proton is being represented by Quinn Emanuel Urquhart &amp; Sullivan LLP and Cohen Milstein Sellers &amp; Toll PLLC. The full complaint in the case of Proton v. Apple can be found <a rel=\"noopener noreferrer\" target=\"_blank\" href=\"https://res.cloudinary.com/dbulfrlrz/images/v1751299117/wp-pme/proton-v--apple-class-action-complaint/proton-v--apple-class-action-complaint.pdf?_i=AA\">here</a>.</em></p>","contentLength":10993,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44426128"},{"title":"That XOR Trick (2020)","url":"https://florian.github.io//xor-trick/","date":1751304761,"author":"hundredwatt","guid":182731,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44425880"},{"title":"I write type-safe generic data structures in C","url":"https://danielchasehooper.com/posts/typechecked-generic-c-data-structures/","date":1751302520,"author":"todsacerdoti","guid":176848,"unread":true,"content":"<p>I write type safe generic data structures in C using a technique that I haven’t seen elsewhere. It uses unions to associate type information with a generic data structure, but we’ll get to that. My approach works for any type of data structure: maps, arrays, binary trees… but for this article I illustrate the ideas by implementing a basic linked list. Since many people aren’t aware you can do C generics , I figured I’d start simple and build up to this:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>I hesitate to even mention this, because I do not like it, but its worth comparing to the technique at the end of this article. It works like this: you write your data structure in a header, using macros for your types, and then  the header multiple times; once for each type the data structure will be used with.</p><p>While it  generic and type safe, it has downsides:</p><ul><li>makes it hard to find where types and functions are defined (because they’re constructed by macros)</li><li>code completion may not handle them well</li><li>bloats your binary size and build times with copies of the same functions</li><li>requires using type-prefixed functions: <code>Foo_list_prepend() and int_list_prepend()</code> vs just </li></ul><p>Another way to make a data structure generic is to use . It’s not type safe but we’ll get to that.</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Note:  is used for familiarity, but I highly recommend Arenas instead. You can <a href=\"https://www.youtube.com/watch?v=TZ5a3gCCZYo\" target=\"_blank\" rel=\"noopener\">watch</a> or <a href=\"https://www.rfleury.com/p/untangling-lifetimes-the-arena-allocator\" target=\"_blank\" rel=\"noopener\">read</a> about them.</p><p>Having  and its  as separate allocations isn’t ideal from a memory and performance perspective. It requires 2 allocations per node when one would do, the  pointer uses memory unnecessarily, and you will likely get two cache misses per node when traversing the list: once getting the next node, and once getting its data. We can fix these issues with…</p><h3>Generics level 2: Inline storage</h3><p>Instead of storing a pointer to the node’s data, we can use a <a href=\"https://en.wikipedia.org/wiki/Flexible_array_member\" target=\"_blank\" rel=\"noopener\">Flexible Array Member</a> to store the data inside the node. To do so, we make a single allocation large enough for both the node and the type it stores:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Now  and the actual contents of  are beside each other in memory, solving the issues of the  approach. Unfortunately we now have to pass the size, but we’ll fix that in the next section</p><p>If you wanted to avoid the , and initialize the node’s memory directly, you could do so with a  function:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><h3>Generics level 3: Type Checking</h3><p>The part you’ve all been waiting for: how to get the compiler to error when we try to add the wrong type to a list. The way I found to do this is to use a union with a  member that has a parameterized type:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>How does that help us? Well, we can use the ternary operator to enforce that the  parameter is the same type as the list’s :</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>The macro also handles passing the item size for us! This is the error Clang produces when adding the wrong type to the list:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Macros get a bad rep, but I think this is fairly understandable. Some things to note:  is never used at runtime, it exists just for type information at compile time. Using a union makes  not consume any memory.</p><p>If you’re writing a generic function that needs to return a pointer to contained data, you can use  to cast the return type from  to the data structure’s  type.  is supported in all three big C compilers (clang, gcc,  msvc since version 19.39).</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>If for some reason you don’t like using the ternary operator to ensure two types are the same, a previous version of this article used a different technique:</p><p>One annoying thing about C compilers released prior to late 2025 is that they do not consider these two variables to have the same type:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>Even though the variables have identical type definitions, the compiler still errors because they are . A  avoids the issue:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>You can use this for any type of data structure, even ones with multiple associated types, like a hash map:</p><div><pre tabindex=\"0\"><code data-lang=\"c\"></code></pre></div><p>For more detail, like how the  macro is implemented, see the code <a href=\"https://gist.github.com/danielchasehooper/a646a109b62441ca1b4d75d94436b5cf\" target=\"_blank\" rel=\"noopener\">here</a></p><p>Thanks to <a href=\"https://forkingpaths.dev/index.html\" target=\"_blank\" rel=\"noopener\">Martin Fouilleul</a> for the encouragement to finish this post, which I’ve been sitting on for months, and the feedback on early drafts.</p>","contentLength":3921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44425461"},{"title":"A CarFax for Used PCs; Hewlett Packard wants to give old laptops new life","url":"https://spectrum.ieee.org/carmax-used-pcs","date":1751301523,"author":"rubenbe","guid":178869,"unread":true,"content":"<p><a href=\"https://ewastemonitor.info/the-global-e-waste-monitor-2024/\" target=\"_blank\"> E-waste Monitor</a><a href=\"https://spectrum.ieee.org/the-cybersecurity-of-e-waste\" target=\"_self\">e-waste</a></p><p>Many enterprises follow a standard three-year replacement cycle, assuming older computers are inefficient. However, many of these devices are still functional and could perform well with minor upgrades or maintenance. The issue is, no one knows what the weak points are for a particular machine, or what the needed maintenance is, and the <a href=\"https://spectrum.ieee.org/tag/diagnostics\">diagnostics</a> would be too costly and time-consuming. It’s easier to just buy brand new <a data-linked-post=\"2670648506\" href=\"https://spectrum.ieee.org/risc-v-laptops\" target=\"_blank\">laptops</a>.</p><p>When buying a used car, dealerships and individual buyers can access each car’s particular <a href=\"https://www.carfax.com/\" target=\"_blank\">CarFax</a> report, detailing the vehicle’s usage and maintenance history. Armed with this information, dealerships can perform the necessary fixes or upgrades before reselling the car. And individuals can decide whether to trust that vehicle’s performance. We at <a href=\"https://www.hp.com/us-en/home.html\" target=\"_blank\">HP</a> realized that, to prevent unnecessary e-waste, we need to collect and make available usage and maintenance data for each laptop, like a CarFax for used PCs.</p><p>There is a particular challenge to collecting usage data for a PC, however. We need to make sure to protect the user’s privacy and security. So, we set out to design a data-collection protocol for PCs that manages to remain secure.</p><h2>The firmware-level data collector</h2><p> Luckily, the sensors that can collect the necessary data are already installed in each PC. There are thermal sensors that monitor CPU temperature, power-consumption monitors that track <a href=\"https://spectrum.ieee.org/tag/energy-efficiency\">energy efficiency</a>, storage health indicators that assess <a href=\"https://spectrum.ieee.org/tag/solid-state-drive\">solid state drive</a> (<a href=\"https://spectrum.ieee.org/tag/ssd\">SSD</a>) wear levels, performance counters that measure system utilization, fan-rotation-speed sensors that detect cooling efficiency, and more. The key is to collect and store all that data in a secure yet useful way.</p><p> We decided that the best way to do this is to integrate the life-cycle records into the <a href=\"https://spectrum.ieee.org/tag/firmware\">firmware</a> layer. By embedding <a href=\"https://spectrum.ieee.org/tag/telemetry\">telemetry</a> capabilities directly within the firmware, we ensure that device health and usage data is captured the moment it is collected. This data is stored securely on <a href=\"https://spectrum.ieee.org/tag/hp\">HP</a> SSD drives, leveraging hardware-based security measures to protect against unauthorized access or manipulation. </p><p>The secure telemetry protocol we’ve developed at HP works as follows. We gather the critical hardware and sensor data and store it in a designated area of the SSD. This area is write-locked, meaning only authorized firmware components can write to it, preventing accidental modification or tampering. That authorized firmware component we use is the Endpoint Security Controller, a dedicated piece of hardware embedded in business-class HP PCs. It plays a critical role in strengthening platform-level security and works independently from the main CPU to provide foundational protection.</p><p>The endpoint security controller establishes a secure session by retaining the secret key within the controller itself. This mechanism enables read data protection on the SSD—where telemetry and sensitive data are stored—by preventing unauthorized access, even if the operating system is reinstalled or the system environment is otherwise altered.</p><p>Then, the collected data is recorded in a time-stamped file, stored within a dedicated telemetry log on the SSD. Storing these records on the SSD has the benefit of ensuring the data is persistent even if the operating system is reinstalled or some other drastic change in software environment occurs.</p><p>The telemetry log employs a cyclic buffer design, automatically overwriting older entries when the log reaches full capacity. Then, the telemetry log can be accessed by authorized applications at the operating system level.</p><p>The telemetry log serves as the foundation for a comprehensive device history report. Much like a CarFax report for used cars, this report, which we call PCFax, will provide both current users and potential buyers with crucial information.</p><p>The PCFax report aggregates data from multiple sources beyond just the on-device telemetry logs. It combines the secure firmware-level usage data with information from HP’s factory and supply-chain records, digital-services platforms, customer-support service records, diagnostic logs, and more. Additionally, the system can integrate data from external sources including partner sales and service records, refurbishment partner <a href=\"https://spectrum.ieee.org/tag/databases\">databases</a>, third-party component manufacturers like <a href=\"https://spectrum.ieee.org/tag/intel\">Intel</a>, and other original equipment manufacturers. This multisource approach creates a complete picture of the device’s entire life cycle, from manufacturing through all subsequent ownership and service events.</p><p>For IT teams within organizations, we hope the PCFax will bring simplicity and give opportunities for optimization. Having access to fine-grained usage and health information for each device in their fleet can help IT managers decide which devices are sent to which users, as well as when maintenance is scheduled. This data can also help device managers decide which specific devices to replace rather than issuing new computers automatically, enhancing <a href=\"https://spectrum.ieee.org/tag/sustainability\">sustainability</a>. And this can help with security: With real-time monitoring and firmware-level protection, IT teams can mitigate risks and respond swiftly to emerging threats. All of this can facilitate more efficient use of PC resources, cutting down on unnecessary waste.</p><p>We also hope that, much as the CarFax gives people confidence in buying used cars, the PCFax can encourage resale of used PCs. For enterprises and consumers purchasing second-life PCs, it provides detailed visibility into the complete service and support history of each system, including any repairs, upgrades, or performance issues encountered during its initial deployment. By making this comprehensive device history readily available, PCFax enables more PCs to find productive second lives rather than being prematurely discarded, directly addressing the e-waste challenge while providing economic benefits to both sellers and buyers in the secondary PC market.</p><p>While HP’s solutions represent a significant step forward, challenges remain. Standardizing telemetry frameworks across diverse ecosystems is critical for broader adoption. Additionally, educating organizations about the benefits of life-cycle records will be essential to driving uptake. </p><p>We are also working on integrating AI into our dashboards. We hope to use <a href=\"https://spectrum.ieee.org/tag/ai-models\">AI models</a> to analyze historical telemetry data and predict failures before they happen, such as detecting increasing SSD write cycles to forecast impending failure and alert IT teams for proactive replacement, or predicting battery degradation and automatically generating a service ticket to ensure a replacement battery is ready before failure, minimizing downtime.</p><p>We plan to start rolling out these features at the beginning of 2026.</p>","contentLength":6710,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44425253"},{"title":"Donkey Kong Country 2 and Open Bus","url":"https://jsgroth.dev/blog/posts/dkc2-open-bus/","date":1751295672,"author":"colejohnson66","guid":176784,"unread":true,"content":"\n<p>Article URL: <a href=\"https://jsgroth.dev/blog/posts/dkc2-open-bus/\">https://jsgroth.dev/blog/posts/dkc2-open-bus/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44424194\">https://news.ycombinator.com/item?id=44424194</a></p>\n<p>Points: 248</p>\n<p># Comments: 60</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nano-engineered thermoelectrics enable scalable, compressor-free cooling","url":"https://www.jhuapl.edu/news/news-releases/250521-apl-thermoelectrics-enable-compressor-free-cooling","date":1751295098,"author":"mcswell","guid":183195,"unread":true,"content":"\n<p>Research paper: <a href=\"https://www.nature.com/articles/s41467-025-59698-y\" rel=\"nofollow\">https://www.nature.com/articles/s41467-025-59698-y</a></p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44424087\">https://news.ycombinator.com/item?id=44424087</a></p>\n<p>Points: 99</p>\n<p># Comments: 53</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"There are no new ideas in AI, only new datasets","url":"https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only","date":1751294626,"author":"bilsbie","guid":176847,"unread":true,"content":"\n<p>Article URL: <a href=\"https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only\">https://blog.jxmo.io/p/there-are-no-new-ideas-in-ai-only</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44423983\">https://news.ycombinator.com/item?id=44423983</a></p>\n<p>Points: 425</p>\n<p># Comments: 226</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The story of Max, a real programmer","url":"https://incoherency.co.uk/blog/stories/the-story-of-max.html","date":1751288278,"author":"surprisetalk","guid":183194,"unread":true,"content":"\n<p>Article URL: <a href=\"https://incoherency.co.uk/blog/stories/the-story-of-max.html\">https://incoherency.co.uk/blog/stories/the-story-of-max.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44422722\">https://news.ycombinator.com/item?id=44422722</a></p>\n<p>Points: 101</p>\n<p># Comments: 66</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: TokenDagger – A tokenizer faster than OpenAI's Tiktoken","url":"https://github.com/M4THYOU/TokenDagger","date":1751286838,"author":"matthewolfe","guid":176699,"unread":true,"content":"<p>TokenDagger is a drop-in replacement for OpenAI’s Tiktoken (the tokenizer behind Llama 3, Mistral, GPT-3.*, etc.). It’s written in C++ 17 with thin Python bindings, keeps the exact same BPE vocab/special-token rules, and focuses on raw speed.</p><p>I’m teaching myself LLM internals by re-implementing the stack from first principles. Profiling TikToken’s Python/Rust implementation showed a lot of time was spent doing regex matching. Most of my perf gains come from a) using a faster jit-compiled regex engine; and b) simplifying the algorithm to forego regex matching special tokens at all.</p><p>Benchmarking code is included. Notable results show:\n- 4x faster code sample tokenization on a single thread.\n- 2-3x higher throughput when tested on a 1GB natural language text file.</p>","contentLength":777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44422480"},{"title":"Reverse Engineering Vercel's BotID","url":"https://www.nullpt.rs/reversing-botid","date":1751285985,"author":"hazebooth","guid":176930,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.nullpt.rs/reversing-botid\">https://www.nullpt.rs/reversing-botid</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44422356\">https://news.ycombinator.com/item?id=44422356</a></p>\n<p>Points: 79</p>\n<p># Comments: 12</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: New Ensō – first public beta","url":"https://untested.sonnet.io/notes/new-enso-first-public-beta/","date":1751281375,"author":"rpastuszak","guid":176698,"unread":true,"content":"<p>The new version of <a href=\"https://enso.sonnet.io\" target=\"_blank\" rel=\"noopener noreferrer\">Ensō</a> (codename: Occult Vampire Keanu) is available for public testing!</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/IYuLv6hkrd-2817.png\" alt=\"\" width=\"2817\" height=\"1470\">\nThis is a temporary icon I used for testing. I am considering creating a simplified version of it. PS. here's the <a href=\"https://www.potato.horse/p/MVrXRp0XziSuakcrMGzO8\" target=\"_blank\" rel=\"noopener noreferrer\">original image</a> (on <a href=\"https://potato.horse\" target=\"_blank\" rel=\"noopener noreferrer\">potato.horse</a>, of course)</p><p>Following <a href=\"https://untested.sonnet.io/notes/miss-make-it-stupid-simple/\">MISS</a>, my focus is on removing distractions over adding new features. This can be surprisingly challenging (e.g. how do I tell users about feature X or Y without breaking their flow?) but also gives me time to focus on polishing the app.</p><p>(we will discuss these in more detail in future posts)</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-short-version-as-explained-by-hermes-trismegistus\" aria-hidden=\"true\" tabindex=\"-1\"></a>Short version (as explained by Hermes Trismegistus)</h3><ul><li> Simplified, more accessible UI</li></ul><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-an-even-more-simple-streamlined-ui-following-the-miss-philosophy\" aria-hidden=\"true\" tabindex=\"-1\"></a>An even more simple, streamlined UI, following the <a href=\"https://untested.sonnet.io/notes/miss-make-it-stupid-simple/\">MISS</a> philosophy.</h3><p>Most of the UI has been moved to the application menu bar for easier discoverability and shortcut access. So far no one has missed the old inline UI, but you can read more about it towards the end of this note.</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-5-accessibility-friendly-themes-to-choose-from\" aria-hidden=\"true\" tabindex=\"-1\"></a>5½ Accessibility-friendly themes to choose from</h3><p>We have  5½ predefined themes focussed on accessibility and specific use patterns based on feedback I've collected over the years.</p><ul><li>writing during the day in regular light conditions</li><li>writing in low light for devices with OLED screens\n</li><li>writing in extremely low light conditions, with reduced light exposure  (See <a href=\"https://untested.sonnet.io/notes/midnight/\">Midnight</a>, <a href=\"https://untested.sonnet.io/notes/obsidian-for-vampires/\">Obsidian for Vampires</a>)\n<ul><li>designed for OLED screens</li><li>the main use case here is writing at night, to put myself to sleep.</li></ul></li></ul><p>5½ and not 6 because one theme still needs some work. Is there a specific use case or theme you'd like to see in Ensō? Let me know!</p><p>This is one of the few truly new features in Ensō. Coffeeshop mode allows you to stop worrying that someone standing behind you might see what you're typing. The text itself is concealed but you still know what you're writing.  Use  to toggle on and off at any time.</p><p>I've been using it for a couple of months and found it super helpful, especially for journaling in public places, but not only (read more here: <a href=\"https://untested.sonnet.io/notes/sketch-enso-coffeeshop-mode/\">Sketch - Ensō Coffeeshop Mode</a>).</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-a-few-smaller-accessibility-improvements\" aria-hidden=\"true\" tabindex=\"-1\"></a>A few smaller accessibility improvements</h3><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/rwyV2cqnxt-974.png\" alt=\"\" width=\"974\" height=\"348\">\nNote: if you remove the  menu and call it , MacOs won't add its AI crap to your settings.</p><ul><li> toggle autocorrect, autocapitalise, spelling</li><li> control text size (previously not possible in the native version)</li></ul><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-a-new-polished-text-rendering-engine\" aria-hidden=\"true\" tabindex=\"-1\"></a>A new, polished text rendering engine</h3><p>The new text rendering engine allows for better control over typography settings, supports alternative display modes like Coffeeshop, and uses a custom caret.</p><p>I don't know how to describe it objectively (and I obviously lack the distance to) but writing in the new UI feels different, more fluid. The text is easy to read, but also somewhat softer (though not blurry).</p><p>Less is more, so why do I care about it?  less is more. I want Ensō to feel familiar and high-quality, like a good Moleskine notebook. I want people to feel comfortable paying $10 for a typing app without text selection. I want them to enjoy it as much as I do. Fewer features allow me to focus more on what  there.</p><p>Ensō will be published via the AppStore by default. We will keep the old version on Gumroad, but there's no reason to maintain it, since the new version is better in every possible way and functionally the same by default.</p><p>The reasons I decided to <a href=\"https://untested.sonnet.io/notes/skip-the-appstore-and-use-gumroad/\">skip the AppStore and use Gumroad</a>, plus what I learned from that are beyond the scope of this note (you can click the link to request that particular write-up).</p><ul><li>several users complained that Gumroad payment looked, for the lack of a better word, shady, especially at the step with a PayPal payment screen. The ones who messaged me still bought the app, but I imagine there were many who turned back.</li><li>AppStore with all its flaws makes delivering apps... slow and annoying, but also relatively easy without much code.</li><li>I can add OTA updates and re-publish Ensō via Gumroad later, which makes sense as an iterative improvement.</li></ul><p><strong>The Gumroad version of Ensō will stay as a backup, but will not be maintained.</strong></p><p>I've been using Ensō daily for 6 years. I've also received a ton of high-quality feedback, not via analytics but from users who were kind enough to reach out to me. I like to think that I have a fairly good idea of how and why people use Ensō.</p><p>The previous version of Ensō would pass an anonymous impression event on load. Now, by design, no network traffic is made at all. Here's our new Privacy Page.</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/2seRd5O6WS-1588.png\" alt=\"\" width=\"1588\" height=\"842\">\nCurrent version of our Privacy page (<a href=\"https://enso.sonnet.io/app-privacy\" target=\"_blank\" rel=\"noopener noreferrer\">source</a>)</p><p>It will come, but the new version is already so much better than the previous, that I feel like waiting for more features would be a wasted opportunity.</p><p>I'm working on a UX that balances discoverability with staying focussed. Each option, each new choice is a chance for you to get distracted, so the key is to do this thoughtfully and with respect towards my users' time.</p><h3><a href=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/#h-rtl-or-non-ltr-language-support\" aria-hidden=\"true\" tabindex=\"-1\"></a>RTL (or non-LTR) language support</h3><p><strong>This one will be included in the next test build.</strong> Many Ensō users speak languages written in non-Latin alphabets (to my knowledge, mainly Persian, Arabic and Hebrew).</p><p>It makes me both grateful and somewhat sad that one (non-techie) user went as far as even sharing a code sample with me when asking for fixing the issue. Adding rudimentary RTL support can be as simple as a one-line change in your code. Even if it's not perfect - it's still a huge improvement that your non-Latin script users will notice, believe me.</p><p>The previous version of Ensō displayed the UI in the same space as the text. That's not the case any more.</p><p>I'm still considering adding a hamburger menu in the main app canvas, however only two (less frequent) users of Ensō have brought it up so far.</p><ol><li>ease of use, reducing distractions\n</li></ol><p>There's tension between 1. and 2. as every new feature implies more choices on the user's part; every new choice is an opportunity for distraction. This might seem pedantic, but small, seemingly insignificant changes do add up.</p><p>Removing things is harder than adding them (see 3.). Perhaps that's why commits with negative LoC count feel so good.</p><p><img loading=\"lazy\" decoding=\"async\" src=\"https://untested.sonnet.io/notes/new-enso-first-public-beta/POXoaXUuMD-1668.png\" alt=\"\" width=\"1668\" height=\"1559\">\nWhere to go from here?</p><ol><li>Collect the test feedback and respond to it</li><li>Prepare basic marketing materials\n<ul><li>I might put an ad on social media, trying to get people off it (<a href=\"https://sonnet.io/posts/sit/\" target=\"_blank\" rel=\"noopener noreferrer\">Sit.</a>) but what I call marketing is mostly talking about Ensō and related subjects here, plus engaging with communities I already know, such as forums</li></ul></li></ol><ul><li> Windows and Linux support — I'll revisit it in the next few months. I'm moving towards supporting myself from my own projects and I need to be selective how I use my time. If you're interested in testing a Windows or Linux build, <a href=\"mailto:hello@sonnet.io\">let me know</a>.</li><li> Quick Save - hitting  would automatically save a snapshot of your notes to a predefined directory with a time-stamped file name, e.g. </li><li> Toybox - an optional menu feature with experimental tools released episodically, such as:<ul><li> visual experiments (e.g. different typography styles or letters and words turning into vines that grow as you type)</li></ul></li></ul><p>If Toybox becomes a reality, it'll be buried in the menus to avoid distractions and will act mainly as my platform for experimentation and play with users. If there's a chance it might introduce more distractions - it'll become a separate app. (<a href=\"https://untested.sonnet.io/notes/kind-software/\">Kind software</a>)</p><p>Every day in small chunks and some days in longer stretches.</p><p>I'm approaching this just like <a href=\"https://untested.sonnet.io/notes/exhibition-in-porto-janusz-enters-the-fashion-industry-draft-1/\">My Recent Art Exhibition</a> - working on different things simultaneously, focussing on their interplay rather than looking at each feature in isolation.</p><p>While I believe you should <a href=\"https://untested.sonnet.io/notes/share-your-unfinished-scrappy-work/\">Share your unfinished, scrappy work</a>, I know Ensō well enough that I can allow myself more flexibility. This style of work gives me a lot of joy and the end results have so far been better than expected.</p><p><strong>The new Ensō is not the type of project I can share in small unfinished bits, feature by feature.</strong> I will repeat this ad nauseam: I want to give you something that will get out of your way but also feel beautiful, polished, yours.</p><p>This is akin to good typography or UX - when it's there, you don't notice it, but at a subconscious level, you feel more comfortable with the tool and want to spend more time using it. That has been my experience so far.</p><p><strong>Tauri is much more mature than when I released the first macOS version of Ensō.</strong> I spent weeks getting the previous version to build properly on Mac with notarisation, provisioning profiles and undocumented AppStore Connect APIs. Now, most of the things just work (sometimes with a bit of scripting, which is where Claude Code turned out to be indispensable).</p><p>I'm not an \"IndieHacker\", I'm not in a rush, I'm a wannabe-carpenter (<a href=\"https://untested.sonnet.io/notes/projects-and-apps-i-built-for-my-own-well-being/\">Brief History of Galician Carpentry</a>) and Ensō happens to be made of stuff that can be worked in a carpentry-like manner. The small feature set means I can afford to take time to work on this with enough care, which I hope shows in the final product.</p><p><strong>Building a theme switcher can be a weirdly complex problem</strong> (if you complicate it well enough). The difficult part was letting users set themes for dark/light/sync with OS mode, with previews, making it obvious when changes are saved, all in a single piece of UI, with max 2-3 clicks.</p><p>Most of my attempts at this resulted in something that looks more than the Dwarf Fortress GUI than a simple theme picker. I understand now why almost no one is doing this and why the few who do split the UI in several steps.</p><p><strong>I'm still happy with using a browser as the text rendering engine.</strong> Especially with Safari, the amount of control over typography is just excellent (e.g.  ).</p><p>I wish there was an easy way of getting the native accent colour from the OS, but that's not possible at the moment.  can be customised, but not read.</p><p><strong>I'm not planning to remove the free web version of Ensō.</strong> I want to get paid for my work, but people reach out to me and buy it with virtually no marketing. I'm hopeful, even optimistic that the trust I've earned so far, as well as the quality of the final product, will be enough for it to grow slowly but steadily.</p><p>That's all for today. Thanks for reading!</p>","contentLength":9803,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44421776"},{"title":"The provenance memory model for C","url":"https://gustedt.wordpress.com/2025/06/30/the-provenance-memory-model-for-c/","date":1751275514,"author":"HexDecOctBin","guid":176651,"unread":true,"content":"\n<p>Article URL: <a href=\"https://gustedt.wordpress.com/2025/06/30/the-provenance-memory-model-for-c/\">https://gustedt.wordpress.com/2025/06/30/the-provenance-memory-model-for-c/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44421185\">https://news.ycombinator.com/item?id=44421185</a></p>\n<p>Points: 200</p>\n<p># Comments: 106</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Want to meet people, try charging them for it?","url":"https://notes.eatonphil.com/2025-06-28-want-to-meet-people-charge-them.html","date":1751263990,"author":"ArneVogel","guid":176471,"unread":true,"content":"\n<p>Article URL: <a href=\"https://notes.eatonphil.com/2025-06-28-want-to-meet-people-charge-them.html\">https://notes.eatonphil.com/2025-06-28-want-to-meet-people-charge-them.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44419986\">https://news.ycombinator.com/item?id=44419986</a></p>\n<p>Points: 139</p>\n<p># Comments: 70</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LetsEncrypt – Expiration Notification Service Has Ended","url":"https://letsencrypt.org/2025/06/26/expiration-notification-service-has-ended/","date":1751258979,"author":"zdw","guid":176470,"unread":true,"content":"\n<p>Article URL: <a href=\"https://letsencrypt.org/2025/06/26/expiration-notification-service-has-ended/\">https://letsencrypt.org/2025/06/26/expiration-notification-service-has-ended/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44419496\">https://news.ycombinator.com/item?id=44419496</a></p>\n<p>Points: 82</p>\n<p># Comments: 43</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bought myself an Ampere Altra system","url":"https://marcin.juszkiewicz.com.pl/2025/06/27/bought-myself-an-ampere-altra-system/","date":1751258329,"author":"pabs3","guid":176355,"unread":true,"content":"\n<p>Article URL: <a href=\"https://marcin.juszkiewicz.com.pl/2025/06/27/bought-myself-an-ampere-altra-system/\">https://marcin.juszkiewicz.com.pl/2025/06/27/bought-myself-an-ampere-altra-system/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44419446\">https://news.ycombinator.com/item?id=44419446</a></p>\n<p>Points: 139</p>\n<p># Comments: 42</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gridfinity: The modular, open-source grid storage system","url":"https://gridfinity.xyz/","date":1751254641,"author":"nateb2022","guid":176469,"unread":true,"content":"\n<p>Article URL: <a href=\"https://gridfinity.xyz/\">https://gridfinity.xyz/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44419091\">https://news.ycombinator.com/item?id=44419091</a></p>\n<p>Points: 377</p>\n<p># Comments: 162</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Use keyword-only arguments in Python dataclasses","url":"https://chipx86.blog/2025/06/29/tip-use-keyword-only-arguments-in-python-dataclasses/","date":1751244336,"author":"Bogdanp","guid":176650,"unread":true,"content":"\n<p>Article URL: <a href=\"https://chipx86.blog/2025/06/29/tip-use-keyword-only-arguments-in-python-dataclasses/\">https://chipx86.blog/2025/06/29/tip-use-keyword-only-arguments-in-python-dataclasses/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44418036\">https://news.ycombinator.com/item?id=44418036</a></p>\n<p>Points: 75</p>\n<p># Comments: 32</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nearly 20% of cancer drugs defective in four African nations","url":"https://www.dw.com/en/nearly-20-of-cancer-drugs-defective-in-4-african-nations/a-73062221","date":1751239389,"author":"woldemariam","guid":175535,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.dw.com/en/nearly-20-of-cancer-drugs-defective-in-4-african-nations/a-73062221\">https://www.dw.com/en/nearly-20-of-cancer-drugs-defective-in-4-african-nations/a-73062221</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44417549\">https://news.ycombinator.com/item?id=44417549</a></p>\n<p>Points: 121</p>\n<p># Comments: 61</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding a former Australian prime minister’s passport number on Instagram (2020)","url":"https://mango.pdf.zone/finding-former-australian-prime-minister-tony-abbotts-passport-number-on-instagram/","date":1751235752,"author":"guiambros","guid":175517,"unread":true,"content":"\n<p>Article URL: <a href=\"https://mango.pdf.zone/finding-former-australian-prime-minister-tony-abbotts-passport-number-on-instagram/\">https://mango.pdf.zone/finding-former-australian-prime-minister-tony-abbotts-passport-number-on-instagram/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44417091\">https://news.ycombinator.com/item?id=44417091</a></p>\n<p>Points: 127</p>\n<p># Comments: 48</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cell Towers Can Double as Cheap Radar Systems for Ports and Harbors (2014)","url":"https://spectrum.ieee.org/cell-tower-signals-can-improve-port-security","date":1751233692,"author":"transpute","guid":176354,"unread":true,"content":"\n<p>Article URL: <a href=\"https://spectrum.ieee.org/cell-tower-signals-can-improve-port-security\">https://spectrum.ieee.org/cell-tower-signals-can-improve-port-security</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44416761\">https://news.ycombinator.com/item?id=44416761</a></p>\n<p>Points: 119</p>\n<p># Comments: 64</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"China Dominates 44% of Visible Fishing Activity Worldwide","url":"https://oceana.org/press-releases/china-dominates-44-of-visible-fishing-activity-worldwide/","date":1751233430,"author":"scubakid","guid":175478,"unread":true,"content":"\n<p>Article URL: <a href=\"https://oceana.org/press-releases/china-dominates-44-of-visible-fishing-activity-worldwide/\">https://oceana.org/press-releases/china-dominates-44-of-visible-fishing-activity-worldwide/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44416710\">https://news.ycombinator.com/item?id=44416710</a></p>\n<p>Points: 103</p>\n<p># Comments: 57</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anticheat Update Tracking","url":"https://not-matthias.github.io/posts/anticheat-update-tracking/","date":1751231217,"author":"not-matthias","guid":176649,"unread":true,"content":"\n<p>Article URL: <a href=\"https://not-matthias.github.io/posts/anticheat-update-tracking/\">https://not-matthias.github.io/posts/anticheat-update-tracking/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44416421\">https://news.ycombinator.com/item?id=44416421</a></p>\n<p>Points: 101</p>\n<p># Comments: 47</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Error handling in Rust","url":"https://felix-knorr.net/posts/2025-06-29-rust-error-handling.html","date":1751228905,"author":"emschwartz","guid":175445,"unread":true,"content":"\n<p>Article URL: <a href=\"https://felix-knorr.net/posts/2025-06-29-rust-error-handling.html\">https://felix-knorr.net/posts/2025-06-29-rust-error-handling.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44416157\">https://news.ycombinator.com/item?id=44416157</a></p>\n<p>Points: 125</p>\n<p># Comments: 102</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask HN: What Are You Working On? (June 2025)","url":"https://news.ycombinator.com/item?id=44416093","date":1751228488,"author":"david927","guid":175477,"unread":true,"content":"\n<p>What are you working on?  Any new ideas which you're thinking about?</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44416093\">https://news.ycombinator.com/item?id=44416093</a></p>\n<p>Points: 400</p>\n<p># Comments: 1246</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YouTube No Translation","url":"https://addons.mozilla.org/en-GB/firefox/addon/youtube-no-translation/","date":1751227854,"author":"doener","guid":178723,"unread":true,"content":"\n<p>Article URL: <a href=\"https://addons.mozilla.org/en-GB/firefox/addon/youtube-no-translation/\">https://addons.mozilla.org/en-GB/firefox/addon/youtube-no-translation/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44416009\">https://news.ycombinator.com/item?id=44416009</a></p>\n<p>Points: 358</p>\n<p># Comments: 264</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Many ransomware strains will abort if they detect a Russian keyboard installed (2021)","url":"https://krebsonsecurity.com/2021/05/try-this-one-weird-trick-russian-hackers-hate/","date":1751221795,"author":"air7","guid":175400,"unread":true,"content":"\n<p>Article URL: <a href=\"https://krebsonsecurity.com/2021/05/try-this-one-weird-trick-russian-hackers-hate/\">https://krebsonsecurity.com/2021/05/try-this-one-weird-trick-russian-hackers-hate/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44415233\">https://news.ycombinator.com/item?id=44415233</a></p>\n<p>Points: 332</p>\n<p># Comments: 183</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tools I love: mise(-en-place)","url":"https://blog.vbang.dk/2025/06/29/tools-i-love-mise/","date":1751219781,"author":"micvbang","guid":175387,"unread":true,"content":"\n<p>Article URL: <a href=\"https://blog.vbang.dk/2025/06/29/tools-i-love-mise/\">https://blog.vbang.dk/2025/06/29/tools-i-love-mise/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44414987\">https://news.ycombinator.com/item?id=44414987</a></p>\n<p>Points: 116</p>\n<p># Comments: 39</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A list is a monad","url":"https://alexyorke.github.io//2025/06/29/a-list-is-a-monad/","date":1751219622,"author":"polygot","guid":181352,"unread":true,"content":"\n<p>Article URL: <a href=\"https://alexyorke.github.io//2025/06/29/a-list-is-a-monad/\">https://alexyorke.github.io//2025/06/29/a-list-is-a-monad/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44414965\">https://news.ycombinator.com/item?id=44414965</a></p>\n<p>Points: 123</p>\n<p># Comments: 138</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Loss of key US satellite data could send hurricane forecasting back 'decades'","url":"https://www.theguardian.com/us-news/2025/jun/28/noaa-cuts-hurricane-forecasting-climate","date":1751218748,"author":"trauco","guid":175357,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.theguardian.com/us-news/2025/jun/28/noaa-cuts-hurricane-forecasting-climate\">https://www.theguardian.com/us-news/2025/jun/28/noaa-cuts-hurricane-forecasting-climate</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44414853\">https://news.ycombinator.com/item?id=44414853</a></p>\n<p>Points: 266</p>\n<p># Comments: 121</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Personal care products disrupt the human oxidation field","url":"https://www.science.org/doi/10.1126/sciadv.ads7908","date":1751217616,"author":"XzetaU8","guid":175340,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.science.org/doi/10.1126/sciadv.ads7908\">https://www.science.org/doi/10.1126/sciadv.ads7908</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44414719\">https://news.ycombinator.com/item?id=44414719</a></p>\n<p>Points: 180</p>\n<p># Comments: 129</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Evolution of Caching Libraries in Go","url":"https://maypok86.github.io/otter/blog/cache-evolution/","date":1751216825,"author":"maypok86","guid":181697,"unread":true,"content":"\n<p>Article URL: <a href=\"https://maypok86.github.io/otter/blog/cache-evolution/\">https://maypok86.github.io/otter/blog/cache-evolution/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44414630\">https://news.ycombinator.com/item?id=44414630</a></p>\n<p>Points: 101</p>\n<p># Comments: 24</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"We accidentally solved robotics by watching 1M hours of YouTube","url":"https://ksagar.bearblog.dev/vjepa/","date":1751213333,"author":"alexcos","guid":175516,"unread":true,"content":"\n<p>Article URL: <a href=\"https://ksagar.bearblog.dev/vjepa/\">https://ksagar.bearblog.dev/vjepa/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44414171\">https://news.ycombinator.com/item?id=44414171</a></p>\n<p>Points: 157</p>\n<p># Comments: 120</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The $25k car is going extinct?","url":"https://media.hubspot.com/why-the-25000-car-is-going-extinct","date":1751212796,"author":"pseudolus","guid":175515,"unread":true,"content":"\n<p>Article URL: <a href=\"https://media.hubspot.com/why-the-25000-car-is-going-extinct\">https://media.hubspot.com/why-the-25000-car-is-going-extinct</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44414115\">https://news.ycombinator.com/item?id=44414115</a></p>\n<p>Points: 272</p>\n<p># Comments: 780</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Event – Fast, In-Process Event Dispatcher","url":"https://github.com/kelindar/event","date":1751210359,"author":"kelindar","guid":175386,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/kelindar/event\">https://github.com/kelindar/event</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44413809\">https://news.ycombinator.com/item?id=44413809</a></p>\n<p>Points: 163</p>\n<p># Comments: 35</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Medley Interlisp Project: Reviving a Historical Software System [pdf]","url":"https://interlisp.org/documentation/young-ccece2025.pdf","date":1751208310,"author":"pamoroso","guid":175444,"unread":true,"content":"\n<p>Article URL: <a href=\"https://interlisp.org/documentation/young-ccece2025.pdf\">https://interlisp.org/documentation/young-ccece2025.pdf</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44413574\">https://news.ycombinator.com/item?id=44413574</a></p>\n<p>Points: 107</p>\n<p># Comments: 19</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made my VM think it has a CPU fan","url":"https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html","date":1751205318,"author":"todsacerdoti","guid":175224,"unread":true,"content":"\n<p>Article URL: <a href=\"https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html\">https://wbenny.github.io/2025/06/29/i-made-my-vm-think-it-has-a-cpu-fan.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44413185\">https://news.ycombinator.com/item?id=44413185</a></p>\n<p>Points: 634</p>\n<p># Comments: 167</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Zen of Quakerism (2016)","url":"https://www.friendsjournal.org/the-zen-of-quakerism/","date":1751204419,"author":"surprisetalk","guid":181351,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.friendsjournal.org/the-zen-of-quakerism/\">https://www.friendsjournal.org/the-zen-of-quakerism/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44413076\">https://news.ycombinator.com/item?id=44413076</a></p>\n<p>Points: 99</p>\n<p># Comments: 80</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A proof-of-concept neural brain implant providing speech","url":"https://arstechnica.com/science/2025/06/a-neural-brain-implant-provides-near-instantaneous-speech/","date":1751199374,"author":"LorenDB","guid":180577,"unread":true,"content":"\n<p>Article URL: <a href=\"https://arstechnica.com/science/2025/06/a-neural-brain-implant-provides-near-instantaneous-speech/\">https://arstechnica.com/science/2025/06/a-neural-brain-implant-provides-near-instantaneous-speech/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44412488\">https://news.ycombinator.com/item?id=44412488</a></p>\n<p>Points: 100</p>\n<p># Comments: 61</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bloom Filters by Example","url":"https://llimllib.github.io/bloomfilter-tutorial/","date":1751198169,"author":"ibobev","guid":175339,"unread":true,"content":"\n<p>Article URL: <a href=\"https://llimllib.github.io/bloomfilter-tutorial/\">https://llimllib.github.io/bloomfilter-tutorial/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44412370\">https://news.ycombinator.com/item?id=44412370</a></p>\n<p>Points: 201</p>\n<p># Comments: 30</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Octelium – FOSS Alternative to Teleport, Cloudflare, Tailscale, Ngrok","url":"https://github.com/octelium/octelium","date":1751196257,"author":"geoctl","guid":175223,"unread":true,"content":"<p>I have been working on Octelium for quite a few years now but it was open sourced only by late May 2025. Octelium, as described more in detail in the repo's README, is simply an open source, self-hosted, unified platform for zero trust resource access that is primarily meant to be a modern alternative to corporate VPNs and remote access tools. It can operate as a remote access/corporate VPN (i.e. alternative to Twingate, Tailscale, OpenVPN Access Server, etc...), a ZTNA/BeyondCorp platform (i.e. alterntive to Cloudflare Access, Teleport, Google BeyondCorp, etc...), and it can also operate as an API/AI gateway, an infrastructure for MCP and A2A architectures and meshes, an ngrok alternative, a homelab infrastructure or even as a more advanced Kubernetes ingress. It's basically designed to operate like a unified Kubernetes-like scalable architecture for zero trust secure/remote access that's suitable for different human-to-workload and workload-to-workload environments. You can read more in detail the full set of main features and links about how it works in the repo's README or directly in the docs <a href=\"https://octelium.com/docs\" rel=\"nofollow\">https://octelium.com/docs</a></p>","contentLength":1140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44412207"}],"tags":["hn"]}