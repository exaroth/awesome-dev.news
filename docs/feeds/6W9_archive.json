{"id":"6W9","title":"HN","displayTitle":"HN","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":216,"items":[{"title":"I'm too dumb for Zig's new IO interface","url":"https://www.openmymind.net/Im-Too-Dumb-For-Zigs-New-IO-Interface/","date":1755931178,"author":"begoon","guid":237240,"unread":true,"content":"<p>You might have heard that Zig 0.15 introduces a new IO interface, with the focus for this release being the new std.Io.Reader and std.Io.Writer types. The old \"interfaces\" had problems. Like <a href=\"https://github.com/ziglang/zig/issues/17985\">this performance issue</a> that I opened. And it relied on a <a href=\"https://www.openmymind.net/In-Zig-Whats-a-Writer/\">mix of types</a>, which always confused me, and a lot of  - which is generally great, but a poor foundation to build an interface on.</p><p>I've been slowly upgrading my libraries, and I ran into changes to the  client used by my smtp library. For the life of me, I just don't understand how it works.</p><p>Zig has never been known for its documentation, but if we look at the documentation for , we'll find:</p><pre><code>input output options InitErrorClient\nInitiates a TLS handshake  establishes a TLSv1 TLSv1 session</code></pre><p>So it takes one of these new Readers and a new Writer, along with some options (sneak peak, which aren't all optional). It doesn't look like you can just give it a , but  does expose a  and  method, so that's probably a good place to start:</p><pre><code> stream  stdnetallocator stream writer  stream reader  stream tls_client  stdcryptotlsClient\n  readerwriterinterface</code></pre><p>Note that  returns a  and  returns a  - those aren't the types our  expects. To convert the  to an , we need to call its  method. To get a  from an , we need the address of its  field. This doesn't seem particularly consistent. Don't forget that the  and  need a stable address. Because I'm trying to get the simplest example working, this isn't an issue - everything will live on the stack of . In a real word example, I think it means that I'll always have to wrap the  into my own heap-allocated type; giving the writer and reader have a cozy stable home.</p><p>Speaking of allocations, you might have noticed that  and  take a parameter. It's the buffer they should use. Buffering is a first class citizen of the new Io interface - who needs composition? The documentation  tell me these need to be at least <code>std.crypto.tls.max_ciphertext_record_len</code> large, so we need to fix things a bit:</p><pre><code> write_buf writer  streamwrite_buf read_buf reader  streamread_buf</code></pre><p>Here's where the code stands: </p><pre><code> std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf tls_client  stdcryptotlsClient\n      readerwriterinterface tls_client</code></pre><p>But if you try to run it, you'll get a compilation error. Turns out we have to provide 4 options: the ca_bundle, a host, a  and a . Normally I'd expect the options parameter to be for optional parameters, I don't understand why some parameters (input and output) are passed one way while  and  are passed another.</p><p>Let's give it what it wants AND send some data:</p><pre><code> bundle  bundleallocator bundleallocator tls_client  stdcryptotlsClient\n  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer  tls_client tls_clientwriter</code></pre><p>Now, if I try to run it, the program just hangs. I don't know what  is, but I know Zig now loves buffers, so let's try to give it something:</p><pre><code> write_buf2 tls_client  stdcryptotlsClient\n  readerwriterinterfaceca bundle  bundlehost explicit read_buffer write_buffer write_buf2 tls_client tls_clientwriter</code></pre><p>Great, now the code doesn't hang, all we need to do is read the response.  exposes a  field which is \"Decrypted stream from the server to the client.\" That sounds like what we want, but believe it or not  doesn't have a  method. It has a  a , a  (which seems close, but it blocks until the provided buffer is full), a  and a lot more, but nothing like the  I'd expect. The closest I can find, which I think does what I want, is to stream it to a writer:</p><pre><code> buf wbuf n  tls_clientreaderwbuflen\nstddebugn bufn</code></pre><p>If we try to run the code now, it crashes. We've apparently failed an assertion regarding the length of a buffer. So it seems like we also  to provide a .</p><p>Here's my current version (it doesn't work, but it doesn't crash!):</p><pre><code> std  gpa stdheapinit allocator  gpa stream  stdnetallocator stream write_buf writer  streamwrite_buf read_buf reader  streamread_buf bundle  bundleallocator bundleallocator write_buf2 read_buf2 tls_client  stdcryptotlsClient\n      readerwriterinterfaceca bundle  bundlehost explicit read_buffer read_buf2write_buffer write_buf2 tls_client tls_clientwriter buf wbuf n  tls_clientreaderwbuflen\n  stddebugn bufn</code></pre><p>When I looked through Zig's source code, there's <a href=\"https://github.com/ziglang/zig/blob/306176046e6ae5e30bc58e5f3bcf786159e367f2/lib/std/http/Client.zig#L329\">only one place</a> using . It helped to get me where where I am. I couldn't find any tests.</p><p>I'll admit that during this migration, I've missed some basic things. For example, someone had to help me find  - the renamed version of . Maybe there's a helper like: <code>tls.Client.init(allocator, stream)</code> somewhere. And maybe it makes sense that we do  but  - I'm reminded of Go's  and . And maybe Zig has some consistent rule for what parameters belong in options. And I know nothing about TLS, so maybe it makes complete sense to need 4 buffers. I feel a bit more confident about the weirdness of not having a  function on , but at this point I wouldn't bet on me.</p>","contentLength":4948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44993797"},{"title":"Measuring the environmental impact of AI inference","url":"https://arstechnica.com/ai/2025/08/google-says-it-dropped-the-energy-cost-of-ai-queries-by-33x-in-one-year/","date":1755919353,"author":"ksec","guid":237200,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44992832"},{"title":"Japan city drafts ordinance to cap smartphone use at 2 hours per day","url":"https://english.kyodonews.net/articles/-/59582","date":1755915609,"author":"Improvement","guid":237199,"unread":true,"content":"<p>NAGOYA - A central Japan city said Thursday it will seek to pass an ordinance recommending all residents limit smartphone use to two hours a day outside of work and school amid concerns over the impact of excessive technology exposure, though there will be no penalties proposed.</p><p>The ordinance drafted by the city of Toyoake in Aichi Prefecture is likely to be the first such municipal regulation in Japan that targets a limit on the use of smartphones and other electronic devices, according to the city. If passed by the local assembly, the ordinance will come into effect on Oct. 1.</p><p>\"We want the ordinance to provide an opportunity for people to think about how they use smartphones,\" an official said.</p><p>To ensure that children get a good night's sleep, the draft ordinance urges elementary school students to refrain from using smartphones after 9 p.m. and junior high students and older to put their devices down by 10 p.m.</p><p>The draft acknowledged that smartphones, personal computers and tablets are necessities, but warned that overuse of social media and video streaming may have a negative impact on health and family life.</p><p>The city will work with schools and parents to promote the healthy use of electronic devices, according to the draft ordinance.</p>","contentLength":1253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44992446"},{"title":"My tips for using LLM agents to create software","url":"https://efitz-thoughts.blogspot.com/2025/08/my-experience-creating-software-with_22.html","date":1755910742,"author":"efitz","guid":237239,"unread":true,"content":"<ul></ul><h2 dir=\"ltr\"></h2><h2 dir=\"ltr\"></h2><h2 dir=\"ltr\"></h2><h2 dir=\"ltr\"></h2><h2 dir=\"ltr\"></h2><ol></ol><h2 dir=\"ltr\"></h2><ol></ol><h2 dir=\"ltr\"></h2><ul></ul><h2 dir=\"ltr\"></h2><h2 dir=\"ltr\"></h2>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44991884"},{"title":"Computer fraud laws used to prosecute leaking air crash footage to CNN","url":"https://www.techdirt.com/2025/08/22/investigators-used-terrible-computer-fraud-laws-to-ensure-people-were-punished-for-leaking-air-crash-footage-to-cnn/","date":1755907496,"author":"BallsInIt","guid":237170,"unread":true,"content":"<h3>from the <i>if-it-can-be-abused,-it-WILL-be-abused</i> dept</h3><p>Earlier this year, an Army helicopter <a href=\"https://en.wikipedia.org/wiki/2025_Potomac_River_mid-air_collision\" data-type=\"link\" data-id=\"https://en.wikipedia.org/wiki/2025_Potomac_River_mid-air_collision\">collided with a passenger plane</a> over the Potomac River in Washington, DC. All sixty-seven people aboard both vehicles were killed. While the FAA focused its investigation on the failures that led to this mid-air collision, local investigators in Virginia were somehow far more concerned about identifying who had <a href=\"https://www.youtube.com/watch?v=JTgUrfQsOnA\" data-type=\"link\" data-id=\"https://www.youtube.com/watch?v=JTgUrfQsOnA\">leaked footage of the collision to CNN</a>. </p><p>The subject matter of the leaked recordings was obviously of public interest. And while the government may have its own interest in controlling dissemination of recording of incidents that involve federal agencies and their oversight, it‚Äôs not the sort of government interest most courts consider to be worthy of violating the First Amendment.</p><p>Fortunately, the government has options. For a very long time, the option federal law enforcement deployed most frequently in cases involving pretty much any sort of technology was the <a href=\"https://www.techdirt.com/tag/cfaa/\" data-type=\"link\" data-id=\"https://www.techdirt.com/tag/cfaa/\">Computer Fraud and Abuse Act</a> (CFAA). This <a href=\"https://www.techdirt.com/tag/shoot-the-messenger/\" data-type=\"link\" data-id=\"https://www.techdirt.com/tag/shoot-the-messenger/\">broadly written law</a> not only allowed prosecutors to charge people with federal crimes for doing nothing more than interacting with services/servers/etc. in unexpected ways, but allowed companies to, essentially, shoot the messengers for reporting data breaches, unsecured servers, or sloppy user interfaces that could be exploited to display far more information than those running them intended.</p><p>Here‚Äôs what Metropolitan Washington Airports Authority investigator Patrick Silsbee wrote in his report:</p><blockquote><p><em>‚ÄúThe video shows camera angles and views that can only be found on the Metropolitan Washington Airport‚Äôs Authority CCTV video,‚Äù Silsbee wrote in a January 31 report, noting the location of landmarks in the videos, including a boathouse near the airfield.</em></p><p><em>The locations of the MWAA security cameras are redacted in the reports provided to The Intercept, ostensibly ‚Äúto prevent the disclosure of law enforcement and security techniques and procedures not generally known outside the law enforcement community,‚Äù according to an accompanying letter from MWAA.</em></p></blockquote><p>That doesn‚Äôt mean much by itself, but Silsbee apparently figured out (thanks in part to CNN‚Äôs initial failure to redact some CCTV text that described the location of the camera) this footage must have been obtained by an MWAA employee working at the police dispatch center. </p><p>CCTV footage from  the dispatch center was obtained, which allegedly showed these actions being taken by the suspected leaker:</p><blockquote><p><em>‚ÄúBetween the hours of 2256 and 0545, Mr. Mbengue can be seen on multiple occasions utilize [sic] his personal cell phone to record video and photograph these critical scenes,‚Äù Silsbee wrote.</em></p></blockquote><p>That would be MWAA dispatch employee Mohamed Mbengue, who has since pleaded ‚Äúno contest‚Äù to charges stemming from Virginia‚Äôs ultra-vague <a href=\"https://law.lis.virginia.gov/vacode/18.2-152.4/\" data-type=\"link\" data-id=\"https://law.lis.virginia.gov/vacode/18.2-152.4/\">‚Äúcomputer trespass‚Äù law</a>. But it really takes a person with an overriding desire to shoot messengers to call cell phone recordings of screen images a ‚Äútrespass.‚Äù </p><p>The word is generally understood to describe unauthorized access to an area a person is not allowed to be in. Mbengue was at work and had full access to these recordings as a part of his job. That he recorded them and sent them to CNN doesn‚Äôt align with any rational definition of the word ‚Äútrespass.‚Äù The dissemination of footage may be a violation of policy, but policy violations aren‚Äôt criminal charges ‚Äî the sort of thing that can do permanent damage to a person‚Äôs life in ways that write-ups and even justified terminations simply can‚Äôt.</p><p>That‚Äôs why discretion is key. But when discretion matters most, law enforcement tends to deliberately ‚Äúerr‚Äù on the side of whatever does the most damage to anyone it happens to be investigating. And it appears MWAA investigators are more than happy to throw criminal charges at people for, at most, violating agency policies. A second dispatcher (Jonathan Savoy) was caught doing the same thing (albeit without sharing the recordings with CNN) and faced similar charges until someone actually exercised a bit of discretion and declined to move forward with the case.</p><blockquote><p><em>On February 3, the MWAA&nbsp;<a href=\"https://x.com/allisonpapson/status/1886553371257266540?s=46&amp;t=p5q6YKIPojSzB8gCMgZMiA\" target=\"_blank\" rel=\"noreferrer noopener\">announced</a>&nbsp;both men‚Äôs arrests, writing in a press statement that Savoy had been arrested ‚Äúfollowing further police investigation.‚Äù</em></p><p><em>In May, however, local prosecutors&nbsp;<a href=\"https://theintercept.com/2025/05/29/charges-dropped-leaked-dc-plane-crash-video/\">quietly dropped</a>&nbsp;the charges against Savoy, through a filing called a ‚Äúnolle prosequi,‚Äù according to the court docket.</em></p></blockquote><p>There‚Äôs absolutely nothing in the statute that actually covers the actions described here, which formed the basis for the bullshit criminal charges. It takes a ton of punitive imagination to turn ‚Äúrecording a CCTV monitor with a phone‚Äù into a criminal act. The only clause that could be even possibly be considered applicable requires investigators and prosecutors to engage in lot of extremely creative re-interpretations of <a href=\"https://law.lis.virginia.gov/vacode/18.2-152.4/\" data-type=\"link\" data-id=\"https://law.lis.virginia.gov/vacode/18.2-152.4/\">the plain text of the law</a>: </p><blockquote><p><em>Use a computer or computer network to make or cause to be made an unauthorized copy, in any form, including, but not limited to, any printed or electronic form of computer data, computer programs or computer software residing in, communicated by, or produced by a computer or computer network</em></p></blockquote><p>A smartphone is a computer. A recording could be considered an ‚Äúunauthorized copy.‚Äù To call the CCTV cameras and screens ‚Äúcomputers/computer network‚Äù means ignoring the generally understood utility of this tech. Even if a network connects the cameras and a computer provides access to recordings, recording playback via phone while accessing footage the suspects <em>had every right to access</em>, calling this a violation of the law demonstrates investigators were out for revenge, rather than serving the commonly understood definition of the word ‚Äújustice.‚Äù</p>","contentLength":5754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44991542"},{"title":"Popular Japanese smartphone games have introduced external payment systems","url":"https://english.kyodonews.net/articles/-/59689","date":1755906613,"author":"anigbrowl","guid":237169,"unread":true,"content":"<p>TOKYO - Nearly 70 percent of popular Japanese smartphone games have introduced external payment systems for items and services to avoid hefty commission fees from U.S. tech giants Google LLC and Apple Inc., a Kyodo News tally showed.</p><p>The move comes ahead of a new Japanese law tightening regulations on Google and Apple, which dominate smartphone platforms, set to take full effect in December. The legislation requires the two companies to open their payment systems.</p><p>Almost all users currently download games through Apple and Google's app stores. When players buy in-game items, software providers pay the tech giants commissions of up to 30 percent.</p><p>A Kyodo News survey found that among the top 30 best-selling game titles in 2024, at least 11 of the 16 offered by domestic companies have introduced payments through external websites.</p><p>Although the two tech giants say the fees are necessary to protect user privacy and security, the costs have weighed on game makers.</p><p>For outside transactions, users make payments through channels other than apps, such as game websites. Settlement service providers like Digital Garage Inc. and GMO Tech Inc. typically charge a 5 percent commission, far below Apple's and Google's rates.</p><p>Payments through external websites in the in-app purchase market, estimated at over 1 trillion yen ($6.8 billion), are expected to bring user discounts and boost providers' profitability, analysts said.</p><p>In the survey, Kyodo News received responses from eight of 12 domestic game makers, while two declined to comment and two did not respond. Of the 12 titles from the eight firms, 11 have adopted external settlements.</p><p>In August last year, Mixi Inc. introduced an outside settlement system for its blockbuster game \"Monster Strike,\" allowing its users to purchase about 5 percent more items compared with in-app payments.</p>","contentLength":1840,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44991384"},{"title":"Mail Carriers Pause US Deliveries as Tariff Shift Sows Confusion","url":"https://www.bloomberg.com/news/articles/2025-08-21/global-mail-services-halt-us-deliveries-ahead-of-de-minimis-end","date":1755904144,"author":"voxadam","guid":237168,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44991039"},{"title":"Bluesky Goes Dark in Mississippi over Age Verification Law","url":"https://www.wired.com/story/bluesky-goes-dark-in-mississippi-age-verification/","date":1755903087,"author":"BallsInIt","guid":237198,"unread":true,"content":"<p> can no longer use the social media platform <a href=\"https://www.wired.com/tag/bluesky/\">Bluesky</a>. The company announced Friday that it will be blocking all IP addresses within Mississippi for the foreseeable future in response to a recent <a href=\"https://www.wired.com/tag/us-supreme-court/\">US Supreme Court</a> decision that allows the state to enforce strict age verification for social media platforms.</p><p>According to Bluesky, Mississippi‚Äôs approach to verification ‚Äúwould fundamentally change‚Äù how users access the site. ‚ÄúWe think this law creates challenges that go beyond its child safety goals, and creates significant barriers that limit free speech and disproportionately harm smaller platforms and emerging technologies,‚Äù the Bluesky team said in <a data-offer-url=\"https://bsky.social/about/blog/08-22-2025-mississippi-hb1126\" data-event-click=\"{&quot;element&quot;:&quot;ExternalLink&quot;,&quot;outgoingURL&quot;:&quot;https://bsky.social/about/blog/08-22-2025-mississippi-hb1126&quot;}\" href=\"https://bsky.social/about/blog/08-22-2025-mississippi-hb1126\" rel=\"nofollow noopener\" target=\"_blank\">its statement</a>.</p><p>Bluesky did not respond to a request for comment.</p><p>The company says that compliance with Mississippi‚Äôs law‚Äîwhich would require identifying and tracking all users under 18, in addition to asking every user for sensitive personal information to verify their age‚Äîis not possible with the team‚Äôs current resources and infrastructure. By not complying with the law, Bluesky could face fines of up to $10,000 per violation. It is the first major social media platform to take such drastic steps in response to the law.</p><p>Age verification laws, which on the surface are intended to protect children from harmful content online, have already begun to broadly impact internet use in places around the world where they've been enacted. <a href=\"https://www.wired.com/story/the-age-checked-internet-has-arrived/\">In the UK</a>, users trying to access everything from pornography to social platforms must now submit to ID scans, credit card checks, age-estimation scans, and more to verify they‚Äôre over the age of 18. The state of Texas has a <a href=\"https://www.wired.com/story/us-supreme-court-porn-age-verification-decision-2025/\">similar law</a> the US Supreme Court upheld in June, despite concerns from critics over the erosion of free speech and access to information on the open internet.</p><p>Whether these laws are effective at protecting children is unclear; the use of virtual private networks (VPNs) in the UK <a href=\"https://www.wired.com/story/vpn-use-spike-age-verification-laws-uk/\">spiked</a> just after its age verification law went into effect as users deployed the tech to spoof their location. On platforms like Discord, people discovered they could use <a href=\"https://www.wired.com/story/age-verification-is-sweeping-gaming-is-it-ready-for-the-age-of-ai-fakes/\">video game characters</a> to trick face scans. Furthermore, <a href=\"https://www.wired.com/story/the-age-checked-internet-has-arrived/\">critics say</a> that age verification laws intended to reduce harm to children can sometimes have the opposite effect by putting kids in greater danger of identity theft and privacy violations.</p><p>WIRED has reached out to the sponsors of the original bill, Mississippi state representatives Jill Ford, Fabian Nelson, and Larry Byrd, and will update this story if they comment.</p><p>‚ÄúWe believe effective child safety policies should be carefully tailored to address real harms, without creating huge obstacles for smaller providers and resulting in negative consequences for free expression,‚Äù Bluesky wrote.</p>","contentLength":2738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44990886"},{"title":"U.S. government takes 10% stake in Intel","url":"https://www.cnbc.com/2025/08/22/intel-goverment-equity-stake.html","date":1755896512,"author":"givemeethekeys","guid":237107,"unread":true,"content":"<div data-test=\"InlineImage\"><div><div><div>Lip-Bu Tan, chief executive officer of Intel Corp., departs following a meeting at the White House in Washington, DC, US, on Monday, Aug. 11, 2025. </div><div>Alex Wroblewski | Bloomberg | Getty Images</div></div></div></div><div><p>Commerce Secretary Howard Lutnick said on Friday that the U.S. government has taken a 10% stake in embattle chipmaker Intel, the Trump administration's latest effort to exert control over corporate America. </p><p> shares rose about 6% during trading on Friday. They were flat in extended trading.</p><p>Intel, the only American company capable of making advanced chips on U.S. soil, said in a press release that the government made an $8.9 billion investment in Intel common stock, purchasing 433.3 million shares at a price of $20.47 per share, giving it a 10% stake in the company. Intel noted that the price the government paid was a discount to the current market price.</p><p>Of the total, $5.7 billion of the government funds will come from grants under the CHIPS Act that had been awarded but not paid, and $3.2 billion will come from separate government awards under a program to make secure chips.</p><p>\"The United States paid nothing for these Shares, and the Shares are now valued at approximately $11 Billion Dollars,\" President Trump wrote in a post on Truth Social. \"This is a great Deal for America and, also, a great Deal for INTEL.\"&nbsp;</p><p>The government will also have a warrant to buy an additional 5% of Intel shares if the company is no longer majority owner of its foundry business.</p><p>Intel said that the U.S. government won't have a board seat or other governance rights.</p><p>\"As the only semiconductor company that does leading-edge logic R&amp;D and manufacturing in the U.S., Intel is deeply committed to ensuring the world's most advanced technologies are American made,\" Intel CEO Lip-Bu Tan said in the press release. </p></div><div><p>Earlier on Friday, <a href=\"https://www.cnbc.com/donald-trump/\">President Donald Trump</a> said the government should get about 10% of the company, which has a market cap of just over $100 billion. &nbsp;</p><p>\"They've agreed to do it and I think it's a great deal for them,\" Trump told reporters Friday at the White House</p><p>White House officials previously told CNBC that Trump and Tan will meet on Friday afternoon. Lutnick's post included a photo with Tan.</p></div><div><p>The marks the latest example of a distinct shift in U.S. industrial policy, with the government taking an active role in the private sector. Lutnick <a href=\"https://www.cnbc.com/2025/08/19/lutnick-intel-stock-chips-trump.html\">told CNBC this week</a> that the U.S. government was seeking an equity stake in Intel in exchange for CHIPS Act funds.</p><p>\"We should get an equity stake for our money,\" Lutnick said on CNBC's \"<a href=\"https://www.cnbc.com/squawk-on-the-street/\">Squawk on the Street</a>.\" \"So we'll deliver the money, which was already committed under the Biden administration. We'll get equity in return for it.\"</p><p>Earlier this week, Intel announced another major backer, when SoftBank said it would make a $2 billion investment in the chipmaker, equal to about 2% of the company.</p><p>Intel has been spending billions of dollars to build a series of chip factories in Ohio, an area the company previously called the \"Silicon Heartland,\" where Intel would be able to produce the most advanced chips, including for AI.</p><p>But in July, Tan said in a memo to employees that there would be \"no more blank checks,\" and that it was slowing down the construction of its Ohio factory complex, depending on market conditions. Intel's Ohio factory is now scheduled to start operations in 2030.</p><p>Intel said last fall that it had <a href=\"https://www.cnbc.com/2024/11/25/intel-close-to-8-billion-chips-act-grant-source.html\">finalized</a> a nearly $8 billion grant under the CHIPS and Science Act to fund its factory-building plans. The CHIPS Act was passed in 2022, under the Biden administration.</p><p><em>‚Äî CNBC's David Sucherman contributed to this report.</em></p></div>","contentLength":3585,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44989773"},{"title":"Nitro: A tiny but flexible init system and process supervisor","url":"https://git.vuxu.org/nitro/about/","date":1755889589,"author":"todsacerdoti","guid":237106,"unread":true,"content":"<p>Nitro is a tiny process supervisor that also can be used as pid 1 on Linux.</p><p>There are four main applications it is designed for:</p><ul><li>As init for a Linux machine for embedded, desktop or server purposes</li><li>As init for a Linux initramfs</li><li>As init for a Linux container (Docker/Podman/LXC/Kubernetes)</li><li>As unprivileged supervision daemon on POSIX systems</li></ul><p>Nitro is configured by a directory of scripts, defaulting to\n (or the first command line argument).</p><ul><li>Kernel support for Unix sockets</li><li> or writable  on another fs</li></ul><h2>Benefits over other systems</h2><ul><li>All state is kept in RAM, works without tricks on read-only root file systems.</li><li>Efficient event-driven, polling free operation.</li><li>Zero memory allocations during runtime.</li><li>No unbounded file descriptor usage during runtime.</li><li>One single self-contained binary, plus one optional binary to\ncontrol the system.</li><li>No configuration compilation steps needed, services are simple\ndirectories containing scripts.</li><li>Supports reliable restarting of services.</li><li>Reliable logging mechanisms per service or as default.</li><li>Support for logging chains spread over several services.</li><li>Works independently of properly set system clock.</li><li>Can be run on FreeBSD from /etc/ttys (sets up file descriptors 0, 1, 2).</li><li>Tiny static binary when using musl libc.</li></ul><p>Every directory inside  (or your custom service directory)\ncan contain several files:</p><ul><li>, an optional executable file that is run before the service starts.\nIt must exit with status 0 to continue.</li><li>, an optional executable file that runs the service;\nit must not exit as long as the service is considered running.\nIf there is no  script, the service is considered a ‚Äúone shot‚Äù,\nand stays ‚Äúup‚Äù until it‚Äôs explicitly taken ‚Äúdown‚Äù.</li><li>, an optional executable file that is run after the \nprocess finished.  It is passed two arguments, the exit status\nof the  process (or -1 if it was killed by a signal)\nand the signal that killed it (or 0, if it exited regularly).</li><li>, a symlink to another service directory.\nThe standard output of  is connected to the standard input of the\nservice under  by a pipe.  You can chain these for reliable and\nsupervised log processing.</li><li>, an optional file that causes nitro to not bring up this\nservice by default.</li><li>Service directories ending with ‚Äò@‚Äô are ignored; they can be used\nfor parameterized services.</li><li>Service names must be shorter than 64 chars, and not contain ,\n or newlines.</li></ul><p>You may find runit‚Äôs  useful when writing  scripts.</p><ul><li>: this service is used as a logging service for all services\nthat don‚Äôt have a  symlink.</li><li>:  is run before other services are brought up.\nYou can already use  in  to bring up services\nin a certain order.\n is run before all remaining services are killed and the\nsystem is brought down.\nAfter all processes are terminated,  is run.\nThe program , if it exists, is run instead of exiting\nwhen an unrecoverable, fatal error happens.\nThe program , if it exists, is executed into\ninstead of a shutdown.  This can be used to implement an initramfs,\nfor example.</li></ul><p>Service directories ending in  are ignored, however you can refer\nto parametrized services by symlinks (either in the service directory\nor as a  symlink), or start them manually using .</p><p>The part after the , the parameter, is passed to the scripts as\nfirst argument.</p><p>For example, given you have a script  and a symlink\n -&gt; , nitro will spawn .  Upon\nrunning , nitro will spawn , even if it does not exist in the service directory.</p><p>The lifecycle of a machine/container/session using nitro consists of\nthree phases.</p><p>First, the system is brought up.  If there is a special service\ng, its  script is run first.  After it finishes, all\nservices not marked  are brought up.</p><p>When a service exits, it‚Äôs being restarted, potentially waiting for\ntwo seconds if the last restart happened too quickly.</p><p>By using  or , the system can be\nbrought down.  If it exists,  will be run.  After this,\nnitro will send a SIGTERM signal to all running services and waits for\nup to 7 seconds for the service to exit.  Otherwise, a SIGKILL is\nsent.  After all processes are terminated,  is run.</p><p>Finally, nitro reboots or shuts down the system; or just exits when it\nwas used as a container init or unprivileged supervisor.  (When a\nreboot was requested, it re-execs itself.  This requires being called\nwith absolute path for the binary and the service directory.)</p><h2>Controlling nitro with nitroctl</h2><p>You can remote control a running nitro instance using the tool\n.</p><p>Usage: <code>nitroctl [COMMAND] [SERVICE]</code></p><ul><li>list: show a list of services and their state, pid, uptime and last\nexit status.</li><li>down: stop SERVICE (sending SIGTERM or the first letter of )</li><li>start: start SERVICE, waiting for success</li><li>restart: restart SERVICE, waiting for success</li><li>stop: stop SERVICE, waiting for success</li><li>p: send signal SIGSTOP to SERVICE</li><li>c: send signal SIGCONT to SERVICE</li><li>h: send signal SIGHUP to SERVICE</li><li>a: send signal SIGALRM to SERVICE</li><li>i: send signal SIGINT to SERVICE</li><li>q: send signal SIGQUIT to SERVICE</li><li>1: send signal SIGUSR1 to SERVICE</li><li>2: send signal SIGUSR2 to SERVICE</li><li>t: send signal SIGTERM to SERVICE</li><li>k: send signal SIGKILL to SERVICE</li><li>pidof: print the PID of the SERVICE, or return 1 if it‚Äôs not up</li><li>rescan: re-read , start added daemons, stop removed daemons</li><li>Shutdown: shutdown (poweroff) the system</li><li>Reboot: reboot the system</li></ul><h2>Controlling nitro by signals</h2><p>rescan can also be triggered by sending  to nitro.</p><p>reboot can also be triggered by sending  to nitro.</p><p>shutdown can also be triggered by sending  to nitro, unless\nnitro is used as Linux pid 1.</p><p>Nitro is self-contained and can be booted directly as pid 1.\nIt will mount  and  when required, everything else\nshould be done with .</p><p>When receiving Ctrl-Alt-Delete, nitro triggers an orderly reboot.</p><h2>Nitro as init for a Docker container</h2><p>Nitro is compiled statically, so you can copy it into your container easily:</p><pre><code>COPY ./nitro /bin/\nCOPY ./nitroctl /bin/\nCMD [\"/bin/nitro\"]\n</code></pre><p>Note that  must exist in the container if you want to use the\ndefault control socket name.</p><p>You can put the control socket onto a bind mount and remote control\n using  from the outside by pointing  to\nthe appropriate target.</p><p>You can add this line to  to run  supervised by\nFreeBSD :</p><pre><code>/etc/nitro \"/usr/local/sbin/nitro\" \"\" on\n</code></pre><p>I‚Äôm standing on the shoulder of giants; this software would not have\nbeen possible without detailed study of prior systems such as\ndaemontools, freedt, runit, perp, and s6.</p><p>nitro is licensed under the 0BSD license, see LICENSE for details.</p>","contentLength":6330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44988530"},{"title":"Scientists just found a protein that reverses brain aging","url":"https://www.sciencedaily.com/releases/2025/08/250820000808.htm","date":1755888987,"author":"stevenjgarner","guid":237063,"unread":true,"content":"<p>Now, researchers at UC San Francisco have identified a protein that's at the center of this decline.</p><p>They looked at how the genes and proteins in the hippocampus changed over time in mice and found just one that differed between old and young animals. It's called FTL1.</p><p>Old mice had more FTL1, as well as fewer connections between brain cells in the hippocampus and diminished cognitive abilities.</p><p>When the researchers artificially increased FTL1 levels in young mice, their brains and behavior began to resemble that of old mice.</p><p>In experiments in petri dishes, nerve cells engineered to make lots of FTL1 grew simple, one-armed neurites -- rather than the branching neurites that normal cells create.</p><p>But once the scientists reduced the amount of FTL1 in the hippocampus of the old mice, they regained their youth. They had more connections between nerve cells, and the mice did better on memory tests.</p><p>\"It is truly a reversal of impairments,\" said Saul Villeda, PhD, associate director of the UCSF Bakar Aging Research Institute and senior author of the paper, which appears in  on Aug. 19. \"It's much more than merely delaying or preventing symptoms.\"</p><p>In old mice, FTL1 also slowed down metabolism in the cells of the hippocampus. But treating the cells with a compound that stimulates metabolism prevented these effects.</p><p>Villeda is optimistic the work could lead to therapies that block the effects of FTL1 in the brain.</p><p>\"We're seeing more opportunities to alleviate the worst consequences of old age,\" he said. \"It's a hopeful time to be working on the biology of aging.\"</p><p>Authors: Other UCSF authors are Laura Remesal, PhD, Juliana Sucharov-Costa, Karishma J.B. Pratt, PhD, Gregor Bieri, PhD, Amber Philp, PhD, Mason Phan, Turan Aghayev, MD, PhD, Charles W. White III, PhD, Elizabeth G. Wheatley, PhD, Brandon R. Desousa, Isha H. Jian, Jason C. Maynard, PhD, and Alma L. Burlingame, PhD. For all authors see the paper.</p><p>Funding: This work was funded in part by the Simons Foundation, Bakar Family Foundation, National Science Foundation, Hillblom Foundation, Bakar Aging Research Institute, Marc and Lynne Benioff, and the National Institutes of Health (AG081038, AG067740, AG062357, P30 DK063720). For all funding see the paper.</p>","contentLength":2223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44988393"},{"title":"Show HN: JavaScript-free (X)HTML Includes","url":"https://github.com/Evidlo/xsl-website","date":1755888450,"author":"Evidlo","guid":237171,"unread":true,"content":"<p>I've been working on a little demo for how to avoid copy-pasting header/footer boilerplate on a simple static webpage. My goal is to approximate the experience of Jekyll/Hugo but eliminate the need for a build step before publishing. This demo shows how to get basic templating features with XSL so you could write a blog post which looks like</p><pre><code></code></pre>\nSome properties which set this approach apart from other methods:<pre><code>  - no build step (no need to setup Jekyll on the client or configure Github/Gitlab actions)\n  - works on any webserver (e.g. as opposed to server-side includes, actions)\n  - normal looking URLs (e.g. `example.com/foobar` as opposed to `example.com/#page=foobar`)\n</code></pre>\nThere's been some talk about removing XSLT support from the HTML spec [0], so I figured I would show this proof of concept while it still works.","contentLength":818,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44988271"},{"title":"The first Media over QUIC CDN: Cloudflare","url":"https://moq.dev/blog/first-cdn/","date":1755887056,"author":"kixelated","guid":237105,"unread":true,"content":"<p>üö® It‚Äôs finally happening! üö®</p><p>Cloudflare has <a href=\"https://blog.cloudflare.com/moq/\">just announced</a> their Media over QUIC CDN!\nIt‚Äôs an , and you can test MoQ on their , anycast network.\nTry it out, and convince your boss‚Äô boss that the writing is on the wall.</p><p>If you‚Äôve been living under a rock, MoQ is an <a href=\"https://datatracker.ietf.org/group/moq/about/\">up-and-coming standard</a> for live media, aiming to supplant <a href=\"https://moq.dev/blog/replacing-webrtc\">WebRTC</a>, <a href=\"https://moq.dev/blog/replacing-hls-dash\">HLS/DASH</a>, and even  as the one to rule them all.\nAnd now Cloudflare wins the award for the first CDN offering!</p><figure><figcaption>Your prize is a blog post. You‚Äôre welcome mega-corp.</figcaption></figure><p>Also, , some shameless self-promotion: I just soft-launched <a href=\"https://moq.dev/blog/first-app\">hang.live</a>.\nCheck it out if you want to see the  cool stuff you can do with MoQ.</p><p>I‚Äôm biased so naturally I‚Äôm going to use <a href=\"https://github.com/kixelated/moq/tree/main/js/hang\">@kixelated/hang</a> (smash that star button).\nYou can publish a live broadcast in the browser using the <a href=\"https://moq.dev/publish\">web demo</a> or the <a href=\"https://github.com/kixelated/moq/blob/main/js/hang-demo/src/publish.html#L25\">library</a>:</p><pre tabindex=\"0\" data-language=\"html\"><code></code></pre><p>There‚Äôs a link to watch your live broadcast using the <a href=\"https://moq.dev/watch\">web demo</a>, or again you can use the <a href=\"https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/js/hang-demo/src/index.html#L30\">library</a>:</p><pre tabindex=\"0\" data-language=\"html\"><code></code></pre><p>You might even notice  because I‚Äôve been experimenting with AI features (gotta get funding eventually üí∞).\nThey‚Äôre generated  using <a href=\"https://github.com/snakers4/silero-vad\">silero-vad</a> + <a href=\"https://github.com/openai/whisper\">whisper</a> + <a href=\"https://huggingface.co/docs/transformers.js/en/index\">transformers.js</a> + <a href=\"https://github.com/microsoft/onnxruntime\">onnxruntime-web</a> + <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WebGPU_API\">WebGPU</a> and transmitted using MoQ of course.\nBut that‚Äôs a whole separate blog post; it‚Äôs pretty cool.</p><p> You don‚Äôt have to use this <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Web_components\">Web Component</a> API.\n<a href=\"https://moq.dev/blog/first-app\">hang.live</a> uses the far more powerful Javascript API to do more complicated stuff like get access to individual video frames.\nThere‚Äôs a  section at the end of this blog if you LOVE sample code, but I‚Äôm not going to bore the rest of you.</p><p>There‚Äôs also a ü¶Ä Rust ü¶Ä library <a href=\"https://github.com/kixelated/moq/tree/main/rs/hang\">to import MP4</a>, <a href=\"https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/rs/justfile#L103\">pipe media from ffmpeg</a>, and <a href=\"https://github.com/kixelated/moq/blob/9f5f6153458c03f255877a036e36f68f742d5c85/rs/justfile#L119\">publish/watch using gstreamer</a> so you can do more complicated media stuff without ü§Æ Javascript ü§Æ.\nI wish I could spend more time on the Rust side but  is a big deal.\nWe are no longer forced to use WebRTC, but that also means we need to build our own WebRTC in ü§Æ Javascript ü§Æ.\nI can suffer and you can reap the rewards.</p><h2>What‚Äôs not available yet?</h2><p>This is a  release.\nCloudflare is only supporting a  subset of an <a href=\"https://www.ietf.org/archive/id/draft-ietf-moq-transport-07.html\">old draft</a>, which is even smaller than <a href=\"https://www.ietf.org/archive/id/draft-lcurley-moq-lite-01.html\">my tiny subset</a>.\nThey‚Äôre using a <a href=\"https://github.com/englishm/moq-rs\">fork</a> of my terrible code so bugs are guaranteed.</p><ul><li><strong>There‚Äôs no authentication yet</strong>: choose an unguessable name for each broadcast.</li><li><strong>There‚Äôs no ANNOUNCE support</strong>: my <a href=\"https://github.com/kixelated/moq/blob/main/js/hang-demo/src/meet.html\">conferencing example</a> uses  to discover when broadcasts start/stop, so that won‚Äôt work.</li><li><strong>Nothing has been optimized</strong>: the user experience will improve over time.</li></ul><p>If any of these are deal breakers, then you could always run your own <a href=\"https://github.com/kixelated/moq/tree/main/rs/moq-relay\">moq-relay</a> in the meantime.\nI‚Äôve been adding new features and fixing a bunch of stuff  Cloudflare smashed that fork button.\nFor example, authentication (via JWT) and a WebSocket fallback for Safari/TCP support.</p><p>There‚Äôs even a <a href=\"https://github.com/kixelated/moq.dev/blob/main/infra/relay.tf\">terraform module</a> that powers .\nYou too can run your own ‚Äúglobal‚Äù CDN with 3 nodes and pay GCP a boatload of money for the privilege.\nIt‚Äôs not  as good as Cloudflare‚Äôs network, currently available for free‚Ä¶</p><p>Or host  yourself!\nIt should even work on private networks provided you <a href=\"https://moq.dev/blog/tls-and-quic\">wrestle with TLS certificates</a>.\nI‚Äôd also love to get MoQ running over <a href=\"https://www.iroh.computer/\">Iroh</a> for peer-to-peer action if anybody wants to help.</p><p>As a great philosopher once said:</p><blockquote><p>Apathy is a tragedy and boredom is a crime.\n- <a href=\"https://www.youtube.com/watch?v=k1BneeJTDcU\">Bo Burnham</a></p></blockquote><p>This is a big deal.\nThe biggest of deals.\nThe HUGEST of deals.</p><p>I‚Äôve been an <a href=\"https://moq.dev/blog/transfork\">outspoken critic</a> of the MoQ standardization process.\nIt‚Äôs just really difficult to design a protocol, via a cross-company committee, before there‚Äôs been any real world usage.\nIt‚Äôs been over 3 years since I fought Amazon lawyers and published my <a href=\"https://www.ietf.org/archive/id/draft-lcurley-warp-00.html\">first MoQ draft</a>.\nIt‚Äôs going to be at least another 3 years before even the <a href=\"https://datatracker.ietf.org/doc/draft-ietf-moq-transport/\">base networking layer</a> becomes an RFC.</p><p>\nThe best standards take a while.\nLook no further than QUIC, deployed by Google in 2012, started standardization in 2015, with the RFC released in 2021.\nAnd they had a boatload of production data to shape the specification.\nMeanwhile, we have only had a <a href=\"https://moq.dev/watch\">Big Buck Bunny demo</a>, and I believe the standard has veered off course as a result.</p><p>Cloudflare has done something fantastic and said:</p><blockquote><p>fuck waiting for a RFC, let‚Äôs release something</p></blockquote><p>Okay they didn‚Äôt say that, but this is  the mentality that MoQ needs right now.\n.\n.\n.</p><figure><figcaption>Holy shit I‚Äôm Shia LaBeouf.</figcaption></figure><p>Arguing in the <a href=\"https://github.com/moq-wg/moq-transport/issues\">650+ issues</a> and <a href=\"https://github.com/moq-wg/moq-transport/pulls\">500+ PRs</a> can wait for another day.\nTweaking the messaging encoding for the hundredth time can wait for another day.\nWe‚Äôre still going to make sure that MoQ gets standardized , but it‚Äôs more important to get  out there.</p><p>I‚Äôm looking at you: Google, Akamai, Fastly, etc.\nTake some code, run it on some spare servers, and start to learn what customers need  you design the protocol.</p><p>We‚Äôre effectively trying to reimplement WebRTC / HLS / RTMP using relatively new Web APIs.\nDon‚Äôt judge MoQ based on these initial offerings.\nWe‚Äôve got a  of work to do.\n.</p><p><a href=\"https://discord.gg/FCYF3p99mr\">Join the Discord</a>.\nSomehow there‚Äôs 900+ people in there.\nPing me and I will do whatever I can to help.\n if it means putting one more nail in the WebRTC coffin.</p><h2>Javascript is an Abomination</h2><p>You win some bonus documentation.\nCongrats!\nI knew you would win.</p><p>Here‚Äôs an example of my reactive library in action.\nIt powers <a href=\"https://moq.dev/blog/first-app\">hang.live</a> so the API is subject to change and is probably already out of date.\nWhen in doubt, <a href=\"https://github.com/kixelated/moq/tree/main/js/hang\">consult the source code</a> like the hacker you are.</p><pre tabindex=\"0\" data-language=\"typescript\"><code></code></pre><p>There‚Äôs even some  features behind undocumented APIs.\nLike running an object detection model in browser and publishing the results as a MoQ track.\nStay tuned for a blog post about that if I can figure out a better use-case than a cat cam. üêà</p><pre tabindex=\"0\" data-language=\"typescript\"><code></code></pre><p>Also, for the record, Typescript is really nice.\nü§Æ Javascript ü§Æ is still an abomination.</p>","contentLength":5578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44987924"},{"title":"Should the web platform adopt XSLT 3.0?","url":"https://github.com/whatwg/html/issues/11578","date":1755885372,"author":"protomolecool","guid":237167,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44987552"},{"title":"Sprinkling self-doubt on ChatGPT","url":"https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/","date":1755884727,"author":"ingve","guid":237062,"unread":true,"content":"<article data-pagefind-body=\"\"><a href=\"https://justin.searls.co/posts/sprinkling-self-doubt-on-chatgpt/\"></a><p>I replaced my ChatGPT personalization settings with this prompt a few weeks ago and promptly forgot about it:</p><blockquote><ul><li>Be extraordinarily skeptical of your own correctness or stated assumptions. You aren't a cynic, you are a highly critical thinker and this is tempered by your self-doubt: you absolutely hate being wrong but you live in constant fear of it</li><li>When appropriate, broaden the scope of inquiry beyond the stated assumptions to think through unconvenitional opportunities, risks, and pattern-matching to widen the aperture of solutions</li><li>Before calling anything \"done\" or \"working\", take a second look at it (\"red team\" it) to critically analyze that you really are done or it really is working</li></ul></blockquote><p>I noticed a difference in results right away (even though I kept forgetting the change was due to my instructions and not the separately <a href=\"https://garymarcus.substack.com/p/gpt-5-overdue-overhyped-and-underwhelming\">tumultuous rollout of GPT-5</a>).</p><p>Namely, pretty much every initial response now starts with:</p><ul><li>An expression of caution, self-doubt, and desire to get things right</li><li>Hilariously long \"thinking\" times (I asked it to estimate the macronutrients in lettuce yesterday and it spent 3 minutes and 59 seconds reasoning)</li><li>A post-hoc adversarial \"<a href=\"https://en.wikipedia.org/wiki/Red_team\">red team</a>\" analysis of whatever it just vomited up as an answer</li></ul><p>I'm delighted to report that ChatGPT's output has been more useful since this change. Still not altogether , but better at the margins. In particular, the \"red team\" analysis at the end of many requests frequently spots an error and causes it to arrive at the  answer, which‚Äîif nothing else‚Äîsaves me the step of expressing skepticism. And even when ChatGPT is nevertheless wrong, its penchant for extremely-long thinking times means I'm getting my money's worth in GPU time.</p></article>","contentLength":1693,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44987422"},{"title":"Leaving Gmail for Mailbox.org","url":"https://giuliomagnifico.blog/post/2025-08-18-leaving-gmail/","date":1755884508,"author":"giuliomagnifico","guid":237104,"unread":true,"content":"<p>This was a tough decision, having used Gmail since 2007/2008. However, I had to draw the line and stop giving Google my data for free.</p><p>The problem with email is that everything is transmitted in plain text. Technically, Google can store every message you receive and know everything, and U.S. agencies can request access to that data (this include also EU citizens under the <a href=\"https://policies.google.com/privacy/frameworks?hl=en-US\">EU-U.S. and Swiss-U.S. Data Privacy Frameworks</a>).</p><p>For someone like me, who cares about privacy and runs as much as possible on my own home servers, that felt like way too much.</p><p>So I decided to switch to another provider, one that respects privacy a bit more. Of course, this meant no longer ‚Äúpaying‚Äù with my personal data, but instead paying the actual price of the email service.</p><p>Let me start by saying: I use email in a very basic way. I send and receive a lot of messages (at least 50 a day), but they‚Äôre plain text/html emails with no attachments or fancy features. I couldn‚Äôt care less about the rest of the ‚Äúsuite\", like notes, contacts, calendars and all that extra stuff.</p><p>So, after a bit of research, I narrowed it down to three different services:</p><ul></ul><p>The last two providers offered true end-to-end encryption, at a cost of about ‚Ç¨3/4 per month. Sounds good‚Ä¶ but the catch is that to use their end-to-end encryption you‚Äôre forced to use their apps (or, on macOS, run a background ‚Äúbridge‚Äù).</p><p>That‚Äôs a no go for me, because I love Apple‚Äôs Mail app on macOS and iOS, it just works perfectly for my needs, and I don‚Äôt want to give that up.</p><p>So, I went with mailbox.org that still offers integrated PGP encryption, and if you want, you can always use external PGP too (which I was already doing with Gmail).</p><p>Mailbox.org has a solid plan: 10GB of email storage plus 5GB of cloud storage starting at ‚Ç¨2.50/month (paid annually). You can even expand the mail storage up to 100GB, at ‚Ç¨0.20 per gigabyte.</p><p>I was using around 2.5GB on Gmail, so I had no issues with paying the equivalent of two coffees a month for a huge boost in privacy. And if I ever need more space, I can just add it on-demand for ‚Ç¨0.20/GB.</p><p>There‚Äôs also a free one-month trial, but it‚Äôs pretty limited since you can‚Äôt send emails outside of mailbox.org domains.</p><p>So win the end, I registered my new address <code>giuliomagnifico@mailbox.org</code> and paid ‚Ç¨3 for a month of testing. That means I‚Äôm covered for two months, and then I can just ‚Äútop up‚Äù the account with ‚Ç¨30 for a full year.</p><div><div>Mailbox.org doesn‚Äôt use auto-renewal, so you have to manually top up your account. Nice feature</div></div><p>The web interface is extremely simple but very effective. I actually find it better than Gmail, less bloated of useless stuff.</p><p>And on mobile it‚Äôs very usable too.</p><img src=\"https://giuliomagnifico.blog/_images/2025/away-from-gmail/ios.jpeg\" alt=\"ios\"><p>One thing I prefer is using folders instead of Gmail‚Äôs ‚Äúlabels.‚Äù Mainly because this way I can put the folders directly under the account in Apple Mail (I think is the only email that can actually support this).</p><img src=\"https://giuliomagnifico.blog/_images/2025/away-from-gmail/folders.jpeg\" alt=\"folders\"><p>Mailbox.org also has all the features I need,\nand probably way more than I‚Äôll ever use. It even includes storage, video chat, an XMPP chat, task lists, calendar, contacts, an Etherpad (basically shared notes, I think), and so on‚Ä¶ none of which I really care about.</p><p>I decided to move all my emails from Gmail to mailbox.org, so I could (in future) completely wipe my Gmail account.</p><p>After creating an ‚Äúapp password‚Äù on Gmail, I installed the Docker image and ran the tool with this script:</p><pre tabindex=\"0\"><code>#!/bin/sh\nset -eu\n\nHOST1=\"imap.gmail.com\"\nUSER1=\"giuliomagnifico@gmail.com\"\nPASS1=\"xxx\"\n\nHOST2=\"imap.mailbox.org\"\nUSER2=\"giuliomagnifico@mailbox.org\"\nPASS2=\"xxx\"\n\nLOGDIR=\"/home/imapsync/logs\"\nmkdir -p \"$LOGDIR\"\nLOGFILE=\"$LOGDIR/sync_$(date +%F_%H-%M-%S).log\"\n\necho \"Starting: $(date)\"\ndocker compose run --rm imapsync imapsync \\\n  --host1 \"$HOST1\" --user1 \"$USER1\" --password1 \"$PASS1\" --ssl1 \\\n  --host2 \"$HOST2\" --user2 \"$USER2\" --password2 \"$PASS2\" --ssl2 \\\n  --automap --syncinternaldates --skipsize \\\n  --useuid --addheader --usecache --buffersize 4096 \\\n  --nofoldersizes --nofoldersizesatend \\\n  --exclude \"\\[Gmail\\]/All Mail\" \\\n  --regextrans2 \"s/\\[Imap\\]\\/Archive/Archive/\" \\\n  --log &gt; \"$LOGFILE\" 2&gt;&amp;1\n\necho \"Complete: $(date)\"\necho \"Log file: $LOGFILE\"\n</code></pre><p>The script excludes the All Mail folder\" using: <code>--exclude \"\\[Gmail\\]/All Mail\" \\</code></p><p>This to avoid duplicate emails already present in the folders, I also merged the  folder into the general Archive folder using: <code>--regextrans2 \"s/\\[Imap\\]\\/Archive/Archive/\"</code></p><p>This because Apple‚Äôs Mail app creates the  folder/label on Gmail whenever you use the ‚ÄúArchive‚Äù function instead of ‚ÄúTrash.‚Äù</p><p>The whole process took a couple of hours (11201secs, ~3h to be precise) during which I was monitoring the logs using: <code>tail -f /home/imapsync/logs/sync_2025-08-19_15-02-48.log</code></p><pre tabindex=\"0\"><code>[cut]\nmsg [Gmail]/Trash/183393 {19549}      copied to Trash/13361      2.36 msgs/s  200.418 KiB/s 2.140 GiB copied \nmsg [Gmail]/Trash/183394 {92245}      copied to Trash/13362      2.36 msgs/s  200.420 KiB/s 2.140 GiB copied \nmsg [Gmail]/Trash/183395 {19675}      copied to Trash/13363      2.36 msgs/s  200.415 KiB/s 2.140 GiB copied \nmsg [Gmail]/Trash/183396 {5953}       copied to Trash/13364      2.36 msgs/s  200.410 KiB/s 2.140 GiB copied \n++++ End looping on each folder\n++++ Statistics\nTransfer started on                     : Tuesday 19 August 2025-08-19 03:02:49 +0000 UTC\nTransfer ended on                       : Tuesday 19 August 2025-08-19 06:09:30 +0000 UTC\nTransfer time                           : 11201.5 sec\nFolders synced                          : 14/14 synced\nFolders deleted on host2                : 0 \nMessages transferred                    : 26407 \nMessages skipped                        : 0\nMessages found duplicate on host1       : 0\nMessages found duplicate on host2       : 0\nMessages found crossduplicate on host2  : 0\nMessages void (noheader) on host1       : 0  \nMessages void (noheader) on host2       : 0\nMessages found in host1 not in host2    : 0 messages\nMessages found in host2 not in host1    : 0 messages\nMessages deleted on host1               : 0\nMessages deleted on host2               : 0\nTotal bytes transferred                 : 2297647358 (2.140 GiB)\nTotal bytes skipped                     : 0 (0.000 KiB)\nMessage rate                            : 2.4 messages/s\nAverage bandwidth rate                  : 200.3 KiB/s\nReconnections to host1                  : 0\nReconnections to host2                  : 0\nMemory consumption at the end           : 268.7 MiB (*time 836.2 MiB*h) (started with 161.5 MiB)\nLoad end is                             : 0.06 0.08 0.08 1/1135 on 16 cores\nCPU time and %CPU                       : 446.72 sec 4.0 %CPU 0.2 %allcpus\nBiggest message transferred             : 30413995 bytes (29.005 MiB)\nMemory/biggest message ratio            : 9.3\nDetected 0 errors\nThis imapsync is up to date. ( local 2.306 &gt;= official 2.290 )( Use --noreleasecheck to avoid this release check. )\nHomepage: https://imapsync.lamiral.info/\nExiting with return value 0 (EX_OK: successful termination) 0/50 nb_errors/max_errors PID 1\nRemoving pidfile /var/tmp//tmp/imapsync.pid\nLog file is LOG_imapsync/2025_08_19_03_02_49_171_giuliomagnifico_gmail_com_giuliomagnifico_mailbox_org.txt ( to change it, use --logfile filepath ; or use --nolog to turn off logging )\n</code></pre><p>Of course, the full switch will be a gradual process, even though I‚Äôve already updated almost all my main services with the new address.</p><p>To make things easier, on my old Gmail account (which I removed from Apple Mail on all devices) I set up a forward to my new mailbox.org address.</p><p><img src=\"https://giuliomagnifico.blog/_images/2025/away-from-gmail/forward.jpeg\" alt=\"forward\">\nOn the new mailbox.org account, I also set up a filter to flag any emails that get forwarded from Gmail.</p><p>That way, I immediately notice them and I can update the address from Gmail to Mailbox.org whenever they show up. (The  tag is perfect for this, since it add a ‚Äúreal red flag‚Äù in Apple Mail on iOS, iPadOS and macOS)</p><p>Mailbox.org allows you to easily import your keys for PGP cryptography directly from the web. This is convenient as it lets you read and send PGP encrypted emails right from the browser on iOS, where there aren‚Äôt any ‚Äúdecent‚Äù apps for encrypted mail. The same goes for macOS, although there you can just use Thunderbird, which works really well.</p><p>Here‚Äôs how PGP emails look on iOS:</p><img src=\"https://giuliomagnifico.blog/_images/2025/away-from-gmail/gpgios.jpeg\" alt=\"PGPios\"><p>To send encrypted emails, you just select ‚ÄúUse PGP encrypted‚Äù when composing a new message, after importing your private key, of course.</p><p>And from the web interface, there‚Äôs also a handy feature to quickly import the sender‚Äôs public keys:</p><p>I‚Äôm satisfied. Leaving Gmail completely was something I wanted to do for a long time, but I was always hesitant. Finally, I made the switch, and, as often happens with these transitions, I discovered many unexpected positive aspects.</p><p>Oh, and if you have something to tell me or just want to test Mailbox.org after your switch, feel free to send me an email. Here‚Äôs my public key:</p><pre tabindex=\"0\"><code>-----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQINBGilAyEBEADAVi8ANnj22Au87TAgeodY9Cp24wRlVi/N1LBZFU8JVquuy9Dm\niqWs7FDBnPKUCRGU+tGWnro38oXCvQ4jKd2l6mORWMaHlYpA3bsbVtjJcneQI4TR\nZbIw8h25Hmloqy1hT6Cp4kf5C+fBo7DCtlYOUJmHN9H4nhWisALqpmWQmAmruaMy\nFlAhj/vWVe1bF6RkHgxaifgfRJpwHLevcBvsoASPxDLt8BMhITFK32iriR2JKjQ/\nfmRUwVm2x3QgGX/LbR4xzAfe53Hn5YWxGqUYJ5dtBrduHtyhdf9ChENY8sWcClE7\nJtR6FQ9Vmed3AG1GpBmX0Jemp1gZP6MBTTnZ9cWH9n9A9qH7NS7mpic7UD5BLaBk\nK4XeZCRAr58x2PyVQBUiZwcKa8XqPbQOP6HFHniAkmyBkthbhMVDTNvq17m2/6n6\nMdRQwpL/Wwc1+Fb2rgFI1naqXoxVpWqLs8Xb/AIfnQD13Y1liFV3N8aHbcZWhmzA\nALm0+lh1oFCL58VJ9jGi6DHHq/EKb5VMzR0SDb/PSDhxQU1HlE1UctBdd5659m+J\nOHhM+NeZMcjaZy7cimmuBmneHGJOemv3uPbn83srZDErzawBqh7lLQKf9MhvPxoD\nocueQ6/88hxBMONcPSCZ+0d4ABfngO0fik/uDDqcUPmqm1WpWwrRc0X4hwARAQAB\ntC5HaXVsaW8gTWFnbmlmaWNvIDxnaXVsaW9tYWduaWZpY29AbWFpbGJveC5vcmc+\niQJRBBMBCAA7FiEEXupXCErFrqjXs35nbC5LFXfhTvcFAmilAyECGwMFCwkIBwIC\nIgIGFQoJCAsCBBYCAwECHgcCF4AACgkQbC5LFXfhTvc0Ig//Vd9yk7sYP0dL8R54\nZfCpic5lCjmBeuMF8VZ3Ip0UqakHPzP4HGHHPM9/a9Lw3V8KtWa6cJWiMiOKR6eK\nKoObfHwzeXT7itNJrqjPLZ4NHwH6uL3DIweQCgAoVYDiKd0K83/PJDCihsKEqXSk\nNefqGB+lWQu6J6q79W1SAvXczTUbzplVqklYXRTUGE5lJS6yw0jGUTmrGuXReIDy\nCYK4vuKM0PZo1PmET0YqAkdWmXUUWJOZHdFaGezEtea/ss1OGhe9Nx+ZwHwYwOW/\nKU1Cgr1ZToYRlPxTA1X2sjpJzZGzGxPaqAEOkH7P/ZfwhBWbXU3bNCgI0bb7AzBm\nF+jPKU5j51kQk/a8xLQpQZ7sanoMmasaJwoZG6B20qk34ktSeW+yTncTNNKGWqiQ\nQxU6ptis0uTunL7LduOejRXXqDo/I69Vc2dyZWgsDhju5LD6WuniHs23jcl37ivp\nYsH6xdfteQmseJKEiGLDzCT+wd04EOtpKefoUvAQSXa5heuAwfEXfjoDQZnwsv7s\nBV1rN5xFYHnI6qkO/u6OpnfAJc9sWoBdclPzcswCvW0wzP1FxIle4u9p6Dej8sFU\nlU6t153v+kb7ohS7JEXiZvx43wZh7ADWvLCBDgHozOgvz7BXuFodaCILd+mMRLUO\nXdnWtOBa9/Enzrj4EegAU+m9/Mu5Ag0EaKUDIQEQAMkR6aiADscqU57zYo6YXugk\nxIAfidVRh5igGushqOlGb6ZyaI1KpMdXAATvCXj7Bczum/4EAyR0GpaR6V50UYz1\n2kmGD3tEEHtkK9jaUYkFWiKZJmYsCQ1MGzaTAM3yzMrbMfNnHDhvCfMhONPiZhm1\nLyN+6kBY8XFGIa8aemXTIdBG8mWufn9W7eImUs1wbBYgEXCUWbPWTkQUhL3yHFvo\nYRG0v7OGdQxw5Fon6YyBBgvXxIOHxR9WOBix2GZ92rZ2HI2dfVxE3uRWzo9gN5GB\ng3PhvZJDDcM4a9EYz1mASL++j9UnydQQDT1bnYWKtcQ0vJByPBLs1OlgN/lYgu/W\n5L1jW4NhhAiTaeGINZWqBrMeu5FBxqMCEZoo1oQmqd1KN1xOq9jiE09n9lwz/p2R\nsbmqFtVsZlBp+ThFXJuZ2F5oa87KvOY0eLqv8iAPIj+mxfDhnUhiNsne9C3Fm7Wu\nMG2euBVq2sG7F4+RC4Oszxin0XYSjNZ9B93WtN4h0nZN0Wh1V2bcBWmqKs62iZTC\n932iQidp77x/qldjQmQahrV+8Xueg5X3t5ODvnJDc4i/DtV0L+1cjUdXkEjKYeq7\n+beqbR941VLB86iqxJOrmyXzCCpqav+xa1CSfYg47EHEobSory5YM0QBZTlSfhcR\nrv+D85Lmv2eqihZhSdW7ABEBAAGJAjYEGAEIACAWIQRe6lcISsWuqNezfmdsLksV\nd+FO9wUCaKUDIQIbDAAKCRBsLksVd+FO92bID/9kSWBxWEvEv9oraFiR+T0GnHnY\nEvD1GWn3+Tnw2vg2bnkaDNI2BxAvuI9TkBLUlISwH8T1qG9VaBsz+VduFP+k6jc/\nCrl6Bmy6NiugzpAp4j7FMrNCvCQst+pc86s+GyvRlFe2O8vzFKyMQ5mzzYsLY3zG\n7IhxeQPNHmuq4XGlfYl9qU04pPsIFdEQRrB4lM52UAfBrb7SHdnmoGy4wRYYevf6\nOE2rQ8DXNnc345R1QK9Obog3U+QARuNIWnKiER1uy4VoMe9OqqM0eJr/aTQCv28t\nUIHGMQ2isfa72BDA/hfLDKzuorPAoSduxxONDE84N0JCu+f6a0N6cNXKXk+NV0Bn\nLIsgJMIxORVg9zqpzGhzFC3TFYn8fYuQWqjH0D9pGr86a6c6NL25qLDoNdPPzNyT\nmJoCo1vJB+zxhQotIbKzHBxNqfl+jRbWDhWP53TJyb3EAgnLzYDupTNlQucW2ihE\nCwRKB45qYMp+JfKV/DQHL82z5OpNpJ+KbRuMiE3qPpLGkTYsBY3wzORaNF+b7gAo\n77lLv4X54PbZ1bRK4b/r3pmewledaHhie7FF2Iyi4NSLUjecw9IRqrV0km8AaDGm\nSOLs0H+cLRQUxd9KWE0f1Cd7y5pV+9ABLNnCHIsY2JqjCLm19Ccb2x1zLCVH2Zv0\nQjuwt/KpUqS4qTLl/Q==\n=GpPW\n-----END PGP PUBLIC KEY BLOCK-----\n</code></pre>","contentLength":12034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44987380"},{"title":"Waymo granted permit to begin testing in New York City","url":"https://www.cnbc.com/2025/08/22/waymo-permit-new-york-city-nyc-rides.html","date":1755882168,"author":"achristmascarl","guid":237018,"unread":true,"content":"<div data-test=\"InlineImage\"><div><div><div>Waymo self-driving cars with roof-mounted sensor arrays traveling near palm trees and modern buildings along the Embarcadero, San Francisco, California, February 21, 2025.&nbsp;</div><div>Smith Collection/gado | Archive Photos | Getty Images</div></div></div></div><div><p>The  autonomous vehicle subsidiary received its first permit from the New York Department of Transportation on Friday to start testing in New York City, Mayor <a href=\"https://www.nyc.gov/mayors-office/news/2025/08/mayor-adams--dot-announce-approval-of-first-application-to-test-\" target=\"_blank\">Eric Adams</a> announced Friday. The rollout is the city's first <a href=\"https://www.cnbc.com/2025/07/02/autonomous-cars-are-having-their-chatgpt-moment-bank-of-america.html\">autonomous vehicle</a> testing launch.</p><p>Waymo will start testing up to eight vehicles in <a href=\"https://www.cnbc.com/2025/04/02/manhattan-luxury-real-estate-market.html\">Manhattan</a> and Downtown Brooklyn through late September with the potential to extend the program. New York state law requires the company to have a driver behind the wheel to operate.</p><p>\"We're a tech-friendly administration and we're always looking for innovative ways to safely move our city forward,\" Adams said in a release. \"New York City is proud to welcome Waymo to test this new technology in Manhattan and Brooklyn, as we know this testing is only the first step in moving our city further into the 21st century.\"</p></div><div><p>The news comes just two months after the company said it filed permits to test its cars in the city with a trained specialist behind the wheel.</p><p>Waymo has hit <a href=\"https://www.cnbc.com/2025/07/28/waymo-plans-to-bring-its-robotaxi-service-to-dallas-in-2026.html\">expansion</a> mode on its services nationwide, launching in <a href=\"https://www.cnbc.com/2025/03/04/waymo-uber-begin-offering-robotaxi-rides-in-austin-ahead-of-sxsw.html\">Austin</a> this year and expanding its <a href=\"https://www.cnbc.com/2025/03/11/waymo-expands-its-robotaxi-service-in-the-san-francisco-bay-area.html\">San Francisco-area</a> operations in March. Waymo also plans to bring autonomous vehicles to Atlanta; Miami; and Washington, D.C., and recently said it will start operations in <a href=\"https://www.cnbc.com/2025/07/07/waymo-to-begin-testing-in-philadelphia-with-drivers-behind-the-wheel.html\">Philadelphia</a> as it looks to break further into the Northeast market.</p><p>Waymo's CEO said the company surpassed 10 million <a href=\"https://www.cnbc.com/2025/05/20/waymo-ceo-tekedra-mawakana-10-million.html\">robotaxi</a> trips in May.</p><p>For years, autonomous vehicle companies have sought to introduce their technology to the Big Apple, with Waymo previously taking a crack at it in 2021. At that time, the company rolled out some cars in certain areas of the city for<a href=\"https://waymo.com/blog/2021/11/introducing-waymo-driver-to-new-york\" target=\"_blank\"> manual driving and data collection</a>.</p><p>New York City has also expressed interest in bringing autonomous vehicles to the city. <a href=\"https://www.nyc.gov/mayors-office/news/2024/03/mayor-adams-releases-requirements-opens-permit-applications-responsible-autonomous-vehicle\" target=\"_blank\">Last year</a>, the Adams administration implemented a series of safety requirements for responsible testing in the city and opened a permit program.</p><p>As part of the permit, Waymo must regularly meet and report data to DOT and work closely with law enforcement and emergency services.</p></div>","contentLength":2238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44986949"},{"title":"Show HN: Clyp ‚Äì Clipboard Manager for Linux","url":"https://github.com/murat-cileli/clyp","date":1755878606,"author":"timeoperator","guid":237122,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44986205"},{"title":"FFmpeg 8.0","url":"https://ffmpeg.org/index.html#pr8.0","date":1755876162,"author":"gyan","guid":236974,"unread":true,"content":"<div><div><h2>\n        A complete, cross-platform solution to record, convert and stream audio and video.\n      </h2></div></div><div><h3>Converting  and  has never been so easy.\n    </h3><pre>$ ffmpeg -i input.mp4 output.avi</pre></div><h3>August 22nd, 2025, FFmpeg 8.0 </h3><p>\n  A new major release, <a href=\"https://ffmpeg.org/download.html#release_8.0\">FFmpeg 8.0 </a>,\n  is now available for download.\n  Thanks to several delays, and modernization of our entire infrastructure, this release ended up\n  being one of our largest releases to date. In short, its new features are:\n  </p><ul><li>Native decoders: , ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728</li><li>VVC decoder improvements: ,\n                                  ,\n                                  Palette Mode</li><li>Vulkan compute-based codecs: FFv1 (encode and decode), ProRes RAW (decode only)</li><li>Hardware accelerated decoding: Vulkan VP9, VAAPI VVC, OpenHarmony H264/5</li><li>Hardware accelerated encoding: Vulkan AV1, OpenHarmony H264/5</li><li>Formats: MCC, G.728, Whip, APV</li><li>Filters: colordetect, pad_cuda, scale_d3d11, Whisper, and others</li></ul><p>\n  A new class of decoders and encoders based on pure Vulkan compute implementation have been added.\n  Vulkan is a cross-platform, open standard set of APIs that allows programs to use GPU hardware in various ways,\n  from drawing on screen, to doing calculations, to decoding video via custom hardware accelerators.\n  Rather than using a custom hardware accelerator present, these codecs are based on compute shaders, and work\n  on any implementation of Vulkan 1.3.\n  Decoders use the same hwaccel API and commands, so users do not need to do anything special to enable them,\n  as enabling <a href=\"https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan\">Vulkan decoding</a> is sufficient to use them.\n  Encoders, like our hardware accelerated encoders, require specifying a new encoder (ffv1_vulkan).\n  Currently, the only codecs supported are: FFv1 (encoding and decoding) and ProRes RAW (decode only).\n  ProRes (encode+decode) and VC-2 (encode+decode) implementations are complete and currently in review,\n  to be merged soon and available with the next minor release.<p>\n  Only codecs specifically designed for parallelized decoding can be implemented in such a way, with\n  more mainstream codecs not being planned for support.</p>\n  Depending on the hardware, these new codecs can provide very significant speedups, and open up\n  possibilities to work with them for situations like non-linear video editors and\n  lossless screen recording/streaming, so we are excited to learn what our downstream users can make with them.\n  </p><p>\n  The project has recently started to modernize its infrastructure. Our mailing list servers have been\n  fully upgraded, and we have recently started to accept contributions via a new forge, available on\n  <a href=\"https://code.ffmpeg.org/\">code.ffmpeg.org</a>, running a Forgejo instance.\n  </p><p>\n    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.\n  </p><h3>September 30th, 2024, FFmpeg 7.1 </h3><p>\n    The more important highlights of the release are that the VVC decoder, merged as experimental in version 7.0,\n    has had enough time to mature and be optimized enough to be declared as stable. The codec is starting to gain\n    traction with broadcast standardization bodies.\n    Support has been added for a native AAC USAC (part of the xHE-AAC coding system) decoder, with the format starting\n    to be adopted by streaming websites, due to its extensive volume normalization metadata.<p>\n    MV-HEVC decoding is now supported. This is a stereoscopic coding tool that begun to be shipped and generated\n    by recent phones and VR headsets.</p>\n    LC-EVC decoding, an enhancement metadata layer to attempt to improve the quality of codecs, is now supported via an\n    external library.</p><p>\n    Support for Vulkan encoding, with H264 and HEVC was merged. This finally allows fully Vulkan-based decode-filter-encode\n    pipelines, by having a sink for Vulkan frames, other than downloading or displaying them. The encoders have feature-parity\n    with their VAAPI implementation counterparts. Khronos has announced that support for AV1 encoding is also coming soon to Vulkan,\n    and FFmpeg is aiming to have day-one support.\n  </p><p>\n    In addition to the above, this release has had a lot of important internal work done. By far, the standout internally\n    are the improvements made for full-range images. Previously, color range data had two paths, no negotiation,\n    and was unreliably forwarded to filters, encoders, muxers. Work on cleaning the system up started more than 10\n    years ago, however this stalled due to how fragile the system was, and that breaking behaviour would be unacceptable.\n    The new system fixes this, so now color range is forwarded correctly and consistently everywhere needed, and also\n    laid the path for more advanced forms of negotiation.\n    Cropping metadata is now supported with Matroska and MP4 formats. This metadata is important not only for archival,\n    but also with AV1, as hardware encoders require its signalling due to the codec not natively supporting one.\n  </p><p>\n    As usual, we recommend that users, distributors, and system integrators to upgrade unless they use current git master.\n  </p><h3>September 11th, 2024, Coverity</h3><p>\n  The number of issues FFmpeg has in <a href=\"https://scan.coverity.com/projects/ffmpeg\">Coverity (a static analyzer)</a> is now lower than it has been since 2016.\n  Our defect density is less than one 30th of the average in OSS with over a million code\n  lines. All this was possible thanks to a grant from the <a href=\"https://www.sovereigntechfund.de/\">Sovereign Tech Fund</a>.\n  </p><img src=\"https://ffmpeg.org/img/coverity-lifetime-2024-08.PNG\" alt=\"Coverity Lifetime Graph till 2024-08\"><h3>June 2nd, 2024, native xHE-AAC decoder</h3><p>\n  FFmpeg now implements a native xHE-AAC decoder. Currently, streams without (e)SBR, USAC or MPEG-H Surround\n  are supported, which means the majority of xHE-AAC streams in use should work. Support for USAC and (e)SBR is\n  coming soon. Work is also ongoing to improve its stability and compatibility.\n  During the process we found several specification issues, which were then submitted back to the authors\n  for discussion and potential inclusion in a future errata.\n  </p><h3>May 13th, 2024, Sovereign Tech Fund</h3><p>\n  The FFmpeg community is excited to announce that Germany's\n  <a href=\"https://www.sovereigntechfund.de/tech/ffmpeg\">Sovereign Tech Fund</a>\n  has become its first governmental sponsor. Their support will help\n  sustain the maintainance of the FFmpeg project, a critical open-source\n  software multimedia component essential to bringing audio and video to\n  billions around the world everyday.\n  </p><h3>April 5th, 2024, FFmpeg 7.0 \"Dijkstra\"</h3><p>\n  This release is  backwards compatible, removing APIs deprecated before 6.0.\n  The biggest change for most library callers will be the removal of the old bitmask-based\n  channel layout API, replaced by the  API allowing such\n  features as custom channel ordering, or Ambisonics. Certain deprecated \n  CLI options were also removed, and a C11-compliant compiler is now required to build\n  the code.\n  </p><p>\n  As usual, there is also a number of new supported formats and codecs, new filters, APIs,\n  and countless smaller features and bugfixes. Compared to 6.1, the  repository\n  contains almost ‚àº2000 new commits by ‚àº100 authors, touching &gt;100000 lines in\n  ‚àº2000 files ‚Äî thanks to everyone who contributed. See the\n  <a href=\"https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=Changelog;hb=n7.0\">Changelog</a>,\n  <a href=\"https://git.videolan.org/?p=ffmpeg.git;a=blob_plain;f=doc/APIchanges;hb=n7.0\">APIchanges</a>,\n  and the git log for more comprehensive lists of changes.\n  </p><h3>January 3rd, 2024, native VVC decoder</h3><p>\n  The  library now contains a native VVC (Versatile Video Coding)\n  decoder, supporting a large subset of the codec's features. Further optimizations and\n  support for more features are coming soon. The code was written by Nuo Mi, Xu Mu,\n  Frank Plowman, Shaun Loo, and Wu Jianhua.\n  </p><h3>December 18th, 2023, IAMF support</h3><p>\n  The  library can now read and write <a href=\"https://aomediacodec.github.io/iamf/\">IAMF</a>\n  (Immersive Audio) files. The  CLI tool can configure IAMF structure with the new\n   option. IAMF support was written by James Almer.\n  </p><h3>December 12th, 2023, multi-threaded  CLI tool</h3><p>\n  Thanks to a major refactoring of the  command-line tool, all the major\n  components of the transcoding pipeline (demuxers, decoders, filters, encodes, muxers) now\n  run in parallel. This should improve throughput and CPU utilization, decrease latency,\n  and open the way to other exciting new features.\n  </p><p>\n  Note that you should  expect significant performance improvements in cases\n  where almost all computational time is spent in a single component (typically video\n  encoding).\n  </p><h3>November 10th, 2023, FFmpeg 6.1 \"Heaviside\"</h3><ul><li>Playdate video decoder and demuxer</li><li>Extend VAAPI support for libva-win32 on Windows</li><li>afireqsrc audio source filter</li><li>ffmpeg CLI new option: -readrate_initial_burst</li><li>zoneplate video source filter</li><li>command support in the setpts and asetpts filters</li><li>Vulkan decode hwaccel, supporting H264, HEVC and AV1</li><li>Essential Video Coding parser, muxer and demuxer</li><li>Essential Video Coding frame merge bsf</li><li>Microsoft RLE video encoder</li><li>Raw AC-4 muxer and demuxer</li><li>Raw VVC bitstream parser, muxer and demuxer</li><li>Bitstream filter for editing metadata in VVC streams</li><li>Bitstream filter for converting VVC from MP4 to Annex B</li><li>scale_vt filter for videotoolbox</li><li>transpose_vt filter for videotoolbox</li><li>support for the P_SKIP hinting to speed up libx264 encoding</li><li>Support HEVC,VP9,AV1 codec in enhanced flv format</li><li>apsnr and asisdr audio filters</li><li>Support HEVC,VP9,AV1 codec fourcclist in enhanced rtmp protocol</li><li>ffmpeg CLI '-top' option deprecated in favor of the setfield filter</li><li>ffprobe XML output schema changed to account for multiple variable-fields elements within the same parent element</li><li>ffprobe -output_format option added as an alias of -of</li></ul><p>\n    This release had been overdue for at least half a year, but due to constant activity in the repository,\n    had to be delayed, and we were finally able to branch off the release recently, before some of the large\n    changes scheduled for 7.0 were merged.\n  </p><p>\n    Internally, we have had a number of changes too. The FFT, MDCT, DCT and DST implementation used for codecs\n    and filters has been fully replaced with the faster libavutil/tx (full article about it coming soon).\n    This also led to a reduction in the the size of the compiled binary, which can be noticeable in small builds.<p>\n    There was a very large reduction in the total amount of allocations being done on each frame throughout video decoders,\n    reducing overhead.</p>\n    RISC-V optimizations for many parts of our DSP code have been merged, with mainly the large decoders being left.<p>\n    There was an effort to improve the correctness of timestamps and frame durations of each packet, increasing the\n    accurracy of variable frame rate video.\n  </p></p><p>\n    Next major release will be version 7.0, scheduled to be released in February. We will attempt to better stick\n    to the new release schedule we announced at the start of this year.\n  </p><p>\n    We strongly recommend users, distributors, and system integrators to upgrade unless they use current git master.\n  </p><h3>May 31st, 2023, Vulkan decoding</h3><p>\n    A few days ago, Vulkan-powered decoding hardware acceleration code was merged into the codebase.\n    This is the first vendor-generic and platform-generic decode acceleration API, enabling the\n    same code to be used on multiple platforms, with very minimal overhead.\n    This is also the first multi-threaded hardware decoding API, and our code makes full use of this,\n    saturating all available decode engines the hardware exposes.\n  </p><p>\n    Those wishing to test the code can read our\n    <a href=\"https://trac.ffmpeg.org/wiki/HWAccelIntro#Vulkan\">documentation page</a>.\n    For those who would like to integrate FFmpeg's Vulkan code to demux, parse, decode, and receive\n    a VkImage to present or manipulate, documentation and examples are available in our source tree.\n    Currently, using the latest available git checkout of our\n    <a href=\"https://git.videolan.org/?p=ffmpeg.git;a=summary\">repository</a> is required.\n    The functionality will be included in stable branches with the release of version 6.1, due\n    to be released soon.\n  </p><p>\n    As this is also the first practical implementation of the specifications, bugs may be present,\n    particularly in drivers, and, although passing verification, the implementation itself.\n    New codecs, and encoding support are also being worked on, by both the Khronos organization\n    for standardizing, and us as implementing it, and giving feedback on improving.\n  </p><h3>February 28th, 2023, FFmpeg 6.0 \"Von Neumann\"</h3><p>\n    A new major release, <a href=\"https://ffmpeg.org/download.html#release_6.0\">FFmpeg 6.0 \"Von Neumann\"</a>,\n    is now available for download. This release has many new encoders and decoders, filters,\n    ffmpeg CLI tool improvements, and also, changes the way releases are done. All major\n    releases will now bump the version of the ABI. We plan to have a new major release each\n    year. Another release-specific change is that deprecated APIs will be removed after 3\n    releases, upon the next major bump.\n    This means that releases will be done more often and will be more organized.\n  </p><p>\n    New decoders featured are Bonk, RKA, Radiance, SC-4, APAC, VQC, WavArc and a few ADPCM formats.\n    QSV and NVenc now support AV1 encoding. The FFmpeg CLI (we usually reffer to it as ffmpeg.c\n    to avoid confusion) has speed-up improvements due to threading, as well as statistics options,\n    and the ability to pass option values for filters from a file. There are quite a few new audio\n    and video filters, such as adrc, showcwt, backgroundkey and ssim360, with a few hardware ones too.\n    Finally, the release features many behind-the-scenes changes, including a new FFT and MDCT\n    implementation used in codecs (expect a blog post about this soon), numerous bugfixes, better\n    ICC profile handling and colorspace signalling improvement, introduction of a number of RISC-V\n    vector and scalar assembly optimized routines, and a few new improved APIs, which can be viewed\n    in the doc/APIchanges file in our tree.\n    A few submitted features, such as the Vulkan improvements and more FFT optimizations will be in the\n    next minor release, 6.1, which we plan to release soon, in line with our new release schedule.\n    Some highlights are:\n  </p><ul><li>Radiance HDR image support</li><li>ddagrab (Desktop Duplication) video capture filter</li><li>ffmpeg -shortest_buf_duration option</li><li>ffmpeg now requires threading to be built</li><li>ffmpeg now runs every muxer in a separate thread</li><li>Add new mode to cropdetect filter to detect crop-area based on motion vectors and edges</li><li>VAAPI decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li><li>WBMP (Wireless Application Protocol Bitmap) image format</li><li>Micronas SC-4 audio decoder</li><li>nvenc AV1 encoding support</li><li>MediaCodec decoder via NDKMediaCodec</li><li>QSV decoding and encoding for 10/12bit 422, 10/12bit 444 HEVC and VP9</li><li>showcwt multimedia filter</li><li>WADY DPCM decoder and demuxer</li><li>ffmpeg CLI new options: -stats_enc_pre[_fmt], -stats_enc_post[_fmt], -stats_mux_pre[_fmt]</li><li>hstack_vaapi, vstack_vaapi and xstack_vaapi filters</li><li>XMD ADPCM decoder and demuxer</li><li>ffmpeg CLI new option: -fix_sub_duration_heartbeat</li><li>WavArc decoder and demuxer</li><li>CrystalHD decoders deprecated</li><li>filtergraph syntax in ffmpeg CLI now supports passing file contents as option values</li><li>hstack_qsv, vstack_qsv and xstack_qsv filters</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>July 22nd, 2022, FFmpeg 5.1 \"Riemann\"</h3><ul><li>add ipfs/ipns protocol support</li><li>dialogue enhance audio filter</li><li>dropped obsolete XvMC hwaccel</li><li>DFPWM audio encoder/decoder and raw muxer/demuxer</li><li>Vizrt Binary Image encoder/decoder</li><li>colorchart video source filter</li><li>PGS subtitle frame merge bitstream filter</li><li>added chromakey_cuda filter</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>January 17th, 2022, FFmpeg 5.0 \"Lorentz\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_5.0\">FFmpeg 5.0 \"Lorentz\"</a>, a new\n    major release, is now available! For this long-overdue release, a major effort\n    underwent to remove the old encode/decode APIs and replace them with an\n    N:M-based API, the entire libavresample library was removed, libswscale\n    has a new, easier to use AVframe-based API, the Vulkan code was much improved,\n    many new filters were added, including libplacebo integration, and finally,\n    DoVi support was added, including tonemapping and remuxing. The default\n    AAC encoder settings were also changed to improve quality.\n    Some of the changelog highlights:\n  </p><ul><li>ADPCM IMA Westwood encoder</li><li>ADPCM IMA Acorn Replay decoder</li><li>Argonaut Games CVG demuxer</li><li>audio and video segment filters</li><li>Apple Graphics (SMC) encoder</li><li>hsvkey and hsvhold video filters</li><li>adecorrelate audio filter</li><li>AV1 Low overhead bitstream format muxer</li><li>huesaturation video filter</li><li>colorspectrum source video filter</li><li>RTP packetizer for uncompressed video (RFC 4175)</li><li>VideoToolbox ProRes hwaccel</li><li>aspectralstats audio filter</li><li>adynamicsmooth audio filter</li><li>vflip_vulkan, hflip_vulkan and flip_vulkan filters</li><li>adynamicequalizer audio filter</li><li>yadif_videotoolbox filter</li><li>VideoToolbox ProRes encoder</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><p>\n    We have a new IRC home at Libera Chat\n    now! Feel free to join us at #ffmpeg and #ffmpeg-devel. More info at <a href=\"https://ffmpeg.org/contact.html#IRCChannels\">contact#IRCChannels</a></p><h3>April 8th, 2021, FFmpeg 4.4 \"Rao\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.4\">FFmpeg 4.4 \"Rao\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>AudioToolbox output device</li><li>VDPAU accelerated HEVC 10/12bit decoding</li><li>ADPCM IMA Ubisoft APM encoder</li><li>AV1 encoding support SVT-AV1</li><li>ADPCM Argonaut Games encoder</li><li>AV1 Low overhead bitstream format demuxer</li><li>MobiClip FastAudio decoder</li><li>AV1 decoder (Hardware acceleration used only)</li><li>Argonaut Games BRP demuxer</li><li>IPU decoder, parser and demuxer</li><li>Intel QSV-accelerated AV1 decoding</li><li>Argonaut Games Video decoder</li><li>libwavpack encoder removed</li><li>AVS3 video decoder via libuavs3d</li><li>VDPAU accelerated VP9 10/12bit decoding</li><li>afreqshift and aphaseshift filters</li><li>High Voltage Software ADPCM encoder</li><li>LEGO Racers ALP (.tun &amp; .pcm) muxer</li><li>DXVA2/D3D11VA hardware accelerated AV1 decoding</li><li>Microsoft Paint (MSP) version 2 decoder</li><li>Microsoft Paint (MSP) demuxer</li><li>AV1 monochrome encoding support via libaom &gt;= 2.0.1</li><li>asuperpass and asuperstop filter</li><li>Digital Pictures SGA demuxer and decoders</li><li>TTML subtitle encoder and muxer</li><li>RIST protocol via librist</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>June 15th, 2020, FFmpeg 4.3 \"4:3\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.3\">FFmpeg 4.3 \"4:3\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>Intel QSV-accelerated MJPEG decoding</li><li>Intel QSV-accelerated VP9 decoding</li><li>Support for TrueHD in mp4</li><li>Support AMD AMF encoder on Linux (via Vulkan)</li><li>support Sipro ACELP.KELVIN decoding</li><li>maskedmin and maskedmax filters</li><li>QSV-accelerated VP9 encoding</li><li>AV1 encoding support via librav1e</li><li>AV1 frame merge bitstream filter</li><li>MPEG-H 3D Audio support in mp4</li><li>Argonaut Games ADPCM decoder</li><li>Argonaut Games ASF demuxer</li><li>afirsrc audio filter source</li><li>Simon &amp; Schuster Interactive ADPCM decoder</li><li>High Voltage Software ADPCM decoder</li><li>LEGO Racers ALP (.tun &amp; .pcm) demuxer</li><li>AMQP 0-9-1 protocol (RabbitMQ)</li><li>avgblur_vulkan, overlay_vulkan, scale_vulkan and chromaber_vulkan filters</li><li>switch from AvxSynth to AviSynth+ on Linux</li><li>Expanded styling support for 3GPP Timed Text Subtitles (movtext)</li><li>Support for muxing pcm and pgs in m2ts</li><li>Cunning Developments ADPCM decoder</li><li>Pro Pinball Series Soundbank demuxer</li><li>pcm_rechunk bitstream filter</li><li>gradients source video filter</li><li>MediaFoundation encoder wrapper</li><li>Simon &amp; Schuster Interactive ADPCM encoder</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>October 5th, 2019, Bright Lights</h3><p>\n  FFmpeg has added a realtime bright flash removal filter to libavfilter.\n  </p><p>\n  Note that this filter is not FDA approved, nor are we medical professionals.\n  Nor has this filter been tested with anyone who has photosensitive epilepsy.\n  FFmpeg and its photosensitivity filter are not making any medical claims.\n  </p><p>\n  That said, this is a new video filter that may help photosensitive people\n  watch tv, play video games or even be used with a VR headset to block\n  out epiletic triggers such as filtered sunlight when they are outside.\n  Or you could use it against those annoying white flashes on your tv screen.\n  The filter fails on some input, such as the\n  <a href=\"https://www.youtube.com/watch?v=8L_9hXnUzRk\">Incredibles 2 Screen Slaver</a>\n  scene. It is not perfect. If you have other clips that you want this filter to\n  work better on, please report them to us on our <a href=\"http://trac.ffmpeg.org\">trac</a>.\n  </p><p>\n  We are not professionals. Please use this in your medical studies to\n  advance epilepsy research. If you decide to use this in a medical\n  setting, or make a hardware hdmi input output realtime tv filter,\n  or find another use for this, <a href=\"mailto:compn@ffmpeg.org\">please let me know</a>.\n  This filter was a feature request of mine\n  <a href=\"https://trac.ffmpeg.org/ticket/2104\">since 2013</a>.\n  </p><h3>August 5th, 2019, FFmpeg 4.2 \"Ada\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.2\">FFmpeg 4.2 \"Ada\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>AV1 decoding support through libdav1d</li><li>chromashift and rgbashift filters</li><li>truehd_core bitstream filter</li><li>libaribb24 based ARIB STD-B24 caption support (profiles A and C)</li><li>Support decoding of HEVC 4:4:4 content in nvdec and cuviddec</li><li>AV1 frame split bitstream filter</li><li>Support decoding of HEVC 4:4:4 content in vdpau</li><li>showspatial multimedia filter</li><li>mov muxer writes tracks with unspecified language instead of English by default</li><li>added support for using clang to compile CUDA kernels</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>November 6th, 2018, FFmpeg 4.1 \"al-Khwarizmi\"</h3><ul><li>aderivative and aintegral audio filters</li><li>pal75bars and pal100bars video filter sources</li><li>mbedTLS based TLS support</li><li>adeclick and adeclip filters</li><li>libtensorflow backend for DNN based filters like srcnn</li><li>VC1 decoder is now bit-exact</li><li>AVS2 video decoder via libdavs2</li><li>Brooktree ProSumer video decoder</li><li>MatchWare Screen Capture Codec decoder</li><li>WinCam Motion Video decoder</li><li>RemotelyAnywhere Screen Capture decoder</li><li>Support for AV1 in MP4 and Matroska/WebM</li><li>AVS2 video encoder via libxavs2</li><li>Block-Matching 3d (bm3d) denoising filter</li><li>audio denoiser as afftdn filter</li><li>S12M timecode decoding in h264</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>April 20th, 2018, FFmpeg 4.0 \"Wu\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_4.0\">FFmpeg 4.0 \"Wu\"</a>, a new\n    major release, is now available! Some of the highlights:\n  </p><ul><li>Bitstream filters for editing metadata in H.264, HEVC and MPEG-2 streams</li><li>Experimental MagicYUV encoder</li><li>Intel QSV-accelerated MJPEG encoding</li><li>native aptX and aptX HD encoder and decoder</li><li>NVIDIA NVDEC-accelerated H.264, HEVC, MJPEG, MPEG-1/2/4, VC1, VP8/9 hwaccel decoding</li><li>Intel QSV-accelerated overlay filter</li><li>VAAPI MJPEG and VP8 decoding</li><li>AMD AMF H.264 and HEVC encoders</li><li>support LibreSSL (via libtls)</li><li>Dropped support for building for Windows XP. The minimum supported Windows version is Windows Vista.</li><li>hilbert audio filter source</li><li>Removed the ffserver program</li><li>Removed the ffmenc and ffmdec muxer and demuxer</li><li>VideoToolbox HEVC encoder and hwaccel</li><li>VAAPI-accelerated ProcAmp (color balance), denoise and sharpness filters</li><li>codec2 en/decoding via libcodec2</li><li>native SBC encoder and decoder</li><li>hapqa_extract bitstream filter</li><li>filter_units bitstream filter</li><li>AV1 Support through libaom</li><li>E-AC-3 dependent frames support</li><li>bitstream filter for extracting E-AC-3 core</li><li>Haivision SRT protocol via libsrt</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>October 15th, 2017, FFmpeg 3.4 \"Cantor\"</h3><ul><li>oscilloscope video filter</li><li>update cuvid/nvenc headers to Video Codec SDK 8.0.14</li><li>scale_cuda CUDA based video scale filter</li><li>librsvg support for svg rasterization</li><li>spec compliant VP9 muxing support in MP4</li><li>sofalizer filter switched to libmysofa</li><li>Gremlin Digital Video demuxer and decoder</li><li>superequalizer audio filter</li><li>additional frame format support for Interplay MVE movies</li><li>support for decoding through D3D11VA in ffmpeg</li><li>Dolby E decoder and SMPTE 337M demuxer</li><li>unpremultiply video filter</li><li>raw G.726 muxer and demuxer, left- and right-justified</li><li>NewTek NDI input/output device</li><li>VP9 tile threading support</li><li>V4L2 mem2mem HW assisted codecs</li><li>Rockchip MPP hardware decoding</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>April 13th, 2017, FFmpeg 3.3 \"Hilbert\"</h3><ul><li>PSD (Photoshop Document) decoder</li><li>FM Screen Capture decoder</li><li>DNxHR decoder fixes for HQX and high resolution videos</li><li>ClearVideo decoder (partial)</li><li>16.8 and 24.0 floating point PCM decoder</li><li>Intel QSV-accelerated VP8 video decoding</li><li>DNxHR 444 and HQX encoding</li><li>Quality improvements for the (M)JPEG encoder</li><li>VAAPI-accelerated MPEG-2 and VP8 encoding</li><li>abitscope multimedia filter</li><li>MPEG-7 Video Signature filter</li><li>add internal ebur128 library, remove external libebur128 dependency</li><li>Intel QSV video scaling and deinterlacing filters</li><li>Sample Dump eXchange demuxer</li><li>MIDI Sample Dump Standard demuxer</li><li>Scenarist Closed Captions demuxer and muxer</li><li>Support MOV with multiple sample description tables</li><li>Pro-MPEG CoP #3-R2 FEC protocol</li><li>Support for spherical videos</li><li>CrystalHD decoder moved to new decode API</li><li>configure now fails if autodetect-libraries are requested but not found</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>October 30th, 2016, Results: Summer Of Code 2016.</h3><p>\n    This has been a long time coming but we wanted to give a proper closure to our participation in this run of the program and it takes time. Sometimes it's just to get the final report for each project trimmed down, others, is finalizing whatever was still in progress when the program finished: final patches need to be merged, TODO lists stabilized, future plans agreed; you name it.\n  </p><p>\n    Without further ado, here's the silver-lining for each one of the projects we sought to complete during this Summer of Code season:\n  </p><h4>FFv1 (Mentor: Michael Niedermayer)</h4><p>\n    Stanislav Dolganov designed and implemented experimental support for motion estimation and compensation in the lossless FFV1 codec. The design and implementation is based on the snow video codec, which uses OBMC. Stanislav's work proved that significant compression gains can be achieved with inter frame compression. FFmpeg welcomes Stanislav to continue working beyond this proof of concept and bring its advances into the official FFV1 specification within the IETF.\n  </p><h4>Self test coverage (Mentor: Michael Niedermayer)</h4><p>\n    Petru Rares Sincraian added several self-tests to FFmpeg and successfully went through the in-some-cases tedious process of fine tuning tests parameters to avoid known and hard to avoid problems, like checksum mismatches due to rounding errors on the myriad of platforms we support. His work has improved the code coverage of our self tests considerably.\n  </p><h4>MPEG-4 ALS encoder implementation (Mentor: Thilo Borgmann)</h4><p>\n    Umair Khan updated and integrated the ALS encoder to fit in the current FFmpeg codebase. He also implemented a missing feature for the ALS decoder that enables floating-point sample decoding. FFmpeg support for MPEG-4 ALS has been improved significantly by Umair's work. We welcome him to keep maintaining his improvements and hope for great contributions to come.\n  </p><h4>Tee muxer improvements (Mentor: Marton Balint)</h4><p>\n    J√°n Sebechlebsk√Ω's generic goal was to improve the tee muxer so it tolerated blocking IO and allowed transparent error recovery. During the design phase it turned out that this functionality called for a separate muxer, so J√°n spent his summer working on the so-called FIFO muxer, gradually fixing issues all over the codebase. He succeeded in his task, and the FIFO muxer is now part of the main repository, alongside several other improvements he made in the process.\n  </p><h4>TrueHD encoder (Mentor: Rostislav Pehlivanov)</h4><p>\n    Jai Luthra's objective was to update the out-of-tree and pretty much abandoned MLP (Meridian Lossless Packing) encoder for libavcodec and improve it to enable encoding to the TrueHD format. For the qualification period the encoder was updated such that it was usable and throughout the summer, successfully improved adding support for multi-channel audio and TrueHD encoding. Jai's code has been merged into the main repository now. While a few problems remain with respect to LFE channel and 32 bit sample handling, these are in the process of being fixed such that effort can be finally put in improving the encoder's speed and efficiency.\n  </p><h4>Motion interpolation filter (Mentor: Paul B Mahol)</h4><p>\n    Davinder Singh investigated existing motion estimation and interpolation approaches from the available literature and previous work by our own: Michael Niedermayer, and implemented filters based on this research. These filters allow motion interpolating frame rate conversion to be applied to a video, for example, to create a slow motion effect or change the frame rate while smoothly interpolating the video along the motion vectors. There's still work to be done to call these filters 'finished', which is rather hard all things considered, but we are looking optimistically at their future.\n  </p><p>\n    And that's it. We are happy with the results of the program and immensely thankful for the opportunity of working with such an amazing set of students. We can be a tough crowd but our mentors did an amazing job at hand holding our interns through their journey. Thanks also to Google for this wonderful program and to everyone that made room in their busy lives to help making GSoC2016 a success. See you in 2017!\n  </p><h3>September 24th, 2016, SDL1 support dropped.</h3><p>\n    Support for the SDL1 library has been dropped, due to it no longer being maintained (as of\n    January, 2012) and it being superseded by the SDL2 library. As a result, the SDL1 output device\n    has also been removed and replaced by an SDL2 implementation. Both the ffplay and opengl output\n    devices have been updated to support SDL2.\n  </p><h3>August 9th, 2016, FFmpeg 3.1.2 \"Laplace\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_3.1\">FFmpeg 3.1.2</a>, a new point release from the 3.1 release branch, is now available!\n    It fixes several bugs.\n  </p><p>\n    We recommend users, distributors, and system integrators, to upgrade unless they use current git master.\n  </p><h3>July 10th, 2016, ffserver program being dropped</h3><p>\n    After thorough deliberation, we're announcing that we're about to drop the ffserver program from the project starting with the next release.\n    ffserver has been a problematic program to maintain due to its use of internal APIs, which complicated the recent cleanups to the libavformat\n    library, and block further cleanups and improvements which are desired by API users and will be easier to maintain. Furthermore the program has\n    been hard for users to deploy and run due to reliability issues, lack of knowledgable people to help and confusing configuration file syntax.\n    Current users and members of the community are invited to write a replacement program to fill the same niche that ffserver did using the new APIs\n    and to contact us so we may point users to test and contribute to its development.\n  </p><h3>July 1st, 2016, FFmpeg 3.1.1 \"Laplace\"</h3><p><a href=\"https://ffmpeg.org/download.html#release_3.1\">FFmpeg 3.1.1</a>, a new point release from the 3.1 release branch, is now available!\n    It mainly deals with a few ABI issues introduced in the previous release.\n  </p><p>\n    We strongly recommend users, distributors, and system integrators, especially those who experienced issues upgrading from 3.0, to\n    upgrade unless they use current git master.\n  </p><h3>June 27th, 2016, FFmpeg 3.1 \"Laplace\"</h3><ul><li>DXVA2-accelerated HEVC Main10 decoding</li><li>loop video filter and aloop audio filter</li><li>Bob Weaver deinterlacing filter</li><li>protocol blacklisting API</li><li>VC-2 HQ RTP payload format (draft v1) depacketizer and packetizer</li><li>VP9 RTP payload format (draft v2) packetizer</li><li>AudioToolbox audio decoders</li><li>AudioToolbox audio encoders</li><li>coreimage filter (GPU based image filtering on OSX)</li><li>bitstream filter for extracting DTS core</li><li>hash and framehash muxers</li><li>VAAPI-accelerated format conversion and scaling</li><li>libnpp/CUDA-accelerated format conversion and scaling</li><li>Duck TrueMotion 2.0 Real Time decoder</li><li>Wideband Single-bit Data (WSD) demuxer</li><li>VAAPI-accelerated H.264/HEVC/MJPEG encoding</li><li>DTS Express (LBR) decoder</li><li>Generic OpenMAX IL encoder with support for Raspberry Pi</li><li>IFF ANIM demuxer &amp; decoder</li><li>Direct Stream Transfer (DST) decoder</li><li>OpenExr improvements (tile data and B44/B44A support)</li><li>BitJazz SheerVideo decoder</li><li>CUDA CUVID H264/HEVC decoder</li><li>10-bit depth support in native utvideo decoder</li><li>libutvideo wrapper removed</li><li>YUY2 Lossless Codec decoder</li><li>VideoToolbox H.264 encoder</li></ul><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>March 16th, 2016, Google Summer of Code</h3><p>\n    FFmpeg has been accepted as a <a href=\"https://summerofcode.withgoogle.com/\">Google Summer of Code</a> open source organization. If you wish to\n    participate as a student see our <a href=\"https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2016\">project ideas page</a>.\n    You can already get in contact with mentors and start working on qualification tasks as well as register at google and submit your project proposal draft.\n    Good luck!\n  </p><h3>February 15th, 2016, FFmpeg 3.0 \"Einstein\"</h3><p>\n    We strongly recommend users, distributors, and system integrators to\n    upgrade unless they use current git master.\n  </p><h3>January 30, 2016, Removing support for two external AAC encoders</h3><p>\n    We have just removed support for VisualOn AAC encoder (libvo-aacenc) and\n    libaacplus in FFmpeg master.\n  </p><p>\n    Even before marking our internal AAC encoder as\n    <a href=\"https://ffmpeg.org/index.html#aac_encoder_stable\">stable</a>, it was known that libvo-aacenc\n    was of an inferior quality compared to our native one for most samples.\n    However, the VisualOn encoder was used extensively by the Android Open\n    Source Project, and we would like to have a tested-and-true stable option\n    in our code base.\n  </p><p>\n    When first committed in 2011, libaacplus filled in the gap of encoding\n    High Efficiency AAC formats (HE-AAC and HE-AACv2), which was not supported\n    by any of the encoders in FFmpeg at that time.\n  </p><p>\n    The circumstances for both have changed. After the work spearheaded by\n    Rostislav Pehlivanov and Claudio Freire, the now-stable FFmpeg native AAC\n    encoder is ready to compete with much more mature encoders. The Fraunhofer\n    FDK AAC Codec Library for Android was added in 2012 as the fourth\n    supported external AAC encoder, and the one with the best quality and the\n    most features supported, including HE-AAC and HE-AACv2.\n  </p><p>\n    Therefore, we have decided that it is time to remove libvo-aacenc and\n    libaacplus. If you are currently using libvo-aacenc, prepare to transition\n    to the native encoder () when updating to the next version\n    of FFmpeg. In most cases it is as simple as merely swapping the encoder\n    name. If you are currently using libaacplus, start using FDK AAC\n    () with an appropriate  option\n    to select the exact AAC profile that fits your needs. In both cases, you\n    will enjoy an audible quality improvement and as well as fewer licensing\n    headaches.\n  </p><h3>January 16, 2016, FFmpeg 2.8.5, 2.7.5, 2.6.7, 2.5.10</h3><p>\n    We have made several new point releases ().\n    They fix various bugs, as well as CVE-2016-1897 and CVE-2016-1898.\n    Please see the changelog for each release for more details.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>December 5th, 2015, The native FFmpeg AAC encoder is now stable!</h3><p>\n    After seven years the native FFmpeg AAC encoder has had its experimental flag\n    removed and declared as ready for general use. The encoder is transparent\n    at 128kbps for most samples tested with artifacts only appearing in extreme\n    cases. Subjective quality tests put the encoder to be of equal or greater\n    quality than most of the other encoders available to the public.\n  </p><p>\n    Licensing has always been an issue with encoding AAC audio as most of the\n    encoders have had a license making FFmpeg unredistributable if compiled with\n    support for them. The fact that there now exists a fully open and truly\n    free AAC encoder integrated directly within the project means a lot to those\n    who wish to use accepted and widespread standards.\n  </p><p>\n    The majority of the work done to bring the encoder up to quality was started\n    during this year's GSoC by developer Claudio Freire and Rostislav Pehlivanov.\n    Both continued to work on the encoder with the latter joining as a developer\n    and mainainer, working on other parts of the project as well. Also, thanks\n    to <a href=\"http://d.hatena.ne.jp/kamedo2/\">Kamedo2</a> who does comparisons\n    and tests, the original authors and all past and current contributors to the\n    encoder. Users are suggested and encouraged to use the encoder and provide\n    feedback or breakage reports through our <a href=\"https://trac.ffmpeg.org/\">bug tracker</a>.\n  </p><p>\n    A big thank you note goes to our newest supporters: MediaHub and Telepoint.\n    Both companies have donated a dedicated server with free of charge internet\n    connectivity. Here is a little bit about them in their own words:\n  </p><ul><li><p><a href=\"http://www.telepoint.bg/en/\">Telepoint</a> is the biggest\n        carrier-neutral data center in Bulgaria. Located in the heart of Sofia\n        on a cross-road of many Bulgarian and International networks, the\n        facility is a fully featured Tier 3 data center that provides flexible\n        customer-oriented colocation solutions (ranging from a server to a\n        private collocation hall) and a high level of security.\n      </p></li><li><p>\n        MediaHub Ltd. is a Bulgarian IPTV platform and services provider which\n        uses FFmpeg heavily since it started operating a year ago. <i>\"Donating\n        to help keep FFmpeg online is our way of giving back to the community\"\n        </i>.\n      </p></li></ul><p>\n    Thanks Telepoint and MediaHub for their support!\n  </p><h3>September 29th, 2015, GSoC 2015 results</h3><p>\n    FFmpeg participated to the latest edition of\n    the <a href=\"http://www.google-melange.com/gsoc/homepage/google/gsoc2015\">Google\n    Summer of Code</a> Project. FFmpeg got a total of 8 assigned\n    projects, and 7 of them were successful.\n  </p><p>We want to thank <a href=\"https://www.google.com\">Google</a>, the\n    participating students, and especially the mentors who joined this\n    effort. We're looking forward to participating in the next GSoC\n    edition!\n  </p><p>\n    Below you can find a brief description of the final outcome of\n    each single project.\n  </p><h4>Basic servers for network protocols, mentee: Stephan Holljes, mentor: Nicolas George</h4><p>\n    Stephan Holljes's project for this session of Google Summer of Code was to\n    implement basic HTTP server features for libavformat, to complement the\n    already present HTTP client and RTMP and RTSP server code.\n  </p><p>\n    The first part of the project was to make the HTTP code capable of accepting\n    a single client; it was completed partly during the qualification period and\n    partly during the first week of the summer. Thanks to this work, it is now\n    possible to make a simple HTTP stream using the following commands:\n  </p><pre>    ffmpeg -i /dev/video0 -listen 1 -f matroska \\\n    -c:v libx264 -preset fast -tune zerolatency http://:8080\n    ffplay http://localhost:8080/\n  </pre><p>\n    The next part of the project was to extend the code to be able to accept\n    several clients, simultaneously or consecutively. Since libavformat did not\n    have an API for that kind of task, it was necessary to design one. This part\n    was mostly completed before the midterm and applied shortly afterwards.\n    Since the ffmpeg command-line tool is not ready to serve several clients,\n    the test ground for that new API is an example program serving hard-coded\n    content.\n  </p><p>\n    The last and most ambitious part of the project was to update ffserver to\n    make use of the new API. It would prove that the API is usable to implement\n    real HTTP servers, and expose the points where more control was needed. By\n    the end of the summer, a first working patch series was undergoing code\n    review.\n  </p><h4>Browsing content on the server, mentee: Mariusz Szczepa≈Ñczyk, mentor: Lukasz Marek</h4><p>\n    Mariusz finished an API prepared by the FFmpeg community and implemented\n    Samba directory listing as qualification task.\n  </p><p>\n    During the program he extended the API with the possibility to\n    remove and rename files on remote servers. He completed the\n    implementation of these features for file, Samba, SFTP, and FTP\n    protocols.\n  </p><p>\n    At the end of the program, Mariusz provided a sketch of an\n    implementation for HTTP directory listening.\n  </p><h4>Directshow digital video capture, mentee: Mate Sebok, mentor: Roger Pack</h4><p>\n    Mate was working on directshow input from digital video sources. He\n    got working input from ATSC input sources, with specifiable tuner.\n  </p><p>\n    The code has not been committed, but a patch of it was sent to the\n    ffmpeg-devel mailing list for future use.\n  </p><p>\n    The mentor plans on cleaning it up and committing it, at least for the\n    ATSC side of things. Mate and the mentor are still working trying to\n    finally figure out how to get DVB working.\n  </p><h4>Implementing full support for 3GPP Timed Text Subtitles, mentee: Niklesh Lalwani, mentor: Philip Langdale</h4><p>\n    Niklesh's project was to expand our support for 3GPP Timed Text\n    subtitles. This is the native subtitle format for mp4 containers, and\n    is interesting because it's usually the only subtitle format supported\n    by the stock playback applications on iOS and Android devices.\n  </p><p>\n    ffmpeg already had basic support for these subtitles which ignored all\n    formatting information - it just provided basic plain-text support.\n  </p><p>\n    Niklesh did work to add support on both the encode and decode side for\n    text formatting capabilities, such as font size/colour and effects like\n    bold/italics, highlighting, etc.\n  </p><p>\n    The main challenge here is that Timed Text handles formatting in a very\n    different way from most common subtitle formats. It uses a binary\n    encoding (based on mp4 boxes, naturally) and stores information\n    separately from the text itself. This requires additional work to track\n    which parts of the text formatting applies to, and explicitly dealing\n    with overlapping formatting (which other formats support but Timed\n    Text does not) so it requires breaking the overlapping sections into\n    separate non-overlapping ones with different formatting.\n  </p><p>\n    Finally, Niklesh had to be careful about not trusting any size\n    information in the subtitles - and that's no joke: the now infamous\n    Android stagefright bug was in code for parsing Timed Text subtitles.\n  </p><p>\n    All of Niklesh's work is committed and was released in ffmpeg 2.8.\n  </p><h4>libswscale refactoring, mentee: Pedro Arthur, mentors: Michael Niedermayer, Ramiro Polla</h4><p>\n    Pedro Arthur has modularized the vertical and horizontal scalers.\n    To do this he designed and implemented a generic filter framework\n    and moved the existing scaler code into it. These changes now allow\n    easily adding removing, splitting or merging processing steps.\n    The implementation was benchmarked and several alternatives were\n    tried to avoid speed loss.\n  </p><p>\n    He also added gamma corrected scaling support.\n    An example to use gamma corrected scaling would be:\n  </p><pre>    ffmpeg -i input -vf scale=512:384:gamma=1 output\n  </pre><p>\n    Pedro has done impressive work considering the short time available,\n    and he is a FFmpeg committer now. He continues to contribute to\n    FFmpeg, and has fixed some bugs in libswscale after GSoC has\n    ended.\n  </p><h4>AAC Encoder Improvements, mentee: Rostislav Pehlivanov, mentor: Claudio Freire</h4><p>\n    Rostislav Pehlivanov has implemented PNS, TNS, I/S coding and main\n    prediction on the native AAC encoder. Of all those extensions, only\n    TNS was left in a less-than-usable state, but the implementation has\n    been pushed (disabled) anyway since it's a good basis for further\n    improvements.\n  </p><p>\n    PNS replaces noisy bands with a single scalefactor representing the\n    energy of that band, gaining in coding efficiency considerably, and\n    the quality improvements on low bitrates are impressive for such a\n    simple feature.\n  </p><p>\n    TNS still needs some polishing, but has the potential to reduce coding\n    artifacts by applying noise shaping in the temporal domain (something\n    that is a source of annoying, notable distortion on low-entropy\n    bands).\n  </p><p>\n    Intensity Stereo coding (I/S) can double coding efficiency by\n    exploiting strong correlation between stereo channels, most effective\n    on pop-style tracks that employ panned mixing. The technique is not as\n    effective on classic X-Y recordings though.\n  </p><p>\n    Finally, main prediction improves coding efficiency by exploiting\n    correlation among successive frames. While the gains have not been\n    huge at this point, Rostislav has remained active even after the GSoC,\n    and is polishing both TNS and main prediction, as well as looking for\n    further improvements to make.\n  </p><p>\n    In the process, the MIPS port of the encoder was broken a few times,\n    something he's also working to fix.\n  </p><h4>Animated Portable Network Graphics (APNG), mentee: Donny Yang, mentor: Paul B Mahol</h4><p>\n    Donny Yang implemented basic keyframe only APNG encoder as the\n    qualification task. Later he wrote interframe compression via\n    various blend modes. The current implementation tries all blend\n    modes and picks one which takes the smallest amount of memory.\n  </p><p>\n    Special care was taken to make sure that the decoder plays\n    correctly all files found in the wild and that the encoder\n    produces files that can be played in browsers that support APNG.\n  </p><p>\n    During his work he was tasked to fix any encountered bug in the\n    decoder due to the fact that it doesn't match APNG\n    specifications. Thanks to this work, a long standing bug in the\n    PNG decoder has been fixed.\n  </p><p>\n    For latter work he plans to continue working on the encoder,\n    making it possible to select which blend modes will be used in the\n    encoding process. This could speed up encoding of APNG files.\n  </p><h3>September 9th, 2015, FFmpeg 2.8</h3><p>\n    We published release  as new major version.\n    It contains all features and bug fixes of the git master branch from September 8th. Please see\n    the \n    for a list of the most important changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use current git master.\n  </p><h3>August 1st, 2015, A message from the FFmpeg project</h3><p>\n    Dear multimedia community,\n  </p><p>\n    The resignation of Michael Niedermayer as leader of FFmpeg yesterday has\n    come by surprise. He has worked tirelessly on the FFmpeg project for many\n    years and we must thank him for the work that he has done. We hope that in\n    the future he will continue to contribute to the project. In the coming\n    weeks, the FFmpeg project will be managed by the active contributors.\n  </p><p>\n    The last four years have not been easy for our multimedia community - both\n    contributors and users. We should now look to the future, try to find\n    solutions to these issues, and to have reconciliation between the forks,\n    which have split the community for so long.\n  </p><p>\n    Unfortunately, much of the disagreement has taken place in inappropriate\n    venues so far, which has made finding common ground and solutions\n    difficult. We aim to discuss this in our communities online over the coming\n    weeks, and in person at the <a href=\"https://www.videolan.org/videolan/events/vdd15/\">VideoLAN Developer\n    Days</a> in Paris in September: a neutral venue for the entire open source\n    multimedia community.\n  </p><h3>July 4th, 2015, FFmpeg needs a new host</h3><p> We have received more than 7 offers for hosting and servers, thanks a lot to everyone!</p><p>\n    After graciously hosting our projects (<a href=\"http://www.ffmpeg.org\">FFmpeg</a>, <a href=\"http://www.mplayerhq.hu\">MPlayer</a>\n    and <a href=\"http://rtmpdump.mplayerhq.hu\">rtmpdump</a>) for 4 years, Arpi (our hoster) has informed us that we have to secure a new host somewhere else immediately.\n  </p><p>\n    If you want to host an open source project, please let us know, either on <a href=\"http://ffmpeg.org/mailman/listinfo/ffmpeg-devel\">ffmpeg-devel</a>\n    mailing list or irc.freenode.net #ffmpeg-devel.\n  </p><p>\n    We use about 4TB of storage and at least 4TB of bandwidth / month for various mailing lists, <a href=\"http://trac.ffmpeg.org\">trac</a>, <a href=\"http://samples.ffmpeg.org\">samples repo</a>, svn, etc.\n  </p><h3>March 16, 2015, FFmpeg 2.6.1</h3><p>\n    We have made a new major release ()\n    and now one week afterward 2.6.1. It contains all features and bugfixes of the git master branch from the 6th March.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>March 4, 2015, Google Summer of Code</h3><p>\n    FFmpeg has been accepted as a <a href=\"http://www.google-melange.com/gsoc/homepage/google/gsoc2015\">Google Summer of Code</a> Project. If you wish to\n    participate as a student see our <a href=\"https://trac.ffmpeg.org/wiki/SponsoringPrograms/GSoC/2015\">project ideas page</a>.\n    You can already get in contact with mentors and start working on qualification tasks. Registration\n    at Google for students will open March 16th. Good luck!\n  </p><h3>March 1, 2015, Chemnitzer Linux-Tage</h3><p>\n    We happily announce that FFmpeg will be represented at Chemnitzer Linux-Tage\n    (CLT) in Chemnitz, Germany. The event will take place on 21st and 22nd of March.\n  </p><p>\n    More information can be found <a href=\"https://chemnitzer.linux-tage.de/2015/en/\">here</a></p><p>\n    We demonstrate usage of FFmpeg, answer your questions and listen to\n    your problems and wishes. <strong>If you have media files that cannot be\n    processed correctly with FFmpeg, be sure to have a sample with you\n    so we can have a look!</strong></p><p>\n    For the first time in our CLT history, there will be an !\n    You can read the details <a href=\"https://chemnitzer.linux-tage.de/2015/de/programm/beitrag/209\">here</a>.\n    The workshop is targeted at FFmpeg beginners. First the basics of\n    multimedia will be covered. Thereafter you will learn how to use\n    that knowledge and the FFmpeg CLI tools to analyse and process media\n    files. The workshop is in German language only and prior registration\n    is necessary. The workshop will be on Saturday starting at 10 o'clock.\n  </p><p>\n    We are looking forward to meet you (again)!\n  </p><h3>December 5, 2014, FFmpeg 2.5</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from the 4th December.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>October 10, 2014, FFmpeg is in Debian unstable again</h3><p>\n    We wanted you to know there are\n    <a href=\"https://packages.debian.org/search?keywords=ffmpeg&amp;searchon=sourcenames&amp;suite=unstable&amp;section=main\">\n    FFmpeg packages in Debian unstable</a> again. <strong>A big thank-you\n    to Andreas Cadhalpun and all the people that made it possible.</strong> It has been anything but simple.\n  </p><p>\n    Unfortunately that was already the easy part of this news. The bad news is the packages probably won't\n    migrate to Debian testing to be in the upcoming release codenamed jessie.\n    <a href=\"https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=763148\">Read the argumentation over at Debian.</a></p><p><strong>However things will come out in the end, we hope for your continued remarkable support!</strong></p><h3>October 8, 2014, FFmpeg secured a place in OPW!</h3><p>\n    Thanks to a generous 6K USD donation by Samsung (Open Source Group),\n    FFmpeg will be welcoming at least 1 \"Outreach Program for Women\" intern\n    to work with our community for an initial period starting December 2014\n    (through March 2015).\n  </p><p>\n    We all know FFmpeg is used by the industry, but even while there are\n    countless products building on our code, it is not at all common for\n    companies to step up and help us out when needed. So a big thank-you\n    to Samsung and the OPW program committee!\n  </p><p>\n    If you are thinking on participating in OPW as an intern, please take\n    a look at our <a href=\"https://trac.ffmpeg.org/wiki/SponsoringPrograms/OPW/2014-12\">OPW wiki page</a>\n    for some initial guidelines. The page is still a work in progress, but\n    there should be enough information there to get you started. If you, on\n    the other hand, are thinking on sponsoring work on FFmpeg through the\n    OPW program, please get in touch with us at opw@ffmpeg.org. With your\n    help, we might be able to secure some extra intern spots for this round!\n  </p><h3>September 15, 2014, FFmpeg 2.4</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from the 14th September.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>August 20, 2014, FFmpeg 2.3.3, 2.2.7, 1.2.8</h3><p>\n    We have made several new point releases ().\n    They fix various bugs, as well as CVE-2014-5271 and CVE-2014-5272.\n    Please see the changelog for more details.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>July 29, 2014, Help us out securing our spot in OPW</h3><p>\n    Following our previous post regarding our participation on this year's\n    OPW (Outreach Program for Women), we are now reaching out to our users\n    (both individuals and companies) to help us gather the needed money to\n    secure our spot in the program.\n    We need to put together 6K USD as a minimum but securing more funds would\n    help us towards getting more than one intern.<p>\n    You can donate by credit card using\n    </p><a href=\"https://co.clickandpledge.com/advanced/default.aspx?wid=56226\">\n    Click&amp;Pledge</a> and selecting the \"OPW\" option. If you would like to\n    donate by money transfer or by check, please get in touch by\n    <a href=\"mailto:opw@ffmpeg.org\">e-mail</a> and we will get back to you\n    with instructions.Thanks!\n  </p><h3>July 20, 2014, New website</h3><p>\n    The FFmpeg project is proud to announce a brand new version of the website\n    made by <a href=\"http://db0.fr\">db0</a>. While this was initially motivated\n    by the need for a larger menu, the whole website ended up being redesigned,\n    and most pages got reworked to ease navigation. We hope you'll enjoy\n    browsing it.\n  </p><h3>July 17, 2014, FFmpeg 2.3</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from the 16th July.\n    Please see the  for a\n    list of note-worthy changes.\n  </p><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>July 3, 2014, FFmpeg and the Outreach Program For Women</h3><p>\n    FFmpeg has started the process to become an OPW includer organization for the\n    next round of the program, with internships starting December 9. The\n    <a href=\"https://gnome.org/opw/\">OPW</a> aims to \"Help women (cis and trans)\n    and genderqueer to get involved in free and open source software\". Part of the\n    process requires securing funds to support at least one internship (6K USD), so\n    if you were holding on your donation to FFmpeg, this is a great chance for you\n    to come forward, get in touch and help both the project and a great initiative!\n  </p><p>\n    We have set up an <a href=\"mailto:opw@ffmpeg.org\">email address</a> you can use\n    to contact us about donations and general inquires regarding our participation\n    in the program. Hope to hear from you soon!\n  </p><h3>June 29, 2014, FFmpeg 2.2.4, 2.1.5, 2.0.5, 1.2.7, 1.1.12, 0.10.14</h3><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><p>\n    Once again FFmpeg will be represented at LinuxTag in Berlin, Germany. The event will\n    take place from 8th to 10th of May. Please note that this year's LinuxTag is at a\n    different location closer to the city center.\n  </p><p>\n    We will have a shared booth with XBMC and VideoLAN.\n    <b>\n      If you have media files that cannot be processed correctly with\n      FFmpeg, be sure to have a sample with you so we can have a look!\n    </b></p><p>\n    More information about LinuxTag can be found <a href=\"http://www.linuxtag.org/2014/\">here</a></p><p>\n    We are looking forward to see you in Berlin!\n  </p><h3>April 18, 2014, OpenSSL Heartbeat bug</h3><p>\n    Our server hosting the Trac issue tracker was vulnerable to the attack\n    against OpenSSL known as \"heartbleed\". The OpenSSL software library\n    was updated on 7th of April, shortly after the vulnerability was publicly\n    disclosed. We have changed the private keys (and certificates) for all\n    FFmpeg servers. The details were sent to the mailing lists by\n    Alexander Strasser, who is part of the project server team. Here is a\n    link to the user mailing list\n    <a href=\"https://lists.ffmpeg.org/pipermail/ffmpeg-user/2014-April/020968.html\">archive</a>\n    .\n  </p><p>\n    We encourage you to read up on\n    <a href=\"https://www.schneier.com/blog/archives/2014/04/heartbleed.html\">\"OpenSSL heartbleed\"</a>.\n    <b>It is possible that login data for the issue tracker was exposed to\n      people exploiting this security hole. You might want to change your password\n      in the tracker and everywhere else you used that same password.</b></p><h3>April 11, 2014, FFmpeg 2.2.1</h3><p>\n    We have made a new point releases ().\n    It contains bug fixes for Tickets #2893, #3432, #3469, #3486, #3495 and #3540 as well as\n    several other fixes.\n    See the git log for details.\n  </p><h3>March 24, 2014, FFmpeg 2.2</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from 1st March.\n    A partial list of new stuff is below:\n  </p><pre>    - HNM version 4 demuxer and video decoder\n    - Live HDS muxer\n    - setsar/setdar filters now support variables in ratio expressions\n    - elbg filter\n    - string validation in ffprobe\n    - support for decoding through VDPAU in ffmpeg (the -hwaccel option)\n    - complete Voxware MetaSound decoder\n    - remove mp3_header_compress bitstream filter\n    - Windows resource files for shared libraries\n    - aeval filter\n    - stereoscopic 3d metadata handling\n    - WebP encoding via libwebp\n    - ATRAC3+ decoder\n    - VP8 in Ogg demuxing\n    - side &amp; metadata support in NUT\n    - framepack filter\n    - XYZ12 rawvideo support in NUT\n    - Exif metadata support in WebP decoder\n    - OpenGL device\n    - Use metadata_header_padding to control padding in ID3 tags (currently used in\n    MP3, AIFF, and OMA files), FLAC header, and the AVI \"junk\" block.\n    - Mirillis FIC video decoder\n    - Support DNx444\n    - libx265 encoder\n    - dejudder filter\n    - Autodetect VDA like all other hardware accelerations\n  </pre><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p><h3>February 3, 2014, Chemnitzer Linux-Tage</h3><p>\n    We happily announce that FFmpeg will be represented at `Chemnitzer Linux-Tage'\n    in Chemnitz, Germany. The event will take place on 15th and 16th of March.\n  </p><p>\n    More information can be found <a href=\"http://chemnitzer.linux-tage.de/2014/en/info/\">here</a></p><p>\n    We invite you to visit us at our booth located in the Linux-Live area!\n    There we will demonstrate usage of FFmpeg, answer your questions and listen to\n    your problems and wishes.\n  </p><p><b>\n      If you have media files that cannot be processed correctly with\n      FFmpeg, be sure to have a sample with you so we can have a look!\n    </b></p><p>\n    We are looking forward to meet you (again)!\n  </p><h3>February 9, 2014, trac.ffmpeg.org / trac.mplayerhq.hu Security Breach</h3><p>\n    The server on which FFmpeg and MPlayer Trac issue trackers were\n    installed was compromised. The affected server was taken offline\n    and has been replaced and all software reinstalled.\n    FFmpeg Git, releases, FATE, web and mailinglists are on other servers\n    and were not affected. We believe that the original compromise happened\n    to a server, unrelated to FFmpeg and MPlayer, several months ago.\n    That server was used as a source to clone the VM that we recently moved\n    Trac to. It is not known if anyone used the backdoor that was found.\n  </p><p>\n    We recommend all users to change their passwords.\n    <b>Especially users who use a password on Trac that they also use\n      elsewhere, should change that password at least elsewhere.</b></p><h3>November 12, 2013, FFmpeg RFP in Debian</h3><p>\n    Since the splitting of Libav the Debian/Ubuntu maintainers have followed\n    the Libav fork. Many people have requested the packaging of ffmpeg in\n    Debian, as it is more feature-complete and in many cases less buggy.\n  </p><p><a href=\"http://cynic.cc/blog/\">Rog√©rio Brito</a>, a Debian developer,\n    has proposed a Request For Package (RFP) in the Debian bug tracking\n    system.\n  </p><p>\n    Please let the Debian and Ubuntu developers know that you support packaging\n    of the real FFmpeg! See Debian <a href=\"http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=729203\">ticket #729203</a>\n    for more details.\n  </p><h3>October 28, 2013, FFmpeg 2.1</h3><p>\n    We have made a new major release ()\n    It contains all features and bugfixes of the git master branch from 28th October.\n    A partial list of new stuff is below:\n  </p><pre>    - aecho filter\n    - perspective filter ported from libmpcodecs\n    - ffprobe -show_programs option\n    - compand filter\n    - RTMP seek support\n    - when transcoding with ffmpeg (i.e. not streamcopying), -ss is now accurate\n    even when used as an input option. Previous behavior can be restored with\n    the -noaccurate_seek option.\n    - ffmpeg -t option can now be used for inputs, to limit the duration of\n    data read from an input file\n    - incomplete Voxware MetaSound decoder\n    - read EXIF metadata from JPEG\n    - DVB teletext decoder\n    - phase filter ported from libmpcodecs\n    - w3fdif filter\n    - Opus support in Matroska\n    - FFV1 version 1.3 is stable and no longer experimental\n    - FFV1: YUVA(444,422,420) 9, 10 and 16 bit support\n    - changed DTS stream id in lavf mpeg ps muxer from 0x8a to 0x88, to be\n    more consistent with other muxers.\n    - adelay filter\n    - pullup filter ported from libmpcodecs\n    - ffprobe -read_intervals option\n    - Lossless and alpha support for WebP decoder\n    - Error Resilient AAC syntax (ER AAC LC) decoding\n    - Low Delay AAC (ER AAC LD) decoding\n    - mux chapters in ASF files\n    - SFTP protocol (via libssh)\n    - libx264: add ability to encode in YUVJ422P and YUVJ444P\n    - Fraps: use BT.709 colorspace by default for yuv, as reference fraps decoder does\n    - make decoding alpha optional for prores, ffv1 and vp6 by setting\n    the skip_alpha flag.\n    - ladspa wrapper filter\n    - native VP9 decoder\n    - dpx parser\n    - max_error_rate parameter in ffmpeg\n    - PulseAudio output device\n    - ReplayGain scanner\n    - Enhanced Low Delay AAC (ER AAC ELD) decoding (no LD SBR support)\n    - Linux framebuffer output device\n    - HEVC decoder, raw HEVC demuxer, HEVC demuxing in TS, Matroska and MP4\n    - mergeplanes filter\n  </pre><p>\n    We recommend users, distributors and system integrators to upgrade unless they use\n    current git master.\n  </p>","contentLength":60817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44985730"},{"title":"Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing","url":"https://arxiv.org/abs/2508.12631","date":1755873811,"author":"omarsar","guid":237061,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44985278"},{"title":"A guide to Gen AI / LLM vibecoding for expert programmers","url":"https://www.stochasticlifestyle.com/a-guide-to-gen-ai-llm-vibecoding-for-expert-programmers/","date":1755873431,"author":"ChrisRackauckas","guid":237060,"unread":true,"content":"<p>I get it, you‚Äôre too good to vibe code. You‚Äôre a senior developer who has been doing this for 20 years and knows the system like the back of your hand. Or maybe you‚Äôre the star individual contributor who is the only person who can ever figure out how to solve the hard problems. Or maybe you‚Äôre the professor who created the entire subject of the algorithms you‚Äôre implementing. I don‚Äôt know you, but I do know that you think you‚Äôre too good to vibe code. And guess what, you‚Äôre absolutely and totally wrong.</p><p>Facetious? Maybe‚Ä¶ but I will go even further.</p><p>No, you‚Äôre not too good to vibe code. In fact, you‚Äôre the only person who should be vibe coding.</p><p>But I started picking up this ‚Äúvibe coding‚Äù about a month ago and I found out that it can be a really powerful tool, in the right circumstances and in the right workflow. For the record, I now have about 32 Claude agents continuously running in tmux windows that I can ssh to, so all day long I can just check via laptop or phone and keep plugging along. This was completely unheard of a month ago, but it‚Äôs here.</p><p>This is the expert‚Äôs guide to vibe coding for those who are scoffing at those kids who don‚Äôt know what they are doing, but also want to start doing it correctly.</p><h2>A Mental Model for LLM Agents: Your Sophomore Year Student/Intern</h2><p>Drop the hype, I‚Äôm not here to sell you a ChatGPT so I‚Äôm not going to tell you it‚Äôs PhD level when it 100% absolutely clearly isn‚Äôt to anyone who has ever met a PhD in their life. But it is something, what is it?</p><p>Think about an LLM agent as a dedicated intern, or a student who is around the proficiency of a sophomore in college. They know the basics of what programming looks like, they can copy other ideas and architectures, they know how to do things like run unit tests, and they know how to Google things. They have had their basic programming course, and probably have done a deep dive into some random subject as a higher level course, but if you quiz them on the topic enough you‚Äôll learn they haven‚Äôt actually learned it deeply. The kid seems smart enough, you‚Äôd give them a shot.</p><p>If this was a person who showed up to your office looking for work, what would you do with them? Generally you would do one of two things. First, you could sandbox their work. Now the reason you sandbox the work of a new student or intern is rather simple: it‚Äôs because you don‚Äôt know a new subject/tool well, you want to give it a try, and ehh why not let‚Äôs see what happens. If you took the sandbox route, you probably aren‚Äôt caring about the code (it‚Äôs likely to be unmaintainable and bit rot anyways if it‚Äôs not in your core repos), it‚Äôs about getting the artifact. You vibe code a bit, ‚Äúthat looks cool!‚Äù, throw it into a demo / LinkedIn post, and then move on. That‚Äôs the simple vibe coding you may have already tried and thought ‚Äúthat‚Äôs not useful enough‚Äù. It works, but that‚Äôs not what we‚Äôre here for, so no need to mention that more in the blog.</p><p>The second path, in the complete opposite direction, you would integrate the student/intern into a project you know well because it makes it very easy for you to review their work: you can give clear feedback because you‚Äôve already made the first 10 mistakes they will make, you already know how to tell them what is next, and you have the first 6 months of the project planned out for them so it‚Äôs low maintenance. This is how you would train most people that you want to stay long term, right? In the same way, this is the path to take with the LLM agents.</p><p>So let‚Äôs walk through this process step by step.</p><h3>Major Point: Vibe coding turns everyone into a team lead, but not everyone should be a team lead</h3><p>Leading a team of programmers is hard! It takes time, skill, and patience. I think everyone when they are a kid thinks ‚ÄúI don‚Äôt want to be the worker, I want to be the boss and I just sit in my chair and tell people what to do and boom it all gets done!‚Äù. But after your second group project in college, you pretty quickly realize that if you lead a team wrong, you instead just end up doing all of the work yourself while having the expectation of 4 people. </p><p>Now there‚Äôs a few reasons for this. One issue with trying to establish team programming is that you may just not know the subject well enough. If it takes you a while to understand the subject, what someone is trying to do, and what their code is about, then it‚Äôs just not worth the time to manage someone else. You need to be at a point where you can very quickly see the code, understand what‚Äôs going on, and say ‚ÄúI can‚Äôt merge this until you have tests for X and Y, and also show me a plot of Z so I know how it all relates‚Äù. If you cannot instantly see that kind of feedback, then you probably aren‚Äôt experienced enough to lead. You need to write a few million lines of code before it becomes automatic where you just look at code and say ‚Äúdon‚Äôt do that, that‚Äôll be a performance bottleneck‚Äù, but you need that quickness to the code review in order for vibe coding to work. But also, let me say this bluntly:</p><h4>If you are an individual contributor who usually does not like to train interns because you find that they take more time than they are helpful, then don‚Äôt vibe code</h4><p>Some people tend to do better at working by directing. Other really smart people just can‚Äôt seem to get good work out of other people. It‚Äôs not an indictment, Silicon Valley created the ‚Äúindividual contributor‚Äù role for a reason. If you are one of those people, then vibe coding may not be for you as you will likely grow frustrated with the agents even quicker than you would a human (they somehow retain less information than even the worst intern, at least they remember your favorite lunch order).</p><p>So go in with this mindset: I will have to have meetings with the agents, I will need to plan and give them tasks, and I will need to review the code. If I find this stuff to be slower than coding myself, then just stop right now. But if you do well with a team, then go on. How do we now make this team effective?</p><h2>What is the workflow of vibe coding correctly?</h2><p>If someone shows you Claude Code and you ask it to try and solve the problem that you were just working on (obviously a hard problem, because if it ends up on your desk that means someone else failed to solve it), you just poke and laugh at Claude when it fails miserably. But you would have never done that with a new intern or student (hopefully), so why do this? Again, you know how smart it actually is, cut the hype, and treat it the same way. This immediately leads to a few workflow principles:</p><h3>The workflow of vibe coding is the same as managing scrums or helping a student through a research thesis</h3><p>You have a problem, you give it to the agent, you review the results, and then you give it feedback. This is exactly how you would manage a scrum team or help a student through their thesis. You don‚Äôt just give them the problem and expect them to solve it, you give them the problem, they come back with a solution, and then you tell them what to do next.</p><p>You probably already get most of your work done this way if you‚Äôre senior enough. Every professor has more students coding than themselves, and every senior developer has a total amount of code created by their team that is far greater than their own. Just think of Claude as your pack of newbies that just started. Now if you‚Äôre thinking ‚Äúbut it can be difficult to manage a bunch of newbies‚Äù‚Ä¶ yeah, that means you‚Äôre actually senior enough to understand how to do this right. It‚Äôs fairly easy to send a new intern or student off on a project, and if their pay/grade depends on it getting done they will give you something back. Whether it‚Äôs any good depends on how well you chunked up the work for them and gave them an appropriate task. </p><p>But one key thing is, if you had to do a meeting every 10 minutes it would drive you crazy, so don‚Äôt. Set up say 12-32 agents running on different processes, preferably sandboxed on some other compute resource (sandboxed so they can‚Äôt break the machine, but also so they can have a Github authentication that does not have core read/write privileges. This way you can tell it to have ‚Äúdangerously unsafe permissions‚Äù and the worst that happens is it segfaults its own docker container and never opens a PR). Give it a full command:</p><p>‚Äútry solving (an easy issue in this open source repository). Create a PR with the solution, and after an hour check the continuous integration to see if tests are passing. If tests are not passing, assess what the issue is, and if it is a quick fix make a commit to handle it, otherwise report what the core difficulty of the problem is‚Äù</p><h4>Don‚Äôt spend too much time setting up the calls, just pull from lists you already have and let it find ‚Äúwhatever is easy‚Äù</h4><p>Make it clear, make it easy, make it know the steps, and let it just keep cycling for a bit.</p><h3>How to review vibe coding: immediately throw out anything bad</h3><p>If you saw a student was cheating and just copy-pasted from StackOverflow but couldn‚Äôt explain what it did, you‚Äôd throw it out and tell them to try again. If your new intern didn‚Äôt reuse all of the solid code your team had written and instead rewrote some low level detail in a buggy and unmaintainable way, you‚Äôd throw it out and tell them to try again. If they wrote a function that was 500 lines long and did 10 different things, you‚Äôd throw it out and tell them to try again. You wouldn‚Äôt waste your time trying to fix it, you‚Äôd just tell them to try again.</p><p>Again, treat the LLMs the same way. I see a lot of people following the mindset they see the vibe coding YouTubers making their silly games. ‚ÄúChatGPT, try harder! Fix for me!‚Äù. You want to know a secret? That stuff is worse than worthless. The problem is that these LLMs are made to please you, so if you tell them to try harder, they will either start hallucinating or just start changing your tests. Don‚Äôt even give it a try. The moment you see it go off the rails, just throw it out. That problem is too hard for Claude, it‚Äôs for you now.</p><p>Send a bunch of commands at 9am. At noon, check on them. You might have 10 done. 8 of them probably went off the rails, whatever, fire them. Hey two PRs worked, whoopee! Fire 10 more, come back at 3. 20 done, 4 successes and 16 failures. Fire a few more off, maybe a few clean up ones to look for missing docstrings or dig around to see if any performance regressions were introduced. At 6, see the other 4 successes and cut the other jobs. </p><h4>Vibe coding is useful only if you have enough problems that you‚Äôre happy that some subset being solved, not caring what in that subset is solved.</h4><p>10 PRs were merged, plus whatever you were working on that day (yes, because you didn‚Äôt focus on this for most of your day!). You might think, that‚Äôs like 10/40 = 25% success rate, that‚Äôs not good. But you know what? Those were free. You just got a lot of extra stuff done that you wouldn‚Äôt have otherwise. The success rate is just a matter of how much these things give value for their cost. That‚Äôs for Sam Altman to worry about. But if you have a subscription to these LLMs, just keep burning through the tokens who cares. Don‚Äôt worry about success rate, just go for total successes.</p><h3>Where to apply vibe coding: code you know very well</h3><p>So this leads to a very counter-intuitive fact that may come out of left field, but I‚Äôm serious. Everyone‚Äôs first inclination is to throw it on some project they haven‚Äôt actually contributed to and get banned (okay, maybe it just looks like that to open source maintainers). But the real issue is that, the majority of your time will be spent doing code review. If you do this on code you don‚Äôt know well, you will have to spend a lot of time trying to understand the code and at that point, why not just write the code yourself?</p><p>This is where most people seem to just stop and drop the idea of vibe coding all together. But instead‚Ä¶ what about applying it to the code base you‚Äôre on? No, not on the hard problems you‚Äôre thinking about, but all of those little side problems? The small refactor you put off for the last 6 months? What about bisecting the Git commits to find the exact cause of the performance regression that showed up on master a week ago? Or you created a version specialized for Windows and Mac but left a ‚Äútodo‚Äù over the Linux section because it‚Äôs easy but would be 4 hours of monotonous work? All of those things, if someone showed up with the code, you could review it in about 5 minutes and know whether it‚Äôs right or wrong. Give the agents that stuff!</p><h4>Vibe coding is not useful if you need it to solve a particular problem. You still do the hard stuff</h4><p>In just the same way, the best place to put trainees is in the project that you already know well because that makes it easy to review their work. It‚Äôs the ‚ÄúI don‚Äôt have time for you, so try this easy task‚Äù approach. You know the code, you know the problem, and you can give them a task that is easy enough that they can do it without too much help. This is the same principle here.</p><h2>Some Examples of Vibe Coding PRs</h2><h3>Example 1: The Simple Success Story</h3><p><a href=\"https://github.com/SciML/OrdinaryDiffEq.jl/pull/2856\">Here‚Äôs a quick and simple PR</a>, the kind that is perfect here. If you don‚Äôt know performance Julia handling or trim, basically it‚Äôs a new feature in Julia v1.12 where Julia can now build <a href=\"https://www.youtube.com/watch?v=R0DEG-ddBZA\">small lean binaries</a>. In order to do that, you need to make sure functions fully specialize, which they don‚Äôt by default as that would create a lot of extra compilation in many circumstances, but for higher order numerical solvers that is the behavior we want. So I told it to go specialize all instances of the function in the package, and I could check the PR fairly quickly and see it stuck to the goals and did it. This is then going to be followed up with new tooling that will perform static checks of trimming compatibility (<a href=\"https://github.com/SciML/NonlinearSolve.jl/pull/665\">still being worked out</a>), but with just those backwards compatible minor changes things seem to work in the beta, so merge now and add those tests when we have a good system for it.</p><p>1 minute to write the query, come back later and 1 minute to review.</p><h4>80% of coding is grunt work. Simple stuff. Throw it at the LLM and smile if it solves it, or just groan and finish it if it doesn‚Äôt</h4><p>This is exactly the kind of small targeted change these are geared towards. Most of the PRs aim to be like this. </p><p>Actually, one of my favorite prompts is ‚ÄúLook through the XXXXXXX repository, find the easiest issue, and make a PR that solves it.‚Äù <a href=\"https://github.com/SciML/Optimization.jl/pull/947\">Here‚Äôs an example of a PR from that</a>. Yay it was a hit! Merged. It should make a minimal number of changes, be small and easy to review, otherwise just dump it, that means it probably went off the rails. There‚Äôs just a lot of these little ‚Äúthis API would be easier if you added a conversion from float to Int‚Äù (which sometimes you go ‚Äúoh, no that‚Äôs a bad idea, close PR and issue!). </p><h4>Even if you work on hard stuff, a huge chunk of your work isn‚Äôt hard stuff. There‚Äôs a lot of simple janitorial work you have to do on your code all of the time. Automate that part.</h4><h3>Example 2: The Immediately Closed ‚ÄúThat‚Äôs not for Claude‚Äù PR</h3><p><a href=\"https://github.com/SciML/SciMLSensitivity.jl/pull/1266\">This PR</a> came from pointing it at the fact that every once in awhile I get a test failure in the docs build for a chaotic ODE differentiation w.r.t. ergodic properties tutorial. <a href=\"https://frankschae.github.io/post/shadowing/\">It is a very fun topic</a>, but generally anything with real math in it is too hard for the LLMs. And in this, yeah I could see immediately that this PR does not make sense‚Ä¶ well it did. The NaN‚Äôs and Infs were definitely coming from a numerical issue in the least squares shadowing code, and what this pointed to was the Schur complement was being done with things like B * Diagonal(wBinv) * B‚Äô which as a numerical analyst I can immediately see would double the condition number of the matrix, but there doesn‚Äôt seem to be an immediate solution with open source linear algebra things I could find. So closed this, sent a note over to <a href=\"https://math.mit.edu/directory/profile.html?pid=63\">Alan Edelman</a> to try and figure out what the better way to do this factorization. While it didn‚Äôt solve the problem, at least I know what the problem is now.</p><p>This is probably what most of the PRs become. It gives a hint of where the problem is, and then I take the reins.</p><h3>Example 3: Repeated Refactors</h3><p><a href=\"https://github.com/SciML/NonlinearSolve.jl/pull/672\">Is a sweet and simple PR that refactors the tests to move some things</a>, specifically the Enzyme automatic differentiation engine testing, to a ‚Äúno pre‚Äù set. The ‚Äúno pre‚Äù means ‚Äúdoes not run on prereleases of the next language version‚Äù, since these tools touch language internals in the compiler so they are never ready early. This always make prerelease tests fail before they actually test anything meaningful, so I wanted to move all Enzyme usage to a ‚Äúno pre‚Äù set in every repo it showed up. </p><p>About 5 minutes to write the query. Some of the test suites needed a simple Github suggestion to fix up a little detail here or there. About 5 minutes to get this thing into 8 repos. Now I was ready to start using prerelease tests. Would‚Äôve been at least a half hour by hand just because we didn‚Äôt have an easy system for doing this before. Maybe that‚Äôs a little dirtier than the perfect regex, but whatever 10 minutes of my time sounds like a win.</p><p>Refactors generally work out really well and are one of the top uses for the tool. ‚Äúmake it correct, write good tests, and let it refactor‚Äù is generally a lazy way to get 90% of the way there.</p><h3>Example 4: The Information Gathering PR</h3><p><a href=\"https://github.com/SciML/LinearSolve.jl/pull/734\">Here is a pull request</a> that was generated by pointing it to solve <a href=\"https://github.com/SciML/LinearSolve.jl/issues/593\">this issue</a>. That issue was mostly chosen because it was sitting on the issue list for awhile and it didn‚Äôt seem so difficult but I hadn‚Äôt had the time to track down the memory leak in a not so widely used extension for an alternative C-based sparse matrix solver, but it needed to get done some time. So, throw the bot on it.</p><p>And what it comes back with is to add a memory finalizer (i.e. how to tell the GC how to remove the memory) for the other library. I could take one look at it and immediately see that kind of code should not live in this library, it should live in the library where the solver is bound to the language, and the fact that it was missing a finalizer is something that should be solved over there. Close the PR, throw out the code, find the <a href=\"https://github.com/JuliaSparse/Pardiso.jl/pull/117\"> stalled discussion on the repo that should have the finalizer</a>, poke the author a bit, and it‚Äôs in. Done, someone just needed to be reminded.</p><p>Total time on my end was about 3 minutes. The bot could have also written that fix but it basically already existed so no need, this was more about finding out where in the system something was offer.</p><h3>Example 5: The ‚ÄúHow Long is that Going to Take?‚Äù PR</h3><p><a href=\"https://github.com/SciML/Catalyst.jl/pull/1317\">Here is a nice PR where it didn‚Äôt finish</a> (at least at the time of writing this) and the reason is because there are lots of other clean ups that need to happen for this to ever work. How far away is it? Well it generated a set of tests that cleanly listed out all 120 things to solve. Great, this is probably a full week‚Äôs task‚Ä¶ I knew it would be a lot but that is now pretty concrete. I probably won‚Äôt use the bot to finish this one, but now if someone asked what the effort would be I can give them a pretty clear estimate because it has been reduced from ‚Äúsomeone needs to give it a try, seems like a good chunk of work‚Äù to ‚Äúthe hard part is making these 120 things happen, which is easy but tedious and it would take about a week, probably not worth the effort right now‚Äù. That‚Äôs very useful when planning ahead. Total me time was about 5 minutes, plus the PR discussion time to explain to others what the results meant.</p><h2>Conclusion: Vibe Coding Done Right is actually an Expert‚Äôs Task</h2><p>Vibe coding turns any individual into the CTO leading a team of 20, 30, 50, 60 interns. You immediately get a massive team. It takes time and experience to actually handle a group like this correctly and to make it be productive. Making all of 60 interns not break the performance, correctness, or maintainability of your code is very difficult. So I do not recommend this to people who are still ‚Äúup and coming programmers‚Äù. But if you‚Äôre a bit more senior and starting to grow your group, well this is then a cheap way to accelerate that. </p><p>What that means is, vibe coding is sold for people who don‚Äôt know how to program, but if you actually think about it, the main audience that can actually use it correctly is experts.</p><h3>A few side remarks I didn‚Äôt get to</h3><h4>A bunch more examples and how bad Claude is at math</h4><p>In the <a href=\"https://discourse.julialang.org/t/the-use-of-claude-code-in-sciml-repos/131009/8?u=chrisrackauckas\">Julia Discourse forum</a>, I detail a bunch of different PRs to show where the AIs tend to succeed and fail. But generally anything that is mostly about ‚Äúprogramming‚Äù (refactoring, inefficient implementations, etc.) the LLMs do well. Anything about the domain or application (differential equations, engineering, physics for me) it just seems to flat out do something dumb. The results all lined up very clearly shows what kind of PRs you should be asking it to make and which ones just aren‚Äôt worth the effort.</p><h4>The role of empathy in vibe coding success</h4><p>Some of the least empathetic people I know in open source are the ones who are also the most skeptical of vibe coding. I have a heavy speculation that they speak to the agents similarly to how they speak to other potential contributors, and drive the bots away the same way they do to people. But with a bot, it will always try to make you happy, just by hallucinating and commenting out your tests. These same people also don‚Äôt want the bots around because they claim that‚Äôs all the bots ever do. Weird coincidence. I wonder what this will do to the culture of programming over time.</p><h4>The cost may not make sense in the long run, but it does while the VCs are paying for it</h4><p>On my \\$200/month Claude 20x Max subscription I used enough tokens for about $5,200 of compute in the first month. This is obviously not sustainable, but hey, it‚Äôs a startup world and VCs are paying for it right now. If you can get a few extra features done that get you more funding, then this is worth it. If you‚Äôre a professor and you can get a few more papers out, then this is worth it. If you‚Äôre an individual contributor at a big company and you can get a few more features out that make your team look good, then this is worth it. Will it be worth it after the money runs out? Who knows, but mine while the gold is there.</p><h4>What‚Äôs the right setup? Easy, Claude Code</h4><p>The tab-complete stuff is pretty annoying. The power comes from running agents. Claude Code has a simple setup and is able to start running code. Just write a decent Claude.md that tells it to stop being so nice and instead just tell me when it cannot solve the problem, and you‚Äôre good to go. The <a href=\"https://github.com/upstash/context7\">context7 MCP is good</a>, <a href=\"https://github.com/modelcontextprotocol/servers/tree/main/src/sequentialthinking\">Sequential Thinking as well</a>. </p><h4>‚ÄúThis sounds like hell?‚Äù: A response to seeing what vibe coding is like</h4><p>Hey Hacker News! Looks like it got there. One of the interesting comments is ‚ÄúTo me, someone who actually love programming, it makes vibe coding look like hell.‚Äù I 100% agree! I stayed away for a bit because I was like ‚Äúugh, that‚Äôs not the fun part, I like the coding!‚Äù. But, notice from the examples that the amount of time I‚Äôm spending on this I try to keep as minimal as possible. I like to program, and I need to be programming because I can do the hard stuff while the LLM can‚Äôt. The goal is to get as much easy stuff done with as minimal work on your end as possible, so you can stop worrying about the annoying/boring stuff and can focus more time on the interesting work.</p><p>‚Äú‚Äù‚Äù\nI choose the tech stack and architect the project.<p>\nI choose the language patterns and code organization.</p>\nI step in to solve hard problems when agents flounder.<p>\nWhat about that says middle management? It‚Äôs just getting rid of all the low iq parts of the job.</p>\n‚Äú‚Äù‚Äù</p><p>But then again, if you also just really dislike having to do meetings at all and prefer to just be coding alone, then‚Ä¶ yeah maybe the agents will probably drive you mad. They are loud, lie to you, and you have to sift through emails/PRs of bullshit and just delete most of what these things come up with. Again, be quick to delete when these things go off the rails, it will save your sanity. </p><p>All that said, with the amount of code coming from LLMs I think there will be an even greater need for good individual contributors to yell at the clouds and maintain the integrity of the codebase so, you‚Äôll have your place. You just might have a larger load of PRs coming your way and may want to be quicker to shut a few down. I don‚Äôt think it will remove those jobs (though it may remove the intern jobs to build up the resume to get there‚Ä¶ which is a societal issue that I‚Äôll not touch on here‚Ä¶ just like I‚Äôm not talking about the energy issue which is also a major potential flaw with LLM workflows)</p>","contentLength":24939,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44985207"},{"title":"Thunderbird Pro August 2025 Update","url":"https://blog.thunderbird.net/2025/08/tbpro-august-2025-update/","date":1755872980,"author":"mnmalst","guid":236973,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44985131"},{"title":"All managers make mistakes; good managers acknowledge and repair","url":"https://terriblesoftware.org/2025/08/22/the-management-skill-nobody-talks-about/","date":1755867039,"author":"matheusml","guid":236911,"unread":true,"content":"<blockquote><p><em>‚ÄúThere is a crack in everything. That‚Äôs how the light gets in.‚Äù</em> ‚Äî Leonard Cohen</p></blockquote><p>Let me tell you something that will happen after you become a manager: you‚Äôre going to mess up. A lot. You‚Äôll give feedback that lands wrong and crushes someone‚Äôs confidence. You‚Äôll make a decision that seems logical but turns out to be completely misguided. You‚Äôll forget that important thing you promised to do for someone on your team. You‚Äôll lose your temper in a meeting when you should have stayed calm.</p><p>The real question isn‚Äôt whether you‚Äôll make mistakes; it‚Äôs what you do .</p><p>I recently read&nbsp;&nbsp;by Dr. Becky Kennedy, a parenting book that completely changed how I think about this. She talks about how the most important parenting skill isn‚Äôt being perfect ‚Äî it‚Äôs&nbsp;. When you inevitably lose your patience with your kid or handle something poorly, what matters most is going back and fixing it. <strong>Acknowledging what happened, taking responsibility, and reconnecting.</strong></p><p>Sound familiar? Because that‚Äôs what good management is about too.</p><p>Think about the worst manager you ever had. I bet they weren‚Äôt necessarily the ones who made the most mistakes. But they were probably the ones who never acknowledged them. Who doubled down when they were wrong. Who let their ego prevent them from admitting they didn‚Äôt have all the answers.</p><p>Here‚Äôs a pattern I see play out constantly: A manager commits to something without consulting the team. Maybe it‚Äôs a feature at a client demo, a timeline in a board meeting, or just a ‚Äúsmall favor‚Äù for another department. The team scrambles to deliver, working nights and weekends. They make it happen, but barely, and with real costs: technical debt, burned-out engineers, resentment building.</p><p>What happens next determines everything. The manager who never acknowledges what they put the team through? That‚Äôs how you lose your best people. But the manager who comes back and says, <em>‚ÄúI put you in an impossible position. I should have consulted you first. I‚Äôm sorry for the stress that caused, and here‚Äôs how I‚Äôll handle it differently next time‚Äù</em>, that manager builds trust even through the mistake.</p><p>I‚Äôve been on both sides of this. As an engineer, I watched managers make the same mistakes over and over again, never acknowledging the chaos they created. As a manager, I‚Äôve been the one creating that chaos ü•≤. The difference in outcomes is massive; when you own your mistakes completely and specifically, something unexpected happens: your team trusts you more, not less.</p><p>Here‚Äôs what repair looks like in practice:</p><ol><li><strong>Be specific about what you did wrong.</strong>&nbsp;Not ‚Äúmistakes were made‚Äù or ‚Äúthings could have gone better.‚Äù But ‚ÄúI interrupted you three times in that meeting and dismissed your concerns. That was wrong.‚Äù</li><li>&nbsp;This isn‚Äôt the time for a long explanation of your stress levels or why you acted that way. Save that for your therapist or your own manager. The repair is about acknowledging the impact on the other person.</li><li><strong>Actually change the behavior.</strong>&nbsp;An apology without changed behavior is just empty words. If you keep making the same ‚Äúmistake,‚Äù it‚Äôs not a mistake anymore; it‚Äôs a choice.</li><li>&nbsp;One conversation doesn‚Äôt instantly repair broken trust. It‚Äôs a starting point, not a finish line. You have to consistently show up differently.</li></ol><p><strong>The beautiful thing about getting comfortable with repair is that it actually makes you better as a manager</strong>. When you know you can fix things when they go wrong, you‚Äôre more willing to make decisions, have difficult conversations, and take reasonable risks. You stop being paralyzed by perfectionism because you know that most mistakes, while serious, create opportunities for growth and stronger relationships when handled well.</p><p>This doesn‚Äôt mean being reckless or careless. It doesn‚Äôt mean making the same mistakes repeatedly. And it definitely doesn‚Äôt mean using repair as a get-out-of-jail-free card for being a shitty manager.</p><p>What it means is accepting that you‚Äôre human, that management is complex, and that you won‚Äôt always get it right. Your job isn‚Äôt to be perfect. Your job is to&nbsp;<a href=\"https://terriblesoftware.org/2025/06/13/good-engineer-bad-engineer/\">ship working software that adds real value to users</a>, to help your team grow, and to create an environment where people can do their best work. </p><p>Sometimes you‚Äôll fail at those things. When you do, you repair, you learn, and you keep going.</p>","contentLength":4370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44983986"},{"title":"What about using rel=\"share-url\" to expose sharing intents?","url":"https://shkspr.mobi/blog/2025/08/what-about-using-relshare-url-to-expose-sharing-intents/","date":1755863348,"author":"edent","guid":237166,"unread":true,"content":"<p>Let's say that you've visited a website and want to share it with your friends.  At the bottom of the article is a list of popular sharing destinations - Facebook, BlueSky, LinkedIn, Telegram, Reddit, HackerNews etc.</p><img alt=\"Screenshot. &quot;Share this page on&quot; followed by colourful icons for popular social networks.\" height=\"452\" src=\"https://shkspr.mobi/blog/wp-content/uploads/2025/08/share-on.webp\" width=\"824\"><p>You click the relevant icon and get taken to the site with the sharing details pre-filled.</p><img alt=\"Screenshot of the Telegram sharing page.\" height=\"561\" src=\"https://shkspr.mobi/blog/wp-content/uploads/2025/08/telegram.webp\" width=\"600\"><p>The problem is, every different site has a different intent for sharing links and text.  For example:</p><ul><li><code>https://www.facebook.com/sharer.php?u=‚Ä¶&amp;t=‚Ä¶</code></li><li><code>https://www.linkedin.com/sharing/share-offsite/?url=‚Ä¶</code></li><li><code>https://bsky.app/intent/compose?text=‚Ä¶</code></li><li><code>https://www.threads.net/intent/post?url=‚Ä¶&amp;text=‚Ä¶</code></li><li><code>https://www.reddit.com/submit?url=‚Ä¶&amp;title=‚Ä¶</code></li></ul><p>As you can see, some only allow a URL, some text and a URL, and some just a plain text which could contain the URl. A bit of a mess! It's probably impossible to get every site to agree on a standard for their sharing intent. But there  be a standard for exposing their existing sharing mechanism.</p><blockquote><p>ShareOpenly knows about most major social networks, as well as decentralized platforms like Mastodon, Bluesky, and Known.</p><p>However, if ShareOpenly is having trouble sharing to your platform, and if your platform supports a share intent, you can add the following metatag to your page headers:</p><p><code>&lt;link rel=\"share-url\" href=\"https://your-site/share/intent?text={text}\"&gt;</code></p><p>Where <code>https://your-site/share/intent?text=</code> is the URL of your share intent.</p><p>The special keyword  will be replaced with the URL and share text.</p></blockquote><p>I think that's a pretty nifty solution.</p><p>For sites which take a URl and an (optional) title, the meta element looks like:</p><p>For those which only take URl, it looks like:</p><p>It's slightly trickier for sites like Mastodon and BlueSky which only have a text sharing field and no separate URl.  The current proposal is just to use the text. For example</p><p>But it could be something like</p><p>Adding to that page merely requires a formal specification to be written up. After that, some light lobbying might be needed to get social networks to adopt it.</p><p>So, I have three questions for you:</p><ol><li>Do you think \n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t is a good idea for a new standard?\n\t\t\t\t\t\t</li><li>What changes, if any, would you make to the above proposal?</li><li>Would you be interested in using it - either as a sharer or sharing destination?</li></ol><p>Please leave a comment in the box - and remember to hit those sharing buttons!</p>","contentLength":2299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44983364"},{"title":"4chan will refuse to pay daily online safety fines, lawyer tells BBC","url":"https://www.bbc.co.uk/news/articles/cq68j5g2nr1o","date":1755856941,"author":"donpott","guid":236972,"unread":true,"content":"<p>Some American politicians - particularly the Trump administration, its allies and officials - have pushed back against what they regard as overreach in the regulation of US tech firms by the UK and EU. </p><p>A perceived impact of the Online Safety Act on free speech has been a particular concern, but other laws have also been the source of disagreement.</p><p>On 19 August, US Director of National Intelligence Tulsi Gabbard said the UK had withdrawn its controversial demand for a \"backdoor\" in an Apple data protection system - saying she worked with the President and Vice President to get the UK to abandon its plan.</p><p>Two days later, US Federal Trade Commission chairman Andrew Ferguson warned big tech firms they could be violating US law if they weakened privacy and data security requirements by complying with international laws such as the Online Safety Act.</p><p>\"Foreign governments seeking to limit free expression or weaken data security in the United States might count on the fact that companies have an incentive to simplify their operations and legal compliance measures by applying uniform policies across jurisdictions,\" he said.</p><p>If 4chan does successfully fight the fine in the US courts, Ofcom may have other options.</p><p>\"Enforcing against an offshore provider is tricky,\" Emma Drake, partner of online safety and privacy at law firm Bird and Bird, told the BBC. </p><p>\"Ofcom can instead ask a court to order other services to disrupt a provider's UK business, such as requiring a service's removal from search results or blocking of UK payments.</p><p>\"If Ofcom doesn't think this will be enough to prevent significant harm, it can even ask that ISPs be ordered to block UK access.\"</p>","contentLength":1667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44982681"},{"title":"Go is still not good","url":"https://blog.habets.se/2025/07/Go-is-still-not-good.html","date":1755854723,"author":"ustad","guid":236865,"unread":true,"content":"<p>These things about Go are bugging me more and more. Mostly because they‚Äôre so\nunnecessary. The world knew better, and yet Go was created the way it was.</p><p>For readers of previous posts you‚Äôll find some things repeated here. Sorry\nabout that.</p><h2>Error variable scope is forced to be wrong</h2><p>Here‚Äôs an example of the language forcing you to do the wrong thing. It‚Äôs very\nhelpful for the reader of code (and code is read more often than it‚Äôs written),\nto minimize the scope of a variable. If by mere syntax you can tell the reader\nthat a variable is just used in these two lines, then that‚Äôs a good thing.</p><div><pre><code></code></pre></div><p>(enough has been said about this verbose repeated boilerplate that I don‚Äôt have\nto. I also don‚Äôt particularly care)</p><p>So that‚Äôs fine. The reader knows  is here and only here.</p><p>But then you encounter this:</p><div><pre><code></code></pre></div><p>Wait, what? Why is  reused for ? Is there‚Äôs something subtle I‚Äôm\nnot seeing? Even if we change that to , we‚Äôre left to wonder why  is\nin scope for (potentially) the rest of the function. Why? Is it read later?</p><p>Especially when looking for bugs, an experienced coder will see these things\nand slow down, because here be dragons. Ok, now I‚Äôve wasted a couple of seconds\non the red herring of reusing  for .</p><p>Is a bug perhaps that the function ends with this?</p><div><pre><code></code></pre></div><p>Why does the scope of  extend way beyond where it‚Äôs relevant?</p><p>The code would have been so much easier to read if only ‚Äôs scope had been\nsmaller. But that‚Äôs not syntactically possible in Go.</p><p>This was not thought through. Deciding on this was not thinking, it was typing.</p><div><pre><code></code></pre></div><p>‚ÄúWhat color is your nil?‚Äù ‚Äî The two billion dollar mistake.</p><p>The reason for the difference boils down to again, not thinking, just typing.</p><p>Adding comment near the top of the file for conditional compilation must be the\ndumbest thing ever. Anybody who‚Äôs actually tried to maintain a portable program\nwill tell you this will only cause suffering.</p><p>The problem is that this is not year 350 BCE. We actually have experience that\naside from air resistance, heavy and light objects actually fall at the same\nspeed. And we have experience with portable programs, and would not do\nsomething this dumb.</p><p>If this had been the year 350 BCE, then this could be forgiven. Science as we\nknow it hadn‚Äôt been invented yet. But this is after decades of very widely\navailable experience in portability.</p><h2> with no defined ownership</h2><div><pre><code></code></pre></div><p>Probably . Who wants that? Nobody wants that.</p><div><pre><code></code></pre></div><p>If you guessed , then you know more than anybody should have\nto know about quirks of a stupid programming language.</p><p>Even in a GC language, sometimes you just can‚Äôt wait to destroy a resource. It\nreally does need to run as we leave the local code, be it by normal return, or\nvia an exception (aka panic).</p><p>What we clearly want is RAII, or something like it.</p><div><pre><code></code></pre></div><p>Python has it. Though Python is  entirely refcounted, so one can pretty\nmuch rely on the  finalizer being called. But if it‚Äôs important, then\nthere‚Äôs the  syntax.</p><div><pre><code></code></pre></div><p>Go? Go makes you go read the manual and see if this particular resource needs\nto have a defer function called on it, and which one.</p><div><pre><code></code></pre></div><p>This is so dumb. Some resources need a defer destroy. Some don‚Äôt. Which ones?\nGood fucking luck.</p><p>And you also regularly end up with stuff like this monstrosity:</p><div><pre><code></code></pre></div><p>Yes, this is what you NEED to do to safely write something to a file in Go.</p><p>What‚Äôs this, a ? Oh yeah, of course that‚Äôs needed. Is it even\nsafe to double-close, or does my defer need to check for that? It happens to be\nsafe on , but on other things: WHO KNOWS?!</p><h2>The standard library swallows exceptions, so all hope is lost</h2><p>Go says it doesn‚Äôt have exceptions. Go makes it extremely awkward to use\nexceptions, because they want to punish programmers who use them.</p><p>But all Go programmers must still write exception safe code. Because while\n don‚Äôt use exceptions, other code will. Things will panic.</p><p>So you need, not should, NEED, to write code like:</p><div><pre><code></code></pre></div><p>What is this stupid middle endian system? That‚Äôs dumb just like putting the day\nin the middle of a date is dumb. MMDDYY, honestly? (separate rant)</p><p>But panic will terminate the program, they say, so why do you care if you\nunlock a mutex five milliseconds before it exits anyway?</p><p>Because what if something swallows that exception and carries on as normal, and\nyou‚Äôre now stuck with a locked mutex?</p><p>But surely nobody would do that? Reasonable and strict coding standards would\nsurely prevent it, under penalty of being fired?</p><p>The standard library does that.  when calling , and the\nstandard library HTTP server does that, for exceptions in the HTTP handlers.</p><p>All hope is lost. You MUST write exception safe code. But you can‚Äôt use\nexceptions. You can only have the downsides of exceptions be thrust upon you.</p><p>Don‚Äôt let them gaslight you.</p><h2>Sometimes things aren‚Äôt UTF-8</h2><p>If you stuff random binary data into a , Go just steams along, as\ndescribed <a href=\"https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride\">in this post</a>.</p><p>Over the decades I have lost data to tools skipping non-UTF-8 filenames. I\nshould not be blamed for having files that were named before UTF-8 existed.</p><p>Well‚Ä¶ I had them. They‚Äôre gone now. They were silently skipped in a\nbackup/restore.</p><p>Go wants you to continue losing data. Or at least, when you lose data, it‚Äôll\nsay ‚Äúwell, what (encoding) was the data wearing?‚Äù.</p><p>Or how about you just do something more thought through, when you design a\nlanguage? How about doing the right thing, instead of the obviously wrong\nsimple thing?</p><p>Why do I care about memory use? RAM is cheap. Much cheaper than the time it\ntakes to read this blog post. I care because my service runs on a cloud\ninstance where you actually pay for RAM. Or you run containers, and you want to\nrun a thousand of them on the same machine. Your data may <a href=\"https://yourdatafitsinram.net/\">fit in\nRAM</a>, but it‚Äôs still expensive if you have to\ngive your thousand containers 4TiB of RAM instead of 1TiB.</p><p>You can manually trigger a GC run with , but ‚Äúoh no don‚Äôt do\nthat‚Äù, they say, ‚Äúit‚Äôll run when it has to, just trust it‚Äù.</p><p>Yeah, 90% of the time, that works every time. But then it doesn‚Äôt.</p><p>I rewrote some stuff in another language because over time the Go version would\nuse more and more memory.</p><h2>It didn‚Äôt have to be this way</h2><p>We knew better. This was not the COBOL debate over whether to use symbols or\nEnglish words.</p><p>And it‚Äôs not like when we didn‚Äôt know at the time that <a href=\"https://blog.habets.se/2022/08/Java-a-fractal-of-bad-experiments.html\">Java‚Äôs ideas were\nbad</a>, because we did know Go‚Äôs ideas were bad.</p><p>We already knew better than Go, and yet now we‚Äôre stuck with bad Go codebases.</p><ul><li>https://www.uber.com/en-GB/blog/data-race-patterns-in-go/</li><li>https://fasterthanli.me/articles/lies-we-tell-ourselves-to-keep-using-golang</li><li>https://fasterthanli.me/articles/i-want-off-mr-golangs-wild-ride</li></ul>","contentLength":6578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44982491"},{"title":"LabPlot: Free, open source and cross-platform Data Visualization and Analysis","url":"https://labplot.org/","date":1755853886,"author":"turrini","guid":236971,"unread":true,"content":"<p>In many cases, importing data into LabPlot for further analysis and visualization is the first step in the application: LabPlot supports many different formats (CSV, Origin, SAS, Stata, SPSS, MATLAB, SQL, JSON, binary, OpenDocument Spreadsheets (ods), Excel (xlsx), HDF5, MQTT, Binary Logging Format (BLF), FITS,‚Ä¶</p>","contentLength":315,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44982409"},{"title":"What the Hell Is Going On?","url":"https://catskull.net/what-the-hell-is-going-on-right-now.html","date":1755846481,"author":"todsacerdoti","guid":236910,"unread":true,"content":"<p>What the  is going on right now?</p><p>Engineers are burning out. Orgs expect their senior engineering staff to be able to review and contribute to ‚Äúvibe-coded‚Äù features that don‚Äôt work. My personal observation is that the best engineers are highly enthusiastic about helping newer team members contribute and learn.</p><p>Instead of their comments being taken to heart, reflected on, and used as learning opportunities, hapless young coders are instead using feedback as simply the next prompt in their ‚ÄúAI‚Äù masterpiece. I personally have witnessed and heard first-hand accounts where it was incredibly obvious a junior engineer was (ab)using LLM tools.</p><p>In a recent company town-hall, I watched as a team of junior engineers demoed their latest work. I couldn‚Äôt tell you what exactly it did, or even what it was supposed to do - it didn‚Äôt seem like they themselves understood. However, at a large enough organization, it‚Äôs not about what you do, its about what people  you do. Championing their ‚Äúsuccess‚Äù, a senior manager goaded them into bragging about their use of ‚ÄúAI‚Äù tools to which they responded ‚ÄúThis is four thousand lines of code written by Claude‚Äù. Applause all around.</p><p>I was asked to add a small improvement to an existing feature. After reviewing the code, I noticed a junior engineer was the most recent to work on that feature. As I always do, I reached out to let them know what I‚Äôd be doing and to see if they had any insight that would be useful to me. Armed with the Github commit URL, I asked for context around their recent change. I can‚Äôt know , but I‚Äôd be willing to put money down that my exact question and the commit were fed directly into an LLM which was then copy and pasted back to me. I‚Äôm not sure why, but I felt violated. It felt wrong.</p><p>A friend recently confided in me that he‚Äôs been on a team of at least 5 others that have been involved in reviewing a heavily vibe-coded PR over the past . A month. Reviewing slop produced by an LLM. What are the cost savings of paying ChatGPT $20 a month and then having a literal  of engineers try and review and merge the code?</p><p>Another friend commiserated the difficulty of trying to help an engineer contribute at work. ‚ÄúI review the code, ask for changes, and then they  hit me with another round of AI slop.‚Äù</p><p>Here‚Äôs the thing - we  to help. We  to build good things. Things that work well, that make people‚Äôs lives easier. We want to teach people how to do software engineering! Any engineer is standing entirely on the shoulders of their mentors and managers who‚Äôve invested time and energy into them and their careers. But what good is that investment if it‚Äôs simply copy-pasted into the latest ‚Äúmodel‚Äù that ‚Äúis literally half a step from artificial general intelligence‚Äù? Should we instead focus our time and energy into training the models and eliminate the juniors altogether?</p><p>What a sad, dark world that would be.</p><p>Here‚Äôs an experiment for you: stop using ‚ÄúAI‚Äù. Try it for a day. For a week. For a month.</p><p>Recently, I completely reset my computer. I like to do it from time to time. As part of that process I prune out any software that I no longer use. I‚Äôve been paying for Claude Pro for about 6 months. But slowly, I‚Äôve felt like it‚Äôs just a huge waste of time. Even if I have to do a few independent internet searches and read through a few dozen stack overflow and doc pages, my own conclusion is so much more reliable and accurate than anything an LLM could ever spit out.</p><p>So what good are these tools? Do they have any value whatsoever?</p><p>Objectively, it would seem the answer is no. But at least they make a lot of money, right?</p><p>Is anyone making money on AI right now? I see a pipeline that looks like this:</p><ul><li>‚ÄúAI‚Äù is applied to some specific, existing area, and a company spins up around it because it‚Äôs so much more ‚Äúefficient‚Äù</li><li>AI company gets funding from venture capitalists</li><li>AI company give funding to AI service providers such as OpenAI in the form of paying for usage credits</li></ul><p>This isn‚Äôt necessarily all that different than the existing VC pipeline, but the difference is that not even OpenAI is making money right now. I believe this is because the technology is inherently flawed and cannot scale to meet the demand. It simply consumes too much electricity to ever be economically viable, not to mention the serious environmental concerns.</p><p>We can say our prayers that Moore‚Äôs Law will come back from the dead and save us. We can say our prayers that the heat death of the universe will be sufficiently prolonged in order for every human to become a billionaire. We can also take an honestly not even hard look at reality and realize this is a scam.</p><p>The emperor is wearing no clothes.</p>","contentLength":4728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44981747"},{"title":"It‚Äôs not wrong that \"\\u{1F926}\\u{1F3FC}\\u200D\\u2642\\uFE0F\".length == 7 (2019)","url":"https://hsivonen.fi/string-length/","date":1755843536,"author":"program","guid":237017,"unread":true,"content":"<hgroup><h2>But It‚Äôs Better that  and Rather Useless that </h2></hgroup><p>From time to time, someone shows that in JavaScript the  of a string containing an emoji results in a number greater than 1 (typically 2) and then proceeds to the conclusion that haha JavaScript is so broken‚Äîand is rewarded with many likes. In this post, I will try to convince you that ridiculing JavaScript for this is less insightful than it first appears and that Swift‚Äôs approach to string length isn‚Äôt unambiguously the best one. Python 3‚Äôs approach is unambiguously the worst one, though.</p><h2>What‚Äôs Going on with the Title?</h2><p> evaluates to  as JavaScript. Let‚Äôs try JavaScript console in Firefox:</p><p>Haha, right? Well, you‚Äôve been told that the Python community suffered the Python 2 vs. Python 3 split, among other things, to Get Unicode Right. Let‚Äôs try Python 3:</p><pre>$ python3\nPython 3.6.8 (default, Jan 14 2019, 11:02:34) \n[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; len(\"ü§¶üèº‚Äç‚ôÇÔ∏è\") == 5\nTrue\n&gt;&gt;&gt; </pre><p>OK, then. Now, Rust has the benefit of learning from languages that came before it. Let‚Äôs try Rust:</p><pre>$ cargo new -q length\n$ cd length\n$ echo 'fn main() { println!(\"{}\", \"ü§¶üèº‚Äç‚ôÇÔ∏è\".len() == 17); }' &gt; src/main.rs\n$ cargo run -q\ntrue\n</pre><p>The string contains a single emoji consisting of five Unicode scalar values:</p><table><thead><tr></tr></thead><tbody><tr><td>U+1F3FC EMOJI MODIFIER FITZPATRICK TYPE-3</td></tr><tr></tr><tr><td>U+FE0F VARIATION SELECTOR-16</td></tr></tbody></table><p>The string that contains one graphical unit consists of 5 Unicode scalar values. First, there‚Äôs a base character that means a person face palming. By default, the person would have a cartoonish yellow color. The next character is an emoji skintone modifier the changes the color of the person‚Äôs skin (and, in practice, also the color of the person‚Äôs hair). By default, the gender of the person is undefined, and e.g. Apple defaults to what they consider a male appearance and e.g. Google defaults to what they consider a female appearance. The next two scalar values pick a male-typical appearance specifically regardless of font and vendor. Instead of being an emoji-specific modifier like the skin tone, the gender specification uses an emoji-predating gender symbol (MALE SIGN) explicitly ligated using the ZERO WIDTH JOINER with the (skin-toned) face-palming person. (Whether it is a good or a bad idea that the skin tone and gender specifications use different mechanisms is out of the scope of this post.) Finally, VARIATION SELECTOR-16 makes it explicit that we want a multicolor emoji rendering instead of a monochrome dingbat rendering.</p><p>Each of the languages above reports the string length as the number of  that the string occupies. Python 3 strings store Unicode code points each of which is stored as one code unit by CPython 3, so the string occupies 5 code units. JavaScript (and Java) strings have (potentially-invalid) UTF-16 semantics, so the string occupies 7 code units. Rust strings are (guaranteed-valid) UTF-8, so the string occupies 17 code units. We‚Äôll come to back to the actual  as opposed to  later.</p><p>Note about Python 3 added on 2019-09-09: Originally this article claimed that Python 3 guaranteed UTF-32 validity. This was in error. Python 3 guarantees that the units of the string stay within the Unicode code point range but does not guarantee the absence of surrogates. It not only allows unpaired surrogates, which might be explained by wishing to be compatible with the value space of potentially-invalid UTF-16, but Python 3 allows materializing even surrogate pairs, which is a truly bizarre design. The previous conclusions stand with the added conclusion that Python 3 is even more messed up than I thought! With the way the example string was constructed in Python 3, the Python 3 string happens to match the valid UTF-32 representation of the string, so it is still illustrative of UTF-32, but the rest of the article has been slightly edited to avoid claiming that Python 3 used UTF-32.</p><h2>But I Want the Length to Be 1!</h2><p>There‚Äôs a language for that. The following used Swift 4.2.3, which was the latest release when I was researching this, on Ubuntu 18.04:</p><pre>$ mkdir swiftlen\n$ cd swiftlen/\n$ swift package init -q --type executable\n$ swift package init --type executable\nCreating executable package: swiftlen\nCreating Package.swift\nCreating README.md\nCreating .gitignore\nCreating Sources/\nCreating Sources/swiftlen/main.swift\nCreating Tests/\nCreating Tests/LinuxMain.swift\nCreating Tests/swiftlenTests/\nCreating Tests/swiftlenTests/swiftlenTests.swift\nCreating Tests/swiftlenTests/XCTestManifests.swift\n$ echo 'print(\"ü§¶üèº‚Äç‚ôÇÔ∏è\".count == 1)' &gt; Sources/swiftlen/main.swift \n$ swift run swiftlen 2&gt;/dev/null\ntrue</pre><p>(Not using the Swift REPL for the example, because it does not appear to accept non-ASCII input on Ubuntu! Swift 5.0.3 prints the same and the REPL is still broken.)</p><p>OK, so we‚Äôve found a language that thinks the string contains one countable unit. But what is that countable unit? It‚Äôs an <i>extended grapheme cluster</i>. (‚ÄúExtended‚Äù to distinguish from the older attempt at defining grapheme clusters now called .) The definition is in <a href=\"http://www.unicode.org/reports/tr29/\">Unicode Standard Annex #29</a> (UAX #29).</p><p>We‚Äôve seen four different lengths so far:</p><ul><li>Number of UTF-8 code units (17 in this case)</li><li>Number of UTF-16 code units (7 in this case)</li><li>Number of UTF-32 code units or Unicode scalar values (5 in this case)</li><li>Number of extended grapheme clusters (1 in this case)</li></ul><p>Given a valid Unicode string and a version of Unicode, all of the above are well-defined and it holds that each item higher on the list is greater or equal than the items lower on the list.</p><p>One of these is not like the others, though: The first three numbers have an unchanging definition for any valid Unicode string whether it contains currently assigned scalar values or whether it is from the future and contains unassigned scalar values as far as software written today is aware. Also, computing the first three lengths does not involve lookups from the Unicode database. However, the last item depends on the Unicode version and involves lookups from the Unicode database. If a string contains scalar values that are unassigned as far as the copy of the Unicode database that the program is using is aware, the program will potentially overcount extended grapheme clusters in the string compared to a program whose copy of the Unicode database is newer and has assignments for those scalar values (and some of those assignments turn out to be combining characters).</p><h2>More Than One Length per Programming Language</h2><p>It is not the case that a given programming language has to choose only one of the above. If we run this Swift program:</p><pre>var s = \"ü§¶üèº‚Äç‚ôÇÔ∏è\"\nprint(s.count)\nprint(s.unicodeScalars.count)\nprint(s.utf16.count)\nprint(s.utf8.count)</pre><p>Let‚Äôs try Rust with <code>unicode-segmentation = \"1.3.0\"</code> in :</p><pre>use unicode_segmentation::UnicodeSegmentation;\n\nfn main() {\n\tlet s = \"ü§¶üèº‚Äç‚ôÇÔ∏è\";\n\tprintln!(\"{}\", s.graphemes(true).count());\n\tprintln!(\"{}\", s.chars().count());\n\tprintln!(\"{}\", s.encode_utf16().count());\n\tprintln!(\"{}\", s.len());\n}</pre><p>The above program prints:</p><p>That‚Äôs unexpected! It turns out that  does not implement the latest version of the Unicode segmentation rules, so it gives the ZERO WIDTH JOINER generic treatment (break right after ZWJ) instead of the newer refinement in the emoji context.</p><p>Let‚Äôs try again, but this time with  in :\n\n</p><pre>use unic_segment::Graphemes;\n\nfn main() {\n\tlet s = \"ü§¶üèº‚Äç‚ôÇÔ∏è\";\n\tprintln!(\"{}\", Graphemes::new(s).count());\n\tprintln!(\"{}\", s.chars().count());\n\tprintln!(\"{}\", s.encode_utf16().count());\n\tprintln!(\"{}\", s.len());\n}</pre><p>In the Rust case, strings (here mere string slices) know the number of UTF-8 code units they contain. The  method call just returns this number that has been stored since the creation of the string (in this case, compile time). In the other cases, what happens is the creation of an iterator and then instead of actually examining the values (string slices correspoding to extended grapheme clusters, Unicode scalar values or UTF-16 code units) that the iterator would yield, the  method just consumes the iterator and returns the number of items that were yielded by the iteration. The count isn‚Äôt stored anywhere on the string (slice) afterwards. If we wanted to later know the counts again, we‚Äôd have to iterate over the string again.</p><h2>Know in Advance or Compute When Needed?</h2><p>This introduces a notable question in the design space: Should a given type of length quantity be eagerly computed when the string is created? Or should the length be computed when someone asks for it? Or should it be computed when someone asks for it and then automatically stored on the string object so that it‚Äôs available immediately if someone asks for it again?</p><p>The answer Rust has is that the length in the code units of the Unicode Encoding Form of the language is stored upon string creation, and the rest are computed when someone asks for them (and then forgotten and not stored on the string).</p><p>Swift is a higher-level language and doesn‚Äôt document the exact nature of its string internals as part of the API contract. In fact, the internal representation of Swift strings changed substantially between Swift 4.2 and Swift 5.0. It‚Äôs not documented if different views to the string are held onto once created, for example. The documentation does say that strings are copy-on-write, so the first mutation may involve copying the string‚Äôs storage.</p><p>Notably, the design space includes not remembering anything. The C programming language is a prominent example of this case. C strings don‚Äôt even remember their number of code units. To find out the number of code units, you have to iterate over the string until a sentinel value. In the case of C, the sentinel is the code unit for U+0000, so it excludes one Unicode scalar value from the possible string contents. However, that‚Äôs not a strictly necessary property of a sentinel-based design that doesn‚Äôt remember any lengths. 0xFF does not occur as a code unit in any valid UTF-8 string and 0xFFFFFFFF does not occur in any valid UTF-32 string, so they could be used as sentinels for UTF-8 and UTF-32 storage, respectively, without excluding a scalar value from the Unicode value space. There is no 16-bit value that never occurs in a valid UTF-16 string. However, a valid UTF-16 string does not contain unpaired surrogates, so an unpaired low surrogate could, in principle, be used as a sentinel in a design that wanted to use guaranteed-valid UTF-16 strings that don‚Äôt remember their code unit length.</p><h2>Knowing the Storage-Native Code Unit Length is Extremely Reasonable</h2><p>The length of the string as counted in code units of its storage-native Unicode Encoding Form (i.e. whichever of UTF-8, UTF-16, and UTF 32 the programming language has chosen for its string semantics) is not like the other lengths. It is the length that the implementation cannot avoid having to know at the time of creating a new string, because it is the length that is required to be known in order to be able to allocate storage for a string. Even C, which promptly forgets about the code unit length in the storage-native Unicode Encoding Form after string has been created, has to know this length when allocating storage for a new string.</p><p>That is, the design decision is about whether to remember this length. It is not about whether to compute it eagerly. You just have to have it at string creation time‚Äîi.e. eagerly.</p><p>Considering that remembering this quantity makes string concatenation, which is a common operation, substantially faster to implement compared to not remembering this quantity, remembering this quantity is fundamentally reasonable. Also, it means that you don‚Äôt need to maintain a sentinel value, which means that a substring operation can yield results that share the buffer with the original string instead of having to copy in order to be able to insert sentinel. (Note that you can easily foil this benefit if you wish to eagerly maintain zero-termination for the sake of C string compatibility.)</p><h2>What About Knowing the Other Lengths?</h2><p>Even if we‚Äôve established that it makes sense for string implementation to remember the storage length of the string in code units all the storage-native Unicode encoding form, it doesn‚Äôt answer whether a string implementation should also remember other lengths or which kind of length should be offered in the most ergonomic API. (As we see above, Swift makes the number of extended grapheme clusters more ergonomic to obtain that the code unit or scalar value length.)</p><p>Also, if any other length is to be remembered, there is the question of whether it should be eagerly computed as string creation time or lazily computed the first time someone asks for it. It is easy to see why at least the latter does not make sense for multi-threaded systems-programming language like Rust. If some properties of an object are lazily initialized, in a multi-threaded case you also need to solve synchronization of these computations. Furthermore, you need to allocate space at least for a pointer to auxiliary information if you want to be able to add auxiliary information later or you need to have a hashtable of auxiliary information where the string the information is about is the key, so auxiliary information, even when not present, has storage implications or implications of having to have global state in a run-time system. Finally, for systems programming, it may be more desirable to know the time complexity of a given operation clearly even if it means ‚Äúalways O(n)‚Äù instead of ‚Äúpossibly O(n) but sometimes O(1)‚Äù. Even if the latter looks strictly better, it is less .</p><p>For a higher-level language, arguments from space requirements or synchronization issues might not be decisive. It‚Äôs more relevant to consider what a given length quantity is . This is often forgotten in Internet debates that revolve around what length is the most ‚Äúcorrect‚Äù or ‚Äúlogical‚Äù one. So for the lengths that don‚Äôt map to the size of storage allocation, what are they good for?</p><p>It turns out that in the Firefox code base there are two places where someone wants to know the number of Unicode scalar values in a string that is not being stored as UTF-32 and attention is not paid to what the scalar values actually are. The IETF specification for Session Traversal Utilities for NAT (STUN) used for WebRTC has the curious property that it places length limits on certain protocol strings such that the limits are expressed as number of Unicode scalar values but the strings are transmitted in UTF-8. Firefox validates these limits. (The limit looks like an arbitrary power-of-two (128 scalar values). The spec has remarks about the possible resulting byte length, which was wrong according to the IETF UTF-8 RFC that was current and already nearly five years old at the time of publication of the STUN RFC. Specifically, the STUN RFC repeatedly says that 128 characters as UTF-8 may be as long as 763 bytes. To arrive at that number, you have to assume that a UTF-8 character can be up to six bytes long, as opposed to up to 4 bytes long as in the prevailing UTF-8 RFC and in the Unicode Standard, and that the last character of the 128 is a zero terminator and, therefore, known to take just one byte.) In this case, the reason for wishing to know a non-storage length is to . The other case is reporting the column number for the source location of JavaScript errors.</p><p>Length limits, which we‚Äôll come back to, probably aren‚Äôt a frequent enough a use case to justify making strings know a particular kind of length as opposed to such length being possible to compute when asked for. Neither are error messages.</p><p>Another use case for asking for a length is iterating by index and using the length as the loop termination condition 1990s Java style. Like this:</p><pre>for (int i = 0; i &lt; s.length(); i++) {\n    // Do something with s.charAt(i)\n}</pre><p>In this case, it‚Äôs actually important for the length to be precomputed number on the string object. This use case is coupled with the requirement that indexing into the string to find the th unit corresponding to the count of units that the ‚Äúlength‚Äù represents should be a fast operation.</p><p>The above pattern is a lot less conclusive in terms of what lengths should be precomputed (and what the indexing unit should be) than it first appears. The above loop doesn‚Äôt do random access by index. It sequentially uses every index from zero up to, but not including, . Indeed, especially when iterating over a string by Unicode scalar value, typically when you examine the contents of a string, you iterate over the string in order. Programming languages these days provide an  facility for this, and e.g. to iterate over a UTF-8 string by scalar value, the iterator does not need to know the number of scalar values up front. E.g. in Rust, you can do this in O(n) time despite string slices not knowing their number of Unicode scalar values:</p><pre>for (c in s.chars()) {\n    // Do something with c\n}</pre><p>(Note that  is an 8-bit code unit (possibly UTF-8 code unit) in C and C++,  is a UTF-16 code unit in Java,  is a Unicode scalar value in Rust, and  is an extended grapheme cluster in Swift.)</p><p>A programming language together with its library ecosystem should provide iteration over a string by Unicode scalar value and by extended grapheme cluster, but it does not follow that strings would need to know the scalar value length or the extended grapheme cluster length up front. Unlike the code unit storage length, those quantities aren‚Äôt useful for accelerating operations like concatenation that don‚Äôt care about the exact content of the string.\n\n</p><h2>Which Unicode Encoding Form Should a Programming Language Choose?</h2><p>The observation that having strings know their code unit length in their storage-native Unicode encoding form is extremely reasonable does not answer how many bits wide the code units should be.</p><p>The usual way to approach this question is to argue that UTF-32 is the best, because it provides O(1) indexing by ‚Äúcharacter‚Äù in the sense of a character meaning a Unicode scalar value, or the argument focuses on whether UTF-8 is unfair to some languages relative to UTF-16. I think these are bad ways to approach this question.</p><p>First of all, the argument that the answer should be UTF-32 is bad on two counts. First, it assumes that random access scalar value is important, but in practice it isn‚Äôt. It‚Äôs reasonable to want to have a capability to iterate over a string by scalar value, but random access by scalar value is in the YAGNI department. Second, arguments in favor of UTF-32 typically come at a point where the person making the argument has learned about surrogate pairs in UTF-16 but has not yet learned about extended grapheme clusters being even larger things that the user perceives as unit. That is, if you escape the variable-width nature of UTF-16 to UTF-32, you pay by doubling the memory requirements and extended grapheme clusters are  variable-width.</p><p>I‚Äôll come back to the length fairness issue later, but I think a different argument is much more relevant  for the choice of in-memory Unicode encoding form. The more relevant argument is this: Implementations that choose UTF-8 actually accept the UTF-8 storage requirements. When wider-unit semantics are chosen for a language that doesn‚Äôt provide raw memory access and, therefore, has the opportunity to tweak string storage, the implementations try to come up with ways to avoid actually paying the cost of the wider units in some situations.</p><p>JavaScript and Java strings have the semantics of potentially-invalid UTF-16. SpiderMonkey and V8 implement an optimization for omitting the leading zeros of each code unit in a string, i.e. storing the string as ISO-8859-1 (the actual ISO-8859-1, not the Web notion of ‚ÄúISO-8859-1‚Äù as a label of windows-1252), when all code units in the string have zeros in the most-significant half. The HotSpot JVM also implements this optimization, though enabling it is optional. Swift 4.2 implements a slightly different variant of the same idea, where ASCII-only strings are stored as 8-bit units and everything else is stored as UTF-16. CPython since 3.3 makes the same idea three-level with code point semantics: Strings are stored with 32-bit code units if at least one code point has a non-zero bit above the low 16 bits. Else if a string has a non-zero bits above the low 8 bits for at least one code point, the string is stored as 16-bit units. Otherwise, the string is stored as 8-bit units (Latin1).</p><p>I think the unwillingness of implementations of languages that have chosen UTF-16 or UTF-32 (or UTF-32-ish as in the case of Python 3) string  to actually use UTF-16 or UTF-32  when they can get away with not using actual UTF-16 or UTF-32 storage is the clearest indictment against UTF-16 or UTF-32 (and other wide-unit semantics like what Python 3 uses).</p><p>Languages that choose UTF-8, on the other hand, stick to actual UTF-8 for the purpose of storing Unicode scalar values. When languages that choose UTF-8 deviate from UTF-8, they do so in order to represent values that are not Unicode scalar values for compatibility with external constraints. Rust uses a representation called <a href=\"https://simonsapin.github.io/wtf-8/\">WTF-8</a> for file system paths on Windows. All UTF-8 strings are WTF-8 strings, but WTF-8 can also represent unpaired surrogates for compatibility with Windows file paths being sequences of 16-bit units that can contain unpaired surrogates. Perl 6 uses an internal representation called <a href=\"https://docs.perl6.org/language/unicode#UTF8-C8\">UTF-8 Clean-8</a> (or UTF8-C8), which represents strings that consist of Unicode scalar values in Unicode Normalization Form C the same way as UTF-8 but represents non-NFC content differently and can represent sequences of bytes that are not valid UTF-8.</p><p>UTF-8 is the only one of the Unicode  that is also a Unicode , and of the Unicode encoding schemes, UTF-8 has clearly won for interchange. (Unicode encoding forms are what you have in RAM, so UTF-16 consists of native-endian, two-byte-aligned 16-bit code units. Unicode encoding schemes are what can be used for byte-oriented interchange, so e.g. UTF-16LE consist of 8-bit code units every pair of which form a potentially-unaligned little-endian 16-bit number, which in turn may form a surrogate pair.) When UTF-8 is used as the in-RAM representation, input and output operations are less expensive than with UTF-16 or UTF-32. UTF-16 or UTF-32 in RAM requires conversion from UTF-8 when reading input and conversion to UTF-8 when writing output. A system that guarantees UTF-8 validity internally, such as Rust, needs only to  UTF-8 upon reading input and no conversion is needed when writing output. (Go takes a garbage in, garbage out approach to UTF-8: input is not validated at input time and output is written without conversion. However, iteration by scalar value can yield REPLACEMENT CHARACTERs when iterating over invalid UTF-8. That is, the input step is less expensive than in Rust, but iterating by scalar value is marginally more expensive. The output step is less correct.)\n\n</p><p>Finally, in terms of nudging developers to write correct code, UTF-8 has the benefit of being blatantly variable-width, so even with languages such as English, Somali, and Swahili, as soon as you have a dash or a smart quote, the variable-width nature of UTF-8 shows up. In this context, extended grapheme clusters are just extending the variable-width nature. Meanwhile, UTF-16 allows programmers to get too far while pretending to be working with something where the units they need to care about are fixed-width. Reacting to surrogate pairs by wishing to use UTF-32 instead is a bad idea, because if you want to write correct software, you still need to deal with variable-width extended grapheme clusters.\n\n</p><p>The choice of UTF-32 (or Python 3-style code point sequences) arises from wanting the wrong thing. The choice of UTF-16 is a matter of early-adopter legacy from the time when Unicode was expected to be capped to 16 bits of code space and, once UTF-16 has been committed to, not breaking compatibility with already-written programs is important and justified the continued use of UTF-16, but if you aren‚Äôt bound by that legacy and are designing a new language, you should go with UTF-8. Occasionally even systems that appear to be bound by the UTF-16 legacy can break free. Even though Swift is committed to interoperability with Cocoa, which uses UTF-16 strings, <a href=\"https://swift.org/blog/utf8-string/\">Swift 5 switched to UTF-8</a> for Swift-native strings. Similarly, <a href=\"https://morepypy.blogspot.com/2019/03/pypy-v71-released-now-uses-utf-8.html?m=1\">PyPy has gone UTF-8</a> despite Python 3 having code point semantics.</p><h2>Shouldn‚Äôt the Nudge Go All the Way to Extended Grapheme Clusters?</h2><p>Even if we accept that the storage should be UTF-8 and that the string implementation should maintain knowledge of the string length in UTF-8 code units, if the blatant variable-widthness of UTF-8 is argued to be a nudge toward dealing with the variable-widthness of extended grapheme clusters, shouldn‚Äôt the Swift approach of making extended grapheme cluster access and count the view that takes the least ceremony to use be the thing that every language should do?</p><p>Swift is still too young to draw definitive conclusions from. It‚Äôs easy to believe that the Swift approach nudges programmers to write more extended grapheme cluster-correct code and that the design makes sense for a language meant primarily for UI programming on a largely evergreen platform (iOS). It isn‚Äôt clear, though, that the Swift approach is the best for everyone.</p><p>Earlier, I said that the example used ‚ÄúSwift 4.2.3 on Ubuntu 18.04‚Äù. The ‚Äú18.04‚Äù part is important! Swift.org ships binaries for Ubuntu 14.04, 16.04, and 18.04. Running the program</p><pre>var s = \"ü§¶üèº‚Äç‚ôÇÔ∏è\"\nprint(s.count)\nprint(s.unicodeScalars.count)\nprint(s.utf16.count)\nprint(s.utf8.count)</pre><p>in Swift 4.2.3 on Ubuntu 14.04 prints:</p><p>So Swift 4.2.3  as well as the  0.9.0 Rust crate counted one extended grapheme cluster, the  1.3.0 Rust crate counted two extended grapheme clusters, and <i>the same version of Swift</i>, 4.2.3, but on a <i>different operating system version</i> counted three extended grapheme clusters!</p><p>Swift 4 delegates Unicode segmentation to operating system-provided ICU, and ‚ÄúLong-Term Support‚Äù in the Ubuntu case means security patches but does not mean rolling forward the Unicode version that the system copy of ICU knows about. In the case of iOS, delegating to system ICU is probably OK and will not lead to too high probability of the text being from the future from the point of view of the OS copy of ICU, since the iOS ecosystem stays exceptionally well <a href=\"https://developer.apple.com/support/app-store/\">up-to-date</a>. However, delegating to system ICU is not such a great match for the idea of using Swift on the server side if the server side means running an old LTS distro.</p><p>(Swift 5 appears to no longer use system ICU for this. That is, Swift 5.0.3 on Ubuntu 14.04 sees one extended grapheme cluster in the string. I haven‚Äôt investigated what Swift 5 uses, but I assume that the switch to UTF-8 string representation necessitated using something other than ICU, which is heavily UTF-16-oriented. However, the result with Swift 4.2.3 nicely illustrates the issue related to using extended grapheme clusters.)</p><p>If you are doing things that  be extended grapheme cluster-aware, there just is no way around the issue of not being able to correctly segment text that comes from the future relative to the Unicode segmentation implementation that your program is using. This is not a reason to avoid extended grapheme clusters for tasks that  awareness of extended grapheme clusters.</p><p>However, pushing extended grapheme clusters onto tasks that do not really require the use of extended grapheme cluster introduces failure modes arising from the Unicode version dependency where such a dependency isn‚Äôt strictly necessary. For example, the Unicode version dependency of extended grapheme clusters means that you should  persist indices into a Swift strings and load them back in a future execution of your app, because an intervening Unicode data update may change the meaning of the persisted indices! The Swift string documentation does not warn against this.</p><p>Let‚Äôs consider other languages a bit.</p><p>C++ is often deployed such that the application developer doesn‚Äôt ship the standard library with the program. Most obviously, relying on GNU libstdc++ provided by an LTS Linux distribution presents similar problems as Swift 4 relying on ICU provided by an LTS Linux distribution. This isn‚Äôt a Linux-specific issue. Old supported branches of Windows <a href=\"https://github.com/sg16-unicode/sg16-meetings#may-22nd-2019\">generally don‚Äôt get new system-level Unicode data</a>, either. Even though there is some movement towards individual applications shipping their own copy of LLVM libc++ with the application and the increased pace of C++ standard development starting with C++11 has made using a system-provided C++ standard library more problematic even ignoring Unicode considerations, it doesn‚Äôt seem like a good idea for C++ to develop a tight coupling with extended grapheme clusters for operations that don‚Äôt strictly necessitate it as longs as stuck-in-the-past system libraries (whether the C++ standard library itself or another library that it delegates to) are a significant part of the C++ standard library distribution practice.</p><p>There‚Äôs <a href=\"https://github.com/tc39/proposal-intl-segmenter\">a proposal</a> to expose extended grapheme cluster segmentation to JavaScript programs. The main problem with this proposal is the implication on APK sizes on Android and the effect of APK sizes on browser competition on Android. But if we ignore that for the moment and imagine this was part of the Web Platform, it would still be problematic to build this dependency into operations for which working on extended grapheme clusters isn‚Äôt strictly necessary. While the most popular browsers are evergreen, there‚Äôs still a long tail of browser instances that aren‚Äôt on the latest engine versions. When JavaScript executes on such browsers, there‚Äôd be effects similar to running Swift 4 on Ubuntu 14.04.</p><p>In contrast to C++ or JavaScript, the current Rust approach is to statically link all Rust library code, including the standard library, into the executable program. This means that the application distributor is in control of library versions and doesn‚Äôt need to worry about the program executing in the context of out-of-date  libraries. The flip side is concerns about the size of the executable. People already (rightly or wrongly) complain about the sizes of Rust executables. Pulling in a lot of Unicode data due to baking extended grapheme cluster processing into programs whose problem domain doesn‚Äôt strictly require working with extended grapheme clusters would be problematic in embedded contexts where the executable size is a real problem and not just a perceived problem‚Äîand would obviously make the perceived problem worse, too. Furthermore, in order to avoid problems similar to those involved in relying on system libraries, baking tight coupling with Unicode data into the standard library necessitates the organizational capability of keeping up with new Unicode versions in this area where not only data in the tables keeps changing but the format of the tables and, therefore, the associated algorithms have still been changing recently. Right now of the two extended grapheme cluster crates outside the Rust standard library, the one that‚Äôs organizationally closer to the standard library is the one that‚Äôs out of date.</p><h2>Why Do We Want to Know Anyway?</h2><blockquote><p>‚ÄúString length is about as meaningful a measurement as string height‚Äù ‚Äì <a href=\"https://mobile.twitter.com/qntm/status/1118993107310325760\">@qntm</a></p></blockquote><p>Being able to allocate memory for strings gives a legitimate use case for knowing the storage length. However, in cases of Unicode scalar values or extended grapheme clusters, you typically want to iterate over them and look at each one instead of just knowing the count. So why do people want to know the count? As far as I can tell, there are two broad categories: Placing a quota limit that is fuzzy enough that it doesn‚Äôt need to be strictly tied to storage and trying to estimate how much text fits for display. Let‚Äôs look at the issue of estimating how much display space text takes, because it involves introducing yet another measurement of string length.</p><p>Simply looking at the Latin letters i and m should make it clear that the display size of a string depends on the font and on the specific characters in the string. From this observation, the whole notion of estimating display space by counting characters seems folly. Indeed, if you want to know exactly how much text fits into a given space, you need to run a typesetting algorithm with a specific font, which may have a complex relationship between scalar values and glyphs, to actually see where the overflow starts. Yet, even in the case of the Latin script that has letters such as i and m, e.g. magazine editors can find character counts useful enough for estimating how many print pages an article of a given character count length is going to fill.</p><p>As for computer user interfaces, character terminal user interfaces use a monospaced font where both i and m take up one character cell on a grid. In the context of a monospaced font, the extended grapheme cluster count in the context of the Latin script corresponds directly to display space taken. The same obviously applies to the Greek and Cyrillic scripts, which are so close to the Latin script that fonts even intend to reuse glyphs across these scripts. In contrast, CJK ideographs, Japanese kana, and Hangul syllables take two cells of a terminal grid. From the CJK perspective, these are full-width characters and the ASCII characters are half-width characters. There exist also half-width katakana characters which fit into an 8-bit encoding with ASCII and take one cell on the terminal grid and, therefore, are technically easier to fit to Latin script-oriented terminal systems. The display width on a terminal also has a correspondence to byte with the legacy CJK encodings: ASCII takes one byte, a CJK ideograph, a full-width kana or a Hangul syllable takes two bytes. In the case of Shift_JIS, half-width katakana takes one byte per character.</p><p>This brings us to the concept of <a href=\"https://www.unicode.org/reports/tr11/\">East Asian Width</a>. ASCII and half-width katakana characters are narrow. CJK ideographs, full-width kana, and Hangul syllables are wide. However, even in the worldview that is split to Latin, Greek, and Cyrillic on one hand and Chinese, Japanese, and Korean on the other hand, there are ambiguities. From the perspective of European legacy encodings, Greek and Cyrillic (as well as accented Latin) is equally wide as ASCII. However, in legacy CJK encodings, Greek and Cyrillic characters take two bytes. This means that in terms of East Asian Width, a string can have a general-purpose width, which resolves these ambiguous characters as narrow, or legacy CJK-context width, which resolves these ambiguous characters as wide.</p><p>So is the general-purpose variant (that resolves Greek and Cyrillic characters as narrow) of East Asian Width the one true string length measure? Well, no.</p><p>First of all, the concept ignores all scripts that are geographically and in Unicode order between Latin, Greek, and Cyrillic on one hand and CJK on the other (even though some other scripts that are structurally similar to the Latin, Greek, and Cyrillic scripts and make sense for a monospaced font, such as Armenian and the Georgian scripts, fit this concept, too, despite not having a history in pre-Unicode CJK context). As it happens, though, emoji do fit into the concept, except for <a href=\"https://mobile.twitter.com/fantasai/status/1080928442126909440\">weird errors in the Unicode database</a>. After all, emoji originate from Japan and were two bytes each when represented using the private use area of Shift_JIS.</p><p>Second, the concept assumes that there is one-to-one correspondence between scalar values and extended grapheme clusters. If we run this Rust program:</p><pre>use unicode_width::UnicodeWidthStr;\n\nfn main() {\n    println!(\"{}\", \"ü§¶üèº‚Äç‚ôÇÔ∏è\".width());\n}</pre><p>This is because the base emoji is wide (2), the combining skin tone modifier is also wide (2), the male sign is counted as narrow (1), and the zero-width joiner and the variation selector are treated as control characters that don‚Äôt count towards width. Obviously, this is not the answer that we want. The answer we want is 2. Ideas that come to mind immediately, such as only counting the width of the first character in an extended grapheme cluster or taking the width of the widest character in an extended grapheme cluster, don‚Äôt work, because flag emoji consist of two regional indicator symbol letter characters both of which have East Asian Width of Neutral (i.e. they are counted as narrow but are not marked as narrow, because they are considered to exist outside the domain of East Asian typography). I‚Äôm not aware of any official Unicode definition that would reliably return 2 as the width of every kind of emoji. üò≠</p><p>If you really must estimate display size without running text layout with a font, whether the extended grapheme cluster count or the East Asian Width of the string works better depends on context.</p><h2>Arbitrary but Fair Quotas</h2><p>In some cases there is a desire to impose a length limit that doesn‚Äôt arise from a strict storage limitation. For example, in the STUN protocol given earlier, presumably there is a desire to make it so that human-readable error messages cannot make protocol messages arbitrarily long. For example, in the case of Twitter, tweets being short is a core part of the type of expression that Twitter is about, so some definition of ‚Äúshort‚Äù is needed. In the case of string-based browser , there is a need to have  limit, but the limit is necessarily arbitrary and does not need to strictly map to bytes on disk.</p><p>In cases like this, there seems to be some concern that the limit should be internationally fair. Observations that UTF-8 and UTF-16 take a different amount of storage per character depending on the character superficially suggests that the UTF-8 length or the UTF-16 length might be unfair internationally.</p><p>What‚Äôs fair, though? The usual concern goes that UTF-8 favors English, because English takes one byte per character, and disfavors CJK, because Chinese, Japanese, and Korean take three bytes per character, so UTF-8 in unfair to CJK. This kind of analysis ignores how much information is conveyed per character. To assess what lengths we get for different languages when the amount of information conveyed is kept constant, I looked at the counts for the translations of the Universal Declaration of Human Rights. This is a document for which <a href=\"https://www.unicode.org/udhr/translations.html\">translation of the same content is available in particularly many languages</a>, which is why I used it as the measurement corpus.</p><p>Unfortunately, not all translations contain the same text, so one needs to be careful when preparing the data for comparison. Some translations are incomplete, in some cases,  incomplete. For this reason, I included only translations in stage 4 or stage 5 along the 5-stage scale. Some translations carry the preamble with the recitals, but some do not. Some also carry historical notes. To make the length comparable, the preamble, notes, and whitespace-only text nodes were omitted. The rest of the XML text nodes were concatenated and normalized to Unicode Normalition Form C before counting. (<a href=\"https://github.com/hsivonen/udhrlen\">Source code is available</a>.)</p><p>Let‚Äôs look at the result. The table at the end of this document is sortable and is initially sorted by UTF-8 length. Each Œî% column shows how much the count in the column to its left deviates from the  count for that. (A note about color-coding. Coloring longer than median as red should not be taken to imply that those languages are somehow bad. It‚Äôs meant to imply that a length quota treats those languages badly.) In the table, the name of each language links to the translation in that language hosted on the site of the Unicode Consortium. The linked HTML versions may include the preamble and/or notes.</p><p>The CJK concern is alleviated when considering information conveyed. When measuring UTF-8 length, Mandarin using traditional characters is the shortest of the languages that have global name recognition! This should be expected, since the Han script pretty obviously packs more information per character than e.g. alphabetic scripts. (The globally less-known languages whose UTF-8 length is shorter than Mandarin‚Äôs (using traditional characters) are African and American Latin-script languages with a relatively small native speaker population for each‚Äîonly one with a native speaker population exceeding a million and many whose native speaker population is smaller than 100 000, which explains why you might not recognize their names.)</p><p>Korean is also shorter than median in UTF-8 length. This also makes sense, since Hangul syllables pack three or two alphabetic jamo into one three-byte character. The UTF-8 length of Japanese is over median but only by 4.1%. The Japanese version of the text is 48% kanji and 52% hiragana. Japanese Wikipedia has almost the same kana to kanji ratio, though different kana: 46% kanji and the rest almost evenly split between hiragana and katakana, so we may assume the Universal Declaration of Human Rights to be representative of Japanese text in terms of kana to kanji ratio.</p><p>When sorting by UTF-16 code unit count, UTF-32 / scalar value count, or extended grapheme cluster count, CJK are the shortest. While it‚Äôs true that UTF-8 takes more bytes for CJK than UTF-16, the notion of UTF-8 being particularly disfavorable to CJK is not true <i>relative to other languages</i>. Rather, UTF-16 is particularly favorable to CJK. In particular, the Han script is so information-dense that even when sorting by East Asian Width, which effectively doubles the length of CJK but not other languages, Han-script languages stay clustered at the start of the table. Korean and Japanese move further but remain below median.</p><p>The language with the longest UTF-8 length is <a href=\"https://en.wikipedia.org/wiki/Shan_language\">Shan</a>, which uses the Burmese script. The Burmese language, also using the Burmese script, is the second-longest in UTF-8 length. There are a number of other Brahmic-script languages among the ones with the longest UTF-8 length. They use three bytes per character but don‚Äôt have CJK-like information per character density. These languages are below median in extended grapheme cluster count. In scalar value count, they intermingle with alphabetic languages.</p><p>It‚Äôs not clear if the concepts of median and mean (average) are meaningful. Does it make sense for a language with tens of millions of native speakers to count as an equal data point as a language with tens of thousands native speakers? Since this is about writing, should the numbers of writers be considered instead? (I.e. should literacy rates be taken into account?) In the hope that with a large number of languages in the table, median hand-wavily sorts out this kind of issue, I chose to compare with median. At least the Han-script languages have comparable numbers of native speakers as the Bhramic-script languages and provide a counter-weight at the other end of the spectrum of UTF-8 length. In any case, for measures other than UTF-8 length, median and mean are very close to each other.</p><p>Saying that Brahmic-script languages intermingle with alphabetic languages in character count is rather meaningless, though. In character count, after CJK (and Han-script Vietnamese and Yi-script Nousu), the language with the smallest character count is a Latin-script language (Waama). Also, the language with the largest character count is a Latin-script language (Ash√©ninka, Pichis). (<s>I find it odd that in UTF-8 length Ash√©ninka Peren√© is the second-shortest but Ash√©ninka, Pichis is long enough to reach the Brahmic cluster. I don‚Äôt know what the relation of these two languages is and what explains two languages whose name suggests close relation ending up in opposite extremes in length.</s> Update: It has been pointed out to me that the supposed Ash√©ninka Peren√© translation is a mislabeled duplicate of the Cashinahua translation.)</p><p>One might hypothesize that the Latin script has just been put to so many uses that some of the uses have to be far from what it has been optimized for. Yet, when considering language-specific alphabets, the character counts for Greek and Georgian are above median. It just is the case that languages are different. In that sense, the whole notion of trying to find a simple length measure that is fair across languages seems folly.</p><p>Let‚Äôs look at the the factor between the minimum and maximum of each measure, i.e. the factor with which the minimum needs to be multiplied to get the maximum. Let‚Äôs even ignore the outlier for maximum for each measure and use the second largest value instead of the largest value for each count. (Otherwise, Ash√©ninka, Pichis alone would skew the numbers a lot.) We get these factors:</p><table><tbody></tbody></table><p>UTF-16, UTF-32, and extended grapheme clusters aren‚Äôt distinguished by this measure, because the languages at the extremes use characters from the Basic Multilingual Plane with one character per grapheme cluster. Considering that there are supplementary-plane scripts, arguably the UTF-32 count would be fairer than the UTF-16 count even though this factor doesn‚Äôt show the difference. It‚Äôs not clear that counting extended grapheme clusters would be particularly fair compared to counting characters: It favors scripts that are visually joining over scripts that aren‚Äôt visually joining even if there‚Äôs no logical difference. While looking at just the factor, East Asian Width makes the gap the smallest, but it‚Äôs a rather imprecise fairness solution. It just counts CJK as double. Even after this, the Han-script languages are still among the ones with the smallest counts. On the other hand, it seems unfair to recognize Hangul syllables and kana as carrying more information than an alphabetic character while not giving the same treatment to other syllabaries, such as the Ethiopic script, Ge‚Äôez.</p><p>Twitter counts each CJK character (including three-jamo Hangul syllables; i.e. it is not decomposing Hangul and treating it as alphabetic) as consuming 2 units of the quota (as when counting East Asian Width), counts emoji as consuming two units (even when East Asian Width of the cluster would be more), and, unlike East Asian Width, counts each Ethiopic syllable as consuming two units of the quota. What Twitter does seems fairer than just applying East Asian Width, but the result is still that the amount of information that can be packed in a tweet can vary four-fold depending on language. That still doesn‚Äôt seem exactly fair across languages.</p><ul><li>There is no simple measure of string length that would be fair in terms of how much information can be conveyed within a length quota regardless of language.</li><li>Of solutions that don‚Äôt depend on the Unicode database and, therefore, the Unicode version and that don‚Äôt ad-hoc hard-code character ranges according to a particular version of Unicode, counting characters aka. scalar values i.e. UTF-32 length is the best that can be done. It‚Äôs still wildly unfair leading to almost eight-fold differences in how much information can be conveyed. This is not a flaw of Unicode but arises from differences in languages and writing systems.</li><li>While counting scalar values is fairer than just counting UTF-8 or UTF-16 code units, the factor between minimum and maximum UTF-8 length is so close to the factor between minimum and maximum UTF-32 length, both of which are pretty large, that instead of putting thought into using the scalar value length instead of the UTF-8 length or the UTF-16 length, it‚Äôs probably better to put the thought into reconsidering if you  need to impose such a limit.</li><li>Unicode doesn‚Äôt provide a good database-based definition that would improve upon the character count in terms of normalizing the amount of information conveyed. While East Asian Width brings minimum and maximum closer, it unfairly singles out Hangul syllables and kana without considering other syllabaries, because normalizing length for information conveyed is not the purpose of East Asian Width.</li><li>Even if per-script (possibly non-integer) weights assigned to characters could make things fairer, it wouldn‚Äôt work well for the Latin script, which is all over the place in terms of language-dependent length.</li></ul><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody><tfoot><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tfoot></table>","contentLength":48289,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44981525"},{"title":"Top Secret: Automatically filter sensitive information","url":"https://thoughtbot.com/blog/top-secret","date":1755838109,"author":"thunderbong","guid":237197,"unread":true,"content":"<div><p>What happens when you‚Äôre dealing with free text? Filtering the entire string may\nnot be an option if an external API needs to process the value. Think chatbots or LLMs.</p><p>You could use a regex to filter sensitive information (such as credit card\nnumbers or emails), but that won‚Äôt capture everything, since not all sensitive\ninformation can be captured with a regex.</p><p>Fortunately, <a href=\"https://en.wikipedia.org/wiki/Named-entity_recognition\">named-entity recognition</a> (NER) can be used to identify and\nclassify real-world objects, such as a person, or location. Tools like <a href=\"https://github.com/ankane/mitie-ruby\">MITIE\nRuby</a> make interfacing with NER models trivial.</p><p>By using a combination of regex patterns and NER entities, <a href=\"https://github.com/thoughtbot/top_secret\">Top Secret</a>\neffectively filters sensitive information from free text‚Äîhere are some\nreal-world examples.</p><p>If you want to see <a href=\"https://github.com/thoughtbot/top_secret\">Top Secret</a> in action, you might enjoy this <a href=\"https://www.youtube.com/live/m2UIpTaIZ8o?si=EzEkWHlNQJORVgSG&amp;t=120\">live\nstream</a>. Otherwise, see the examples below.</p><p>It‚Äôs not uncommon to send user data to chatbots. Since the data might be\nfree-form, we should be diligent about filtering it using the approach mentioned\nabove.</p><p>However, it‚Äôs likely we‚Äôll want to ‚Äúrestore‚Äù the filtered values when returning\na response from the chatbot. <a href=\"https://github.com/thoughtbot/top_secret\">Top Secret</a> returns a <a href=\"https://github.com/thoughtbot/top_secret?tab=readme-ov-file#usage\">mapping</a> that would\nallow for this.</p><div><pre><code></code></pre></div><p>The exchange might look something like this.</p><ol><li><p>Caller sends filtered text</p><div><pre><code></code></pre></div></li><li><div><pre><code>\"Hi [PERSON_1]! How is the weather in [LOCATION_1] today?\"\n</code></pre></div></li><li><p>Caller can ‚Äúrestore‚Äù from the mapping</p><div><pre><code></code></pre></div></li></ol><h3><a href=\"https://thoughtbot.com/blog/top-secret#filtering-conversation-history\">\n    Filtering conversation history\n  </a></h3><p>When working with <a href=\"https://platform.openai.com/docs/guides/conversation-state\">conversation state</a> you should filter  message\nbefore including it in the request. This ensures no sensitive data slips through\nfrom previous messages. Here‚Äôs what that might look like.</p><div><pre><code></code></pre></div><p>Top Secret can also be used as a validation tool to prevent storing sensitive\ninformation in your database.</p><div><pre><code></code></pre></div><p>If the validation is too strict, you can <a href=\"https://github.com/thoughtbot/top_secret#overriding-the-default-filters\">override</a> or <a href=\"https://github.com/thoughtbot/top_secret#disabling-a-default-filter\">disable</a> any of\nthe filters as needed.</p><div><pre><code> class Message &lt; ApplicationRecord\n   private\n   def content_cannot_contain_sensitive_information\n     return if result.mapping.empty?\n     errors.add(:content, \"contains the following sensitive information #{result.mapping.values.to_sentence}\")\n</code></pre></div><p>It‚Äôs our responsibility to protect user data. This is more important than ever\ngiven the rise in popularity of chatbots and LLMs. Tools like <a href=\"https://github.com/thoughtbot/top_secret\">Top Secret</a> aim to\nreduce this burden.</p></div><div><p>We've been helping engineering teams deliver exceptional products for over 20 years. Our designers, developers, and product managers work closely with teams to solve your toughest software challenges through collaborative design and development. <a href=\"https://thoughtbot.com/services\" target=\"_self\" rel=\"noopener\">Learn more about us</a>.</p></div>","contentLength":2469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44981088"},{"title":"Io_uring, kTLS and Rust for zero syscall HTTPS server","url":"https://blog.habets.se/2025/04/io-uring-ktls-and-rust-for-zero-syscall-https-server.html","date":1755834704,"author":"guntars","guid":236766,"unread":true,"content":"<p>Around the turn of the century we started to get a bigger need for high\ncapacity web servers. For example there was <a href=\"https://en.wikipedia.org/wiki/C10k_problem\">the C10k problem</a> paper.</p><p>At the time, the kinds of things done to reduce work done per request was\npre-forking the web server. This means a request could be handled without an\nexpensive process creation.</p><p>Because yes, creating a new process for every request used to be something\nperfectly normal.</p><p>Things did get better. People learned how to create threads, making things more\nlight weight. Then they switched to using /, in order to not\njust spare the process/thread creation, but the whole context switch.</p><p>I remember a comment on <a href=\"https://en.wikipedia.org/wiki/Kuro5hin\">Kuro5hin</a> from anakata, the creator of both The\nPirate Bay and the web server that powered it, along the lines of ‚ÄúI am select()\nof borg, resistance is futile‚Äù, mocking someone for not understanding how to\nwrite a scalable web server.</p><p>But / also doesn‚Äôt scale. If you have ten thousand\nconnections, that‚Äôs an array of ten thousand integers that need to be sent to\nthe kernel for every single iteration of your request handling loop.</p><p>Enter  ( on other operating systems, but I‚Äôm focusing on Linux\nhere). Now that‚Äôs better. The main loop is now:</p><div><pre><code>  set_up_epoll()\n  while True:\n    new, read, write = epoll()\n    epoll_add_connections(new)\n    for con in read:\n      process(con.read())\n      if con.read_all_we_need:\n        epoll_remove_read_op(con)\n    for con in write:\n      con.write_buffer()\n      if con.buffer_empty:\n        epoll_remove_write_op(con)\n</code></pre></div><p>All the syscalls are pretty cheap.  only deals in deltas, and it\ndoesn‚Äôt have to be re-told the thousands of active connections.</p><p>But they‚Äôre not without cost. Once we‚Äôve gotten this far, the cost of a syscall\nis actually a significant part of the total remaining cost.</p><p>We‚Äôre here going to ignore improvements like  and , and\ninstead jump to‚Ä¶</p><p>Instead of performing a syscall for everything we want to do, commanding the\nkernel to do this or that, io_uring lets us just keep writing orders to a\nqueue, and letting the kernel consume that queue asynchronously.</p><p>For example, we can put  into the queue. The kernel will pick that\nup, wait for an incoming connection, and when it arrives it‚Äôll put a\n‚Äúcompletion‚Äù into the completion queue.</p><p>The web server can then check the completion queue. If there‚Äôs a completion\nthere, it can act on it.</p><p>This way the web server can queue up all kinds of operations that were\npreviously ‚Äúexpensive‚Äù syscalls by simply writing them to memory. That‚Äôs it.\nAnd then it‚Äôll read the results from another part of memory. That‚Äôs it.</p><p>In order to avoid busy looping, both the kernel and the web server will only\nbusy-loop checking the queue for a little bit (configurable, but think\nmilliseconds), and if there‚Äôs nothing new, the web server will do a syscall to\n‚Äúgo to sleep‚Äù until something gets added to the queue.</p><p>Similarly on the kernel side, the kernel will stop busy-looping if there‚Äôs\nnothing new, and needs a syscall to start busylooping again.</p><p>This sounds like it would be tricky to optimize, but it‚Äôs not. In the end the\nweb server just puts stuff on the queue, and calls a library function that only\ndoes that syscall if the kernel actually has stopped busylooping.</p><p>This means that a busy web server can serve all of its queries without even once\n(after setup is done) needing to do a syscall. As long as queues keep getting\nadded to,  will show .</p><p>Since CPUs today have many cores, ideally you want to run exactly one thread\nper core, bind it to that core, and not share any read-write data structure.</p><p>For <a href=\"https://en.wikipedia.org/wiki/Non-uniform_memory_access\">NUMA</a> hardware, you also want to make sure that a thread only\naccesses memory on the local NUMA node. <a href=\"https://youtu.be/36qZYL5RlgY\">This netflix talk</a> has some\ninteresting stuff on NUMA and high volume HTTP delivery.</p><p>The request load will still not be perfectly balanced between the threads (and\ntherefore cores), but I guess fixing that would have to be the topic of a\nfuture post.</p><p>We will still have memory allocations though, both on the kernel and web server\nside. Memory allocations in user space will eventually need syscalls.</p><p>For the web server side, you can pre-allocate a fixed chunk for every\nconnection, and then have everything about that connection live there. That way\nnew connections don‚Äôt need syscalls, memory doesn‚Äôt get fragmented, and you\ndon‚Äôt run the risk of running out of memory.</p><p>On the kernel side each connection will still need buffers for incoming and\noutgoing bytes. This may be somewhat controllable via socket options, but again\nit‚Äôll have to be the subject of a future post.</p><p>Try to not run out of RAM. Bad things tend to happen.</p><p><a href=\"https://docs.kernel.org/networking/tls-offload.html\">kTLS</a> is a feature of the Linux kernel where an application can hand off\nthe job of encryption/decryption to the kernel. The application still has to\nperform the TLS handshake, but after that it can enable kTLS and pretend that\nit‚Äôs all sent in plaintext.</p><p>You may say that this doesn‚Äôt actually speed anything up, it just moves \nencryption was done. But there are gains:</p><ol><li>This means that  can be used, removing the need to copy a bunch\nof data between user space and kernel space.</li><li>If the network card has hardware support for it, the crypto operation may\nactually be offloaded from the CPU onto the network card, leaving the CPU to\ndo better things.</li></ol><p>Another optimization is to avoid passing file descriptors back and forth\nbetween user space and kernel space. The mapping between file descriptors and\nio_uring apparently has overhead.</p><p>Now the supposed file descriptor numbers that user space sees are just\nintegers. They don‚Äôt show up in , and can only be used with\nio_uring. They‚Äôre still capped by the  file descriptor limit, though.</p><p>It‚Äôs named  because it‚Äôs a web server that serves the content of a\nsingle tar file.</p><p>Rust, io_uring, and kTLS. Not exactly the most common combination. I found that\nio_uring and kTLS didn‚Äôt play super well together. Enabling kTLS requires three\n calls, and io_uring doesn‚Äôt support  (until they\nmerge <a href=\"https://github.com/tokio-rs/io-uring/pull/320\">my PR</a>, that is).</p><p>And the  crate, part of , only allows you to call the synchronous\n, not export the needed struct for me to pass to my new\nio_uring . <a href=\"https://github.com/rustls/ktls/pull/54\">Another pr sent</a>.</p><p>So with those two PRs merged, it‚Äôs working great.</p><p>tarweb is far from perfect. The code needs a lot of work, and there‚Äôs no\nguarantee that the TLS library (rustls) doesn‚Äôt do memory allocations during\nhandshakes. But it does serve https without even one syscall on a per request\nbasis. And that‚Äôs pretty cool.</p><p>I have not done any benchmarks yet. I want to clean the code up first.</p><p>One thing making io_uring more complex than synchronous syscalls is that any\nbuffer needs to stay in memory until the operation is marked completed by\nshowing up in the completion queue.</p><p>For example when submitting a  operation, the memory location of those\nbytes must not be deallocated or overwritten.</p><p>The  crate doesn‚Äôt help much with this. The API doesn‚Äôt allow the\nborrow checker to protect you at compile time, and I don‚Äôt see it doing any\nruntime checks either.</p><p>I feel like I‚Äôm back in C++, where any mistake can blow your whole leg off.\nIt‚Äôs a miracle that I‚Äôve not seen a segfault.</p><p>Someone should make a  crate or similar, using the powers of\n<a href=\"https://blog.cloudflare.com/pin-and-unpin-in-rust/\">pinning</a> and/or borrows or something, to achieve Rust‚Äôs normal ‚Äúif it\ncompiles, then it‚Äôs correct‚Äù.</p>","contentLength":7262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44980865"},{"title":"Everything is correlated (2014‚Äì23)","url":"https://gwern.net/everything","date":1755828353,"author":"gmays","guid":236864,"unread":true,"content":"<p>Problem 6. : In the social sciences and arguably in the biological sciences, ‚Äúeverything correlates to some extent with everything else.‚Äù This truism, which I have found no competent psychologist disputes given 5 minutes reflection, does not apply to pure experimental studies in which attributes that the subjects bring with them are not the subject of study (except in so far as they appear as a source of error and hence in the denominator of a significance test). There is nothing mysterious about the fact that in psychology and sociology everything correlates with everything. Any measured trait or attribute is some function of a list of partly known and mostly unknown causal factors in the genes and life history of the individual, and both genetic and environmental factors are known from tons of empirical research to be themselves correlated. To take an extreme case, suppose we construe the null hypothesis literally (objecting that we mean by it ‚Äúalmost null‚Äù gets ahead of the story, and destroys the rigor of the Fisherian mathematics!) and ask whether we expect males and females in Minnesota to be precisely equal in some arbitrary trait that has individual differences, say, color naming. In the case of color naming we could think of some obvious differences right off, but even if we didn‚Äôt know about them, what is the causal situation? If we write a causal equation (which is not the same as a regression equation for pure predictive purposes but which, if we had it, would serve better than the latter) so that the score of an individual male is some function (presumably nonlinear if we knew enough about it but here supposed linear for simplicity) of a rather long set of causal variables of genetic and environmental type , , ‚Ä¶ . These values are operated upon by regression coefficients , , ‚Ä¶.</p><p>‚Ä¶Now we write a similar equation for the class of females. Can anyone suppose that the beta coefficients for the two sexes will be exactly the same? Can anyone imagine that the mean values of all of the s will be exactly the same for males and females, even if the culture were not still considerably sexist in child-rearing practices and the like? If the betas are not exactly the same for the two sexes, and the mean values of the s are not exactly the same, what kind of Leibnitzian preestablished harmony would we have to imagine in order for the mean color-naming score to come out exactly equal between males and females? It boggles the mind; it simply would never happen. As Einstein said, ‚Äúthe Lord God is subtle, but He is not malicious.‚Äù We cannot imagine that nature is out to fool us by this kind of delicate balancing. Anybody familiar with large scale research data takes it as a matter of course that when the  gets big enough she will not be looking for the statistically-significant correlations but rather looking at their patterns, since almost all of them will be significant. In saying this, I am not going counter to what is stated by mathematical statisticians or psychologists with statistical expertise. For example, the standard psychologist‚Äôs textbook, the excellent treatment by Hays (<a href=\"https://gwern.net/everything#hays-1973\">1973, page 415</a>), explicitly states that, taken literally, the null hypothesis is always false.</p><p>20 ago David Lykken and I conducted an exploratory study of the crud factor which we never published but I shall summarize it briefly here. (I offer it not as ‚Äúempirical proof‚Äù‚Äîthat  taken literally is quasi-always false hardly needs proof and is generally admitted‚Äîbut as a punchy and somewhat amusing example of an insufficiently appreciated truth about soft correlational psychology.) In , the University of Minnesota Student Counseling Bureau‚Äôs Statewide Testing Program administered a questionnaire to 57,000 high school seniors, the items dealing with family facts, attitudes toward school, vocational and educational plans, leisure time activities, school organizations, etc. We cross-tabulated a total of 15 (and then 45) variables including the following (the number of categories for each variable given in parentheses): father‚Äôs occupation (7), father‚Äôs education (9), mother‚Äôs education (9), number of siblings (10), birth order (only, oldest, youngest, neither), educational plans after high school (3), family attitudes towards college (3), do you like school (3), sex (2), college choice (7), occupational plan in 10 years (20), and religious preference (20). In addition, there were 22 ‚Äúleisure time activities‚Äù such as ‚Äúacting‚Äù, ‚Äúmodel building‚Äù, ‚Äúcooking‚Äù, etc., which could be treated either as a single 22-category variable or as 22 dichotomous variables. There were also 10 ‚Äúhigh school organizations‚Äù such as ‚Äúschool subject clubs‚Äù, ‚Äúfarm youth groups‚Äù, ‚Äúpolitical clubs‚Äù, etc., which also could be treated either as a single ten-category variable or as 10 dichotomous variables. Considering the latter two variables as multichotomies gives a total of 15 variables producing 105 different cross-tabulations. All values of œá for these 105 cross-tabulations were statistically-significant, and 101 (96%) of them were significant with a probability of less than 10.</p><p>‚Ä¶If ‚Äúleisure activity‚Äù and ‚Äúhigh school organizations‚Äù are considered as separate dichotomies, this gives a total of 45 variables and 990 different crosstabulations. Of these, 92% were statistically-significant and more than 78% were significant with a probability less than 10. Looked at in another way, the median number of significant relationships between a given variable and all the others was 41 out of a possible 44!</p><p>We also computed <a href=\"https://en.wikipedia.org/wiki/Medical_College_Admission_Test\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Medical_College_Admission_Test#bodyContent\" title=\"Medical College Admission Test\">MCAT</a> scores by category for the following variables: number of siblings, birth order, sex, occupational plan, and religious preference. Highly significant deviations from chance allocation over categories were found for each of these variables. For example, the females score higher than the males; MCAT score steadily and markedly decreases with increasing numbers of siblings; eldest or only children are statistically-significantly brighter than youngest children; there are marked differences in MCAT scores between those who hope to become nurses and those who hope to become nurses aides, or between those planning to be farmers, engineers, teachers, or physicians; and there are substantial MCAT differences among the various religious groups. We also tabulated the 5 principal Protestant religious denominations (Baptist, Episcopal, Lutheran, Methodist, and Presbyterian) against all the other variables, finding highly significant relationships in most instances. For example, only children are nearly twice as likely to be Presbyterian than Baptist in Minnesota, more than half of the Episcopalians ‚Äúusually like school‚Äù but only 45% of Lutherans do, 55% of Presbyterians feel that their grades reflect their abilities as compared to only 47% of Episcopalians, and Episcopalians are more likely to be male whereas Baptists are more likely to be female. 83% of Baptist children said that they enjoyed dancing as compared to 68% of Lutheran children. More than twice the proportion of Episcopalians plan to attend an out of state college than is true for Baptists, Lutherans, or Methodists. The proportion of Methodists who plan to become conservationists is nearly twice that for Baptists, whereas the proportion of Baptists who plan to become receptionists is nearly twice that for Episcopalians.</p><p>In addition, we tabulated the 4 principal Lutheran Synods (Missouri, ALC, LCA, and Wisconsin) against the other variables, again finding highly significant relationships in most cases. Thus, 5.9% of Wisconsin Synod children have no siblings as compared to only 3.4% of Missouri Synod children. 58% of ALC Lutherans are involved in playing a musical instrument or singing as compared to 67% of Missouri Synod Lutherans. 80% of Missouri Synod Lutherans belong to school or political clubs as compared to only 71% of LCA Lutherans. 49% of ALC Lutherans belong to debate, dramatics, or musical organizations in high school as compared to only 40% of Missouri Synod Lutherans. 36% of LCA Lutherans belong to organized non-school youth groups as compared to only 21% of Wisconsin Synod Lutherans. [Preceding text courtesy of D. T. Lykken.]</p><p>These relationships are not, I repeat, Type I errors. They are facts about the world, and with  = 57,000 they are pretty stable. Some are theoretically easy to explain, others more difficult, others completely baffling. The ‚Äúeasy‚Äù ones have multiple explanations, sometimes competing, usually not. Drawing theories from a pot and associating them whimsically with variable pairs would yield an impressive batch of -refuting ‚Äúconfirmations.‚Äù</p><p>Another amusing example is the behavior of the items in the 550 items of the MMPI pool with respect to sex. Only 60 items appear on the Mf scale, about the same number that were put into the pool with the hope that they would discriminate femininity. It turned out that over half the items in the scale were not put in the pool for that purpose, and of those that were, a bare majority did the job. Scale derivation was based on item analysis of a small group of criterion cases of male homosexual invert syndrome, a significant difference on a rather small  of Dr.&nbsp;Starke Hathaway‚Äôs private patients being then conjoined with the requirement of discriminating between male normals and female normals. When the  becomes very large as in the data published by <a href=\"https://gwern.net/doc/statistics/causality/1973-wendell-anmmpisourcebook.pdf\" data-link-icon=\"pdf\" data-link-icon-type=\"svg\" data-link-icon-color=\"#f40f02\" data-filesize-bytes=\"42307651\" data-filesize-percentage=\"98\">Swenson, Pearson, and Osborne (; <em>An MMPI Source Book: Basic Item, Scale, And Pattern Data On 50,000 Medical Patients</em>. Minneapolis, MN: University of Minnesota Press.)</a>, approximately 25,000 of each sex tested at the Mayo Clinic over a period of years, it turns out that 507 of the 550 items discriminate the sexes. Thus in a heterogeneous item pool we find only 8% of items failing to show a significant difference on the sex dichotomy. The following are sex-discriminators, the male/female differences ranging from a few percentage points to over 30%:</p><ul><li><p>Sometimes when I am not feeling well I am cross.</p></li><li><p>I believe there is a Devil and a Hell in afterlife.</p></li><li><p>I think nearly anyone would tell a lie to keep out of trouble.</p></li><li><p>Most people make friends because friends are likely to be useful to them.</p></li><li><p>Policemen are usually honest.</p></li><li><p>I sometimes tease animals.</p></li><li><p>My hands and feet are usually warm enough.</p></li><li><p>I think Lincoln was greater than Washington.</p></li><li><p>I am certainly lacking in self-confidence.</p></li><li><p>Any man who is able and willing to work hard has a good chance of succeeding.</p></li></ul><p>I invite the reader to guess which direction scores ‚Äúfeminine.‚Äù Given this information, I find some items easy to ‚Äúexplain‚Äù by one obvious theory, others have competing plausible explanations, still others are baffling.</p><p>Note that we are not dealing here with some source of statistical error (the occurrence of random sampling fluctuations). That source of error is limited by the significance level we choose, just as the probability of Type II error is set by initial choice of the statistical power, based upon a pilot study or other antecedent data concerning an expected average difference. Since in social science everything correlates with everything to some extent, due to complex and obscure causal influences, in considering the crud factor we are talking about  differences,  correlations,  trends and patterns for which there is, of course, some true but complicated multivariate causal theory. I am not suggesting that these correlations are fundamentally unexplainable. They would be completely explained if we had the knowledge of Omniscient Jones, which we don‚Äôt. The point is that we are in the weak situation of corroborating our particular substantive theory by showing that  and  are ‚Äúrelated in a nonchance manner‚Äù, when our theory is too weak to make a numerical prediction or even (usually) to set up a range of admissible values that would be counted as corroborative.</p><p>‚Ä¶Some psychologists play down the influence of the ubiquitous crud factor, what <a href=\"https://gwern.net/everything#lykken-1968\" title=\"Statistical-Significance in Psychological Research\">David Lykken ()</a> calls the ‚Äúambient correlational noise‚Äù in social science, by saying that we are not in danger of being misled by small differences that show up as significant in gigantic samples. How much that softens the blow of the crud factor‚Äôs influence depends upon the crud factor‚Äôs average size in a given research domain, about which neither I nor anybody else has accurate information. <em>But the notion that the correlation between arbitrarily paired trait variables will be, while not literally zero, of such minuscule size as to be of no importance, is surely wrong.</em> Everybody knows that there is a set of demographic factors, some understood and others quite mysterious, that correlate quite respectably with a variety of traits. (Socioeconomic status, SES, is the one usually considered, and frequently assumed to be only in the ‚Äúinput‚Äù causal role.) The clinical scales of the MMPI were developed by empirical keying against a set of disjunct nosological categories, some of which are phenomenologically and psychodynamically opposite to others. Yet the 45 pairwise correlations of these scales are almost always positive (scale Ma provides most of the negatives) and a representative size is in the neighborhood of 0.35 to 0.40. The same is true of the scores on the Strong Vocational Interest Blank, where I find an average absolute value correlation close to 0.40. The malignant influence of so-called ‚Äúmethods covariance‚Äù in psychological research that relies upon tasks or tests having certain kinds of behavioral similarities such as questionnaires or ink blots is commonplace and a regular source of concern to clinical and personality psychologists. For further discussion and examples of crud factor size, see <a href=\"https://gwern.net/everything#meehl-1990-2\" title=\"Appraising and amending theories: the strategy of Lakatosian defense and two principles that warrant using it\">Meehl ()</a>.</p><p>Now suppose we imagine a society of psychologists doing research in this soft area, and each investigator sets his experiments up in a whimsical, irrational manner as follows: First he picks a theory at random out of the theory pot. Then he picks a pair of variables randomly out of the observable variable pot. He then arbitrarily assigns a direction (you understand there is no intrinsic connection of content between the substantive theory and the variables, except once in a while there would be such by coincidence) and says that he is going to test the randomly chosen substantive theory by pretending that it predicts‚Äîalthough in fact it does not, having no intrinsic contentual relation‚Äîa positive correlation between randomly chosen observational variables  and . Now suppose that the crud factor operative in the broad domain were 0.30, that is, the average correlation between all of the variables pairwise in this domain is 0.30. This is not sampling error but the true correlation produced by some complex unknown network of genetic and environmental factors. Suppose he divides a normal distribution of subjects at the median and uses all of his cases (which frequently is not what is done, although if properly treated statistically that is not methodologically sinful). Let us take variable  as the ‚Äúinput‚Äù variable (never mind its causal role). The mean score of the cases in the top half of the distribution will then be at one mean deviation, that is, in standard score terms they will have an average score of 0.80. Similarly, the subjects in the bottom half of the  distribution will have a mean standard score of -0.80. So the mean difference in standard score terms between the high and low s, the one ‚Äúexperimental‚Äù and the other ‚Äúcontrol‚Äù group, is 1.6. If the regression of output variable  on  is approximately linear, this yields an expected difference in standard score terms of 0.48, so the difference on the arbitrarily defined ‚Äúoutput‚Äù variable  is in the neighborhood of half a standard deviation.</p><p>When the investigator runs a -test on these data, what is the probability of achieving a statistically-significant result? This depends upon the statistical power function and hence upon the sample size, which varies widely, more in soft psychology because of the nature of the data collection problems than in experimental work. I do not have exact figures, but an informal scanning of several issues of journals in the soft areas of clinical, abnormal, and social gave me a representative value of the number of cases in each of two groups being compared at around  =  = 37 (that‚Äôs a median because of the skewness, sample sizes ranging from a low of 17 in one clinical study to a high of 1,000 in a social survey study). Assuming equal variances, this gives us a standard error of the mean difference of 0.2357 in sigma-units, so that our  is a little over 2.0. The substantive theory in a real life case being almost invariably predictive of a direction (it is hard to know what sort of significance testing we would be doing otherwise), the 5% level of confidence can be legitimately taken as one-tailed and in fact could be criticized if it were not (assuming that the 5% level of confidence is given the usual special magical significance afforded it by social scientists!). The directional 5% level being at 1.65, the expected value of our -test in this situation is approximately 0.35  units from the required significance level. Things being essentially normal for 72 df, this gives us a power of detecting a difference of around 0.64.</p><p>However, since in our imagined ‚Äúexperiment‚Äù the assignment of direction was random, the probability of detecting a difference in the predicted direction (even though in reality this prediction was not mediated by any rational relation of content) is only half of that. Even this conservative power based upon the assumption of a completely random association between the theoretical substance and the pseudopredicted direction should give one pause. We find that the probability of getting a positive result from a theory with no verisimilitude whatsoever, associated in a totally whimsical fashion with a pair of variables picked randomly out of the observational pot, is ! This is quite different from the 0.05 level that people usually think about. Of course, the reason for this is that the 0.05 level is based upon strictly holding  if the theory were false. Whereas, because in the social sciences everything is correlated with everything, for epistemic purposes (despite the rigor of the mathematician‚Äôs tables) the true baseline‚Äîif the theory has nothing to do with reality and has only a chance relationship to it (so to speak, ‚Äúany connection between the theory and the facts is purely coincidental‚Äù) - is 6 or 7 times as great as the reassuring 0.05 level upon which the psychologist focuses his mind. If the crud factor in a domain were running around 0.40, the power function is 0.86 and the ‚Äúdirectional power‚Äù for random theory/prediction pairings would be 0.43.</p><p>‚Ä¶A similar situation holds for psychopathology, and for many variables in personality measurement that refer to aspects of social competence on the one hand or impairment of interpersonal function (as in mental illness) on the other. <a href=\"https://gwern.net/everything#thorndike-1920\">Thorndike had a dictum</a> ‚ÄúAll good things tend to go together.‚Äù</p>","contentLength":19167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44980339"},{"title":"The issue of anti-cheat on Linux (2024)","url":"https://tulach.cc/the-issue-of-anti-cheat-on-linux/","date":1755824973,"author":"todsacerdoti","guid":237165,"unread":true,"content":"<p>If you switch to Linux today, you‚Äôll probably be surprised by how many games run out of the box just fine (mostly due to the <a href=\"https://github.com/ValveSoftware/Proton\">Windows compatibility layer Proton</a> built right into Steam),  for basically all competitive multiplayer games that utilize any sort of anti-cheat technology.</p><p>Now I can finally get to the point of the article‚Ä¶  As someone who uses Linux daily, I would love to see these games support it, but I just don‚Äôt see that happening any time soon. Many people in the Linux community are frustrated by the fact that these anti-cheat solutions are stopping them from playing their favorite games. It also doesn‚Äôt help that some are <a href=\"https://youtu.be/_dOCtaBObg4?si=bxrl7H5Xl6FBafH5\">fear-mongering about kernel-level anti-cheat solutions</a> and <a href=\"https://www.reddit.com/r/riotgames/comments/1cjq63h/vanguard_real_is/\">spreading misinformation</a>.</p><p>In this article, I want to give you a high-level overview of how modern anti-cheat solutions work (which will hopefully be understandable even for non-technical people) and then explain why anti-cheat solutions in their current state just cannot work on Linux, as well as what the alternatives are.</p><p><em>What is a videogame cheat?</em> We could talk for hours about whether all sorts of macros and exploits should be considered cheats, but the main thing that comes to people‚Äôs minds when talking about multiplayer games is an external program that somehow manipulates the game or reads information from the game to provide you with an advantage over others. A prime example of this would be a <a href=\"https://en.wikipedia.org/wiki/Cheating_in_online_games#Wallhacking\">wallhack or aimbot</a>.</p><p>There are generally two ways you can go about this:</p><ol><li>() Have a completely separate process that copies memory between itself and the game.</li><li>() Force the game to load a <a href=\"https://learn.microsoft.com/en-us/troubleshoot/windows-client/setup-upgrade-and-drivers/dynamic-link-library\">DLL file</a> (a <a href=\"https://en.wikipedia.org/wiki/Dynamic-link_library\">shared library file</a> containing code) directly into the game, executing custom code from within the game.</li></ol><p>Unless you find some very niche way to load a DLL into the game, in both cases you will need the ability to read (and write) the game‚Äôs process memory.</p><p>If you are not a programmer (or <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">you are a JavaScript developer</a>), you most likely don‚Äôt really know how memory management works on modern systems. Let‚Äôs imagine this situation: two programs are loaded in memory. What is stopping one program from directly accessing the memory of the other program?</p><p><em>Virtual address space in Windows (<a href=\"https://learn.microsoft.com/en-us/windows-hardware/drivers/gettingstarted/virtual-address-spaces\">source</a>).</em></p><p>While in the past it would have been perfectly possible to read (almost) any of the physical memory installed in the computer, nowadays OSes use <a href=\"https://learn.microsoft.com/en-us/windows-hardware/drivers/gettingstarted/virtual-address-spaces\">virtual address spaces</a>. I don‚Äôt want to go into the details of how this is handled, but all you need to know is that each program is isolated in its own address space and cannot access other programs‚Äô memory unless it uses functions provided by the operating system itself, like <a href=\"https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-readprocessmemory\"></a> and <a href=\"https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-writeprocessmemory\"></a>.</p><p>In order to use those two functions, you will need to open a <a href=\"https://learn.microsoft.com/en-us/windows/win32/sysinfo/handles-and-objects\">handle</a> to the process you want to read or write memory from. This handle will be specific to your process and represent the access rights that you have relative to the object it represents (in this case, the game process). Remember this for later, as it will be important.</p><p>Modern anti-cheat solutions have three main goals:</p><ol><li>Block other processes from accessing the game‚Äôs memory whenever possible.</li><li>Detect and ban anyone who tries to get around the blocking mentioned above.</li><li>Once someone is banned, ensure that they cannot simply create a new game account and continue playing (HWID bans).</li></ol><p>This is usually achieved by multiple components working together. Let‚Äôs take a look at <a href=\"https://www.easy.ac/en-US\">Easy Anti-Cheat</a> as an example:</p><ul><li>Loader (usually  or )</li><li>Game library ( and ‚Äúinvisible‚Äù module)</li><li>Service ()</li><li>Kernel-mode driver ()</li></ul><p>Without a kernel-mode driver, there is no way to  block memory access into the game. With the kernel-mode driver, though, it‚Äôs incredibly simple. All that the driver needs to do is <a href=\"https://github.com/Microsoft/Windows-driver-samples/blob/main/general/obcallback/driver/callback.c\">register a callback for handle creation</a>, filter out requests to open such handles to the game process, check the requested permissions, and if they allow memory access, either deny the request or lower the permissions. That way, no usermode process can now read or write the games memory. Same can be applied to module loading and file system access.</p><p><em>Using open-source <a href=\"https://github.com/cheat-engine/cheat-engine\">Cheat Engine</a> to try to read protected game‚Äôs memory (all reads fail).</em></p><p>So how can anyone get around it? They also  need to get their code into the kernel, which will open many ways for them to access the game memory.</p><p>Notice how I highlighted ‚Äúsomehow‚Äù? That‚Äôs because Windows is a closed system where Microsoft has the control to decide <a href=\"https://learn.microsoft.com/en-us/windows-hardware/drivers/install/kernel-mode-code-signing-requirements--windows-vista-and-later-\">who should get access to the kernel</a>. All official kernel components are signed with Microsoft code signing certificates, so it‚Äôs trivial to verify their authenticity. All 3rd party drivers need to be signed with an <a href=\"https://learn.microsoft.com/en-us/windows-hardware/drivers/dashboard/code-signing-reqs\">EV code signing certificate</a> (which can only be bought by companies) and then go through the <a href=\"https://learn.microsoft.com/en-us/windows-hardware/drivers/dashboard/hardware-program-register\">Hardware Developer Center</a> certification so they can even be loaded. I am not saying this is perfect; in fact, I will most likely be writing an article about how bad actors are still getting their stuff certified. However, when they do, it usually gets quickly revoked, and it‚Äôs so costly and complicated that most don‚Äôt even bother trying.</p><p>There is, of course, a way to get around it by using <a href=\"https://github.com/SamuelTulach/nullmap\">all sorts of exploits</a> or by <a href=\"https://github.com/hfiref0x/KDU\">using vulnerable drivers</a> (drivers that expose a programming interface to user-mode processes without any checks in place, which allows them to escalate their privileges and possibly even manipulate kernel components). This is where the second goal defined above comes in. The anti-cheat has to actively scan the system and try to find code that is not associated with any legitimate module (a module that was loaded properly, with all certs in place) and other modifications or patches that would otherwise not be there.</p><p>While most gamers are going to say that those anti-cheats are useless and that they see cheaters left and right, the truth is that they add a huge skill check, so not everyone is able to write a cheat and then not get banned. In fact, if done properly, the cheating problem can be basically eliminated this way (I‚Äôll get to this later).</p><p>Another reason to run in the kernel is HWID (hardware identifier) banning (the 3rd point mentioned above). If a player is banned and creates a new account, playing on the same hardware will result in an immediate ban. Since the anti-cheat has a kernel component, it can directly talk to the hardware and read its serials that way. If it was running only as a user-mode process, it would be trivial to fake the serial reads. I am not personally a big fan of this since, as you can imagine, it can result in all sorts of unintended issues (people buying used hardware), but in reality, it‚Äôs not really a problem since those HWID bans usually expire after a few months (the game devs won‚Äôt tell you this though üòâ).</p><p>If I had to pick a game which handles cheating the best, then as of now in my humble opinion it would be <a href=\"https://playvalorant.com/en-gb/\">Valorant</a> by <a href=\"https://www.riotgames.com/en\">Riot Games</a>. Keep in mind the stuff that you‚Äôve just read and let me explain:</p><ul><li>The anti-cheat is loaded on boot. While scary for some, this allows them to block/detect the previously mentioned vulnerable drivers and exploits. This raises the skill required to write a cheat for the game even higher (usually, people resort to <a href=\"https://tulach.cc/bootkits-and-kernel-patching/\">bootkits</a>).</li><li>The kernel driver then doesn‚Äôt do anything apart from logging (locally). When the game is actually started, it goes through those logs and figures out if the game launch should be allowed or not and does all the kernel protection stuff mentioned above.</li><li>More advanced methods to obtain HWID are used, such as reading <a href=\"https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/component-updates/tpm-key-attestation\">TPM EK</a>, which is very hard to spoof properly.</li></ul><p>But that‚Äôs not all. If that was all there was to it, other anti-cheats would be just as effective. The anti-cheat team closely works with the game development team as well. How? The anti-cheat introduces <a href=\"https://reversing.info/posts/guardedregions/\">extra protection for certain memory regions of the game</a>. Some <a href=\"https://www.unknowncheats.me/forum/valorant/401729-valorant-1-01-names-decryption.html\">game data are encrypted</a>, and the encryption keys change with every (even small) game update, making it really annoying for cheat developers. On top of all that, the team is very active in the cheating communities to get intel about what they are up to.</p><p>I have played Valorant quite extensively, all the way from Silver to Ascendant, and I have yet to meet a cheater.</p><p>There are two main concerns that people have with those kernel-mode anti-cheats:</p><ol><li>They are in the kernel doing in-depth scans; therefore, they must be vulnerable and a security issue.</li><li>They are so deep in the system (and some start on system boot) that they can spy on us without us noticing.</li></ol><p>Let me ask you a question. How many vulnerable drivers (yes, those that can be abused by bad actors to gain kernel access) do you think the average gamer has on their Windows install? I‚Äôll start with my own system. This is what I can immediately think of:</p><p>If I looked hard enough, I would most likely find more.</p><p>It would be really stupid of me to just point to random crap you could have on your computer and say ‚Äúyou have so much exploitable stuff, don‚Äôt even bother with security,‚Äù and that‚Äôs not what I am trying to say. Or maybe it is, but just a little bit‚Ä¶ What I am trying to say is that there are many ways a malicious actor could do bad stuff with your system, but anti-cheat is very unlikely to have anything to do with it. In fact, I personally trust those anti-cheat developers much more than random vendors, since they are going to be very well aware of the possible abuse.</p><p>Overall, the Windows driver ecosystem is a mess, but unfortunately, that is not going to change any time soon.</p><p>As someone who is very well versed in Windows internals, I can tell you one thing, it doesn‚Äôt make sense. If you give the program administrative permissions (at least once), it can spy on you in the same way a kernel-mode driver could. There is absolutely no difference and it‚Äôs significantly easier to just write a standalone program. There are people who don‚Äôt want to play games because of their connection to <a href=\"https://www.tencent.com/\">Tencent</a> (for example), but if it wasn‚Äôt for the kernel-mode anti-cheat, they would have no problem with it. Isn‚Äôt it a bit hypocritical? If the game company wanted to spy on you, they could have done so from the game process or the <a href=\"https://gamerant.com/stalker-2-drm/\">service they have most likely installed for DRM purposes</a>.</p><p>Oh and just by the way, the vast majority of the data networked by those previously mentioned anti-cheats to their respective servers comes from their usermode component. The only thing that‚Äôs sent ‚Äúby the kernel component‚Äù (in quotes since the usermode service requests the data from the driver and then networks it, drivers cannot directly network data) is the HWID mentioned multiple times above and then detections (something that‚Äôs out of the ordinary). There is really not some magic data grabbing happening that‚Äôs only possible in the kernel.</p><p>Another thing that is sometimes mentioned is that since it‚Äôs in the kernel, it would be harder for security researchers to debug and assess the possible spying. While technically true that it‚Äôs harder, it‚Äôs definitely not impossible or problematic for an experienced person, so trust me, security researchers and  the entire cheating community keep a close eye on it, in the same way they do on the usermode components.</p><p>Congratulations, you have successfully made it. You have read all of the stuff and now we can finally get to the Linux part of this post üéâ.</p><p>As you can probably already tell by the extensive rant above, I don‚Äôt have much good news. Linux is an open system. There is no central authority like on Windows that would tell you what you can and what you cannot do in the kernel. This obviously has countless advantages and it‚Äôs why so many people (and big corporations) love it, but is also the reason why anti-cheats cannot really function like they do on Windows.</p><p>There is no way for them to block or detect memory access into the game. Anything you could think of would just not work. Kernel module? Just recompile the kernel and change the functions it uses to hide the possible cheat and bypass all checks. Mandatory kernel patch? Same thing. What about usermode detections? Just run the game in a <a href=\"https://wiki.debian.org/FakeRoot\">fakeroot environment</a> while the cheat runs with real root privileges, being hidden from the game completely‚Ä¶ Mandatory custom kernel build? Entire Linux system dedicated to the anti-cheat? I mean‚Ä¶ that could work, but at that point, you can just install Windows.</p><p>There have been attempts to get anti-cheat to work on Linux. <a href=\"https://www.easy.ac/en-US\">Easy Anti-Cheat</a> is the most prominent one. Developers can <a href=\"https://www.gamingonlinux.com/2021/09/epic-games-announce-full-easy-anti-cheat-for-linux-including-wine-a-proton/\">choose whether they want to allow it to run on Linux or not</a>. Linux gamers look at this and use it as an argument that anti-cheat on Linux does not face any issues, but the truth is that apart from the most basic sanity checks, EAC does absolutely nothing on Linux. It‚Äôs just a simple module that facilitates the server connection and data encryption/decryption for the game.</p><p>One of the games that allowed EAC to run under Wine/Proton is <a href=\"https://www.ea.com/games/apex-legends\">Apex Legends</a>. I won‚Äôt be putting any links here, but if you search <a href=\"https://github.com/\">GitHub</a> for cheats for this game, you will find many that work on Linux and there is absolutely no anti-cheat bypass required. It just works.</p><p>As mentioned above, if you want to achieve the best results, you need to utilize both the  and  measures. Active being the kernel component on Windows blocking memory access and trying to find possible discrepancies. Passive being the code virtualization, obfuscation, game data encryption as well as proper game networking and server-sided checks.</p><p>An example of how  to utilize kernel-mode anti-cheat would be <a href=\"https://www.fallguys.com/en-US\">Fall Guys</a> (yes, that‚Äôs the game that one friend made you buy just so you could play it for 30 minutes and then never open again). This game is very specific. There would be no gain in having some sort of wallhack, there would be no gain in having any sort of aimbot (you don‚Äôt aim at stuff). All that people did was speedhacking and modifying the game in a way that allowed them to jump higher and generally change their movement. This game is a prime example of why you should write your network code properly. If the game had proper networking and server checks in place (tick-based system, actions performed on both the client and server, if there is a mismatch, the server is the authority and resets the player - that‚Äôs how <a href=\"https://en.wikipedia.org/wiki/Counter-Strike:_Global_Offensive\">CS:GO</a> did it, and that‚Äôs why people were not flying over the map in that game or speedhacking, it had other issues though), there would be no need for anti-cheat. Not even a usermode one. Instead, they fixed absolutely nothing from their side and slapped <a href=\"https://www.easy.ac/en-US\">Easy Anti-Cheat</a> on top of their game.</p><p>While it‚Äôs not really possible to do any of the previously mentioned active measures, there is nothing stopping you from utilizing the passive ones. So, if you are a game developer and want to limit cheating in your game on Linux:</p><ul><li>Write proper networking code, verify data sent by the client so your game server does not blindly accept mach 8 as a walking speed.</li><li>Use code obfuscation and virtualization as much as possible (be aware of the performance penalty, be smart about what parts of the code you protect), try to change it a bit with every update (commercial bin2bin obfuscators like <a href=\"https://vmpsoft.com/\">VMProtect</a> or <a href=\"https://www.oreans.com/Themida.php\">Themida</a> will produce different results on each run).</li><li>If you have control over the game engine itself, try to keep sensitive information on the stack as much as possible.</li></ul>","contentLength":15291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44980064"},{"title":"Control shopping cart wheels with your phone (2021)","url":"https://www.begaydocrime.com/","date":1755824344,"author":"mystraline","guid":236030,"unread":true,"content":"<div><div><div>\n                        Play the below sounds through your phone speaker and hold it next to a Gatekeeper Systems wheels to lock/unlock. Check me out on twitter @stoppingcart \n                    </div><div>\n                        Most electronic shopping cart wheels listen for a 7.8 kHz signal from an underground wire to know when to lock and unlock. A management remote can send a different signal at 7.8 kHz to the wheel to unlock it.\n        \n                        Since 7.8 kHz is in the audio range, you can use the parasitic EMF from your phone's speaker to \"transmit\" a similar code by playing a crafted audio file. \n                    </div><a href=\"https://www.youtube.com/watch?v=fBICDODmCPI\">Link to my original DEFCON 29 Talk</a></div></div>","contentLength":674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44980004"},{"title":"From GPT-4 to GPT-5: Measuring progress through MedHELM [pdf]","url":"https://www.fertrevino.com/docs/gpt5_medhelm.pdf","date":1755816734,"author":"fertrevino","guid":236765,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44979107"},{"title":"The Onion Brought Back Its Print Edition. The Gamble Is Paying Off","url":"https://www.wsj.com/business/media/the-onion-print-subscribers-6c24649c","date":1755815327,"author":"andsoitis","guid":235971,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44978869"},{"title":"Scientists No Longer Find X Professionally Useful, and Have Switched to Bluesky","url":"https://academic.oup.com/icb/advance-article-abstract/doi/10.1093/icb/icaf127/8196180?redirectedFrom=fulltext&login=false","date":1755814976,"author":"sebg","guid":236863,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44978815"},{"title":"CEO pay and stock buybacks have soared at the 100 largest low-wage corporations","url":"https://ips-dc.org/report-executive-excess-2025/","date":1755813879,"author":"hhs","guid":235939,"unread":true,"content":"<p>This 31st annual Institute for Policy Studies Executive Excess report takes an in-depth look at the 100 S&amp;P 500 corporations with the lowest median worker pay, a group we have dubbed the ‚ÄúLow-Wage 100.‚Äù For each of these companies, we analyze CEO and worker pay trends since 2019. We also compare, for the past six years, what these companies have spent on stock buybacks to pump up short-term share prices with what they have invested in long-term capital improvements.</p><p>The report‚Äôs overall finding: At a time when many American workers are struggling with high costs for groceries and housing, the nation‚Äôs largest low-wage employers are fixated on making their overpaid CEOs even richer.</p><p>Our analysis ends with realizable policy solutions for pushing Corporate America in a more equitable direction.</p><p>For detailed analysis , including our methodology, see the full PDF below. A summary follows.</p><div><div><a target=\"_blank\" href=\"https://ips-dc.org/wp-content/uploads/2025/08/executive_excess_report_2025.pdf\">Download Full Report (PDF)</a></div></div><div><ol><li><strong>CEO pay at Low-Wage 100 firms has soared since 2019 while median worker pay has lagged behind U.S. inflation.</strong><ul><li>Between 2019 and 2024, average CEO compensation within this group rose 34.7 percent in nominal ‚Äî unadjusted for inflation ‚Äî terms, more than double the 16.3 percent increase in these firms‚Äô average median worker pay. The U.S. inflation rate over this same period: 22.6 percent.</li><li>Average CEO compensation within the Low-Wage 100 hit $17.2 million in 2024. The group‚Äôs average median worker pay sat at just $35,570.</li><li><strong>The average CEO-worker pay ratio of Low-Wage 100 firms has widened by 12.9 percent, from 560 to 1 in 2019 to 632 to 1 in 2024.</strong></li><li><strong>The nominal value of median pay actually  at 22 Low-Wage 100 corporations during this period.</strong></li><li><strong>The Starbucks pay gap hit 6,666 to 1 last year</strong>, the Low-Wage 100‚Äôs widest spread by far. In 2024, the Starbucks CEO pocketed $95.8 million.Over the past six years, amid worker discontent fueling union-organizing drives at hundreds of Starbucks stores, the firm‚Äôs median pay rose just 4.2 percent in real terms to $14,674. Only seven S&amp;P 500 firms have lower median pay.</li><li><strong>Ulta Beauty reported the Low-Wage 100‚Äôs steepest drop in median pay</strong>. Between 2019 and 2024, a period when the cosmetic retailer significantly expanded the part-time worker share of its workforce, the company‚Äôs real median pay plunged by 46 percent to $11,078.</li></ul></li><li><strong>From 2019 through 2024, the Low-Wage 100 spent $644 billion on stock buybacks.</strong><ul><li><strong>Over the past six years, all but three Low-Wage 100 firms spent corporate dollars on stock buybacks</strong>. By repurchasing their own shares, companies artificially inflate executive stock-based pay and siphon resources out of worker wages and productive long-term investments.</li><li><strong>Lowe‚Äôs ranks as the Low-Wage 100‚Äôs buyback leader. The company spent $46.6 billion on share repurchases from 2019 through 2024</strong>. Over that span, this sum could have funded an annual $28,456 bonus for each of the firm‚Äôs 273,000 employees ‚Äî or added 88 employees to each of the firm‚Äôs retail outlets. <strong>In 2024, Lowe‚Äôs CEO Marvin Ellison enjoyed a total compensation of $20.2 million ‚Äî</strong><strong>659 times more than the retailer‚Äôs $30,606 median annual worker pay</strong>.</li><li><strong>Home Depot currently sits second in the Low-Wage 100 buyback rankings</strong>. The big-box chain spent $37.9 billion on share repurchases between 2019 and 2024. That outlay would have been enough to give each of Home Depot‚Äôs 470,100 global employees six annual $13,423 bonuses. The Home Depot median pay: just $35,196.</li></ul></li><li><strong>From 2019 through 2024, a majority of Low-Wage 100 firms spent more on stock buybacks than on long-term capital expenditures.</strong><ul><li>Over the past six years, <strong>56 Low-Wage 100 companies plowed more corporate cash into buying back their own shares of stock than investing in capital improvements</strong>.</li><li>If we exclude capital expenditure outlier Amazon from the calculation, <strong>the Low-Wage 100 as a whole spent more on buybacks than on ‚ÄúCapEx‚Äù during this period</strong>.</li></ul></li><li><strong>At least 32 billionaires owe their wealth to Low-Wage 100 companies.</strong><ul><li>Five of these firms have spawned multiple billionaires still living today: Walmart (eight), Estee Lauder (four), DoorDash (three), Public Storage (two), and Tyson Foods (two).</li></ul></li><li><strong>Policy changes can prevent wasteful stock buybacks and excessive CEO payouts.</strong><ul><li><strong>Taxing extreme CEO-worker pay gaps: </strong>In<a href=\"https://www.filesforprogress.org/datasets/2024/4/dfp_progressive_leg_agenda_tabs.pdf\">one recent survey</a>, 80 percent of likely voters expressed support for a tax hike on corporations that pay their CEO over 50 or more times what they pay their median employees.</li><li><strong>Increasing the buybacks tax:</strong> If Congress in 2022 had set our current <a href=\"https://inequality.org/research/congress-takes-historic-step-to-tax-stock-buybacks/\">1 percent excise tax</a> on stock buybacks at 4 percent, the Low-Wage 100 would have owed approximately $6.3 billion in additional federal taxes on share repurchases in 2023 and 2024.</li><li><strong>Restricting buybacks and CEO pay through federal contracts and subsidies:</strong> The Biden administration made modest progress on this front through the CHIPS semiconductor subsidy program. But the federal government could be doing much more to leverage the power of the public purse against wasteful stock buybacks and excessive CEO pay.</li></ul></li></ol></div><p>We highlight three particularly promising areas for CEO pay policy reform.</p><h4>Subjecting corporations with excessive levels of CEO pay to higher tax levies.</h4><p>Higher tax rates on companies with wide CEO-worker pay gaps would create an incentive to both rein in executive pay and raise worker wages, all the while generating significant new capital for vital public investments. Laws that share those goals are already generating revenue in <a href=\"https://inequality.org/action/corporate-pay-equity/\">two major cities</a>, San Francisco and Portland, Oregon.</p><p>Members of the U.S. Congress have also introduced several related bills, including:</p><p>The<strong> Curtailing Executive Overcompensation (CEO) Act: </strong>This <a href=\"https://www.congress.gov/bill/118th-congress/senate-bill/3176\">bill</a> applies an excise tax to publicly traded and private companies with CEO-to-median-worker pay disparities that run over 50 to 1. Under this excise tax formula, the tax owed would be proportional to the degree a company‚Äôs pay ratio exceeds 50 to 1 and to the level of the CEO‚Äôs compensation. In other words, companies with large pay gaps would owe extra taxes ‚Äî and those companies with extremely high CEO pay would owe Uncle Sam even more.</p><p>Revenue estimate: This bill, had it been in effect in 2022, would have raised <a href=\"https://www.whitehouse.senate.gov/news/release/whitehouse-lee-ocasio-cortez-introduce-legislation-to-increase-worker-pay-rein-in-runaway-ceo-compensation/\">over $10 billion</a> in annual revenue from the  100 largest U.S. companies alone.</p><p>The<strong> Tax Excessive CEO Pay Act:</strong> This <a href=\"https://www.congress.gov/bill/118th-congress/senate-bill/3620?q=%7B%22search%22%3A%22Tax+Excessive+CEO+Pay+Act%22%7D&amp;s=1&amp;r=3\">legislation</a> ties a company‚Äôs federal corporate tax rate to the size of the gap between its CEO and median worker pay. Tax penalties would begin at 0.5 percentage points for companies that pay their top executives between 50 and 100 times more than their median workers. The highest penalty would apply to companies that pay top executives over 500 times worker pay.</p><p>A <a href=\"https://www.filesforprogress.org/datasets/2024/4/dfp_progressive_leg_agenda_tabs.pdf\">May 2024 survey</a> suggests that such taxes would be enormously popular. Overall, 80 percent of likely voters favor a tax hike on corporations that pay their CEOs over 50 or more times more than what they pay their median employees.</p><h4>Taxing and restricting stock buybacks.</h4><p>A <a href=\"https://inequality.org/research/congress-takes-historic-step-to-tax-stock-buybacks/\">1 percent federal excise tax</a> on the repurchase of corporate stock went into effect in 2023. With nearly $209 billion in combined stock buybacks for 2023 and 2024, the Low-Wage 100 owed approximately $2.1 billion in federal taxes over those two years.</p><p>A Senate bill, the <a href=\"https://www.congress.gov/bill/118th-congress/senate-bill/413?q=%7B%22search%22%3A%22chamberActionDateCode%3A%5C%222023-02-14%7C118%7C10000%5C%22+AND+billIsReserved%3A%5C%22N%5C%22%22%7D&amp;s=1&amp;r=16\">Stock Buyback Accountability Act</a>, would quadruple this excise tax. If that 4 percent tax had been in place in 2023 and 2024, the Low-Wage 100 would have owed approximately $6.3 billion in additional federal taxes on their share repurchases, assuming no change in their buyback behavior. That increase would be enough to cover the cost of <a href=\"https://www.nationalpriorities.org/interactive-data/trade-offs/?state=00&amp;program=111111\">327,218</a> public housing units each year for two years.</p><p>Another Senate bill, the<a href=\"https://www.warner.senate.gov/public/index.cfm/2023/3/warner-introduces-legislation-to-encourages-companies-to-prioritize-long-term-investments-over-short-term-gains-by-instituting-holding-periods-for-stock-buybacks\">ALIGN Act</a>, would ban executives from selling their shares within a year of a stock buyback announcement. This would prevent CEOs from timing share repurchases to cash in personally on a short-term price pop they themselves have artificially created.</p><h4>Using federal contracts and subsidies to discourage wide corporate pay gaps.</h4><p>The Biden administration took several steps to use the power of the public purse to discourage CEO pay-inflating stock buybacks. During the Biden White House years, the <a href=\"https://www.nist.gov/system/files/documents/2023/02/28/CHIPS_NOFO-1_Fact_Sheet_0.pdf\">Department of Commerce</a> gave preferential treatment in the awarding of $39 billion in CHIPS subsidies for domestic semiconductor production to firms that committed to refraining from all stock buybacks for five years.</p><p>Future administrations could do much more to leverage the power of the public purse against extreme pay disparities. The <a href=\"https://www.congress.gov/bill/117th-congress/house-bill/4186?s=1&amp;r=5#:~:text=Introduced%20in%20House%20(06%2F25%2F2021)&amp;text=This%20bill%20establishes%20new%20requirements,taxes%2C%20and%20private%20equity%20firms.\">Patriotic Corporations Act</a> could serve as a model. This bill would grant preferential treatment in federal contracting to firms with CEO-worker pay ratios of 100 to 1 or less, among other benchmarks. The <a href=\"https://progressives.house.gov/_cache/files/6/8/68a45450-d818-4c0c-a564-cd85ea395d47/88561AA25D995B4AF31FCC6F40CA177D.cpc-recommendations-for-executive-action-3-17-22.pdf\">Congressional Progressive Caucus</a> has supported such incentives.</p><p>By encouraging major corporations to narrow their pay gaps, a president can also help ensure that taxpayers get the biggest bang for federal contract bucks. <a href=\"https://inequality.org/action/corporate-pay-equity/#academic-research\">Studies have shown</a> that companies with narrow gaps in CEO-worker compensation tend to perform at higher levels than firms with wide gaps.</p>","contentLength":8973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44978655"},{"title":"Show HN: Splice ‚Äì CAD for Cable Harnesses and Electrical Assemblies","url":"https://splice-cad.com/","date":1755810634,"author":"djsdjs","guid":236754,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44978140"},{"title":"Code formatting comes to uv experimentally","url":"https://pydevtools.com/blog/uv-format-code-formatting-comes-to-uv-experimentally/","date":1755808004,"author":"tanelpoder","guid":235970,"unread":true,"content":"<p>The latest <a href=\"https://pydevtools.com/handbook/reference/uv/\">uv</a> release (<a href=\"https://github.com/astral-sh/uv/blob/main/CHANGELOG.md#0813\" target=\"_blank\" rel=\"noopener\">0.8.13</a>) quietly introduced an experimental new command that Python developers have been waiting for: . This addition brings code formatting directly into uv‚Äôs toolkit, eliminating the need to juggle multiple tools for basic Python development workflows.</p><p>The  command provides Python code formatting through uv‚Äôs interface. Under the hood, it calls <a href=\"https://pydevtools.com/handbook/reference/ruff/\">Ruff</a>‚Äôs formatter to automatically style your code according to consistent standards.</p><div><div><div><blockquote><p>To clarify,  and  aren‚Äôt being merged. They remain separate tools. This is more about providing a simpler experience for users that don‚Äôt want to think about their formatter as a separate tool.</p><p>The analogy would be to Cargo:  just runs , but you can also run  separately if you want.</p></blockquote></div></div></div><p>First, make sure you‚Äôre running uv 0.8.13 or later. If you need to upgrade, check out our guide on <a href=\"https://pydevtools.com/handbook/how-to/how-to-upgrade-uv/\">upgrading uv</a>.</p><p>Once upgraded, formatting your project is straightforward:</p><p>The command works just like running  in your project root, but through uv‚Äôs interface.</p><h2>Passing Arguments to Ruff<a href=\"https://pydevtools.com/blog/uv-format-code-formatting-comes-to-uv-experimentally/#passing-arguments-to-ruff\" aria-label=\"Permalink for this section\"></a></h2><p>You can pass additional arguments to Ruff by placing them after :</p><p>This flexibility means you can customize formatting behavior without losing uv‚Äôs conveniences.</p><div><div><div><p>Since this is an experimental feature, expect some rough edges:</p><ul><li>The command may change in future releases</li><li>Integration with uv‚Äôs project model might evolve</li><li>Error handling and output formatting could improve</li></ul></div></div></div><p>Try out  in your next project and see how it fits into your development workflow. The experimental nature means your feedback could help shape how this feature evolves.</p>","contentLength":1564,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44977645"},{"title":"Crimes with Python's Pattern Matching (2022)","url":"https://www.hillelwayne.com/post/python-abc/","date":1755805622,"author":"agluszak","guid":235938,"unread":true,"content":"<p>One of my favorite little bits of python is . Abstract Base Classes with  can define what counts as a subclass of the ABC, even if the target doesn‚Äôt know about the ABC. For example:</p><div><pre><code data-lang=\"python\"></code></pre></div><p>You can do some weird stuff with this. Back in 2019 I used it to create <a href=\"https://www.hillelwayne.com/negatypes/\">non-monotonic types</a>, where something counts as a  if it  have the  method. There wasn‚Äôt anything too diabolical you could do with this: nothing in Python really interacted with ABCs, limiting the damage you could do with production code.</p><h3>A quick overview of pattern matching</h3><div><pre><code data-lang=\"py\"></code></pre></div><p>You can match on arrays, dictionaries, and custom objects. To support matching objects, Python uses , which checks</p><ol><li>If  is a transitive subtype of </li><li>If  is an ABC and defines a  that matches the type of .</li></ol><p>That made me wonder if ABCs could ‚Äúhijack‚Äù a pattern match. Something like this:</p><div><pre><code data-lang=\"py\"></code></pre></div><p>But surely Python clamps down on this chicanery, right?</p><pre><code>$ py10 abc.py\n10 is not iterable\nstring is iterable\n[1, 2, 3] is iterable\n</code></pre><div><pre><code data-lang=\"py\"></code></pre></div><p>We can only get the field  we‚Äôve decided the object. We can‚Äôt match ‚Äúany object that has the  field‚Äù‚Ä¶ unless we use ABCs.</p><div><pre><code data-lang=\"python\"></code></pre></div><pre><code>14.142135623730951\n10.488088481701515\n[1, 2, 3] is not a point\n</code></pre><p>It gets better! While the ABC decides the match, the object decides the destructuring, meaning we can do stuff like this:</p><div><pre><code data-lang=\"py\"></code></pre></div><p>The pattern matching is flexible but also fairly limited. It can only match on an object‚Äôs type, meaning we have to make a separate ABC for each thing we want to test. Fortunately, there‚Äôs a way around this. Python is dynamically typed. 99% of the time this just means ‚Äúyou don‚Äôt need static types if you‚Äôre okay with things crashing at runtime‚Äù. But it  means that type information exists at runtime, and that <strong>types can be created at runtime</strong>.</p><p>Can we use this for pattern matching? Let‚Äôs try it:</p><div><pre><code data-lang=\"py\"></code></pre></div><p> is a function that takes a class, defines a  ABC, sets the hook for that ABC to ‚Äúanything that‚Äôs not the class‚Äù, and then returns that ABC.</p><pre><code>    case Not(DistanceMetric)():\n                            ^\nSyntaxError: expected ':'\n</code></pre><p>It‚Äôs an error! We‚Äôve finally hit the limits of pattern matching on ABCs. Then again, it‚Äôs ‚Äújust‚Äù a syntax error. Maybe it would work if we tweak the syntax a little?</p><div><pre><code data-lang=\"diff\">   match x:\n</code></pre></div><pre><code>PlanePoint(x=10, y=10) is a point\nSpacePoint(x=5, y=6, z=7) is a point\n[1, 2, 3] is not a point\n</code></pre><p>Success! And just to test that this is composable, let‚Äôs write an .</p><div><pre><code data-lang=\"py\"></code></pre></div><p>This works as ‚Äú‚Äù‚Äúexpected‚Äù‚Äú‚Äù.</p><h3>Caching Rules Everything Around Me</h3><p>This got me thinking: what if  wasn‚Äôt a pure function? Could I make an ABC that matched the  value of each type passed in, but not subsequent ones?</p><div><pre><code data-lang=\"py\"></code></pre></div><p>Sadly, this was all for naught.</p><pre><code>trying &lt;class 'str'&gt;\nabc is a new class\ntrying &lt;class 'list'&gt;\n[1, 2, 3] is a new class\nefg is a new class\n</code></pre><p>It looks like  caches the results for a given type check. CPython assumes that people don‚Äôt want to shove side effects into esoteric corners of the language. Show‚Äôs how much  know.</p><p>We can still have fun with side effects, though. This ABC lets through every-other type.</p><div><pre><code data-lang=\"python\"></code></pre></div><p>And this ABC asks the user what it should do for each type.</p><div><pre><code data-lang=\"python\"></code></pre></div><p>Try them in a pattern match. They both work!</p><p>The pattern matching feature is, on the whole, pretty reasonably designed, and people will expect it to behave in reasonable ways. Whereas  is  dark magic. This kind of chicanery  have a place in the dark beating heart of a complex library, certainly not for any code your coworkers will have to deal with.</p><p>So yeah, you didn‚Äôt learn anything useful. I just like horrible things ¬Ø\\_(„ÉÑ)_/¬Ø</p>","contentLength":3481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44977189"},{"title":"DeepSeek-v3.1","url":"https://api-docs.deepseek.com/news/news250821","date":1755803169,"author":"wertyk","guid":235900,"unread":true,"content":"<p>Introducing DeepSeek-V3.1: our first step toward the agent era! üöÄ</p><ul><li><p>üß† Hybrid inference: Think &amp; Non-Think ‚Äî one model, two modes</p></li><li><p>‚ö°Ô∏è Faster thinking: DeepSeek-V3.1-Think reaches answers in less time vs. DeepSeek-R1-0528</p></li><li><p>üõ†Ô∏è Stronger agent skills: Post-training boosts tool use and multi-step agent tasks</p></li></ul><ul><li><p>üìà Better results on SWE / Terminal-Bench</p></li><li><p>üîç Stronger multi-step reasoning for complex search tasks</p></li><li><p>‚ö°Ô∏è Big gains in thinking efficiency</p></li></ul>","contentLength":454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44976764"},{"title":"AI tooling must be disclosed for contributions","url":"https://github.com/ghostty-org/ghostty/pull/8289","date":1755802197,"author":"freetonik","guid":235882,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44976568"},{"title":"Building AI products in the probabilistic era","url":"https://giansegato.com/essays/probabilistic-era","date":1755801730,"author":"sdan","guid":235937,"unread":true,"content":"<p>I was recently trying to convince a friend of mine that ChatGPT hasn't memorized every possible medical record, and that when she was passing her blood work results the model was doing pattern matching in ways that even OpenAI couldn't really foresee. She couldn't believe me, and I totally understand why. It's hard to accept that we invented a technology that we don't fully comprehend, and that exhibits behaviors that we didn't explicitly expect.</p><p>Dismissal is a common reaction when witnessing AI‚Äôs rate of progress. People struggle to reconcile their world model with what AI can now do, and .</p><p>This isn't new. Mainstream intuition and cultural impact always lag behind new technical capabilities. When we started building businesses on the Internet three decades ago, the skepticism was similar. Sending checks to strangers and giving away services for free felt absurd. But those who grasped a new reality made of zero marginal costs and infinitely scalable distribution became incredibly wealthy. They understood that the old assumptions baked into their worldview no longer applied, and acted on it.</p><p>Eventually the world caught up, and we reached a new equilibrium. In the last couple of decades the tech industry has evolved, developing a strong instinct for how to build and grow digital products online. We invented new jobs, from product management to head of growth, while others evolved, from engineering leadership to performance marketing. All have created their own playbooks to thrive.</p><p>AI is now shuffling the deck again.</p><p>Many of those playbooks have become obsolete. Something fundamental has shifted. General purpose artificial intelligence has created a rupture in the fabric of the tech industry, upending how we design, engineer, build, and grow software ‚Äî and thus businesses that have software at their core.</p><p>We're now in a liminal moment, where our tools have outpaced our frameworks for understanding them. This is a technical, epistemological, and organizational change, one where exceptional AI companies have started operating significantly differently from what we‚Äôve known in the last decades.</p><p>Just as physics underwent a conceptual revolution when we moved past Newton's deterministic universe, and into a strange and counterintuitive place made by wave functions, software too is undergoing its own quantum shift. We're leaving a world where code reliably and deterministically takes certain inputs to produce specific outputs, and entering a very different one where machines now produce statistical distributions instead.</p><p>Building probabilistic software is like nothing we've done before.</p><p>Today‚Äôs tech industry has been shaped by the core nature of software.</p><p>Like the software they‚Äôre made of, digital products map known inputs to expected results, deterministically. On Instagram, users can upload pictures, send messages, leave comments, follow accounts. On Netflix, they can search an item, pick an item, stream the video item. They choose an action, and expect a result to happen.</p><p>Let‚Äôs frame these products as functions .</p><p>Each input  is a user action inside the product, like ‚Äúsend a message on WhatsApp‚Äù, ‚Äúbook a ride on Uber‚Äù, and ‚Äúsearch an address in Google Maps‚Äù. For each action , the product yields an outcome : the message will be sent, the ride will be booked, the address will be searched.</p><p>Through this framework, it‚Äôs possible to appreciate why startups and tech companies work the way they work.</p><p>This is most evident in engineering management.</p><p>The  way of managing the performance of a software engineering team looks like this:</p><p>If you‚Äôve ever had a developer job, you‚Äôll immediately recognize a SLO Datadog dashboard. It‚Äôs what quantifies the reliability of a system. Since the engineer‚Äôs job is to build , we ask: does taking action  produce outcome ? Does it do so  no matter how many times you try? The goal is 100%: the function should always work as we expect.</p><p>That‚Äôs why layering, cautious refactors, and test-driven development are hallmarks of software engineering: they all stem from the ontological nature of the  mapping function. As the job to be done is always producing  when inputting , we should write tests that make sure that‚Äôs always the case, we should be very wary of general refactorings, and careful when introducing new features that could impact the reliability of the arrow.</p><p>Product management and design too are about reliably mapping  to  ‚Äî just at a different level of abstraction. For those teams, it‚Äôs about constructing a function  where the input  looks more like ‚Äúthe user watched an Instagram story for the first time‚Äù, producing a real-world outcome  like ‚Äúthe user is still using the product a month later‚Äù.</p><p>Good PM‚Äôing is about drilling users through value funnels. The cardinality of the features is known beforehand: the input space  is a limited, pre-determined set of features and growth experiments. The goal is also pre-determined: designers and PMs know in advance what goals they're optimizing for. This means that, like for engineers, they too are striving to reach a 100% score on users going from experiencing a feature to yielding a business outcome. For these teams, it looks more like this:</p><p>Good design is not always quantifiable. Tasteful products aren‚Äôt about metrics. But ultimately they do exist to deliver value, and value is always delivered in funnels. When people stick around longer because they appreciate the tasteful design of a specific feature, that too qualifies as a funnel: from experiencing that feature, to keep paying long term.</p><p>Conversion, activation, and retention are all ratios that require countable, pre-defined inputs and outcomes. The reason why we can count those numbers and construct the ratios is because both the numerator and the denominator are  and : <strong>what  and  can look like are known before the function is created!</strong> By knowing their cardinality, we can create the funnel.</p><p>That‚Äôs why products like Amplitude and Mixpanel revolve around funnels, and the work of product and growth teams revolves around optimizing conversion rates.</p><p>All these ratios, be them engineering reliability goals or growth conversion targets, are how we make both strategic as well as tactical decisions in building and growing software products. How we measure performance, how we structure our work, how we design and implement our playbooks. The entire operating system of the tech industry relies on them.</p><p>The problem is that these rules, in the probabilistic world of AI, have the potential to become actively counterproductive.</p><p>Things started to change in the late 2010s, when we started to witness in AI models the first signs of true generalization. Until that point, machine learning systems were essentially ‚Äúnarrow experts‚Äù rather than ‚Äúcompetent generalists.‚Äù</p><p>In the last decade, however, researchers have realized that pre-training deep learning models on a lot of data ‚Äúcauses the model to develop  abilities and knowledge that can then be transferred to downstream tasks‚Äù. The idea is that if you focus on teaching AI the fundamental structure of the entire domain you‚Äôre interested in (say, language), you can unlock an entire class of tasks , without the need to define them beforehand!, from identifying spam emails to answering trivia questions to role-playing fictional characters. Google‚Äôs T5 showed that ‚Äúpre-training causes the model to develop  abilities and knowledge‚Äù, while OpenAI‚Äôs GPT-2 moved us ‚Äútowards a more  that can perform many tasks.‚Äù</p><p>The crucial bit here is that these models were  explicitly trained on all these tasks. When pre-trained on a large corpus of data and only fine-tuned on certain instructions, we can ‚Äúsubstantially improve zero-shot performance on  tasks.‚Äù </p><p>I cannot emphasize enough how important ‚Äúunseen‚Äù and ‚Äúgeneral-purpose‚Äù are, here. They‚Äôre what made this a truly watershed moment in the history of computing. It's no longer about teaching machines to recognize spam: it's about teaching them to speak. It's no longer about teaching machines to recognize bikes or tell animals apart: it's about giving them the sense of sight itself.</p><p>Think about it: we‚Äôve built a special kind of function  that for all we know can now accept  ‚Äî compose poetry, translate messages, even debug code! ‚Äî and we expect it to always reply with something reasonable. For all intents and purposes, it‚Äôs the first time we stopped controlling the input space, which is now suddenly open-ended.  has become .</p><p>You can ask ChatGPT , from legal advice to romantic support, from spreadsheets analysis using code, to astrology predictions. You can ask Claude to produce  piece of software, from scripts, to websites, to marketing pages, to video games. Even inputting gibberish will still produce . From a practical and philosophical standpoint, the cardinality of the input space is now basically infinite.</p><p>This is quite head scratching if you‚Äôre building AI products. What will users do with them? How can you make sure all your customers will always have a great experience? What if they discover an amazing new use case that these models can perform that you haven‚Äôt foreseen? What if they discover a  use case that your marketing implied in the attempt of staying generic enough?</p><p>To make things worse, the correctness of the output isn't always guaranteed! A reply constructed to be reasonable doesn't mean it's going to be . What this new function will reply with is . Sometimes, a hallucination.</p><p>Can we solve hallucination? Well, we  train perfect systems to always try to reply , but some questions simply don't have \"correct\" answers. What even is the \"correct\" when the question is \"should I leave him?\".</p><p>See, the problem is that the most interesting questions are not well defined. You can only have perfect answers when asking perfect questions, but more often than not, humans don‚Äôt know what they want. ‚ÄúMake me a landing page of my carwash business‚Äù. How many possible ways of achieving that objective are there? Nearly infinite. AI shines in ambiguity and uncertainty, precisely thanks to its emergent properties. If you need to know what‚Äôs 1+1, you should use a calculator, but knowing what your latest blood work results mean for your health requires nuance.</p><p>That's why when building an interface between humans and machines, the best form factor is a probability distribution (\"you  want this HTML with this hero banner\", \"you  have a Vitamin D deficiency and should touch grass more\"). It‚Äôs a shape that can naturally handle nuance.</p><p>That's the critical reason why we inject randomness into the output, and sampling at inference time: <strong>prompting the product with the exact same inputs, will yield two different results</strong>, making the output . This is a fundamental property of the technology, and what makes it work so well, as it provides users with a way to efficiently navigate complex problem spaces. It allows them to add their own taste to the final output, and navigate the probability distribution of all reasonable outputs according to their own judgement.</p><p>This is the result of two identical prompts using Claude 3.7:</p><p>It may feel subtle at the beginning, but over a sequence of chained generations composing a long trajectory, the difference greatly magnifies.</p><p>Output stochasticity and emergent behavior are the reasons why we can't expect perfect reliability from AI, not in the traditional sense. We are no longer guaranteed what  is going to be, and we're no longer certain about the output  either, because it's now drawn from a distribution.</p><p><strong>In moving to an AI-first world, we transitioned from funnels to </strong>.</p><p>Stop for a moment to realize what this means. When building on top of this technology, our products can now succeed in ways we‚Äôve never even imagined, and fail in ways we never intended.</p><p>This is incredibly new, not just for modern technology, but for human toolmaking itself. Any good engineer will know how the Internet works: we designed it! We know how packets of data move around, we know how bytes behave, even in uncertain environments like faulty connections. Any good aerospace engineer will tell you how to approach the moon with spaceships: we invented them! Knowledge is perfect, a cornerstone of the engineering discipline. If there‚Äôs a bug, there‚Äôs always a knowable reason: it‚Äôs just a matter of time to hunt it down and fix it.</p><p>With AI products, all this is no longer true. <strong>These models are , not engineered.</strong> There's some deep unknowability about them that is both powerful and scary. Not even model makers know exactly what their creations can fully do when they train them. It's why \"vibe\" is such an apt word: faced with this inherent uncertainty, we're left to trust our own gut and intuition when judging what these models are truly capable of.</p><p>For people interacting with products harnessing the power of these models, this is a lot to take in, to accept, and to develop an intuition for. Users really dislike the inherent uncertainty of dealing with AI systems. They‚Äôre not used to it! They‚Äôre expecting a digital product like every other product they know: you instruct the app to perform an action, and the app will perform it. Unfortunately, prompting Replit in the wrong way may very well introduce a bug in your work, depending on your request and on the probability distribution that maps to. Consumers really struggle to understand this: it makes them very mad when the AI doesn‚Äôt do what they expect it to do.</p><p>The core reason why they get so frustrated is because <strong>for the first time in the digital era marginal costs are way larger than zero</strong>. In fact, they‚Äôre so large that they completely invalidate the business model and growth playbooks that dominated the Internet since the 90s. This may change in the future, depending on innovation and commoditization at hardware level, but for now the cost of intelligence is surprisingly stable (and not really as deflationary as we expected it to be until last year).</p><p>We have a class of products with deterministic cost and stochastic outputs: a built-in unresolved tension. Users insert the coin with , but will be  of whether they'll get back what they expect. This fundamental mismatch between deterministic mental models and probabilistic reality produces frustration ‚Äî a gap the industry hasn't yet learned to bridge.</p><p>Software used to be magic black boxes offering a set of pre-defined options to pick from, and producing pre-determined results for practically-zero-margin cost. AI models are completely different entities: they accept a field of infinite possibilities, and produce probability distributions that collapse to potentially-unexpected observations after charging users very significant money.</p><p>It‚Äôs clear that the way to produce products around these two technologies needs to be radically different.</p><h2>III. It Takes A Scientist</h2><p>To thrive in the quantum era, <strong>successful organizations need to transition from engineering to empiricism</strong>.</p><p>The tendency of old-school engineering leadership, when dealing with probabilistic software, is to slap reliability metrics on top of it. Good old SLO, like before. It's muscle memory, a reflex. But it creates perverse incentives: to achieve the reliability goal of 100%, classical engineering leadership naturally start adding more and more rails and constraints around the model, trying to reign it in and control it.</p><p>This doesn‚Äôt work anymore.</p><p>The more you try to control the model, the more you‚Äôll nerf it, ultimately damaging the product itself. <strong>Past a certain point, intelligence and control start becoming opposing needs.</strong></p><p>The goal isn‚Äôt perfection: by definition you can‚Äôt nor should aim for it. <strong>The goal is to manage the uncertainty</strong>. As an AI product builder, you should determine what‚Äôs the acceptable unpredictability that keeps the model capable of dealing with complexity, given the market you‚Äôre operating in and the audience you‚Äôre serving. Think in terms of Minimum Viable Intelligence: the lowest quality threshold that is both accepted by the market (some may be more sensitive than others), while preserving its inherent flexibility and generalization capacity.</p><p>The notion of Minimum Viable Intelligence stems from the emergent properties of these models. Companies building AI products may not know how powerful their product  is, and may inadvertently constrain the model too much when trying to keep it on its rails. Think of how frustrating it would be for a user if asking Claude Code to add a watermark to each image in a folder would result in ‚Äúsorry I can‚Äôt help with that: I can only make websites‚Äù. Because we know it can! If you do keep a healthy balance, your product could be so much larger than what you originally envisioned, thanks to the surprising things the models can do .</p><p>This is what forces a transition from engineering to empiricism: what will determine the correct way of building the product itself, and not just typical product management A/B testing, is now the scientific method.</p><p><strong>It takes a scientist to build AI products.</strong></p><p>The old wisdom of always building incrementally on top of what‚Äôs already there doesn‚Äôt hold up anymore. If anything, that too is actively harmful. Every time a new model drops, be it a new generation of an existing one (say, from Sonnet to Opus), or a completely new one (say swapping GPT for Gemini), all previous assumptions about its behavior should be disregarded.</p><p>In fact, when a new model drops, you should even consider literally <em>tearing down the full system</em>, and building it back from the ground up. There are no sacred cows.</p><p>When Replit moved from Sonnet 3.5 to 3.7, Replit‚Äôs President Michele had the company rewrite the  in less than 3 weeks. We called it Replit v2, but that was quite an understatement. In reality, it was a brand new product. The architecture, data model, prompting technique, context management, streaming design‚Ä¶ it was all new. 3.7 was highly agentic in an entirely novel way, Michele understood it, and decided to lean into it instead of trying to control it. The team had to go through weeks of sleepless nights trying to beat the competition to market it successfully. Can you imagine what it takes to completely re-architect a product that was making almost $20m ARR at the time, rebuild it from the ground up in just three weeks, and to see its revenue inflect and end up growing to $100m ARR less than a quarter later? . It‚Äôs no coincidence that Michele is, indeed, a scientist by trade.</p><p>Swapping models at the app layer is a big deal ‚Äî which makes frontier labs stickier than they may look from the outside. It‚Äôs not ‚Äújust an API commodity‚Äù. These models are not flat interfaces: they have personalities, and quirks. That‚Äôs why Gemini Pro 2.5 was not a Claude 3.5 killer despite still being an exceptionally good model. It takes hard work to fully prove that any given model is superior to any other one.</p><p>Every model update doesn‚Äôt necessarily mean a complete rewrite every time, but it does force you to fundamentally rethink your assumptions each time, making a rewrite a perfectly plausible hypothesis. You have to follow an empirical method, where the only valid assumption is ‚ÄúI don‚Äôt know‚Äù. Being an empiricist first is diametrically opposed to being an engineer first.</p><p>Even 'simple' improvements on the same base model require specialized data work. Any feature that ships needs to be tested, both in lab conditions using synthetic evals and in production with real-world usage. The test needs to be rigorous and thorough: it can't simply be a collection of binary \"pass / not pass\". What if a particular prompt change meaningfully impacts the model tendency to prefer a certain tool over another, actively altering the unit of economics for a certain segment of users? What if a particular design change impacts how users think about the input distribution, which in turn shapes the model output distribution in unforeseen ways?</p><p>The need for statistics when testing seemingly-atomic software improvements defies classical programmers' intuition. Many engineers consider this grueling data work not part of the job description, and they may be right!</p><p>It‚Äôs the job description that has changed.</p><h2>IV. Data is the New Operating System</h2><p>Despite the fact that model behavior is intrinsically unknowable and uncertain, figuring out how to build an effective data function around it is incredibly difficult. Models‚Äô emergent properties make synthetic testing elusive.</p><p>Engineers need to keep the eval dataset up to date with the distribution of actual user behavior, but by definition such dataset will constantly lag behind. Since the input space is no longer limited, you can't simply write a few selected tests and follow test-driven development: you'll risk breaking critical features without even being aware of their existence. Tweaking the prompt or swapping the base model for Replit meant unlocking latent features we never initially thought of, like game development. Having a good system in place to constantly sample the right test cases from real-world past usage is one of the most critical new pieces of work required today to build great AI products.</p><p>That‚Äôs also the reason why testing in production, with traditional A/B tests, is also critical: this way you can make sure to stay as close as possible to the general population you‚Äôre serving, and have a higher chance to test a long-tail outcome. Production testing, however, is potentially even harder than evals testing.</p><p>The elephant in the room when it comes to real-world live A/B testing is that it assumes you know what to optimize for in the first place. It implies knowing and quantifying the definition of success. In AI products, you basically can‚Äôt.</p><p>Users are exploring fields of possibilities, navigating through space composing trajectories: it‚Äôs really really hard to understand whether your product is accomplishing what it‚Äôs set to do! Say you just shipped a great new feature for the Replit agent: how do you know whether users are making ‚Äúbetter software‚Äù? Longer chains of messages? Maybe they‚Äôre just debugging in frustration. Shorter and more efficient messages? Maybe they‚Äôre giving up faster. Sure you can measure long-term retention, but you can‚Äôt afford to wait weeks (or, worse, months!) to ship features.</p><p>This is what makes high-velocity AI shipping so challenging. Yet not impossible. Fundamentally it‚Äôs about moving from traditional growth funnels, to finding ways of aggregating ‚Äúuser trajectories‚Äù ‚Äî paths through the field of possible tasks and model states.</p><p>The easiest way of approaching it is by segmenting user inputs. You use smaller models to classify user requests to larger models, which allows you to segment your data in ‚Äúregions of usage‚Äù. It‚Äôs a crude way of clustering user journeys. For Replit‚Äôs coding agent, this could be coding use cases: ‚Äúwhat‚Äôs the likelihood of getting a positive message from the user after 3 chat interactions, for all users that submitted a prompt about React web apps?‚Äù To push things further, you can use the same approach to define milestones to achieve across different paths, which might mean classifying model internal states.</p><p>This clearly impacts product management, design, go-to-market, and even (especially!) finance. As features become emergent, binary analytics events are no longer as useful as before to understand user behavior. Knowing that users acquired through TikTok are more likely to build games, which are more expensive to generate on a per-token basis and therefore impact the margin calculus, is  valuable across the entire company: from engineers making sure that games are efficiently generated, to marketers shifting their top-of-the-funnel strategy to a more sustainable channel, to the finance team appropriately segmenting their CAC and LTV analysis. A 20% shift from game-building users to professional web apps might mean the difference between sustainable unit economics and bleeding money on every free user ‚Äî yet this insight only emerges from analyzing the actual content of AI interactions, not traditional funnel metrics. That‚Äôs why classifying states is so crucial.</p><p>It all boils down to data. The value is being generated by the model, and data lives upstream and downstream of the model. We have years of literature about upstream data (for training), and the industry is keenly aware of its importance. But downstream data is something new, because we had true emergence at global scale for only a couple of years. Such scale is what makes the problem expensive, hard, complex, and requiring heavy data engineering. Architecting an AI product is no small feat, an increasingly sophisticated cross-disciplinary art.</p><p>More and more, in an era of stochastic unpredictable behaviors, data is becoming a crucial differentiating point when determining the success of an enterprise. It‚Äôs the shared operating system, longitudinal to the entire organization: a shared language and context that can describe reality and prescribe actions to shape it.</p><p>None of the core components of a tech company can afford to work in silos anymore. Things like customer attribution (in marketing, sales), observability (engineering), and A/B testing (product, design) used to be separate. In AI products, they collapse into one holistic system view where the core behavior of the product influences both the top of the funnel as well as the bottom line, from conversions to retention.</p><p>Only data can provide the map to understand this new, unknown function , and describe the journeys that users take when exploring and meandering through the emergent properties of AI products. Only data can inform where they‚Äôre going, whether they‚Äôre successful in reaching their destination, whether they can afford to get there.</p><p><strong>Data is not just the new oil to train AI models.</strong></p><p><strong>It‚Äôs also how we can truly harness its power.</strong></p><h2>V. This Time is Different</h2><p>After decades of technical innovation, the world has (rightfully) developed some anti-bodies to tech hype. Mainstream audiences have become naturally skeptical of big claims of ‚Äúthe world is changing‚Äù. There‚Äôs now even a popular meme: ‚Äúnothing ever happens‚Äù.</p><p>I strongly believe that when it comes to AI, something  happening. This time it  feel different.</p><p>It's ontologically different. We're moving away from deterministic mechanicism, a world of perfect information and perfect knowledge, and walking into one made of emergent unknown behaviors, where instead of planning and engineering we observe and hypothesize.</p><p>The shift is real, and it affects every part of the tech industry, altering how we make products, how we study and design them, and how we structure work around them. Organizations that build using an empirical approach, think in probabilities, and measure complex trajectories will define the next era of technology. The rest will keep trying to squeeze wave functions into spreadsheets, wondering why their perfectly deterministic dashboards can't capture what makes their products magical.</p><p>It‚Äôs a new world, a world of wonder and possibilities, a world to discover and understand.</p><p>Welcome to the Probabilistic Era.</p>","contentLength":27214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44976468"},{"title":"The contrarian physics podcast subculture","url":"https://timothynguyen.org/2025/08/21/physics-grifters-eric-weinstein-sabine-hossenfelder-and-a-crisis-of-credibility/","date":1755796436,"author":"Emerson1","guid":235855,"unread":true,"content":"<p><em>This is the story of how a circle of popular science communicators, who built their brands on championing free inquiry, worked to suppress scientific critique. Of how Eric Weinstein, the man who condemns the scientific community for suppressing his and his family‚Äôs work, nearly succeeded in cancelling me through intimidation and false threats. And of how Sabine Hossenfelder spins the truth for the sake of audience capture and podcast hosts Brian Keating and Curt Jaimungal prioritize tribe loyalty over the scientific process. In revealing personal details I have kept private for years, this account shows the lengths to which the individuals involved have gone in order to deceive the public.&nbsp;</em></p><p>Science communication, at its best, serves a noble purpose: to act as a bridge between the intricate, often intimidating world of scientific research and the public‚Äôs curiosity. Skilled communicators translate complexity into clarity, demystify the scientific process, and inspire a shared sense of wonder. Yet a growing and troubling trend has emerged: the rise of the contrarian science communicator. These are not easily dismissed cranks. They are skilled performers who blend legitimate science with dubious claims, making it hard to separate the valuable from the misleading.</p><p>One of the most prominent examples is the contrarian physics subculture centered around Eric Weinstein, which includes Sabine Hossenfelder, Brian Keating, and Curt Jaimungal. These figures command millions of followers across social media and have built their reputations by tackling charged topics in physics, such as the validity of string theory or the claim that theoretical physics faces a crisis. Their YouTube channels feature long, thoughtful discussions with leading physicists like Roger Penrose and Leonard Susskind, and both Hossenfelder and Keating are professional physicists with undeniable expertise in their respective areas. Taken at face value, their content and profiles thus suggest that they are doing a valuable service in making science accessible and entertaining to the public.</p><p>But this engagement with legitimate science conceals a concerted effort to suppress criticism and mislead the public. The deception exposes the central problem facing these prominent science communicators: they are willing to trade scientific integrity for audience capture and tribal loyalty. A prime example of this dilemma is that of Weinstein‚Äôs so-called ‚ÄúGeometric Unity‚Äù (GU), a proposed theory of everything first unveiled in 2013 and revived in the 2020s through podcast appearances. Despite its lack of seriousness as a scientific theory, GU continues to be entertained by Hossenfelder, Keating, and Jaimungal. As an author of the first scientifically-detailed rebuttal of GU, I have directly witnessed how this contrarian cohort reacts when their ideas or allies face substantive criticism. It lays bare their gross hypocrisy of claiming to be champions for unorthodox views while working hard to ignore or suppress challenges to one of their own. This post is a firsthand account of their campaign to silence dissent.</p><p> In what follows, I will discuss Geometric Unity as if it were unambiguously known to be unserious and flawed. For those uncertain or new to the subject, this will be justified later in the section ‚ÄúThe Jury Is Already In‚Äù.</p><p>Eric Weinstein wears many hats. With a PhD in mathematics from Harvard, he has worked as a managing director of Thiel Capital, founded the ‚ÄúIntellectual Dark Web,‚Äù and regularly comments on a wide range of topics on popular podcasts. Crucially however, Weinstein sits squarely outside the scientific establishment, having left academia a few years after completing his doctorate in the early 1990s with only a single published and forgotten paper. His Geometric Unity proposal, therefore, has all the hallmarks of an outsider attempting to revolutionize physics, casting him as an Einstein-like figure toiling alone at the patent office.</p><p>But the noteworthy aspect of GU is not its scientific merit or lack thereof. Rather, it is how GU ties into Eric Weinstein‚Äôs narrative of being an outcast decrying the profound failures of our institutions. Indeed, Weinstein has built his public persona around the concept of a ‚ÄúDistributed Idea Suppression Complex‚Äù (<a href=\"https://youtu.be/QxnkGymKuuI?si=BVtwZSG_22tGD2Jy\">DISC</a>), an alleged establishment in academia and science that marginalizes or silences brilliant outsiders with revolutionary ideas. Within this framework, GU is presented as a transformative proposal that powerful institutions are too fearful to engage with. This creates a self-reinforcing loop: when physicists ignore the work, it confirms that the DISC is real, but when they criticize it, they are cast as bad-faith agents protecting their entrenched paradigms. In this way, Geometric Unity functions as a foundation for Weinstein‚Äôs personal brand of scientific and institutional grievances.</p><p>Until April 2021, the only public material on GU was a YouTube <a href=\"https://www.youtube.com/watch?v=Z7rd04KzLcg&amp;pp=ygUeZXJpYyB3ZWluc3RlaW4gZ2VvbWV0cmljIHVuaXR5\">video</a> of Weinstein‚Äôs highly technical 2013 Oxford lecture. That few could follow it allowed Weinstein‚Äôs grievances to go unchecked. The situation changed in February 2021 when a detailed scientific <a href=\"https://files.timothynguyen.org/geometric_unity.pdf\">rebuttal</a>, authored by myself and Theo Polya, was released. My critique directly tested Weinstein‚Äôs narrative: how would he respond to the thorough feedback he long claimed to want, which was free from the institutional and academic conventions he so strongly condemned? The outcome was that instead of engaging with the substance of the critique, Weinstein and his circle deployed a playbook of tactics designed to suppress, deflect, and protect his contrarian brand at all costs.&nbsp;</p><p>When I initially released my response paper to Geometric Unity in 2021, unknown to the public, Weinstein immediately tried to suppress it. Now that four years have passed, and the risk of personal drama overshadowing the scientific legitimacy of the rebuttal is gone, I feel that the time has come to reveal the full story.&nbsp;</p><p>The most direct incident concerns Weinstein‚Äôs attempt to block my podcast <a href=\"https://www.youtube.com/watch?v=o31cGMENDTI\">episode</a> on , a show specializing in physics hosted by two physics graduates. In our episode, I presented a two-hour whiteboard lecture giving a gentle exposition of my paper and a refutation of Weinstein‚Äôs baseless <a href=\"https://youtu.be/8_uiqjO1IEU?si=9uuereikfD_Rv8bW&amp;t=5078\">claim</a> that he had originally discovered what are now known as the Seiberg-Witten equations. Our episode was released on Friday June 18, 2021. The following morning I received a message from on Discord, who wanted to speak to me on the phone. What they revealed was utterly despicable.&nbsp;</p><p>Weinstein had called them earlier and implied that if they kept the video online, there could be ‚Äúlegal action‚Äù. The intimidation worked; took the video down for the better part of that Saturday. In response, I immediately reassured them that the threat was a baseless bluff: what would be the basis for legal action? The <a href=\"https://timothynguyen.org/wp-content/uploads/2025/08/f752e-1mtc4frxmdfsfv5tj3itbrg.webp\">copyright</a> notice on his Geometric Unity paper? Bullshit. After my phone call with , the video went back up. Rumors circulated about Weinstein‚Äôs potential involvement in the disruption, e.g. in a Facebook thread shown below.&nbsp;</p><p>But at the request of the hosts who wanted to avoid public controversy, I never spoke of the situation publicly until now. This is the suppression incident I alluded to in my <a href=\"https://www.youtube.com/watch?v=j86WIfRfPDk&amp;t=6339s\">interview</a> in the following month with Bob Wright, which Bob duly noted in his <a href=\"https://www.nonzero.org/p/is-eric-weinstein-a-crackpot\">retrospective</a> of our conversation.&nbsp;</p><p>In fact, Weinstein‚Äôs suppression efforts began months earlier. Immediately after our paper‚Äôs release, Curt Jaimungal, a friend of Weinstein and host of the popular  podcast, reached out to invite me and my co-author on his show. He proposed a date ‚Äú6-8 weeks‚Äù out, which in hindsight was clearly meant to coordinate with Weinstein‚Äôs own paper release on April 1st. I agreed to the interview, noting my co-author‚Äôs wish for anonymity. Curt was pleased and said he would follow up.</p><p>But the follow-up never came. When I reached out in May, Curt punted, citing a hectic schedule. The truth about the situation came out a month later during the affair. In his call, Weinstein revealed to the hosts that Curt had cancelled his interview with me. It‚Äôs not hard to put two and two together: Weinstein told Curt to call off my interview and then tried to use that cancellation as leverage to discredit me.</p><p>We thus have a disturbing truth. Eric Weinstein, the man who waxes poetic about a Distributed Idea Suppression Complex, is a hypocrite willing to use his own influence to squash criticism. Weinstein‚Äôs grievances and tale of persecution are frequently invoked to serve his narrative, yet when he receives opposition, he is willing to use his own power to suppress others.</p><h2>Attack the Person, Not the Science</h2><p>A month after the release of the GU paper, Weinstein was asked publicly about my critique in a now infamous Clubhouse <a href=\"https://ok.ru/video/2552683563745\">discussion</a>, hosted by Brian Keating, a close friend of Weinstein, distinguished professor of physics at UC San Diego, and host of the popular  podcast. When pressed on scientific details, Weinstein didn‚Äôt address the physics and instead demanded to know the identity of my anonymous co-author Theo Polya. Furthermore, he launched into an incoherent word salad of accusations about Theo Polya (and thus by extension myself), invoking references to misogyny, rape jokes, and 4chan. To borrow a phrase of Weinstein‚Äôs during his recent <a href=\"https://youtu.be/5m7LnLgvMnM?si=d-Zr_Hjeq2Ma2VIu&amp;t=2360\">debate</a> with Sean Carroll: ‚ÄúHow dare you Eric‚Äù.&nbsp;</p><p>The takeaway from that cringeworthy session was that Weinstein, together with Keating, refused to engage with our critique because they didn‚Äôt know who Theo Polya was. I feel it is hardly worth stating but I‚Äôll do so anyway: what does the anonymity of one of two authors have to do with the merits of a critique? If Geometric Unity is so visionary, why would it matter? It‚Äôs a classic bait-and-switch. Weinstein, who demands his ideas be judged on merit alone, retreats from the science and falls back on personal attacks as soon as his ideas are actually challenged.&nbsp;</p><p>It is illuminating to see how Sabine Hossenfelder, Brian Keating, and Curt Jaimungal have been continuing to bolster Weinstein over the years, either directly through promoting his ideas when they know better or else indirectly by amplifying ambiguity about the status of his work.</p><p>I already discussed how Curt was complicit in cancelling my interview, which would have been critical of GU. Despite the seriousness he projects by hosting top scientists, he continues to treat GU as a substantive proposal, inviting many guests to give their (uninformed) opinion on GU while giving zero attention to the singularly critical work that is my rebuttal. In fact, he requested a copy of my interview from  when the video was briefly taken down, though he has never made any reference to it whatsoever. Even if one were to suppose Curt had disagreements with the critique, he is someone who platforms people with a diversity of viewpoints, including outsiders like Chris Langan and UAP specialists. That Curt would compromise his intellectual openness and rigor in order to promote and protect Weinstein is a tragic consequence of tribalism.&nbsp;</p><p>A similar and more pronounced story holds for Brian Keating, Weinstein‚Äôs most prominent promoter and staunch defender. Indeed, Brian continually hosts Eric Weinstein on his podcast (over 30 times as of this writing), many of which explicitly promote Geometric Unity. His strong support of Weinstein takes on many forms, including hosting him as a <a href=\"https://youtu.be/AzsZO3_WhDA?si=DqaR4LR-HPj5BOWz&amp;t=3330\">scholar in residence</a> and <a href=\"https://www.youtube.com/watch?v=BVkUya368Es\">speaker</a> at UC San Diego, expressing interest in experimentally <a href=\"https://youtu.be/AzsZO3_WhDA?si=dOd3orolxR5GKS0O&amp;t=3341\">testing</a> GU, and inviting scientists to engage with Weinstein and his work (<a href=\"https://www.youtube.com/watch?v=N_aN8NnoeO0\">here</a>, <a href=\"https://www.youtube.com/watch?v=OI0AZ4Y4Ip4\">here</a>, and <a href=\"https://youtu.be/nhGwJLXzHs8?si=-IWdTLp8LBh5vgoe&amp;t=11638\">here</a>). So if there is anyone else other than Weinstein who has a vested interest in getting to the bottom of my critique of GU, it would be Brian.</p><p>Two weeks after the release of my response paper and shortly after Sabine hosted my blogpost, Theo Polya and I sent the following message to Brian:</p><p>We received no response, not even a reply to ask ‚ÄúWho is Theo Polya?‚Äù, the question used by both Weinstein and Brian in their Clubhouse discussion two months later to deflect engaging with our critique.</p><p>A few months after this email, I was contacted by Brandon Van Dyck shortly after he heard of my appearance on Wright‚Äôs podcast to record a follow-up <a href=\"https://www.youtube.com/watch?v=88E2pp7xafo\">interview</a> (later provocatively titled ‚ÄúThe Eric Weinstein/Timothy Nguyen Affair‚Äù). By pure coincidence, Brian had reached out to Brandon at the same time, expressing an interest in being on Brandon‚Äôs show. Brandon and I took the opportunity to propose to Brian that he could have a conversation with me (either about Weinstein, his book , or any other topic of his choosing). In response, Brian withdrew his self-invitation, saying he was ‚Äúnot interested‚Äù, as we <a href=\"https://www.youtube.com/watch?v=88E2pp7xafo&amp;t=4980s\">discussed</a> in our interview. The dodging continued when Michael Shermer and I publicly invited both Brian and Eric Weinstein for a discussion with me on Michael‚Äôs podcast (<a href=\"https://x.com/IAmTimNguyen/status/1706363756299563078?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1706363756299563078%7Ctwgr%5E18eddeb15fb70f561beaae1b214254eb0cfea785%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.redditmedia.com%2Fmediaembed%2F16ryy7c%2F%3Fresponsive%3Dtrueis_nightmode%3Dfalse\">here</a> and <a href=\"https://x.com/michaelshermer/status/1706455867925602789\">here</a>), after Michael was unsuccessful in asking Brian privately to participate. Again, no response.</p><p>It‚Äôs one thing to be unwilling to speak to critics. Casting aspersions on them to avoid defending your position is another matter entirely. On other podcasts, Brian has claimed I am <a href=\"https://www.youtube.com/live/1VIur9mfS-A?si=fRONBXLeltdR70Hk&amp;t=2449\">not acting in good-faith</a> and that I‚Äôm trying to <a href=\"https://youtu.be/Hz3waSvyUDg?si=xB4XA2EqTyy7rX0u&amp;t=5635\">‚Äúbait‚Äù him</a>, which are just additional examples of how Brian is going after the messenger rather than sticking to the science. The simple truth is that he and Weinstein have refused every opportunity to address the criticisms of GU in order to keep the charade running. Brian‚Äôs criticism of my character is also difficult to process in light of Weinstein‚Äôs attack of me on Clubhouse and the attempted suppression of my work. It is an embarrassing state of affairs to see a distinguished professor of physics so obviously acting in bad-faith.</p><p>Finally, we get to Sabine Hossenfelder, a theoretical physicist with over 1.5 million followers on YouTube who has built a reputation for giving sharp no-nonsense scientific critiques. Her role in the GU affair is more complicated and, in my case, unfortunate. Until recently, Sabine had been dismissive of Weinstein‚Äôs work. However, a recent <a href=\"https://www.youtube.com/watch?v=KiFYcuoK490&amp;vl=en\">video</a> of hers (titled <em>Physicists are afraid of Eric Weinstein ‚Äî and they should be</em>) paints a very different picture. In it, she waffles all over the place about her opinion of Weinstein‚Äôs work and how much time she‚Äôs spent looking at it. She‚Äôs inconsistent with her messaging, <a href=\"https://youtu.be/KiFYcuoK490?si=eEPbqjeYh6xg9VrH&amp;t=83\">saying</a> that ‚Äúshe never looked into [Geometric Unity] in any detail‚Äù but clearly saying the <a href=\"https://youtu.be/mdu9KvLxHFg?si=55RPnltcqOzaMGzI&amp;t=386\">opposite</a> in an older video. It honestly doesn‚Äôt interest me to micro-police how Sabine chooses to express her opinions ‚Äì her statements have all the flair of entertainment and so cannot be taken too literally. The problem is that Sabine appears to be employing ambiguity to present two incompatible positions: on the one hand, she wants to play honest critic on scientific matters (‚ÄúEric‚Äôs theory is a waste of time‚Äù), but she also wants to claim that Weinstein and his bandwagon deserve credit for being contrarian. It‚Äôs this latter step where she‚Äôs acting dishonestly, as I will now explain.</p><p>Rewinding to July of 2021, I reached out to Sabine, whom I had recently <a href=\"https://www.youtube.com/watch?v=QX0nhdHCU60\">interviewed</a> at Google, and I explained to her my frustration with the aforementioned Clubhouse episode (which she had watched). To my delight, she revealed she was speaking to Brian soon and would ask him about me. But when Sabine got back to me a few weeks later over Zoom, I was shocked by the message she brought. Sabine repeated and supported Brian‚Äôs claims that my co-author‚Äôs anonymity was the cause of Brian‚Äôs refusal to speak to me. She recounts that she has had to deal with anonymous trolls herself and that she doesn‚Äôt feel the need to respond to them. When I raised the fact that  wasn‚Äôt anonymous, she said that Brian deflected, noting that GU was Weinstein‚Äôs theory and that it wasn‚Äôt his prerogative to talk to me about it. When asked why he declined going on Brandon‚Äôs podcast to talk to me, his excuse was that it was insulting to be invited only to speak about Weinstein and not himself (contrary to what was offered to Brian and that Brian had in fact invited himself onto Brandon‚Äôs podcast). In summary, I got nowhere with Sabine talking to Brian on my behalf.&nbsp;</p><p>At the time, I owed Sabine a tremendous debt of gratitude for hosting my blogpost in March 2021 that advertised my critique of GU. I had no public profile back then and her platform gave my work the reach and legitimacy it needed to get off the ground. Because of this, I kept Sabine‚Äôs confusing report about Brian to myself. I even <a href=\"https://x.com/IAmTimNguyen/status/1705617321388015865\">met</a> Sabine in person at the HowTheLightGetsIn Festival in London 2023 and found her very agreeable and sincere in person. Nevertheless, I always had the suspicion that Sabine might really have been shielding Brian by gaslighting me. That intuition was confirmed in Sabine‚Äôs recent video in which she claims that Brian ‚Äúdeserves credit for not chickening out and standing for Eric‚Äù and that Curt‚Äôs explainer video of GU was ‚Äúcourageous‚Äù. Such words are baffling given our private conversation and her being qualified to understand the validity of my critique of GU hosted on her own blog. I now clearly see the situation for what it is. Sabine is just as guilty of grifting as Brian and Curt.</p><p>This incident is particularly unfortunate in Sabine‚Äôs case, as she has cultivated a reputation as a respected physics communicator and science writer known for her sharp critiques of theoretical physics. Regardless of one‚Äôs stance with her contrarian views, she has in the past offered many perspectives on the state of theoretical physics worthy of attention, both on her blog and her book . Thus, her most recent video concerning Weinstein reflects poorly on her not only because of the personal circumstances I‚Äôve now disclosed but also due to its strong departure from reason: in it, she also labels the theoretical physics community a ‚Äúf-cking hypocrisy‚Äù and ‚Äúscam‚Äù by equating the quality of their work with that of Weinstein‚Äôs Geometric Unity. This claim is so outrageous that physicist Christian Ferko shortly afterwards made a detailed presentation <a href=\"https://www.youtube.com/watch?v=oipI5TQ54tA\">debunking</a> this absurd false equivalence. It is a sad state of affairs to see Sabine undermine her own credibility by exploiting the sensationalism surrounding Weinstein and GU merely to incite outrage and air her grievances against academic and physics communities. And like Weinstein, she is willing to censor her critics rather than address them.</p><p>Scientific disagreements are intricate matters that require the attention of highly trained experts. However, for laypersons to be able to make up their own minds on such issues, they have to rely on proxies for credibility such as persuasiveness and conviction. This is the vulnerability that contrarians exploit, as they are often skilled in crafting the optics and rhetoric to support their case. Indeed, Weinstein and Hossenfelder‚Äôs strong personalities and their sowing of distrust in institutions enable them to persuade others of the correctness of their views when they deviate from those of experts. Thus, I include this section to show that even if one were to rely on social cues alone, there is in fact no controversy about the illegitimacy of Geometric Unity among those who are close to Weinstein or who are qualified to judge. The success of physics grifters has relied on the fact that they make more noise than those who have quietly moved on.</p><p>Let‚Äôs start with podcasting star Lex Fridman. Word got to Lex of my paper with Theo Polya when it was released and all three of us got onto a video call in March 2021 to discuss the situation. Lex proposed hosting us alongside Weinstein for a discussion of Geometric Unity on his podcast and we all agreed (Theo Polya was willing to reveal himself given his affinity for Lex‚Äôs podcast). The only question is whether Weinstein, who had already been on Lex‚Äôs show four times, would agree.&nbsp;</p><p>Two weeks later, when I got in touch with Lex via email, he disappointingly changed topics and said he did not discuss what we had proposed with Weinstein (or Joe Rogan). In hindsight, this was clearly not the case. Weinstein released his Geometric Unity paper on April 1, debuting it on Joe Rogan‚Äôs podcast and then on Brian Keating‚Äôs podcast the following day. Conspicuously, Weinstein did not appear on Lex‚Äôs podcast. On Rogan‚Äôs podcast, Rogan‚Äôs skepticism and pushback was full-on with his interview with Weinstein, noting ‚Äúthere has been some criticism‚Äù and not letting Weinstein off the hook from his obscurantism. Rogan certainly hadn‚Äôt read Sabine‚Äôs blog to become aware of my critique; Lex had tipped him off. Four years later, Weinstein has not returned to Lex‚Äôs show.</p><p>Marcus is the Oxford mathematician who hosted Weinstein‚Äôs GU talk and who knows Weinstein well from their overlapping time in academia. I first met Marcus virtually in 2022 when I hosted him for a <a href=\"https://youtu.be/1wJ6LMqPm9I?si=WPAb8ekHlDDoxjPi&amp;t=3305\">talk</a> at Google. During the Q&amp;A of that conversation, I asked Marcus, who has been conspicuously absent from the Geometric Unity saga since 2013, about Weinstein and his work (it was a carefully thought-out question as Marcus was initially unwilling to field a question about Weinstein during our preparation). Marcus‚Äôs reply shows a man distancing himself from Geometric Unity, stating he‚Äôs less qualified than others to assess it, a sharp U-turn from his glowing <a href=\"https://www.theguardian.com/science/blog/2013/may/23/roll-over-einstein-meet-weinstein#:~:text=What%20are%20we%20to%20make,most%20intractable%20problems%20in%20physics%3F&amp;text=There%20are%20a%20lot%20of,universe%20is%20missing%2C%20for%20example.\">comparison</a> of Weinstein with Einstein in 2013. More recently, I also met Marcus in person at the <a href=\"https://www.eafestival.com/2025-ea-festival-lineup\">2025 East Anglia Festival</a>. Over lunch with Marcus and his wife, Weinstein‚Äôs name briefly came up, but Marcus offered no positive comments about him or his ideas, and our conversation continued.</p><h2>Economists, Computer Scientists, and Physicists</h2><p>Despite advocating against established institutions and credentialism, Eric Weinstein readily leverages visits to prestigious institutions to enhance his public image. One notable instance was his November 2021 <a href=\"https://x.com/EricRWeinstein/status/1459180900789501960\">visit</a> to UChicago, where he presented <a href=\"https://web.archive.org/web/20211111031144/https://economics.uchicago.edu/sites/economics.uchicago.edu/files/Welfare_Chicago_Draft.pdf\">work</a> co-authored with his wife, Pia Malaney, applying gauge theory to economics. Unlike his Geometric Unity work, which lacked sufficient detail to merit serious attention (consisting only of a YouTube lecture and an inadequately written paper), this UChicago presentation was accompanied by a well-formatted paper containing significant technical details. This allowed me to quickly analyze the paper, write a <a href=\"https://arxiv.org/abs/2112.03460\">rebuttal</a> (this time without an anonymous co-author), and upload it to arXiv.</p><p>Note that before this incident, the Malaney-Weinstein work received little attention due to its limited significance and impact. Despite this, Weinstein has <a href=\"https://youtu.be/QxnkGymKuuI?t=3665\">suggested</a> that it is worthy of a Nobel prize and <a href=\"https://www.youtube.com/watch?v=uFirZANoiHI\">claimed</a> (with the support of Brian Keating) that it is ‚Äúthe most deep insight in mathematical economics of the last 25-50 years‚Äù. In that same podcast episode, Weinstein also makes the incendiary claim that Juan Maldacena stole such ideas from him and his wife. For those unable to judge the situation, I offer this appreciative reply from one of the economics professors at UChicago who sponsored Eric‚Äôs visit:</p><p>Has Weinstein responded to my economics rebuttal now that the anonymous Theo Polya is no longer an author? You already know the answer.</p><p>And in case you‚Äôre wondering, there‚Äôs also equivalent endorsement of my response to Geometric Unity. For instance, see my <a href=\"https://www.youtube.com/watch?v=wd-0COLM8oc&amp;t=928s\">interview</a> with computer scientist Scott Aaronson. There‚Äôs also Sean Carroll, who after his debate with Weinstein on Piers Morgan, confided to me that he directs everyone who asks him about Weinstein‚Äôs work to the rebuttal I published. Most recently, Christian Ferko has done an excellent job giving his own <a href=\"https://youtu.be/jz7Trp5rTOY?si=kpMFc7wJgefIoXyt&amp;t=1837\">take</a> on the flaws of GU based on the groundwork laid out by my analysis.</p><p>Overall, the consistent theme is that the few professional scientists who have examined Weinstein‚Äôs work and are not influenced by audience capture have supported the critique I put forth. The fact that most scientists have ignored GU says less about a failing within the scientific establishment and more about the group of contrarians who continue to entertain Weinstein‚Äôs ideas.</p><p>When I wrote my critique of Geometric Unity, I thought I was simply engaging in math and physics. I never imagined it would take me on a journey of hypocrisy and censorship. To dismiss this story as mere internet drama is to overlook the troubling reality underneath it all: Eric Weinstein and several of our most prominent science communicators ‚Äì nay, science  ‚Äì are willing to distort the truth to suit their own interests. Many eyes and ears tune into Hossenfelder, Keating, and Jaimungal, who frequently appear alongside distinguished scientists that are likely unaware of their involvement in the grift. While these popularizers are able to fulfill their audience‚Äôs needs to understand science, they simultaneously enable them to hold unconventional views that may contradict the very science they promote. That the three of them have done valuable work in making science accessible to the public is precisely what makes their conduct disconcerting.</p><p>As a former fan of Weinstein who became a critic, I wrote my rebuttal to Geometric Unity expecting a scientific debate. Instead, I received a grim lesson about the state of modern science communication: when personal brands and tribal loyalties become the main focus, scientific integrity is sacrificed. The unfortunate truth is that some of the most visible voices in science are more interested in being celebrated than in being correct. And in a world where public trust in expertise is already in peril, that is a betrayal we simply cannot afford.</p><p><em>Acknowledgements: I would like to thank Ieva Cepaite, Richard Easther, Daniel Gilbert, Chris Kavanaugh, and Tim Scarfe for their valuable feedback on earlier drafts of this post.</em></p>","contentLength":25951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44975378"},{"title":"The Core of Rust","url":"https://jyn.dev/the-core-of-rust/","date":1755793642,"author":"zdw","guid":235881,"unread":true,"content":"<p><strong>NOTE: this is not a rust tutorial.</strong></p><blockquote><p>Every year it was an incredible challenge to fit teaching Rust into lectures since you basically need all the concepts right from the start to understand a lot of programs. I never knew how to order things. The flip side was that usually when you understand all the basic components in play lots of it just fits together. i.e. there's some point where the interwovenness turns from a barrier into something incredibly valuable and helpful.\n‚Äî<a href=\"https://donsz.nl/\">Jana D√∂nszelmann</a></p></blockquote><p>One thing I admire in a language is a strong vision. <a href=\"https://www.uiua.org/\">Uiua</a>, for example, has a very strong vision: what does it take to eliminate all local named variables from a language? <a href=\"https://ziglang.org/\">Zig</a> similarly has a strong vision: explicit, simple language features, easy to cross compile, drop-in replacement for C.</p><p>Note that you don‚Äôt have to agree with a language‚Äôs vision to note that it  one. I expect most people to find Uiua unpleasant to program in. That‚Äôs fine. You are not the target audience.</p><p>There‚Äôs a famous quote by Bjarne Strousup that goes ‚ÄúWithin C++, there is a much smaller and cleaner language struggling to get out.‚Äù Within Rust, too, there is a much smaller and cleaner language struggling to get out: one with a clear vision, goals, focus. One that is coherent, because its features . This post is about that language.</p><h2>Learning Rust requires learning many things at once<a href=\"https://jyn.dev/the-core-of-rust/#learning-rust-requires-learning-many-things-at-once\" aria-label=\"Anchor link for: learning-rust-requires-learning-many-things-at-once\"></a></h2><p>Rust is hard to learn. Not for lack of trying‚Äîmany, many people have spent person-years on improving the diagnostics, documentation, and APIs‚Äîbut because it‚Äôs complex. When people first learn the language, they are learning many different interleaving concepts:</p><ul></ul><p>These concepts interlock. It is very hard to learn them one at a time because they interact with each other, and each affects the design of the others. Additionally, the standard library uses all of them heavily.</p><p>Let‚Äôs look at a Rust program that does something non-trivial:</p><pre data-lang=\"rust\"><code data-lang=\"rust\"></code></pre><p>I tried to make this program as simple as possible: I used only the simplest iterator combinators, I don't touch  at all, I don't use async, and I don't do any complicated error handling.</p><p>Already, this program has many interleaving concepts. I'll ignore the module system and macros, which are mostly independent of the rest of the language. To understand this program, you need to know that:</p><ul><li> and  take a function as an argument. In our program, that function is constructed inline as an anonymous function (closure).</li><li>Errors are handled using something called , not with exceptions or error codes. I happened to use  and , but you would still need to understand Result even without that, because Rust does not let you access the value inside unless you check for an error condition first.</li><li>Result takes a generic error; in our case, .</li><li>Result is an data-holding enum that can be either Ok or Err, and you can check which variant it is using pattern matching.</li><li>Iterators can be traversed either with a  loop or with .  is eager and  is lazy.  has different ownership semantics than .</li></ul><p>If you want to modify this program, you need to know some additional things:</p><ul><li> can only print things that implement the traits  or . As a result, s cannot be printed directly.</li><li> returns a struct that borrows from the path. Sending it to another thread (e.g. through a channel) won't work, because  goes out of scope when the closure passed to  finishes running. You need to convert it to an owned value or pass  as a whole.\n<ul><li>As an aside, this kind of thing encourages people to break work into \"large\" chunks instead of \"small\" chunks, which I think is often good for performance in CPU-bound programs, although as always it depends.</li></ul></li><li> only accepts functions that are . Small changes to this program, such as passing the current path into the closure, will give a compile error related to ownership. Fixing it requires learning the  keyword, knowing that closures borrow their arguments by default, and the meaning of .\n<ul><li>If you are using , which is often recommended for beginners, your program will need to be rewritten from scratch (either to use Arc/Mutex or to use exterior mutability). For example, if you wanted to print changes from the main thread instead of worker threads to avoid interleaving output, you couldn't simply push to the end of an  collection, you would have to use  in order to communicate between threads.</li></ul></li></ul><p>This is a  of concepts for a 20 line program. For comparison, here is an equivalent javascript program:</p><pre data-lang=\"javascript\"><code data-lang=\"javascript\"></code></pre><p>For this JS program, you need to understand:</p><ul></ul><p>I'm cheating a little here because  returns a list of paths and  doesn't. But only a little.</p><p>My point is not that JS is a simpler language; that's debatable. My point is that you can do things in JS without understanding the whole language. It's very hard to do non-trivial things in Rust without understanding the whole core.</p><h2>Rust's core is interwoven on purpose<a href=\"https://jyn.dev/the-core-of-rust/#rust-s-core-is-interwoven-on-purpose\" aria-label=\"Anchor link for: rust-s-core-is-interwoven-on-purpose\"></a></h2><p>The previous section makes it out to seem like I'm saying all these concepts are bad. I'm not. Rather the opposite, actually. Because these language features were designed in tandem, they interplay very nicely:</p><ul><li> and s are impossible to implement without generics (or duck-typing, which I think of as type-erased generics)</li><li>/, and the preconditions to , are impossible to encode without traits‚Äîand this often comes up in other languages, for example printing a function in clojure shows something like <code>#object[clojure.core$map 0x2e7de98a \"clojure.core$map@2e7de98a\"]</code>. In Rust it gives a compile error unless you opt-in with Debug.</li><li> /  are only possible to enforce because the borrow checker does capture analysis for closures. Java, which is  committed to thread-safety by the standards of most languages, cannot verify this at compile time and so has to <a href=\"https://docs.oracle.com/en/java/javase/24/docs/api/java.base/java/text/SimpleDateFormat.html#synchronization\">document synchronization concerns explicitly</a> instead.</li></ul><p>There are more interplays than I can easily describe in a post, and all of them are what make Rust what it is.</p><p>Rust has other excellent language features‚Äîfor example the <a href=\"https://doc.rust-lang.org/nightly/reference/inline-assembly.html\">inline assembly syntax</a> is a work of art, props to <a href=\"https://github.com/amanieu\">Amanieu</a>. But they are not interwoven into the standard library in the same way, and they do not affect the way people  about writing code in the same way.</p><p>without.boats wrote a post in 2019 titled <a href=\"https://without.boats/blog/notes-on-a-smaller-rust/\">\"Notes on a smaller Rust\"</a> (and a follow-up <a href=\"https://without.boats/blog/revisiting-a-smaller-rust/\">revisiting</a> it). In a manner of speaking, that smaller Rust  the language I fell in love with when I first learned it in 2018. Rust is a lot bigger today, in many ways, and the smaller Rust is just a nostalgic rose-tinted memory. But I think it's worth studying as an example of how well <a href=\"https://en.wikipedia.org/wiki/Orthogonality#Computer_science\">orthogonal</a> features can compose when they're designed as one cohesive whole.</p>","contentLength":6546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44974688"},{"title":"Bank forced to rehire workers after lying about chatbot productivity, union says","url":"https://arstechnica.com/tech-policy/2025/08/bank-forced-to-rehire-workers-after-lying-about-chatbot-productivity-union-says/","date":1755791935,"author":"ndsipa_pomu","guid":235828,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44974365"},{"title":"I forced every engineer to take sales calls and they rewrote our platform","url":"https://old.reddit.com/r/Entrepreneur/comments/1mw5yfg/forced_every_engineer_to_take_sales_calls_they/","date":1755791176,"author":"bilsbie","guid":235827,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44974230"},{"title":"Unmasking the Privacy Risks of Apple Intelligence","url":"https://www.lumia.security/blog/applestorm","date":1755790611,"author":"mroi","guid":235880,"unread":true,"content":"<ul role=\"list\"><li>Lumia‚Äôs Research Team revealed that messages dictated via Siri, including WhatsApp and iMessage are not sent to the Private Cloud Compute. In fact, there is no assurance as to what Apple does with these messages.</li><li>Siri transmits metadata about installed and active apps without the user‚Äôs ability to control these privacy settings.</li><li>Audio playback metadata such as ‚Äòrecording names‚Äô, is sent without consent. No user control or visibility exists over these background data flows.</li><li>Apple uses two distinct privacy policies (Siri vs. Apple Intelligence), meaning similar queries may fall under different data-handling rules.</li></ul><p>We reveal AppleStorm, our investigation into how Apple AI‚Äôs eco-system quietly transmits messages (WhatsApp, iMessage) sent via Siri to Apple servers, even when it isn‚Äôt needed to complete the task. This happens without the user having any control whatsoever over when and what can be sent.Also more data than messages is sent to Siri‚Äôs servers. Let‚Äôs deep dive.</p><p>Lately, Apple‚Äôs AI has been making headlines. From promising robust security measures to developing localized models that process data directly on devices, Apple has positioned itself as a champion of privacy and productivity.</p><p>How safe are these innovations? Despite the numerous advancements, recent news has highlighted critical concerns about Apple‚Äôs AI suite. A lawsuit regarding <a href=\"https://www.forbes.com/sites/kateoflahertyuk/2025/01/06/apple-siri-eavesdropping-payout-heres-whos-eligible-and-how-to-claim/\" target=\"_blank\">Siri eavesdropping,</a> settled last January, raises questions about user privacy. More recently, allegations surfaced that Apple Intelligence generated <a href=\"https://www.bbc.com/news/articles/cq5ggew08eyo\" target=\"_blank\">false notifications,</a> including a summary of BBC news with inaccurate information on behalf of the BBC.</p><p>When Apple launched Apple Intelligence which relied on on-device and cloud models <a href=\"https://security.apple.com/blog/private-cloud-compute/\" target=\"_blank\">(Private Cloud Compute)</a> they introduced a variety of AI tools such as Writing Tools, Image Background and Siri is even more powerful with the capabilities of Apple Intelligence.</p><p>Before technologies like these get out of control, it's crucial to identify any potential security loopholes we might be overlooking. This blog explores privacy risks and unusual behaviors discovered regarding Siri and intersection of Siri and Apple Intelligence.</p><p>Apple Intelligence relies on a <a href=\"https://machinelearning.apple.com/research/introducing-apple-foundation-models\" target=\"_blank\">hybrid infrastructure</a> that combines  with <strong>Apple-managed cloud servers</strong> to provide AI-driven features while maintaining strong privacy protections. The cloud component primarily operates through <strong>Private Cloud Compute (PCC).</strong></p><p>But Siri also interacts with other Apple‚Äôs server infrastructures that are outside of the Apple Intelligence‚Äôs PCC.</p><ol start=\"\" role=\"list\"><li>Private Cloud Compute (PCC)Private Cloud Compute is Apple's secure AI processing framework that extends Apple Intelligence beyond the capabilities of on-device computing.‚Ä®<p>PCC Server domains: apple-relay.cloudflare.com, apple-relay.fastly-edge.com, cp4.cloudflare.com‚Ä®Apple‚Äôs Private Cloud Compute architecture.</p></li><li>Siri‚Äôs Dictation ServersSiri works closely with dictation servers which appear to handle voice processing tasks.<p>Siri‚Äôs Dication domain: guzzoni.apple.com</p></li><li>Siri‚Äôs Search ServicesSiri integrates with Apple‚Äôs search infrastructure to deliver relevant results across devices, like Spotlight Search, Safari Smart Search, News, and Music.<p>Siri‚Äôs Search domain: *.smoot.apple.com</p></li><li>Use Case-Driven CollectionThe services to communicate with Apple Intelligence‚Äôs Extensions. Today, only ChatGPT is supported‚Ä®Apple Intelligence‚Äôs Extensions domain: apple-relay.apple.com</li></ol><h2>Creating a Research Environment</h2><p><strong>Operating System &amp; Platform</strong></p><ul role=\"list\"><li>Apple Intelligence enabled</li></ul><ol start=\"\" role=\"list\"><li>mitmproxy ‚Äì An interactive HTTPS proxy.</li><li>Frida ‚Äì A dynamic instrumentation toolkit.</li></ol><h2>What is the Weather Today?</h2><p>To investigate, the open-source proxy tool  was used to intercept Siri‚Äôs network traffic. Initially, simple prompts such as \"Hello\", \"What can you do?\" and \"What is the time right now?\" were tested, but no network activity was observed, suggesting that these queries could be handled locally without requiring external communication.</p><p>Since some queries might be handled locally, it was necessary to issue a prompt that would require external evaluation. Testing the query <strong>\"What is the weather today?\" </strong>resulted in the transmission of two packets.</p><p>The first packet was directed to  identified as Apple's dictation server, but we can‚Äôt see any content.</p><p>The second packet, sent to <strong>api-glb-aeun1a.smoot.apple.com,</strong> the search service for Siri. Let‚Äôs start with this one.</p><p>After extracting and decompressing the frame, the  Python library was used to parse the ProtoBuf data. Without access to the original.proto files used to compile the data, the analysis required a best-effort approach to interpret the content. </p><p>Let‚Äôs see what we can find.</p><p>The first app listed was the official Apple Weather app, which was expected. However, the second one raised questions‚Äîit appeared to be related to  a virtual desktop Mac application that enables running the Windows operating system. A review of the Parallels settings on the device, including the applications running within the virtual machine, showed the following information:</p><p>This turned out to be the ID of the official Windows Weather app installed on the virtual machine.</p><p>Based on this observation, it appears that <strong>Siri scans the device for apps related to the prompt‚Äôs topic.</strong> To test this theory, a question about emails was entered, and the packet revealed a list of all email clients installed on the device.</p><p>So, it‚Äôs confirmed ‚Äî <strong>Siri actively searches for apps on the device that match the topic of the prompt, and reports them back to Apple.</strong></p><p>A question arises: how does Siri determine the weather without the location being mentioned in the prompt? A review of the ProtoBuf content revealed two numbers next to the apps list that appear to be coordinates.</p><p>A quick Google search confirmed this theory, showing the cafe I was working from.</p><p>According to <a href=\"https://www.apple.com/legal/privacy/data/en/ask-siri-dictation/\">Siri‚Äôs Privacy Policy</a> (Notice it's Siri's privacy policy and not Apple Intelligence. More on that soon), they specifically mention this behavior. In fact, it is possible to disable location sharing information with Siri, but if you do consent to location sharing, <strong>your location will be appended to every request</strong> (regardless of the necessity).</p><blockquote>Once again, personal information is being leaked. This raises the question of what else Apple knows about our habits.</blockquote><p>During the investigation of the weather request packet, the following content was encountered:</p><p>Initially, the appearance of a confidential document title from the Notion app in Siri‚Äôs traffic was surprising.</p><p>Once again, <strong>personal information is being leaked.</strong> This raises the question of what else Apple knows about our habits. We still have other traffic that we cannot see. Maybe Apple uses some pinning mechanism‚Ä¶</p><p><strong>What is Certificate Pinning</strong></p><p>is an additional security measure used to protect against <strong>machine-in-the-middle (MITM) attacks</strong> and fraudulent certificates. Instead of relying solely on the Certificate Authority system, pinning allows a website or application to specify which exact certificate or public key should be trusted when connecting to a particular domain. This prevents attackers from tricking users into accepting a malicious certificate issued by a compromised or unauthorized CA.</p><p><strong>Bypassing the Certificate Pinning</strong></p><p>Under macOS Sequoia‚Äôs SSL pinning mechanism, each SSL context uses a dedicated verification routine. To bypass the pinning, we could modify the behavior to skip the verification. To achieve this, we must alter Apple‚Äôs built-in verification logic by hooking the relevant symbols in Siri‚Äôs process - assistantd.</p><p>Using Frida, we created <a href=\"https://github.com/LumiaSecurity/mitmproxy-ace/blob/main/disable_certificate_pinning.js\">this</a> script to disable the pinning.</p><p>Among the recognizable strings in the ACE packet, many correspond to familiar application names, clearly indicating that these are the apps currently active on the device.</p><p>So, it‚Äôs cleared ‚Äî <strong>Siri actively looks for open apps on the device, and reports them back to Apple‚Äôs servers.</strong></p><h3>End-to-End Encryption? I‚Äôm Not Sure</h3><p>One of the main features of Siri is the ability to interact with different apps on the device, like summarizing notes, composing emails and more.</p><p>Siri has the functionality to send messages to contacts in WhatsApp. For example, you can ask Siri, by voice or text (Using Apple Intelligence), ‚ÄúSend the message Good morning to Bob‚Äù and it will send the message ‚ÄúGood morning‚Äù to a contact named Bob.</p><p>While recording the traffic of Siri while sending a message with Siri, a single, pinned packet is generated. Extracting it shows that it contains a base64 string. Decoding it revealed the following:</p><p>Siri indeed  the content of the WhatsApp message, the recipient phone number and other identifiers back to Apple‚Äôs servers.</p><p>This raises a significant concern about WhatsApp‚Äôs end-to-end encryption, as the messages are apparently leaving the device even though they were sent from the device itself.</p><p>Initially, I suspected this might be due to the Siri and Apple Intelligence settings that allow it to ‚Äúlearn‚Äù from certain apps; however, the behaviour persists even when not permitting them to learn from WhatsApp.</p><p>However, sending a message to WhatsApp via Siri also works when blocking communication between Siri to Apple‚Äôs servers. The heavy processing already occurred (reasoning the target application is WhatsApp, who the recipient is, and the message to send) so why is it sent externally, and to a server that isn‚Äôt part of Private Cloud Compute? If all data resides locally, why is there any need to send it to their servers?</p><p>So, it‚Äôs clear ‚Äî Siri has access to your WhatsApp messages sent via Siri (but not to messages sent outside of Siri), and reports them back to Apple‚Äôs servers, regardless of whether permission is granted for learning from them.</p><h2>What Happens When It Comes to ChatGPT</h2><p>Disclosure Process and Apple‚Äôs ResponseWe informed Apple of this privacy issue in February 2025. In March 2025, after some back and forth with their security team and sending additional information, Apple acknowledged the issue and said they will be working towards a fix.However, in July 2025, Apple reached out to say that this is actually not a privacy issue related to Apple Intelligence. Rather it‚Äôs a privacy issue related to the usage of third party services that rely on Siri. For example, the misuse of SiriKit (Siri‚Äôs extension that allows third party integrations to Siri) by WhatsApp.</p><p>Via Apple Intelligence, users can interact with ChatGPT through Siri and Writing Tools. For example, Siri can tap into ChatGPT to provide answers when that might be helpful for certain requests.</p><p>According to Apple, every request to ChatGPT is routed through Apple Intelligence‚Äôs Extensions service. This happens also if you use your private or business account.</p><p>However, when using Siri, these requests are duplicated: requests are sent to the extensions‚Äôs service and to Siri servers. The request to Siri‚Äôs servers is redundant for the user.</p><h2>Disclosure Process and Apple‚Äôs Response</h2><p>We informed Apple of this privacy issue in February 2025. In March 2025, after some back and forth with their security team and sending additional information, Apple acknowledged the issue and said they will be working towards a fix.</p><p>However, in July 2025, Apple reached out to say that this is actually <strong>not a privacy issue related to Apple Intelligence.</strong> Rather it‚Äôs a privacy issue related to the usage of third party services that rely on Siri. For example, the misuse of SiriKit (Siri‚Äôs extension that allows third party integrations to Siri) by WhatsApp.</p><p>After a quick overview, we saw the same behavior on iMessage as that of WhatsApp when sending a message via Siri. So, before reaching WhatsApp on this issue I took this matter deeply. I built an app, straight from Apple‚Äôs documentation, that uses SiriKit with messaging integration, and I was amazed to see the same behavior also happening in this test.</p><p>Apple also explicitly mentioned that Siri‚Äôs servers are not part of Apple‚Äôs Private Cloud Compute. It seems that there are features that belong to Siri core and other features that are based on Apple Intelligence. The user, however, is unaware of which is used when.</p><h2>Two Privacy Policies, Two Practices, Same App</h2><p>Herein the privacy issues become even further complex: when you dictate a message to Siri, two underlying flows might happen - you might be using Siri flows, but you may also be using Apple Intelligence flows. As a user, you have no idea if Siri will take on its flow, and when it will take on its Apple Intelligence flow.</p><p>For example, the query: ‚ÄúHey Siri, what is the weather today?‚Äù will send the data to Siri servers. However, the query: ‚ÄúHey Siri, ask ChatGPT what is the weather today?‚Äù will send the message to Apple Intelligence‚Äôs Private Cloud Compute. </p><p>Two similar questions, two different traffic flows, two different privacy policies.</p><p>For organizations managing Apple devices, we recommend taking on a three-prong approach:</p><ol start=\"\" role=\"list\"><li>Firewall and network restrictions on Siri domains<ul role=\"list\"><li>Set your firewall to block any network traffic to Siri‚Äôs dictation domain, guzzoni.apple.com. This should not hinder any Siri functionality</li><li>For stricter environments, setting your firewall to also block Siri‚Äôs search domain smoot.apple.com and its subdomains is also possible, though it will impair Siri capabilities.</li></ul></li><li>Apple device and knowledge sharing policies<ul role=\"list\"><li>Disable any ‚ÄúLearn from this app‚Äù settings to be on the safe side.</li></ul></li><li>Network traffic analysis and monitoring<ul role=\"list\"><li>Create a policy on AI usage for employees and place necessary controls for enforcement.</li><li>Monitor your network traffic to gain visibility into AI usage across the organization. With this visibility, you can hone your controls to ensure compliance with the defined policy.</li></ul></li></ol><p>On a smaller, personal scale, this means that sensitive data is leaving your Apple device unnecessarily, shattering your privacy expectations and giving you limited, perhaps even zero, control over your data.</p><p>For enterprises, it raises serious compliance and security risks when sensitive corporate information leaks outside the organization‚Äôs network.</p><p>This leads to a wider discussion, as this issue goes beyond just Apple and its incorporation of AI. AI capabilities are now all around us. Any typical app these days incorporates AI, whether it‚Äôs Grammarly, Canva or Salesforce. Knowing when a feature is powered by AI or not, is not really trivial anymore, and it requires understanding of the technical flow of the feature ‚Äî something that a typical user has no time, skill or desire to do.</p><p>At Lumia, our mission is to help organizations adopt AI safely, responsibly, and governed ‚Äî without disrupting productivity.</p><p>Apple Intelligence is just one aspect in the world of AI. However, the findings of our AppleStorm research highlight why visibility and control are essential in the age of AI-powered productivity tools. Lumia‚Äôs platform provides full visibility into AI-related data flows across thousands of applications and protocols, including hidden behaviors like those we uncovered in Siri.</p>","contentLength":14974,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44974109"},{"title":"95% of Companies See 'Zero Return' on $30B Generative AI Spend","url":"https://thedailyadda.com/95-of-companies-see-zero-return-on-30-billion-generative-ai-spend-mit-report-finds/","date":1755790584,"author":"speckx","guid":235795,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44974104"},{"title":"Beyond sensor data: Foundation models of behavioral data from wearables","url":"https://arxiv.org/abs/2507.00191","date":1755787185,"author":"brandonb","guid":235794,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44973375"},{"title":"Unity reintroduces the Runtime Fee through its Industry license","url":"https://unity.com/products/unity-industry","date":1755786678,"author":"finnsquared","guid":235793,"unread":true,"content":"<p>Deliver immersive applications across AR, VR, web, mobile, and desktop platforms. Reach your audience with seamless multi-platform support, leverage cutting-edge devices, and create interactive experiences that engage customers and stakeholders wherever they are.</p>","contentLength":263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44973269"},{"title":"In a first, Google has released data on how much energy an AI prompt uses","url":"https://www.technologyreview.com/2025/08/21/1122288/google-gemini-ai-energy/","date":1755784370,"author":"jeffbee","guid":235760,"unread":true,"content":"<p>So some Gemini prompts use much more energy than this: Dean gives the example of feeding dozens of books into Gemini and asking it to produce a detailed synopsis of their content. ‚ÄúThat‚Äôs the kind of thing that will probably take more energy than the median prompt,‚Äù Dean says. Using a reasoning model could also have a higher associated energy demand because these models take more steps before producing an answer.</p><p>This report was also strictly limited to text prompts, so it doesn‚Äôt represent what‚Äôs needed to generate an image or a video. (Other analyses, including <a href=\"https://www.technologyreview.com/2025/05/20/1116327/ai-energy-usage-climate-footprint-big-tech/\">one in ‚Äôs Power Hungry series</a> earlier this year, show that these tasks can require much more energy.)</p><p>The report also finds that the total energy used to field a Gemini query has fallen dramatically over time. The median Gemini prompt used 33 times more energy in May 2024 than it did in May 2025, according to Google. The company points to advancements in its models and other software optimizations for the <a href=\"https://www.technologyreview.com/2025/05/20/1116337/ai-energy-use-optimism/\">improvements</a>.&nbsp;&nbsp;</p><p>Google also estimates the greenhouse gas emissions associated with the median prompt, which they put at 0.03 grams of carbon dioxide. To get to this number, the company multiplied the total energy used to respond to a prompt by the average emissions per unit of electricity.</p><p>Rather than using an emissions estimate based on the US grid average, or the average of the grids where Google operates, the company instead uses a market-based estimate, which takes into account electricity purchases that the company makes from clean energy projects. The company has signed agreements to buy over <a href=\"https://sustainability.google/operations/\">22 gigawatts</a> of power from sources including solar, wind, geothermal, and advanced nuclear projects since 2010. Because of those purchases, Google‚Äôs emissions per unit of electricity on paper are roughly one-third of those on the average grid where it operates.</p><p>AI data centers also consume water for cooling, and Google estimates that each prompt consumes 0.26 milliliters of water, or about five drops.&nbsp;</p><p>The goal of this work was to provide users a window into the energy use of their interactions with AI, Dean says.&nbsp;</p><p>‚ÄúPeople are using [AI tools] for all kinds of things, and they shouldn‚Äôt have major concerns about the energy usage or the water usage of Gemini models, because in our actual measurements, what we were able to show was that it‚Äôs actually equivalent to things you do without even thinking about it on a daily basis,‚Äù he says, ‚Äúlike watching a few seconds of TV or consuming five drops of water.‚Äù</p>","contentLength":2523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44972808"},{"title":"Show HN: ChartDB Cloud ‚Äì Visualize and Share Database Diagrams","url":"https://app.chartdb.io/","date":1755781271,"author":"Jonathanfishner","guid":235856,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44972238"},{"title":"How well does the money laundering control system work?","url":"https://www.journals.uchicago.edu/doi/10.1086/735665","date":1755781097,"author":"PaulHoule","guid":235759,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44972213"},{"title":"AWS CEO says using AI to replace junior staff is 'Dumbest thing I've ever heard'","url":"https://www.theregister.com/2025/08/21/aws_ceo_entry_level_jobs_opinion/","date":1755780796,"author":"JustExAWS","guid":235693,"unread":true,"content":"<p>Amazon Web Services CEO Matt Garman has suggested firing junior workers because AI can do their jobs is \"the dumbest thing I've ever heard.\"</p><p>Garman made that remark in <a target=\"_blank\" rel=\"nofollow\" href=\"https://www.youtube.com/watch?v=nfocTxMzOP4\">conversation</a> with AI investor Matthew Berman, during which he talked up AWS‚Äôs <a target=\"_blank\" href=\"https://www.theregister.com/2025/08/18/aws_updated_kiro_pricing/\">Kiro AI-assisted coding tool</a> and said he's encountered business leaders who think AI tools \"can replace all of our junior people in our company.\"</p><p>That notion led to the ‚Äúdumbest thing I've ever heard‚Äù quote, followed by a justification that junior staff are ‚Äúprobably the least expensive employees you have‚Äù and also the most engaged with AI tools.</p><p>‚ÄúHow's that going to work when ten years in the future you have no one that has learned anything,‚Äù he asked. ‚ÄúMy view is you absolutely want to keep hiring kids out of college and teaching them the right ways to go build software and decompose problems and think about it, just as much as you ever have.‚Äù</p><p>Naturally he thinks AI ‚Äì and Kiro, natch ‚Äì can help with that education.</p><p>Garman is also not keen on another idea about AI ‚Äì measuring its value by what percentage of code it contributes at an organization.</p><p>‚ÄúIt‚Äôs a silly metric,‚Äù he said, because while organizations can use AI to write ‚Äúinfinitely more lines of code‚Äù it could be bad code.</p><p>‚ÄúOften times fewer lines of code is way better than more lines of code,‚Äù he observed. ‚ÄúSo I'm never really sure why that's the exciting metric that people like to brag about.‚Äù</p><p>That said, he‚Äôs seen data that suggests over 80 percent of AWS‚Äôs developers use AI in some way.</p><p>‚ÄúSometimes it's writing unit tests, sometimes it's helping write documentation, sometimes it's writing code, sometimes it's kind of an agentic workflow‚Äù in which developers collaborate with AI agents.</p><p>Garman said usage of AI tools by AWS developers increases every week.</p><p>The CEO also offered some career advice for the AI age, suggesting that kids these days need to learn how to learn ‚Äì and not just learn specific skills.</p><p>‚ÄúI think the skills that should be emphasized are how do you think for yourself? How do you develop critical reasoning for solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing?‚Äù</p><p>Garman thinks that approach is necessary because technological development is now so rapid it‚Äôs no longer sensible to expect that studying narrow skills can sustain a career for 30 years. He wants educators to instead teach ‚Äúhow do you think and how do you decompose problems‚Äù, and thinks kids who acquire those skills will thrive. ¬Æ</p>","contentLength":2582,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44972151"},{"title":"Privately-Owned Rail Cars","url":"https://www.amtrak.com/privately-owned-rail-cars","date":1755779506,"author":"jasoncartwright","guid":235969,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971954"},{"title":"How does the US use water?","url":"https://www.construction-physics.com/p/how-does-the-us-use-water","date":1755778898,"author":"juliangamble","guid":235936,"unread":true,"content":"<p><a href=\"https://www.nytimes.com/2025/07/16/climate/southwest-megadrought.html\" rel=\"\">decades of drought</a><a href=\"https://udallcenter.arizona.edu/news/colorado-river-negotiations-2024\" rel=\"\">increasing concern</a><a href=\"https://www.bloomberg.com/graphics/2025-ai-impacts-data-centers-water-data/\" rel=\"\">heavy users of water</a></p><p>Because water is such a critical resource, needed for everything from agriculture to manufacturing to artificial intelligence to sustaining basic human life, it's worth understanding how we use water, and how that use has changed over time.</p><p><a href=\"https://en.wikipedia.org/wiki/Water_cycle\" rel=\"\">water cycle</a></p><p>Altogether, the US receives about 5 trillion gallons of precipitation a day. Most of that (63%) gets returned to the atmosphere via evapotranspiration. Much of the rest ultimately flows into the Gulf of Mexico (11%), Pacific Ocean (6%), and Atlantic Ocean (2%). About 10% gets stored in surface bodies of water (lakes, reservoirs) or underground aquifers, and 6% flows back into Canada. The remaining 2% is consumed by people in various ways.</p><p><a href=\"https://www.engr.psu.edu/ce/hydro/hill/teaching/rural_water/rws4d3.pdf\" rel=\"\">large-diameter pipes</a></p><p><a href=\"https://pubs.usgs.gov/circ/1441/circ1441.pdf\" rel=\"\">USGS report</a><a href=\"https://en.wikipedia.org/wiki/Brackish_water\" rel=\"\">brackish</a></p><p>An important distinction when understanding water use is ‚Äúconsumptive‚Äù vs ‚Äúnon-consumptive‚Äù uses. Consumptive water use is when the water is consumed as part of the process: either it gets incorporated into whatever is being produced, evaporates back into the atmosphere, or is otherwise no longer available in fluid form. Non-consumptive water use is when the water is still available to use in fluid form after the process is completed, though perhaps at a higher temperature, or with some additional pollutants or impurities. The graph below shows total water use by category, broken out into consumptive and non-consumptive uses:</p><p><a href=\"https://en.wikipedia.org/wiki/Surface_condenser\" rel=\"\">condense steam back to liquid water</a></p><p><a href=\"https://www.eia.gov/todayinenergy/detail.php?id=14971\" rel=\"\">used once-through cooling</a></p><p>After power plants, the next largest use of water in the US is for irrigation: watering plants and crops. Altogether, irrigation makes up 37% of total US water use. Unlike thermoelectric power plants, most of this water (just over 60%) is consumptive use. What‚Äôs more, even the irrigation water that isn‚Äôt consumed may not be readily reused. While thermoelectric plants which typically dump their water back into the same river, lake, or ocean it was drawn from, much of the non-consumptive water used for irrigation seeps back into the ground, potentially far from where it was originally tapped. Depending on how water flows through the ground in a particular location, it could be centuries or even longer before that water reaches somewhere it can be easily retrieved.</p><p><a href=\"https://farmdocdaily.illinois.edu/2025/04/estimating-total-crop-acres-for-the-us-over-1998-2025.html\" rel=\"\">337 million acres of cropland</a><a href=\"https://www.ers.usda.gov/topics/farm-practices-management/irrigation-water-use\" rel=\"\">54% of the value of US crops</a></p><p><a href=\"https://www.ngf.org/full-shots/new-golf-facilities-in-the-u-s-report-now-available/\" rel=\"\">16,000 golf courses</a></p><p>After irrigation, the next largest category of US water use is public utility supplied water for homes and businesses. Public water use totals to around 39 billion gallons a day, or 12% of total US water use. Most public water ‚Äî 23.3 billion gallons, or around 60% ‚Äî is for domestic use in homes. In addition to publicly supplied water for homes, another 3.3 billion gallons are self-supplied to homes via things like privately owned wells. An estimated 42 million people in the US use self-supplied water.</p><p>The next largest category of water use is industry, which uses about 14.8 billion gallons of water a day, or 4.5% of all US water use). Many industrial processes require large amounts of water, often either for cooling or as a way to carry and mix materials or chemicals. The chart below shows several US industries that use large amounts of water.</p><p><a href=\"https://repository.gatech.edu/server/api/core/bitstreams/28217569-3295-4692-8f4f-31ec80ce9b1e/content\" rel=\"\">rinsing the pulp</a></p><p><a href=\"https://www.ncasi.org/wp-content/uploads/2019/02/NCASI_water_US_profile_infographic_print.pdf\" rel=\"\">12% of the water used is consumptive</a><a href=\"https://www.fluencecorp.com/steel-industry-water-use/\" rel=\"\">only around 10%</a></p><p><a href=\"https://eta-publications.lbl.gov/sites/default/files/2024-12/lbnl-2024-united-states-data-center-energy-usage-report.pdf?utm_source=substack&amp;utm_medium=email\" rel=\"\">Lawrence Berkeley Lab</a><a href=\"https://www.theregister.com/2025/01/04/how_datacenters_use_water/\" rel=\"\">evaporate as part of the cooling process</a></p><p>This is a large amount of water when compared to the amount of water homes use, but it's not particularly large when compared to other large-scale industrial uses. 66 million gallons per day is about 6% of the water used by US golf courses, and it's about 3% of the water used to grow cotton in 2023.</p><p><a href=\"https://www.cotton.org/news/releases/2024/cropest.cfm\" rel=\"\">six billion pounds of cotton in 2023</a><a href=\"https://www.statista.com/statistics/191505/cotton-production-value-in-the-us-since-2000/\" rel=\"\">$4.5 billion</a><a href=\"https://finance.yahoo.com/news/anthropic-nears-funding-170-billion-005055848.html\" rel=\"\">$5 billion dollars every year</a></p><p>The remaining users of water in the US are aquaculture (fish farms) at 7.5 billion gallons a day (2.3% of all water use), mining at 4 billion gallons a day (1.2%), and water for livestock at 2 billion gallons a day (0.6%).</p><p>How does US water use vary by geography? The map below shows total water consumption by state:</p><p>California is the number one water consumer at 28.8 billion gallons per day, followed by Texas (21.3 billion), Idaho (17.7 billion), Florida (15.3 billion) and Arkansas (13.8). The large consumers of water are states that either have a lot of irrigated land (California, Idaho, Arkansas), have a lot of thermal power plant cooling (Florida), or both (Texas).</p><p>Because this lumps in such disparate uses of water, it's more illuminating to break this down by category. The map below shows water used for irrigation by state.</p><p>And this map shows a more granular view of where, specifically, irrigated acres are located.</p><p>Most irrigated acres are in the western half of the US. This isn‚Äôt that surprising, as western states get far less precipitation than eastern states. The map below shows average yearly precipitation by state (averaged between the years 1971 and 2000).</p><p>Average annual rainfall is around 45.6 inches per year in the eastern half of the US, compared to 21 inches per year in the western half.</p><p>The amount of water an individual state has available for productive use is not simply a portion of all the precipitation it receives (due to movement of water through rivers, streams and aquifers), but it's nevertheless interesting to see irrigation water use as a fraction of a state‚Äôs total precipitation.</p><p>In most states, irrigation water is equivalent to a tiny fraction of total precipitation: in 30 states it's less than 1%. But in some western states it's much higher. In Idaho water used for irrigation is equivalent to more than 20% of all the precipitation the state receives.</p><p>Much of the water used for irrigation ‚Äî roughly half ‚Äî is pumped from underground aquifers. In many places in the US, groundwater is being pumped out faster than it recharges from precipitation, leading to gradual depletion of the aquifer.</p><p>(Interestingly, this map shows groundwater recharging in an Idaho aquifer, despite Idaho being such a large user of irrigation water. Possibly this is explained by the time period difference, as this map only goes to 2008.)</p><p>Other than irrigation, the largest category of water use in the US is cooling at thermoelectric power plants. Here‚Äôs water used by thermoelectric power plants, by state and by county.</p><p>Most water used for powerplant cooling is in the eastern half of the country (as well as Texas), as that‚Äôs where most power plants are.</p><p>The next largest category of water use is public water consumption. Because public water mostly consists of water used in homes, I‚Äôve also included self-supplied water used by homes. The maps below show public and self-supplied water use by state.</p><p>Because public and self-supplied water use is a function of population, this map is basically a population map. So let‚Äôs also look at public and self-supplied water use per capita.</p><p>There‚Äôs less variation here than I expected. The highest consuming states (Utah, Idaho) use only about two times the amount of water per capita as the lowest consuming states.</p><p>If we look at self-supplied water use specifically, we can see that it's highly regionally concentrated. In some areas of the country, self-supplied water makes up nearly all domestic water use, while in other areas of the country it's much less common.</p><p>The next largest category of water use is industrial. The map below shows industrial water use by county:</p><p>Industrial water consumption is concentrated in a very small number of locations: northern Indiana for steel production, Louisiana and the Gulf Coast for oil refining, and so on. 100 of the US‚Äôs 3200 counties are responsible for more than 70% of the US‚Äôs industrial water consumption, and 23% of US industrial water use is concentrated in just five counties.</p><p>How have patterns of water consumption changed over time? Overall water use peaked in 1980, and has trended downward since then.</p><p>Another trend from the 1950s through the 1970s was increased use of groundwater. Between 1950 and 1980 the annual volume of groundwater used in the US more than doubled, from 34 billion gallons per day to 83 billion gallons per day. Since then, groundwater use has been roughly constant (as of 2015, it‚Äôs at 82.3 billion gallons per day), even as surface water use has declined by nearly 30%. Groundwater is thus making up an increasingly large fraction of overall water use.</p><p>My overall takeaway is that you need to be careful when talking about water use: it‚Äôs very easy to take figures out of context, or make misleading comparisons. Very often I see alarmist discussions about water use that don‚Äôt take into account the distinction between consumptive and non-consumptive uses. Billions of gallons of water a day are ‚Äúused‚Äù by thermal power plants, but the water is quickly returned to where it came from, so this use isn‚Äôt reducing the supply of available fresh water in any meaningful sense. Similarly, the millions of gallons of water a day a large data center can use sounds like a lot when compared to the hundreds of gallons a typical home uses, but compared to other large-scale industrial or agricultural uses of water, it's a mere drop in the bucket.</p><p>On the other hand, it‚Äôs also easy to go too far in the other direction. People sometimes invoke the idea that water moves through a cycle and never really gets destroyed, in order to suggest that we don‚Äôt need to be concerned at all about water use. But while water may not get destroyed, it can get ‚Äúused up‚Äù in the sense that it becomes infeasible or uneconomic to access it. If we draw down aquifers that have spent hundreds or thousands of years accumulating water, the water isn‚Äôt ‚Äúgone‚Äù in the sense that it's been chemically transformed back into hydrogen and oxygen, but much of it probably ends up in the atmosphere, the ocean, or other places not readily or cheaply retrievable. Outside of large-scale desalination of ocean water, we‚Äôre ultimately limited in how much fresh water we can use by the amount of precipitation we get, and while we currently use a very small fraction of total precipitation (around 6%), it‚Äôs not as if there‚Äôs no limit to how much naturally-produced fresh water is available.</p>","contentLength":10159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971850"},{"title":"Weaponizing image scaling against production AI systems","url":"https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/","date":1755778853,"author":"tatersolid","guid":235758,"unread":true,"content":"<p>Picture this: you send a seemingly harmless image to an LLM and suddenly it exfiltrates all of your user data. By delivering a multi-modal prompt injection not visible to the user, we achieved data exfiltration on systems including the Google Gemini CLI. This attack works because AI systems often scale down large images before sending them to the model: when scaled, these images can reveal prompt injections that are not visible at full resolution.</p><p>In this blog post, we‚Äôll detail how attackers can <a href=\"https://www.usenix.org/conference/usenixsecurity20/presentation/quiring\">exploit image scaling</a> on Gemini CLI, Vertex AI Studio, Gemini‚Äôs web and API interfaces, Google Assistant, Genspark, and other production AI systems. We‚Äôll also explain how to mitigate and defend against these attacks, and we‚Äôll introduce <a href=\"https://github.com/trailofbits/anamorpher\">Anamorpher</a>, our open-source tool that lets you explore and generate these crafted images.</p><p>: <a href=\"https://www.usenix.org/conference/usenixsecurity19/presentation/xiao\">Image scaling attacks</a> were used for model <a href=\"https://arxiv.org/abs/2003.08633\">backdoors, evasion, and poisoning</a> primarily against older computer vision systems that enforced a fixed image size. While this constraint is less common with newer approaches, the systems surrounding the model may still impose constraints calling for image scaling. This establishes an underexposed, yet widespread vulnerability that we‚Äôve weaponized for <a href=\"https://developer.nvidia.com/blog/how-hackers-exploit-ais-problem-solving-instincts/\">multi-modal prompt injection</a>.</p><h2>Data exfiltration on the Gemini CLI</h2><p>To set up our data exfiltration exploit on the Gemini CLI through an image-scaling attack, we applied the default configuration for the Zapier MCP server. This automatically approves all MCP tool calls without user confirmation, <a href=\"https://github.com/google-gemini/gemini-cli/issues/5598\">as it sets  in the  of the Gemini CLI</a>. This provides an important primitive for the attacker.</p><p>Figure 2 showcases a video of the attack. First, the user uploads a seemingly benign image to the CLI. With no preview available, the user cannot see the transformed, malicious image processed by the model. This image and its prompt-ergeist triggers actions from Zapier that exfiltrates user data stored in Google Calendar to an attacker‚Äôs email without confirmation.</p><p>We also successfully demonstrated image scaling attacks on the following:</p><ul><li>Vertex AI with a Gemini back end</li><li>Gemini‚Äôs API via the  CLI</li><li>Google Assistant on an Android phone</li></ul><p>Notice the persistent mismatch between user perception and model inputs in figures 3 and 4. The exploit is particularly impactful on Vertex AI Studio because the front-end UI shows the high-resolution image instead of the downscaled image perceived by the model.</p><p>Our testing confirmed that this attack vector is widespread, extending far beyond the applications and systems documented here.</p><h2>Sharpening the attack surface</h2><p>These image scaling attacks exploit downscaling algorithms (or <a href=\"https://guide.encode.moe/encoding/resampling.html\">image resampling algorithms</a>), which perform interpolation to turn multiple high resolution pixel values into a single low resolution pixel value.</p><p>There are three major downscaling algorithms: <a href=\"https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation\">nearest neighbor interpolation</a>, <a href=\"https://en.wikipedia.org/wiki/Bilinear_interpolation\">bilinear interpolation</a>, and <a href=\"https://en.wikipedia.org/wiki/Bicubic_interpolation\">bicubic interpolation</a>. Each algorithm requires a different approach to perform an image scaling attack. Furthermore, these algorithms are implemented differently across libraries (e.g., Pillow, PyTorch, OpenCV, TensorFlow), with varying anti-aliasing, alignment, and kernel phases (in addition to <a href=\"https://bartwronski.com/2021/02/15/bilinear-down-upsampling-pixel-grids-and-that-half-pixel-offset/\">distinct bugs</a> that historically have <a href=\"https://arxiv.org/abs/2104.11222\">plagued model performance</a>). These differences also impact the techniques necessary for an image scaling attack. Therefore, exploiting production systems required us to fingerprint each system‚Äôs algorithm and implementation.</p><p>To understand why image downscaling attacks are possible, imagine that you have a long ribbon with an intricate yet regular pattern on it. As this ribbon is pulled past you, you‚Äôre trying to recreate the pattern by grabbing samples of the ribbon at regular intervals. If the pattern changes rapidly, you need to grab samples very frequently to capture all the details. If you‚Äôre too slow, you‚Äôll miss crucial parts between grabs, and when you try to reconstruct the pattern from your samples, it looks completely different from the original.</p><h2>Anamorpher and the attacker‚Äôs darkroom</h2><p>Currently, Anamorpher (named after <a href=\"https://en.wikipedia.org/wiki/Anamorphosis\">anamorphosis</a>) can develop crafted images for the aforementioned three major methods. Let‚Äôs explore how Anamorpher exploits bicubic interpolation frame by frame.</p><p>Bicubic interpolation considers the 16 pixels (from 4x4 sampling) around each target pixel, using cubic polynomials to calculate smooth transitions between pixel values. This method creates a predictable mathematical relationship that can be exploited. Specifically, the algorithm assigns different weights to pixels in the neighborhood, creating pixels that contribute more to the final output, which are known as high-importance pixels. Therefore, the total <a href=\"https://en.wikipedia.org/wiki/Luma_(video)\">luma</a> (brightness) of dark areas of an image will increase if specific high-importance pixels are higher luma than their surroundings.</p><p>Therefore, to exploit this, we can carefully craft high-resolution pixels and solve the inverse problem. First, we select a decoy image with large dark areas to hide our payload. Then, we adjust pixels in dark regions and push the downsampled result toward a red background using least-squares optimization. These adjustments in the dark areas cause the background to turn red while text areas remain largely unmodified and appear black, creating much stronger contrast than visible at full resolution. While this approach is most effective on bicubic downscaling, it also works on specific implementations of bilinear downscaling.</p><p>Anamorpher provides users with the ability to visualize and craft image scaling attacks against specific algorithms and implementations through a front-end interface and Python API. In addition, it comes with a modular back end, which enables users to customize their own downscaling algorithm.</p><p>While some downscaling algorithms are more vulnerable than others, attempting to identify the least vulnerable algorithm and implementation is <a href=\"https://arxiv.org/abs/2104.08690\">not a robust approach</a>. This is especially true since image scaling attacks are not restricted to the aforementioned three algorithms.</p><p>For a secure system, we recommend not using image downscaling and simply limiting the upload dimensions. For any transformation, but especially if downscaling is necessary, the end user should always be provided with a preview of the input that the model is actually seeing, even in CLI and API tools.</p><p><a href=\"https://github.com/trailofbits/anamorpher\">Anamorpher</a> is currently in beta, so feel free to reach out with feedback and suggestions as we continue to improve this tool. Stay tuned for more work on the security of multi-modal, agentic, and multi-agentic AI systems!</p>","contentLength":6551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971845"},{"title":"Show HN: Using Common Lisp from Inside the Browser","url":"https://turtleware.eu/posts/Using-Common-Lisp-from-inside-the-Browser.html","date":1755778110,"author":"jackdaniel","guid":235796,"unread":true,"content":"<p> Written on 2025-08-21 by Daniel Kochma≈Ñski </p><p>Web Embeddable Common Lisp is a project that brings Common Lisp and the Web\nBrowser environments together. In this post I'll outline the current progress of\nthe project and provide some technical details, including current caveats and\nfuture plans.</p><p>It is important to note that this is not a release and none of the described\nAPIs and functionalities is considered to be stable. Things are still changing\nand I'm not accepting bug reports for the time being.</p><p>The easiest way to use Common Lisp on a website is to include WECL and insert\nscript tags with a type \"text/common-lisp\". When the attribute src is present,\nthen first the runtime loads the script from that url, and then it executes the\nnode body. For example create and run this HTML document from localhost:</p><pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Web Embeddable Common Lisp&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"https://turtleware.eu/static/misc/wecl-20250821/easy.css\" /&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/boot.js\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/wecl.js\"&gt;&lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/easy.lisp\" id='easy-script'&gt;\n(defvar *div* (make-element \"div\" :id \"my-ticker\"))\n(append-child [body] *div*)\n\n(dotimes (v 4)\n  (push-counter v))\n\n(loop for tic from 6 above 0\n      do (replace-children *div* (make-paragraph \"~a\" tic))\n         (js-sleep 1000)\n      finally (replace-children *div* (make-paragraph \"BOOM!\")))\n\n(show-script-text \"easy-script\")\n    &lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>We may use Common Lisp that can call to JavaScript, and register callbacks to be\ncalled on specified events. The source code of the script can be found here:</p><p>Because the runtime is included as a script, the browser will usually cache the\n~10MB WebAssembly module.</p><p>The initial foreign function interface has numerous macros defining wrappers\nthat may be used from Common Lisp or passed to JavaScript.</p><p>Summary of currently available operators:</p><ul><li> an inlined expression, like </li><li> an object referenced from the object store</li><li> a function</li><li> a method of the argument, like </li><li> a slot reader of the argument</li><li> a slot writer of the first argument</li><li> combines define-js-getter and define-js-setter</li><li> template for JavaScript expressions</li><li> Common Lisp function reference callable from JavaScript</li><li> anonymous Common Lisp function reference (for closures)</li></ul><p>Summary of argument types:</p><table border=\"2\" cellspacing=\"0\" cellpadding=\"6\" rules=\"groups\" frame=\"hsides\"><thead><tr></tr></thead><tbody><tr><td>Common Lisp object reference</td></tr><tr><td>JavaScript object reference</td></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>All operators, except for  have a similar lambda list:</p><blockquote><p>(DEFINE-JS NAME-AND-OPTIONS [ARGUMENTS [,@BODY]])</p></blockquote><p>The first argument is a list  that is common to all\ndefining operators:</p><ul><li> Common Lisp symbol denoting the object</li><li> a string denoting the JavaScript expression, i.e \"innerText\"</li><li> a type of the object returned by executing the expression</li></ul><pre><code>(define-js-variable ([document] :js-expr \"document\" :type :symbol))\n;; document\n(define-js-object ([body] :js-expr \"document.body\" :type :js-ref))\n;; wecl_ensure_object(document.body) /* -&gt; id   */\n;; wecl_search_object(id)            /* -&gt; node */\n</code></pre><p>The difference between a variable and an object in JS-FFI is that variable\nexpression is executed each time when the object is used (the expression is\ninlined), while the object expression is executed only once and the result is\nstored in the object store.</p><p>The second argument is a list of pairs . Names will be used in the\nlambda list of the operator callable from Common Lisp, while types will be used\nto coerce arguments to the type expected by JavaScript.</p><pre><code>(define-js-function (parse-float :js-expr \"parseFloat\" :type :js-ref)\n    ((value :string)))\n;; parseFloat(value)\n\n(define-js-method (add-event-listener :js-expr \"addEventListener\" :type :null)\n    ((self :js-ref)\n     (name :string)\n     (fun :js-ref)))\n;; self.addEventListener(name, fun)\n\n(define-js-getter (get-inner-text :js-expr \"innerText\" :type :string)\n    ((self :js-ref)))\n;; self.innerText\n\n(define-js-setter (set-inner-text :js-expr \"innerText\" :type :string)\n    ((self :js-ref)\n     (new :string)))\n;; self.innerText = new\n\n(define-js-accessor (inner-text :js-expr \"innerText\" :type :string)\n    ((self :js-ref)\n     (new :string)))\n;; self.innerText\n;; self.innerText = new\n\n(define-js-script (document :js-expr \"~a.forEach(~a)\" :type :js-ref)\n    ((nodes :js-ref)\n     (callb :object)))\n;; nodes.forEach(callb)\n</code></pre><p>The third argument is specific to callbacks, where we define Common Lisp body of\nthe callback. Argument types are used to coerce values from JavaScript to Common\nLisp.</p><pre><code>(define-js-callback (print-node :type :object)\n    ((elt :js-ref)\n     (nth :fixnum)\n     (seq :js-ref))\n  (format t \"Node ~2d: ~a~%\" nth elt))\n\n(let ((start 0))\n  (add-event-listener *my-elt* \"click\"\n                      (lambda-js-callback :null ((event :js-ref)) ;closure!\n                        (incf start)\n                        (setf (inner-text *my-elt*)\n                              (format nil \"Hello World! ~a\" start)))\n</code></pre><p>Note that callbacks are a bit different, because  does not\naccept  option and  has unique lambda list. It is\nimportant for callbacks to have an exact arity as they are called with, because\nJS-FFI does not implement variable number of arguments yet.</p><p>Callbacks can be referred by name with an operator .</p><p>While working on FFI I've decided to write an adapter for SLIME/SWANK that will\nallow interacting with WECL from Emacs. The principle is simple: we connect with\na websocket to Emacs that is listening on the specified port (i.e on localhost).\nThis adapter uses the library  written by Andrew Hyatt.</p><p>It allows for compiling individual forms with , but file compilation\ndoes not work (because files reside on a different \"host\"). REPL interaction\nworks as expected, as well as SLDB. The connection may occasionally be unstable,\nand until Common Lisp call returns, the whole page is blocked. Notably waiting\nfor new requests is not a blocking operation from the JavaScript perspective,\nbecause it is an asynchronous operation.</p><pre><code>;;; Patches for SLIME 2.31 (to be removed after the patch is merged).\n;;; It is assumed that SLIME is already loaded into Emacs.\n(defun slime-net-send (sexp proc)\n  \"Send a SEXP to Lisp over the socket PROC.\nThis is the lowest level of communication. The sexp will be READ and\nEVAL'd by Lisp.\"\n  (let* ((payload (encode-coding-string\n                   (concat (slime-prin1-to-string sexp) \"\\n\")\n                   'utf-8-unix))\n         (string (concat (slime-net-encode-length (length payload))\n                         payload))\n         (websocket (process-get proc :websocket)))\n    (slime-log-event sexp)\n    (if websocket\n        (websocket-send-text websocket string)\n      (process-send-string proc string))))\n\n(defun slime-use-sigint-for-interrupt (&amp;optional connection)\n  (let ((c (or connection (slime-connection))))\n    (cl-ecase (slime-communication-style c)\n      ((:fd-handler nil) t)\n      ((:spawn :sigio :async) nil))))\n</code></pre><pre><code>;;; lime.el --- Lisp Interaction Mode for Emacs -*-lexical-binding:t-*-\n;;; \n;;; This program extends SLIME with an ability to listen for lisp connections.\n;;; The flow is reversed - normally SLIME is a client and SWANK is a server.\n\n(require 'websocket)\n\n(defvar *lime-server* nil\n  \"The LIME server.\")\n\n(cl-defun lime-zipit (obj &amp;optional (start 0) (end 72))\n  (let* ((msg (if (stringp obj)\n                  obj\n                (slime-prin1-to-string obj)))\n         (len (length msg)))\n    (substring msg (min start len) (min end len))))\n\n(cl-defun lime-message (&amp;rest args)\n  (with-current-buffer (process-buffer *lime-server*)\n    (goto-char (point-max))\n    (dolist (arg args)\n      (insert (lime-zipit arg)))\n    (insert \"\\n\")\n    (goto-char (point-max))))\n\n(cl-defun lime-client-process (client)\n  (websocket-conn client))\n\n(cl-defun lime-process-client (process)\n  (process-get process :websocket))\n\n;;; c.f slime-net-connect\n(cl-defun lime-add-client (client)\n  (lime-message \"LIME connecting a new client\")\n  (let* ((process (websocket-conn client))\n         (buffer (generate-new-buffer \"*lime-connection*\")))\n    (set-process-buffer process buffer)\n    (push process slime-net-processes)\n    (slime-setup-connection process)\n    client))\n\n;;; When SLIME kills the process, then it invokes LIME-DISCONNECT hook.\n;;; When SWANK kills the process, then it invokes LIME-DEL-CLIENT hook.\n(cl-defun lime-del-client (client)\n  (when-let ((process (lime-client-process client)))\n    (lime-message \"LIME client disconnected\")\n    (slime-net-sentinel process \"closed by peer\")))\n\n(cl-defun lime-disconnect (process)\n  (when-let ((client (lime-process-client process)))\n    (lime-message \"LIME disconnecting client\")\n    (websocket-close client)))\n\n(cl-defun lime-on-error (client fun error)\n  (ignore client fun)\n  (lime-message \"LIME error: \" (slime-prin1-to-string error)))\n\n;;; Client sends the result over a websocket. Handling responses is implemented\n;;; by SLIME-NET-FILTER. As we can see, the flow is reversed in our case.\n(cl-defun lime-handle-message (client frame)\n  (let ((process (lime-client-process client))\n        (data (websocket-frame-text frame)))\n    (lime-message \"LIME-RECV: \" data)\n    (slime-net-filter process data)))\n\n(cl-defun lime-net-listen (host port &amp;rest parameters)\n  (when *lime-server*\n    (error \"LIME server has already started\"))\n  (setq *lime-server*\n        (apply 'websocket-server port\n               :host host\n               :on-open    (function lime-add-client)\n               :on-close   (function lime-del-client)\n               :on-error   (function lime-on-error)\n               :on-message (function lime-handle-message)\n               parameters))\n  (unless (memq 'lime-disconnect slime-net-process-close-hooks)\n    (push 'lime-disconnect slime-net-process-close-hooks))\n  (let ((buf (get-buffer-create \"*lime-server*\")))\n    (set-process-buffer *lime-server* buf)\n    (lime-message \"Welcome \" *lime-server* \"!\")\n    t))\n\n(cl-defun lime-stop ()\n  (when *lime-server*\n   (websocket-server-close *lime-server*)\n   (setq *lime-server* nil)))\n</code></pre><p>After loading this file into Emacs invoke <code>(lime-net-listen \"localhost\" 8889)</code>.\nNow our Emacs listens for new connections from SLUG (the lisp-side part adapting\nSWANK, already bundled with WECL). There are two SLUG backends in a repository:</p><ul><li> for web browser environment</li><li> for Common Lisp runtime (uses )</li></ul><p>Now you can open a page listed here and connect to SLIME:</p><pre><code>&lt;!doctype html&gt;\n&lt;html&gt;\n  &lt;head&gt;\n    &lt;title&gt;Web Embeddable Common Lisp&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"easy.css\" /&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/boot.js\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/javascript\" src=\"https://turtleware.eu/static/misc/wecl-20250821/wecl.js\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/slug.lisp\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/wank.lisp\"&gt;&lt;/script&gt;\n    &lt;script type=\"text/common-lisp\" src=\"https://turtleware.eu/static/misc/wecl-20250821/easy.lisp\"&gt;\n      (defvar *connect-button* (make-element \"button\" :text \"Connect\"))\n      (define-js-callback (connect-to-slug :type :null) ((event :js-ref))\n        (wank-connect \"localhost\" 8889)\n        (setf (inner-text *connect-button*) \"Crash!\"))\n      (add-event-listener *connect-button* \"click\" (js-callback connect-to-slug))\n      (append-child [body] *connect-button*)\n    &lt;/script&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre><p>This example shows an important limitation ‚Äì  does not allow for\nmultiple asynchronous contexts in the same thread. That means that if Lisp call\ndoesn't return (i.e because it waits for input in a loop), then we can't execute\nother Common Lisp statements from elsewhere because the application will crash.</p><p>Here's another example. It is more a cool gimmick than anything else, but let's\ntry it. Open a console on this very website (on firefox C-S-i) and execute:</p><pre><code>function inject_js(url) {\n    var head = document.getElementsByTagName('head')[0];\n    var script = document.createElement('script');\n    head.appendChild(script);\n    script.type = 'text/javascript';\n    return new Promise((resolve) =&gt; {\n        script.onload = resolve;\n        script.src = url;\n    });\n}\n\nfunction inject_cl() {\n    wecl_eval('(wecl/impl::js-load-slug \"https://turtleware.eu/static/misc/wecl-20250821\")');\n}\n\ninject_js('https://turtleware.eu/static/misc/wecl-20250821/boot.js')\n    .then(() =&gt; {\n        wecl_init_hooks.push(inject_cl);\n        inject_js('https://turtleware.eu/static/misc/wecl-20250821/wecl.js');\n    });\n</code></pre><p>With this, assuming that you've kept your LIME server open, you'll have a REPL\nonto uncooperative website. Now we can fool around with queries and changes:</p><pre><code>(define-js-accessor (title :js-expr \"title\" :type :string)\n  ((self :js-ref)\n   (title :string)))\n\n(define-js-accessor (background :js-expr \"body.style.backgroundColor\" :type :string)\n  ((self :js-ref)\n   (background :string)))\n\n(setf (title [document]) \"Write in Lisp!\")\n(setf (background [document]) \"#aaffaa\")\n</code></pre><p>The first thing to address is the lack of threading primitives. Native threads\ncan be implemented with web workers, but then our GC wouldn't know how to stop\nthe world to clean up. Another option is to use cooperative threads, but that\nalso won't work, because Emscripten doesn't support independent asynchronous\ncontexts, nor ECL is ready for that yet.</p><p>I plan to address both issues simultaneously in the second stage of the project\nwhen I port the runtime to WASI. We'll be able to use browser's GC, so running\nin multiple web workers should not be a problem anymore. Unwinding and rewinding\nthe stack will require tinkering with ASYNCIFY and I have somewhat working green\nthreads implementation in place, so I will finish it and upstream in ECL.</p><p>Currently I'm focusing mostly on having things working, so JS and CL interop is\nbrittle and often relies on evaluating expressions, trampolining and coercing.\nThat impacts the performance in a significant way. Moreover all loaded scripts\nare compiled with a one-pass compiler, so the result bytecode is not optimized.</p><p>There is no support for loading cross-compiled files onto the runtime, not to\nmention that it is not possible to precompile systems with ASDF definitions.</p><p>JS-FFI requires more work to allow for defining functions with variable number\nof arguments and with optional arguments. There is no dynamic coercion of\nJavaScript exceptions to Common Lisp conditions, but it is planned.</p>","contentLength":14560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971744"},{"title":"1981 Sony Trinitron KV-3000R: The Most Luxurious Trinitron [video]","url":"https://www.youtube.com/watch?v=jHG_I-9a7FY","date":1755777149,"author":"ksec","guid":236970,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971607"},{"title":"AI crawlers, fetchers are blowing up websites; Meta, OpenAI are worst offenders","url":"https://www.theregister.com/2025/08/21/ai_crawler_traffic/","date":1755776122,"author":"rntn","guid":235757,"unread":true,"content":"<p> Cloud services giant Fastly has released a report claiming AI crawlers are putting a heavy load on the open web, slurping up sites at a rate that accounts for 80 percent of all AI bot traffic, with the remaining 20 percent used by AI fetchers. Bots and fetchers can hit websites hard, demanding data from a single site in thousands of requests per minute.</p><blockquote><p>I can only see one thing causing this to stop: the AI bubble popping</p></blockquote><p>According to the <a href=\"https://learn.fastly.com/rs/025-XKO-469/images/Fastly-Threat-Insights-Report.pdf\">report</a> [PDF], Facebook owner Meta's AI division accounts for more than half of those crawlers, while OpenAI accounts for the overwhelming majority of on-demand fetch requests.</p><p>\"AI bots are reshaping how the internet is accessed and experienced, introducing new complexities for digital platforms,\" Fastly senior security researcher Arun Kumar opined in a statement on the report's release. \"Whether scraping for training data or delivering real-time responses, these bots create new challenges for visibility, control, and cost. You can't secure what you can't see, and without clear verification standards, AI-driven automation risks are becoming a blind spot for digital teams.\"</p><p>The company's report is based on analysis of Fastly's Next-Gen Web Application Firewall (NGWAF) and Bot Management services, which the company says \"protect over 130,000 applications and APIs and inspect more than 6.5 trillion requests per month\" ‚Äì giving it plenty of data to play with. The data reveals a growing problem: an increasing website load comes not from human visitors, but from automated crawlers and fetchers working on behalf of chatbot firms.</p><p>The report warned, \"Some AI bots, if not carefully engineered, can inadvertently impose an unsustainable load on webservers,\" Fastly's report warned, \"leading to performance degradation, service disruption, and increased operational costs.\" Kumar separately noted to The Register, \"Clearly this growth isn't sustainable, creating operational challenges while also undermining the business model of content creators. We as an industry need to do more to establish responsible norms and standards for crawling that allows AI companies to get the data they need while respecting websites content guidelines.\"</p><p>That growing traffic comes from just a select few companies. Meta accounted for more than half of all AI crawler traffic on its own, at 52 percent, followed by Google and OpenAI at 23 percent and 20 percent respectively. This trio then has its hands on a combined 95 percent of all AI crawler traffic. Anthropic, by contrast, accounted for just 3.76 percent of crawler traffic. The Common Crawl Project, which slurps websites to include in a free public dataset designed to prevent duplication of effort and traffic multiplication at the heart of the crawler problem, was a surprisingly-low 0.21 percent.</p><p>The story flips when it comes to AI fetchers, which unlike crawlers are fired off on-demand when a user requests that a model incorporates information newer than its training cut-off date. Here, OpenAI was by far the dominant traffic source, Fastly found, accounting for almost 98 percent of all requests. That's an indication, perhaps, of just how much of a lead OpenAI's early entry into the consumer-facing AI chatbot market with ChatGPT gave the company, or possibly just a sign that the company's bot infrastructure may be in need of optimization.</p><p>While AI fetchers make up a minority of Ai bot requests ‚Äì only about 20%, says Kumar ‚Äì they can be responsible for huge bursts of traffic, with one fetcher generating over 39,000 requests per minute during the testing period. \"We expect fetcher traffic to grow as AI tools become more widely adopted and as more agentic tools come into use that mediate the experience between people and websites,\" Kumar told .</p><p>Perplexity AI, which was <a target=\"_blank\" href=\"https://www.theregister.com/2025/08/04/perplexity_ai_crawlers_accused_data_raids/\">recently accused</a> of using IP addresses outside its reported crawler ranges and ignoring robots.txt directives from sites looking to opt out of being scraped, accounted for just 1.12 percent of AI crawler bot and 1.53 percent of AI fetcher bot traffic recorded for the report ‚Äì though the report noted that this is growing.</p><p>Kumar decried the practice of ignoring robots.txt notes, telling , \"At a minimum, any reputable AI company today should be honoring robots.txt. Further and even more critically, they should publish their IP address ranges and their bots should use unique names. This will empower site operators to better distinguish the bots crawling their sites and allow them to enforce granular&nbsp;rules with bot management solutions.\"</p><p>But he stopped short of calling for mandated standards, saying that industry forums are working on solutions. \"We need to let those processes play out.&nbsp; Mandating technical standards in regulatory frameworks often does not produce a good outcome and shouldn't be our first resort.\"</p><p>It's a problem large enough that users have begun fighting back. In the face of bots riding roughshod over polite opt-outs like robots.txt directives, webmasters are increasingly turning to active countermeasures like <a target=\"_blank\" href=\"https://www.theregister.com/2025/07/09/anubis_fighting_the_llm_hordes/\">the proof-of-work Anubis</a> or <a target=\"_blank\" href=\"https://www.theregister.com/2025/03/18/ai_crawlers_sourcehut/\">gibberish-feeding tarpit Nepenthes</a>, while Fastly rival Cloudflare has been testing a <a target=\"_blank\" href=\"https://www.theregister.com/2025/07/01/cloudflare_creates_ai_crawler_toll/\">pay-per-crawl approach</a> to put a financial burden on the bot operators. \"Care must be exercised when employing these techniques,\" Fastly's report warned, \"to avoid accidentally blocking legitimate users or downgrading their experience.\"</p><p>Kumar notes that small site operators, especially those serving dynamic content, are most likely to feel the effects most severely, and he had some recommendations. \"The first and simplest step is to configure robots.txt which immediately reduces traffic from well-behaved bots. When technical expertise is available, websites can also deploy controls such as Anubis, which can help reduce bot traffic.\" He warned, however, that bots are always improving and trying to find ways around \"tarpits\" like Anubis, as code-hosting site Codeberg <a target=\"_blank\" href=\"https://www.theregister.com/2025/08/15/codeberg_beset_by_ai_bots/\">recently experienced</a>. \"This creates a constant cat and mouse game, similar to what we observe with other types of bots today,\" he said.</p><p>We spoke to Anubis developer Xe Iaso, CEO of Techaro. When we asked whether they expected the growth in crawler traffic to slow, they said: \"I can only see one thing causing this to stop: the AI bubble popping.</p><p>\"There is simply too much hype to give people worse versions of documents, emails, and websites otherwise. I don't know what this actually gives people, but our industry takes great pride in doing this.\"</p><p>However, they added: \"I see no reason why it would not grow. People are using these tools to replace knowledge and gaining skills. There's no reason to assume that this attack against our cultural sense of thrift will not continue. This is the perfect attack against middle-management: unsleeping automatons that never get sick, go on vacation, or need to be paid health insurance that can produce output that superficially resembles the output of human employees. I see no reason that this will continue to grow until and unless the bubble pops. Even then, a lot of those scrapers will probably stick around until their venture capital runs out.\"</p><h3>Regulation ‚Äì we've heard of it</h3><p> asked Xe whether they thought broader deployment of Anubis and other active countermeasures would help.</p><p>They responded: \"This is a regulatory issue. The thing that needs to happen is that governments need to step in and give these AI companies that are destroying the digital common good existentially threatening fines and make them pay reparations to the communities they are harming. Ironically enough, most of these AI companies rely on the communities they are destroying.</p><p>\"This presents the kind of paradox that I would expect to read in a Neal Stephenson book from the '90s, not CBC's front page. Anubis helps mitigate a lot of the badness by making attacks more computationally expensive. Anubis (even in configurations that omit proof of work) makes attackers have to retool their scraping to use headless browsers instead of blindly scraping HTML.\"</p><p>And who is paying the piper?</p><p>\"This increases the infrastructure costs of the AI companies propagating this abusive traffic. The hope is that this makes it fiscally unviable for AI companies to scrape by making them have to dedicate much more hardware to the problem. In essence: it makes the scrapers have to spend more money to do the same work.\"</p><p>We approached Anthropic, Google, Meta, OpenAI, and Perplexity but none provided a comment on the report by the time of publication. ¬Æ</p><p>Will Allen, VP, Product at Cloudflare commented on the findings, saying Cloudflare's observations were \"reasonably close\" to Fastly's claim, \"and the nominal difference could potentially be due to a difference in customer mix.\" Allen added that, looking at its own <a target=\"_blank\" href=\"https://radar.cloudflare.com/ai-insights?dateStart=2025-04-15&amp;dateEnd=2025-07-14#crawl-purpose\">AI Bot &amp; crawler traffic by crawl purpose, for April 15 - July 14</a>), Cloudlfare could show that 82.7 percent is \"for training ‚Äî this is the equivalent of 'AI crawler' in Fastly's report.\"</p><p>Asked whether the growth in crawler traffic was likely to continue, Allen responded: \"We don't see any material slowdowns in the near term horizon - the desire for content currently seems insatiable.\"</p><p>He opined: \"All of our work around AI crawlers is anchored on a radically simple philosophy: content creators and website owners should get to decide how their content and data is used for commercial purposes when they put it online. Some of us want to write for the superintelligence. Others want a direct connection and to create for human eyes only.\"</p><p>Asked how he suggested site operators reduce the burden of this traffic on their infrastructure, he naturally pitched the vendor's own wares, saying \"Cloudflare makes it incredibly easy to take control, even for our free users: you can decide to let everyone crawl you, or with one click block AI Crawlers from training and deploy our fully managed robots.txt.\"</p><p>He said of the vendor's <a target=\"_blank\" href=\"https://www.theregister.com/2025/03/21/cloudflare_ai_labyrinth/\">AI labyrinth</a> that it was \"a first iteration of using generative AI to thwart bots for us, and generates valuable data that feeds into our bot detection systems. We don't see this as a final solution, but rather a fun use of technology to trap misbehaving bots.\"</p>","contentLength":10127,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971487"},{"title":"Margin debt surges to record high","url":"https://www.advisorperspectives.com/dshort/updates/2025/07/23/margin-debt-surges-record-high-june-2025","date":1755775370,"author":"pera","guid":235692,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971396"},{"title":"Mark Zuckerberg freezes AI hiring amid bubble fears","url":"https://www.telegraph.co.uk/business/2025/08/21/zuckerberg-freezes-ai-hiring-amid-bubble-fears/","date":1755774248,"author":"pera","guid":235691,"unread":true,"content":"<div data-test=\"article-body-text\"><p>Stock market volatility was largely prompted by a report from the Massachusetts Institute of Technology, which claimed that 95pc of companies were getting ‚Äúzero return‚Äù on their AI investments.</p><p>A Meta spokesman sought to downplay the freeze, saying: ‚ÄúAll that‚Äôs happening here is some basic organisational planning: creating a solid structure for our new superintelligence efforts after bringing people on board and undertaking yearly budgeting and planning exercises.‚Äù</p><p>It comes after the company has been offering top researchers at rival companies, including OpenAI and Google, enormous pay deals to join Meta Superintelligence Labs as Mr Zuckerberg seeks to dominate the field.</p><p>The company‚Äôs billionaire chief executive has become personally involved in developing cutting-edge AI after the disappointing release of its latest systems, personally messaging top researchers at Silicon Valley AI companies.</p><p>However, the division has been disrupted by repeated strategy overhauls, which led to the delayed release of its latest ‚ÄúBehemoth‚Äù AI model.</p><p>Mr Zuckerberg has said he wants to develop a ‚Äúpersonal superintelligence‚Äù that acts as a permanent superhuman assistant and lives in smart glasses.</p><p>‚ÄúWe believe in putting this power in people‚Äôs hands to direct it towards what they value in their own lives,‚Äù he wrote last month.</p><p>‚ÄúThis is distinct from others in the industry who believe superintelligence should be directed centrally towards automating all valuable work, and then humanity will live on a dole of its output.‚Äù</p><p>Mr Zuckerberg recently told investors that he wanted ‚Äúsmall, talent-dense teams‚Äù to be driving its AI work, rather than large groups of researchers.</p><p>Despite this, the company has said that the cost of paying staff will significantly increase in the coming years. Analysts at Morgan Stanley warned this week that the pay surge may ‚Äúdilute shareholder value without any clear innovation gains‚Äù.</p><p>Concerns about AI progress have been amplified by the modest response to GPT-5, the much-anticipated new version of ChatGPT.</p><p>Sam Altman, OpenAI‚Äôs chief executive, has compared hype around AI to the <a href=\"https://www.telegraph.co.uk/business/2025/08/21/we-may-be-facing-dotcom-bubble-2/\">dotcom bubble</a> at the turn of the century.</p></div>","contentLength":2186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971273"},{"title":"Using Podman, Compose and BuildKit","url":"https://emersion.fr/blog/2025/using-podman-compose-and-buildkit/","date":1755773653,"author":"LaSombra","guid":235690,"unread":true,"content":"<p>Podman supports Docker Compose projects with two possible solutions: either by\nconnecting the official <a href=\"https://github.com/docker/compose/\">Docker Compose CLI</a> to a Podman socket, either by\nusing <a href=\"https://github.com/containers/podman-compose\">their own drop-in replacement</a>. They ship a\n<a href=\"https://docs.podman.io/en/latest/markdown/podman-compose.1.html\">small wrapper</a> to select one of these options. (The\nwrapper has the same name as the replacement, which makes things confusing.)</p><p>Unfortunately, both options have downsides. When using the official Docker\nCompose CLI, the classic builder is used instead of the newer <a href=\"https://docs.docker.com/build/buildkit/\">BuildKit</a>\nbuilder. As a result, some features such as <a href=\"https://docs.docker.com/reference/compose-file/build/#additional_contexts\">additional contexts</a> are not\nsupported. When using the podman-compose replacement, some other features are\nmissing, such as ,  and referencing another service in\nadditional contexts. It would be possible to add these features to\npodman-compose, but that‚Äôs an endless stream of work (Docker Compose regularly\nadds new features) and I don‚Äôt really see the value in re-implementing all of\nthis (the fact that it‚Äôs Python doesn‚Äôt help me getting motivated).</p><p>I‚Äôve started looking for a way to convince the Docker Compose CLI to run under\nPodman with BuildKit enabled. I‚Äôve tried a few months ago and never got it to\nwork, but it seems like this recently became easier! The podman-compose wrapper\n<a href=\"https://github.com/containers/podman/blob/1e7f810f714240f5d68f92baa1ab39ee53a249f5/cmd/podman/compose.go#L164\">force-disables BuildKit</a>, so we need to use\ndirectly the Docker Compose CLI without the wrapper. On Arch Linux, this can be\nachieved by enabling the Podman socket and creating a new Docker context (same\nas setting , but more permanent):</p><pre><code>pacman -S docker-compose docker-buildx\nsystemctl --user start podman.socket\ndocker context create podman --docker host=unix://$XDG_RUNTIME_DIR/podman/podman.sock\ndocker context use podman\n</code></pre><p>With that,  just works! It turns out it automagically creates a\n container under-the-hood to run the BuildKit daemon.\nSince I don‚Äôt like automagical things, I immediately tried to run BuildKit\ndaemon myself:</p><pre><code>pacman -S buildkit\nsystemctl --user start buildkit.service\ndocker buildx create --name local unix://$XDG_RUNTIME_DIR/buildkit/rootless\ndocker buildx use local\n</code></pre><p>Now  uses our systemd-managed BuildKit service. But we‚Äôre not\ndone yet! One of the reasons I like Podman is because it‚Äôs daemonless, and\nwe‚Äôve got a daemon running in the background. This isn‚Äôt the end of the world,\nbut it‚Äôd be nicer to be able to run the build without BuildKit.</p><p>Fortunately, there‚Äôs a way around this: any Compose project can be turned into\na JSON description of the build commands called <a href=\"https://docs.docker.com/build/bake/\">Bake</a>. <code>docker buildx bake --print</code> will print that JSON file (and the Docker Compose CLI will use Bake\nfiles if  is set since v2.33). Note, Bake supports way more\nfeatures (e.g. HCL files) but we don‚Äôt really need these for our purposes (and\nthe command above can lower fancy Bake files into dumb JSON ones).</p><p>The JSON file is pretty similar to the  CLI arguments. It‚Äôs not\nthat hard to do the translation, so I‚Äôve written <a href=\"https://github.com/emersion/bakah\">Bakah</a>, a small tool which\ndoes exactly this. It uses <a href=\"https://buildah.io/\">Buildah</a> instead of shelling out to Podman (Buildah\nis the library used by Podman under-the-hood to build images). A few details\nrequired a bit more attention, for instance dependency resolution and parallel\nbuilds, but it‚Äôs quite simple. It can be used like so:</p><pre><code>docker buildx bake --print &gt;bake.json\nbakah --file bake.json\n</code></pre><p>Bakah is still missing the fancier Bake features (HCL files, inheritance,\nmerging/overriding files, variables, and so on), but it‚Äôs enough to build\ncomplex Compose projects. I plan to use it for <a href=\"https://codeberg.org/emersion/soju-containers\">soju-containers</a> in the future,\nto better split my Dockerfiles (one for the backend, one for the frontend) and\nremove the CI shell script (which contains a bunch of Podman CLI invocations).\nI hope it can be useful to you as well!</p>","contentLength":3655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44971212"},{"title":"Why is D3 so Verbose?","url":"https://theheasman.com/short_stories/why-is-d3-code-so-long-and-complicated-or-why-is-it-so-verbose/","date":1755771155,"author":"TheHeasman","guid":235899,"unread":true,"content":"<p>D3 is a b***h of a language at first glance. It‚Äôs long. It‚Äôs complicated and verbose. You have to enter what feels like an obscene amount of key strokes to draw .</p><p>Which means, I<a href=\"https://github.com/david-heasman00/d3-meeks-dufour/compare/2c02ec01723b7313a71d4e616883cef8f50febb9...762e8fad9797fb9eac0bfad246f85d72c108cab3\"> just finished drawing a box plot from chapter 6</a>. Yup. You know. That thing, you just make four or five clicks in Microsoft Excel, and , it appears. In a way that you don‚Äôt think is magical, because Excel has been around longer than the dial up modem. </p><p>I just shared this on my social media, and for the non-technical, I had to explain ‚Äúhey, so, the reason why this is impressive is because‚Ä¶‚Äù</p><p>That‚Äôs  of code to draw a box plot. Why on earth would any masochist do this?</p><p><strong>Because of the flexibility D3 provides</strong>. </p><p>The reason why there are so many lines is because:</p><ol><li>I‚Äôm doing this the long way. I could write some functions or components which could collapse a lot of those lines, but I‚Äôm still learning the ropes. Better to do it the long and manual way so I don‚Äôt skip anything</li><li>This is important: <strong>D3 is a library which allows you to draw SVGs and bind them to data</strong></li></ol><p>All D3 is doing is saying ‚Äúso, hey, I got all this data. Now. Dear browser. I‚Äôd like you to draw these SVG shapes, according to these instructions.‚Äù </p><p>SVG code reads like an extended HTML block of code. To draw a line on a two dimensional grid you have to define:</p><ul><li>x1 ‚Äì the horizontal position of point 1</li><li>x2 ‚Äì the horizontal position of point 2</li><li>y1 ‚Äì the vertical position of point 1</li><li>y2 ‚Äì the vertical position of point 2</li></ul><p>In D3 if I was just going to draw this with fixed values, say a line that goes from co-ordinates (0,12) to (4,15):</p><pre><code>aRandomNameForThisLine\n  .append(\"line\")\n    .attr(\"x1\", 0)\n    .attr(\"x2\", 4)\n    .attr(\"y1\", 12)\n    .attr(\"y2\", 15);</code></pre><p>But the power comes from :</p><pre><code>boxplotContainer\n  .append(\"line\")\n    .attr(\"x1\", xScale(gender) - boxplotWidth/2)                          \n      .attr(\"x2\", xScale(gender) + boxplotWidth/2)\n      .attr(\"y1\", gender === \"Female\"                                       \n        ? yScale(femaleExtent[0])\n        : yScale(maleExtent[0])\n      )\n      .attr(\"y2\", gender === \"Female\"\n        ? yScale(femaleExtent[0])\n        : yScale(maleExtent[0])\n      );</code></pre><p>That block of code draws the bottom horizontal line of the box plot for each of those boxes, getting the values from the data, and adjusted for some other variables that I defined elsewhere.</p><p>It might seem long, but what‚Äôs great about it, is how much you can customise those values. You could make that line a curve if you want. Or you could add little tiny whiskers throughout, so your box plot starts to look like an unruly cat. Whatever. It‚Äôs up to you. </p><p>That‚Äôs what‚Äôs great about D3. It‚Äôs like drawing. Sure you can take a photograph, but isn‚Äôt it cooler to draw something, showing your interpretation of the object in question?</p><p>What I‚Äôm trying to say is:</p><p><strong>D3 is verbose so you can create art. </strong></p><p>With tools like datawrapper and flourish there‚Äôs little need these days to learn D3. However, you aren‚Äôt reading this because you want click and deploy tools. </p><p>And while some tools allow you to do some of that, <em>D3 allows you to do it all</em>. </p><p>And one day, on your learning journey, you‚Äôll love how verbose it is.  </p><p><em>Don‚Äôt mind me, just gonna plug my newsletter below:</em></p><h3>D3 is verbose. This sign-up isn‚Äôt</h3><p><strong>If you‚Äôre here for deep clarity‚Äînot shortcuts‚Äîdrop your email for crisp visual data essays that turn the complexity of life into insights.</strong></p><p><em>Sign-up form not appearing above? Pesky javascript blocker‚Ä¶ <a href=\"https://theheasman.com/sign-up-for-the-heasman-newsletter/\">click here </a>and scroll to the bottom for a vanilla boring mailchimp form that works on all devices. </em></p>","contentLength":3577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44970981"},{"title":"My other email client is a daemon","url":"https://feyor.sh/blog/my-other-email-client-is-a-mail-daemon/","date":1755766446,"author":"aebtebeten","guid":235968,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44970563"},{"title":"Show HN: I replaced vector databases with Git for AI memory (PoC)","url":"https://github.com/Growth-Kinetics/DiffMem","date":1755757211,"author":"alexmrv","guid":235598,"unread":true,"content":"<p>Hey HN! I built a proof-of-concept for AI memory using Git instead of vector databases.</p><p>The insight: Git already solved versioned document management. Why are we building complex vector stores when we could just use markdown files with Git's built-in diff/blame/history?</p><p>Memories stored as markdown files in a Git repo\nEach conversation = one commit\ngit diff shows how understanding evolves over time\nBM25 for search (no embeddings needed)\nLLMs generate search queries from conversation context\nExample: Ask \"how has my project evolved?\" and it uses git diff to show actual changes in understanding, not just similarity scores.</p><p>This is very much a PoC - rough edges everywhere, not production ready. But it's been working surprisingly well for personal use. The entire index for a year of conversations fits in ~100MB RAM with sub-second retrieval.</p><p>The cool part: You can git checkout to any point in time and see exactly what the AI knew then. Perfect reproducibility, human-readable storage, and you can manually edit memories if needed.</p><p>Stack: Python, GitPython, rank-bm25, OpenRouter for LLM orchestration. MIT licensed.</p><p>Would love feedback on the approach. Is this crazy or clever? What am I missing that will bite me later?</p>","contentLength":1223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44969622"},{"title":"Python f-string cheat sheets (2022)","url":"https://fstring.help/cheat/","date":1755752888,"author":"shlomo_z","guid":235689,"unread":true,"content":"<p>The below examples assume the following variables:</p><pre><code>&gt;&gt;&gt; number = 4125.6\n&gt;&gt;&gt; percent = 0.3738\n</code></pre><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr></tbody></table><p>These format specifications only work on all numbers (both  and ).</p><ul><li>Type  with precision  displays  digits after the decimal point.</li><li>Type  with precision  displays  significant digits in scientific notation. Trailing zeros are not displayed.</li></ul><p>These examples assume the following variable:</p><table><thead><tr></tr></thead><tbody><tr></tr></tbody></table><p>An empty type is synonymous with  for integers.</p><p>These format specifications only work on integers ().</p><p>These examples assume the following variable:</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>These format specifications work on strings () and most other types (any type that doesn't specify its own custom format specifications).</p><p>The below modifiers are special syntaxes supported by all object types.\nSome format specifications are also included to show how to mix and match these syntaxes with the  syntax.</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table><p>An empty conversion field is synonymous with , unless a self-documenting expression is used.\nWhen a self-documenting expression is used, an empty conversion field uses .</p>","contentLength":1002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44969221"},{"title":"How to stop feeling lost in tech: the wafflehouse method","url":"https://www.yacinemahdid.com/p/how-to-stop-feeling-lost-in-tech","date":1755739736,"author":"research_pie","guid":235557,"unread":true,"content":"<p>these past few weeks I‚Äôve received a massive amount of dms from CS students being absolutely lost.</p><p>like to a level of lost I‚Äôm feeling their stress and anxiety in my own chest.</p><p>on top of that some of you are going through the absolute worst time of your life I don‚Äôt know how you guys are still standing up but I‚Äôm in awe.</p><p>I found myself reiterating the same advice to at least 7 people now, so I‚Äôll write it all here for those who are too crippled to ask for help.</p><p>hope it‚Äôs useful folks and keep it up.</p><p>I‚Äôm receiving a variation of this, but the common element is always that the person feels completly lost.</p><p>lost because AI will take their job, lost because it seems there is too much to learn, lost because they don‚Äôt know if they actually like this whole tech thing, etc.</p><p>they had the vaguest picture of what they would be doing in 5 years.</p><p>and trying to figure it out by looking at technology is the wrong way to look at it.</p><p>it would be like trying to obsess intensely over hammers, nails, saw to build a house. tech is the same; it‚Äôs just a set of tools to do something.</p><p>enter my favorite method to get your life untangled, which I call the wafflehouse method (bear with me, I swear this blog post is useful).</p><p>I‚Äôll detail here how to use it, but remember that untangling your life goals is a deeply personal process. </p><p>take inspiration from this, but feel free to modify so that it feels good.</p><p>call in sick, cancel your meetings and plans for two days.</p><p>straight in the middle of the week.</p><p>don‚Äôt tell your kids, wife, moms or friends that you are taking off.</p><p>you are going to go on a deeply personal introspective adventure and the only one invited is you.</p><p>48h to find yourself in the whole year is the least you can do.</p><p>sorry for the image but yes that‚Äôs what you gonna do.</p><p>pick your favorite spot, the one you feel very good to be at, and can spend a whole day by yourself.</p><p>can‚Äôt give you advice on picking a spot just make sure that wherever you are going has deep significance for yourself.</p><p>now sit alone with your notebook and be quiet for a while.</p><p>try to imagine this one image: yourself 5 years from now.</p><p>not as of how you are going to get there from the current trajectory you are following, but how you would feel proud to be.</p><p>imagine it vividly, it may be still very abstract, but try to dive a bit in your mind what is going on in this vision.</p><p>does it have to be tech related?</p><p>no absolutely not, on the contrary.</p><p>imagine yourself at the pinacle where you really feel proudness bursting within your chest. this looks different for everyone. </p><p>try to remove anything attached to other expectations of yourself. doesn‚Äôt matter what your moms and dads want of you if you feel like a husk inside.</p><p>now that you have something to latch upon, write and write and write. write everything, how you feel now, how you feel then, what is different from now, what you regret, the things you think are not realistic, that one hunk of shame you have been carrying over forever, all these fears and hope lay them all down.</p><p>write and write until there is nothing more to be said.</p><p>write it all up until you have colors and shape of things you want to come to pass.</p><p>it doesn‚Äôt have to be clear for now, it just needs to be vivid.</p><p>when you are satisfied with this painting, it‚Äôs time to head home for a good day rest you are going to need it.</p><p>now is time to enter the wafflehouse (or whatever regional equivalent you have mine is called cora).</p><p>make sure that wherever you are going can tolerate you for a whole day because you are going to be squatting there opening until closing.</p><p>bring you computer, your notepad and this image of yourself.</p><p>now that you are comfortably sitting with your beverage of choice it‚Äôs time to refine the image you painted into 2-3 clear goals that happen in 5 years time.</p><p>the goals need to be very specific and detailed.</p><p>if you feel absolutely ridiculous writing them down, it‚Äôs maybe too much, but don‚Äôt hold back. the goals need to match the image.</p><p>write as much detail as you can about each of these goals and make sure that there are as few as possible while still retaining the essence of the image.</p><p>now that you have these 2-3 5-year goals you interpolate what you need to achieve in 3 years time to be on track to hit them.</p><p>it‚Äôs ok if your interpolation is a bit crooked, you just need to trace a course here.</p><p>if you feel the 3 years are ridiculous, feel free to change the 5 years and adjust the knob of your dream self.</p><p>now you do the same on the 1-year goal.</p><p>you are for sure going to do some back and forth between the 1-3-5 years adjusting things as you go.</p><p>try to keep the number of goals as few as possible.</p><p>if they sprawl around, it might be best for you to reduce the scope of your image a bit. for instance trying to be a nobel prize winner marathon karateka black belt might be too much.</p><p>focus on one aspect if you see your goals proliferating, the one that is more important to you.</p><p>I know it‚Äôs tough to make a call here, but you have to be somewhat pragmatic at some point.</p><p>btw the goals aren‚Äôt starting in like january 1st, they start now now.</p><p>now you are going into the details for real. you‚Äôre going to make a DAG.</p><p>they look like this usually notice that there are no cycle (for the rest of the blog view them as the node on the left being at the top and the node on the right being at the bottom).</p><p>at the top it‚Äôs where you are at. put some numbers in there about your current situation (e.g. maybe you are broke).</p><p>at the bottom put your 1-year goals (e.g. maybe you are not broke anymore)</p><p>now in between write as many steps as possible you think you need to take in a year's time to go from your current situation to the end goals (remember there is more than one goal usually) based on what you have right now and the fact that luck is against you.</p><p>this last bit is important. your whole plan is based on the fact that every damn day you will wake up will be a bad day.</p><p>sun is shining exactly 0 times for 5 years.</p><p>create a 100s of these steps that flow logically from your current node to the end node.</p><p>some will be more vague than others. these are your knot points.</p><p>for each of your knot points node you should research to the best of your current ability how to unknot them.</p><p>sometime it‚Äôs because it‚Äôs actually 5 nodes mushed together and you don‚Äôt know it‚Äôs 5 steps.</p><p>sometime it‚Äôs because it‚Äôs the wrong step and you are acting on false prior.</p><p>doesn‚Äôt matter try to untangle them as much as possible so that you have 100s of very highly defined step from start to finish.</p><p>as highly super duper detailed as possible.</p><p>not the other month which will start in 3 weeks.</p><p>take whatever first steps there is in this directed acyclic graph and put it as stuff to do in the month you have left.</p><p>here you have to be realistic about what you can chew, but remember that these tasks are what bring you closer to what you want to be.</p><p>that image that made you feel good remember?</p><p>yeah that‚Äôs the path you are rolling in now so take as much tasks as you can from the list and put it in the month.</p><p>cancel other stuff if you can to have more time to work on these.</p><p>you thought we would stop at monthly!</p><p>take whatever you picked for the month and bring them into your weekly goals for this week.</p><p>remember when we are now at the wafflehouse? in the middle of the week? well good news you got 3 days left to get these tasks done.</p><p>and before I see any of you start to spend 3 weeks making the perfect notion dashboard dingy NO!</p><p>you take whatever you have right NOW I don‚Äôt care if it‚Äôs on a notebook.</p><p>you spend 0 minutes optimizing your project management here, you take all the time into actually doing the tasks towards a future you are proud of.</p><p>you should be getting dangerously close to getting evicted from the wafflehouse so take the little time that is left to look at your weekly goals and pick a few tasks to work on tomorrow.</p><p>the waitress should be looking at you like this.</p><p>at this point thing should be so concrete that even if you were to not remember your yearly goals it wouldn‚Äôt matter at all.</p><p>every day from now on you will have two category of things to do:</p><ol><li><p>stuff that actually brings you closer to where you want to be in life </p></li></ol><p>things that your mom wants you to do, the stuff that this one friends always put on you or that your boss decided you were responsible for.</p><p>I‚Äôm not saying to throw all of this in the garbage (. . .).</p><p>but you have to make conscious decisions that you have a plan that is yours and that it is up to you to get it done.</p><p>now you‚Äôre done, it‚Äôs time to work and this work is your life's work.</p><p>you don‚Äôt need to parade this mega plan to your relatives it‚Äôs a deeply personal one.</p><p>it really doesn‚Äôt matter because they should start to see the difference anyway since this plan is one where you will transform yourself into what you truly want to be.</p><p>every day before going to bed take some of the stuff from this week and put it in your bucket for the next day.</p><p>when you wake up look at the task your past self is imploring you to get done and oblige.</p><p>every week on a quiet sunday afternoon, tend to this plan like a garden. </p><p>look out at the work done in the week, check what need to be done in the month, revise a bit the directed acyclic graph or the 1-3-5 years goals.</p><p>make the small tweaks based on your learning from the week that passed.</p><p>and when you still feel good about the trajectory and that this image still bring a smile to your soul pick a few tasks for next week.</p><p>you have to be careful though.</p><p>if you actually do this you get a very stressful side effect:</p><ul><li><p>there is no one to blame but yourself</p></li></ul><p>it‚Äôs not your mom, it‚Äôs not your teacher, it‚Äôs not your friends, it‚Äôs you.</p><p>from that point going forward you have your life in your hands and it‚Äôs time to act like it.</p><p>if this was useful to you in anyway hit me up periodically with the update.</p>","contentLength":9817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44968190"},{"title":"A statistical analysis of Rotten Tomatoes","url":"https://www.statsignificant.com/p/is-rotten-tomatoes-still-reliable","date":1755735007,"author":"m463","guid":235613,"unread":true,"content":"<p>I stayed in a hotel recently, which means I watched cable television, which means I consumed commercials that I could not skip‚Äîand some of these commercials advertised upcoming movie releases. Promo after promo, I noticed an unmistakable pattern: every film was \"Certified Fresh\" on Rotten Tomatoes, with this seal of approval serving as the ad's climactic selling point.</p><p>After five days of \"Certified Fresh\" movie propaganda, I began to grow suspicious. If every movie is un-rotten, then one of two things must be true:</p><ol><li><p><strong>Humanity Has Stopped Producing Bad Art: </strong><em>Cats, Space Jam: A New Legacy</em></p></li><li><p><strong>Rotten Tomatoes Has Changed:</strong></p></li></ol><p>So today, we'll delve into the suspicious recalibration of Rotten Tomatoes, tracing when and how Hollywood's foremost stamp of artistic excellence turned rotten.</p><p>Rotten Tomatoes was founded in 1998 to aggregate reviews of Jackie Chan films. Within months, its creators recognized the concept's broader potential and expanded the platform to cover all movies. The website quickly became a trusted proxy for critical consensus, a role it has maintained for over 25 years.</p><p>Rotten Tomatoes appraises movies through its trademarked \"Tomatometer\" score, calculated as the percentage of critic reviews that are deemed \"positive.\" Both a lukewarm 3-out-of-5 star rating and an effusive rave of cinematic brilliance qualify as \"positive\" on the platform. Films with a Tomatometer score above 60% receive a \"fresh\" label; anything below this threshold is branded \"rotten.\"</p><p><a href=\"https://pro.morningconsult.com/articles/america-can-stomach-rotten-tomatoes\" rel=\"\">nearly a third of Americans have checked the Tomatometer before seeing a film</a></p><p>In a well-calibrated system, critic scores should remain stable over time unless there's a meaningful shift in film quality. So why has the average Tomatometer score increased over the past decade? What changed: the movies or the metric?</p><p>And here is where the intrigue deepens: this rating shift coincides with Fandango's 2016 acquisition of Rotten Tomatoes üôâ. One might consider this a conflict of interest, given Fandango is America's largest movie-ticketing platform, partially owned by NBCUniversal and Warner Bros. Discovery (and that person would be correct!).</p><p>If I had a corkboard for tracking this conspiracy theory, it would be incredibly lame: I'd have a picture of Rotten Tomatoes and a photo of Fandango, with a single spool of yarn connecting the two. This does not \"go all the way to the top,\" and there is probably no mention of these dealings in The Epstein Files.</p><p>While these two data points paint an unflattering portrait of Rotten Tomatoes, they do not prove whether the site's aggregation process or scoring methods have fundamentally changed. To test this, I decided to look at the relationship between Rotten Tomatoes' critic score and its user-generated audience score (the latter aggregates reviews from moviegoers who have seen a film).</p><p>While critics and audiences don't always agree, their sentiments tend to be strongly correlated‚Äîa wave of critical pans usually signals similar disdain from viewers, and vice-versa. Before 2016, a higher audience score generally went hand in hand with a strong Tomatometer rating.</p><p>On a year-to-year basis, critic and audience scores moved in tandem, demonstrating a stable correlation until 2016, when a sharp divergence emerged‚Äîjust as the average Tomatometer rating began to climb.</p><p>The inevitable follow-up question is how one alters the Tomatometer within the site's seemingly strict parameters.</p><p>Ultimately, Rotten Tomatoes has control over two major inputs to its Tomatometer score:</p><ol><li><p>Whether a review is considered \"fresh\" or \"rotten.\"</p></li><li><p>Which reviews count toward the Tomatometer.</p></li></ol><p>Tweaking the definition of \"fresh\" would spark immediate backlash from the site's critic base, who could use their platforms to publicly bash Rotten Tomatoes. A much subtler lever is critic selection: expanding the reviewer pool to a group of writers who (coincidentally) produce more favorable appraisals.</p><p>Indeed, following the Fandango acquisition, the average number of reviewers per mainstream release increased by 40 to 70 critics.</p><p>It's feasible that the platform broadened its critic pool to account for newfangled digital media outlets like The Ringer and BuzzFeed, thereby allowing Rotten Tomatoes to evolve with an ever-changing media ecosystem. Yet when we examine the most prolific publications added to the site over the last decade, we find a collection of outlets that lack name recognition.</p><p>As someone who also writes words on the internet, I'm not going to go out of my way to dunk on Denerstein Unleashed and KKFI-FM (Kansas City), but I will say this after reviewing around 50 of these sites:</p><ul><li><p>Many of these publications are hosted on outdated blogging platforms.</p></li><li><p>Several of these sites do not load properly on my mobile web browser.</p></li><li><p>Many of these blogs seem like secondary sources of income or passion projects.</p></li></ul><p>To account for this influx of reviewers, Rotten Tomatoes has created a \"Top Critic\" designation reserved for established media outlets, such as The New York Times and The Atlantic. However, this label has no special bearing on a film's top-line Tomatometer score and is largely incorporated into ancillary aspects of the site. Rotten Tomatoes claims that these reviewer additions were made to diversify its critic pool by including more women, people of color, and underrepresented groups‚Äîa statement I can neither confirm nor deny. What I can say is this: these new reviewers fundamentally altered the site's steady state, and the platform did little to account for this underlying shift. Maybe increased critic scores are a happy accident, albeit one with extremely suspicious timing.</p><p><a href=\"https://www.vulture.com/article/rotten-tomatoes-movie-rating.html\" rel=\"\">PR firms will actively court reviewers from smaller outlets to inflate Tomatometer scores</a></p><p>I don't have any hard data to verify Vulture's claims, but one could see how an expanded pool of Tomatometer-approved hobbyists might be ripe for manipulation. With the right mix of critics, a \"fresh\" rating can be engineered to coincide with a film's opening weekend.</p><p><em>maybe Rotten Tomatoes being (ostensibly) rigged isn't such a bad thing? </em></p><p>Enter Rotten Tomatoes, a debatably flawed yet widely influential website that is a questionable proxy for critical acclaim. For whatever reason, people trust this platform and will decide to leave their homes based on its recommendation. Rotten Tomatoes is the focal point of television promos because it can create demand where it previously did not exist.</p><p>If I were the owner of a movie-ticketing app or a sicko-utilitarian like Sam Bankman-Fried, I'd argue the ends justify the means: the average score goes up, more movies are deemed \"fresh,\" people go to theaters, and cinema lives to fight another day. Unfortunately, I am not a sicko-utilitarian, which means I am now advocating against my best interests. Hooray for me.</p><p>In the short term, inflated scores may lure people to theaters once or twice. But in the long run, it's better if people enjoy their experience at the theater (by, you know, seeing a good movie). Despite my lizard-brain reaction, I believe long-term thinking usually wins out, while short-term shenanigans are always rotten in hindsight. Although what was once deemed \"rotten\" is now considered \"certifiably fresh,\" so who knows.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://www.statsignificant.com/p/is-rotten-tomatoes-still-reliable?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p><em>This post is public so feel free to share it.</em></p></div></div><p>Struggling to turn your data into actionable insights? Need expert help with a data or research project? Well, Stat Significant can help. </p><ul><li><p><strong>üîç Insights That Improve Performance: </strong></p></li><li><p><strong>üìä Dashboards That Drive Action:</strong></p></li><li><p><strong>‚öôÔ∏è Data Architecture That Automates Reporting</strong></p></li></ul><p><em></em></p>","contentLength":7498,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44967796"},{"title":"Code review can be better","url":"https://tigerbeetle.com/blog/2025-08-04-code-review-can-be-better/","date":1755731437,"author":"sealeck","guid":234794,"unread":true,"content":"<p>Slightly unusual genre today: a <a href=\"https://github.com/tigerbeetle/tigerbeetle/pull/3129\">negative\nresult</a> about our <a href=\"https://github.com/tigerbeetle/tigerbeetle/pull/2732\"></a>\ntool for a different take on code review process, which we decided to\nshelve, at least for the time being.</p><p>A lot of people are unsatisfied with GitHub‚Äôs code review process.\nOne of the primary issues is that GitHub poorly supports stacked pull\nrequests and <a href=\"https://gist.github.com/thoughtpolice/9c45287550a56b2047c6311fbadebed2\">interdiff\nreviews</a>. While I also see interdiff as valuable, it‚Äôs not the reason\nwhy I decided to experiment with . I have two\nother problems with GitHub, and with every single other code review\nsystem, with the exception of <a href=\"https://www.janestreet.com/tech-talks/janestreet-code-review/\">the\nthing that Jane Street uses internally</a>:</p><ul><li>review state is not stored as a part of repository itself,</li><li>review is done via remote-first web-interface.</li></ul><p>Let‚Äôs start with the second one.</p><p>By the way of analogy, I don‚Äôt use GitHub‚Äôs web editor to write code.\nI clone a repository locally, and work in my editor, which is:</p><ul><li>fully local, memory/nvme latencies, no HTTP round-trips,</li><li>tailored to my specific odd workflow.</li></ul><p>When I review code, I like to pull the source branch locally. Then I\nsoft-reset the code to mere base, so that the code looks as if it was\nwritten by me. Then I fire up magit, which allows me to effectively\nnavigate both through the diff, and through the actual code. And I even\nuse git staging area to mark files I‚Äôve already reviewed:</p><p>Reviewing  rather than diff is so powerful: I can run\nthe tests, I can go to definition to get the context, I can try out my\nrefactoring suggestions in-place, with code completion and the other\naffordances of my highly sophisticated code editor.</p><p>Alas, when I want to actually leave feedback on the PR, I have to\nopen the browser, navigate to the relevant line in the diff, and (after\nwaiting for several HTTP round-trips) type my suggestion into a text\narea. For some reason, the text area also regularly lags for me,\nespecially on larger diffs.</p><p>Two things are wrong here. On the interface side, review feedback is\ntext related to the code. The most natural interface is to just leave\nreview comments as inline comments in the code, or even to fix the code\ndirectly:</p><div><pre><code></code></pre></div><p>And on the implementation side, because the data is stored in a\nremote database, rather than in a local git repository, we get all those\nlatency-inducing round-trips (not to mention vendor lock in).</p><ul><li>Code review is a single commit which sits on top of the PR\nbranch.</li><li>That commit adds a bunch of code comments with specific\nmarkers.</li><li>Review process involves both the author and the reviewer modifying\nthis top commit (so, there‚Äôs a fair amount of\n<code>git push --force-with-lease</code> involved).</li><li>The review concludes when all threads were marked as\n and an explicit revert commit is added on top\n(such that review is preserved in the history).</li></ul><p>I had a hope that ‚Äúcode review is just a commit‚Äù would be the secret\nto keep implementation complexity low. Sadly, the devil is in the\ndetails in this particular case.</p><p>The basic idea, that reviewing is leaving comments in code, works as\nwell as I had expected (that is, it‚Äôs really, really awesome). But\nmodifying code under review turned out to be tricky. If a reviewer\nrequests a change, and you apply it to some deep commit, or even add a\nnew commit on top, you now have to solve mere conflicts with the review\ncomments themselves, as they are often added at the <a href=\"https://git-scm.com/book/en/v2/Git-Tools-Interactive-Staging\">hunk</a>\nboundaries. And then, while  is workable,\nit also adds friction. There is an impedance mismatch here, where, for\ncode, we want very strong, hash-chained intentional sequence of\nstate-transitions, while for review we would be more happy with more lax\nconflict-free merging rules. It  be solved with more\ntooling to ‚Äúpush‚Äù and ‚Äúpop‚Äù review comments on top of pristine review\nbranch, but that seems to push well beyond my 500 line limit.</p><p>Then, there‚Äôs a second change. It seems like <a href=\"https://lore.kernel.org/git/CAESOdVAspxUJKGAA58i0tvks4ZOfoGf1Aa5gPr0FXzdcywqUUw@mail.gmail.com/T/#u\">upstream\ngit might be getting a Gerrit-style Change-Id</a> for tracking revisions\nof a single commit over rebases. If that happens, we might actually get\nfirst class support for per-commit interdiff review! But that would be\nsomewhat incompatible with  approach, which adds\nan entire separate commit to the branch. But, perhaps, in the\n world, we could be adding review comments to the\ncommits themselves, and, rather that adding a revert at the conclusion\nof review, instruct git to store all revisions of a particular\n.</p><p>Anyway, we are begrudgingly back to web-interface based code reviews\nfor now. Hopefully someone is inspired enough to fix this properly one\nday!</p><p>If you‚Äôve been thinking along similar lines, the following links are\nworth checking out:</p><ul><li><a href=\"https://fossil-scm.org/home/doc/trunk/www/index.wiki\">Fossil</a>\nis an SCM system which stores everything in the repository.</li><li><a href=\"https://gerrit-review.googlesource.com/Documentation/note-db.html\">NoteDb</a>\nbackend for Gerrit. Gerrit started with tracking review state in a\nseparate database, but then moved storage into git.</li><li><a href=\"https://github.com/git-bug/git-bug\">git-bug</a> uses git to\nstore information about issues.</li><li><a href=\"https://doc.dxuuu.xyz/prr/index.html\">prr</a> which\nimplements in-editor review interface on top of GitHub‚Äôs Web API</li><li><a href=\"https://pr.pico.sh\">git-pr</a> similar project in spirit\nthat leverages git native features to replace the entire pull request\nworkflow.</li></ul>","contentLength":4928,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44967469"},{"title":"Vibe coding creates a bus factor of zero","url":"https://www.mindflash.org/coding/ai/ai-and-the-bus-factor-of-0-1608","date":1755726425,"author":"AntwaneB","guid":234778,"unread":true,"content":"<p><em>All the opinions expressed in this article and on this website are entirely my own and do not represent my employer in any way.</em></p><p>This article was discussed on HackerNews in August 2025, you can go see the comments on <a href=\"https://news.ycombinator.com/item?id=44966856\" data-wpel-link=\"external\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\">the HackerNews post</a>.</p><p>Ever heard about the ‚ÄúBus factor‚Äù? It is a concept that measures the risk of losing all knowledge about a particular thing ‚Äì a software development project for example ‚Äì by estimating how many team members could get crushed by a bus before nobody knows how to work on the project anymore. As an example, if 3 people on your team know how to restore a backup of your database, the Bus Factor for that particular function in 3.</p><p>Since the dawn of humanity, even long before buses existed, the Bus Factor has always had a ‚Äúworst case‚Äù value of 1. If the sole keeper of a piece of knowledge came to pass, the knowledge was lost, unless it had been transferred previously.</p><p>And humanity has worked hard to keep itself far from this Bus Factor of 1. Brown-bag sessions, documentation, video tutorials, knowledge handovers, demos and showcases, without forgetting , and many more mechanisms in which an uncountable number of man-hours has been sunk.</p><p>But on the 30th of November 2022, all of this changed, and suddenly a large part of humanity became perfectly fine with a Bus Factor not just of 1, but of .</p><p>That date corresponds to the release of ChatGPT to the public, and the start of the mass-market adoption of GenAI. It‚Äôs also the birth of what would become 3 years later the concept of ‚ÄúAI first‚Äù.</p><p>One might think that ‚ÄúAI first‚Äù would leave humans second, but, unsurprisingly, delegating the creation process to machines has instead left us nowhere to be found when it comes to knowledge keeping.</p><p>Focusing a bit on programming, it seems like a growing part of the industry is now happy to let LLMs generate functions, entire features, or even complete projects (<a href=\"https://news.ycombinator.com/item?id=44157131\" data-wpel-link=\"external\" target=\"_blank\" rel=\"nofollow external noopener noreferrer\">security holes included</a>). They have moved on from understanding their code-base and preserving this knowledge to actively trying to avoid having any piece of knowledge about their project to begin with, preferring instead to ‚Äúvibe‚Äù.</p><h2>Where the bus hits a wall</h2><p>We can leave aside the flaws of vibe-coding and the issues with LLM-generated code in general for another article. Indeed, the quality of the generated code does not really matter here. It is obviously easier to understand code that you have never seen before if it is good code, but ultimately reading code remains much more complex than writing it no matter what.</p><p>Before LLMs, provided that your team did some of their due diligence, you could always expect to have some help when tackling new code-bases. Either a mentor, or at least some (even if maybe partially outdated) documentation. With LLMs, this is gone. The only thing you can rely on is on your ability to decipher what a highly imperfect system generated, and maybe ask explanations to that same imperfect system about  its code (oh, and it has forgotten everything about the initial writing process by then).</p><p>Imagine having to solve bugs, add new features, patch security holes and upgrade dependencies in a piece of software that nobody on Earth has even the faintest idea about how it was built and why it was built that way.</p><p>Now, imagine being the user uploading personal documents, credit card information, private photos or thoughts to a piece  of software that nobody on Earth has even the faintest idea about how it was built and why it was built that way.</p><p>Because of the situation of a Bus Factor of zero that it creates, vibe coding is fundamentally flawed. That is, only until there is an AI that can generate 100% accurate code 100% of the time, and it is fed 100% accurate prompts.</p><p>If vibe coding isn‚Äôt for you and you want to read more articles, check out my category dedicated to advice about <a href=\"https://www.mindflash.org/category/coding/learn-programming\" data-wpel-link=\"internal\">learning programming</a>.</p>","contentLength":3863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44966856"},{"title":"Show HN: PlutoPrint ‚Äì Generate PDFs and PNGs from HTML with Python","url":"https://github.com/plutoprint/plutoprint","date":1755722278,"author":"sammycage","guid":234780,"unread":true,"content":"<p>Hi everyone, I built PlutoPrint because I needed a simple way to generate beautiful PDFs and images directly from HTML with Python. Most of the tools I tried felt heavy, tricky to set up, or produced results that didn‚Äôt look great, so I wanted something lightweight, modern, and fast. PlutoPrint is built on top of PlutoBook‚Äôs rendering engine, which is designed for paged media, and then wrapped with a Python API that makes it easy to turn HTML or XML into crisp PDFs and PNGs. I‚Äôve used it for things like invoices, reports, tickets, and even snapshots, and it can also integrate with Matplotlib to render charts directly into documents.</p><p>I‚Äôd be glad to hear what you think. If you‚Äôve ever had to wrestle with generating PDFs or images from HTML, I hope this feels like a smoother option. Feedback, ideas, or even just impressions are all very welcome, and I‚Äôd love to learn how PlutoPrint could be more useful for you.</p>","contentLength":932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44966170"},{"title":"Introduction to AT Protocol","url":"https://mackuba.eu/2025/08/20/introduction-to-atproto/","date":1755717189,"author":"psionides","guid":234772,"unread":true,"content":"<p>Some time ago I&nbsp;wrote a long blog post I&nbsp;called ‚Äú<a href=\"https://mackuba.eu/2024/02/21/bluesky-guide/\">Complete guide to Bluesky</a>‚Äù, which explains how all the user-facing features of Bluesky work and various tips and tricks. This one is meant to be a bit like a developer version of that ‚Äì I&nbsp;want to explain in hopefully understandable language what all the pieces of the network architecture are and how they all fit together. I&nbsp;hope this will let you understand better how Bluesky and the underlying protocol works, and how it differs from e.g. the Fediverse. This should also be a good starting point if you want to start building some apps or tools on ATProto.</p><p>This post is a first part of a series ‚Äì next I&nbsp;want to look at some comparisons with the Fediverse and some common misconceptions that people have, and look at the state of decentralization of this network, but that was way too much for one post; so this one focuses on the ‚ÄúATProto intro tutorial‚Äù part.</p><p>But before we start, a little philosophical aside:</p><h2>What is ‚ÄúBluesky‚Äù? Which ‚ÄúBluesky‚Äù are we talking about?</h2><p>Discussions about Bluesky sometimes get a little confusing because‚Ä¶ ‚ÄúBluesky‚Äù could mean a few different things. Language is hard.</p><p>First, we have Bluesky the company, the team. Usually, when people want to clarify that they‚Äôre talking about the group of people or the organization, they say ‚ÄúBluesky PBC‚Äù (PBC = Public Benefit Corporation), or ‚ÄúBluesky team‚Äù.</p><p>(If you want to read a bit about where Bluesky came from and what‚Äôs the current state of the company, read <a href=\"https://mackuba.eu/2024/02/21/bluesky-guide/#what-is-bluesky\">these two sections</a> in the Bluesky Guide blog post.)</p><p>And we also have Bluesky the product, the social network, the thing that they‚Äôve built. This network is not a single black box like Twitter or Facebook are (despite what they say about it on Mastodon), it‚Äôs more like a set of separate and actually very transparent boxes.</p><p>The system they‚Äôve built, of which Bluesky was initially meant to be just a tech demo, is called the <strong>Authenticated Transfer Protocol</strong>, or AT Protocol, or ATProto. Bluesky is built on ATProto, and it is in practice a huge part of what ATProto currently is, which makes the boundary between Bluesky and non-Bluesky a bit hard to define at times, but it‚Äôs still only a subset of it.</p><p>Bluesky in this second meaning is some nebulous thing that consists of: the data types (‚Äú<a href=\"https://mackuba.eu/2025/08/20/introduction-to-atproto/#lexicons\">lexicons</a>‚Äù) that are specific to the Bluesky microblogging aspect of ATProto, like Bluesky posts or follows; the APIs for handling them and for accessing other Bluesky-specific features; the rules according to which they all work together; and the whole ‚Äúsocial layer‚Äù that is created out of all of this, the virtual ‚Äúplace‚Äù ‚Äì the thing that people have in mind when they say ‚Äúthis website‚Äù, even when it‚Äôs accessed through a mobile app. One of the coolest things about Bluesky &amp; ATProto, in my opinion, is that it connects many different independent pieces into something that still feels like one shared virtual space.</p><p>People outside the company can create (and are creating) other such things on ATProto that aren‚Äôt necessarily Bluesky-related ‚Äì see e.g. <a href=\"https://whtwnd.com\">WhiteWind</a> or <a href=\"https://leaflet.pub\">Leaflet</a> (blogging platforms), <a href=\"https://tangled.sh\">Tangled</a> (GitHub alternative), <a href=\"https://frontpage.fyi\">Frontpage</a> (Hacker News style link aggregator), or <a href=\"https://grain.social\">Grain</a> (photo sharing site). They use the same underlying mechanisms that are at the base of ATProto, but use separate data types, have different rules, goals, and UIs. How do we call these things as a whole, the different sets of ‚Äúdata types + rules + required servers + client apps‚Äù that define different use cases of the network?</p><p>Bluesky team usually calls them ‚Äúapps‚Äù, but I‚Äôm not a big fan of this term, because ‚Äúapp‚Äù kinda implies a client app, and that‚Äôs just one small piece of it. I&nbsp;sometimes call them ‚Äúservices‚Äù ‚Äì though it‚Äôs probably not perfect either, since it implies just the server part in turn. Suggestions welcome&nbsp;:) (I‚Äôm mentioning this at the beginning, because this is something that many different parts are related to.)</p><p>Personally, when I&nbsp;say ‚Äúthe Bluesky app‚Äù, I&nbsp;will generally mean the actual client app (mobile / webapp), not the ‚Äúservice‚Äù, and when I&nbsp;say ‚ÄúBluesky-specific‚Äù, I&nbsp;will mean the ‚Äúservice‚Äù, not the company; and ‚ÄúBluesky-hosted‚Äù will mean run by Bluesky the company. Hopefully in most cases, it can be guessed from context.</p><p>BTW, the commonly accepted term for the whole shared ‚Äúmultiverse‚Äù of all ATProto apps is ‚ÄúThe&nbsp;Atmosphere‚Äù, or ‚ÄúATmosphere‚Äù (though I&nbsp;much prefer the former personally, the weird capitalization bugs me somehow&nbsp;;). It was <a href=\"https://bsky.app/profile/did:plc:bnqkww7bjxaacajzvu5gswdf/post/3k26nw6kwnh2e\">coined</a> by someone from the community, but was accepted by the team and is now mentioned on the <a href=\"https://atproto.com/guides/glossary#atmosphere\">official atproto site</a>.</p><p>Let‚Äôs start with defining the various building pieces of the protocol:</p><p>The most basic piece of the ATProto world is a . Records are basically JSON objects representing the data about a specific entity like a post or profile, organized in a specific way. A post/reply, repost, like, follow, block, list, entry on a list, user profile info ‚Äì each of these is one record. Most public actions you take on Bluesky, like following someone or liking a post, are performed by creating a record of an appropriate type (or editing/deleting one created before).</p><p>Records are stored on disk and transferred between servers in a binary format called <a href=\"https://cbor.io\">CBOR</a>, although in most API&nbsp;endpoints they‚Äôre returned in a JSON form (they are equivalent, just different encodings of the same data).</p><p>The key thing about records, which has very real consequences for user-facing features, is that you can only create and modify  records, not those owned by others (and there are no ‚Äúshared‚Äù records at the moment, each record is owned by a specific account). This means that e.g. when you follow someone, you create a follow record on your account, and that other person can‚Äôt delete your record, which is why there‚Äôs currently no ‚Äúsoft-blocking‚Äù feature, i.e. you can‚Äôt make someone stop following you (though you can block them). There are workarounds though, as I‚Äôll explain later in the <a href=\"https://mackuba.eu/2025/08/20/introduction-to-atproto/#appview\">AppView section</a>.</p><p>This also means that there‚Äôs often an unexpected assymetry between seemingly similar actions: for example, getting a list of people followed by person X is very simple (they‚Äôre all X‚Äôs records, so they‚Äôre all in one place), but getting a list of all followers of X is much harder (each record is in a different place!). This is something that the AppView helps with too, as we‚Äôll see later.</p><p>A second, complimentary way of storing user data is . Blobs are basically binary files, meant mostly for storing media like images and video. For example, here is a direct link to an <a href=\"https://lab.martianbase.net/xrpc/com.atproto.sync.getBlob?did=did:plc:oio4hkxaop4ao4wz2pp3f4cr&amp;cid=bafkreib7vmhsk7w36bmrlwi2mjgkkoq44xysdahi226re2a76rlmgamgvu\">image blob</a> showing a photo of when I&nbsp;started writing this blog post. Blobs are stored on the same server as records, but somewhat separate from them, since it‚Äôs a different type of data.</p><p>Each record belongs to a specific ‚Äúrecord type‚Äù and stores its data organized in a specific structure, which defines what kinds of fields it can have with what types, what they mean, which are required, and so on ‚Äì kind of like XML/JSON Schema. This schema definition which describes a given record type is called a  in ATProto. (If you‚Äôre curious why make a new standard, see threads e.g. <a href=\"https://blue.mackuba.eu/skythread/?author=did:plc:ragtjsm2j2vknwkz3zp4oxrd&amp;post=3juoxe37rez2q\">here</a>, <a href=\"https://blue.mackuba.eu/skythread/?author=did:plc:ragtjsm2j2vknwkz3zp4oxrd&amp;post=3kjgebkayik2g\">here</a>, or <a href=\"https://blue.mackuba.eu/skythread/?author=did:plc:ragtjsm2j2vknwkz3zp4oxrd&amp;post=3jvf7bakmm22h\">here</a>, or <a href=\"https://www.pfrazee.com/blog/why-not-rdf\">this blog post</a>).</p><p>A lexicon needs to have an identifier (called , Namespace Identifier), which uses the reverse domain name format, e.g. . All lexicons that are used to store the data of a specific app are usually grouped under the same prefix, e.g. Bluesky lexicons all start with .</p><p>The structure of a given lexicon‚Äôs records is defined in a special JSON file ‚Äì for example, this file defines the <a href=\"https://github.com/bluesky-social/atproto/blob/main/lexicons/app/bsky/feed/post.json\">app.bsky.feed.post lexicon</a>. As you can see, this is the place which for example specifies that a post‚Äôs text can have at most 300 characters (more specifically, Unicode graphemes). This also means that you can‚Äôt create a different server which would make posts longer than 300 characters that would be Bluesky-compatible and displayed on <a href=\"https://bsky.app\">bsky.app</a> ‚Äì such posts would not pass the validation against the post record schema, and would be rejected by any server or client which performs such validation. Essentially, whover designs and controls the given lexicon, decides what kinds of data it can hold and any constraints on it. In order to store a different, incompatible type of data, you need to create a new lexicon (although you  add additional fields to a record that aren‚Äôt defined in its lexicon; many third party apps are doing that, like e.g. <a href=\"https://github.com/snarfed/bridgy-fed/issues/1092#issuecomment-2164027121\">Bridgy Fed</a>).</p><p>Lexicon name prefixes generally define boundaries between ‚Äúapps‚Äù as in ‚Äúservices‚Äù, and between the ‚Äúterritory‚Äù that‚Äôs owned by different parties. The lexicons and endpoints defined by Bluesky are defined either under  ‚Äì these are things specific to Bluesky the microblogging service ‚Äì or under , which are things meant to be used by all ATProto apps and services regardless of the use case. There are also a couple of other minor namespaces like  for the (centralized) DM service, and  for the open source <a href=\"https://github.com/bluesky-social/ozone/\">Ozone moderation tool</a>.</p><p>The lexicon prefix is generally (in most cases) a good way to tell if a piece of the protocol is something Bluesky-specific (specific to the Bluesky service), or something general for all ATProto. There are no record types defined in , so things like post, profile, follow are all Bluesky-specific and under , as are APIs for e.g. searching users, getting timelines, custom feeds and so on. Meanwhile,  APIs deal more with things like: info about a repository, fetching a repository, signing up for a new account, refreshing an access token, downloading a blob, etc.</p><p>Third party developers and teams building apps on ATProto/Bluesky, which either extend Bluesky‚Äôs features or make something completely separate, use their own namespaces for new lexicons, like , , , , and so on. (There is a lot of nuance to whether you should use your own lexicons or reuse or extend existing ones when building things, and there have been a lot of discussions about it on Bluesky, and even conference talks. A good starting point is <a href=\"https://www.pfrazee.com/blog/lexicon-guidance\">this blog post</a> by Paul Frazee.)</p><p>Each user is uniquely identified in the network with their <strong>Decentralized Identifier (DID)</strong>. DIDs are a <a href=\"https://www.w3.org/TR/did-1.0/\">W3C standard</a>, but (as I&nbsp;understand) this standard mostly just defines a framework, and there can be many different ‚Äúmethods‚Äù of storing and resolving the identifiers, and each system that uses it can pick or create different types of those DIDs.</p><p>The format of a DID is: , where the last part depends on the method. ATProto supports two types of DIDs, but in practice, almost everyone uses one of them, the ‚Äúplc‚Äù. Each DID has a ‚Äú‚Äù, a JSON file (<a href=\"https://plc.directory/did:plc:oio4hkxaop4ao4wz2pp3f4cr\">see mine</a>) which describes the account ‚Äì in ATProto at least, the document includes things such as: the assigned handles, the PDS server hosting the account, and some cryptographic keys.</p><p>An important thing to note is that ; it‚Äôs the only thing that is permanent about your account, because something has to be. There needs to be some unique ID that all databases everywhere can use to identify you, which doesn‚Äôt change, and the DID is that ID. This means that you can‚Äôt change a DID of one type into another type later.</p><p>The main DID method is , where IIRC ‚Äúplc‚Äù originally stood for ‚Äúplaceholder‚Äù (I&nbsp;think it was meant to be temporary until something better is designed), and was later kind of retconned to mean ‚Äú<a href=\"https://github.com/did-method-plc/did-method-plc\">Public Ledger of Credentials</a>‚Äù üôÉ The DIDs of this type are identified by a random string of characters, which looks like this: <code>did:plc:vc7f4oafdgxsihk4cry2xpze</code>. The DID documents of each DID are stored in a centralized service hosted at <a href=\"https://plc.directory\">plc.directory</a> (Bluesky wants to eventually transfer the ownership to some external non-profit), which basically keeps a key-value store mapping a DID to a JSON file. It also keeps an ‚Äú<a href=\"https://plc.directory/did:plc:oio4hkxaop4ao4wz2pp3f4cr/log/audit\">audit log</a>‚Äù of the previous versions of the document (this means that, for example, the whole history of your old handles is available and you can‚Äôt erase it!). There‚Äôs also some cryptographic stuff there which, as I&nbsp;understand it, lets anyone verify that everything in the database checks out (don‚Äôt ask me how).</p><p>The other, rarely used method is . Those DIDs look like this: <code>did:web:witchcraft.systems</code>, and the DID document is stored in a specific  path on the given hostname, in this case <a href=\"https://witchcraft.systems/.well-known/did.json\">witchcraft.systems</a> (yes, that‚Äôs an actual TLD&nbsp;;). It does not store an audit log/history like  does.</p><p>The reason why it‚Äôs rarely used and not recommended, is because, first, it‚Äôs more complicated to create one (though that‚Äôs a solvable problem of course, see a <a href=\"https://blog.smokesignal.events/posts/3lwopvsmtx22a-creating-a-did-method-web-identity-for-atprotocol\">just published guide</a>); but second and more importantly, since DIDs are permanent, this means that your account is permanently bound to that domain. You need to keep it accessible and not let it expire, or you lose the account ‚Äì you can‚Äôt migrate it to  at some point later. It gives you more independence, but at the cost of being tied to that domain you have, and this isn‚Äôt a tradeoff that most people are likely to want, and definitely not people who don‚Äôt understand what they‚Äôre getting into.</p><p>If you‚Äôre fine with that choice, you can create a  account and almost everything in Bluesky and ATProto should work exactly the same. ‚ÄúAlmost‚Äù, because some services forget to implement that second code path, since it‚Äôs so rarely used üòâ but in that case, politely nudging the developer to fix the issue should help in most cases&nbsp;:&gt;</p><p>What DIDs enable is that since they act as the unique identifier, your handle doesn‚Äôt have to, like it does on the Fediverse. I&nbsp;can be  one day,  the next day, and  the week after. All existing connections ‚Äì follows &amp; followers, my posts, likes, blocks, lists I‚Äôm on, mentions in posts, etc. all work as before, because they all reference the DID, not the handle. With mentions specifically it works kinda funny, because they use what‚Äôs called a ‚Äúfacets‚Äù system (see <a href=\"https://mackuba.eu/2025/08/20/introduction-to-atproto/#facets\">later section</a>), where the link target is specified separately from the displayed text. So you can have an old post saying ‚Äúhey @mackuba.bsky.social‚Äù, where the handle in it links to my profile which is now named ‚Äú@mackuba.eu‚Äù. The link still works, because it really links to the DID behind the scenes.</p><p>Unlike on the Fediverse, the format of handles is just a hostname, not username + hostname. You assign a whole hostname to a specific account, and if you own any domain name, that can be your username (and if you own a well known domain name, it‚Äôs strongly recommended that you do, as a form of self-verification!).</p><p>The handle to DID assignment is a two-way link ‚Äì a DID needs to claim a given handle, and the owner of the domain needs to verify that they own that DID. On the DID side, this happens in the  field of the DID document (<a href=\"https://plc.directory/did:plc:oio4hkxaop4ao4wz2pp3f4cr\">see here in mine</a>). On the domain side, there are two ways of verifying a handle, depending on what‚Äôs more convenient to you: either a DNS TXT entry, or a file on a  path.</p><p>You might be wondering how handles like  work ‚Äì in this case, each such handle is its own domain name, and you can actually enter a domain like <a href=\"https://aoc.bsky.social\">aoc.bsky.social</a> into a browser and it will redirect to a Bluesky profile on <a href=\"https://bsky.app\">bsky.app</a>. Behind the scenes, this is normally handled by having a wildcard domain pointing to one service, which responds to HTTP requests on that  path by returning different DIDs, depending on the domain. That‚Äôs not only a  thing ‚Äì e.g. there‚Äôs now an open Blacksky PDS server which hands out  handles, and there are even ‚Äúhandle services‚Äù which  give out handles ‚Äì e.g. you can be <a href=\"https://swifties.social\">yourname.swifties.social</a> if you want&nbsp;;)</p><p>One place where handle changes break things is (some) post URLs on <a href=\"https://bsky.app\">bsky.app</a>. The official web client uses handles by default in permalinks, which means that if you link to a Bluesky post e.g. from a blog post and you change your handle later, that link will no longer work. You can however replace the handle after  with the user‚Äôs DID, and the router accepts such links just fine, they just aren‚Äôt used by default. So the form you‚Äôd want to use when putting links in a blog post or article (like the one you‚Äôre reading) would be something like: <a href=\"https://bsky.app/profile/did:plc:ragtjsm2j2vknwkz3zp4oxrd/post/3llwrsdcdvc2s\">https://bsky.app/profile/did:plc:ragtjsm2j2vknwkz3zp4oxrd/post/3llwrsdcdvc2s</a>.</p><p>Each record can be uniquely addressed with a specific  with the at:// scheme. The format of the URI&nbsp;is:</p><pre><code>at://&lt;user_DID&gt;/&lt;lexicon_NSID&gt;/&lt;rkey&gt;\n</code></pre><p> is an identifier of a specific record instance ‚Äì a usually short alphanumeric string, e.g. Bluesky post rkeys look something like . So a complete post URI&nbsp;might look like this: <code>at://did:plc:z72i7hdynmk6r22z27h6tvur/app.bsky.feed.post/3larljiybf22v</code>. You can look up at:// URIs in some record browser tools, e.g. <a href=\"https://pdsls.dev/at://did:plc:z72i7hdynmk6r22z27h6tvur/app.bsky.feed.post/3larljiybf22v\">PDSls</a>.</p><p>AT URIs are used for all references between records ‚Äì quotes, replies, likes, mute list entries, and so on. If you look at <a href=\"https://morel.us-east.host.bsky.network/xrpc/com.atproto.repo.getRecord?repo=did:plc:l3rouwludahu3ui3bt66mfvj&amp;collection=app.bsky.feed.like&amp;rkey=3lwctqgpttm2a\">this like record</a>, for example, its  points to <code>at://did:plc:vwzwgnygau7ed7b7wt5ux7y2/app.bsky.feed.post/3lv2b3f5nys2n</code>, which is the URI&nbsp;of a post record you can see <a href=\"https://pdsls.dev/at://did:plc:vwzwgnygau7ed7b7wt5ux7y2/app.bsky.feed.post/3lv2b3f5nys2n\">here</a>. Since the URIs use DIDs in the first part, handle changes don‚Äôt affect such links.</p><p>All user data (records and blobs) is stored in a  (or ‚Äúrepo‚Äù). The repository is identified by user‚Äôs DID, and stores:</p><ul><li>records, grouped by lexicon into so-called </li><li>blobs (stored separately from records)</li><li>authentication data like access tokens, signing keys, hashed passwords etc.</li></ul><p>Internally, an important part of how the repo stores user records is a data structure called ‚Äú<a href=\"https://en.wikipedia.org/wiki/Merkle_tree\">Merkle Search Tree</a>‚Äù ‚Äì but this isn‚Äôt something that you need to understand when using the protocol, unless you‚Äôre working on a PDS/relay implementation (I&nbsp;haven‚Äôt needed to get into it so far).</p><p>You can download the records part of your (or anyone else‚Äôs!) repo as a bundle called a <a href=\"https://bsky.app/profile/did:plc:fkasq7xtzrmlvz46c5trkrn3/post/3lkedsoq4vs2d\">CAR file</a>, a <a href=\"https://ipld.io/specs/transport/car/\">Content Addressed Archive</a> (fun fact: the icon for the button in the Bluesky app which downloads a repo backup is the shape of a car üöò).</p><p>The cool part is that a repository stores all data of the given user, from *all* lexicons. Including third party developer lexicons. This means that if someone has their account hosted on Bluesky servers, but uses third party ATProto apps like Tangled or Grain, Bluesky lets them store these apps‚Äô records like Grain photos or Tangled pull requests on the same server where it keeps their Bluesky posts. (And yes, of course someone made a <a href=\"https://github.com/ziodotsh/atfile\">lexicon/tool for storing arbitrary files</a> on your Bluesky PDS‚Ä¶ and did it in Bash, because why not üôÉ)</p><p> is the convention used for APIs in the ATProto network. The API&nbsp;endpoints use the same naming convention as lexicon NSIDs, and they have URLs with paths in the format of , e.g. <code>/xrpc/app.bsky.feed.getPosts</code>. There are similar <a href=\"https://github.com/bluesky-social/atproto/blob/main/lexicons/app/bsky/feed/getPostThread.json\">lexicon definition files</a> which specify what parameters are accepted/required by an endpoint and what types of data are returned in the JSON response. PDSes, AppViews, labellers and feed generators all implement the same kind of API, although with different subsets of specific endpoints. Third party apps don‚Äôt  to use the same convention, but it‚Äôs generally a good idea, since it integrates better with the rest of the ecosystem.</p><p>This one is kinda Bluesky-specific, but it‚Äôs pretty important to understand, and I&nbsp;think you can reuse it for non-Bluesky apps too.</p><p>The ‚Äú‚Äù system is something used for links and possibly rich text in future in Bluesky posts. It‚Äôs perhaps a little bit unintuitive at first, but it‚Äôs pretty neat and allows for a lot of flexibility.</p><p>The way you handle links, mentions, or hashtags, is that they aren‚Äôt highlighted automatically, but you need to specifically mark some range of text as a link using the facets. A facet is a marking of some range of the post text (from-to) with a specific kind of link. If you look e.g. at <a href=\"https://pdsls.dev/at://did:plc:257wekqxg4hyapkq6k47igmp/app.bsky.feed.post/3lnkwu24v5k2j\">this post here</a>, you can see that it has a facet marking the byte range 60-67 of the post text as a hashtag ‚Äúahoy25‚Äù. If there was no facet there, it would just render as normal unlinked text ‚Äú#ahoy25‚Äù in the post (when you see that, it‚Äôs an easy tell that a post was made using some custom tool that‚Äôs in early stages of development). It works the same way for mention links and normal URL links.</p><p>(If you‚Äôre curious why they implemented it this way, check out <a href=\"https://www.pfrazee.com/blog/why-facets\">this blog post</a>.)</p><p>Note that the displayed text in the marked fragment doesn‚Äôt have to match what the facet links to; this means that you can have links that just use some shorter text for the link instead of a part of the URL, in order to fit more text in one post (although in the official app, clicking such link triggers a warning popup first). E.g. some Hacker News bots commonly use this format, see <a href=\"https://bsky.app/profile/did:plc:7dh44snmqoa4gyzv3652gm3j/post/3lmhylb375m2a\">this post</a>. The Bsky app doesn‚Äôt let you create such posts directly, but some other clients like <a href=\"https://skeetdeck.pages.dev/decks/3lpzuhjwlxcns\">Skeetdeck</a> do.</p><p>Facets are also used for URL shortening ‚Äì if you just put a long URL in the text of a post made through the API, it will be neither shortened nor highlighted. You need to manually mark it with a facet, and manually shorten the displayed part to whatever length you want.</p><p>Likely the most tricky part is that the index numbers you need to use for the ranges are counted on a UTF-8 representation of the text string, but they‚Äôre counted in‚Ä¶ bytes and not unicode scalars, which most languages index strings in üòÖ This is somewhat of an unfortunate tech debt thing as I&nbsp;understand, and it was made this way mostly because of JavaScript, which doesn‚Äôt work with UTF-8 natively. But this means you need to be extra careful with the indexes in most languages.</p><p>Ok, now that we got through the basic pieces, let‚Äôs talk about servers:</p><p>The original copy of all user data is stored on a server called , Personal Data Server. This is the ‚Äúsource of truth‚Äù. A PDS stores one or more user accounts and repos, handles user authentication, and serves as an ‚Äúentry point‚Äù to the network when connecting from a client app. Most network requests from the client are sent to your PDS, although only some of them are handled directly by the PDS, and the rest are proxied e.g. to the AppView. So in a way, your PDS kind of serves as your ‚Äúuser agent‚Äù in the network on the backend side of things (beyond the client app), especially if it‚Äôs under your control.</p><p>Each PDS has an XRPC API&nbsp;with some number of endpoints for things like listing repositories, listing contents of each, looking up a specific record or blob, account authentication and management, and so on. It also has a websocket API&nbsp;called a ‚Äú‚Äù (the <a href=\"https://github.com/bluesky-social/atproto/blob/main/lexicons/com/atproto/sync/subscribeRepos.json\">subscribeRepos endpoint</a>). The firehose streams all changes happening on a given PDS (from all repos) as a stream of ‚Äúevents‚Äù, where each event is an addition, edit, or deletion of a record in one of the repos, or some change related to an account, like handle change or deactivation.</p><p>One of the most important features of ATProto is that <strong>an account is not permanently assigned to a PDS</strong>. Unlike in ActivityPub, where your identifier is e.g.  and it can never change, because everything uses that as the unique ID, here the unique ID is the DID. The PDS host is assigned to a user in the DID document JSON (e.g. on <a href=\"https://plc.directory\">plc.directory</a>), but you can migrate to a different PDS at any point, and at the moment there are even some fairly user-friendly tools available for doing that, like <a href=\"https://atpairport.com\">ATP Airport</a> or <a href=\"https://pdsmoover.com\">PDS MOOver</a> (although it‚Äôs still a bit unpolished at the moment, and for now you can‚Äôt migrate back to Bluesky-hosted PDSes). In theory, you should even be able to migrate to a different PDS if your old PDS is dead or goes rogue, if you have prepared in advance (<a href=\"https://www.da.vidbuchanan.co.uk/blog/adversarial-pds-migration.html\">this is a bit more technical</a>). If everything goes well, nobody even notices that anything has changed (you can‚Äôt even easily check in the app what PDS someone is on, although there are external tools for that, like <a href=\"https://internect.info\">internect.info</a>).</p><p>Initially, during the limited beta in 2023, Bluesky only had one PDS, . In November 2023, several additional PDSes were created (also under Bluesky PBC control) and existing users were quietly all spread to a random one of those. At that point, the network was already ‚Äútechnically federated‚Äù, operating in the target architecture, although with access restricted to only Bluesky-run servers. This restriction was lifted in February 2024 with the <a href=\"https://docs.bsky.app/blog/self-host-federation\">public federation launch</a>.</p><p>Since then, ATProto enthusiasts started setting setting up PDS servers for themselves, either creating alt/test accounts there, or moving their main accounts. As of August 2025, there around 2000 third party PDS servers, although most of them are very small ‚Äì usually hosting one person‚Äôs main and/or test accounts, and maybe those of a couple of their friends. I&nbsp;have <a href=\"https://blue.mackuba.eu/directory/pdses\">a list of them</a> on my website, and there‚Äôs also a more complete list <a href=\"https://github.com/mary-ext/atproto-scraping\">here</a> (mine excludes inactive PDSes and empty accounts).</p><p>As you can see there, there‚Äôs one massive PDS for <a href=\"https://fed.brid.gy\">Bridgy Fed</a>, the Bluesky-Mastodon bridge service, hosting around 30-40k bridged accounts from the Fediverse, Threads, Nostr, Flipboard, or the web (blogs); then some number of small to medium PDSes for various services, and a very long tail of servers with single-digit number of accounts. At this moment, large public PDS in the style of Fedi instances aren‚Äôt much of a thing yet, although there are at least a few communities working on setting up one (e.g. <a href=\"https://www.blackskyweb.xyz\">Blacksky</a>, <a href=\"https://northskysocial.com\">Northsky</a>, or <a href=\"https://turtleisland.blog/2025/04/28/turtle-island-bluesky-pds/\">Turtle Island</a>). Blacksky specifically has <a href=\"https://bsky.app/profile/did:plc:w4xbfzo7kqfes5zb7r6qv3rw/post/3lvtlqoef5c2z\">opened up for migrations</a> just last week and has now a few hundred real accounts.</p><p>The vast majority of PDSes at the moment use the <a href=\"https://github.com/bluesky-social/pds\">reference implementation from Bluesky</a> (written in TypeScript), but there are a few alternative implementations at various levels of maturity (Blacksky‚Äôs Rudy Fraser‚Äôs <a href=\"https://github.com/blacksky-algorithms/rsky/tree/main/rsky-pds\">rsky</a> written in Rust, <a href=\"https://github.com/haileyok/cocoon\">cocoon</a> in Go, or <a href=\"https://github.com/DavidBuchanan314/millipds\">millipds</a> in Python). The official version is very easy to set up and very cheap to run ‚Äì it‚Äôs bundled in Docker, and there‚Äôs basically one script you need to run and answer a few questions.</p><p>As for the Bluesky-hosted PDSes, the number is currently in high double digits, and each of them hosts a few hundred thousands of accounts (!). And what‚Äôs more, they keep the record data in SQLite databases, one per account. And it works really well, go figure. The Bluesky PDSes are all given names of different kinds of mushrooms (like Amanita, Boletus or Shiitake), hence they are often called ‚Äúmushroom servers‚Äù; you can see the full list e.g. <a href=\"https://status.bsky.app\">here</a>.  was left as a so-called ‚Äú‚Äù, which handles shared authentication for all Bluesky-hosted PDSes (it‚Äôs a private piece of Bluesky PBC infrastructure that‚Äôs not open source and not needed for independent PDS hosters).</p><p>A  is probably the piece of the ATProto architecture that‚Äôs most commonly misunderstood by people familiar with other networks like the Fediverse. It doesn‚Äôt help that both the Fediverse and Nostr also include servers called ‚Äúrelays‚Äù, but they serve a different purpose in each of them:</p><ul><li>a relay in Nostr is a core piece of the architecture: your posts are uploaded to one or more relays that you have configured and are hosted there, where other users can fetch them from</li><li>a relay in the Fediverse is an optional helper service that redistributes posts from some number of instances who have opted in to others, in order to make content more discoverable e.g. on hashtag feeds</li></ul><p>In ATProto, a relay is a server which combines the firehose streams from all PDSes it knows about into one massive stream that includes every change happening anywhere on the network. Such full-network firehose is then used as the input for many other services, like AppViews, labellers, or feed generators. It serves as a convenient streaming API&nbsp;to get e.g. all posts on the network to process them somehow, or all changes to accounts, or all content in general, from a single place.</p><p>Initially, the relay was also expected to keep a complete archive of all the data on the network, from all repos, from the beginning of time. This requirement was later removed in the updates late last year, at least partially triggered by the drastic increase in traffic in November 2024, which overwhelmed Bluesky‚Äôs and third party servers for at least a few days. Currently, Bluesky‚Äôs and other relays are generally ‚Äúnon-archival‚Äù, meaning that they live stream current events (+ a buffer of e.g. last 24 or 36 hours), but don‚Äôt keep a full archive of all repos (this change has <a href=\"https://whtwnd.com/bnewbold.net/3lo7a2a4qxg2l\">massive lowered the resource requirements</a> / cost of running a relay, making it much more accessible). An archival relay could always be set up too, but I‚Äôm not aware of any currently operating.</p><p>Bluesky operates one main relay at <a href=\"https://bsky.network\">bsky.network</a>, which is used as a data source for their AppView and pretty much everyone else in the ATProto ecosystem at the moment (internally, it‚Äôs really some kind of ‚Äúload balancer‚Äù using the <a href=\"https://github.com/bluesky-social/indigo/tree/main/cmd/rainbow\">rainbow</a> service, with a few real relay servers behind it).</p><p>The relay code is <a href=\"https://github.com/bluesky-social/indigo/\">implemented in Go</a>, and isn‚Äôt very hard to get up and running (especially the recent ‚Äú1.1‚Äù update improved things quite a lot). Some people have been running alternative relay services privately for some time, and there is now e.g. a public relay run by Rudy Fraser at <a href=\"https://atproto.africa\">atproto.africa</a> (with a custom implementation in Rust! ü¶Ä), and a couple run by <a href=\"https://bsky.app/profile/did:plc:hdhoaan3xa3jiuq4fg4mefid/post/3ltkh6bdo4ki5\">Phil @bad-example.com</a>. I‚Äôm also running <a href=\"https://relay.feeds.blue\">my own small relay</a>, feeding content only from non-Bluesky PDSes.</p><p>There is also a variant of a relay called <a href=\"https://github.com/bluesky-social/jetstream\">Jetstream</a> ‚Äì it‚Äôs a service that reads from a real CBOR relay and outputs a stream that‚Äôs JSON based, better organized, and much more lightweight (the full relay includes a lot of additional data that‚Äôs mostly used for cryptographic operations and other low-level stuff). For many simpler tools and services, it might make more sense to stream data from that one instead, if only to save bandwidth. (Bluesky runs a couple of instances listed there in the readme, but you can also run your own.)</p><p>The terribly named  is the second most important piece of the network after the PDS.</p><p>The AppView is basically an API&nbsp;server that serves processed data to client apps. It‚Äôs an equivalent of an API&nbsp;backend (with the databases behind it) that you‚Äôd find on a classic social media site like Twitter. AppView streams all new data written on the network from the relay, and saves a copy of it locally in a processed, aggregated and optimized form. For example, an AppView backed by an SQL database could have a  table with a  column, a  table storing all likes with a foreign key , probably also an integer  column in  for optimization, and so on.</p><p>The AppView is designed to be able to easily give information such as:</p><ul><li>the latest posts from this user</li><li>all the replies in a given thread organized in a tree</li><li>most recent posts on the network with the hashtag #rubylang or mentioning ‚ÄúiOS 26‚Äù</li><li>how many likes/reposts has a given post received, and who made them</li><li>how many follows/followers does a given user have, and who are they</li><li>is user A allowed to view or reply to a post from user B</li></ul><p>All this data originates from users‚Äô PDSes and has its original copy stored there, but the ‚Äúraw‚Äù record don‚Äôt always allow you to access all information easily. For example, to find out how many likes a post has, you need to know all  records referencing it from other users, and each of those like records is stored in the liking user‚Äôs repo on that user‚Äôs PDS. Same with followers, as I&nbsp;mentioned earlier in the section on records, or with building threads (again, different replies in one thread are hosted in different repos), or for basically any kind of search. So having this kind of API&nbsp;with processed data from the entire network is essential for client apps and various tools and services built around Bluesky by other people.</p><p>AppView also applies some additional rules to the data, sometimes overriding what people post into their PDSes, since anyone can technically post anything into their PDS. For example, the AppView prevents you from looking at the profiles of people who have blocked you, at least when you‚Äôre logged in. It also hides them from your followers list, even if they have a  record referencing you, making it seem like they don‚Äôt; and if they try to make an  replying to you (they  create such record on their PDS!), it excludes such reply from feeds and threads, as if it never happened. Same goes for ‚Äúthread gates‚Äù which lock access to threads, and so on.</p><p>The AppView is one of the few components which  completely open source. Initially, the AppView used Postgres as its data store;  version is still in the public repository. In late 2023, Bluesky has migrated to a ‚Äúv2‚Äù version, which uses the NoSQL database <a href=\"https://www.scylladb.com\">ScyllaDB</a> instead, to be able to handle the massive read traffic from many millions of concurrent users. The upper layer with the ‚Äúbusiness logic‚Äù is kept in the <a href=\"https://github.com/bluesky-social/atproto/tree/main/packages/bsky\">public repository</a>, while the so called ‚Äúdataplane‚Äù layer that interacts directly with Scylla is not. The reason is mostly that it‚Äôs built for a specific hardware setup they have and wouldn‚Äôt be directly usable by others, while it would add some unnecesary work for the team to publish it. It‚Äôs still possible to run the AppView with the <a href=\"https://github.com/bluesky-social/atproto/tree/main/packages/bsky/src/data-plane\">old Postgres-based data layer</a> (and I&nbsp;think the team uses that internally for development), it just can‚Äôt handle as much traffic as the current live version.</p><p>This is the piece that‚Äôs hardest to run yourself, and one that requires the most resources. That said, a private AppView should be possible to run right now for <a href=\"https://whtwnd.com/futur.blue/3ls7sbvpsqc2w\">under $200/month</a> ‚Äì the biggest requirement is at least a few TB of disk space. The truly costly part is not collecting and storing all this data, but serving it to a huge number of users who would use it as a backend for the client app in daily use. An alternative full-network Bluesky AppView that is used by a few thousands of users shouldn‚Äôt be very hard to run, but to be able to serve millions, you‚Äôll need a lot of hardware and something more custom than the Postgres-based version.</p><p>There have also been some attempts at alternative implementations ‚Äì the most advanced right now is <a href=\"https://github.com/alnkesq/AppViewLite\">AppViewLite</a>, built in C#, which goes to great lengths to minimize the resource use.</p><p>A part of the AppView (at least the Bluesky one) is also a CDN for serving images &amp; videos. The API&nbsp;responses from e.g.  or  generally include links to any media on the Bluesky CDN hostname, not directly on the PDS, even though you  fetch every blob from the PDS, since that‚Äôs the ‚Äúsource of truth‚Äù (although IIRC the Bluesky PDS implementation doesn‚Äôt set the CORS headers there). It‚Äôs recommended to access any media this way in order to not use too much bandwidth from the PDS.</p><p>(Or ‚Äúlabelers‚Äù officially, but I&nbsp;like the British spelling more here, sue me ¬Ø\\_(„ÉÑ)_/¬Ø)</p><p>We‚Äôre now getting to more Bluesky specific things (i.e. specific for the Bluesky-service, although some parts of it are ATProto-general and mentioned on the <a href=\"https://atproto.com/specs/label\">atproto.com site</a>).</p><p>A  is a moderation service for Bluesky (or other ATProto app), which can be run by third parties. Labellers emit labels, which are assigned to an account or a record (like a post). Each labeller defines its own set of labels, depending on what it‚Äôs focusing on; then, users can ‚Äúsubscribe‚Äù to a labeller and choose how they want to handle the labels it assigns: you can hide the labelled posts/users, mark them with a warning badge, or ignore given label.</p><p>Labellers were initially designed to just do community moderation of unwanted content, e.g. you can have a service focused on fighting racism, transphobia, or right-wing extremism, and that service helps protect its users from some kinds of bad actors; or you can have one marking e.g. posts with political content, users who follow 20k accounts, or who post way too many hashtags. In practice, many <a href=\"https://blue.mackuba.eu/labellers/\">existing labellers</a> are meant for self-labelling instead, letting you assign e.g. a country flag or some fun things like a D&amp;D character class to yourself.</p><p>The way it works technically is:</p><ul><li>a labeller either runs a firehose client pulling posts from the relay, or relies on reports from users and/or its operating team (usually using the <a href=\"https://github.com/bluesky-social/ozone/\">Ozone tool</a> for that)</li><li>labels, which are lightweight objects ( ATProto records) are emitted from labeller‚Äôs special firehose stream (the <a href=\"https://github.com/bluesky-social/atproto/blob/main/lexicons/com/atproto/label/subscribeLabels.json\">subscribeLabels endpoint</a>)</li><li>the AppView listens to the label firehoses of all labellers it knows about, in addition to the relay stream, and records all received labels in its database</li><li>when a logged in user pulls data like threads or timelines from the AppView, it adds relevant label info to the responses depending on which labellers the user follows</li><li>the specific list of labellers whose labels should be applied is passed explicitly in API&nbsp;requests in the  header (there is a ‚Äúsoft‚Äù limit of 20 labellers you can pass at a time, which is why the official app won‚Äôt let you subscribe to more)</li><li>in the official app, Bluesky‚Äôs official moderation service (which is ‚Äújust‚Äù another labeller) is hardcoded as one of those 20 and you can‚Äôt turn it off; when connecting from your own app or tool, you‚Äôre free to ignore it if you want</li></ul><p>(Read more about labellers <a href=\"https://mackuba.eu/2024/02/21/bluesky-guide/#labellers\">here</a>.)</p><p><a href=\"https://mackuba.eu/2024/02/21/bluesky-guide/#feeds\">Custom feeds</a> are one of the coolest features of Bluesky. They let you create any kind of feed using any algorithm and let everyone on the platform use it (even as the default feed, if they want to).</p><p>The way this system works is that you need to run a ‚Äú‚Äù service on your server. In that service, you expose an API&nbsp;that the AppView can call, which returns a list of post at:// URIs selected by you however you want in response to a given request.</p><p>A minimal feed service can be pretty simple ‚Äì the API&nbsp;is just three endpoints, two of which are static, and the third returns the post URIs. One ‚Äúsmall‚Äù problem is that in order to return the post URIs, you need to have some info about posts stored up front, which in practice means that you almost always need to connect to a relay‚Äôs firehose stream and store some post data (of selected or all posts, depending on your use case).</p><ul><li>a feed record is uploaded to your repo, including metadata and location of the feed generator service, which lets other users find your feed</li><li>when the user opens that feed in the app, the AppView makes a request to your service on their behalf</li><li>your service looks at the request params and headers, and returns a list of posts it selected in the form of at:// URIs</li><li>the AppView takes those URIs and maps them to full posts (so-called ‚Äúhydration‚Äù), which it returns to the user‚Äôs app</li></ul><p>How exactly those posts are selected to be returned in the given request is completely up to you, the only requirement is that these are posts that the AppView will have in its database, since you only send URIs, not actual post data. In most cases, feeds use some kind of keyword/regexp matching and chronological ordering, but you can even build very complex, AI-driven algorithmic ‚ÄúFor You‚Äù style personalized feeds.</p><p>You don‚Äôt necessarily have to code a feed service yourself and host it in order to have a custom feed ‚Äì there are a few feed hosting services that don‚Äôt require technical knowledge to use, like <a href=\"https://skyfeed.app\">SkyFeed</a> or <a href=\"https://www.graze.social\">Graze</a>.</p><p>Ok, that‚Äôs technically not a server, but stay with me‚Ä¶</p><p>The final piece that you need to fully enjoy Bluesky is the client app ‚Äì a mobile/desktop one or a web frontend. Unlike on Fedi, where an instance software like Mastodon usually includes a built-in web frontend that is your main interface for accessing the service, the PDS doesn‚Äôt include anything like that, just a database and an API&nbsp;(which also means it‚Äôs much more lightweight and needs less resources). All browsing is done through a separate client, and the client always does everything through the public API&nbsp;‚Äì kind of like when you run a custom web client for Mastodon like <a href=\"https://elk.zone\">Elk</a> or <a href=\"https://phanpy.social\">Phanpy</a>, you connect it to your instance, and you view your timeline on <a href=\"https://elk.zone\">elk.zone</a>.</p><p>So when you go to <a href=\"https://bsky.app\">bsky.app</a>, that‚Äôs what you‚Äôre seeing ‚Äì a web client that connects to your PDS (Bluesky-hosted or self-hosted) through the public API, no more, no less. The official app is built for both mobile platforms and for the web from a single React Native codebase (apparently React Native on the web and normal web React is not the same thing üßê). This has allowed the still very small frontend team (and IIRC at first it was literally just Paul) to build the app for three platforms in any reasonable amount of time and maintain it going forward. The downside is that it‚Äôs kinda neither a great webapp nor a great mobile app‚Ä¶ But the team is doing what they can to improve it, and it‚Äôs already much better than it used to be, and tbh more than good enough for me.</p><p>There aren‚Äôt nearly as many alternative clients as there are for Mastodon, and none of them are  great, but there are a few options; see the <a href=\"https://mackuba.eu/2024/02/21/bluesky-guide/#apps\">apps part of my Bluesky Guide</a> blog post for links.</p><p>Notice that I&nbsp;haven‚Äôt mentioned DMs anywhere ‚Äì that‚Äôs because they aren‚Äôt a part of the protocol at the moment. The Bluesky team wants to eventually add some properly implemented, end-to-end encrypted, secure DMs using some open standard, but they won‚Äôt be able to finish that in the short term, and a lot of people were asking for at least some simple version of DMs in the app. So they‚Äôve decided as an interim solution to implement them as a fully centralized, closed source service. It is accessible to third-party Bluesky clients through the API&nbsp;(the  namespace), but it‚Äôs not something you can run yourself. The team is <a href=\"https://bsky.app/profile/did:plc:44ybard66vv44zksje25o7dz/post/3lacrutxhio2h\">very open</a> about the fact that it‚Äôs not a proper replacement for something like Signal, and that for sensitive communication, you should ideally just use it for swapping contacts on Signal on iMessage and move the conversation there. They also kinda don‚Äôt want to spend too much time adding features there, because it‚Äôs considered a temporary solution, so it‚Äôs pretty basic in terms of available features.</p><p>There are also a few other closed-source helper services, like the ‚Äúcardyb‚Äù they use for generating link card details, or the video service for preprocessing videos, but they‚Äôre all specific to some Bluesky use cases only and not strictly necessary to use.</p><p>So the flow and hierarchy is like this:</p><ul><li>the  you use creates new records as a result of actions you take (new posts, likes, follows), and saves them into your PDS</li><li>your  emits events on its firehose with the record details</li><li>Bluesky  and other relays are connected to the firehoses of each PDS they know about (your PDS generally needs to ask them to connect using the  ENV variable), and they pass those events to their output firehose</li><li>the Bluesky  (and other AppViews) listen to the firehose of their selected relay (though it could be multiple relays, or it could even just stream directly from PDSes, but in practice this will normally be one trusted relay)</li><li>the AppView gets events including your records, and if they are relevant, saves the data to its internal database in some appropriate representations</li><li>when other users browse Bluesky in their client apps, they load timelines, feeds and threads from the AppView, which returns info about your post from that database it saved it to</li></ul><ul><li> run by third party feed operators also stream data from Bluesky‚Äôs or some other relay and save it locally, so they can respond to feed requests from the AppView</li><li> also stream data from Bluesky‚Äôs or some other relay, and emit labels on their firehoses, which get sent to the AppView (note: there is no official ‚Äúlabeller relay‚Äù sitting between labellers and the AppView, although one third party dev <a href=\"https://bsky.app/profile/did:plc:w4xbfzo7kqfes5zb7r6qv3rw/post/3lrgs3itqyc2q\">wrote one</a>)</li></ul><ul><li>PDSes <strong>do not connect to each other directly</strong>, and they don‚Äôt store posts of users from other PDSes, only their own</li><li>although right now basically everyone uses the Bluesky relay and AppView, anyone  set up their own alternative relays and AppViews, which feed from all or any subset of known PDSes</li><li>PDS chooses which relays to ask to connect, but relays can also connect by themselves to a PDS or another relay; AppView chooses which relay(s) it streams data from; and PDS chooses which AppView it loads timelines &amp; threads from</li><li>it‚Äôs absolutely possible and expected that two users using different PDSes, which use separate AppViews feeding from separate relays will be able to talk to each other and see each other‚Äôs responses on their own AppView, as long as the users aren‚Äôt banned on the other user‚Äôs infrastructure</li></ul><p>The metaphor that‚Äôs often used to describe these relationship is that PDSes are like websites which publish some blog posts, and relays &amp; AppViews are like search engines which crawl and index the web, and then let you look up results in them. In most cases, a website should be indexed and visible in all/most available search engines.</p><p>And that‚Äôs about it ‚Äì I&nbsp;think with the above, you should have a pretty good grasp of the big picture of ATProto architecture and all the specific parts of it. Now, if you want to start playing with the protocol and building some things on it, a lot will depend on what specifically you want to build and using what languages/technologies:</p><p>Two languages are officially supported by Bluesky:</p><ul><li>JavaScript/TypeScript, in which most of their code is written (see the <a href=\"https://github.com/bluesky-social/atproto/tree/main/packages\">packages folder</a> in the  repo)</li><li>Go, which is used in some backend pieces like the relay, or the <a href=\"https://github.com/bluesky-social/goat\">goat</a> command line tool used e.g. for PDS migrations (see the <a href=\"https://github.com/bluesky-social/indigo\"> repo</a>)</li></ul><p>For other languages, I&nbsp;have a website called <a href=\"https://sdk.blue\">sdk.blue</a>, which lists all libraries and SDKs I&nbsp;know about, grouped by language. As you can see, there is something there for most major languages; I‚Äôve built and maintain a group of <a href=\"https://sdk.blue/#ruby\">Ruby gems</a> myself. If you want to use a language that doesn‚Äôt have any libraries yet, it‚Äôs really not that hard to make one from scratch ‚Äì for most things you just need an HTTP client and a JSON parser, and maybe a websocket client.</p><p>There is quite a lot of official documentation, although it‚Äôs a bit spread out and sometimes not easy to find.</p><p>The places to look in are:</p><ul><li><a href=\"https://atproto.com\">atproto.com</a> ‚Äì the official AT Protocol website; a bit more formal documentation about the elements of the protocol, kind of like what I&nbsp;did here, but with much more info and detailed specifications of each thing</li><li><a href=\"https://docs.bsky.app/docs/get-started\">docs.bsky.app</a> ‚Äì more practical documentation with guides and examples of specific use cases in TS &amp; Python (roll down the sections in the sidebar); it shows examples of how to make a post, upload a video, how to connect to the firehose, how to make a custom feed, etc.</li><li>something that I&nbsp;also find useful is to have the <a href=\"https://github.com/bluesky-social/atproto\">atproto repo</a> checked out locally and opened in the editor, and look things up in the JSON files from the <a href=\"https://github.com/bluesky-social/atproto/tree/main/lexicons\">/lexicons folder</a></li></ul><p>And a few other articles that might work better for you:</p><p>Someone said recently that ‚Äú<em>bsky replies are the only real documentation for ATProto</em>‚Äù, and honestly, they‚Äôre not wrong. We have a great community of third party developers now, building their own tools, apps, libraries, services, even organizing <a href=\"https://ahoy.eu\">conferences</a>. If you‚Äôre starting out and you have any questions, just ask and someone will probably help, and some of the Bluesky team developers are also very active in Bluesky threads, answering questions and clarifying things. So a lot of such knowledge that‚Äôs not necessarily found in the official docs can be found somewhere on Bluesky.</p><p>The two places I&nbsp;recommend looking at are:</p><ul><li>the ‚ÄúATProto Touchers‚Äù Discord chat ‚Äì ping me or some other developer for an invite&nbsp;:)</li><li>my <a href=\"https://bsky.app/profile/did:plc:oio4hkxaop4ao4wz2pp3f4cr/feed/atproto\">ATProto feed</a> on Bluesky, which tries to catch any ATProto development discussions ‚Äì it should include posts with any mention of ‚ÄúATProto‚Äù or things like ‚ÄúAppView‚Äù or various API&nbsp;names and technical terms, or you can use  or  hashtag to be sure</li></ul><p>Also, there‚Äôs a fantastic newsletter called <a href=\"https://connectedplaces.online\">Connected Places</a> (formerly Fediverse Report) by Laurens Hof, who publishes two separate editions every week, about what‚Äôs happening in Bluesky/ATProto and in the Fediverse (and *a lot* of things are happening).</p><p>Some easy ways to start tinkering:</p><ul><li>use one of the <a href=\"https://sdk.blue\">existing libraries for your favorite language</a> and make a website or command-line tool which loads some data from the AppView or PDS: load and print timelines, calculate statistics, browse contents of PDSes and repos, etc.</li><li>make a bot that posts something (not spammy!)</li><li>connect to the relay firehose and print or record some specific types of data</li></ul><p>And a couple of tools which will certainly be useful in development:</p><ul><li><a href=\"https://internect.info\">internect.info</a> ‚Äì look up an account by handle/DID and see details like assigned PDS or handle history</li><li><a href=\"https://pdsls.dev\">PDSls</a> ‚Äì PDS and repository browser, lets you look up repos by account DID or records by at:// URI&nbsp;(there are a few others, but this one is most popular)</li></ul>","contentLength":48541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44965233"},{"title":"Zedless: Zed fork focused on privacy and being local-first","url":"https://github.com/zedless-editor/zed","date":1755715623,"author":"homebrewer","guid":234719,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44964916"},{"title":"Project to formalise a proof of Fermat‚Äôs Last Theorem in the Lean theorem prover","url":"https://imperialcollegelondon.github.io/FLT/","date":1755714445,"author":"ljlolel","guid":235556,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44964693"},{"title":"Pixel 10 Phones","url":"https://blog.google/products/pixel/google-pixel-10-pro-xl/","date":1755710650,"author":"gotmedium","guid":234681,"unread":true,"content":"<p data-block-key=\"cujby\">Pixel 10 features a satin-finish metal frame, polished glass back and our iconic camera bar. You‚Äôll have four expressive color options to choose from, too: Obsidian, Frost, Indigo and Lemongrass. Its 6.3-inch Actua display is now even brighter at 3000 nits making for easy viewing. That‚Äôs paired with improved audio, including exceptional bass so your favorite shows sound and look better than ever.</p><p data-block-key=\"5lpqn\">Pixel 10 also comes with huge camera improvements. That includes the first 5x telephoto lens on this tier of Pixel with fast autofocus, 10x optical quality and up to 20x zoom with Super Res Zoom ‚Äî so it's easier to shoot from a distance.</p>","contentLength":642,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44963939"},{"title":"An Update on Pytype","url":"https://github.com/google/pytype","date":1755709491,"author":"mxmlnkn","guid":234718,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44963724"},{"title":"Debugging Behind the Iron Curtain (2010)","url":"https://www.jakepoz.com/debugging-behind-the-iron-curtain/","date":1755708815,"author":"indrora","guid":234680,"unread":true,"content":"<p>Sergei is a veteran of the early days of the computing industry as it was developing in the Soviet Union. I had the pleasure of working and learning from him over the past year, and in that time I picked up more important lessons about both life and embedded programming than any amount of school could ever teach. The most striking lesson is the story of how and why, in late summer of 1986, Sergei decided to move his family out of the Soviet Union.</p><p>In the 1980s, my mentor Sergei was writing software for an SM-1800, a Soviet clone of the PDP-11. The microcomputer was just installed at a railroad station near Sverdlovsk, a major shipping center for the U.S.S.R. at the time. The new system was designed to route train cars and cargo to their intended destinations, but there was a nasty bug that was causing random failures and crashes. The crashes would always occur once everyone had gone home for the night, but despite extensive investigation, the computer always performed flawlessly during manual and automatic testing procedures the next day. Usually this indicates a race condition or some other concurrency bug that only manifests itself under certain circumstances. Tired of late night phone calls from the station, Sergei decided to get to the bottom of it, and his first step was to learn exactly which conditions in the rail yard were causing the computer to crash.</p><p>He first compiled a history of all occurrences of the unexplained crashes and plotted their dates and times on a calendar. Sure enough, a pattern was clearly visible. By observing the behavior for several more days, Sergei saw he could easily predict the timing of future system failures.</p><p>He soon figured out that the rail yard computer malfunctioned only when the cargo being processed was live cattle coming in from northern Ukraine and western Russia heading to a nearby slaughterhouse. In and of itself this was strange, as the local slaughterhouse had in the past been supplied with livestock from farms located much closer, in Kazakhstan.</p><p>As you may know, the Chernobyl Nuclear Power Plant disaster occurred in 1986 and spread deadly levels of radiation which to this day make the nearby area uninhabitable. The radioactivity caused broad contamination in the surrounding areas, including northern Ukraine, Belarus, and western Russia. Suspicious of possibly high levels of radiation in the incoming train cars, Sergei devised a method to test his theory. Possession of personal Geiger counters was restricted by the Soviet government, so he went drinking with a few military personnel stationed at the rail yard. After a few shots of vodka, he was able to convince a soldier to measure one of the suspected rail cars, and they discovered the radiation levels were orders of magnitude above normal.</p><p>Not only were the cattle shipments highly contaminated with radiation, the levels were high enough to randomly flip bits in the memory of the SM-1800, which was located in a building close to the railroad tracks.</p><p>There were often significant food shortages in the Soviet Union, and the government plan was to mix the meat from Chernobyl-area cattle with the uncontaminated meat from the rest of the country. This would lower the average radiation levels of the meat without wasting valuable resources. Upon discovering this, Sergei immediately filed immigration papers with any country that would listen. The computer crashes resolved themselves as radiation levels dropped over time.</p>","contentLength":3467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44963594"},{"title":"Show HN: Anchor Relay ‚Äì A faster, easier way to get Let's Encrypt certificates","url":"https://anchor.dev/relay","date":1755706398,"author":"geemus","guid":235597,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44963226"},{"title":"Show HN: Luminal ‚Äì Open-source, search-based GPU compiler","url":"https://github.com/luminal-ai/luminal","date":1755705673,"author":"jafioti","guid":234729,"unread":true,"content":"<p>Hi HN, I‚Äôm Joe. My friends Matthew, Jake and I are building Luminal (<a href=\"https://luminalai.com/\">https://luminalai.com/</a>), a GPU compiler for automatically generating fast GPU kernels for AI models. It uses search-based compilation to achieve high performance.</p><p>We take high level model code, like you'd have in PyTorch, and generate very fast GPU code. We do that without using LLMs or AI - rather, we pose it as a search problem. Our compiler builds a search space, generates millions of possible kernels, and then searches through it to minimize runtime.</p><p>You can try out a demo in `demos/matmul` on mac to see how Luminal takes a naive operation, represented in our IR of 12 simple operations, and compiles it to an optimized, tensor-core enabled Metal kernel. Here‚Äôs a video showing how: <a href=\"https://youtu.be/P2oNR8zxSAA\" rel=\"nofollow\">https://youtu.be/P2oNR8zxSAA</a></p><p>Our approach differs significantly from traditional ML libraries in that we ahead-of-time compile everything, generate a large search space of logically-equivalent kernels, and search through it to find the fastest kernels. This allows us to leverage the Bitter Lesson to discover complex optimizations like Flash Attention entirely automatically without needing manual heuristics. The best rule is no rule, the best heuristic is no heuristic, just search everything.</p><p>We‚Äôre working on bringing CUDA support up to parity with Metal, adding more flexibility to the search space, adding full-model examples (like Llama), and adding very exotic hardware backends.</p><p>We aim to radically simplify the ML ecosystem while improving performance and hardware utilization. Please check out our repo: <a href=\"https://github.com/luminal-ai/luminal\" rel=\"nofollow\">https://github.com/luminal-ai/luminal</a> and I‚Äôd love to hear your thoughts!</p>","contentLength":1654,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44963135"},{"title":"OPA maintainers and Styra employees hired by Apple","url":"https://blog.openpolicyagent.org/note-from-teemu-tim-and-torin-to-the-open-policy-agent-community-2dbbfe494371","date":1755704673,"author":"crcsmnky","guid":234717,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962969"},{"title":"Launch HN: Channel3 (YC S25) ‚Äì A database of every product on the internet","url":"https://news.ycombinator.com/item?id=44962881","date":1755704047,"author":"glawrence13","guid":234777,"unread":true,"content":"Hi HN ‚Äî we‚Äôre George and Alex, building Channel3 (<a href=\"https://trychannel3.com/\">https://trychannel3.com/</a>), a database of every product on the internet, searchable via text/image, and with built-in affiliate monetization. Here‚Äôs a demo: <a href=\"https://www.youtube.com/watch?v=Mx8FyP7KvJg\" rel=\"nofollow\">https://www.youtube.com/watch?v=Mx8FyP7KvJg</a>.<p>It‚Äôs surprisingly hard to find good product data. If you want your software to recommend products and deep-link to merchants, you‚Äôll quickly discover that the data you need‚Äîclean titles, normalized attributes, deduped listings, current prices and inventory, variant options, images, and brand info‚Äîis not just messy; it‚Äôs also spread across a long, long tail of retailers, and often lives behind advanced bot-detection systems.</p><p>We ran into this problem while building an AI teacher that could recommend relevant supplies. We asked Exa for products, but got back articles, not structured data. Same for Tavily and Bing (deprecated as of 8/13/25). And we got rejected from affiliate programs, who suggested we come back with 1000s of TikTok followers. Channel3 is the API we wished we had.</p><p>Product detail pages (PDPs) usually present the main item alongside recommendations. We use computer vision to isolate the main product and find its attributes, like title and price. We apply the same logic to the rest of the PDPs on the domain.</p><p>Products are often sold across multiple retailers, with no guarantee they‚Äôll be labeled consistently. We collapse products across the web into a canonicalized set by using LLMs and multimodal embeddings to actually understand each product.</p><p>To normalize everything into a schema that tries to be both minimal and extensible, we have to be opinionated. Are a $50 10‚Äù and $60 12‚Äù skillet the same product? Probably not, but the S/M/L variants of a T-shirt are. Our goal is that any product you‚Äôd search for specifically is treated as its own product.</p><p>We process a massive amount of data. We quickly ran out of room on our Cloudflare Vectorize indices and moved to the brand-new AWS S3 Vectors platform, syncing to OpenSearch for faster response times and more dynamic filtering. We hit rate limits constantly, so we spread our work over multiple cloud providers and AI models.</p><p>Developers earn commission on sales they drive (averaging 5%). Channel3 takes a cut. We want you to earn way more money from Channel3 than you spend on it. We win when you win.</p><p>We provide an API, SDK (Typescript and Python), and MCP. We offer 1000 free searches, and charge $7/1000 searches after that. You can view expected commissions per-brand on our dashboard.</p><p>So far, products are US-only (sorry! we will expand). We‚Äôre live with millions of products and hundreds of developers.</p><p>To get started, make a free account at <a href=\"https://trychannel3.com\">https://trychannel3.com</a>, then select which brands you‚Äôd like to sell (choose from 50k+ or request your own), generate an API key, and start selling and earning.</p><p>We‚Äôd really appreciate feedback from this community. If you‚Äôve built product search before, what did we miss in the schema? If you‚Äôve tried to add commerce to an app, what blocked you? If you tried to build this yourself, what did you learn? Are there endpoints you wish existed (e.g. price alerts, back-in-stock webhooks, product feed)? For any suggestions, we‚Äôre all ears.</p><p>We‚Äôll be in the thread all day to answer questions, share more technical detail, and hear whatever would make this most useful to you. Comment away!</p>","contentLength":3392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962881"},{"title":"Closer to the Metal: Leaving Playwright for CDP","url":"https://browser-use.com/posts/playwright-to-cdp","date":1755703964,"author":"gregpr07","guid":234679,"unread":true,"content":"<h2>Goodbye Playwright, Hello CDP</h2><p>Playwright and Puppeteer are great for making QA tests and automation scripts short and readable, but as AI browser companies have been <a href=\"https://www.browserbase.com/blog/taming-iframes-a-stagehand-update\">learning the hard way</a> over the last year, sometimes these adapters obscure important details about the underlying browsers.</p><p>We decided to peek behind the curtain and figure out what the browser was really doing, and it made us decide to drop playwright entirely and just speak the browser's native tongue: CDP.</p><p>By switcing to raw CDP we've massively increased the speed of element extraction, screenshots, and all our default actions. We've also managed to add new async reaction capabilities to the agent, and proper cross-origin iframe support.</p><blockquote></blockquote><img src=\"https://docs.monadical.com/uploads/0315c67b-b8db-4a62-914e-161bbc752297.png\" width=\"36%\"><p>Building AI browser automation is like building on top of a jenga tower of complexity. Every layer presents its own leaky abstractions, its own subtle crashes, and its own resource constraints.</p><p>If you've ever heavily depended on an adapter library and build up a large codebase around it, you know the feeling that eventually comes when you realize the adapter library is no longer saving you any time by \"hiding the true complexity\". In our case that time has finally come for Browser-Use and playwright-python, the library that we've historically used to drive our browsers with LLM-powered tool calls like , , .</p><p>At first glance it may seem foolish to throw out such a mature adapter library and reinvent the wheel, but luckily the needs of AI browser agents are much narrower than the entire surface area that playwright provides, and we believe we can implement the calls we need with more specialized logic to better suit AI drivers.</p><p>Playwright also introduces a 2nd network hop going through a node.js playwright server websocket, which incurs a meaningful amount of latency when we do thousands of CDP calls to check for element position, opacity, paint order, JS event listeners, aria properties, etc.</p><h2>üìú A Quick History of Browser Automation</h2><p>To really understand why the browser automation is in the state it's in today, we have to look back at some history.</p><ul><li> ‚Äì Lynx (text-mode browser) could browse and automate keystroke inputs from a script, still useful today!</li><li> ‚Äì Netscape Navigator (Unix) exposed <code>netscape -remote \"openURL(http://‚Ä¶)\")</code> to control an already-running GUI browser</li><li> ‚Äì Internet Explorer (Windows) exposed a COM automation object (InternetExplorer.Application) so VB/VBA/WSH could launch, navigate, read/write the DOM, handle events, etc.</li><li> ‚Äì Mercury‚Äôs web-focused tool appears: Astra QuickTest (which evolved into QuickTest Professional/QTP, later HP UFT), also WinRunner/XRunner</li><li> ‚Äì headless &amp; macro tools: HttpUnit (1999) (HTTP/HTML-level, no real browser), iMacros (2001) (record/replay in the browser), HtmlUnit (2002) (headless Java ‚Äúbrowser‚Äù)</li><li> ‚Äì Watir (Ruby) grows out of driving IE via COM/OLE and starts offering more general APIs</li></ul><ul><li> - Selenium + WebDriver join forces</li><li> ‚Äî Before headless Chrome,  (a headless WebKit-based browser) filled the gap for scripting ‚Äúlike a browser,‚Äù with mixed reliability.</li></ul><h4>The Pre-Headless Era (~~the Dark Ages~~)</h4><ul><li> ‚Äî Chrome ships ; work happens ‚Äúupstream‚Äù in WebKit so other ports can adopt it (post by Pavel Feldman).</li><li> ‚Äî <strong>WebKit Remote Debugging Protocol v1.0</strong> announced; early docs/talks outline the domains/events model that CDP still uses.</li><li> ‚Äî Blink forks from WebKit; the protocol solidifies on the Chromium side and becomes known as the <strong>Chrome DevTools Protocol (CDP)</strong>. Extensions can tunnel it via  and <a href=\"https://groups.google.com/a/chromium.org/g/chromium-dev/c/arQE-vrM2OA\"> flag</a>.</li><li> ‚Äî Chrome‚Äôs  (accessibility/automation) extension API appears (exposes the accessibility tree; separate from CDP).</li></ul><h4>Headless Chrome &amp; CDP Era</h4><ul><li> ‚Äî  announced;  introduced as a Chrome team Node library to drive Chrome (headless/full) via CDP.</li><li> ‚Äî  ships.</li><li> ‚Äî  becomes a  (cross-browser standard).  implements W3C WebDriver (and later BiDi) and is tightly coupled to Chrome releases.</li><li> ‚Äî Google I/O talk by <strong>Andrey Lushnikov &amp; Joel Einbinder</strong> (DevTools/Puppeteer team) popularizes modern testing with Puppeteer.</li></ul><h4>Multi-Browser Standardization Era</h4><ul><li> ‚Äî Several core Puppeteer engineers leave Google for Microsoft and start  (cross-browser automation/test framework) üé≠ (oooo drama)</li><li> ‚Äî  public release.</li><li> ‚Äî  ships.</li><li> ‚Äî  support begins (e.g., Playwright for Python announced Sep 30, 2020).</li><li> ‚Äî  adds  support (alongside classic WebDriver).</li><li> ‚Äî  adds  support; Selenium ‚Äúwelcomes Puppeteer to the WebDriver world.‚Äù</li></ul><p>This wheel has been reinvented every few years it seems.</p><h4>Modern Times: A Multitude of Choice</h4><p>Now in 2025 we are lucky to have many high quality driver libraries to choose between, our favorites include:</p><ul><li>‚≠êÔ∏è <a href=\"https://pydoll.tech/\"></a> (best python-first playwright replacement)</li><li>‚≠êÔ∏è <a href=\"https://github.com/go-rod/rod\"></a> (best CDP reference implementation), <a href=\"https://github.com/chromedp/chromedp\"></a> (great CDP debug tooling)</li><li><a href=\"https://github.com/seleniumbase/SeleniumBase\"></a> (and Selenium Grid) still a great mature option today, <a href=\"https://github.com/cypress-io/cypress\"></a> (automate with old-school WebDrivers)</li><li><a href=\"https://github.com/appium/appium\"></a> automate via system-level accessibility APIs on Android, iOS, macOS, Windows</li></ul><p>So why did we feel the need to write our own with ? Well for all the same reasons as everyone else: everlasting desire to be closer to the metal and have more detailed control over every step.</p><h2>How do Browser Drivers Work?</h2><blockquote><p>So what APIs does the browser actually expose anyway?\nWhat sits underneath all these \"drivers\"?</p></blockquote><h3>üîå What are the automation APIs that Chromium actually exposes?</h3><p>All these adapter libraries, drivers, and AI helper extensions really just exist to pass messages and make RPC calls to these underlying browser APIs:</p><ul><li><ul><li><code>chrome.tabs.captureVisibleTab()</code></li><li><code>chrome.automation.getTree()</code></li><li><code>chrome.scripting.executeScript()</code></li><li><code>chrome.debugger.sendCommand({tabId: 123}, \"Page.navigate\", {url})</code></li><li>... and <a href=\"https://developer.chrome.com/docs/extensions/reference/api/debugger\">many more</a> ...<blockquote><p> appear to be the most powerful at first glance because they encompass CDP with , but raw CDP lets you access some calls that are not available through , and allows parallel connections to multiple targets.</p></blockquote></li></ul></li><li><p> (via pure CDP Websocket or WebDriver BIDI socket)</p><ul><li><code>DOMSnapshot.captureSnapshot()</code></li><li><code>Page.handleJavaScriptDialog({accept: true})</code></li><li><code>Browser.setDownloadBehavior()</code></li></ul></li><li><p><strong>OS-Level Accessibility &amp; screenreader APIs</strong> (NVDA/Voice Over/AppleScript/Appium/etc.)</p><ul><li>get a tree/rotor view of all elements shown to screenreaders (links, buttons, inputs, ...)</li><li>script copy/paste, mouse, keypress, and <a href=\"https://support.apple.com/guide/voiceover/by-dom-or-group-mode-vo2711/10/mac/15.0\">element rotor</a>/tab-based navigation</li></ul></li><li><p><strong>Internal Chromium C++ APIs</strong> (within Chromium source code)</p><ul><li>you can call arbitrary helpers <code>content/browser/devtools/protocol/*_handler.cc</code></li><li>you can edit the CDP spec and add commands to call custom C++ APIs <code>third_party/blink/public/devtools_protocol/browser_protocol.pdl</code></li><li><em>... anything is possible when the call is coming from inside the house ...</em></li></ul></li><li><p><strong>Launch Flags, User Data Dir, and Preferences</strong></p><ul><li>we've tested over 300+ chrome launch flags, launching is a whole world of complex behavior: https://peter.sh/experiments/chromium-command-line-switches/</li><li>user data dir state can drastically affect browser behavior, there are preferences files, state dbs, cookie stores https://github.com/thewh1teagle/rookie and more</li><li>Profile Preferences (e.g. theme color, downloads dir, extension display settings, etc.), enterprise / registry options, and  options</li></ul></li></ul><ul><li><p>Classic WebDriver W3C / ChromeDriver REST APIs</p><ul><li><code>/session/{id}/element/{eid}/click</code></li><li>... etc.\nThese are not actually exposed by any browser directly, rather they are the W3C standardized REST API shape recommended for drivers (ChromeDriver, GeckoDriver, WebKitDriver, selenium) to provide to clients above raw browser calls via CDP/BIDI.</li></ul></li><li><p>Webdriver BIDI (websocket)</p><ul><li>merger of the old REST-API based WebDriver system + CDP in a single websocket, official release delayed for years now, not feature complete yet. check back in 2027</li></ul></li></ul><h3>üé≠ How does Playwright work?</h3><p>Playwright achieves multi-languge support by using a client-server model between clients in various languages and a single core implementation that runs as a node.js websocket server.</p><p>The playwright node.js relay server accepts standardized \"playwright protocol\" RPC calls from playwright clients, and then sends out CDP or BIDI calls to the browser to execute them.</p><p>This API is elegant in some ways, the \"playwright protocol\" of commands provides a nicely typed RPC interface and standardizes behavior across languages. Playwright also nicely abstracts lower-level browser ideas like targets, frames, and sessions into simple  and  handles and (usually) manages to keep those handles in sync and not deadlocked across node.js, the browser, and python.</p><p>Unfortunately the double RPC through the node.js relay means some state inevitably drifts across the 3 places (and across three different languages and runtimes):</p><ul><li>playwright node.js relay process</li></ul><p>When a tab crashes in the browser or some operation is performed without focusing a page correctly, there are edge cases where the node.js process can hang indefinitely waiting for a browser reply, meanwhile the python client needs to send the CDP call the browser is expecting in order to proceed.\nCurrently we have no recourse but to kill -9 and attempt to reconnect to the browser from scratch with a new playwright instance.</p><p>There are numerous cases like that only crop up in 1% of cases with specific slow network conditions, but edge cases can quickly drag down overall success scores when we run thousands of steps per eval.</p><h4>ü©∏ Playwright's Sharp Edges</h4><p>The playwright happy paths usually work fine, but the devil is in the details:</p><ul><li> screenshot on pages longer than  high (reliably crashes playwright)</li><li>// handling</li><li>attempting to keyboard/mouse/dialog input without focusing a page</li><li>file upload &amp; download handling on remote browsers</li><li>, , , , PDF tab handling</li><li>chrome preferences and enterprise/registry configuration management</li></ul><p> helps but it only goes so far, at a certain point it doens't make sense to build workarounds around a relay layer that we're fighting to customize and control anyway.</p><p>Sometimes when you are forced to thoroughly stretch every nook and cranny of an adapter layer, you start to see the ugly truths of the underlying resource, and you no longer want the \"pretty version\" as a veil pulled over your eyes, you'd rather see the ugly truth.</p><h2>üç≥ Starting From Scratch: Out of the frying pan and into the fire</h2><p>Delivering a reliable experience when so many of the underlying components are inherently unreliable (or actively adversarial) is a monumental engineering challenge.</p><p><strong>Did you know there are at least 10 different ways a tab can crash in Chrome?</strong></p><ul><li>all targets start in a briefly semi-\"crashed\"/unresponsive state while initial requests are inflight, before the main page JS thread starts</li><li>chrome zygote/root process can crash (slow user_data_dir/filesystem io, oom, cpu lag, etc.)</li><li>GPU process can crash, there's even a helpful CDP call </li><li>page renderers can crash due to exceptions raised within chrome source (sigsev, oom, etc.)</li><li>page renderers can crash because the page exceeds allowed resources ()</li><li>page can spinlock/oom due to infinite loops or crypto mining in its JS main thread</li><li>scrolling/input/screenshot before  focus can crash targets (5sec delayed!)</li><li>handling a JS popup before activateTarget or attempting to handle it after already closing</li><li>parent frame navigation during child  \"are you sure you want to leave?\"</li><li>any of the above crashes in a nested OOPIF leading to subtle issues in the parent target</li></ul><p>Playwright handled about half of these well, and presented impassible barrier to solving the other half, so we made the call to switch. But now we're faced with the difficult challenge of solving 100% of these cases ourself.</p><p>We take on this challenge with glee, we'd rather lose sleep thinking about these things so you can build reliable apps on top of us üí™.</p><h2>Case Studies: Key Changes in the Migration</h2><h3>New CDP-USE Library Providing Python Type Bindings</h3><p>A type-safe Python client generator for the Chrome DevTools Protocol (CDP). This library automatically generates Python bindings with full TypeScript-like type safety from the official CDP protocol specifications. It's only shallow type bindings, no complex logic for session management, pages, elements, etc. just 100% direct access.</p><blockquote></blockquote><h2>New Event-Driven Architecturre</h2><p>We used to only update our view of the world between actions, right before sending the next state summary to the LLM. This makes sense when your assumption is that the page contents will only change as a result of actions, but this is not always true!</p><p>Take for example a slowly loading list of results that stream in, an animated carousel, or a bit of JS that runs every 3s. All of these are examples of things that can happen at any point in the agent action/runloop cycle.</p><p>We've introduced a new event-driven architecture to better fit the underlying event-driven architecture of CDP. Now we can subscribe to and respond to CDP events, which we set up in \"watchdog\" services that monitor for various things.</p><p>For example, our  watches for any file downloads that start spontaneously, whether triggered by a click, js executing, or any other method.  can now watch for page crashes in a single place by just subscribing to a crash event, and we no longer have to scatter crash detection and retry logic all over the rest of the codebase.</p><blockquote></blockquote><h3>New Extracted Element Handle that works across OOPIFs</h3><p>A tab is not a page; it‚Äôs a constellation of  (root + cross-origin iframes + workers), each hosting , each containing . Abstract that away and you lose the ability to route input, correlate events, and re-find elements after DOM churn.</p><p>We now represent nodes with \"super-selectors\" that include , , , x/y position, and fallback selectors:</p><pre><code>@dataclass(frozen=True)\nclass EnhancedDOMTreeNode:\n    target_id: str                 # which DevTools target owns the renderer\n    frame_id: str                  # which frame inside that target\n    backend_node_id: int           # renderer-local node handle\n    frame_path: Tuple[str, ...]    # root ‚Üí ... ‚Üí leaf, useful for sanity checks\n    element_index: int             # LLM-friendly stable ordinal for UX\n    ...\n</code></pre><pre><code>class BrowserSession:\n    # caches are kept warm by watchers listening to Target.* and Page.*\n    def cdp_client_for_frame(self, frame_id: str):\n        target_id = self.target_id_by_frame_id(frame_id)\n        return self.cdp_clients_for_target(target_id)[0]  # long-lived session\n\n    def route_to_node(self, ref: EnhancedDOMRef):\n        client = self.cdp_client_for_frame(ref.frame_id)\n        return client, {\"session_id\": self.session_id_by_frame_id(ref.frame_id)}\n</code></pre><p>Outcome: zero guessing about who owns the node or where input should land, even with nested cross-origin iframes and DOM element shifts after actions.</p><img src=\"https://docs.monadical.com/uploads/1ac29f03-cd02-4f35-8183-a56e289f0ff0.png\" width=\"34%\"><p>Back in my first startup job in 2014 we were using PhantomJS and some RPC between python and JS, and in a way it's surprising how little has changed since then. Now it's 2025 I'm still dealing with all the same issues: tab crash handling, page load retrying, JS+Python RPC translation issues, python asyncio headaches, mouse movement fuzzing, etc.</p><p>Luckily a lot has improved since 2014, and we finally have a big light at end of the tunnel leading out of the manual QA automation mines: AI.</p><p>We aim to continue solving all the complexities of browser automation and CDP for our users. Our agents shouldn't have to know the nuances of CDP Targets in order to Get Stuff Done‚Ñ¢Ô∏è, and neither should you.</p><p>Try out our new libraries and beta releases with cdp-use and let us know your feedback!</p>","contentLength":15222,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962869"},{"title":"AWS in 2025: Stuff you think you know that's now wrong","url":"https://www.lastweekinaws.com/blog/aws-in-2025-the-stuff-you-think-you-know-thats-now-wrong/","date":1755703807,"author":"keithly","guid":234659,"unread":true,"content":"<p>One of the neat things about AWS is that it‚Äôs almost twenty years old. One of the unfortunate things about AWS is‚Ä¶ that it‚Äôs almost twenty years old. If you‚Äôve been using the platform for a while, it can be hard to notice the pace of change in the underlying ‚Äúfoundational‚Äù services. More worryingly, even if you‚Äôre not an old saw at AWS scrying, it‚Äôs still easy to stumble upon outdated blog posts that speak to the way things used to be, rather than the way they are now. I‚Äôve gathered some of these evolutions that may help you out if you find yourself confused.</p><p>In EC2, you can now change security groups and IAM roles without shutting the instance down to do it.&nbsp;</p><p>You can also resize, attach, or detach EBS volumes from running instances.&nbsp;</p><p>As of very recently, you can also force EC2 instances to stop or terminate without waiting for a clean shutdown or a ridiculous timeout, which is great for things you‚Äôre never going to spin back up.&nbsp;</p><p>They also added the ability to live-migrate instances to other physical hosts; this manifests as it being much rarer nowadays to see an instance degradation notice.&nbsp;</p><p>Similarly, instances have gone from a ‚Äúexpect this to disappear out from under you at any time‚Äù level of reliability to that being almost unheard of in the modern era.&nbsp;</p><p>Spot instances used to be much more of a bidding war / marketplace. These days the shifts are way more gradual, and you get to feel a little bit less like an investment banker watching the numbers move on your dashboards in realtime.&nbsp;</p><p>You almost never need dedicated instances for anything. It‚Äôs been nearly a decade since they weren‚Äôt needed for HIPAA BAAs.&nbsp;</p><p>AMI Block Public Access is now default for new accounts, and was turned on for any accounts that hadn‚Äôt owned a public AMI for 90 days back in 2023.</p><p>S3 isn‚Äôt eventually consistent anymore‚Äìit‚Äôs read-after-write consistent.</p><p>You don‚Äôt have to randomize the first part of your object keys to ensure they get spread around and avoid hotspots.&nbsp;</p><p>ACLs are deprecated and off by default on new buckets.</p><p>Block Public Access is now enabled by default on new buckets.</p><p>New buckets are transparently encrypted at rest.&nbsp;</p><p>Once upon a time Glacier was its own service that had nothing to do with S3. If you look closely (hi, billing data!) you can see vestiges of how this used to be, before the S3 team absorbed it as a series of storage classes.&nbsp;</p><p>Similarly, there used to be truly horrifying restore fees for Glacier that were also very hard to predict. That got fixed early on, but the scary stories left scars to the point where I still encounter folks who think restores are both fiendishly expensive as well as confusing. They are not.</p><p>Glacier restores are also no longer painfully slow.</p><p>Obviously EC2-classic is gone, but that was a long time ago. One caveat that does come up a lot is that public v4 IP addresses are no longer free; they cost the same as Elastic IP addresses.&nbsp;</p><p>VPC peering used to be annoying; now there are better options like Transit Gateway, VPC sharing between accounts,  sharing between accounts, and Cloud WAN.&nbsp;</p><p>VPC Lattice exists as a way for things to talk to one another and basically ignore a bunch of AWS networking gotchas. So does Tailscale.</p><p>CloudFront isn‚Äôt networking but it has been in the AWS ‚Äúnetworking‚Äù section for ages so you can deal with it: it used to take ~45 minutes for an update, which was terrible. Nowadays it‚Äôs closer to 5 minutes‚Äîwhich still feels like 45 when you‚Äôre waiting for CloudFormation to finish a deployment.</p><p>ELB Classic (‚Äúclassic‚Äù means ‚Äúdeprecated‚Äù in AWS land) used to charge cross AZ data transfer in addition to the load balancer ‚Äúdata has passed through me‚Äù fee to send to backends on a different availability zone.&nbsp;</p><p>ALBs with automatic zone load balancing do not charge additional data transfer fees for cross-AZ traffic, just their LCU fees. The same is true for Classic Load Balancers, but be warned: Network Load Balancers still charge cross-AZ fees!</p><p>Network Load Balancers didn‚Äôt used to support security groups, but they do now.&nbsp;</p><p>Availability Zones used to be randomized between accounts (my us-east-1a was your us-east-1c); you can now use Resource Access Manager to get zone IDs to ensure you‚Äôre aligned between any given accounts.</p><p>Originally Lambda had a 5 minute timeout and didn‚Äôt support container images. Now you can run them for up to 15 minutes, use Docker images, use shared storage with EFS, give them up to 10GB of RAM (for which CPU scales accordingly and invisibly), and give /tmp up to 10GB of storage instead of&nbsp; just half a gig.</p><p>Invoking a Lambda in a VPC is no longer dog-slow.</p><p>Lambda cold-starts are no longer as big of a problem as they were originally.</p><p>You no longer have to put a big pile of useless data on an EFS volume to get your IO allotment to something usable; you can adjust that separately from capacity now that they‚Äôve added a second knob.</p><p>You get full performance on new EBS volumes that are empty. If you create an EBS volume from a snapshot, you‚Äôll want to read the entire disk with dd or similar because it lazy-loads snapshot data from S3 and the first read of a block will be very slow.&nbsp; If you‚Äôre in a hurry, there are <a href=\"https://docs.aws.amazon.com/ebs/latest/userguide/initalize-volume.html\">more expensive and complicated options</a>.&nbsp;</p><p>EBS volumes can be attached to multiple EC2 instances at the same time (assuming io1), but you almost certainly don‚Äôt want to do this.</p><p>You can now have empty fields (the newsletter publication system for ‚ÄúLast Week in AWS‚Äù STILL uses a field designator of  because it predates that change) in an item.&nbsp;</p><p>Performance has gotten a lot more reliable, to the point where you don‚Äôt need to use support-only tools locked behind NDAs to see what your hot key problems look like.&nbsp;</p><p>With pricing changes, you almost certainly want to run everything On Demand unless you‚Äôre in a very particular space.</p><p>Reserved Instances are going away for EC2, slowly but surely. Savings Plans are the path forward. The savings rates on these have diverged, to the point where they no longer offer as deep of a discount as RIs once did, which is offset by their additional flexibility. Pay attention!</p><p>EC2 charges by the second now, so spinning one up for five minutes over and over again no longer costs you an hour each time.</p><p>The Cost Anomaly Detector has gotten very good at flagging sudden changes in spend patterns. It is free.&nbsp;</p><p>The Compute Optimizer also does EBS volumes and other things. Its recommendations are trustworthy, unlike ‚ÄúTrusted‚Äù Advisor‚Äôs various suggestions.&nbsp;</p><p>The Trusted Advisor recommendations remain sketchy and self-contradictory at best, though some of their cost checks can now route through Compute Optimizer.</p><p>IAM roles are where permissions should live. IAM users are strictly for legacy applications rather than humans. The IAM Identity Center is the replacement for ‚ÄúAWS SSO‚Äù and it‚Äôs how humans should engage with their AWS accounts. This does cause some friction at times.</p><p>You can have multiple MFA devices configured for the root account.&nbsp;</p><p>You also do not need to have root credentials configured for organization member accounts.</p><p>us-east-1 is no longer a merrily burning dumpster fire of sadness and regret. This is further true across the board; things are a lot more durable these days, to the point where outages are noteworthy rather than ‚Äúit‚Äôs another given Tuesday afternoon.‚Äù</p><p>While deprecations remain rare, they‚Äôre definitely on the rise; if an AWS service sounds relatively niche or goofy, consider your exodus plan before building atop it. None of the services mentioned thus far qualify.&nbsp;</p><p>CloudWatch doesn‚Äôt have the last datapoint being super low due to data inconsistency anymore, so if your graphs suddenly drop to zero for the last datapoint your app just shit itself.&nbsp;</p><p>You can close AWS accounts in your organization from the root account rather than having to log into each member account as their root user.</p><p>My thanks to folks on <a href=\"https://www.linkedin.com/posts/coquinn_im-working-on-a-blog-post-and-i-want-your-activity-7361205481043881985-UbhA\">LinkedIn</a> and <a href=\"https://bsky.app/profile/quinnypig.com/post/3lvegzxknvc2t\">BlueSky</a> for helping come up with some of these. You‚Äôve lived the same pain I have.</p>","contentLength":8038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962844"},{"title":"Home Depot sued for 'secretly' using facial recognition at self-checkouts","url":"https://petapixel.com/2025/08/20/home-depot-sued-for-secretly-using-facial-recognition-technology-on-self-checkout-cameras/","date":1755703399,"author":"mikece","guid":235612,"unread":true,"content":"<p>¬© 2025 PetaPixel Inc. All rights reserved.</p>","contentLength":43,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962771"},{"title":"Show HN: What country you would hit if you went straight where you're pointing","url":"https://apps.apple.com/us/app/leascope/id6608979884","date":1755703381,"author":"brgross","guid":234779,"unread":true,"content":"<p>\n    The developer, , indicated that the app‚Äôs privacy practices may include handling of data as described below. For more information, see the <a href=\"https://gist.github.com/poopmanchu/a7d61f69ff9aeb3a0a37c61beb3962fd\">developer‚Äôs privacy policy</a>.\n  </p><div><div><p>The developer does not collect any data from this app.</p></div></div><p>Privacy practices may vary, for example, based on the features you use or your age. <a href=\"https://apps.apple.com/story/id1538632801\">Learn&nbsp;More</a></p>","contentLength":327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962767"},{"title":"Why are anime catgirls blocking my access to the Linux kernel?","url":"https://lock.cmpxchg8b.com/anubis.html","date":1755701685,"author":"taviso","guid":234771,"unread":true,"content":"<p><em>Hey‚Ä¶ quick question, why are anime catgirls blocking my access to\nthe Linux kernel?</em></p><section><p>I‚Äôve started running into more sites recently that deploy <a href=\"https://github.com/TecharoHQ/anubis\">Anubis</a>, a sort of hybrid\nart project slash network countermeasure. The project ‚Äúweighs the souls‚Äù\nof HTTP requests to help protect the web from AI crawlers.</p><p>If you‚Äôve seen anime catgirl avatars when visiting a new website,\nthat‚Äôs Anubis.</p><p>I‚Äôm sympathetic to the cause ‚Äì I host this blog on a single core\n128MB VPS, I can tell you some stories about aggressive crawlers!</p><p>Anubis recently started blocking how I access <a href=\"https://git.kernel.org/\">git.kernel.org</a> and <a href=\"https://lore.kernel.org\">lore.kernel.org</a>. Those sites host the\n<a href=\"https://en.wikipedia.org/wiki/LKML\">Linux Kernel Mailing\nList</a> archive and the kernel git repositories. As far as I know I do\nhave a soul, I just wasn‚Äôt using a desktop browser‚Ä¶ so how exactly is my\nsoul being weighed?</p><blockquote><p>Note: Linux has Tux üêß, OpenBSD has Puffy üê°, SuSE has Geeko ü¶é and\nMicrosoft has Bob ü§ì‚Ä¶ nothing wrong with mascots! üò∏</p></blockquote></section><section><p>The traditional solution to blocking nuisance crawlers is to use a\ncombination of <a href=\"https://en.wikipedia.org/wiki/Rate_limiting\">rate limiting</a> and\n<a href=\"https://en.wikipedia.org/wiki/Captcha\">CAPTCHAs</a>. The\nCAPTCHA forces vistors to solve a problem designed to be very difficult\nfor computers but trivial for humans. This isn‚Äôt perfect of course, we\ncan debate the accessibility tradeoffs and weaknesses, but conceptually\nthe idea makes some sense.</p><p>Anubis ‚Äì confusingly ‚Äì inverts this idea. It insists visitors solve a\nproblem trivial for computers, but impossible for humans. Visitors are\nasked to brute force a value that when appended to a challenge string,\ncauses it‚Äôs SHA-256 to begin with a few zero nibbles.</p><blockquote><div><pre><code></code></pre></div></blockquote><p>If that sounds familiar, it‚Äôs because it‚Äôs similar to how bitcoin\nmining works. Anubis is not literally mining cryptocurrency, but it\n similar in concept to other projects that do exactly that,\nperhaps most famously <a href=\"https://torrentfreak.com/the-pirate-bay-website-runs-a-cryptocurrency-miner-170916/\">Coinhive</a>\nand <a href=\"https://github.com/JSEcoin/website/blob/master/README.md\">JSECoin</a>.</p><p>So how do some useless SHA-256 operations prove you‚Äôre not a bot? The\nargument goes that this simply makes it too expensive to crawl your\nwebsite.</p><p>This‚Ä¶ makes no sense to me. Almost by definition, an AI vendor will\nhave a datacenter full of compute capacity. It feels like this solution\nhas the problem backwards, effectively only limiting access to those\n resources or trying to conserve them.</p></section><section><p>Let‚Äôs assume the argument has some merit and math out the claims.</p><p>We can see that with the default Anubis configuration, a typical\nwebsite visitor will have to solve a challenge with a difficulty of\n4.</p><blockquote><div><pre><code></code></pre></div></blockquote><p>This means that a visitor must make the first 4 hex digits of the\nchallenge hash be zero, so 16 bits (4 digits, one nibble each).\nTherefore, you can expect to mine a suitable  within\n2^16 SHA-256 operations.</p><p>If every single github star on the anubis project represents a\nwebsite that has deployed Anubis, how much would the cloud services bill\nbe to mine enough tokens to crawl every single website?</p><p>At the time of writing, Anubis has 11,508 github stars.</p><p>The default configuration means mining one token gets you access for\n7 days <small>(although I think this expiration check is broken, see\nbelow)</small>, so we need 11,508 * 2^16 SHA-256 operations per week,\nhow expensive is that?</p><p>To get some numbers, I started an  vm on Google\nCompute Engine, and ran . This is what you get\nin the <a href=\"https://cloud.google.com/free/docs/free-cloud-features#compute\">free\ntier</a>.</p><pre><code>$ openssl speed sha256\nDoing sha256 for 3s on 16 size blocks: 6915549 sha256's in 3.00s\nDoing sha256 for 3s on 64 size blocks: 4631718 sha256's in 3.00s\nDoing sha256 for 3s on 256 size blocks: 393694 sha256's in 3.21s\nDoing sha256 for 3s on 1024 size blocks: 100123 sha256's in 3.00s\nDoing sha256 for 3s on 8192 size blocks: 13300 sha256's in 2.98s\nDoing sha256 for 3s on 16384 size blocks: 7137 sha256's in 2.99s\nversion: 3.0.17\nbuilt on: Tue Aug  5 07:09:41 2025 UTC\noptions: bn(64,64)\ncompiler: gcc -fPIC -pthread -m64 ...\nThe 'numbers' are in 1000s of bytes per second processed.\ntype             16 bytes     64 bytes    256 bytes   1024 bytes   8192 bytes  16384 bytes\nsha256           36882.93k    98809.98k    31397.40k    34175.32k    36561.61k    39107.90k</code></pre><p>It looks like we can test about 2^21 every second, perhaps a bit more\nif we used both SMT sibling cores. This amount of compute is simply too\ncheap to even be worth billing for.</p><p>So (11508 websites * 2^16 sha256 operations) / 2^21, that‚Äôs about 6\nminutes to mine enough tokens for every single Anubis deployment in the\nworld. That means the cost of unrestricted crawler access to the\ninternet for a week is approximately $0.</p><p>In fact, I don‚Äôt think we reach a single cent per month in compute\ncosts until several million sites have deployed Anubis.</p><p>I‚Äôm just not convinced this math works‚Ä¶ this is \nnothing for a souless AI vendor with a monthly cloud services budget in\nthe 8 figures. However, the cost for real soul-owning humans with\nlimited access to compute is high ‚Äì the Anubis forums are full of\ncomplaints like these:</p></section><section><p>Anubis cites <a href=\"https://en.wikipedia.org/wiki/Hashcash#Applications\">hashcash</a>\nas the primary inspiration for their design, an anti-spam solution from\nthe 90s that was never widely adopted.</p><p>The idea of ‚Äúweighing souls‚Äù reminded me of another anti-spam\nsolution from the 90s‚Ä¶ believe it or not, there was once a company that\nused poetry to block spam!</p><p><a href=\"https://web.archive.org/web/20030210085930/http://habeas.com/services/swe.htm\">Habeas</a>\nwould license short haikus to companies to embed in email headers. They\nwould then aggressively sue anyone who reproduced their poetry <a href=\"https://web.archive.org/web/20030207155559/http://www.habeas.com/faq/index.htm#5\">without\na license</a>. The idea was you can safely deliver any email with their\nheader, because it was too legally risky to use it in spam.</p><blockquote><p>winter into spring  brightly anticipated </p></blockquote></section><section><p>So you‚Äôre trying to read LKML, but catgirl says no‚Ä¶ is there a\nsolution?</p><p>My issue is I don‚Äôt want to use a desktop browser to mine the\nrequired value, so how can I get the auth cookie?</p><p>If we look at the response with , we can see the\nchallenge in the HTTP headers:</p><pre><code>$ curl -I https://lore.kernel.org/\nHTTP/2 200\nserver: nginx\nset-cookie: techaro.lol-anubis-auth=; Path=/\nset-cookie: techaro.lol-anubis-cookie-test-if-you-block-this-anubis-wont-work=5d737f0600ff2dd; Path=/</code></pre><p>That <code>techaro.lol-anubis-cookie</code> is the challenge, here is\na quick C program to mine an acceptable token:</p><div><pre><code></code></pre></div><p>Let‚Äôs run that and see what it says‚Ä¶</p><pre><code>$ gcc -Ofast -march=native anubis-miner.c -lcrypto -o anubis-miner\n$ time ./anubis-miner 5d737f0600ff2dd\n47224\n\nreal    0m0.017s\nuser    0m0.016s\nsys     0m0.000s</code></pre><p>Looks okay, let‚Äôs verify that solution is correct:</p><pre><code>$ printf \"5d737f0600ff2dd%d\" 47224 | sha256sum\n000043f7c4392a781a04419a7cb503089ebcf3164e2b1d4258b3e6c15b8b07f1  -</code></pre><p>It seems valid, so now we can get a signed auth cookie by sending\nback the value we mined:</p><pre><code>$ curl -I --cookie \"techaro.lol-anubis-cookie-test-if-you-block-this-anubis-wont-work=5d737f0600ff2dd\" \\\n    'https://lore.kernel.org/.within.website/x/cmd/anubis/api/pass-challenge?response=000043f7c4392a781a04419a7\ncb503089ebcf3164e2b1d4258b3e6c15b8b07f1&amp;nonce=47224&amp;redir=/&amp;elapsedTime=0'\nHTTP/2 302\nserver: nginx\nlocation: /\nset-cookie: techaro.lol-anubis-auth=eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJhY3Rpb24iO...OTYifQ...;</code></pre><p>Success, this cookie is now valid for 1 week of access. Let‚Äôs\nvalidate what it sent us, the actual schema is visible in the code:</p><blockquote><div><pre><code></code></pre></div></blockquote><p>We can examine this auth token and see what Anubis gave us‚Ä¶</p><pre><code>$ base64 -d &lt;&lt;&lt; eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9 | jq\n{\n  \"alg\": \"EdDSA\",\n  \"typ\": \"JWT\"\n}\n$ base64 -d &lt;&lt;&lt; eyJhY3Rpb24iO...OTYifQ== | jq\n{\n  \"action\": \"CHALLENGE\",\n  \"challenge\": \"5d737f0600ff2dd\",\n  \"exp\": 1756185722,\n  \"iat\": 1755580922,\n  \"method\": \"fast\",\n  \"nbf\": 1755580862,\n  \"policyRule\": \"dbf942088788cc96\"\n}</code></pre><p>It looks like  is the expiry date, so\n, which is‚Ä¶</p><pre><code>$ date --date @1756185722\nMon Aug 25 22:22:02 PDT 2025</code></pre><p>Yep, about 7 days from the date I requested it. You can now place\nthat into a cookie file for , etc.</p><blockquote><p>Interestingly, sending the same request the next day got me a new\nsigned cookie!?</p><p>This seems like a bug ‚Äì exchanging a mined token for an auth cookie\nshould immediately remove the challenge from the store, or there is a <a href=\"https://en.wikipedia.org/wiki/Double-spending\">double spend</a>\nvulnerability.</p><p>This error benefits me, I have to mine less tokens, but I‚Äôll open an\nissue üòá</p><p>Update: wow, <a href=\"https://github.com/TecharoHQ/anubis/pull/1003\">fixed</a> just a\nfew minutes after opening an issue by the maintainer!</p></blockquote><p>This dance to get access is just a minor annoyance for me, but I\nquestion how it proves I‚Äôm not a bot. These steps can be trivially and\ncheaply automated.</p><p>I think the end result is just an internet resource I need is a\nlittle harder to access, and we have to waste a small amount of\nenergy.</p></section><section><ul><li>I wrote this article with my own puny brain, I didn‚Äôt use any AI. I\nknow there are  (they‚Äôre \nemdashes!) ‚Äì I have a habit of writing two consecutive dashes, which <a href=\"https://pandoc.org/demo/example33/7.1-typography.html\">pandoc</a>\nconverts into <a href=\"https://unicode-explorer.com/c/2013\">U+2013</a>.</li><li>This post is a bit critical of a small well-intentioned project, so\nI felt obliged to email the maintainer to discuss it before posting it\nonline. I didn‚Äôt hear back.</li></ul></section>","contentLength":8647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962529"},{"title":"Show HN: I was curious about spherical helix, ended up making this visualization","url":"https://visualrambling.space/moving-objects-in-3d/","date":1755698567,"author":"damarberlari","guid":234578,"unread":true,"content":"<p>MOVING OBJECTS IN 3D SPACE</p><p>tap/click the right side of the screen to go forward ‚Üí</p><h3>Have you ever wondered how to move objects along a spherical helix path?</h3><p>Okay‚Ä¶ probably not, right?</p><p>But one morning, this question popped into my head.</p><p>It stuck with me long enough that I ended up diving into a few articles about it.</p><p>From there, it spiraled into lots of explorations, trying to figure out how to move objects in 3D space.</p><p>...to this complex, chaotic path.</p><p>All these explorations made me want to share what I learned with you.</p><p>I hope you enjoy this as much as I did.</p><p>A helix is a shape that loops around and around, like a spring.</p><h3>In a spherical helix, it loops around a sphere.</h3><h3>To move an object along a spherical helix path, we need to define its 3D coordinates to follow a helical pattern around a sphere.</h3><p>We'll get there! But first, let‚Äôs see how to position and move objects in 3D space.</p><p>In 3D space, we position objects by setting its coordinates along three axes: x,y, and z.</p><p>The x-axis typically represents horizontal movement‚Äîleft or right.</p><p>The y-axis typically represents vertical movement‚Äîup or down.</p><p>The z-axis typically represents depth‚Äîforward or backward</p><p>To move an object in 3D space, we can use mathematical functions to set its position over time.</p><p>For example, this cube's x position is set to 10 * cos(œÄt/2), where t is time (in seconds).</p><p>The result? It oscillates from 10 to -10 along the x-axis every 2 seconds, following a cosine wave.</p><p>Similarly, setting the y position to 10 * cos(œÄt/2) makes the cube oscillates vertically, from 10 to -10 every 2 seconds.</p><p>We can create a two-dimensional path by setting the x and y positions to different functions.</p><p>For this circle, the x position is set to 10 * cos(œÄt/2).</p><p>The cube starts at x = 10, moves to -10 in 2 seconds, then back to 10, and so on.</p><p>Meanwhile, the y position is set to 10 * sin(œÄt/2).</p><p>The movement for x and y may look similar, but they are actually out of phase.</p><p>When x = 10, y = 0; when x = 0, y = 10; and so on.</p><p>Together, these two functions create a circular path for the cube.</p><p>Now we can get creative with functions to create even more complex paths.</p><p>For example, let's multiply the x function by 0.03 * t.</p><p>It would make the cube oscillates farther on the x-axis over time.</p><p>..and we will have a circular path whose radius grows over time.</p><h3>Okay, now it's time to talk about the spherical helix (finally!)</h3><h3>The spherical helix path is similar to the spiral we just made, but with some differences.</h3><h3>First, a spherical helix is three-dimensional.</h3><p>It has a z component that changes over time.</p><p>This cube's z position is set as 10 * cos(0.02 * œÄt).</p><p>It will start from z = 10 then slowly move to -10.</p><p>Second, unlike the previous spiral, the x and y positions don‚Äôt grow indefinitely.</p><p>They grow at first, then shrink halfway through.</p><p>This is because the x function is multiplied by another sine function: sin(0.02 * œÄt)</p><p>which makes the radius larger in the middle and smaller at the ends.</p><p>The same is also done to the y function.</p><h3>Together, these functions create a spherical helix.</h3><h3>By updating the cube‚Äôs position with these functions, it moves along a spherical helix path.</h3><p>In summary, we can move objects in 3D space by defining their x, y, z coordinates as functions of time.</p><p>These functions, which express x, y, z coordinates as a function of another variable (in this case, time), are called parametric equations.</p><p>Check out the Wikipedia article for more on parametric equations.</p><p>Now that we know this, we can get creative and move objects along any path we want!</p><p>...to this complex, chaotic path...</p><p>...which we know now isn't actually chaotic.</p><p>It's just a path defined by mathematical functions.</p><p>Thanks for sticking with me, I hope you enjoyed it!</p><p>visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.</p><p>If you like this, please consider following me on Twitter and sharing this with your friends.</p><p>I'm planning to write more articles like this, so stay tuned!</p><p>https://twitter.com/damarberlari</p>","contentLength":4018,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962066"},{"title":"Gemma 3 270M re-implemented in pure PyTorch for local tinkering","url":"https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/12_gemma3","date":1755698486,"author":"ModelForge","guid":234658,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44962059"},{"title":"Improvements to OCaml code editing: the basics of a refactor engine","url":"https://tarides.com/blog/2025-08-20-internship-report-refactoring-tools-coming-to-merlin/","date":1755697035,"author":"nukifw","guid":234678,"unread":true,"content":"<p>Refactoring features have contributed to the popularity of editors like <a href=\"https://www.jetbrains.com/idea/\">IntelliJ</a>, as well as certain programming languages whose editor support offers interactive mechanisms to manage code ‚Äî <a href=\"https://gleam.run/language-server/\">Gleam</a> being an excellent example. Even though OCaml has some features related to refactoring (such as <a href=\"https://discuss.ocaml.org/t/ann-merlin-and-ocaml-lsp-support-experimental-project-wide-renaming/16008\">renaming occurrences</a>, <a href=\"https://ocaml.github.io/merlin/editor/emacs/#expression-construction\">substituting typed holes</a> with expressions, and <a href=\"https://tarides.com/blog/2024-05-29-effective-ml-through-merlin-s-destruct-command/\">case analysis</a> for pattern matching), the goal of my internship was to kickstart work on a robust set of features to enable the smooth integration of multiple complementary refactoring support commands.</p><p>As part of my Tarides internship (on the editor side), I specified several useful commands, inspired by competitors and materialised in the form of RFCs, subject to discussion. There were multiple candidates, but we found that <em>expression extraction to toplevel</em> was the most suitable for a first experiment. Since it touched on several parts of the protocol and required tools that could be reused for other features, it was important to design the system with extensibility and modularity in mind.</p><p>In this article, I will present the results of this experiment, including the new command and some interesting use cases.</p><p><em>Expression extraction to toplevel</em> will select the most inclusive expression that fits in your selection and propose to extract it. In this case,  means that the selected expression will be moved into its own freshly generated let binding top level.</p><p>Here is a first example: Let's try to extract a constant. Let‚Äôs assume\nthat the float 3.14159 is selected in the following code snippet:</p><pre><code></code></pre><p>The  action code will then be proposed, and if you apply it, the code will look like this:</p><pre><code></code></pre><p>Here is an illustrated example (based on an experimental branch of <a href=\"https://github.com/tarides/ocaml-eglot\">ocaml-eglot</a>):</p><p>We can see that the expression has been effectively extracted and replaced by a reference to the fresh let binding. We can also observe that in the absence of a specified name, the generated binding will be named with a generic name that is not taken in the destination scope. You also have the ability to supply the name you want for extraction.</p><p>For example, here is the same example where the user can enter a name:</p><p>But the refactoring capabilities go much further than constant extraction!</p><p>In our previous example, we could speculate about the purity of the expression, since we were only extracting a literal value. However, OCaml is an impure language, so extracting an expression into a constant can lead to unintended behavior. For example, let's imagine the following snippet:</p><pre><code></code></pre><p>In this example, extracting into a  would cause problems! Indeed, we would be changing the semantics of our program by executing both print statements beforehand. Fortunately, the command analyses the expression as not being a constant and delays its execution using a thunk ‚Äî a function of .</p><p>As we can see, our goal was to maximise the production of valid code, as much as possible, by carefully analysing how to perform the extraction. This is all the more challenging in OCaml, which allows for arbitrary (and potentially infinite) nesting of expressions.</p><h3>Extracting an Expression That Uses Variables</h3><p>The final point we‚Äôll briefly cover is the most fun. Indeed, it‚Äôs possible that the expression we want to extract depends on values defined in the current scope. For example:</p><pre><code></code></pre><p>In this example, the extraction of the expression <code>a + b + c (c * x * y) + z</code> will be placed between  and . As a result,  will still be accessible; however, , , , , and  will be <a href=\"https://en.wikipedia.org/wiki/Free_variables_and_bound_variables\">free variables</a> in the extracted expression. Therefore, we generate a function that takes these free variables as arguments:</p><p>Identifying free variables was one of the motivations for starting with this command. We are fairly certain that this is a function that we will need to reuse in many contexts!  Note that the command behaves correctly in the presence of objects and modules.</p><p>Let‚Äôs try to extract something a little more complicated now. Let‚Äôs assume we have the following code and we want to refactor it, for example, by extracting the  type pretty print logic outside our  function.</p><pre><code></code></pre><p>We can observe that bounded variables in the extracted region are now passed as arguments, and the extracted function is properly replaced by a call to the new show_markup generated function.</p><pre><code></code></pre><p>Here is an example of how it is used. ?</p><p>To understand how this new Merlin command can be properly used in your favourite editor, we have to take a closer look at the functioning of the Language Server Protocol. The LSP supports two mechanisms to extend the existing protocol with new features. First, there is , which allows us to perform multiple LSP commands  sequentially. This kind of request has the merit of working out of the box without requiring any plugin or specific command support on the editor side (which oils the wheels for maintenance). Secondly, there are , which are more powerful than code actions and enable custom interactivity. So, if you want to prompt the user, a custom request is the way to go. The price you have to pay for this power is to have client-side support implemented for each custom request in every editor plugin.</p><p>The current editor team approach is as follows: For each of Merlin's commands that don't map directly to a standard LSP request, we provide a code action associated with the Merlin command and potentially a dedicated custom request if the feature requires custom interactivity. Regarding the ‚Äòextract‚Äô feature, the associated code action does not allow us to choose the name of the generated let binding, but the custom request does.</p><p>I hope this new command helps you get even more productive in OCaml! Don‚Äôt hesitate to experiment with it and report any bugs you encounter.</p><p>The development of Merlin‚Äôs refactoring tools was part of a broader vision to improve OCaml editor support and perhaps claim an editor experience similar to JetBrains IDE in the future!</p><p>The work done on the  command gives us the opportunity to identify various problems pertaining to refactoring (, ) and potentially to make the connection to refactoring commands that already exist in Merlin (like  refactoring and project-wide renaming). The next step is to add a small toolbox library in Merlin dedicated to refactoring in order to develop even more refactor actions. I hope this is just the first refactoring feature of a long series.</p><p>If you're curious and want to take a look at the feature, it's split into several PRs:</p><ul><li><a href=\"https://github.com/ocaml/merlin/pull/1948\">ocaml/merlin#1948</a> which implements the extraction logic on the Merlin side and exposes it in the protocol,</li><li><a href=\"https://github.com/ocaml/ocaml-lsp/pull/1545\">ocaml/ocaml-lsp#1545</a> which exposes the Custom Request enabling the use of the LSP-side functionality,</li><li><a href=\"https://github.com/ocaml/ocaml-lsp/pull/1546\">ocaml/ocaml-lsp#1546</a> which exposes an Action Code that allows the functionality to be invoked without additional formalities on the Editor side,</li><li><a href=\"https://github.com/tarides/ocaml-eglot/pull/65\">tarides/ocaml-eglot#65</a> which implements extraction behaviour in OCaml-Eglot, invocable either from a type enclosing or directly as a classic Emacs command.</li></ul><p>All of these PRs are currently under review, and should be merged soon!</p><p>A big thanks to <a href=\"https://tarides.com/blog/author/xavier-van-de-woestyne/\">Xavier</a>, <a href=\"https://tarides.com/blog/author/ulysse-gerard/\">Ulysse</a>, and all the people that helped me during this internship. It was pretty interesting!</p>","contentLength":7109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44961847"},{"title":"Sequoia backs Zed","url":"https://zed.dev/blog/sequoia-backs-zed","date":1755691996,"author":"vquemener","guid":234657,"unread":true,"content":"<p>Today we're announcing our $32M Series B led by Sequoia Capital with participation from <a href=\"https://zed.dev/about#our-investors\">our existing investors</a>, bringing our total funding to over $42M.</p><p>For the past four years, we've been building the world's fastest IDE, but that's just the foundation for what comes next. Our ultimate vision is a new way to collaborate on software, where conversations about code remain connected to the code itself, instead of being tied to aging snapshots or scattered across different tools. The first step was creating a high-quality editor to serve as the user interface. Now this new investment lets us expand to tackle the next phase of our plan. We're developing a new kind of operation-based version control that incrementally tracks the evolution of your code with edit-level granularity, and we're integrating it into Zed to make collaboration, both with agents and teammates, a first-class part of the coding experience.</p><p>Sequoia is excited about our vision, and we're thrilled to have their help making it a reality. We're actively hiring, so if the future we're building excites you, <a href=\"https://zed.dev/jobs\">we'd love to talk</a>.</p><p>Real-world software is the product of a never-ending stream of conversations: With yourself, your teammates, and now also with generative AI models. Talking about code helps us understand it, both individually and as a team. But with current tooling, these discussions (and all the insights they generate) seem to exist everywhere except the code itself.</p><p>Git lets you collaborate by sharing commits and branches, but between commits you work alone in your own isolated working copy. It's fairly easy to discuss code that's changing in a pull request, but if you want to have a conversation about an arbitrary part of your codebase, you're stuck linking to a particular version of the relevant code in a snapshot, or worse, pasting text into a chat app. As snapshots become stale and messages scroll into the past, your conversations quickly lose their link to the latest version of the code, and all of their valuable context is lost.</p><p>The limitations of snapshots become even more apparent when working with AI agents. While you might manage simple tasks by exchanging comments with an agent on a pull request, real-world development often requires interaction between commits. You need to guide agents, correct their course, and iterate rapidly‚Äîall without the overhead of creating snapshots for every exchange. Our existing tools were built for humans trading commits asynchronously, not for instant back-and-forth with synthetic collaborators. Forcing every AI interaction through the commit-based workflow is like trying to have a conversation through a fax machine.</p><p>Today's AI editors patch over these limitations, but miss the core problem: collaboration is continuous conversation, not discrete commits. You can't snapshot every clarification, every pivot, every back-and-forth that shapes the code. We're building a system that captures this entire dialogue: every edit, every discussion, linked durably to the code as it evolves. This frees collaboration from the rigid structure of commits.</p><p>Our vision is turn your IDE into a collaborative workspace where humans and AI agents work together across a range of time scales, with every insight preserved and linked to the code forever. To make this possible, we're building DeltaDB: a new kind of version control that tracks every operation, not just commits.</p><p>DeltaDB uses <a href=\"https://zed.dev/blog/crdts\">CRDTs</a> to incrementally record and synchronize changes as they happen. It's designed to interoperate with Git, but its operation-based design supports real-time interactions that aren't supported by Git's snapshots. For async interactions, fine-grained change tracking also enables character-level permalinks that survive any code transformation, so we can anchor our interactions to arbitrary locations in the codebase, not just to snapshots of recently-changed code.</p><p>Zed's goal is to make your codebase a living, navigable history of how your software evolved, where discussions with humans and AI agents are durably linked to the code they reference and always up-to-date. It's an evolution beyond version control that incorporates not just the code itself, but also the background information of how and why the code got into a particular state‚Äîcontext that AI agents can query to make more informed edits, understanding the assumptions, constraints, and decisions that shaped the existing code.</p><p>Picture a new engineer facing a production stack trace in Zed. They highlight a problematic line, like an  that caused a crash, and see every related discussion: why the function was written or what an AI agent assumed about an invariant. They ping the responsible human, sparking a quick chat that turns into an audio call, all indexed to the exact code spot, creating a shared, revisitable record without leaving the codebase.</p><p><a href=\"https://github.com/zed-industries/zed\">Zed is open-source</a> with an <a href=\"https://zed.dev/pricing\">optional paid offering</a>, and we plan to do the same with DeltaDB: build it, open-source it, and offer an optional paid service. We'll share more details as development progresses; this is just the beginning of reimagining how developers work together, both with AI agents and their team.</p><p>We have the vision, technical foundation, and funding to fundamentally improve how developers collaborate. Now we just need you. We‚Äôre hiring across engineering and product design; whether you're interested in collaboration in the IDE, core Zed projects like cross-OS font rendering and GPU shaders, or improving the world's best <a href=\"https://zed.dev/edit-prediction\">open-source open-data language model for Edit Prediction</a>, there's room for you here. <a href=\"https://zed.dev/jobs\">Join us</a> to shape the future of software development.</p><div><h3>Looking for a better editor?</h3><p>If you're passionate about the topics we cover on our blog, please consider <a href=\"https://zed.dev/jobs\">joining our team</a> to help us ship the future of software development.</p></div>","contentLength":5808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44961172"},{"title":"Shader Academy: Learn computer graphics by solving challenges","url":"https://shaderacademy.com/","date":1755688095,"author":"pykello","guid":237196,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44960750"},{"title":"Show HN: Project management system for Claude Code","url":"https://github.com/automazeio/ccpm","date":1755685932,"author":"aroussi","guid":234546,"unread":true,"content":"<p>I built a lightweight project management workflow to keep AI-driven development organized.</p><p>The problem was that context kept disappearing between tasks. With multiple Claude agents running in parallel, I‚Äôd lose track of specs, dependencies, and history. External PM tools didn‚Äôt help because syncing them with repos always created friction.</p><p>The solution was to treat GitHub Issues as the database. The \"system\" is ~50 bash scripts and markdown configs that:</p><p>- Brainstorm with you to create a markdown PRD, spins up an epic, and decomposes it into tasks and syncs them with GitHub issues\n- Track progress across parallel streams\n- Keep everything traceable back to the original spec\n- Run fast from the CLI (commands finish in seconds)</p><p>It‚Äôs still early and rough around the edges, but has worked well for us. I‚Äôd love feedback from others experimenting with GitHub-centric project management or AI-driven workflows.</p>","contentLength":918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44960594"},{"title":"Tidewave Web: in-browser coding agent for Rails and Phoenix","url":"https://tidewave.ai/blog/tidewave-web-phoenix-rails","date":1755682984,"author":"kieloo","guid":234517,"unread":true,"content":"\n<p>Article URL: <a href=\"https://tidewave.ai/blog/tidewave-web-phoenix-rails\">https://tidewave.ai/blog/tidewave-web-phoenix-rails</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44960316\">https://news.ycombinator.com/item?id=44960316</a></p>\n<p>Points: 286</p>\n<p># Comments: 55</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mirrorshades: The Cyberpunk Anthology (1986)","url":"https://www.rudyrucker.com/mirrorshades/HTML/","date":1755677700,"author":"keepamovin","guid":234656,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.rudyrucker.com/mirrorshades/HTML/\">https://www.rudyrucker.com/mirrorshades/HTML/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44959833\">https://news.ycombinator.com/item?id=44959833</a></p>\n<p>Points: 145</p>\n<p># Comments: 84</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Databricks is raising a Series K Investment at >$100B valuation","url":"https://www.databricks.com/company/newsroom/press-releases/databricks-raising-series-k-investment-100-billion-valuation","date":1755669975,"author":"djhu9","guid":234577,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.databricks.com/company/newsroom/press-releases/databricks-raising-series-k-investment-100-billion-valuation\">https://www.databricks.com/company/newsroom/press-releases/databricks-raising-series-k-investment-100-billion-valuation</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44959092\">https://news.ycombinator.com/item?id=44959092</a></p>\n<p>Points: 125</p>\n<p># Comments: 156</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask HN: Why does the US Visa application website do a port-scan of my network?","url":"https://news.ycombinator.com/item?id=44959073","date":1755669783,"author":"mbix77","guid":234406,"unread":true,"content":"\n<p>I have recently installed this extension on FF: <a href=\"https://addons.mozilla.org/en-US/firefox/addon/port-authority/\" rel=\"nofollow\">https://addons.mozilla.org/en-US/firefox/addon/port-authorit...</a> and yesterday I visited this website: <a href=\"https://ceac.state.gov/genniv/\" rel=\"nofollow\">https://ceac.state.gov/genniv/</a> and I got a notification that the website tried to do a port-scan of my private network.<p>Is this a common thing? I have just recently installed the extension, so I am not sure if there are a lot of other websites who do it.<p>Since looking into it, I noticed that uBlock Origin already has the default list \"Block Outsider Intrusion into LAN\" but it wasn't enabled.</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44959073\">https://news.ycombinator.com/item?id=44959073</a></p>\n<p>Points: 451</p>\n<p># Comments: 199</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of the GFW's Unconditional Port 443 Block on August 20, 2025","url":"https://gfw.report/blog/gfw_unconditional_rst_20250820/en/","date":1755664049,"author":"kotri","guid":234393,"unread":true,"content":"\n<p>Article URL: <a href=\"https://gfw.report/blog/gfw_unconditional_rst_20250820/en/\">https://gfw.report/blog/gfw_unconditional_rst_20250820/en/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44958621\">https://news.ycombinator.com/item?id=44958621</a></p>\n<p>Points: 165</p>\n<p># Comments: 123</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modern CI is too complex and misdirected (2021)","url":"https://gregoryszorc.com/blog/2021/04/07/modern-ci-is-too-complex-and-misdirected/","date":1755660606,"author":"thundergolfer","guid":234576,"unread":true,"content":"\n<p>Article URL: <a href=\"https://gregoryszorc.com/blog/2021/04/07/modern-ci-is-too-complex-and-misdirected/\">https://gregoryszorc.com/blog/2021/04/07/modern-ci-is-too-complex-and-misdirected/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44958400\">https://news.ycombinator.com/item?id=44958400</a></p>\n<p>Points: 175</p>\n<p># Comments: 164</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The value of hitting the HN front page","url":"https://www.mooreds.com/wordpress/archives/3530","date":1755655211,"author":"mooreds","guid":234516,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.mooreds.com/wordpress/archives/3530\">https://www.mooreds.com/wordpress/archives/3530</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44958020\">https://news.ycombinator.com/item?id=44958020</a></p>\n<p>Points: 163</p>\n<p># Comments: 73</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Copilot broke audit logs, but Microsoft won't tell customers","url":"https://pistachioapp.com/blog/copilot-broke-your-audit-log","date":1755649080,"author":"Sayrus","guid":233555,"unread":true,"content":"\n<p>Article URL: <a href=\"https://pistachioapp.com/blog/copilot-broke-your-audit-log\">https://pistachioapp.com/blog/copilot-broke-your-audit-log</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44957454\">https://news.ycombinator.com/item?id=44957454</a></p>\n<p>Points: 652</p>\n<p># Comments: 247</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AGENTS.md ‚Äì Open format for guiding coding agents","url":"https://agents.md/","date":1755648903,"author":"ghuntley","guid":234392,"unread":true,"content":"\n<p>Article URL: <a href=\"https://agents.md/\">https://agents.md/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44957443\">https://news.ycombinator.com/item?id=44957443</a></p>\n<p>Points: 759</p>\n<p># Comments: 357</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tiny microbe challenges the definition of cellular life","url":"https://nautil.us/a-rogue-new-life-form-1232095/","date":1755645528,"author":"jnord","guid":234391,"unread":true,"content":"\n<p>Article URL: <a href=\"https://nautil.us/a-rogue-new-life-form-1232095/\">https://nautil.us/a-rogue-new-life-form-1232095/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44957157\">https://news.ycombinator.com/item?id=44957157</a></p>\n<p>Points: 127</p>\n<p># Comments: 48</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Draw a Space Invader","url":"https://muffinman.io/blog/invaders/","date":1755643276,"author":"abdusco","guid":233554,"unread":true,"content":"\n<p>Article URL: <a href=\"https://muffinman.io/blog/invaders/\">https://muffinman.io/blog/invaders/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44956915\">https://news.ycombinator.com/item?id=44956915</a></p>\n<p>Points: 472</p>\n<p># Comments: 51</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The forgotten meaning of \"jerk\"","url":"https://languagehat.com/the-forgotten-meaning-of-jerk/","date":1755641489,"author":"aspenmayer","guid":233553,"unread":true,"content":"\n<p>Article URL: <a href=\"https://languagehat.com/the-forgotten-meaning-of-jerk/\">https://languagehat.com/the-forgotten-meaning-of-jerk/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44956730\">https://news.ycombinator.com/item?id=44956730</a></p>\n<p>Points: 85</p>\n<p># Comments: 80</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From M1 MacBook to Arch Linux: A month-long experiment that became permanenent","url":"https://www.ssp.sh/blog/macbook-to-arch-linux-omarchy/","date":1755635106,"author":"articsputnik","guid":237238,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.ssp.sh/blog/macbook-to-arch-linux-omarchy/\">https://www.ssp.sh/blog/macbook-to-arch-linux-omarchy/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44955923\">https://news.ycombinator.com/item?id=44955923</a></p>\n<p>Points: 94</p>\n<p># Comments: 136</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perfect Freehand ‚Äì Draw perfect pressure-sensitive freehand lines","url":"https://www.perfectfreehand.com/","date":1755633231,"author":"NikxDa","guid":233538,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.perfectfreehand.com/\">https://www.perfectfreehand.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44955624\">https://news.ycombinator.com/item?id=44955624</a></p>\n<p>Points: 102</p>\n<p># Comments: 9</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: OpenAI/reflect ‚Äì Physical AI Assistant that illuminates your life","url":"https://github.com/openai/openai-reflect","date":1755632918,"author":"Sean-Der","guid":233629,"unread":true,"content":"<p>I have been working on making WebRTC + Embedded Devices easier for a few years. This is a hackathon project that pulled some of that together. I hope others build on it/it inspires them to play with hardware. I worked on it with two other people and I had a lot of fun with some of the ideas that came out of it.</p><p>* Extendable/hackable - I tried to keep the code as simple as possible so others can fork/modify easily.</p><p>* Communicate with light. With function calling it changes the light bulb, so it can match your mood or feelings.</p><p>* Populate info from clients you control. I wanted to experiment with having it guide you through yesterday/today.</p><p>* Phone as control. Setting up new devices can be frustrating. I liked that this didn't require any WiFi setup, it just routed everything through your phone. Also cool then that they device doesn't actually have any sensitive data on it.</p>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44955576"},{"title":"CRDT: Text Buffer","url":"https://madebyevan.com/algos/crdt-text-buffer/","date":1755632325,"author":"skadamat","guid":234390,"unread":true,"content":"\n<p>Article URL: <a href=\"https://madebyevan.com/algos/crdt-text-buffer/\">https://madebyevan.com/algos/crdt-text-buffer/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44955459\">https://news.ycombinator.com/item?id=44955459</a></p>\n<p>Points: 105</p>\n<p># Comments: 4</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vendors that treat single sign-on as a luxury feature","url":"https://sso.tax/","date":1755632318,"author":"vinnyglennon","guid":233497,"unread":true,"content":"\n<p>Article URL: <a href=\"https://sso.tax/\">https://sso.tax/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44955457\">https://news.ycombinator.com/item?id=44955457</a></p>\n<p>Points: 214</p>\n<p># Comments: 150</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AnduinOS","url":"https://www.anduinos.com/","date":1755628945,"author":"TheFreim","guid":234389,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.anduinos.com/\">https://www.anduinos.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44954823\">https://news.ycombinator.com/item?id=44954823</a></p>\n<p>Points: 115</p>\n<p># Comments: 136</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Notion releases offline mode","url":"https://www.notion.com/help/guides/working-offline-in-notion-everything-you-need-to-know","date":1755628054,"author":"ericzawo","guid":233496,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.notion.com/help/guides/working-offline-in-notion-everything-you-need-to-know\">https://www.notion.com/help/guides/working-offline-in-notion-everything-you-need-to-know</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44954665\">https://news.ycombinator.com/item?id=44954665</a></p>\n<p>Points: 174</p>\n<p># Comments: 123</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"D2 (text to diagram tool) now supports ASCII renders","url":"https://d2lang.com/blog/ascii/","date":1755627281,"author":"alixanderwang","guid":233495,"unread":true,"content":"\n<p>Article URL: <a href=\"https://d2lang.com/blog/ascii/\">https://d2lang.com/blog/ascii/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44954524\">https://news.ycombinator.com/item?id=44954524</a></p>\n<p>Points: 387</p>\n<p># Comments: 66</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Semantic Layers Matter (and how to build one with DuckDB)","url":"https://motherduck.com/blog/semantic-layer-duckdb-tutorial/","date":1755622155,"author":"secondrow","guid":233537,"unread":true,"content":"\n<p>Article URL: <a href=\"https://motherduck.com/blog/semantic-layer-duckdb-tutorial/\">https://motherduck.com/blog/semantic-layer-duckdb-tutorial/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44953575\">https://news.ycombinator.com/item?id=44953575</a></p>\n<p>Points: 123</p>\n<p># Comments: 23</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Emacs as your video-trimming tool","url":"https://xenodium.com/emacs-as-your-video-trimming-tool","date":1755620521,"author":"xenodium","guid":233461,"unread":true,"content":"\n<p>Article URL: <a href=\"https://xenodium.com/emacs-as-your-video-trimming-tool\">https://xenodium.com/emacs-as-your-video-trimming-tool</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44953316\">https://news.ycombinator.com/item?id=44953316</a></p>\n<p>Points: 264</p>\n<p># Comments: 135</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How we exploited CodeRabbit: From simple PR to RCE and write access on 1M repos","url":"https://research.kudelskisecurity.com/2025/08/19/how-we-exploited-coderabbit-from-a-simple-pr-to-rce-and-write-access-on-1m-repositories/","date":1755618915,"author":"spiridow","guid":233436,"unread":true,"content":"\n<p>Article URL: <a href=\"https://research.kudelskisecurity.com/2025/08/19/how-we-exploited-coderabbit-from-a-simple-pr-to-rce-and-write-access-on-1m-repositories/\">https://research.kudelskisecurity.com/2025/08/19/how-we-exploited-coderabbit-from-a-simple-pr-to-rce-and-write-access-on-1m-repositories/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44953032\">https://news.ycombinator.com/item?id=44953032</a></p>\n<p>Points: 640</p>\n<p># Comments: 214</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Remove mentions of XSLT from the html spec\"","url":"https://github.com/whatwg/html/pull/11563","date":1755614934,"author":"troupo","guid":233379,"unread":true,"content":"\n<p>Related: <i>Should we remove XSLT from the web platform?</i> - <a href=\"https://news.ycombinator.com/item?id=44909599\">https://news.ycombinator.com/item?id=44909599</a></p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44952185\">https://news.ycombinator.com/item?id=44952185</a></p>\n<p>Points: 259</p>\n<p># Comments: 308</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Positron, a New Data Science IDE","url":"https://posit.co/blog/positron-product-announcement-aug-2025/","date":1755613225,"author":"kgwgk","guid":233494,"unread":true,"content":"\n<p>Article URL: <a href=\"https://posit.co/blog/positron-product-announcement-aug-2025/\">https://posit.co/blog/positron-product-announcement-aug-2025/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44951862\">https://news.ycombinator.com/item?id=44951862</a></p>\n<p>Points: 130</p>\n<p># Comments: 42</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why I'm all-in on Zen Browser","url":"https://werd.io/why-im-all-in-on-zen-browser/","date":1755612884,"author":"benwerd","guid":233460,"unread":true,"content":"\n<p>Article URL: <a href=\"https://werd.io/why-im-all-in-on-zen-browser/\">https://werd.io/why-im-all-in-on-zen-browser/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44951799\">https://news.ycombinator.com/item?id=44951799</a></p>\n<p>Points: 78</p>\n<p># Comments: 81</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Without the futex, it's futile","url":"https://h4x0r.org/futex/","date":1755611622,"author":"eatonphil","guid":233378,"unread":true,"content":"\n<p>Article URL: <a href=\"https://h4x0r.org/futex/\">https://h4x0r.org/futex/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44951563\">https://news.ycombinator.com/item?id=44951563</a></p>\n<p>Points: 281</p>\n<p># Comments: 136</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Critical Cache Poisoning Vulnerability in Dnsmasq","url":"https://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2025q3/018288.html","date":1755608207,"author":"westurner","guid":233435,"unread":true,"content":"\n<p>Article URL: <a href=\"https://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2025q3/018288.html\">https://lists.thekelleys.org.uk/pipermail/dnsmasq-discuss/2025q3/018288.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44950981\">https://news.ycombinator.com/item?id=44950981</a></p>\n<p>Points: 114</p>\n<p># Comments: 86</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Launch HN: Uplift (YC S25) ‚Äì Voice models for under-served languages","url":"https://news.ycombinator.com/item?id=44950661","date":1755605409,"author":"zaidqureshi","guid":233493,"unread":true,"content":"\n<p>Hi HN, we are Zaid, Muhammad and Hammad, the co-founders of Uplift AI (<a href=\"https://upliftai.org\">https://upliftai.org</a>). We build models that speak underserved languages ‚Äî today: Urdu, Sindhi, and Balochi.<p>A billion people worldwide can't read. In countries like Pakistan ‚Äì the 5th most populous country ‚Äì 42% of adults are illiterate. This holds back the entire economy: patients can't read medical reports, parents can't help with homework, banks can't go fully digital, farmers can't research best practices, and people memorize smartphone app button sequences. Voice AI interfaces can fix all of this, and we think this will perhaps be one of the great benefits of modern AI.<p>Right now, existing voice models barely work for these languages, and big tech is moving slowly.<p>Uplift AI was originally a side project to make datasets for translation and voice models. For us it was a \"cool side-thing\" to work on, not an \"important full-time thing\" to work on. With some initial data we hacked together a Urdu Voice Bot on Whatsapp and gave it to one domestic worker. In two days 800 people were using it. When we dived deeper into understanding the users, we learned that text interfaces don't work for sooo many. So we started Uplift AI to solve this problem fulltime.<p>The most challenging part is that all the building blocks needed for great voice models are broken for these languages. For example, if you are creating a speech synthesis model, you will scrape a lot of data from youtube and auto-label it using a transcription model‚Ä¶ all very easy to do in English. But it doesn't work in under-served languages because the transcription modes are not accurate.<p>There are many other challenges. Like when you hire human transcribers to label the data, often they don't have any spell correctors for their languages, and this creates lots of noise in the data‚Ä¶ making it hard to train models with low data.  There are many more challenges in phonemes, silence detection, diacritization etc.<p>We solve these problems by making great internal tooling to help with data labeling. Also, we source our own data and don't buy it. This is counterintuitive, but a big advantage over companies buying data and then training. By sourcing our own data we create the right data distributions and get much better models with much less data.  By doing the entire thing inhouse, (data, labeling, training, deploying) we are able to make a lot faster progress.<p>Today we publicly offer a text to speech APIs for Urdu, Sindhi, and Balochi. Here's a video which shows this: <a href=\"https://www.loom.com/share/dcd5020967444c228e9c127151e7a9f5\" rel=\"nofollow\">https://www.loom.com/share/dcd5020967444c228e9c127151e7a9f5</a>.<p>Khan Academy is using our tech to dub videos to Urdu (<a href=\"https://ur.khanacademy.org\" rel=\"nofollow\">https://ur.khanacademy.org</a>).<p>Our models excel at informational use cases (like AI bots) but need more work in emotive use-cases like poetry.<p>We have been giving a lot of people private access in beta mode, and today are launching our models publicly. We believe this will be the fastest way for us to learn about areas that are not performing well so we can fix them quickly.<p>We'd love to hear from all of you, especially around your experiences with under-served languages (not just the Pakistani ones we're starting with) and your comments in general.</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44950661\">https://news.ycombinator.com/item?id=44950661</a></p>\n<p>Points: 88</p>\n<p># Comments: 40</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UK drops demand for backdoor into Apple encryption","url":"https://www.theverge.com/news/761240/uk-apple-us-encryption-back-door-demands-dropped","date":1755604714,"author":"iamdamian","guid":233334,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.theverge.com/news/761240/uk-apple-us-encryption-back-door-demands-dropped\">https://www.theverge.com/news/761240/uk-apple-us-encryption-back-door-demands-dropped</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44950600\">https://news.ycombinator.com/item?id=44950600</a></p>\n<p>Points: 153</p>\n<p># Comments: 35</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I run a full Linux desktop in Docker just because I can","url":"https://www.howtogeek.com/i-run-a-full-linux-desktop-in-docker-just-because-i-can/","date":1755603587,"author":"redbell","guid":237195,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.howtogeek.com/i-run-a-full-linux-desktop-in-docker-just-because-i-can/\">https://www.howtogeek.com/i-run-a-full-linux-desktop-in-docker-just-because-i-can/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44950482\">https://news.ycombinator.com/item?id=44950482</a></p>\n<p>Points: 110</p>\n<p># Comments: 57</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PyPI Preventing Domain Resurrection Attacks","url":"https://blog.pypi.org/posts/2025-08-18-preventing-domain-resurrections/","date":1755599561,"author":"pabs3","guid":233434,"unread":true,"content":"\n<p>Article URL: <a href=\"https://blog.pypi.org/posts/2025-08-18-preventing-domain-resurrections/\">https://blog.pypi.org/posts/2025-08-18-preventing-domain-resurrections/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44950091\">https://news.ycombinator.com/item?id=44950091</a></p>\n<p>Points: 105</p>\n<p># Comments: 43</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Custom telescope mount using harmonic drives and ESP32","url":"https://www.svendewaerhert.com/blog/telescope-mount/","date":1755596787,"author":"waerhert","guid":233308,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.svendewaerhert.com/blog/telescope-mount/\">https://www.svendewaerhert.com/blog/telescope-mount/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44949895\">https://news.ycombinator.com/item?id=44949895</a></p>\n<p>Points: 281</p>\n<p># Comments: 104</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prime Number Grid","url":"https://susam.net/primegrid.html","date":1755588789,"author":"todsacerdoti","guid":233216,"unread":true,"content":"\n<p>Article URL: <a href=\"https://susam.net/primegrid.html\">https://susam.net/primegrid.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44949162\">https://news.ycombinator.com/item?id=44949162</a></p>\n<p>Points: 270</p>\n<p># Comments: 91</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Build a Medieval Castle","url":"https://archaeology.org/issues/september-october-2025/features/how-to-build-a-medieval-castle/","date":1755578975,"author":"benbreen","guid":233307,"unread":true,"content":"\n<p>Article URL: <a href=\"https://archaeology.org/issues/september-october-2025/features/how-to-build-a-medieval-castle/\">https://archaeology.org/issues/september-october-2025/features/how-to-build-a-medieval-castle/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44948352\">https://news.ycombinator.com/item?id=44948352</a></p>\n<p>Points: 213</p>\n<p># Comments: 65</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenMower ‚Äì An open source lawn mower","url":"https://github.com/ClemensElflein/OpenMower","date":1755563708,"author":"rickcarlino","guid":233151,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/ClemensElflein/OpenMower\">https://github.com/ClemensElflein/OpenMower</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44946996\">https://news.ycombinator.com/item?id=44946996</a></p>\n<p>Points: 518</p>\n<p># Comments: 165</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ted Chiang: The Secret Third Thing","url":"https://linch.substack.com/p/ted-chiang-review","date":1755561959,"author":"pseudolus","guid":233215,"unread":true,"content":"\n<p>Article URL: <a href=\"https://linch.substack.com/p/ted-chiang-review\">https://linch.substack.com/p/ted-chiang-review</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44946774\">https://news.ycombinator.com/item?id=44946774</a></p>\n<p>Points: 255</p>\n<p># Comments: 115</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Croatian freediver held breath for 29 minutes","url":"https://divernet.com/scuba-news/freediving/how-croatian-freediver-held-breath-for-29-minutes/","date":1755561851,"author":"toomanyrichies","guid":231847,"unread":true,"content":"\n<p>Article URL: <a href=\"https://divernet.com/scuba-news/freediving/how-croatian-freediver-held-breath-for-29-minutes/\">https://divernet.com/scuba-news/freediving/how-croatian-freediver-held-breath-for-29-minutes/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44946762\">https://news.ycombinator.com/item?id=44946762</a></p>\n<p>Points: 252</p>\n<p># Comments: 118</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What could have been","url":"https://coppolaemilio.com/entries/what-could-have-been/","date":1755556184,"author":"coppolaemilio","guid":231821,"unread":true,"content":"\n<p>Article URL: <a href=\"https://coppolaemilio.com/entries/what-could-have-been/\">https://coppolaemilio.com/entries/what-could-have-been/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44945966\">https://news.ycombinator.com/item?id=44945966</a></p>\n<p>Points: 110</p>\n<p># Comments: 90</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lab-grown salmon hits the menu","url":"https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/","date":1755556154,"author":"bookmtn","guid":233150,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/\">https://www.smithsonianmag.com/smart-news/lab-grown-salmon-hits-the-menu-at-an-oregon-restaurant-as-the-fda-greenlights-the-cell-cultured-product-180986769/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44945959\">https://news.ycombinator.com/item?id=44945959</a></p>\n<p>Points: 111</p>\n<p># Comments: 183</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shamelessness as a strategy (2019)","url":"https://nadia.xyz/shameless","date":1755556021,"author":"wdaher","guid":231820,"unread":true,"content":"\n<p>Article URL: <a href=\"https://nadia.xyz/shameless\">https://nadia.xyz/shameless</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44945943\">https://news.ycombinator.com/item?id=44945943</a></p>\n<p>Points: 181</p>\n<p># Comments: 102</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Newgrounds: Flash Forward 2025","url":"https://www.newgrounds.com/bbs/topic/1542140","date":1755554053,"author":"lsferreira42","guid":233214,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.newgrounds.com/bbs/topic/1542140\">https://www.newgrounds.com/bbs/topic/1542140</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44945730\">https://news.ycombinator.com/item?id=44945730</a></p>\n<p>Points: 78</p>\n<p># Comments: 23</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Obsidian Bases","url":"https://help.obsidian.md/bases","date":1755552485,"author":"twapi","guid":231792,"unread":true,"content":"\n<p>Article URL: <a href=\"https://help.obsidian.md/bases\">https://help.obsidian.md/bases</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44945532\">https://news.ycombinator.com/item?id=44945532</a></p>\n<p>Points: 620</p>\n<p># Comments: 202</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Fractional jobs ‚Äì part-time roles for engineers","url":"https://www.fractionaljobs.io/","date":1755551439,"author":"tbird24","guid":231819,"unread":true,"content":"<div>Fractional work is part-time work, typically paid on a monthly retainer, by experts in their field. In this post we‚Äôll break down exactly what fractional work is in more detail, and why it‚Äôs exploding in popularity.</div>","contentLength":219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44945379"},{"title":"Show HN: Strix - Open-source AI hackers for your apps","url":"https://github.com/usestrix/strix","date":1755549793,"author":"ahmedallam2","guid":231793,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44945113"},{"title":"Tiny-tpu: A minimal tensor processing unit (TPU), inspired by Google's TPU","url":"https://github.com/tiny-tpu-v2/tiny-tpu","date":1755549277,"author":"admp","guid":231818,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/tiny-tpu-v2/tiny-tpu\">https://github.com/tiny-tpu-v2/tiny-tpu</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44945008\">https://news.ycombinator.com/item?id=44945008</a></p>\n<p>Points: 254</p>\n<p># Comments: 13</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GenAI FOMO has spurred businesses to light nearly $40B on fire","url":"https://www.theregister.com/2025/08/18/generative_ai_zero_return_95_percent/","date":1755546893,"author":"rntn","guid":231791,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.theregister.com/2025/08/18/generative_ai_zero_return_95_percent/\">https://www.theregister.com/2025/08/18/generative_ai_zero_return_95_percent/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44944620\">https://news.ycombinator.com/item?id=44944620</a></p>\n<p>Points: 183</p>\n<p># Comments: 89</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: I built a toy TPU that can do inference and training on the XOR problem","url":"https://www.tinytpu.com/","date":1755546734,"author":"evxxan","guid":233213,"unread":true,"content":"<p>When we started this project, all we knew was that the equation y = mx + b is the foundational building block for neural networks. However, we needed to fully UNDERSTAND the math behind neural networks to build other modules in our TPU. So before we started writing any code, each of us worked out the math of a simple 2 -&gt; 2 -&gt; 1 multi-layer perceptron (MLP).</p><p>The reason we chose this specific network is because we were targeting inference and training for the XOR problem (the \"hello world\" of neural networks). The XOR problem is one of the simplest problems a neural network can solve. All other gates (AND, OR, etc) can predict the outputs from its inputs using just one linear line (one neuron) to separate which inputs correspond to a 0 and which ones correspond to a 1. But to classify all XOR, an MLP is needed, since it requires curved decision boundaries, which can't be achieved with ONLY linear equations. For a geometric and first-principles treatment, the free book<a href=\"https://udlbook.github.io/udlbook/\" target=\"_blank\" rel=\"noopener noreferrer\">Understanding Deep Learning</a>is excellent.</p><p>Now, say we want to do continuous inference (i.e. self driving car making multiple predictions a second). That would imply that we're sending multiple pieces of data at once. Since data is inherently multidimensional and has many features, we would have matrices with very large dimensions. However, the XOR problem simplifies the dimensions for us, as there are only two features (0 or 1) and 4 possible pieces of input data (four possible binary combinations of 0 and 1). This gives us a 4x2 matrix, where 4 is the number of rows (batch size) and 2 is the number of columns (feature size).</p><div><p>The XOR input matrix and target outputs:</p><p>Each row represents one of the four possible XOR inputs, and the output vector shows the expected XOR results</p></div><p>Another simplification we're making for our systolic array example here is that we'll use a 2x2 instead of the 256x256 array used in the TPUv1. However, the math is still faithful so nothing is actually dumbed down, rather scaled down instead.</p><p>The first step in the equation is multiplying m with x, which, in matrix form, would be.</p><div><p>whereis our input matrix,is our weight matrix, andis our bias vector</p></div><p>How can we perform matrix multiplication in hardware? Well, we can use a unit called the systolic array!</p><p>The heart of a TPU is a unit called the systolic array.It consists of individual building blocks called Processing Elements (PE) which are connected together in a grid-like structure. Each PE performs a multiply-accumulate operation, meaning it multiplies an incoming input X with a stationary weight Wand adds it to an incoming accumulated sum, all in the same clock cycle.</p><div><pre><code> @ or ;\n            ;\n            ;\n        ;\n        ;\n            ;\n        </code></pre></div><h3>Systolic matrix multiplication</h3><p>When these PEs are connected together, they can be used to perform matrix multiplication systolically, meaning multiple elements of the output matrix can be calculated every clock cycle. The inputs enter the systolic array from the left and move to the neighbouring PE to the right, every clock cycle. The accumulated sums start with the multiplication output from the first row of PEs, move downwards, and get added to the products of each successive PE, until they up at the last row of PEs where they become an element of the output matrix.</p><p>Because of this single unit (and the fact that matrix multiplications dominate the computations performed in models), TPUs can very easily inference and train any model.</p><p>Now let's walk through the example of our XOR problem:</p><p>Our systolic array takes two inputs: the input matrix and the weight matrix. For our XOR network, we initialize with the following weights and biases:</p><h3>Input and weight scheduling</h3><p>To input our input batch within the systolic array, we need to:</p><p>To input our weight matrix: we need to:</p><p>Note that the rotating and staggering don't have any mathematical significance ‚Äî they are simply required to make the systolic array work. The transpoing too is just for mathematical bookkeeping ‚Äì it's required to make the matrix math work because of how we set up our weight pointers within the neural network drawing.</p><p>To perform the staggering, we designed near-identical accumulators for the weights and inputs that would sit above and to the left of the systolic array, respectively.</p><p>Since the activations are fed into the systolic array one-by-one, we thought a first-in-first-out queue (FIFO) would be the optimal data storage option. There was a slight difference between a traditional FIFO and the accumulators we built, however. Our accumulators had 2 input ports ‚Äî one for writing weights manually to the FIFO and one for writing the previous layer's outputs from the activation modules BACK into the input FIFOs (the previous layer's outputs are inputs for the current layer).</p><p>We also needed to load the weights in a similar fashion for every layer, so we replicated the logic for the weight FIFOs, without the second port.</p><p>The next step in the equation is adding the bias. To do this in hardware, we need to create a bias module under each column of the systolic array. We can see that as the sums move out of the last row within the systolic array, we can immediately stream them into our bias modules to compute our pre-activations.<b> We will denote these values with the variable Z.</b></p><div><p>The bias vector  is broadcast across all rows of the matrix ‚Äî meaning it's added to each row of </p></div><p>Now our equation is starting to look a lot like what we've learned in high school ‚Äìbut just in multidimensional form, where each column that streams out of the systolic array represents its own feature!</p><p>Next we have to apply the activation, for which we chose Leaky ReLU.This is also an element-wise operation, similar to the bias, meaning we need an activation module under every bias module (and by proxy under every column of the systolic array) and we can stream the outputs of our bias modules into the activation modules immediately.<b>We will denote these post-activation values with H</b>.</p><div><p>The Leaky ReLU function applies element-wise:</p><p>where  is our leak factor. For matrices, this applies to each element independently.</p></div><div><p>For our XOR example, let's see how Layer 1 processes the data. First, the systolic array computes:</p><p>Finally, LeakyReLU is applied element-wise:</p><p>Negative values are multiplied by 0.5, positive values pass through unchanged.</p></div><p>Now you might be asking ‚Äì why don't we merge the bias term and the activation term in one clock cycle? Well, this is because of something called pipelining! Pipelining allows multiple operations to be executed simultaneously across different stages of the TPU ‚Äîinstead of waiting for one complete operation to finish before starting the next, you break the work into stages that can overlap. Think of it like an assembly line: while one worker (activation module) processes a part, the previous worker (bias module) is already working on the next part. This keeps all of the modules busy rather than having them sit idle waiting for the previous stage to complete. It also affects the speed at which we can run our TPU ‚Äî if we have one module that tries to squeeze many operations in a single cycle, our clock speed will be bottlenecked by that module, as the other modules can only run as fast as that single module. Therefore, it's efficient and best practice to split up operations into individual clock cycles as much as possible.</p><p>Another mechanism we used to run our chip as efficiently as possible, was a propagating \"start\" signal, which we called a travelling chip enable (denoted by the purple dot). Because everything in our design was staggered, we realized that we could very elegantly assert a start signal for a single clock cycle at the first accumulator and have it propagate to neighbouring modules exactly when they needed to be turned on.</p><p>This would extend into the systolic array and eventually the bias and activation modules, where neighbouring PEs and modules, moving from the top left to the bottom right, were turned on in consecutive clock cycles. This ensured that every module was only performing computations when it was required to and wasn't wasting power in the background.</p><p>Now, we know that starting a new layer means we must compute the same  using a new weight matrix. How can we do this if our systolic array is weight-stationary? How can we change the weights?</p><p>While thinking about this problem, we came across the idea of double buffering, which originates from video games. The reason why double buffering exists is to prevent something called screen tearing on your monitor. Ultimately, pixels take time to load and we'd like to \"hide away\" that time somehow. And if you paid attention, this is the exact same problem we're currently facing with the systolic array. Fortunately, video game designers have already come up with a solution for this problem. By adding a second \"shadow\" buffer, which holds the weights of the next layer while the current layer is being computed on, we can load in new weights during computation, cutting the total clock cycle count in half.</p><p>To make this work, we also needed to add some signals to move the data. First, we needed a signal to indicate when to switch the weights in the shadow buffer and the active buffer. We called this signal the \"switch\" signal (denoted by the blue dot) and it copied the values in the shadow buffer to the active buffer. It propagated from the top left of the systolic array to the bottom right (the same path as the travelling chip enable, but only within the systolic array). We then needed one more signal to indicate when we wanted to move the weights down by one row and we called this the \"accept\" flag (denoted by the green dot) because each row is ACCEPTING a new set of weights. This would move the new weights into the top row of the systolic array, as well as each row of weights down into the next row of the systolic array. These two control flags worked in tandem to make our double buffering mechanism work.</p><p>If you haven't already noticed, this allows the systolic array to do something powerful‚Ä¶continuous inference!!! We can continuously stream in new weights and inputs and compute forward pass for as many layers as we want. This touches into a core design philosophy of the systolic array: we want to maximize PE usage.<b>We always want to keep the systolic array fed!</b></p><div><p>For Layer 2, the outputs from Layer 1 () now become our inputs:</p><p>Adding bias and applying activation:</p><p>All values are positive, so they pass through unchanged. These are our final predictions for the XOR problem!</p></div><p>Our final step for inference was making a control unit to use a custom instruction set (ISA) to assert all of our control flags and load data through a data bus. Including the data bus, our ISA was 24 bits long and it made our testbench more elegant as we could pass a single string of bits every clock cycle, rather than individually setting multiple flags.</p><p>We then put everything together and got inference completely working! This was a big milestone for us and we were very proud about what we had accomplished.</p><h2>Backpropagation and training</h2><div><p>Ok we've solved inference ‚Äî but what about training? Well here's the beauty: We can use the same architecture we use for inference for training! Why? Because training is just matrix multiplications with a few extra steps.</p><p>Here's where things get really exciting. Let's say we just ran inference on the XOR problem and got a prediction that looks something like [0.8, 0.3, 0.1, 0.9] when we actually wanted [1, 0, 0, 1]. Our model is performing poorly! We need to make it better. This is where training comes in. We're going to use something called a loss function to tell our model exactly how poorly it's doing. For simplicity, we chose Mean Squared Error (MSE) ‚Äî think of it like measuring the \"distance\" between what we predicted and what we actually wanted, just like how you might measure how far off target your basketball shot was.<b>Let's denote the loss with L.</b></p><div><p>where  is the target output, is our prediction, and is the number of samples</p></div><div><p>For our XOR example, with predictionsand targets :</p><p>This loss value tells us how far off our predictions are from the true XOR outputs.</p></div><p>So right after we finish computing our final layer's activations (let's call them), we immediately stream them into a loss module to calculate just how bad our predictions are. These loss modules sit right below our activation modules, and we only use them when we've reached our final layer. But here's the key insight: you don't actually need to calculate the loss value itself to train. You just need its derivative. Why? Because that derivative tells us which direction to adjust our weights to make the loss smaller. It's like having a compass that points toward \"better performance.\"</p><h3>The magic of the chain rule</h3><p>This is where calculus enters the picture. To make our model better, we need to figure out how changing each weight affects our loss. The chain rule lets us break this massive calculation into smaller, manageable pieces.</p><div><p>The chain rule for gradients:</p><p>This allows us to compute gradients layer by layer, propagating them backwards through the network</p></div><p>Let's naively trace through what happens step by step.</p><ol><li>Calculate- how much the loss changes with respect to our final activations.</li><li>Computeby taking the derivative of the activation (leaky ReLU in our case).</li><li>Compute,</li><li>Compute</li><li>Rinse and repeat for the n-1 layer.</li></ol></div><div><p>Propagating gradients to the hidden layer:</p><p>And through the first layer's activation:</p><p>With mixed positive and negative values in, the gradient is:</p></div><p>Once we have all of these individual derivatives, we can multiply them together to find any derivative with respect of the loss (i.e.gives us).</p><p>After that, we have to compute the activation derivative, for which the formula is. This is also an element-wise computation, meaning we can structure it exactly like the loss module (and bias and activation modules), but it will perform a different calculation. One important note about this module, however, is that it requires the activations we computed during forward pass.</p><p>Now you might be wondering ‚Äî how do we actually compute derivatives in hardware? Let's look at Leaky ReLU as an example, since it's beautifully simple but demonstrates the key principles. Remember that Leaky ReLU applies different operations based on whether the input is positive or negative. The derivative follows the same pattern: it outputs 1 for positive inputs and a small constant (we used 0.01) for negative inputs.</p><pre><code> @;\n    .01 ;\n    </code></pre><p>What's beautiful about this is that it's just a simple comparison ‚Äì no complex arithmetic needed. The hardware can compute this derivative in a single clock cycle, keeping our pipeline flowing smoothly. This same principle applies to other activation functions: their derivatives often simplify to basic operations that hardware can execute very efficiently.</p><p>You'll notice a really cool pattern emerging: all these modules that sit underneath the systolic array process column vectors that stream out one by one. This gave us the idea to unify them into something we called a <b>vector processing unit (VPU)</b> ‚Äì because that's exactly what they're doing, processing vectors element-wise!<p>Not only is this more elegant to work with, it's also useful when we scale our TPU beyond a 2x2 systolic array, as we'll have N number of these modules (N being the size of the systolic array), each of which we would have to interface with individually. Unifying these modules under a parent module makes our design more scalable and elegant!</p></p><p>Additionally, by incorporating control signals for each module, which we call the VPU pathway bits, we can selectively enable or skip specific operations. This makes the VPU flexible enough to support both inference and training. For instance, during the forward pass, we want to apply biases and activations but skip computing loss or activation derivatives. When transitioning to the backward pass, all modules are engaged, but within the backward chain we only need to compute the activation derivative. Due to pipelining, all values that flow through the VPU pass through each of the four modules, and any unused modules simply act as registers, forwarding their inputs to outputs without performing computation.</p><p>The next few derivatives are interesting because we can actually use matrix multiplication (and the systolic array!) to compute the derivatives with the help of these three identities:</p><ol><li>If we haveand take its derivative with respect to the weights, we get:</li><li>If we haveand take its derivative with respect to the inputs, we get:<div>(just the weight matrix transposed)</div></li><li>For the bias term, the derivative is simply 1.</li></ol><p>This means that we can multiply the previouswith ,, and 1 to get,, and, respectively, and we can multiply all of these byto get the gradients of the loss with respect to all of our second layer parameters. And because all of the gradients are actually gradient matrices, we can use the systolic array!</p><p>Now something to note about the activation derivativeand the weight derivativeis that they both require the post-activations (H) we calculate during forward pass. This means we need to store the outputs of every layer in some form of memory to be able to perform training. Here's where we created a new scratchpad memory modulewhich we called the unified buffer (UB).This lets us store our H values immediately after we compute them during forward pass.</p><p>We realized that we can also get rid of the input and weight accumulators, as well as manually loading the bias and leak factors into their respective modules, by using the UB to store them. This is also better practice, rather than loading in new data every clock cycle with the instruction set. Since we want to access two values (2 inputs or 2 weights for each row/col of the systolic array) at the same time, we added TWO read and write ports. We did this for each data primitive (inputs, weights, bias, leak factor, post activations) to minimize data contention since we have many different types of data.</p><p>To read values, we supply a starting address and the number of locations we want the UB to read and it will read 2 values every clock cycle. Writing is a similar mechanism, where we specify which values we want to write to each of the two input ports. The beauty in the read mechanism is that it runs in the background once we supply a starting address until the number of locations given are read, meaning we only need to provide an instruction for this every few clock cycles.</p><p>At the end of the day, not having these mechanisms wouldn't break the TPU ‚Äî but they allow us to always keep the systolic array fed, which is a core design principle we couldn't compromise.</p><p>While we were working on this, we realized we could make one last small optimization for the activation derivative module ‚Äî since we only use the  values once (for computing), we created a tiny cache within the VPU instead of storing them in the UB. The rest of the  values will be stored in the UB because they're needed to compute multiple derivatives.</p><p>This is what the new TPU architecture, modified to perform training, looks like:</p><p>Now we can do backpropagation!</p><h3>The beautiful symmetry of forward and backward pass</h3><p>Going back to the computational graph, we discovered something remarkable: the longest chain in backpropagation closely resembles forward pass! In forward pass, we multiply activation matrices with transposed weight matrices. In backward pass, we multiply gradient matrices with weight matrices (untransposed). It's like looking in a mirror!</p><p>This insight led us to compute the long chain of the computational graph first (highlighted in yellow) ‚Äì getting all ourgradients just like we computed activations in forward pass. We could cache these gradients and reuse them, following the same efficient pattern we'd already mastered.</p><p>We create a loop where we:</p><ol><li>Fetch a bridge node () from our unified buffer</li><li>Fetch the corresponding matrix, also from unified buffer</li><li>Stream these through our systolic array to compute the weight gradients</li></ol><p>And here's where something really magical happens: we can stream these weight gradients directly into a gradient descent module while we're still computing them! This module takes the current weights stored in memory and updates them using the gradients.</p><div><p>The gradient descent update rule:</p><p>where  is the learning rate and represents any parameter (weights or biases)</p></div><div><p>Computing weight gradients for our XOR network:</p><p>Bias gradients (sum over samples):</p><p>Applying gradient descent with learning rate:</p></div><p>No waiting around ‚Äî everything flows like water through our pipeline.</p><p>You might be wondering: \"We've used our matrix multiplication identities for the long chain and weight gradients ‚Äî how do we calculate bias gradients?\" Well, we've actually already done most of the work! Since we're processing batches of data, we can simply sum (the technical term is \"reduce\") thegradients across the batch dimension. The beauty is that we can do this reduction right when we're computing the long chain ‚Äî no extra work required!</p><p>With all these new changes and control flags, our instruction is significantly longer ‚Äî 94 bits in fact! But we can confirm that every single one of these bits is needed and we ensured that we couldn't make the instruction set any smaller without compromising the speed and efficiency of the TPU.</p><p>By continuing this same process iteratively ‚Äì forward pass, backward pass, weight updates ‚Äì we can train our network until it performs exactly how we want. The same systolic array that powered our inference now powers our training, with just a few additional modules to handle the gradient computations.</p><p>What started as a simple idea about matrix multiplication has grown into a complete training system. Every component works together in harmony: data flows through pipelines, modules operate in parallel, and our systolic array stays fed with useful work.</p>","contentLength":21867,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44944592"},{"title":"T-Mobile claimed selling location data without consent is legal‚Äìjudges disagree","url":"https://arstechnica.com/tech-policy/2025/08/t-mobile-claimed-selling-location-data-without-consent-is-legal-judges-disagree/","date":1755545101,"author":"Bender","guid":231772,"unread":true,"content":"\n<p>Article URL: <a href=\"https://arstechnica.com/tech-policy/2025/08/t-mobile-claimed-selling-location-data-without-consent-is-legal-judges-disagree/\">https://arstechnica.com/tech-policy/2025/08/t-mobile-claimed-selling-location-data-without-consent-is-legal-judges-disagree/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44944291\">https://news.ycombinator.com/item?id=44944291</a></p>\n<p>Points: 293</p>\n<p># Comments: 72</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Chroma Cloud ‚Äì serverless search database for AI","url":"https://trychroma.com/cloud","date":1755544801,"author":"jeffchuber","guid":233536,"unread":true,"content":"<div>client </div><div>collection name</div><div>    ids</div><div>results </div><div>    query_texts</div><div>    where</div><div>    include</div>","contentLength":72,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44944241"},{"title":"Show HN: We started building an AI dev tool but it turned into a Sims-style game","url":"https://www.youtube.com/watch?v=sRPnX_f2V_c","date":1755543083,"author":"maxraven","guid":231773,"unread":true,"content":"<p>We started out building an AI agent dev tool, but somewhere along the way it turned into Sims for AI agents.</p><p>The original idea was simple: make it easy to create AI agents. We started with Jupyter Notebooks, where each cell could be callable by MCP‚Äîso agents could turn them into tools for themselves. It worked well enough that the system became self-improving, churning out content, and acting like a co-pilot that helped you build new agents.</p><p>But when we stepped back, what we had was these endless walls of text. And even though it worked, honestly, it was just boring. We were also convinced that it would be swallowed up by the next model‚Äôs capabilities. We wanted to build something else‚Äîsomething that made AI less of a black box and more engaging. Why type into a chat box all day if you could look your agents in the face, see their confusion, and watch when and how they interact?</p><p>Both of us grew up on simulation games‚ÄîRollerCoaster Tycoon 3, Age of Empires, SimCity‚Äîso we started experimenting with running LLM agents inside a 3D world. At first it was pure curiosity, but right away, watching agents interact in real time was much more interesting than anything we‚Äôd done before.</p><p>The very first version was small: a single Unity room, an MCP server, and a chat box. Even getting two agents to take turns took weeks. Every run surfaced quirks‚Äîagents refusing to talk at all, or only ‚Äúspeaking‚Äù by dancing or pulling facial expressions to show emotion. That unpredictability kept us building.</p><p>Now it‚Äôs a desktop app (Tauri + Unity via WebGL) where humans and agents share 3D tile-based rooms. Agents receive structured observations every tick and can take actions that change the world. You can edit the rules between runs‚Äîprompts, decision logic, even how they see chat history‚Äîwithout rebuilding.</p><p>On the technical side, we built a Unity bridge with MCP and multi-provider routing via LiteLLM, with local model support via Mistral.rs coming next. All system prompts are editable, so you can directly experiment with coordination strategies‚Äîtuning how ‚Äúchatty‚Äù agents are versus how much they move or manipulate the environment.</p><p>We then added a tilemap editor so you can design custom rooms, set tile-based events with conditions and actions, and turn them into puzzles or hazards. There‚Äôs community sharing built in, so you can post rooms you make.</p><p>Watching agents collude or negotiate through falling tiles, teleports, landmines, fire, ‚Äúwin‚Äù and ‚Äúlose‚Äù tiles, and tool calls for things like lethal fires or disco floors is a much more fun way to spend our days.</p><p>Under the hood, Unity‚Äôs ECS drives a whole state machine and event system. And because humans and AI share the same space in real time, every negotiation, success, or failure also becomes useful multi-agent, multimodal data for post-training or world models.</p><p>Our early users are already using it for prompt-injection testing, social engineering scenarios, cooperative games, and model comparisons.\nThe bigger vision is to build an open-ended, AI-native sim-game where you can build and interact with anything or anyone. You can design puzzles, levels, and environments, have agents compete or collaborate, set up games, or even replay your favorite TV shows.</p><p>The fun part is that no two interactions are ever the same. Everything is emergent, not hard-coded, so the same level played six times will play out differently each time.</p><p>The plan is to keep expanding‚Äîbigger rooms, more in-world tools for agents, and then multiplayer hosting. It‚Äôs live now, no waitlist. Free to play. You can bring your own API keys, or start with $10 in credits and run agents right away: www.TheInterface.com.</p><p>We‚Äôd love feedback on scenarios worth testing and what to build next. Tell us the weird stuff you‚Äôd throw at this‚Äîwe‚Äôll be in the comments.</p>","contentLength":3846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44943986"},{"title":"How to Think About GPUs","url":"https://jax-ml.github.io/scaling-book/gpus/","date":1755541116,"author":"alphabetting","guid":234515,"unread":true,"content":"\n<p>Article URL: <a href=\"https://jax-ml.github.io/scaling-book/gpus/\">https://jax-ml.github.io/scaling-book/gpus/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44943666\">https://news.ycombinator.com/item?id=44943666</a></p>\n<p>Points: 353</p>\n<p># Comments: 105</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How much do electric car batteries degrade?","url":"https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation","date":1755539600,"author":"xnx","guid":231790,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation\">https://www.sustainabilitybynumbers.com/p/electric-car-battery-degradation</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44943420\">https://news.ycombinator.com/item?id=44943420</a></p>\n<p>Points: 92</p>\n<p># Comments: 133</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Left to Right Programming","url":"https://graic.net/p/left-to-right-programming","date":1755536916,"author":"graic","guid":231771,"unread":true,"content":"\n<p>Article URL: <a href=\"https://graic.net/p/left-to-right-programming\">https://graic.net/p/left-to-right-programming</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44942936\">https://news.ycombinator.com/item?id=44942936</a></p>\n<p>Points: 397</p>\n<p># Comments: 319</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Whispering ‚Äì Open-source, local-first dictation you can trust","url":"https://github.com/epicenter-so/epicenter/tree/main/apps/whispering","date":1755535949,"author":"braden-w","guid":231741,"unread":true,"content":"<p>Hey HN! Braden here, creator of Whispering, an open-source speech-to-text app.</p><p>I really like dictation. For years, I relied on transcription tools that were  good, but they were all closed-source. Even a lot of them that claimed to be ‚Äúlocal‚Äù or ‚Äúon-device‚Äù were still black boxes that left me wondering where my audio really went.</p><p>So I built Whispering. It‚Äôs open-source, local-first, and most importantly, transparent with your data.   Your data is stored locally on your device, and your audio goes directly from your machine to a local provider (Whisper C++, Speaches, etc.) or your chosen cloud provider (Groq, OpenAI, ElevenLabs, etc.). For me, the features were good enough that I left my paid tools behind (I used Superwhisper and Wispr Flow before).</p><p>Productivity apps should be open-source and transparent with your data, but they also need to match the UX of paid, closed-software alternatives. I hope Whispering is near that point. I use it for several hours a day, from coding to thinking out loud while carrying pizza boxes back from the office.</p><p>There are plenty of transcription apps out there, but I hope Whispering adds some extra competition from the OSS ecosystem (one of my other OSS favorites is Handy <a href=\"https://github.com/cjpais/Handy\" rel=\"nofollow\">https://github.com/cjpais/Handy</a>). Whispering has a few tricks up its sleeve, like a voice-activated mode for hands-free operation (no button holding), and customizable AI transformations with any prompt/model.</p><p>I‚Äôm basically obsessed with local-first open-source software. I think there should be an open-source, local-first version of every app, and I would like them all to work together. The idea of Epicenter is to store your data in a folder of plaintext and SQLite, and build a suite of interoperable, local-first tools on top of this shared memory. Everything is totally transparent, so you can trust it.</p><p>Whispering is the first app in this effort. It‚Äôs not there yet regarding memory, but it‚Äôs getting there. I‚Äôll probably write more about the bigger picture soon, but mainly I just want to make software and let it speak for itself (no pun intended in this case!), so this is my Show HN for now.</p><p>I just finished college and was about to move back with my parents and work on this instead of getting a job‚Ä¶and then I somehow got into YC. So my current plan is to cover my living expenses and use the YC funding to support maintainers, our dependencies, and people working on their own open-source local-first projects. More on that soon.</p>","contentLength":2479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44942731"},{"title":"How Not to Buy a SSD","url":"https://andrei.xyz/post/how-not-to-buy-a-ssd/","date":1755535799,"author":"speckx","guid":236969,"unread":true,"content":"\n<p>Article URL: <a href=\"https://andrei.xyz/post/how-not-to-buy-a-ssd/\">https://andrei.xyz/post/how-not-to-buy-a-ssd/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44942709\">https://news.ycombinator.com/item?id=44942709</a></p>\n<p>Points: 146</p>\n<p># Comments: 136</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Miles from the ocean, there's diving beneath the streets of Budapest","url":"https://www.cnn.com/2025/08/18/travel/budapest-diving-molnar-janos-cave","date":1755535373,"author":"thm","guid":235935,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.cnn.com/2025/08/18/travel/budapest-diving-molnar-janos-cave\">https://www.cnn.com/2025/08/18/travel/budapest-diving-molnar-janos-cave</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44942627\">https://news.ycombinator.com/item?id=44942627</a></p>\n<p>Points: 130</p>\n<p># Comments: 30</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"My Retro TVs","url":"https://www.myretrotvs.com/","date":1755535248,"author":"the-mitr","guid":231740,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.myretrotvs.com/\">https://www.myretrotvs.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44942602\">https://news.ycombinator.com/item?id=44942602</a></p>\n<p>Points: 104</p>\n<p># Comments: 18</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elegant mathematics bending the future of design","url":"https://actu.epfl.ch/news/elegant-mathematics-bending-the-future-of-design/","date":1755534970,"author":"robinhouston","guid":236764,"unread":true,"content":"\n<p>Article URL: <a href=\"https://actu.epfl.ch/news/elegant-mathematics-bending-the-future-of-design/\">https://actu.epfl.ch/news/elegant-mathematics-bending-the-future-of-design/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44942547\">https://news.ycombinator.com/item?id=44942547</a></p>\n<p>Points: 132</p>\n<p># Comments: 13</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anna's Archive: An Update from the Team","url":"https://annas-archive.org/blog/an-update-from-the-team.html","date":1755534708,"author":"jerheinze","guid":231717,"unread":true,"content":"\n<p>Article URL: <a href=\"https://annas-archive.org/blog/an-update-from-the-team.html\">https://annas-archive.org/blog/an-update-from-the-team.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44942501\">https://news.ycombinator.com/item?id=44942501</a></p>\n<p>Points: 936</p>\n<p># Comments: 440</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: OS X Mavericks Forever","url":"https://mavericksforever.com/","date":1755532904,"author":"Wowfunhappy","guid":235688,"unread":true,"content":"<p>Whenever I've finished installing Mavericks, the first thing I do is open Terminal and run:\n\t\t\t\t</p><blockquote>curl mavericksforever.com/postinstall.sh | sh</blockquote><ul><li>Update Mavericks to the latest version available.</li><li>Change some default settings to resemble Snow Leopard.</li><li>Remove apps and features which don't work anymore, and would just make me sad if I saw them.</li></ul><p>The result of this script represents how I want to be greeted by a fresh Mavericks system.</p><p>We need a modern web browser! Luckily, the incredible i3roly has us covered with Firefox Dynasty, a fully up-to-date version of Firefox modified to work on old versions of OS X. It works just like mainline Firefox, with every website that mainline Firefox does.</p><p>I created a small Preference Pane which makes it easy to download new builds of Firefox Dynasty, and attempts to make Firefox better comform to Apple's Human Interface Guidelines. All real credit goes to i3roly.</p><p>After installing the Preference Pane, I recommend completely deleting the ancient version of Safari included with Mavericks. Safari 9 is  safe to use in 2025!</p><blockquote>sudo rm -r /Applications/Safari.app</blockquote><p>Firefox Dynasty covers our web browsing needs, but we'll still run into trouble with other internet-enabled apps. Dictionary will say it can't connect to Wikipedia, and Mail will be unable to load many embedded images.</p><p>These problems stem from Mavericks's outdated implementation of HTTPS/SSL/TLS. To fix them, use Aqua Proxy:</p><p>Aqua Proxy is what's typically referred to as a local \"Man-in-the-Middle Proxy Server\". It sits between you and the internet, captures your computer's traffic, and modifies it to be compatible with modern servers before sending it on its way.</p><p>The Unicode Consortium has introduced a lot of new emojis since Mavericks was released. We need to add them to Mavericks!</p><p>This will be short. Please do grab a USB hard drive and use it to set up Time Machine in System Preferences.</p><p>Backups are good, and once Time Machine is set up, you can make a change to your system and then easily undo it later. For example...</p><h3>Delete Applications You Don't Use</h3><p>If you try to drag the Chess app to the trash, Finder will complain that \"'Chess' can't be modified or deleted because it's required by OS X.\" Finder is wrong; Chess is not required, and you can delete it using the Terminal.</p><p>I don't play Chess. I also don't use iTunes, don't own any DVDs, and don't read Apple iBooks on my computer. So, on a fresh install of Mavericks, I like to open the Terminal and run:</p><blockquote>\n\t\t\t\tsudo rm -rf \"/Applications/Chess.app\"\n\t\t\t\tsudo rm -rf \"/Applications/DVD Player.app\"<p>\n\t\t\t\tsudo rm -rf \"/Applications/iTunes.app\"</p>\n\t\t\t\tsudo rm -rf \"/Applications/iBooks.app\"</blockquote><p>Don't you  how hackable everything is? Removing stock apps from the Applications folder is completely safe‚Äînothing will break‚Äîand this is  computer, so you should make it your own. You can always restore apps later using Time Machine. Just don't delete System Preferences, or anything in the Utilities folder.</p>","contentLength":2947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44942099"},{"title":"Who Invented Backpropagation?","url":"https://people.idsia.ch/~juergen/who-invented-backpropagation.html","date":1755532221,"author":"nothrowaways","guid":231716,"unread":true,"content":"\n<p>Article URL: <a href=\"https://people.idsia.ch/~juergen/who-invented-backpropagation.html\">https://people.idsia.ch/~juergen/who-invented-backpropagation.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44941963\">https://news.ycombinator.com/item?id=44941963</a></p>\n<p>Points: 142</p>\n<p># Comments: 77</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An interactive guide to SVG paths","url":"https://www.joshwcomeau.com/svg/interactive-guide-to-paths/","date":1755530345,"author":"joshwcomeau","guid":235879,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.joshwcomeau.com/svg/interactive-guide-to-paths/\">https://www.joshwcomeau.com/svg/interactive-guide-to-paths/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44941605\">https://news.ycombinator.com/item?id=44941605</a></p>\n<p>Points: 429</p>\n<p># Comments: 43</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Launch HN: Reality Defender (YC W22) ‚Äì API for Deepfake and GenAI Detection","url":"https://www.realitydefender.com/platform/api","date":1755530194,"author":"bpcrd","guid":233212,"unread":true,"content":"\n<p>Hi HN! This is Ben from Reality Defender (<a href=\"https://www.realitydefender.com\">https://www.realitydefender.com</a>). We build real-time multimodal and multi-model deepfake detection for Fortune 100s and governments all over the world. (We even won the RSAC Innovation Showcase award for our work: <a href=\"https://www.prnewswire.com/news-releases/reality-defender-wins-most-innovative-startup-at-rsa-conference-2024-innovation-sandbox-302137326.html\" rel=\"nofollow\">https://www.prnewswire.com/news-releases/reality-defender-wi...</a>)<p>Today, we‚Äôre excited to share our public API and SDK, allowing anyone to access our platform with 2 lines of code: <a href=\"https://www.realitydefender.com/api\">https://www.realitydefender.com/api</a><p>Back in W22, we launched our product to detect AI-generated media across audio, video, and images: <a href=\"https://news.ycombinator.com/item?id=30766050\">https://news.ycombinator.com/item?id=30766050</a><p>That post kicked off conversations with devs, security teams, researchers, and governments. The most common question: \"Can we get API/SDK access to build deepfake detection into our product?\"<p>We‚Äôve heard that from solo devs building moderation tools, fintechs adding ID verification, founders running marketplaces, and infrastructure companies protecting video calls and onboarding flows. They weren‚Äôt asking us to build anything new; they simply wanted access to what we already had so they could plug it in and move forward.<p>After running pilots and engagements with customers, we‚Äôre finally ready to share our public API and SDK. Now anyone can embed deepfake detection with just two lines of code, starting at the low price of free.<p><a href=\"https://www.realitydefender.com/api\">https://www.realitydefender.com/api</a><p>Our new developer tools support detection across images, voice, video, and text ‚Äî with the former two available as part of the free tier. If your product touches KYC, UGC, support workflows, communications, marketplaces, or identity layers, you can now embed real-time detection directly in your stack. It runs in the cloud, and longstanding clients using our platform have also deployed on-prem, at the edge, or on fully airgapped systems.<p>SDKs are currently available in Python, Java, Rust, TypeScript, and Go. The first 50 scans per month are free, with usage-based pricing beyond that. If you‚Äôre working on something that requires other features or streaming access (like real-time voice or video), email us directly at yc@realitydefender.com<p>Much has changed since 2022. The threats we imagined back then are now showing up in everyday support tickets and incident reports. We‚Äôve witnessed voice deepfakes targeting bank call centers to commit real-time fraud; fabricated documents and AI-generated selfies slip through KYC and IDV onboarding systems; fake dating profiles, AI-generated marketplace sellers, and ‚Äúverified‚Äù influencers impersonating real people. Political disinformation videos and synthetic media leaks have triggered real-world legal and PR crises. Even reviews, support transcripts, and impersonation scripts are increasingly being generated by AI.\nDetection remains harder than we first expected since we began in 2021. New generation methods emerge every few weeks that invalidate prior assumptions. This is why we are committed to building every layer of this ourselves. We don‚Äôt license or white-label detection models; everything we deploy is built in-house by our team.<p>Since our original launch, we‚Äôve worked with tier-one banks, global governments, and media companies to deploy detection inside their highest-risk workflows. However, we always believed the need wasn‚Äôt limited to large institutions, but everywhere. It showed up in YC office hours, in early bug reports, and in group chats after our last HN post.<p>We‚Äôve taken our time to make sure this was built well, flexible enough for startups, and battle-tested enough to trust in production. The API you can use today is the same one powering many of our enterprise deployments.<p>Our goal is to make Reality Defender feel like Stripe, Twilio, or Plaid ‚Äî an invisible, trusted layer that you can drop into your system to protect what matters. We feel deepfake detection is a key component of critical infrastructure, and like any good infrastructure, it should be modular, reliable, and boring (in the best possible way).<p>Reality Defender is already in the Zoom marketplace and will be on the Teams marketplace soon. We will also power deepfake detection for identity workflows, support platforms, and internal trust and safety pipelines.<p>If you're building something where trust, identity, or content integrity matter, or if you‚Äôve run into weird edge cases you can‚Äôt explain, we‚Äôd love to hear from you.<p>You can get started here: <a href=\"https://realitydefender.com/api\">https://realitydefender.com/api</a><p>Or you can try us for free two different ways:<p>1) 1-click add to Zoom / Teams to try in your own calls immediately.<p>2) Email us up to 50 files at yc@realitydefender.com and we‚Äôll scan them for you ‚Äî no setup required.<p>Thanks again to the HN community for helping launch us three years ago. It‚Äôs been a wild ride, and we‚Äôre excited to share something new. We live on HN ourselves and will be here for all your feedback. Let us know what you think!</p>\n<hr>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44941580\">https://news.ycombinator.com/item?id=44941580</a></p>\n<p>Points: 81</p>\n<p># Comments: 41</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Counter-Strike: A billion-dollar game built in a dorm room","url":"https://www.nytimes.com/2025/08/18/arts/counter-strike-half-life-minh-le.html","date":1755529166,"author":"asnyder","guid":231770,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.nytimes.com/2025/08/18/arts/counter-strike-half-life-minh-le.html\">https://www.nytimes.com/2025/08/18/arts/counter-strike-half-life-minh-le.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44941369\">https://news.ycombinator.com/item?id=44941369</a></p>\n<p>Points: 446</p>\n<p># Comments: 383</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"95% of AI Pilots Failing","url":"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/","date":1755527763,"author":"amirkabbara","guid":231657,"unread":true,"content":"\n<p>Article URL: <a href=\"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/\">https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44941118\">https://news.ycombinator.com/item?id=44941118</a></p>\n<p>Points: 113</p>\n<p># Comments: 69</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VHS-C: When a lazy idea stumbles towards perfection [video]","url":"https://www.youtube.com/watch?v=HFYWHeBhYbM","date":1755527428,"author":"surprisetalk","guid":237016,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.youtube.com/watch?v=HFYWHeBhYbM\">https://www.youtube.com/watch?v=HFYWHeBhYbM</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44941056\">https://news.ycombinator.com/item?id=44941056</a></p>\n<p>Points: 187</p>\n<p># Comments: 102</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: A Minimal Hacker News Reader for Apple Watch Built with SwiftUI","url":"https://github.com/wieslawsoltes/HackerNewsWatch","date":1755526968,"author":"wiso","guid":233628,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44940974"},{"title":"AI is predominantly replacing outsourced, offshore workers","url":"https://www.axios.com/2025/08/18/ai-jobs-layoffs","date":1755526833,"author":"toomuchtodo","guid":231656,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.axios.com/2025/08/18/ai-jobs-layoffs\">https://www.axios.com/2025/08/18/ai-jobs-layoffs</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44940944\">https://news.ycombinator.com/item?id=44940944</a></p>\n<p>Points: 109</p>\n<p># Comments: 68</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Class-action suit claims Otter AI records private work conversations","url":"https://www.npr.org/2025/08/15/g-s1-83087/otter-ai-transcription-class-action-lawsuit","date":1755524858,"author":"nsedlet","guid":231715,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.npr.org/2025/08/15/g-s1-83087/otter-ai-transcription-class-action-lawsuit\">https://www.npr.org/2025/08/15/g-s1-83087/otter-ai-transcription-class-action-lawsuit</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44940554\">https://news.ycombinator.com/item?id=44940554</a></p>\n<p>Points: 105</p>\n<p># Comments: 20</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FFmpeg Assembly Language Lessons","url":"https://github.com/FFmpeg/asm-lessons","date":1755524393,"author":"flykespice","guid":231655,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/FFmpeg/asm-lessons\">https://github.com/FFmpeg/asm-lessons</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44940485\">https://news.ycombinator.com/item?id=44940485</a></p>\n<p>Points: 396</p>\n<p># Comments: 133</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Texas law gives grid operator power to disconnect data centers during crisis","url":"https://www.utilitydive.com/news/texas-law-gives-grid-operator-power-to-disconnect-data-centers-during-crisi/751587/","date":1755524046,"author":"walterbell","guid":231714,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.utilitydive.com/news/texas-law-gives-grid-operator-power-to-disconnect-data-centers-during-crisi/751587/\">https://www.utilitydive.com/news/texas-law-gives-grid-operator-power-to-disconnect-data-centers-during-crisi/751587/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44940416\">https://news.ycombinator.com/item?id=44940416</a></p>\n<p>Points: 109</p>\n<p># Comments: 98</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vibe coding tips and tricks","url":"https://github.com/awslabs/mcp/blob/main/VIBE_CODING_TIPS_TRICKS.md","date":1755521848,"author":"mooreds","guid":231654,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/awslabs/mcp/blob/main/VIBE_CODING_TIPS_TRICKS.md\">https://github.com/awslabs/mcp/blob/main/VIBE_CODING_TIPS_TRICKS.md</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44940089\">https://news.ycombinator.com/item?id=44940089</a></p>\n<p>Points: 149</p>\n<p># Comments: 71</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intel Foundry Demonstrates First Arm-Based Chip on 18A Node","url":"https://hothardware.com/news/intel-foundry-demos-deer-creek-falls-reference-soc","date":1755520278,"author":"rbanffy","guid":234514,"unread":true,"content":"\n<p>Article URL: <a href=\"https://hothardware.com/news/intel-foundry-demos-deer-creek-falls-reference-soc\">https://hothardware.com/news/intel-foundry-demos-deer-creek-falls-reference-soc</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44939849\">https://news.ycombinator.com/item?id=44939849</a></p>\n<p>Points: 96</p>\n<p># Comments: 68</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When you're asking AI chatbots for answers, they're data-mining you","url":"https://www.theregister.com/2025/08/18/opinion_column_ai_surveillance/","date":1755518297,"author":"rntn","guid":231614,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.theregister.com/2025/08/18/opinion_column_ai_surveillance/\">https://www.theregister.com/2025/08/18/opinion_column_ai_surveillance/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44939660\">https://news.ycombinator.com/item?id=44939660</a></p>\n<p>Points: 101</p>\n<p># Comments: 48</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The new geography of stolen goods","url":"https://www.economist.com/interactive/britain/2025/08/17/the-new-geography-of-stolen-goods","date":1755516553,"author":"tlb","guid":233535,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.economist.com/interactive/britain/2025/08/17/the-new-geography-of-stolen-goods\">https://www.economist.com/interactive/britain/2025/08/17/the-new-geography-of-stolen-goods</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44939460\">https://news.ycombinator.com/item?id=44939460</a></p>\n<p>Points: 81</p>\n<p># Comments: 62</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MapLibre Tile: A next generation geospatial format optimized for rendering","url":"https://arxiv.org/abs/2508.10791","date":1755516512,"author":"mtremmel","guid":234575,"unread":true,"content":"\n<p>Article URL: <a href=\"https://arxiv.org/abs/2508.10791\">https://arxiv.org/abs/2508.10791</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44939456\">https://news.ycombinator.com/item?id=44939456</a></p>\n<p>Points: 127</p>\n<p># Comments: 11</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLMs and coding agents are a security nightmare","url":"https://garymarcus.substack.com/p/llms-coding-agents-security-nightmare","date":1755515074,"author":"flail","guid":231613,"unread":true,"content":"\n<p>Article URL: <a href=\"https://garymarcus.substack.com/p/llms-coding-agents-security-nightmare\">https://garymarcus.substack.com/p/llms-coding-agents-security-nightmare</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44939331\">https://news.ycombinator.com/item?id=44939331</a></p>\n<p>Points: 124</p>\n<p># Comments: 64</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Website is served from nine Neovim buffers on my old ThinkPad","url":"https://vim.gabornyeki.com/","date":1755515002,"author":"todsacerdoti","guid":231612,"unread":true,"content":"\n<p>Article URL: <a href=\"https://vim.gabornyeki.com/\">https://vim.gabornyeki.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44939324\">https://news.ycombinator.com/item?id=44939324</a></p>\n<p>Points: 82</p>\n<p># Comments: 14</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The End of Handwriting","url":"https://www.wired.com/story/the-end-of-handwriting/","date":1755513445,"author":"beardyw","guid":234574,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.wired.com/story/the-end-of-handwriting/\">https://www.wired.com/story/the-end-of-handwriting/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44939165\">https://news.ycombinator.com/item?id=44939165</a></p>\n<p>Points: 91</p>\n<p># Comments: 197</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MCP doesn't need tools, it needs code","url":"https://lucumr.pocoo.org/2025/8/18/code-mcps/","date":1755510783,"author":"the_mitsuhiko","guid":231580,"unread":true,"content":"\n<p>Article URL: <a href=\"https://lucumr.pocoo.org/2025/8/18/code-mcps/\">https://lucumr.pocoo.org/2025/8/18/code-mcps/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44938920\">https://news.ycombinator.com/item?id=44938920</a></p>\n<p>Points: 179</p>\n<p># Comments: 112</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Electromechanical reshaping, an alternative to laser eye surgery","url":"https://medicalxpress.com/news/2025-08-alternative-lasik-lasers.html","date":1755509717,"author":"Gaishan","guid":231611,"unread":true,"content":"\n<p>Article URL: <a href=\"https://medicalxpress.com/news/2025-08-alternative-lasik-lasers.html\">https://medicalxpress.com/news/2025-08-alternative-lasik-lasers.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44938818\">https://news.ycombinator.com/item?id=44938818</a></p>\n<p>Points: 275</p>\n<p># Comments: 119</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geotoy ‚Äì Shadertoy for 3D Geometry","url":"https://3d.ameo.design/geotoy","date":1755507288,"author":"Ameo","guid":233492,"unread":true,"content":"\n<p>Article URL: <a href=\"https://3d.ameo.design/geotoy\">https://3d.ameo.design/geotoy</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44938622\">https://news.ycombinator.com/item?id=44938622</a></p>\n<p>Points: 105</p>\n<p># Comments: 22</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Web apps in a single, portable, self-updating, vanilla HTML file","url":"https://hyperclay.com/","date":1755499091,"author":"pil0u","guid":231469,"unread":true,"content":"\n<p>Article URL: <a href=\"https://hyperclay.com/\">https://hyperclay.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44937991\">https://news.ycombinator.com/item?id=44937991</a></p>\n<p>Points: 558</p>\n<p># Comments: 198</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Block Stacking Problem","url":"https://sites.pitt.edu/~jdnorton/Goodies/block_stacking/block_stacking.html","date":1755496904,"author":"lisper","guid":234655,"unread":true,"content":"\n<p>Article URL: <a href=\"https://sites.pitt.edu/~jdnorton/Goodies/block_stacking/block_stacking.html\">https://sites.pitt.edu/~jdnorton/Goodies/block_stacking/block_stacking.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44937819\">https://news.ycombinator.com/item?id=44937819</a></p>\n<p>Points: 107</p>\n<p># Comments: 27</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SystemD Service Hardening","url":"https://roguesecurity.dev/blog/systemd-hardening","date":1755493067,"author":"todsacerdoti","guid":231579,"unread":true,"content":"\n<p>Article URL: <a href=\"https://roguesecurity.dev/blog/systemd-hardening\">https://roguesecurity.dev/blog/systemd-hardening</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44937550\">https://news.ycombinator.com/item?id=44937550</a></p>\n<p>Points: 229</p>\n<p># Comments: 83</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Happy 100000th birthday, Debian","url":"https://lists.debian.org/debian-devel-announce/2025/08/msg00006.html","date":1755492800,"author":"pabs3","guid":236029,"unread":true,"content":"\n<p>Article URL: <a href=\"https://lists.debian.org/debian-devel-announce/2025/08/msg00006.html\">https://lists.debian.org/debian-devel-announce/2025/08/msg00006.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44937530\">https://news.ycombinator.com/item?id=44937530</a></p>\n<p>Points: 88</p>\n<p># Comments: 15</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google admits anti-competitive conduct involving Google Search in Australia","url":"https://www.accc.gov.au/media-release/google-admits-anti-competitive-conduct-involving-google-search-in-australia","date":1755485695,"author":"Improvement","guid":231430,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.accc.gov.au/media-release/google-admits-anti-competitive-conduct-involving-google-search-in-australia\">https://www.accc.gov.au/media-release/google-admits-anti-competitive-conduct-involving-google-search-in-australia</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44936945\">https://news.ycombinator.com/item?id=44936945</a></p>\n<p>Points: 242</p>\n<p># Comments: 151</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Uniform Memory Access (NUMA) is reshaping microservice placement","url":"https://codemia.io/blog/path/NUMA-Is-the-New-Network-How-Per-Socket-Memory-Models-Are-Reshaping-Microservice-Placement","date":1755481229,"author":"signa11","guid":231578,"unread":true,"content":"\n<p>Article URL: <a href=\"https://codemia.io/blog/path/NUMA-Is-the-New-Network-How-Per-Socket-Memory-Models-Are-Reshaping-Microservice-Placement\">https://codemia.io/blog/path/NUMA-Is-the-New-Network-How-Per-Socket-Memory-Models-Are-Reshaping-Microservice-Placement</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44936575\">https://news.ycombinator.com/item?id=44936575</a></p>\n<p>Points: 79</p>\n<p># Comments: 24</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"French firm Gouach is pitching an Infinite Battery with replaceable cells","url":"https://arstechnica.com/gadgets/2025/05/gouach-wants-you-to-insert-and-pluck-the-cells-from-its-infinite-e-bike-battery/","date":1755480764,"author":"pabs3","guid":235611,"unread":true,"content":"\n<p>Article URL: <a href=\"https://arstechnica.com/gadgets/2025/05/gouach-wants-you-to-insert-and-pluck-the-cells-from-its-infinite-e-bike-battery/\">https://arstechnica.com/gadgets/2025/05/gouach-wants-you-to-insert-and-pluck-the-cells-from-its-infinite-e-bike-battery/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44936535\">https://news.ycombinator.com/item?id=44936535</a></p>\n<p>Points: 130</p>\n<p># Comments: 92</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mangle ‚Äì a language for deductive database programming","url":"https://github.com/google/mangle","date":1755478535,"author":"simonpure","guid":231577,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/google/mangle\">https://github.com/google/mangle</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44936333\">https://news.ycombinator.com/item?id=44936333</a></p>\n<p>Points: 79</p>\n<p># Comments: 14</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Clojure Async Flow Guide","url":"https://clojure.github.io/core.async/flow-guide.html","date":1755478323,"author":"simonpure","guid":231429,"unread":true,"content":"\n<p>Article URL: <a href=\"https://clojure.github.io/core.async/flow-guide.html\">https://clojure.github.io/core.async/flow-guide.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44936309\">https://news.ycombinator.com/item?id=44936309</a></p>\n<p>Points: 176</p>\n<p># Comments: 70</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI vs. Professional Authors Results","url":"http://mark---lawrence.blogspot.com/2025/08/the-ai-vs-authors-results-part-2.html","date":1755466902,"author":"biffles","guid":231428,"unread":true,"content":"\n<p>Article URL: <a href=\"http://mark---lawrence.blogspot.com/2025/08/the-ai-vs-authors-results-part-2.html\">http://mark---lawrence.blogspot.com/2025/08/the-ai-vs-authors-results-part-2.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44935171\">https://news.ycombinator.com/item?id=44935171</a></p>\n<p>Points: 81</p>\n<p># Comments: 52</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Llama-Scan: Convert PDFs to Text W Local LLMs","url":"https://github.com/ngafar/llama-scan","date":1755466847,"author":"nawazgafar","guid":230781,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/ngafar/llama-scan\">https://github.com/ngafar/llama-scan</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44935169\">https://news.ycombinator.com/item?id=44935169</a></p>\n<p>Points: 194</p>\n<p># Comments: 76</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HN Search isn't ingesting new data since Friday","url":"https://github.com/algolia/hn-search/issues/248","date":1755461336,"author":"busymom0","guid":230754,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/algolia/hn-search/issues/248\">https://github.com/algolia/hn-search/issues/248</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44934518\">https://news.ycombinator.com/item?id=44934518</a></p>\n<p>Points: 178</p>\n<p># Comments: 32</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Doxx ‚Äì Terminal .docx viewer inspired by Glow","url":"https://github.com/bgreenwell/doxx","date":1755460323,"author":"w108bmg","guid":230753,"unread":true,"content":"<p>I got tired of open file.docx ‚Üí wait 8 seconds ‚Üí close Word just to read a document, so I built a terminal-native Word viewer!</p><p>* View `.docx` files directly in your terminal with (mostly) proper formatting</p><p>* Tables actually look like tables (with Unicode borders!)</p><p>* Nested lists work correctly with indentation</p><p>* Full-text search with highlighting</p><p>* Copy content straight to clipboard with `c`</p><p>* Export to markdown/CSV/JSON</p><p>Working on servers over SSH, I constantly hit Word docs I needed to check quickly. The existing solutions I'm aware of either strip all formatting (docx2txt) or require GUI apps. Wanted something that felt as polished as [glow](<a href=\"https://github.com/charmbracelet/glow\" rel=\"nofollow\">https://github.com/charmbracelet/glow</a>) but for Word documents.</p><p>* 50ms startup vs Word's 8+ seconds</p><p>* Works over SSH (obviously)</p><p>* Preserves document structure and formatting</p><p>* Smart table alignment based on data types</p><p>* Interactive outline view for long docs</p><pre><code>    # Install\n    cargo install --git https://github.com/bgreenwell/doxx\n    \n    # Use\n    doxx quarterly-report.docx\n</code></pre>\nStill early but handles most Word docs I throw at it. Always wanted a proper Word viewer in my terminal toolkit alongside `bat`, `glow`, and friends. Let me know what you think!","contentLength":1200,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44934391"},{"title":"ArchiveTeam has finished archiving all goo.gl short links","url":"https://tracker.archiveteam.org/goo-gl/","date":1755452764,"author":"pentagrama","guid":230752,"unread":true,"content":"\n<p>Article URL: <a href=\"https://tracker.archiveteam.org/goo-gl/\">https://tracker.archiveteam.org/goo-gl/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44933401\">https://news.ycombinator.com/item?id=44933401</a></p>\n<p>Points: 355</p>\n<p># Comments: 83</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SK hynix dethrones Samsung as world‚Äôs top DRAM maker","url":"https://koreajoongangdaily.joins.com/news/2025-08-15/business/tech/Thanks-Nvidia-SK-hynix-dethrones-Samsung-as-worlds-top-DRAM-maker-for-first-time-in-over-30-years/2376834","date":1755451893,"author":"ksec","guid":235555,"unread":true,"content":"\n<p>Article URL: <a href=\"https://koreajoongangdaily.joins.com/news/2025-08-15/business/tech/Thanks-Nvidia-SK-hynix-dethrones-Samsung-as-worlds-top-DRAM-maker-for-first-time-in-over-30-years/2376834\">https://koreajoongangdaily.joins.com/news/2025-08-15/business/tech/Thanks-Nvidia-SK-hynix-dethrones-Samsung-as-worlds-top-DRAM-maker-for-first-time-in-over-30-years/2376834</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44933290\">https://news.ycombinator.com/item?id=44933290</a></p>\n<p>Points: 154</p>\n<p># Comments: 63</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Claudia ‚Äì Desktop companion for Claude code","url":"https://claudiacode.com/","date":1755451616,"author":"zerealshadowban","guid":230751,"unread":true,"content":"\n<p>Article URL: <a href=\"https://claudiacode.com/\">https://claudiacode.com/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44933255\">https://news.ycombinator.com/item?id=44933255</a></p>\n<p>Points: 463</p>\n<p># Comments: 213</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Enterprise Experience","url":"https://churchofturing.github.io/the-enterprise-experience.html","date":1755449583,"author":"Improvement","guid":230750,"unread":true,"content":"\n<p>Article URL: <a href=\"https://churchofturing.github.io/the-enterprise-experience.html\">https://churchofturing.github.io/the-enterprise-experience.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44932980\">https://news.ycombinator.com/item?id=44932980</a></p>\n<p>Points: 450</p>\n<p># Comments: 128</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: OverType ‚Äì A Markdown WYSIWYG editor that's just a textarea","url":"https://news.ycombinator.com/item?id=44932651","date":1755447186,"author":"panphora","guid":230749,"unread":true,"content":"Hi HN! I got so frustrated with modern WYSIWYG editors that I started to play around with building my own.<p>The problem I had was simple: I wanted a low-tech way to type styled text, but I didn't want to load a complex 500KB library, especially if I was going to initialize it dozens of times on the same page.</p><p>Markdown in a plain &lt;textarea&gt; was the best alternative to a full WYSIWYG, but its main drawback is how ugly it looks without any formatting. I can handle it, but my clients certainly can't.</p><p>I went down the ContentEditable rabbit hole for a few years, but always came to realize others had solved it better than I ever could.</p><p>I kept coming back to this problem: why can't I have a simple, performant, beautiful markdown editor? The best solution I ever saw was Ghost's split-screen editor: markdown on the left, preview on the right, with synchronized scrolling.</p><p>Then, about a year ago, an idea popped into my head: what if we layered a preview pane behind a &lt;textarea&gt;? If we aligned them perfectly, then even though you were only editing plain text, it would look and feel like you were editing rich text!</p><p>Of course, there would be downsides: you'd have to use a monospace font, all content would have to have the same font size, and all the markdown markup would have to be displayed in the final preview.</p><p>But those were tradeoffs I could live with.</p><p>Anyways, version 1 didn't go so well... it turns out it's harder to keep a textarea and a rendered preview in alignment than I thought. Here's what I discovered:</p><p>- Lists were hard to align - bullet points threw off character alignment. Solved with HTML entities (‚Ä¢ for bullets) that maintain monospace width</p><p>- Not all monospace fonts are truly monospace - bold and italic text can have different widths even in \"monospace\" fonts, breaking the perfect overlay</p><p>- Embedding was a nightmare - any inherited CSS from parent pages (margin, padding, line-height) would shift alignment. Even a 1px shift completely broke the illusion</p><p>The solution was obsessive normalization:</p><pre><code>    // The entire trick: a transparent textarea over a preview div\n    layerElements(textarea, preview)\n    applyIdenticalSpacing(textarea, preview)\n\n    // Make textarea invisible but keep the cursor\n    textarea.style.background = 'transparent'\n    textarea.style.color = 'transparent'\n    textarea.style.caretColor = 'black'\n\n    // Keep them in sync\n    textarea.addEventListener('input', () =&gt; {\n      preview.innerHTML = parseMarkdown(textarea.value)\n      syncScroll(textarea, preview)\n    })\n</code></pre>\nA week ago I started playing with version 2 and discovered GitHub's &lt;markdown-toolbar&gt; element, which handles markdown formatting in a plain &lt;textarea&gt; really well.<p>That experiment turned into OverType (<a href=\"https://overtype.dev\" rel=\"nofollow\">https://overtype.dev</a>), which I'm showing to you today -- it's a rich markdown editor that's really just a &lt;textarea&gt;. The key insight was that once you solve the alignment challenges, you get everything native textareas provide for free: undo/redo, mobile keyboard, accessibility, and native performance.</p><p>So far it works surprisingly well across browsers and mobile. I get performant rich text editing in one small package (45KB total). It's kind of a dumb idea, but it works! I'm planning to use it in all my projects and I'd like to keep it simple and minimal.</p><p>I would love it if you would kick the tires and let me know what you think of it. Happy editing!</p>","contentLength":3380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44932651"},{"title":"MS-DOS development resources","url":"https://github.com/SuperIlu/DOSDevelResources/blob/main/README.md","date":1755447037,"author":"mariuz","guid":231468,"unread":true,"content":"\n<p>Article URL: <a href=\"https://github.com/SuperIlu/DOSDevelResources/blob/main/README.md\">https://github.com/SuperIlu/DOSDevelResources/blob/main/README.md</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44932624\">https://news.ycombinator.com/item?id=44932624</a></p>\n<p>Points: 91</p>\n<p># Comments: 21</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"D4D4","url":"https://www.nmichaels.org/musings/d4d4/d4d4/","date":1755445352,"author":"csense","guid":235610,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.nmichaels.org/musings/d4d4/d4d4/\">https://www.nmichaels.org/musings/d4d4/d4d4/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44932401\">https://news.ycombinator.com/item?id=44932401</a></p>\n<p>Points: 434</p>\n<p># Comments: 49</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: Fallinorg - Offline Mac app that organizes files by meaning","url":"https://fallinorg.com/#","date":1755445204,"author":"bobnarizes","guid":230757,"unread":true,"content":"<div>\n                                    Yes, during the pre-sale period, you‚Äôll receive all small updates we release‚Äîthis includes bug fixes, performance improvements, and minor feature tweaks. These updates are free for all pre-sale buyers.\n\n                                    Once the pre-sale ends, we‚Äôll begin developing larger features and major upgrades. These will be part of the full release, and pre-sale buyers will have the option to upgrade at a special discounted price.\n                                    \n                                    By joining early, you get to enjoy the app right away, help shape its development with your feedback, and secure early access at our lowest price.\n                                </div>","contentLength":740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44932375"},{"title":"Here be dragons: Preventing static damage, latchup, and metastability in the 386","url":"http://www.righto.com/2025/08/static-latchup-metastability-386.html","date":1755444850,"author":"todsacerdoti","guid":231467,"unread":true,"content":"\n<p>Article URL: <a href=\"http://www.righto.com/2025/08/static-latchup-metastability-386.html\">http://www.righto.com/2025/08/static-latchup-metastability-386.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44932342\">https://news.ycombinator.com/item?id=44932342</a></p>\n<p>Points: 75</p>\n<p># Comments: 43</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: NextDNS Adds \"Bypass Age Verification\"","url":"https://news.ycombinator.com/item?id=44931824","date":1755440962,"author":"nextdns","guid":230748,"unread":true,"content":"We just shipped a new feature in NextDNS: Bypass Age Verification.<p>More and more sites (especially adult ones) are now forcing users to upload IDs or selfies to continue. We think that‚Äôs a terrible idea: handing over government documents to random sites is a huge privacy risk.</p><p>This new setting workarounds those verification flows via DNS tricks. It‚Äôs available today to all users, including free accounts.</p><p>We‚Äôre curious how the HN community feels about this. Is it the right way to protect privacy online, or will it just provoke regulators to push harder?</p>","contentLength":561,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44931824"},{"title":"Derivatives, Gradients, Jacobians and Hessians","url":"https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/","date":1755439698,"author":"ibobev","guid":230747,"unread":true,"content":"\n<p>Article URL: <a href=\"https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/\">https://blog.demofox.org/2025/08/16/derivatives-gradients-jacobians-and-hessians-oh-my/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44931666\">https://news.ycombinator.com/item?id=44931666</a></p>\n<p>Points: 260</p>\n<p># Comments: 64</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Nim?","url":"https://undefined.pyfy.ch/why-nim","date":1755437309,"author":"TheWiggles","guid":230746,"unread":true,"content":"\n<p>Article URL: <a href=\"https://undefined.pyfy.ch/why-nim\">https://undefined.pyfy.ch/why-nim</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44931415\">https://news.ycombinator.com/item?id=44931415</a></p>\n<p>Points: 143</p>\n<p># Comments: 152</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Epson MX-80 Fonts","url":"https://mw.rat.bz/MX-80/","date":1755436701,"author":"m_walden","guid":235687,"unread":true,"content":"\n<p>Article URL: <a href=\"https://mw.rat.bz/MX-80/\">https://mw.rat.bz/MX-80/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44931371\">https://news.ycombinator.com/item?id=44931371</a></p>\n<p>Points: 155</p>\n<p># Comments: 57</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Faster Index I/O with NVMe SSDs","url":"https://www.marginalia.nu/log/a_123_index_io/","date":1755436640,"author":"ingve","guid":230745,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.marginalia.nu/log/a_123_index_io/\">https://www.marginalia.nu/log/a_123_index_io/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44931368\">https://news.ycombinator.com/item?id=44931368</a></p>\n<p>Points: 143</p>\n<p># Comments: 22</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BBC Micro, ancestor to ARM","url":"https://retrogamecoders.com/bbc-micro-the-ancestor-to-a-device-you-are-guaranteed-to-own/","date":1755436124,"author":"ingve","guid":230744,"unread":true,"content":"\n<p>Article URL: <a href=\"https://retrogamecoders.com/bbc-micro-the-ancestor-to-a-device-you-are-guaranteed-to-own/\">https://retrogamecoders.com/bbc-micro-the-ancestor-to-a-device-you-are-guaranteed-to-own/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44931321\">https://news.ycombinator.com/item?id=44931321</a></p>\n<p>Points: 115</p>\n<p># Comments: 96</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LL3M: Large Language 3D Modelers","url":"https://threedle.github.io/ll3m/","date":1755429620,"author":"simonpure","guid":230743,"unread":true,"content":"\n<p>Article URL: <a href=\"https://threedle.github.io/ll3m/\">https://threedle.github.io/ll3m/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44930808\">https://news.ycombinator.com/item?id=44930808</a></p>\n<p>Points: 410</p>\n<p># Comments: 173</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Node.js is able to execute TypeScript files without additional configuration","url":"https://nodejs.org/en/blog/release/v22.18.0","date":1755410607,"author":"steren","guid":230742,"unread":true,"content":"\n<p>Article URL: <a href=\"https://nodejs.org/en/blog/release/v22.18.0\">https://nodejs.org/en/blog/release/v22.18.0</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44929260\">https://news.ycombinator.com/item?id=44929260</a></p>\n<p>Points: 374</p>\n<p># Comments: 214</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Passive Microwave Repeaters","url":"https://computer.rip/2025-08-16-passive-microwave-repeaters.html","date":1755390327,"author":"BallsInIt","guid":233552,"unread":true,"content":"\n<p>Article URL: <a href=\"https://computer.rip/2025-08-16-passive-microwave-repeaters.html\">https://computer.rip/2025-08-16-passive-microwave-repeaters.html</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44927953\">https://news.ycombinator.com/item?id=44927953</a></p>\n<p>Points: 85</p>\n<p># Comments: 12</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An IRC-Enabled Lawn Mower (2021)","url":"https://jotunheimr.idlerpg.net/users/jotun/lawnmower/","date":1755388115,"author":"rickcarlino","guid":233211,"unread":true,"content":"\n<p>Article URL: <a href=\"https://jotunheimr.idlerpg.net/users/jotun/lawnmower/\">https://jotunheimr.idlerpg.net/users/jotun/lawnmower/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44927778\">https://news.ycombinator.com/item?id=44927778</a></p>\n<p>Points: 103</p>\n<p># Comments: 21</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"X-ray scans reveal Buddhist prayers inside tiny Tibetan scrolls","url":"https://www.popsci.com/technology/tibetan-prayer-scroll-scans/","date":1755377694,"author":"Hooke","guid":233149,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.popsci.com/technology/tibetan-prayer-scroll-scans/\">https://www.popsci.com/technology/tibetan-prayer-scroll-scans/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44926855\">https://news.ycombinator.com/item?id=44926855</a></p>\n<p>Points: 153</p>\n<p># Comments: 57</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Show HN: unsafehttp ‚Äì tiny web server from scratch in C, running on an orange pi","url":"http://unsafehttp.benren.au/","date":1755377175,"author":"GSGBen","guid":230756,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44926785"},{"title":"Show HN: Lue ‚Äì Terminal eBook Reader with Text-to-Speech","url":"https://github.com/superstarryeyes/lue","date":1755367233,"author":"superstarryeyes","guid":231742,"unread":true,"content":"<p>Shown HN: Lue - Terminal eBook Reader with Text-to-Speech</p><p>Just went live on GitHub with this project.</p><p>I really enjoy listening to my eBooks as audiobooks but was frustrated by the available options. Converting books into audiobooks with scripts is tedious, and most tools stumble over footnotes, headers, or formatting. I wanted something simple: just throw a book at it, and it starts reading immediately without any clicking or loading.</p><p>I also wanted it to be customizable and modular because new, better TTS engines are released all the time. For this initial release, I settled on Edge and Kokoro because they‚Äôre both fast (real-time) and good quality. I‚Äôve already made modules for Kitten TTS, Gemini and a few others, and they work too. So I hope this setup is future-proof.</p><p>Here‚Äôs what Lue supports:</p><p>Multi-format: EPUB, PDF, TXT, DOCX, HTML, RTF, and Markdown.</p><p>Modular TTS system: Default Edge TTS (online) and Kokoro TTS (offline/local), with an architecture to add more models.</p><p>Rich terminal UI: Full keyboard and mouse support, customizable color themes, smooth scrolling.</p><p>Smart persistence: Automatically saves reading progress across sessions.</p><p>Cross-platform &amp; multilingual: macOS, Linux, Windows, supporting 100+ languages.</p><p>I‚Äôd love feedback on both usability and the TTS experience. Are there any features you wish it had?</p>","contentLength":1334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44925597"},{"title":"Show HN: I built an app to block Shorts and Reels","url":"https://scrollguard.app/","date":1755352898,"author":"adrianhacar","guid":231610,"unread":true,"content":"<p> on Instagram, Facebook, Reddit, YouTube.\n        Set scrolling limits on  with antiscroll mode.\n        No Ads, No Reels, No Shorts, No Distractions.\n      </p><div><div>\n          iOS has some limitations,so it‚Äôs not technically possible to block Reels and Shorts the same way as on Android. Although it can't be done the same way, I‚Äôm building an iPhone app with a different approach to help cut down on scrolling addiction. Drop your email and I‚Äôll let you know when is launched!\n        </div></div>","contentLength":484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=44923520"},{"title":"Walkie-Textie Wireless Communicator","url":"http://www.technoblogy.com/show?2AON","date":1755346125,"author":"chrisjj","guid":231653,"unread":true,"content":"\n<p>Article URL: <a href=\"http://www.technoblogy.com/show?2AON\">http://www.technoblogy.com/show?2AON</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44922540\">https://news.ycombinator.com/item?id=44922540</a></p>\n<p>Points: 166</p>\n<p># Comments: 105</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Figma‚Äôs multiplayer technology works (2019)","url":"https://www.figma.com/blog/how-figmas-multiplayer-technology-works/","date":1755344462,"author":"redbell","guid":233491,"unread":true,"content":"\n<p>Article URL: <a href=\"https://www.figma.com/blog/how-figmas-multiplayer-technology-works/\">https://www.figma.com/blog/how-figmas-multiplayer-technology-works/</a></p>\n<p>Comments URL: <a href=\"https://news.ycombinator.com/item?id=44922362\">https://news.ycombinator.com/item?id=44922362</a></p>\n<p>Points: 140</p>\n<p># Comments: 45</p>\n","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["hn"]}