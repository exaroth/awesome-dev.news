{"id":"7caSwCnanpF82DyESN","title":"Official News","displayTitle":"Official News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":13,"items":[{"title":"Introducing Impressions at Netflix","url":"https://netflixtechblog.com/introducing-impressions-at-netflix-e2b67c88c9fb?source=rss----2615bd06b42e---4","date":1739582000,"author":"Netflix Technology Blog","guid":31,"unread":true,"content":"<h4>Part 1: Creating the Source of Truth for Impressions</h4><p>Imagine scrolling through Netflix, where each movie poster or promotional banner competes for your attention. Every image you hover over isn‚Äôt just a visual placeholder; it‚Äôs a critical data point that fuels our sophisticated personalization engine. At Netflix, we call these images ‚Äòimpressions,‚Äô and they play a pivotal role in transforming your interaction from simple browsing into an immersive binge-watching experience, all tailored to your unique&nbsp;tastes.</p><p>Capturing these moments and turning them into a personalized journey is no simple feat. It requires a state-of-the-art system that can track and process these impressions while maintaining a detailed history of each profile‚Äôs exposure. This nuanced integration of data and technology empowers us to offer bespoke content recommendations.</p><p>In this multi-part blog series, we take you behind the scenes of our system that processes billions of impressions daily. We will explore the challenges we encounter and unveil how we are building a resilient solution that transforms these client-side impressions into a personalized content discovery experience for every Netflix&nbsp;viewer.</p><h3>Why do we need impression history?</h3><p>To tailor recommendations more effectively, it‚Äôs crucial to track what content a user has already encountered. Having impression history helps us achieve this by allowing us to identify content that has been displayed on the homepage but not engaged with, helping us deliver fresh, engaging recommendations.</p><p>By maintaining a history of impressions, we can implement frequency capping to prevent over-exposure to the same content. This ensures users aren‚Äôt repeatedly shown identical options, keeping the viewing experience vibrant and reducing the risk of frustration or disengagement.</p><h4>Highlighting New&nbsp;Releases</h4><p>For new content, impression history helps us monitor initial user interactions and adjust our merchandising efforts accordingly. We can experiment with different content placements or promotional strategies to boost visibility and engagement.</p><p>Additionally, impression history offers insightful information for addressing a number of platform-related analytics queries. Analyzing impression history, for example, might help determine how well a specific row on the home page is functioning or assess the effectiveness of a merchandising strategy.</p><p>The first pivotal step in managing impressions begins with the creation of a Source-of-Truth (SOT) dataset. This foundational dataset is essential, as it supports various downstream workflows and enables a multitude of use&nbsp;cases.</p><h4>Collecting Raw Impression Events</h4><p>As Netflix members explore our platform, their interactions with the user interface spark a vast array of raw events. These events are promptly relayed from the client side to our servers, entering a centralized event processing queue. This queue ensures we are consistently capturing raw events from our global user&nbsp;base.</p><p>After raw events are collected into a centralized queue, a custom event extractor processes this data to identify and extract all impression events. These extracted events are then routed to an Apache Kafka topic for immediate processing needs and simultaneously stored in an Apache Iceberg table for long-term retention and historical analysis. This dual-path approach leverages Kafka‚Äôs capability for low-latency streaming and Iceberg‚Äôs efficient management of large-scale, immutable datasets, ensuring both real-time responsiveness and comprehensive historical data availability.</p><h4>Filtering &amp; Enriching Raw Impressions</h4><p>Once the raw impression events are queued, a stateless Apache Flink job takes charge, meticulously processing this data. It filters out any invalid entries and enriches the valid ones with additional metadata, such as show or movie title details, and the specific page and row location where each impression was presented to users. This refined output is then structured using an Avro schema, establishing a definitive source of truth for Netflix‚Äôs impression data. The enriched data is seamlessly accessible for both real-time applications via Kafka and historical analysis through storage in an Apache Iceberg table. This dual availability ensures immediate processing capabilities alongside comprehensive long-term data retention.</p><h4>Ensuring High Quality Impressions</h4><p>Maintaining the highest quality of impressions is a top priority. We accomplish this by gathering detailed column-level metrics that offer insights into the state and quality of each impression. These metrics include everything from validating identifiers to checking that essential columns are properly filled. The data collected feeds into a comprehensive quality dashboard and supports a tiered threshold-based alerting system. These alerts promptly notify us of any potential issues, enabling us to swiftly address regressions. Additionally, while enriching the data, we ensure that all columns are in agreement with each other, offering in-place corrections wherever possible to deliver accurate&nbsp;data.</p><p>We handle a staggering volume of 1 to 1.5 million impression events globally every second, with each event approximately 1.2KB in size. To efficiently process this massive influx in real-time, we employ Apache Flink for its low-latency stream processing capabilities, which seamlessly integrates both batch and stream processing to facilitate efficient backfilling of historical data and ensure consistency across real-time and historical analyses. Our Flink configuration includes 8 task managers per region, each equipped with 8 CPU cores and 32GB of memory, operating at a parallelism of 48, allowing us to handle the necessary scale and speed for seamless performance delivery. The Flink job‚Äôs sink is equipped with a data mesh connector, as detailed in our <a href=\"https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873\">Data Mesh platform</a> which has two outputs: Kafka and Iceberg. This setup allows for efficient streaming of real-time data through Kafka and the preservation of historical data in Iceberg, providing a comprehensive and flexible data processing and storage solution.</p><p>We utilize the ‚Äòisland model‚Äô for deploying our Flink jobs, where all dependencies for a given application reside within a single region. This approach ensures high availability by isolating regions, so if one becomes degraded, others remain unaffected, allowing traffic to be shifted between regions to maintain service continuity. Thus, all data in one region is processed by the Flink job deployed within that&nbsp;region.</p><h4>Addressing the Challenge of Unschematized Events</h4><p>Allowing raw events to land on our centralized processing queue unschematized offers significant flexibility, but it also introduces challenges. Without a defined schema, it can be difficult to determine whether missing data was intentional or due to a logging error. We are investigating solutions to introduce schema management that maintains flexibility while providing clarity.</p><h4>Automating Performance Tuning with Autoscalers</h4><p>Tuning the performance of our Apache Flink jobs is currently a manual process. The next step is to integrate with autoscalers, which can dynamically adjust resources based on workload demands. This integration will not only optimize performance but also ensure more efficient resource utilization.</p><h4>Improving Data Quality&nbsp;Alerts</h4><p>Right now, there‚Äôs a lot of business rules dictating when a data quality alert needs to be fired. This leads to a lot of false positives that require manual judgement. A lot of times it is difficult to track changes leading to regression due to inadequate data lineage information. We are investing in building a comprehensive data quality platform that more intelligently identifies anomalies in our impression stream, keeps track of data lineage and data governance, and also, generates alerts notifying producers of any regressions. This approach will enhance efficiency, reduce manual oversight, and ensure a higher standard of data integrity.</p><p>Creating a reliable source of truth for impressions is a complex but essential task that enhances personalization and discovery experience. Stay tuned for the next part of this series, where we‚Äôll delve into how we use this SOT dataset to create a microservice that provides impression histories. We invite you to share your thoughts in the comments and continue with us on this journey of discovering impressions.</p><p>We are genuinely grateful to our amazing colleagues whose contributions were essential to the success of Impressions: Julian Jaffe, Bryan Keller, Yun Wang, Brandon Bremen, Kyle Alford, Ron Brown and Shriya&nbsp;Arora.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=e2b67c88c9fb\" width=\"1\" height=\"1\" alt=\"\">","contentLength":8607,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Support the open source projects you love this Valentine‚Äôs Day","url":"https://github.blog/open-source/support-the-open-source-projects-you-love-this-valentines-day/","date":1739552404,"author":"Martin Woodward","guid":38,"unread":true,"content":"<p>While everyone likes flowers and chocolates, why not show your love for your favorite open source projects this Valentine‚Äôs and give appreciation to the maintainers who keep them running?</p><p>Many of the open source projects we rely on every day are maintained by dedicated volunteers. Sponsoring projects isn‚Äôt just about altruism, it‚Äôs about investing in the future. Many maintainers work on open source projects in their spare time. Sponsorships can help them dedicate more time to the projects you depend on. With financial support, maintainers can help cover costs such as development and hosting.</p><p>When you sponsor a maintainer, you‚Äôre helping them continue their work and letting them know you value the creativity and love they‚Äôve poured into their project. For this Valentine‚Äôs Day, let‚Äôs show love to the maintainers who keep our favorite projects alive and thriving.</p><p><strong>The benefits of financial sponsorship</strong></p><p>Sponsorship is a tangible way to show support for the open source community. It can be a huge morale boost, as well as bring visibility and validation. Sponsored projects often gain more attention, leading to a virtuous cycle of more contributors and users, which results in better software for everyone.</p><h2>Getting started: How to best invest in open source<a href=\"https://github.blog/open-source/support-the-open-source-projects-you-love-this-valentines-day/#getting-started-how-to-best-invest-in-open-source\" aria-label=\"Getting started: How to best invest in open source\"></a></h2><ol><li><strong>Identify critical dependencies:</strong><a href=\"https://docs.github.com/en/sponsors/getting-started-with-github-sponsors/navigating-your-sponsors-dashboard#viewing-dependencies\">Review your project‚Äôs dependencies</a> to identify which open source libraries and tools are crucial to your operations. If you use tools like npm, pip, or maven, you can review their dependencies and prioritize which are critical to your project‚Äôs success.  </li><li><strong>Evaluate the project activity:</strong> Check the recent and trending activity of repositories and look for signs of contributions, shipped features, bug fixes, active maintenance, and community engagement. Helpful metrics like recent commits, issue resolution, and community engagement are all good markers for where funding could help. Projects with a high impact but low funding could be great candidates to prioritize for sponsorship.  </li><li> Take the opportunity to look at the issues and contribution guidelines. Try to understand their funding needs and how your support makes a difference. But also consider investing in ways beyond just financial support. You could provide bug fixes if you feel confident in your code. You could also take the time to tell someone about the project and why you think it‚Äôs awesome. </li></ol><p>Maintainers can show love for their sponsors as well. Sponsors appreciate knowing their contribution is making a difference. Whether it‚Äôs a simple mention on social media to acknowledge their contribution, featuring them on your GitHub Sponsors profile, exclusive updates and behind-the-scenes insights about your project and community, or even a brief personalized thank-you message, tokens of appreciation can help bring more funding your way.</p><p>This Valentine‚Äôs Day, let‚Äôs show our favorite projects a bit more appreciation. Whether it‚Äôs a heartfelt contribution to fix bugs or docs, spreading the word about why it‚Äôs a great project and helping people use it, or a sweet one-time donation, every bit helps keep our beloved digital world turning.</p><p>Ready to start sponsoring? Visit <a href=\"https://github.com/sponsors\">GitHub Sponsors</a> and find the projects that matter most to you.</p><p>Happy Valentine‚Äôs Day to all the open source contributors and supporters out there! And remember, you don‚Äôt need to wait for a special occasion like Valentine‚Äôs Day to show your appreciation for maintainers‚Äîevery day is a perfect day to support open source! üíñ</p>","contentLength":3497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unlocking global AI potential with next-generation subsea infrastructure","url":"https://engineering.fb.com/2025/02/14/connectivity/project-waterworth-ai-subsea-infrastructure/","date":1739550486,"author":"","guid":408,"unread":true,"content":"<p><a href=\"https://globaldigitalinclusion.org/wp-content/uploads/2024/01/GDIP-Good-Practices-for-Subsea-Cables-Policy-Investing-in-Digital-Inclusion.pdf\"></a></p><p><a href=\"https://engineering.fb.com/2021/03/28/connectivity/echo-bifrost/\"></a><a href=\"https://engineering.fb.com/2021/09/28/connectivity/2africa-pearls/\"></a></p>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Cloud Controller Manager Chicken and Egg Problem","url":"https://kubernetes.io/blog/2025/02/14/cloud-controller-manager-chicken-egg-problem/","date":1739491200,"author":"","guid":406,"unread":true,"content":"<p>Kubernetes 1.31\n<a href=\"https://kubernetes.io/blog/2024/05/20/completing-cloud-provider-migration/\">completed the largest migration in Kubernetes history</a>, removing the in-tree\ncloud provider. While the component migration is now done, this leaves some additional\ncomplexity for users and installer projects (for example, kOps or Cluster API) . We will go\nover those additional steps and failure points and make recommendations for cluster owners.\nThis migration was complex and some logic had to be extracted from the core components,\nbuilding four new subsystems.</p><p>One of the most critical functionalities of the cloud controller manager is the node controller,\nwhich is responsible for the initialization of the nodes.</p><p>As you can see in the following diagram, when the  starts, it registers the \nobject with the apiserver, Tainting the node so it can be processed first by the\ncloud-controller-manager. The initial  is missing the cloud-provider specific information,\nlike the Node Addresses and the Labels with the cloud provider specific information like the\nNode, Region and Instance type information.</p><div>sequenceDiagram\nautonumber\nrect rgb(191, 223, 255)\nKubelet-&gt;&gt;+Kube-apiserver: Create Node\nNote over Kubelet: Taint: node.cloudprovider.kubernetes.io\nKube-apiserver-&gt;&gt;-Kubelet: Node Created\nend\nNote over Kube-apiserver: Node is Not Ready<p> Tainted, Missing Node Addresses*, ...\nNote over Kube-apiserver: Send Updates\nrect rgb(200, 150, 255)\nKube-apiserver-&gt;&gt;+Cloud-controller-manager: Watch: New Node Created\nNote over Cloud-controller-manager: Initialize Node:</p>Cloud Provider Labels, Node Addresses, ...\nCloud-controller-manager-&gt;&gt;-Kube-apiserver: Update Node\nend\nNote over Kube-apiserver: Node is Ready\n</div><p>This new initialization process adds some latency to the node readiness. Previously, the kubelet\nwas able to initialize the node at the same time it created the node. Since the logic has moved\nto the cloud-controller-manager, this can cause a <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#chicken-and-egg\">chicken and egg problem</a>\nduring the cluster bootstrapping for those Kubernetes architectures that do not deploy the\ncontroller manager as the other components of the control plane, commonly as static pods,\nstandalone binaries or daemonsets/deployments with tolerations to the taints and using\n (more on this below)</p><h2>Examples of the dependency problem</h2><p>As noted above, it is possible during bootstrapping for the cloud-controller-manager to be\nunschedulable and as such the cluster will not initialize properly. The following are a few\nconcrete examples of how this problem can be expressed and the root causes for why they might\noccur.</p><p>These examples assume you are running your cloud-controller-manager using a Kubernetes resource\n(e.g. Deployment, DaemonSet, or similar) to control its lifecycle. Because these methods\nrely on Kubernetes to schedule the cloud-controller-manager, care must be taken to ensure it\nwill schedule properly.</p><h3>Example: Cloud controller manager not scheduling due to uninitialized taint</h3><p>As <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/#running-cloud-controller-manager\">noted in the Kubernetes documentation</a>, when the kubelet is started with the command line\nflag <code>--cloud-provider=external</code>, its corresponding  object will have a no schedule taint\nnamed <code>node.cloudprovider.kubernetes.io/uninitialized</code> added. Because the cloud-controller-manager\nis responsible for removing the no schedule taint, this can create a situation where a\ncloud-controller-manager that is being managed by a Kubernetes resource, such as a \nor , may not be able to schedule.</p><p>If the cloud-controller-manager is not able to be scheduled during the initialization of the\ncontrol plane, then the resulting  objects will all have the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> no schedule taint. It also means that this taint\nwill not be removed as the cloud-controller-manager is responsible for its removal. If the no\nschedule taint is not removed, then critical workloads, such as the container network interface\ncontrollers, will not be able to schedule, and the cluster will be left in an unhealthy state.</p><h3>Example: Cloud controller manager not scheduling due to not-ready taint</h3><p>The next example would be possible in situations where the container network interface (CNI) is\nwaiting for IP address information from the cloud-controller-manager (CCM), and the CCM has not\ntolerated the taint which would be removed by the CNI.</p><blockquote><p>\"The Node controller detects whether a Node is ready by monitoring its health and adds or removes this taint accordingly.\"</p></blockquote><p>One of the conditions that can lead to a  resource having this taint is when the container\nnetwork has not yet been initialized on that node. As the cloud-controller-manager is responsible\nfor adding the IP addresses to a  resource, and the IP addresses are needed by the container\nnetwork controllers to properly configure the container network, it is possible in some\ncircumstances for a node to become stuck as not ready and uninitialized permanently.</p><p>This situation occurs for a similar reason as the first example, although in this case, the\n<code>node.kubernetes.io/not-ready</code> taint is used with the no execute effect and thus will cause the\ncloud-controller-manager not to run on the node with the taint. If the cloud-controller-manager is\nnot able to execute, then it will not initialize the node. It will cascade into the container\nnetwork controllers not being able to run properly, and the node will end up carrying both the\n<code>node.cloudprovider.kubernetes.io/uninitialized</code> and <code>node.kubernetes.io/not-ready</code> taints,\nleaving the cluster in an unhealthy state.</p><p>There is no one ‚Äúcorrect way‚Äù to run a cloud-controller-manager. The details will depend on the\nspecific needs of the cluster administrators and users. When planning your clusters and the\nlifecycle of the cloud-controller-managers please consider the following guidance:</p><p>For cloud-controller-managers running in the same cluster, they are managing.</p><ol><li>Use host network mode, rather than the pod network: in most cases, a cloud controller manager\nwill need to communicate with an API service endpoint associated with the infrastructure.\nSetting ‚ÄúhostNetwork‚Äù to true will ensure that the cloud controller is using the host\nnetworking instead of the container network and, as such, will have the same network access as\nthe host operating system. It will also remove the dependency on the networking plugin. This\nwill ensure that the cloud controller has access to the infrastructure endpoint (always check\nyour networking configuration against your infrastructure provider‚Äôs instructions).</li><li>Use a scalable resource type.  and  are useful for controlling the\nlifecycle of a cloud controller. They allow easy access to running multiple copies for redundancy\nas well as using the Kubernetes scheduling to ensure proper placement in the cluster. When using\nthese primitives to control the lifecycle of your cloud controllers and running multiple\nreplicas, you must remember to enable leader election, or else your controllers will collide\nwith each other which could lead to nodes not being initialized in the cluster.</li><li>Target the controller manager containers to the control plane. There might exist other\ncontrollers which need to run outside the control plane (for example, Azure‚Äôs node manager\ncontroller). Still, the controller managers themselves should be deployed to the control plane.\nUse a node selector or affinity stanza to direct the scheduling of cloud controllers to the\ncontrol plane to ensure that they are running in a protected space. Cloud controllers are vital\nto adding and removing nodes to a cluster as they form a link between Kubernetes and the\nphysical infrastructure. Running them on the control plane will help to ensure that they run\nwith a similar priority as other core cluster controllers and that they have some separation\nfrom non-privileged user workloads.\n<ol><li>It is worth noting that an anti-affinity stanza to prevent cloud controllers from running\non the same host is also very useful to ensure that a single node failure will not degrade\nthe cloud controller performance.</li></ol></li><li>Ensure that the tolerations allow operation. Use tolerations on the manifest for the cloud\ncontroller container to ensure that it will schedule to the correct nodes and that it can run\nin situations where a node is initializing. This means that cloud controllers should tolerate\nthe <code>node.cloudprovider.kubernetes.io/uninitialized</code> taint, and it should also tolerate any\ntaints associated with the control plane (for example, <code>node-role.kubernetes.io/control-plane</code>\nor <code>node-role.kubernetes.io/master</code>). It can also be useful to tolerate the\n<code>node.kubernetes.io/not-ready</code> taint to ensure that the cloud controller can run even when the\nnode is not yet available for health monitoring.</li></ol><p>For cloud-controller-managers that will not be running on the cluster they manage (for example,\nin a hosted control plane on a separate cluster), then the rules are much more constrained by the\ndependencies of the environment of the cluster running the cloud-controller-manager. The advice\nfor running on a self-managed cluster may not be appropriate as the types of conflicts and network\nconstraints will be different. Please consult the architecture and requirements of your topology\nfor these scenarios.</p><p>This is an example of a Kubernetes Deployment highlighting the guidance shown above. It is\nimportant to note that this is for demonstration purposes only, for production uses please\nconsult your cloud provider‚Äôs documentation.</p><pre tabindex=\"0\"><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nname: cloud-controller-manager\nnamespace: kube-system\nspec:\nreplicas: 2\nselector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\nstrategy:\ntype: Recreate\ntemplate:\nmetadata:\nlabels:\napp.kubernetes.io/name: cloud-controller-manager\nannotations:\nkubernetes.io/description: Cloud controller manager for my infrastructure\nspec:\ncontainers: # the container details will depend on your specific cloud controller manager\n- name: cloud-controller-manager\ncommand:\n- /bin/my-infrastructure-cloud-controller-manager\n- --leader-elect=true\n- -v=1\nimage: registry/my-infrastructure-cloud-controller-manager@latest\nresources:\nrequests:\ncpu: 200m\nmemory: 50Mi\nhostNetwork: true # these Pods are part of the control plane\nnodeSelector:\nnode-role.kubernetes.io/control-plane: \"\"\naffinity:\npodAntiAffinity:\nrequiredDuringSchedulingIgnoredDuringExecution:\n- topologyKey: \"kubernetes.io/hostname\"\nlabelSelector:\nmatchLabels:\napp.kubernetes.io/name: cloud-controller-manager\ntolerations:\n- effect: NoSchedule\nkey: node-role.kubernetes.io/master\noperator: Exists\n- effect: NoExecute\nkey: node.kubernetes.io/unreachable\noperator: Exists\ntolerationSeconds: 120\n- effect: NoExecute\nkey: node.kubernetes.io/not-ready\noperator: Exists\ntolerationSeconds: 120\n- effect: NoSchedule\nkey: node.cloudprovider.kubernetes.io/uninitialized\noperator: Exists\n- effect: NoSchedule\nkey: node.kubernetes.io/not-ready\noperator: Exists\n</code></pre><p>When deciding how to deploy your cloud controller manager it is worth noting that\ncluster-proportional, or resource-based, pod autoscaling is not recommended. Running multiple\nreplicas of a cloud controller manager is good practice for ensuring high-availability and\nredundancy, but does not contribute to better performance. In general, only a single instance\nof a cloud controller manager will be reconciling a cluster at any given time.</p>","contentLength":11247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Looking back at our Bug Bounty program in 2024","url":"https://engineering.fb.com/2025/02/13/security/looking-back-at-our-bug-bounty-program-in-2024/","date":1739466046,"author":"","guid":407,"unread":true,"content":"<ul><li aria-level=\"1\"><a href=\"https://about.fb.com/news/2019/01/designing-security-for-billions/\" target=\"_blank\" rel=\"noopener\"></a></li></ul><h2></h2><ul></ul><h2></h2><h2></h2><p><b>Ads audience tools designed to help people choose a target audience for their ads: </b><a href=\"https://bugbounty.meta.com/payout-guidelines/ads-audience/\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://www.facebook.com/business/help/717368264947302\" target=\"_blank\" rel=\"noopener\"></a><a href=\"https://bugbounty.meta.com/payout-guidelines/ads-audience/\" target=\"_blank\" rel=\"noopener\"></a></p><p><b>Mixed reality hardware products:</b><a href=\"http://hardwear.io\" target=\"_blank\" rel=\"noopener\"></a></p><h2></h2><p><b>Organizing community events and presenting joint research:</b></p><p><b>Providing resources and timely updates for the research community:</b></p>","contentLength":239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Extensible Wasm Applications with Go","url":"https://go.dev/blog/wasmexport","date":1739404800,"author":"Cherry Mui","guid":202,"unread":true,"content":"<p>\n      Cherry Mui\n      13 February 2025\n      </p><p>Go 1.24 enhances its WebAssembly (Wasm) capabilities with the\naddition of the  directive and the ability to build a reactor\nfor WebAssembly System Interface (WASI).\nThese features enable Go developers to export Go functions to Wasm,\nfacilitating better integration with Wasm hosts and expanding the possibilities\nfor Go-based Wasm applications.</p><h2>WebAssembly and the WebAssembly System Interface</h2><p><a href=\"https://webassembly.org/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly (Wasm)</a> is a binary instruction format\nthat was initially created for web browsers, providing the execution of\nhigh-performance, low-level code at speeds approaching native performance.\nSince then, Wasm‚Äôs utility has expanded, and it is now used in various\nenvironments beyond the browser.\nNotably, cloud providers offer services that directly execute Wasm\nexecutables, taking advantage of the\n<a href=\"https://wasi.dev/\" rel=\"noreferrer\" target=\"_blank\">WebAssembly System Interface (WASI)</a> system call API.\nWASI allows these executables to interact with system resources.</p><p>Go first added support for compiling to Wasm in the 1.11 release, through the\n port.\nGo 1.21 added a new port targeting the WASI preview 1 syscall API through the\nnew  port.</p><h2>Exporting Go Functions to Wasm with </h2><p>Go 1.24 introduces a new compiler directive, , which allows\ndevelopers to export Go functions to be called from outside of the\nWasm module, typically from a host application that runs the Wasm runtime.\nThis directive instructs the compiler to make the annotated function available\nas a Wasm <a href=\"https://webassembly.github.io/spec/core/valid/modules.html?highlight=export#exports\" rel=\"noreferrer\" target=\"_blank\">export</a>\nin the resulting Wasm binary.</p><p>To use the  directive, simply add it to a function definition:</p><pre><code>//go:wasmexport add\nfunc add(a, b int32) int32 { return a + b }\n</code></pre><p>With this, the Wasm module will have an exported function named  that\ncan be called from the host.</p><p>This is analogous to the <a href=\"https://go.dev/cmd/cgo#hdr-C_references_to_Go\">cgo  directive</a>,\nwhich makes the function available to be called from C,\nthough  uses a different, simpler mechanism.</p><p>A WASI reactor is a WebAssembly module that operates continuously, and\ncan be called upon multiple times to react on events or requests.\nUnlike a ‚Äúcommand‚Äù module, which terminates after its main function finishes,\na reactor instance remains live after initialization, and its exports remain\naccessible.</p><p>With Go 1.24, one can build a WASI reactor with the  build\nflag.</p><pre><code>$ GOOS=wasip1 GOARCH=wasm go build -buildmode=c-shared -o reactor.wasm\n</code></pre><p>The build flag signals to the linker not to generate the  function\n(the entry point for a command module), and instead generate an\n function, which performs runtime and package initialization,\nalong with any exported functions and their dependencies.\nThe  function must be called before any other exported functions.\nThe  function will not be automatically invoked.</p><p>To use a WASI reactor, the host application first initializes it by calling\n, then simply invoke the exported functions.\nHere is an example using <a href=\"https://wazero.io/\" rel=\"noreferrer\" target=\"_blank\">Wazero</a>, a Go-based Wasm runtime\nimplementation:</p><pre><code>// Create a Wasm runtime, set up WASI.\nr := wazero.NewRuntime(ctx)\ndefer r.Close(ctx)\nwasi_snapshot_preview1.MustInstantiate(ctx, r)\n\n// Configure the module to initialize the reactor.\nconfig := wazero.NewModuleConfig().WithStartFunctions(\"_initialize\")\n\n// Instantiate the module.\nwasmModule, _ := r.InstantiateWithConfig(ctx, wasmFile, config)\n\n// Call the exported function.\nfn := wasmModule.ExportedFunction(\"add\")\nvar a, b int32 = 1, 2\nres, _ := fn.Call(ctx, api.EncodeI32(a), api.EncodeI32(b))\nc := api.DecodeI32(res[0])\nfmt.Printf(\"add(%d, %d) = %d\\n\", a, b, c)\n\n// The instance is still alive. We can call the function again.\nres, _ = fn.Call(ctx, api.EncodeI32(b), api.EncodeI32(c))\nfmt.Printf(\"add(%d, %d) = %d\\n\", b, c, api.DecodeI32(res[0]))\n</code></pre><p>The  directive and the reactor build mode allow applications to\nbe extended by calling into Go-based Wasm code.\nThis is particularly valuable for applications that have adopted Wasm as a\nplugin or extension mechanism with well-defined interfaces.\nBy exporting Go functions, applications can leverage the Go Wasm modules to\nprovide functionality without needing to recompile the entire application.\nFurthermore, building as a reactor ensures that the exported functions can be\ncalled multiple times without requiring reinitialization, making it suitable\nfor long-running applications or services.</p><h2>Supporting rich types between the host and the client</h2><p>Go 1.24 also relaxes the constraints on types that can be used as input and\nresult parameters with  functions.\nFor example, one can pass a bool, a string, a pointer to an , or a\npointer to a struct which embeds  and contains supported\nfield types\n(see the <a href=\"https://go.dev/cmd/compile#hdr-WebAssembly_Directives\">documentation</a> for detail).\nThis allows Go Wasm applications to be written in a more natural and ergonomic\nway, and removes some unnecessary type conversions.</p><p>While Go 1.24 has made significant enhancements to its Wasm capabilities,\nthere are still some notable limitations.</p><p>Wasm is a single-threaded architecture with no parallelism.\nA  function can spawn new goroutines.\nBut if a function creates a background goroutine, it will not continue\nexecuting when the  function returns, until calling back into\nthe Go-based Wasm module.</p><p>While some type restrictions have been relaxed in Go 1.24, there are still\nlimitations on the types that can be used with  and\n functions.\nDue to the unfortunate mismatch between the 64-bit architecture of the client\nand the 32-bit architecture of the host, it is not possible to pass pointers in\nmemory.\nFor example, a  function cannot take a pointer to a struct that\ncontains a pointer-typed field.</p><p>The addition of the ability to build a WASI reactor and export Go functions to\nWasm in Go 1.24 represent a significant step forward for Go‚Äôs WebAssembly\ncapabilities.\nThese features empower developers to create more versatile and powerful Go-based\nWasm applications, opening up new possibilities for Go in the Wasm ecosystem.</p>","contentLength":5777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"2024 State of Rust Survey Results","url":"https://blog.rust-lang.org/2025/02/13/2024-State-Of-Rust-Survey-results.html","date":1739404800,"author":"The Rust Survey Team","guid":200,"unread":true,"content":"<p>The Rust Survey Team is excited to share the results of our <a href=\"https://blog.rust-lang.org/2024/12/05/annual-survey-2024-launch.html\">2024 survey on the Rust Programming language</a>, conducted between December 5, 2024 and December 23, 2024.\nAs in previous years, the 2024 State of Rust Survey was focused on gathering insights and feedback from Rust users, and all those who are interested in the future of Rust more generally.</p><p>This ninth edition of the survey surfaced new insights and learning opportunities straight from the global Rust language community, which we will summarize below. In addition to this blog post,  containing charts with aggregated results of all questions in the survey.</p><p><strong>Our sincerest thanks to every community member who took the time to express their opinions and experiences with Rust over the past year. Your participation will help us make Rust better for everyone.</strong></p><p>There's a lot of data to go through, so strap in and enjoy!</p><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table><p>As shown above, in 2024, we have received fewer survey views than in the previous year. This was likely caused simply by the fact that the survey ran only for two weeks, while in the previous year it ran for almost a month. However, the completion rate has also dropped, which seems to suggest that the survey might be a bit too long. We will take this into consideration for the next edition of the survey.</p><p>The State of Rust survey not only gives us excellent insight into how many Rust users around the world are using and experiencing the language but also gives us insight into the makeup of our global community. This information gives us a sense of where the language is being used and where access gaps might exist for us to address over time. We hope that this data and our related analysis help further important discussions about how we can continue to prioritize global access and inclusivity in the Rust community.</p><p>Same as every year, we asked our respondents in which country they live in. The top 10 countries represented were, in order: United States (22%), Germany (14%), United Kingdom (6%), France (6%), China (5%), Canada (3%), Netherlands (3%), Russia (3%), Australia (2%), and Sweden (2%). We are happy to see that Rust is enjoyed by users from all around the world! You can try to find your country in the chart below:</p><p>We also asked whether respondents consider themselves members of a marginalized community. Out of those who answered, 74.5% selected no, 15.5% selected yes, and 10% preferred not to say.</p><p>We have asked the group that selected ‚Äúyes‚Äù which specific groups they identified as being a member of. The majority of those who consider themselves a member of an underrepresented or marginalized group in technology identify as lesbian, gay, bisexual, or otherwise non-heterosexual. The second most selected option was neurodivergent at 46% followed by trans at 35%.</p><p>Each year, we must acknowledge the diversity, equity, and inclusivity (DEI) related gaps in the Rust community and open source as a whole. We believe that excellent work is underway at the Rust Foundation to advance global access to Rust community gatherings and distribute grants to a diverse pool of maintainers each cycle, which you can learn more about <a href=\"https://rustfoundation.org/community\">here</a>. Even so, global inclusion and access is just one element of DEI, and the survey working group will continue to advocate for progress in this domain.</p><p>The number of respondents that self-identify as a Rust user was quite similar to last year, around 92%. This high number is not surprising, since we primarily target existing Rust developers with this survey.</p><p>Similarly as last year, around 31% of those who did not identify as Rust users cited the perception of difficulty as the primary reason for not using Rust. The most common reason for not using Rust was that the respondents simply haven‚Äôt had the chance to try it yet.</p><p>Of the former Rust users who participated in the 2024 survey, 36% cited factors outside their control as a reason why they no longer use Rust, which is a 10pp decrease from last year. This year, we also asked respondents if they would consider using Rust again if an opportunity comes up, which turns out to be true for a large fraction of the respondents (63%). That is good to hear!</p><blockquote><p>Closed answers marked with N/A were not present in the previous version(s) of the survey.</p></blockquote><p>Those not using Rust anymore told us that it is because they don't really need it (or the goals of their company changed) or because it was not the right tool for the job. A few reported being overwhelmed by the language or its ecosystem in general or that switching to or introducing Rust would have been too expensive in terms of human effort.</p><p>Of those who used Rust in 2024, 53% did so on a daily (or nearly daily) basis ‚Äî an increase of 4pp from the previous year. We can observe an upward trend in the frequency of Rust usage over the past few years, which suggests that Rust is being increasingly used at work. This is also confirmed by other answers mentioned in the Rust at Work section later below.</p><p>Rust expertise is also continually increasing amongst our respondents! 20% of respondents can write (only) simple programs in Rust (a decrease of 3pp from 2023), while 53% consider themselves productive using Rust ‚Äî up from 47% in 2023. While the survey is just one tool to measure the changes in Rust expertise overall, these numbers are heartening as they represent knowledge growth for many Rustaceans returning to the survey year over year.</p><p>Unsurprisingly, the most popular version of Rust is , either the most recent one or whichever comes with the users' Linux distribution. Almost a third of users also use the latest nightly release, due to various reasons (see below). However, it seems that the beta toolchain is not used much, which is a bit unfortunate. We would like to encourage Rust users to use the beta toolchain more (e.g. in CI environments) to help test soon-to-be stabilized versions of Rust.</p><p>People that use the nightly toolchain mostly do it to gain access to specific unstable language features. Several users have also mentioned that rustfmt works better for them on nightly or that they use the nightly compiler because of faster compilation times.</p><p>To use Rust, programmers first have to learn it, so we are always interested in finding out how do they approach that. Based on the survey results, it seems that most users learn from Rust documentation and also from <a href=\"https://doc.rust-lang.org/book/\">The Rust Programming Language</a> book, which has been a favourite learning resource of new Rustaceans for a long time. Many people also seem to learn by reading the source code of Rust crates. The fact that both the documentation and source code of tens of thousands of Rust crates is available on <a href=\"https://docs.rs\">docs.rs</a> and GitHub makes this easier.</p><p>In terms of answers belonging to the \"Other\" category, they can be clustered into three categories: people using LLM (large language model) assistants (Copilot, ChatGPT, Claude, etc.), reading the official Rust forums (Discord, <a href=\"https://users.rust-lang.org/\">URLO</a>) or being mentored while contributing to Rust projects. We would like to extend a big thank you to those making our spaces friendly and welcoming for newcomers, as it is important work and it pays off. Interestingly, a non-trivial number of people \"learned by doing\" and used rustc error messages and clippy as a guide, which is a good indicator of the quality of Rust diagnostics.</p><p>In terms of formal education, it seems that Rust has not yet penetrated university curriculums, as this is typically a very slowly moving area. Only a very small number of respondents (around 3%) have taken a university Rust course or used university learning materials.</p><p>In terms of operating systems used by Rustaceans, Linux was the most popular choice, and it seems that it is getting increasingly popular year after year. It is followed by macOS and Windows, which have a very similar share of usage.</p><blockquote><p>As you can see in the <a href=\"https://blog.rust-lang.org/images/2025-02-13-rust-survey-2024/which-os-do-you-use-wordcloud.png\">wordcloud</a>, there are also a few users that prefer Arch, btw.</p></blockquote><p>Rust programmers target a diverse set of platforms with their Rust programs. We saw a slight uptick in users targeting embedded and mobile platforms, but otherwise the distribution of platforms stayed mostly the same as last year. Since the WebAssembly target is quite diverse, we have split it into two separate categories this time. Based on the results it is clear that when using WebAssembly, it is mostly in the context of browsers (23%) rather than other use-cases (7%).</p><p>We cannot of course forget the favourite topic of many programmers: which IDE (developer environment) they use. Although Visual Studio Code still remains the most popular option, its share has dropped by 5pp this year. On the other hand, the Zed editor seems to have gained considerable traction recently. The small percentage of those who selected \"Other\" are using a wide range of different tools: from CursorAI to classics like Kate or Notepad++. Special mention to the 3 people using \"ed\", that's quite an achievement.</p><blockquote><p>You can also take a look at the linked <a href=\"https://blog.rust-lang.org/images/2025-02-13-rust-survey-2024/what-ide-do-you-use-wordcloud.png\">wordcloud</a> that summarizes open answers to this question (the \"Other\" category), to see what other editors are also popular.</p></blockquote><p>We were excited to see that more and more people use Rust at work for the majority of their coding, 38% vs 34% from last year. There is a clear upward trend in this metric over the past few years.</p><p>The usage of Rust within companies also seems to be rising, as 45% of respondents answered that their organisation makes non-trivial use of Rust, which is a 7pp increase from 2023.</p><p>Once again, the top reason employers of our survey respondents invested in Rust was the ability to build relatively correct and bug-free software. The second most popular reason was Rust‚Äôs performance characteristics. 21% of respondents that use Rust at work do so because they already know it, and it's thus their default choice, an uptick of 5pp from 2023. This seems to suggest that Rust is becoming one of the baseline languages of choice for more and more companies.</p><p>Similarly to the previous year, a large percentage of respondents (82%) report that Rust helped their company achieve its goals. In general, it seems that programmers and companies are quite happy with their usage of Rust, which is great!</p><p>In terms of technology domains, the situation is quite similar to the previous year. Rust seems to be especially popular for creating server backends, web and networking services and cloud technologies. It also seems to be gaining more traction for embedded use-cases.</p><blockquote><p>You can scroll the chart to the right to see more domains. Note that the Automotive domain was not offered as a closed answer in the 2023 survey (it was merely entered through open answers), which might explain the large jump.</p></blockquote><p>It is exciting to see the continued growth of professional Rust usage and the confidence so many users feel in its performance, control, security and safety, enjoyability, and more!</p><p>As always, one of the main goals of the State of Rust survey is to shed light on challenges, concerns, and priorities on Rustaceans‚Äô minds over the past year.</p><p>We have asked our users about aspects of Rust that limit their productivity. Perhaps unsurprisingly, slow compilation was at the top of the list, as it seems to be a perennial concern of Rust users. As always, there are efforts underway to improve the speed of the compiler, such as enabling the <a href=\"https://blog.rust-lang.org/2023/11/09/parallel-rustc.html\">parallel frontend</a> or switching to a <a href=\"https://blog.rust-lang.org/2024/05/17/enabling-rust-lld-on-linux.html\">faster linker by default</a>. We invite you to test these improvements and let us know if you encounter any issues.</p><p>Other challenges included subpar support for debugging Rust and high disk usage of Rust compiler artifacts. On the other hand, most Rust users seem to be very happy with its runtime performance, the correctness and stability of the compiler and also Rust's documentation.</p><p>In terms of specific unstable (or missing) features that Rust users want to be stabilized (or implemented), the most desired ones were async closures and if/let while chains. Well, we have good news! Async closures will be stabilized in the next version of Rust (1.85), and if/let while chains will hopefully follow <a href=\"https://github.com/rust-lang/rust/pull/132833\">soon after</a>, once Edition 2024 is released (which will also happen in Rust 1.85).</p><p>Other coveted features are generators (both sync and async) and more powerful generic const expressions. You can follow the <a href=\"https://rust-lang.github.io/rust-project-goals/2025h1/goals.html\">Rust Project Goals</a> to track the progress of these (and other) features.</p><p>In the open answers to this question, people were really helpful and tried hard to describe the most notable issues limiting their productivity. We have seen mentions of struggles with async programming (an all-time favourite), debuggability of errors (which people generally love, but they are not perfect for everyone) or Rust tooling being slow or resource intensive (rust-analyzer and rustfmt). Some users also want a better IDE story and improved interoperability with other languages.</p><p>This year, we have also included a new question about the speed of Rust's evolution. While most people seem to be content with the status quo, more than a quarter of people who responded to this question would like Rust to stabilize and/or add features more quickly, and only 7% of respondents would prefer Rust to slow down or completely stop adding new features.</p><p>Interestingly, when we asked respondents about their main worries for the future of Rust, one of the top answers remained the worry that Rust will become too complex. This seems to be in contrast with the answers to the previous question. Perhaps Rust users still seem to consider the complexity of Rust to be manageable, but they worry that one day it might become too much.</p><p>We are happy to see that the amount of respondents concerned about Rust Project governance and lacking support of the Rust Foundation has dropped by about 6pp from 2023.</p><p>Each year, the results of the State of Rust survey help reveal the areas that need improvement in many areas across the Rust Project and ecosystem, as well as the aspects that are working well for our community.</p><p>If you have any suggestions for the Rust Annual survey, please <a href=\"https://github.com/rust-lang/surveys/issues\">let us know</a>!</p><p>We are immensely grateful to those who participated in the 2024 State of Rust Survey and facilitated its creation. While there are always challenges associated with developing and maintaining a programming language, this year we were pleased to see a high level of survey participation and candid feedback that will truly help us make Rust work better for everyone.</p><p>If you‚Äôd like to dig into more details, we recommend you to browse through the full <a href=\"https://raw.githubusercontent.com/rust-lang/surveys/main/surveys/2024-annual-survey/report/annual-survey-2024-report.pdf\">survey report</a>.</p>","contentLength":14399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GitHub Availability Report: January 2025","url":"https://github.blog/news-insights/company-news/github-availability-report-january-2025/","date":1739393376,"author":"Jakub Oleksy","guid":37,"unread":true,"content":"<p>In January, we experienced three incidents that resulted in degraded performance across GitHub services.</p><p><strong>January 09 1:26 UTC (lasting 31 minutes)</strong></p><p>On January 9, 2025, between 01:26 UTC and 01:56 UTC, GitHub experienced widespread disruption to many services, with users receiving 500 responses when trying to access various functionality. This was due to a deployment which introduced a query that saturated a primary database server. On average, the error rate was 6% and peaked at 6.85% of update requests.</p><p>We were able to mitigate the incident by identifying the source of the problematic query and rolling back the deployment. The internal tooling and our dashboards surfaced the relevant data that helped us quickly identify the problematic query. It took us a total of 14 minutes from the time to engage to finding the errant query.</p><p>However, we are investing in tooling to detect problematic queries prior to deployment to prevent and to reduce our time to detection and mitigation of issues like this one in the future.</p><p><strong>January 13 23:35 UTC (lasting 49 minutes)</strong></p><p>On January 13, 2025, between 23:35 UTC and 00:24 UTC, all Git operations were unavailable due to a configuration change related to traffic routing and testing that caused our internal load balancer to drop requests between services that Git relies upon.</p><p>We mitigated the incident by rolling back the configuration change.</p><p>We are improving our monitoring and deployment practices to improve our time to detection and automated mitigation for issues like this in the future.</p><p><strong>January 30 14:22 UTC (lasting 26 minutes)</strong></p><p>On January 30, 2025, between 14:22 UTC and 14:48 UTC, web requests to github.com experienced failures (at peak the error rate was 44%), with the average successful request taking over three seconds to complete.</p><p>This outage was caused by a hardware failure in the caching layer that supports rate limiting. In addition, the impact was prolonged due to a lack of automated failover for the caching layer. A manual failover of the primary to trusted hardware was performed following recovery to ensure that the issue would not reoccur under similar circumstances.</p><p>As a result of this incident, we will be moving to a high availability cache configuration and adding resilience to cache failures at this layer to ensure requests are able to be handled should similar circumstances happen in the future.</p><p>Please follow our <a href=\"https://www.githubstatus.com/\">status page</a> for real-time updates on status changes and post-incident recaps. To learn more about what we‚Äôre working on, check out the <a href=\"https://github.blog/category/engineering/\">GitHub Engineering Blog</a>.</p>","contentLength":2548,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How GitHub uses CodeQL to secure GitHub","url":"https://github.blog/engineering/how-github-uses-codeql-to-secure-github/","date":1739379604,"author":"Brandon Stewart","guid":36,"unread":true,"content":"<p>GitHub‚Äôs Product Security Engineering team writes code and implements tools that help secure the code that powers GitHub. We use GitHub Advanced Security (GHAS) to discover, track, and remediate vulnerabilities and enforce secure coding standards at scale. One tool we rely heavily on to analyze our code at scale is <a href=\"https://codeql.github.com\">CodeQL</a>.</p><p>CodeQL is GitHub‚Äôs static analysis engine that powers automated security analyses. You can use it to query code in much the same way you would query a database. It provides a much more robust way to analyze code and uncover problems than an old-fashioned text search through a codebase.</p><p>The following post will detail how we use CodeQL to keep GitHub secure and how you can apply these lessons to your own organization. You will learn why and how we use:</p><ul><li>Custom query packs (and how we create and manage them).  </li><li>Variant analysis to uncover potentially insecure programming practices.</li></ul><p>We employ CodeQL in a variety of ways at GitHub.</p><ol><li> with the <a href=\"https://docs.github.com/en/code-security/code-scanning/managing-your-code-scanning-configuration/codeql-query-suites#built-in-codeql-query-suites\">default and security-extended query suites</a>\nDefault setup with the default and security-extended query suites meets the needs of the vast majority of our over 10,000 repositories. With these settings, pull requests automatically get a security review from CodeQL.  </li><li>\nA few repositories, like our large Ruby monolith, need extra special attention, so we use advanced setup with a <a href=\"https://docs.github.com/en/code-security/codeql-cli/getting-started-with-the-codeql-cli/customizing-analysis-with-codeql-packs#about-codeql-packs\">query pack</a> containing custom queries to really tailor to our needs.  </li><li>\nTo conduct variant analysis and quick auditing, we use MRVA. We also write custom CodeQL queries to detect code patterns that are either specific to GitHub‚Äôs codebases or patterns we want a security engineer to manually review.</li></ol><p>The specific custom Actions workflow step we use on our monolith is pretty simple. It looks like this:</p><pre><code>- name: Initialize CodeQL\n    uses: github/codeql-action/init@v3\n    with:\n      languages: ${{ matrix.language }}\n      config-file: ./.github/codeql/${{ matrix.language }}/codeql-config.yml\n</code></pre><p>Our Ruby configuration is pretty standard, but advanced setup offers a variety of <a href=\"https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/customizing-your-advanced-setup-for-code-scanning#using-a-custom-configuration-file\">configuration options using custom configuration files</a>. The interesting part is the  option, which is how we enable our custom query pack as part of the CodeQL analysis. This pack contains a collection of CodeQL queries we have written for Ruby, specifically for the GitHub codebase.</p><p>So, let‚Äôs dive deeper into why we did that‚Äîand how!</p><h2>Publishing our CodeQL query pack<a href=\"https://github.blog/engineering/how-github-uses-codeql-to-secure-github/#publishing-our-codeql-query-pack\" aria-label=\"Publishing our CodeQL query pack\"></a></h2><p>Initially, we published CodeQL query files directly to the GitHub monolith repository, but we moved away from this approach for several reasons:</p><ul><li>It required going through the production deployment process for each new or updated query.  </li><li>Queries not included in a query pack were <a href=\"https://docs.github.com/en/code-security/codeql-cli/using-the-advanced-functionality-of-the-codeql-cli/publishing-and-using-codeql-packs#about-codeql-pack-compatibility\">not pre-compiled</a>, which slowed down CodeQL analysis in CI.  </li><li>Our <a href=\"https://docs.github.com/en/code-security/codeql-cli/using-the-advanced-functionality-of-the-codeql-cli/testing-custom-queries\">test suite for CodeQL queries</a> ran as part of the monolith‚Äôs CI jobs. When a new version of the CodeQL CLI was released, it sometimes caused the query tests to fail because of changes in the query output, even when there were no changes to the code in the pull request. This often led to confusion and frustration among engineers, as the failure wasn‚Äôt related to their pull request changes.</li></ul><p>By switching to publishing a query pack to <a href=\"https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry\">GitHub Container Registry (GCR)</a>, we‚Äôve simplified our process and eliminated many of these pain points, making it easier to ship and maintain our CodeQL queries. So while it‚Äôs  to deploy custom CodeQL query files directly to a repository, we recommend publishing CodeQL queries as a query pack to the GCR for easier deployment and faster iteration.</p><p>When setting up our custom <a href=\"https://docs.github.com/en/code-security/codeql-cli/using-the-advanced-functionality-of-the-codeql-cli/creating-and-working-with-codeql-packs#codeql-pack-structure\">query pack</a>, we faced several considerations, particularly around managing dependencies like the  package.</p><p>To ensure our custom queries remain maintainable and concise, we extend classes from the <a href=\"https://docs.github.com/en/code-security/code-scanning/managing-your-code-scanning-configuration/ruby-built-in-queries\">default query suite</a>, such as the  library. This allows us to leverage existing functionality rather than reinventing the wheel, keeping our queries concise and maintainable. However, changes to the CodeQL library API can introduce breaking changes, potentially deprecating our queries or causing errors. Since CodeQL runs as part of our CI, we wanted to minimize the chance of this happening, as this can lead to frustration and loss of trust from developers.</p><p>We develop our queries against the latest version of the  package, ensuring we‚Äôre always working with the most up-to-date functionality. To mitigate the risk of breaking changes affecting CI, we pin the  version when we‚Äôre ready to release, <a href=\"https://docs.github.com/en/code-security/codeql-cli/using-the-advanced-functionality-of-the-codeql-cli/creating-and-working-with-codeql-packs%5C%5C#adding-and-installing-dependencies-on-a-codeql-pack\">locking it in the  file</a>. This guarantees that when our queries are deployed, they will run with the specific version of  we‚Äôve tested, avoiding potential issues from unintentional updates.</p><p>Here‚Äôs how we manage this setup:</p><ul><li>In our qlpack.yml, we set the dependency to use the latest version of </li><li>During development, this configuration <a href=\"https://docs.github.com/en/code-security/codeql-cli/using-the-advanced-functionality-of-the-codeql-cli/creating-and-working-with-codeql-packs#adding-and-installing-dependencies-on-a-codeql-pack%5C\">pulls in the latest version</a>) of  when running , ensuring we‚Äôre always up to date.\n<pre><code>// Our custom query pack's qlpack.yml\n\nlibrary: false\nname: github/internal-ruby-codeql\nversion: 0.2.3\nextractor: 'ruby'\ndependencies:\n  codeql/ruby-all: \"*\"\ntests: 'test'\ndescription: \"Ruby CodeQL queries used internally at GitHub\"\n</code></pre></li><li>Before releasing, we lock the version in the <a href=\"https://docs.github.com/en/code-security/codeql-cli/using-the-advanced-functionality-of-the-codeql-cli/creating-and-working-with-codeql-packs#codeql-pack-structure\"></a> file, specifying the exact version to ensure stability and prevent issues in CI.\n<pre><code>// Our custom query pack's codeql-pack.lock.yml\n\nlockVersion: 1.0.0\ndependencies:\n ...\n codeql/ruby-all:\n   version: 1.0.6\n</code></pre></li></ul><p>This approach allows us to balance developing against the latest features of the  package while ensuring stability when we release.</p><p>We also have a set of <a href=\"https://docs.github.com/en/code-security/codeql-cli/using-the-advanced-functionality-of-the-codeql-cli/testing-custom-queries\">CodeQL unit tests</a> that exercise our queries against sample code snippets, which helps us quickly determine if any query will cause errors before we publish our pack. These tests are run as part of the CI process in our query pack repository, providing an early check for issues. We strongly recommend writing unit tests for your custom CodeQL queries to ensure stability and reliability.</p><p>Altogether, the basic flow for releasing new CodeQL queries via our pack is as follows:</p><ul><li>Open a pull request with the new query.  </li><li>Write unit tests for the new query.  </li><li>Increment the pack version in a new pull request.  </li><li>Run  to resolve dependencies.  </li><li>Correct unit tests as needed.  </li><li>Publish the query pack to the GitHub Container Registry (GCR).  </li><li>Repositories with the query pack in their config will start using the updated queries.</li></ul><p>We have found this flow balances our team‚Äôs development experience while ensuring stability in our published query pack.</p><h2>Configuring our repository to use our custom query pack<a href=\"https://github.blog/engineering/how-github-uses-codeql-to-secure-github/#configuring-our-repository-to-use-our-custom-query-pack\" aria-label=\"Configuring our repository to use our custom query pack\"></a></h2><p>We won‚Äôt provide a general recommendation on configuration here, given that it ultimately depends on how your organization deploys code. We opted against locking our pack to a particular version in our <a href=\"https://docs.github.com/en/code-security/code-scanning/creating-an-advanced-setup-for-code-scanning/customizing-your-advanced-setup-for-code-scanning#using-codeql-model-packs\">CodeQL configuration file</a> (see above). Instead, we chose to manage our versioning by publishing the CodeQL package in GCR. This results in the GitHub monolith retrieving the latest published version of the query pack. To roll back changes, we simply have to republish the package. In one instance, we released a query that had a high number of false positives and we were able to publish a new version of the pack that removed that query in less than 15 minutes. This is faster than the time it would have taken us to merge a pull request on the monolith repository to roll back the version in the CodeQL configuration file.</p><p>One of the problems we encountered with publishing the query pack in GCR was how to easily make the package available to multiple repositories within our enterprise. There are several approaches we explored.</p><ul><li><strong>Grant access permissions for individual repositories.</strong> On the package management page, you can grant permissions for individual repositories to access your package. This was not a good solution for us since we have too many repositories for it to be feasible to do manually, yet there is not currently a way to configure programmatically using an API.   </li><li><strong>Mint a personal access token for the CodeQL action runner.</strong> We could have minted a personal access token (PAT) that has access to read all packages for our organization and added that to the CodeQL action runner. However, this would have required managing a new token, and it seemed a bit more permissive than we wanted because it could read  of our private packages rather than ones we explicitly allow it to have access to.  </li></ul><h2>CodeQL query pack queries<a href=\"https://github.blog/engineering/how-github-uses-codeql-to-secure-github/#codeql-query-pack-queries\" aria-label=\"CodeQL query pack queries\"></a></h2><p>We write a variety of custom queries to be used in our custom query packs. These cover GitHub-specific patterns that aren‚Äôt included in the default CodeQL query pack. This allows us to tailor the analysis to patterns and preferences that are specific to our company and codebase. Some of the types of things we alert on using our custom query pack include:</p><ul><li>High-risk APIs specific to GitHub‚Äôs code that can be dangerous if they receive unsanitized user input.  </li><li>Use of specific built-in Rails methods for which we have safer, custom methods or functions.    </li><li>Required authorization methods not being used in our REST API endpoint definitions and GraphQL object/mutation definitions.  </li><li>REST API endpoints and GraphQL mutations that require engineers to define access control methods to determine which actors can access them. (Specifically, the query detects the absence of this method definition to ensure that the actors‚Äô permissions are being checked for these endpoints.)  </li><li>Use of signed tokens so we can nudge engineers to include Product Security as a reviewer when using them.</li></ul><p>Custom queries can be used more for educational purposes rather than being blockers to shipping code. For example, we want to alert engineers when they use the <a href=\"https://guides.rubyonrails.org/active_record_encryption.html#encrypt-and-decrypt\">ActiveRecord::decrypt</a> method. This method should generally not be used in production code, as it will cause an encrypted column to become decrypted. We use the recommendation severity in the <a href=\"https://codeql.github.com/docs/writing-codeql-queries/metadata-for-codeql-queries/#metadata-properties\">query metadata</a> so these alerts are treated as more of an informational alert. That means this may trigger an alert in a pull request, but it won‚Äôt cause the CodeQL CI job to fail. We use this lower severity level to allow engineers to assess the impact of new queries without immediate blocking. Additionally, this alert level isn‚Äôt tracked through our <a href=\"https://github.blog/engineering/githubs-engineering-fundamentals-program-how-we-deliver-on-availability-security-and-accessibility/\">Fundamentals program</a>, meaning it doesn‚Äôt require immediate action, reflecting the query‚Äôs maturity as we continue to refine its relevance and risk assessment.</p><pre><code>/**\n * @id rb/github/use-of-activerecord-decrypt\n * @description Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save\n * them unencrypted, effectively undoing encryption and possibly making the attributes inaccessible.\n * If you need to access the unencrypted value of any attribute, you can do so by calling my_model.attribute_name.\n * @kind problem\n * @severity recommendation\n * @name Use of ActiveRecord decrypt method\n * @tags security\n *      github-internal\n */\n\nimport ruby\nimport DataFlow\nimport codeql.ruby.DataFlow\nimport codeql.ruby.frameworks.ActiveRecord\n\n/** Match against .decrypt method calls where the receiver may be an ActiveRecord object */\nclass ActiveRecordDecryptMethodCall extends ActiveRecordInstanceMethodCall {\n  ActiveRecordDecryptMethodCall() { this.getMethodName() = \"decrypt\" }\n}\n\nfrom ActiveRecordDecryptMethodCall call\nselect call,\n  \"Do not use the .decrypt method on AR models, this will decrypt all encrypted attributes and save them unencrypted.\n</code></pre><p>Another educational query is the one mentioned above in which we detect the absence of the `control_access` method in a class that defines a REST API endpoint. If a pull request introduces a new endpoint without `control_access`,  a comment will appear on the pull request saying that the `control_access` method wasn‚Äôt found and it‚Äôs a requirement for REST API endpoints. This will notify the reviewer of a potential issue and prompt the developer to fix it.</p><pre><code>/**\n * @id rb/github/api-control-access\n * @name Rest API Without 'control_access'\n * @description All REST API endpoints must call the 'control_access' method, to ensure that only specified actor types are able to access the given endpoint.\n * @kind problem\n * @tags security\n * github-internal\n * @precision high\n * @problem.severity recommendation\n */\n\nimport codeql.ruby.AST\nimport codeql.ruby.DataFlow\nimport codeql.ruby.TaintTracking\nimport codeql.ruby.ApiGraphs\n\n// Api::App REST API endpoints should generally call the control_access method\nprivate DataFlow::ModuleNode appModule() {\n  result = API::getTopLevelMember(\"Api\").getMember(\"App\").getADescendentModule() and\n  not result = protectedApiModule() and\n  not result = staffAppApiModule()\n}\n\n// Api::Admin, Api::Staff, Api::Internal, and Api::ThirdParty REST API endpoints do not need to call the control_access method\nprivate DataFlow::ModuleNode protectedApiModule() {\n  result =\n    API::getTopLevelMember([\"Api\"])\n        .getMember([\"Admin\", \"Staff\", \"Internal\", \"ThirdParty\"])\n        .getADescendentModule()\n}\n\n// Api::Staff::App REST API endpoints do not need to call the control_access method\nprivate DataFlow::ModuleNode staffAppApiModule() {\n  result =\n    API::getTopLevelMember([\"Api\"]).getMember(\"Staff\").getMember(\"App\").getADescendentModule()\n}\n\nprivate class ApiRouteWithoutControlAccess extends DataFlow::CallNode {\n  ApiRouteWithoutControlAccess() {\n    this = appModule().getAModuleLevelCall([\"get\", \"post\", \"delete\", \"patch\", \"put\"]) and\n    not performsAccessControl(this.getBlock())\n  }\n}\n\npredicate performsAccessControl(DataFlow::BlockNode blocknode) {\n  accessControlCalled(blocknode.asExpr().getExpr())\n}\n\npredicate accessControlCalled(Block block) {\n  // the method `control_access` is called somewhere inside `block`\n  block.getAStmt().getAChild*().(MethodCall).getMethodName() = \"control_access\"\n}\n\nfrom ApiRouteWithoutControlAccess api\nselect api.getLocation(),\n  \"The control_access method was not detected in this REST API endpoint. All REST API endpoints must call this method to ensure that the endpoint is only accessible to the specified actor types.\"\n</code></pre><p><a href=\"https://codeql.github.com/docs/codeql-overview/about-codeql/#about-variant-analysis\">Variant analysis (VA)</a> refers to the process of searching for variants of security vulnerabilities. This is particularly useful when we‚Äôre responding to a <a href=\"https://bounty.github.com/\">bug bounty submission</a> or a security incident. We use a combination of tools to do this, including GitHub‚Äôs code search functionality, custom scripts, and CodeQL.  We will often start by using code search to find patterns similar to the one that caused a particular vulnerability across numerous repositories. This is sometimes not good enough, as code search is not semantically aware, meaning that it cannot determine whether a given variable is an Active Record object or whether it is being used in an `if` expression. To answer those types of questions we turn to CodeQL.</p><p>When we write CodeQL queries for variant analysis we are much less concerned about false positives, since the goal is to provide results for security engineers to analyze. The quality of the code is also not quite as important, as these queries will only be used for the duration of the VA effort. Some of the types of things we use CodeQL for during VAs are:</p><ul><li>Where are we using SHA1 hashes?  </li><li>One of our internal API endpoints was vulnerable to SQLi according to a recent bug bounty report. Where are we passing user input to that API endpoint?   </li><li>There is a problem with how some HTTP request libraries in Ruby handle the proxy setting. Can we look at places we are instantiating our HTTP request libraries with a proxy setting?</li></ul><p>One recent example involved a subtle vulnerability in Rails. We wanted to detect when the following condition was present in our code:</p><ul><li>A parameter was used to look up an Active Record object.  </li><li>That parameter is later reused after the Active Record object is looked up.</li></ul><p>The concern with this condition is that it could lead to an <a href=\"https://portswigger.net/web-security/access-control/idor\">insecure direct object</a> reference (IDOR) vulnerability because Active Record finder methods can accept an array. If the code looks up an Active Record object in one call to determine if a given entity has access to a resource, but later uses a different element from that array to find an object reference, that can lead to an IDOR vulnerability. It would be difficult to write a query to detect  vulnerable instances of this pattern, but we were able to write a query that found potential vulnerabilities that gave us a list of code paths to manually analyze. We ran the query against a large number of our Ruby codebases using CodeQL‚Äôs MRVA.</p><p>The query, which is a bit hacky and not quite production grade, is below:</p><pre><code>/**\n * @name wip array query\n * @description an array is passed to an AR finder object\n */\n\nimport ruby\nimport codeql.ruby.AST\nimport codeql.ruby.ApiGraphs\nimport codeql.ruby.frameworks.Rails\nimport codeql.ruby.frameworks.ActiveRecord\nimport codeql.ruby.frameworks.ActionController\nimport codeql.ruby.DataFlow\nimport codeql.ruby.Frameworks\nimport codeql.ruby.TaintTracking\n\n// Gets the \"final\" receiver in a chain of method calls.\n// For example, in `Foo.bar`, this would give the `Foo` access, and in\n// `foo.bar.baz(\"arg\")` it would give the `foo` variable access\nprivate Expr getUltimateReceiver(MethodCall call) {\n  exists(Expr recv |\n    recv = call.getReceiver() and\n    (\n      result = getUltimateReceiver(recv)\n      or\n      not recv instanceof MethodCall and result = recv\n    )\n  )\n}\n\n// Names of class methods on ActiveRecord models that may return one or more\n// instances of that model. This also includes the `initialize` method.\n// See https://api.rubyonrails.org/classes/ActiveRecord/FinderMethods.html\nprivate string staticFinderMethodName() {\n  exists(string baseName |\n    baseName = [\"find_by\", \"find_or_create_by\", \"find_or_initialize_by\", \"where\"] and\n    result = baseName + [\"\", \"!\"]\n  )\n  // or\n  // result = [\"new\", \"create\"]\n}\n\nprivate class ActiveRecordModelFinderCall extends ActiveRecordModelInstantiation, DataFlow::CallNode\n{\n  private ActiveRecordModelClass cls;\n\n  ActiveRecordModelFinderCall() {\n    exists(MethodCall call, Expr recv |\n      call = this.asExpr().getExpr() and\n      recv = getUltimateReceiver(call) and\n      (\n        // The receiver refers to an `ActiveRecordModelClass` by name\n        recv.(ConstantReadAccess).getAQualifiedName() = cls.getAQualifiedName()\n        or\n        // The receiver is self, and the call is within a singleton method of\n        // the `ActiveRecordModelClass`\n        recv instanceof SelfVariableAccess and\n        exists(SingletonMethod callScope |\n          callScope = call.getCfgScope() and\n          callScope = cls.getAMethod()\n        )\n      ) and\n      (\n        call.getMethodName() = staticFinderMethodName()\n        or\n        // dynamically generated finder methods\n        call.getMethodName().indexOf(\"find_by_\") = 0\n      )\n    )\n  }\n\n  final override ActiveRecordModelClass getClass() { result = cls }\n}\n\nclass FinderCallArgument extends DataFlow::Node {\n  private ActiveRecordModelFinderCall finderCallNode;\n\n  FinderCallArgument() { this = finderCallNode.getArgument(_) }\n}\n\nclass ParamsHashReference extends DataFlow::CallNode {\n  private Rails::ParamsCall params;\n\n  // TODO: only direct element references against `params` calls are considered\n  ParamsHashReference() { this.getReceiver().asExpr().getExpr() = params }\n\n  string getArgString() {\n    result = this.getArgument(0).asExpr().getConstantValue().getStringlikeValue()\n  }\n}\n\nclass ArrayPassedToActiveRecordFinder extends TaintTracking::Configuration {\n  ArrayPassedToActiveRecordFinder() { this = \"ArrayPassedToActiveRecordFinder\" }\n\n  override predicate isSource(DataFlow::Node source) { source instanceof ParamsHashReference }\n\n  override predicate isSink(DataFlow::Node sink) {\n    sink instanceof FinderCallArgument\n  }\n\n  string getParamsArg(DataFlow::CallNode paramsCall) {\n    result = paramsCall.getArgument(0).asExpr().getConstantValue().getStringlikeValue()\n  }\n\n  // this doesn't check for anything fancy like whether it's reuse in a if/else\n  // only intended for quick manual audit filtering of interesting candidates\n  // so remains fairly broad to not induce false negatives\n  predicate paramsUsedAfterLookups(DataFlow::Node source) {\n    exists(DataFlow::CallNode y | y instanceof ParamsHashReference\n    and source.getEnclosingMethod() = y.getEnclosingMethod()\n    and source != y\n    and getParamsArg(source) = getParamsArg(y)\n    // we only care if it's used again AFTER an object lookup\n    and y.getLocation().getStartLine() &gt; source.getLocation().getStartLine())\n  }\n}\n\nfrom ArrayPassedToActiveRecordFinder config, DataFlow::Node source, DataFlow::Node sink\nwhere config.hasFlow(source, sink) and config.paramsUsedAfterLookups(source)\nselect source, sink.getLocation()\n</code></pre><p>CodeQL can be very useful for product security engineering teams to detect and prevent vulnerabilities at scale. We use a combination of queries that run in CI using our query pack and one-off queries run through MRVA to find potential vulnerabilities and communicate them to engineers. CodeQL isn‚Äôt only useful for finding security vulnerabilities, though; it is also useful for detecting the presence or absence of security controls that are defined in code. This saves our security team time by surfacing certain security problems automatically, and saves our engineers time by detecting them earlier in the development process.</p><h2>Writing custom CodeQL queries<a href=\"https://github.blog/engineering/how-github-uses-codeql-to-secure-github/#writing-custom-codeql-queries\" aria-label=\"Writing custom CodeQL queries\"></a></h2><p>We have a large number of articles and resources for writing custom CodeQL queries. If you haven‚Äôt written custom CodeQL queries before, here are some resources to help get you started:</p><p><em>Michael Recachinas, GitHub Staff Security Engineer, also contributed to this blog post.</em></p>","contentLength":21522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Python 3.14.0 alpha 5 is out","url":"https://pythoninsider.blogspot.com/2025/02/python-3140-alpha-5-is-out.html","date":1739302860,"author":"Hugo","guid":203,"unread":true,"content":"<p>Here comes the antepenultimate alpha.</p><p><strong>This is an early developer preview of Python\n3.14</strong></p><p>Python 3.14 is still in development. This release, 3.14.0a5, is the\nfifth of seven planned alpha releases.</p><p>Alpha releases are intended to make it easier to test the current\nstate of new features and bug fixes and to test the release process.</p><p>During the alpha phase, features may be added up until the start of\nthe beta phase (2025-05-06) and, if necessary, may be modified or\ndeleted up until the release candidate phase (2025-07-22). Please keep\nin mind that this is a preview release and its use is\n recommended for production environments.</p><p>Many new features for Python 3.14 are still being planned and\nwritten. Among the new major new features and changes so far:</p><p>The next pre-release of Python 3.14 will be the penultimate alpha,\n3.14.0a6, currently scheduled for 2025-03-14.</p><p>2025-01-29 marked the start of a new lunar year, the Year of the\nSnake üêç (and the Year of Python?).</p><p>For centuries, œÄ was often approximated as 3 in China. Some time\nbetween the years 1 and 5 CE, astronomer, librarian, mathematician and\npolitician Liu Xin (ÂäâÊ≠Ü) calculated œÄ as 3.154.</p><p>Around 130 CE, mathematician, astronomer, and geographer Zhang Heng\n(ÂºµË°°, 78‚Äì139) compared the celestial circle with the diameter of the\nearth as 736:232 to get 3.1724. He also came up with a formula for the\nratio between a cube and inscribed sphere as 8:5, implying the ratio of\na square‚Äôs area to an inscribed circle is ‚àö8:‚àö5. From this, he\ncalculated œÄ as ‚àö10 (~3.162).</p><p>Third century mathematician Liu Hui (ÂàòÂæΩ) came up with an algorithm\nfor calculating œÄ iteratively: calculate the area of a polygon inscribed\nin a circle, then as the number of sides of the polygon is increased,\nthe area becomes closer to that of the circle, from which you can\napproximate œÄ.</p><p>This algorithm is similar to the method used by Archimedes in the 3rd\ncentury BCE and Ludolph van Ceulen in the 16th century CE (see <a href=\"https://blog.python.org/2024/11/python-3140-alpha-2-released.html\">3.14.0a2\n  release notes</a>), but Archimedes only went up to a 96-sided polygon\n(96-gon). Liu Hui went up to a 192-gon to approximate œÄ as 157/50 (3.14)\nand later a 3072-gon for 3.14159.</p><p>Liu Hu wrote a commentary on the book The Nine Chapters on the\nMathematical Art which included his œÄ approximations.</p><p>In the fifth century, astronomer, inventor, mathematician,\npolitician, and writer Zu Chongzhi (Á•ñÊ≤ñ‰πã, 429‚Äì500) used Liu Hui‚Äôs\nalgorithm to inscribe a 12,288-gon to compute œÄ between 3.1415926 and\n3.1415927, correct to seven decimal places. This was more accurate than\nHellenistic calculations and wouldn‚Äôt be improved upon for 900\nyears.</p><p>Thanks to all of the many volunteers who help make Python Development\nand these releases possible! Please consider supporting our efforts by\nvolunteering yourself or through organisation contributions to the <a href=\"https://www.python.org/psf-landing/\">Python Software\nFoundation</a>.</p><p>Regards from a remarkably snowless Helsinki,</p><p>Your release team, Hugo van KemenadeSteve Dower</p>","contentLength":2941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to refactor code with GitHub Copilot","url":"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/","date":1739293205,"author":"Anthony Grutta","guid":35,"unread":true,"content":"<p>We‚Äôve all been there‚Äîstaring at a function that looks like it was written by an over-caffeinated goblin at 3 AM (maybe even your alter ego). You  pretend it doesn‚Äôt exist, or you could refactor it. Luckily, GitHub Copilot makes the second option less painful.</p><h2>What is code refactoring?<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#what-is-code-refactoring\" aria-label=\"What is code refactoring?\"></a></h2><p>Feel free to breeze past this section if you already know what‚Äôs involved with refactoring code, but we wanted to take a moment to cover what we‚Äôll be looking at today.</p><p>Think of refactoring as giving your project some digital spring cleaning‚Äîa glow up for your functions, classes, and modules. But instead of just tidying up, you‚Äôre making your code more efficient, maintainable, and readable, all without changing its external behavior.</p><p>Some standard ways of refactoring include:</p><ul><li><strong>Simplifying complex conditionals</strong> (because no one should need a PhD to read your  statements)  </li><li><strong>Extracting duplicated logic</strong> (so you‚Äôre not trying to maintain code in ten different places)  </li><li><strong>Improving variable and function names</strong> (because  is a crime against humanity)  </li><li><strong>Converting monolithic functions into smaller, modular pieces</strong> (to prevent the dreaded ‚Äúfunction that spans multiple screens‚Äù scenario)</li></ul><p>Refactoring isn‚Äôt just about tidiness‚Äîit‚Äôs about making your codebase more resilient, scalable, and enjoyable to work with. Let‚Äôs find out how GitHub Copilot can help you do it faster and with fewer headaches.</p><h2>Know what your code does before you refactor anything<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#know-what-your-code-does-before-you-refactor-anything\" aria-label=\"Know what your code does before you refactor anything\"></a></h2><p>It might seem obvious to say this, but before you can refactor any code you need to understand how it works. If you don‚Äôt know what your code is doing, you won‚Äôt know whether or not the ‚Äúimprovements‚Äù you‚Äôre making are changing the core functionality of the code.</p><p>Consider the following method:</p><pre><code>public String getSound(String animal) {\n  if (animal == null) {\n      System.out.println(\"Oops! A null animal?\");\n  } else if (animal.equalsIgnoreCase(\"Dog\")) {\n      return \"Bark\";\n  } else if ( animal.equalsIgnoreCase(\"Cat\")) {\n      return \"Meow\";\n  } else if ( animal.equalsIgnoreCase(\"Bird\")) {\n      return \"Tweet\";\n  }\n  return \"Unknown\";\n}\n</code></pre><p>You might look at this and immediately think ‚Äúthey should use a switch statement,‚Äù and that would be one example of refactoring the code. But having that knee jerk reaction requires you to know how if-statements and switch-statements work. You can only make that suggestion if you understand this code will continue cycling through if-statements until it finds an appropriate match. Otherwise, it will return the value of .</p><p>As codebases get more complex and make calls between multiple files, this gets (much) more complicated. And this is one way Copilot can help you.</p><p>You can ask Copilot Chat to explain how some code works, either by asking in plain language or using the /explain slash command. To limit the scope of what Copilot looks at, select the code in your IDE before asking your query, or specify specific files for it to consider by using . While you‚Äôre at it, you can even ask it to add code comments to help you (or anyone else reading the code) in the future.</p><p><strong>Here are some sample prompts:</strong></p><ul><li><code>Explain what this code does.</code></li><li><code>Add comments to this code to make it more understandable.</code></li></ul><p>You should use Copilot Chat to analyze and explain your codebase until you fully understand the code you‚Äôre looking to refactor.</p><h2>Try some blanket improvements to refactor your code<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#try-some-blanket-improvements-to-refactor-your-code\" aria-label=\"Try some blanket improvements to refactor your code\"></a></h2><p>Like most things in life, it‚Äôs usually best to start small. When you‚Äôre first getting started with refactoring code, keep it simple: open up Copilot Chat in your project and ask ‚Äúhow would you improve this?‚Äù Just like when you are asking GitHub Copilot to explain some code, you can specify what it looks at by highlighting sections of code or identifying specific files by using .</p><p><strong>Here are some sample prompts:</strong></p><ul><li><code>How would you improve this?</code></li><li><code>Improve the variable names in this function.</code></li><li><code>#file:pageInit.js, #file:socketConnector.js Offer suggestions to simplify this code.</code></li></ul><p>Copilot will then offer suggestions to improve the code in the way that you specified. This is great for getting started, but Copilot can do much more if you give it some guidance.</p><p>When working with any generative AI-powered developer tool, it is often useful to include more context in your prompts‚Äîones that are more likely to get you the results you‚Äôre looking for. By being specific about what you want the tool to do, it focuses the efforts toward that end.</p><p>It‚Äôs a bit like if someone tells you to code something, and you have so many ideas and questions that you‚Äôre not quite sure where to begin. But if someone tells you they specifically want you to code a class that sorts a list, you can focus on the task at hand.</p><p>The prompts above don‚Äôt offer much specificity or context, which leaves Copilot to explore all the possible ways your code could be improved. The upside? You may see options that you might not have considered. The downside is some of the proposed solutions might not address your specific concerns.</p><h2>Make a plan for refactoring your codebase<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#make-a-plan-for-refactoring-your-codebase\" aria-label=\"Make a plan for refactoring your codebase\"></a></h2><p>What do you want to do to your code? Do you want to make it more readable? Or do you want to find redundant code and remove it? Coming up with a plan for the improvements you want to make will help you to hit your goals. This comes back to making sure you understand your code. If you know how it works, you can come up with a plan for the type of improvements that you want to make.</p><p>Maybe your code base has a bunch of different scripts that all perform the same general function. You could strip out the code into a common module to import into each of the different scripts, making the code easier to change and maintain.</p><p>To do so, you can direct Copilot to look for these common code sections and to pull them into a single module.</p><pre><code>Inspect all my js files for GitHub API calls and create a new class that will manage all the GitHub API calls.\n</code></pre><p>Now that we have provided some guidance and additional context, Copilot will provide suggestions targeting this specific improvement that we want to make to our code.</p><p>You can also provide a laundry list of tasks, or ask Copilot to keep things in mind while it is doing the refactoring.</p><p><strong>In that vein, here‚Äôs a sample prompt to consider:</strong></p><pre><code>Can you refactor the GitHubController class to:\n- remove nested logic structures\n- make the code more concise\n- while doing this, check if the code is safe and add comments if not\n</code></pre><p>The takeaway here is Copilot is very good at taking directions‚Äîand the more specific your directions are, the more it will generate outputs in line with your intended end goal.</p><h2>A real-world example: Using GitHub Copilot to refactor code<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#a-real-world-example-using-github-copilot-to-refactor-code\" aria-label=\"A real-world example: Using GitHub Copilot to refactor code\"></a></h2><p>Sometimes the best way to understand something is to see it in use. To help with this, we wanted to share an experience where we needed to use GitHub Copilot to refactor our own code, interspersed with some general guidance and pointers.</p><p>It started with a couple of organizations independently requesting help with their individual large migration projects, which involved moving from other data centers into tens of thousands of repositories on GitHub. After building a proof of concept, we started creating individual systems, each tailored to the needs of the individual customer and their unique systems. While we were doing this, a third organization made a similar request for help, and we realized this was a common need.</p><p>At this point in time, we had two systems that were both suited for large-scale migrations, but each of them was heavily customized for the original company making the request. We quickly realized the best solution involved refactoring our code to improve its modularity so we could use it across different organizations. This would not be easy‚Äîthe codebase was made up of several different scripts and workflows with a lot of moving parts. And that‚Äôs when we turned to Copilot for help.</p><p>First we came up with a plan. We created a set of high-level goals and used these to guide us during the refactoring process. We wanted to make sure the code was:</p><ul><li><strong>Modular and reusable for different organizations:</strong> The code should be organized into small, self-contained modules that could be reused and combined for new functionality.  </li><li> The code should be easy to maintain and extend as needed. That meant making it well-organized, well-documented, and easy to understand.  </li><li><strong>Customizable for different organizations:</strong> The code should be flexible enough to meet the unique needs of each organization.  </li><li><strong>Following coding standards and best practices:</strong> The code should be consistent, readable, and easy to understand.</li></ul><p>With this plan in place, it was time to ask Copilot for help.</p><h3>Step 1: Creating the initial module<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#step-1-creating-the-initial-module\" aria-label=\"Step 1: Creating the initial module\"></a></h3><p>Our first step was creating a common module we could import into each of the different scripts. This would allow us to reuse the same code across many different scripts, making it easier to maintain and extend. We created a skeleton of a module called . Then we used Copilot to fill in the details.</p><p><strong>Here was the prompt we gave Copilot:</strong></p><pre><code>I am creating a module, gh-migrations, which will hold common functionality to be used through out my project. I have already created an index.js for the module. Please inspect the other js files for GitHub API calls, create a new class for the module which will manage all GitHub API calls. This class should also account for rate limiting and errors. Do not make edits to the javascript files, rather identify the API calls, and add the functionality to the new GitHub API class.\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>I'll help create a GitHub API management class based on the API calls found in the provided files. Here's a step-by-step solution:\n\n1. From analyzing the files, I identified these common GitHub API operations: \n  * Issues creation and updates\n  * Issue search\n  * Rate limit handling (403 errors)\n  * Common error handling patterns\n\n2. I'll create a class that encapsulates these operations with proper error handling and rate limiting.\n\n.github/lib/gh-migrations/index.js\nHere's the GitHub API management class implementation:\n\n&lt;code&gt;\n\nThis implementation:\n\n1. Creates a GitHubApiManager class that handles all GitHub API interactions\n2. Implements automatic rate limiting retry logic in withRateLimit\n3. Provides methods for common operations: \n  * createIssue: Creates a new issue\n  * updateIssue: Updates an existing issue\n  * searchIssues: Searches for issues using GitHub's search API\n  * findIssueByTitle: Helper method to find issues by title\n  * getIssue: Gets a specific issue by number\n\nUsage example:\n\n&lt;code&gt;\n\nThe class handles:\n  * Automatic retry on rate limits\n  * Consistent error handling\n  * Repository context management\n  * Common GitHub API operations used across the codebase\n\nThis centralizes all GitHub API interactions and makes the code more maintainable and consistent.\n</code></pre><p>After reviewing the code (as you should always do!), we made the recommended changes. The next step was putting these changes into a separate file while ensuring the  file could reference it.</p><p><strong>Here was the prompt we entered for Copilot:</strong></p><pre><code>Rename the class GitHubApiManager to GHApi and put it in a separate file that is referenced in the index.js file.\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>I'll help you split the code into two files. Here's the step by step solution:\n\n1. Create a new file gh-api.js for the GHApi class\n2. Update index.js to export the renamed class from the new file\n\n.github/lib/gh-migrations/gh-api.js\nCreate a new file containing the renamed class:\n\n&lt;code&gt;\n\n.github/lib/gh-migrations/index.js\nUpdate the index file to export the renamed class from the new file:\n\n&lt;code&gt;\n\nThe changes:\n1. Created a new file gh-api.js with the renamed class\n2. Moved all the class implementation to the new file\n3. Updated the class name from GitHubApiManager to GHApi\n4. Modified index.js to require and export the class from the new file\n\nNow consumers of this module can continue to import it the same way, but the implementation is better organized.\n</code></pre><p>After making these changes, we then had to repeat the process for each of the files that contained GitHub API calls. Be aware, we didn‚Äôt want to refactor the code directly at this stage. Instead, we wanted to create a common module we could use for refactoring code later.</p><p><strong>Here was the prompt we entered for Copilot:</strong></p><pre><code>The gh-api.js file is a new class that will be used to manage all GitHub API calls. Please review the other javascript files for GitHub API calls to be added to gh-api.js. Do not edit the javascript files, only add new functionality to gh-api.js.\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>I'll analyze the JavaScript files and add the missing API functionality to the GHApi class.\n\n.github/lib/gh-migrations/gh-api.js\nAdd methods for issue operations, searching, and deletion based on the API calls found in migration.js and application.js:\n\n&lt;code&gt;\n\nThis implementation adds all the GitHub API functionality found in the other files:\n\n1. From migration.js:\n  * createComment for updating issues with comments\n  * getIssue and isMigrationIssue for checking migration status\n2. From application.js:\n  * searchIssues and searchIssuesByTitle for finding issues\n  * getIssueNodeId and deleteIssue for issue deletion via GraphQL\n  * findJiraIssue helper method to get Jira ticket information\n\nThe methods maintain the same rate limiting and error handling patterns while providing a cleaner interface for the API operations.\n</code></pre><h3>Step 2: Adding new classes<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#step-2-adding-new-classes\" aria-label=\"Step 2: Adding new classes\"></a></h3><p>Once we had the new  class to handle the GitHub API, we want to create some reusable classes for different entities in our application. Since our application is a migration tool managed via GitHub Issues, we wanted to create classes for the following entities:</p><ul><li> represent repository migrations from various sources. They contain configuration and metadata about the repository and how the migration should be performed.  </li><li> represent a batch of migrations. They contain more high-level metadata about groups of individual migration issues such as custom application ids and status of the batch.  </li><li> represent the migration of a group of users, their associated repositories, and the permissions to those repositories.  </li><li> represent Azure DevOps pipelines rewire operations. They allow for the reconfiguration of Azure DevOps pipelines to point to the new GitHub repositories after a migration has been completed.</li></ul><p><strong>Here was the prompt we entered for Copilot:</strong></p><pre><code>Review the create-migration-issue.js and develop a new class called MigrationIssue that contains the properties associated with a migration. This shoud be added to the gh-migrations module where index.js is located.\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>I'll help create a new MigrationIssue class based on the migration configuration and status shown in the code. Here's the step-by-step solution:\n\n1. Create a new file migration-issue.js in the gh-migrations directory\n2. Update the index.js file to expose the new class\n3. Extract configuration and status properties from the create-migration-issues.js file into the new class\n\nmigration-issue.js\nCreate a new class to represent a migration issue with all necessary properties and methods.\n\n&lt;code&gt;\n\nindex.js\nUpdate the index.js to expose the new MigrationIssue class.\n\n&lt;code&gt;\n\nThe new MigrationIssue class encapsulates all the properties that were previously spread across various objects in the create-migration-issues.js file. It provides a clean interface for creating and managing migration issues, with helper methods for generating issue titles and source types.\n\nYou can now use this class in create-migration-issues.js by replacing the manual object creation with the MigrationIssue class. For example, the createMigrationConfig function could be replaced with:\n\n&lt;code&gt;\n</code></pre><p>The process for creating new classes for the other entities was similar to the above. We created new classes for , , and , each with their own properties and methods.</p><p>After we built out our entity classes, we realized they should manage their own state. This is a common pattern in object-oriented programming, where the class is responsible for managing its state and behavior. Remember our goal about following good coding standards?</p><p>The class should include methods to get and set the state of the object, as well as methods to perform actions on the object. It should also be responsible for utilizing the GitHub API and manage the state of the object rather than the client code. This will help standardize and protect the state of the object, as well as give the client code a simpler interface to work with.</p><p><strong>Here was the prompt we entered for Copilot:</strong></p><pre><code>The MigrationIssue class should contain a initializer that uses the gh-api.js to create the resulting GitHub Issue. The initializer should take in an optional parameter called issueNumber, if the issueNumber is omitted the initializer should create a new Migration Issue and return the issue number created, if the issueNumber is passed to the initializer, the gh-api.js should be used to return the title and body of the issue which the class will parse to create the properties of the MigrationIssue.\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>I'll help you modify the code to add initialization logic to the MigrationIssue class. Here's the step-by-step solution:\n\n1. Add a static method to parse issue body into configuration\n2. Add initialization methods to handle both new and existing issues\n3. Modify constructor to accept GHApi instance\n4. Add method to generate issue body\n5. Add method to create or load issue\n\nmigration-issue.js\nUpdate the MigrationIssue class to handle initialization from GHApi.\n\n&lt;code&gt;\n\nThe main changes include:\n\n1. Constructor now accepts a GHApi instance as first parameter\n2. Added parseIssueBody static method to parse configuration from issue body\n3. Added generateIssueBody method to create the issue body format\n4. Added initialize async method that either creates a new issue or loads an existing one\n5. The initialize method returns the issue number in both cases\n\nTo use this updated class:\n\n&lt;code&gt;\n</code></pre><p>Remember how we said it‚Äôs always important to review suggestions from Copilot? In this case, the  method wasn‚Äôt connected to the constructor, like we would expect. We asked Copilot to address this issue with an iterative prompt.</p><p><strong>Here was the prompt we entered for Copilot:</strong></p><pre><code>The MigrationIssue class has an initialize function that encapsulates functionality that should be included in the constructor. Rewire the constuctor method to include the initialzie function.\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>Let's modify the class to move the initialization logic into the constructor. Here's the step-by-step solution:\n\n1. Make constructors async\n2. Move initialization logic into constructor\n3. Remove separate initialize method\n4. Keep static parseIssueBody method\n\nmigration-issue.js\nMove initialization logic into constructor.\n\n&lt;code&gt;\n\nThe main changes:\n\n1. Removed initialize method from the class\n2. Added static create and load factory methods\n3. Added save method to handle issue creation\n4. Constructor remains synchronous but now only handles property initialization\n\nUsage example:\n\n&lt;code&gt;\n\nThis approach follows better object-oriented principles by:\n\n1. Using factory methods for different creation scenarios\n2. Keeping the constructor simple and synchronous\n3. Separating issue creation into a distinct operation\n4. Making the API more intuitive\n</code></pre><p>In this case, Copilot suggested a different approach than we were thinking of with a factory method (which is a great way to handle different scenarios when creating or loading an issue). Even if it wasn‚Äôt what we were thinking, it worked and also kept the constructor clean and focused on just initializing the object. Moreover, the  method is a good way to encapsulate the logic for creating a new issue, and it makes the code more readable.</p><p>Now we just needed to repeat this process with each of the new classes we added.</p><h3>Step 4: Final refactoring<a href=\"https://github.blog/ai-and-ml/github-copilot/how-to-refactor-code-with-github-copilot/#step-4-final-refactoring\" aria-label=\"Step 4: Final refactoring\"></a></h3><p>After we built a common module, we needed to refactor the client code to make it much cleaner and easier to read and maintain. After that, we‚Äôd also be able to identify and add any missing functionality in the common module.</p><p>This was an iterative process, and we needed to go back and forth between the client code and the common module to make sure everything worked properly. Because of the way this project evolved, it afforded us the opportunity to make sweeping changes to the codebase. It‚Äôs much more common to make small, incremental changes to the codebase, especially if it‚Äôs code that‚Äôs actively in use. In both of these cases, don‚Äôt forget to write tests to ensure the code works as expected.</p><p><strong>Here was the prompt we entered for Copilot:</strong></p><pre><code>Using the gh-migrations module, refactor the create-migration-issues.js\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>I'll help you refactor the create-migration-issues.js file to use the gh-migrations module. Here's the step-by-step solution:\n\n1. Import the gh-migrations module\n2. Create GHApi instance\n3. Use MigrationIssue class for creating and managing migration issues\n4. Simplify the main logic using the new classes\n\ncreate-migration-issues.js\nRefactor to use the gh-migrations module for better code organization and reusability.\n\n&lt;code&gt;\n\nThe refactoring:\n\n1. Removes redundant code by using the MigrationIssue class\n2. Simplifies issue creation and management\n3. Uses GHApi for GitHub API calls with built-in rate limiting\n4. Removes duplicate code for different source types\n5. Improves code organization and maintainability\n\nThe main changes:\n  * Removed several helper functions as they're now part of the MigrationIssue class\n  * Simplified the main logic flow\n  * Added better error handling\n  * Used structured configuration object\n  * Maintained the same functionality while reducing code complexity\n</code></pre><p>Now we needed to refactor the remaining scripts in the same manner. For example, this is how we used Copilot to help with refactoring the  script.</p><p><strong>Here was the prompt we entered for Copilot:</strong></p><pre><code>Using the gh-migrations module refactor the create-team-issues.js\n</code></pre><p><strong>And here was Copilot‚Äôs response:</strong></p><pre><code>I'll help you refactor the create-team-issues.js file to use the gh-migrations module. Here's the step-by-step solution:\n\n1. Import the gh-migrations module\n2. Replace manual issue creation with TeamIssue class\n3. Simplify team creation logic using the new class\n4. Maintain the same JiraIssue tracking functionality\n\ncreate-team-issues.js\nRefactor to use the gh-migrations module for creating team issues.\n\n&lt;code&gt;\n\nThe refactoring:\n\n1. Uses the TeamIssue class to manage team issue creation\n2. Simplifies the code by removing redundant config and status objects\n3. Uses GHApi for GitHub operations with built-in rate limiting\n4. Maintains the same JiraIssue tracking functionality\n5. Reduces code duplication by using a loop for team creation\n\nThe code is now more maintainable and follows the same pattern as the migration issues implementation.\n</code></pre><p>From there, it was a matter of rinsing and repeating this process with each of our scripts.</p><p>By first creating the common module, we could guide Copilot  and make sure that the final, refactored codebase was something we could use with multiple organizations. It gave us a foundation we could reuse and customize based on the needs of the system we were migrating.</p><p>Sometimes refactoring code can be really easy, like our first example with the sequential  statements. But the more you dig into it, the more it grows, and you might find yourself facing down the Herculean task of trying to refactor a codebase that stretches across multiple files from a larger team of developers.</p><p>Speaking from experience: It can be daunting to even know where to start.</p><p>Remember to take a step back and start with the basics. Your first step should always be improving your  understanding of the codebase you want to refactor. The more you know about what you‚Äôre starting out with, the greater chance that you will be able to think of ways to refactor it effectively. And once you come up with some ideas, you can always ask GitHub Copilot to help turn your vision into reality. It‚Äôs got your back and is more than willing to help.</p><div><p><strong>Start using GitHub Copilot for free</strong>\nOur free version of GitHub Copilot is included by default in personal GitHub accounts and VS Code to help you start new projects, manage existing repositories, and more.</p></div>","contentLength":24271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.24 is released!","url":"https://go.dev/blog/go1.24","date":1739232000,"author":"Junyang Shao, on behalf of the Go team","guid":201,"unread":true,"content":"<div data-slug=\"/blog/go1.24\"><p>\n      Junyang Shao, on behalf of the Go team\n      11 February 2025\n      </p><p>Today the Go team is excited to release Go 1.24,\nwhich you can get by visiting the <a href=\"https://go.dev/dl/\">download page</a>.</p><p>Go 1.24 comes with many improvements over Go 1.23. Here are some of the notable\nchanges; for the full list, refer to the <a href=\"https://go.dev/doc/go1.24\">release notes</a>.</p><p>Several performance improvements in the runtime have decreased CPU overhead\nby 2‚Äì3% on average across a suite of representative benchmarks. These\nimprovements include a new builtin  implementation based on\n<a href=\"https://abseil.io/about/design/swisstables\" rel=\"noreferrer\" target=\"_blank\">Swiss Tables</a>, more efficient\nmemory allocation of small objects, and a new runtime-internal mutex\nimplementation.</p><ul><li>The  command now provides a mechanism for tracking tool dependencies for a\nmodule. Use  to add a  directive to the current module. Use\n to run the tools declared with the  directive.\nRead more on the <a href=\"https://go.dev/doc/go1.24#go-command\">go command</a> in the release notes.</li><li>The new  analyzer in  subcommand reports common mistakes in\ndeclarations of tests, fuzzers, benchmarks, and examples in test packages.\nRead more on <a href=\"https://go.dev/doc/go1.24#vet\">vet</a> in the release notes.</li></ul><h2>Standard library additions</h2><h2>Improved WebAssembly support</h2><p>Go 1.24 adds a new  directive for Go programs to export\nfunctions to the WebAssembly host, and supports building a Go program as a WASI\n<a href=\"https://github.com/WebAssembly/WASI/blob/63a46f61052a21bfab75a76558485cf097c0dbba/legacy/application-abi.md#current-unstable-abi\" rel=\"noreferrer\" target=\"_blank\">reactor/library</a>.\nRead more on <a href=\"https://go.dev/doc/go1.24#wasm\">WebAssembly</a> in the release notes.</p><p>Please read the <a href=\"https://go.dev/doc/go1.24\">Go 1.24 release notes</a> for the complete and\ndetailed information. Don‚Äôt forget to watch for follow-up blog posts that\nwill go in more depth on some of the topics mentioned here!</p><p>Thank you to everyone who contributed to this release by writing code and\ndocumentation, reporting bugs, sharing feedback, and testing the release\ncandidates. Your efforts helped to ensure that Go 1.24 is as stable as possible.\nAs always, if you notice any problems, please <a href=\"https://go.dev/issue/new\">file an issue</a>.</p></div>","contentLength":1768,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Micro Frontends with Angular and Native Federation","url":"https://blog.angular.dev/micro-frontends-with-angular-and-native-federation-7623cfc5f413?source=rss----447683c3d9a3---4","date":1739199751,"author":"Manfred Steyer","guid":451,"unread":true,"content":"<p>Enterprise-scale software systems are often implemented by several cross-functional teams. To enable such teams to provide new features efficiently, it is desirable to minimize the need for coordination between them. This calls for a modularization that verticalizes the system into low-coupled areas individual teams can take care&nbsp;of.</p><p>There are several possibilities for implementing such high-level modules (also called verticals). For instance, they can be implemented using a respective folder structure or in the form of several libraries in a Monorepo. Micro Frontends go one step further and designate a separate application for each vertical. This architectural style promises several advantages, such as a high amount of team autonomy, but it also comes with numerous challenges.</p><p>The first part of this article provides a critical overview of the benefits and drawbacks of Micro Frontends in the space of single-page applications. The second part discusses how such an architecture can be implemented with Native Federation, a community project built upon web standards that provide close integration with the Angular&nbsp;CLI.</p><h3>Motivation Behind Micro Frontends</h3><p>Like Micro Services, Micro Frontends promise several advantages, both technically and with respect to the organization. As applying Micro Frontend architectures results in several smaller applications, testing, performance tuning, and isolating faults in one part of the overarching system becomes more effortless, according to several&nbsp;sources.</p><p>However, the increased team autonomy was the main reason for applying this architectural style in the numerous cases I was involved in as a consultant. Individual teams are not blocked by waiting for other teams and can deploy separately anytime. This might not be a significant concern in a vast number of projects. Still, as soon as we talk about multi-team projects in a corporate environment with long communication paths and decision times, this aspect quickly becomes vital for the project‚Äôs success.</p><p>Teams can also make their own decisions that best fit their goals‚Ää‚Äî‚Ääarchitecturally and technology-wise. Mixing multiple client-side frameworks in the same application is considered an anti-pattern and should be avoided. However, it can help to create a migration path over to a new stack in the long run. The concern in corporate environments is that we find software solutions that usually outlive the average technology stack</p><p>Since Micro Frontends result in separate build processes, combining them with incremental builds, where only changed applications need to be rebuilt, has a massive potential for build-time improvements. For instance, the well-known <a href=\"https://nx.dev/\">Nx build system</a> provides this option. Interestingly, this feature can also be used without applying other aspects such as aligning teams with individual applications or separate deployments. There is debate about whether leveraging this tempting option automatically leads to micro frontend architectures.</p><p>A system consisting of several smaller applications can provide further organizational advantages: It‚Äôs easier to onboard new members and scale the development by adding further micro frontends. Team autonomy also leads to faster release&nbsp;cycles.</p><h3>Challenges to keep in&nbsp;mind</h3><p>Every architectural decision has consequences that need to be evaluated, and Micro Frontends are no exception. Besides the positive consequences outlined above, there are also several negative ones to consider.</p><p>For instance, individually developed Micro Frontends can diverge in UI/UX, leading to an inconsistent appearance. Also, loading several applications increases the number of bundles that need to be downloaded, adversely affecting loading times and increasing the memory pressure.</p><p>Splitting an application into low-coupled parts might be a best practice in general. However, it is often hard to define the boundaries between verticals clearly enough to implement them as individual applications. Also, while having several small applications at first glance simplifies the implementation, integrating them into an overarching solution brings additional complexity.</p><p>This leads to one of the biggest challenges I have seen in practice: we are moving away from a compile-time integration towards a runtime integration. This has severe consequences because we cannot easily foresee problems that might arise when individually developed and deployed applications start interacting at runtime. Besides the chance of technical conflicts, we also have to see that the current generation of SPA frameworks has not been built with such an operation mode in&nbsp;mind.</p><p>Instead, modern SPA frameworks, especially Angular, have been developed to focus on compile time optimizations. A powerful compiler leverages type checks to identify technical conflicts and emits efficient source code optimized for tree-shaking. Furthermore, the CLI in the Angular space provides a highly optimized build process. An off-label usage necessary for implementing Micro Frontends undermines some of these achievements.</p><h3>Angular Does Not Officially Support Micro Frontends</h3><p>For all the outlined reasons, the Angular team recommends checking if alternatives, like implementing the individual verticals in Monorepos, which can be compiled together, are suitable. For instance, Google adopted this approach years ago and manages all its products and libraries in a <a href=\"https://dl.acm.org/doi/10.1145/2854146\">single Monorepo</a>.</p><p>Of course, there are also ways to compensate for the disadvantages outlined here, and some of them, like establishing a design system to help with a consistent UI/UX or lazy loading individual system parts, might be needed in general. More details on such compensation strategies can be found in this <a href=\"https://www.angulararchitects.io/blog/consequences-of-micro-frontends-survey-results\">survey</a> of more than 150 Micro Frontend practitioners.</p><p>All architectural decisions have benefits and drawbacks and should be evaluated with those considerations if you are going to implement a solution. If such an evaluation reveals that Micro Frontends provide more advantages than alternatives for achieving your very goals, the following sections provide you a well-lit path for implementing this architectural pattern with&nbsp;Angular.</p><h3>Micro Frontends with Federation</h3><p><a href=\"https://module-federation.io/\">Module Federation</a> is a popular technology for implementing Micro Frontends and sharing dependencies. Shipped initially with webpack 5, it comes with a tooling-agnostic runtime and provides compile-time integration into  and . Besides the usage of the vite dev server, these technologies are currently not supported by the Angular CLI. However, promising community solutions like <a href=\"https://angular-rspack.dev/\"></a>and <a href=\"https://analogjs.org/\">AnalogJS</a> allow them to be used with Angular. <a href=\"https://nx.dev/\">Nx</a> and my <a href=\"https://www.npmjs.com/package/@angular-architects/module-federation\">CLI-plugin</a> provide an effortless integration.</p><p>Module Federation enables an application to load parts of other separately built and deployed applications lazily. The loading application is referred to as the host; the integrated ones are called&nbsp;remotes:</p><p>Federation, if permitted by the library version, can share dependencies like Angular or RxJS between the host and remotes. There are several configuration options for <a href=\"https://www.angulararchitects.io/blog/getting-out-of-version-mismatch-hell-with-module-federation/\">preventing version mismatches.</a> Since MF can only decide which dependencies to share at runtime, tree-shaking for shared parts is not possible.</p><p>To inform the host about the remotes and their shared dependencies, Module Federation creates a metadata file, the so-called remote entry, during the build. This file needs to be loaded into the&nbsp;host.</p><p>To fully decouple the idea of Federation from specific bundlers, I started the project <a href=\"https://www.npmjs.com/package/@angular-architects/native-federation\">Native Federation</a> several years ago. Its API surface is very similar to that of Module Federation. The focus is on portability and standards like ECMAScript modules and Import Maps. Its compile time acts as a wrapper around existing bundlers. For the communication with the bundler, it uses an exchangeable adapter:</p><p>The integration into the Angular CLI directly delegates to Angular‚Äôs  that leverages the fast bundler esbuild, and is the foundation for several current features like partial hydration. Because of its architecture, Native Federation can also be ported to further builders or other innovations the CLI might provide in the long&nbsp;run.</p><p>For integrating Micro Frontends built with Angular‚Äôs webpack-based builder, there is a <a href=\"https://www.angulararchitects.io/blog/combining-native-federation-and-module-federation/\">bridging solution</a> allowing the loading of such remotes into a Native Federation host. This solution enables the gradual adoption of the CLI‚Äôs new  and permits the sharing of dependencies between the two kinds of Federation. One of the features added recently is support for <a href=\"https://www.angulararchitects.io/blog/ssr-and-hydration-with-native-federation-for-angular/\">SSR and Hydration</a>, which is vital for performance-critical applications like public portals and web&nbsp;shops.</p><p>Native Federation for Angular is close to the CLI‚Äôs , but its compilation mode is for shared dependencies differs. While it works well for packages that align with <a href=\"https://angular.dev/tools/libraries/angular-package-format\">Angular‚Äôs Package Format</a>, which is the case for all libraries built with the CLI, other libraries might provide some challenges, especially older ones that still use CommonJS or older conventions for providing metadata.</p><h3>Using Native Federation in&nbsp;Angular</h3><p>For the setup Native Federation provides a schematic:</p><pre>ng add @angular-architects/native-federation --project mfe1 --port 4201 --type remote</pre><p>The switch  defines the kind of the application. Possible options are , , and . The latter is a host configured with a configuration file (federation manifest) during application start. This manifest informs the application of the locations of the remotes and can be switched out by another manifest during deployment:</p><pre>{    \"mfe1\" : \"http://localhost:4201/remoteEntry.json\"</pre><p>The key, in this case, is a short name the host uses to refer to the Micro Frontend. The value is the location of the remote entry with the metadata mentioned above. Alternatively, the manifest can be replaced by a service that informs the host of the current location of all deployed remotes and acts as a Micro Frontend registry.</p><p>The schematic configures the Native Federation builder delegation to the  and creates a configuration file :</p><pre>const { withNativeFederation, shareAll }     = require('@angular-architects/native-federation/config');<p>module.exports = withNativeFederation({</p>    './Component': './projects/mfe1/src/app/app.component.ts',    ...shareAll({}),    'rxjs/ajax',    'rxjs/testing',    // Add further packages you don't need at runtime</pre><p>The configuration assigns a unique name to the remote or host and defines which dependencies to share. Instead of providing an exhaustive list of all the dependencies to share, the configuration uses the helper function , which adds all dependencies found in the project‚Äôs . The skip list is used to opt out of sharing some of them or their secondary entry&nbsp;points.</p><p>Remotes also define exposed EcmaScript modules that can be loaded into the shell. For this, the  node maps the paths of the modules to short names such as&nbsp; in the example&nbsp;shown.</p><p>The schematic also adds code to initialize Native Federation to the . For the host, this code points to the federation manifest:</p><pre>import { initFederation } from '@angular-architects/native-federation';<p>initFederation('federation.manifest.json')</p>  .catch(err =&gt; console.error(err))<p>  .then(_ =&gt; import('./bootstrap'))</p>  .catch(err =&gt; console.error(err));</pre><p>After initializing federation, the file , also created by the schematic, is loaded. It contains the usual code for bootstrapping Angular, e.g., via  when the application uses Standalone Components.</p><p>To load a component or routing configuration exposed by a remote, traditional lazy loading is combined with Native Federation‚Äôs  function:</p><pre>import { loadRemoteModule } from '@angular-architects/native-federation';<p>export const APP_ROUTES: Routes = [</p>  [...]    path: 'flights',      loadRemoteModule('mfe1', './Component').then((m) =&gt; m.AppComponent),];</pre><p>Here,  is the key defined in the manifest, and&nbsp; points to the respective exposed module in the remote‚Äôs federation configuration.</p><p>More information on Native Federation can be found in this <a href=\"https://www.angulararchitects.io/blog/micro-frontends-with-modern-angular-part-1-standalone-and-esbuild/\">blog article</a> and in the project‚Äôs <a href=\"https://www.npmjs.com/package/@angular-architects/native-federation\">readme</a>, which also links to a tutorial.</p><p>Micro Frontends promise significant advantages for enterprise-scale applications, such as enhanced team autonomy and independent deployment. These benefits make this architectural style particularly appealing in multi-team corporate environments where streamlined communication and rapid development cycles are critical. Additionally, they support gradual migration to new technologies and optimize build times by leveraging incremental builds.</p><p>However, these advantages come with trade-offs. Micro Frontends can lead to inconsistent UI/UX, increased load times, and complex runtime integrations. Defining clear vertical boundaries and managing inter-application communication add to the challenge. Furthermore, frameworks like Angular, designed for compile-time optimization, face limitations in runtime integration scenarios. The Angular team, therefore, recommends alternatives such as splitting an application into libraries managed within a Monorepo, which aligns better with Angular‚Äôs strengths in type safety and efficient compilation.</p><p>Module Federation has emerged as a popular solution to address some challenges by enabling lazy loading and dependency sharing. Native Federation builds on these concepts with a focus on standards and portability. It provides a seamless integration into the Angular CLI and its performant esbuild-based , which is also the foundation for advanced features like SSR and hydration.</p><p><em>Together with this team at </em><a href=\"http://www.angulararchitects.io\"></a>,<em> Manfred Steyer helps companies around the globe establish maintainable Angular-based Architectures. He is a trainer, consultant, and Google Developer Expert (GDE) and writes for O‚ÄôReilly, the German Java Magazin, windows.developer, and Heise Developer. Furthermore, he regularly speaks at conferences.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=7623cfc5f413\" width=\"1\" height=\"1\" alt=\"\">","contentLength":13726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["official"]}