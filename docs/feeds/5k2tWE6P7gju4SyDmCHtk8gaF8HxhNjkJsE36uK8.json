{"id":"5k2tWE6P7gju4SyDmCHtk8gaF8HxhNjkJsE36uK8","title":"cs updates on arXiv.org","displayTitle":"Arxiv","url":"https://rss.arxiv.org/atom/cs","feedLink":"https://rss.arxiv.org/atom/cs","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":1681,"items":[{"title":"Direct Debiased Machine Learning via Bregman Divergence Minimization","url":"https://arxiv.org/abs/2510.23534","date":1761883200,"author":"","guid":322517,"unread":true,"content":"<article>arXiv:2510.23534v2 Announce Type: replace-cross \nAbstract: We develop a direct debiased machine learning framework comprising Neyman targeted estimation and generalized Riesz regression. Our framework unifies Riesz regression for automatic debiased machine learning, covariate balancing, targeted maximum likelihood estimation (TMLE), and density-ratio estimation. In many problems involving causal effects or structural models, the parameters of interest depend on regression functions. Plugging regression functions estimated by machine learning methods into the identifying equations can yield poor performance because of first-stage bias. To reduce such bias, debiased machine learning employs Neyman orthogonal estimating equations. Debiased machine learning typically requires estimation of the Riesz representer and the regression function. For this problem, we develop a direct debiased machine learning framework with an end-to-end algorithm. We formulate estimation of the nuisance parameters, the regression function and the Riesz representer, as minimizing the discrepancy between Neyman orthogonal scores computed with known and unknown nuisance parameters, which we refer to as Neyman targeted estimation. Neyman targeted estimation includes Riesz representer estimation, and we measure discrepancies using the Bregman divergence. The Bregman divergence encompasses various loss functions as special cases, where the squared loss yields Riesz regression and the Kullback-Leibler divergence yields entropy balancing. We refer to this Riesz representer estimation as generalized Riesz regression. Neyman targeted estimation also yields TMLE as a special case for regression function estimation. Furthermore, for specific pairs of models and Riesz representer estimation methods, we can automatically obtain the covariate balancing property without explicitly solving the covariate balancing objective.</article>","contentLength":1913,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Larger holes as narrower degree distributions in complex networks","url":"https://arxiv.org/abs/2510.22720","date":1761883200,"author":"","guid":322518,"unread":true,"content":"<article>arXiv:2510.22720v2 Announce Type: replace-cross \nAbstract: Although the analysis of loops is not so much because of the complications, it has already been found that heuristically enhancing loops decreases the variance of degree distributions for improving the robustness of connectivity. While many real scale-free networks are known to contain shorter loops such as triangles, it remains to investigate the distributions of longer loops in more wide class of networks. We find a relation between narrower degree distributions and longer loops in investigating the lengths of the shortest loops in various networks with continuously changing degree distributions, including three typical types of realistic scale-free networks, classical Erd\\\"os-R\\'enyi random graphs, and regular networks. In particular, we show that narrower degree distributions contain longer shortest loops, as a universal property in a wide class of random networks. We suggest that the robustness of connectivity is enhanced by constructing long loops of O(log N).</article>","contentLength":1039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GPU-Accelerated Primal Heuristics for Mixed Integer Programming","url":"https://arxiv.org/abs/2510.20499","date":1761883200,"author":"","guid":322519,"unread":true,"content":"<article>arXiv:2510.20499v2 Announce Type: replace-cross \nAbstract: We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer Programming. Leveraging GPU acceleration enables exploration of larger search regions and faster iterations. A GPU-accelerated PDLP serves as an approximate LP solver, while a new probing cache facilitates rapid roundings and early infeasibility detection. Several state-of-the-art heuristics, including Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further accelerated and enhanced. The combined approach of these GPU-driven algorithms yields significant improvements over existing methods, both in the number of feasible solutions and the quality of objectives by achieving 221 feasible solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved dataset.</article>","contentLength":825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cryo-CMOS Antenna for Wireless Communications within a Quantum Computer Cryostat","url":"https://arxiv.org/abs/2510.13627","date":1761883200,"author":"","guid":322520,"unread":true,"content":"<article>arXiv:2510.13627v2 Announce Type: replace-cross \nAbstract: Scaling quantum computers from a few qubits to large numbers remains one of the critical challenges in realizing practical quantum advantage. Multi-core quantum architectures have emerged as a promising solution, enabling scalability through distributed quantum processing units (QPUs) interconnected via classical and quantum links. However, the bottleneck of wired connections persists, as densely packed wired interconnects, both vertically across temperature stages and horizontally within the same layer, introduce spatial constraints, power dissipation, and latency, which could hinder performance as the number of QPUs increases. To overcome these limitations, this work proposes a cryo-compatible on-chip differential dipole antenna operating at 28 GHz to enable short-range wireless communication within a quantum computer cryostat. Temperature-dependent material properties are incorporated to accurately capture antenna behavior at 4 K. Moreover, by embedding the antenna in a realistic cryostat structure, we evaluate the feasibility of antenna operation within the cryogenic environment. The proposed antenna achieves a reflection coefficient of -20.8 dB in free space and -18.38 dB within the cryostat, demonstrating efficient impedance matching.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond the Use-and-then-Forget (UatF) Bound: Fixed Point Algorithms for Statistical Max-Min Power Control","url":"https://arxiv.org/abs/2510.11582","date":1761883200,"author":"","guid":322521,"unread":true,"content":"<article>arXiv:2510.11582v2 Announce Type: replace-cross \nAbstract: We introduce mathematical tools and fixed point algorithms for optimal statistical max-min power control in cellular and cell-less massive MIMO systems. Unlike previous studies that rely on the use-and-then-forget (UatF) lower bound on Shannon achievable (ergodic) rates, our proposed framework can deal with alternative bounds that explicitly consider perfect or imperfect channel state information (CSI) at the decoder. In doing so, we address limitations of UatF-based power control algorithms, which inherit the shortcomings of the UatF bound. For example, the UatF bound can be overly conservative: in extreme cases, under fully statistical (nonadaptive) beamforming in zero-mean channels, the UatF bound produces trivial (zero) rate bounds. It also lacks scale invariance: merely scaling the beamformers can change the bound drastically. In contrast, our framework is compatible with information-theoretic bounds that do not suffer from the above drawbacks. We illustrate the framework by solving a max-min power control problem considering a standard bound that exploits instantaneous CSI at the decoder.</article>","contentLength":1170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rough Path Signatures: Learning Neural RDEs for Portfolio Optimization","url":"https://arxiv.org/abs/2510.10728","date":1761883200,"author":"","guid":322522,"unread":true,"content":"<article>arXiv:2510.10728v2 Announce Type: replace-cross \nAbstract: We tackle high-dimensional, path-dependent valuation and control and introduce a deep BSDE/2BSDE solver that couples truncated log-signatures with a neural rough differential equation (RDE) backbone. The architecture aligns stochastic analysis with sequence-to-path learning: a CVaR-tilted terminal objective targets left-tail risk, while an optional second-order (2BSDE) head supplies curvature estimates for risk-sensitive control. Under matched compute and parameter budgets, the method improves accuracy, tail fidelity, and training stability across Asian and barrier option pricing and portfolio control: at d=200 it achieves CVaR(0.99)=9.80% versus 12.00-13.10% for strong baselines, attains the lowest HJB residual (0.011), and yields the lowest RMSEs for Z and Gamma. Ablations over truncation depth, local windows, and tilt parameters confirm complementary gains from the sequence-to-path representation and the 2BSDE head. Taken together, the results highlight a bidirectional dialogue between stochastic analysis and modern deep learning: stochastic tools inform representations and objectives, while sequence-to-path models expand the class of solvable financial models at scale.</article>","contentLength":1250,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No exponential quantum speedup for $\\mathrm{SIS}^\\infty$ anymore","url":"https://arxiv.org/abs/2510.07515","date":1761883200,"author":"","guid":322523,"unread":true,"content":"<article>arXiv:2510.07515v2 Announce Type: replace-cross \nAbstract: In 2021, Chen, Liu, and Zhandry presented an efficient quantum algorithm for the average-case $\\ell_\\infty$-Short Integer Solution ($\\mathrm{SIS}^\\infty$) problem, in a parameter range outside the normal range of cryptographic interest, but still with no known efficient classical algorithm. This was particularly exciting since $\\mathrm{SIS}^\\infty$ is a simple problem without structure, and their algorithmic techniques were different from those used in prior exponential quantum speedups.\n  We present efficient classical algorithms for all of the $\\mathrm{SIS}^\\infty$ and (more general) Constrained Integer Solution problems studied in their paper, showing there is no exponential quantum speedup anymore.</article>","contentLength":770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction","url":"https://arxiv.org/abs/2510.02781","date":1761883200,"author":"","guid":322524,"unread":true,"content":"<article>arXiv:2510.02781v2 Announce Type: replace-cross \nAbstract: Age Related Macular Degeneration(AMD) has been one of the most leading causes of permanent vision impairment in ophthalmology. Though treatments, such as anti VEGF drugs or photodynamic therapies, were developed to slow down the degenerative process of AMD, there is still no specific cure to reverse vision loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD or AMD itself within the patient retina in early stages is a crucial task to reduce the possibility of vision impairment. Apart from traditional approaches, deep learning based methods, especially attention mechanism based CNNs and GradCAM based XAI analysis on OCT scans, exhibited successful performance in distinguishing AMD retina from normal retinas, making it possible to use AI driven models to aid medical diagnosis and analysis by ophthalmologists regarding AMD. However, though having significant success, previous works mostly focused on prediction performance itself, not pathologies or underlying causal mechanisms of AMD, which can prohibit intervention analysis on specific factors or even lead to less reliable decisions. Thus, this paper introduces a novel causal AMD analysis model: GCVAMD, which incorporates a modified CausalVAE approach that can extract latent causal factors from only raw OCT images. By considering causality in AMD detection, GCVAMD enables causal inference such as treatment simulation or intervention analysis regarding major risk factors: drusen and neovascularization, while returning informative latent causal features that can enhance downstream tasks. Results show that through GCVAMD, drusen status and neovascularization status can be identified with AMD causal mechanisms in GCVAMD latent spaces, which can in turn be used for various tasks from AMD detection(classification) to intervention analysis.</article>","contentLength":1896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReCon-GS: Continuum-Preserved Gaussian Streaming for Fast and Compact Reconstruction of Dynamic Scenes","url":"https://arxiv.org/abs/2509.24325","date":1761883200,"author":"","guid":322525,"unread":true,"content":"<article>arXiv:2509.24325v2 Announce Type: replace-cross \nAbstract: Online free-viewpoint video (FVV) reconstruction is challenged by slow per-frame optimization, inconsistent motion estimation, and unsustainable storage demands. To address these challenges, we propose the Reconfigurable Continuum Gaussian Stream, dubbed ReCon-GS, a novel storage-aware framework that enables high fidelity online dynamic scene reconstruction and real-time rendering. Specifically, we dynamically allocate multi-level Anchor Gaussians in a density-adaptive fashion to capture inter-frame geometric deformations, thereby decomposing scene motion into compact coarse-to-fine representations. Then, we design a dynamic hierarchy reconfiguration strategy that preserves localized motion expressiveness through on-demand anchor re-hierarchization, while ensuring temporal consistency through intra-hierarchical deformation inheritance that confines transformation priors to their respective hierarchy levels. Furthermore, we introduce a storage-aware optimization mechanism that flexibly adjusts the density of Anchor Gaussians at different hierarchy levels, enabling a controllable trade-off between reconstruction fidelity and memory usage. Extensive experiments on three widely used datasets demonstrate that, compared to state-of-the-art methods, ReCon-GS improves training efficiency by approximately 15% and achieves superior FVV synthesis quality with enhanced robustness and stability. Moreover, at equivalent rendering quality, ReCon-GS slashes memory requirements by over 50% compared to leading state-of-the-art methods.</article>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring the Early Universe with Deep Learning","url":"https://arxiv.org/abs/2509.22018","date":1761883200,"author":"","guid":322526,"unread":true,"content":"<article>arXiv:2509.22018v2 Announce Type: replace-cross \nAbstract: Hydrogen is the most abundant element in our Universe. The first generation of stars and galaxies produced photons that ionized hydrogen gas, driving a cosmological event known as the Epoch of Reionization (EoR). The upcoming Square Kilometre Array Observatory (SKAO) will map the distribution of neutral hydrogen during this era, aiding in the study of the properties of these first-generation objects. Extracting astrophysical information will be challenging, as SKAO will produce a tremendous amount of data where the hydrogen signal will be contaminated with undesired foreground contamination and instrumental systematics. To address this, we develop the latest deep learning techniques to extract information from the 2D power spectra of the hydrogen signal expected from SKAO. We apply a series of neural network models to these measurements and quantify their ability to predict the history of cosmic hydrogen reionization, which is connected to the increasing number and efficiency of early photon sources. We show that the study of the early Universe benefits from modern deep learning technology. In particular, we demonstrate that dedicated machine learning algorithms can achieve more than a $0.95$ $R^2$ score on average in recovering the reionization history. This enables accurate and precise cosmological and astrophysical inference of structure formation in the early Universe.</article>","contentLength":1454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Phoenix-VAD: Streaming Semantic Endpoint Detection for Full-Duplex Speech Interaction","url":"https://arxiv.org/abs/2509.20410","date":1761883200,"author":"","guid":322527,"unread":true,"content":"<article>arXiv:2509.20410v3 Announce Type: replace-cross \nAbstract: Spoken dialogue models have significantly advanced intelligent human-computer interaction, yet they lack a plug-and-play full-duplex prediction module for semantic endpoint detection, hindering seamless audio interactions. In this paper, we introduce Phoenix-VAD, an LLM-based model that enables streaming semantic endpoint detection. Specifically, Phoenix-VAD leverages the semantic comprehension capability of the LLM and a sliding window training strategy to achieve reliable semantic endpoint detection while supporting streaming inference. Experiments on both semantically complete and incomplete speech scenarios indicate that Phoenix-VAD achieves excellent and competitive performance. Furthermore, this design enables the full-duplex prediction module to be optimized independently of the dialogue model, providing more reliable and flexible support for next-generation human-computer interaction.</article>","contentLength":964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention","url":"https://arxiv.org/abs/2509.19331","date":1761883200,"author":"","guid":322528,"unread":true,"content":"<article>arXiv:2509.19331v2 Announce Type: replace-cross \nAbstract: Complex-valued signals encode both amplitude and phase, yet most deep models treat attention as real-valued correlation, overlooking interference effects. We introduce the Holographic Transformer, a physics-inspired architecture that incorporates wave interference principles into self-attention. Holographic attention modulates interactions by relative phase and coherently superimposes values, ensuring consistency between amplitude and phase. A dual-headed decoder simultaneously reconstructs the input and predicts task outputs, preventing phase collapse when losses prioritize magnitude over phase. We demonstrate that holographic attention implements a discrete interference operator and maintains phase consistency under linear mixing. Experiments on PolSAR image classification and wireless channel prediction show strong performance, achieving high classification accuracy and F1 scores, low regression error, and increased robustness to phase perturbations. These results highlight that enforcing physical consistency in attention leads to generalizable improvements in complex-valued learning and provides a unified, physics-based framework for coherent signal modeling. The code is available at https://github.com/EonHao/Holographic-Transformers.</article>","contentLength":1317,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PoPStat-COVID19: Leveraging Population Pyramids to Quantify Demographic Vulnerability to COVID-19","url":"https://arxiv.org/abs/2509.14213","date":1761883200,"author":"","guid":322529,"unread":true,"content":"<article>arXiv:2509.14213v2 Announce Type: replace-cross \nAbstract: Understanding how population age structure shapes COVID-19 burden is crucial for pandemic preparedness, yet common summary measures such as median age ignore key distributional features like skewness, bimodality, and the proportional weight of high-risk cohorts. We extend the PoPStat framework, originally devised to link entire population pyramids with cause-specific mortality by applying it to COVID-19. Using 2019 United Nations World Population Prospects age-sex distributions together with cumulative cases and deaths per million recorded up to 5 May 2023 by Our World in Data, we calculate PoPDivergence (the Kullback-Leibler divergence from an optimised reference pyramid) for 180+ countries and derive PoPStat-COVID19 as the Pearson correlation between that divergence and log-transformed incidence or mortality. Optimisation selects Malta's old-skewed pyramid as the reference, yielding strong negative correlations for cases (r=-0.86, p&lt;0.001, R^2=0.74) and deaths (r=-0.82, p&lt;0.001, R^2=0.67). Sensitivity tests across twenty additional, similarly old-skewed references confirm that these associations are robust to reference choice. Benchmarking against eight standard indicators like gross domestic product per capita, Gini index, Human Development Index, life expectancy at birth, median age, population density, Socio-demographic Index, and Universal Health Coverage Index shows that PoPStat-COVID19 surpasses GDP per capita, median age, population density, and several other traditional measures, and outperforms every comparator for fatality burden. PoPStat-COVID19 therefore provides a concise, distribution-aware scalar for quantifying demographic vulnerability to COVID-19.</article>","contentLength":1754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy","url":"https://arxiv.org/abs/2509.09695","date":1761883200,"author":"","guid":322530,"unread":true,"content":"<article>arXiv:2509.09695v2 Announce Type: replace-cross \nAbstract: Machine learning (ML) has the potential to support and improve expert performance in monitoring the brain function of at-risk newborns. Developing accurate and reliable ML models depends on access to high-quality, annotated data, a resource in short supply. ML competitions address this need by providing researchers access to expertly annotated datasets, fostering shared learning through direct model comparisons, and leveraging the benefits of crowdsourcing diverse expertise. We compiled a retrospective dataset containing 353 hours of EEG from 102 individual newborns from a multi-centre study. The data was fully anonymised and divided into training, testing, and held-out validation datasets. EEGs were graded for the severity of abnormal background patterns. Next, we created a web-based competition platform and hosted a machine learning competition to develop ML models for classifying the severity of EEG background patterns in newborns. After the competition closed, the top 4 performing models were evaluated offline on a separate held-out validation dataset. Although a feature-based model ranked first on the testing dataset, deep learning models generalised better on the validation sets. All methods had a significant decline in validation performance compared to the testing performance. This highlights the challenges for model generalisation on unseen data, emphasising the need for held-out validation datasets in ML studies with neonatal EEG. The study underscores the importance of training ML models on large and diverse datasets to ensure robust generalisation. The competition's outcome demonstrates the potential for open-access data and collaborative ML development to foster a collaborative research environment and expedite the development of clinical decision-support tools for neonatal neuromonitoring.</article>","contentLength":1893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes","url":"https://arxiv.org/abs/2509.06159","date":1761883200,"author":"","guid":322531,"unread":true,"content":"<article>arXiv:2509.06159v3 Announce Type: replace-cross \nAbstract: The growing popularity of robotic minimally invasive surgeries has made deep learning-based surgical training a key area of research. A thorough understanding of the surgical scene components is crucial, which semantic segmentation models can help achieve. However, most existing work focuses on surgical tools and overlooks anatomical objects. Additionally, current state-of-the-art (SOTA) models struggle to balance capturing high-level contextual features and low-level edge features. We propose a Feature-Adaptive Spatial Localization model (FASL-Seg), designed to capture features at multiple levels of detail through two distinct processing streams, namely a Low-Level Feature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream, for varying feature resolutions - enabling precise segmentation of anatomy and surgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark datasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model achieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy segmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU of 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation, respectively, outperforming SOTA overall performance, with comparable per-class SOTA results in both datasets and consistent performance in various classes for anatomy and instruments, demonstrating the effectiveness of distinct processing streams for varying feature resolutions.</article>","contentLength":1550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nearly Minimax Discrete Distribution Estimation in Kullback-Leibler Divergence with High Probability","url":"https://arxiv.org/abs/2507.17316","date":1761883200,"author":"","guid":322532,"unread":true,"content":"<article>arXiv:2507.17316v2 Announce Type: replace-cross \nAbstract: We consider the fundamental problem of estimating a discrete distribution on a domain of size~$K$ with high probability in Kullback-Leibler divergence. We provide upper and lower bounds on the minimax estimation rate, which show that the optimal rate is between $\\big(K + \\ln(K)\\ln(1/\\delta)\\big) /n$ and $\\big(K\\ln\\ln(K) + \\ln(K)\\ln(1/\\delta)\\big) /n$ at error probability $\\delta$ and sample size $n$, which pins down the rate up to the doubly logarithmic factor $\\ln \\ln K$ that multiplies $K$. Our upper bound uses techniques from online learning to construct a novel estimator via online-to-batch conversion. Perhaps surprisingly, the tail behavior of the minimax rate is worse than for the squared total variation and squared Hellinger distance, for which it is $\\big(K + \\ln(1/\\delta)\\big) /n$, i.e.\\ without the $\\ln K$ multiplying $\\ln (1/\\delta)$. As a consequence, we cannot obtain a fully tight lower bound from the usual reduction to these smaller distances. Moreover, we show that this lower bound cannot be achieved by the standard lower bound approach based on a reduction to hypothesis testing, and instead we need to introduce a new reduction to what we call weak hypothesis testing. We investigate the source of the gap with other divergences further in refined results, which show that the total variation rate is achievable for Kullback-Leibler divergence after all (in fact by he maximum likelihood estimator) if we rule out outcome probabilities smaller than $O(\\ln(K/\\delta) / n)$, which is a vanishing set as $n$ increases for fixed $K$ and~$\\delta$. This explains why minimax Kullback-Leibler estimation is more difficult than asymptotic estimation.</article>","contentLength":1734,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood","url":"https://arxiv.org/abs/2507.08896","date":1761883200,"author":"","guid":322533,"unread":true,"content":"<article>arXiv:2507.08896v2 Announce Type: replace-cross \nAbstract: This study introduces an integrated framework for predictive causal inference designed to overcome limitations inherent in conventional single model approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial health state estimation with a Multi Task and Multi Graph Convolutional Network (MTGCN) for capturing temporal outcome trajectories. The framework asymmetrically treats temporal and spatial information regarding them as endogenous variables in the outcome regression, and exogenous variables in the propensity score model, thereby expanding the standard doubly robust treatment effect estimation to jointly enhance bias correction and predictive accuracy. To demonstrate its utility, we focus on clinical domains such as cancer, dementia, and Parkinson disease, where treatment effects are challenging to observe directly. Simulation studies are conducted to emulate latent disease dynamics and evaluate the model performance under varying conditions. Overall, the proposed framework advances predictive causal inference by structurally adapting to spatiotemporal complexities common in biomedical data.</article>","contentLength":1188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Abstract computation over first-order structures. Part IIb: Moschovakis' operator and other non-determinisms","url":"https://arxiv.org/abs/2507.03827","date":1761883200,"author":"","guid":322534,"unread":true,"content":"<article>arXiv:2507.03827v2 Announce Type: replace-cross \nAbstract: BSS RAMs were introduced to provide a mathematical framework for characterizing algorithms over first-order structures. Non-deterministic BSS RAMs help to model different non-deterministic approaches. Here, we deal with different types of binary non-determinisms and study the consequences of the decidability of the identity relation and the decidability of finite sets consisting of one or two constants. We compare the binary non-determinism resulting from a non-deterministic branching process, the digital non-determinism resulting from the restriction of guesses to two constants, and some other non-determinisms resulting from the use of Moschovakis' operator applied to oracle sets restricted to tuples of constants. Moreover, we show that the performance capability and the efficiency of individual machines are influenced by the following properties. 1. The identity relation belongs to the underlying structure. 2. The identity is semi-decidable over the underlying structure. 3. Two single-element sets of constants are semi-decidable. 4. A set of two constants is semi-decidable. The order of these properties corresponds to the strength of their influence. In all cases mentioned, the semi-decidability of the sets implies their decidability.</article>","contentLength":1315,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partially-Supervised Neural Network Model For Quadratic Multiparametric Programming","url":"https://arxiv.org/abs/2506.05567","date":1761883200,"author":"","guid":322535,"unread":true,"content":"<article>arXiv:2506.05567v2 Announce Type: replace-cross \nAbstract: Neural Networks (NN) with ReLU activation functions are used to model multiparametric quadratic optimization problems (mp-QP) in diverse engineering applications. Researchers have suggested leveraging the piecewise affine property of deep NN models to solve mp-QP with linear constraints, which also exhibit piecewise affine behaviour. However, traditional deep NN applications to mp-QP fall short of providing optimal and feasible predictions, even when trained on large datasets. This study proposes a partially-supervised NN (PSNN) architecture that directly represents the mathematical structure of the global solution function. In contrast to generic NN training approaches, the proposed PSNN method derives a large proportion of model weights directly from the mathematical properties of the optimization problem, producing more accurate solutions despite significantly smaller training data sets. Many energy management problems are formulated as QP, so we apply the proposed approach to energy systems (specifically DC optimal power flow) to demonstrate proof of concept. Model performance in terms of solution accuracy and speed of predictions was compared against a commercial solver and a generic Deep NN model based on classical training. Results show KKT sufficient conditions for PSNN consistently outperform generic NN architectures with classical training using far less data, including when tested on extreme, out-of-training distribution test data. Given its speed advantages over traditional solvers, the PSNN model can quickly produce optimal and feasible solutions within a second for millions of input parameters sampled from a distribution of stochastic demands and renewable generator dispatches, which can be used for simulations and long term planning.</article>","contentLength":1837,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection","url":"https://arxiv.org/abs/2506.03237","date":1761883200,"author":"","guid":322536,"unread":true,"content":"<article>arXiv:2506.03237v2 Announce Type: replace-cross \nAbstract: The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein-ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite.</article>","contentLength":1746,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Online Change Detection via Random Fourier Features","url":"https://arxiv.org/abs/2505.17789","date":1761883200,"author":"","guid":322537,"unread":true,"content":"<article>arXiv:2505.17789v2 Announce Type: replace-cross \nAbstract: This article studies the problem of online non-parametric change point detection in multivariate data streams. We approach the problem through the lens of kernel-based two-sample testing and introduce a sequential testing procedure based on random Fourier features, running with logarithmic time complexity per observation and with overall logarithmic space complexity. The algorithm has two advantages compared to the state of the art. First, our approach is genuinely online, and no access to training data known to be from the pre-change distribution is necessary. Second, the algorithm does not require the user to specify a window parameter over which local tests are to be calculated. We prove strong theoretical guarantees on the algorithm's performance, including information-theoretic bounds demonstrating that the detection delay is optimal in the minimax sense. Numerical studies on real and synthetic data show that our algorithm is competitive with respect to the state of the art.</article>","contentLength":1053,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Fourier-based inference method for learning interaction kernels in particle systems","url":"https://arxiv.org/abs/2505.05207","date":1761883200,"author":"","guid":322538,"unread":true,"content":"<article>arXiv:2505.05207v2 Announce Type: replace-cross \nAbstract: We consider the problem of inferring the interaction kernel of stochastic interacting particle systems from observations of a single particle. We adopt a semi-parametric approach and represent the interaction kernel in terms of a generalized Fourier series. The basis functions in this expansion are tailored to the problem at hand and are chosen to be orthogonal polynomials with respect to the invariant measure of the mean-field dynamics. The generalized Fourier coefficients are obtained as the solution of an appropriate linear system whose coefficients depend on the moments of the invariant measure, and which are approximated from the particle trajectory that we observe. We quantify the approximation error in the Lebesgue space weighted by the invariant measure and study the asymptotic properties of the estimator in the joint limit as the observation interval and the number of particles tend to infinity, i.e. the joint large time-mean field limit. We also explore the regime where an increasing number of generalized Fourier coefficients is needed to represent the interaction kernel. Our theoretical results are supported by extensive numerical simulations.</article>","contentLength":1231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accurate predictive model of band gap with selected important features based on explainable machine learning","url":"https://arxiv.org/abs/2503.04492","date":1761883200,"author":"","guid":322539,"unread":true,"content":"<article>arXiv:2503.04492v2 Announce Type: replace-cross \nAbstract: In the rapidly advancing field of materials informatics, nonlinear machine learning models have demonstrated exceptional predictive capabilities for material properties. However, their black-box nature limits interpretability, and they may incorporate features that do not contribute to, or even deteriorate, model performance. This study employs explainable ML (XML) techniques, including permutation feature importance and the SHapley Additive exPlanation, applied to a pristine support vector regression model designed to predict band gaps at the GW level using 18 input features. Guided by XML-derived individual feature importance, a simple framework is proposed to construct reduced-feature predictive models. Model evaluations indicate that an XML-guided compact model, consisting of the top five features, achieves comparable accuracy to the pristine model on in-domain datasets (0.254 vs. 0.247 eV) while demonstrating superior generalization with lower prediction errors on out-of-domain data (0.461 vs. 0.341 eV). Additionally, the study underscores the necessity for eliminating strongly correlated features (correlation coefficient greater than 0.8) to prevent misinterpretation and overestimation of feature importance before applying XML. This study highlights XML's effectiveness in developing simplified yet highly accurate machine learning models by clarifying feature roles, thereby reducing computational costs for feature acquisition and enhancing model trustworthiness for materials discovery.</article>","contentLength":1574,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Impact of Performative Risk Minimization for Binary Random Variables","url":"https://arxiv.org/abs/2502.02331","date":1761883200,"author":"","guid":322540,"unread":true,"content":"<article>arXiv:2502.02331v2 Announce Type: replace-cross \nAbstract: Performativity, the phenomenon where outcomes are influenced by predictions, is particularly prevalent in social contexts where individuals strategically respond to a deployed model. In order to preserve the high accuracy of machine learning models under distribution shifts caused by performativity, Perdomo et al. (2020) introduced the concept of performative risk minimization (PRM). While this framework ensures model accuracy, it overlooks the impact of the PRM on the underlying distributions and the predictions of the model. In this paper, we initiate the analysis of the impact of PRM, by studying performativity for a sequential performative risk minimization problem with binary random variables and linear performative shifts. We formulate two natural measures of impact. In the case of full information, where the distribution dynamics are known, we derive explicit formulas for the PRM solution and our impact measures. In the case of partial information, we provide performative-aware statistical estimators, as well as simulations. Our analysis contrasts PRM to alternatives that do not model data shift and indicates that PRM can have amplified side effects compared to such methods.</article>","contentLength":1259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond likelihood ratio bias: Nested multi-time-scale stochastic approximation for likelihood-free parameter estimation","url":"https://arxiv.org/abs/2411.12995","date":1761883200,"author":"","guid":322541,"unread":true,"content":"<article>arXiv:2411.12995v2 Announce Type: replace-cross \nAbstract: We study parameter inference in simulation-based stochastic models where the analytical form of the likelihood is unknown. The main difficulty is that score evaluation as a ratio of noisy Monte Carlo estimators induces bias and instability, which we overcome with a ratio-free nested multi-time-scale (NMTS) stochastic approximation (SA) method that simultaneously tracks the score and drives the parameter update. We provide a comprehensive theoretical analysis of the proposed NMTS algorithm for solving likelihood-free inference problems, including strong convergence, asymptotic normality, and convergence rates. We show that our algorithm can eliminate the original asymptotic bias $O\\big(\\sqrt{\\frac{1}{N}}\\big)$ and accelerate the convergence rate from $O\\big(\\beta_k+\\sqrt{\\frac{1}{N}}\\big)$ to $O\\big(\\frac{\\beta_k}{\\alpha_k}+\\sqrt{\\frac{\\alpha_k}{N}}\\big)$, where $N$ is the fixed batch size, $\\alpha_k$ and $\\beta_k$ are decreasing step sizes with $\\alpha_k$, $\\beta_k$, $\\beta_k/\\alpha_k\\rightarrow 0$. With proper choice of $\\alpha_k$ and $\\beta_k$, our convergence rates can match the optimal rate in the multi-time-scale SA literature. Numerical experiments demonstrate that our algorithm can improve the estimation accuracy by one to two orders of magnitude at the same computational cost, making it efficient for parameter estimation in stochastic systems.</article>","contentLength":1432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Protein Sequence and Expression Level to Analysis Molecular Characterization of Breast Cancer Subtypes","url":"https://arxiv.org/abs/2410.01755","date":1761883200,"author":"","guid":322542,"unread":true,"content":"<article>arXiv:2410.01755v3 Announce Type: replace-cross \nAbstract: Breast cancer's complexity and variability pose significant challenges in understanding its progression and guiding effective treatment. This study aims to integrate protein sequence data with expression levels to improve the molecular characterization of breast cancer subtypes and predict clinical outcomes. Using ProtGPT2, a language model specifically designed for protein sequences, we generated embeddings that capture the functional and structural properties of proteins. These embeddings were integrated with protein expression levels to form enriched biological representations, which were analyzed using machine learning methods, such as ensemble K-means for clustering and XGBoost for classification. Our approach enabled the successful clustering of patients into biologically distinct groups and accurately predicted clinical outcomes such as survival and biomarker status, achieving high performance metrics, notably an F1 score of 0.88 for survival and 0.87 for biomarker status prediction. Feature importance analysis identified KMT2C, CLASP2, and MYO1B as key proteins involved in hormone signaling, cytoskeletal remodeling, and therapy resistance in hormone receptor-positive and triple-negative breast cancer, with potential influence on breast cancer subtype behavior and progression. Furthermore, protein-protein interaction networks and correlation analyses revealed functional interdependencies among proteins that may influence the behavior and progression of breast cancer subtypes. These findings suggest that integrating protein sequence and expression data provides valuable insights into tumor biology and has significant potential to enhance personalized treatment strategies in breast cancer care.</article>","contentLength":1787,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Infinite-dimensional Mahalanobis Distance with Applications to Kernelized Novelty Detection","url":"https://arxiv.org/abs/2407.11873","date":1761883200,"author":"","guid":322543,"unread":true,"content":"<article>arXiv:2407.11873v3 Announce Type: replace-cross \nAbstract: The Mahalanobis distance is a classical tool used to measure the covariance-adjusted distance between points in $\\bbR^d$. In this work, we extend the concept of Mahalanobis distance to separable Banach spaces by reinterpreting it as a Cameron-Martin norm associated with a probability measure. This approach leads to a basis-free, data-driven notion of anomaly distance through the so-called variance norm, which can naturally be estimated using empirical measures of a sample. Our framework generalizes the classical $\\bbR^d$, functional $(L^2[0,1])^d$, and kernelized settings; importantly, it incorporates non-injective covariance operators. We prove that the variance norm is invariant under invertible bounded linear transformations of the data, extending previous results which are limited to unitary operators. In the Hilbert space setting, we connect the variance norm to the RKHS of the covariance operator, and establish consistency and convergence results for estimation using empirical measures with Tikhonov regularization. Using the variance norm, we introduce the notion of a kernelized nearest-neighbour Mahalanobis distance, and study some of its finite-sample concentration properties. In an empirical study on 12 real-world data sets, we demonstrate that the kernelized nearest-neighbour Mahalanobis distance outperforms the traditional kernelized Mahalanobis distance for multivariate time series novelty detection, using state-of-the-art time series kernels such as the signature, global alignment, and Volterra reservoir kernels.</article>","contentLength":1610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Random pairing MLE for estimation of item parameters in Rasch model","url":"https://arxiv.org/abs/2406.13989","date":1761883200,"author":"","guid":322544,"unread":true,"content":"<article>arXiv:2406.13989v2 Announce Type: replace-cross \nAbstract: The Rasch model, a classical model in the item response theory, is widely used in psychometrics to model the relationship between individuals' latent traits and their binary responses to assessments or questionnaires. In this paper, we introduce a new likelihood-based estimator -- random pairing maximum likelihood estimator ($\\mathrm{RP\\text{-}MLE}$) and its bootstrapped variant multiple random pairing MLE ($\\mathrm{MRP\\text{-}MLE}$) which faithfully estimate the item parameters in the Rasch model. The new estimators have several appealing features compared to existing ones. First, both work for sparse observations, an increasingly important scenario in the big data era. Second, both estimators are provably minimax optimal in terms of finite sample $\\ell_{\\infty}$ estimation error. Lastly, both admit precise distributional characterization that allows uncertainty quantification on the item parameters, e.g., construction of confidence intervals for the item parameters. The main idea underlying $\\mathrm{RP\\text{-}MLE}$ and $\\mathrm{MRP\\text{-}MLE}$ is to randomly pair user-item responses to form item-item comparisons. This is carefully designed to reduce the problem size while retaining statistical independence. We also provide empirical evidence of the efficacy of the two new estimators using both simulated and real data.</article>","contentLength":1401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detecting the Use of Generative AI in Crowdsourced Surveys: Implications for Data Integrity","url":"https://arxiv.org/abs/2510.24594","date":1761883200,"author":"","guid":322545,"unread":true,"content":"<article>arXiv:2510.24594v2 Announce Type: replace \nAbstract: The widespread adoption of generative AI (GenAI) has introduced new challenges in crowdsourced data collection, particularly in survey-based research. While GenAI offers powerful capabilities, its unintended use in crowdsourcing, such as generating automated survey responses, threatens the integrity of empirical research and complicates efforts to understand public opinion and behavior. In this study, we investigate and evaluate two approaches for detecting AI-generated responses in online surveys: LLM-based detection and signature-based detection. We conducted experiments across seven survey studies, comparing responses collected before 2022 with those collected after the release of ChatGPT. Our findings reveal a significant increase in AI-generated responses in the post-2022 studies, highlighting how GenAI may silently distort crowdsourced data. This work raises broader concerns about evolving landscape of data integrity, where GenAI can compromise data quality, mislead researchers, and influence downstream findings in fields such as health, politics, and social behavior. By surfacing detection strategies and empirical evidence of GenAI's impact, we aim to contribute to ongoing conversation about safeguarding research integrity and supporting scholars navigating these methodological and ethical challenges.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization","url":"https://arxiv.org/abs/2510.24592","date":1761883200,"author":"","guid":322546,"unread":true,"content":"<article>arXiv:2510.24592v2 Announce Type: replace \nAbstract: Autoformalization, which translates natural language mathematics into machine-verifiable formal statements, is critical for using formal mathematical reasoning to solve math problems stated in natural language. While Large Language Models can generate syntactically correct formal statements, they often fail to preserve the original problem's semantic intent. This limitation arises from the LLM approaches' treating autoformalization as a simplistic translation task which lacks mechanisms for self-reflection and iterative refinement that human experts naturally employ. To address these issues, we propose ReForm, a Reflective Autoformalization method that tightly integrates semantic consistency evaluation into the autoformalization process. This enables the model to iteratively generate formal statements, assess its semantic fidelity, and self-correct identified errors through progressive refinement. To effectively train this reflective model, we introduce Prospective Bounded Sequence Optimization (PBSO), which employs different rewards at different sequence positions to ensure that the model develops both accurate autoformalization and correct semantic validations, preventing superficial critiques that would undermine the purpose of reflection. Extensive experiments across four autoformalization benchmarks demonstrate that ReForm achieves an average improvement of 22.6 percentage points over the strongest baselines. To further ensure evaluation reliability, we introduce ConsistencyCheck, a benchmark of 859 expert-annotated items that not only validates LLMs as judges but also reveals that autoformalization is inherently difficult: even human experts produce semantic errors in up to 38.5% of cases.</article>","contentLength":1777,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Audio Signal Processing Using Time Domain Mel-Frequency Wavelet Coefficient","url":"https://arxiv.org/abs/2510.24519","date":1761883200,"author":"","guid":322547,"unread":true,"content":"<article>arXiv:2510.24519v2 Announce Type: replace \nAbstract: Extracting features from the speech is the most critical process in speech signal processing. Mel Frequency Cepstral Coefficients (MFCC) are the most widely used features in the majority of the speaker and speech recognition applications, as the filtering in this feature is similar to the filtering taking place in the human ear. But the main drawback of this feature is that it provides only the frequency information of the signal but does not provide the information about at what time which frequency is present. The wavelet transform, with its flexible time-frequency window, provides time and frequency information of the signal and is an appropriate tool for the analysis of non-stationary signals like speech. On the other hand, because of its uniform frequency scaling, a typical wavelet transform may be less effective in analysing speech signals, have poorer frequency resolution in low frequencies, and be less in line with human auditory perception. Hence, it is necessary to develop a feature that incorporates the merits of both MFCC and wavelet transform. A great deal of studies are trying to combine both these features. The present Wavelet Transform based Mel-scaled feature extraction methods require more computation when a wavelet transform is applied on top of Mel-scale filtering, since it adds extra processing steps. Here we are proposing a method to extract Mel scale features in time domain combining the concept of wavelet transform, thus reducing the computational burden of time-frequency conversion and the complexity of wavelet extraction. Combining our proposed Time domain Mel frequency Wavelet Coefficient(TMFWC) technique with the reservoir computing methodology has significantly improved the efficiency of audio signal processing.</article>","contentLength":1823,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CodeWiki: Evaluating AI's Ability to Generate Holistic Documentation for Large-Scale Codebases","url":"https://arxiv.org/abs/2510.24428","date":1761883200,"author":"","guid":322548,"unread":true,"content":"<article>arXiv:2510.24428v2 Announce Type: replace \nAbstract: Given a large and evolving codebase, the ability to automatically generate holistic, architecture-aware documentation that captures not only individual functions but also cross-file, cross-module, and system-level interactions remains an open challenge. Comprehensive documentation is essential for long-term software maintenance and collaboration, yet current automated approaches still fail to model the rich semantic dependencies and architectural structures that define real-world software systems. We present \\textbf{CodeWiki}, a unified framework for automated repository-level documentation across seven programming languages. CodeWiki introduces three key innovations: (i) hierarchical decomposition that preserves architectural context across multiple levels of granularity, (ii) recursive multi-agent processing with dynamic task delegation for scalable generation, and (iii) multi-modal synthesis that integrates textual descriptions with visual artifacts such as architecture diagrams and data-flow representations. To enable rigorous evaluation, we introduce \\textbf{CodeWikiBench}, a comprehensive benchmark featuring multi-dimensional rubrics and LLM-based assessment protocols. Experimental results show that CodeWiki achieves a 68.79\\% quality score with proprietary models, outperforming the closed-source DeepWiki baseline (64.06\\%) by 4.73\\%, with particularly strong improvements on high-level scripting languages (+10.47\\%). We open-source CodeWiki to foster future research and community adoption.</article>","contentLength":1573,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI for a Planet Under Pressure","url":"https://arxiv.org/abs/2510.24373","date":1761883200,"author":"","guid":322549,"unread":true,"content":"<article>arXiv:2510.24373v2 Announce Type: replace \nAbstract: Artificial intelligence (AI) is already driving scientific breakthroughs in a variety of research fields, ranging from the life sciences to mathematics. This raises a critical question: can AI be applied both responsibly and effectively to address complex and interconnected sustainability challenges? This report is the result of a collaboration between the Stockholm resilience Centre (Stockholm University), the Potsdam Institute for Climate Impact Research (PIK), and Google DeepMind. Our work explores the potential and limitations of using AI as a research method to help tackle eight broad sustainability challenges. The results build on iterated expert dialogues and assessments, a systematic AI-supported literature overview including over 8,500 academic publications, and expert deep-dives into eight specific issue areas. The report also includes recommendations to sustainability scientists, research funders, the private sector, and philanthropies.</article>","contentLength":1014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VC4VG: Optimizing Video Captions for Text-to-Video Generation","url":"https://arxiv.org/abs/2510.24134","date":1761883200,"author":"","guid":322550,"unread":true,"content":"<article>arXiv:2510.24134v2 Announce Type: replace \nAbstract: Recent advances in text-to-video (T2V) generation highlight the critical role of high-quality video-text pairs in training models capable of producing coherent and instruction-aligned videos. However, strategies for optimizing video captions specifically for T2V training remain underexplored. In this paper, we introduce VC4VG (Video Captioning for Video Generation), a comprehensive caption optimization framework tailored to the needs of T2V models. We begin by analyzing caption content from a T2V perspective, decomposing the essential elements required for video reconstruction into multiple dimensions, and proposing a principled caption design methodology. To support evaluation, we construct VC4VG-Bench, a new benchmark featuring fine-grained, multi-dimensional, and necessity-graded metrics aligned with T2V-specific requirements. Extensive T2V fine-tuning experiments demonstrate a strong correlation between improved caption quality and video generation performance, validating the effectiveness of our approach. We release all benchmark tools and code at https://github.com/alimama-creative/VC4VG to support further research.</article>","contentLength":1192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling","url":"https://arxiv.org/abs/2510.24106","date":1761883200,"author":"","guid":322551,"unread":true,"content":"<article>arXiv:2510.24106v2 Announce Type: replace \nAbstract: Aerodynamic simulation of the surface pressure field around objects is crucial for many engineering problems. In recent years, deep neural networks have emerged as an efficient alternative to traditional, computationally expensive CFD simulations for modeling surface pressure fields. However, data scarcity remains a fundamental challenge, limiting the application of neural networks. To address this limitation, we propose to integrate aerodynamic data from multiple subfields and conduct joint training to learn more general field representations. We consolidate five different datasets covering various fields, including automobiles, trains, aircraft, and general shapes. Facing significant data differences across different domains, we propose UniField, which employs a domain-agnostic Transformer module to extract general point cloud features and customizes domain-specific flow-conditioned adapters to adapt to the flow information in different subfields. Despite the fact that aerodynamic data from different subfields are typically governed by different equations, we compare models trained jointly on all data with those trained separately on individual datasets and find that the jointly-trained model commonly demonstrates better performance. This indicates that these data complement each other to help the model learn better flow field representations. These results highlight the potential of UniField as a universal flow field representation model and lay the foundation for broader applications of neural networks in aerodynamic analysis.</article>","contentLength":1609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection","url":"https://arxiv.org/abs/2510.24043","date":1761883200,"author":"","guid":322552,"unread":true,"content":"<article>arXiv:2510.24043v2 Announce Type: replace \nAbstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection framework that overcomes the coexisting limitations of conventional projection-based methods: their reliance on a fixed statistical metric and their assumption of a single data structure. Our framework uniquely synthesizes three key concepts: (1) a generalized loss-based outlyingness measure (PLO) that replaces the fixed metric with flexible, adaptive loss functions like our proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear data structures; and (3) a subsequent local clustering stage to handle multi-modal distributions. Comprehensive 5-fold cross-validation experiments on 10 benchmark datasets, with automated hyperparameter optimization, demonstrate that Two-Stage LKPLO achieves state-of-the-art performance. It significantly outperforms strong baselines on datasets with challenging structures where existing methods fail, most notably on multi-cluster data (Optdigits) and complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study empirically confirms that the synergistic combination of both the kernelization and localization stages is indispensable for its superior performance. This work contributes a powerful new tool for a significant class of outlier detection problems and underscores the importance of hybrid, multi-stage architectures.</article>","contentLength":1431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TEXT2DB: Integration-Aware Information Extraction with Large Language Model Agents","url":"https://arxiv.org/abs/2510.24014","date":1761883200,"author":"","guid":322553,"unread":true,"content":"<article>arXiv:2510.24014v2 Announce Type: replace \nAbstract: The task of information extraction (IE) is to extract structured knowledge from text. However, it is often not straightforward to utilize IE output due to the mismatch between the IE ontology and the downstream application needs. We propose a new formulation of IE TEXT2DB that emphasizes the integration of IE output and the target database (or knowledge base). Given a user instruction, a document set, and a database, our task requires the model to update the database with values from the document set to satisfy the user instruction. This task requires understanding user instructions for what to extract and adapting to the given DB/KB schema for how to extract on the fly. To evaluate this new task, we introduce a new benchmark featuring common demands such as data infilling, row population, and column addition. In addition, we propose an LLM agent framework OPAL (Observe-PlanAnalyze LLM) which includes an Observer component that interacts with the database, the Planner component that generates a code-based plan with calls to IE models, and the Analyzer component that provides feedback regarding code quality before execution. Experiments show that OPAL can successfully adapt to diverse database schemas by generating different code plans and calling the required IE models. We also highlight difficult cases such as dealing with large databases with complex dependencies and extraction hallucination, which we believe deserve further investigation. Source code: https://github.com/yzjiao/Text2DB</article>","contentLength":1565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TeleEgo: Benchmarking Egocentric AI Assistants in the Wild","url":"https://arxiv.org/abs/2510.23981","date":1761883200,"author":"","guid":322554,"unread":true,"content":"<article>arXiv:2510.23981v2 Announce Type: replace \nAbstract: Egocentric AI assistants in real-world settings must process multi-modal inputs (video, audio, text), respond in real time, and retain evolving long-term memory. However, existing benchmarks typically evaluate these abilities in isolation, lack realistic streaming scenarios, or support only short-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming, omni-modal benchmark for evaluating egocentric AI assistants in realistic daily contexts. The dataset features over 14 hours per participant of synchronized egocentric video, audio, and text across four domains: work \\&amp; study, lifestyle \\&amp; routines, social activities, and outings \\&amp; culture. All data is aligned on a unified global timeline and includes high-quality visual narrations and speech transcripts, curated through human refinement.TeleEgo defines 12 diagnostic subtasks across three core capabilities: Memory (recalling past events), Understanding (interpreting the current moment), and Cross-Memory Reasoning (linking distant events). It contains 3,291 human-verified QA items spanning multiple question formats (single-choice, binary, multi-choice, and open-ended), evaluated strictly in a streaming setting. We propose two key metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess correctness, temporal responsiveness, and long-term retention. TeleEgo provides a realistic and comprehensive evaluation to advance the development of practical AI assistants.</article>","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reasoning Visual Language Model for Chest X-Ray Analysis","url":"https://arxiv.org/abs/2510.23968","date":1761883200,"author":"","guid":322555,"unread":true,"content":"<article>arXiv:2510.23968v2 Announce Type: replace \nAbstract: Vision-language models (VLMs) have shown strong promise for medical image analysis, but most remain opaque, offering predictions without the transparent, stepwise reasoning clinicians rely on. We present a framework that brings chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by reasoning-first training paradigms, our approach is designed to learn how experts reason, not just what they conclude, by aligning intermediate steps with observable image evidence and radiology workflow. Beyond accuracy, the explicit reasoning traces support clinical auditability: they reveal why a conclusion was reached, which alternatives were considered, and where uncertainty remains, enabling quality assurance, error analysis, and safer human-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training recipe: a reasoning-style supervised fine-tuning (SFT) followed by reinforcement learning (RL) that uses verifiable rewards over a list of X-ray abnormalities. The model outputs reasoning that mirrors radiologists systematic thought process, uncertainty, and differential diagnosis. In out-of-distribution evaluation, the approach achieves competitive multi-label classification while improving interpretability. In a reader study with expert radiologists, full reasoning traces increased confidence, supported error auditing, and reduced time to finalize reports. We release code and the model NV-Reason-CXR-3B to support community progress toward trustworthy, explainable AI in chest radiography and other medical imaging tasks where reasoning quality is as critical as prediction quality.</article>","contentLength":1694,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Latent Chain-of-Thought for Visual Reasoning","url":"https://arxiv.org/abs/2510.23925","date":1761883200,"author":"","guid":322556,"unread":true,"content":"<article>arXiv:2510.23925v2 Announce Type: replace \nAbstract: Chain-of-thought (CoT) reasoning is critical for improving the interpretability and reliability of Large Vision-Language Models (LVLMs). However, existing training algorithms such as SFT, PPO, and GRPO may not generalize well across unseen reasoning tasks and heavily rely on a biased reward model. To address this challenge, we reformulate reasoning in LVLMs as posterior inference and propose a scalable training algorithm based on amortized variational inference. By leveraging diversity-seeking reinforcement learning algorithms, we introduce a novel sparse reward function for token-level learning signals that encourage diverse, high-likelihood latent CoT, overcoming deterministic sampling limitations and avoiding reward hacking. Additionally, we implement a Bayesian inference-scaling strategy that replaces costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank optimal rationales and answers. We empirically demonstrate that the proposed method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in terms of effectiveness, generalization, and interpretability.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Genomics into Multimodal EHR Foundation Models","url":"https://arxiv.org/abs/2510.23639","date":1761883200,"author":"","guid":322557,"unread":true,"content":"<article>arXiv:2510.23639v2 Announce Type: replace \nAbstract: This paper introduces an innovative Electronic Health Record (EHR) foundation model that integrates Polygenic Risk Scores (PRS) as a foundational data modality, moving beyond traditional EHR-only approaches to build more holistic health profiles. Leveraging the extensive and diverse data from the All of Us (AoU) Research Program, this multimodal framework aims to learn complex relationships between clinical data and genetic predispositions. The methodology extends advancements in generative AI to the EHR foundation model space, enhancing predictive capabilities and interpretability. Evaluation on AoU data demonstrates the model's predictive value for the onset of various conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay between PRS and EHR data. The work also explores transfer learning for custom classification tasks, showcasing the architecture's versatility and efficiency. This approach is pivotal for unlocking new insights into disease prediction, proactive health management, risk stratification, and personalized treatment strategies, laying the groundwork for more personalized, equitable, and actionable real-world evidence generation in healthcare.</article>","contentLength":1251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Agent Evolve: LLM Self-Improve through Co-evolution","url":"https://arxiv.org/abs/2510.23595","date":1761883200,"author":"","guid":322558,"unread":true,"content":"<article>arXiv:2510.23595v3 Announce Type: replace \nAbstract: Reinforcement Learning (RL) has demonstrated significant potential in enhancing the reasoning capabilities of large language models (LLMs). However, the success of RL for LLMs heavily relies on human-curated datasets and verifiable rewards, which limit their scalability and generality. Recent Self-Play RL methods, inspired by the success of the paradigm in games and Go, aim to enhance LLM reasoning capabilities without human-annotated data. However, their methods primarily depend on a grounded environment for feedback (e.g., a Python interpreter or a game engine); extending them to general domains remains challenging. To address these challenges, we propose Multi-Agent Evolve (MAE), a framework that enables LLMs to self-evolve in solving diverse tasks, including mathematics, reasoning, and general knowledge Q&amp;A. The core design of MAE is based on a triplet of interacting agents (Proposer, Solver, Judge) that are instantiated from a single LLM, and applies reinforcement learning to optimize their behaviors. The Proposer generates questions, the Solver attempts solutions, and the Judge evaluates both while co-evolving. Experiments on Qwen2.5-3B-Instruct demonstrate that MAE achieves an average improvement of 4.54% on multiple benchmarks. These results highlight MAE as a scalable, data-efficient method for enhancing the general reasoning abilities of LLMs with minimal reliance on human-curated supervision.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FARMER: Flow AutoRegressive Transformer over Pixels","url":"https://arxiv.org/abs/2510.23588","date":1761883200,"author":"","guid":322559,"unread":true,"content":"<article>arXiv:2510.23588v2 Announce Type: replace \nAbstract: Directly modeling the explicit likelihood of the raw data distribution is key topic in the machine learning area, which achieves the scaling successes in Large Language Models by autoregressive modeling. However, continuous AR modeling over visual pixel data suffer from extremely long sequences and high-dimensional spaces. In this paper, we present FARMER, a novel end-to-end generative framework that unifies Normalizing Flows (NF) and Autoregressive (AR) models for tractable likelihood estimation and high-quality image synthesis directly from raw pixels. FARMER employs an invertible autoregressive flow to transform images into latent sequences, whose distribution is modeled implicitly by an autoregressive model. To address the redundancy and complexity in pixel-level modeling, we propose a self-supervised dimension reduction scheme that partitions NF latent channels into informative and redundant groups, enabling more effective and efficient AR modeling. Furthermore, we design a one-step distillation scheme to significantly accelerate inference speed and introduce a resampling-based classifier-free guidance algorithm to boost image generation quality. Extensive experiments demonstrate that FARMER achieves competitive performance compared to existing pixel-based generative models while providing exact likelihoods and scalable training.</article>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human-Like Goalkeeping in a Realistic Football Simulation: a Sample-Efficient Reinforcement Learning Approach","url":"https://arxiv.org/abs/2510.23216","date":1761883200,"author":"","guid":322560,"unread":true,"content":"<article>arXiv:2510.23216v3 Announce Type: replace \nAbstract: While several high profile video games have served as testbeds for Deep Reinforcement Learning (DRL), this technique has rarely been employed by the game industry for crafting authentic AI behaviors. Previous research focuses on training super-human agents with large models, which is impractical for game studios with limited resources aiming for human-like agents. This paper proposes a sample-efficient DRL method tailored for training and fine-tuning agents in industrial settings such as the video game industry. Our method improves sample efficiency of value-based DRL by leveraging pre-collected data and increasing network plasticity. We evaluate our method training a goalkeeper agent in EA SPORTS FC 25, one of the best-selling football simulations today. Our agent outperforms the game's built-in AI by 10% in ball saving rate. Ablation studies show that our method trains agents 50% faster compared to standard DRL methods. Finally, qualitative evaluation from domain experts indicates that our approach creates more human-like gameplay compared to hand-crafted agents. As a testament to the impact of the approach, the method has been adopted for use in the most recent release of the series.</article>","contentLength":1258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lost in Tokenization: Context as the Key to Unlocking Biomolecular Understanding in Scientific LLMs","url":"https://arxiv.org/abs/2510.23127","date":1761883200,"author":"","guid":322561,"unread":true,"content":"<article>arXiv:2510.23127v2 Announce Type: replace \nAbstract: Scientific Large Language Models (Sci-LLMs) have emerged as a promising frontier for accelerating biological discovery. However, these models face a fundamental challenge when processing raw biomolecular sequences: the tokenization dilemma. Whether treating sequences as a specialized language, risking the loss of functional motif information, or as a separate modality, introducing formidable alignment challenges, current strategies fundamentally limit their reasoning capacity. We challenge this sequence-centric paradigm by positing that a more effective strategy is to provide Sci-LLMs with high-level structured context derived from established bioinformatics tools, thereby bypassing the need to interpret low-level noisy sequence data directly. Through a systematic comparison of leading Sci-LLMs on biological reasoning tasks, we tested three input modes: sequence-only, context-only, and a combination of both. Our findings are striking: the context-only approach consistently and substantially outperforms all other modes. Even more revealing, the inclusion of the raw sequence alongside its high-level context consistently degrades performance, indicating that raw sequences act as informational noise, even for models with specialized tokenization schemes. These results suggest that the primary strength of existing Sci-LLMs lies not in their nascent ability to interpret biomolecular syntax from scratch, but in their profound capacity for reasoning over structured, human-readable knowledge. Therefore, we argue for reframing Sci-LLMs not as sequence decoders, but as powerful reasoning engines over expert knowledge. This work lays the foundation for a new class of hybrid scientific AI agents, repositioning the developmental focus from direct sequence interpretation towards high-level knowledge synthesis. The code is available at https://github.com/opendatalab-raiser/CoKE.</article>","contentLength":1948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeing Structural Failure Before it Happens: An Image-Based Physics-Informed Neural Network (PINN) for Spaghetti Bridge Load Prediction","url":"https://arxiv.org/abs/2510.23117","date":1761883200,"author":"","guid":322562,"unread":true,"content":"<article>arXiv:2510.23117v2 Announce Type: replace \nAbstract: Physics Informed Neural Networks (PINNs) are gaining attention for their ability to embed physical laws into deep learning models, which is particularly useful in structural engineering tasks with limited data. This paper aims to explore the use of PINNs to predict the weight of small scale spaghetti bridges, a task relevant to understanding load limits and potential failure modes in simplified structural models. Our proposed framework incorporates physics-based constraints to the prediction model for improved performance. In addition to standard PINNs, we introduce a novel architecture named Physics Informed Kolmogorov Arnold Network (PIKAN), which blends universal function approximation theory with physical insights. The structural parameters provided as input to the model are collected either manually or through computer vision methods. Our dataset includes 15 real bridges, augmented to 100 samples, and our best model achieves an $R^2$ score of 0.9603 and a mean absolute error (MAE) of 10.50 units. From applied perspective, we also provide a web based interface for parameter entry and prediction. These results show that PINNs can offer reliable estimates of structural weight, even with limited data, and may help inform early stage failure analysis in lightweight bridge designs.\n  The complete data and code are available at https://github.com/OmerJauhar/PINNS-For-Spaghetti-Bridges.</article>","contentLength":1459,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multivariate Rational Approximation of Scattered Data Using the p-AAA Algorithm","url":"https://arxiv.org/abs/2510.22861","date":1761883200,"author":"","guid":322563,"unread":true,"content":"<article>arXiv:2510.22861v2 Announce Type: replace \nAbstract: Many algorithms for approximating data with rational functions are built on interpolation or least-squares approximation. Inspired by the adaptive Antoulas-Anderson (AAA) algorithm for the univariate case, the parametric adaptive Antoulas-Anderson (p-AAA) algorithm extends this idea to the multivariate setting, combining least-squares and interpolation formulations into a single effective approximation procedure. In its original formulation p-AAA operates on grid data, requiring access to function samples at every combination of discrete sampling points in each variable. In this work we extend the p-AAA algorithm to scattered data sets, without requiring uniform/grid sampling. In other words, our proposed p-AAA formulation operates on a set of arbitrary sampling points and is not restricted to a grid structure for the sampled data. Towards this goal, we introduce several formulations for rational least-squares optimization problems that incorporate interpolation conditions via constraints. We analyze the structure of the resulting optimization problems and introduce structured matrices whose singular value decompositions yield closed-form solutions to the underlying least-squares problems. Several examples illustrate computational aspects and the effectiveness of our proposed procedure.</article>","contentLength":1360,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProGQL: A Provenance Graph Query System for Cyber Attack Investigation","url":"https://arxiv.org/abs/2510.22400","date":1761883200,"author":"","guid":322564,"unread":true,"content":"<article>arXiv:2510.22400v2 Announce Type: replace \nAbstract: Provenance analysis (PA) has recently emerged as an important solution for cyber attack investigation. PA leverages system monitoring to monitor system activities as a series of system audit events and organizes these events as a provenance graph to show the dependencies among system activities, which can reveal steps of cyber attacks. Despite their potential, existing PA techniques face two critical challenges: (1) they are inflexible and non-extensible, making it difficult to incorporate analyst expertise, and (2) they are memory inefficient, often requiring&gt;100GB of RAM to hold entire event streams, which fundamentally limits scalability and deployment in real-world environments. To address these limitations, we propose the ProGQL framework, which provides a domain-specific graph search language with a well-engineered query engine, allowing PA over system audit events and expert knowledge to be jointly expressed as a graph search query and thereby facilitating the investigation of complex cyberattacks. In particular, to support dependency searches from a starting edge required in PA, ProGQL introduces new language constructs for constrained graph traversal, edge weight computation, value propagation along weighted edges, and graph merging to integrate multiple searches. Moreover, the ProGQL query engine is optimized for efficient incremental graph search across heterogeneous database backends, eliminating the need for full in-memory materialization and reducing memory overhead. Our evaluations on real attacks demonstrate the effectiveness of the ProGQL language in expressing a diverse set of complex attacks compared with the state-of-the-art graph query language Cypher, and the comparison with the SOTA PA technique DEPIMPACT further demonstrates the significant improvement of the scalability brought by our ProGQL framework's design.</article>","contentLength":1920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping","url":"https://arxiv.org/abs/2510.22319","date":1761883200,"author":"","guid":322565,"unread":true,"content":"<article>arXiv:2510.22319v2 Announce Type: replace \nAbstract: Recently, GRPO-based reinforcement learning has shown remarkable progress in optimizing flow-matching models, effectively improving their alignment with task-specific rewards. Within these frameworks, the policy update relies on importance-ratio clipping to constrain overconfident positive and negative gradients. However, in practice, we observe a systematic shift in the importance-ratio distribution-its mean falls below 1 and its variance differs substantially across timesteps. This left-shifted and inconsistent distribution prevents positive-advantage samples from entering the clipped region, causing the mechanism to fail in constraining overconfident positive updates. As a result, the policy model inevitably enters an implicit over-optimization stage-while the proxy reward continues to increase, essential metrics such as image quality and text-prompt alignment deteriorate sharply, ultimately making the learned policy impractical for real-world use. To address this issue, we introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO frameworks. Our method incorporates ratio normalization, which restores a balanced and step-consistent importance ratio, ensuring that PPO clipping properly constrains harmful updates across denoising timesteps. In addition, a gradient reweighting strategy equalizes policy gradients over noise conditions, preventing excessive updates from particular timestep regions. Together, these designs act as a regulated clipping mechanism, stabilizing optimization and substantially mitigating implicit over-optimization without relying on heavy KL regularization. Extensive experiments on multiple diffusion backbones (e.g., SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard significantly reduces over-optimization while maintaining or even improving generation quality.</article>","contentLength":1905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training","url":"https://arxiv.org/abs/2510.22094","date":1761883200,"author":"","guid":322566,"unread":true,"content":"<article>arXiv:2510.22094v2 Announce Type: replace \nAbstract: Climate events arise from intricate, multivariate dynamics governed by global-scale drivers, profoundly impacting food, energy, and infrastructure. Yet, accurate weather prediction remains elusive due to physical processes unfolding across diverse spatio-temporal scales, which fixed-resolution methods cannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscale representation, but nonlinear downward mappings often erase global trends, weakening the integration of physics into forecasts. We introduce HiFlowCast and its ensemble variant HiAntFlow, HGNNs that embed physics within a multiscale prediction framework. Two innovations underpin their design: a Latent-Memory-Retention mechanism that preserves global trends during downward traversal, and a Latent-to-Physics branch that integrates PDE solution fields across diverse scales. Our Flow models cut errors by over 5% at 13-day lead times and by 5-8% under 1st and 99th quantile extremes, improving reliability for rare events. Leveraging pretrained model weights, they converge within a single epoch, reducing training cost and their carbon footprint. Such efficiency is vital as the growing scale of machine learning challenges sustainability and limits research accessibility. Code and model weights are in the supplementary materials.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data","url":"https://arxiv.org/abs/2510.22033","date":1761883200,"author":"","guid":322567,"unread":true,"content":"<article>arXiv:2510.22033v2 Announce Type: replace \nAbstract: Single-cell technologies generate high-dimensional point clouds of cells, enabling detailed characterization of complex patient states and treatment responses. Yet each patient is represented by an irregular point cloud rather than a simple vector, making it difficult to directly quantify and compare biological differences between individuals. Nonlinear methods such as kernels and neural networks achieve predictive accuracy but act as black boxes, offering little biological interpretability.\n  To address these limitations, we adapt the Linear Optimal Transport (LOT) framework to this setting, embedding irregular point clouds into a fixed-dimensional Euclidean space while preserving distributional structure. This embedding provides a principled linear representation that preserves optimal transport geometry while enabling downstream analysis. It also forms a registration between any two patients, enabling direct comparison of their cellular distributions. Within this space, LOT enables: (i) \\textbf{accurate and interpretable classification} of COVID-19 patient states, where classifier weights map back to specific markers and spatial regions driving predictions; and (ii) \\textbf{synthetic data generation} for patient-derived organoids, exploiting the linearity of the LOT embedding. LOT barycenters yield averaged cellular profiles representing combined conditions or samples, supporting drug interaction testing.\n  Together, these results establish LOT as a unified framework that bridges predictive performance, interpretability, and generative modeling. By transforming heterogeneous point clouds into structured embeddings directly traceable to the original data, LOT opens new opportunities for understanding immune variation and treatment effects in high-dimensional biological systems.</article>","contentLength":1863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Quantitative Approach to Estimating Bias, Favouritism and Distortion in Scientific Journalism","url":"https://arxiv.org/abs/2510.21838","date":1761883200,"author":"","guid":322568,"unread":true,"content":"<article>arXiv:2510.21838v2 Announce Type: replace \nAbstract: While traditionally not considered part of the scientific method, science communication is increasingly playing a pivotal role in shaping scientific practice. Researchers are now frequently compelled to publicise their findings in response to institutional impact metrics and competitive grant environments. This shift underscores the growing influence of media narratives on both scientific priorities and public perception. In a current trend of personality-driven reporting, we examine patterns in science communication that may indicate biases of different types, towards topics and researchers. We focused and applied our methodology to a corpus of media coverage from three of the most prominent scientific media outlets: Wired, Quanta, and The New Scientist -- spanning the past 5 to 10 years. By mapping linguistic patterns, citation flows, and topical convergence, our objective was to quantify the dimensions and degree of bias that influence the credibility of scientific journalism. In doing so, we seek to illuminate the systemic features that shape science communication today and to interrogate their broader implications for epistemic integrity and public accountability in science. We present our results with anonymised journalist names but conclude that personality-driven media coverage distorts science and the practice of science flattening rather than expanding scientific coverage perception. Keywords : selective sourcing, bias, scientific journalism, Quanta, Wired, New Scientist, fairness, balance, neutrality, standard practices, distortion, personal promotion, communication, media outlets.</article>","contentLength":1672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wisdom and Delusion of LLM Ensembles for Code Generation and Repair","url":"https://arxiv.org/abs/2510.21513","date":1761883200,"author":"","guid":322569,"unread":true,"content":"<article>arXiv:2510.21513v2 Announce Type: replace \nAbstract: Today's pursuit of a single Large Language Model (LMM) for all software engineering tasks is resource-intensive and overlooks the potential benefits of complementarity, where different models contribute unique strengths. However, the degree to which coding LLMs complement each other and the best strategy for maximizing an ensemble's potential are unclear, leaving practitioners without a clear path to move beyond single-model systems. To address this gap, we empirically compare ten individual LLMs from five families, and three ensembles of these LLMs across three software engineering benchmarks covering code generation and program repair. We assess the complementarity between models and the performance gap between the best individual model and the ensembles. Next, we evaluate various selection heuristics to identify correct solutions from an ensemble's candidate pool. We find that the theoretical upperbound for an ensemble's performance can be 83% above the best single model. Our results show that consensus-based strategies for selecting solutions fall into a \"popularity trap,\" amplifying common but incorrect outputs. In contrast, a diversity-based strategy realizes up to 95% of this theoretical potential, and proves effective even in small two-model ensembles, enabling a cost-efficient way to enhance performance by leveraging multiple LLMs.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scalpel: Automotive Deep Learning Framework Testing via Assembling Model Components","url":"https://arxiv.org/abs/2510.21451","date":1761883200,"author":"","guid":322570,"unread":true,"content":"<article>arXiv:2510.21451v2 Announce Type: replace \nAbstract: Deep learning (DL) plays a key role in autonomous driving systems. DL models support perception modules, equipped with tasks such as object detection and sensor fusion. These DL models enable vehicles to process multi-sensor inputs to understand complex surroundings. Deploying DL models in autonomous driving systems faces stringent challenges, including real-time processing, limited computational resources, and strict power constraints. To address these challenges, automotive DL frameworks (e.g., PaddleInference) have emerged to optimize inference efficiency. However, these frameworks encounter unique quality issues due to their more complex deployment environments, such as crashes stemming from limited scheduled memory and incorrect memory allocation. Unfortunately, existing DL framework testing methods fail to detect these quality issues due to the failure in deploying generated test input models, as these models lack three essential capabilities: (1) multi-input/output tensor processing, (2) multi-modal data processing, and (3) multi-level data feature extraction. These capabilities necessitate specialized model components, which existing testing methods neglect during model generation. To bridge this gap, we propose Scalpel, an automotive DL frameworks testing method that generates test input models at the model component level. Scalpel generates models by assembling model components (heads, necks, backbones) to support capabilities required by autonomous driving systems. Specifically, Scalpel maintains and updates a repository of model components, generating test inputs by selecting, mutating, and assembling them. Successfully generated models are added back to enrich the repository. Newly generated models are then deployed within the autonomous driving system to test automotive DL frameworks via differential testing.</article>","contentLength":1907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Buffer layers for Test-Time Adaptation","url":"https://arxiv.org/abs/2510.21271","date":1761883200,"author":"","guid":322571,"unread":true,"content":"<article>arXiv:2510.21271v2 Announce Type: replace \nAbstract: In recent advancements in Test Time Adaptation (TTA), most existing methodologies focus on updating normalization layers to adapt to the test domain. However, the reliance on normalization-based adaptation presents key challenges. First, normalization layers such as Batch Normalization (BN) are highly sensitive to small batch sizes, leading to unstable and inaccurate statistics. Moreover, normalization-based adaptation is inherently constrained by the structure of the pre-trained model, as it relies on training-time statistics that may not generalize well to unseen domains. These issues limit the effectiveness of normalization-based TTA approaches, especially under significant domain shift. In this paper, we introduce a novel paradigm based on the concept of a Buffer layer, which addresses the fundamental limitations of normalization layer updates. Unlike existing methods that modify the core parameters of the model, our approach preserves the integrity of the pre-trained backbone, inherently mitigating the risk of catastrophic forgetting during online adaptation. Through comprehensive experimentation, we demonstrate that our approach not only outperforms traditional methods in mitigating domain shift and enhancing model robustness, but also exhibits strong resilience to forgetting. Furthermore, our Buffer layer is modular and can be seamlessly integrated into nearly all existing TTA frameworks, resulting in consistent performance improvements across various architectures. These findings validate the effectiveness and versatility of the proposed solution in real-world domain adaptation scenarios. The code is available at https://github.com/hyeongyu-kim/Buffer_TTA.</article>","contentLength":1745,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's Next, Cloud? A Forensic Framework for Analyzing Self-Hosted Cloud Storage Solutions","url":"https://arxiv.org/abs/2510.21246","date":1761883200,"author":"","guid":322572,"unread":true,"content":"<article>arXiv:2510.21246v2 Announce Type: replace \nAbstract: Self-hosted cloud storage platforms like Nextcloud are gaining popularity among individuals and organizations seeking greater control over their data. However, this shift introduces new challenges for digital forensic investigations, particularly in systematically analyzing both client and server components. Despite Nextcloud's widespread use, it has received limited attention in forensic research. In this work, we critically examine existing cloud storage forensic frameworks and highlight their limitations. To address the gaps, we propose an extended forensic framework that incorporates device monitoring and leverages cloud APIs for structured, repeatable evidence acquisition. Using Nextcloud as a case study, we demonstrate how its native APIs can be used to reliably access forensic artifacts, and we introduce an open-source acquisition tool that implements this approach. Our framework equips investigators with a more flexible method for analyzing self-hosted cloud storage systems, and offers a foundation for further development in this evolving area of digital forensics.</article>","contentLength":1142,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset","url":"https://arxiv.org/abs/2510.21038","date":1761883200,"author":"","guid":322573,"unread":true,"content":"<article>arXiv:2510.21038v2 Announce Type: replace \nAbstract: Non-invasive brain-computer interfaces (BCIs) are beginning to benefit from large, public benchmarks. However, current benchmarks target relatively simple, foundational tasks like Speech Detection and Phoneme Classification, while application-ready results on tasks like Brain-to-Text remain elusive. We propose Keyword Spotting (KWS) as a practically applicable, privacy-aware intermediate task. Using the deep 52-hour, within-subject LibriBrain corpus, we provide standardized train/validation/test splits for reproducible benchmarking, and adopt an evaluation protocol tailored to extreme class imbalance. Concretely, we use area under the precision-recall curve (AUPRC) as a robust evaluation metric, complemented by false alarms per hour (FA/h) at fixed recall to capture user-facing trade-offs. To simplify deployment and further experimentation within the research community, we are releasing an updated version of the pnpl library with word-level dataloaders and Colab-ready tutorials. As an initial reference model, we present a compact 1-D Conv/ResNet baseline with focal loss and top-k pooling that is trainable on a single consumer-class GPU. The reference model achieves approximately 13x the permutation baseline AUPRC on held-out sessions, demonstrating the viability of the task. Exploratory analyses reveal: (i) predictable within-subject scaling - performance improves log-linearly with more training hours - and (ii) the existence of word-level factors (frequency and duration) that systematically modulate detectability.</article>","contentLength":1593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"C-NAV: Towards Self-Evolving Continual Object Navigation in Open World","url":"https://arxiv.org/abs/2510.20685","date":1761883200,"author":"","guid":322574,"unread":true,"content":"<article>arXiv:2510.20685v2 Announce Type: replace \nAbstract: Embodied agents are expected to perform object navigation in dynamic, open-world environments. However, existing approaches typically rely on static trajectories and a fixed set of object categories during training, overlooking the real-world requirement for continual adaptation to evolving scenarios. To facilitate related studies, we introduce the continual object navigation benchmark, which requires agents to acquire navigation skills for new object categories while avoiding catastrophic forgetting of previously learned knowledge. To tackle this challenge, we propose C-Nav, a continual visual navigation framework that integrates two key innovations: (1) A dual-path anti-forgetting mechanism, which comprises feature distillation that aligns multi-modal inputs into a consistent representation space to ensure representation consistency, and feature replay that retains temporal features within the action decoder to ensure policy consistency. (2) An adaptive sampling strategy that selects diverse and informative experiences, thereby reducing redundancy and minimizing memory overhead. Extensive experiments across multiple model architectures demonstrate that C-Nav consistently outperforms existing approaches, achieving superior performance even compared to baselines with full trajectory retention, while significantly lowering memory requirements. The code will be publicly available at https://bigtree765.github.io/C-Nav-project.</article>","contentLength":1500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training","url":"https://arxiv.org/abs/2510.20059","date":1761883200,"author":"","guid":322575,"unread":true,"content":"<article>arXiv:2510.20059v2 Announce Type: replace \nAbstract: Enhancing reasoning capabilities in small language models is critical for specialized applications such as medical question answering, particularly in underrepresented languages like Persian. In this study, we employ Reinforcement Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to improve the reasoning skills of a general-purpose Persian language model. To achieve this, we translated a multiple-choice medical question-answering dataset into Persian and used RLAIF to generate rejected-preferred answer pairs, which are essential for DPO training. By prompting both teacher and student models to produce Chain-of-Thought (CoT) reasoning responses, we compiled a dataset containing correct and incorrect reasoning trajectories. This dataset, comprising 2 million tokens in preferred answers and 2.5 million tokens in rejected ones, was used to train a baseline model, significantly enhancing its medical reasoning capabilities in Persian. Remarkably, the resulting model outperformed its predecessor, gaokerena-V, which was trained on approximately 57 million tokens, despite leveraging a much smaller dataset. These results highlight the efficiency and effectiveness of reasoning-focused training approaches in developing domain-specific language models with limited data availability.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents","url":"https://arxiv.org/abs/2510.19771","date":1761883200,"author":"","guid":322576,"unread":true,"content":"<article>arXiv:2510.19771v2 Announce Type: replace \nAbstract: LLM-based agents are increasingly moving towards proactivity: rather than awaiting instruction, they exercise agency to anticipate user needs and solve them autonomously. However, evaluating proactivity is challenging; current benchmarks are constrained to localized context, limiting their ability to test reasoning across sources and longer time horizons. To address this gap, we present PROBE (Proactive Resolution Of BottlEnecks). PROBE decomposes proactivity as a pipeline of three core capabilities: (1) searching for unspecified issues, (2) identifying specific bottlenecks, and (3) executing appropriate resolutions. We apply PROBE to evaluate leading LLMs and popular agentic frameworks, showing that even state-of-the-art models struggle to solve this benchmark. Computing our consistent measurements across frontier LLMs and agents, we find that the best end-to-end performance of 40% is achieved by both GPT-5 and Claude Opus-4.1. Additionally, we demonstrate the relative capabilities of each model and analyze mutual failure modes. Our results highlight the current limitations of autonomous action in agentic systems, and expose promising future research directions.</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UNO-Bench: A Unified Benchmark for Exploring the Compositional Law Between Uni-modal and Omni-modal in Omni Models","url":"https://arxiv.org/abs/2510.18915","date":1761883200,"author":"","guid":322577,"unread":true,"content":"<article>arXiv:2510.18915v3 Announce Type: replace \nAbstract: Multimodal Large Languages models have been progressing from uni-modal understanding toward unifying visual, audio and language modalities, collectively termed omni models. However, the correlation between uni-modal and omni-modal remains unclear, which requires comprehensive evaluation to drive omni model's intelligence evolution. In this work, we introduce a novel, high-quality, and UNified Omni model benchmark, UNO-Bench. This benchmark is designed to effectively evaluate both UNi-modal and Omni-modal capabilities under a unified ability taxonomy, spanning 44 task types and 5 modality combinations. It includes 1250 human curated samples for omni-modal with 98% cross-modality solvability, and 2480 enhanced uni-modal samples. The human-generated dataset is well-suited to real-world scenarios, particularly within the Chinese context, whereas the automatically compressed dataset offers a 90% increase in speed and maintains 98% consistency across 18 public benchmarks. In addition to traditional multi-choice questions, we propose an innovative multi-step open-ended question format to assess complex reasoning. A general scoring model is incorporated, supporting 6 question types for automated evaluation with 95% accuracy. Experimental result shows the Compositional Law between omni-modal and uni-modal performance and the omni-modal capability manifests as a bottleneck effect on weak models, while exhibiting synergistic promotion on strong models.</article>","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices","url":"https://arxiv.org/abs/2510.18480","date":1761883200,"author":"","guid":322578,"unread":true,"content":"<article>arXiv:2510.18480v2 Announce Type: replace \nAbstract: Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.</article>","contentLength":989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources","url":"https://arxiv.org/abs/2510.18477","date":1761883200,"author":"","guid":322579,"unread":true,"content":"<article>arXiv:2510.18477v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.</article>","contentLength":1592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning","url":"https://arxiv.org/abs/2510.18318","date":1761883200,"author":"","guid":322580,"unread":true,"content":"<article>arXiv:2510.18318v2 Announce Type: replace \nAbstract: Geospatial data offers immense potential for understanding our planet. However, the sheer volume and diversity of this data along with its varied resolutions, timescales, and sparsity pose significant challenges for thorough analysis and interpretation. This paper introduces Earth AI, a family of geospatial AI models and agentic reasoning that enables significant advances in our ability to unlock novel and profound insights into our planet. This approach is built upon foundation models across three key domains--Planet-scale Imagery, Population, and Environment--and an intelligent Gemini-powered reasoning engine. We present rigorous benchmarks showcasing the power and novel capabilities of our foundation models and validate that when used together, they provide complementary value for geospatial inference and their synergies unlock superior predictive capabilities. To handle complex, multi-step queries, we developed a Gemini-powered agent that jointly reasons over our multiple foundation models along with large geospatial data sources and tools. On a new benchmark of real-world crisis scenarios, our agent demonstrates the ability to deliver critical and timely insights, effectively bridging the gap between raw geospatial data and actionable understanding.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration","url":"https://arxiv.org/abs/2510.17670","date":1761883200,"author":"","guid":322581,"unread":true,"content":"<article>arXiv:2510.17670v2 Announce Type: replace \nAbstract: Open-vocabulary object detection (OVD) models offer remarkable flexibility by detecting objects from arbitrary text queries. However, their zero-shot performance in specialized domains like Remote Sensing (RS) is often compromised by the inherent ambiguity of natural language, limiting critical downstream applications. For instance, an OVD model may struggle to distinguish between fine-grained classes such as \"fishing boat\" and \"yacht\" since their embeddings are similar and often inseparable. This can hamper specific user goals, such as monitoring illegal fishing, by producing irrelevant detections. To address this, we propose a cascaded approach that couples the broad generalization of a large pre-trained OVD model with a lightweight few-shot classifier. Our method first employs the zero-shot model to generate high-recall object proposals. These proposals are then refined for high precision by a compact classifier trained in real-time on only a handful of user-annotated examples - drastically reducing the high costs of RS imagery annotation.The core of our framework is FLAME, a one-step active learning strategy that selects the most informative samples for training. FLAME identifies, on the fly, uncertain marginal candidates near the decision boundary using density estimation, followed by clustering to ensure sample diversity. This efficient sampling technique achieves high accuracy without costly full-model fine-tuning and enables instant adaptation, within less then a minute, which is significantly faster than state-of-the-art alternatives.Our method consistently surpasses state-of-the-art performance on RS benchmarks, establishing a practical and resource-efficient framework for adapting foundation models to specific user needs.</article>","contentLength":1815,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SmartSustain Recommender System: Navigating Sustainability Trade-offs in Personalized City Trip Planning","url":"https://arxiv.org/abs/2510.17355","date":1761883200,"author":"","guid":322582,"unread":true,"content":"<article>arXiv:2510.17355v2 Announce Type: replace \nAbstract: Tourism is a major contributor to global carbon emissions and over-tourism, creating an urgent need for recommender systems that not only inform but also gently steer users toward more sustainable travel decisions. Such choices, however, often require balancing complex trade-offs between environmental impact, cost, convenience, and personal interests. To address this, we present the SmartSustain Recommender, a web application designed to nudge users toward eco-friendlier options through an interactive, user-centric interface. The system visualizes the broader consequences of travel decisions by combining CO2e emissions, destination popularity, and seasonality with personalized interest matching. It employs mechanisms such as interactive city cards for quick comparisons, dynamic banners that surface sustainable alternatives in specific trade-off scenarios, and real-time impact feedback using animated environmental indicators. A preliminary user study with 21 participants indicated strong usability and perceived effectiveness. The system is accessible at https://smartsustainrecommender.web.app.</article>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment","url":"https://arxiv.org/abs/2510.17148","date":1761883200,"author":"","guid":322583,"unread":true,"content":"<article>arXiv:2510.17148v3 Announce Type: replace \nAbstract: Conventional end-to-end (E2E) driving models are effective at generating physically plausible trajectories, but often fail to generalize to long-tail scenarios due to the lack of essential world knowledge to understand and reason about surrounding environments. In contrast, Vision-Language-Action (VLA) models leverage world knowledge to handle challenging cases, but their limited 3D reasoning capability can lead to physically infeasible actions. In this work we introduce DiffVLA++, an enhanced autonomous driving framework that explicitly bridges cognitive reasoning and E2E planning through metric-guided alignment. First, we build a VLA module directly generating semantically grounded driving trajectories. Second, we design an E2E module with a dense trajectory vocabulary that ensures physical feasibility. Third, and most critically, we introduce a metric-guided trajectory scorer that guides and aligns the outputs of the VLA and E2E modules, thereby integrating their complementary strengths. The experiment on the ICCV 2025 Autonomous Grand Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Impossibility of Retrain Equivalence in Machine Unlearning","url":"https://arxiv.org/abs/2510.16629","date":1761883200,"author":"","guid":322584,"unread":true,"content":"<article>arXiv:2510.16629v2 Announce Type: replace \nAbstract: Machine unlearning seeks to selectively remove the \"influence\" of specific training data on a model's outputs. The ideal goal is Retrain Equivalence--behavior identical to a model trained from scratch on only the retained data. This goal was formulated for models trained on i.i.d. data batches, but modern pipelines often involve multi-stage training, with each stage having a distinct data distribution and objective. Examples include LLM fine-tuning for alignment, reasoning ability, etc. Our study shows via theory and experiments that this shift to multi-stage training introduces a fundamental barrier for machine unlearning. The theory indicates that the outcome of local unlearning--methods that only use gradients computed on the forget set--is path-dependent. That is, a model's behavior during unlearning is influenced by the order of its training stages during learning, making it impossible for path-oblivious algorithms to universally achieve Retrain Equivalence. We empirically demonstrate the same phenomenon in LLM post-training across Llama and Qwen models (1B to 14B) with gradient ascent, NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different orderings of identical training stages diverge in behavior during unlearning, with the degradation in GSM8K accuracy after unlearning varying by over 20% across paths. We also observe that some learning paths consistently produce models that unlearn slowly. During unlearning, whether the probability mass gets squeezed into paraphrasing or alternative concepts is also path-dependent. These results consistently show that Retrain Equivalence is an ill-posed target for local unlearning algorithms, so long as the target models are trained in stages. In situations where access to models' training histories is hard, the current work calls for rethinking the definition and desiderata of machine unlearning.</article>","contentLength":1946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fit for Purpose? Deepfake Detection in the Real World","url":"https://arxiv.org/abs/2510.16556","date":1761883200,"author":"","guid":322585,"unread":true,"content":"<article>arXiv:2510.16556v2 Announce Type: replace \nAbstract: The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.</article>","contentLength":1587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation","url":"https://arxiv.org/abs/2510.16396","date":1761883200,"author":"","guid":322586,"unread":true,"content":"<article>arXiv:2510.16396v3 Announce Type: replace \nAbstract: With the increasing ubiquity of AR/VR devices, the deployment of deep learning models on edge devices has become a critical challenge. These devices require real-time inference, low power consumption, and minimal latency. Many framework designers face the conundrum of balancing efficiency and performance. We design a light framework that adopts an encoder-decoder architecture and introduces several key contributions aimed at improving both efficiency and accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency improvement. Moreover, we propose our SPLite decoder. This new architecture significantly boosts the decoding process's frame rate by 3.1x on the Raspberry Pi 5, while maintaining accuracy on par. To further optimize performance, we apply quantization-aware training, reducing memory usage while preserving accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5 CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on compound benchmark datasets, demonstrating comparable accuracy to state-of-the-art approaches while significantly enhancing computational efficiency.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surrogate-Assisted Evolutionary Optimization Based on Interpretable Convolution Network","url":"https://arxiv.org/abs/2510.16386","date":1761883200,"author":"","guid":322587,"unread":true,"content":"<article>arXiv:2510.16386v2 Announce Type: replace \nAbstract: When performing evolutionary optimization for computationally expensive objective, surrogate-assisted evolutionary algorithm(SAEA) is an effective approach. However, due to the limited availability of data in these scenarios, it can be challenging to create a highly accurate surrogate model, leading to reduced optimization effectiveness. To address this issue, we propose an Interpretable Convolution Network(ICN) for offline surrogate-assited evolutionary optimization. ICN retains the non-linear expression ability of traditional neural networks, while possessing the advantages of clear physical structure and the ability to incorporate prior knowledge during network parameter design and training process. We compare ICN-SAEA with tri-training method(TT-DDEA) and model-ensemble method(DDEA-SA) in several benchmark problems. Experimental results show that ICN-SAEA is better in searching optimal solution than compared algorithms.</article>","contentLength":990,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MaskCaptioner: Learning to Jointly Segment and Caption Object Trajectories in Videos","url":"https://arxiv.org/abs/2510.14904","date":1761883200,"author":"","guid":322588,"unread":true,"content":"<article>arXiv:2510.14904v2 Announce Type: replace \nAbstract: Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.</article>","contentLength":1082,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media","url":"https://arxiv.org/abs/2510.14889","date":1761883200,"author":"","guid":322589,"unread":true,"content":"<article>arXiv:2510.14889v2 Announce Type: replace \nAbstract: On social media, many individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by 15% over individual-only baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.</article>","contentLength":1203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-Time Neural Video Compression with Unified Intra and Inter Coding","url":"https://arxiv.org/abs/2510.14431","date":1761883200,"author":"","guid":322590,"unread":true,"content":"<article>arXiv:2510.14431v3 Announce Type: replace \nAbstract: Neural video compression (NVC) technologies have advanced rapidly in recent years, yielding state-of-the-art schemes such as DCVC-RT that offer superior compression efficiency to H.266/VVC and real-time encoding/decoding capabilities. Nonetheless, existing NVC schemes have several limitations, including inefficiency in dealing with disocclusion and new content, interframe error propagation and accumulation, among others. To eliminate these limitations, we borrow the idea from classic video coding schemes, which allow intra coding within inter-coded frames. With the intra coding tool enabled, disocclusion and new content are properly handled, and interframe error propagation is naturally intercepted without the need for manual refresh mechanisms. We present an NVC framework with unified intra and inter coding, where every frame is processed by a single model that is trained to perform intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame compression design to exploit interframe redundancy not only forwardly but also backwardly. Experimental results show that our scheme outperforms DCVC-RT by an average of 12.1% BD-rate reduction, delivers more stable bitrate and quality per frame, and retains real-time encoding/decoding performances. Code and models will be released.</article>","contentLength":1362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Systolic Array Acceleration of Diagonal-Optimized Sparse-Sparse Matrix Multiplication for Efficient Quantum Simulation","url":"https://arxiv.org/abs/2510.14172","date":1761883200,"author":"","guid":322591,"unread":true,"content":"<article>arXiv:2510.14172v2 Announce Type: replace \nAbstract: Hamiltonian simulation is a key workload in quantum computing, enabling the study of complex quantum systems and serving as a critical tool for classical verification of quantum devices. However, it is computationally challenging because the Hilbert space dimension grows exponentially with the number of qubits. The growing dimensions make matrix exponentiation, the key kernel in Hamiltonian simulations, increasingly expensive. Matrix exponentiation is typically approximated by the Taylor series, which contains a series of matrix multiplications. Since Hermitian operators are often sparse, sparse matrix multiplication accelerators are essential for improving the scalability of classical Hamiltonian simulation. Yet, existing accelerators are primarily designed for machine learning workloads and tuned to their characteristic sparsity patterns, which differ fundamentally from those in Hamiltonian simulations that are often dominated by structured diagonals.\n  In this work, we present \\name, the first diagonal-optimized quantum simulation accelerator. It exploits the diagonal structure commonly found in problem-Hamiltonian (Hermitian) matrices and leverages a restructured systolic array dataflow to transform diagonally sparse matrices into dense computations, enabling high utilization and performance. Through detailed cycle-level simulation of diverse benchmarks in HamLib, \\name{} demonstrates average performance improvements of $10.26\\times$, $33.58\\times$, and $53.15\\times$ over SIGMA, Outer Product, and Gustavson's algorithm, respectively, with peak speedups up to $127.03\\times$ while reducing energy consumption by an average of $471.55\\times$ and up to $4630.58\\times$ compared to SIGMA.</article>","contentLength":1767,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents","url":"https://arxiv.org/abs/2510.11695","date":1761883200,"author":"","guid":322592,"unread":true,"content":"<article>arXiv:2510.11695v2 Announce Type: replace \nAbstract: Although Large Language Model (LLM)-based agents are increasingly used in financial trading, it remains unclear whether they can reason and adapt in live markets, as most studies test models instead of agents, cover limited periods and assets, and rely on unverified data. To address these gaps, we introduce Agent Market Arena (AMA), the first lifelong, real-time benchmark for evaluating LLM-based trading agents across multiple markets. AMA integrates verified trading data, expert-checked news, and diverse agent architectures within a unified trading framework, enabling fair and continuous comparison under real conditions. It implements four agents, including InvestorAgent as a single-agent baseline, TradeAgent and HedgeFundAgent with different risk styles, and DeepFundAgent with memory-based reasoning, and evaluates them across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets demonstrate that agent frameworks display markedly distinct behavioral patterns, spanning from aggressive risk-taking to conservative decision-making, whereas model backbones contribute less to outcome variation. AMA thus establishes a foundation for rigorous, reproducible, and continuously evolving evaluation of financial reasoning and trading intelligence in LLM-based agents.</article>","contentLength":1405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proceedings of the Access InContext Workshop @ CHI'25 Conference on Human Factors in Computing Systems","url":"https://arxiv.org/abs/2510.11280","date":1761883200,"author":"","guid":322593,"unread":true,"content":"<article>arXiv:2510.11280v3 Announce Type: replace \nAbstract: This is the Proceedings of the Access InContext Workshop, which was held at the CHI'25 Conference on Human Factors in Computing Systems, in Yokohama, Japan, on April 26th 2025.</article>","contentLength":229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decoupled Multimodal Fusion for User Interest Modeling in Click-Through Rate Prediction","url":"https://arxiv.org/abs/2510.11066","date":1761883200,"author":"","guid":322594,"unread":true,"content":"<article>arXiv:2510.11066v2 Announce Type: replace \nAbstract: Modern industrial recommendation systems improve recommendation performance by integrating multimodal representations from pre-trained models into ID-based Click-Through Rate (CTR) prediction frameworks. However, existing approaches typically adopt modality-centric modeling strategies that process ID-based and multimodal embeddings independently, failing to capture fine-grained interactions between content semantics and behavioral signals. In this paper, we propose Decoupled Multimodal Fusion (DMF), which introduces a modality-enriched modeling strategy to enable fine-grained interactions between ID-based collaborative representations and multimodal representations for user interest modeling. Specifically, we construct target-aware features to bridge the semantic gap across different embedding spaces and leverage them as side information to enhance the effectiveness of user interest modeling. Furthermore, we design an inference-optimized attention mechanism that decouples the computation of target-aware features and ID-based embeddings before the attention layer, thereby alleviating the computational bottleneck introduced by incorporating target-aware features. To achieve comprehensive multimodal integration, DMF combines user interest representations learned under the modality-centric and modality-enriched modeling strategies. Offline experiments on public and industrial datasets demonstrate the effectiveness of DMF. Moreover, DMF has been deployed on the product recommendation system of the international e-commerce platform Lazada, achieving relative improvements of 5.30% in CTCVR and 7.43% in GMV with negligible computational overhead.</article>","contentLength":1719,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution","url":"https://arxiv.org/abs/2510.08771","date":1761883200,"author":"","guid":322595,"unread":true,"content":"<article>arXiv:2510.08771v2 Announce Type: replace \nAbstract: Generative models for Image Super-Resolution (SR) are increasingly powerful, yet their reliance on self-attention's quadratic complexity (O(N^2)) creates a major computational bottleneck. Linear Attention offers an O(N) solution, but its promise for photorealistic SR has remained largely untapped, historically hindered by a cascade of interrelated and previously unsolved challenges. This paper introduces LinearSR, a holistic framework that, for the first time, systematically overcomes these critical hurdles. Specifically, we resolve a fundamental, training instability that causes catastrophic model divergence using our novel \"knee point\"-based Early-Stopping Guided Fine-tuning (ESGF) strategy. Furthermore, we mitigate the classic perception-distortion trade-off with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we establish an effective and lightweight guidance paradigm, TAG, derived from our \"precision-over-volume\" principle. Our resulting LinearSR model simultaneously delivers state-of-the-art perceptual quality with exceptional efficiency. Its core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its overall multi-step inference time remains highly competitive. This work provides the first robust methodology for applying Linear Attention in the photorealistic SR domain, establishing a foundational paradigm for future research in efficient generative super-resolution.</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LatentBreak: Jailbreaking Large Language Models through Latent Space Feedback","url":"https://arxiv.org/abs/2510.08604","date":1761883200,"author":"","guid":322596,"unread":true,"content":"<article>arXiv:2510.08604v2 Announce Type: replace \nAbstract: Jailbreaks are adversarial attacks designed to bypass the built-in safety mechanisms of large language models. Automated jailbreaks typically optimize an adversarial suffix or adapt long prompt templates by forcing the model to generate the initial part of a restricted or harmful response. In this work, we show that existing jailbreak attacks that leverage such mechanisms to unlock the model response can be detected by a straightforward perplexity-based filtering on the input prompt. To overcome this issue, we propose LatentBreak, a white-box jailbreak attack that generates natural adversarial prompts with low perplexity capable of evading such defenses. LatentBreak substitutes words in the input prompt with semantically-equivalent ones, preserving the initial intent of the prompt, instead of adding high-perplexity adversarial suffixes or long templates. These words are chosen by minimizing the distance in the latent space between the representation of the adversarial prompt and that of harmless requests. Our extensive evaluation shows that LatentBreak leads to shorter and low-perplexity prompts, thus outperforming competing jailbreak algorithms against perplexity-based filters on multiple safety-aligned models.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parameterized Complexity of s-Club Cluster Edge Deletion: When Is the Diameter Bound Necessary?","url":"https://arxiv.org/abs/2510.07065","date":1761883200,"author":"","guid":322597,"unread":true,"content":"<article>arXiv:2510.07065v3 Announce Type: replace \nAbstract: We study when the diameter bound s is essential for the tractability of the s-Club Cluster Edge Deletion problem. Given a graph G = (V, E) and integers k and s, the goal is to delete at most k edges so that every connected component of the resulting graph has diameter at most s. This problem generalizes Cluster Edge Deletion (s = 1) and captures distance-bounded clustering tasks.\n  Montecchiani et al. (Information and Computation, 2023) proved that the problem is fixed-parameter tractable when parameterized by s + tw(G) and asked whether dependence on s is necessary. We answer negatively by showing W[1]-hardness when parameterized by pathwidth (and hence by treewidth), proving that s cannot in general be dropped. On the positive side, we show FPT algorithms parameterized by treedepth, neighborhood diversity, or cluster vertex deletion number, extending results of Italiano et al. (Algorithmica, 2023) and Komusiewicz and Uhlmann (SOFSEM, 2011). We also show that no polynomial kernel exists when parameterized by vertex cover number, even for s = 2.\n  Classically, the problem is NP-hard on split graphs for s = 2, complementing the polynomial case s = 1. We give an FPT bicriteria approximation scheme running in f(k, 1/epsilon) * n^{O(1)} that outputs a set of at most k deletions whose components have diameter at most (1 + epsilon)s. Finally, we introduce a directed generalization, s-Club Cluster Arc Deletion, extending the undirected case to reachability distances, and show it is W[1]-hard in parameter k even on directed acyclic graphs (DAGs).</article>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Think Then Embed: Generative Context Improves Multimodal Embedding","url":"https://arxiv.org/abs/2510.05014","date":1761883200,"author":"","guid":322598,"unread":true,"content":"<article>arXiv:2510.05014v3 Announce Type: replace \nAbstract: There is a growing interest in Universal Multimodal Embeddings (UME), where models are required to generate task-specific representations. While recent studies show that Multimodal Large Language Models (MLLMs) perform well on such tasks, they treat MLLMs solely as encoders, overlooking their generative capacity. However, such an encoding paradigm becomes less effective as instructions become more complex and require compositional reasoning. Inspired by the proven effectiveness of chain-of-thought reasoning, we propose a general Think-Then-Embed (TTE) framework for UME, composed of a reasoner and an embedder. The reasoner MLLM first generates reasoning traces that explain complex queries, followed by an embedder that produces representations conditioned on both the original query and the intermediate reasoning. This explicit reasoning step enables more nuanced understanding of complex multimodal instructions. Our contributions are threefold. First, by leveraging a powerful MLLM reasoner, we achieve state-of-the-art performance on the MMEB-V2 benchmark, surpassing proprietary models trained on massive in-house datasets. Second, to reduce the dependency on large MLLM reasoners, we finetune a smaller MLLM reasoner using high-quality embedding-centric reasoning traces, achieving the best performance among open-source models with a 7% absolute gain over recently proposed models. Third, we investigate strategies for integrating the reasoner and embedder into a unified model for improved efficiency without sacrificing performance.</article>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Truncated Kernel Stochastic Gradient Descent with General Losses and Spherical Radial Basis Functions","url":"https://arxiv.org/abs/2510.04237","date":1761883200,"author":"","guid":322599,"unread":true,"content":"<article>arXiv:2510.04237v3 Announce Type: replace \nAbstract: In this paper, we propose a novel kernel stochastic gradient descent (SGD) algorithm for large-scale supervised learning with general losses. Compared to traditional kernel SGD, our algorithm improves efficiency and scalability through an innovative regularization strategy. By leveraging the infinite series expansion of spherical radial basis functions, this strategy projects the stochastic gradient onto a finite-dimensional hypothesis space, which is adaptively scaled according to the bias-variance trade-off, thereby enhancing generalization performance. Based on a new estimation of the spectral structure of the kernel-induced covariance operator, we develop an analytical framework that unifies optimization and generalization analyses. We prove that both the last iterate and the suffix average converge at minimax-optimal rates, and we further establish optimal strong convergence in the reproducing kernel Hilbert space. Our framework accommodates a broad class of classical loss functions, including least-squares, Huber, and logistic losses. Moreover, the proposed algorithm significantly reduces computational complexity and achieves optimal storage complexity by incorporating coordinate-wise updates from linear SGD, thereby avoiding the costly pairwise operations typical of kernel SGD and enabling efficient processing of streaming data. Finally, extensive numerical experiments demonstrate the efficiency of our approach.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Epistemic Diversity and Knowledge Collapse in Large Language Models","url":"https://arxiv.org/abs/2510.04226","date":1761883200,"author":"","guid":322600,"unread":true,"content":"<article>arXiv:2510.04226v4 Announce Type: replace \nAbstract: Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning","url":"https://arxiv.org/abs/2510.02091","date":1761883200,"author":"","guid":322601,"unread":true,"content":"<article>arXiv:2510.02091v2 Announce Type: replace \nAbstract: Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PepCompass: Navigating peptide embedding spaces using Riemannian Geometry","url":"https://arxiv.org/abs/2510.01988","date":1761883200,"author":"","guid":322602,"unread":true,"content":"<article>arXiv:2510.01988v3 Announce Type: replace \nAbstract: Antimicrobial peptide discovery is challenged by the astronomical size of peptide space and the relative scarcity of active peptides. Generative models provide continuous latent \"maps\" of peptide space, but conventionally ignore decoder-induced geometry and rely on flat Euclidean metrics, rendering exploration and optimization distorted and inefficient. Prior manifold-based remedies assume fixed intrinsic dimensionality, which critically fails in practice for peptide data. Here, we introduce PepCompass, a geometry-aware framework for peptide exploration and optimization. At its core, we define a Union of $\\kappa$-Stable Riemannian Manifolds $\\mathbb{M}^{\\kappa}$, a family of decoder-induced manifolds that captures local geometry while ensuring computational stability. We propose two local exploration methods: Second-Order Riemannian Brownian Efficient Sampling, which provides a convergent second-order approximation to Riemannian Brownian motion, and Mutation Enumeration in Tangent Space, which reinterprets tangent directions as discrete amino-acid substitutions. Combining these yields Local Enumeration Bayesian Optimization (LE-BO), an efficient algorithm for local activity optimization. Finally, we introduce Potential-minimizing Geodesic Search (PoGS), which interpolates between prototype embeddings along property-enriched geodesics, biasing discovery toward seeds, i.e. peptides with favorable activity. In-vitro validation confirms the effectiveness of PepCompass: PoGS yields four novel seeds, and subsequent optimization with LE-BO discovers 25 highly active peptides with broad-spectrum activity, including against resistant bacterial strains. These results demonstrate that geometry-informed exploration provides a powerful new paradigm for antimicrobial peptide design.</article>","contentLength":1852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the equivalence of NMDS codes","url":"https://arxiv.org/abs/2509.25645","date":1761883200,"author":"","guid":322603,"unread":true,"content":"<article>arXiv:2509.25645v2 Announce Type: replace \nAbstract: An $[n,k,d]$ linear code is said to be maximum distance separable (MDS) or almost maximum distance separable (AMDS) if $d=n-k+1$ or $d=n-k$, respectively. If a code and its dual code are both AMDS, then the code is said to be near maximum distance separable (NMDS). For $k=3$ and $k=4$, there are many constructions of NMDS codes by adding some suitable projective points to arcs in $\\mathrm{PG}(k-1,q)$. In this paper, we consider the monomial equivalence problem for some NMDS codes with the same weight distributions and present new constructions of NMDS codes.</article>","contentLength":617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation","url":"https://arxiv.org/abs/2509.25358","date":1761883200,"author":"","guid":322604,"unread":true,"content":"<article>arXiv:2509.25358v3 Announce Type: replace \nAbstract: Large-scale robot learning has recently shown promise for enabling robots to perform complex tasks by integrating perception, control, and language understanding. Yet, it struggles with long-horizon, contact-rich manipulation such as deformable object handling, where demonstration quality is inconsistent. Reward modeling offers a natural solution: by providing grounded progress signals, it transforms noisy demonstrations into stable supervision that generalizes across diverse trajectories. We introduce a stage-aware, video-based reward modeling framework that jointly predicts high-level task stages and fine-grained progress. Reward labels are automatically derived from natural language subtask annotations, ensuring consistent progress estimation across variable-length demonstrations. This design overcomes frame-index labeling, which fails in variable-duration tasks like folding a T-shirt. Our reward model demonstrates robustness to variability, generalization to out-of-distribution settings, and strong utility for policy training. Building on it, we propose Reward-Aligned Behavior Cloning (RA-BC), which filters high-quality data and reweights samples by reward. Experiments show the reward model alone outperforms baselines on validation and real robot rollouts. Integrated into RA-BC, our approach achieves 83% success on folding T-shirts from the flattened state and 67% from the crumpled state -- far surpassing vanilla behavior cloning, which attains only 8% and 0% success. Overall, our results highlight reward modeling as a key enabler for scalable, annotation-efficient, and robust imitation learning in long-horizon manipulation.</article>","contentLength":1709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cycle Diffusion Model for Counterfactual Image Generation","url":"https://arxiv.org/abs/2509.24267","date":1761883200,"author":"","guid":322605,"unread":true,"content":"<article>arXiv:2509.24267v2 Announce Type: replace \nAbstract: Deep generative models have demonstrated remarkable success in medical image synthesis. However, ensuring conditioning faithfulness and high-quality synthetic images for direct or counterfactual generation remains a challenge. In this work, we introduce a cycle training framework to fine-tune diffusion models for improved conditioning adherence and enhanced synthetic image realism. Our approach, Cycle Diffusion Model (CDM), enforces consistency between generated and original images by incorporating cycle constraints, enabling more reliable direct and counterfactual generation. Experiments on a combined 3D brain MRI dataset (from ABCD, HCP aging &amp; young adults, ADNI, and PPMI) show that our method improves conditioning accuracy and enhances image quality as measured by FID and SSIM. The results suggest that the cycle strategy used in CDM can be an effective method for refining diffusion-based medical image generation, with applications in data augmentation, counterfactual, and disease progression modeling.</article>","contentLength":1073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized Inference","url":"https://arxiv.org/abs/2509.24257","date":1761883200,"author":"","guid":322606,"unread":true,"content":"<article>arXiv:2509.24257v2 Announce Type: replace \nAbstract: Decentralized inference provides a scalable and resilient paradigm for serving large language models (LLMs), enabling distributed resource utilization and reducing reliance on centralized providers. However, in a permissionless environment without trusted nodes, ensuring the correctness of model outputs remains a core challenge. We introduce VeriLLM, a publicly verifiable protocol for decentralized LLM inference that achieves security under a one-honest-verifier assumption while maintaining practical efficiency. VeriLLM combines lightweight empirical rerunning with cryptographic commitments, allowing verifiers to validate results at approximately 1% of the underlying inference cost. To prevent verification bottlenecks, we design an isomorphic inference-verification architecture that multiplexes both inference and verification roles across the same GPU workers. This design (i) improves GPU utilization and overall throughput, (ii) enlarges the effective validator set, enhancing robustness and liveness, and (iii) enforces task indistinguishability to prevent node-specific optimizations or selective behavior. Through theoretical analysis and system-level evaluation, we show that VeriLLM achieves reliable public verifiability with minimal overhead, offering a practical foundation for trustworthy and scalable decentralized LLM inference.</article>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tunable-Generalization Diffusion Powered by Self-Supervised Contextual Sub-Data for Low-Dose CT Reconstruction","url":"https://arxiv.org/abs/2509.23885","date":1761883200,"author":"","guid":322607,"unread":true,"content":"<article>arXiv:2509.23885v2 Announce Type: replace \nAbstract: Current models based on deep learning for low-dose CT denoising rely heavily on paired data and generalize poorly. Even the more concerned diffusion models need to learn the distribution of clean data for reconstruction, which is difficult to satisfy in medical clinical applications. At the same time, self-supervised-based methods face the challenge of significant degradation of generalizability of models pre-trained for the current dose to expand to other doses. To address these issues, this work proposes a novel method of TUnable-geneRalizatioN Diffusion (TurnDiff) powered by self-supervised contextual sub-data for low-dose CT reconstruction. Firstly, a contextual subdata self-enhancing similarity strategy is designed for denoising centered on the LDCT projection domain, which provides an initial prior for the subsequent progress. Subsequently, the initial prior is used to combine knowledge distillation with a deep combination of latent diffusion models for optimizing image details. The pre-trained model is used for inference reconstruction, and the pixel-level self-correcting fusion technique is proposed for fine-grained reconstruction of the image domain to enhance the image fidelity, using the initial prior and the LDCT image as a guide. In addition, the technique is flexibly applied to the generalization of upper and lower doses or even unseen doses. Dual-domain strategy cascade for self-supervised LDCT denoising, TurnDiff requires only LDCT projection domain data for training and testing. Comprehensive evaluation on both benchmark datasets and real-world data demonstrates that TurnDiff consistently outperforms state-of-the-art methods in both reconstruction and generalization.</article>","contentLength":1765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RLBFF: Binary Flexible Feedback to bridge between Human Feedback & Verifiable Rewards","url":"https://arxiv.org/abs/2509.21319","date":1761883200,"author":"","guid":322608,"unread":true,"content":"<article>arXiv:2509.21319v2 Announce Type: replace \nAbstract: Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) are the main RL paradigms used in LLM post-training, each offering distinct advantages. However, RLHF struggles with interpretability and reward hacking because it relies on human judgments that usually lack explicit criteria, whereas RLVR is limited in scope by its focus on correctness-based verifiers. We propose Reinforcement Learning with Binary Flexible Feedback (RLBFF), which combines the versatility of human-driven preferences with the precision of rule-based verification, enabling reward models to capture nuanced aspects of response quality beyond mere correctness. RLBFF extracts principles that can be answered in a binary fashion (e.g. accuracy of information: yes, or code readability: no) from natural language feedback. Such principles can then be used to ground Reward Model training as an entailment task (response satisfies or does not satisfy an arbitrary principle). We show that Reward Models trained in this manner can outperform Bradley-Terry models when matched for data and achieve top performance on RM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24, 2025). Additionally, users can specify principles of interest at inference time to customize the focus of our reward models, in contrast to Bradley-Terry models. Finally, we present a fully open source recipe (including data) to align Qwen3-32B using RLBFF and our Reward Model, to match or exceed the performance of o3-mini and DeepSeek R1 on general alignment benchmarks of MT-Bench, WildBench, and Arena Hard v2 (at &lt;5% of the inference cost). Models: https://huggingface.co/collections/nvidia/reward-models-10-2025</article>","contentLength":1787,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2509.18667","date":1761883200,"author":"","guid":322609,"unread":true,"content":"<article>arXiv:2509.18667v2 Announce Type: replace \nAbstract: Graph-based Retrieval-augmented generation (RAG) has become a widely studied approach for improving the reasoning, accuracy, and factuality of Large Language Models (LLMs). However, many existing graph-based RAG systems overlook the high cost associated with LLM token usage during graph construction, hindering large-scale adoption. To address this, we propose TERAG, a simple yet effective framework designed to build informative graphs at a significantly lower cost. Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the retrieval phase, and we achieve at least 80% of the accuracy of widely used graph-based RAG methods while consuming only 3%-11% of the output tokens. With its low token footprint and efficient construction pipeline, TERAG is well-suited for large-scale and cost-sensitive deployment scenarios.</article>","contentLength":892,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shilling Recommender Systems by Generating Side-feature-aware Fake User Profiles","url":"https://arxiv.org/abs/2509.17918","date":1761883200,"author":"","guid":322610,"unread":true,"content":"<article>arXiv:2509.17918v5 Announce Type: replace \nAbstract: Recommender systems (RS) greatly influence users' consumption decisions, making them attractive targets for malicious shilling attacks that inject fake user profiles to manipulate recommendations. Existing shilling methods can generate effective and stealthy fake profiles when training data only contain rating matrix, but they lack comprehensive solutions for scenarios where side features are present and utilized by the recommender. To address this gap, we extend the Leg-UP framework by enhancing the generator architecture to incorporate side features, enabling the generation of side-feature-aware fake user profiles. Experiments on benchmarks show that our method achieves strong attack performance while maintaining stealthiness.</article>","contentLength":791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revealing Multimodal Causality with Large Language Models","url":"https://arxiv.org/abs/2509.17784","date":1761883200,"author":"","guid":322611,"unread":true,"content":"<article>arXiv:2509.17784v2 Announce Type: replace \nAbstract: Uncovering cause-and-effect mechanisms from data is fundamental to scientific progress. While large language models (LLMs) show promise for enhancing causal discovery (CD) from unstructured data, their application to the increasingly prevalent multimodal setting remains a critical challenge. Even with the advent of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two primary limitations: (1) difficulty in exploring intra- and inter-modal interactions for comprehensive causal variable identification; and (2) insufficiency to handle structural ambiguities with purely observational data. To address these challenges, we propose MLLM-CD, a novel framework for multimodal causal discovery from unstructured data. It consists of three key components: (1) a novel contrastive factor discovery module to identify genuine multimodal factors based on the interactions explored from contrastive sample pairs; (2) a statistical causal structure discovery module to infer causal relationships among discovered factors; and (3) an iterative multimodal counterfactual reasoning module to refine the discovery outcomes iteratively by incorporating the world knowledge and reasoning capabilities of MLLMs. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed MLLM-CD in revealing genuine factors and causal relationships among them from multimodal unstructured data.</article>","contentLength":1489,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing","url":"https://arxiv.org/abs/2509.17197","date":1761883200,"author":"","guid":322612,"unread":true,"content":"<article>arXiv:2509.17197v2 Announce Type: replace \nAbstract: Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.</article>","contentLength":1730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs","url":"https://arxiv.org/abs/2509.16648","date":1761883200,"author":"","guid":322613,"unread":true,"content":"<article>arXiv:2509.16648v2 Announce Type: replace \nAbstract: The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.</article>","contentLength":1292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural Atlas Graphs for Dynamic Scene Decomposition and Editing","url":"https://arxiv.org/abs/2509.16336","date":1761883200,"author":"","guid":322614,"unread":true,"content":"<article>arXiv:2509.16336v2 Announce Type: replace \nAbstract: Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes.\n  Project Page: https://princeton-computational-imaging.github.io/nag/</article>","contentLength":1656,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adversarial generalization of unfolding (model-based) networks","url":"https://arxiv.org/abs/2509.15370","date":1761883200,"author":"","guid":322615,"unread":true,"content":"<article>arXiv:2509.15370v3 Announce Type: replace \nAbstract: Unfolding networks are interpretable networks emerging from iterative algorithms, incorporate prior knowledge of data structure, and are designed to solve inverse problems like compressed sensing, which deals with recovering data from noisy, missing observations. Compressed sensing finds applications in critical domains, from medical imaging to cryptography, where adversarial robustness is crucial to prevent catastrophic failures. However, a solid theoretical understanding of the performance of unfolding networks in the presence of adversarial attacks is still in its infancy. In this paper, we study the adversarial generalization of unfolding networks when perturbed with $l_2$-norm constrained attacks, generated by the fast gradient sign method. Particularly, we choose a family of state-of-the-art overaparameterized unfolding networks and deploy a new framework to estimate their adversarial Rademacher complexity. Given this estimate, we provide adversarial generalization error bounds for the networks under study, which are tight with respect to the attack level. To our knowledge, this is the first theoretical analysis on the adversarial generalization of unfolding networks. We further present a series of experiments on real-world data, with results corroborating our derived theory, consistently for all data. Finally, we observe that the family's overparameterization can be exploited to promote adversarial robustness, shedding light on how to efficiently robustify neural networks.</article>","contentLength":1557,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph","url":"https://arxiv.org/abs/2509.13733","date":1761883200,"author":"","guid":322616,"unread":true,"content":"<article>arXiv:2509.13733v2 Announce Type: replace \nAbstract: Visual-Language Navigation (VLN) is a fundamental challenge in robotic systems, with broad applications for the deployment of embodied agents in real-world environments. Despite recent advances, existing approaches are limited in long-range spatial reasoning, often exhibiting low success rates and high inference latency, particularly in long-range navigation tasks. To address these limitations, we propose FSR-VLN, a vision-language navigation system that combines a Hierarchical Multi-modal Scene Graph (HMSG) with Fast-to-Slow Navigation Reasoning (FSR). The HMSG provides a multi-modal map representation supporting progressive retrieval, from coarse room-level localization to fine-grained goal view and object identification. Building on HMSG, FSR first performs fast matching to efficiently select candidate rooms, views, and objects, then applies VLM-driven refinement for final goal selection. We evaluated FSR-VLN across four comprehensive indoor datasets collected by humanoid robots, utilizing 87 instructions that encompass a diverse range of object categories. FSR-VLN achieves state-of-the-art (SOTA) performance in all datasets, measured by the retrieval success rate (RSR), while reducing the response time by 82% compared to VLM-based methods on tour videos by activating slow reasoning only when fast intuition fails. Furthermore, we integrate FSR-VLN with speech interaction, planning, and control modules on a Unitree-G1 humanoid robot, enabling natural language interaction and real-time navigation.</article>","contentLength":1576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Similarity-Distance-Magnitude Activations","url":"https://arxiv.org/abs/2509.12760","date":1761883200,"author":"","guid":322617,"unread":true,"content":"<article>arXiv:2509.12760v2 Announce Type: replace \nAbstract: We introduce the Similarity-Distance-Magnitude (SDM) activation function, a more robust and interpretable formulation of the standard softmax activation function, adding Similarity (i.e., correctly predicted depth-matches into training) awareness and Distance-to-training-distribution awareness to the existing output Magnitude (i.e., decision-boundary) awareness, and enabling interpretability-by-exemplar via dense matching. We further introduce the SDM estimator, based on a data-driven partitioning of the class-wise empirical CDFs via the SDM activation, to control the class- and prediction-conditional accuracy among selective classifications. When used as the final-layer activation over pre-trained language models for selective classification, the SDM estimator is more robust to co-variate shifts and out-of-distribution inputs than existing calibration methods using softmax activations, while remaining informative over in-distribution data.</article>","contentLength":1007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FEDONet : Fourier-Embedded DeepONet for Spectrally Accurate Operator Learning","url":"https://arxiv.org/abs/2509.12344","date":1761883200,"author":"","guid":322618,"unread":true,"content":"<article>arXiv:2509.12344v2 Announce Type: replace \nAbstract: Deep Operator Networks (DeepONets) have recently emerged as powerful data-driven frameworks for learning nonlinear operators, particularly suited for approximating solutions to partial differential equations. Despite their promising capabilities, the standard implementation of DeepONets, which typically employs fully connected linear layers in the trunk network, can encounter limitations in capturing complex spatial structures inherent to various PDEs. To address this limitation, we introduce Fourier-embedded trunk networks within the DeepONet architecture, leveraging random Fourier feature mappings to enrich spatial representation capabilities. Our proposed Fourier-embedded DeepONet (FEDONet) demonstrates superior performance compared to the traditional DeepONet across a comprehensive suite of PDE-driven datasets, including the two-dimensional Poisson, Burgers', Lorenz-63, Eikonal, Allen-Cahn, and the Kuramoto-Sivashinsky equation. Empirical evaluations of FEDONet consistently show significant improvements in solution reconstruction accuracy, with average relative $L^2$ performance gains ranging between 2-3$\\times$ compared to the DeepONet baseline. This study highlights the effectiveness of Fourier embeddings in enhancing neural operator learning, offering a robust and broadly applicable methodology for PDE surrogate modeling.</article>","contentLength":1403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Locality in Image Diffusion Models Emerges from Data Statistics","url":"https://arxiv.org/abs/2509.09672","date":1761883200,"author":"","guid":322619,"unread":true,"content":"<article>arXiv:2509.09672v2 Announce Type: replace \nAbstract: Recent work has shown that the generalization ability of image diffusion models arises from the locality properties of the trained neural network. In particular, when denoising a particular pixel, the model relies on a limited neighborhood of the input image around that pixel, which, according to the previous work, is tightly related to the ability of these models to produce novel images. Since locality is central to generalization, it is crucial to understand why diffusion models learn local behavior in the first place, as well as the factors that govern the properties of locality patterns. In this work, we present evidence that the locality in deep diffusion models emerges as a statistical property of the image dataset and is not due to the inductive bias of convolutional neural networks, as suggested in previous work. Specifically, we demonstrate that an optimal parametric linear denoiser exhibits similar locality properties to deep neural denoisers. We show, both theoretically and experimentally, that this locality arises directly from pixel correlations present in the image datasets. Moreover, locality patterns are drastically different on specialized datasets, approximating principal components of the data's covariance. We use these insights to craft an analytical denoiser that better matches scores predicted by a deep diffusion model than prior expert-crafted alternatives. Our key takeaway is that while neural network architectures influence generation quality, their primary role is to capture locality patterns inherent in the data.</article>","contentLength":1618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-time CBCT reconstructions using Krylov solvers in repeated scanning procedures","url":"https://arxiv.org/abs/2509.08574","date":1761883200,"author":"","guid":322620,"unread":true,"content":"<article>arXiv:2509.08574v2 Announce Type: replace \nAbstract: This work introduces a new efficient iterative solver for the reconstruction of real-time cone-beam computed tomography (CBCT), which is based on the Prior Image Constrained Compressed Sensing (PICCS) regularization and leverages the efficiency of Krylov subspace methods. In particular, we focus on the setting where a sequence of under-sampled CT scans are taken on the same object with only local changes (e.g. changes in a tumour size or the introduction of a surgical tool). This is very common, for example, in image-guided surgery, where the amount of measurements is limited to ensure the safety of the patient. In this case, we can also typically assume that a (good) initial reconstruction for the solution exists, coming from a previously over-sampled scan, so we can use this information to aid the subsequent reconstructions. The effectiveness of this method is demonstrated in both a synthetic scan and using real CT data, where it can be observed that the PICCS framework is very effective for the reduction of artifacts, and that the new method is faster than other common alternatives used in the same setting.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning - A Benchmark Dataset and Method","url":"https://arxiv.org/abs/2509.06771","date":1761883200,"author":"","guid":322621,"unread":true,"content":"<article>arXiv:2509.06771v2 Announce Type: replace \nAbstract: Dark humor in online memes poses unique challenges due to its reliance on implicit, sensitive, and culturally contextual cues. To address the lack of resources and methods for detecting dark humor in multimodal content, we introduce a novel dataset of 4,379 Reddit memes annotated for dark humor, target category (gender, mental health, violence, race, disability, and other), and a three-level intensity rating (mild, moderate, severe). Building on this resource, we propose a reasoning-augmented framework that first generates structured explanations for each meme using a Large Vision-Language Model (VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective to iteratively refine its explanations, ensuring completeness and alignment. We then extract textual features from both the OCR transcript and the self-refined reasoning via a text encoder, while visual features are obtained using a vision transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three streams, text, image, and reasoning, via pairwise attention mechanisms, producing a unified representation for classification. Experimental results demonstrate that our approach outperforms strong baselines across three tasks: dark humor detection, target identification, and intensity prediction. The dataset, annotations, and code are released to facilitate further research in multimodal humor understanding and content moderation. Code and Dataset are available at: https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning</article>","contentLength":1627,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No Such Thing as Free Brain Time: For a Pigouvian Tax on Attention Capture","url":"https://arxiv.org/abs/2509.06453","date":1761883200,"author":"","guid":322622,"unread":true,"content":"<article>arXiv:2509.06453v2 Announce Type: replace \nAbstract: In our age of digital platforms, human attention has become a scarce and highly valuable resource, rivalrous, tradable, and increasingly subject to market dynamics. This article explores the commodification of attention within the framework of the attention economy, arguing that attention should be understood as a common good threatened by over-exploitation. Drawing from philosophical, economic, and legal perspectives, we first conceptualize attention not only as an individual cognitive process but as a collective and infrastructural phenomenon susceptible to enclosure by digital intermediaries. We then identify and analyze negative externalities of the attention economy, particularly those stemming from excessive screen time: diminished individual agency, adverse health outcomes, and societal and political harms, including democratic erosion and inequality. These harms are largely unpriced by market actors and constitute a significant market failure. In response, among a spectrum of public policy tools ranging from informational campaigns to outright restrictions, we propose a Pigouvian tax on attention capture as a promising regulatory instrument to internalize the externalities and, in particular, the social cost of compulsive digital engagement. Such a tax would incentivize structural changes in platform design while preserving user autonomy. By reclaiming attention as a shared resource vital to human agency, health, and democracy, this article contributes a novel economic and policy lens to the debate on digital regulation. Ultimately, this article advocates for a paradigm shift: from treating attention as a private, monetizable asset to protecting it as a collective resource vital for humanity.</article>","contentLength":1782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection","url":"https://arxiv.org/abs/2509.04448","date":1761883200,"author":"","guid":322623,"unread":true,"content":"<article>arXiv:2509.04448v2 Announce Type: replace \nAbstract: Multimodal misinformation, encompassing textual, visual, and cross-modal distortions, poses an increasing societal threat that is amplified by generative AI. Existing methods typically focus on a single type of distortion and struggle to generalize to unseen scenarios. In this work, we observe that different distortion types share common reasoning capabilities while also requiring task-specific skills. We hypothesize that joint training across distortion types facilitates knowledge sharing and enhances the model's ability to generalize. To this end, we introduce TRUST-VL, a unified and explainable vision-language model for general multimodal misinformation detection. TRUST-VL incorporates a novel Question-Aware Visual Amplifier module, designed to extract task-specific visual features. To support training, we also construct TRUST-Instruct, a large-scale instruction dataset containing 198K samples featuring structured reasoning chains aligned with human fact-checking workflows. Extensive experiments on both in-domain and zero-shot benchmarks demonstrate that TRUST-VL achieves state-of-the-art performance, while also offering strong generalization and interpretability.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Convolutional Hierarchical Deep-learning Neural Network (C-HiDeNN) Framework for Non-linear Finite Element Analysis","url":"https://arxiv.org/abs/2509.02435","date":1761883200,"author":"","guid":322624,"unread":true,"content":"<article>arXiv:2509.02435v2 Announce Type: replace \nAbstract: We present a framework for the Convolutional Hierarchical Deep-learning Neural Network (C-HiDeNN) tailored for nonlinear finite element analysis. Building upon the structured foundation of HiDeNN, C-HiDeNN introduces a convolution operator to enhance numerical approximation. A distinctive feature of C-HiDeNN is its higher-order accurate approximation achieved through an expanded set of parameters, such as the polynomial order 'p,' dilation parameter 'a,' patch size 's,' and nodal position 'X'. These parameters function as the functional equivalents of weights and biases within each C-HiDeNN patch. In addition, C-HiDeNN can be selectively applied to regions requiring high resolution to adaptively improve local prediction accuracy. To demonstrate the effectiveness of this framework, we provide numerical examples in the context of nonlinear finite element analysis. The results show that our approach achieves significantly higher accuracy than conventional Finite Element Method (FEM) while substantially reducing computational costs.</article>","contentLength":1097,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DSDE: Dynamic Speculative Decoding with KLD Stability for Real-World Serving","url":"https://arxiv.org/abs/2509.01083","date":1761883200,"author":"","guid":322625,"unread":true,"content":"<article>arXiv:2509.01083v3 Announce Type: replace \nAbstract: Speculative decoding accelerates large language model inference, but its reliance on a fixed speculation length is suboptimal in large-batch serving environments with diverse requests. This paper explores a new direction for dynamic adaptation by investigating a novel class of post-hoc, diagnostic signals. We propose Dynamic Speculative Decoding Engine (DSDE), a training-free framework built on two primary components: (1) a predictive signal based on the variance of the Kullback-Leibler (KLD) divergence, which diagnoses the generation's regional stability, and (2) an adaptive speculation length cap to mitigate the straggler problem in per-sequence decoding. Experiments demonstrate the potential of using KLD-based stability signals for dynamic adaptation. An algorithm guided by these signals achieves end-to-end latency competitive with leading baselines and exhibits superior robustness across diverse workloads. This robustness is particularly valuable in challenging low-acceptance-rate regimes, where the proposed signal maintains its diagnostic utility. Collectively, these findings validate post-hoc signals as a valuable component for building more robust and intelligent LLM inference systems, and highlight a promising direction for future research on dynamic speculation length adaptation.</article>","contentLength":1362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RelP: Faithful and Efficient Circuit Discovery in Language Models via Relevance Patching","url":"https://arxiv.org/abs/2508.21258","date":1761883200,"author":"","guid":322626,"unread":true,"content":"<article>arXiv:2508.21258v2 Announce Type: replace \nAbstract: Activation patching is a standard method in mechanistic interpretability for localizing the components of a model responsible for specific behaviors, but it is computationally expensive to apply at scale. Attribution patching offers a faster, gradient-based approximation, yet suffers from noise and reduced reliability in deep, highly non-linear networks. In this work, we introduce Relevance Patching (RelP), which replaces the local gradients in attribution patching with propagation coefficients derived from Layer-wise Relevance Propagation (LRP). LRP propagates the network's output backward through the layers, redistributing relevance to lower-level components according to local propagation rules that ensure properties such as relevance conservation or improved signal-to-noise ratio. Like attribution patching, RelP requires only two forward passes and one backward pass, maintaining computational efficiency while improving faithfulness. We validate RelP across a range of models and tasks, showing that it more accurately approximates activation patching than standard attribution patching, particularly when analyzing residual stream and MLP outputs in the Indirect Object Identification (IOI) task. For instance, for MLP outputs in GPT-2 Large, attribution patching achieves a Pearson correlation of 0.006, whereas RelP reaches 0.956, highlighting the improvement offered by RelP. Additionally, we compare the faithfulness of sparse feature circuits identified by RelP and Integrated Gradients (IG), showing that RelP achieves comparable faithfulness without the extra computational cost associated with IG.</article>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding","url":"https://arxiv.org/abs/2508.21204","date":1761883200,"author":"","guid":322627,"unread":true,"content":"<article>arXiv:2508.21204v2 Announce Type: replace \nAbstract: We study how prompt-level inductive biases influence the cognitive behavior of large language models (LLMs) in instructional dialogue. We introduce a symbolic scaffolding method paired with a short-term memory schema designed to promote adaptive, structured reasoning in Socratic tutoring. Using controlled ablation across five system variants, we evaluate model outputs via expert-designed rubrics covering scaffolding, responsiveness, symbolic reasoning, and conversational memory. We present preliminary results using an LLM-based evaluation framework aligned to a cognitively grounded rubric. This enables scalable, systematic comparisons across architectural variants in early-stage experimentation. The preliminary results show that our full system consistently outperforms baseline variants. Analysis reveals that removing memory or symbolic structure degrades key cognitive behaviors, including abstraction, adaptive probing, and conceptual continuity. These findings support a processing-level account in which prompt-level cognitive scaffolds can reliably shape emergent instructional strategies in LLMs.</article>","contentLength":1167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recursive Experiment Design for Closed-Loop Identification with Output Perturbation Limits","url":"https://arxiv.org/abs/2508.18813","date":1761883200,"author":"","guid":322628,"unread":true,"content":"<article>arXiv:2508.18813v2 Announce Type: replace \nAbstract: In many applications, system identification experiments must be performed under output feedback to ensure safety or to maintain system operation. In this paper, we consider the online design of informative experiments for ARMAX models by applying a bounded perturbation to the input signal generated by a fixed output feedback controller. Specifically, the design constrains the resulting output perturbation within user-specified limits and can be efficiently computed in closed form. We demonstrate the effectiveness of the method in a numerical experiment.</article>","contentLength":612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symbolic Constraints in Polyhedral Enclosure and Tetrahedral Decomposition in Genus-0 Polyhedra","url":"https://arxiv.org/abs/2508.18222","date":1761883200,"author":"","guid":322629,"unread":true,"content":"<article>arXiv:2508.18222v5 Announce Type: replace \nAbstract: I present a coordinate-free, symbolic framework for determining whether a given set of polygonal faces can form a closed, genus-zero polyhedral surface and for predicting how such a surface could be decomposed into internal tetrahedra. The method uses only discrete incidence variables, such as the number of internal tetrahedra $T$, internal gluing triangles $N_i$, and internal triangulation segments $S_i$, and applies combinatorial feasibility checks before any geometric embedding is attempted. For polyhedra in \\emph{normal form}, I record exact incidence identities linking $V,E,F$ to a flatness parameter $S:=\\sum_f(\\tmop{deg} f-3)$, and I identify parity-sensitive effects in $E$, $F$, and $S$. The external identities and parity-sensitive bounds hold universally for genus-0 polyhedral graphs. For internal quantities, I prove exact relations $N_i=2T-V+2$ and $T-N_i+S_i=1$ (with $S_i$ taken to be the number of interior edges) and obtain restricted linear ranges for internally decomposed polyhedra with the minimal number of added internal edges. Consequently, I propose a symbolic workflow that yields rapid pre-checks for structural impossibility, reducing the need for costly geometric validation in computational geometry, graphics, and automated modeling.</article>","contentLength":1325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution","url":"https://arxiv.org/abs/2508.15840","date":1761883200,"author":"","guid":322630,"unread":true,"content":"<article>arXiv:2508.15840v3 Announce Type: replace \nAbstract: When using a public communication channel -- whether formal or informal, such as commenting or posting on social media -- end users have no expectation of privacy: they compose a message and broadcast it for the world to see. Even if an end user takes utmost precautions to anonymize their online presence -- using an alias or pseudonym; masking their IP address; spoofing their geolocation; concealing their operating system and user agent; deploying encryption; registering with a disposable phone number or email; disabling non-essential settings; revoking permissions; and blocking cookies and fingerprinting -- one obvious element still lingers: the message itself. Assuming they avoid lapses in judgment or accidental self-exposure, there should be little evidence to validate their actual identity, right? Wrong. The content of their message -- necessarily open for public consumption -- exposes an attack vector: stylometric analysis, or author profiling. In this paper, we dissect the technique of stylometry, discuss an antithetical counter-strategy in adversarial stylometry, and devise enhancements through Unicode steganography.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TinyTim: A Family of Language Models for Divergent Generation","url":"https://arxiv.org/abs/2508.11607","date":1761883200,"author":"","guid":322631,"unread":true,"content":"<article>arXiv:2508.11607v2 Announce Type: replace \nAbstract: In the search for artificial general intelligence, model development and training has focused primarily on vast datasets of known problems and their accepted solutions. This process necessarily produces convergent systems which are fundamentally incapable of the conceptual reframing that is required for genuine creative breakthroughs. Inspired by the divergent cognitive processes that allow humans to make such creative leaps, our work introduces a family of language models, TinyTim, to serve as sources of divergent generation within broader systems. These models have been created by fine-tuning on the anti-parsimonious text of James Joyce's `Finnegans Wake'. Quantitative analysis of both an unsupervised fine-tuned model (TinyTim-V1) and a new instruction-tuned variant (TinyTim-V2) demonstrates a profound capacity for lexical invention; the foundational V1 model exhibits a Yule's K score for lexical richness over twenty times greater than that of convergent baselines. This trait is a stable property of the family, as the instruction-tuned V2 maintains a statistically distinct profile and resists factual convergence, sacrificing benchmark performance to preserve its core generative style. This work establishes a methodology for engineering specialized divergent models that, when paired with convergent systems, can reframe problems and force breakthroughs beyond the reach of statistical optimization alone.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating Compilers for Qubit Mapping and Routing","url":"https://arxiv.org/abs/2508.10781","date":1761883200,"author":"","guid":322632,"unread":true,"content":"<article>arXiv:2508.10781v2 Announce Type: replace \nAbstract: To evaluate a quantum circuit on a quantum processor, one must find a mapping from circuit qubits to processor qubits and plan the instruction execution while satisfying the processor's constraints. This is known as the qubit mapping and routing (QMR) problem. High-quality QMR solutions are key to maximizing the utility of scarce quantum resources and minimizing the probability of logical errors affecting computation. The challenge is that the landscape of quantum processors is incredibly diverse and fast-evolving. Given this diversity, dozens of papers have addressed the QMR problem for different qubit hardware, connectivity constraints, and quantum error correction schemes by a developing a new algorithm for a particular context. We present an alternative approach: automatically generating qubit mapping and routing compilers for arbitrary quantum processors. Though each QMR problem is different, we identify a common core structure-device state machine-that we use to formulate an abstract QMR problem. Our formulation naturally leads to a compact domain-specific language for specifying QMR problems and a powerful parametric algorithm that can be instantiated for any QMR specification. Our thorough evaluation on case studies of important QMR problems shows that generated compilers are competitive with handwritten, specialized compilers in terms of runtime and solution quality.</article>","contentLength":1451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HM-Talker: Hybrid Motion Modeling for High-Fidelity Talking Head Synthesis","url":"https://arxiv.org/abs/2508.10566","date":1761883200,"author":"","guid":322633,"unread":true,"content":"<article>arXiv:2508.10566v2 Announce Type: replace \nAbstract: Audio-driven talking head video generation enhances user engagement in human-computer interaction. However, current methods frequently produce videos with motion blur and lip jitter, primarily due to their reliance on implicit modeling of audio-facial motion correlations--an approach lacking explicit articulatory priors (i.e., anatomical guidance for speech-related facial movements). To overcome this limitation, we propose HM-Talker, a novel framework for generating high-fidelity, temporally coherent talking heads. HM-Talker leverages a hybrid motion representation combining both implicit and explicit motion cues. Explicit cues use Action Units (AUs), anatomically defined facial muscle movements, alongside implicit features to minimize phoneme-viseme misalignment. Specifically, our Cross-Modal Disentanglement Module (CMDM) extracts complementary implicit/explicit motion features while predicting AUs directly from audio input aligned to visual cues. To mitigate identity-dependent biases in explicit features and enhance cross-subject generalization, we introduce the Hybrid Motion Modeling Module (HMMM). This module dynamically merges randomly paired implicit/explicit features, enforcing identity-agnostic learning. Together, these components enable robust lip synchronization across diverse identities, advancing personalized talking head synthesis. Extensive experiments demonstrate HM-Talker's superiority over state-of-the-art methods in visual quality and lip-sync accuracy.</article>","contentLength":1548,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distributed optimization: designed for federated learning","url":"https://arxiv.org/abs/2508.08606","date":1761883200,"author":"","guid":322634,"unread":true,"content":"<article>arXiv:2508.08606v3 Announce Type: replace \nAbstract: Federated learning (FL), as a distributed collaborative machine learning (ML) framework under privacy-preserving constraints, has garnered increasing research attention in cross-organizational data collaboration scenarios. This paper proposes a class of distributed optimization algorithms based on the augmented Lagrangian technique, designed to accommodate diverse communication topologies in both centralized and decentralized FL settings. Furthermore, we develop multiple termination criteria and parameter update mechanisms to enhance computational efficiency, accompanied by rigorous theoretical guarantees of convergence. By generalizing the augmented Lagrangian relaxation through the incorporation of proximal relaxation and quadratic approximation, our framework systematically recovers a broad of classical unconstrained optimization methods, including proximal algorithm, classic gradient descent, and stochastic gradient descent, among others. Notably, the convergence properties of these methods can be naturally derived within the proposed theoretical framework. Numerical experiments demonstrate that the proposed algorithm exhibits strong performance in large-scale settings with significant statistical heterogeneity across clients.</article>","contentLength":1303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning","url":"https://arxiv.org/abs/2508.08186","date":1761883200,"author":"","guid":322635,"unread":true,"content":"<article>arXiv:2508.08186v2 Announce Type: replace \nAbstract: Semantic segmentation of structural defects in civil infrastructure remains challenging due to variable defect appearances, harsh imaging conditions, and significant class imbalance. Current deep learning methods, despite their effectiveness, typically require millions of parameters, rendering them impractical for real-time inspection systems. We introduce KARMA (Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient semantic segmentation framework that models complex defect patterns through compositions of one-dimensional functions rather than conventional convolutions. KARMA features three technical innovations: (1) a parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging low-rank factorization for KAN-based feature transformation; (2) an optimized feature pyramid structure with separable convolutions for multi-scale defect analysis; and (3) a static-dynamic prototype mechanism that enhances feature representation for imbalanced classes. Extensive experiments on benchmark infrastructure inspection datasets demonstrate that KARMA achieves competitive or superior mean IoU performance compared to state-of-the-art approaches, while using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction). Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for real-time deployment, enabling practical automated infrastructure inspection systems without compromising accuracy. The source code can be accessed at the following URL: https://github.com/faeyelab/karma.</article>","contentLength":1595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation","url":"https://arxiv.org/abs/2508.07981","date":1761883200,"author":"","guid":322636,"unread":true,"content":"<article>arXiv:2508.07981v3 Announce Type: replace \nAbstract: Visual effects (VFX) are essential visual enhancements fundamental to modern cinematic production. Although video generation models offer cost-efficient solutions for VFX production, current methods are constrained by per-effect LoRA training, which limits generation to single effects. This fundamental limitation impedes applications that require spatially controllable composite effects, i.e., the concurrent generation of multiple effects at designated locations. However, integrating diverse effects into a unified framework faces major challenges: interference from effect variations and spatial uncontrollability during multi-VFX joint training. To tackle these challenges, we propose Omni-Effects, a first unified framework capable of generating prompt-guided effects and spatially controllable composite effects. The core of our framework comprises two key innovations: (1) LoRA-based Mixture of Experts (LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects within a unified model while effectively mitigating cross-task interference. (2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the text token, enabling precise spatial control. Furthermore, we introduce an Independent-Information Flow (IIF) module integrated within the SAP, isolating the control signals corresponding to individual effects to prevent any unwanted blending. To facilitate this research, we construct a comprehensive VFX dataset Omni-VFX via a novel data collection pipeline combining image editing and First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX evaluation framework for validating model performance. Extensive experiments demonstrate that Omni-Effects achieves precise spatial control and diverse effect generation, enabling users to specify both the category and location of desired effects.</article>","contentLength":1910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GFlowNets for Learning Better Drug-Drug Interaction Representations","url":"https://arxiv.org/abs/2508.06576","date":1761883200,"author":"","guid":322637,"unread":true,"content":"<article>arXiv:2508.06576v2 Announce Type: replace \nAbstract: Drug-drug interactions pose a significant challenge in clinical pharmacology, with severe class imbalance among interaction types limiting the effectiveness of predictive models. Common interactions dominate datasets, while rare but critical interactions remain underrepresented, leading to poor model performance on infrequent cases. Existing methods often treat DDI prediction as a binary problem, ignoring class-specific nuances and exacerbating bias toward frequent interactions. To address this, we propose a framework combining Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE) to generate synthetic samples for rare classes, improving model balance and generate effective and novel DDI pairs. Our approach enhances predictive performance across interaction types, ensuring better clinical reliability.</article>","contentLength":890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Smoothing Slot Attention Iterations and Recurrences","url":"https://arxiv.org/abs/2508.05417","date":1761883200,"author":"","guid":322638,"unread":true,"content":"<article>arXiv:2508.05417v2 Announce Type: replace \nAbstract: Slot Attention (SA) and its variants lie at the heart of mainstream Object-Centric Learning (OCL). Objects in an image can be aggregated into respective slot vectors, by \\textit{iteratively} refining cold-start query vectors, typically three times, via SA on image features. For video, such aggregation is \\textit{recurrently} shared across frames, with queries cold-started on the first frame while transitioned from the previous frame's slots on non-first frames. However, the cold-start queries lack sample-specific cues thus hinder precise aggregation on the image or video's first frame; Also, non-first frames' queries are already sample-specific thus require transforms different from the first frame's aggregation. We address these issues for the first time with our \\textit{SmoothSA}: (1) To smooth SA iterations on the image or video's first frame, we \\textit{preheat} the cold-start queries with rich information of input features, via a tiny module self-distilled inside OCL; (2) To smooth SA recurrences across all video frames, we \\textit{differentiate} the homogeneous transforms on the first and non-first frames, by using full and single iterations respectively. Comprehensive experiments on object discovery, recognition and downstream benchmarks validate our method's effectiveness. Further analyses intuitively illuminate how our method smooths SA iterations and recurrences. Our source code, model checkpoints and training logs are available on https://github.com/Genera1Z/SmoothSA.</article>","contentLength":1556,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of logics with arithmetic","url":"https://arxiv.org/abs/2508.03574","date":1761883200,"author":"","guid":322639,"unread":true,"content":"<article>arXiv:2508.03574v2 Announce Type: replace \nAbstract: We present new results on finite satisfiability of logics with counting and arithmetic. One result is a tight bound on the complexity of satisfiability of logics with so-called local Presburger quantifiers, which sum over neighbors of a node in a graph. A second contribution concerns computing a semilinear representation of the cardinalities associated with a formula in two variable logic extended with counting quantifiers. Such a representation allows you to get bounds not only on satisfiability for these logics, but for satisfiability in the presence of additional ``global cardinality constraints'': restrictions on cardinalities of unary formulas, expressed using arbitrary decidability logics over arithmetic. In the process, we provide simpler proofs of some key prior results on finite satisfiability and semi-linearity of the spectrum for these logics.</article>","contentLength":919,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning","url":"https://arxiv.org/abs/2508.03024","date":1761883200,"author":"","guid":322640,"unread":true,"content":"<article>arXiv:2508.03024v2 Announce Type: replace \nAbstract: Accurate and robust indoor localization is critical for smart building applications, yet existing Wi-Fi-based systems are often vulnerable to environmental conditions. This work presents a novel indoor localization system, called LiGen, that leverages the spectral intensity patterns of ambient light as fingerprints, offering a more stable and infrastructure-free alternative to radio signals. To address the limited spectral data, we design a data augmentation framework based on generative adversarial networks (GANs), featuring two variants: PointGAN, which generates fingerprints conditioned on coordinates, and FreeGAN, which uses a weak localization model to label unconditioned samples. Our positioning model, leveraging a Multi-Layer Perceptron (MLP) architecture to train on synthesized data, achieves submeter-level accuracy, outperforming Wi-Fi-based baselines by over 50\\%. LiGen also demonstrates strong robustness in cluttered environments. To the best of our knowledge, this is the first system to combine spectral fingerprints with GAN-based data augmentation for indoor localization.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Refine-n-Judge: Curating High-Quality Preference Chains for LLM-Fine-Tuning","url":"https://arxiv.org/abs/2508.01543","date":1761883200,"author":"","guid":322641,"unread":true,"content":"<article>arXiv:2508.01543v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable progress through preference-based fine-tuning, which critically depends on the quality of the underlying training data. While human feedback is essential for improving data quality, it is costly and does not scale well. In this paper, we introduce Refine-n-Judge, an automated iterative approach that leverages a single LLM as both a refiner and a judge to enhance dataset quality. Unlike existing iterative refinement methods, Refine-n-Judge employs an LLM to both generate refinements and explicitly evaluate each improvement, ensuring that every iteration meaningfully enhances the dataset without requiring additional human annotation or a separate reward model. At each step, the LLM refines a response and judges whether the refinement is an improvement over the previous answer. This process continues until the LLM prefers the initial answer over the refinement, indicating no further improvements. This produces sequences of increasing quality, preference-labeled responses ideal for fine-tuning.\n  We demonstrate the effectiveness of Refine-n-Judge across a range of public datasets spanning five corpora, targeting tasks such as coding, math, and conversation. Models (Llama 3.1-8B and Llama 3.3-70B) fine-tuned on Refine-n-Judge-enhanced datasets were preferred by LLM judges in over 74% of comparisons against models tuned on the original dataset by GPT-4. Additionally, we report performance gains: +5% on AlpacaEval and AlpacaEval 2.0, and +19% on MT-Bench. Our results indicate that Refine-n-Judge produces high-quality datasets and scalable model improvements.</article>","contentLength":1688,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Predicting Video Slot Attention Queries from Random Slot-Feature Pairs","url":"https://arxiv.org/abs/2508.01345","date":1761883200,"author":"","guid":322642,"unread":true,"content":"<article>arXiv:2508.01345v3 Announce Type: replace \nAbstract: Unsupervised video Object-Centric Learning (OCL) is promising as it enables object-level scene representation and dynamics modeling as we humans do. Mainstream video OCL methods adopt a recurrent architecture: An aggregator aggregates current video frame into object features, termed slots, under some queries; A transitioner transits current slots to queries for the next frame. This is an effective architecture but all existing implementations both (\\textit{i1}) neglect to incorporate next frame features, the most informative source for query prediction, and (\\textit{i2}) fail to learn transition dynamics, the knowledge essential for query prediction. To address these issues, we propose Random Slot-Feature pair for learning Query prediction (RandSF.Q): (\\textit{t1}) We design a new transitioner to incorporate both slots and features, which provides more information for query prediction; (\\textit{t2}) We train the transitioner to predict queries from slot-feature pairs randomly sampled from available recurrences, which drives it to learn transition dynamics. Experiments on scene representation demonstrate that our method surpass existing video OCL methods significantly, e.g., up to 10 points on object discovery, setting new state-of-the-art. Such superiority also benefits downstream tasks like dynamics modeling. Our core source code, model checkpoints and training logs are available on https://github.com/Genera1Z/RandSF.Q.</article>","contentLength":1497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CompoST: A Benchmark for Analyzing the Ability of LLMs To Compositionally Interpret Questions in a QALD Setting","url":"https://arxiv.org/abs/2507.21257","date":1761883200,"author":"","guid":322643,"unread":true,"content":"<article>arXiv:2507.21257v2 Announce Type: replace \nAbstract: Language interpretation is a compositional process, in which the meaning of more complex linguistic structures is inferred from the meaning of their parts. Large language models possess remarkable language interpretation capabilities and have been successfully applied to interpret questions by mapping them to SPARQL queries. An open question is how systematic this interpretation process is. Toward this question, in this paper, we propose a benchmark for investigating to what extent the abilities of LLMs to interpret questions are actually compositional. For this, we generate three datasets of varying difficulty based on graph patterns in DBpedia, relying on Lemon lexica for verbalization. Our datasets are created in a very controlled fashion in order to test the ability of LLMs to interpret structurally complex questions, given that they have seen the atomic building blocks. This allows us to evaluate to what degree LLMs are able to interpret complex questions for which they \"understand\" the atomic parts. We conduct experiments with models of different sizes using both various prompt and few-shot optimization techniques as well as fine-tuning. Our results show that performance in terms of macro $F_1$ degrades from $0.45$ over $0.26$ down to $0.09$ with increasing deviation from the samples optimized on. Even when all necessary information was provided to the model in the input, the $F_1$ scores do not exceed $0.57$ for the dataset of lowest complexity. We thus conclude that LLMs struggle to systematically and compositionally interpret questions and map them into SPARQL queries.</article>","contentLength":1657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs","url":"https://arxiv.org/abs/2507.19756","date":1761883200,"author":"","guid":322644,"unread":true,"content":"<article>arXiv:2507.19756v2 Announce Type: replace \nAbstract: In addition to its more widely studied cultural movements, American Evangelicalism has a well-developed but less externally visible literary side. Christian Fiction, however, has been little studied, and what scholarly attention there is has focused on the explosively popular Left Behind series. In this work, we use computational tools to provide both a broad topical overview of Christian Fiction as a genre and a more directed exploration of how its authors depict divine acts. Working with human annotators, we first developed a codebook for identifying \"acts of God.\" We then adapted the codebook for use by a recent, lightweight LM with the assistance of a much larger model. The laptop-scale LM is largely capable of matching human annotations, even when the task is subtle and challenging. Using these annotations, we show that significant and meaningful differences exist between divine acts depicted by the Left Behind books and Christian Fiction more broadly.</article>","contentLength":1024,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Falconry-like palm landing by a flapping-wing drone based on the human gesture interaction and distance-aware flight planning","url":"https://arxiv.org/abs/2507.17144","date":1761883200,"author":"","guid":322645,"unread":true,"content":"<article>arXiv:2507.17144v2 Announce Type: replace \nAbstract: Flapping-wing drones have attracted significant attention due to their biomimetic flight. They are considered more human-friendly due to their characteristics such as low noise and flexible wings, making them suitable for human-drone interactions. However, few studies have explored the practical interaction between humans and flapping-wing drones. On establishing a physical interaction system with flapping-wing drones, we can acquire inspirations from falconers who guide birds of prey to land on their arms. This interaction interprets the human body as a dynamic landing platform, which can be utilized in various scenarios such as crowded or spatially constrained environments. Thus, in this study, we propose a falconry-like interaction system in which a flapping-wing drone performs a palm landing motion on a human hand. To achieve a safe approach toward humans, we design a trajectory planning method that considers both physical and psychological factors of the human safety such as the drone's velocity and distance from the user. We use a commercial flapping platform with our implemented motion planning and conduct experiments to evaluate the palm landing performance and safety. The results demonstrate that our approach enables safe and smooth hand landing interactions. To the best of our knowledge, it is the first time to achieve a contact-based interaction between flapping-wing drones and humans.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction","url":"https://arxiv.org/abs/2507.16271","date":1761883200,"author":"","guid":322646,"unread":true,"content":"<article>arXiv:2507.16271v2 Announce Type: replace \nAbstract: With the emergence of large language models (LLMs), there is an expectation that LLMs can effectively extract explicit information from complex real-world documents (e.g., papers, reports). However, most LLMs generate paragraph-style answers that are chaotic, disorganized, and untraceable. To bridge this gap, we introduce the Arranged and Organized Extraction Benchmark (AOE), a new bilingual benchmark with data and documents of varying lengths designed to systematically evaluate the ability of LLMs to comprehend fragmented documents and reconstruct isolated information into one organized table. Unlike conventional text-to-table tasks, which rely on fixed schema and narrow task domains, AOE includes 11 carefully crafted tasks across three diverse domains, requiring models to generate context-specific schema tailored to varied input queries. In the experiment, we evaluated both open-source and closed-source state-of-the-art LLMs. The results show that even the most advanced models struggled significantly. The benchmark is available at https://anonymous.4open.science/r/AOE-Benchmark/.</article>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Convex computation of regions of attraction from data using Sums-of-Squares programming","url":"https://arxiv.org/abs/2507.14073","date":1761883200,"author":"","guid":322647,"unread":true,"content":"<article>arXiv:2507.14073v2 Announce Type: replace \nAbstract: The paper concentrates on the analysis of the Region of Attraction (RoA) for unknown autonomous dynamical systems. The aim is to explore a data-driven approach based on moment Sum-of-Squares (SoS) hierarchy, which enables novel RoA outer approximations despite the reduced information on the structure of the dynamics. The main contribution of this work is bypassing the system model and, consequently, the recurring constraint on its polynomial structure. Numerical experimentation showcases the influence of data on learned approximating sets, offering a promising outlook on the potential of this method.</article>","contentLength":660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It","url":"https://arxiv.org/abs/2507.13328","date":1761883200,"author":"","guid":322648,"unread":true,"content":"<article>arXiv:2507.13328v2 Announce Type: replace \nAbstract: Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? Most results in the literature have shown inconsistent or marginal differences, both behaviorally and representationally. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update","url":"https://arxiv.org/abs/2507.11847","date":1761883200,"author":"","guid":322649,"unread":true,"content":"<article>arXiv:2507.11847v2 Announce Type: replace \nAbstract: We study the generalized linear bandit (GLB) problem, a contextual multi-armed bandit framework that extends the classical linear model by incorporating a non-linear link function, thereby modeling a broad class of reward distributions such as Bernoulli and Poisson. While GLBs are widely applicable to real-world scenarios, their non-linear nature introduces significant challenges in achieving both computational and statistical efficiency. Existing methods typically trade off between two objectives, either incurring high per-round costs for optimal regret guarantees or compromising statistical efficiency to enable constant-time updates. In this paper, we propose a jointly efficient algorithm that attains a nearly optimal regret bound with $\\mathcal{O}(1)$ time and space complexities per round. The core of our method is a tight confidence set for the online mirror descent (OMD) estimator, which is derived through a novel analysis that leverages the notion of mix loss from online prediction. The analysis shows that our OMD estimator, even with its one-pass updates, achieves statistical efficiency comparable to maximum likelihood estimation, thereby leading to a jointly efficient optimistic method.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oneiros: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving","url":"https://arxiv.org/abs/2507.11507","date":1761883200,"author":"","guid":322650,"unread":true,"content":"<article>arXiv:2507.11507v2 Announce Type: replace \nAbstract: KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce Oneiros, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that Oneiros significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM. Source code of Oneiros is available at https://github.com/UT-SysML/Oneiros/.</article>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training","url":"https://arxiv.org/abs/2507.09846","date":1761883200,"author":"","guid":322651,"unread":true,"content":"<article>arXiv:2507.09846v3 Announce Type: replace \nAbstract: As both model and dataset sizes continue to scale rapidly, conventional pretraining strategies with fixed compute budgets-such as cosine learning rate schedules-are increasingly inadequate for large-scale training. Recent alternatives, including warmup-stable-decay (WSD) schedules and weight averaging, offer greater flexibility. However, WSD relies on explicit decay phases to track progress, while weight averaging addresses this limitation at the cost of additional memory. In search of a more principled and scalable alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024], which has shown strong empirical performance across diverse settings. We show that SF-AdamW effectively navigates the \"river\" structure of the loss landscape without decay phases or auxiliary averaging, making it particularly suitable for continuously scaling training workloads. To understand this behavior, we conduct a theoretical and empirical analysis of SF dynamics, revealing that it implicitly performs weight averaging without memory overhead. Guided by this analysis, we propose a refined variant of SF that improves robustness to momentum and performs better under large batch sizes, addressing key limitations of the original method. Together, these results establish SF as a practical, scalable, and theoretically grounded approach for language model training.</article>","contentLength":1426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From One to More: Contextual Part Latents for 3D Generation","url":"https://arxiv.org/abs/2507.08772","date":1761883200,"author":"","guid":322652,"unread":true,"content":"<article>arXiv:2507.08772v2 Announce Type: replace \nAbstract: Recent advances in 3D generation have transitioned from multi-view 2D rendering approaches to 3D-native latent diffusion frameworks that exploit geometric priors in ground truth data. Despite progress, three key limitations persist: (1) Single-latent representations fail to capture complex multi-part geometries, causing detail degradation; (2) Holistic latent coding neglects part independence and interrelationships critical for compositional design; (3) Global conditioning mechanisms lack fine-grained controllability. Inspired by human 3D design workflows, we propose CoPart - a part-aware diffusion framework that decomposes 3D objects into contextual part latents for coherent multi-part generation. This paradigm offers three advantages: i) Reduces encoding complexity through part decomposition; ii) Enables explicit part relationship modeling; iii) Supports part-level conditioning. We further develop a mutual guidance strategy to fine-tune pre-trained diffusion models for joint part latent denoising, ensuring both geometric coherence and foundation model priors. To enable large-scale training, we construct Partverse - a novel 3D part dataset derived from Objaverse through automated mesh segmentation and human-verified annotations. Extensive experiments demonstrate CoPart's superior capabilities in part-level editing, articulated object generation, and scene composition with unprecedented controllability.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics","url":"https://arxiv.org/abs/2507.07830","date":1761883200,"author":"","guid":322653,"unread":true,"content":"<article>arXiv:2507.07830v3 Announce Type: replace \nAbstract: A model-order reduction framework for the meshless smoothed-particle hydrodynamics (SPH) method is presented. The proposed framework introduces the concept of modal reference spaces to overcome the challenges of discovering low-dimensional subspaces from unstructured, dynamic, and mixing numerical topology that occurs in SPH simulations. These reference spaces enable a low-dimensional representation of the field equations while maintaining the inherent meshless qualities of SPH. Modal reference spaces are constructed by projecting snapshot data onto a reference space where low-dimensionality of field quantities can be discovered via traditional modal decomposition techniques (e.g., the proper orthogonal decomposition (POD)). Modal quantities are mapped back to the meshless SPH space via scattered data interpolation during the online predictive stage. The proposed model-order reduction framework is cast into the meshless Galerkin POD and the Adjoint Petrov-Galerkin projection model-order reduction (PMOR) formulation. The PMORs are tested on three numerical experiments: 1) the Taylor--Green vortex; 2) the lid-driven cavity; and 3) the flow past an open cavity. Results show good agreement in reconstructed and predictive velocity fields, which showcase the ability of this framework to evolve the field equations in a low-dimensional subspace on an unstructured, dynamic, and mixing numerical topology. Results also show that the pressure field is sensitive to the projection error due to the stiff weakly-compressible assumption made in the current SPH framework, but this sensitivity can be alleviated through nonlinear approximations, such as the APG approach. The proposed meshless model-order reduction framework reports up to 90,000x dimensional compression within 10% error in quantities of interest, marking a step toward drastic cost reduction in SPH simulations.</article>","contentLength":1941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ScoreAdv: Score-based Targeted Generation of Natural Adversarial Examples via Diffusion Models","url":"https://arxiv.org/abs/2507.06078","date":1761883200,"author":"","guid":322654,"unread":true,"content":"<article>arXiv:2507.06078v2 Announce Type: replace \nAbstract: Despite the success of deep learning across various domains, it remains vulnerable to adversarial attacks. Although many existing adversarial attack methods achieve high success rates, they typically rely on $\\ell_{p}$-norm perturbation constraints, which do not align with human perceptual capabilities. Consequently, researchers have shifted their focus toward generating natural, unrestricted adversarial examples (UAEs). GAN-based approaches suffer from inherent limitations, such as poor image quality due to instability and mode collapse. Meanwhile, diffusion models have been employed for UAE generation, but they still rely on iterative PGD perturbation injection, without fully leveraging their central denoising capabilities. In this paper, we introduce a novel approach for generating UAEs based on diffusion models, named ScoreAdv. This method incorporates an interpretable adversarial guidance mechanism to gradually shift the sampling distribution towards the adversarial distribution, while using an interpretable saliency map to inject the visual information of a reference image into the generated samples. Notably, our method is capable of generating an unlimited number of natural adversarial examples and can attack not only classification models but also retrieval models. We conduct extensive experiments on ImageNet and CelebA datasets, validating the performance of ScoreAdv across ten target models in both black-box and white-box settings. Our results demonstrate that ScoreAdv achieves state-of-the-art attack success rates and image quality, while maintaining inference efficiency. Furthermore, the dynamic balance between denoising and adversarial perturbation enables ScoreAdv to remain robust even under defensive measures.</article>","contentLength":1807,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Controlling Thinking Speed in Reasoning Models","url":"https://arxiv.org/abs/2507.03704","date":1761883200,"author":"","guid":322655,"unread":true,"content":"<article>arXiv:2507.03704v2 Announce Type: replace \nAbstract: Human cognition is theorized to operate in two modes: fast, intuitive System 1 thinking and slow, deliberate System 2 thinking. While current Large Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform fast thinking leads to high computational overhead and latency. In this work, we enable LRMs to approximate human intelligence through dynamic thinking speed adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses two key questions: (1) how to control thinking speed in LRMs, and (2) when to adjust it for optimal performance. For the first question, we identify the steering vector that governs slow-fast thinking transitions in LRMs' representation space. Using this vector, we achieve the first representation editing-based test-time scaling effect, outperforming existing prompt-based scaling methods. For the second question, we apply real-time difficulty estimation to signal reasoning segments of varying complexity. Combining these techniques, we propose the first reasoning strategy that enables fast processing of easy steps and deeper analysis for complex reasoning. Without any training or additional cost, our plug-in module delivers an average +1.3% accuracy with -8.6% token usage across leading LRMs and advanced reasoning benchmarks. All of our algorithms are implemented based on vLLM and are expected to support broader applications and inspire future research.</article>","contentLength":1482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis and Optimized CXL-Attached Memory Allocation for Long-Context LLM Fine-Tuning","url":"https://arxiv.org/abs/2507.03305","date":1761883200,"author":"","guid":322656,"unread":true,"content":"<article>arXiv:2507.03305v2 Announce Type: replace \nAbstract: The substantial memory requirements of Large Language Models (LLMs), particularly for long-context fine-tuning, have renewed interest in CPU offloading to augment limited GPU memory. However, as context lengths grow, relying on CPU memory for intermediate states introduces a significant bottleneck that can exhaust the capacity of mainstream client platforms. To address this limitation, this work investigates the effectiveness of Compute Express Link (CXL) add-in card (AIC) memory as an extension to CPU memory, enabling larger model sizes and longer context lengths during fine-tuning. Extensive benchmarking reveals two critical challenges. First, current deep learning frameworks such as PyTorch lack fine-grained, per-tensor control over NUMA memory allocation, exposing only coarse, process-level policies. Second, due to this lack of control, when the memory footprint of fine-tuning is offloaded across local DRAM and CXL-attached memory, naively placing optimizer data in higher-latency CXL leads to substantial slowdowns in the optimizer step (e.g., 4x once data exceeds 20M elements). To overcome these challenges, this work introduces a PyTorch extension that enables tensor-level system memory control and a CXL-aware memory allocator that pins latency-critical tensors in local DRAM while maximizing bandwidth by striping latency-tolerant tensors across one or more CXL devices. Evaluated on a real hardware setup with 7B and 12B models, 4K-32K contexts, and a single GPU, our approach recovers throughput to 97-99% of DRAM-only with a single AIC and approximately 100% with two AICs, delivering up to 21% improvement over naive interleaving while preserving DRAM-like DMA bandwidth for GPU transfers. These results show that carefully managed CXL-attached memory is a practical path to scaling long-context fine-tuning beyond DRAM limits.</article>","contentLength":1909,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding","url":"https://arxiv.org/abs/2507.02843","date":1761883200,"author":"","guid":322657,"unread":true,"content":"<article>arXiv:2507.02843v2 Announce Type: replace \nAbstract: Estimating treatment effects is crucial for personalized decision-making in medicine, but this task faces unique challenges in clinical practice. At training time, models for estimating treatment effects are typically trained on well-structured medical datasets that contain detailed patient information. However, at inference time, predictions are often made using textual descriptions (e.g., descriptions with self-reported symptoms), which are incomplete representations of the original patient information. In this work, we make three contributions. (1) We show that the discrepancy between the data available during training time and inference time can lead to biased estimates of treatment effects. We formalize this issue as an inference time text confounding problem, where confounders are fully observed during training time but only partially available through text at inference time. (2) To address this problem, we propose a novel framework for estimating treatment effects that explicitly accounts for inference time text confounding. Our framework leverages large language models together with a custom doubly robust learner to mitigate biases caused by the inference time text confounding. (3) Through a series of experiments, we demonstrate the effectiveness of our framework in real-world applications.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Generalization in Node and Link Prediction","url":"https://arxiv.org/abs/2507.00927","date":1761883200,"author":"","guid":322658,"unread":true,"content":"<article>arXiv:2507.00927v3 Announce Type: replace \nAbstract: Using message-passing graph neural networks (MPNNs) for node and link prediction is crucial in various scientific and industrial domains, which has led to the development of diverse MPNN architectures. Besides working well in practical settings, their ability to generalize beyond the training set remains poorly understood. While some studies have explored MPNNs' generalization in graph-level prediction tasks, much less attention has been given to node- and link-level predictions. Existing works often rely on unrealistic i.i.d.\\@ assumptions, overlooking possible correlations between nodes or links, and assuming fixed aggregation and impractical loss functions while neglecting the influence of graph structure. In this work, we introduce a unified framework to analyze the generalization properties of MPNNs in inductive and transductive node and link prediction settings, incorporating diverse architectural parameters and loss functions and quantifying the influence of graph structure. Additionally, our proposed generalization framework can be applied beyond graphs to any classification task under the inductive or transductive setting. Our empirical study supports our theoretical insights, deepening our understanding of MPNNs' generalization capabilities in these tasks.</article>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DDL: A Large-Scale Datasets for Deepfake Detection and Localization in Diversified Real-World Scenarios","url":"https://arxiv.org/abs/2506.23292","date":1761883200,"author":"","guid":322659,"unread":true,"content":"<article>arXiv:2506.23292v2 Announce Type: replace \nAbstract: Recent advances in AIGC have exacerbated the misuse of malicious deepfake content, making the development of reliable deepfake detection methods an essential means to address this challenge. Although existing deepfake detection models demonstrate outstanding performance in detection metrics, most methods only provide simple binary classification results, lacking interpretability. Recent studies have attempted to enhance the interpretability of classification results by providing spatial manipulation masks or temporal forgery segments. However, due to the limitations of forgery datasets, the practical effectiveness of these methods remains suboptimal. The primary reason lies in the fact that most existing deepfake datasets contain only binary labels, with limited variety in forgery scenarios, insufficient diversity in deepfake types, and relatively small data scales, making them inadequate for complex real-world scenarios.To address this predicament, we construct a novel large-scale deepfake detection and localization (\\textbf{DDL}) dataset containing over $\\textbf{1.4M+}$ forged samples and encompassing up to $\\textbf{80}$ distinct deepfake methods. The DDL design incorporates four key innovations: (1) \\textbf{Comprehensive Deepfake Methods} (covering 7 different generation architectures and a total of 80 methods), (2) \\textbf{Varied Manipulation Modes} (incorporating 7 classic and 3 novel forgery modes), (3) \\textbf{Diverse Forgery Scenarios and Modalities} (including 3 scenarios and 3 modalities), and (4) \\textbf{Fine-grained Forgery Annotations} (providing 1.18M+ precise spatial masks and 0.23M+ precise temporal segments).Through these improvements, our DDL not only provides a more challenging benchmark for complex real-world forgeries but also offers crucial support for building next-generation deepfake detection, localization, and interpretability methods.</article>","contentLength":1946,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Efficient Excavation Force Estimation for Wheel Loaders","url":"https://arxiv.org/abs/2506.22579","date":1761883200,"author":"","guid":322660,"unread":true,"content":"<article>arXiv:2506.22579v2 Announce Type: replace \nAbstract: Accurate prediction of excavation forces is critical for enabling autonomous operation and optimizing control strategies in earthmoving machinery. Conventional approaches often depend on extensive data collection or computationally expensive simulations across multiple soil types, which limits their scalability and adaptability. This study presents a data-efficient framework that calibrates soil parameters using force measurements from the preceding bucket-loading cycle. The proposed method is based on an analytical soil-tool interaction model formulated through the fundamental earthmoving equation, and employs a multi-stage optimization procedure during the loading phase to identify relevant soil parameters. These estimated parameters are then used to predict excavation forces in the subsequent cycle, allowing the system to adapt its control inputs without relying on large-scale datasets or machine learning model training. The framework is validated through high-fidelity simulations in the Algoryx Dynamics engine under different soil types and excavation trajectories, achieving root-mean-square prediction errors between 10% and 15%. This cycle-to-cycle adaptation demonstrates strong potential for scalable, online force estimation and efficient path planning in wheel loader operations.</article>","contentLength":1359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Boosting Generative Adversarial Transferability with Self-supervised Vision Transformer Features","url":"https://arxiv.org/abs/2506.21046","date":1761883200,"author":"","guid":322661,"unread":true,"content":"<article>arXiv:2506.21046v2 Announce Type: replace \nAbstract: The ability of deep neural networks (DNNs) come from extracting and interpreting features from the data provided. By exploiting intermediate features in DNNs instead of relying on hard labels, we craft adversarial perturbation that generalize more effectively, boosting black-box transferability. These features ubiquitously come from supervised learning in previous work. Inspired by the exceptional synergy between self-supervised learning and the Transformer architecture, this paper explores whether exploiting self-supervised Vision Transformer (ViT) representations can improve adversarial transferability. We present dSVA -- a generative dual self-supervised ViT features attack, that exploits both global structural features from contrastive learning (CL) and local textural features from masked image modeling (MIM), the self-supervised learning paradigm duo for ViTs. We design a novel generative training framework that incorporates a generator to create black-box adversarial examples, and strategies to train the generator by exploiting joint features and the attention mechanism of self-supervised ViTs. Our findings show that CL and MIM enable ViTs to attend to distinct feature tendencies, which, when exploited in tandem, boast great adversarial generalizability. By disrupting dual deep features distilled by self-supervised ViTs, we are rewarded with remarkable black-box transferability to models of various architectures that outperform state-of-the-arts. Code available at https://github.com/spencerwooo/dSVA.</article>","contentLength":1584,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AIMeter: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads","url":"https://arxiv.org/abs/2506.20535","date":1761883200,"author":"","guid":322662,"unread":true,"content":"<article>arXiv:2506.20535v2 Announce Type: replace \nAbstract: The rapid advancement of AI, particularly large language models (LLMs), has raised significant concerns about the energy use and carbon emissions associated with model training and inference. However, existing tools for measuring and reporting such impacts are often fragmented, lacking systematic metric integration and offering limited support for correlation analysis among them. This paper presents AIMeter, a comprehensive software toolkit for the measurement, analysis, and visualization of energy use, power draw, hardware performance, and carbon emissions across AI workloads. By seamlessly integrating with existing AI frameworks, AIMeter offers standardized reports and exports fine-grained time-series data to support benchmarking and reproducibility in a lightweight manner. It further enables in-depth correlation analysis between hardware metrics and model performance and thus facilitates bottleneck identification and performance enhancement. By addressing critical limitations in existing tools, AIMeter encourages the research community to weigh environmental impact alongside raw performance of AI workloads and advances the shift toward more sustainable \"Green AI\" practices. The code is available at https://github.com/SusCom-Lab/AIMeter.</article>","contentLength":1312,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling","url":"https://arxiv.org/abs/2506.19816","date":1761883200,"author":"","guid":322663,"unread":true,"content":"<article>arXiv:2506.19816v2 Announce Type: replace \nAbstract: Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong performance in robotic manipulation. However, these models remain constrained by the single-frame image paradigm and fail to fully leverage the temporal information offered by multi-frame histories, as directly feeding multiple frames into VLM backbones incurs substantial computational overhead and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm. CronusVLA follows a two-stage process: (1) Single-frame pretraining on large-scale embodied datasets with autoregressive prediction of action tokens, establishing an effective embodied vision-language foundation; (2) Multi-frame post-training, which adapts the prediction of the vision-language backbone from discrete tokens to learnable features, and aggregates historical information via feature chunking. CronusVLA effectively addresses the existing challenges of multi-frame modeling while enhancing performance and observational robustness. To evaluate the robustness under temporal and spatial disturbances, we introduce SimplerEnv-OR, a novel benchmark featuring 24 types of observational disturbances and 120 severity levels. Experiments across three embodiments in simulated and real-world environments demonstrate that CronusVLA achieves leading performance and superior robustness, with a 70.9% success rate on SimplerEnv, a 26.8% improvement over OpenVLA on LIBERO, and the highest robustness score on SimplerEnv-OR. These results highlight the potential of efficient multi-frame adaptation in VLA models for more powerful and robust real-world deployment.</article>","contentLength":1766,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spectral Approximation to Fractional Integral Operators","url":"https://arxiv.org/abs/2506.19332","date":1761883200,"author":"","guid":322664,"unread":true,"content":"<article>arXiv:2506.19332v4 Announce Type: replace \nAbstract: We propose a fast and stable method for constructing matrix approximations to fractional integral operators applied to series in the Chebyshev fractional polynomials. This method utilizes a recurrence relation satisfied by the fractional integrals of mapped Chebyshev polynomials and significantly outperforms existing methods. Through numerical examples, we highlight the broad applicability of these matrix approximations, including the solution of boundary value problems for fractional integral and differential equations. Additional applications include fractional differential equation initial value problems and fractional eigenvalue problems.</article>","contentLength":703,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A geometric framework for momentum-based optimizers for low-rank training","url":"https://arxiv.org/abs/2506.17475","date":1761883200,"author":"","guid":322665,"unread":true,"content":"<article>arXiv:2506.17475v3 Announce Type: replace \nAbstract: Low-rank pre-training and fine-tuning have recently emerged as promising techniques for reducing the computational and storage costs of large neural networks. Training low-rank parameterizations typically relies on conventional optimizers such as heavy ball momentum methods or Adam. In this work, we identify and analyze potential difficulties that these training methods encounter when used to train low-rank parameterizations of weights. In particular, we show that classical momentum methods can struggle to converge to a local optimum due to the geometry of the underlying optimization landscape. To address this, we introduce novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. Our approach leverages and combines tools from dynamical low-rank approximation and momentum-based optimization to design optimizers that respect the intrinsic geometry of the parameter space. We validate our methods through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets.</article>","contentLength":1164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality","url":"https://arxiv.org/abs/2506.14681","date":1761883200,"author":"","guid":322666,"unread":true,"content":"<article>arXiv:2506.14681v2 Announce Type: replace \nAbstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness, often surpassing superficial similarity between the training data and the benchmark, and that mid-layer weight changes correlate most strongly with performance gains. We release these 1,000+ SFT models and benchmark results to accelerate further research. All resources are available at https://github.com/llm-jp/massive-sft.</article>","contentLength":1126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Comparison of Precinct and District Voting Data Using Persistent Homology to Identify Gerrymandering in North Carolina","url":"https://arxiv.org/abs/2506.13997","date":1761883200,"author":"","guid":322667,"unread":true,"content":"<article>arXiv:2506.13997v4 Announce Type: replace \nAbstract: Gerrymandering is one of the biggest threats to American democracy. By manipulating district lines, politicians effectively choose their voters rather than the other way around. Current gerrymandering identification methods (namely the Polsby-Popper and Reock scores) focus on the compactness of congressional districts, making them extremely sensitive to physical geography. To address this gap, we extend Feng and Porter's 2021 paper, which used the level-set method to turn geographic shapefiles into filtered simplicial complexes, in order to compare precinct level voting data to district level voting data. As precincts are regarded as too small to be gerrymandered, we are able to identify discrepancies between precinct and district level voting data to quantify gerrymandering in the United States. By comparing the persistent homologies of Democratic voting regions at the precinct and district levels, we detect when areas have been \"cracked\" (split across multiple districts) or \"packed\" (compressed into one district) for partisan gain. This analysis was conducted for North Carolina House of Representatives elections (2012-2024). North Carolina has been redistricted four times in the past ten years, unusually frequent as most states redistrict decennially, making it a valuable case study. By comparing persistence barcodes at the precinct and district levels (using the bottleneck distance), we show that precinct level voting patterns do not significantly fluctuate biannually, while district level patterns do, suggesting that shifts are likely a result of redistricting rather than voter behavior, providing strong evidence of gerrymandering. This research presents a novel application of topological data analysis in evaluating gerrymandering and shows persistent homology can be useful in discerning gerrymandered districts.</article>","contentLength":1900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study","url":"https://arxiv.org/abs/2506.13464","date":1761883200,"author":"","guid":322668,"unread":true,"content":"<article>arXiv:2506.13464v2 Announce Type: replace \nAbstract: Large language models (LLMs) have shown impressive capabilities across tasks such as mathematics, coding, and reasoning, yet their learning ability, which is crucial for adapting to dynamic environments and acquiring new knowledge, remains underexplored. In this work, we address this gap by introducing a framework inspired by cognitive psychology and education. Specifically, we decompose general learning ability into three distinct, complementary dimensions: Learning from Instructor (acquiring knowledge via explicit guidance), Learning from Concept (internalizing abstract structures and generalizing to new contexts), and Learning from Experience (adapting through accumulated exploration and feedback). We conduct a comprehensive empirical study across the three learning dimensions and identify several insightful findings, such as (i) interaction improves learning; (ii) conceptual understanding is scale-emergent and benefits larger models; and (iii) LLMs are effective few-shot learners but not many-shot learners. Based on our framework and empirical findings, we introduce a benchmark that provides a unified and realistic evaluation of LLMs' general learning abilities across three learning cognition dimensions. It enables diagnostic insights and supports evaluation and development of more adaptive and human-like models.</article>","contentLength":1391,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation","url":"https://arxiv.org/abs/2506.13229","date":1761883200,"author":"","guid":322669,"unread":true,"content":"<article>arXiv:2506.13229v3 Announce Type: replace \nAbstract: Large Language Models (LLMs) have shown strong potential for recommendation by framing item prediction as a token-by-token language generation task. However, existing methods treat all item tokens equally, simply pursuing likelihood maximization during both optimization and decoding. This overlooks crucial token-level differences in decisiveness-many tokens contribute little to item discrimination yet can dominate optimization or decoding. To quantify token decisiveness, we propose a novel perspective that models item generation as a decision process, measuring token decisiveness by the Information Gain (IG) each token provides in reducing uncertainty about the generated item. Our empirical analysis reveals that most tokens have low IG but often correspond to high logits, disproportionately influencing training loss and decoding, which may impair model performance. Building on these insights, we introduce an Information Gain-based Decisiveness-aware Token handling (IGD) strategy that integrates token decisiveness into both tuning and decoding. Specifically, IGD downweights low-IG tokens during tuning and rebalances decoding to emphasize tokens with high IG. In this way, IGD moves beyond pure likelihood maximization, effectively prioritizing high-decisiveness tokens. Extensive experiments on four benchmark datasets with two LLM backbones demonstrate that IGD consistently improves recommendation accuracy, achieving significant gains on widely used ranking metrics compared to strong baselines.</article>","contentLength":1568,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time-Optimal and Energy-Efficient Deterministic Consensus","url":"https://arxiv.org/abs/2506.12282","date":1761883200,"author":"","guid":322670,"unread":true,"content":"<article>arXiv:2506.12282v2 Announce Type: replace \nAbstract: We study fault-tolerant consensus in a variant of the synchronous message passing model, where, in each round, every node can choose to be awake or asleep. This is known as the sleeping model (Chatterjee, Gmyr, Pandurangan PODC 2020) and defines the awake complexity (also called \\emph{energy complexity}), which measures the maximum number of rounds that any node is awake throughout the execution. Only awake nodes can send and receive messages in a given round and all messages sent to sleeping nodes are lost. We present new deterministic consensus algorithms that tolerate up to $f&lt;n$ crash failures, where $n$ is the number of nodes. Our algorithms match the optimal time complexity lower bound of $f+1$ rounds. For multi-value consensus, where the input values are chosen from some possibly large set, we achieve an energy complexity of ${O}(\\lceil f^2 / n \\rceil)$ rounds, whereas for binary consensus, we show that ${O}(\\lceil f / \\sqrt{n} \\rceil)$ rounds are possible.</article>","contentLength":1031,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs","url":"https://arxiv.org/abs/2506.11094","date":1761883200,"author":"","guid":322671,"unread":true,"content":"<article>arXiv:2506.11094v2 Announce Type: replace \nAbstract: With the rapid advancement of artificial intelligence, Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), including content generation, human-computer interaction, machine translation, and code generation. However, their widespread deployment has also raised significant safety concerns. In particular, LLM-generated content can exhibit unsafe behaviors such as toxicity, bias, or misinformation, especially in adversarial contexts, which has attracted increasing attention from both academia and industry. Although numerous studies have attempted to evaluate these risks, a comprehensive and systematic survey on safety evaluation of LLMs is still lacking. This work aims to fill this gap by presenting a structured overview of recent advances in safety evaluation of LLMs. Specifically, we propose a four-dimensional taxonomy: (i) Why to evaluate, which explores the background of safety evaluation of LLMs, how they differ from general LLMs evaluation, and the significance of such evaluation; (ii) What to evaluate, which examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and related aspects; (iii) Where to evaluate, which summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (iv) How to evaluate, which reviews existing mainstream evaluation methods based on the roles of the evaluators and some evaluation frameworks that integrate the entire evaluation pipeline. Finally, we identify the challenges in safety evaluation of LLMs and propose promising research directions to promote further advancement in this field. We emphasize the necessity of prioritizing safety evaluation to ensure the reliable and responsible deployment of LLMs in real-world applications.</article>","contentLength":1945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in Diffusion Models via RKE Score","url":"https://arxiv.org/abs/2506.10173","date":1761883200,"author":"","guid":322672,"unread":true,"content":"<article>arXiv:2506.10173v2 Announce Type: replace \nAbstract: Diffusion models have demonstrated remarkable success in high-fidelity image synthesis and prompt-guided generative modeling. However, ensuring adequate diversity in generated samples of prompt-guided diffusion models remains a challenge, particularly when the prompts span a broad semantic spectrum and the diversity of generated data needs to be evaluated in a prompt-aware fashion across semantically similar prompts. Recent methods have introduced guidance via diversity measures to encourage more varied generations. In this work, we extend the diversity measure-based approaches by proposing the Scalable Prompt-Aware R\\'eny Kernel Entropy Diversity Guidance (SPARKE) method for prompt-aware diversity guidance. SPARKE utilizes conditional entropy for diversity guidance, which dynamically conditions diversity measurement on similar prompts and enables prompt-aware diversity control. While the entropy-based guidance approach enhances prompt-aware diversity, its reliance on the matrix-based entropy scores poses computational challenges in large-scale generation settings. To address this, we focus on the special case of Conditional latent RKE Score Guidance, reducing entropy computation and gradient-based optimization complexity from the $O(n^3)$ of general entropy measures to $O(n)$. The reduced computational complexity allows for diversity-guided sampling over potentially thousands of generation rounds on different prompts. We numerically test the SPARKE method on several text-to-image diffusion models, demonstrating that the proposed method improves the prompt-aware diversity of the generated data without incurring significant computational costs. We release our code on the project page: https://mjalali.github.io/SPARKE</article>","contentLength":1798,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SAFE: Multitask Failure Detection for Vision-Language-Action Models","url":"https://arxiv.org/abs/2506.09937","date":1761883200,"author":"","guid":322673,"unread":true,"content":"<article>arXiv:2506.09937v2 Announce Type: replace \nAbstract: While vision-language-action models (VLAs) have shown promising robotic behaviors across a diverse set of manipulation tasks, they achieve limited success rates when deployed on novel tasks out of the box. To allow these policies to safely interact with their environments, we need a failure detector that gives a timely alert such that the robot can stop, backtrack, or ask for help. However, existing failure detectors are trained and tested only on one or a few specific tasks, while generalist VLAs require the detector to generalize and detect failures also in unseen tasks and novel environments. In this paper, we introduce the multitask failure detection problem and propose SAFE, a failure detector for generalist robot policies such as VLAs. We analyze the VLA feature space and find that VLAs have sufficient high-level knowledge about task success and failure, which is generic across different tasks. Based on this insight, we design SAFE to learn from VLA internal features and predict a single scalar indicating the likelihood of task failure. SAFE is trained on both successful and failed rollouts and is evaluated on unseen tasks. SAFE is compatible with different policy architectures. We test it on OpenVLA, $\\pi_0$, and $\\pi_0$-FAST in both simulated and real-world environments extensively. We compare SAFE with diverse baselines and show that SAFE achieves state-of-the-art failure detection performance and the best trade-off between accuracy and detection time using conformal prediction. More qualitative results and code can be found at the project webpage: https://vla-safe.github.io/</article>","contentLength":1664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comparing human and LLM politeness strategies in free production","url":"https://arxiv.org/abs/2506.09391","date":1761883200,"author":"","guid":322674,"unread":true,"content":"<article>arXiv:2506.09391v2 Announce Type: replace \nAbstract: Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals -- from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness). We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses in both constrained and open-ended production tasks. We find that larger models ($\\ge$70B parameters) successfully replicate key preferences from the computational pragmatics literature, and human evaluators surprisingly prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies even in positive contexts, potentially leading to misinterpretations. While modern LLMs demonstrate an impressive handle on politeness strategies, these subtle differences raise important questions about pragmatic alignment in AI systems.</article>","contentLength":1140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Kernels Multiply, Clusters Unify: Fusing Embeddings with the Kronecker Product","url":"https://arxiv.org/abs/2506.08645","date":1761883200,"author":"","guid":322675,"unread":true,"content":"<article>arXiv:2506.08645v2 Announce Type: replace \nAbstract: State-of-the-art embeddings often capture distinct yet complementary discriminative features: For instance, one image embedding model may excel at distinguishing fine-grained textures, while another focuses on object-level structure. Motivated by this observation, we propose a principled approach to fuse such complementary representations through kernel multiplication. Multiplying the kernel similarity functions of two embeddings allows their discriminative structures to interact, producing a fused representation whose kernel encodes the union of the clusters identified by each parent embedding. This formulation also provides a natural way to construct joint kernels for paired multi-modal data (e.g., image-text tuples), where the product of modality-specific kernels inherits structure from both domains. We highlight that this kernel product is mathematically realized via the Kronecker product of the embedding feature maps, yielding our proposed KrossFuse framework for embedding fusion. To address the computational cost of the resulting high-dimensional Kronecker space, we further develop RP-KrossFuse, a scalable variant that leverages random projections for efficient approximation. As a key application, we use this framework to bridge the performance gap between cross-modal embeddings (e.g., CLIP, BLIP) and unimodal experts (e.g., DINOv2, E5). Experiments show that RP-KrossFuse effectively integrates these models, enhancing modality-specific performance while preserving cross-modal alignment. The project code is available at https://github.com/yokiwuuu/KrossFuse.</article>","contentLength":1642,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Forecasting Public Sentiments via Mean Field Games","url":"https://arxiv.org/abs/2506.08465","date":1761883200,"author":"","guid":322676,"unread":true,"content":"<article>arXiv:2506.08465v3 Announce Type: replace \nAbstract: Motivated by the goal of forecasting public sentiments, we consider a forecasting problem in the context of the Mean Field Games theory. We develop a numerical method, which is a version of the so-called convexification method. We provide theoretical convergence analysis that establishes global convergence of the method with a convergence rate. We also conduct numerical experiments that demonstrate the accurate performance of the convexification technique and highlight some promising features of this approach.</article>","contentLength":568,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens","url":"https://arxiv.org/abs/2506.08410","date":1761883200,"author":"","guid":322677,"unread":true,"content":"<article>arXiv:2506.08410v2 Announce Type: replace \nAbstract: Previous research has primarily focused on the cognitive error detection capabilities of Large Language Models (LLMs), often prompting them to analyze mistakes in reasoning chains. However, few studies have examined the meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors), which are crucial for their reliability. While studies on LLM self-evaluation present some measures, such as perplexity, which can reflect the answer correctness and be viewed as the lens of meta-cognition, they lack step-level analysis and adaptation. This paper studies the evaluation of LLM meta-cognition using the current lenses and how to improve these lenses. Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation framework for benchmarking the existing lenses. Furthermore, a training-free Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost current meta-cognition lenses. Experimental results on three mathematical reasoning datasets and three LLMs show the reasonableness of AutoMeco by comparing it with Best-of-N verification. Moreover, the meta-cognition ability of LLMs can be better evaluated using MIRA.</article>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors","url":"https://arxiv.org/abs/2506.08188","date":1761883200,"author":"","guid":322678,"unread":true,"content":"<article>arXiv:2506.08188v2 Announce Type: replace \nAbstract: In this paper, we introduce GradEscape, the first gradient-based evader designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the undifferentiable computation problem, caused by the discrete nature of text, by introducing a novel approach to construct weighted embeddings for the detector input. It then updates the evader model parameters using feedback from victim detectors, achieving high attack success with minimal text modification. To address the issue of tokenizer mismatch between the evader and the detector, we introduce a warm-started evader method, enabling GradEscape to adapt to detectors across any language model architecture. Moreover, we employ novel tokenizer inference and model extraction techniques, facilitating effective evasion even in query-only access.\n  We evaluate GradEscape on four datasets and three widely-used language models, benchmarking it against four state-of-the-art AIGT evaders. Experimental results demonstrate that GradEscape outperforms existing evaders in various scenarios, including with an 11B paraphrase model, while utilizing only 139M parameters. We have successfully applied GradEscape to two real-world commercial AIGT detectors. Our analysis reveals that the primary vulnerability stems from disparity in text expression styles within the training data. We also propose a potential defense strategy to mitigate the threat of AIGT evaders. We open-source our GradEscape for developing more robust AIGT detectors.</article>","contentLength":1543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks","url":"https://arxiv.org/abs/2506.07500","date":1761883200,"author":"","guid":322679,"unread":true,"content":"<article>arXiv:2506.07500v2 Announce Type: replace \nAbstract: Modern neural networks demonstrate state-of-the-art performance on numerous existing benchmarks; however, their high computational requirements and energy consumption prompt researchers to seek more efficient solutions for real-world deployment. Logic gate networks (LGNs) learns a large network of logic gates for efficient image classification. However, learning a network that can solve a simple problem like CIFAR-10 can take days to weeks to train. Even then, almost half of the network remains unused, causing a discretization gap. This discretization gap hinders real-world deployment of LGNs, as the performance drop between training and inference negatively impacts accuracy. We inject Gumbel noise with a straight-through estimator during training to significantly speed up training, improve neuron utilization, and decrease the discretization gap. We theoretically show that this results from implicit Hessian regularization, which improves the convergence properties of LGNs. We train networks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap by $98\\%$, and reduce the number of unused gates by $100\\%$.</article>","contentLength":1189,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human-assisted Robotic Policy Refinement via Action Preference Optimization","url":"https://arxiv.org/abs/2506.07127","date":1761883200,"author":"","guid":322680,"unread":true,"content":"<article>arXiv:2506.07127v3 Announce Type: replace \nAbstract: Establishing a reliable and iteratively refined robotic system is essential for deploying real-world applications. While Vision-Language-Action (VLA) models are widely recognized as the foundation model for such robotic deployment, their reliance on offline expert demonstrations critically limits their capacity for post-deployment refinement. To mitigate this limitation, we introduce Action Preference Optimization (APO), a method designed to refine VLA models by human-assisted preference alignment gathered through interaction with environments. This method begins with a human-robot collaboration framework for reliable failure correction and interaction trajectory collection through human intervention. However, directly leveraging these interaction trajectories for preference optimization is non-trivial due to the challenges of irreversible robotic actions and token distribution mismatch. To solve this, APO proposes an adaptive reweighting algorithm with binary desirability signals derived from interaction, empowering VLA models effectively suppress failure-prone actions while enhancing corrective action adaptation. Ultimately, APO equips VLA models with the crucial capability to learn from failure, paving the way for their iterative refinement and reliable deployment in dynamic environments. The experiments conducted in simulation and real-world scenarios prove superior generalization and robustness of our human-assisted framework across a variety of manipulation tasks. We believe this work could bring insights for efficient and stable optimization of VLA models through human-robot collaboration. The code and dataset are released at https://github.com/GeWu-Lab/Action-Preference-Optimization</article>","contentLength":1772,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text","url":"https://arxiv.org/abs/2506.07001","date":1761883200,"author":"","guid":322681,"unread":true,"content":"<article>arXiv:2506.07001v2 Announce Type: replace \nAbstract: The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. While various AI-generated text detectors have been proposed to mitigate these risks, many remain vulnerable to simple evasion techniques such as paraphrasing. However, recent detectors have shown greater robustness against such basic attacks. In this work, we introduce Adversarial Paraphrasing, a training-free attack framework that universally humanizes any AI-generated text to evade detection more effectively. Our approach leverages an off-the-shelf instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, producing adversarial examples that are specifically optimized to bypass detection. Extensive experiments show that our attack is both broadly effective and highly transferable across several detection systems. For instance, compared to simple paraphrasing attack--which, ironically, increases the true positive at 1% false positive (T@1%F) by 8.57% on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on Fast-DetectGPT. Across a diverse set of detectors--including neural network-based, watermark-based, and zero-shot approaches--our attack achieves an average T@1%F reduction of 87.88% under the guidance of OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and attack success to find that our method can significantly reduce detection rates, with mostly a slight degradation in text quality. Our adversarial setup highlights the need for more robust and resilient detection strategies in the light of increasingly sophisticated evasion techniques.</article>","contentLength":1836,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GenIR: Generative Visual Feedback for Mental Image Retrieval","url":"https://arxiv.org/abs/2506.06220","date":1761883200,"author":"","guid":322682,"unread":true,"content":"<article>arXiv:2506.06220v2 Announce Type: replace \nAbstract: Vision-language models (VLMs) have shown strong performance on text-to-image retrieval benchmarks. However, bridging this success to real-world applications remains a challenge. In practice, human search behavior is rarely a one-shot action. Instead, it is often a multi-round process guided by clues in mind. That is, a mental image ranging from vague recollections to vivid mental representations of the target image. Motivated by this gap, we study the task of Mental Image Retrieval (MIR), which targets the realistic yet underexplored setting where users refine their search for a mentally envisioned image through multi-round interactions with an image search engine. Central to successful interactive retrieval is the capability of machines to provide users with clear, actionable feedback; however, existing methods rely on indirect or abstract verbal feedback, which can be ambiguous, misleading, or ineffective for users to refine the query. To overcome this, we propose GenIR, a generative multi-round retrieval paradigm leveraging diffusion-based image generation to explicitly reify the AI system's understanding at each round. These synthetic visual representations provide clear, interpretable feedback, enabling users to refine their queries intuitively and effectively. We further introduce a fully automated pipeline to generate a high-quality multi-round MIR dataset. Experimental results demonstrate that GenIR significantly outperforms existing interactive methods in the MIR scenario. This work establishes a new task with a dataset and an effective generative retrieval method, providing a foundation for future research in this direction</article>","contentLength":1714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation","url":"https://arxiv.org/abs/2506.05768","date":1761883200,"author":"","guid":322683,"unread":true,"content":"<article>arXiv:2506.05768v2 Announce Type: replace \nAbstract: Virtual screening (VS) is a critical component of modern drug discovery, yet most existing methods--whether physics-based or deep learning-based--are developed around holo protein structures with known ligand-bound pockets. Consequently, their performance degrades significantly on apo or predicted structures such as those from AlphaFold2, which are more representative of real-world early-stage drug discovery, where pocket information is often missing. In this paper, we introduce an alignment-and-aggregation framework to enable accurate virtual screening under structural uncertainty. Our method comprises two core components: (1) a tri-modal contrastive learning module that aligns representations of the ligand, the holo pocket, and cavities detected from structures, thereby enhancing robustness to pocket localization error; and (2) a cross-attention based adapter for dynamically aggregating candidate binding sites, enabling the model to learn from activity data even without precise pocket annotations. We evaluated our method on a newly curated benchmark of apo structures, where it significantly outperforms state-of-the-art methods in blind apo setting, improving the early enrichment factor (EF1%) from 11.75 to 37.19. Notably, it also maintains strong performance on holo structures. These results demonstrate the promise of our approach in advancing first-in-class drug discovery, particularly in scenarios lacking experimentally resolved protein-ligand complexes. Our implementation is publicly available at https://github.com/Wiley-Z/AANet.</article>","contentLength":1613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory","url":"https://arxiv.org/abs/2506.05696","date":1761883200,"author":"","guid":322684,"unread":true,"content":"<article>arXiv:2506.05696v2 Announce Type: replace \nAbstract: Recent advances in vision-language models have enabled rich semantic understanding across modalities. However, these encoding methods lack the ability to interpret or reason about the moral dimensions of content-a crucial aspect of human cognition. In this paper, we address this gap by introducing MoralCLIP, a novel embedding representation method that extends multimodal learning with explicit moral grounding based on Moral Foundations Theory (MFT). Our approach integrates visual and textual moral cues into a unified embedding space, enabling cross-modal moral alignment. MoralCLIP is grounded on the multi-label dataset Social-Moral Image Database to identify co-occurring moral foundations in visual content. For MoralCLIP training, we design a moral data augmentation strategy to scale our annotated dataset to 15,000 image-text pairs labeled with MFT-aligned dimensions. Our results demonstrate that explicit moral supervision improves both unimodal and multimodal understanding of moral content, establishing a foundation for morally-aware AI systems capable of recognizing and aligning with human moral values.</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Smallest Suffixient Sets as a Repetitiveness Measure","url":"https://arxiv.org/abs/2506.05638","date":1761883200,"author":"","guid":322685,"unread":true,"content":"<article>arXiv:2506.05638v2 Announce Type: replace \nAbstract: A suffixient set is a novel combinatorial object that captures the essential information of repetitive strings in a way that, provided with a random access mechanism, supports various forms of pattern matching. In this paper, we study the size $\\chi$ of the smallest suffixient set as a repetitiveness measure: we place it between known measures and study its sensitivity to various string operations. As a corollary of our results, we give a simple online algorithm to compute smallest suffixient sets.</article>","contentLength":556,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPARTA ALIGNMENT: Collectively Aligning Multiple Language Models through Combat","url":"https://arxiv.org/abs/2506.04721","date":1761883200,"author":"","guid":322686,"unread":true,"content":"<article>arXiv:2506.04721v2 Announce Type: replace \nAbstract: We propose SPARTA ALIGNMENT, an algorithm to collectively align multiple LLMs through competition and combat. To complement a single model's lack of diversity in generation and biases in evaluation, multiple LLMs form a \"sparta tribe\" to compete against each other in fulfilling instructions while serving as judges for the competition of others. For each iteration, one instruction and two models are selected for a duel, the other models evaluate the two responses, and their evaluation scores are aggregated through a adapted elo-ranking based reputation system, where winners/losers of combat gain/lose weight in evaluating others. The peer-evaluated combat results then become preference pairs where the winning response is preferred over the losing one, and all models learn from these preferences at the end of each iteration. SPARTA ALIGNMENT enables the self-evolution of multiple LLMs in an iterative and collective competition process. Extensive experiments demonstrate that SPARTA ALIGNMENT outperforms initial models and 4 self-alignment baselines across 10 out of 12 tasks and datasets with 7.0% average improvement. Further analysis reveals that SPARTA ALIGNMENT generalizes more effectively to unseen tasks and leverages the expertise diversity of participating models to produce more logical, direct and informative outputs.</article>","contentLength":1394,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver","url":"https://arxiv.org/abs/2506.02935","date":1761883200,"author":"","guid":322687,"unread":true,"content":"<article>arXiv:2506.02935v3 Announce Type: replace \nAbstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a promising approach to train a unified model capable of solving multiple Vehicle Routing Problem (VRP) variants. However, existing Reinforcement Learning (RL)-based multi-task methods can only train light decoder models on small-scale problems, exhibiting limited generalization ability when solving large-scale problems. To overcome this limitation, this work introduces a novel multi-task learning method driven by knowledge distillation (MTL-KD), which enables the efficient training of heavy decoder models with strong generalization ability. The proposed MTL-KD method transfers policy knowledge from multiple distinct RL-based single-task models to a single heavy decoder model, facilitating label-free training and effectively improving the model's generalization ability across diverse tasks. In addition, we introduce a flexible inference strategy termed Random Reordering Re-Construction (R3C), which is specifically adapted for diverse VRP tasks and further boosts the performance of the multi-task model. Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that our proposed method consistently achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection","url":"https://arxiv.org/abs/2506.02393","date":1761883200,"author":"","guid":322688,"unread":true,"content":"<article>arXiv:2506.02393v3 Announce Type: replace \nAbstract: Infrared small target detection is a challenging task due to its unique characteristics (e.g., small, dim, shapeless and changeable). Recently published CNN-based methods have achieved promising performance with heavy feature extraction and fusion modules. To achieve efficient and effective detection, we propose a recurrent reusable-convolution attention network (RRCA-Net) for infrared small target detection. Specifically, RRCA-Net incorporates reusable-convolution block (RuCB) in a recurrent manner without introducing extra parameters. With the help of the repetitive iteration in RuCB, the high-level information of small targets in the deep layers can be well maintained and further refined. Then, a dual interactive attention aggregation module (DIAAM) is proposed to promote the mutual enhancement and fusion of refined information. In this way, RRCA-Net can both achieve high-level feature refinement and enhance the correlation of contextual information between adjacent layers. Moreover, to achieve steady convergence, we design a target characteristic inspired loss function (DpT-k loss) by integrating physical and mathematical constraints. Experimental results on three benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate that our RRCA-Net can achieve comparable performance to the state-of-the-art methods while maintaining a small number of parameters, and act as a plug and play module to introduce consistent performance improvement for several popular IRSTD methods.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning","url":"https://arxiv.org/abs/2506.02392","date":1761883200,"author":"","guid":322689,"unread":true,"content":"<article>arXiv:2506.02392v2 Announce Type: replace \nAbstract: Neural Combinatorial Optimization (NCO) has emerged as a promising learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by minimizing the need for extensive manual engineering. While existing NCO methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated considerable success on problems of similar scale, their performance significantly degrades when applied to large-scale scenarios. This degradation arises from the distributional shift between training and testing data, rendering policies learned on small instances ineffective for larger problems. To overcome this limitation, we introduce a novel learning framework driven by Large Language Models (LLMs). This framework learns a projection between the training and testing distributions, which is then deployed to enhance the scalability of the NCO model. Notably, unlike prevailing techniques that necessitate joint training with the neural network, our approach operates exclusively during the inference phase, obviating the need for model retraining. Extensive experiments demonstrate that our method enables a backbone model (trained on 100-node instances) to achieve superior performance on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) of up to 100K nodes from diverse distributions.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Debate Aids Assessment of Controversial Claims","url":"https://arxiv.org/abs/2506.02175","date":1761883200,"author":"","guid":322690,"unread":true,"content":"<article>arXiv:2506.02175v2 Announce Type: replace \nAbstract: As AI grows more powerful, it will increasingly shape how we understand the world. But with this influence comes the risk of amplifying misinformation and deepening social divides-especially on consequential topics where factual accuracy directly impacts well-being. Scalable Oversight aims to ensure AI systems remain truthful even when their capabilities exceed those of their evaluators. Yet when humans serve as evaluators, their own beliefs and biases can impair judgment. We study whether AI debate can guide biased judges toward the truth by having two AI systems debate opposing sides of controversial factuality claims on COVID-19 and climate change where people hold strong prior beliefs. We conduct two studies. Study I recruits human judges with either mainstream or skeptical beliefs who evaluate claims through two protocols: debate (interaction with two AI advisors arguing opposing sides) or consultancy (interaction with a single AI advisor). Study II uses AI judges with and without human-like personas to evaluate the same protocols. In Study I, debate consistently improves human judgment accuracy and confidence calibration, outperforming consultancy by 4-10% across COVID-19 and climate change claims. The improvement is most significant for judges with mainstream beliefs (up to +15.2% accuracy on COVID-19 claims), though debate also helps skeptical judges who initially misjudge claims move toward accurate views (+4.7% accuracy). In Study II, AI judges with human-like personas achieve even higher accuracy (78.5%) than human judges (70.1%) and default AI judges without personas (69.8%), suggesting their potential for supervising frontier AI models. These findings highlight AI debate as a promising path toward scalable, bias-resilient oversight in contested domains.</article>","contentLength":1849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Forcrat: Automatic I/O API Translation from C to Rust via Origin and Capability Analysis","url":"https://arxiv.org/abs/2506.01427","date":1761883200,"author":"","guid":322691,"unread":true,"content":"<article>arXiv:2506.01427v3 Announce Type: replace \nAbstract: Translating C to Rust is a promising way to enhance the reliability of legacy system programs. Although the industry has developed an automatic C-to-Rust translator, C2Rust, its translation remains unsatisfactory. One major reason is that C2Rust retains C standard library (libc) function calls instead of replacing them with functions from the Rust standard library (Rust std). However, little work has been done on replacing library functions in C2Rust-generated code. In this work, we focus on replacing the I/O API, an important subset of library functions. This poses challenges due to the semantically different designs of I/O APIs in libc and Rust std. First, the two APIs offer different sets of types that represent the origins (e.g., standard input, files) and capabilities (e.g., read, write) of streams used for I/O. Second, they use different error-checking mechanisms: libc uses internal indicators, while Rust std uses return values. To address these challenges, we propose two static analysis techniques, origin and capability analysis and error source analysis, and use their results to replace the I/O API. Our evaluation shows that the proposed approach is (1) correct, with all 32 programs that have test suites passing the tests after transformation, (2) efficient, analyzing and transforming 422k LOC in 14 seconds, and (3) widely applicable, replacing 82% of I/O API calls.</article>","contentLength":1449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incentivizing LLMs to Self-Verify Their Answers","url":"https://arxiv.org/abs/2506.01369","date":1761883200,"author":"","guid":322692,"unread":true,"content":"<article>arXiv:2506.01369v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find that only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance at inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Regression-Based Training of Normalizing Flows for Boltzmann Generators","url":"https://arxiv.org/abs/2506.01158","date":1761883200,"author":"","guid":322693,"unread":true,"content":"<article>arXiv:2506.01158v2 Announce Type: replace \nAbstract: Simulation-free training frameworks have been at the forefront of the generative modelling revolution in continuous spaces, leading to large-scale diffusion and flow matching models. However, such modern generative models suffer from expensive inference, inhibiting their use in numerous scientific applications like Boltzmann Generators (BGs) for molecular conformations that require fast likelihood evaluation. In this paper, we revisit classical normalizing flows in the context of BGs that offer efficient sampling and likelihoods, but whose training via maximum likelihood is often unstable and computationally challenging. We propose Regression Training of Normalizing Flows (RegFlow), a novel and scalable regression-based training objective that bypasses the numerical instability and computational challenge of conventional maximum likelihood training in favour of a simple $\\ell_2$-regression objective. Specifically, RegFlow maps prior samples under our flow to targets computed using optimal transport couplings or a pre-trained continuous normalizing flow (CNF). To enhance numerical stability, RegFlow employs effective regularization strategies such as a new forward-backward self-consistency loss that enjoys painless implementation. Empirically, we demonstrate that RegFlow unlocks a broader class of architectures that were previously intractable to train for BGs with maximum likelihood. We also show RegFlow exceeds the performance, computational cost, and stability of maximum likelihood training in equilibrium sampling in Cartesian coordinates of alanine dipeptide, tripeptide, and tetrapeptide, showcasing its potential in molecular systems.</article>","contentLength":1718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Convergence Analysis of An Alternating Nonlinear GMRES on Linear Systems","url":"https://arxiv.org/abs/2506.01081","date":1761883200,"author":"","guid":322694,"unread":true,"content":"<article>arXiv:2506.01081v2 Announce Type: replace \nAbstract: In this work, we develop an alternating nonlinear Generalized Minimum Residual (NGMRES) algorithm with depth $m$ and periodicity $p$, denoted by aNGMRES($m, p$), applied to linear systems. We provide a theoretical analysis to quantify by how much one-step NGMRES($m$) using Richardson iterations as initial guesses can improve the convergence speed of the underlying fixed-point iteration for diagonalizable and symmetric positive definite cases. Our theoretical analysis gives us a better understanding of which factors affect the convergence speed. Moreover, under certain conditions, we prove the periodic equivalence between the proposed aNGMRES applied to Richardson iteration and GMRES. Specifically, aNGMRES($\\infty,p$) and full GMRES are identical at the iteration index $jp$. Therefore, aNGMRES($\\infty,p$) can be regarded as an alternative to GMRES for solving linear systems. For finite $m$, the iterates of aNGMRES($m,m+1$) and restarted GMRES (GMRES($m+1$)) are the same at the end of each periodic interval of length $p$, i.e, at the iteration index $jp$. In Addition, we present a convergence analysis of aNGMRES when applied to accelerate Richardson iteration. The advantages of aNGMRES($m,p$) method are that there is no need to solve a least-squares problem at each iteration which can reduce the computational cost, and it can enhance the robustness against stagnations, which could occur for NGMRES($m$).</article>","contentLength":1477,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STATE-NAV: Stability-Aware Traversability Estimation for Bipedal Navigation on Rough Terrain","url":"https://arxiv.org/abs/2506.01046","date":1761883200,"author":"","guid":322695,"unread":true,"content":"<article>arXiv:2506.01046v3 Announce Type: replace \nAbstract: Bipedal robots have advantages in maneuvering human-centered environments, but face greater failure risk compared to other stable mobile plarforms such as wheeled or quadrupedal robots. While learning-based traversability has been widely studied for these platforms, bipedal traversability has instead relied on manually designed rules with limited consideration of locomotion stability on rough terrain. In this work, we present the first learning-based traversability estimation and risk-sensitive navigation framework for bipedal robots operating in diverse, uneven environments. TravFormer, a transformer-based neural network, is trained to predict bipedal instability with uncertainty, enabling risk-aware and adaptive planning. Based on the network, we define traversability as stability-aware command velocity-the fastest command velocity that keeps instability below a user-defined limit. This velocity-based traversability is integrated into a hierarchical planner that combines traversability-informed Rapid Random Tree Star (TravRRT*) for time-efficient planning and Model Predictive Control (MPC) for safe execution. We validate our method in MuJoCo simulation and the real world, demonstrating improved navigation performance, with enhanced robustness and time efficiency across varying terrains compared to existing methods.</article>","contentLength":1391,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Predicting Any Human Trajectory In Context","url":"https://arxiv.org/abs/2506.00871","date":1761883200,"author":"","guid":322696,"unread":true,"content":"<article>arXiv:2506.00871v2 Announce Type: replace \nAbstract: Predicting accurate future trajectories of pedestrians is essential for autonomous systems but remains a challenging task due to the need for adaptability in different environments and domains. A common approach involves collecting scenario-specific data and performing fine-tuning via backpropagation. However, the need to fine-tune for each new scenario is often impractical for deployment on edge devices. To address this challenge, we introduce \\paper, an In-Context Learning (ICL) framework for pedestrian trajectory prediction that enables adaptation without fine-tuning on the scenario-specific data at inference time without requiring weight updates. We propose a spatio-temporal similarity-based example selection (STES) method that selects relevant examples from previously observed trajectories within the same scene by identifying similar motion patterns at corresponding locations. To further refine this selection, we introduce prediction-guided example selection (PG-ES), which selects examples based on both the past trajectory and the predicted future trajectory, rather than relying solely on the past trajectory. This approach allows the model to account for long-term dynamics when selecting examples. Finally, instead of relying on small real-world datasets with limited scenario diversity, we train our model on a large-scale synthetic dataset to enhance its prediction ability by leveraging in-context examples. Extensive experiments demonstrate that TrajICL achieves remarkable adaptation across both in-domain and cross-domain scenarios, outperforming even fine-tuned approaches across multiple public benchmarks. Project Page: https://fujiry0.github.io/TrajICL-project-page/.</article>","contentLength":1754,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees","url":"https://arxiv.org/abs/2505.24627","date":1761883200,"author":"","guid":322697,"unread":true,"content":"<article>arXiv:2505.24627v3 Announce Type: replace \nAbstract: Recent neural combinatorial optimization (NCO) methods have shown promising problem-solving ability without requiring domain-specific expertise. Most existing NCO methods use training and testing data with a fixed constraint value and lack research on the effect of constraint tightness on the performance of NCO methods. This paper takes the capacity-constrained vehicle routing problem (CVRP) as an example to empirically analyze the NCO performance under different tightness degrees of the capacity constraint. Our analysis reveals that existing NCO methods overfit the capacity constraint, and they can only perform satisfactorily on a small range of the constraint values but poorly on other values. To tackle this drawback of existing NCO methods, we develop an efficient training scheme that explicitly considers varying degrees of constraint tightness and proposes a multi-expert module to learn a generally adaptable solving strategy. Experimental results show that the proposed method can effectively overcome the overfitting issue, demonstrating superior performances on the CVRP and CVRP with time windows (CVRPTW) with various constraint tightness degrees.</article>","contentLength":1222,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation","url":"https://arxiv.org/abs/2505.24518","date":1761883200,"author":"","guid":322698,"unread":true,"content":"<article>arXiv:2505.24518v2 Announce Type: replace \nAbstract: Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and, noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding.</article>","contentLength":1622,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2505.24388","date":1761883200,"author":"","guid":322699,"unread":true,"content":"<article>arXiv:2505.24388v2 Announce Type: replace \nAbstract: Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs) with external knowledge to improve factuality. However, existing RAG systems frequently underutilize the retrieved documents, failing to extract and integrate the key clues needed to support faithful and interpretable reasoning, especially in cases where relevant evidence is implicit, scattered, or obscured by noise. To address this issue, we propose ClueAnchor, a novel framework for enhancing RAG via clue-anchored reasoning exploration and optimization. ClueAnchor extracts key clues from retrieved content and generates multiple reasoning paths based on different knowledge configurations, optimizing the model by selecting the most appropriate reasoning path for the given context through reward-based preference optimization. Experiments show that ClueAnchor significantly outperforms prior RAG baselines in the completeness and robustness of reasoning. Further analysis confirms its strong resilience to noisy or partially relevant retrieved content, as well as its capability to identify supporting evidence even in the absence of explicit clue supervision during inference. All codes are available at https://github.com/thunlp/ClueAnchor.</article>","contentLength":1277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MemAscend: System Memory Optimization for SSD-Offloaded LLM Fine-Tuning","url":"https://arxiv.org/abs/2505.23254","date":1761883200,"author":"","guid":322700,"unread":true,"content":"<article>arXiv:2505.23254v3 Announce Type: replace \nAbstract: Owing to the huge success of generative artificial intelligence (AI), large language models (LLMs) have emerged as a core subclass, underpinning applications such as question answering, text generation, and code completion. While fine-tuning these models on domain-specific data can yield significant performance gains, it also poses daunting computational challenges, especially for researchers and small organizations with limited hardware resources. Although SSD offloading (i.e., ZeRO-Infinity) has emerged as a viable strategy to overcome the GPU memory barrier via leveraging both system memory (i.e., CPU DRAM) and storage space (i.e., solid-state devices, SSDs), its design primarily targets model-centric performance issues. As a result, key system-level issues, including system memory fragmentation, inefficient pinned buffer allocation, peak CPU usage spikes, and file system overhead, remain unaddressed, stifling scalability and inflating costs. Such an observation motivates this paper to introduce MemAscend, a framework that systematically tackles the underexplored system memory bottlenecks in SSD-offloaded LLM training, with a focus on resource-constrained environments. By streamlining pinned-memory allocation, eradicating fragmentation, and mitigating peak overhead, MemAscend reclaims a substantial system memory budget, enabling larger models, longer context windows, and higher batch sizes without exceeding modest hardware limits. Across diverse LLM benchmarks, MemAscend reduces peak system-memory consumption by an average of 55.7% compared with standard SSD offloading techniques, lowering the hardware barrier for fine-tuning and unlocking new possibilities for cost-effective large-scale training on limited-resource machines.</article>","contentLength":1811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering","url":"https://arxiv.org/abs/2505.23158","date":1761883200,"author":"","guid":322701,"unread":true,"content":"<article>arXiv:2505.23158v2 Announce Type: replace \nAbstract: In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements.</article>","contentLength":1011,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oryx: a Scalable Sequence Model for Many-Agent Coordination in Offline MARL","url":"https://arxiv.org/abs/2505.22151","date":1761883200,"author":"","guid":322702,"unread":true,"content":"<article>arXiv:2505.22151v2 Announce Type: replace \nAbstract: A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline autoregressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over long trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works -- SMAC, RWARE, and Multi-Agent MuJoCo -- covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings.</article>","contentLength":1246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning World Models for Interactive Video Generation","url":"https://arxiv.org/abs/2505.21996","date":1761883200,"author":"","guid":322703,"unread":true,"content":"<article>arXiv:2505.21996v2 Announce Type: replace \nAbstract: Foundational world models must be both interactive and preserve spatiotemporal coherence for effective future planning with action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. We enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatiotemporal consistency of world models. In contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Paper2Poster: Towards Multimodal Poster Automation from Scientific Papers","url":"https://arxiv.org/abs/2505.21497","date":1761883200,"author":"","guid":322704,"unread":true,"content":"<article>arXiv:2505.21497v2 Announce Type: replace \nAbstract: Academic poster generation is a crucial yet challenging task in scientific communication, requiring the compression of long-context interleaved documents into a single, visually coherent page. To address this challenge, we introduce the first benchmark and metric suite for poster generation, which pairs recent conference papers with author-designed posters and evaluates outputs on (i)Visual Quality-semantic alignment with human posters, (ii)Textual Coherence-language fluency, (iii)Holistic Assessment-six fine-grained aesthetic and informational criteria scored by a VLM-as-judge, and notably (iv)PaperQuiz-the poster's ability to convey core paper content as measured by VLMs answering generated quizzes. Building on this benchmark, we propose PosterAgent, a top-down, visual-in-the-loop multi-agent pipeline: the (a)Parser distills the paper into a structured asset library; the (b)Planner aligns text-visual pairs into a binary-tree layout that preserves reading order and spatial balance; and the (c)Painter-Commenter loop refines each panel by executing rendering code and using VLM feedback to eliminate overflow and ensure alignment. In our comprehensive evaluation, we find that GPT-4o outputs-though visually appealing at first glance-often exhibit noisy text and poor PaperQuiz scores, and we find that reader engagement is the primary aesthetic bottleneck, as human-designed posters rely largely on visual semantics to convey meaning. Our fully open-source variants (e.g. based on the Qwen-2.5 series) outperform existing 4o-driven multi-agent systems across nearly all metrics, while using 87% fewer tokens. It transforms a 22-page paper into a finalized yet editable .pptx poster - all for just $0.005. These findings chart clear directions for the next generation of fully automated poster-generation models. The code and datasets are available at https://github.com/Paper2Poster/Paper2Poster.</article>","contentLength":1965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IRCopilot: Automated Incident Response with Large Language Models","url":"https://arxiv.org/abs/2505.20945","date":1761883200,"author":"","guid":322705,"unread":true,"content":"<article>arXiv:2505.20945v3 Announce Type: replace \nAbstract: Incident response plays a pivotal role in mitigating the impact of cyber attacks. In recent years, the intensity and complexity of global cyber threats have grown significantly, making it increasingly challenging for traditional threat detection and incident response methods to operate effectively in complex network environments. While Large Language Models (LLMs) have shown great potential in early threat detection, their capabilities remain limited when it comes to automated incident response after an intrusion. To address this gap, we construct an incremental benchmark based on real-world incident response tasks to thoroughly evaluate the performance of LLMs in this domain. Our analysis reveals several key challenges that hinder the practical application of contemporary LLMs, including context loss, hallucinations, privacy protection concerns, and their limited ability to provide accurate, context-specific recommendations. In response to these challenges, we propose IRCopilot, a novel framework for automated incident response powered by LLMs. IRCopilot mimics the three dynamic phases of a real-world incident response team using four collaborative LLM-based session components. These components are designed with clear divisions of responsibility, reducing issues such as hallucinations and context loss. Our method leverages diverse prompt designs and strategic responsibility segmentation, significantly improving the system's practicality and efficiency. Experimental results demonstrate that IRCopilot outperforms baseline LLMs across key benchmarks, achieving sub-task completion rates of 150%, 138%, 136%, 119%, and 114% for various response tasks. Moreover, IRCopilot exhibits robust performance on public incident response platforms and in real-world attack scenarios, showcasing its strong applicability.</article>","contentLength":1886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Numerical Identification of a Time-Dependent Coefficient in a Time-Fractional Diffusion Equation with Integral Constraints","url":"https://arxiv.org/abs/2505.19738","date":1761883200,"author":"","guid":322706,"unread":true,"content":"<article>arXiv:2505.19738v2 Announce Type: replace \nAbstract: In this paper, we numerically address the inverse problem of identifying a time-dependent coefficient in the time-fractional diffusion equation. An a priori estimate is established to ensure uniqueness and stability of the solution. A fully implicit finite-difference scheme is proposed and rigorously analysed for stability and convergence. An efficient algorithm based on an integral formulation is implemented and verified through numerical experiments, demonstrating accuracy and robustness under noisy data.</article>","contentLength":565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations","url":"https://arxiv.org/abs/2505.18766","date":1761883200,"author":"","guid":322707,"unread":true,"content":"<article>arXiv:2505.18766v2 Announce Type: replace \nAbstract: Recently, text-to-image diffusion models have been widely used for style mimicry and personalized customization through methods such as DreamBooth and Textual Inversion. This has raised concerns about intellectual property protection and the generation of deceptive content. Recent studies, such as Glaze and Anti-DreamBooth, have proposed using adversarial noise to protect images from these attacks. However, recent purification-based methods, such as DiffPure and Noise Upscaling, have successfully attacked these latest defenses, showing the vulnerabilities of these methods. Moreover, present methods show limited transferability across models, making them less effective against unknown text-to-image models. To address these issues, we propose a novel anti-mimicry method, StyleGuard. We propose a novel style loss that optimizes the style-related features in the latent space to make it deviate from the original image, which improves model-agnostic transferability. Additionally, to enhance the perturbation's ability to bypass diffusion-based purification, we designed a novel upscale loss that involves ensemble purifiers and upscalers during training. Extensive experiments on the WikiArt and CelebA datasets demonstrate that StyleGuard outperforms existing methods in robustness against various transformations and purifications, effectively countering style mimicry in various models. Moreover, StyleGuard is effective on different style mimicry methods, including DreamBooth and Textual Inversion. The code is available at https://github.com/PolyLiYJ/StyleGuard.</article>","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations","url":"https://arxiv.org/abs/2505.18584","date":1761883200,"author":"","guid":322708,"unread":true,"content":"<article>arXiv:2505.18584v2 Announce Type: replace \nAbstract: Pre-trained stable diffusion models (SD) have shown great advances in visual correspondence. In this paper, we investigate the capabilities of Diffusion Transformers (DiTs) for accurate dense correspondence. Distinct from SD, DiTs exhibit a critical phenomenon in which very few feature activations exhibit significantly larger values than others, known as \\textit{massive activations}, leading to uninformative representations and significant performance degradation for DiTs. The massive activations consistently concentrate at very few fixed dimensions across all image patch tokens, holding little local information. We trace these dimension-concentrated massive activations and find that such concentration can be effectively localized by the zero-initialized Adaptive Layer Norm (AdaLN-zero). Building on these findings, we propose Diffusion Transformer Feature (DiTF), a training-free framework designed to extract semantic-discriminative features from DiTs. Specifically, DiTF employs AdaLN to adaptively localize and normalize massive activations with channel-wise modulation. In addition, we develop a channel discard strategy to further eliminate the negative impacts from massive activations. Experimental results demonstrate that our DiTF outperforms both DINO and SD-based models and establishes a new state-of-the-art performance for DiTs in different visual correspondence tasks (\\eg, with +9.4\\% on Spair-71k and +4.4\\% on AP-10K-C.S.).</article>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Embracing Contradiction: Theoretical Inconsistency Will Not Impede the Road of Building Responsible AI Systems","url":"https://arxiv.org/abs/2505.18139","date":1761883200,"author":"","guid":322709,"unread":true,"content":"<article>arXiv:2505.18139v3 Announce Type: replace \nAbstract: This position paper argues that the theoretical inconsistency often observed among Responsible AI (RAI) metrics, such as differing fairness definitions or tradeoffs between accuracy and privacy, should be embraced as a valuable feature rather than a flaw to be eliminated. We contend that navigating these inconsistencies, by treating metrics as divergent objectives, yields three key benefits: (1) Normative Pluralism: Maintaining a full suite of potentially contradictory metrics ensures that the diverse moral stances and stakeholder values inherent in RAI are adequately represented. (2) Epistemological Completeness: The use of multiple, sometimes conflicting, metrics allows for a more comprehensive capture of multifaceted ethical concepts, thereby preserving greater informational fidelity about these concepts than any single, simplified definition. (3) Implicit Regularization: Jointly optimizing for theoretically conflicting objectives discourages overfitting to one specific metric, steering models towards solutions with enhanced generalization and robustness under real-world complexities. In contrast, efforts to enforce theoretical consistency by simplifying or pruning metrics risk narrowing this value diversity, losing conceptual depth, and degrading model performance. We therefore advocate for a shift in RAI theory and practice: from getting trapped in inconsistency to characterizing acceptable inconsistency thresholds and elucidating the mechanisms that permit robust, approximated consistency in practice.</article>","contentLength":1585,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields","url":"https://arxiv.org/abs/2505.18125","date":1761883200,"author":"","guid":322710,"unread":true,"content":"<article>arXiv:2505.18125v2 Announce Type: replace \nAbstract: While deep learning has achieved remarkable success across many domains, it has historically underperformed on tabular learning tasks, which remain dominated by gradient boosting decision trees. However, recent advancements are paving the way for Tabular Foundation Models, which can leverage real-world knowledge and generalize across diverse datasets, particularly when the data contains free-text. Although incorporating language model capabilities into tabular tasks has been explored, most existing methods utilize static, target-agnostic textual representations, limiting their effectiveness. We introduce TabSTAR: a Tabular Foundation Model with Semantically Target-Aware Representations. TabSTAR is designed to enable transfer learning on tabular data with textual features, with an architecture free of dataset-specific parameters. It unfreezes a pretrained text encoder and takes as input target tokens, which provide the model with the context needed to learn task-specific embeddings. TabSTAR achieves state-of-the-art performance for both medium- and large-sized datasets across known benchmarks of classification tasks with text features, and its pretraining phase exhibits scaling laws in the number of datasets, offering a pathway for further performance improvements.</article>","contentLength":1337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models","url":"https://arxiv.org/abs/2505.17773","date":1761883200,"author":"","guid":322711,"unread":true,"content":"<article>arXiv:2505.17773v3 Announce Type: replace \nAbstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (C-LoRA) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments on LLaMA2-7B models demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes. Although our experiments are limited to 7B models, our method is architecture-agnostic and, in principle, applies beyond this scale; studying its scaling to larger models remains an open problem. Our code is available at https://github.com/ahra99/c_lora.</article>","contentLength":1611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3D Equivariant Visuomotor Policy Learning via Spherical Projection","url":"https://arxiv.org/abs/2505.16969","date":1761883200,"author":"","guid":322712,"unread":true,"content":"<article>arXiv:2505.16969v3 Announce Type: replace \nAbstract: Equivariant models have recently been shown to improve the data efficiency of diffusion policy by a significant margin. However, prior work that explored this direction focused primarily on point cloud inputs generated by multiple cameras fixed in the workspace. This type of point cloud input is not compatible with the now-common setting where the primary input modality is an eye-in-hand RGB camera like a GoPro. This paper closes this gap by incorporating into the diffusion policy model a process that projects features from the 2D RGB camera image onto a sphere. This enables us to reason about symmetries in $\\mathrm{SO}(3)$ without explicitly reconstructing a point cloud. We perform extensive experiments in both simulation and the real world that demonstrate that our method consistently outperforms strong baselines in terms of both performance and sample efficiency. Our work, Image-to-Sphere Policy ($\\textbf{ISP}$), is the first $\\mathrm{SO}(3)$-equivariant policy learning framework for robotic manipulation that works using only monocular RGB inputs.</article>","contentLength":1119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution","url":"https://arxiv.org/abs/2505.16239","date":1761883200,"author":"","guid":322713,"unread":true,"content":"<article>arXiv:2505.16239v2 Announce Type: replace \nAbstract: Diffusion models have demonstrated promising performance in real-world video super-resolution (VSR). However, the dozens of sampling steps they require, make inference extremely slow. Sampling acceleration techniques, particularly single-step, provide a potential solution. Nonetheless, achieving one step in VSR remains challenging, due to the high training overhead on video data and stringent fidelity demands. To tackle the above issues, we propose DOVE, an efficient one-step diffusion model for real-world VSR. DOVE is obtained by fine-tuning a pretrained video diffusion model (i.e., CogVideoX). To effectively train DOVE, we introduce the latent-pixel training strategy. The strategy employs a two-stage scheme to gradually adapt the model to the video super-resolution task. Meanwhile, we design a video processing pipeline to construct a high-quality dataset tailored for VSR, termed HQ-VSR. Fine-tuning on this dataset further enhances the restoration capability of DOVE. Extensive experiments show that DOVE exhibits comparable or superior performance to multi-step diffusion-based VSR methods. It also offers outstanding inference efficiency, achieving up to a 28$\\times$ speed-up over existing methods such as MGLD-VSR. Code is available at: https://github.com/zhengchen1999/DOVE.</article>","contentLength":1347,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the creation of narrow AI: hierarchy and nonlocality of neural network skills","url":"https://arxiv.org/abs/2505.15811","date":1761883200,"author":"","guid":322714,"unread":true,"content":"<article>arXiv:2505.15811v2 Announce Type: replace \nAbstract: We study the problem of creating strong, yet narrow, AI systems. While recent AI progress has been driven by the training of large general-purpose foundation models, the creation of smaller models specialized for narrow domains could be valuable for both efficiency and safety. In this work, we explore two challenges involved in creating such systems, having to do with basic properties of how neural networks learn and structure their representations. The first challenge regards when it is possible to train narrow models from scratch. Through experiments on a synthetic task, we find that it is sometimes necessary to train networks on a wide distribution of data to learn certain narrow skills within that distribution. This effect arises when skills depend on each other hierarchically, and training on a broad distribution introduces a curriculum which substantially accelerates learning. The second challenge regards how to transfer particular skills from large general models into small specialized models. We find that model skills are often not perfectly localized to a particular set of prunable components. However, we find that methods based on pruning can still outperform distillation. We investigate the use of a regularization objective to align desired skills with prunable components while unlearning unnecessary skills.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English","url":"https://arxiv.org/abs/2505.15095","date":1761883200,"author":"","guid":322715,"unread":true,"content":"<article>arXiv:2505.15095v2 Announce Type: replace \nAbstract: Sarcasm is a challenge to sentiment analysis because of the incongruity between stated and implied sentiment. The challenge is exacerbated when the implication may be relevant to a specific country or geographical region. Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that has been used for pragmatic reasoning. In this paper, we harness PMP for explainable sarcasm detection for Australian and Indian English, alongside a benchmark dataset for standard English. We manually add sarcasm explanations to an existing sarcasm-labeled dataset for Australian and Indian English called BESSTIE, and compare the performance for explainable sarcasm detection for them with FLUTE, a standard English dataset containing sarcasm explanations. Our approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA) achieves statistically significant performance improvement across all tasks and datasets when compared with four alternative prompting strategies. We also find that alternative techniques such as agentic prompting mitigate context-related failures by enabling external knowledge retrieval. The focused contribution of our work is utilising PMP in generating sarcasm explanations for varieties of English.</article>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Evolving Curriculum for LLM Reasoning","url":"https://arxiv.org/abs/2505.14970","date":1761883200,"author":"","guid":322716,"unread":true,"content":"<article>arXiv:2505.14970v4 Announce Type: replace \nAbstract: Reinforcement learning (RL) has proven effective for fine-tuning large language models (LLMs), significantly enhancing their reasoning abilities in domains such as mathematics and code generation. A crucial factor influencing RL fine-tuning success is the training curriculum: the order in which training problems are presented. While random curricula serve as common baselines, they remain suboptimal; manually designed curricula often rely heavily on heuristics, and online filtering methods can be computationally prohibitive. To address these limitations, we propose Self-Evolving Curriculum (SEC), an automatic curriculum learning method that learns a curriculum policy concurrently with the RL fine-tuning process. Our approach formulates curriculum selection as a non-stationary Multi-Armed Bandit problem, treating each problem category (e.g., difficulty level or problem type) as an individual arm. We leverage the absolute advantage from policy gradient methods as a proxy measure for immediate learning gain. At each training step, the curriculum policy selects categories to maximize this reward signal and is updated using the TD(0) method. Across three distinct reasoning domains: planning, inductive reasoning, and mathematics, our experiments demonstrate that SEC significantly improves models' reasoning capabilities, enabling better generalization to harder, out-of-distribution test problems. Additionally, our approach achieves better skill balance when fine-tuning simultaneously on multiple reasoning domains. These findings highlight SEC as a promising strategy for RL fine-tuning of LLMs.</article>","contentLength":1665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Let LRMs Break Free from Overthinking via Self-Braking Tuning","url":"https://arxiv.org/abs/2505.14604","date":1761883200,"author":"","guid":322717,"unread":true,"content":"<article>arXiv:2505.14604v4 Announce Type: replace \nAbstract: Large reasoning models (LRMs), such as OpenAI o1 and DeepSeek-R1, have significantly enhanced their reasoning capabilities by generating longer chains of thought, demonstrating outstanding performance across a variety of tasks. However, this performance gain comes at the cost of a substantial increase in redundant reasoning during the generation process, leading to high computational overhead and exacerbating the issue of overthinking. Although numerous existing approaches aim to address the problem of overthinking, they often rely on external interventions. In this paper, we propose a novel framework, Self-Braking Tuning (SBT), which tackles overthinking from the perspective of allowing the model to regulate its own reasoning process, thus eliminating the reliance on external control mechanisms. We construct a set of overthinking identification metrics based on standard answers and design a systematic method to detect redundant reasoning. This method accurately identifies unnecessary steps within the reasoning trajectory and generates training signals for learning self-regulation behaviors. Building on this foundation, we develop a complete strategy for constructing data with adaptive reasoning lengths and introduce an innovative braking prompt mechanism that enables the model to naturally learn when to terminate reasoning at an appropriate point. Experiments across mathematical benchmarks (AIME, AMC, MATH500, GSM8K) demonstrate that our method reduces token consumption by up to 60% while maintaining comparable accuracy to unconstrained models.</article>","contentLength":1624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Insert for Constructive Neural Vehicle Routing Solver","url":"https://arxiv.org/abs/2505.13904","date":1761883200,"author":"","guid":322718,"unread":true,"content":"<article>arXiv:2505.13904v3 Announce Type: replace \nAbstract: Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes.</article>","contentLength":1370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiple Proposer Transaction Fee Mechanism Design: Robust Incentives Against Censorship and Bribery","url":"https://arxiv.org/abs/2505.13751","date":1761883200,"author":"","guid":322719,"unread":true,"content":"<article>arXiv:2505.13751v2 Announce Type: replace \nAbstract: Censorship resistance is one of the core value proposition of blockchains. A recurring design pattern aimed at providing censorship resistance is enabling multiple proposers to contribute inputs into block construction. Notably, Fork-Choice Enforced Inclusion Lists (FOCIL) is proposed to be included in Ethereum. However, the current proposal relies on altruistic behavior, without a Transaction Fee Mechanism (TFM). This study aims to address this gap by exploring how multiple proposers should be rewarded to incentivize censorship resistance. The main contribution of this work is the identification of TFMs that ensure censorship resistance under bribery attacks, while also satisfying the incentive compatibility properties of EIP-1559. We provide a concrete payment mechanism for FOCIL, along with generalizable contributions to the literature by analyzing 1) incentive compatibility of TFMs in the presence of a bribing adversary, 2) TFMs in protocols with multiple phases of transaction inclusion, and 3) TFMs of protocols in which parties are uncertain about the behavior and the possible bribe of others.</article>","contentLength":1168,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models","url":"https://arxiv.org/abs/2505.13444","date":1761883200,"author":"","guid":322720,"unread":true,"content":"<article>arXiv:2505.13444v2 Announce Type: replace \nAbstract: Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.</article>","contentLength":1576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space","url":"https://arxiv.org/abs/2505.13308","date":1761883200,"author":"","guid":322721,"unread":true,"content":"<article>arXiv:2505.13308v2 Announce Type: replace \nAbstract: Reasoning ability, a core component of human intelligence, continues to pose a significant challenge for Large Language Models (LLMs) in the pursuit of AGI. Although model performance has improved under the training scaling law, significant challenges remain, particularly with respect to training algorithms, such as catastrophic forgetting, and the limited availability of novel training data. As an alternative, test-time scaling enhances reasoning performance by increasing test-time computation without parameter updating. Unlike prior methods in this paradigm focused on token space, we propose leveraging latent space for more effective reasoning and better adherence to the test-time scaling law. We introduce LatentSeek, a novel framework that enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA) within the model's latent space. Specifically, LatentSeek leverages policy gradient to iteratively update latent representations, guided by self-generated reward signals. LatentSeek is evaluated on a range of reasoning benchmarks, including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures. Results show that LatentSeek consistently outperforms strong baselines, such as Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our analysis demonstrates that LatentSeek is highly efficient, typically converging within a few iterations for problems of average complexity, while also benefiting from additional iterations, thereby highlighting the potential of test-time scaling in the latent space. These findings position LatentSeek as a lightweight, scalable, and effective solution for enhancing the reasoning capabilities of LLMs.</article>","contentLength":1744,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neurosymbolic Diffusion Models","url":"https://arxiv.org/abs/2505.13138","date":1761883200,"author":"","guid":322722,"unread":true,"content":"<article>arXiv:2505.13138v2 Announce Type: replace \nAbstract: Neurosymbolic (NeSy) predictors combine neural perception with symbolic reasoning to solve tasks like visual reasoning. However, standard NeSy predictors assume conditional independence between the symbols they extract, thus limiting their ability to model interactions and uncertainty - often leading to overconfident predictions and poor out-of-distribution generalisation. To overcome the limitations of the independence assumption, we introduce neurosymbolic diffusion models (NeSyDMs), a new class of NeSy predictors that use discrete diffusion to model dependencies between symbols. Our approach reuses the independence assumption from NeSy predictors at each step of the diffusion process, enabling scalable learning while capturing symbol dependencies and uncertainty quantification. Across both synthetic and real-world benchmarks - including high-dimensional visual path planning and rule-based autonomous driving - NeSyDMs achieve state-of-the-art accuracy among NeSy predictors and demonstrate strong calibration.</article>","contentLength":1078,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks","url":"https://arxiv.org/abs/2505.12371","date":1761883200,"author":"","guid":322723,"unread":true,"content":"<article>arXiv:2505.12371v2 Announce Type: replace \nAbstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest in multi-agent collaboration for addressing complex medical tasks. However, the practical advantages of multi-agent collaboration approaches remain insufficiently understood. Existing evaluations often lack generalizability, failing to cover diverse tasks reflective of real-world clinical practice, and frequently omit rigorous comparisons against both single-LLM-based and established conventional methods. To address this critical gap, we introduce MedAgentBoard, a comprehensive benchmark for the systematic evaluation of multi-agent collaboration, single-LLM, and conventional approaches. MedAgentBoard encompasses four diverse medical task categories: (1) medical (visual) question answering, (2) lay summary generation, (3) structured Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow automation, across text, medical images, and structured EHR data. Our extensive experiments reveal a nuanced landscape: while multi-agent collaboration demonstrates benefits in specific scenarios, such as enhancing task completeness in clinical workflow automation, it does not consistently outperform advanced single LLMs (e.g., in textual medical QA) or, critically, specialized conventional methods that generally maintain better performance in tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital resource and actionable insights, emphasizing the necessity of a task-specific, evidence-based approach to selecting and developing AI solutions in medicine. It underscores that the inherent complexity and overhead of multi-agent collaboration must be carefully weighed against tangible performance gains. All code, datasets, detailed prompts, and experimental results are open-sourced at https://medagentboard.netlify.app/.</article>","contentLength":1901,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Curriculum Abductive Learning","url":"https://arxiv.org/abs/2505.12275","date":1761883200,"author":"","guid":322724,"unread":true,"content":"<article>arXiv:2505.12275v2 Announce Type: replace \nAbstract: Abductive Learning (ABL) integrates machine learning with logical reasoning in a loop: a learning model predicts symbolic concept labels from raw inputs, which are revised through abduction using domain knowledge and then fed back for retraining. However, due to the nondeterminism of abduction, the training process often suffers from instability, especially when the knowledge base is large and complex, resulting in a prohibitively large abduction space. While prior works focus on improving candidate selection within this space, they typically treat the knowledge base as a static black box. In this work, we propose Curriculum Abductive Learning (C-ABL), a method that explicitly leverages the internal structure of the knowledge base to address the ABL training challenges. C-ABL partitions the knowledge base into a sequence of sub-bases, progressively introduced during training. This reduces the abduction space throughout training and enables the model to incorporate logic in a stepwise, smooth way. Experiments across multiple tasks show that C-ABL outperforms previous ABL implementations, significantly improves training stability, convergence speed, and final accuracy, especially under complex knowledge setting.</article>","contentLength":1282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum","url":"https://arxiv.org/abs/2505.12191","date":1761883200,"author":"","guid":322725,"unread":true,"content":"<article>arXiv:2505.12191v2 Announce Type: replace \nAbstract: Self-Supervised Learning (SSL) has become a powerful solution to extract rich representations from unlabeled data. Yet, SSL research is mostly focused on clean, curated and high-quality datasets. As a result, applying SSL on noisy data remains a challenge, despite being crucial to applications such as astrophysics, medical imaging, geophysics or finance. In this work, we present a fully self-supervised framework that enables noise-robust representation learning without requiring a denoiser at inference or downstream fine-tuning. Our method first trains an SSL denoiser on noisy data, then uses it to construct a denoised-to-noisy data curriculum (i.e., training first on denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2), combined with a teacher-guided regularization that anchors noisy embeddings to their denoised counterparts. This process encourages the model to internalize noise robustness. Notably, the denoiser can be discarded after pretraining, simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise ($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by 4.8% over DINOv2, demonstrating that denoiser-free robustness can emerge from noise-aware pretraining. The code is available at https://github.com/wenquanlu/noisy_dinov2.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling","url":"https://arxiv.org/abs/2505.11730","date":1761883200,"author":"","guid":322726,"unread":true,"content":"<article>arXiv:2505.11730v2 Announce Type: replace \nAbstract: Test-time scaling (TTS) has proven effective in enhancing the reasoning capabilities of large language models (LLMs). Verification plays a key role in TTS, simultaneously influencing (1) reasoning performance and (2) compute efficiency, due to the quality and computational cost of verification. In this work, we challenge the conventional paradigms of verification, and make the first attempt toward systematically investigating the impact of verification granularity-that is, how frequently the verifier is invoked during generation, beyond verifying only the final output or individual generation steps. To this end, we introduce Variable Granularity Search (VG-Search), a unified algorithm that generalizes beam search and Best-of-N sampling via a tunable granularity parameter g. Extensive experiments with VG-Search under varying compute budgets, generator-verifier configurations, and task attributes reveal that dynamically selecting g can improve the compute efficiency and scaling behavior. Building on these findings, we propose adaptive VG-Search strategies that achieve accuracy gains of up to 3.1\\% over Beam Search and 3.6\\% over Best-of-N, while reducing FLOPs by over 52\\%. We will open-source the code to support future research.</article>","contentLength":1300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cybersecurity threat detection based on a UEBA framework using Deep Autoencoders","url":"https://arxiv.org/abs/2505.11542","date":1761883200,"author":"","guid":322727,"unread":true,"content":"<article>arXiv:2505.11542v2 Announce Type: replace \nAbstract: User and Entity Behaviour Analytics (UEBA) is a broad branch of data analytics that attempts to build a normal behavioural profile in order to detect anomalous events. Among the techniques used to detect anomalies, Deep Autoencoders constitute one of the most promising deep learning models on UEBA tasks, allowing explainable detection of security incidents that could lead to the leak of personal data, hijacking of systems, or access to sensitive business information. In this study, we introduce the first implementation of an explainable UEBA-based anomaly detection framework that leverages Deep Autoencoders in combination with Doc2Vec to process both numerical and textual features. Additionally, based on the theoretical foundations of neural networks, we offer a novel proof demonstrating the equivalence of two widely used definitions for fully-connected neural networks. The experimental results demonstrate the proposed framework capability to detect real and synthetic anomalies effectively generated from real attack data, showing that the models provide not only correct identification of anomalies but also explainable results that enable the reconstruction of the possible origin of the anomaly. Our findings suggest that the proposed UEBA framework can be seamlessly integrated into enterprise environments, complementing existing security systems for explainable threat detection.</article>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Grokking a Computational Glass Relaxation?","url":"https://arxiv.org/abs/2505.11411","date":1761883200,"author":"","guid":322728,"unread":true,"content":"<article>arXiv:2505.11411v2 Announce Type: replace \nAbstract: Understanding neural network's (NN) generalizability remains a central question in deep learning research. The special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches a near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability. Here we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. This mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. Our experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition. We identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. Inspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. This provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design.</article>","contentLength":1742,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference","url":"https://arxiv.org/abs/2505.11329","date":1761883200,"author":"","guid":322729,"unread":true,"content":"<article>arXiv:2505.11329v4 Announce Type: replace \nAbstract: Distributed inference of large language models (LLMs) can introduce overheads of up to 20% even over GPUs connected via high-speed interconnects such as NVLink. Multiple techniques have been proposed to mitigate these overheads by decomposing computations into finer-grained tasks and overlapping communication with sub-tasks as they complete. However, fine-grained decomposition of a large computation into many smaller computations on GPUs results in overheads. Furthermore, the communication itself uses many streaming multiprocessors (SMs), adding to the overhead.\n  We present TokenWeave to address these challenges. TokenWeave proposes a Token-Splitting technique that divides the tokens in the inference batch into two approximately equal subsets in a wave-aware manner. The communication of one subset is then overlapped with the computation of the other. In addition, TokenWeave optimizes the order of the layer normalization computation with respect to communication operations and implements a novel fused AllReduce--RMSNorm kernel that carefully leverages Multimem instruction support available on Hopper and Blackwell NVIDIA GPUs. These optimizations allow TokenWeave to perform communication and RMSNorm using only 2-8 SMs. Moreover, our kernel enables the memory-bound RMSNorm to be overlapped with the other batch's computation, providing additional gains.\n  Our evaluations demonstrate up to 1.29x speedup in latency and 1.26x higher throughput across multiple models and workloads. In several settings, TokenWeave results in better performance compared to an equivalent model with all communication removed.</article>","contentLength":1678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs","url":"https://arxiv.org/abs/2505.10603","date":1761883200,"author":"","guid":322730,"unread":true,"content":"<article>arXiv:2505.10603v2 Announce Type: replace \nAbstract: Generative artificial intelligence (Gen AI) systems represent a critical technology with far-reaching implications across multiple domains of society. However, their deployment entails a range of risks and challenges that require careful evaluation. To date, there has been a lack of comprehensive, interdisciplinary studies offering a systematic comparison between open-source and proprietary (closed) generative AI systems, particularly regarding their respective advantages and drawbacks. This study aims to: i) critically evaluate and compare the characteristics, opportunities, and challenges of open and closed generative AI models; and ii) propose foundational elements for the development of an Open, Public, and Safe Gen AI framework. As a methodology, we adopted a combined approach that integrates three methods: literature review, critical analysis, and comparative analysis. The proposed framework outlines key dimensions, openness, public governance, and security, as essential pillars for shaping the future of trustworthy and inclusive Gen AI. Our findings reveal that open models offer greater transparency, auditability, and flexibility, enabling independent scrutiny and bias mitigation. In contrast, closed systems often provide better technical support and ease of implementation, but at the cost of unequal access, accountability, and ethical oversight. The research also highlights the importance of multi-stakeholder governance, environmental sustainability, and regulatory frameworks in ensuring responsible development.</article>","contentLength":1598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Plasticity as the Mirror of Empowerment","url":"https://arxiv.org/abs/2505.10361","date":1761883200,"author":"","guid":322731,"unread":true,"content":"<article>arXiv:2505.10361v2 Announce Type: replace \nAbstract: Agents are minimally entities that are influenced by their past observations and act to influence future observations. This latter capacity is captured by empowerment, which has served as a vital framing concept across artificial intelligence and cognitive science. This former capacity, however, is equally foundational: In what ways, and to what extent, can an agent be influenced by what it observes? In this paper, we ground this concept in a universal agent-centric measure that we refer to as plasticity, and reveal a fundamental connection to empowerment. Following a set of desiderata on a suitable definition, we define plasticity using a new information-theoretic quantity we call the generalized directed information. We show that this new quantity strictly generalizes the directed information introduced by Massey (1990) while preserving all of its desirable properties. Under this definition, we find that plasticity is well thought of as the mirror of empowerment: The two concepts are defined using the same measure, with only the direction of influence reversed. Our main result establishes a tension between the plasticity and empowerment of an agent, suggesting that agent design needs to be mindful of both characteristics. We explore the implications of these findings, and suggest that plasticity, empowerment, and their relationship are essential to understanding agency</article>","contentLength":1446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity","url":"https://arxiv.org/abs/2505.09922","date":1761883200,"author":"","guid":322732,"unread":true,"content":"<article>arXiv:2505.09922v2 Announce Type: replace \nAbstract: Euclidean diffusion models have achieved remarkable success in generative modeling across diverse domains, and they have been extended to manifold cases in recent advances. Instead of explicitly utilizing the structure of special manifolds as studied in previous works, in this paper we investigate direct sampling of the Euclidean diffusion models for general manifold-structured data. We reveal the multiscale singularity of the score function in the ambient space, which hinders the accuracy of diffusion-generated samples. We then present an elaborate theoretical analysis of the singularity structure of the score function by decomposing it along the tangential and normal directions of the manifold. To mitigate the singularity and improve the sampling accuracy, we propose two novel methods: (1) Niso-DM, which reduces the scale discrepancies in the score function by utilizing a non-isotropic noise, and (2) Tango-DM, which trains only the tangential component of the score function using a tangential-only loss function. Numerical experiments demonstrate that our methods achieve superior performance on distributions over various manifolds with complex geometries.</article>","contentLength":1227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Robust and Non-Iterative Tensor Decomposition Method with Automatic Thresholding","url":"https://arxiv.org/abs/2505.06203","date":1761883200,"author":"","guid":322733,"unread":true,"content":"<article>arXiv:2505.06203v2 Announce Type: replace \nAbstract: Recent advances in IoT and biometric sensing technologies have led to the generation of massive and high-dimensional tensor data, yet achieving accurate and efficient low-rank approximation remains a major challenge. Existing tensor decomposition methods typically require prior specification of the tensor rank and rely on iterative optimization, which often results in heavy computational costs and dependence on the analyst's expertise. In this study, we propose a novel low-rank approximation method for tensor data that requires neither prior rank specification nor iterative optimization. The proposed method performs statistical singular value hard thresholding on the mode-wise unfolded matrices to automatically extract only statistically significant components, thereby achieving noise reduction while preserving the intrinsic tensor structure. Theoretically, the optimal threshold for each mode is derived based on the asymptotic properties of the Mar\\v{c}enko--Pastur distribution. Simulation experiments demonstrate that the proposed method outperforms conventional approaches such as Higher-Order Singular Value Decomposition, Higher-Order Orthogonal Iteration, and Tucker-L2E in terms of both estimation accuracy and computational efficiency. These results indicate that our method provides an effective and theoretically grounded framework for automatic, non-iterative, and analyst-independent tensor decomposition.</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AnomalyMatch: Discovering Rare Objects of Interest with Semi-supervised and Active Learning","url":"https://arxiv.org/abs/2505.03509","date":1761883200,"author":"","guid":322734,"unread":true,"content":"<article>arXiv:2505.03509v2 Announce Type: replace \nAbstract: Anomaly detection in large datasets is essential in astronomy and computer vision. However, due to a scarcity of labelled data, it is often infeasible to apply supervised methods to anomaly detection. We present AnomalyMatch, an anomaly detection framework combining the semi-supervised FixMatch algorithm using EfficientNet classifiers with active learning. AnomalyMatch is tailored for large-scale applications and integrated into the ESA Datalabs science platform. In this method, we treat anomaly detection as a binary classification problem and efficiently utilise limited labelled and abundant unlabelled images for training. We enable active learning via a user interface for verification of high-confidence anomalies and correction of false positives. Evaluations on the GalaxyMNIST astronomical dataset and the miniImageNet natural-image benchmark under severe class imbalance display strong performance. Starting from five to ten labelled anomalies, we achieve an average AUROC of 0.96 (miniImageNet) and 0.89 (GalaxyMNIST), with respective AUPRC of 0.82 and 0.77. After three active learning cycles, anomalies are ranked with 76% (miniImageNet) to 94% (GalaxyMNIST) precision in the top 1% of the highest-ranking images by score. We compare to the established Astronomaly software on selected 'odd' galaxies from the 'Galaxy Zoo - The Galaxy Challenge' dataset, achieving comparable performance with an average AUROC of 0.83. Our results underscore the exceptional utility and scalability of this approach for anomaly discovery, highlighting the value of specialised approaches for domains characterised by severe label scarcity.</article>","contentLength":1693,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AutoLibra: Agent Metric Induction from Open-Ended Human Feedback","url":"https://arxiv.org/abs/2505.02820","date":1761883200,"author":"","guid":322735,"unread":true,"content":"<article>arXiv:2505.02820v3 Announce Type: replace \nAbstract: Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose **AutoLibra**, a framework for agent evaluation, that transforms open-ended human feedback *e.g.* \"If you find that the button is disabled, don't click it again\", or \"This agent has too much autonomy to decide what to do on its own\" into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta metrics to evaluate the alignment of a set of (induced) metrics with open feedback: \"coverage\" and \"redundancy\". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra serve human prompt engineers for diagonalize agent failures and improve prompts iterative. Moreover, we find that AutoLibra can induce metrics for automatic optimization for agents, which makes agents improve through self-regulation. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.</article>","contentLength":1647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empowering Agentic Video Analytics Systems with Video Language Models","url":"https://arxiv.org/abs/2505.00254","date":1761883200,"author":"","guid":322736,"unread":true,"content":"<article>arXiv:2505.00254v4 Announce Type: replace \nAbstract: AI-driven video analytics has become increasingly important across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Vision Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively-significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%. The source code of AVA is available at https://github.com/I-ESC/Project-Ava. The AVA-100 benchmark can be accessed at https://huggingface.co/datasets/iesc/Ava-100.</article>","contentLength":1791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Causes and Canonicalization for Unreproducible Builds in Java","url":"https://arxiv.org/abs/2504.21679","date":1761883200,"author":"","guid":322737,"unread":true,"content":"<article>arXiv:2504.21679v3 Announce Type: replace \nAbstract: The increasing complexity of software supply chains and the rise of supply chain attacks have elevated concerns around software integrity. Users and stakeholders face significant challenges in validating that a given software artifact corresponds to its declared source. Reproducible Builds address this challenge by ensuring that independently performed builds from identical source code produce identical binaries. However, achieving reproducibility at scale remains difficult, especially in Java, due to a range of non-deterministic factors and caveats in the build process. In this work, we focus on reproducibility in Java-based software, archetypal of enterprise applications. We introduce a conceptual framework for reproducible builds, we analyze a large dataset from Reproducible Central, and we develop a novel taxonomy of six root causes of unreproducibility. We study actionable mitigations: artifact and bytecode canonicalization using OSS-Rebuild and jNorm respectively. Finally, we present Chains-Rebuild, a tool that achieve successfulcanonicalization for 26.60% on 12,803 unreproducible artifacts To sum up, our contributions are the first large-scale taxonomy of build unreproducibility causes in Java, a publicly available dataset of unreproducible builds, and Chains-Rebuild, a canonicalization tool for mitigating unreproducible builds in Java.</article>","contentLength":1418,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advancing Local Clustering on Graphs via Compressive Sensing: Semi-supervised and Unsupervised Methods","url":"https://arxiv.org/abs/2504.19419","date":1761883200,"author":"","guid":322738,"unread":true,"content":"<article>arXiv:2504.19419v2 Announce Type: replace \nAbstract: Local clustering aims to identify specific substructures within a large graph without any additional structural information of the graph. These substructures are typically small compared to the overall graph, enabling the problem to be approached by finding a sparse solution to a linear system associated with the graph Laplacian. In this work, we first propose a method for identifying specific local clusters when very few labeled data are given, which we term semi-supervised local clustering. We then extend this approach to the unsupervised setting when no prior information on labels is available. The proposed methods involve randomly sampling the graph, applying diffusion through local cluster extraction, then examining the overlap among the results to find each cluster. We establish the co-membership conditions for any pair of nodes, and rigorously prove the correctness of our methods. Additionally, we conduct extensive experiments to demonstrate that the proposed methods achieve state of the art results in the low-label rates regime.</article>","contentLength":1105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating Argon2 Adoption and Effectiveness in Real-World Software","url":"https://arxiv.org/abs/2504.17121","date":1761883200,"author":"","guid":322739,"unread":true,"content":"<article>arXiv:2504.17121v2 Announce Type: replace \nAbstract: Modern password hashing remains a critical defense against credential cracking, yet the transition from theoretically secure algorithms to robust real-world implementations remains fraught with challenges. This paper presents a dual analysis of Argon2, the Password Hashing Competition winner, combining attack simulations quantifying how parameter configurations impact guessing costs under realistic budgets, with the first large-scale empirical study of Argon2 adoption across public GitHub software repositories. Our economic model, validated against cryptocurrency mining benchmarks, demonstrates that OWASP's recommended 46 MiB configuration reduces compromise rates by 42.5% compared to SHA-256 at \\$1/account attack budgets for strong user passwords. However, memory-hardness exhibits diminishing returns as increasing allocations to RFC 9106's 2048 MiB provides just 23.3% (\\$1) and 17.7% (\\$20) additional protection despite 44.5 times greater memory demands. Crucially, both configurations fail to mitigate risks from weak passwords, with 96.9-99.8% compromise rates for RockYou-like credentials regardless of algorithm choice. Our repository analysis shows accelerating Argon2 adoption, yet weak configuration practices: 46.6% of deployments use weaker-than-OWASP parameters. Surprisingly, sensitive applications (password managers, encryption tools) show no stronger configurations than general software. Our findings highlight that a secure algorithm alone cannot ensure security, effective parameter guidance and developer education remain essential for realizing Argon2's theoretical advantages.</article>","contentLength":1664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kernel-learning parameter prediction and evaluation in algebraic multigrid method for several PDEs","url":"https://arxiv.org/abs/2504.14930","date":1761883200,"author":"","guid":322740,"unread":true,"content":"<article>arXiv:2504.14930v2 Announce Type: replace \nAbstract: This paper explores the application of kernel learning methods for parameter prediction and evaluation in the Algebraic Multigrid Method (AMG), focusing on several Partial Differential Equation (PDE) problems. AMG is an efficient iterative solver for large-scale sparse linear systems, particularly those derived from elliptic and parabolic PDE discretizations. However, its performance heavily relies on numerous parameters, which are often set empirically and are highly sensitive to AMG's effectiveness. Traditional parameter optimization methods are either computationally expensive or lack theoretical support. To address this, we propose a Gaussian Process Regression (GPR)-based strategy to optimize AMG parameters and introduce evaluation metrics to assess their effectiveness. Trained on small-scale datasets, GPR predicts nearly optimal parameters, bypassing the time-consuming parameter sweeping process. We also use kernel learning techniques to build a kernel function library and determine the optimal kernel function through linear combination, enhancing prediction accuracy. In numerical experiments, we tested typical PDEs such as the constant-coefficient Poisson equation, variable-coefficient Poisson equation, diffusion equation, and Helmholtz equation. Results show that GPR-predicted parameters match grid search results in iteration counts while significantly reducing computational time. A comprehensive analysis using metrics like mean squared error, prediction interval coverage, and Bayesian information criterion confirms GPR's efficiency and reliability. These findings validate GPR's effectiveness in AMG parameter optimization and provide theoretical support for AMG's practical application.</article>","contentLength":1775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis","url":"https://arxiv.org/abs/2504.11331","date":1761883200,"author":"","guid":322741,"unread":true,"content":"<article>arXiv:2504.11331v2 Announce Type: replace \nAbstract: Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract fine-grained information from image-text pairs to identify aspect terms and determine their sentiment polarity. However, existing approaches often fall short in simultaneously addressing three core challenges: Sentiment Cue Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise Elimination (SNE). To overcome these limitations, we propose DASCO (\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework), a fine-grained scope-oriented framework that enhances aspect-level sentiment reasoning by leveraging dependency parsing trees. First, we designed a multi-task pretraining strategy for MABSA on our base model, combining aspect-oriented enhancement, image-text matching, and aspect-level sentiment-sensitive cognition. This improved the model's perception of aspect terms and sentiment cues while achieving effective image-text alignment, addressing key challenges like SCP and MIM. Furthermore, we incorporate dependency trees as syntactic branch combining with semantic branch, guiding the model to selectively attend to critical contextual elements within a target-specific scope while effectively filtering out irrelevant noise for addressing SNE problem. Extensive experiments on two benchmark datasets across three subtasks demonstrate that DASCO achieves state-of-the-art performance in MABSA, with notable gains in JMASA (+2.3\\% F1 and +3.5\\% precision on Twitter2015). The source code is available at https://github.com/LHaoooo/DASCO .</article>","contentLength":1613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Privacy-Preserving Distributed Link Predictions Among Peers in Online Classrooms Using Federated Learning","url":"https://arxiv.org/abs/2504.10456","date":1761883200,"author":"","guid":322742,"unread":true,"content":"<article>arXiv:2504.10456v2 Announce Type: replace \nAbstract: Social interactions among classroom peers, represented as social learning networks (SLNs), play a crucial role in enhancing learning outcomes. While SLN analysis has recently garnered attention, most existing approaches rely on centralized training, where data is aggregated and processed on a local/cloud server with direct access to raw data. However, in real-world educational settings, such direct access across multiple classrooms is often restricted due to privacy concerns. Furthermore, training models on isolated classroom data prevents the identification of common interaction patterns that exist across multiple classrooms, thereby limiting model performance. To address these challenges, we propose one of the first frameworks that integrates Federated Learning (FL), a distributed and collaborative machine learning (ML) paradigm, with SLNs derived from students' interactions in multiple classrooms' online forums to predict future link formations (i.e., interactions) among students. By leveraging FL, our approach enables collaborative model training across multiple classrooms while preserving data privacy, as it eliminates the need for raw data centralization. Recognizing that each classroom may exhibit unique student interaction dynamics, we further employ model personalization techniques to adapt the FL model to individual classroom characteristics. Our results demonstrate the effectiveness of our approach in capturing both shared and classroom-specific representations of student interactions in SLNs. Additionally, we utilize explainable AI (XAI) techniques to interpret model predictions, identifying key factors that influence link formation across different classrooms. These insights unveil the drivers of social learning interactions within a privacy-preserving, collaborative, and distributed ML framework -- an aspect that has not been explored before.</article>","contentLength":1941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SD-ReID: View-aware Stable Diffusion for Aerial-Ground Person Re-Identification","url":"https://arxiv.org/abs/2504.09549","date":1761883200,"author":"","guid":322743,"unread":true,"content":"<article>arXiv:2504.09549v2 Announce Type: replace \nAbstract: Aerial-Ground Person Re-IDentification (AG-ReID) aims to retrieve specific persons across cameras with different viewpoints. Previous works focus on designing discriminative models to maintain the identity consistency despite drastic changes in camera viewpoints. The core idea behind these methods is quite natural, but designing a view-robust model is a very challenging task. Moreover, they overlook the contribution of view-specific features in enhancing the model's ability to represent persons. To address these issues, we propose a novel generative framework named SD-ReID for AG-ReID, which leverages generative models to mimic the feature distribution of different views while extracting robust identity representations. More specifically, we first train a ViT-based model to extract person representations along with controllable conditions, including identity and view conditions. We then fine-tune the Stable Diffusion (SD) model to enhance person representations guided by these controllable conditions. Furthermore, we introduce the View-Refined Decoder (VRD) to bridge the gap between instance-level and global-level features. Finally, both person representations and all-view features are employed to retrieve target persons. Extensive experiments on five AG-ReID benchmarks (i.e., CARGO, AG-ReIDv1, AG-ReIDv2, LAGPeR and G2APS-ReID) demonstrate the effectiveness of our proposed method. The source code will be available.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Smart Exploration in Reinforcement Learning using Bounded Uncertainty Models","url":"https://arxiv.org/abs/2504.05978","date":1761883200,"author":"","guid":322744,"unread":true,"content":"<article>arXiv:2504.05978v2 Announce Type: replace \nAbstract: Reinforcement learning (RL) is a powerful framework for decision-making in uncertain environments, but it often requires large amounts of data to learn an optimal policy. We address this challenge by incorporating prior model knowledge to guide exploration and accelerate the learning process. Specifically, we assume access to a model set that contains the true transition kernel and reward function. We optimize over this model set to obtain upper and lower bounds on the Q-function, which are then used to guide the exploration of the agent. We provide theoretical guarantees on the convergence of the Q-function to the optimal Q-function under the proposed class of exploring policies. Furthermore, we also introduce a data-driven regularized version of the model set optimization problem that ensures the convergence of the class of exploring policies to the optimal policy. Lastly, we show that when the model set has a specific structure, namely the bounded-parameter MDP (BMDP) framework, the regularized model set optimization problem becomes convex and simple to implement. In this setting, we also prove finite-time convergence to the optimal policy under mild assumptions. We demonstrate the effectiveness of the proposed exploration strategy, which we call BUMEX (Bounded Uncertainty Model-based Exploration), in a simulation study. The results indicate that the proposed method can significantly accelerate learning in benchmark examples. A toolbox is available at https://github.com/JvHulst/BUMEX.</article>","contentLength":1565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SEA-LION: Southeast Asian Languages in One Network","url":"https://arxiv.org/abs/2504.05747","date":1761883200,"author":"","guid":322745,"unread":true,"content":"<article>arXiv:2504.05747v4 Announce Type: replace \nAbstract: Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.</article>","contentLength":1114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"M-Prometheus: A Suite of Open Multilingual LLM Judges","url":"https://arxiv.org/abs/2504.04953","date":1761883200,"author":"","guid":322746,"unread":true,"content":"<article>arXiv:2504.04953v2 Announce Type: replace \nAbstract: The use of language models for automatically evaluating long-form text (LLM-as-a-judge) is becoming increasingly common, yet most LLM judges are optimized exclusively for English, with strategies for enhancing their multilingual evaluation capabilities remaining largely unexplored in the current literature. This has created a disparity in the quality of automatic evaluation methods for non-English languages, ultimately hindering the development of models with better multilingual capabilities. To bridge this gap, we introduce M-Prometheus, a suite of open-weight LLM judges ranging from 3B to 14B parameters that can provide both direct assessment and pairwise comparison feedback on multilingual outputs. M-Prometheus models outperform state-of-the-art open LLM judges on multilingual reward benchmarks spanning more than 20 languages, as well as on literary machine translation (MT) evaluation covering 4 language pairs. Furthermore, M-Prometheus models can be leveraged at decoding time to significantly improve generated outputs across all 3 tested languages, showcasing their utility for the development of better multilingual models. Lastly, through extensive ablations, we identify the key factors for obtaining an effective multilingual judge, including backbone model selection and training on synthetic multilingual feedback data instead of translated data. We release our models, training dataset, and code.</article>","contentLength":1476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explainable post-training bias mitigation with distribution-based fairness metrics","url":"https://arxiv.org/abs/2504.01223","date":1761883200,"author":"","guid":322747,"unread":true,"content":"<article>arXiv:2504.01223v3 Announce Type: replace \nAbstract: We develop a novel bias mitigation framework with distribution-based fairness constraints suitable for producing demographically blind and explainable machine-learning models across a wide range of fairness levels. This is accomplished through post-processing, allowing fairer models to be generated efficiently without retraining the underlying model. Our framework, which is based on stochastic gradient descent, can be applied to a wide range of model types, with a particular emphasis on the post-processing of gradient-boosted decision trees. Additionally, we design a broad family of global fairness metrics, along with differentiable and consistent estimators compatible with our framework, building on previous work. We empirically test our methodology on a variety of datasets and compare it with alternative post-processing approaches, including Bayesian search, optimal transport projection, and direct neural network training.</article>","contentLength":991,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero-shot Benchmarking: A Framework for Flexible and Scalable Automatic Evaluation of Language Models","url":"https://arxiv.org/abs/2504.01001","date":1761883200,"author":"","guid":322748,"unread":true,"content":"<article>arXiv:2504.01001v2 Announce Type: replace \nAbstract: As language models improve and become capable of performing more complex tasks across modalities, evaluating them automatically becomes increasingly challenging. Developing strong and robust task-specific automatic metrics gets harder, and human-annotated test sets -- which are expensive to create -- saturate more quickly. A compelling alternative is to design reliable strategies to automate the creation of test data and evaluation, but previous attempts either rely on pre-existing data, or focus solely on individual tasks. We present Zero-shot Benchmarking (ZSB), a framework for creating high-quality benchmarks for any task by leveraging language models for both synthetic test data creation and evaluation. ZSB is simple and flexible: it requires only the creation of a prompt for data generation and one for evaluation; it is scalable to tasks and languages where collecting real-world data is costly or impractical; it is model-agnostic, allowing the creation of increasingly challenging benchmarks as models improve. To assess the effectiveness of our framework, we create benchmarks for five text-only tasks and a multi-modal one: general capabilities in four languages (English, Chinese, French, and Korean), translation, and general vision-language capabilities in English. We then rank a broad range of open and closed systems on our benchmarks. ZSB rankings consistently correlate strongly with human rankings, outperforming widely-adopted standard benchmarks. Through ablations, we find that strong benchmarks can be created with open models, and that judge model size and dataset variety are crucial drivers of performance. We release all our benchmarks, and code to reproduce our experiments and to produce new benchmarks.</article>","contentLength":1796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground Person Re-Identification","url":"https://arxiv.org/abs/2503.23722","date":1761883200,"author":"","guid":322749,"unread":true,"content":"<article>arXiv:2503.23722v3 Announce Type: replace \nAbstract: As an important task in intelligent transportation systems, Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specific persons across heterogeneous cameras in different viewpoints. Previous methods typically adopt deep learning-based models, focusing on extracting view-invariant features. However, they usually overlook the semantic information in person attributes. In addition, existing training strategies often rely on full fine-tuning large-scale models, which significantly increases training costs. To address these issues, we propose a novel framework named LATex for AG-ReID, which adopts prompt-tuning strategies to leverage attribute-based text knowledge. Specifically, with the Contrastive Language-Image Pre-training (CLIP) model, we first propose an Attribute-aware Image Encoder (AIE) to extract both global semantic features and attribute-aware features from input images. Then, with these features, we propose a Prompted Attribute Classifier Group (PACG) to predict person attributes and obtain attribute representations. Finally, we design a Coupled Prompt Template (CPT) to transform attribute representations and view information into structured sentences. These sentences are processed by the text encoder of CLIP to generate more discriminative features. As a result, our framework can fully leverage attribute-based text knowledge to improve AG-ReID performance. Extensive experiments on three AG-ReID benchmarks demonstrate the effectiveness of our proposed methods. The source code is available at https://github.com/kevinhu314/LATex.</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disentangled 4D Gaussian Splatting: Rendering High-Resolution Dynamic World at 343 FPS","url":"https://arxiv.org/abs/2503.22159","date":1761883200,"author":"","guid":322750,"unread":true,"content":"<article>arXiv:2503.22159v3 Announce Type: replace \nAbstract: While dynamic novel view synthesis from 2D videos has seen progress, achieving efficient reconstruction and rendering of dynamic scenes remains a challenging task. In this paper, we introduce Disentangled 4D Gaussian Splatting (Disentangled4DGS), a novel representation and rendering pipeline that achieves real-time performance without compromising visual fidelity. Disentangled4DGS decouples the temporal and spatial components of 4D Gaussians, avoiding the need for slicing first and four-dimensional matrix calculations in prior methods. By projecting temporal and spatial deformations into dynamic 2D Gaussians and deferring temporal processing, we minimize redundant computations of 4DGS. Our approach also features a gradient-guided flow loss and temporal splitting strategy to reduce artifacts. Experiments demonstrate a significant improvement in rendering speed and quality, achieving 343 FPS when render 1352*1014 resolution images on a single RTX3090 while reducing storage requirements by at least 4.5%. Our approach sets a new benchmark for dynamic novel view synthesis, outperforming existing methods on both multi-view and monocular dynamic scene datasets.</article>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Task-Centric Perspective on Recommendation Systems","url":"https://arxiv.org/abs/2503.21188","date":1761883200,"author":"","guid":322751,"unread":true,"content":"<article>arXiv:2503.21188v3 Announce Type: replace \nAbstract: Many studies in recommender systems (RecSys) adopt a general problem definition, i.e., to recommend preferred items to users based on past interactions. Such abstraction often lacks the domain-specific nuances necessary for practical deployment. However, models are frequently evaluated using datasets collected from online recommender platforms, which inherently reflect domain or task specificities. In this paper, we analyze RecSys task formulations, emphasizing key components such as input-output structures, temporal dynamics, and candidate item selection. All these factors directly impact offline evaluation. We further examine the complexities of user-item interactions, including decision-making costs, multi-step engagements, and unobservable interactions, which may influence model design. Additionally, we explore the balance between task specificity and model generalizability, highlighting how well-defined task formulations serve as the foundation for robust evaluation and effective solution development. By clarifying task definitions and their implications, this work provides a structured perspective on RecSys research. The goal is to help researchers better navigate the field, particularly in understanding specificities of the RecSys tasks and ensuring fair and meaningful evaluations.</article>","contentLength":1362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Guided Model Merging for Hybrid Data Learning: Leveraging Centralized Data to Refine Decentralized Models","url":"https://arxiv.org/abs/2503.20138","date":1761883200,"author":"","guid":322752,"unread":true,"content":"<article>arXiv:2503.20138v2 Announce Type: replace \nAbstract: Current network training paradigms primarily focus on either centralized or decentralized data regimes. However, in practice, data availability often exhibits a hybrid nature, where both regimes coexist. This hybrid setting presents new opportunities for model training, as the two regimes offer complementary trade-offs: decentralized data is abundant but subject to heterogeneity and communication constraints, while centralized data, though limited in volume and potentially unrepresentative, enables better curation and high-throughput access. Despite its potential, effectively combining these paradigms remains challenging, and few frameworks are tailored to hybrid data regimes. To address this, we propose a novel framework that constructs a model atlas from decentralized models and leverages centralized data to refine a global model within this structured space. The refined model is then used to reinitialize the decentralized models. Our method synergizes federated learning (to exploit decentralized data) and model merging (to utilize centralized data), enabling effective training under hybrid data availability. Theoretically, we show that our approach achieves faster convergence than methods relying solely on decentralized data, due to variance reduction in the merging process. Extensive experiments demonstrate that our framework consistently outperforms purely centralized, purely decentralized, and existing hybrid-adaptable methods. Notably, our method remains robust even when the centralized and decentralized data domains differ or when decentralized data contains noise, significantly broadening its applicability.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Loop Closure from Two Views: Revisiting PGO for Scalable Trajectory Estimation through Monocular Priors","url":"https://arxiv.org/abs/2503.16275","date":1761883200,"author":"","guid":322753,"unread":true,"content":"<article>arXiv:2503.16275v2 Announce Type: replace \nAbstract: (Visual) Simultaneous Localization and Mapping (SLAM) remains a fundamental challenge in enabling autonomous systems to navigate and understand large-scale environments. Traditional SLAM approaches struggle to balance efficiency and accuracy, particularly in large-scale settings where extensive computational resources are required for scene reconstruction and Bundle Adjustment (BA). However, this scene reconstruction, in the form of sparse pointclouds of visual landmarks, is often only used within the SLAM system because navigation and planning methods require different map representations. In this work, we therefore investigate a more scalable Visual SLAM (VSLAM) approach without reconstruction, mainly based on approaches for two-view loop closures. By restricting the map to a sparse keyframed pose graph without dense geometry representations, our `2GO' system achieves efficient optimization with competitive absolute trajectory accuracy. In particular, we find that recent advancements in image matching and monocular depth priors enable very accurate trajectory optimization without BA. We conduct extensive experiments on diverse datasets, including large-scale scenarios, and provide a detailed analysis of the trade-offs between runtime, accuracy, and map size. Our results demonstrate that this streamlined approach supports real-time performance, scales well in map size and trajectory duration, and effectively broadens the capabilities of VSLAM for long-duration deployments to large environments.</article>","contentLength":1573,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advancing Mobile GUI Agents: A Verifier-Driven Approach to Practical Deployment","url":"https://arxiv.org/abs/2503.15937","date":1761883200,"author":"","guid":322754,"unread":true,"content":"<article>arXiv:2503.15937v4 Announce Type: replace \nAbstract: We propose V-Droid, a mobile GUI task automation agent. Unlike previous mobile agents that utilize Large Language Models (LLMs) as generators to directly generate actions at each step, V-Droid employs LLMs as verifiers to evaluate candidate actions before making final decisions. To realize this novel paradigm, we introduce a comprehensive framework for constructing verifier-driven mobile agents: the discretized action space construction coupled with the prefilling-only workflow to accelerate the verification process, the pair-wise progress preference training to significantly enhance the verifier's decision-making capabilities, and the scalable human-agent joint annotation scheme to efficiently collect the necessary data at scale. V-Droid obtains a substantial task success rate across several public mobile task automation benchmarks: 59.5% on AndroidWorld, 38.3% on AndroidLab, and 49% on MobileAgentBench, surpassing existing agents by 5.2%, 2.1%, and 9%, respectively. Furthermore, V-Droid achieves a remarkably low latency of 4.3s per step, which is 6.1x faster compared with existing mobile agents. The source code is available at https://github.com/V-Droid-Agent/V-Droid.</article>","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Constrained Saddle Search Approach for Constructing Singular and Flexible Bar Frameworks","url":"https://arxiv.org/abs/2503.14807","date":1761883200,"author":"","guid":322755,"unread":true,"content":"<article>arXiv:2503.14807v2 Announce Type: replace \nAbstract: Singularity analysis is essential in robot kinematics, as singular configurations cause loss of control and kinematic indeterminacy. This paper models singularities in bar frameworks as saddle points on constrained manifolds. Given an under-constrained, non-singular bar framework, by allowing one edge to vary its length while fixing lengths of others, we define the squared length of the free edge as an energy functional and show that its local saddle points correspond to singular and flexible frameworks. Using our constrained saddle search approach, we identify previously unknown singular and flexible bar frameworks, providing new insights into singular robotics design and analysis.</article>","contentLength":744,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language-guided Open-world Video Anomaly Detection under Weak Supervision","url":"https://arxiv.org/abs/2503.13160","date":1761883200,"author":"","guid":322756,"unread":true,"content":"<article>arXiv:2503.13160v2 Announce Type: replace \nAbstract: Video anomaly detection (VAD) aims to detect anomalies that deviate from what is expected. In open-world scenarios, the expected events may change as requirements change. For example, not wearing a mask may be considered abnormal during a flu outbreak but normal otherwise. However, existing methods assume that the definition of anomalies is invariable, and thus are not applicable to the open world. To address this, we propose a novel open-world VAD paradigm with variable definitions, allowing guided detection through user-provided natural language at inference time. This paradigm necessitates establishing a robust mapping from video and textual definition to anomaly scores. Therefore, we propose LaGoVAD (Language-guided Open-world Video Anomaly Detector), a model that dynamically adapts anomaly definitions under weak supervision with two regularization strategies: diversifying the relative durations of anomalies via dynamic video synthesis, and enhancing feature robustness through contrastive learning with negative mining. Training such adaptable models requires diverse anomaly definitions, but existing datasets typically provide labels without semantic descriptions. To bridge this gap, we collect PreVAD (Pre-training Video Anomaly Dataset), the largest and most diverse video anomaly dataset to date, featuring 35,279 annotated videos with multi-level category labels and descriptions that explicitly define anomalies. Zero-shot experiments on seven datasets demonstrate LaGoVAD's SOTA performance. Our dataset and code will be released at https://github.com/Kamino666/LaGoVAD-PreVAD.</article>","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Experiments with Optimal Model Trees","url":"https://arxiv.org/abs/2503.12902","date":1761883200,"author":"","guid":322757,"unread":true,"content":"<article>arXiv:2503.12902v2 Announce Type: replace \nAbstract: Model trees provide an appealing way to perform interpretable machine learning for both classification and regression problems. In contrast to ``classic'' decision trees with constant values in their leaves, model trees can use linear combinations of predictor variables in their leaf nodes to form predictions, which can help achieve higher accuracy and smaller trees. Typical algorithms for learning model trees from training data work in a greedy fashion, growing the tree in a top-down manner by recursively splitting the data into smaller and smaller subsets. Crucially, the selected splits are only locally optimal, potentially rendering the tree overly complex and less accurate than a tree whose structure is globally optimal for the training data. In this paper, we empirically investigate the effect of constructing globally optimal model trees for classification and regression with linear support vector machines at the leaf nodes. To this end, we present mixed-integer linear programming formulations to learn optimal trees, compute such trees for a large collection of benchmark data sets, and compare their performance against greedily grown model trees in terms of interpretability and accuracy. We also compare to classic optimal and greedily grown decision trees, random forests, and support vector machines. Our results show that optimal model trees can achieve competitive accuracy with very small trees. We also investigate the effect on the accuracy of replacing axis-parallel splits with multivariate ones, foregoing interpretability while potentially obtaining greater accuracy.</article>","contentLength":1655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?","url":"https://arxiv.org/abs/2503.09499","date":1761883200,"author":"","guid":322758,"unread":true,"content":"<article>arXiv:2503.09499v3 Announce Type: replace \nAbstract: Large foundation models face challenges in acquiring transferable, structured thinking abilities, especially when supervised with rigid templates or crowd-annotated instruction datasets. Unlike prior approaches, we focus on a thinking-centric data synthesis paradigm that enables models to evolve through self-generated, cognitively guided data. We propose MindGYM, a structured and scalable framework for question synthesis, composed of: (1) Cognitive Thinking Process Injection, which infuses high-level reasoning objectives to shape the model's synthesis behavior; (2) Seed Single-Hop Question Synthesis, generating atomic questions from diverse semantic types to encourage broader thinking; and (3) Challenging Multi-Hop QA Synthesis, composing more complex multi-hop questions based on QA seeds for deeper reasoning. Detailed analysis shows that synthetic data generated by our method achieves 16.7% higher average quality and 67.91% lower quality variance compared to baseline sources, highlighting that both high-quality and self-contained data are essential for effective, thinking-oriented fine-tuning. MindGYM improves performance on six reasoning benchmarks, achieving gains of up to 16% on MathVision using only 400 data samples, and generalizable improvements across different model sizes and architectures. MindGYM underscores the viability of self-challenging mechanisms in refining large model capabilities while minimizing human intervention and resource demands. Code and data are released to promote data-centric research into self-evolving foundation models driven by their internal reasoning capabilities.</article>","contentLength":1679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quality Over Quantity? LLM-Based Curation for a Data-Efficient Audio-Video Foundation Model","url":"https://arxiv.org/abs/2503.09205","date":1761883200,"author":"","guid":322759,"unread":true,"content":"<article>arXiv:2503.09205v3 Announce Type: replace \nAbstract: Integrating audio and visual data for training multimodal foundational models remains a challenge. The Audio-Video Vector Alignment (AVVA) framework addresses this by considering AV scene alignment beyond mere temporal synchronization, and leveraging Large Language Models (LLMs) for data curation. AVVA implements a scoring mechanism for selecting aligned training data segments. It integrates Whisper, a speech-based foundation model, for audio and DINOv2 for video analysis in a dual-encoder structure with contrastive learning on AV pairs. Evaluations on AudioCaps, VALOR, and VGGSound demonstrate the effectiveness of the proposed model architecture and data curation approach. AVVA achieves a significant improvement in top-k accuracies for video-to-audio retrieval on all datasets compared to DenseAV, while using only 192 hrs of curated training data. Furthermore, an ablation study indicates that the data curation process effectively trades data quality for data quantity, yielding increases in top-k retrieval accuracies on AudioCaps, VALOR, and VGGSound, compared to training on the full spectrum of uncurated data.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data","url":"https://arxiv.org/abs/2503.04852","date":1761883200,"author":"","guid":322760,"unread":true,"content":"<article>arXiv:2503.04852v3 Announce Type: replace \nAbstract: True intelligence hinges on the ability to uncover and leverage hidden causal relations. Despite significant progress in AI and computer vision (CV), there remains a lack of benchmarks for assessing models' abilities to infer latent causality from complex visual data. In this paper, we introduce \\textsc{\\textbf{Causal3D}}, a novel and comprehensive benchmark that integrates structured data (tables) with corresponding visual representations (images) to evaluate causal reasoning. Designed within a systematic framework, Causal3D comprises 19 3D-scene datasets capturing diverse causal relations, views, and backgrounds, enabling evaluations across scenes of varying complexity. We assess multiple state-of-the-art methods, including classical causal discovery, causal representation learning, and large/vision-language models (LLMs/VLMs). Our experiments show that as causal structures grow more complex without prior knowledge, performance declines significantly, highlighting the challenges even advanced methods face in complex causal scenarios. Causal3D serves as a vital resource for advancing causal reasoning in CV and fostering trustworthy AI in critical domains.</article>","contentLength":1227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving LLM Safety Alignment with Dual-Objective Optimization","url":"https://arxiv.org/abs/2503.03710","date":1761883200,"author":"","guid":322761,"unread":true,"content":"<article>arXiv:2503.03710v3 Announce Type: replace \nAbstract: Existing training-time safety alignment techniques for large language models (LLMs) remain vulnerable to jailbreak attacks. Direct preference optimization (DPO), a widely deployed alignment method, exhibits limitations in both experimental and theoretical contexts as its loss function proves suboptimal for refusal learning. Through gradient-based analysis, we identify these shortcomings and propose an improved safety alignment that disentangles DPO objectives into two components: (1) robust refusal training, which encourages refusal even when partial unsafe generations are produced, and (2) targeted unlearning of harmful knowledge. This approach significantly increases LLM robustness against a wide range of jailbreak attacks, including prefilling, suffix, and multi-turn attacks across both in-distribution and out-of-distribution scenarios. Furthermore, we introduce a method to emphasize critical refusal tokens by incorporating a reward-based token-level weighting mechanism for refusal learning, which further improves the robustness against adversarial exploits. Our research also suggests that robustness to jailbreak attacks is correlated with token distribution shifts in the training process and internal representations of refusal and harmful tokens, offering valuable directions for future research in LLM safety alignment. The code is available at https://github.com/wicai24/DOOR-Alignment</article>","contentLength":1464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Models can Self-Improve at State-Value Estimation for Better Search","url":"https://arxiv.org/abs/2503.02878","date":1761883200,"author":"","guid":322762,"unread":true,"content":"<article>arXiv:2503.02878v3 Announce Type: replace \nAbstract: Collecting ground-truth rewards or human demonstrations for multi-step reasoning tasks is often prohibitively expensive, particularly in interactive domains such as web tasks. We introduce Self-Taught Lookahead (STL), a reward-free framework that improves language model-based value functions by reasoning explicitly about state transitions. STL can be viewed as a chain-of-thought analogue of the value iteration algorithm: instead of regressing directly on numeric values, a value LLM is trained to simulate a step of lookahead in natural language - predicting the next action, resulting state, and rationale for its value, thereby refining value estimates without any labeled data. This self-supervised procedure yields more accurate state-value predictions, which in turn enable lightweight search algorithms to expand fewer states while maintaining strong performance. Empirically, STL-trained value models built on moderately sized (8B parameter) open-weight LLMs boost web agent success rates by 39%, achieving comparable performance with proprietary models. STL also generalizes to multi-hop QA and math puzzles. We find that STL enables small open-source models to guide efficient search, reducing inference costs by integrating explicit reasoning with value learning.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective","url":"https://arxiv.org/abs/2503.02631","date":1761883200,"author":"","guid":322763,"unread":true,"content":"<article>arXiv:2503.02631v2 Announce Type: replace \nAbstract: Human-AI collaborative tools attract attentions from the data storytelling community to lower the expertise barrier and streamline the workflow. The recent advance in large-scale generative AI techniques, e.g., large language models (LLMs) and text-to-image models, has the potential to enhance data storytelling with their power in visual and narration generation. After two years since these techniques were publicly available, it is important to reflect our progress of applying them and have an outlook for future opportunities. To achieve the goal, we compare the collaboration patterns of the latest tools with those of earlier ones using a dedicated framework for understanding human-AI collaboration in data storytelling. Through comparison, we identify consistently widely studied patterns, e.g., human-creator + AI-assistant, and newly explored or emerging ones, e.g., AI-creator + human-reviewer. The benefits of these AI techniques and implications to human-AI collaboration are also revealed. We further propose future directions to hopefully ignite innovations.</article>","contentLength":1128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SIMS: Surgeon-Intention-driven Motion Scaling for Efficient and Precise Teleoperation","url":"https://arxiv.org/abs/2503.01216","date":1761883200,"author":"","guid":322764,"unread":true,"content":"<article>arXiv:2503.01216v2 Announce Type: replace \nAbstract: Telerobotic surgery often relies on a fixed motion scaling factor (MSF) to map the surgeon's hand motions to robotic instruments, but this introduces a trade-off between precision and efficiency: small MSF enables delicate manipulation but slows large movements, while large MSF accelerates transfer at the cost of accuracy. We propose a Surgeon-Intention driven Motion Scaling (SIMS) system, which dynamically adjusts MSF in real time based solely on kinematic cues. SIMS extracts linear speed, tool motion alignment, and dual-arm coordination features to classify motion intent via fuzzy C-means clustering and applies confidence-based updates independently for both arms. In a user study (n=10, three surgical training tasks) conducted on the da Vinci Research Kit, SIMS significantly reduced collisions (mean reduction of 83%), lowered mental and physical workload, and maintained task completion efficiency compared to fixed MSF. These findings demonstrate that SIMS is a practical and lightweight approach for safer, more efficient, and user-adaptive telesurgical control.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3D Dynamic Fluid Assets from Single-View Videos with Generative Gaussian Splatting","url":"https://arxiv.org/abs/2503.00868","date":1761883200,"author":"","guid":322765,"unread":true,"content":"<article>arXiv:2503.00868v2 Announce Type: replace \nAbstract: While the generation of 3D content from single-view images has been extensively studied, the creation of physically consistent 3D dynamic scenes from videos remains in its early stages. We propose a novel framework leveraging generative 3D Gaussian Splatting (3DGS) models to extract and re-simulate 3D dynamic fluid objects from single-view videos using simulation methods. The fluid geometry represented by 3DGS is initially generated and optimized from single-view images, then denoised, densified, and aligned across frames. We estimate the fluid surface velocity using optical flow, propose a mainstream extraction algorithm to refine it. The 3D volumetric velocity field is then derived from the velocity of the fluid's enclosed surface. The velocity field is therewith converted into a divergence-free, grid-based representation, enabling the optimization of simulation parameters through its differentiability across frames. This process outputs simulation-ready fluid assets with physical dynamics closely matching those observed in the source video. Our approach is applicable to various liquid fluids, including inviscid and viscous types, and allows users to edit the output geometry or extend movement durations seamlessly. This automatic method for creating 3D dynamic fluid assets from single-view videos, easily obtainable from the internet, shows great potential for generating large-scale 3D fluid assets at a low cost.</article>","contentLength":1490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"More of the Same: Persistent Representational Harms Under Increased Representation","url":"https://arxiv.org/abs/2503.00333","date":1761883200,"author":"","guid":322766,"unread":true,"content":"<article>arXiv:2503.00333v2 Announce Type: replace \nAbstract: To recognize and mitigate the harms of generative AI systems, it is crucial to consider who is represented in the outputs of generative AI systems and how people are represented. A critical gap emerges when naively improving who is represented, as this does not imply bias mitigation efforts have been applied to address how people are represented. We critically examined this by investigating gender representation in occupation across state-of-the-art large language models. We first show evidence suggesting that over time there have been interventions to models altering the resulting gender distribution, and we find that women are more represented than men when models are prompted to generate biographies or personas. We then demonstrate that representational biases persist in how different genders are represented by examining statistically significant word differences across genders. This results in a proliferation of representational harms, stereotypes, and neoliberalism ideals that, despite existing interventions to increase female representation, reinforce existing systems of oppression.</article>","contentLength":1158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Learning Approach to Games","url":"https://arxiv.org/abs/2503.00227","date":1761883200,"author":"","guid":322767,"unread":true,"content":"<article>arXiv:2503.00227v3 Announce Type: replace \nAbstract: This work introduces a unified framework for analyzing games in greater depth. In the existing literature, players' strategies are typically assigned scalar values, and equilibrium concepts are used to identify compatible choices. However, this approach neglects the internal structure of players, thereby failing to accurately model observed behaviors.\n  To address this limitation, we propose an abstract definition of a player, consistent with constructions in reinforcement learning. Instead of defining games as external settings, our framework defines them in terms of the players themselves. This offers a language that enables a deeper connection between games and learning. To illustrate the need for this generality, we study a simple two-player game and show that even in basic settings, a sophisticated player may adopt dynamic strategies that cannot be captured by simpler models or compatibility analysis.\n  For a general definition of a player, we discuss natural conditions on its components and define competition through their behavior. In the discrete setting, we consider players whose estimates largely follow the standard framework from the literature. We explore connections to correlated equilibrium and highlight that dynamic programming naturally applies to all estimates. In the mean-field setting, we exploit symmetry to construct explicit examples of equilibria. Finally, we conclude by examining relations to reinforcement learning.</article>","contentLength":1515,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems","url":"https://arxiv.org/abs/2502.20995","date":1761883200,"author":"","guid":322768,"unread":true,"content":"<article>arXiv:2502.20995v3 Announce Type: replace \nAbstract: With the growing adoption of retrieval-augmented generation (RAG) systems, various attack methods have been proposed to degrade their performance. However, most existing approaches rely on unrealistic assumptions in which external attackers have access to internal components such as the retriever. To address this issue, we introduce a realistic black-box attack based on the RAG paradox, a structural vulnerability arising from the system's effort to enhance trust by revealing both the retrieved documents and their sources to users. This transparency enables attackers to observe which sources are used and how information is phrased, allowing them to craft poisoned documents that are more likely to be retrieved and upload them to the identified sources. Moreover, as RAG systems directly provide retrieved content to users, these documents must not only be retrievable but also appear natural and credible to maintain user confidence in the search results. Unlike prior work that focuses solely on improving document retrievability, our attack method explicitly considers both retrievability and user trust in the retrieved content. Both offline and online experiments demonstrate that our method significantly degrades system performance without internal access, while generating natural-looking poisoned documents.</article>","contentLength":1376,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decoding for Punctured Convolutional and Turbo Codes: A Deep Learning Solution for Protocols Compliance","url":"https://arxiv.org/abs/2502.15475","date":1761883200,"author":"","guid":322769,"unread":true,"content":"<article>arXiv:2502.15475v3 Announce Type: replace \nAbstract: Neural network-based decoding methods show promise in enhancing error correction performance but face challenges with punctured codes. In particular, existing methods struggle to adapt to variable code rates or meet protocol compatibility requirements. This paper proposes a unified long short-term memory (LSTM)-based neural decoder for punctured convolutional and Turbo codes to address these challenges. The key component of the proposed LSTM-based neural decoder is puncturing-aware embedding, which integrates puncturing patterns directly into the neural network to enable seamless adaptation to different code rates. Moreover, a balanced bit error rate training strategy is designed to ensure the decoder's robustness across various code lengths, rates, and channels. In this way, the protocol compatibility requirement can be realized. Extensive simulations in both additive white Gaussian noise (AWGN) and Rayleigh fading channels demonstrate that the proposed neural decoder outperforms conventional decoding techniques, offering significant improvements in decoding accuracy and robustness.</article>","contentLength":1153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey of Internet Censorship and its Measurement: Methodology, Trends, and Challenges","url":"https://arxiv.org/abs/2502.14945","date":1761883200,"author":"","guid":322770,"unread":true,"content":"<article>arXiv:2502.14945v2 Announce Type: replace \nAbstract: Internet censorship limits the access of nodes residing within a specific network environment to the public Internet, and vice versa. During the last decade, techniques for conducting Internet censorship have been developed further. Consequently, methodology for measuring Internet censorship had been improved as well.\n  In this paper, we firstly provide a survey of network-level Internet censorship techniques. Secondly, we survey censorship measurement methodology. We further cover the censorship of circumvention tools and its measurement, as well as available datasets. In cases where it is beneficial, we bridge the terminology and taxonomy of Internet censorship with related domains, namely traffic obfuscation and information hiding. We further extend the technical perspective with recent trends and challenges, including human aspects of Internet censorship.</article>","contentLength":924,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unstructured Evidence Attribution for Long Context Query Focused Summarization","url":"https://arxiv.org/abs/2502.14409","date":1761883200,"author":"","guid":322771,"unread":true,"content":"<article>arXiv:2502.14409v2 Announce Type: replace \nAbstract: Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query, and extracting and citing evidence spans helps improve the trustworthiness of these summaries. Whereas previous work has focused on evidence citation with fixed levels of granularity (e.g. sentence, paragraph, document, etc.), we propose to extract unstructured (i.e., spans of any length) evidence in order to acquire more relevant and consistent evidence than in the fixed granularity case. We show how existing systems struggle to copy and properly cite unstructured evidence, which also tends to be \"lost-in-the-middle\". To help models perform this task, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel pipeline, which can be used as training supervision for unstructured evidence summarization. We demonstrate across 5 LLMs and 4 datasets spanning human written, synthetic, single, and multi-document settings that LLMs adapted with SUnsET generate more relevant and factually consistent evidence with their summaries, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries than baselines with no fine-tuning and fixed granularity evidence. We release SUnsET and our generation code to the public.</article>","contentLength":1407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data","url":"https://arxiv.org/abs/2502.09969","date":1761883200,"author":"","guid":322772,"unread":true,"content":"<article>arXiv:2502.09969v4 Announce Type: replace \nAbstract: Influence functions provide crucial insights into model training, but existing methods suffer from large computational costs and limited generalization. Particularly, recent works have proposed various metrics and algorithms to calculate the influence of data using language models, which do not scale well with large models and datasets. This is because of the expensive forward and backward passes required for computation, substantial memory requirements to store large models, and poor generalization of influence estimates to new data. In this paper, we explore the use of small neural networks -- which we refer to as the InfluenceNetwork -- to estimate influence values, achieving up to 99% cost reduction. Our evaluation demonstrates that influence values can be estimated with models just 0.0027% the size of full language models (we use 7B and 8B versions). We apply our algorithm of estimating influence values (called NN-CIFT: Neural Networks for effiCient Instruction Fine-Tuning) to the downstream task of subset selection for general instruction fine-tuning. In our study, we include four state-of-the-art influence functions and show no compromise in performance, despite large speedups, between NN-CIFT and the original influence functions. We provide an in-depth hyperparameter analyses of NN-CIFT. The code for our method can be found here: https://github.com/agarwalishika/NN-CIFT.</article>","contentLength":1454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RecCocktail: A Generalizable and Efficient Framework for LLM-Based Recommendation","url":"https://arxiv.org/abs/2502.08271","date":1761883200,"author":"","guid":322773,"unread":true,"content":"<article>arXiv:2502.08271v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have achieved remarkable success in recent years, owing to their impressive generalization capabilities and rich world knowledge. To capitalize on the potential of using LLMs as recommender systems, mainstream approaches typically focus on two paradigms. The first paradigm designs multi-domain or multi-task instruction data for generalizable recommendation, so as to align LLMs with general recommendation areas and deal with cold-start recommendation. The second paradigm focuses on enhancing domain-specific recommendation tasks, improving performance in warm recommendation scenarios. While most previous works treat these two paradigms separately, we argue that they have complementary advantages, and combining them can yield better results. In this paper, we propose a generalizable and efficient LLM-based recommendation framework RecCocktail. Our approach begins with fine-tuning a \"base spirit\" LoRA module using domain-general recommendation instruction data to align LLM with recommendation knowledge. Next, given users' behavior of a specific domain, we construct a domain-specific \"ingredient\" LoRA module. We then provide an entropy-guided adaptive merging method to mix the \"base spirit\" and the \"ingredient\" in the weight space. Please note that, RecCocktail combines the advantages of the existing two paradigms without introducing additional time or space overhead during the inference phase. Moreover, RecCocktail is efficient with plug and play, as the \"base spirit\" LoRA is trained only once, and any domain-specific \"ingredient\" can be efficiently mixed with only domain-specific fine-tuning. Extensive experiments on multiple datasets under both warm and cold-start recommendation scenarios validate the effectiveness and generality of the proposed RecCocktail.</article>","contentLength":1868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data","url":"https://arxiv.org/abs/2502.04380","date":1761883200,"author":"","guid":322774,"unread":true,"content":"<article>arXiv:2502.04380v3 Announce Type: replace \nAbstract: Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this work, we investigate the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-design for LLMs.</article>","contentLength":1295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fully discrete analysis of the Galerkin POD neural network approximation with application to 3D acoustic wave scattering","url":"https://arxiv.org/abs/2502.01859","date":1761883200,"author":"","guid":322775,"unread":true,"content":"<article>arXiv:2502.01859v3 Announce Type: replace \nAbstract: In this work, we consider the approximation of parametric maps using the so-called Galerkin POD-NN method. This technique combines the computation of a reduced basis via proper orthogonal decomposition (POD) and artificial neural networks (NNs) for the construction of fast surrogates of said parametric maps. In contrast to the existing literature, which has studied the approximation properties of this kind of architecture on a continuous level, we provide a fully discrete error analysis of this approach. More precisely, our estimates also account for discretization errors during the construction of the NN architecture. We consider the number of reduced basis in the approximation of the solution manifold, truncation in the parameter space, and, most importantly, the number of samples in the computation of the reduced space, together with the effect of the use of NNs in the approximation of the reduced coefficients. Following this error analysis, we provide a-priori bounds on the required POD tolerance, the resulting POD ranks, and NN parameters to maintain the order of convergence of quasi Monte Carlo sampling techniques. We conclude this work by showcasing the applicability of this method through a practical industrial application: the sound-soft acoustic scattering problem by a parametrically defined scatterer in three physical dimensions.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Omni-Mol: Multitask Molecular Model for Any-to-any Modalities","url":"https://arxiv.org/abs/2502.01074","date":1761883200,"author":"","guid":322776,"unread":true,"content":"<article>arXiv:2502.01074v3 Announce Type: replace \nAbstract: In the molecular domain, numerous studies have explored the use of multimodal large language models (LLMs) to construct a general-purpose, multi-task molecular model. However, these efforts are still far from achieving a truly universal molecular model. We identify three key challenges in this endeavor: (1) Existing molecular task datasets are typically small in scale and lack comprehensive domain coverage. (2) Tasks from different molecular subfields are difficult to effectively learn jointly through LLMs due to significant distributional shifts and competition among tasks, which introduces instability in the learning process. (3) Both inter-task and intra-task molecular representations demand different intrinsic dimensions in the language space, making it challenging to balance between redundancy and insufficiency in language model representations. To address these challenges, we innovatively categorize existing small-molecule tasks into four types: Mol2Mol, Mol2Text, Mol2Num, and Text2Mol. We then collect a dataset encompassing over 16 tasks with more than 1.4 million samples, making it the largest molecular instruction-tuning dataset to date. Leveraging the extensive pretraining of LLMs on existing chemical literature, we propose a novel multimodal LLM framework, named Omni-Mol, which unifies all small-molecule tasks and supports both molecular generation and understanding. The core of Omni-Mol is our proposed MoGE, which dynamically adapts to the intrinsic rank of different tasks. This mixture-of-experts architecture enhances the model's ability to handle diverse tasks and modalities effectively. Our model achieves unified instruction tuning across 16 tasks and attains state-of-the-art performance on 13 of them. Extensive experiments further demonstrate the scalability and versatility of Omni-Mol.</article>","contentLength":1886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model Provenance Testing for Large Language Models","url":"https://arxiv.org/abs/2502.00706","date":1761883200,"author":"","guid":322777,"unread":true,"content":"<article>arXiv:2502.00706v2 Announce Type: replace \nAbstract: Large language models are increasingly customized through fine-tuning and other adaptations, creating challenges in enforcing licensing terms and managing downstream impacts. Tracking model origins is crucial both for protecting intellectual property and for identifying derived models when biases or vulnerabilities are discovered in foundation models. We address this challenge by developing a framework for testing model provenance: Whether one model is derived from another. Our approach is based on the key observation that real-world model derivations preserve significant similarities in model outputs that can be detected through statistical analysis. Using only black-box access to models, we employ multiple hypothesis testing to compare model similarities against a baseline established by unrelated models. On two comprehensive real-world benchmarks spanning models from 30M to 4B parameters and comprising over 600 models, our tester achieves 90-95% precision and 80-90% recall in identifying derived models. These results demonstrate the viability of systematic provenance verification in production environments even when only API access is available.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agile and Cooperative Aerial Manipulation of a Cable-Suspended Load","url":"https://arxiv.org/abs/2501.18802","date":1761883200,"author":"","guid":322778,"unread":true,"content":"<article>arXiv:2501.18802v2 Announce Type: replace \nAbstract: Quadrotors can carry slung loads to hard-to-reach locations at high speed. Since a single quadrotor has limited payload capacities, using a team of quadrotors to collaboratively manipulate a heavy object is a scalable and promising solution. However, existing control algorithms for multi-lifting systems only enable low-speed and low-acceleration operations due to the complex dynamic coupling between quadrotors and the load, limiting their use in time-critical missions such as search and rescue. In this work, we present a solution to significantly enhance the agility of cable-suspended multi-lifting systems. Unlike traditional cascaded solutions, we introduce a trajectory-based framework that solves the whole-body kinodynamic motion planning problem online, accounting for the dynamic coupling effects and constraints between the quadrotors and the load. The planned trajectory is provided to the quadrotors as a reference in a receding-horizon fashion and is tracked by an onboard controller that observes and compensates for the cable tension. Real-world experiments demonstrate that our framework can achieve at least eight times greater acceleration than state-of-the-art methods to follow agile trajectories. Our method can even perform complex maneuvers such as flying through narrow passages at high speed. Additionally, it exhibits high robustness against load uncertainties and does not require adding any sensors to the load, demonstrating strong practicality.</article>","contentLength":1532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Network Oblivious Transfer via Noisy Channels: Limits and Capacities","url":"https://arxiv.org/abs/2501.17021","date":1761883200,"author":"","guid":322779,"unread":true,"content":"<article>arXiv:2501.17021v2 Announce Type: replace \nAbstract: In this paper, we aim to study the information-theoretical limits of oblivious transfer. This work also investigates the problem of oblivious transfer over a noisy multiple access channel involving two non-colluding senders and a single receiver. The channel model is characterized by correlations among the parties, with the parties assumed to be either honest-but-curious or, in the receiver's case, potentially malicious. At first, we study the information-theoretical limits of oblivious transfer between two parties and extend it to the multiple access channel model. We propose a multiparty protocol for honest-but-curious parties where the general multiple access channel is reduced to a certain correlation. In scenarios where the receiver is malicious, the protocol achieves an achievable rate region.</article>","contentLength":863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location","url":"https://arxiv.org/abs/2501.14808","date":1761883200,"author":"","guid":322780,"unread":true,"content":"<article>arXiv:2501.14808v4 Announce Type: replace \nAbstract: Large language models (LLMs) have facilitated a wide range of applications with distinct service-level objectives (SLOs), from latency-sensitive online tasks like interactive chatbots to throughput-oriented offline workloads like data synthesis. The existing deployment model, which dedicates machines to each workload, simplifies SLO management but often leads to poor resource utilization. This paper introduces HyGen, an interference-aware LLM serving system that enables efficient co-location of online and offline workloads while preserving SLOs. HyGen incorporates two key innovations: (1) performance control mechanisms, including a latency predictor to estimate batch execution time and an SLO-aware profiler to quantify latency interference, and (2) SLO-aware offline scheduling policies that maximize serving throughput and prevent starvation. Our evaluation on production workloads shows that HyGen achieves up to 3.9-5.8x throughput gains over online and hybrid serving baselines, while ensuring latency SLOs. The code of HyGen is publicly available at https://github.com/UIUC-MLSys/HyGen.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiplication-Free Parallelizable Spiking Neurons with Efficient Spatio-Temporal Dynamics","url":"https://arxiv.org/abs/2501.14490","date":1761883200,"author":"","guid":322781,"unread":true,"content":"<article>arXiv:2501.14490v2 Announce Type: replace \nAbstract: Spiking Neural Networks (SNNs) are distinguished from Artificial Neural Networks (ANNs) for their complex neuronal dynamics and sparse binary activations (spikes) inspired by the biological neural system. Traditional neuron models use iterative step-by-step dynamics, resulting in serial computation and slow training speed of SNNs. Recently, parallelizable spiking neuron models have been proposed to fully utilize the massive parallel computing ability of graphics processing units to accelerate the training of SNNs. However, existing parallelizable spiking neuron models involve dense floating operations and can only achieve high long-term dependencies learning ability with a large order at the cost of huge computational and memory costs. To solve the dilemma of performance and costs, we propose the mul-free channel-wise Parallel Spiking Neuron, which is hardware-friendly and suitable for SNNs' resource-restricted application scenarios. The proposed neuron imports the channel-wise convolution to enhance the learning ability, induces the sawtooth dilations to reduce the neuron order, and employs the bit-shift operation to avoid multiplications. The algorithm for the design and implementation of acceleration methods is discussed extensively. Our methods are validated in neuromorphic Spiking Heidelberg Digits voices, sequential CIFAR images, and neuromorphic DVS-Lip vision datasets, achieving superior performance over SOTA spiking neurons. Training speed results demonstrate the effectiveness of our acceleration methods, providing a practical reference for future research. Our code is available at \\href{https://github.com/PengXue0812/Multiplication-Free-Parallelizable-Spiking-Neurons-with-Efficient-Spatio-Temporal-Dynamics}{Github}.</article>","contentLength":1808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GameFactory: Creating New Games with Generative Interactive Videos","url":"https://arxiv.org/abs/2501.08325","date":1761883200,"author":"","guid":322782,"unread":true,"content":"<article>arXiv:2501.08325v4 Announce Type: replace \nAbstract: Generative videos have the potential to revolutionize game development by autonomously creating new content. In this paper, we present GameFactory, a framework for action-controlled scene-generalizable game video generation. We first address the fundamental challenge of action controllability by introducing GF-Minecraft, an action-annotated game video dataset without human bias, and developing an action control module that enables precise control over both keyboard and mouse inputs. We further extend to support autoregressive generation for unlimited-length interactive videos. More importantly, GameFactory tackles the critical challenge of scene-generalizable action control, which most existing methods fail to address. To enable the creation of entirely new and diverse games beyond fixed styles and scenes, we leverage the open-domain generative priors from pre-trained video diffusion models. To bridge the domain gap between open-domain priors and small-scale game datasets, we propose a multi-phase training strategy with a domain adapter that decouples game style learning from action control. This decoupling ensures that action control learning is no longer bound to specific game styles, thereby achieving scene-generalizable action control. Experimental results demonstrate that GameFactory effectively generates open-domain action-controllable game videos, representing a significant step forward in AI-driven game generation.</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UV-Attack: Physical-World Adversarial Attacks for Person Detection via Dynamic-NeRF-based UV Mapping","url":"https://arxiv.org/abs/2501.05783","date":1761883200,"author":"","guid":322783,"unread":true,"content":"<article>arXiv:2501.05783v2 Announce Type: replace \nAbstract: In recent research, adversarial attacks on person detectors using patches or static 3D model-based texture modifications have struggled with low success rates due to the flexible nature of human movement. Modeling the 3D deformations caused by various actions has been a major challenge. Fortunately, advancements in Neural Radiance Fields (NeRF) for dynamic human modeling offer new possibilities. In this paper, we introduce UV-Attack, a groundbreaking approach that achieves high success rates even with extensive and unseen human actions. We address the challenge above by leveraging dynamic-NeRF-based UV mapping. UV-Attack can generate human images across diverse actions and viewpoints, and even create novel actions by sampling from the SMPL parameter space. While dynamic NeRF models are capable of modeling human bodies, modifying clothing textures is challenging because they are embedded in neural network parameters. To tackle this, UV-Attack generates UV maps instead of RGB images and modifies the texture stacks. This approach enables real-time texture edits and makes the attack more practical. We also propose a novel Expectation over Pose Transformation loss (EoPT) to improve the evasion success rate on unseen poses and views. Our experiments show that UV-Attack achieves a 92.7% attack success rate against the FastRCNN model across varied poses in dynamic video settings, significantly outperforming the state-of-the-art AdvCamou attack, which only had a 28.5% ASR. Moreover, we achieve 49.5% ASR on the latest YOLOv8 detector in black-box settings. This work highlights the potential of dynamic NeRF-based UV mapping for creating more effective adversarial attacks on person detectors, addressing key challenges in modeling human movement and texture modification. The code is available at https://github.com/PolyLiYJ/UV-Attack.</article>","contentLength":1905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SHARE: Optimizing Secure Hub Allocation and Routing Efficiency in Payment Channel Networks","url":"https://arxiv.org/abs/2501.04236","date":1761883200,"author":"","guid":322784,"unread":true,"content":"<article>arXiv:2501.04236v2 Announce Type: replace \nAbstract: Payment channel hub (PCH), by leveraging a powerful hub to reliably provide off-chain payment services, offers an effective enhancement to payment channel networks (PCNs). However, existing approaches typically rely on a single hub to relay transactions and provide relationship anonymity between participants. This design lacks flexibility under high-frequency transaction scenarios and fail to adequately balance the security of off-chain payments with PCH efficiency. Moreover, current PCNs often adopt source routing, where each transaction path is predetermined without considering the dynamic distribution of large-scale payment requests, leading to load imbalance and even transaction deadlocks. To address these issues, we propose SHARE, a multi-PCH distributed routing scheme based on trusted execution environments (TEE), designed to optimize secure hub allocation and routing efficiency in PCNs. For the multi-hub allocation problem, SHARE balances the management and synchronization costs among participants, and employs mixed-integer linear programming along with supermodular optimization techniques to transform the NP-hard problem into a solvable form, enabling optimal or approximate solutions across various PCN scales. At the routing layer, SHARE integrates global network state with local sender requests to design a TEE-assisted, privacy-preserving distributed routing protocol that dynamically adjusts multipath flow rates, achieving high-throughput and deadlock-free transaction forwarding. We formally prove the security of the SHARE protocol under the universally composable framework. Experimental results demonstrate that SHARE achieves a 43.6% improvement in transaction success ratio and an over 181.5% enhancement in system throughput compared to state-of-the-art PCN solutions, effectively realizing a secure extension of PCNs.</article>","contentLength":1911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning","url":"https://arxiv.org/abs/2412.20392","date":1761883200,"author":"","guid":322785,"unread":true,"content":"<article>arXiv:2412.20392v4 Announce Type: replace \nAbstract: Multimodal contrastive learning models (e.g., CLIP) can learn high-quality representations from large-scale image-text datasets, while they exhibit significant vulnerabilities to backdoor attacks, raising serious safety concerns. In this paper, we reveal that CLIP's vulnerabilities primarily stem from its tendency to encode features beyond in-dataset predictive patterns, compromising its visual feature resistivity to input perturbations. This makes its encoded features highly susceptible to being reshaped by backdoor triggers. To address this challenge, we propose Repulsive Visual Prompt Tuning (RVPT), a novel defense approach that employs deep visual prompt tuning with a specially designed feature-repelling loss. Specifically, RVPT adversarially repels the encoded features from deeper layers while optimizing the standard cross-entropy loss, ensuring that only predictive features in downstream tasks are encoded, thereby enhancing CLIP's visual feature resistivity against input perturbations and mitigating its susceptibility to backdoor attacks. Unlike existing multimodal backdoor defense methods that typically require the availability of poisoned data or involve fine-tuning the entire model, RVPT leverages few-shot downstream clean samples and only tunes a small number of parameters. Empirical results demonstrate that RVPT tunes only 0.27\\% of the parameters in CLIP, yet it significantly outperforms state-of-the-art defense methods, reducing the attack success rate from 89.70\\% to 2.76\\% against the most advanced multimodal attacks on ImageNet and effectively generalizes its defensive capabilities across multiple datasets.</article>","contentLength":1703,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In Defence of Post-hoc Explainability","url":"https://arxiv.org/abs/2412.17883","date":1761883200,"author":"","guid":322786,"unread":true,"content":"<article>arXiv:2412.17883v2 Announce Type: replace \nAbstract: This position paper defends post-hoc explainability methods as legitimate tools for scientific knowledge production in machine learning. Addressing criticism of these methods' reliability and epistemic status, we develop a philosophical framework grounded in mediated understanding and bounded factivity. We argue that scientific insights can emerge through structured interpretation of model behaviour without requiring complete mechanistic transparency, provided explanations acknowledge their approximative nature and undergo rigorous empirical validation. Through analysis of recent biomedical ML applications, we demonstrate how post-hoc methods, when properly integrated into scientific practice, generate novel hypotheses and advance phenomenal understanding.</article>","contentLength":819,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Resource Efficient Multi-stain Kidney Glomeruli Segmentation via Self-supervision","url":"https://arxiv.org/abs/2412.15389","date":1761883200,"author":"","guid":322787,"unread":true,"content":"<article>arXiv:2412.15389v3 Announce Type: replace \nAbstract: Semantic segmentation under domain shift remains a fundamental challenge in computer vision, particularly when labelled training data is scarce. This challenge is particularly exemplified in histopathology image analysis, where the same tissue structures must be segmented across images captured under different imaging conditions (stains), each representing a distinct visual domain. Traditional deep learning methods like UNet require extensive labels, which is both costly and time-consuming, particularly when dealing with multiple domains (or stains). To mitigate this, various unsupervised domain adaptation based methods such as UDAGAN have been proposed, which reduce the need for labels by requiring only one (source) stain to be labelled. Nonetheless, obtaining source stain labels can still be challenging. This article shows that through self-supervised pre-training -- including SimCLR, BYOL, and a novel approach, HR-CS-CO -- the performance of these segmentation methods (UNet, and UDAGAN) can be retained even with 95% fewer labels. Notably, with self-supervised pre-training and using only 5% labels, the performance drops are minimal: 5.9% for UNet and 6.2% for UDAGAN, averaged over all stains, compared to their respective fully supervised counterparts (without pre-training, using 100% labels). Furthermore, these findings are shown to generalise beyond their training distribution to public benchmark datasets. Implementations and pre-trained models are publicly available \\href{https://github.com/zeeshannisar/resource-effecient-multi-stain-kidney-glomeruli-segmentation.git}{online}.</article>","contentLength":1660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HoGA: Higher-Order Graph Attention via Diversity-Aware k-Hop Sampling","url":"https://arxiv.org/abs/2411.12052","date":1761883200,"author":"","guid":322788,"unread":true,"content":"<article>arXiv:2411.12052v2 Announce Type: replace \nAbstract: Graphs model latent variable relationships in many real-world systems, and Message Passing Neural Networks (MPNNs) are widely used to learn such structures for downstream tasks. While edge-based MPNNs effectively capture local interactions, their expressive power is theoretically bounded, limiting the discovery of higher-order relationships. We introduce the Higher-Order Graph Attention (HoGA) module, which constructs a k-order attention matrix by sampling subgraphs to maximize diversity among feature vectors. Unlike existing higher-order attention methods that greedily resample similar k-order relationships, HoGA targets diverse modalities in higher-order topology, reducing redundancy and expanding the range of captured substructures. Applied to two single-hop attention models, HoGA achieves at least a 5% accuracy gain on all benchmark node classification datasets and outperforms recent baselines on six of eight datasets. Code is available at https://github.com/TB862/Higher_Order.</article>","contentLength":1049,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hysteresis Activation Function for Efficient Inference","url":"https://arxiv.org/abs/2411.10573","date":1761883200,"author":"","guid":322789,"unread":true,"content":"<article>arXiv:2411.10573v3 Announce Type: replace \nAbstract: The widely used ReLU is favored for its hardware efficiency, {as the implementation at inference is a one bit sign case,} yet suffers from issues such as the ``dying ReLU'' problem, where during training, neurons fail to activate and constantly remain at zero, as highlighted by Lu et al. Traditional approaches to mitigate this issue often introduce more complex and less hardware-friendly activation functions. In this work, we propose a Hysteresis Rectified Linear Unit (HeLU), an efficient activation function designed to address the ``dying ReLU'' problem with minimal complexity. Unlike traditional activation functions with fixed thresholds for training and inference, HeLU employs a variable threshold that refines the backpropagation. This refined mechanism allows simpler activation functions to achieve competitive performance comparable to their more complex counterparts without introducing unnecessary complexity or requiring inductive biases. Empirical evaluations demonstrate that HeLU enhances model generalization across diverse datasets, offering a promising solution for efficient and effective inference suitable for a wide range of neural network architectures.</article>","contentLength":1236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models","url":"https://arxiv.org/abs/2411.10501","date":1761883200,"author":"","guid":322790,"unread":true,"content":"<article>arXiv:2411.10501v2 Announce Type: replace \nAbstract: We consider the problem of text-to-video generation tasks with precise control for various applications such as camera movement control and video-to-video editing. Most methods tacking this problem rely on providing user-defined controls, such as binary masks or camera movement embeddings. In our approach we propose OnlyFlow, an approach leveraging the optical flow firstly extracted from an input video to condition the motion of generated videos. Using a text prompt and an input video, OnlyFlow allows the user to generate videos that respect the motion of the input video as well as the text prompt. This is implemented through an optical flow estimation model applied on the input video, which is then fed to a trainable optical flow encoder. The output feature maps are then injected into the text-to-video backbone model. We perform quantitative, qualitative and user preference studies to show that OnlyFlow positively compares to state-of-the-art methods on a wide range of tasks, even though OnlyFlow was not specifically trained for such tasks. OnlyFlow thus constitutes a versatile, lightweight yet efficient method for controlling motion in text-to-video generation. Models and code will be made available on GitHub and HuggingFace.</article>","contentLength":1300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Continuous and Interpretable Morphometric for Robust Quantification of Dynamic Biological Shapes","url":"https://arxiv.org/abs/2410.21004","date":1761883200,"author":"","guid":322791,"unread":true,"content":"<article>arXiv:2410.21004v2 Announce Type: replace \nAbstract: We introduce the Push-Forward Signed Distance Morphometric (PF-SDM) for shape quantification in biomedical imaging. The PF-SDM compactly encodes geometric and topological properties of closed shapes, including their skeleton and symmetries. This provides robust and interpretable features for shape comparison and machine learning. The PF-SDM is mathematically smooth, providing access to gradients and differential-geometric quantities. It also extends to temporal dynamics and allows fusing spatial intensity distributions, such as genetic markers, with shape dynamics. We present the PF-SDM theory, benchmark it on synthetic data, and apply it to predicting body-axis formation in mouse gastruloids, outperforming a CNN baseline in both accuracy and speed.</article>","contentLength":812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Candidate is [MASK]. Prompt-based Sentiment Extraction and Reference Letters","url":"https://arxiv.org/abs/2410.16325","date":1761883200,"author":"","guid":322792,"unread":true,"content":"<article>arXiv:2410.16325v3 Announce Type: replace \nAbstract: I propose a relatively simple way to deploy pre-trained large language models (LLMs) in order to extract sentiment and other useful features from text data. The method, which I refer to as prompt-based sentiment extraction, offers multiple advantages over other methods used in economics and finance. In particular, it accepts the text input as is (without pre-processing) and produces a sentiment score that has a probability interpretation. Unlike other LLM-based approaches, it does not require any fine-tuning or labeled data. I apply my prompt-based strategy to a hand-collected corpus of confidential reference letters (RLs). I show that the sentiment contents of RLs are clearly reflected in job market outcomes. Candidates with higher average sentiment in their RLs perform markedly better regardless of the measure of success chosen. Moreover, I show that sentiment dispersion among letter writers negatively affects the job market candidate's performance. I compare my sentiment extraction approach to other commonly used methods for sentiment analysis: `bag-of-words' approaches, fine-tuned language models, and querying advanced chatbots. No other method can fully reproduce the results obtained by prompt-based sentiment extraction. Finally, I slightly modify the method to obtain `gendered' sentiment scores (as in Eberhardt et al., 2023). I show that RLs written for female candidates emphasize `grindstone' personality traits, whereas male candidates' letters emphasize `standout' traits. These gender differences negatively affect women's job market outcomes.</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vital Insight: Assisting Experts' Context-Driven Sensemaking of Multi-modal Personal Tracking Data Using Visualization and Human-In-The-Loop LLM","url":"https://arxiv.org/abs/2410.14879","date":1761883200,"author":"","guid":322793,"unread":true,"content":"<article>arXiv:2410.14879v4 Announce Type: replace \nAbstract: Passive tracking methods, such as phone and wearable sensing, have become dominant in monitoring human behaviors in modern ubiquitous computing studies. While there have been significant advances in machine-learning approaches to translate periods of raw sensor data to model momentary behaviors, (e.g., physical activity recognition), there still remains a significant gap in the translation of these sensing streams into meaningful, high-level, context-aware insights that are required for various applications (e.g., summarizing an individual's daily routine). To bridge this gap, experts often need to employ a context-driven sensemaking process in real-world studies to derive insights. This process often requires manual effort and can be challenging even for experienced researchers due to the complexity of human behaviors.\n  We conducted three rounds of user studies with 21 experts to explore solutions to address challenges with sensemaking. We follow a human-centered design process to identify needs and design, iterate, build, and evaluate Vital Insight (VI), a novel, LLM-assisted, prototype system to enable human-in-the-loop inference (sensemaking) and visualizations of multi-modal passive sensing data from smartphones and wearables. Using the prototype as a technology probe, we observe experts' interactions with it and develop an expert sensemaking model that explains how experts move between direct data representations and AI-supported inferences to explore, question, and validate insights. Through this iterative process, we also synthesize and discuss a list of design implications for the design of future AI-augmented visualization systems to better assist experts' sensemaking processes in multi-modal health sensing data.</article>","contentLength":1806,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Model Preference Evaluation with Multiple Weak Evaluators","url":"https://arxiv.org/abs/2410.12869","date":1761883200,"author":"","guid":322794,"unread":true,"content":"<article>arXiv:2410.12869v4 Announce Type: replace \nAbstract: Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs' quality regarding preference remains a critical challenge. While existing works usually leverage a strong LLM as the judge for comparing LLMs' response pairwisely, such a single-evaluator approach is vulnerable to cyclic preference, i.e., output A is better than B, B than C, but C is better than A, causing contradictory evaluation results. To address this, we introduce PGED (Preference Graph Ensemble and Denoise), a novel approach that leverages multiple model-based evaluators to construct preference graphs, and then ensembles and denoises these graphs for acyclic, non-contradictory evaluation results. We provide theoretical guarantees for our framework, demonstrating its efficacy in recovering the ground truth preference structure. Extensive experiments on ten benchmarks demonstrate PGED 's superiority in three applications: 1) model ranking for evaluation, 2) response selection for test-time scaling, and 3) data selection for model fine-tuning. Notably, PGED combines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to outperform strong ones (e.g., Qwen2-72B), showcasing its effectiveness in enhancing evaluation reliability and improving model performance.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Constrained Posterior Sampling: Time Series Generation with Hard Constraints","url":"https://arxiv.org/abs/2410.12652","date":1761883200,"author":"","guid":322795,"unread":true,"content":"<article>arXiv:2410.12652v2 Announce Type: replace \nAbstract: Generating realistic time series samples is crucial for stress-testing models and protecting user privacy by using synthetic data. In engineering and safety-critical applications, these samples must meet certain hard constraints that are domain-specific or naturally imposed by physics or nature. Consider, for example, generating electricity demand patterns with constraints on peak demand times. This can be used to stress-test the functioning of power grids during adverse weather conditions. Existing approaches for generating constrained time series are either not scalable or degrade sample quality. To address these challenges, we introduce Constrained Posterior Sampling (CPS), a diffusion-based sampling algorithm that aims to project the posterior mean estimate into the constraint set after each denoising update. Notably, CPS scales to a large number of constraints ($\\sim100$) without requiring additional training. We provide theoretical justifications highlighting the impact of our projection step on sampling. Empirically, CPS outperforms state-of-the-art methods in sample quality and similarity to real time series by around 70\\% and 22\\%, respectively, on real-world stocks, traffic, and air quality datasets.</article>","contentLength":1282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stability and Sharper Risk Bounds with Convergence Rate $\\tilde{O}(1/n^2)$","url":"https://arxiv.org/abs/2410.09766","date":1761883200,"author":"","guid":322796,"unread":true,"content":"<article>arXiv:2410.09766v2 Announce Type: replace \nAbstract: Prior work (Klochkov $\\&amp;$ Zhivotovskiy, 2021) establishes at most $O\\left(\\log (n)/n\\right)$ excess risk bounds via algorithmic stability for strongly-convex learners with high probability. We show that under the similar common assumptions -- - Polyak-Lojasiewicz condition, smoothness, and Lipschitz continous for losses -- - rates of $O\\left(\\log^2(n)/n^2\\right)$ are at most achievable. To our knowledge, our analysis also provides the tightest high-probability bounds for gradient-based generalization gaps in nonconvex settings.</article>","contentLength":586,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentiation Through Black-Box Quadratic Programming Solvers","url":"https://arxiv.org/abs/2410.06324","date":1761883200,"author":"","guid":322797,"unread":true,"content":"<article>arXiv:2410.06324v4 Announce Type: replace \nAbstract: Differentiable optimization has attracted significant research interest, particularly for quadratic programming (QP). Existing approaches for differentiating the solution of a QP with respect to its defining parameters often rely on specific integrated solvers. This integration limits their applicability, including their use in neural network architectures and bi-level optimization tasks, restricting users to a narrow selection of solver choices. To address this limitation, we introduce dQP, a modular and solver-agnostic framework for plug-and-play differentiation of virtually any QP solver. A key insight we leverage to achieve modularity is that, once the active set of inequality constraints is known, both the solution and its derivative can be expressed using simplified linear systems that share the same matrix. This formulation fully decouples the computation of the QP solution from its differentiation. Building on this result, we provide a minimal-overhead, open-source implementation ( https://github.com/cwmagoon/dQP ) that seamlessly integrates with over 15 state-of-the-art solvers. Comprehensive benchmark experiments demonstrate dQP's robustness and scalability, particularly highlighting its advantages in large-scale sparse problems.</article>","contentLength":1312,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digital Labor and the Inconspicuous Production of Artificial Intelligence","url":"https://arxiv.org/abs/2410.05910","date":1761883200,"author":"","guid":322798,"unread":true,"content":"<article>arXiv:2410.05910v2 Announce Type: replace \nAbstract: Digital platforms capitalize on users' labor, often disguising essential contributions as casual activities or consumption, regardless of users' recognition of their efforts. Data annotation, content creation, and engagement with advertising are all aspects of this hidden productivity. Despite playing a crucial role in driving AI development, such tasks remain largely unrecognized and undercompensated. This chapter exposes the systemic devaluation of these activities in the digital economy, by drawing on historical theories about unrecognized labor, from housework to audience labor. This approach advocates for a broader understanding of digital labor by introducing the concept of ''inconspicuous production.'' It moves beyond the traditional notion of ''invisible work'' to highlight the hidden elements inherent in all job types, especially in light of growing automation and platform-based employment.</article>","contentLength":965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unified Error Correction Code Transformer with Low Complexity","url":"https://arxiv.org/abs/2410.03364","date":1761883200,"author":"","guid":322799,"unread":true,"content":"<article>arXiv:2410.03364v4 Announce Type: replace \nAbstract: Channel coding is vital for reliable sixth-generation (6G) data transmission, employing diverse error correction codes for various application scenarios. Traditional decoders require dedicated hardware for each code, leading to high hardware costs. Recently, artificial intelligence (AI)-driven approaches, such as the error correction code Transformer (ECCT) and its enhanced version, the foundation error correction code Transformer (FECCT), have been proposed to reduce the hardware cost by leveraging the Transformer to decode multiple codes. However, their excessively high computational complexity of $\\mathcal{O}(N^2)$ due to the self-attention mechanism in the Transformer limits scalability, where $N$ represents the sequence length. To reduce computational complexity, we propose a unified Transformer-based decoder that handles multiple linear block codes within a single framework. Specifically, a standardized unit is employed to align code length and code rate across different code types, while a redesigned low-rank unified attention module, with computational complexity of $\\mathcal{O}(N)$, is shared across various heads in the Transformer. Additionally, a sparse mask, derived from the parity-check matrix's sparsity, is introduced to enhance the decoder's ability to capture inherent constraints between information and parity-check bits, improving decoding accuracy and further reducing computational complexity by $86\\%$. Extensive experimental results demonstrate that the proposed unified Transformer-based decoder outperforms existing methods and provides a high-performance, low-complexity solution for next-generation wireless communication systems.</article>","contentLength":1730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning","url":"https://arxiv.org/abs/2410.03348","date":1761883200,"author":"","guid":322800,"unread":true,"content":"<article>arXiv:2410.03348v5 Announce Type: replace \nAbstract: Neurosymbolic learning enables the integration of symbolic reasoning with deep learning but faces significant challenges in scaling to complex symbolic programs, large datasets, or both. We introduce DOLPHIN, a framework that tackles these challenges by supporting neurosymbolic programs in Python, executing complex symbolic reasoning on the CPU while vectorizing probabilistic computations and gradient propagation on the GPU. Across 13 benchmarks spanning tasks over text, image, and video data, with symbolic reasoning features like recursion and black-box functions, DOLPHIN converges to state-of-the-art accuracies on the more complex benchmarks while existing frameworks such as Scallop, ISED, and IndeCateR+ fail to converge within the time limit. On simpler benchmarks, DOLPHIN matches their performance, while achieving these results 1.71x to 62x faster than the baselines. Overall, DOLPHIN advances the scalability of neurosymbolic frameworks, achieving state-of-the-art efficiency and convergence on difficult benchmarks where existing frameworks struggle. The code is published at https://github.com/Dolphin-NeSy/Dolphin.</article>","contentLength":1187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"End-to-end guarantees for indirect data-driven control of bilinear systems with finite stochastic data","url":"https://arxiv.org/abs/2409.18010","date":1761883200,"author":"","guid":322801,"unread":true,"content":"<article>arXiv:2409.18010v3 Announce Type: replace \nAbstract: In this paper we propose an end-to-end algorithm for indirect data-driven control for bilinear systems with stability guarantees. We consider the case where the collected i.i.d. data is affected by probabilistic noise with possibly unbounded support and leverage tools from statistical learning theory to derive finite sample identification error bounds. To this end, we solve the bilinear identification problem by solving a set of linear and affine identification problems, by a particular choice of a control input during the data collection phase. We provide a priori as well as data-dependent finite sample identification error bounds on the individual matrices as well as ellipsoidal bounds, both of which are structurally suitable for control. Further, we integrate the structure of the derived identification error bounds in a robust controller design to obtain an exponentially stable closed-loop. By means of an extensive numerical study we showcase the interplay between the controller design and the derived identification error bounds. Moreover, we note appealing connections of our results to indirect data-driven control of general nonlinear systems through Koopman operator theory and discuss how our results may be applied in this setup.</article>","contentLength":1307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IntelliRadar: A Comprehensive Platform to Pinpoint Malicious Packages Information from Cyber Intelligence","url":"https://arxiv.org/abs/2409.15049","date":1761883200,"author":"","guid":322802,"unread":true,"content":"<article>arXiv:2409.15049v4 Announce Type: replace \nAbstract: Malicious packages in public registries pose serious threats to software supply chain security. While current software component analysis (SCA) tools rely on databases like OSV and Snyk to detect these threats, these databases suffer from delayed updates and incomplete coverage. However, they miss intelligence from unstructured sources like social media and developer forums, where new threats are often first reported. This delay extends the lifecycle of malicious packages and increases risks for downstream users. To address this, we developed a novel and comprehensive approach to construct a platform IntelliRadar to collect disclosed malicious package names from unstructured web content. Specifically, by exhaustively searching and snowballing the public sources of malicious package names, and incorporating large language models (LLMs) with domain-specialized Least to Most prompts, IntelliRadar ensures comprehensive collection of historical and current disclosed malicious package names from diverse unstructured sources. As a result, we constructed a comprehensive malicious package database containing 34,313 malicious NPM and PyPI package names. Our evaluation shows that IntelliRadar achieves high performance (97.91% precision) on malicious package intelligence extraction. Compared to existing databases, IntelliRadar identifies 7,542 more malicious package names than OSV and 12,684 more than Snyk. Furthermore, 76.6% of NPM components and 70.3% of PyPI components in IntelliRadar were collected earlier than in Snyk's database. IntelliRadar is also more cost-efficient, with a cost of $0.003 per piece of malicious package intelligence and only $7 per month for continuous monitoring. Furthermore, we identified and received confirmation for 1,981 malicious packages in downstream package manager mirror registries through the IntelliRadar.</article>","contentLength":1914,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Approximating the signature of Brownian motion for high order SDE simulation","url":"https://arxiv.org/abs/2409.10118","date":1761883200,"author":"","guid":322803,"unread":true,"content":"<article>arXiv:2409.10118v2 Announce Type: replace \nAbstract: The signature is a collection of iterated integrals describing the \"shape\" of a path. It appears naturally in the Taylor expansions of controlled differential equations and, as a consequence, is arguably the central object within rough path theory. In this paper, we will consider the signature of Brownian motion with time, and present both new and recently developed approximations for some of its integrals. Since these integrals (or equivalent L\\'{e}vy areas) are nonlinear functions of the Brownian path, they are not Gaussian and known to be challenging to simulate. To conclude the paper, we will present some applications of these approximations to the high order numerical simulation of stochastic differential equations (SDEs).</article>","contentLength":790,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Speak & Spell: LLM-Driven Controllable Phonetic Error Augmentation for Robust Dialogue State Tracking","url":"https://arxiv.org/abs/2409.06263","date":1761883200,"author":"","guid":322804,"unread":true,"content":"<article>arXiv:2409.06263v2 Announce Type: replace \nAbstract: Dialogue State Tracking (DST) is a key part of task-oriented dialogue systems, identifying important information in conversations. However, its accuracy drops significantly in spoken dialogue environments due to named entity errors from Automatic Speech Recognition (ASR) systems. We introduce a simple yet effective data augmentation method that targets those entities to improve the robustness of DST model. Our novel method can control the placement of errors using keyword-highlighted prompts while introducing phonetically similar errors. As a result, our method generated sufficient error patterns on keywords, leading to improved accuracy in noised and low-accuracy ASR environments.</article>","contentLength":743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Static for Dynamic: Towards a Deeper Understanding of Dynamic Facial Expressions Using Static Expression Data","url":"https://arxiv.org/abs/2409.06154","date":1761883200,"author":"","guid":322805,"unread":true,"content":"<article>arXiv:2409.06154v3 Announce Type: replace \nAbstract: Dynamic facial expression recognition (DFER) infers emotions from the temporal evolution of expressions, unlike static facial expression recognition (SFER), which relies solely on a single snapshot. This temporal analysis provides richer information and promises greater recognition capability. However, current DFER methods often exhibit unsatisfied performance largely due to fewer training samples compared to SFER. Given the inherent correlation between static and dynamic expressions, we hypothesize that leveraging the abundant SFER data can enhance DFER. To this end, we propose Static-for-Dynamic (S4D), a unified dual-modal learning framework that integrates SFER data as a complementary resource for DFER. Specifically, S4D employs dual-modal self-supervised pre-training on facial images and videos using a shared Vision Transformer (ViT) encoder-decoder architecture, yielding improved spatiotemporal representations. The pre-trained encoder is then fine-tuned on static and dynamic expression datasets in a multi-task learning setup to facilitate emotional information interaction. Unfortunately, vanilla multi-task learning in our study results in negative transfer. To address this, we propose an innovative Mixture of Adapter Experts (MoAE) module that facilitates task-specific knowledge acquisition while effectively extracting shared knowledge from both static and dynamic expression data. Extensive experiments demonstrate that S4D achieves a deeper understanding of DFER, setting new state-of-the-art performance on FERV39K, MAFW, and DFEW benchmarks, with weighted average recall (WAR) of 53.65\\%, 58.44\\%, and 76.68\\%, respectively. Additionally, a systematic correlation analysis between SFER and DFER tasks is presented, which further elucidates the potential benefits of leveraging SFER.</article>","contentLength":1866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion Map Autoencoder","url":"https://arxiv.org/abs/2409.05901","date":1761883200,"author":"","guid":322806,"unread":true,"content":"<article>arXiv:2409.05901v3 Announce Type: replace \nAbstract: Diffusion-Map-AutoEncoder (DMAE) pairs a diffusion-map encoder (using the Nystr\\\"om method) with linear or RBF Gaussian-Process latent mean decoders, yielding closed-form inductive mappings and strong reconstructions.</article>","contentLength":270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Constraining Participation: Affordances of Feedback Features in Interfaces to Large Language Models","url":"https://arxiv.org/abs/2408.15066","date":1761883200,"author":"","guid":322807,"unread":true,"content":"<article>arXiv:2408.15066v2 Announce Type: replace \nAbstract: Large language models (LLMs) are now accessible to anyone with a computer, a web browser, and an internet connection via browser-based interfaces, shifting the dynamics of participation in AI development. This article examines how interactive feedback features in ChatGPT's interface afford user participation in LLM iteration. Drawing on a survey of early ChatGPT users and applying the mechanisms and conditions framework of affordances, we analyse how these features shape user input. Our analysis indicates that these features encourage simple, frequent, and performance-focused feedback while discouraging collective input and discussions among users. Drawing on participatory design literature, we argue such constraints, if replicated across broader user bases, risk reinforcing power imbalances between users, the public, and companies developing LLMs. Our analysis contributes to the growing literature on participatory AI by critically examining the limitations of existing feedback processes and proposing directions for redesign. Rather than focusing solely on aligning model outputs with specific user preferences, we advocate for creating infrastructure that supports sustained dialogue about the purpose and applications of LLMs. This approach requires attention to the ongoing work of \"infrastructuring\" - creating and sustaining the social, technical, and institutional structures necessary to address matters of concern to stakeholders impacted by LLM development and deployment.</article>","contentLength":1550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parallel Unlearning in Inherited Model Networks","url":"https://arxiv.org/abs/2408.08493","date":1761883200,"author":"","guid":322808,"unread":true,"content":"<article>arXiv:2408.08493v3 Announce Type: replace \nAbstract: Unlearning is challenging in generic learning frameworks with the continuous growth and updates of models exhibiting complex inheritance relationships. This paper presents a novel unlearning framework that enables fully parallel unlearning among models exhibiting inheritance. We use a chronologically Directed Acyclic Graph (DAG) to capture various unlearning scenarios occurring in model inheritance networks. Central to our framework is the Fisher Inheritance Unlearning (FIUn) method, designed to enable efficient parallel unlearning within the DAG. FIUn utilizes the Fisher Information Matrix (FIM) to assess the significance of model parameters for unlearning tasks and adjusts them accordingly. To handle multiple unlearning requests simultaneously, we propose the Merging-FIM (MFIM) function, which consolidates FIMs from multiple upstream models into a unified matrix. This design supports all unlearning scenarios captured by the DAG, enabling one-shot removal of inherited knowledge while significantly reducing computational overhead. Experiments confirm the effectiveness of our unlearning framework. For single-class tasks, it achieves complete unlearning with 0% accuracy for unlearned labels while maintaining 94.53% accuracy for retained labels. For multi-class tasks, the accuracy is 1.07% for unlearned labels and 84.77% for retained labels. Our framework accelerates unlearning by 99% compared to alternative methods. Code is in https://github.com/MJLee00/Parallel-Unlearning-in-Inherited-Model-Networks.</article>","contentLength":1577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural active manifolds: nonlinear dimensionality reduction for uncertainty quantification","url":"https://arxiv.org/abs/2408.03534","date":1761883200,"author":"","guid":322809,"unread":true,"content":"<article>arXiv:2408.03534v2 Announce Type: replace \nAbstract: We present a new approach for nonlinear dimensionality reduction, specifically designed for computationally expensive mathematical models. We leverage autoencoders to discover a one-dimensional neural active manifold (NeurAM) capturing the model output variability, through the aid of a simultaneously learnt surrogate model with inputs on this manifold. Our method only relies on model evaluations and does not require the knowledge of gradients. The proposed dimensionality reduction framework can then be applied to assist outer loop many-query tasks in scientific computing, like sensitivity analysis and multifidelity uncertainty propagation. In particular, we prove, both theoretically under idealized conditions, and numerically in challenging test cases, how NeurAM can be used to obtain multifidelity sampling estimators with reduced variance by sampling the models on the discovered low-dimensional and shared manifold among models. Several numerical examples illustrate the main features of the proposed dimensionality reduction strategy and highlight its advantages with respect to existing approaches in the literature.</article>","contentLength":1185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Maven-Hijack: Software Supply Chain Attack Exploiting Packaging Order","url":"https://arxiv.org/abs/2407.18760","date":1761883200,"author":"","guid":322810,"unread":true,"content":"<article>arXiv:2407.18760v4 Announce Type: replace \nAbstract: Java projects frequently rely on package managers such as Maven to manage complex webs of external dependencies. While these tools streamline development, they also introduce subtle risks to the software supply chain. In this paper, we present Maven-Hijack, a novel attack that exploits the order in which Maven packages dependencies and the way the Java Virtual Machine resolves classes at runtime. By injecting a malicious class with the same fully qualified name as a legitimate one into a dependency that is packaged earlier, an attacker can silently override core application behavior without modifying the main codebase or library names. We demonstrate the real-world feasibility of this attack by compromising the Corona-Warn-App, a widely used open-source COVID-19 contact tracing system, and gaining control over its database connection logic. We evaluate three mitigation strategies, such as sealed JARs, Java Modules, and the Maven Enforcer plugin. Our results show that, while Java Modules offer strong protection, the Maven Enforcer plugin with duplicate class detection provides the most practical and effective defense for current Java projects. These findings highlight the urgent need for improved safeguards in Java's build and dependency management processes to prevent stealthy supply chain attacks.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI's Social Forcefield: Reshaping Distributed Cognition in Human-AI Teams","url":"https://arxiv.org/abs/2407.17489","date":1761883200,"author":"","guid":322811,"unread":true,"content":"<article>arXiv:2407.17489v2 Announce Type: replace \nAbstract: AI is not only a neutral tool in team settings; it actively reshapes the social and cognitive fabric of collaboration. We advance a unified framework of alignment in distributed cognition in human-AI teams -- a process through which linguistic, cognitive, and social coordination emerge as human and AI agents co-construct a shared representational space. Across two studies, we show that exposure to AI-generated language shapes not only how people speak, but also how they think, what they attend to, and how they relate to each other. Together, these findings reveal how AI participation reorganizes the distributed cognitive architecture of teams: AI systems function as implicit social forcefields. Our findings highlight the double-edged impact of AI: the same mechanisms that enable efficient collaboration can also erode epistemic diversity and undermine natural alignment processes. We argue for rethinking AI in teams as a socially influential actor and call for new design paradigms that foreground transparency, controllability, and group-level dynamics to foster responsible, productive human-AI collaboration.</article>","contentLength":1176,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NerfBaselines: Consistent and Reproducible Evaluation of Novel View Synthesis Methods","url":"https://arxiv.org/abs/2406.17345","date":1761883200,"author":"","guid":322812,"unread":true,"content":"<article>arXiv:2406.17345v2 Announce Type: replace \nAbstract: Novel view synthesis is an important problem with many applications, including AR/VR, gaming, and robotic simulations. With the recent rapid development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) methods, it is becoming difficult to keep track of the current state of the art (SoTA) due to methods using different evaluation protocols, codebases being difficult to install and use, and methods not generalizing well to novel 3D scenes. In our experiments, we show that even tiny differences in the evaluation protocols of various methods can artificially boost the performance of these methods. This raises questions about the validity of quantitative comparisons performed in the literature. To address these questions, we propose NerfBaselines, an evaluation framework which provides consistent benchmarking tools, ensures reproducibility, and simplifies the installation and use of various methods. We validate our implementation experimentally by reproducing the numbers reported in the original papers. For improved accessibility, we release a web platform that compares commonly used methods on standard benchmarks. We strongly believe NerfBaselines is a valuable contribution to the community as it ensures that quantitative results are comparable and thus truly measure progress in the field of novel view synthesis.</article>","contentLength":1401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor Generation","url":"https://arxiv.org/abs/2406.15863","date":1761883200,"author":"","guid":322813,"unread":true,"content":"<article>arXiv:2406.15863v3 Announce Type: replace \nAbstract: Text-to-image diffusion models can generate realistic images based on textual inputs, enabling users to convey their opinions visually through language. Meanwhile, within language, emotion plays a crucial role in expressing personal opinions in our daily lives and the inclusion of maliciously negative content can lead users astray, exacerbating negative emotions. Recognizing the success of diffusion models and the significance of emotion, we investigate a previously overlooked risk associated with text-to-image diffusion models, that is, utilizing emotion in the input texts to introduce negative content and provoke unfavorable emotions in users. Specifically, we identify a new backdoor attack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces malicious negative content triggered by emotional texts during image generation. We formulate such an attack as a diffusion personalization problem to avoid extensive model retraining and propose the EmoBooth. Unlike existing personalization methods, our approach fine-tunes a pre-trained diffusion model by establishing a mapping between a cluster of emotional words and a given reference image containing malicious negative content. To validate the effectiveness of our method, we built a dataset and conducted extensive analysis and discussion about its effectiveness. Given consumers' widespread use of diffusion models, uncovering this threat is critical for society.</article>","contentLength":1494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentiable Programming for Differential Equations: A Review","url":"https://arxiv.org/abs/2406.09699","date":1761883200,"author":"","guid":322814,"unread":true,"content":"<article>arXiv:2406.09699v2 Announce Type: replace \nAbstract: The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.</article>","contentLength":1634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A mathematical certification for positivity conditions in Neural Networks with applications to partial monotonicity and Trustworthy AI","url":"https://arxiv.org/abs/2406.08525","date":1761883200,"author":"","guid":322815,"unread":true,"content":"<article>arXiv:2406.08525v2 Announce Type: replace \nAbstract: Artificial Neural Networks (ANNs) have become a powerful tool for modeling complex relationships in large-scale datasets. However, their black-box nature poses trustworthiness challenges. In certain situations, ensuring trust in predictions might require following specific partial monotonicity constraints. However, certifying if an already-trained ANN is partially monotonic is challenging. Therefore, ANNs are often disregarded in some critical applications, such as credit scoring, where partial monotonicity is required. To address this challenge, this paper presents a novel algorithm (LipVor) that certifies if a black-box model, such as an ANN, is positive based on a finite number of evaluations. Consequently, since partial monotonicity can be expressed as a positivity condition on partial derivatives, LipVor can certify whether an ANN is partially monotonic. To do so, for every positively evaluated point, the Lipschitzianity of the black-box model is used to construct a specific neighborhood where the function remains positive. Next, based on the Voronoi diagram of the evaluated points, a sufficient condition is stated to certify if the function is positive in the domain. Unlike prior methods, our approach certifies partial monotonicity without constrained architectures or piece-wise linear activations. Therefore, LipVor could open up the possibility of using unconstrained ANN in some critical fields. Moreover, some other properties of an ANN, such as convexity, can be posed as positivity conditions, and therefore, LipVor could also be applied.</article>","contentLength":1624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models","url":"https://arxiv.org/abs/2406.05948","date":1761883200,"author":"","guid":322816,"unread":true,"content":"<article>arXiv:2406.05948v4 Announce Type: replace \nAbstract: Large Language Models (LLMs), especially those accessed via APIs, have demonstrated impressive capabilities across various domains. However, users without technical expertise often turn to (untrustworthy) third-party services, such as prompt engineering, to enhance their LLM experience, creating vulnerabilities to adversarial threats like backdoor attacks. Backdoor-compromised LLMs generate malicious outputs to users when inputs contain specific \"triggers\" set by attackers. Traditional defense strategies, originally designed for small-scale models, are impractical for API-accessible LLMs due to limited model access, high computational costs, and data requirements. To address these limitations, we propose Chain-of-Scrutiny (CoS) which leverages LLMs' unique reasoning abilities to mitigate backdoor attacks. It guides the LLM to generate reasoning steps for a given input and scrutinizes for consistency with the final output -- any inconsistencies indicating a potential attack. It is well-suited for the popular API-only LLM deployments, enabling detection at minimal cost and with little data. User-friendly and driven by natural language, it allows non-experts to perform the defense independently while maintaining transparency. We validate the effectiveness of CoS through extensive experiments on various tasks and LLMs, with results showing greater benefits for more powerful LLMs.</article>","contentLength":1451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chaos-based reinforcement learning with TD3","url":"https://arxiv.org/abs/2405.09086","date":1761883200,"author":"","guid":322817,"unread":true,"content":"<article>arXiv:2405.09086v2 Announce Type: replace \nAbstract: Chaos-based reinforcement learning (CBRL) is a method in which the agent's internal chaotic dynamics drives exploration. However, the learning algorithms in CBRL have not been thoroughly developed in previous studies, nor have they incorporated recent advances in reinforcement learning. This study introduced Twin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the state-of-the-art deep reinforcement learning algorithms that can treat deterministic and continuous action spaces, to CBRL. The validation results provide several insights. First, TD3 works as a learning algorithm for CBRL in a simple goal-reaching task. Second, CBRL agents with TD3 can autonomously suppress their exploratory behavior as learning progresses and resume exploration when the environment changes. Finally, examining the effect of the agent's chaoticity on learning shows that there exists a suitable range of chaos strength in the agent's model to flexibly switch between exploration and exploitation and adapt to environmental changes.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Modelling for Cyber-Physical Systems: A Systematic Literature Review","url":"https://arxiv.org/abs/2404.07527","date":1761883200,"author":"","guid":322818,"unread":true,"content":"<article>arXiv:2404.07527v3 Announce Type: replace \nAbstract: Cyber-physical systems are at the intersection of digital technology and engineering domains, rendering them high-value targets of sophisticated and well-funded cybersecurity threat actors. Prominent cybersecurity attacks on CPS have brought attention to the vulnerability of these systems and the inherent weaknesses of critical infrastructure reliant on them. Security modelling for CPS is an important mechanism to systematically identify and assess vulnerabilities, threats, and risks throughout system life cycles, and to ultimately ensure system resilience, safety, and reliability. This survey delves into state-of-the-art research on CPS security modelling, encompassing both threat and attack modelling. While these terms are sometimes used interchangeably, they are different concepts. This paper elaborates on the differences between threat and attack modelling, examining their implications for CPS security. We conducted a systematic search that yielded 449 papers, from which 32 were selected and categorised into three clusters: those focused on threat modelling methods, attack modelling methods, and literature reviews. Specifically, we sought to examine what security modelling methods exist today, and how they address real-world cybersecurity threats and CPS-specific attacker capabilities throughout the life cycle of CPS, which typically span longer durations compared to traditional IT systems. This paper also highlights several limitations in existing research, wherein security models adopt simplistic approaches that do not adequately consider the dynamic, multi-layer, multi-path, and multi-agent characteristics of real-world cyber-physical attacks.</article>","contentLength":1731,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks","url":"https://arxiv.org/abs/2404.00176","date":1761883200,"author":"","guid":322819,"unread":true,"content":"<article>arXiv:2404.00176v2 Announce Type: replace \nAbstract: Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task, which is usually operationalized based on two subsequently applied usage-level tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages. Then, these labels are represented in a graph on which Word Sense Induction (WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by comparing sense clusters over time. This modularity is reflected in most LSCD datasets and models. It also leads to a large heterogeneity in modeling options and task definitions, which is exacerbated by a variety of dataset versions, preprocessing options and evaluation metrics. This heterogeneity makes it difficult to evaluate models under comparable conditions, to choose optimal model combinations or to reproduce results. Hence, we provide a benchmark repository standardizing LSCD evaluation. Through transparent implementation results become easily reproducible and by standardization different components can be freely combined. The repository reflects the task's modularity by allowing model evaluation for WiC, WSI and LSCD. This allows for careful evaluation of increasingly complex model components providing new ways of model optimization. We use the implemented benchmark to conduct a number of experiments with recent models and systematically improve the state-of-the-art.</article>","contentLength":1426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Symmetric Positive Definite (SPD) coarse spaces perform well for indefinite Helmholtz problems?","url":"https://arxiv.org/abs/2403.18378","date":1761883200,"author":"","guid":322820,"unread":true,"content":"<article>arXiv:2403.18378v3 Announce Type: replace \nAbstract: The purpose of this work is to improve the estimates for the $\\Delta$-GenEO method from the paper \"Overlapping Schwarz methods with GenEO coarse spaces for indefinite and nonself-adjoint problems\" by N. Bootland, V. Dolean, I. G Graham, C. Ma, R. Scheichl (https://doi.org/10.1093/imanum/drac036) when applied to the indefinite Helmholtz equation. We derive k-dependent estimates of quantities of interest ensuring the robustness of the method.</article>","contentLength":497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VerifIoU - Robustness of Object Detection to Perturbations","url":"https://arxiv.org/abs/2403.08788","date":1761883200,"author":"","guid":322821,"unread":true,"content":"<article>arXiv:2403.08788v2 Announce Type: replace \nAbstract: We introduce a novel Interval Bound Propagation (IBP) approach for the formal verification of object detection models, specifically targeting the Intersection over Union (IoU) metric. The approach has been implemented in an open source code, named IBP IoU, compatible with popular abstract interpretation based verification tools. The resulting verifier is evaluated on landing approach runway detection and handwritten digit recognition case studies. Comparisons against a baseline (Vanilla IBP IoU) highlight the superior performance of IBP IoU in ensuring accuracy and stability, contributing to more secure and robust machine learning applications.</article>","contentLength":705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Algorithm for Fast and Correct Computation of Reeb Spaces for PL Bivariate Fields","url":"https://arxiv.org/abs/2403.06564","date":1761883200,"author":"","guid":322822,"unread":true,"content":"<article>arXiv:2403.06564v3 Announce Type: replace \nAbstract: The Reeb space is a fundamental data structure in computational topology that represents the fiber topology of a multi-field (or multiple scalar fields), extending the level set topology of a scalar field. Efficient algorithms have been designed for computing Reeb graphs, however, computing correct Reeb spaces for PL bivariate fields, is a challenging open problem. There are only a few implementable algorithms in the literature for computing Reeb space or its approximation via range quantization or by computing a Jacobi fiber surface which are computationally expensive or have correctness issues, i.e., the computed Reeb space may not be topologically equivalent or homeomorphic to the actual Reeb space. In the current paper, we propose a novel algorithm for fast and correct computation of the Reeb space corresponding to a generic PL bivariate field defined on a triangulation $\\mathbb{M}$ of a $3$-manifold without boundary, leveraging the fast algorithms for computing Reeb graphs in the literature.\n  Our algorithm is based on the computation of a Multi-Dimensional Reeb Graph (MDRG) which is first proved to be homeomorphic with the Reeb space. For the correct computation of the MDRG, we compute the Jacobi set of the PL bivariate field and its projection into the Reeb space, called the Jacobi structure. Finally, the correct Reeb space is obtained by computing a net-like structure embedded in the Reeb space and then computing its $2$-sheets in the net-like structure. The time complexity of our algorithm is $\\mathcal{O}(n^2 + n(c_{int})\\log (n) + nc_L^2)$, where $n$ is the total number of simplices in $\\mathbb{M}$, $c_{int}$ is the number of intersections of the projections of the non-adjacent Jacobi set edges on the range of the bivariate field and $c_L$ is the upper bound on the number of simplices in the link of an edge of $\\mathbb{M}$.</article>","contentLength":1918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time Weaver: A Conditional Time Series Generation Model","url":"https://arxiv.org/abs/2403.02682","date":1761883200,"author":"","guid":322823,"unread":true,"content":"<article>arXiv:2403.02682v2 Announce Type: replace \nAbstract: Imagine generating a city's electricity demand pattern based on weather, the presence of an electric vehicle, and location, which could be used for capacity planning during a winter freeze. Such real-world time series are often enriched with paired heterogeneous contextual metadata (e.g., weather and location). Current approaches to time series generation often ignore this paired metadata. Additionally, the heterogeneity in metadata poses several practical challenges in adapting existing conditional generation approaches from the image, audio, and video domains to the time series domain. To address this gap, we introduce TIME WEAVER, a novel diffusion-based model that leverages the heterogeneous metadata in the form of categorical, continuous, and even time-variant variables to significantly improve time series generation. Additionally, we show that naive extensions of standard evaluation metrics from the image to the time series domain are insufficient. These metrics do not penalize conditional generation approaches for their poor specificity in reproducing the metadata-specific features in the generated time series. Thus, we innovate a novel evaluation metric that accurately captures the specificity of conditional generation and the realism of the generated time series. We show that TIME WEAVER outperforms state-of-the-art benchmarks, such as Generative Adversarial Networks (GANs), by up to 30% in downstream classification tasks on real-world energy, medical, air quality, and traffic datasets.</article>","contentLength":1573,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"High Performance Distributed Control for Large-Scale Linear Systems: A Cover-Based Distributed Observer Approach","url":"https://arxiv.org/abs/2402.06903","date":1761883200,"author":"","guid":322824,"unread":true,"content":"<article>arXiv:2402.06903v4 Announce Type: replace \nAbstract: In recent years, the distributed-observer-based distributed control law has shown powerful ability to arbitrarily approximate the centralized control performance. However, the traditional distributed observer requires each local observer to reconstruct the state information of the whole system, which is unrealistic for large-scale scenarios. To fill this gap, This paper presents a coverage solution algorithm for large-scale systems that accounts for both physical and communication network characteristics, which can significantly reduce the dimension of local observers. Then, the cover-based distributed observer for large-scale systems is proposed to overcome the problem that the system dynamics are difficult to estimate due to the coupling between cover sets. Furthermore, the two-layer Lyapunov analysis method is adopted and the dynamic transformation lemma of compact errors is proved, which solves the problem of analyzing stability of the error dynamic of the cover-based distributed observer. Finally, it is proved that the distributed control law based on the cover-based distributed observer can also arbitrarily approximate the control performance of the centralized control law, and the dimension of the local observer is greatly reduced compared with the traditional method. The simulation results show the validity of the developed theories.</article>","contentLength":1416,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SafEDMD: A Koopman-based data-driven controller design framework for nonlinear dynamical systems","url":"https://arxiv.org/abs/2402.03145","date":1761883200,"author":"","guid":322825,"unread":true,"content":"<article>arXiv:2402.03145v4 Announce Type: replace \nAbstract: The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose SafEDMD, a novel stability- and feedback-oriented EDMD-based controller design framework. Our approach leverages a reliable surrogate model generated in a data-driven fashion in order to provide closed-loop guarantees. In particular, we establish a controller design based on semi-definite programming with guaranteed stabilization of the underlying nonlinear system. As central ingredient, we derive proportional error bounds that vanish at the origin and are tailored to control tasks. We illustrate the developed method by means of several benchmark examples and highlight the advantages over state-of-the-art methods.</article>","contentLength":903,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Traceback Learning for Medical Report Generation","url":"https://arxiv.org/abs/2401.13267","date":1761883200,"author":"","guid":322826,"unread":true,"content":"<article>arXiv:2401.13267v4 Announce Type: replace \nAbstract: Automated medical report generation has demonstrated the potential to significantly reduce the workload associated with time-consuming medical reporting. Recent generative representation learning methods have shown promise in integrating vision and language modalities for medical report generation. However, when trained end-to-end and applied directly to medical image-to-text generation, they face two significant challenges: i) difficulty in accurately capturing subtle yet crucial pathological details, and ii) reliance on both visual and textual inputs during inference, leading to performance degradation in zero-shot inference when only images are available. To address these challenges, this study proposes a novel multimodal dynamic traceback learning framework (DTrace). Specifically, we introduce a traceback mechanism to supervise the semantic validity of generated content and a dynamic learning strategy to adapt to various proportions of image and text input, enabling text generation without strong reliance on the input from both modalities during inference. The learning of cross-modal knowledge is enhanced by supervising the model to recover masked semantic information from a complementary counterpart. Extensive experiments conducted on two benchmark datasets, IU-Xray and MIMIC-CXR, demonstrate that the proposed DTrace framework outperforms state-of-the-art methods for medical report generation.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GSE: Group-wise Sparse and Explainable Adversarial Attacks","url":"https://arxiv.org/abs/2311.17434","date":1761883200,"author":"","guid":322827,"unread":true,"content":"<article>arXiv:2311.17434v5 Announce Type: replace \nAbstract: Sparse adversarial attacks fool deep neural networks (DNNs) through minimal pixel perturbations, often regularized by the $\\ell_0$ norm. Recent efforts have replaced this norm with a structural sparsity regularizer, such as the nuclear group norm, to craft group-wise sparse adversarial attacks. The resulting perturbations are thus explainable and hold significant practical relevance, shedding light on an even greater vulnerability of DNNs. However, crafting such attacks poses an optimization challenge, as it involves computing norms for groups of pixels within a non-convex objective. We address this by presenting a two-phase algorithm that generates group-wise sparse attacks within semantically meaningful areas of an image. Initially, we optimize a quasinorm adversarial loss using the $1/2-$quasinorm proximal operator tailored for non-convex programming. Subsequently, the algorithm transitions to a projected Nesterov's accelerated gradient descent with $2-$norm regularization applied to perturbation magnitudes. Rigorous evaluations on CIFAR-10 and ImageNet datasets demonstrate a remarkable increase in group-wise sparsity, e.g., $50.9\\%$ on CIFAR-10 and $38.4\\%$ on ImageNet (average case, targeted attack). This performance improvement is accompanied by significantly faster computation times, improved explainability, and a $100\\%$ attack success rate.</article>","contentLength":1424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quality-Aware Prototype Memory for Face Representation Learning","url":"https://arxiv.org/abs/2311.07734","date":1761883200,"author":"","guid":322828,"unread":true,"content":"<article>arXiv:2311.07734v2 Announce Type: replace \nAbstract: Prototype Memory is a powerful model for face representation learning. It enables training face recognition models on datasets of any size by generating prototypes (classifier weights) on the fly and efficiently utilizing them. Prototype Memory demonstrated strong results in many face recognition benchmarks. However, the algorithm of prototype generation, used in it, is prone to the problems of imperfectly calculated prototypes in case of low-quality or poorly recognizable faces in the images, selected for the prototype creation. All images of the same person presented in the mini-batch are used with equal weights, and the resulting averaged prototype can be contaminated by imperfect embeddings of low-quality face images. This may lead to misleading training signals and degrade the performance of the trained models. In this paper, we propose a simple and effective way to improve Prototype Memory with quality-aware prototype generation. Quality-Aware Prototype Memory uses different weights for images of different quality in the process of prototype generation. With this improvement, prototypes receive more informative signals from high-quality images and are less affected by low-quality ones. We propose and compare several methods of quality estimation and usage, perform extensive experiments on the different face recognition benchmarks and demonstrate the advantages of the proposed model compared to the basic version of Prototype Memory.</article>","contentLength":1514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning","url":"https://arxiv.org/abs/2311.06736","date":1761883200,"author":"","guid":322829,"unread":true,"content":"<article>arXiv:2311.06736v3 Announce Type: replace \nAbstract: Logical reasoning is a pivotal component in the field of artificial intelligence. Proof planning, particularly in contexts requiring the validation of explanation accuracy, continues to present challenges. The recent advancement of large language models (LLMs) has led to significant progress in natural language proof planning, evolving from one-stage generators to more complex three-stage systems that include additional searchers or verifiers. While these assisted methods improve the quality of generated results, they also introduce increased search efforts and computational costs. Furthermore, the generative process itself remains underexplored. In this study, we propose a stepwise decoding approach augmented by contrastive learning to address two common errors encountered during the LLM generator's decoding process. We fine-tune the language model using both vanilla and enhanced hard negatives to mitigate these decoding errors. Empirical results demonstrate the effectiveness of our strategy. Additionally, our further analysis reveals that even larger LLMs still struggle to generate rigorous logical chains.</article>","contentLength":1178,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Direct Access for Conjunctive Queries with Negations","url":"https://arxiv.org/abs/2310.15800","date":1761883200,"author":"","guid":322830,"unread":true,"content":"<article>arXiv:2310.15800v3 Announce Type: replace \nAbstract: Given a conjunctive query $Q$ and a database $D$, a direct access to the answers of $Q$ over $D$ is the operation of returning, given an index $k$, the $k$-th answer for some order on its answers. While this problem is #P-hard in general with respect to combined complexity, many conjunctive queries have an underlying structure that allows for a direct access to their answers for some lexicographical ordering that takes polylogarithmic time in the size of the database after a polynomial time precomputation. Previous work has precisely characterised the tractable classes and given fine-grained lower bounds on the precomputation time needed depending on the structure of the query.\n  In this paper, we generalise these tractability results to the case of signed conjunctive queries, that is, conjunctive queries that may contain negative atoms. Our technique is based on a class of circuits that can represent relational data. We first show that this class supports tractable direct access after a polynomial time preprocessing. We then give bounds on the size of the circuit needed to represent the answer set of signed conjunctive queries depending on their structure. Both results combined together allow us to prove the tractability of direct access for a large class of conjunctive queries. On the one hand, we recover the known tractable classes from the literature in the case of positive conjunctive queries. On the other hand, we generalise and unify known tractability results about negative conjunctive queries -- that is, queries having only negated atoms. In particular, we show that the class of $\\beta$-acyclic negative conjunctive queries and the class of bounded nest set width negative conjunctive queries admit tractable direct access.</article>","contentLength":1812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Doubly Fair Parity Games","url":"https://arxiv.org/abs/2310.13612","date":1761883200,"author":"","guid":322831,"unread":true,"content":"<article>arXiv:2310.13612v2 Announce Type: replace \nAbstract: We consider two-player games over finite graphs in which both players are restricted by fairness constraints on their moves. Given a two player game graph $G=(V,E)$ and a set of fair moves $E_f\\subseteq E$ a player is said to play \"fair\" in $G$ if they choose an edge $e \\in E_f$ infinitely often whenever the source vertex of $e$ is visited infinitely often. Otherwise, they play \"unfair\". We equip such games with two $\\omega$-regular winning conditions $\\alpha$ and $\\beta$ deciding the winner of mutually fair and mutually unfair plays, respectively. Whenever one player plays fair and the other plays unfair, the fairly playing player wins the game. The resulting games are called \"fair $\\alpha/\\beta$ games\".\n  We formalize fair $\\alpha/\\beta$ games and show that they are determined. For fair parity/parity games, i.e., fair $\\alpha/\\beta$ games where $\\alpha$ and $\\beta$ are given each by a parity condition over $G$, we provide a polynomial reduction to (normal) parity games via a gadget construction inspired by the reduction of stochastic parity games to parity games. We further give a direct symbolic fixpoint algorithm to solve fair parity/parity games. On a conceptual level, we illustrate the translation between the gadget-based reduction and the direct symbolic algorithm which uncovers the underlying similarities of solution algorithms for fair and stochastic parity games, as well as for the recently considered class of fair games where only one player is restricted by fair moves.</article>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monte-Carlo/Moments micro-macro Parareal method for unimodal and bimodal scalar McKean-Vlasov SDEs","url":"https://arxiv.org/abs/2310.11365","date":1761883200,"author":"","guid":322832,"unread":true,"content":"<article>arXiv:2310.11365v3 Announce Type: replace \nAbstract: We propose a micro-macro parallel-in-time Parareal method for scalar McKean-Vlasov stochastic differential equations (SDEs). In the algorithm, the fine Parareal propagator is a Monte Carlo simulation of an ensemble of particles, while an approximate ordinary differential equation (ODE) description of the mean and the variance of the particle distribution is used as a coarse Parareal propagator to achieve speedup. We analyse the convergence behaviour of our method for a linear problem and provide numerical experiments indicating the parallel weak scaling of the algorithm on a set of examples. We show, with numerical experiments, that convergence typically takes place in a low number of iterations, depending on the quality of the ODE predictor. For bimodal SDEs, we avoid quality deterioration of the coarse predictor (compared to unimodal SDEs) through the usage of multiple ODEs, each describing the mean and variance of the particle distribution in locally unimodal regions of the phase space. The benefit of the proposed algorithm can be viewed through two lenses: (i) through the parallel-in-time lens, speedup is obtained through the use of a very cheap coarse integrator (an ODE moment model), and (ii) through the moment models lens, accuracy is iteratively gained through the use of parallel machinery as a corrector. In contrast to the isolated use of a moment model, the proposed method (iteratively) converges to the true distribution generated by the SDE.</article>","contentLength":1529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Runtime Repeated Recursion Unfolding in CHR: A Just-In-Time Online Program Optimization Strategy That Can Achieve Super-Linear Speedup","url":"https://arxiv.org/abs/2307.02180","date":1761883200,"author":"","guid":322833,"unread":true,"content":"<article>arXiv:2307.02180v5 Announce Type: replace \nAbstract: We introduce a just-in-time runtime program transformation strategy based on repeated recursion unfolding. Our online program optimization generates several versions of a recursion differentiated by the minimal number of recursive steps covered. The base case of the recursion is ignored in our technique.\n  Our method is introduced here on the basis of single linear direct recursive rules. When a recursive call is encountered at runtime, first an unfolder creates specializations of the associated recursive rule on-the-fly and then an interpreter applies these rules to the call. Our approach reduces the number of recursive rule applications to its logarithm at the expense of introducing a logarithmic number of generic unfolded rules.\n  We prove correctness of our online optimization technique and determine its time complexity. For recursions which have enough simplifyable unfoldings, a super-linear is possible, i.e. speedup by more than a constant factor. The necessary simplification is problem-specific and has to be provided at compile-time. In our speedup analysis, we prove a sufficient condition as well as a sufficient and necessary condition for super-linear speedup relating the complexity of the recursive steps of the original rule and the unfolded rules.\n  We have implemented an unfolder and meta-interpreter for runtime repeated recursion unfolding with just five rules in Constraint Handling Rules (CHR) embedded in Prolog. We illustrate the feasibility of our approach with simplifications, time complexity results and benchmarks for some basic tractable algorithms. The simplifications require some insight and were derived manually. The runtime improvement quickly reaches several orders of magnitude, consistent with the super-linear speedup predicted by our theorems.</article>","contentLength":1852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reward Collapse in Aligning Large Language Models","url":"https://arxiv.org/abs/2305.17608","date":1761883200,"author":"","guid":322834,"unread":true,"content":"<article>arXiv:2305.17608v2 Announce Type: replace \nAbstract: The extraordinary capabilities of large language models (LLMs) such as ChatGPT and GPT-4 are in part unleashed by aligning them with reward models that are trained on human preferences, which are often represented as rankings of responses to prompts. In this paper, we document the phenomenon of \\textit{reward collapse}, an empirical observation where the prevailing ranking-based approach results in an \\textit{identical} reward distribution \\textit{regardless} of the prompts during the terminal phase of training. This outcome is undesirable as open-ended prompts like ``write a short story about your best friend'' should yield a continuous range of rewards for their completions, while specific prompts like ``what is the capital of New Zealand'' should generate either high or low rewards. Our theoretical investigation reveals that reward collapse is primarily due to the insufficiency of the ranking-based objective function to incorporate prompt-related information during optimization. This insight allows us to derive closed-form expressions for the reward distribution associated with a set of utility functions in an asymptotic regime. To overcome reward collapse, we introduce a prompt-aware optimization scheme that provably admits a prompt-dependent reward distribution within the interpolating regime. Our experimental results suggest that our proposed prompt-aware utility functions significantly alleviate reward collapse during the training of reward models.</article>","contentLength":1532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rewriting Modulo Traced Comonoid Structure","url":"https://arxiv.org/abs/2302.09631","date":1761883200,"author":"","guid":322835,"unread":true,"content":"<article>arXiv:2302.09631v5 Announce Type: replace \nAbstract: In this paper we adapt previous work on rewriting string diagrams using hypergraphs to the case where the underlying category has a traced comonoid structure, in which wires can be forked and the outputs of a morphism can be connected to its input. Such a structure is particularly interesting because any traced Cartesian (dataflow) category has an underlying traced comonoid structure. We show that certain subclasses of hypergraphs are fully complete for traced comonoid categories: that is to say, every term in such a category has a unique corresponding hypergraph up to isomorphism, and from every hypergraph with the desired properties, a unique term in the category can be retrieved up to the axioms of traced comonoid categories. We also show how the framework of double pushout rewriting (DPO) can be adapted for traced comonoid categories by characterising the valid pushout complements for rewriting in our setting. We conclude by presenting a case study in the form of recent work on an equational theory for sequential circuits: circuits built from primitive logic gates with delay and feedback. The graph rewriting framework allows for the definition of an operational semantics for sequential circuits.</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two Heads are Better than One: Robust Learning Meets Multi-branch Models","url":"https://arxiv.org/abs/2208.08083","date":1761883200,"author":"","guid":322836,"unread":true,"content":"<article>arXiv:2208.08083v3 Announce Type: replace \nAbstract: Deep neural networks (DNNs) are vulnerable to adversarial examples, in which DNNs are misled to false outputs due to inputs containing imperceptible perturbations. Adversarial training, a reliable and effective method of defense, may significantly reduce the vulnerability of neural networks and becomes the de facto standard for robust learning. While many recent works practice the data-centric philosophy, such as how to generate better adversarial examples or use generative models to produce additional training data, we look back to the models themselves and revisit the adversarial robustness from the perspective of deep feature distribution as an insightful complementarity. In this paper, we propose \\textit{Branch Orthogonality adveRsarial Training} (BORT) to obtain state-of-the-art performance with solely the original dataset for adversarial training. To practice our design idea of integrating multiple orthogonal solution spaces, we leverage a simple and straightforward multi-branch neural network that eclipses adversarial attacks with no increase in inference time. We heuristically propose a corresponding loss function, branch-orthogonal loss, to make each solution space of the multi-branch model orthogonal. We evaluate our approach on CIFAR-10, CIFAR-100 and SVHN against $\\ell_{\\infty}$ norm-bounded perturbations of size $\\epsilon = 8/255$, respectively. Exhaustive experiments are conducted to show that our method goes beyond all state-of-the-art methods without any tricks. Compared to all methods that do not use additional data for training, our models achieve 67.3\\% and 41.5\\% robust accuracy on CIFAR-10 and CIFAR-100 (improving upon the state-of-the-art by +7.23\\% and +9.07\\%). We also outperform methods using a training set with a far larger scale than ours.</article>","contentLength":1849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measuring the Availability and Response Times of Public Encrypted DNS Resolvers","url":"https://arxiv.org/abs/2208.04999","date":1761883200,"author":"","guid":322837,"unread":true,"content":"<article>arXiv:2208.04999v2 Announce Type: replace \nAbstract: Unencrypted DNS traffic between users and DNS resolvers can lead to privacy and security concerns. In response to these privacy risks, many browser vendors have deployed DNS-over-HTTPS (DoH) to encrypt queries between users and DNS resolvers. Today, many client-side deployments of DoH, particularly in browsers, select between only a few resolvers, despite the fact that many more encrypted DNS resolvers are deployed in practice. Unfortunately, if users only have a few choices of encrypted resolver, and only a few perform well from any particular vantage point, then the privacy problems that DoH was deployed to help address merely shift to a different set of third parties. It is thus important to assess the performance characteristics of more encrypted DNS resolvers, to determine how many options for encrypted DNS resolvers users tend to have in practice. In this paper, we explore the performance of a large group of encrypted DNS resolvers supporting DoH by measuring DNS query response times from global vantage points in North America, Europe, and Asia. Our results show that many non-mainstream resolvers have higher response times than mainstream resolvers, particularly for non-mainstream resolvers that are queried from more distant vantage points -- suggesting that most encrypted DNS resolvers are not replicated or anycast. In some cases, however, certain non-mainstream resolvers perform at least as well as mainstream resolvers, suggesting that users may be able to use a broader set of encrypted DNS resolvers than those that are available in current browser configurations.</article>","contentLength":1651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Central Submonads and Notions of Computation: Soundness, Completeness and Internal Languages","url":"https://arxiv.org/abs/2207.09190","date":1761883200,"author":"","guid":322838,"unread":true,"content":"<article>arXiv:2207.09190v3 Announce Type: replace \nAbstract: Monads in category theory are algebraic structures that can be used to model computational effects in programming languages. We show how the notion of \"centre\", and more generally \"centrality\", i.e. the property for an effect to commute with all other effects, may be formulated for strong monads acting on symmetric monoidal categories. We identify three equivalent conditions which characterise the existence of the centre of a strong monad (some of which relate it to the premonoidal centre of Power and Robinson) and we show that every strong monad on many well-known naturally occurring categories does admit a centre, thereby showing that this new notion is ubiquitous. More generally, we study central submonads, which are necessarily commutative, just like the centre of a strong monad. We provide a computational interpretation by formulating equational theories of lambda calculi equipped with central submonads, we describe categorical models for these theories and prove soundness, completeness and internal language results for our semantics.</article>","contentLength":1108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-layer State Evolution Under Random Convolutional Design","url":"https://arxiv.org/abs/2205.13503","date":1761883200,"author":"","guid":322839,"unread":true,"content":"<article>arXiv:2205.13503v3 Announce Type: replace \nAbstract: Signal recovery under generative neural network priors has emerged as a promising direction in statistical inference and computational imaging. Theoretical analysis of reconstruction algorithms under generative priors is, however, challenging. For generative priors with fully connected layers and Gaussian i.i.d. weights, this was achieved by the multi-layer approximate message (ML-AMP) algorithm via a rigorous state evolution. However, practical generative priors are typically convolutional, allowing for computational benefits and inductive biases, and so the Gaussian i.i.d. weight assumption is very limiting. In this paper, we overcome this limitation and establish the state evolution of ML-AMP for random convolutional layers. We prove in particular that random convolutional layers belong to the same universality class as Gaussian matrices. Our proof technique is of an independent interest as it establishes a mapping between convolutional matrices and spatially coupled sensing matrices used in coding theory.</article>","contentLength":1077,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Theory for Causal Inference: Direct Debiased Machine Learning via Bregman-Riesz Regression","url":"https://arxiv.org/abs/2510.26783","date":1761883200,"author":"","guid":322840,"unread":true,"content":"<article>arXiv:2510.26783v1 Announce Type: cross \nAbstract: This note introduces a unified theory for causal inference that integrates Riesz regression, covariate balancing, density-ratio estimation (DRE), targeted maximum likelihood estimation (TMLE), and the matching estimator in average treatment effect (ATE) estimation. In ATE estimation, the balancing weights and the regression functions of the outcome play important roles, where the balancing weights are referred to as the Riesz representer, bias-correction term, and clever covariates, depending on the context. Riesz regression, covariate balancing, DRE, and the matching estimator are methods for estimating the balancing weights, where Riesz regression is essentially equivalent to DRE in the ATE context, the matching estimator is a special case of DRE, and DRE is in a dual relationship with covariate balancing. TMLE is a method for constructing regression function estimators such that the leading bias term becomes zero. Nearest Neighbor Matching is equivalent to Least Squares Density Ratio Estimation and Riesz Regression.</article>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MORE: Multi-Organ Medical Image REconstruction Dataset","url":"https://arxiv.org/abs/2510.26759","date":1761883200,"author":"","guid":322841,"unread":true,"content":"<article>arXiv:2510.26759v1 Announce Type: cross \nAbstract: CT reconstruction provides radiologists with images for diagnosis and treatment, yet current deep learning methods are typically limited to specific anatomies and datasets, hindering generalization ability to unseen anatomies and lesions. To address this, we introduce the Multi-Organ medical image REconstruction (MORE) dataset, comprising CT scans across 9 diverse anatomies with 15 lesion types. This dataset serves two key purposes: (1) enabling robust training of deep learning models on extensive, heterogeneous data, and (2) facilitating rigorous evaluation of model generalization for CT reconstruction. We further establish a strong baseline solution that outperforms prior approaches under these challenging conditions. Our results demonstrate that: (1) a comprehensive dataset helps improve the generalization capability of models, and (2) optimization-based methods offer enhanced robustness for unseen anatomies. The MORE dataset is freely accessible under CC-BY-NC 4.0 at our project page https://more-med.github.io/</article>","contentLength":1081,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neither Consent nor Property: A Policy Lab for Data Law","url":"https://arxiv.org/abs/2510.26727","date":1761883200,"author":"","guid":322842,"unread":true,"content":"<article>arXiv:2510.26727v1 Announce Type: cross \nAbstract: This paper makes the opaque data market in the AI economy empirically legible for the first time, constructing a computational testbed to address a core epistemic failure: regulators governing a market defined by structural opacity, fragile price discovery, and brittle technical safeguards that have paralyzed traditional empirics and fragmented policy. The pipeline begins with multi-year fieldwork to extract the market's hidden logic, and then embeds these grounded behaviors into a high-fidelity ABM, parameterized via a novel LLM-based discrete-choice experiment that captures the preferences of unsurveyable populations. The pipeline is validated against reality, reproducing observed trade patterns. This policy laboratory delivers clear, counter-intuitive results. First, property-style relief is a false promise: ''anonymous-data'' carve-outs expand trade but ignore risk, causing aggregate welfare to collapse once external harms are priced in. Second, social welfare peaks when the downstream buyer internalizes the full substantive risk. This least-cost avoider approach induces efficient safeguards, simultaneously raising welfare and sustaining trade, and provides a robust empirical foundation for the legal drift toward two-sided reachability. The contribution is a reproducible pipeline designed to end the reliance on intuition. It converts qualitative insight into testable, comparative policy experiments, obsoleting armchair conjecture by replacing it with controlled evidence on how legal rules actually shift risk and surplus. This is the forward-looking engine that moves the field from competing intuitions to direct, computational analysis.</article>","contentLength":1718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging the Gap between Empirical Welfare Maximization and Conditional Average Treatment Effect Estimation in Policy Learning","url":"https://arxiv.org/abs/2510.26723","date":1761883200,"author":"","guid":322843,"unread":true,"content":"<article>arXiv:2510.26723v1 Announce Type: cross \nAbstract: The goal of policy learning is to train a policy function that recommends a treatment given covariates to maximize population welfare. There are two major approaches in policy learning: the empirical welfare maximization (EWM) approach and the plug-in approach. The EWM approach is analogous to a classification problem, where one first builds an estimator of the population welfare, which is a functional of policy functions, and then trains a policy by maximizing the estimated welfare. In contrast, the plug-in approach is based on regression, where one first estimates the conditional average treatment effect (CATE) and then recommends the treatment with the highest estimated outcome. This study bridges the gap between the two approaches by showing that both are based on essentially the same optimization problem. In particular, we prove an exact equivalence between EWM and least squares over a reparameterization of the policy class. As a consequence, the two approaches are interchangeable in several respects and share the same theoretical guarantees under common conditions. Leveraging this equivalence, we propose a novel regularization method for policy learning. Our findings yield a convex and computationally efficient training procedure that avoids the NP-hard combinatorial step typically required in EWM.</article>","contentLength":1376,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reducing base drag on road vehicles using pulsed jets optimized by hybrid genetic algorithms","url":"https://arxiv.org/abs/2510.26718","date":1761883200,"author":"","guid":322844,"unread":true,"content":"<article>arXiv:2510.26718v1 Announce Type: cross \nAbstract: Aerodynamic drag on flat-backed vehicles like vans and trucks is dominated by a low-pressure wake, whose control is critical for reducing fuel consumption. This paper presents an experimental study at $Re_W\\approx 78,300$ on active flow control using four pulsed jets at the rear edges of a bluff body model. A hybrid genetic algorithm, combining a global search with a local gradient-based optimizer, was used to determine the optimal jet actuation parameters in an experiment-in-the-loop setup. The cost function was designed to achieve a net energy saving by simultaneously minimizing aerodynamic drag and penalizing the actuation's energy consumption. The optimization campaign successfully identified a control strategy that yields a drag reduction of approximately 10%. The optimal control law features a strong, low-frequency actuation from the bottom jet, which targets the main vortex shedding, while the top and lateral jets address higher-frequency, less energetic phenomena. Particle Image Velocimetry analysis reveals a significant upward shift and stabilization of the wake, leading to substantial pressure recovery on the model's lower base. Ultimately, this work demonstrates that a model-free optimization approach can successfully identify non-intuitive, multi-faceted actuation strategies that yield significant and energetically efficient drag reduction.</article>","contentLength":1425,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection","url":"https://arxiv.org/abs/2510.26703","date":1761883200,"author":"","guid":322845,"unread":true,"content":"<article>arXiv:2510.26703v1 Announce Type: cross \nAbstract: Purpose: Medical foundation models (FMs) offer a path to build high-performance diagnostic systems. However, their application to prostate cancer (PCa) detection from micro-ultrasound ({\\mu}US) remains untested in clinical settings. We present ProstNFound+, an adaptation of FMs for PCa detection from {\\mu}US, along with its first prospective validation. Methods: ProstNFound+ incorporates a medical FM, adapter tuning, and a custom prompt encoder that embeds PCa-specific clinical biomarkers. The model generates a cancer heatmap and a risk score for clinically significant PCa. Following training on multi-center retrospective data, the model is prospectively evaluated on data acquired five years later from a new clinical site. Model predictions are benchmarked against standard clinical scoring protocols (PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to the prospective data, with no performance degradation compared to retrospective evaluation. It aligns closely with clinical scores and produces interpretable heatmaps consistent with biopsy-confirmed lesions. Conclusion: The results highlight its potential for clinical deployment, offering a scalable and interpretable alternative to expert-driven protocols.</article>","contentLength":1297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Assessment of the conditional exchangeability assumption in causal machine learning models: a simulation study","url":"https://arxiv.org/abs/2510.26700","date":1761883200,"author":"","guid":322846,"unread":true,"content":"<article>arXiv:2510.26700v1 Announce Type: cross \nAbstract: Observational studies developing causal machine learning (ML) models for the prediction of individualized treatment effects (ITEs) seldom conduct empirical evaluations to assess the conditional exchangeability assumption. We aimed to evaluate the performance of these models under conditional exchangeability violations and the utility of negative control outcomes (NCOs) as a diagnostic. We conducted a simulation study to examine confounding bias in ITE estimates generated by causal forest and X-learner models under varying conditions, including the presence or absence of true heterogeneity. We simulated data to reflect real-world scenarios with differing levels of confounding, sample size, and NCO confounding structures. We then estimated and compared subgroup-level treatment effects on the primary outcome and NCOs across settings with and without unmeasured confounding. When conditional exchangeability was violated, causal forest and X-learner models failed to recover true treatment effect heterogeneity and, in some cases, falsely indicated heterogeneity when there was none. NCOs successfully identified subgroups affected by unmeasured confounding. Even when NCOs did not perfectly satisfy its ideal assumptions, it remained informative, flagging potential bias in subgroup level estimates, though not always pinpointing the subgroup with the largest confounding. Violations of conditional exchangeability substantially limit the validity of ITE estimates from causal ML models in routinely collected observational data. NCOs serve a useful empirical diagnostic tool for detecting subgroup-specific unmeasured confounding and should be incorporated into causal ML workflows to support the credibility of individualized inference.</article>","contentLength":1798,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FlowQ-Net: A Generative Framework for Automated Quantum Circuit Design","url":"https://arxiv.org/abs/2510.26688","date":1761883200,"author":"","guid":322847,"unread":true,"content":"<article>arXiv:2510.26688v1 Announce Type: cross \nAbstract: Designing efficient quantum circuits is a central bottleneck to exploring the potential of quantum computing, particularly for noisy intermediate-scale quantum (NISQ) devices, where circuit efficiency and resilience to errors are paramount. The search space of gate sequences grows combinatorially, and handcrafted templates often waste scarce qubit and depth budgets. We introduce \\textsc{FlowQ-Net} (Flow-based Quantum design Network), a generative framework for automated quantum circuit synthesis based on Generative Flow Networks (GFlowNets). This framework learns a stochastic policy to construct circuits sequentially, sampling them in proportion to a flexible, user-defined reward function that can encode multiple design objectives such as performance, depth, and gate count. This approach uniquely enables the generation of a diverse ensemble of high-quality circuits, moving beyond single-solution optimization. We demonstrate the efficacy of \\textsc{FlowQ-Net} through an extensive set of simulations. We apply our method to Variational Quantum Algorithm (VQA) ansatz design for molecular ground state estimation, Max-Cut, and image classification, key challenges in near-term quantum computing. Circuits designed by \\textsc{FlowQ-Net} achieve significant improvements, yielding circuits that are 10$\\times$-30$\\times$ more compact in terms of parameters, gates, and depth compared to commonly used unitary baselines, without compromising accuracy. This trend holds even when subjected to error profiles from real-world quantum devices. Our results underline the potential of generative models as a general-purpose methodology for automated quantum circuit design, offering a promising path towards more efficient quantum algorithms and accelerating scientific discovery in the quantum domain.</article>","contentLength":1856,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Action-Driven Processes for Continuous-Time Control","url":"https://arxiv.org/abs/2510.26672","date":1761883200,"author":"","guid":322848,"unread":true,"content":"<article>arXiv:2510.26672v1 Announce Type: cross \nAbstract: At the heart of reinforcement learning are actions - decisions made in response to observations of the environment. Actions are equally fundamental in the modeling of stochastic processes, as they trigger discontinuous state transitions and enable the flow of information through large, complex systems. In this paper, we unify the perspectives of stochastic processes and reinforcement learning through action- driven processes, and illustrate their application to spiking neural networks. Leveraging ideas from control-as-inference, we show that minimizing the Kullback-Leibler divergence between a policy-driven true distribution and a reward-driven model distribution for a suitably defined action-driven process is equivalent to maximum entropy reinforcement learning.</article>","contentLength":824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BRIQA: Balanced Reweighting in Image Quality Assessment of Pediatric Brain MRI","url":"https://arxiv.org/abs/2510.26661","date":1761883200,"author":"","guid":322849,"unread":true,"content":"<article>arXiv:2510.26661v1 Announce Type: cross \nAbstract: Assessing the severity of artifacts in pediatric brain Magnetic Resonance Imaging (MRI) is critical for diagnostic accuracy, especially in low-field systems where the signal-to-noise ratio is reduced. Manual quality assessment is time-consuming and subjective, motivating the need for robust automated solutions. In this work, we propose BRIQA (Balanced Reweighting in Image Quality Assessment), which addresses class imbalance in artifact severity levels. BRIQA uses gradient-based loss reweighting to dynamically adjust per-class contributions and employs a rotating batching scheme to ensure consistent exposure to underrepresented classes. Through experiments, no single architecture performs best across all artifact types, emphasizing the importance of architectural diversity. The rotating batching configuration improves performance across metrics by promoting balanced learning when combined with cross-entropy loss. BRIQA improves average macro F1 score from 0.659 to 0.706, with notable gains in Noise (0.430), Zipper (0.098), Positioning (0.097), Contrast (0.217), Motion (0.022), and Banding (0.012) artifact severity classification. The code is available at https://github.com/BioMedIA-MBZUAI/BRIQA.</article>","contentLength":1264,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SAMRI: Segment Anything Model for MRI","url":"https://arxiv.org/abs/2510.26635","date":1761883200,"author":"","guid":322850,"unread":true,"content":"<article>arXiv:2510.26635v1 Announce Type: cross \nAbstract: Accurate magnetic resonance imaging (MRI) segmentation is crucial for clinical decision-making, but remains labor-intensive when performed manually. Convolutional neural network (CNN)-based methods can be accurate and efficient, but often generalize poorly to MRI's variable contrast, intensity inhomogeneity, and protocols. Although the transformer-based Segment Anything Model (SAM) has demonstrated remarkable generalizability in natural images, existing adaptations often treat MRI as another imaging modality, overlooking these modality-specific challenges. We present SAMRI, an MRI-specialized SAM trained and validated on 1.1 million labeled MR slices spanning whole-body organs and pathologies. We demonstrate that SAM can be effectively adapted to MRI by simply fine-tuning its mask decoder using a two-stage strategy, reducing training time by 94% and trainable parameters by 96% versus full-model retraining. Across diverse MRI segmentation tasks, SAMRI achieves a mean Dice of 0.87, delivering state-of-the-art accuracy across anatomical regions and robust generalization on unseen structures, particularly small and clinically important structures.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Statistically Adaptive Differential Protection for AC Microgrids Based on Kullback-Leibler Divergence","url":"https://arxiv.org/abs/2510.26604","date":1761883200,"author":"","guid":322851,"unread":true,"content":"<article>arXiv:2510.26604v1 Announce Type: cross \nAbstract: The proliferation of inverter-based resources challenges traditional microgrid protection by introducing variable fault currents and complex transients. This paper presents a statistically adaptive differential protection scheme based on Kullback-Leibler divergence, implemented via a Bartlett-corrected G-statistic computed on logarithm-transformed current magnitudes. The method is a multivariate fault detection engine that employs the Mahalanobis distance to distinguish healthy and faulty states, enabling robust detection even in noisy environments. Detection thresholds are statistically derived from a chi-squared distribution for precise control over the false alarm rate. Upon detection, a lightweight classifier identifies the fault type by assessing per-phase G-statistics against dedicated thresholds, enhanced by a temporal persistence filter for security. Extensive simulations on a modified CIGRE 14-bus microgrid show high efficacy: sub-cycle average detection delays, high detection and classification accuracy across operating modes, resilience to high-impedance faults up to 250 Ohms, tolerance to 10 ms communication delay, and noise levels down to a 20 dB signal-to-noise ratio. These findings demonstrate a reproducible and computationally efficient solution for next-generation AC microgrid protection.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bijections Between Smirnov Words and Hamiltonian Cycles in Complete Multipartite Graphs","url":"https://arxiv.org/abs/2510.26597","date":1761883200,"author":"","guid":322852,"unread":true,"content":"<article>arXiv:2510.26597v1 Announce Type: cross \nAbstract: We establish a bijective correspondence between Smirnov words with balanced letter multiplicities and Hamiltonian paths in complete $m$-partite graphs $K_{n,n,\\ldots,n}$. This bijection allows us to derive closed inclusion-exclusion formulas for the number of Hamiltonian cycles in such graphs. We further extend the enumeration to the generalized nonuniform case $K_{n_1,n_2,\\ldots,n_m}$. We also provide an asymptotic analysis based on Stirling's approximation, which yields compact factorial expressions and logarithmic expansions describing the growth of the number of Hamiltonian cycles in the considered graphs. Our approach unifies the combinatorial study of adjacency-constrained words and the enumeration of Hamiltonian cycles within a single analytical framework.</article>","contentLength":824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hybrid Physical-Neural Simulator for Fast Cosmological Hydrodynamics","url":"https://arxiv.org/abs/2510.26593","date":1761883200,"author":"","guid":322853,"unread":true,"content":"<article>arXiv:2510.26593v1 Announce Type: cross \nAbstract: Cosmological field-level inference requires differentiable forward models that solve the challenging dynamics of gas and dark matter under hydrodynamics and gravity. We propose a hybrid approach where gravitational forces are computed using a differentiable particle-mesh solver, while the hydrodynamics are parametrized by a neural network that maps local quantities to an effective pressure field. We demonstrate that our method improves upon alternative approaches, such as an Enthalpy Gradient Descent baseline, both at the field and summary-statistic level. The approach is furthermore highly data efficient, with a single reference simulation of cosmological structure formation being sufficient to constrain the neural pressure model. This opens the door for future applications where the model is fit directly to observational data, rather than a training set of simulations.</article>","contentLength":934,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Physics-Informed Mixture Models and Surrogate Models for Precision Additive Manufacturing","url":"https://arxiv.org/abs/2510.26586","date":1761883200,"author":"","guid":322854,"unread":true,"content":"<article>arXiv:2510.26586v1 Announce Type: cross \nAbstract: In this study, we leverage a mixture model learning approach to identify defects in laser-based Additive Manufacturing (AM) processes. By incorporating physics based principles, we also ensure that the model is sensitive to meaningful physical parameter variations. The empirical evaluation was conducted by analyzing real-world data from two AM processes: Directed Energy Deposition and Laser Powder Bed Fusion. In addition, we also studied the performance of the developed framework over public datasets with different alloy type and experimental parameter information. The results show the potential of physics-guided mixture models to examine the underlying physical behavior of an AM system.</article>","contentLength":747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comparative Analysis of Deep Learning Models for Olive Tree Crown and Shadow Segmentation Towards Biovolume Estimation","url":"https://arxiv.org/abs/2510.26573","date":1761883200,"author":"","guid":322855,"unread":true,"content":"<article>arXiv:2510.26573v1 Announce Type: cross \nAbstract: Olive tree biovolume estimation is a key task in precision agriculture, supporting yield prediction and resource management, especially in Mediterranean regions severely impacted by climate-induced stress. This study presents a comparative analysis of three deep learning models U-Net, YOLOv11m-seg, and Mask RCNN for segmenting olive tree crowns and their shadows in ultra-high resolution UAV imagery. The UAV dataset, acquired over Vicopisano, Italy, includes manually annotated crown and shadow masks. Building on these annotations, the methodology emphasizes spatial feature extraction and robust segmentation; per-tree biovolume is then estimated by combining crown projected area with shadow-derived height using solar geometry. In testing, Mask R-CNN achieved the best overall accuracy (F1 = 0.86; mIoU = 0.72), while YOLOv11m-seg provided the fastest throughput (0.12 second per image). The estimated biovolumes spanned from approximately 4 to 24 cubic meters, reflecting clear structural differences among trees. These results indicate Mask R-CNN is preferable when biovolume accuracy is paramount, whereas YOLOv11m-seg suits large-area deployments where speed is critical; U-Net remains a lightweight, high-sensitivity option. The framework enables accurate, scalable orchard monitoring and can be further strengthened with DEM or DSM integration and field calibration for operational decision support.</article>","contentLength":1463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proxemics and Permeability of the Pedestrian Group","url":"https://arxiv.org/abs/2510.26571","date":1761883200,"author":"","guid":322856,"unread":true,"content":"<article>arXiv:2510.26571v1 Announce Type: cross \nAbstract: People tend to walk in groups, and interactions with those groups have a significant impact on crowd behavior and pedestrian traffic dynamics. Social norms can be seen as unwritten rules regulating people interactions in social settings. This article studies people interactions with groups and the emergence of group proxemics. Group zones, zone occupancy counts and people clearance from the group are studied using naturalistic data. Analysis indicate potential presence of three different zones in addition to the public zone. People tend to remain in the public zone and only progressively get closer to groups, and those closer approaches happen in a low frequency and for brief periods of time.</article>","contentLength":752,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tackling the Challenges of Adding Pulse-level Support to a Heterogeneous HPCQC Software Stack: MQSS Pulse","url":"https://arxiv.org/abs/2510.26565","date":1761883200,"author":"","guid":322857,"unread":true,"content":"<article>arXiv:2510.26565v1 Announce Type: cross \nAbstract: We study the problem of adding native pulse-level control to heterogeneous High Performance Computing-Quantum Computing (HPCQC) software stacks, using the Munich Quantum Software Stack (MQSS) as a case study. The goal is to expand the capabilities of HPCQC environments by offering the ability for low-level access and control, currently typically not foreseen for such hybrid systems. For this, we need to establish new interfaces that integrate such pulse-level control into the lower layers of the software stack, including the need for proper representation.\n  Pulse-level quantum programs can be fully described with only three low-level abstractions: ports (input/output channels), frames (reference signals), and waveforms (pulse envelopes). We identify four key challenges to represent those pulse abstractions at: the user-interface level, at the compiler level (including the Intermediate Representation (IR)), and at the backend-interface level (including the appropriate exchange format). For each challenge, we propose concrete solutions in the context of MQSS. These include introducing a compiled (C/C++) pulse Application Programming Interface (API) to overcome Python runtime overhead, extending its LLVM support to include pulse-related instructions, using its C-based backend interface to query relevant hardware constraints, and designing a portable exchange format for pulse sequences. Our integrated approach provides an end-to-end path for pulse-aware compilation and runtime execution in HPCQC environments. This work lays out the architectural blueprint for extending HPCQC integration to support pulse-level quantum operations without disrupting state-of-the-art classical workflows.</article>","contentLength":1760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Life-cycle Modeling and the Walking Behavior of the Pedestrian-Group as an Emergent Agent: With Empirical Data on the Cohesion of the Group Formation","url":"https://arxiv.org/abs/2510.26534","date":1761883200,"author":"","guid":322858,"unread":true,"content":"<article>arXiv:2510.26534v1 Announce Type: cross \nAbstract: This article investigates the pedestrian group as an emergent agent. The article explores empirical data to derive emergent agency and formation state spaces and outline recurring patterns of walking behavior. In this analysis, pedestrian trajectories extracted from surveillance videos are used along with manually annotated pedestrian group memberships. We conducted manual expert evaluation of observed groups, produced new manual annotations for relevant events pertaining to group behavior and extracted metrics relevant group formation. This information along with quantitative analysis was used to model the life-cycle and formation of the group agent. Those models give structure to expectations around walking behavior of groups; from pedestrian walking independently to the emergence of a collective intention where group members tended to maintain bounded distance between each other. Disturbances to this bounded distance often happened in association with changes in either their agency or their formation states. We summarized the patterns of behavior along with the sequences of state transitions into abstract patterns, which can aid in the development of more detailed group agents in simulation and in the design of engineering systems to interact with such groups.</article>","contentLength":1334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On a semi-discrete model of Maxwell's equations in three and two dimensions","url":"https://arxiv.org/abs/2510.26427","date":1761883200,"author":"","guid":322859,"unread":true,"content":"<article>arXiv:2510.26427v1 Announce Type: cross \nAbstract: In this paper, we develop a geometric, structure-preserving semi-discrete formulation of Maxwell's equations in both three- and two-dimensional settings within the framework of discrete exterior calculus. This approach preserves the intrinsic geometric and topological structures of the continuous theory while providing a consistent spatial discretization. We analyze the essential properties of the proposed semi-discrete model and compare them with those of the classical Maxwell's equations. As a special case, the model is illustrated on a combinatorial two-dimensional torus, where the semi-discrete Maxwell's equations take the form of a system of first-order linear ordinary differential equations. An explicit expression for the general solution of this system is also derived.</article>","contentLength":837,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Output Robust and Conjugate Gaussian Processes","url":"https://arxiv.org/abs/2510.26401","date":1761883200,"author":"","guid":322860,"unread":true,"content":"<article>arXiv:2510.26401v1 Announce Type: cross \nAbstract: Multi-output Gaussian process (MOGP) regression allows modelling dependencies among multiple correlated response variables. Similarly to standard Gaussian processes, MOGPs are sensitive to model misspecification and outliers, which can distort predictions within individual outputs. This situation can be further exacerbated by multiple anomalous response variables whose errors propagate due to correlations between outputs. To handle this situation, we extend and generalise the robust and conjugate Gaussian process (RCGP) framework introduced by Altamirano et al. (2024). This results in the multi-output RCGP (MO-RCGP): a provably robust MOGP that is conjugate, and jointly captures correlations across outputs. We thoroughly evaluate our approach through applications in finance and cancer research.</article>","contentLength":856,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPG-CDENet: Spatial Prior-Guided Cross Dual Encoder Network for Multi-Organ Segmentation","url":"https://arxiv.org/abs/2510.26390","date":1761883200,"author":"","guid":322861,"unread":true,"content":"<article>arXiv:2510.26390v1 Announce Type: cross \nAbstract: Multi-organ segmentation is a critical task in computer-aided diagnosis. While recent deep learning methods have achieved remarkable success in image segmentation, huge variations in organ size and shape challenge their effectiveness in multi-organ segmentation. To address these challenges, we propose a Spatial Prior-Guided Cross Dual Encoder Network (SPG-CDENet), a novel two-stage segmentation paradigm designed to improve multi-organ segmentation accuracy. Our SPG-CDENet consists of two key components: a spatial prior network and a cross dual encoder network. The prior network generates coarse localization maps that delineate the approximate ROI, serving as spatial guidance for the dual encoder network. The cross dual encoder network comprises four essential components: a global encoder, a local encoder, a symmetric cross-attention module, and a flow-based decoder. The global encoder captures global semantic features from the entire image, while the local encoder focuses on features from the prior network. To enhance the interaction between the global and local encoders, a symmetric cross-attention module is proposed across all layers of the encoders to fuse and refine features. Furthermore, the flow-based decoder directly propagates high-level semantic features from the final encoder layer to all decoder layers, maximizing feature preservation and utilization. Extensive qualitative and quantitative experiments on two public datasets demonstrate the superior performance of SPG-CDENet compared to existing segmentation methods. Furthermore, ablation studies further validate the effectiveness of the proposed modules in improving segmentation accuracy.</article>","contentLength":1728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SABER: Symbolic Regression-based Angle of Arrival and Beam Pattern Estimator","url":"https://arxiv.org/abs/2510.26340","date":1761883200,"author":"","guid":322862,"unread":true,"content":"<article>arXiv:2510.26340v1 Announce Type: cross \nAbstract: Accurate Angle-of-arrival (AoA) estimation is essential for next-generation wireless communication systems to enable reliable beamforming, high-precision localization, and integrated sensing. Unfortunately, classical high-resolution techniques require multi-element arrays and extensive snapshot collection, while generic Machine Learning (ML) approaches often yield black-box models that lack physical interpretability. To address these limitations, we propose a Symbolic Regression (SR)-based ML framework. Namely, Symbolic Regression-based Angle of Arrival and Beam Pattern Estimator (SABER), a constrained symbolic-regression framework that automatically discovers closed-form beam pattern and AoA models from path loss measurements with interpretability. SABER achieves high accuracy while bridging the gap between opaque ML methods and interpretable physics-driven estimators. First, we validate our approach in a controlled free-space anechoic chamber, showing that both direct inversion of the known $\\cos^n$ beam and a low-order polynomial surrogate achieve sub-0.5 degree Mean Absolute Error (MAE). A purely unconstrained SR method can further reduce the error of the predicted angles, but produces complex formulas that lack physical insight. Then, we implement the same SR-learned inversions in a real-world, Reconfigurable Intelligent Surface (RIS)-aided indoor testbed. SABER and unconstrained SR models accurately recover the true AoA with near-zero error. Finally, we benchmark SABER against the Cram\\'er-Rao Lower Bounds (CRLBs). Our results demonstrate that SABER is an interpretable and accurate alternative to state-of-the-art and black-box ML-based methods for AoA estimation.</article>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Limitation of Quantum Walk Approach to the Maximum Matching Problem","url":"https://arxiv.org/abs/2510.26246","date":1761883200,"author":"","guid":322863,"unread":true,"content":"<article>arXiv:2510.26246v1 Announce Type: cross \nAbstract: The Maximum Matching problem has a quantum query complexity lower bound of $\\Omega(n^{3/2})$ for graphs on $n$ vertices represented by an adjacency matrix. The current best quantum algorithm has the query complexity $O(n^{7/4})$, which is an improvement over the trivial bound $O(n^2)$. Constructing a quantum algorithm for this problem with a query complexity improving the upper bound $O(n^{7/4})$ is an open problem. The quantum walk technique is a general framework for constructing quantum algorithms by transforming a classical random walk search into a quantum search, and has been successfully applied to constructing an algorithm with a tight query complexity for another problem. In this work we show that the quantum walk technique fails to produce a fast algorithm improving the known (or even the trivial) upper bound on the query complexity. Specifically, if a quantum walk algorithm designed with the known technique solves the Maximum Matching problem using $O(n^{2-\\epsilon})$ queries with any constant $\\epsilon&gt;0$, and if the underlying classical random walk is independent of an input graph, then the guaranteed time complexity is larger than any polynomial of $n$.</article>","contentLength":1236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Design of Orthogonal Phase of Arrival Positioning Scheme Based on 5G PRS and Optimization of TOA Performance","url":"https://arxiv.org/abs/2510.26245","date":1761883200,"author":"","guid":322864,"unread":true,"content":"<article>arXiv:2510.26245v1 Announce Type: cross \nAbstract: This study analyzes the performance of positioning techniques based on configuration changes of 5G New Radio signals. In 5G networks, a terminal position is determined from the Time of Arrival of Positioning Reference Signals transmitted by base stations. We propose an algorithm that improves TOA accuracy under low sampling rate constraints and implement 5G PRS for positioning in a software defined modem. We also examine how flexible time frequency resource allocation of PRS affects TOA estimation accuracy and discuss optimal PRS configurations for a given signal environment.</article>","contentLength":633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hybrid LLM and Higher-Order Quantum Approximate Optimization for CSA Collateral Management","url":"https://arxiv.org/abs/2510.26217","date":1761883200,"author":"","guid":322865,"unread":true,"content":"<article>arXiv:2510.26217v1 Announce Type: cross \nAbstract: We address finance-native collateral optimization under ISDA Credit Support Annexes (CSAs), where integer lots, Schedule A haircuts, RA/MTA gating, and issuer/currency/class caps create rugged, legally bounded search spaces. We introduce a certifiable hybrid pipeline purpose-built for this domain: (i) an evidence-gated LLM that extracts CSA terms to a normalized JSON (abstain-by-default, span-cited); (ii) a quantum-inspired explorer that interleaves simulated annealing with micro higher order QAOA (HO-QAOA) on binding sub-QUBOs (subset size n &lt;= 16, order k &lt;= 4) to coordinate multi-asset moves across caps and RA-induced discreteness; (iii) a weighted risk-aware objective (Movement, CVaR, funding-priced overshoot) with an explicit coverage window U &lt;= Reff+B; and (iv) CP-SAT as single arbiter to certify feasibility and gaps, including a U-cap pre-check that reports the minimal feasible buffer B*. Encoding caps/rounding as higher-order terms lets HO-QAOA target the domain couplings that defeat local swaps. On government bond datasets and multi-CSA inputs, the hybrid improves a strong classical baseline (BL-3) by 9.1%, 9.6%, and 10.7% across representative harnesses, delivering better cost-movement-tail frontiers under governance settings. We release governance grade artifacts-span citations, valuation matrix audit, weight provenance, QUBO manifests, and CP-SAT traces-to make results auditable and reproducible.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Numerical Investigation of Single-Core to Split-Core Transitions in Nematic Liquid Crystals","url":"https://arxiv.org/abs/2510.26215","date":1761883200,"author":"","guid":322866,"unread":true,"content":"<article>arXiv:2510.26215v1 Announce Type: cross \nAbstract: We analyze single-core and split-core defect structures in nematic liquid crystals within the Landau-de Gennes framework by studying minimizers of the associated energy functional. A bifurcation occurs at a critical temperature threshold, below which both split-core and single-core configurations are solutions to the Euler-Lagrange equation, with the split-core defect possessing lower energy. Above the threshold, the split-core configuration vanishes, leaving the single-core defect as the only stable solution. We analyze the dependence of such temperature threshold on the domain size and characterize the nature of the transition between the two defect types. We carry out a quantitative study of defect core sizes as functions of temperature and domain size for both single and split core defects.</article>","contentLength":856,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sequential Change Detection Under A Markov Setup With Unknown Pre-Change and Post-Change Distributions","url":"https://arxiv.org/abs/2510.26204","date":1761883200,"author":"","guid":322867,"unread":true,"content":"<article>arXiv:2510.26204v1 Announce Type: cross \nAbstract: In this work we extend the results developed in 2022 for a sequential change detection algorithm making use of Page's CUSUM statistic, the empirical distribution as an estimate of the pre-change distribution, and a universal code as a tool for estimating the post-change distribution, from the i.i.d. case to the Markov setup.</article>","contentLength":377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Practical hybrid decoding scheme for parity-encoded spin systems","url":"https://arxiv.org/abs/2510.26189","date":1761883200,"author":"","guid":322868,"unread":true,"content":"<article>arXiv:2510.26189v1 Announce Type: cross \nAbstract: We propose a practical hybrid decoding scheme for the parity-encoding architecture. This architecture was first introduced by N. Sourlas as a computational technique for tackling hard optimization problems, especially those modeled by spin systems such as the Ising model and spin glasses, and reinvented by W. Lechner, P. Hauke, and P. Zoller to develop quantum annealing devices. We study the specific model, called the SLHZ model, aiming to achieve a near-term quantum annealing device implemented solely through geometrically local spin interactions. Taking account of the close connection between the SLHZ model and a classical low-density-parity-check code, two approaches can be chosen for the decoding: (1) finding the ground state of a spin Hamiltonian derived from the SLHZ model, which can be achieved via stochastic decoders such as quantum annealing or classical Monte Carlo samplers; (2) using deterministic decoding techniques for the classical LDPC code, such as belief propagation and bit-flip decoder. The proposed hybrid approach combines the two approaches by applying bit-flip decoding to the readout of the stochastic decoder based on the SLHZ model. We present simulations demonstrating that this approach can reveal the latent potential of the SLHZ model, realizing soft-annealing concept proposed by Sourlas.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Manage Investment Portfolios beyond Simple Utility Functions","url":"https://arxiv.org/abs/2510.26165","date":1761883200,"author":"","guid":322869,"unread":true,"content":"<article>arXiv:2510.26165v1 Announce Type: cross \nAbstract: While investment funds publicly disclose their objectives in broad terms, their managers optimize for complex combinations of competing goals that go beyond simple risk-return trade-offs. Traditional approaches attempt to model this through multi-objective utility functions, but face fundamental challenges in specification and parameterization. We propose a generative framework that learns latent representations of fund manager strategies without requiring explicit utility specification.\n  Our approach directly models the conditional probability of a fund's portfolio weights, given stock characteristics, historical returns, previous weights, and a latent variable representing the fund's strategy. Unlike methods based on reinforcement learning or imitation learning, which require specified rewards or labeled expert objectives, our GAN-based architecture learns directly from the joint distribution of observed holdings and market data.\n  We validate our framework on a dataset of 1436 U.S. equity mutual funds. The learned representations successfully capture known investment styles, such as \"growth\" and \"value,\" while also revealing implicit manager objectives. For instance, we find that while many funds exhibit characteristics of Markowitz-like optimization, they do so with heterogeneous realizations for turnover, concentration, and latent factors.\n  To analyze and interpret the end-to-end model, we develop a series of tests that explain the model, and we show that the benchmark's expert labeling are contained in our model's encoding in a linear interpretable way.\n  Our framework provides a data-driven approach for characterizing investment strategies for applications in market simulation, strategy attribution, and regulatory oversight.</article>","contentLength":1814,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty-Aware Diagnostics for Physics-Informed Machine Learning","url":"https://arxiv.org/abs/2510.26121","date":1761883200,"author":"","guid":322870,"unread":true,"content":"<article>arXiv:2510.26121v1 Announce Type: cross \nAbstract: Physics-informed machine learning (PIML) integrates prior physical information, often in the form of differential equation constraints, into the process of fitting machine learning models to physical data. Popular PIML approaches, including neural operators, physics-informed neural networks, neural ordinary differential equations, and neural discrete equilibria, are typically fit to objectives that simultaneously include both data and physical constraints. However, the multi-objective nature of this approach creates ambiguity in the measurement of model quality. This is related to a poor understanding of epistemic uncertainty, and it can lead to surprising failure modes, even when existing statistical metrics suggest strong fits. Working within a Gaussian process regression framework, we introduce the Physics-Informed Log Evidence (PILE) score. Bypassing the ambiguities of test losses, the PILE score is a single, uncertainty-aware metric that provides a selection principle for hyperparameters of a PIML model. We show that PILE minimization yields excellent choices for a wide variety of model parameters, including kernel bandwidth, least squares regularization weights, and even kernel function selection. We also show that, even prior to data acquisition, a special 'data-free' case of the PILE score identifies a priori kernel choices that are 'well-adapted' to a given PDE. Beyond the kernel setting, we anticipate that the PILE score can be extended to PIML at large, and we outline approaches to do so.</article>","contentLength":1575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Super-Capacity SRS Channel Inpainting via Diffusion Models","url":"https://arxiv.org/abs/2510.26097","date":1761883200,"author":"","guid":322871,"unread":true,"content":"<article>arXiv:2510.26097v1 Announce Type: cross \nAbstract: Accurate channel state information (CSI) is essential for reliable multiuser MIMO operation. In 5G NR, reciprocity-based beamforming via uplink Sounding Reference Signals (SRS) face resource and coverage constraints, motivating sparse non-uniform SRS allocation. Prior masked-autoencoder (MAE) approaches improve coverage but overfit to training masks and degrade under unseen distortions (e.g., additional masking, interference, clipping, non-Gaussian noise). We propose a diffusion-based channel inpainting framework that integrates system-model knowledge at inference via a likelihood-gradient term, enabling a single trained model to adapt across mismatched conditions. On standardized CDL channels, the score-based diffusion variant consistently outperforms a UNet score-model baseline and the one-step MAE under distribution shift, with improvements up to 14 dB NMSE in challenging settings (e.g., Laplace noise, user interference), while retaining competitive accuracy under matched conditions. These results demonstrate that diffusion-guided inpainting is a robust and generalizable approach for super-capacity SRS design in 5G NR systems.</article>","contentLength":1198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Industry Members' Perceptions about ABET-based Accreditation: An Exploratory Study in a Developing Country","url":"https://arxiv.org/abs/2510.26087","date":1761883200,"author":"","guid":322872,"unread":true,"content":"<article>arXiv:2510.26087v1 Announce Type: cross \nAbstract: ABET accreditation is an increasingly prominent system of global accreditation of engineering programs, and the assessment requires programs to demonstrate that they meet the needs of the program's stakeholders, typically industrial potential employers of graduates. To obtain these inputs, programs are required to assemble an advisory committee board. The views of the advisory board on the relevance of the degree outcomes are an essential part of this process. The purpose of this qualitative research study is to explore the viewpoints that industry stakeholders have on this type of process. The context for the study was an Ecuadorian engineering program which had successfully achieved the ABET accreditation. The study drew on interviews undertaken with industry members who were part of the advisory board. This study focuses on how they perceive the process and the accreditation awarded, analyzing their views of its usefulness, especially in relation to the employability of graduates. Based on the findings, we offer critical insights into this accreditation process when it takes place in contexts beyond highly industrialized countries.</article>","contentLength":1203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems","url":"https://arxiv.org/abs/2510.26061","date":1761883200,"author":"","guid":322873,"unread":true,"content":"<article>arXiv:2510.26061v1 Announce Type: cross \nAbstract: We propose a data-driven framework for efficiently solving quadratic programming (QP) problems by reducing the number of variables in high-dimensional QPs using instance-specific projection. A graph neural network-based model is designed to generate projections tailored to each QP instance, enabling us to produce high-quality solutions even for previously unseen problems. The model is trained on heterogeneous QPs to minimize the expected objective value evaluated on the projected solutions. This is formulated as a bilevel optimization problem; the inner optimization solves the QP under a given projection using a QP solver, while the outer optimization updates the model parameters. We develop an efficient algorithm to solve this bilevel optimization problem, which computes parameter gradients without backpropagating through the solver. We provide a theoretical analysis of the generalization ability of solving QPs with projection matrices generated by neural networks. Experimental results demonstrate that our method produces high-quality feasible solutions with reduced computation time, outperforming existing methods.</article>","contentLength":1184,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Strong Birthday Problem Revisited","url":"https://arxiv.org/abs/2510.26056","date":1761883200,"author":"","guid":322874,"unread":true,"content":"<article>arXiv:2510.26056v1 Announce Type: cross \nAbstract: We revisit the Strong Birthday Problem (SBP) introduced in [1]. The problem is stated as follows: what is the minimum number of people we have to choose so that everyone has a shared birthday with probability at least 1/2? We derive recurrence relations to compute the probability, and further show a nice connection to the associated Stirling numbers of the second kind to derive additional recurrences. We implement the recurrences using dynamic programming as well as compute the values using the combinatorial formula, and provide numerical results.</article>","contentLength":604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bias-Corrected Data Synthesis for Imbalanced Learning","url":"https://arxiv.org/abs/2510.26046","date":1761883200,"author":"","guid":322875,"unread":true,"content":"<article>arXiv:2510.26046v1 Announce Type: cross \nAbstract: Imbalanced data, where the positive samples represent only a small proportion compared to the negative samples, makes it challenging for classification problems to balance the false positive and false negative rates. A common approach to addressing the challenge involves generating synthetic data for the minority group and then training classification models with both observed and synthetic data. However, since the synthetic data depends on the observed data and fails to replicate the original data distribution accurately, prediction accuracy is reduced when the synthetic data is naively treated as the true data. In this paper, we address the bias introduced by synthetic data and provide consistent estimators for this bias by borrowing information from the majority group. We propose a bias correction procedure to mitigate the adverse effects of synthetic data, enhancing prediction accuracy while avoiding overfitting. This procedure is extended to broader scenarios with imbalanced data, such as imbalanced multi-task learning and causal inference. Theoretical properties, including bounds on bias estimation errors and improvements in prediction accuracy, are provided. Simulation results and data analysis on handwritten digit datasets demonstrate the effectiveness of our method.</article>","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$L_1$-norm Regularized Indefinite Kernel Logistic Regression","url":"https://arxiv.org/abs/2510.26043","date":1761883200,"author":"","guid":322876,"unread":true,"content":"<article>arXiv:2510.26043v1 Announce Type: cross \nAbstract: Kernel logistic regression (KLR) is a powerful classification method widely applied across diverse domains. In many real-world scenarios, indefinite kernels capture more domain-specific structural information than positive definite kernels. This paper proposes a novel $L_1$-norm regularized indefinite kernel logistic regression (RIKLR) model, which extends the existing IKLR framework by introducing sparsity via an $L_1$-norm penalty. The introduction of this regularization enhances interpretability and generalization while introducing nonsmoothness and nonconvexity into the optimization landscape. To address these challenges, a theoretically grounded and computationally efficient proximal linearized algorithm is developed. Experimental results on multiple benchmark datasets demonstrate the superior performance of the proposed method in terms of both accuracy and sparsity.</article>","contentLength":935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Parallelized Cutting-Plane Algorithm for Computationally Efficient Modelling to Generate Alternatives","url":"https://arxiv.org/abs/2510.26029","date":1761883200,"author":"","guid":322877,"unread":true,"content":"<article>arXiv:2510.26029v1 Announce Type: cross \nAbstract: Contemporary macro energy systems modelling is characterized by the need to represent strategic and operational decisions with high temporal and spatial resolution and represent discrete investment and retirement decisions. This drive towards greater fidelity, however, conflicts with a simultaneous push towards greater model representation of inherent complexity in decision making, including methods like Modelling to Generate Alternatives (MGA). MGA aims to map the feasible space of a model within a cost slack by varying investment parameters without changing the operational constraints, a process which frequently requires hundreds of solutions. For large, detailed energy system models this is impossible with traditional methods, leading researchers to reduce complexity with linearized investments and zonal or temporal aggregation. This research presents a new solution method for MGA type problems using cutting-plane methods based on a tailored reformulation of Benders Decomposition. We accelerate the algorithm by sharing cuts between MGA master problems and grouping MGA objectives. We find that our new solution method consistently solves MGA problems times faster and requires less memory than existing monolithic Modelling to Generate Alternatives solution methods on linear problems, enabling rapid computation of a greater number of solutions to highly resolved models. We also show that our novel cutting-plane algorithm enables the solution of very large MGA problems with integer investment decisions.</article>","contentLength":1577,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation","url":"https://arxiv.org/abs/2510.26026","date":1761883200,"author":"","guid":322878,"unread":true,"content":"<article>arXiv:2510.26026v1 Announce Type: cross \nAbstract: Reliable uncertainty quantification is crucial for reinforcement learning (RL) in high-stakes settings. We propose a unified conformal prediction framework for infinite-horizon policy evaluation that constructs distribution-free prediction intervals {for returns} in both on-policy and off-policy settings. Our method integrates distributional RL with conformal calibration, addressing challenges such as unobserved returns, temporal dependencies, and distributional shifts. We propose a modular pseudo-return construction based on truncated rollouts and a time-aware calibration strategy using experience replay and weighted subsampling. These innovations mitigate model bias and restore approximate exchangeability, enabling uncertainty quantification even under policy shifts. Our theoretical analysis provides coverage guarantees that account for model misspecification and importance weight estimation. Empirical results, including experiments in synthetic and benchmark environments like Mountain Car, show that our method significantly improves coverage and reliability over standard distributional RL baselines.</article>","contentLength":1170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Groupwise Registration with Physics-Informed Test-Time Adaptation on Multi-parametric Cardiac MRI","url":"https://arxiv.org/abs/2510.26022","date":1761883200,"author":"","guid":322879,"unread":true,"content":"<article>arXiv:2510.26022v1 Announce Type: cross \nAbstract: Multiparametric mapping MRI has become a viable tool for myocardial tissue characterization. However, misalignment between multiparametric maps makes pixel-wise analysis challenging. To address this challenge, we developed a generalizable physics-informed deep-learning model using test-time adaptation to enable group image registration across contrast weighted images acquired from multiple physical models (e.g., a T1 mapping model and T2 mapping model). The physics-informed adaptation utilized the synthetic images from specific physics model as registration reference, allows for transductive learning for various tissue contrast. We validated the model in healthy volunteers with various MRI sequences, demonstrating its improvement for multi-modal registration with a wide range of image contrast variability.</article>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enabling Fast and Accurate Neutral Atom Readout through Image Denoising","url":"https://arxiv.org/abs/2510.25982","date":1761883200,"author":"","guid":322880,"unread":true,"content":"<article>arXiv:2510.25982v1 Announce Type: cross \nAbstract: Neutral atom quantum computers hold promise for scaling up to hundreds of thousands of qubits, but their progress is constrained by slow qubit readout. Measuring qubits currently takes milliseconds-much longer than the underlying quantum gate operations-making readout the primary bottleneck in deploying quantum error correction. Because each round of QEC depends on measurement, long readout times increase cycle duration and slow down program execution. Reducing the readout duration speeds up cycles and reduces decoherence errors that accumulate while qubits idle, but it also lowers the number of collected photons, making measurements noisier and more error-prone. This tradeoff leaves neutral atom systems stuck between slow but accurate readout and fast but unreliable readout.\n  We show that image denoising can resolve this tension. Our framework, GANDALF, uses explicit denoising using image translation to reconstruct clear signals from short, low-photon measurements, enabling reliable classification at up to 1.6x shorter readout times. Combined with lightweight classifiers and a pipelined readout design, our approach both reduces logical error rate by up to 35x and overall QEC cycle time up to 1.77x compared to state-of-the-art CNN-based readout for Cesium (Cs) Neutral Atom arrays.</article>","contentLength":1353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InputDSA: Demixing then Comparing Recurrent and Externally Driven Dynamics","url":"https://arxiv.org/abs/2510.25943","date":1761883200,"author":"","guid":322881,"unread":true,"content":"<article>arXiv:2510.25943v1 Announce Type: cross \nAbstract: In control problems and basic scientific modeling, it is important to compare observations with dynamical simulations. For example, comparing two neural systems can shed light on the nature of emergent computations in the brain and deep neural networks. Recently, Ostrow et al. (2023) introduced Dynamical Similarity Analysis (DSA), a method to measure the similarity of two systems based on their recurrent dynamics rather than geometry or topology. However, DSA does not consider how inputs affect the dynamics, meaning that two similar systems, if driven differently, may be classified as different. Because real-world dynamical systems are rarely autonomous, it is important to account for the effects of input drive. To this end, we introduce a novel metric for comparing both intrinsic (recurrent) and input-driven dynamics, called InputDSA (iDSA). InputDSA extends the DSA framework by estimating and comparing both input and intrinsic dynamic operators using a variant of Dynamic Mode Decomposition with control (DMDc) based on subspace identification. We demonstrate that InputDSA can successfully compare partially observed, input-driven systems from noisy data. We show that when the true inputs are unknown, surrogate inputs can be substituted without a major deterioration in similarity estimates. We apply InputDSA on Recurrent Neural Networks (RNNs) trained with Deep Reinforcement Learning, identifying that high-performing networks are dynamically similar to one another, while low-performing networks are more diverse. Lastly, we apply InputDSA to neural data recorded from rats performing a cognitive task, demonstrating that it identifies a transition from input-driven evidence accumulation to intrinsically-driven decision-making. Our work demonstrates that InputDSA is a robust and efficient method for comparing intrinsic dynamics and the effect of external input on dynamical systems.</article>","contentLength":1960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Equation Discovery, Parametric Simulation, and Optimization Using the Physics-Informed Neural Network (PINN) Method for the Heat Conduction Problem","url":"https://arxiv.org/abs/2510.25925","date":1761883200,"author":"","guid":322882,"unread":true,"content":"<article>arXiv:2510.25925v1 Announce Type: cross \nAbstract: In this study, the capabilities of the Physics-Informed Neural Network (PINN) method are investigated for three major tasks: modeling, simulation, and optimization in the context of the heat conduction problem. In the modeling phase, the governing equation of heat transfer by conduction is reconstructed through equation discovery using fractional-order derivatives, enabling the identification of the fractional derivative order that best describes the physical behavior. In the simulation phase, the thermal conductivity is treated as a physical parameter, and a parametric simulation is performed to analyze its influence on the temperature field. In the optimization phase, the focus is placed on the inverse problem, where the goal is to infer unknown physical properties from observed data. The effectiveness of the PINN approach is evaluated across these three fundamental engineering problem types and compared against conventional numerical methods. The results demonstrate that although PINNs may not yet outperform traditional numerical solvers in terms of speed and accuracy for forward problems, they offer a powerful and flexible framework for parametric simulation, optimization, and equation discovery, making them highly valuable for inverse and data-driven modeling applications.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Stochastic Gradient Descent in its continuous-time limit based on the Wigner formulation of Open Quantum Systems","url":"https://arxiv.org/abs/2510.25910","date":1761883200,"author":"","guid":322883,"unread":true,"content":"<article>arXiv:2510.25910v1 Announce Type: cross \nAbstract: The main ideas behind a research plan to use the Wigner formulation as a bridge between classical and quantum probabilistic algorithms are presented, focusing on a particular case: the Quantum analog of Stochastic Gradient Descent in its continuous-time limit based on the Wigner formulation of Open Quantum Systems.</article>","contentLength":367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Mirror-Image Peptide Sequence Design for Data Storage via Peptide Bond Cleavage Prediction","url":"https://arxiv.org/abs/2510.25814","date":1761883200,"author":"","guid":322884,"unread":true,"content":"<article>arXiv:2510.25814v1 Announce Type: cross \nAbstract: Traditional non-biological storage media, such as hard drives, face limitations in both storage density and lifespan due to the rapid growth of data in the big data era. Mirror-image peptides composed of D-amino acids have emerged as a promising biological storage medium due to their high storage density, structural stability, and long lifespan. The sequencing of mirror-image peptides relies on \\textit{de-novo} technology. However, its accuracy is limited by the scarcity of tandem mass spectrometry datasets and the challenges that current algorithms encounter when processing these peptides directly. This study is the first to propose improving sequencing accuracy indirectly by optimizing the design of mirror-image peptide sequences. In this work, we introduce DBond, a deep neural network based model that integrates sequence features, precursor ion properties, and mass spectrometry environmental factors for the prediction of mirror-image peptide bond cleavage. In this process, sequences with a high peptide bond cleavage ratio, which are easy to sequence, are selected. The main contributions of this study are as follows. First, we constructed MiPD513, a tandem mass spectrometry dataset containing 513 mirror-image peptides. Second, we developed the peptide bond cleavage labeling algorithm (PBCLA), which generated approximately 12.5 million labeled data based on MiPD513. Third, we proposed a dual prediction strategy that combines multi-label and single-label classification. On an independent test set, the single-label classification strategy outperformed other methods in both single and multiple peptide bond cleavage prediction tasks, offering a strong foundation for sequence optimization.</article>","contentLength":1765,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms","url":"https://arxiv.org/abs/2510.25811","date":1761883200,"author":"","guid":322885,"unread":true,"content":"<article>arXiv:2510.25811v1 Announce Type: cross \nAbstract: We consider a stochastic multi-armed bandit problem with i.i.d. rewards where the expected reward function is multimodal with at most m modes. We propose the first known computationally tractable algorithm for computing the solution to the Graves-Lai optimization problem, which in turn enables the implementation of asymptotically optimal algorithms for this bandit problem. The code for the proposed algorithms is publicly available at https://github.com/wilrev/MultimodalBandits</article>","contentLength":532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discovering Interpretable Biological Concepts in Single-cell RNA-seq Foundation Models","url":"https://arxiv.org/abs/2510.25807","date":1761883200,"author":"","guid":322886,"unread":true,"content":"<article>arXiv:2510.25807v1 Announce Type: cross \nAbstract: Single-cell RNA-seq foundation models achieve strong performance on downstream tasks but remain black boxes, limiting their utility for biological discovery. Recent work has shown that sparse dictionary learning can extract concepts from deep learning models, with promising applications in biomedical imaging and protein models. However, interpreting biological concepts remains challenging, as biological sequences are not inherently human-interpretable. We introduce a novel concept-based interpretability framework for single-cell RNA-seq models with a focus on concept interpretation and evaluation. We propose an attribution method with counterfactual perturbations that identifies genes that influence concept activation, moving beyond correlational approaches like differential expression analysis. We then provide two complementary interpretation approaches: an expert-driven analysis facilitated by an interactive interface and an ontology-driven method with attribution-based biological pathway enrichment. Applying our framework to two well-known single-cell RNA-seq models from the literature, we interpret concepts extracted by Top-K Sparse Auto-Encoders trained on two immune cell datasets. With a domain expert in immunology, we show that concepts improve interpretability compared to individual neurons while preserving the richness and informativeness of the latent representations. This work provides a principled framework for interpreting what biological knowledge foundation models have encoded, paving the way for their use for hypothesis generation and discovery.</article>","contentLength":1638,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pulsar Detection with Deep Learning","url":"https://arxiv.org/abs/2510.25774","date":1761883200,"author":"","guid":322887,"unread":true,"content":"<article>arXiv:2510.25774v1 Announce Type: cross \nAbstract: Pulsar surveys generate millions of candidates per run, overwhelming manual inspection. This thesis builds a deep learning pipeline for radio pulsar candidate selection that fuses array-derived features with image diagnostics. From approximately 500 GB of Giant Metrewave Radio Telescope (GMRT) data, raw voltages are converted to filterbanks (SIGPROC), then de-dispersed and folded across trial dispersion measures (PRESTO) to produce approximately 32,000 candidates. Each candidate yields four diagnostics--summed profile, time vs. phase, subbands vs. phase, and DM curve--represented as arrays and images. A baseline stacked model (ANNs for arrays + CNNs for images with logistic-regression fusion) reaches 68% accuracy. We then refine the CNN architecture and training (regularization, learning-rate scheduling, max-norm constraints) and mitigate class imbalance via targeted augmentation, including a GAN-based generator for the minority class. The enhanced CNN attains 87% accuracy; the final GAN+CNN system achieves 94% accuracy with balanced precision and recall on a held-out test set, while remaining lightweight enough for near--real-time triage. The results show that combining array and image channels improves separability over image-only approaches, and that modest generative augmentation substantially boosts minority (pulsar) recall. The methods are survey-agnostic and extensible to forthcoming high-throughput facilities.</article>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RNAGenScape: Property-guided Optimization and Interpolation of mRNA Sequences with Manifold Langevin Dynamics","url":"https://arxiv.org/abs/2510.24736","date":1761883200,"author":"","guid":322888,"unread":true,"content":"<article>arXiv:2510.24736v1 Announce Type: cross \nAbstract: mRNA design and optimization are important in synthetic biology and therapeutic development, but remain understudied in machine learning. Systematic optimization of mRNAs is hindered by the scarce and imbalanced data as well as complex sequence-function relationships. We present RNAGenScape, a property-guided manifold Langevin dynamics framework that iteratively updates mRNA sequences within a learned latent manifold. RNAGenScape combines an organized autoencoder, which structures the latent space by target properties for efficient and biologically plausible exploration, with a manifold projector that contracts each step of update back to the manifold. RNAGenScape supports property-guided optimization and smooth interpolation between sequences, while remaining robust under scarce and undersampled data, and ensuring that intermediate products are close to the viable mRNA manifold. Across three real mRNA datasets, RNAGenScape improves the target properties with high success rates and efficiency, outperforming various generative or optimization methods developed for proteins or non-biological data. By providing continuous, data-aligned trajectories that reveal how edits influence function, RNAGenScape establishes a scalable paradigm for controllable mRNA design and latent space exploration in mRNA sequence modeling.</article>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Action-Dependent Channels","url":"https://arxiv.org/abs/2510.09834","date":1761883200,"author":"","guid":322889,"unread":true,"content":"<article>arXiv:2510.09834v1 Announce Type: cross \nAbstract: We study the quantum action-dependent channel. The model can be viewed as a quantum analog of the classical action-dependent channel model. In this setting, the communication channel has two inputs: Alice's transmission and the input environment. The action-dependent mechanism enables the transmitter to influence the channel's environment through an action channel. Specifically, Alice encodes her message into a quantum action, which subsequently affects the environment state. For example, a quantum measurement at the encoder can induce a state collapse of the environment. In addition, Alice has access to side information. Unlike the classical model, she cannot have a copy of the environment state due to the no-cloning theorem. Instead, she shares entanglement with this environment. We establish an achievable communication rate for reliable message transmission via the quantum action-dependent channel, thereby extending the classical action-dependent framework to the quantum domain.</article>","contentLength":1047,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark","url":"https://arxiv.org/abs/2510.26802","date":1761883200,"author":"","guid":322890,"unread":true,"content":"<article>arXiv:2510.26802v1 Announce Type: new \nAbstract: Recent video generation models can produce high-fidelity, temporally coherent videos, indicating that they may encode substantial world knowledge. Beyond realistic synthesis, they also exhibit emerging behaviors indicative of visual perception, modeling, and manipulation. Yet, an important question still remains: Are video models ready to serve as zero-shot reasoners in challenging visual reasoning scenarios? In this work, we conduct an empirical study to comprehensively investigate this question, focusing on the leading and popular Veo-3. We evaluate its reasoning behavior across 12 dimensions, including spatial, geometric, physical, temporal, and embodied logic, systematically characterizing both its strengths and failure modes. To standardize this study, we curate the evaluation data into MME-CoF, a compact benchmark that enables in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our findings reveal that while current video models demonstrate promising reasoning patterns on short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics, they remain limited in long-horizon causal reasoning, strict geometric constraints, and abstract logic. Overall, they are not yet reliable as standalone zero-shot reasoners, but exhibit encouraging signs as complementary visual engines alongside dedicated reasoning models. Project page: https://video-cof.github.io</article>","contentLength":1463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes","url":"https://arxiv.org/abs/2510.26800","date":1761883200,"author":"","guid":322891,"unread":true,"content":"<article>arXiv:2510.26800v1 Announce Type: new \nAbstract: There are two prevalent ways to constructing 3D scenes: procedural generation and 2D lifting. Among them, panorama-based 2D lifting has emerged as a promising technique, leveraging powerful 2D generative priors to produce immersive, realistic, and diverse 3D environments. In this work, we advance this technique to generate graphics-ready 3D scenes suitable for physically based rendering (PBR), relighting, and simulation. Our key insight is to repurpose 2D generative models for panoramic perception of geometry, textures, and PBR materials. Unlike existing 2D lifting approaches that emphasize appearance generation and ignore the perception of intrinsic properties, we present OmniX, a versatile and unified framework. Based on a lightweight and efficient cross-modal adapter structure, OmniX reuses 2D generative priors for a broad range of panoramic vision tasks, including panoramic perception, generation, and completion. Furthermore, we construct a large-scale synthetic panorama dataset containing high-quality multimodal panoramas from diverse indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness of our model in panoramic visual perception and graphics-ready 3D scene generation, opening new possibilities for immersive and physically realistic virtual world generation.</article>","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Masked Diffusion Captioning for Visual Feature Learning","url":"https://arxiv.org/abs/2510.26799","date":1761883200,"author":"","guid":322892,"unread":true,"content":"<article>arXiv:2510.26799v1 Announce Type: new \nAbstract: We learn visual features by captioning images with an image-conditioned masked diffusion language model, a formulation we call masked diffusion captioning (MDC). During training, text tokens in each image-caption pair are masked at a randomly chosen ratio, and a decoder conditioned on visual features is trained to reconstruct the original text. After training, the learned visual features can be applied to downstream vision tasks. Unlike autoregressive captioning, the strength of the visual learning signal in MDC does not depend on each token's position in the sequence, reducing the need for auxiliary objectives. Linear probing experiments across a variety of academic-scale models and datasets show that the learned visual features are competitive with those produced by autoregressive and contrastive approaches.</article>","contentLength":870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting","url":"https://arxiv.org/abs/2510.26796","date":1761883200,"author":"","guid":322893,"unread":true,"content":"<article>arXiv:2510.26796v1 Announce Type: new \nAbstract: Immersive applications call for synthesizing spatiotemporal 4D content from casual videos without costly 3D supervision. Existing video-to-4D methods typically rely on manually annotated camera poses, which are labor-intensive and brittle for in-the-wild footage. Recent warp-then-inpaint approaches mitigate the need for pose labels by warping input frames along a novel camera trajectory and using an inpainting model to fill missing regions, thereby depicting the 4D scene from diverse viewpoints. However, this trajectory-to-trajectory formulation often entangles camera motion with scene dynamics and complicates both modeling and inference. We introduce SEE4D, a pose-free, trajectory-to-camera framework that replaces explicit trajectory prediction with rendering to a bank of fixed virtual cameras, thereby separating camera control from scene modeling. A view-conditional video inpainting model is trained to learn a robust geometry prior by denoising realistically synthesized warped images and to inpaint occluded or missing regions across virtual viewpoints, eliminating the need for explicit 3D annotations. Building on this inpainting core, we design a spatiotemporal autoregressive inference pipeline that traverses virtual-camera splines and extends videos with overlapping windows, enabling coherent generation at bounded per-step complexity. We validate See4D on cross-view video generation and sparse reconstruction benchmarks. Across quantitative metrics and qualitative assessments, our method achieves superior generalization and improved performance relative to pose- or trajectory-conditioned baselines, advancing practical 4D world modeling from casual videos.</article>","contentLength":1734,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Image Geo-Localization to Continent Level","url":"https://arxiv.org/abs/2510.26795","date":1761883200,"author":"","guid":322894,"unread":true,"content":"<article>arXiv:2510.26795v1 Announce Type: new \nAbstract: Determining the precise geographic location of an image at a global scale remains an unsolved challenge. Standard image retrieval techniques are inefficient due to the sheer volume of images (&gt;100M) and fail when coverage is insufficient. Scalable solutions, however, involve a trade-off: global classification typically yields coarse results (10+ kilometers), while cross-view retrieval between ground and aerial imagery suffers from a domain gap and has been primarily studied on smaller regions. This paper introduces a hybrid approach that achieves fine-grained geo-localization across a large geographic expanse the size of a continent. We leverage a proxy classification task during training to learn rich feature representations that implicitly encode precise location information. We combine these learned prototypes with embeddings of aerial imagery to increase robustness to the sparsity of ground-level data. This enables direct, fine-grained retrieval over areas spanning multiple countries. Our extensive evaluation demonstrates that our approach can localize within 200m more than 68\\% of queries of a dataset covering a large part of Europe. The code is publicly available at https://scaling-geoloc.github.io.</article>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Quest for Generalizable Motion Generation: Data, Model, and Evaluation","url":"https://arxiv.org/abs/2510.26794","date":1761883200,"author":"","guid":322895,"unread":true,"content":"<article>arXiv:2510.26794v1 Announce Type: new \nAbstract: Despite recent advances in 3D human motion generation (MoGen) on standard benchmarks, existing models still face a fundamental bottleneck in their generalization capability. In contrast, adjacent generative fields, most notably video generation (ViGen), have demonstrated remarkable generalization in modeling human behaviors, highlighting transferable insights that MoGen can leverage. Motivated by this observation, we present a comprehensive framework that systematically transfers knowledge from ViGen to MoGen across three key pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a large-scale dataset comprising 228,000 high-quality motion samples that integrates high-fidelity optical MoCap data with semantically annotated motions from web videos and synthesized samples generated by state-of-the-art ViGen models. The dataset includes both text-motion pairs and text-video-motion triplets, substantially expanding semantic diversity. Second, we propose ViMoGen, a flow-matching-based diffusion transformer that unifies priors from MoCap data and ViGen models through gated multimodal conditioning. To enhance efficiency, we further develop ViMoGen-light, a distilled variant that eliminates video generation dependencies while preserving strong generalization. Finally, we present MBench, a hierarchical benchmark designed for fine-grained evaluation across motion quality, prompt fidelity, and generalization ability. Extensive experiments show that our framework significantly outperforms existing approaches in both automatic and human evaluations. The code, data, and benchmark will be made publicly available.</article>","contentLength":1694,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimized Log Parsing with Syntactic Modifications","url":"https://arxiv.org/abs/2510.26793","date":1761883200,"author":"","guid":322896,"unread":true,"content":"<article>arXiv:2510.26793v1 Announce Type: new \nAbstract: Logs provide valuable insights into system runtime and assist in software development and maintenance. Log parsing, which converts semi-structured log data into structured log data, is often the first step in automated log analysis. Given the wide range of log parsers utilizing diverse techniques, it is essential to evaluate them to understand their characteristics and performance. In this paper, we conduct a comprehensive empirical study comparing syntax- and semantic-based log parsers, as well as single-phase and two-phase parsing architectures. Our experiments reveal that semantic-based methods perform better at identifying the correct templates and syntax-based log parsers are 10 to 1,000 times more efficient and provide better grouping accuracy although they fall short in accurate template identification. Moreover, two-phase architecture consistently improves accuracy compared to single-phase architecture. Based on the findings of this study, we propose SynLog+, a template identification module that acts as the second phase in a two-phase log parsing architecture. SynLog+ improves the parsing accuracy of syntax-based and semantic-based log parsers by 236\\% and 20\\% on average, respectively, with virtually no additional runtime cost.</article>","contentLength":1306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability","url":"https://arxiv.org/abs/2510.26792","date":1761883200,"author":"","guid":322897,"unread":true,"content":"<article>arXiv:2510.26792v1 Announce Type: new \nAbstract: We study the ability of Transformer models to learn sequences generated by Permuted Congruential Generators (PCGs), a widely used family of pseudo-random number generators (PRNGs). PCGs introduce substantial additional difficulty over linear congruential generators (LCGs) by applying a series of bit-wise shifts, XORs, rotations and truncations to the hidden state. We show that Transformers can nevertheless successfully perform in-context prediction on unseen sequences from diverse PCG variants, in tasks that are beyond published classical attacks. In our experiments we scale moduli up to $2^{22}$ using up to $50$ million model parameters and datasets with up to $5$ billion tokens. Surprisingly, we find even when the output is truncated to a single bit, it can be reliably predicted by the model. When multiple distinct PRNGs are presented together during training, the model can jointly learn them, identifying structures from different permutations. We demonstrate a scaling law with modulus $m$: the number of in-context sequence elements required for near-perfect prediction grows as $\\sqrt{m}$. For larger moduli, optimization enters extended stagnation phases; in our experiments, learning moduli $m \\geq 2^{20}$ requires incorporating training data from smaller moduli, demonstrating a critical necessity for curriculum learning. Finally, we analyze embedding layers and uncover a novel clustering phenomenon: the model spontaneously groups the integer inputs into bitwise rotationally-invariant clusters, revealing how representations can transfer from smaller to larger moduli.</article>","contentLength":1644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gistify! Codebase-Level Understanding via Runtime Execution","url":"https://arxiv.org/abs/2510.26790","date":1761883200,"author":"","guid":322898,"unread":true,"content":"<article>arXiv:2510.26790v1 Announce Type: new \nAbstract: As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.</article>","contentLength":971,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Defeating the Training-Inference Mismatch via FP16","url":"https://arxiv.org/abs/2510.26788","date":1761883200,"author":"","guid":322899,"unread":true,"content":"<article>arXiv:2510.26788v1 Announce Type: new \nAbstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \\textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.</article>","contentLength":1118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Remote Labor Index: Measuring AI Automation of Remote Work","url":"https://arxiv.org/abs/2510.26787","date":1761883200,"author":"","guid":322900,"unread":true,"content":"<article>arXiv:2510.26787v1 Announce Type: new \nAbstract: AIs have made rapid progress on research-oriented benchmarks of knowledge and reasoning, but it remains unclear how these gains translate into economic value and automation. To measure this, we introduce the Remote Labor Index (RLI), a broadly multi-sector benchmark comprising real-world, economically valuable projects designed to evaluate end-to-end agent performance in practical settings. AI agents perform near the floor on RLI, with the highest-performing agent achieving an automation rate of 2.5%. These results help ground discussions of AI automation in empirical evidence, setting a common basis for tracking AI impacts and enabling stakeholders to proactively navigate AI-driven labor automation.</article>","contentLength":758,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HEIR: Learning Graph-Based Motion Hierarchies","url":"https://arxiv.org/abs/2510.26786","date":1761883200,"author":"","guid":322901,"unread":true,"content":"<article>arXiv:2510.26786v1 Announce Type: new \nAbstract: Hierarchical structures of motion exist across research fields, including computer vision, graphics, and robotics, where complex dynamics typically arise from coordinated interactions among simpler motion components. Existing methods to model such dynamics typically rely on manually-defined or heuristic hierarchies with fixed motion primitives, limiting their generalizability across different tasks. In this work, we propose a general hierarchical motion modeling method that learns structured, interpretable motion relationships directly from data. Our method represents observed motions using graph-based hierarchies, explicitly decomposing global absolute motions into parent-inherited patterns and local motion residuals. We formulate hierarchy inference as a differentiable graph learning problem, where vertices represent elemental motions and directed edges capture learned parent-child dependencies through graph neural networks. We evaluate our hierarchical reconstruction approach on three examples: 1D translational motion, 2D rotational motion, and dynamic 3D scene deformation via Gaussian splatting. Experimental results show that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases, and produces more realistic and interpretable deformations compared to the baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable, data-driven hierarchical modeling paradigm, our method offers a formulation applicable to a broad range of motion-centric tasks. Project Page: https://light.princeton.edu/HEIR/</article>","contentLength":1600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLMs Process Lists With General Filter Heads","url":"https://arxiv.org/abs/2510.26784","date":1761883200,"author":"","guid":322902,"unread":true,"content":"<article>arXiv:2510.26784v1 Announce Type: new \nAbstract: We investigate the mechanisms underlying a range of list-processing tasks in LLMs, and we find that LLMs have learned to encode a compact, causal representation of a general filtering operation that mirrors the generic \"filter\" function of functional programming. Using causal mediation analysis on a diverse set of list-processing tasks, we find that a small number of attention heads, which we dub filter heads, encode a compact representation of the filtering predicate in their query states at certain tokens. We demonstrate that this predicate representation is general and portable: it can be extracted and reapplied to execute the same filtering operation on different collections, presented in different formats, languages, or even in tasks. However, we also identify situations where transformer LMs can exploit a different strategy for filtering: eagerly evaluating if an item satisfies the predicate and storing this intermediate result as a flag directly in the item representations. Our results reveal that transformer LMs can develop human-interpretable implementations of abstract computational operations that generalize in ways that are surprisingly similar to strategies used in traditional functional programming patterns.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Clone Deterministic 3D Worlds with Geometrically-Regularized World Models","url":"https://arxiv.org/abs/2510.26782","date":1761883200,"author":"","guid":322903,"unread":true,"content":"<article>arXiv:2510.26782v1 Announce Type: new \nAbstract: A world model is an internal model that simulates how the world evolves. Given past observations and actions, it predicts the future of both the embodied agent and its environment. Accurate world models are essential for enabling agents to think, plan, and reason effectively in complex, dynamic settings. Despite rapid progress, current world models remain brittle and degrade over long horizons. We argue that a central cause is representation quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or entangled latents make dynamics learning unnecessarily hard. We therefore ask whether improving representation learning alone can substantially improve world-model performance. In this work, we take a step toward building a truly accurate world model by addressing a fundamental yet open problem: constructing a model that can fully clone and overfit to a deterministic 3D world. We propose Geometrically-Regularized World Models (GRWM), which enforces that consecutive points along a natural sensory trajectory remain close in latent representation space. This approach yields significantly improved latent representations that align closely with the true topology of the environment. GRWM is plug-and-play, requires only minimal architectural modification, scales with trajectory length, and is compatible with diverse latent generative backbones. Across deterministic 3D settings and long-horizon prediction tasks, GRWM significantly increases rollout fidelity and stability. Analyses show that its benefits stem from learning a latent manifold with superior geometric structure. These findings support a clear takeaway: improving representation learning is a direct and useful path to robust world models, delivering reliable long-horizon predictions without enlarging the dynamics module.</article>","contentLength":1868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChartAB: A Benchmark for Chart Grounding & Dense Alignment","url":"https://arxiv.org/abs/2510.26781","date":1761883200,"author":"","guid":322904,"unread":true,"content":"<article>arXiv:2510.26781v1 Announce Type: new \nAbstract: Charts play an important role in visualization, reasoning, data analysis, and the exchange of ideas among humans. However, existing vision-language models (VLMs) still lack accurate perception of details and struggle to extract fine-grained structures from charts. Such limitations in chart grounding also hinder their ability to compare multiple charts and reason over them. In this paper, we introduce a novel \"ChartAlign Benchmark (ChartAB)\" to provide a comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting tabular data, localizing visualization elements, and recognizing various attributes from charts of diverse types and complexities. We design a JSON template to facilitate the calculation of evaluation metrics specifically tailored for each grounding task. By incorporating a novel two-stage inference workflow, the benchmark can further evaluate VLMs' capability to align and compare elements/attributes across two charts. Our analysis of evaluations on several recent VLMs reveals new insights into their perception biases, weaknesses, robustness, and hallucinations in chart understanding. These findings highlight the fine-grained discrepancies among VLMs in chart understanding tasks and point to specific skills that need to be strengthened in current models.</article>","contentLength":1347,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance","url":"https://arxiv.org/abs/2510.26778","date":1761883200,"author":"","guid":322905,"unread":true,"content":"<article>arXiv:2510.26778v1 Announce Type: new \nAbstract: Age-related macular degeneration (AMD) is one of the leading causes of irreversible vision impairment in people over the age of 60. This research focuses on semantic segmentation for AMD lesion detection in RGB fundus images, a non-invasive and cost-effective imaging technique. The results of the ADAM challenge - the most comprehensive AMD detection from RGB fundus images research competition and open dataset to date - serve as a benchmark for our evaluation. Taking the U-Net connectivity as a base of our framework, we evaluate and compare several approaches to improve the segmentation model's architecture and training pipeline, including pre-processing techniques, encoder (backbone) deep network types of varying complexity, and specialized loss functions to mitigate class imbalances on image and pixel levels. The main outcome of this research is the final configuration of the AMD detection framework, which outperforms all the prior ADAM challenge submissions on the multi-class segmentation of different AMD lesion types in non-invasive RGB fundus images. The source code used to conduct the experiments presented in this paper is made freely available.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification","url":"https://arxiv.org/abs/2510.26777","date":1761883200,"author":"","guid":322906,"unread":true,"content":"<article>arXiv:2510.26777v1 Announce Type: new \nAbstract: Recent research on time series foundation models has primarily focused on forecasting, leaving it unclear how generalizable their learned representations are. In this study, we examine whether frozen pre-trained forecasting models can provide effective representations for classification. To this end, we compare different representation extraction strategies and introduce two model-agnostic embedding augmentations. Our experiments show that the best forecasting models achieve classification accuracy that matches or even surpasses that of state-of-the-art models pre-trained specifically for classification. Moreover, we observe a positive correlation between forecasting and classification performance. These findings challenge the assumption that task-specific pre-training is necessary, and suggest that learning to forecast may provide a powerful route toward constructing general-purpose time series foundation models.</article>","contentLength":976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Faithful and Fast Influence Function via Advanced Sampling","url":"https://arxiv.org/abs/2510.26776","date":1761883200,"author":"","guid":322907,"unread":true,"content":"<article>arXiv:2510.26776v1 Announce Type: new \nAbstract: How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1% and memory usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization","url":"https://arxiv.org/abs/2510.26771","date":1761883200,"author":"","guid":322908,"unread":true,"content":"<article>arXiv:2510.26771v1 Announce Type: new \nAbstract: Quantization is the key method for reducing inference latency, power and memory footprint of generative AI models. However, accuracy often degrades sharply when activations are quantized below eight bits. Recent work suggests that invertible linear transformations (e.g. rotations) can aid quantization, by reparameterizing feature channels and weights. In this paper, we propose \\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a novel strategy that applies linear transformations along the \\textit{sequence} dimension to exploit the strong local correlation in language and visual data. By keeping a small number of tokens in each intermediate activation at higher precision, we can maintain model accuracy at lower (average) activations bit-widths. We evaluate STaMP on recent LVM and LLM architectures, demonstrating that it significantly improves low bit width activation quantization and complements established activation and weight quantization methods including recent feature transformations.</article>","contentLength":1077,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models","url":"https://arxiv.org/abs/2510.26769","date":1761883200,"author":"","guid":322909,"unread":true,"content":"<article>arXiv:2510.26769v1 Announce Type: new \nAbstract: This work introduces SteerVLM, a lightweight steering module designed to guide Vision-Language Models (VLMs) towards outputs that better adhere to desired instructions. Our approach learns from the latent embeddings of paired prompts encoding target and converse behaviors to dynamically adjust activations connecting the language modality with image context. This allows for fine-grained, inference-time control over complex output semantics without modifying model weights while preserving performance on off-target tasks. Our steering module requires learning parameters equal to 0.14% of the original VLM's size. Our steering module gains model control through dimension-wise activation modulation and adaptive steering across layers without requiring pre-extracted static vectors or manual tuning of intervention points. Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a multimodal dataset specifically created to facilitate the development and evaluation of VLM steering techniques. Our method outperforms existing intervention techniques on steering and hallucination mitigation benchmarks for VLMs and proposes a robust solution for multimodal model control through activation engineering.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMO-Bench: Large Language Models Still Struggle in High School Math Competitions","url":"https://arxiv.org/abs/2510.26768","date":1761883200,"author":"","guid":322910,"unread":true,"content":"<article>arXiv:2510.26768v1 Announce Type: new \nAbstract: We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. https://amo-bench.github.io/</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Oversight Game: Learning to Cooperatively Balance an AI Agent's Safety and Autonomy","url":"https://arxiv.org/abs/2510.26752","date":1761883200,"author":"","guid":322911,"unread":true,"content":"<article>arXiv:2510.26752v1 Announce Type: new \nAbstract: As increasingly capable agents are deployed, a central safety question is how to retain meaningful human control without modifying the underlying system. We study a minimal control interface where an agent chooses whether to act autonomously (play) or defer (ask), while a human simultaneously chooses whether to be permissive (trust) or to engage in oversight (oversee). If the agent defers, the human's choice determines the outcome, potentially leading to a corrective action or a system shutdown. We model this interaction as a two-player Markov Game. Our analysis focuses on cases where this game qualifies as a Markov Potential Game (MPG), a class of games where we can provide an alignment guarantee: under a structural assumption on the human's value function, any decision by the agent to act more autonomously that benefits itself cannot harm the human's value. We also analyze extensions to this MPG framework. Theoretically, this perspective provides conditions for a specific form of intrinsic alignment. If the reward structures of the human-agent game meet these conditions, we have a formal guarantee that the agent improving its own outcome will not harm the human's. Practically, this model motivates a transparent control layer with predictable incentives where the agent learns to defer when risky and act when safe, while its pretrained policy and the environment's reward structure remain untouched. Our gridworld simulation shows that through independent learning, the agent and human discover their optimal oversight roles. The agent learns to ask when uncertain and the human learns when to oversee, leading to an emergent collaboration that avoids safety violations introduced post-training. This demonstrates a practical method for making misaligned models safer after deployment.</article>","contentLength":1856,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProfOlaf: Semi-Automated Tool for Systematic Literature Reviews","url":"https://arxiv.org/abs/2510.26750","date":1761883200,"author":"","guid":322912,"unread":true,"content":"<article>arXiv:2510.26750v1 Announce Type: new \nAbstract: Systematic reviews and mapping studies are critical for synthesizing research, identifying gaps, and guiding future work, but they are often labor-intensive and time-consuming. Existing tools provide partial support for specific steps, leaving much of the process manual and error-prone. We present ProfOlaf, a semi-automated tool designed to streamline systematic reviews while maintaining methodological rigor. ProfOlaf supports iterative snowballing for article collection with human-in-the-loop filtering and uses large language models to assist in analyzing articles, extracting key topics, and answering queries about the content of papers. By combining automation with guided manual effort, ProfOlaf enhances the efficiency, quality, and reproducibility of systematic reviews across research fields. A video describing and demonstrating ProfOlaf is available at: https://youtu.be/4noUXfcmxsE</article>","contentLength":947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep sequence models tend to memorize geometrically; it is unclear why","url":"https://arxiv.org/abs/2510.26745","date":1761883200,"author":"","guid":322913,"unread":true,"content":"<article>arXiv:2510.26745v1 Announce Type: new \nAbstract: In sequence modeling, the parametric memory of atomic facts has been predominantly abstracted as a brute-force lookup of co-occurrences between entities. We contrast this associative view against a geometric view of how memory is stored. We begin by isolating a clean and analyzable instance of Transformer reasoning that is incompatible with memory as strictly a storage of the local co-occurrences specified during training. Instead, the model must have somehow synthesized its own geometry of atomic facts, encoding global relationships between all entities, including non-co-occurring ones. This in turn has simplified a hard reasoning task involving an $\\ell$-fold composition into an easy-to-learn 1-step geometric task.\n  From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, despite optimizing over mere local associations, cannot be straightforwardly attributed to typical architectural or optimizational pressures. Counterintuitively, an elegant geometry is learned even when it is not more succinct than a brute-force lookup of associations.\n  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery and unlearning.</article>","contentLength":1718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Running VLAs at Real-time Speed","url":"https://arxiv.org/abs/2510.26742","date":1761883200,"author":"","guid":322914,"unread":true,"content":"<article>arXiv:2510.26742v1 Announce Type: new \nAbstract: In this paper, we show how to run pi0-level multi-view VLA at 30Hz frame rate and at most 480Hz trajectory frequency using a single consumer GPU. This enables dynamic and real-time tasks that were previously believed to be unattainable by large VLA models. To achieve it, we introduce a bag of strategies to eliminate the overheads in model inference. The real-world experiment shows that the pi0 policy with our strategy achieves a 100% success rate in grasping a falling pen task. Based on the results, we further propose a full streaming inference framework for real-time robot control of VLA. Code is available at https://github.com/Dexmal/realtime-vla.</article>","contentLength":706,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A General Incentives-Based Framework for Fairness in Multi-agent Resource Allocation","url":"https://arxiv.org/abs/2510.26740","date":1761883200,"author":"","guid":322915,"unread":true,"content":"<article>arXiv:2510.26740v1 Announce Type: new \nAbstract: We introduce the General Incentives-based Framework for Fairness (GIFF), a novel approach for fair multi-agent resource allocation that infers fair decision-making from standard value functions. In resource-constrained settings, agents optimizing for efficiency often create inequitable outcomes. Our approach leverages the action-value (Q-)function to balance efficiency and fairness without requiring additional training. Specifically, our method computes a local fairness gain for each action and introduces a counterfactual advantage correction term to discourage over-allocation to already well-off agents. This approach is formalized within a centralized control setting, where an arbitrator uses the GIFF-modified Q-values to solve an allocation problem.\n  Empirical evaluations across diverse domains, including dynamic ridesharing, homelessness prevention, and a complex job allocation task-demonstrate that our framework consistently outperforms strong baselines and can discover far-sighted, equitable policies. The framework's effectiveness is supported by a theoretical foundation; we prove its fairness surrogate is a principled lower bound on the true fairness improvement and that its trade-off parameter offers monotonic tuning. Our findings establish GIFF as a robust and principled framework for leveraging standard reinforcement learning components to achieve more equitable outcomes in complex multi-agent systems.</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models","url":"https://arxiv.org/abs/2510.26732","date":1761883200,"author":"","guid":322916,"unread":true,"content":"<article>arXiv:2510.26732v1 Announce Type: new \nAbstract: This paper presents a comprehensive cross-platform evaluation of reasoning capabilities in contemporary foundation models, establishing an infrastructure-agnostic benchmark across three computational paradigms: HPC supercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), and university clusters (a node with eight H200 GPUs).\n  We evaluate 15 foundation models across 79 problems spanning eight academic domains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics, Calculus, and Optimization) through three experimental phases: (1) Baseline establishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b, Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishing methodology and reference performance; (2) Infrastructure validation: The 19-problem benchmark repeated on university cluster (seven models including Falcon-Mamba state-space architecture) and Nebius AI Studio (nine state-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen3 30B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnostic reproducibility; (3) Extended evaluation: Full 79-problem assessment on both university cluster and Nebius platforms, probing generalization at scale across architectural diversity.\n  The findings challenge conventional scaling assumptions, establish training data quality as more critical than model size, and provide actionable guidelines for model selection across educational, production, and research contexts. The tri-infrastructure methodology and 79-problem benchmark enable longitudinal tracking of reasoning capabilities as foundation models evolve.</article>","contentLength":1694,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference","url":"https://arxiv.org/abs/2510.26730","date":1761883200,"author":"","guid":322917,"unread":true,"content":"<article>arXiv:2510.26730v1 Announce Type: new \nAbstract: The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.</article>","contentLength":1683,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off","url":"https://arxiv.org/abs/2510.26722","date":1761883200,"author":"","guid":322918,"unread":true,"content":"<article>arXiv:2510.26722v1 Announce Type: new \nAbstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \\emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \\emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.</article>","contentLength":1575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unveiling Intrinsic Text Bias in Multimodal Large Language Models through Attention Key-Space Analysis","url":"https://arxiv.org/abs/2510.26721","date":1761883200,"author":"","guid":322919,"unread":true,"content":"<article>arXiv:2510.26721v1 Announce Type: new \nAbstract: Multimodal large language models (MLLMs) exhibit a pronounced preference for textual inputs when processing vision-language data, limiting their ability to reason effectively from visual evidence. Unlike prior studies that attribute this text bias to external factors such as data imbalance or instruction tuning, we propose that the bias originates from the model's internal architecture. Specifically, we hypothesize that visual key vectors (Visual Keys) are out-of-distribution (OOD) relative to the text key space learned during language-only pretraining. Consequently, these visual keys receive systematically lower similarity scores during attention computation, leading to their under-utilization in the context representation. To validate this hypothesis, we extract key vectors from LLaVA and Qwen2.5-VL and analyze their distributional structures using qualitative (t-SNE) and quantitative (Jensen-Shannon divergence) methods. The results provide direct evidence that visual and textual keys occupy markedly distinct subspaces within the attention space. The inter-modal divergence is statistically significant, exceeding intra-modal variation by several orders of magnitude. These findings reveal that text bias arises from an intrinsic misalignment within the attention key space rather than solely from external data factors.</article>","contentLength":1387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Purely Private Covariance Estimation","url":"https://arxiv.org/abs/2510.26717","date":1761883200,"author":"","guid":322920,"unread":true,"content":"<article>arXiv:2510.26717v1 Announce Type: new \nAbstract: We present a simple perturbation mechanism for the release of $d$-dimensional covariance matrices $\\Sigma$ under pure differential privacy. For large datasets with at least $n\\geq d^2/\\varepsilon$ elements, our mechanism recovers the provably optimal Frobenius norm error guarantees of \\cite{nikolov2023private}, while simultaneously achieving best known error for all other $p$-Schatten norms, with $p\\in [1,\\infty]$. Our error is information-theoretically optimal for all $p\\ge 2$, in particular, our mechanism is the first purely private covariance estimator that achieves optimal error in spectral norm.\n  For small datasets $n&lt; d^2/\\varepsilon$, we further show that by projecting the output onto the nuclear norm ball of appropriate radius, our algorithm achieves the optimal Frobenius norm error $O(\\sqrt{d\\;\\text{Tr}(\\Sigma) /n})$, improving over the known bounds of $O(\\sqrt{d/n})$ of \\cite{nikolov2023private} and ${O}\\big(d^{3/4}\\sqrt{\\text{Tr}(\\Sigma)/n}\\big)$ of \\cite{dong2022differentially}.</article>","contentLength":1055,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LSM-MS2: A Foundation Model Bridging Spectral Identification and Biological Interpretation","url":"https://arxiv.org/abs/2510.26715","date":1761883200,"author":"","guid":322921,"unread":true,"content":"<article>arXiv:2510.26715v1 Announce Type: new \nAbstract: A vast majority of mass spectrometry data remains uncharacterized, leaving much of its biological and chemical information untapped. Recent advances in machine learning have begun to address this gap, particularly for tasks such as spectral identification in tandem mass spectrometry data. Here, we present the latest generation of LSM-MS2, a large-scale deep learning foundation model trained on millions of spectra to learn a semantic chemical space. LSM-MS2 achieves state-of-the-art performance in spectral identification, improving on existing methods by 30% in accuracy of identifying challenging isomeric compounds, yielding 42% more correct identifications in complex biological samples, and maintaining robustness under low-concentration conditions. Furthermore, LSM-MS2 produces rich spectral embeddings that enable direct biological interpretation from minimal downstream data, successfully differentiating disease states and predicting clinical outcomes across diverse translational applications.</article>","contentLength":1057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the limitation of evaluating machine unlearning using only a single training seed","url":"https://arxiv.org/abs/2510.26714","date":1761883200,"author":"","guid":322922,"unread":true,"content":"<article>arXiv:2510.26714v1 Announce Type: new \nAbstract: Machine unlearning (MU) aims to remove the influence of certain data points from a trained model without costly retraining. Most practical MU algorithms are only approximate and their performance can only be assessed empirically. Care must therefore be taken to make empirical comparisons as representative as possible. A common practice is to run the MU algorithm multiple times independently starting from the same trained model. In this work, we demonstrate that this practice can give highly non-representative results because -- even for the same architecture and same dataset -- some MU methods can be highly sensitive to the choice of random number seed used for model training. We therefore recommend that empirical comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should also reflect the variability across different model training seeds.</article>","contentLength":917,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time-Optimal Model Predictive Control for Linear Systems with Multiplicative Uncertainties","url":"https://arxiv.org/abs/2510.26712","date":1761883200,"author":"","guid":322923,"unread":true,"content":"<article>arXiv:2510.26712v1 Announce Type: new \nAbstract: This paper presents a time-optimal Model Predictive Control (MPC) scheme for linear discrete-time systems subject to multiplicative uncertainties represented by interval matrices. To render the uncertainty propagation computationally tractable, the set-valued error system dynamics are approximated using a matrix-zonotope-based bounding operator. Recursive feasibility and finite-time convergence are ensured through an adaptive terminal constraint mechanism. A key advantage of the proposed approach is that all the necessary bounding sets can be computed offline, substantially reducing the online computational burden. The effectiveness of the method is illustrated via a numerical case study on an orbital rendezvous maneuver between two satellites.</article>","contentLength":803,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning","url":"https://arxiv.org/abs/2510.26709","date":1761883200,"author":"","guid":322924,"unread":true,"content":"<article>arXiv:2510.26709v1 Announce Type: new \nAbstract: Communication remains a central bottleneck in large-scale distributed machine learning, and gradient sparsification has emerged as a promising strategy to alleviate this challenge. However, existing gradient compressors face notable limitations: Rand-$K$\\ discards structural information and performs poorly in practice, while Top-$K$\\ preserves informative entries but loses the contraction property and requires costly All-Gather operations. In this paper, we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that aligns sparsity patterns across nodes using a lightweight sketch of the gradient, enabling index-free All-Reduce while preserving globally significant information. ARC-Top-$K$\\ is provably contractive and, when combined with momentum error feedback (EF21M), achieves linear speedup and sharper convergence rates than the original EF21M under standard assumptions. Empirically, ARC-Top-$K$\\ matches the accuracy of Top-$K$\\ while reducing wall-clock training time by up to 60.7\\%, offering an efficient and scalable solution that combines the robustness of Rand-$K$\\ with the strong performance of Top-$K$.</article>","contentLength":1188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pareto-Optimal Sampling and Resource Allocation for Timely Communication in Shared-Spectrum Low-Altitude Networks","url":"https://arxiv.org/abs/2510.26708","date":1761883200,"author":"","guid":322925,"unread":true,"content":"<article>arXiv:2510.26708v1 Announce Type: new \nAbstract: Guaranteeing stringent data freshness for low-altitude unmanned aerial vehicles (UAVs) in shared spectrum forces a critical trade-off between two operational costs: the UAV's own energy consumption and the occupation of terrestrial channel resources. The core challenge is to satisfy the aerial data freshness while finding a Pareto-optimal balance between these costs. Leveraging predictive channel models and predictive UAV trajectories, we formulate a bi-objective Pareto optimization problem over a long-term planning horizon to jointly optimize the sampling timing for aerial traffic and the power and spectrum allocation for fair coexistence. However, the problem's non-convex, mixed-integer nature renders classical methods incapable of fully characterizing the complete Pareto frontier. Notably, we show monotonicity properties of the frontier, building on which we transform the bi-objective problem into several single-objective problems. We then propose a new graph-based algorithm and prove that it can find the complete set of Pareto optima with low complexity, linear in the horizon and near-quadratic in the resource block (RB) budget. Numerical comparisons show that our approach meets the stringent timeliness requirement and achieves a six-fold reduction in RB utilization or a 6 dB energy saving compared to benchmarks.</article>","contentLength":1387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Value Drifts: Tracing Value Alignment During LLM Post-Training","url":"https://arxiv.org/abs/2510.26707","date":1761883200,"author":"","guid":322926,"unread":true,"content":"<article>arXiv:2510.26707v1 Announce Type: new \nAbstract: As LLMs occupy an increasingly important role in society, they are more and more confronted with questions that require them not only to draw on their general knowledge but also to align with certain human value systems. Therefore, studying the alignment of LLMs with human values has become a crucial field of inquiry. Prior work, however, mostly focuses on evaluating the alignment of fully trained models, overlooking the training dynamics by which models learn to express human values. In this work, we investigate how and at which stage value alignment arises during the course of a model's post-training. Our analysis disentangles the effects of post-training algorithms and datasets, measuring both the magnitude and time of value drifts during training. Experimenting with Llama-3 and Qwen-3 models of different sizes and popular supervised fine-tuning (SFT) and preference optimization datasets and algorithms, we find that the SFT phase generally establishes a model's values, and subsequent preference optimization rarely re-aligns these values. Furthermore, using a synthetic preference dataset that enables controlled manipulation of values, we find that different preference optimization algorithms lead to different value alignment outcomes, even when preference data is held constant. Our findings provide actionable insights into how values are learned during post-training and help to inform data curation, as well as the selection of models and algorithms for preference optimization to improve model alignment to human values.</article>","contentLength":1595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Budgeted Multiple-Expert Deferral","url":"https://arxiv.org/abs/2510.26706","date":1761883200,"author":"","guid":322927,"unread":true,"content":"<article>arXiv:2510.26706v1 Announce Type: new \nAbstract: Learning to defer uncertain predictions to costly experts offers a powerful strategy for improving the accuracy and efficiency of machine learning systems. However, standard training procedures for deferral algorithms typically require querying all experts for every training instance, an approach that becomes prohibitively expensive when expert queries incur significant computational or resource costs. This undermines the core goal of deferral: to limit unnecessary expert usage. To overcome this challenge, we introduce the budgeted deferral framework, which aims to train effective deferral algorithms while minimizing expert query costs during training. We propose new algorithms for both two-stage and single-stage multiple-expert deferral settings that selectively query only a subset of experts per training example. While inspired by active learning, our setting is fundamentally different: labels are already known, and the core challenge is to decide which experts to query in order to balance cost and predictive performance. We establish theoretical guarantees for both of our algorithms, including generalization bounds and label complexity analyses. Empirical results across several domains show that our algorithms substantially reduce training costs without sacrificing prediction accuracy, demonstrating the practical value of our budget-aware deferral algorithms.</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators","url":"https://arxiv.org/abs/2510.26704","date":1761883200,"author":"","guid":322928,"unread":true,"content":"<article>arXiv:2510.26704v1 Announce Type: new \nAbstract: Can regularization terms in the training of invertible neural networks lead to known Bayesian point estimators in reconstruction? Invertible networks are attractive for inverse problems due to their inherent stability and interpretability. Recently, optimization strategies for invertible neural networks that approximate either a reconstruction map or the forward operator have been studied from a Bayesian perspective, but each has limitations. To address this, we introduce and analyze two regularization terms for the network training that, upon inversion of the network, recover properties of classical Bayesian point estimators: while the first can be connected to the posterior mean, the second resembles the MAP estimator. Our theoretical analysis characterizes how each loss shapes both the learned forward operator and its inverse reconstruction map. Numerical experiments support our findings and demonstrate how these loss-term regularizers introduce data-dependence in a stable and interpretable way.</article>","contentLength":1062,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delegated Authorization for Agents Constrained to Semantic Task-to-Scope Matching","url":"https://arxiv.org/abs/2510.26702","date":1761883200,"author":"","guid":322929,"unread":true,"content":"<article>arXiv:2510.26702v1 Announce Type: new \nAbstract: Authorizing Large Language Model driven agents to dynamically invoke tools and access protected resources introduces significant risks, since current methods for delegating authorization grant overly broad permissions and give access to tools allowing agents to operate beyond the intended task scope. We introduce and assess a delegated authorization model enabling authorization servers to semantically inspect access requests to protected resources, and issue access tokens constrained to the minimal set of scopes necessary for the agents' assigned tasks. Given the unavailability of datasets centered on delegated authorization flows, particularly including both semantically appropriate and inappropriate scope requests for a given task, we introduce ASTRA, a dataset and data generation pipeline for benchmarking semantic matching between tasks and scopes. Our experiments show both the potential and current limitations of model-based matching, particularly as the number of scopes needed for task completion increases. Our results highlight the need for further research into semantic matching techniques enabling intent-aware authorization for multi-agent and tool-augmented applications, including fine-grained control, such as Task-Based Access Control (TBAC).</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph approach for observability analysis in power system dynamic state estimation","url":"https://arxiv.org/abs/2510.26701","date":1761883200,"author":"","guid":322930,"unread":true,"content":"<article>arXiv:2510.26701v1 Announce Type: new \nAbstract: The proposed approach yields a numerical method that provably executes in linear time with respect to the number of nodes and edges in a graph. The graph, constructed from the power system model, requires only knowledge of the dependencies between state-to-state and output-to-state variables within a state-space framework. While graph-based observability analysis methods exist for power system static-state estimation, the approach presented here is the first for dynamic-state estimation (DSE). We examine decentralized and centralized DSE scenarios and compare our findings with a well-established, albeit non-scalable, observability analysis method in the literature. When compared to the latter in a centralized DSE setting, our method reduced computation time by 1440x.</article>","contentLength":826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using Copilot Agent Mode to Automate Library Migration: A Quantitative Assessment","url":"https://arxiv.org/abs/2510.26699","date":1761883200,"author":"","guid":322931,"unread":true,"content":"<article>arXiv:2510.26699v1 Announce Type: new \nAbstract: Keeping software systems up to date is essential to avoid technical debt, security vulnerabilities, and the rigidity typical of legacy systems. However, updating libraries and frameworks remains a time consuming and error-prone process. Recent advances in Large Language Models (LLMs) and agentic coding systems offer new opportunities for automating such maintenance tasks. In this paper, we evaluate the update of a well-known Python library, SQLAlchemy, across a dataset of ten client applications. For this task, we use the Github's Copilot Agent Mode, an autonomous AI systema capable of planning and executing multi-step migration workflows. To assess the effectiveness of the automated migration, we also introduce Migration Coverage, a metric that quantifies the proportion of API usage points correctly migrated. The results of our study show that the LLM agent was capable of migrating functionalities and API usages between SQLAlchemy versions (migration coverage: 100%, median), but failed to maintain the application functionality, leading to a low test-pass rate (39.75%, median).</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The End of Manual Decoding: Towards Truly End-to-End Language Models","url":"https://arxiv.org/abs/2510.26697","date":1761883200,"author":"","guid":322932,"unread":true,"content":"<article>arXiv:2510.26697v1 Announce Type: new \nAbstract: The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly \"end-to-end\" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from \"hacking the test set\"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., \"generate with low randomness\") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Impact and Outlook of 3D Gaussian Splatting","url":"https://arxiv.org/abs/2510.26694","date":1761883200,"author":"","guid":322933,"unread":true,"content":"<article>arXiv:2510.26694v1 Announce Type: new \nAbstract: Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed the landscape of 3D scene representations, inspiring an extensive body of associated research. Follow-up work includes analyses and contributions that enhance the efficiency, scalability, and real-world applicability of 3DGS. In this summary, we present an overview of several key directions that have emerged in the wake of 3DGS. We highlight advances enabling resource-efficient training and rendering, the evolution toward dynamic (or four-dimensional, 4DGS) representations, and deeper exploration of the mathematical foundations underlying its appearance modeling and rendering process. Furthermore, we examine efforts to bring 3DGS to mobile and virtual reality platforms, its extension to massive-scale environments, and recent progress toward near-instant radiance field reconstruction via feed-forward or distributed computation. Collectively, these developments illustrate how 3DGS has evolved from a breakthrough representation into a versatile and foundational tool for 3D vision and graphics.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kimi Linear: An Expressive, Efficient Attention Architecture","url":"https://arxiv.org/abs/2510.26692","date":1761883200,"author":"","guid":322934,"unread":true,"content":"<article>arXiv:2510.26692v1 Announce Type: new \nAbstract: We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.\n  To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.</article>","contentLength":1591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits","url":"https://arxiv.org/abs/2510.26690","date":1761883200,"author":"","guid":322935,"unread":true,"content":"<article>arXiv:2510.26690v1 Announce Type: new \nAbstract: Low-Rank Adaptation (LoRA) has become a popular technique for parameter-efficient fine-tuning of large language models (LLMs). In many real-world scenarios, multiple adapters are loaded simultaneously to enable LLM customization for personalized user experiences or to support a diverse range of tasks. Although each adapter is lightweight in isolation, their aggregate cost becomes substantial at scale. To address this, we propose LoRAQuant, a mixed-precision post-training quantization method tailored to LoRA. Specifically, LoRAQuant reparameterizes each adapter by singular value decomposition (SVD) to concentrate the most important information into specific rows and columns. This makes it possible to quantize the important components to higher precision, while quantizing the rest to ultra-low bitwidth. We conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B models on mathematical reasoning, coding, and summarization tasks. Results show that our LoRAQuant uses significantly lower bits than other quantization methods, but achieves comparable or even higher performance.</article>","contentLength":1157,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill","url":"https://arxiv.org/abs/2510.26684","date":1761883200,"author":"","guid":322936,"unread":true,"content":"<article>arXiv:2510.26684v1 Announce Type: new \nAbstract: We present a long-term deployment study of a machine vision-based anomaly detection system for failure prediction in a steel rolling mill. The system integrates industrial cameras to monitor equipment operation, alignment, and hot bar motion in real time along the process line. Live video streams are processed on a centralized video server using deep learning models, enabling early prediction of equipment failures and process interruptions, thereby reducing unplanned breakdown costs. Server-based inference minimizes the computational load on industrial process control systems (PLCs), supporting scalable deployment across production lines with minimal additional resources. By jointly analyzing sensor data from data acquisition systems and visual inputs, the system identifies the location and probable root causes of failures, providing actionable insights for proactive maintenance. This integrated approach enhances operational reliability, productivity, and profitability in industrial manufacturing environments.</article>","contentLength":1074,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models","url":"https://arxiv.org/abs/2510.26683","date":1761883200,"author":"","guid":322937,"unread":true,"content":"<article>arXiv:2510.26683v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated exceptional capabilities across multiple domains by leveraging massive pre-training and curated fine-tuning data. However, in data-sensitive fields such as healthcare, the lack of high-quality, domain-specific training corpus hinders LLMs' adaptation for specialized applications. Meanwhile, domain experts have distilled domain wisdom into ontology rules, which formalize relationships among concepts and ensure the integrity of knowledge management repositories. Viewing LLMs as implicit repositories of human knowledge, we propose Evontree, a novel framework that leverages a small set of high-quality ontology rules to systematically extract, validate, and enhance domain knowledge within LLMs, without requiring extensive external datasets. Specifically, Evontree extracts domain ontology from raw models, detects inconsistencies using two core ontology rules, and reinforces the refined knowledge via self-distilled fine-tuning. Extensive experiments on medical QA benchmarks with Llama3-8B-Instruct and Med42-v2 demonstrate consistent outperformance over both unmodified models and leading supervised baselines, achieving up to a 3.7% improvement in accuracy. These results confirm the effectiveness, efficiency, and robustness of our approach for low-resource domain adaptation of LLMs.</article>","contentLength":1390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Classification of Occluded Objects through Scene Context","url":"https://arxiv.org/abs/2510.26681","date":1761883200,"author":"","guid":322938,"unread":true,"content":"<article>arXiv:2510.26681v1 Announce Type: new \nAbstract: The presence of occlusions has provided substantial challenges to typically-powerful object recognition algorithms. Additional sources of information can be extremely valuable to reduce errors caused by occlusions. Scene context is known to aid in object recognition in biological vision. In this work, we attempt to add robustness into existing Region Proposal Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks through two distinct scene-based information fusion techniques. We present one algorithm under each methodology: the first operates prior to prediction, selecting a custom object network to use based on the identified background scene, and the second operates after detection, fusing scene knowledge into initial object scores output by the RPN. We demonstrate our algorithms on challenging datasets featuring partial occlusions, which show overall improvement in both recall and precision against baseline methods. In addition, our experiments contrast multiple training methodologies for occlusion handling, finding that training on a combination of both occluded and unoccluded images demonstrates an improvement over the others. Our method is interpretable and can easily be adapted to other datasets, offering many future directions for research and practical applications.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tight Differentially Private PCA via Matrix Coherence","url":"https://arxiv.org/abs/2510.26679","date":1761883200,"author":"","guid":322939,"unread":true,"content":"<article>arXiv:2510.26679v1 Announce Type: new \nAbstract: We revisit the task of computing the span of the top $r$ singular vectors $u_1, \\ldots, u_r$ of a matrix under differential privacy. We show that a simple and efficient algorithm -- based on singular value decomposition and standard perturbation mechanisms -- returns a private rank-$r$ approximation whose error depends only on the \\emph{rank-$r$ coherence} of $u_1, \\ldots, u_r$ and the spectral gap $\\sigma_r - \\sigma_{r+1}$. This resolves a question posed by Hardt and Roth~\\cite{hardt2013beyond}. Our estimator outperforms the state of the art -- significantly so in some regimes. In particular, we show that in the dense setting, it achieves the same guarantees for single-spike PCA in the Wishart model as those attained by optimal non-private algorithms, whereas prior private algorithms failed to do so.\n  In addition, we prove that (rank-$r$) coherence does not increase under Gaussian perturbations. This implies that any estimator based on the Gaussian mechanism -- including ours -- preserves the coherence of the input. We conjecture that similar behavior holds for other structured models, including planted problems in graphs.\n  We also explore applications of coherence to graph problems. In particular, we present a differentially private algorithm for Max-Cut and other constraint satisfaction problems under low coherence assumptions.</article>","contentLength":1403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Process-based Indicators of Vulnerability Re-Introducing Code Changes: An Exploratory Case Study","url":"https://arxiv.org/abs/2510.26676","date":1761883200,"author":"","guid":322940,"unread":true,"content":"<article>arXiv:2510.26676v1 Announce Type: new \nAbstract: Software vulnerabilities often persist or re-emerge even after being fixed, revealing the complex interplay between code evolution and socio-technical factors. While source code metrics provide useful indicators of vulnerabilities, software engineering process metrics can uncover patterns that lead to their introduction. Yet few studies have explored whether process metrics can reveal risky development activities over time -- insights that are essential for anticipating and mitigating software vulnerabilities. This work highlights the critical role of process metrics along with code changes in understanding and mitigating vulnerability reintroduction. We move beyond file-level prediction and instead analyze security fixes at the commit level, focusing not only on whether a single fix introduces a vulnerability but also on the longer sequences of changes through which vulnerabilities evolve and re-emerge. Our approach emphasizes that reintroduction is rarely the result of one isolated action, but emerges from cumulative development activities and socio-technical conditions. To support this analysis, we conducted a case study on the ImageMagick project by correlating longitudinal process metrics such as bus factor, issue density, and issue spoilage with vulnerability reintroduction activities, encompassing 76 instances of reintroduced vulnerabilities. Our findings show that reintroductions often align with increased issue spoilage and fluctuating issue density, reflecting short-term inefficiencies in issue management and team responsiveness. These observations provide a foundation for broader studies that combine process and code metrics to predict risky fixes and strengthen software security.</article>","contentLength":1769,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and Real-Time Efficiency in Robotic Manipulation","url":"https://arxiv.org/abs/2510.26670","date":1761883200,"author":"","guid":322941,"unread":true,"content":"<article>arXiv:2510.26670v1 Announce Type: new \nAbstract: In visuomotor policy learning, diffusion-based imitation learning has become widely adopted for its ability to capture diverse behaviors. However, approaches built on ordinary and stochastic denoising processes struggle to jointly achieve fast sampling and strong multi-modality. To address these challenges, we propose the Hybrid Consistency Policy (HCP). HCP runs a short stochastic prefix up to an adaptive switch time, and then applies a one-step consistency jump to produce the final action. To align this one-jump generation, HCP performs time-varying consistency distillation that combines a trajectory-consistency objective to keep neighboring predictions coherent and a denoising-matching objective to improve local fidelity. In both simulation and on a real robot, HCP with 25 SDE steps plus one jump approaches the 80-step DDPM teacher in accuracy and mode coverage while significantly reducing latency. These results show that multi-modality does not require slow inference, and a switch time decouples mode retention from speed. It yields a practical accuracy efficiency trade-off for robot policies.</article>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Era of Agentic Organization: Learning to Organize with Language Models","url":"https://arxiv.org/abs/2510.26658","date":1761883200,"author":"","guid":322942,"unread":true,"content":"<article>arXiv:2510.26658v1 Announce Type: new \nAbstract: We envision a new era of AI, termed agentic organization, where agents solve complex problems by working collaboratively and concurrently, enabling outcomes beyond individual intelligence. To realize this vision, we introduce asynchronous thinking (AsyncThink) as a new paradigm of reasoning with large language models, which organizes the internal thinking process into concurrently executable structures. Specifically, we propose a thinking protocol where an organizer dynamically assigns sub-queries to workers, merges intermediate knowledge, and produces coherent solutions. More importantly, the thinking structure in this protocol can be further optimized through reinforcement learning. Experiments demonstrate that AsyncThink achieves 28% lower inference latency compared to parallel thinking while improving accuracy on mathematical reasoning. Moreover, AsyncThink generalizes its learned asynchronous thinking capabilities, effectively tackling unseen tasks without additional training.</article>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems","url":"https://arxiv.org/abs/2510.26656","date":1761883200,"author":"","guid":322943,"unread":true,"content":"<article>arXiv:2510.26656v1 Announce Type: new \nAbstract: In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridge and Bound: A Logic-Based Framework for Abstracting (Preliminary Report)","url":"https://arxiv.org/abs/2510.26654","date":1761883200,"author":"","guid":322944,"unread":true,"content":"<article>arXiv:2510.26654v1 Announce Type: new \nAbstract: At its core, abstraction is the process of generalizing from specific instances to broader concepts or models, with the primary objective of reducing complexity while preserving properties essential to the intended purpose. It is a fundamental, often implicit, principle that structures the understanding, communication, and development of both scientific knowledge and everyday beliefs. Studies on abstraction have evolved from its origins in Ancient Greek philosophy through methodological approaches in psychological and philosophical theories to computational frameworks.\n  Formally, abstraction can be understood as the transformation of a source representation into an abstract representation that discards certain details while retaining desirable features. In real-world modeling and reasoning, abstraction is crucial, particularly when managing imperfect or incomplete information that calls for approximate representations. This paper introduces a novel logic-based framework for modeling abstraction processes that goes beyond the traditional entailment of necessary conditions to encompass sufficient conditions as well. We define approximate abstractions, study their tightest and exact forms, and extend the approach to layered abstractions, enabling hierarchical simplification of complex systems and models. The computational complexity of the related reasoning tasks is also discussed.\n  For clarity, our framework is developed within classical logic, chosen for its simplicity, expressiveness, and computational friendliness.</article>","contentLength":1592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2","url":"https://arxiv.org/abs/2510.26653","date":1761883200,"author":"","guid":322945,"unread":true,"content":"<article>arXiv:2510.26653v1 Announce Type: new \nAbstract: Accurate estimation of sea ice drift is critical for Arctic navigation, climate research, and operational forecasting. While optical flow, a computer vision technique for estimating pixel wise motion between consecutive images, has advanced rapidly in computer vision, its applicability to geophysical problems and to satellite SAR imagery remains underexplored. Classical optical flow methods rely on mathematical models and strong assumptions about motion, which limit their accuracy in complex scenarios. Recent deep learning based approaches have substantially improved performance and are now the standard in computer vision, motivating their application to sea ice drift estimation. We present the first large scale benchmark of 48 deep learning optical flow models on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the spatial scales of sea ice motion and typical navigation requirements in the Arctic. Our results demonstrate that the models are capable of capturing consistent regional drift patterns and that recent deep learning based optical flow methods, which have substantially improved motion estimation accuracy compared to classical methods, can be effectively transferred to polar remote sensing. Optical flow produces spatially continuous drift fields, providing motion estimates for every image pixel rather than at sparse buoy locations, offering new opportunities for navigation and climate modeling.</article>","contentLength":1651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hybrid DQN-TD3 Reinforcement Learning for Autonomous Navigation in Dynamic Environments","url":"https://arxiv.org/abs/2510.26646","date":1761883200,"author":"","guid":322946,"unread":true,"content":"<article>arXiv:2510.26646v1 Announce Type: new \nAbstract: This paper presents a hierarchical path-planning and control framework that combines a high-level Deep Q-Network (DQN) for discrete sub-goal selection with a low-level Twin Delayed Deep Deterministic Policy Gradient (TD3) controller for continuous actuation. The high-level module selects behaviors and sub-goals; the low-level module executes smooth velocity commands. We design a practical reward shaping scheme (direction, distance, obstacle avoidance, action smoothness, collision penalty, time penalty, and progress), together with a LiDAR-based safety gate that prevents unsafe motions. The system is implemented in ROS + Gazebo (TurtleBot3) and evaluated with PathBench metrics, including success rate, collision rate, path efficiency, and re-planning efficiency, in dynamic and partially observable environments. Experiments show improved success rate and sample efficiency over single-algorithm baselines (DQN or TD3 alone) and rule-based planners, with better generalization to unseen obstacle configurations and reduced abrupt control changes. Code and evaluation scripts are available at the project repository.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Curly Flow Matching for Learning Non-gradient Field Dynamics","url":"https://arxiv.org/abs/2510.26645","date":1761883200,"author":"","guid":322947,"unread":true,"content":"<article>arXiv:2510.26645v1 Announce Type: new \nAbstract: Modeling the transport dynamics of natural processes from population-level observations is a ubiquitous problem in the natural sciences. Such models rely on key assumptions about the underlying process in order to enable faithful learning of governing dynamics that mimic the actual system behavior. The de facto assumption in current approaches relies on the principle of least action that results in gradient field dynamics and leads to trajectories minimizing an energy functional between two probability measures. However, many real-world systems, such as cell cycles in single-cell RNA, are known to exhibit non-gradient, periodic behavior, which fundamentally cannot be captured by current state-of-the-art methods such as flow and bridge matching. In this paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is capable of learning non-gradient field dynamics by designing and solving a Schr\\\"odinger bridge problem with a non-zero drift reference process -- in stark contrast to typical zero-drift reference processes -- which is constructed using inferred velocities in addition to population snapshot data. We showcase Curly-FM by solving the trajectory inference problems for single cells, computational fluid dynamics, and ocean currents with approximate velocities. We demonstrate that Curly-FM can learn trajectories that better match both the reference process and population marginals. Curly-FM expands flow matching models beyond the modeling of populations and towards the modeling of known periodic behavior in physical systems. Our code repository is accessible at: https://github.com/kpetrovicc/curly-flow-matching.git</article>","contentLength":1709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection","url":"https://arxiv.org/abs/2510.26643","date":1761883200,"author":"","guid":322948,"unread":true,"content":"<article>arXiv:2510.26643v1 Announce Type: new \nAbstract: Anomaly detection is a fundamental task for time series analytics with important implications for the downstream performance of many applications. Despite increasing academic interest and the large number of methods proposed in the literature, recent benchmarks and evaluation studies demonstrated that no overall best anomaly detection methods exist when applied to very heterogeneous time series datasets. Therefore, the only scalable and viable solution to solve anomaly detection over very different time series collected from diverse domains is to propose a model selection method that will select, based on time series characteristics, the best anomaly detection methods to run. Existing AutoML solutions are, unfortunately, not directly applicable to time series anomaly detection, and no evaluation of time series-based approaches for model selection exists. Towards that direction, this paper studies the performance of time series classification methods used as model selection for anomaly detection. In total, we evaluate 234 model configurations derived from 16 base classifiers across more than 1980 time series, and we propose the first extensive experimental evaluation of time series classification as model selection for anomaly detection. Our results demonstrate that model selection methods outperform every single anomaly detection method while being in the same order of magnitude regarding execution time. This evaluation is the first step to demonstrate the accuracy and efficiency of time series classification algorithms for anomaly detection, and represents a strong baseline that can then be used to guide the model selection step in general AutoML pipelines. Preprint version of an article accepted at the VLDB Journal.</article>","contentLength":1796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles","url":"https://arxiv.org/abs/2510.26641","date":1761883200,"author":"","guid":322949,"unread":true,"content":"<article>arXiv:2510.26641v1 Announce Type: new \nAbstract: Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.</article>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"REALMS2 - Resilient Exploration And Lunar Mapping System 2 - A Comprehensive Approach","url":"https://arxiv.org/abs/2510.26638","date":1761883200,"author":"","guid":322950,"unread":true,"content":"<article>arXiv:2510.26638v1 Announce Type: new \nAbstract: The European Space Agency (ESA) and the European Space Resources Innovation Centre (ESRIC) created the Space Resources Challenge to invite researchers and companies to propose innovative solutions for Multi-Robot Systems (MRS) space prospection. This paper proposes the Resilient Exploration And Lunar Mapping System 2 (REALMS2), a MRS framework for planetary prospection and mapping. Based on Robot Operating System version 2 (ROS 2) and enhanced with Visual Simultaneous Localisation And Mapping (vSLAM) for map generation, REALMS2 uses a mesh network for a robust ad hoc network. A single graphical user interface (GUI) controls all the rovers, providing a simple overview of the robotic mission. This system is designed for heterogeneous multi-robot exploratory missions, tackling the challenges presented by extraterrestrial environments. REALMS2 was used during the second field test of the ESA-ESRIC Challenge and allowed to map around 60% of the area, using three homogeneous rovers while handling communication delays and blackouts.</article>","contentLength":1090,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stitch: Step-by-step LLM Guided Tutoring for Scratch","url":"https://arxiv.org/abs/2510.26634","date":1761883200,"author":"","guid":322951,"unread":true,"content":"<article>arXiv:2510.26634v1 Announce Type: new \nAbstract: Block-based environments such as Scratch are increasingly popular in programming education. While block syntax reduces surface errors, semantic bugs remain common and challenging for novices to resolve. Existing debugging workflows typically show the correct program directly to learners, a strategy that may fix errors but undermines the development of problem-solving skills.\n  We present Stitch, an interactive tutoring system that replaces \"showing the answer\" with step-by-step scaffolding. The system's Diff-Analyze module contrasts a student's project with a reference implementation, identifies the most critical differences, and uses a large language model to explain why these changes matter. Learners inspect highlighted blocks through a custom rendering engine, understand the explanations, and selectively apply partial fixes. This iterative process continues until the intended functionality is achieved.\n  We evaluate Stitch in an empirical study, comparing it against a state-of-the-art automated feedback generation tool for Scratch. Our key insight is that simply presenting the correct program is pedagogically ineffective. In contrast, our interactive, step-by-step guided system promotes a more effective learning experience. More broadly, what constitutes effective feedback in block-based programming remains an open question. Our evaluation provides new evidence that step-by-step tutoring significantly enhances learning outcomes, outperforming both direct-answer approaches and current automated feedback generation tools.</article>","contentLength":1597,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization","url":"https://arxiv.org/abs/2510.26633","date":1761883200,"author":"","guid":322952,"unread":true,"content":"<article>arXiv:2510.26633v1 Announce Type: new \nAbstract: Bayesian Optimization (BO) has the potential to solve various combinatorial tasks, ranging from materials science to neural architecture search. However, BO requires specialized kernels to effectively model combinatorial domains. Recent efforts have introduced several combinatorial kernels, but the relationships among them are not well understood. To bridge this gap, we develop a unifying framework based on heat kernels, which we derive in a systematic way and express as simple closed-form expressions. Using this framework, we prove that many successful combinatorial kernels are either related or equivalent to heat kernels, and validate this theoretical claim in our experiments. Moreover, our analysis confirms and extends the results presented in Bounce: certain algorithms' performance decreases substantially when the unknown optima of the function do not have a certain structure. In contrast, heat kernels are not sensitive to the location of the optima. Lastly, we show that a fast and simple pipeline, relying on heat kernels, is able to achieve state-of-the-art results, matching or even outperforming certain slow or complex algorithms.</article>","contentLength":1203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus","url":"https://arxiv.org/abs/2510.26630","date":1761883200,"author":"","guid":322953,"unread":true,"content":"<article>arXiv:2510.26630v1 Announce Type: new \nAbstract: To address the challenges in UAV object detection, such as complex backgrounds, severe occlusion, dense small objects, and varying lighting conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection algorithm specifically designed for small objects in UAV imagery. In the backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module to enhance feature extraction for small objects. Additionally,we design the Median-Frequency Feature Fusion (MFFF) module,which effectively improves the model's ability to capture small-object details and contextual information. Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box matching capability and increase its sensitivity to small-object features, thereby further enhancing detection accuracy and robustness. Compared with RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the VisDrone2019 dataset with lower computational complexity and fewer parameters, demonstrating its robustness and feasibility for small-object detection tasks.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Low-Altitude UAV-Carried Movable Antenna for Joint Wireless Power Transfer and Covert Communications","url":"https://arxiv.org/abs/2510.26628","date":1761883200,"author":"","guid":322954,"unread":true,"content":"<article>arXiv:2510.26628v1 Announce Type: new \nAbstract: The proliferation of Internet of Things (IoT) networks has created an urgent need for sustainable energy solutions, particularly for the battery-constrained spatially distributed IoT nodes. While low-altitude uncrewed aerial vehicles (UAVs) employed with wireless power transfer (WPT) capabilities offer a promising solution, the line-of-sight channels that facilitate efficient energy delivery also expose sensitive operational data to adversaries. This paper proposes a novel low-altitude UAV-carried movable antenna-enhanced transmission system joint WPT and covert communications, which simultaneously performs energy supplements to IoT nodes and establishes transmission links with a covert user by leveraging wireless energy signals as a natural cover. Then, we formulate a multi-objective optimization problem that jointly maximizes the total harvested energy of IoT nodes and sum achievable rate of the covert user, while minimizing the propulsion energy consumption of the low-altitude UAV. To address the non-convex and temporally coupled optimization problem, we propose a mixture-of-experts-augmented soft actor-critic (MoE-SAC) algorithm that employs a sparse Top-K gated mixture-of-shallow-experts architecture to represent multimodal policy distributions arising from the conflicting optimization objectives. We also incorporate an action projection module that explicitly enforces per-time-slot power budget constraints and antenna position constraints. Simulation results demonstrate that the proposed approach significantly outperforms some baseline approaches and other state-of-the-art deep reinforcement learning algorithms.</article>","contentLength":1694,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Sliding-Window Filter for Online Continuous-Time Continuum Robot State Estimation","url":"https://arxiv.org/abs/2510.26623","date":1761883200,"author":"","guid":322955,"unread":true,"content":"<article>arXiv:2510.26623v1 Announce Type: new \nAbstract: Stochastic state estimation methods for continuum robots (CRs) often struggle to balance accuracy and computational efficiency. While several recent works have explored sliding-window formulations for CRs, these methods are limited to simplified, discrete-time approximations and do not provide stochastic representations. In contrast, current stochastic filter methods must run at the speed of measurements, limiting their full potential. Recent works in continuous-time estimation techniques for CRs show a principled approach to addressing this runtime constraint, but are currently restricted to offline operation. In this work, we present a sliding-window filter (SWF) for continuous-time state estimation of CRs that improves upon the accuracy of a filter approach while enabling continuous-time methods to operate online, all while running at faster-than-real-time speeds. This represents the first stochastic SWF specifically designed for CRs, providing a promising direction for future research in this area.</article>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model","url":"https://arxiv.org/abs/2510.26622","date":1761883200,"author":"","guid":322956,"unread":true,"content":"<article>arXiv:2510.26622v1 Announce Type: new \nAbstract: Recent large language model (LLM) research has undergone an architectural shift from encoder-decoder modeling to nowadays the dominant decoder-only modeling. This rapid transition, however, comes without a rigorous comparative analysis especially \\textit{from the scaling perspective}, raising concerns that the potential of encoder-decoder models may have been overlooked. To fill this gap, we revisit encoder-decoder LLM (RedLLM), enhancing it with recent recipes from decoder-only LLM (DecLLM). We conduct a comprehensive comparison between RedLLM, pretrained with prefix language modeling (LM), and DecLLM, pretrained with causal LM, at different model scales, ranging from $\\sim$150M to $\\sim$8B. Using RedPajama V1 (1.6T tokens) for pretraining and FLAN for instruction tuning, our experiments show that RedLLM produces compelling scaling properties and surprisingly strong performance. While DecLLM is overall more compute-optimal during pretraining, RedLLM demonstrates comparable scaling and context length extrapolation capabilities. After instruction tuning, RedLLM achieves comparable and even better results on various downstream tasks while enjoying substantially better inference efficiency. We hope our findings could inspire more efforts on re-examining RedLLM, unlocking its potential for developing powerful and efficient LLMs.</article>","contentLength":1395,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Toward Automated Security Risk Detection in Large Software Using Call Graph Analysis","url":"https://arxiv.org/abs/2510.26620","date":1761883200,"author":"","guid":322957,"unread":true,"content":"<article>arXiv:2510.26620v1 Announce Type: new \nAbstract: Threat modeling plays a critical role in the identification and mitigation of security risks; however, manual approaches are often labor intensive and prone to error. This paper investigates the automation of software threat modeling through the clustering of call graphs using density-based and community detection algorithms, followed by an analysis of the threats associated with the identified clusters. The proposed method was evaluated through a case study of the Splunk Forwarder Operator (SFO), wherein selected clustering metrics were applied to the software's call graph to assess pertinent code-density security weaknesses. The results demonstrate the viability of the approach and underscore its potential to facilitate systematic threat assessment. This work contributes to the advancement of scalable, semi-automated threat modeling frameworks tailored for modern cloud-native environments.</article>","contentLength":953,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aeolus: A Multi-structural Flight Delay Dataset","url":"https://arxiv.org/abs/2510.26616","date":1761883200,"author":"","guid":322958,"unread":true,"content":"<article>arXiv:2510.26616v1 Announce Type: new \nAbstract: We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designed to advance research on flight delay prediction and support the development of foundation models for tabular data. Existing datasets in this domain are typically limited to flat tabular structures and fail to capture the spatiotemporal dynamics inherent in delay propagation. Aeolus addresses this limitation by providing three aligned modalities: (i) a tabular dataset with rich operational, meteorological, and airportlevel features for over 50 million flights; (ii) a flight chain module that models delay propagation along sequential flight legs, capturing upstream and downstream dependencies; and (iii) a flight network graph that encodes shared aircraft, crew, and airport resource connections, enabling cross-flight relational reasoning. The dataset is carefully constructed with temporal splits, comprehensive features, and strict leakage prevention to support realistic and reproducible machine learning evaluation. Aeolus supports a broad range of tasks, including regression, classification, temporal structure modeling, and graph learning, serving as a unified benchmark across tabular, sequential, and graph modalities. We release baseline experiments and preprocessing tools to facilitate adoption. Aeolus fills a key gap for both domain-specific modeling and general-purpose structured data research.Our source code and data can be accessed at https://github.com/Flnny/Delay-data</article>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding","url":"https://arxiv.org/abs/2510.26615","date":1761883200,"author":"","guid":322959,"unread":true,"content":"<article>arXiv:2510.26615v1 Announce Type: new \nAbstract: Multi-page visual documents such as manuals, brochures, presentations, and posters convey key information through layout, colors, icons, and cross-slide references. While large language models (LLMs) offer opportunities in document understanding, current systems struggle with complex, multi-page visual documents, particularly in fine-grained reasoning over elements and pages. We introduce SlideAgent, a versatile agentic framework for understanding multi-modal, multi-page, and multi-layout documents, especially slide decks. SlideAgent employs specialized agents and decomposes reasoning into three specialized levels-global, page, and element-to construct a structured, query-agnostic representation that captures both overarching themes and detailed visual or textual cues. During inference, SlideAgent selectively activates specialized agents for multi-level reasoning and integrates their outputs into coherent, context-aware answers. Extensive experiments show that SlideAgent achieves significant improvement over both proprietary (+7.9 overall) and open-source models (+9.8 overall).</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras","url":"https://arxiv.org/abs/2510.26614","date":1761883200,"author":"","guid":322960,"unread":true,"content":"<article>arXiv:2510.26614v1 Announce Type: new \nAbstract: We propose tokenization of events and present a tokenizer, Spiking Patches, specifically designed for event cameras. Given a stream of asynchronous and spatially sparse events, our goal is to discover an event representation that preserves these properties. Prior works have represented events as frames or as voxels. However, while these representations yield high accuracy, both frames and voxels are synchronous and decrease the spatial sparsity. Spiking Patches gives the means to preserve the unique properties of event cameras and we show in our experiments that this comes without sacrificing accuracy. We evaluate our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and object detection. Tokens from Spiking Patches yield inference times that are up to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We achieve this while matching their accuracy and even surpassing in some cases with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for object detection. Thus, tokenization constitutes a novel direction in event-based vision and marks a step towards methods that preserve the properties of event cameras.</article>","contentLength":1230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast tensor-based electrostatic energy calculations in the perspective of protein-ligand docking problem","url":"https://arxiv.org/abs/2510.26611","date":1761883200,"author":"","guid":322961,"unread":true,"content":"<article>arXiv:2510.26611v1 Announce Type: new \nAbstract: We propose and justify a new approach for fast calculation of the electrostatic interaction energy of clusters of charged particles in constrained energy minimization in the framework of rigid protein-ligand docking. Our ``blind search'' docking technique is based on the low-rank range-separated (RS) tensor-based representation of the free-space electrostatic potential of the biomolecule represented on large $n\\times n\\times n$ 3D grid. We show that both the collective electrostatic potential of a complex protein-ligand system and the respective electrostatic interaction energy can be calculated by tensor techniques in $O(n)$-complexity, such that the numerical cost for energy calculation only mildly (logarithmically) depends on the number of particles in the system. Moreover, tensor representation of the electrostatic potential enables usage of large 3D Cartesian grids (of the order of $n^3 \\sim 10^{12}$), which could allow the accurate modeling of complexes with several large proteins. In our approach selection of the correct geometric pose predictions in the localized posing process is based on the control of van der Waals distance between the target molecular clusters. Here, we confine ourselves by constrained minimization of the energy functional by using only fast tensor-based free-space electrostatic energy recalculation for various rotations and translations of both clusters. Numerical tests of the electrostatic energy-based ``protein-ligand docking'' algorithm applied to synthetic and realistic input data present a proof of concept for rather complex particle configurations. The method may be used in the framework of the traditional stochastic or deterministic posing/docking techniques.</article>","contentLength":1773,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A DRL-Empowered Multi-Level Jamming Approach for Secure Semantic Communication","url":"https://arxiv.org/abs/2510.26610","date":1761883200,"author":"","guid":322962,"unread":true,"content":"<article>arXiv:2510.26610v1 Announce Type: new \nAbstract: Semantic communication (SemCom) aims to transmit only task-relevant information, thereby improving communication efficiency but also exposing semantic information to potential eavesdropping. In this paper, we propose a deep reinforcement learning (DRL)-empowered multi-level jamming approach to enhance the security of SemCom systems over MIMO fading wiretap channels. This approach combines semantic layer jamming, achieved by encoding task-irrelevant text, and physical layer jamming, achieved by encoding random Gaussian noise. These two-level jamming signals are superposed with task-relevant semantic information to protect the transmitted semantics from eavesdropping. A deep deterministic policy gradient (DDPG) algorithm is further introduced to dynamically design and optimize the precoding matrices for both taskrelevant semantic information and multi-level jamming signals, aiming to enhance the legitimate user's image reconstruction while degrading the eavesdropper's performance. To jointly train the SemCom model and the DDPG agent, we propose an alternating optimization strategy where the two modules are updated iteratively. Experimental results demonstrate that, compared with both the encryption-based (ESCS) and encoded jammer-based (EJ) benchmarks, our method achieves comparable security while improving the legitimate user's peak signalto-noise ratio (PSNR) by up to approximately 0.6 dB.</article>","contentLength":1461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing","url":"https://arxiv.org/abs/2510.26609","date":1761883200,"author":"","guid":322963,"unread":true,"content":"<article>arXiv:2510.26609v1 Announce Type: new \nAbstract: Accurate and timely crop yield prediction is crucial for global food security and modern agricultural management. Traditional methods often lack the scalability and granularity required for precision farming. This paper introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing), a deep learning model designed for high-resolution, intra-field canola yield prediction. CYPRESS leverages a pre-trained, large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for a continuous regression task, transforming multi-temporal satellite imagery into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from the Canadian Prairies, CYPRESS demonstrates superior performance over existing deep learning-based yield prediction models, highlighting the effectiveness of fine-tuning foundation models for specialized agricultural applications. By providing a continuous, high-resolution output, CYPRESS offers a more actionable tool for precision agriculture than conventional classification or county-level aggregation methods. This work validates a novel approach that bridges the gap between large-scale Earth observation and on-farm decision-making, offering a scalable solution for detailed agricultural monitoring.</article>","contentLength":1334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis","url":"https://arxiv.org/abs/2510.26607","date":1761883200,"author":"","guid":322964,"unread":true,"content":"<article>arXiv:2510.26607v1 Announce Type: new \nAbstract: This paper considers the problem of regression over distributions, which is becoming increasingly important in machine learning. Existing approaches often ignore the geometry of the probability space or are computationally expensive. To overcome these limitations, a new method is proposed that combines the parameterization of probability trajectories using a Bernstein basis and the minimization of the Wasserstein distance between distributions. The key idea is to model a conditional distribution as a smooth probability trajectory defined by a weighted sum of Gaussian components whose parameters -- the mean and covariance -- are functions of the input variable constructed using Bernstein polynomials. The loss function is the averaged squared Wasserstein distance between the predicted Gaussian distributions and the empirical data, which takes into account the geometry of the distributions. An autodiff-based optimization method is used to train the model. Experiments on synthetic datasets that include complex trajectories demonstrated that the proposed method provides competitive approximation quality in terms of the Wasserstein distance, Energy Distance, and RMSE metrics, especially in cases of pronounced nonlinearity. The model demonstrates trajectory smoothness that is better than or comparable to alternatives and robustness to changes in data structure, while maintaining high interpretability due to explicit parameterization via control points. The developed approach represents a balanced solution that combines geometric accuracy, computational practicality, and interpretability. Prospects for further research include extending the method to non-Gaussian distributions, applying entropy regularization to speed up computations, and adapting the approach to working with high-dimensional data for approximating surfaces and more complex structures.</article>","contentLength":1925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Normative Reasoning in Large Language Models: A Comparative Benchmark from Logical and Modal Perspectives","url":"https://arxiv.org/abs/2510.26606","date":1761883200,"author":"","guid":322965,"unread":true,"content":"<article>arXiv:2510.26606v1 Announce Type: new \nAbstract: Normative reasoning is a type of reasoning that involves normative or deontic modality, such as obligation and permission. While large language models (LLMs) have demonstrated remarkable performance across various reasoning tasks, their ability to handle normative reasoning remains underexplored. In this paper, we systematically evaluate LLMs' reasoning capabilities in the normative domain from both logical and modal perspectives. Specifically, to assess how well LLMs reason with normative modals, we make a comparison between their reasoning with normative modals and their reasoning with epistemic modals, which share a common formal structure. To this end, we introduce a new dataset covering a wide range of formal patterns of reasoning in both normative and epistemic domains, while also incorporating non-formal cognitive factors that influence human reasoning. Our results indicate that, although LLMs generally adhere to valid reasoning patterns, they exhibit notable inconsistencies in specific types of normative reasoning and display cognitive biases similar to those observed in psychological studies of human reasoning. These findings highlight challenges in achieving logical consistency in LLMs' normative reasoning and provide insights for enhancing their reliability. All data and code are released publicly at https://github.com/kmineshima/NeuBAROCO.</article>","contentLength":1422,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic AI Home Energy Management System: A Large Language Model Framework for Residential Load Scheduling","url":"https://arxiv.org/abs/2510.26603","date":1761883200,"author":"","guid":322966,"unread":true,"content":"<article>arXiv:2510.26603v1 Announce Type: new \nAbstract: The electricity sector transition requires substantial increases in residential demand response capacity, yet Home Energy Management Systems (HEMS) adoption remains limited by user interaction barriers requiring translation of everyday preferences into technical parameters. While large language models have been applied to energy systems as code generators and parameter extractors, no existing implementation deploys LLMs as autonomous coordinators managing the complete workflow from natural language input to multi-appliance scheduling. This paper presents an agentic AI HEMS where LLMs autonomously coordinate multi-appliance scheduling from natural language requests to device control, achieving optimal scheduling without example demonstrations. A hierarchical architecture combining one orchestrator with three specialist agents uses the ReAct pattern for iterative reasoning, enabling dynamic coordination without hardcoded workflows while integrating Google Calendar for context-aware deadline extraction. Evaluation across three open-source models using real Austrian day-ahead electricity prices reveals substantial capability differences. Llama-3.3-70B successfully coordinates all appliances across all scenarios to match cost-optimal benchmarks computed via mixed-integer linear programming, while other models achieve perfect single-appliance performance but struggle to coordinate all appliances simultaneously. Progressive prompt engineering experiments demonstrate that analytical query handling without explicit guidance remains unreliable despite models' general reasoning capabilities. We open-source the complete system including orchestration logic, agent prompts, tools, and web interfaces to enable reproducibility, extension, and future research.</article>","contentLength":1822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Bidding and Coordinated Dispatch of Hybrid Energy Systems in Regulation Markets","url":"https://arxiv.org/abs/2510.26602","date":1761883200,"author":"","guid":322967,"unread":true,"content":"<article>arXiv:2510.26602v1 Announce Type: new \nAbstract: The increasing integration of renewable energy sources and distributed energy resources (DER) into modern power systems introduces significant uncertainty, posing challenges for maintaining grid flexibility and reliability. Hybrid energy systems (HES), composed of controllable generators, flexible loads, and battery storage, offer a decentralized solution to enhance flexibility compared to single centralized resources. This paper presents a two-level framework to enable HES participation in frequency regulation markets. The upper level performs a chance-constrained optimization to choose capacity bids based on historical regulation signals. At the lower level, a real-time control strategy disaggregates the regulation power among the constituent resources. This real-time control strategy is then benchmarked against an offline optimal dispatch to evaluate flexibility performance. Additionally, the framework evaluates the profitability of overbidding strategies and identifies thresholds beyond which performance degradation may lead to market penalties or disqualification. The proposed framework also compare the impact of imbalance of power capacities on performance and battery state of charge (SoC) through asymmetric HES configurations.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching","url":"https://arxiv.org/abs/2510.26601","date":1761883200,"author":"","guid":322968,"unread":true,"content":"<article>arXiv:2510.26601v1 Announce Type: new \nAbstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite being an ill-posed problem, a long history. At its very core, CSR is about finding a prior that can be used to extrapolate frequencies in a micrograph that have never been imaged by the image-generating microscope. It stands to reason that, with the advent of better data-driven machine learning techniques, stronger prior can be learned and hence CSR can lead to better results. Here, we present ResMatching, a novel CSR method that uses guided conditional flow matching to learn such improved data-priors. We evaluate ResMatching on 4 diverse biological structures from the BioSR dataset and compare its results against 7 baselines. ResMatching consistently achieves competitive results, demonstrating in all cases the best trade-off between data fidelity and perceptual realism. We observe that CSR using ResMatching is particularly effective in cases where a strong prior is hard to learn, e.g. when the given low-resolution images contain a lot of noise. Additionally, we show that ResMatching can be used to sample from an implicitly learned posterior distribution and that this distribution is calibrated for all tested use-cases, enabling our method to deliver a pixel-wise data-uncertainty term that can guide future users to reject uncertain predictions.</article>","contentLength":1390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FLYINGTRUST: A Benchmark for Quadrotor Navigation Across Scenarios and Vehicles","url":"https://arxiv.org/abs/2510.26588","date":1761883200,"author":"","guid":322969,"unread":true,"content":"<article>arXiv:2510.26588v1 Announce Type: new \nAbstract: Visual navigation algorithms for quadrotors often exhibit a large variation in performance when transferred across different vehicle platforms and scene geometries, which increases the cost and risk of field deployment. To support systematic early-stage evaluation, we introduce FLYINGTRUST, a high-fidelity, configurable benchmarking framework that measures how platform kinodynamics and scenario structure jointly affect navigation robustness. FLYINGTRUST models vehicle capability with two compact, physically interpretable indicators: maximum thrust-to-weight ratio and axis-wise maximum angular acceleration. The benchmark pairs a diverse scenario library with a heterogeneous set of real and virtual platforms and prescribes a standardized evaluation protocol together with a composite scoring method that balances scenario importance, platform importance and performance stability. We use FLYINGTRUST to compare representative optimization-based and learning-based navigation approaches under identical conditions, performing repeated trials per platform-scenario combination and reporting uncertainty-aware metrics. The results reveal systematic patterns: navigation success depends predictably on platform capability and scene geometry, and different algorithms exhibit distinct preferences and failure modes across the evaluated conditions. These observations highlight the practical necessity of incorporating both platform capability and scenario structure into algorithm design, evaluation, and selection, and they motivate future work on methods that remain robust across diverse platforms and scenarios.</article>","contentLength":1667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tensor decomposition beyond uniqueness, with an application to the minrank problem","url":"https://arxiv.org/abs/2510.26587","date":1761883200,"author":"","guid":322970,"unread":true,"content":"<article>arXiv:2510.26587v1 Announce Type: new \nAbstract: We prove a generalization to Jennrich's uniqueness theorem for tensor decompositions in the undercomplete setting. Our uniqueness theorem is based on an alternative definition of the standard tensor decomposition, which we call matrix-vector decomposition. Moreover, in the same settings in which our uniqueness theorem applies, we also design and analyze an efficient randomized algorithm to compute the unique minimum matrix-vector decomposition (and thus a tensor rank decomposition of minimum rank).\n  As an application of our uniqueness theorem and our efficient algorithm, we show how to compute all matrices of minimum rank (up to scalar multiples) in certain generic vector spaces of matrices.</article>","contentLength":750,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems","url":"https://arxiv.org/abs/2510.26585","date":1761883200,"author":"","guid":322971,"unread":true,"content":"<article>arXiv:2510.26585v1 Announce Type: new \nAbstract: While Multi-Agent Systems (MAS) excel at complex tasks, their growing autonomy with operational complexity often leads to critical inefficiencies, such as excessive token consumption and failures arising from misinformation. Existing methods primarily focus on post-hoc failure attribution, lacking proactive, real-time interventions to enhance robustness and efficiency. To this end, we introduce SupervisorAgent, a lightweight and modular framework for runtime, adaptive supervision that operates without altering the base agent's architecture. Triggered by an LLM-free adaptive filter, SupervisorAgent intervenes at critical junctures to proactively correct errors, guide inefficient behaviors, and purify observations. On the challenging GAIA benchmark, SupervisorAgent reduces the token consumption of the Smolagent framework by an average of 29.45% without compromising its success rate. Extensive experiments across five additional benchmarks (math reasoning, code generation, and question answering) and various SoTA foundation models validate the broad applicability and robustness of our approach. The code is available at https://github.com/LINs-lab/SupervisorAgent.</article>","contentLength":1226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Emu3.5: Native Multimodal Models are World Learners","url":"https://arxiv.org/abs/2510.26583","date":1761883200,"author":"","guid":322972,"unread":true,"content":"<article>arXiv:2510.26583v1 Announce Type: new \nAbstract: We introduce Emu3.5, a large-scale multimodal world model that natively predicts the next state across vision and language. Emu3.5 is pre-trained end-to-end with a unified next-token prediction objective on a corpus of vision-language interleaved data containing over 10 trillion tokens, primarily derived from sequential frames and transcripts of internet videos. The model naturally accepts interleaved vision-language inputs and generates interleaved vision-language outputs. Emu3.5 is further post-trained with large-scale reinforcement learning to enhance multimodal reasoning and generation. To improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA), which converts token-by-token decoding into bidirectional parallel prediction, accelerating per-image inference by about 20x without sacrificing performance. Emu3.5 exhibits strong native multimodal capabilities, including long-horizon vision-language generation, any-to-image (X2I) generation, and complex text-rich image generation. It also exhibits generalizable world-modeling abilities, enabling spatiotemporally consistent world exploration and open-world embodied manipulation across diverse scenarios and tasks. For comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image (Nano Banana) on image generation and editing tasks and demonstrates superior results on a suite of interleaved generation tasks. We open-source Emu3.5 at https://github.com/baaivision/Emu3.5 to support community research.</article>","contentLength":1552,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CATCH: A Modular Cross-domain Adaptive Template with Hook","url":"https://arxiv.org/abs/2510.26582","date":1761883200,"author":"","guid":322973,"unread":true,"content":"<article>arXiv:2510.26582v1 Announce Type: new \nAbstract: Recent advances in Visual Question Answering (VQA) have demonstrated impressive performance in natural image domains, with models like LLaVA leveraging large language models (LLMs) for open-ended reasoning. However, their generalization degrades significantly when transferred to out-of-domain scenarios such as remote sensing, medical imaging, or math diagrams, due to large distributional shifts and the lack of effective domain adaptation mechanisms. Existing approaches typically rely on per-domain fine-tuning or bespoke pipelines, which are costly, inflexible, and not scalable across diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for cross-domain adaptation that improves the generalization of VQA models while requiring minimal changes to their core architecture. Our key idea is to decouple visual and linguistic adaptation by introducing two lightweight modules: a domain classifier to identify the input image type, and a dual adapter mechanism comprising a Prompt Adapter for language modulation and a Visual Adapter for vision feature adjustment. Both modules are dynamically injected via a unified hook interface, requiring no retraining of the backbone model. Experimental results across four domain-specific VQA benchmarks demonstrate that our framework achieves consistent performance gains without retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH provides a scalable and extensible approach to multi-domain VQA, enabling practical deployment across diverse application domains.</article>","contentLength":1672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios","url":"https://arxiv.org/abs/2510.26580","date":1761883200,"author":"","guid":322974,"unread":true,"content":"<article>arXiv:2510.26580v1 Announce Type: new \nAbstract: In real-world environments, AI systems often face unfamiliar scenarios without labeled data, creating a major challenge for conventional scene understanding models. The inability to generalize across unseen contexts limits the deployment of vision-based applications in dynamic, unstructured settings. This work introduces a Dynamic Context-Aware Scene Reasoning framework that leverages Vision-Language Alignment to address zero-shot real-world scenarios. The goal is to enable intelligent systems to infer and adapt to new environments without prior task-specific training. The proposed approach integrates pre-trained vision transformers and large language models to align visual semantics with natural language descriptions, enhancing contextual comprehension. A dynamic reasoning module refines predictions by combining global scene cues and object-level interactions guided by linguistic priors. Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and Open Images demonstrate up to 18% improvement in scene understanding accuracy over baseline models in complex and unseen environments. Results also show robust performance in ambiguous or cluttered scenes due to the synergistic fusion of vision and language. This framework offers a scalable and interpretable approach for context-aware reasoning, advancing zero-shot generalization in dynamic real-world settings.</article>","contentLength":1444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online and Interactive Bayesian Inference Debugging","url":"https://arxiv.org/abs/2510.26579","date":1761883200,"author":"","guid":322975,"unread":true,"content":"<article>arXiv:2510.26579v1 Announce Type: new \nAbstract: Probabilistic programming is a rapidly developing programming paradigm which enables the formulation of Bayesian models as programs and the automation of posterior inference. It facilitates the development of models and conducting Bayesian inference, which makes these techniques available to practitioners from multiple fields. Nevertheless, probabilistic programming is notoriously difficult as identifying and repairing issues with inference requires a lot of time and deep knowledge. Through this work, we introduce a novel approach to debugging Bayesian inference that reduces time and required knowledge significantly. We discuss several requirements a Bayesian inference debugging framework has to fulfill, and propose a new tool that meets these key requirements directly within the development environment. We evaluate our results in a study with 18 experienced participants and show that our approach to online and interactive debugging of Bayesian inference significantly reduces time and difficulty on inference debugging tasks.</article>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two-Timescale Optimization Framework for IAB-Enabled Heterogeneous UAV Networks","url":"https://arxiv.org/abs/2510.26578","date":1761883200,"author":"","guid":322976,"unread":true,"content":"<article>arXiv:2510.26578v1 Announce Type: new \nAbstract: In post-disaster scenarios, the rapid deployment of adequate communication infrastructure is essential to support disaster search, rescue, and recovery operations. To achieve this, uncrewed aerial vehicle (UAV) has emerged as a promising solution for emergency communication due to its low cost and deployment flexibility. However, conventional untethered UAV (U-UAV) is constrained by size, weight, and power (SWaP) limitations, making it incapable of maintaining the operation of a macro base station. To address this limitation, we propose a heterogeneous UAV-based framework that integrates tethered UAV (T-UAV) and U-UAVs, where U-UAVs are utilized to enhance the throughput of cell-edge ground user equipments (G-UEs) and guarantee seamless connectivity during G-UEs' mobility to safe zones. It is noted that the integrated access and backhaul (IAB) technique is adopted to support the wireless backhaul of U-UAVs. Accordingly, we formulate a two-timescale joint user scheduling and trajectory control optimization problem, aiming to maximize the downlink throughput under asymmetric traffic demands and G-UEs' mobility. To solve the formulated problem, we proposed a two-timescale multi-agent deep deterministic policy gradient (TTS-MADDPG) algorithm based on the centralized training and distributed execution paradigm. Numerical results show that the proposed algorithm outperforms other benchmarks, including the two-timescale multi-agent proximal policy optimization (TTS-MAPPO) algorithm and MADDPG scheduling method, with robust and higher throughput. Specifically, the proposed algorithm obtains up to 12.2\\% average throughput gain compared to the MADDPG scheduling method.</article>","contentLength":1737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inference-Cost-Aware Dynamic Tree Construction for Efficient Inference in Large Language Models","url":"https://arxiv.org/abs/2510.26577","date":1761883200,"author":"","guid":322977,"unread":true,"content":"<article>arXiv:2510.26577v1 Announce Type: new \nAbstract: Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes.\n  Therefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5% to 20%.</article>","contentLength":1047,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Show Me You Comply... Without Showing Me Anything\": Zero-Knowledge Software Auditing for AI-Enabled Systems","url":"https://arxiv.org/abs/2510.26576","date":1761883200,"author":"","guid":322978,"unread":true,"content":"<article>arXiv:2510.26576v1 Announce Type: new \nAbstract: The increasing exploitation of Artificial Intelligence (AI) enabled systems in critical domains has made trustworthiness concerns a paramount showstopper, requiring verifiable accountability, often by regulation (e.g., the EU AI Act). Classical software verification and validation techniques, such as procedural audits, formal methods, or model documentation, are the mechanisms used to achieve this. However, these methods are either expensive or heavily manual and ill-suited for the opaque, \"black box\" nature of most AI models. An intractable conflict emerges: high auditability and verifiability are required by law, but such transparency conflicts with the need to protect assets being audited-e.g., confidential data and proprietary models-leading to weakened accountability. To address this challenge, this paper introduces ZKMLOps, a novel MLOps verification framework that operationalizes Zero-Knowledge Proofs (ZKPs)-cryptographic protocols allowing a prover to convince a verifier that a statement is true without revealing additional information-within Machine-Learning Operations lifecycles. By integrating ZKPs with established software engineering patterns, ZKMLOps provides a modular and repeatable process for generating verifiable cryptographic proof of compliance. We evaluate the framework's practicality through a study of regulatory compliance in financial risk auditing and assess feasibility through an empirical evaluation of top ZKP protocols, analyzing performance trade-offs for ML models of increasing complexity.</article>","contentLength":1593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InfoFlow: Reinforcing Search Agent Via Reward Density Optimization","url":"https://arxiv.org/abs/2510.26575","date":1761883200,"author":"","guid":322979,"unread":true,"content":"<article>arXiv:2510.26575v1 Announce Type: new \nAbstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising approach for enhancing agentic deep search. However, its application is often hindered by low \\textbf{Reward Density} in deep search scenarios, where agents expend significant exploratory costs for infrequent and often null final rewards. In this paper, we formalize this challenge as the \\textbf{Reward Density Optimization} problem, which aims to improve the reward obtained per unit of exploration cost. This paper introduce \\textbf{InfoFlow}, a systematic framework that tackles this problem from three aspects. 1) \\textbf{Subproblem decomposition}: breaking down long-range tasks to assign process rewards, thereby providing denser learning signals. 2) \\textbf{Failure-guided hints}: injecting corrective guidance into stalled trajectories to increase the probability of successful outcomes. 3) \\textbf{Dual-agent refinement}: employing a dual-agent architecture to offload the cognitive burden of deep exploration. A refiner agent synthesizes the search history, which effectively compresses the researcher's perceived trajectory, thereby reducing exploration cost and increasing the overall reward density. We evaluate InfoFlow on multiple agentic search benchmarks, where it significantly outperforms strong baselines, enabling lightweight LLMs to achieve performance comparable to advanced proprietary LLMs.</article>","contentLength":1434,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerated decomposition of bistochastic kernel matrices by low rank approximation","url":"https://arxiv.org/abs/2510.26574","date":1761883200,"author":"","guid":322980,"unread":true,"content":"<article>arXiv:2510.26574v1 Announce Type: new \nAbstract: We develop an accelerated algorithm for computing an approximate eigenvalue decomposition of bistochastic normalized kernel matrices. Our approach constructs a low rank approximation of the original kernel matrix by the pivoted partial Cholesky algorithm and uses it to compute an approximate decomposition of its bistochastic normalization without requiring the formation of the full kernel matrix. The cost of the proposed algorithm depends linearly on the size of the employed training dataset and quadratically on the rank of the low rank approximation, offering a significant cost reduction compared to the naive approach. We apply the proposed algorithm to the kernel based extraction of spatiotemporal patterns from chaotic dynamics, demonstrating its accuracy while also comparing it with an alternative algorithm consisting of subsampling and Nystroem extension.</article>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping","url":"https://arxiv.org/abs/2510.26569","date":1761883200,"author":"","guid":322981,"unread":true,"content":"<article>arXiv:2510.26569v1 Announce Type: new \nAbstract: Advertisers commonly need multiple versions of the same advertisement (ad) at varying durations for a single campaign. The traditional approach involves manually selecting and re-editing shots from longer video ads to create shorter versions, which is labor-intensive and time-consuming. In this paper, we introduce a framework for automated video ad clipping using video summarization techniques. We are the first to frame video clipping as a shot selection problem, tailored specifically for advertising. Unlike existing general video summarization methods that primarily focus on visual content, our approach emphasizes the critical role of audio in advertising. To achieve this, we develop a two-stream audio-visual fusion model that predicts the importance of video frames, where importance is defined as the likelihood of a frame being selected in the firm-produced short ad. To address the lack of ad-specific datasets, we present AdSum204, a novel dataset comprising 102 pairs of 30-second and 15-second ads from real advertising campaigns. Extensive experiments demonstrate that our model outperforms state-of-the-art methods across various metrics, including Average Precision, Area Under Curve, Spearman, and Kendall.</article>","contentLength":1277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging","url":"https://arxiv.org/abs/2510.26568","date":1761883200,"author":"","guid":322982,"unread":true,"content":"<article>arXiv:2510.26568v1 Announce Type: new \nAbstract: Spine segmentation, based on ultrasound volume projection imaging (VPI), plays a vital role for intelligent scoliosis diagnosis in clinical applications. However, this task faces several significant challenges. Firstly, the global contextual knowledge of spines may not be well-learned if we neglect the high spatial correlation of different bone features. Secondly, the spine bones contain rich structural knowledge regarding their shapes and positions, which deserves to be encoded into the segmentation process. To address these challenges, we propose a novel scale-adaptive structure-aware network (SA$^{2}$Net) for effective spine segmentation. First, we propose a scale-adaptive complementary strategy to learn the cross-dimensional long-distance correlation features for spinal images. Second, motivated by the consistency between multi-head self-attention in Transformers and semantic level affinity, we propose structure-affinity transformation to transform semantic features with class-specific affinity and combine it with a Transformer decoder for structure-aware reasoning. In addition, we adopt a feature mixing loss aggregation method to enhance model training. This method improves the robustness and accuracy of the segmentation process. The experimental results demonstrate that our SA$^{2}$Net achieves superior segmentation performance compared to other state-of-the-art methods. Moreover, the adaptability of SA$^{2}$Net to various backbones enhances its potential as a promising tool for advanced scoliosis diagnosis using intelligent spinal image analysis. The code and experimental demo are available at https://github.com/taetiseo09/SA2Net.</article>","contentLength":1714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiclass Local Calibration With the Jensen-Shannon Distance","url":"https://arxiv.org/abs/2510.26566","date":1761883200,"author":"","guid":322983,"unread":true,"content":"<article>arXiv:2510.26566v1 Announce Type: new \nAbstract: Developing trustworthy Machine Learning (ML) models requires their predicted probabilities to be well-calibrated, meaning they should reflect true-class frequencies. Among calibration notions in multiclass classification, strong calibration is the most stringent, as it requires all predicted probabilities to be simultaneously calibrated across all classes. However, existing approaches to multiclass calibration lack a notion of distance among inputs, which makes them vulnerable to proximity bias: predictions in sparse regions of the feature space are systematically miscalibrated. This is especially relevant in high-stakes settings, such as healthcare, where the sparse instances are exactly those most at risk of biased treatment. In this work, we address this main shortcoming by introducing a local perspective on multiclass calibration. First, we formally define multiclass local calibration and establish its relationship with strong calibration. Second, we theoretically analyze the pitfalls of existing evaluation metrics when applied to multiclass local calibration. Third, we propose a practical method for enhancing local calibration in Neural Networks, which enforces alignment between predicted probabilities and local estimates of class frequencies using the Jensen-Shannon distance. Finally, we empirically validate our approach against existing multiclass calibration techniques.</article>","contentLength":1449,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Measuring Localization of Shortcuts in Deep Networks","url":"https://arxiv.org/abs/2510.26560","date":1761883200,"author":"","guid":322984,"unread":true,"content":"<article>arXiv:2510.26560v1 Announce Type: new \nAbstract: Shortcuts, spurious rules that perform well during training but fail to generalize, present a major challenge to the reliability of deep networks (Geirhos et al., 2020). However, the impact of shortcuts on feature representations remains understudied, obstructing the design of principled shortcut-mitigation methods. To overcome this limitation, we investigate the layer-wise localization of shortcuts in deep models. Our novel experiment design quantifies the layer-wise contribution to accuracy degradation caused by a shortcut-inducing skew by counterfactual training on clean and skewed datasets. We employ our design to study shortcuts on CIFAR-10, Waterbirds, and CelebA datasets across VGG, ResNet, DeiT, and ConvNeXt architectures. We find that shortcut learning is not localized in specific layers but distributed throughout the network. Different network parts play different roles in this process: shallow layers predominantly encode spurious features, while deeper layers predominantly forget core features that are predictive on clean data. We also analyze the differences in localization and describe its principal axes of variation. Finally, our analysis of layer-wise shortcut-mitigation strategies suggests the hardness of designing general methods, supporting dataset- and architecture-specific approaches instead.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices","url":"https://arxiv.org/abs/2510.26557","date":1761883200,"author":"","guid":322985,"unread":true,"content":"<article>arXiv:2510.26557v1 Announce Type: new \nAbstract: Deploying machine learning models on compute-constrained devices has become a key building block of modern IoT applications. In this work, we present a compression scheme for boosted decision trees, addressing the growing need for lightweight machine learning models. Specifically, we provide techniques for training compact boosted decision tree ensembles that exhibit a reduced memory footprint by rewarding, among other things, the reuse of features and thresholds during training. Our experimental evaluation shows that models achieved the same performance with a compression ratio of 4-16x compared to LightGBM models using an adapted training process and an alternative memory layout. Once deployed, the corresponding IoT devices can operate independently of constant communication or external energy supply, and, thus, autonomously, requiring only minimal computing power and energy. This capability opens the door to a wide range of IoT applications, including remote monitoring, edge analytics, and real-time decision making in isolated or power-limited environments.</article>","contentLength":1125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the number of non-degenerate canalizing Boolean functions","url":"https://arxiv.org/abs/2510.26556","date":1761883200,"author":"","guid":322986,"unread":true,"content":"<article>arXiv:2510.26556v1 Announce Type: new \nAbstract: Canalization is a key organizing principle in complex systems, particularly in gene regulatory networks. It describes how certain input variables exert dominant control over a function's output, thereby imposing hierarchical structure and conferring robustness to perturbations. Degeneracy, in contrast, captures redundancy among input variables and reflects the complete dominance of some variables by others. Both properties influence the stability and dynamics of discrete dynamical systems, yet their combinatorial underpinnings remain incompletely understood. Here, we derive recursive formulas for counting Boolean functions with prescribed numbers of essential variables and given canalizing properties. In particular, we determine the number of non-degenerate canalizing Boolean functions -- that is, functions for which all variables are essential and at least one variable is canalizing. Our approach extends earlier enumeration results on canalizing and nested canalizing functions. It provides a rigorous foundation for quantifying how frequently canalization occurs among random Boolean functions and for assessing its pronounced over-representation in biological network models, where it contributes to both robustness and to the emergence of distinct regulatory roles.</article>","contentLength":1332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Comprehensive Evaluation and Practice of System Penetration Testing","url":"https://arxiv.org/abs/2510.26555","date":1761883200,"author":"","guid":322987,"unread":true,"content":"<article>arXiv:2510.26555v1 Announce Type: new \nAbstract: With the rapid advancement of information technology, the complexity of applications continues to increase, and the cybersecurity challenges we face are also escalating. This paper aims to investigate the methods and practices of system security penetration testing, exploring how to enhance system security through systematic penetration testing processes and technical approaches. It also examines existing penetration tools, analyzing their strengths, weaknesses, and applicable domains to guide penetration testers in tool selection. Furthermore, based on the penetration testing process outlined in this paper, appropriate tools are selected to replicate attack processes using target ranges and target machines. Finally, through practical case analysis, lessons learned from successful attacks are summarized to inform future research.</article>","contentLength":890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Entropy Functions on Two-Dimensional Faces of Polymatroidal Region of Degree Four: Part II: Information Theoretic Constraints Breed New Combinatorial Structures","url":"https://arxiv.org/abs/2510.26552","date":1761883200,"author":"","guid":322988,"unread":true,"content":"<article>arXiv:2510.26552v1 Announce Type: new \nAbstract: Characterization of entropy functions is of fundamental importance in information theory. By imposing constraints on their Shannon outer bound, i.e., the polymatroidal region, one obtains the faces of the region and entropy functions on them with special structures. In this series of two papers, we characterize entropy functions on the $2$-dimensional faces of the polymatroidal region $\\Gamma_4$. In Part I, we formulated the problem, enumerated all $59$ types of $2$-dimensional faces of $\\Gamma_4$ by a algorithm, and fully characterized entropy functions on $49$ types of them. In this paper, i.e., Part II, we will characterize entropy functions on the remaining $10$ types of faces, among which $8$ types are fully characterized and $2$ types are partially characterized. To characterize these types of faces, we introduce some new combinatorial design structures which are interesting themself.</article>","contentLength":952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Inverse Kinematics Framework for Learning Variable-Length Tool Manipulation in Robotics","url":"https://arxiv.org/abs/2510.26551","date":1761883200,"author":"","guid":322989,"unread":true,"content":"<article>arXiv:2510.26551v1 Announce Type: new \nAbstract: Conventional robots possess a limited understanding of their kinematics and are confined to preprogrammed tasks, hindering their ability to leverage tools efficiently. Driven by the essential components of tool usage - grasping the desired outcome, selecting the most suitable tool, determining optimal tool orientation, and executing precise manipulations - we introduce a pioneering framework. Our novel approach expands the capabilities of the robot's inverse kinematics solver, empowering it to acquire a sequential repertoire of actions using tools of varying lengths. By integrating a simulation-learned action trajectory with the tool, we showcase the practicality of transferring acquired skills from simulation to real-world scenarios through comprehensive experimentation. Remarkably, our extended inverse kinematics solver demonstrates an impressive error rate of less than 1 cm. Furthermore, our trained policy achieves a mean error of 8 cm in simulation. Noteworthy, our model achieves virtually indistinguishable performance when employing two distinct tools of different lengths. This research provides an indication of potential advances in the exploration of all four fundamental aspects of tool usage, enabling robots to master the intricate art of tool manipulation across diverse tasks.</article>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EdgeRunner 20B: Military Task Parity with GPT-5 while Running on the Edge","url":"https://arxiv.org/abs/2510.26550","date":1761883200,"author":"","guid":322990,"unread":true,"content":"<article>arXiv:2510.26550v1 Announce Type: new \nAbstract: We present EdgeRunner 20B, a fine-tuned version of gpt-oss-20b optimized for military tasks. EdgeRunner 20B was trained on 1.6M high-quality records curated from military documentation and websites. We also present four new tests sets: (a) combat arms, (b) combat medic, (c) cyber operations, and (d) mil-bench-5k (general military knowledge). On these military test sets, EdgeRunner 20B matches or exceeds GPT-5 task performance with 95%+ statistical significance, except for the high reasoning setting on the combat medic test set and the low reasoning setting on the mil-bench-5k test set. Versus gpt-oss-20b, there is no statistically-significant regression on general-purpose benchmarks like ARC-C, GPQA Diamond, GSM8k, IFEval, MMLU Pro, or TruthfulQA, except for GSM8k in the low reasoning setting. We also present analyses on hyperparameter settings, cost, and throughput. These findings show that small, locally-hosted models are ideal solutions for data-sensitive operations such as in the military domain, allowing for deployment in air-gapped edge devices.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A GenEO-type coarse space with smaller eigenproblems","url":"https://arxiv.org/abs/2510.26548","date":1761883200,"author":"","guid":322991,"unread":true,"content":"<article>arXiv:2510.26548v1 Announce Type: new \nAbstract: Coarse spaces are essential to ensure robustness w.r.t. the number of subdomains in two-level overlapping Schwarz methods. Robustness with respect to the coefficients of the underlying partial differential equation (PDE) can be achieved by adaptive (or spectral) coarse spaces involving the solution of local eigenproblems. The solution of these eigenproblems, although scalable, entails a large setup cost which may exceed the cost for the iteration phase. In this paper we present and analyse a new variant of the GenEO (Generalised Eigenproblems in the Overlap) coarse space which involves solving eigenproblems only in a strip connected to the boundary of the subdomain. This leads to a significant reduction of the setup cost while the method satisfies a similar coefficient-robust condition number estimate as the original method, albeit with a possibly larger coarse space.</article>","contentLength":929,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WeaveRec: An LLM-Based Cross-Domain Sequential Recommendation Framework with Model Merging","url":"https://arxiv.org/abs/2510.26546","date":1761883200,"author":"","guid":322992,"unread":true,"content":"<article>arXiv:2510.26546v1 Announce Type: new \nAbstract: Cross-Domain Sequential Recommendation (CDSR) seeks to improve user preference modeling by transferring knowledge from multiple domains. Despite the progress made in CDSR, most existing methods rely on overlapping users or items to establish cross-domain correlations-a requirement that rarely holds in real-world settings. The advent of large language models (LLM) and model-merging techniques appears to overcome this limitation by unifying multi-domain data without explicit overlaps. Yet, our empirical study shows that naively training an LLM on combined domains-or simply merging several domain-specific LLMs-often degrades performance relative to a model trained solely on the target domain. To address these challenges, we first experimentally investigate the cause of suboptimal performance in LLM-based cross-domain recommendation and model merging. Building on these insights, we introduce WeaveRec, which cross-trains multiple LoRA modules with source and target domain data in a weaving fashion, and fuses them via model merging. WeaveRec can be extended to multi-source domain scenarios and notably does not introduce additional inference-time cost in terms of latency or memory. Furthermore, we provide a theoretical guarantee that WeaveRec can reduce the upper bound of the expected error in the target domain. Extensive experiments on single-source, multi-source, and cross-platform cross-domain recommendation scenarios validate that WeaveRec effectively mitigates performance degradation and consistently outperforms baseline approaches in real-world recommendation tasks.</article>","contentLength":1640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Structure of Relation Decoding Linear Operators in Large Language Models","url":"https://arxiv.org/abs/2510.26543","date":1761883200,"author":"","guid":322993,"unread":true,"content":"<article>arXiv:2510.26543v1 Announce Type: new \nAbstract: This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific.</article>","contentLength":1188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reduced order modelling of Hopf bifurcations for the Navier-Stokes equations through invariant manifolds","url":"https://arxiv.org/abs/2510.26542","date":1761883200,"author":"","guid":322994,"unread":true,"content":"<article>arXiv:2510.26542v1 Announce Type: new \nAbstract: This work introduces a parametric simulation-free reduced order model for incompressible flows undergoing a Hopf bifurcation, leveraging the parametrisation method for invariant manifolds. Unlike data-driven approaches, this method operates directly on the governing equations, eliminating the need for full-order simulations. The proposed model is computed at a single value of the bifurcation parameter yet remains valid over a range of values. The approach systematically constructs an invariant manifold and embedded dynamics, providing an accurate and efficient reduction of the original system. The ability to capture pre-critical steady states, the bifurcation point, and post-critical limit cycle oscillations is demonstrated by a strong agreement between the reduced order model and full order simulations, while achieving significant computational speed-up.</article>","contentLength":916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains","url":"https://arxiv.org/abs/2510.26541","date":1761883200,"author":"","guid":322995,"unread":true,"content":"<article>arXiv:2510.26541v1 Announce Type: new \nAbstract: The use of ML in engineering has grown steadily to support a wide array of applications. Among these methods, deep neural networks have been widely adopted due to their performance and accessibility, but they require large, high-quality datasets. Experimental data are often sparse, noisy, or insufficient to build resilient data-driven models. Transfer learning, which leverages relevant data-abundant source domains to assist learning in data-scarce target domains, has shown efficacy. Parameter transfer, where pretrained weights are reused, is common but degrades under large domain shifts. Domain-adversarial neural networks (DANNs) help address this issue by learning domain-invariant representations, thereby improving transfer under greater domain shifts in a semi-supervised setting. However, DANNs can be unstable during training and lack a native means for uncertainty quantification. This study introduces a fully-supervised three-stage framework, the staged Bayesian domain-adversarial neural network (staged B-DANN), that combines parameter transfer and shared latent space adaptation. In Stage 1, a deterministic feature extractor is trained on the source domain. This feature extractor is then adversarially refined using a DANN in Stage 2. In Stage 3, a Bayesian neural network is built on the adapted feature extractor for fine-tuning on the target domain to handle conditional shifts and yield calibrated uncertainty estimates. This staged B-DANN approach was first validated on a synthetic benchmark, where it was shown to significantly outperform standard transfer techniques. It was then applied to the task of predicting critical heat flux in rectangular channels, leveraging data from tube experiments as the source domain. The results of this study show that the staged B-DANN method can improve predictive accuracy and generalization, potentially assisting other domains in nuclear engineering.</article>","contentLength":1969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reflecting on Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models","url":"https://arxiv.org/abs/2510.26538","date":1761883200,"author":"","guid":322996,"unread":true,"content":"<article>arXiv:2510.26538v1 Announce Type: new \nAbstract: Software Engineering (SE) research involving the use of Large Language Models (LLMs) has introduced several new challenges related to rigour in benchmarking, contamination, replicability, and sustainability. In this paper, we invite the research community to reflect on how these challenges are addressed in SE. Our results provide a structured overview of current LLM-based SE research at ICSE, highlighting both encouraging practices and persistent shortcomings. We conclude with recommendations to strengthen benchmarking rigour, improve replicability, and address the financial and environmental costs of LLM-based SE.</article>","contentLength":671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration","url":"https://arxiv.org/abs/2510.26536","date":1761883200,"author":"","guid":322997,"unread":true,"content":"<article>arXiv:2510.26536v1 Announce Type: new \nAbstract: The proliferation of collaborative robots across diverse tasks and embodiments presents a central challenge: achieving lifelong adaptability, scalable coordination, and robust scheduling in multi-agent systems. Existing approaches, from vision-language-action (VLA) models to hierarchical frameworks, fall short due to their reliance on limited or dividual-agent memory. This fundamentally constrains their ability to learn over long horizons, scale to heterogeneous teams, or recover from failures, highlighting the need for a unified memory representation. To address these limitations, we introduce RoboOS-NeXT, a unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. At the core of RoboOS-NeXT is the novel Spatio-Temporal-Embodiment Memory (STEM), which integrates spatial scene geometry, temporal event history, and embodiment profiles into a shared representation. This memory-centric design is integrated into a brain-cerebellum framework, where a high-level brain model performs global planning by retrieving and updating STEM, while low-level controllers execute actions locally. This closed loop between cognition, memory, and execution enables dynamic task allocation, fault-tolerant collaboration, and consistent state synchronization. We conduct extensive experiments spanning complex coordination tasks in restaurants, supermarkets, and households. Our results demonstrate that RoboOS-NeXT achieves superior performance across heterogeneous embodiments, validating its effectiveness in enabling lifelong, scalable, and robust multi-robot collaboration. Project website: https://flagopen.github.io/RoboOS/</article>","contentLength":1707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Higher-Order Regularization Learning on Hypergraphs","url":"https://arxiv.org/abs/2510.26533","date":1761883200,"author":"","guid":322998,"unread":true,"content":"<article>arXiv:2510.26533v1 Announce Type: new \nAbstract: Higher-Order Hypergraph Learning (HOHL) was recently introduced as a principled alternative to classical hypergraph regularization, enforcing higher-order smoothness via powers of multiscale Laplacians induced by the hypergraph structure. Prior work established the well- and ill-posedness of HOHL through an asymptotic consistency analysis in geometric settings. We extend this theoretical foundation by proving the consistency of a truncated version of HOHL and deriving explicit convergence rates when HOHL is used as a regularizer in fully supervised learning. We further demonstrate its strong empirical performance in active learning and in datasets lacking an underlying geometric structure, highlighting HOHL's versatility and robustness across diverse learning settings.</article>","contentLength":828,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Collision-Avoidance Constraints for Ellipsoidal Obstacles in Optimal Control: Application to Path-Following MPC and UAVs","url":"https://arxiv.org/abs/2510.26531","date":1761883200,"author":"","guid":322999,"unread":true,"content":"<article>arXiv:2510.26531v1 Announce Type: new \nAbstract: This article proposes a modular optimal control framework for local three-dimensional ellipsoidal obstacle avoidance, exemplarily applied to model predictive path-following control. Static as well as moving obstacles are considered. Central to the approach is a computationally efficient and continuously differentiable condition for detecting collisions with ellipsoidal obstacles. A novel two-stage optimization approach mitigates numerical issues arising from the structure of the resulting optimal control problem. The effectiveness of the approach is demonstrated through simulations and real-world experiments with the Crazyflie quadrotor. This represents the first hardware demonstration of an MPC controller of this kind for UAVs in a three-dimensional task.</article>","contentLength":815,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Polybasic Speculative Decoding Through a Theoretical Perspective","url":"https://arxiv.org/abs/2510.26527","date":1761883200,"author":"","guid":323000,"unread":true,"content":"<article>arXiv:2510.26527v1 Announce Type: new \nAbstract: Inference latency stands as a critical bottleneck in the large-scale deployment of Large Language Models (LLMs). Speculative decoding methods have recently shown promise in accelerating inference without compromising the output distribution. However, existing work typically relies on a dualistic draft-verify framework and lacks rigorous theoretical grounding. In this paper, we introduce a novel \\emph{polybasic} speculative decoding framework, underpinned by a comprehensive theoretical analysis. Specifically, we prove a fundamental theorem that characterizes the optimal inference time for multi-model speculative decoding systems, shedding light on how to extend beyond the dualistic approach to a more general polybasic paradigm. Through our theoretical investigation of multi-model token generation, we expose and optimize the interplay between model capabilities, acceptance lengths, and overall computational cost. Our framework supports both standalone implementation and integration with existing speculative techniques, leading to accelerated performance in practice. Experimental results across multiple model families demonstrate that our approach yields speedup ratios ranging from $3.31\\times$ to $4.01\\times$ for LLaMA2-Chat 7B, up to $3.87 \\times$ for LLaMA3-8B, up to $4.43 \\times$ for Vicuna-7B and up to $3.85 \\times$ for Qwen2-7B -- all while preserving the original output distribution. We release our theoretical proofs and implementation code to facilitate further investigation into polybasic speculative decoding.</article>","contentLength":1590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models","url":"https://arxiv.org/abs/2510.26524","date":1761883200,"author":"","guid":323001,"unread":true,"content":"<article>arXiv:2510.26524v1 Announce Type: new \nAbstract: Heavy-tailed distributions, prevalent in a lot of real-world applications such as finance, telecommunications, queuing theory, and natural language processing, are challenging to model accurately owing to their slow tail decay. Bernstein phase-type (BPH) distributions, through their analytical tractability and good approximations in the non-tail region, can present a good solution, but they suffer from an inability to reproduce these heavy-tailed behaviors exactly, thus leading to inadequate performance in important tail areas. On the contrary, while highly adaptable to heavy-tailed distributions, hyperexponential (HE) models struggle in the body part of the distribution. Additionally, they are highly sensitive to initial parameter selection, significantly affecting their precision.\n  To solve these issues, we propose a novel hybrid model of BPH and HE distributions, borrowing the most desirable features from each for enhanced approximation quality. Specifically, we leverage an optimization to set initial parameters for the HE component, significantly enhancing its robustness and reducing the possibility that the associated procedure results in an invalid HE model. Experimental validation demonstrates that the novel hybrid approach is more performant than individual application of BPH or HE models. More precisely, it can capture both the body and the tail of heavy-tailed distributions, with a considerable enhancement in matching parameters such as mean and coefficient of variation. Additional validation through experiments utilizing queuing theory proves the practical usefulness, accuracy, and precision of our hybrid approach.</article>","contentLength":1703,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interdependent Privacy in Smart Homes: Hunting for Bystanders in Privacy Policies","url":"https://arxiv.org/abs/2510.26523","date":1761883200,"author":"","guid":323002,"unread":true,"content":"<article>arXiv:2510.26523v1 Announce Type: new \nAbstract: Smart home devices such as video doorbells and security cameras are becoming increasingly common in everyday life. While these devices offer convenience and safety, they also raise new privacy concerns: how these devices affect others, like neighbors, visitors, or people passing by. This issue is generally known as interdependent privacy, where one person's actions (or inaction) may impact the privacy of others, and, specifically, bystander privacy in the context of smart homes. Given lax data protection regulations in terms of shared physical spaces and amateur joint data controllers, we expect that the privacy policies of smart home products reflect the missing regulatory incentives. This paper presents a focused privacy policy analysis of 20 video doorbell and smart camera products, concentrating explicitly on the bystander aspect. We show that although some of the vendors acknowledge bystanders, they address it only to the extent of including disclaimers, shifting the ethical responsibility for collecting the data of non-users to the device owner. In addition, we identify and examine real-world cases related to bystander privacy, demonstrating how current deployments can impact non-users. Based on our findings, we analyze vendor privacy policies in light of existing legal frameworks and technical capabilities, and we provide practical recommendations for both policy language and system design to enhance transparency and empower both bystanders and device owners.</article>","contentLength":1539,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hebrew Diacritics Restoration using Visual Representation","url":"https://arxiv.org/abs/2510.26521","date":1761883200,"author":"","guid":323003,"unread":true,"content":"<article>arXiv:2510.26521v1 Announce Type: new \nAbstract: Diacritics restoration in Hebrew is a fundamental task for ensuring accurate word pronunciation and disambiguating textual meaning. Despite the language's high degree of ambiguity when unvocalized, recent machine learning approaches have significantly advanced performance on this task.\n  In this work, we present DIVRIT, a novel system for Hebrew diacritization that frames the task as a zero-shot classification problem. Our approach operates at the word level, selecting the most appropriate diacritization pattern for each undiacritized word from a dynamically generated candidate set, conditioned on the surrounding textual context. A key innovation of DIVRIT is its use of a Hebrew Visual Language Model, which processes undiacritized text as an image, allowing diacritic information to be embedded directly within the input's vector representation.\n  Through a comprehensive evaluation across various configurations, we demonstrate that the system effectively performs diacritization without relying on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting where the correct diacritized form is guaranteed to be among the provided candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic architectural enhancements and optimized training methodologies yield significant improvements in the system's overall generalization capabilities. These findings highlight the promising potential of visual representations for accurate and automated Hebrew diacritization.</article>","contentLength":1555,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Think Outside the Policy: In-Context Steered Policy Optimization","url":"https://arxiv.org/abs/2510.26519","date":1761883200,"author":"","guid":323004,"unread":true,"content":"<article>arXiv:2510.26519v1 Announce Type: new \nAbstract: Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such as Group Relative Policy Optimization (GRPO), have achieved remarkable progress in improving the reasoning capabilities of Large Reasoning Models (LRMs). However, they exhibit limited exploration due to reliance on on-policy rollouts where confined to the current policy's distribution, resulting in narrow trajectory diversity. Recent approaches attempt to expand policy coverage by incorporating trajectories generated from stronger expert models, yet this reliance increases computational cost and such advaned models are often inaccessible. To address these issues, we propose In-Context Steered Policy Optimization (ICPO), a unified framework that leverages the inherent in-context learning capability of LRMs to provide expert guidance using existing datasets. ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands exploration beyond the current policy distribution without requiring advanced LRM trajectories. To further stabilize optimization, ICPO integrates Expert Region Reject Sampling to filter unreliable off-policy trajectories and Annealed Expert-Bonus Reward Shaping to balance early expert guidance with later autonomous improvement. Results demonstrate that ICPO consistently enhances reinforcement learning performance and training stability on mathematical reasoning benchmarks, revealing a scalable and effective RLVR paradigm for LRMs.</article>","contentLength":1508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human-AI Complementarity: A Goal for Amplified Oversight","url":"https://arxiv.org/abs/2510.26518","date":1761883200,"author":"","guid":323005,"unread":true,"content":"<article>arXiv:2510.26518v1 Announce Type: new \nAbstract: Human feedback is critical for aligning AI systems to human values. As AI capabilities improve and AI is used to tackle more challenging tasks, verifying quality and safety becomes increasingly challenging. This paper explores how we can leverage AI to improve the quality of human oversight. We focus on an important safety problem that is already challenging for humans: fact-verification of AI outputs. We find that combining AI ratings and human ratings based on AI rater confidence is better than relying on either alone. Giving humans an AI fact-verification assistant further improves their accuracy, but the type of assistance matters. Displaying AI explanation, confidence, and labels leads to over-reliance, but just showing search results and evidence fosters more appropriate trust. These results have implications for Amplified Oversight -- the challenge of combining humans and AI to supervise AI systems even as they surpass human expert performance.</article>","contentLength":1014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Envisioning Future Interactive Web Development: Editing Webpage with Natural Language","url":"https://arxiv.org/abs/2510.26516","date":1761883200,"author":"","guid":323006,"unread":true,"content":"<article>arXiv:2510.26516v1 Announce Type: new \nAbstract: The evolution of web applications relies on iterative code modifications, a process that is traditionally manual and time-consuming. While Large Language Models (LLMs) can generate UI code, their ability to edit existing code from new design requirements (e.g., \"center the logo\") remains a challenge. This is largely due to the absence of large-scale, high-quality tuning data to align model performance with human expectations. In this paper, we introduce a novel, automated data generation pipeline that uses LLMs to synthesize a high-quality fine-tuning dataset for web editing, named Instruct4Edit. Our approach generates diverse instructions, applies the corresponding code modifications, and performs visual verification to ensure correctness. By fine-tuning models on Instruct4Edit, we demonstrate consistent improvement in translating human intent into precise, structurally coherent, and visually accurate code changes. This work provides a scalable and transparent foundation for natural language based web editing, demonstrating that fine-tuning smaller open-source models can achieve competitive performance with proprietary systems. We release all data, code implementations, and model checkpoints for reproduction.</article>","contentLength":1278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs","url":"https://arxiv.org/abs/2510.26512","date":1761883200,"author":"","guid":323007,"unread":true,"content":"<article>arXiv:2510.26512v1 Announce Type: new \nAbstract: Human smuggling networks are increasingly adaptive and difficult to analyze. Legal case documents offer critical insights but are often unstructured, lexically dense, and filled with ambiguous or shifting references, which pose significant challenges for automated knowledge graph (KG) construction. While recent LLM-based approaches improve over static templates, they still generate noisy, fragmented graphs with duplicate nodes due to the absence of guided extraction and coreference resolution. The recently proposed CORE-KG framework addresses these limitations by integrating a type-aware coreference module and domain-guided structured prompts, significantly reducing node duplication and legal noise. In this work, we present a systematic ablation study of CORE-KG to quantify the individual contributions of its two key components. Our results show that removing coreference resolution results in a 28.32% increase in node duplication and a 4.32% increase in noisy nodes, while removing structured prompts leads to a 4.34% increase in node duplication and a 73.33% increase in noisy nodes. These findings offer empirical insights for designing robust LLM-based pipelines for extracting structured representations from complex legal texts.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection","url":"https://arxiv.org/abs/2510.26510","date":1761883200,"author":"","guid":323008,"unread":true,"content":"<article>arXiv:2510.26510v1 Announce Type: new \nAbstract: Model and hyperparameter selection are critical but challenging in machine learning, typically requiring expert intuition or expensive automated search. We investigate whether large language models (LLMs) can act as in-context meta-learners for this task. By converting each dataset into interpretable metadata, we prompt an LLM to recommend both model families and hyperparameters. We study two prompting strategies: (1) a zero-shot mode relying solely on pretrained knowledge, and (2) a meta-informed mode augmented with examples of models and their performance on past tasks. Across synthetic and real-world benchmarks, we show that LLMs can exploit dataset metadata to recommend competitive models and hyperparameters without search, and that improvements from meta-informed prompting demonstrate their capacity for in-context meta-learning. These results highlight a promising new role for LLMs as lightweight, general-purpose assistants for model selection and hyperparameter optimization.</article>","contentLength":1044,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm","url":"https://arxiv.org/abs/2510.26509","date":1761883200,"author":"","guid":323009,"unread":true,"content":"<article>arXiv:2510.26509v1 Announce Type: new \nAbstract: The edge detection task is essential in image processing aiming to extract relevant information from an image. One recurring problem in this task is the weaknesses found in some detectors, such as the difficulty in detecting loose edges and the lack of context to extract relevant information from specific problems. To address these weaknesses and adapt the detector to the properties of an image, an adaptable detector described by two-dimensional cellular automaton and optimized by meta-heuristic combined with transfer learning techniques was developed. This study aims to analyze the impact of expanding the search space of the optimization phase and the robustness of the adaptability of the detector in identifying edges of a set of natural images and specialized subsets extracted from the same image set. The results obtained prove that expanding the search space of the optimization phase was not effective for the chosen image set. The study also analyzed the adaptability of the model through a series of experiments and validation techniques and found that, regardless of the validation, the model was able to adapt to the input and the transfer learning techniques applied to the model showed no significant improvements.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Metacognition and Confidence Dynamics in Advice Taking from Generative AI","url":"https://arxiv.org/abs/2510.26508","date":1761883200,"author":"","guid":323010,"unread":true,"content":"<article>arXiv:2510.26508v1 Announce Type: new \nAbstract: Generative Artificial Intelligence (GenAI) can aid humans in a wide range of tasks, but its effectiveness critically depends on users being able to evaluate the accuracy of GenAI outputs and their own expertise. Here we asked how confidence in self and GenAI contributes to decisions to seek and rely on advice from GenAI ('prospective confidence'), and how advice-taking in turn shapes this confidence ('retrospective confidence'). In a novel paradigm involving text generation, participants formulated plans for events, and could request advice from a GenAI (Study 1; N=200) or were randomly assigned to receive advice (Study 2; N=300), which they could rely on or ignore. Advice requests in Study 1 were related to higher prospective confidence in GenAI and lower confidence in self. Advice-seekers showed increased retrospective confidence in GenAI, while those who declined advice showed increased confidence in self. Random assignment in Study 2 revealed that advice exposure increases confidence in GenAI and in self, suggesting that GenAI advice-taking causally boosts retrospective confidence. These results were mirrored in advice reliance, operationalised as the textual similarity between GenAI advice and participants' responses, with reliance associated with increased retrospective confidence in both GenAI and self. Critically, participants who chose to obtain/rely on advice provided more detailed responses (likely due to the output's verbosity), but failed to check the output thoroughly, missing key information. These findings underscore a key role for confidence in interactions with GenAI, shaped by both prior beliefs about oneself and the reliability of AI, and context-dependent exposure to advice.</article>","contentLength":1773,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters","url":"https://arxiv.org/abs/2510.26501","date":1761883200,"author":"","guid":323011,"unread":true,"content":"<article>arXiv:2510.26501v1 Announce Type: new \nAbstract: Continuous electrocardiogram (ECG) monitoring via wearables offers significant potential for early cardiovascular disease (CVD) detection. However, deploying deep learning models for automated analysis in resource-constrained environments faces reliability challenges due to inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen pathologies or noisecorrupted signals, often cause erroneous, high-confidence predictions by standard classifiers, compromising patient safety. Existing OOD detection methods either neglect computational constraints or address noise and unseen classes separately. This paper explores Unsupervised Anomaly Detection (UAD) as an independent, upstream filtering mechanism to improve robustness. We benchmark six UAD approaches, including Deep SVDD, reconstruction-based models, Masked Anomaly Detection, normalizing flows, and diffusion models, optimized via Neural Architecture Search (NAS) under strict resource constraints (at most 512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection of OOD CVD classes and signals unsuitable for analysis due to noise. Results show Deep SVDD consistently achieves the best trade-off between detection and efficiency. In a realistic deployment simulation, integrating the optimized Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21 percentage points over a classifier-only baseline. This study demonstrates that optimized UAD filters can safeguard automated ECG analysis, enabling safer, more reliable continuous cardiovascular monitoring on wearables.</article>","contentLength":1632,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CyberNER: A Harmonized STIX Corpus for Cybersecurity Named Entity Recognition","url":"https://arxiv.org/abs/2510.26499","date":1761883200,"author":"","guid":323012,"unread":true,"content":"<article>arXiv:2510.26499v1 Announce Type: new \nAbstract: Extracting structured intelligence via Named Entity Recognition (NER) is critical for cybersecurity, but the proliferation of datasets with incompatible annotation schemas hinders the development of comprehensive models. While combining these resources is desirable, we empirically demonstrate that naively concatenating them results in a noisy label space that severely degrades model performance. To overcome this critical limitation, we introduce CyberNER, a large-scale, unified corpus created by systematically harmonizing four prominent datasets (CyNER, DNRTI, APTNER, and Attacker) onto the STIX 2.1 standard. Our principled methodology resolves semantic ambiguities and consolidates over 50 disparate source tags into 21 coherent entity types. Our experiments show that models trained on CyberNER achieve a substantial performance gain, with a relative F1-score improvement of approximately 30% over the naive concatenation baseline. By publicly releasing the CyberNER corpus, we provide a crucial, standardized benchmark that enables the creation and rigorous comparison of more robust and generalizable entity extraction models for the cybersecurity domain.</article>","contentLength":1216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Multi-agent Large Language Model Framework to Automatically Assess Performance of a Clinical AI Triage Tool","url":"https://arxiv.org/abs/2510.26498","date":1761883200,"author":"","guid":323013,"unread":true,"content":"<article>arXiv:2510.26498v1 Announce Type: new \nAbstract: Purpose: The purpose of this study was to determine if an ensemble of multiple LLM agents could be used collectively to provide a more reliable assessment of a pixel-based AI triage tool than a single LLM.\n  Methods: 29,766 non-contrast CT head exams from fourteen hospitals were processed by a commercial intracranial hemorrhage (ICH) AI detection tool. Radiology reports were analyzed by an ensemble of eight open-source LLM models and a HIPAA compliant internal version of GPT-4o using a single multi-shot prompt that assessed for presence of ICH. 1,726 examples were manually reviewed. Performance characteristics of the eight open-source models and consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were tested for rating the performance of the triage tool.\n  Results: The cohort consisted of 29,766 head CTs exam-report pairs. The highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78). The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 &amp; 0.76). Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3 Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522 (0.500-0.543). No statistically significant differences were observed between Top-3, Full-9, and Consensus (p &gt; 0.05).\n  Conclusion: An ensemble of medium to large sized open-source LLMs provides a more consistent and reliable method to derive a ground truth retrospective evaluation of a clinical AI triage tool over a single LLM alone.</article>","contentLength":1704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration","url":"https://arxiv.org/abs/2510.26495","date":1761883200,"author":"","guid":323014,"unread":true,"content":"<article>arXiv:2510.26495v1 Announce Type: new \nAbstract: Recent advances in Text-to-SQL have achieved strong results in static, single-turn tasks, where models generate SQL queries from natural language questions. However, these systems fall short in real-world interactive scenarios, where user intents evolve and queries must be refined over multiple turns. In applications such as finance and business analytics, users iteratively adjust query constraints or dimensions based on intermediate results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a benchmark assessing model performance under evolving user interactions. Unlike previous manually curated datasets, DySQL-Bench is built through an automated two-stage pipeline of task synthesis and verification. Structured tree representations derived from raw database tables guide LLM-based task generation, followed by interaction-oriented filtering and expert validation. Human evaluation confirms 100% correctness of the synthesized data. We further propose a multi-turn evaluation framework simulating realistic interactions among an LLM-simulated user, the model under test, and an executable database. The model must adapt its reasoning and SQL generation as user intents change. DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling 1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the Pass@5 metric, underscoring the benchmark's difficulty. All code and data are released at https://github.com/Aurora-slz/Real-World-SQL-Bench .</article>","contentLength":1548,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simulating and Experimenting with Social Media Mobilization Using LLM Agents","url":"https://arxiv.org/abs/2510.26494","date":1761883200,"author":"","guid":323015,"unread":true,"content":"<article>arXiv:2510.26494v1 Announce Type: new \nAbstract: Online social networks have transformed the ways in which political mobilization messages are disseminated, raising new questions about how peer influence operates at scale. Building on the landmark 61-million-person Facebook experiment \\citep{bond201261}, we develop an agent-based simulation framework that integrates real U.S. Census demographic distributions, authentic Twitter network topology, and heterogeneous large language model (LLM) agents to examine the effect of mobilization messages on voter turnout. Each simulated agent is assigned demographic attributes, a personal political stance, and an LLM variant (\\texttt{GPT-4.1}, \\texttt{GPT-4.1-Mini}, or \\texttt{GPT-4.1-Nano}) reflecting its political sophistication. Agents interact over realistic social network structures, receiving personalized feeds and dynamically updating their engagement behaviors and voting intentions. Experimental conditions replicate the informational and social mobilization treatments of the original Facebook study. Across scenarios, the simulator reproduces qualitative patterns observed in field experiments, including stronger mobilization effects under social message treatments and measurable peer spillovers. Our framework provides a controlled, reproducible environment for testing counterfactual designs and sensitivity analyses in political mobilization research, offering a bridge between high-validity field experiments and flexible computational modeling.\\footnote{Code and data available at https://github.com/CausalMP/LLM-SocioPol}</article>","contentLength":1590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Context Engineering 2.0: The Context of Context Engineering","url":"https://arxiv.org/abs/2510.26493","date":1761883200,"author":"","guid":323016,"unread":true,"content":"<article>arXiv:2510.26493v1 Announce Type: new \nAbstract: Karl Marx once wrote that ``the human essence is the ensemble of social relations'', suggesting that individuals are not isolated entities but are fundamentally shaped by their interactions with other entities, within which contexts play a constitutive and essential role. With the advent of computers and artificial intelligence, these contexts are no longer limited to purely human--human interactions: human--machine interactions are included as well. Then a central question emerges: How can machines better understand our situations and purposes? To address this challenge, researchers have recently introduced the concept of context engineering. Although it is often regarded as a recent innovation of the agent era, we argue that related practices can be traced back more than twenty years. Since the early 1990s, the field has evolved through distinct historical phases, each shaped by the intelligence level of machines: from early human--computer interaction frameworks built around primitive computers, to today's human--agent interaction paradigms driven by intelligent agents, and potentially to human--level or superhuman intelligence in the future. In this paper, we situate context engineering, provide a systematic definition, outline its historical and conceptual landscape, and examine key design considerations for practice. By addressing these questions, we aim to offer a conceptual foundation for context engineering and sketch its promising future. This paper is a stepping stone for a broader community effort toward systematic context engineering in AI systems.</article>","contentLength":1636,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wireless Sensor Networks as Parallel and Distributed Hardware Platform for Artificial Neural Networks","url":"https://arxiv.org/abs/2510.26492","date":1761883200,"author":"","guid":323017,"unread":true,"content":"<article>arXiv:2510.26492v1 Announce Type: new \nAbstract: We are proposing fully parallel and maximally distributed hardware realization of a generic neuro-computing system. More specifically, the proposal relates to the wireless sensor networks technology to serve as a massively parallel and fully distributed hardware platform to implement and realize artificial neural network (ANN) algorithms. A parallel and distributed (PDP) hardware realization of ANNs makes it possible to have real time computation of large-scale (and complex) problems in a highly robust framework. We will demonstrate how a network of hundreds of thousands of processing nodes (or motes of a wireless sensor network), which have on-board processing and wireless communication features, can be used to implement fully parallel and massively distributed computation of artificial neural network algorithms for solution of truly large-scale problems in real time. The realization of artificial neural network algorithms in a massively parallel and fully distributed hardware has been the goal of neural network computing researchers. This is because a parallel and distributed computation of artificial neural network algorithms could not have been achieved against all the advancements in silicon- or optics-based computing. Accordingly, artificial neural networks could not be applied to very large-scale problems for real time computation of solutions. This hindered the development of neural algorithms for affordable and practical solutions of challenging problems since often special-purpose computing approaches in hardware, software or hybrid (non-neural) had to be developed for and fine-tuned to specific problems that are very large-scale and highly complex. Successful implementation is likely to revolutionize computing as we know it by making it possible to solve very large scale scientific, engineering or technical problems in real time.</article>","contentLength":1921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Efficient RLVR via Off-Policy Influence Guidance","url":"https://arxiv.org/abs/2510.26491","date":1761883200,"author":"","guid":323018,"unread":true,"content":"<article>arXiv:2510.26491v1 Announce Type: new \nAbstract: Data selection is a critical aspect of Reinforcement Learning with Verifiable Rewards (RLVR) for enhancing the reasoning capabilities of large language models (LLMs). Current data selection methods are largely heuristic-based, lacking theoretical guarantees and generalizability. This work proposes a theoretically-grounded approach using influence functions to estimate the contribution of each data point to the learning objective. To overcome the prohibitive computational cost of policy rollouts required for online influence estimation, we introduce an off-policy influence estimation method that efficiently approximates data influence using pre-collected offline trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we employ sparse random projection to reduce dimensionality and improve storage and computation efficiency. Leveraging these techniques, we develop \\textbf{C}urriculum \\textbf{R}L with \\textbf{O}ff-\\textbf{P}olicy \\text{I}nfluence guidance (\\textbf{CROPI}), a multi-stage RL framework that iteratively selects the most influential data for the current policy. Experiments on models up to 7B parameters demonstrate that CROPI significantly accelerates training. On a 1.5B model, it achieves a 2.66x step-level acceleration while using only 10\\% of the data per stage compared to full-dataset training. Our results highlight the substantial potential of influence-based data selection for efficient RLVR.</article>","contentLength":1498,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaffolding Creativity: How Divergent and Convergent LLM Personas Shape Human Machine Creative Problem-Solving","url":"https://arxiv.org/abs/2510.26490","date":1761883200,"author":"","guid":323019,"unread":true,"content":"<article>arXiv:2510.26490v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly shaping creative work and problem-solving; however, prior research suggests that they may diminish unassisted creativity. To address this tension, a coach-like LLM environment was developed that embodies divergent and convergent thinking personas as two complementary processes. Effectiveness and user behavior were assessed through a controlled experiment in which participants interacted with either persona, while a control group engaged with a standard LLM providing direct answers.\n  Notably, users' perceptions of which persona best supported their creativity often diverged from objective performance measures. Trait-based analyses revealed that individual differences predict when people utilize divergent versus convergent personas, suggesting opportunities for adaptive sequencing. Furthermore, interaction patterns reflected the design thinking model, demonstrating how persona-guided support shapes creative problem-solving.\n  Our findings provide design principles for creativity support systems that strike a balance between exploration and convergence through persona-based guidance and personalization. These insights advance human-AI collaboration tools that scaffold rather than overshadow human creativity.</article>","contentLength":1320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Gated Recurrent GAN with Gaussian Uncertainty for Network Anomaly Detection","url":"https://arxiv.org/abs/2510.26487","date":1761883200,"author":"","guid":323020,"unread":true,"content":"<article>arXiv:2510.26487v1 Announce Type: new \nAbstract: Anomaly detection in time-series data is a critical challenge with significant implications for network security. Recent quantum machine learning approaches, such as quantum kernel methods and variational quantum circuits, have shown promise in capturing complex data distributions for anomaly detection but remain constrained by limited qubit counts. We introduce in this work a novel Quantum Gated Recurrent Unit (QGRU)-based Generative Adversarial Network (GAN) employing Successive Data Injection (SuDaI) and a multi-metric gating strategy for robust network anomaly detection. Our model uniquely utilizes a quantum-enhanced generator that outputs parameters (mean and log-variance) of a Gaussian distribution via reparameterization, combined with a Wasserstein critic to stabilize adversarial training. Anomalies are identified through a novel gating mechanism that initially flags potential anomalies based on Gaussian uncertainty estimates and subsequently verifies them using a composite of critic scores and reconstruction errors. Evaluated on benchmark datasets, our method achieves a high time-series aware F1 score (TaF1) of 89.43% demonstrating superior capability in detecting anomalies accurately and promptly as compared to existing classical and quantum models. Furthermore, the trained QGRU-WGAN was deployed on real IBM Quantum hardware, where it retained high anomaly detection performance, confirming its robustness and practical feasibility on current noisy intermediate-scale quantum (NISQ) devices.</article>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human Smuggling Networks","url":"https://arxiv.org/abs/2510.26486","date":1761883200,"author":"","guid":323021,"unread":true,"content":"<article>arXiv:2510.26486v1 Announce Type: new \nAbstract: Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bayesian Network Fusion of Large Language Models for Sentiment Analysis","url":"https://arxiv.org/abs/2510.26484","date":1761883200,"author":"","guid":323022,"unread":true,"content":"<article>arXiv:2510.26484v1 Announce Type: new \nAbstract: Large language models (LLMs) continue to advance, with an increasing number of domain-specific variants tailored for specialised tasks. However, these models often lack transparency and explainability, can be costly to fine-tune, require substantial prompt engineering, yield inconsistent results across domains, and impose significant adverse environmental impact due to their high computational demands. To address these challenges, we propose the Bayesian network LLM fusion (BNLF) framework, which integrates predictions from three LLMs, including FinBERT, RoBERTa, and BERTweet, through a probabilistic mechanism for sentiment analysis. BNLF performs late fusion by modelling the sentiment predictions from multiple LLMs as probabilistic nodes within a Bayesian network. Evaluated across three human-annotated financial corpora with distinct linguistic and contextual characteristics, BNLF demonstrates consistent gains of about six percent in accuracy over the baseline LLMs, underscoring its robustness to dataset variability and the effectiveness of probabilistic fusion for interpretable sentiment classification.</article>","contentLength":1171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Who Has The Final Say? Conformity Dynamics in ChatGPT's Selections","url":"https://arxiv.org/abs/2510.26481","date":1761883200,"author":"","guid":323023,"unread":true,"content":"<article>arXiv:2510.26481v1 Announce Type: new \nAbstract: Large language models (LLMs) such as ChatGPT are increasingly integrated into high-stakes decision-making, yet little is known about their susceptibility to social influence. We conducted three preregistered conformity experiments with GPT-4o in a hiring context. In a baseline study, GPT consistently favored the same candidate (Profile C), reported moderate expertise (M = 3.01) and high certainty (M = 3.89), and rarely changed its choice. In Study 1 (GPT + 8), GPT faced unanimous opposition from eight simulated partners and almost always conformed (99.9%), reporting lower certainty and significantly elevated self-reported informational and normative conformity (p &lt; .001). In Study 2 (GPT + 1), GPT interacted with a single partner and still conformed in 40.2% of disagreement trials, reporting less certainty and more normative conformity. Across studies, results demonstrate that GPT does not act as an independent observer but adapts to perceived social consensus. These findings highlight risks of treating LLMs as neutral decision aids and underline the need to elicit AI judgments prior to exposing them to human opinions.</article>","contentLength":1185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Extract Method Refactoring with Open-Source LLMs: A Comparative Study","url":"https://arxiv.org/abs/2510.26480","date":1761883200,"author":"","guid":323024,"unread":true,"content":"<article>arXiv:2510.26480v1 Announce Type: new \nAbstract: Automating the Extract Method refactoring (EMR) remains challenging and largely manual despite its importance in improving code readability and maintainability. Recent advances in open-source, resource-efficient Large Language Models (LLMs) offer promising new approaches for automating such high-level tasks. In this work, we critically evaluate five state-of-the-art open-source LLMs, spanning 3B to 8B parameter sizes, on the EMR task for Python code. We systematically assess functional correctness and code quality using automated metrics and investigate the impact of prompting strategies by comparing one-shot prompting to a Recursive criticism and improvement (RCI) approach. RCI-based prompting consistently outperforms one-shot prompting in test pass rates and refactoring quality. The best-performing models, Deepseek-Coder-RCI and Qwen2.5-Coder-RCI, achieve test pass percentage (TPP) scores of 0.829 and 0.808, while reducing lines of code (LOC) per method from 12.103 to 6.192 and 5.577, and cyclomatic complexity (CC) from 4.602 to 3.453 and 3.294, respectively. A developer survey on RCI-generated refactorings shows over 70% acceptance, with Qwen2.5-Coder rated highest across all evaluation criteria. In contrast, the original code scored below neutral, particularly in readability and maintainability, underscoring the benefits of automated refactoring guided by quality prompts. While traditional metrics like CC and LOC provide useful signals, they often diverge from human judgments, emphasizing the need for human-in-the-loop evaluation. Our open-source benchmark offers a foundation for future research on automated refactoring with LLMs.</article>","contentLength":1711,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems","url":"https://arxiv.org/abs/2510.26475","date":1761883200,"author":"","guid":323025,"unread":true,"content":"<article>arXiv:2510.26475v1 Announce Type: new \nAbstract: Adapting large language models (LLMs) via reinforcement learning (RL) is often bottlenecked by the generation stage, which can consume over 75\\% of the training time. Speculative decoding (SD) accelerates autoregressive generation in serving systems, but its behavior under RL training remains largely unexplored. We identify three critical gaps that hinder the naive integration of SD into RL systems: diminishing speedups at large batch sizes, drafter staleness under continual actor updates, and drafter-induced policy degradation.\n  To address these gaps, we present ReSpec, a system that adapts SD to RL through three complementary mechanisms: dynamically tuning SD configurations, evolving the drafter via knowledge distillation, and weighting updates by rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup while preserving reward convergence and training stability, providing a practical solution for efficient RL-based LLM adaptation.</article>","contentLength":1017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing","url":"https://arxiv.org/abs/2510.26474","date":1761883200,"author":"","guid":323026,"unread":true,"content":"<article>arXiv:2510.26474v1 Announce Type: new \nAbstract: Self-improvement has emerged as a mainstream paradigm for advancing the reasoning capabilities of large vision-language models (LVLMs), where models explore and learn from successful trajectories iteratively. However, we identify a critical issue during this process: the model excels at generating high-quality trajectories for simple queries (i.e., head data) but struggles with more complex ones (i.e., tail data). This leads to an imbalanced optimization that drives the model to prioritize simple reasoning skills, while hindering its ability to tackle more complex reasoning tasks. Over iterations, this imbalance becomes increasingly pronounced--a dynamic we term the \"Matthew effect\"--which ultimately hinders further model improvement and leads to performance bottlenecks. To counteract this challenge, we introduce four efficient strategies from two perspectives: distribution-reshaping and trajectory-resampling, to achieve head-tail re-balancing during the exploration-and-learning self-improvement process. Extensive experiments on Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks demonstrate that our methods consistently improve visual reasoning capabilities, outperforming vanilla self-improvement by 3.86 points on average.</article>","contentLength":1317,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wireless Memory Approximation for Energy-efficient Task-specific IoT Data Retrieval","url":"https://arxiv.org/abs/2510.26473","date":1761883200,"author":"","guid":323027,"unread":true,"content":"<article>arXiv:2510.26473v1 Announce Type: new \nAbstract: The use of Dynamic Random Access Memory (DRAM) for storing Machine Learning (ML) models plays a critical role in accelerating ML inference tasks in the next generation of communication systems. However, periodic refreshment of DRAM results in wasteful energy consumption during standby periods, which is significant for resource-constrained Internet of Things (IoT) devices. To solve this problem, this work advocates two novel approaches: 1) wireless memory activation and 2) wireless memory approximation. These enable the wireless devices to efficiently manage the available memory by considering the timing aspects and relevance of ML model usage; hence, reducing the overall energy consumption. Numerical results show that our proposed scheme can realize smaller energy consumption than the always-on approach while satisfying the retrieval accuracy constraint.</article>","contentLength":915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition","url":"https://arxiv.org/abs/2510.26466","date":1761883200,"author":"","guid":323028,"unread":true,"content":"<article>arXiv:2510.26466v1 Announce Type: new \nAbstract: Object-context shortcuts remain a persistent challenge in vision-language models, undermining zero-shot reliability when test-time scenes differ from familiar training co-occurrences. We recast this issue as a causal inference problem and ask: Would the prediction remain if the object appeared in a different environment? To answer this at inference time, we estimate object and background expectations within CLIP's representation space, and synthesize counterfactual embeddings by recombining object features with diverse alternative contexts sampled from external datasets, batch neighbors, or text-derived descriptions. By estimating the Total Direct Effect and simulating intervention, we further subtract background-only activation, preserving beneficial object-context interactions while mitigating hallucinated scores. Without retraining or prompt design, our method substantially improves both worst-group and average accuracy on context-sensitive benchmarks, establishing a new zero-shot state of the art. Beyond performance, our framework provides a lightweight representation-level counterfactual approach, offering a practical causal avenue for debiased and reliable multimodal reasoning.</article>","contentLength":1251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection","url":"https://arxiv.org/abs/2510.26464","date":1761883200,"author":"","guid":323029,"unread":true,"content":"<article>arXiv:2510.26464v1 Announce Type: new \nAbstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few known normal samples. Most existing methods rely on the generalization ability of pre-trained vision-language models (VLMs) to recognize potentially anomalous regions through feature similarity between text descriptions and images. However, due to the lack of detailed textual descriptions, these methods can only pre-define image-level descriptions to match each visual patch token to identify potential anomalous regions, which leads to the semantic misalignment between image descriptions and patch-level visual anomalies, achieving sub-optimal localization performance. To address the above issues, we propose the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and fine-grained textual descriptions for existing anomaly detection datasets with automatic construction pipeline. Based on the MFSC, we propose a novel framework named FineGrainedAD to improve anomaly localization performance, which consists of two components: Multi-Level Learnable Prompt (MLLP) and Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics into multi-level learnable prompts through automatic replacement and concatenation mechanism, while MLSA designs region aggregation strategy and multi-level alignment training to facilitate learnable prompts better align with corresponding visual regions. Experiments demonstrate that the proposed FineGrainedAD achieves superior overall performance in few-shot settings on MVTec-AD and VisA datasets.</article>","contentLength":1599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MIREDO: MIP-Driven Resource-Efficient Dataflow Optimization for Computing-in-Memory Accelerator","url":"https://arxiv.org/abs/2510.26463","date":1761883200,"author":"","guid":323030,"unread":true,"content":"<article>arXiv:2510.26463v1 Announce Type: new \nAbstract: Computing-in-Memory (CIM) architectures have emerged as a promising solution for accelerating Deep Neural Networks (DNNs) by mitigating data movement bottlenecks. However, realizing the potential of CIM requires specialized dataflow optimizations, which are challenged by an expansive design space and strict architectural constraints. Existing optimization approaches often fail to fully exploit CIM accelerators, leading to noticeable gaps between theoretical and actual system-level efficiency. To address these limitations, we propose the MIREDO framework, which formulates dataflow optimization as a Mixed-Integer Programming (MIP) problem. MIREDO introduces a hierarchical hardware abstraction coupled with an analytical latency model designed to accurately reflect the complex data transfer behaviors within CIM systems. By jointly modeling workload characteristics, dataflow strategies, and CIM-specific constraints, MIREDO systematically navigates the vast design space to determine the optimal dataflow configurations. Evaluation results demonstrate that MIREDO significantly enhances performance, achieving up to $3.2\\times$ improvement across various DNN models and hardware setups.</article>","contentLength":1243,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vectorized Context-Aware Embeddings for GAT-Based Collaborative Filtering","url":"https://arxiv.org/abs/2510.26461","date":1761883200,"author":"","guid":323031,"unread":true,"content":"<article>arXiv:2510.26461v1 Announce Type: new \nAbstract: Recommender systems often struggle with data sparsity and cold-start scenarios, limiting their ability to provide accurate suggestions for new or infrequent users. This paper presents a Graph Attention Network (GAT) based Collaborative Filtering (CF) framework enhanced with Large Language Model (LLM) driven context aware embeddings. Specifically, we generate concise textual user profiles and unify item metadata (titles, genres, overviews) into rich textual embeddings, injecting these as initial node features in a bipartite user item graph. To further optimize ranking performance, we introduce a hybrid loss function that combines Bayesian Personalized Ranking (BPR) with a cosine similarity term and robust negative sampling, ensuring explicit negative feedback is distinguished from unobserved data. Experiments on the MovieLens 100k and 1M datasets show consistent improvements over state-of-the-art baselines in Precision, NDCG, and MAP while demonstrating robustness for users with limited interaction history. Ablation studies confirm the critical role of LLM-augmented embeddings and the cosine similarity term in capturing nuanced semantic relationships. Our approach effectively mitigates sparsity and cold-start limitations by integrating LLM-derived contextual understanding into graph-based architectures. Future directions include balancing recommendation accuracy with coverage and diversity, and introducing fairness-aware constraints and interpretability features to enhance system performance further.</article>","contentLength":1573,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning","url":"https://arxiv.org/abs/2510.26457","date":1761883200,"author":"","guid":323032,"unread":true,"content":"<article>arXiv:2510.26457v1 Announce Type: new \nAbstract: Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches have been proposed, where LLM-based methods have significantly advanced the capabilities of automated review generation. However, existing models primarily focus on general-purpose code review, their effectiveness in identifying and addressing security-related issues remains underexplored. Moreover, adapting existing code review approaches to target security issues faces substantial challenges, including data scarcity and inadequate evaluation metrics. To address these limitations, we propose SecureReviewer, a new approach designed for enhancing LLMs' ability to identify and resolve security-related issues during code review. Specifically, we first construct a dataset tailored for training and evaluating secure code review capabilities. Leveraging this dataset, we fine-tune LLMs to generate code review comments that can effectively identify security issues and provide fix suggestions with our proposed secure-aware fine-tuning strategy. To mitigate hallucination in LLMs and enhance the reliability of their outputs, we integrate the RAG technique, which grounds the generated comments in domain-specific security knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric designed to assess the effectiveness of review comments in addressing security issues. Experimental results demonstrate that SecureReviewer outperforms state-of-the-art baselines in both security issue detection accuracy and the overall quality and practical utility of generated review comments.</article>","contentLength":1947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PolarZero: A Reinforcement Learning Approach for Low-Complexity Polarization Kernel Design","url":"https://arxiv.org/abs/2510.26452","date":1761883200,"author":"","guid":323033,"unread":true,"content":"<article>arXiv:2510.26452v1 Announce Type: new \nAbstract: Polar codes with large kernels can achieve improved error exponents but are challenging to design with low decoding com- plexity. This work investigates kernel construction under recursive maximum likelihood decoding (RMLD) using a reinforcement learning framework based on the Gumbel AlphaZero algorithm. The proposed method efficiently explores the design space and identifies large-size kernels that satisfy a given error exponent while minimizing decoding complexity. For a size-16 kernel, it achieves 17% lower decoding complexity than handcrafted designs while reaching an error exponent of 0.5183 compared to 0.5 for Arikan's kernel, demonstrating the effectiveness of the learning-based approach for practical polar code construction.</article>","contentLength":791,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Graph Condensation via Classification Complexity Mitigation","url":"https://arxiv.org/abs/2510.26451","date":1761883200,"author":"","guid":323034,"unread":true,"content":"<article>arXiv:2510.26451v1 Announce Type: new \nAbstract: Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel Manifold-constrained Robust Graph Condensation framework named MRGC. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of \\ModelName\\ across diverse attack scenarios.</article>","contentLength":1399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models","url":"https://arxiv.org/abs/2510.26446","date":1761883200,"author":"","guid":323035,"unread":true,"content":"<article>arXiv:2510.26446v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in language comprehension and generation; however, their widespread adoption is constrained by substantial bandwidth and computational demands. While pruning and low-rank approximation have each demonstrated promising performance individually, their synergy for LLMs remains underexplored. We introduce \\underline{S}ynergistic \\underline{S}parse and \\underline{L}ow-Rank \\underline{C}ompression (SSLC) methods for LLMs, which leverages the strengths of both techniques: low-rank approximation compresses the model by retaining its essential structure with minimal information loss, whereas sparse optimization eliminates non-essential weights, preserving those crucial for generalization. Based on theoretical analysis, we first formulate the low-rank approximation and sparse optimization as a unified problem and solve it by iterative optimization algorithm. Experiments on LLaMA and Qwen2.5 models (7B-70B) show that SSLC, without any additional training steps, consistently surpasses standalone methods, achieving state-of-the-arts results. Notably, SSLC compresses Qwen2.5 by 50\\% with no performance drop and achieves at least 1.63$\\times$ speedup, offering a practical solution for efficient LLM deployment.</article>","contentLength":1333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Personalized Treatment Outcome Prediction from Scarce Data via Dual-Channel Knowledge Distillation and Adaptive Fusion","url":"https://arxiv.org/abs/2510.26444","date":1761883200,"author":"","guid":323036,"unread":true,"content":"<article>arXiv:2510.26444v1 Announce Type: new \nAbstract: Personalized treatment outcome prediction based on trial data for small-sample and rare patient groups is critical in precision medicine. However, the costly trial data limit the prediction performance. To address this issue, we propose a cross-fidelity knowledge distillation and adaptive fusion network (CFKD-AFN), which leverages abundant but low-fidelity simulation data to enhance predictions on scarce but high-fidelity trial data. CFKD-AFN incorporates a dual-channel knowledge distillation module to extract complementary knowledge from the low-fidelity model, along with an attention-guided fusion module to dynamically integrate multi-source information. Experiments on treatment outcome prediction for the chronic obstructive pulmonary disease demonstrates significant improvements of CFKD-AFN over state-of-the-art methods in prediction accuracy, ranging from 6.67\\% to 74.55\\%, and strong robustness to varying high-fidelity dataset sizes. Furthermore, we extend CFKD-AFN to an interpretable variant, enabling the exploration of latent medical semantics to support clinical decision-making.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PointSt3R: Point Tracking through 3D Grounded Correspondence","url":"https://arxiv.org/abs/2510.26443","date":1761883200,"author":"","guid":323037,"unread":true,"content":"<article>arXiv:2510.26443v1 Announce Type: new \nAbstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and MASt3R, have shown great potential in 2D and 3D correspondence in static scenes. In this paper, we propose to adapt them for the task of point tracking through 3D grounded correspondence. We first demonstrate that these models are competitive point trackers when focusing on static points, present in current point tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose to combine the reconstruction loss with training for dynamic correspondence along with a visibility head, and fine-tuning MASt3R for point tracking using a relatively small amount of synthetic data. Importantly, we only train and evaluate on pairs of frames where one contains the query point, effectively removing any temporal context. Using a mix of dynamic and static point correspondences, we achieve competitive or superior point tracking results on four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\% occlusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs 82.8). We also present results on 3D point tracking along with several ablations on training datasets and percentage of dynamic correspondences.</article>","contentLength":1345,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion-Aided Bandwidth-Efficient Semantic Communication with Adaptive Requests","url":"https://arxiv.org/abs/2510.26442","date":1761883200,"author":"","guid":323038,"unread":true,"content":"<article>arXiv:2510.26442v1 Announce Type: new \nAbstract: Semantic communication focuses on conveying the intrinsic meaning of data rather than its raw symbolic representation. For visual content, this paradigm shifts from traditional pixel-level transmission toward leveraging the semantic structure of images to communicate visual meaning. Existing approaches generally follow one of two paths: transmitting only text descriptions, which often fail to capture precise spatial layouts and fine-grained appearance details; or transmitting text alongside dense latent visual features, which tends to introduce substantial semantic redundancy. A key challenge, therefore, is to reduce semantic redundancy while preserving semantic understanding and visual fidelity, thereby improving overall transmission efficiency. This paper introduces a diffusion-based semantic communication framework with adaptive retransmission. The system transmits concise text descriptions together with a limited set of key latent visual features, and employs a diffusion-based inpainting model to reconstruct the image. A receiver-side semantic consistency mechanism is designed to evaluate the alignment between the reconstructed image and the original text description. When a semantic discrepancy is detected, the receiver triggers a retransmission to request a small set of additional latent blocks and refine the image reconstruction. This approach significantly reduces bandwidth usage while preserving high semantic accuracy, achieving an efficient balance between reconstruction quality and transmission overhead.</article>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models","url":"https://arxiv.org/abs/2510.26441","date":1761883200,"author":"","guid":323039,"unread":true,"content":"<article>arXiv:2510.26441v1 Announce Type: new \nAbstract: Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs' reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.</article>","contentLength":1796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The evolving surface morphochemical reaction-diffusion system for battery modeling","url":"https://arxiv.org/abs/2510.26437","date":1761883200,"author":"","guid":323040,"unread":true,"content":"<article>arXiv:2510.26437v1 Announce Type: new \nAbstract: It is well known that phase formation by electrodeposition yields films of poorly controllable morphology. This typically leads to a range of technological issues in many fields of electrochemical technology. Presently, a particularly relevant case is that of high-energy density next-generation batteries with metal anodes, that cannot yet reach practical cyclability targets, owing to uncontrolled elelctrode shape evolution. In this scenario, mathematical modelling is a key tool to lay the knowledge-base for materials-science advancements liable to lead to concretely stable battery material architectures. In this work, we introduce the Evolving Surface DIB (ESDIB) model, a reaction-diffusion system posed on a dynamically evolving electrode surface. Unlike previous fixed-surface formulations, the ESDIB model couples surface evolution to the local concentration of electrochemical species, allowing the geometry of the electrode itself to adapt in response to deposition. To handle the challenges related to the coupling between surface motion and species transport, we numerically solve the system by proposing an extension of the Lumped Evolving Surface Finite Element Method (LESFEM) for spatial discretisation, combined with an IMEX Euler scheme for time integration. The model is validated through six numerical experiments, each compared with laboratory images of electrodeposition. Results demonstrate that the ESDIB framework accurately captures branching and dendritic growth, providing a predictive and physically consistent tool for studying metal deposition phenomena in energy storage devices.</article>","contentLength":1664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Co-Evolving Latent Action World Models","url":"https://arxiv.org/abs/2510.26433","date":1761883200,"author":"","guid":323041,"unread":true,"content":"<article>arXiv:2510.26433v1 Announce Type: new \nAbstract: Adapting pre-trained video generation models into controllable world models via latent actions is a promising step towards creating generalist world models. The dominant paradigm adopts a two-stage approach that trains latent action model (LAM) and the world model separately, resulting in redundant training and limiting their potential for co-adaptation. A conceptually simple and appealing idea is to directly replace the forward dynamic model in LAM with a powerful world model and training them jointly, but it is non-trivial and prone to representational collapse. In this work, we propose CoLA-World, which for the first time successfully realizes this synergistic paradigm, resolving the core challenge in joint learning through a critical warm-up phase that effectively aligns the representations of the from-scratch LAM with the pre-trained world model. This unlocks a co-evolution cycle: the world model acts as a knowledgeable tutor, providing gradients to shape a high-quality LAM, while the LAM offers a more precise and adaptable control interface to the world model. Empirically, CoLA-World matches or outperforms prior two-stage methods in both video simulation quality and downstream visual planning, establishing a robust and efficient new paradigm for the field.</article>","contentLength":1331,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CHCVerif: A Portfolio-Based Solver for Constrained Horn Clauses","url":"https://arxiv.org/abs/2510.26431","date":1761883200,"author":"","guid":323042,"unread":true,"content":"<article>arXiv:2510.26431v1 Announce Type: new \nAbstract: Constrained Horn Clauses (CHCs) are widely adopted as intermediate representations for a variety of verification tasks, including safety checking, invariant synthesis, and interprocedural analysis. This paper introduces CHCVERIF, a portfolio-based CHC solver that adopts a software verification approach for solving CHCs. This approach enables us to reuse mature software verification tools to tackle CHC benchmarks, particularly those involving bitvectors and low-level semantics. Our evaluation shows that while the method enjoys only moderate success with linear integer arithmetic, it achieves modest success on bitvector benchmarks. Moreover, our results demonstrate the viability and potential of using software verification tools as backends for CHC solving, particularly when supported by a carefully constructed portfolio.</article>","contentLength":880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Theta as a Horn Solver","url":"https://arxiv.org/abs/2510.26430","date":1761883200,"author":"","guid":323043,"unread":true,"content":"<article>arXiv:2510.26430v1 Announce Type: new \nAbstract: Theta is a verification framework that has participated in the CHC-COMP competition since 2023. While its core approach -- based on transforming constrained Horn clauses (CHCs) into control-flow automata (CFAs) for analysis -- has remained mostly unchanged, Theta's verification techniques, design trade-offs, and limitations have remained mostly unexplored in the context of CHCs. This paper fills that gap: we provide a detailed description of the algorithms employed by Theta, highlighting the unique features that distinguish it from other CHC solvers. We also analyze the strengths and weaknesses of the tool in the context of CHC-COMP benchmarks. Notably, in the 2025 edition of the competition, Theta's performance was impacted by a configuration issue, leading to suboptimal results. To provide a clearer picture of Theta's actual capabilities, we re-execute the tool on the competition benchmarks under corrected settings and report on the resulting performance.</article>","contentLength":1020,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semantic Properties of Computations Defined by Elementary Inference Systems","url":"https://arxiv.org/abs/2510.26429","date":1761883200,"author":"","guid":323044,"unread":true,"content":"<article>arXiv:2510.26429v1 Announce Type: new \nAbstract: We consider sets/relations/computations defined by *Elementary Inference Systems* I, which are obtained from Smullyan's *elementary formal systems* using Gentzen's notation for inference rules, and proof trees for atoms P(t_1,...,t_n), where predicate P represents the considered set/relation/computation. A first-order theory Th(I), actually a set of definite Horn clauses, is given to I. Properties of objects defined by I are expressed as first-order sentences F, which are proved true or false by *satisfaction* M |= F of F in a *canonical* model M of Th(I). For this reason, we call F a *semantic property* of I. Since canonical models are, in general, incomputable, we show how to (dis)prove semantic properties  by satisfiability in an *arbitrary* model A of Th(I). We apply these ideas to the analysis of properties of programming languages and systems whose computations can be described by means of an elementary inference system. In particular, rewriting-based systems. \n</article>","contentLength":1031,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding Regular Herbrand Models for CHCs using Answer Set Programming","url":"https://arxiv.org/abs/2510.26428","date":1761883200,"author":"","guid":323045,"unread":true,"content":"<article>arXiv:2510.26428v1 Announce Type: new \nAbstract: We are interested in proving satisfiability of Constrained Horn Clauses (CHCs) over Algebraic Data Types (ADTs). We propose to prove satisfiability by building a tree automaton recognizing the Herbrand model of the CHCs. If such an automaton exists then the model is said to be regular, i.e., the Herbrand model is a regular set of atoms. Kostyukov et al. have shown how to derive an automaton when CVC4 finds a finite model of the CHCs. We propose an alternative way to build the automaton using an encoding into a SAT problem using Clingo, an Answer Set Programming (ASP) tool. We implemented a translation of CHCs with ADTs into an ASP problem. Combined with Clingo, we obtain a semi-complete satisfiability checker: it finds a tree automaton if a regular Herbrand model exists or finds a counter-example if the problem is unsatisfiable.</article>","contentLength":889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nexus: Execution-Grounded Multi-Agent Test Oracle Synthesis","url":"https://arxiv.org/abs/2510.26423","date":1761883200,"author":"","guid":323046,"unread":true,"content":"<article>arXiv:2510.26423v1 Announce Type: new \nAbstract: Test oracle generation in non-regression testing is a longstanding challenge in software engineering, where the goal is to produce oracles that can accurately determine whether a function under test (FUT) behaves as intended for a given input. In this paper, we introduce Nexus, a novel multi-agent framework to address this challenge. Nexus generates test oracles by leveraging a diverse set of specialized agents that synthesize test oracles through a structured process of deliberation, validation, and iterative self-refinement. During the deliberation phase, a panel of four specialist agents, each embodying a distinct testing philosophy, collaboratively critiques and refines an initial set of test oracles. Then, in the validation phase, Nexus generates a plausible candidate implementation of the FUT and executes the proposed oracles against it in a secure sandbox. For any oracle that fails this execution-based check, Nexus activates an automated selfrefinement loop, using the specific runtime error to debug and correct the oracle before re-validation. Our extensive evaluation on seven diverse benchmarks demonstrates that Nexus consistently and substantially outperforms state-of-theart baselines. For instance, Nexus improves the test-level oracle accuracy on the LiveCodeBench from 46.30% to 57.73% for GPT-4.1-Mini. The improved accuracy also significantly enhances downstream tasks: the bug detection rate of GPT4.1-Mini generated test oracles on HumanEval increases from 90.91% to 95.45% for Nexus compared to baselines, and the success rate of automated program repair improves from 35.23% to 69.32%.</article>","contentLength":1671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education","url":"https://arxiv.org/abs/2510.26422","date":1761883200,"author":"","guid":323047,"unread":true,"content":"<article>arXiv:2510.26422v1 Announce Type: new \nAbstract: With the rapid development of large language models (LLMs), various LLM-based works have been widely applied in educational fields. However, most existing LLMs and their benchmarks focus primarily on the knowledge dimension, largely neglecting the evaluation of cultivation capabilities that are essential for real-world educational scenarios. Additionally, current benchmarks are often limited to a single subject or question type, lacking sufficient diversity. This issue is particularly prominent within the Chinese context. To address this gap, we introduce OmniEduBench, a comprehensive Chinese educational benchmark. OmniEduBench consists of 24.602K high-quality question-answer pairs. The data is meticulously divided into two core dimensions: the knowledge dimension and the cultivation dimension, which contain 18.121K and 6.481K entries, respectively. Each dimension is further subdivided into 6 fine-grained categories, covering a total of 61 different subjects (41 in the knowledge and 20 in the cultivation). Furthermore, the dataset features a rich variety of question formats, including 11 common exam question types, providing a solid foundation for comprehensively evaluating LLMs' capabilities in education. Extensive experiments on 11 mainstream open-source and closed-source LLMs reveal a clear performance gap. In the knowledge dimension, only Gemini-2.5 Pro surpassed 60\\% accuracy, while in the cultivation dimension, the best-performing model, QWQ, still trailed human intelligence by nearly 30\\%. These results highlight the substantial room for improvement and underscore the challenges of applying LLMs in education.</article>","contentLength":1692,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification","url":"https://arxiv.org/abs/2510.26420","date":1761883200,"author":"","guid":323048,"unread":true,"content":"<article>arXiv:2510.26420v1 Announce Type: new \nAbstract: The rapid advancement of deep neural networks (DNNs) heavily relies on large-scale, high-quality datasets. However, unauthorized commercial use of these datasets severely violates the intellectual property rights of dataset owners. Existing backdoor-based dataset ownership verification methods suffer from inherent limitations: poison-label watermarks are easily detectable due to label inconsistencies, while clean-label watermarks face high technical complexity and failure on high-resolution images. Moreover, both approaches employ static watermark patterns that are vulnerable to detection and removal. To address these issues, this paper proposes a sample-specific clean-label backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked sample generator, this method generates unique watermarks for each sample, fundamentally overcoming the vulnerability of static watermark patterns. The core innovation lies in designing a composite loss function with three components: target sample loss ensures watermark effectiveness, non-target sample loss guarantees trigger reliability, and perceptual similarity loss maintains visual imperceptibility. During ownership verification, black-box testing is employed to check whether suspicious models exhibit predefined backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method and its robustness against potential watermark removal attacks.</article>","contentLength":1513,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chain-of-Thought Hijacking","url":"https://arxiv.org/abs/2510.26418","date":1761883200,"author":"","guid":323049,"unread":true,"content":"<article>arXiv:2510.26418v1 Announce Type: new \nAbstract: Large reasoning models (LRMs) achieve higher task performance by allocating more inference-time compute, and prior works suggest this scaled reasoning may also strengthen safety by improving refusal. Yet we find the opposite: the same reasoning can be used to bypass safeguards. We introduce Chain-of-Thought Hijacking, a jailbreak attack on reasoning models. The attack pads harmful requests with long sequences of harmless puzzle reasoning. Across HarmBench, CoT Hijacking reaches a 99%, 94%, 100%, and 94% attack success rate (ASR) on Gemini 2.5 Pro, GPT o4 mini, Grok 3 mini, and Claude 4 Sonnet, respectively - far exceeding prior jailbreak methods for LRMs. To understand the effectiveness of our attack, we turn to a mechanistic analysis, which shows that mid layers encode the strength of safety checking, while late layers encode the verification outcome. Long benign CoT dilutes both signals by shifting attention away from harmful tokens. Targeted ablations of attention heads identified by this analysis causally decrease refusal, confirming their role in a safety subnetwork. These results show that the most interpretable form of reasoning - explicit CoT - can itself become a jailbreak vector when combined with final-answer cues. We release prompts, outputs, and judge decisions to facilitate replication.</article>","contentLength":1370,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Environmental Impact of CI/CD Pipelines","url":"https://arxiv.org/abs/2510.26413","date":1761883200,"author":"","guid":323050,"unread":true,"content":"<article>arXiv:2510.26413v1 Announce Type: new \nAbstract: CI/CD pipelines are widely used in software development, yet their environmental impact, particularly carbon and water footprints (CWF), remains largely unknown to developers, as CI service providers typically do not disclose such information. With the growing environmental impact of cloud computing, understanding the CWF of CI/CD services has become increasingly important.\n  This work investigates the CWF of using GitHub Actions, focusing on open-source repositories where usage is free and unlimited for standard runners. We build upon a methodology from the Cloud Carbon Footprint framework and we use the largest dataset of workflow runs reported in the literature to date, comprising over 2.2 million workflow runs from more than 18,000 repositories.\n  Our analysis reveals that the GitHub Actions ecosystem results in a substantial CWF. Our estimates for the carbon footprint in 2024 range from 150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5 kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon footprint and 5,738.2 kiloliters for water footprint. To provide perspective, the carbon footprint in the most likely scenario is equivalent to the carbon captured by 7,615 urban trees in a year, and the water footprint is comparable to the water consumed by an average American family over 5,053 years.\n  We explore strategies to mitigate this impact, primarily by reducing wasted computational resources. Key recommendations include deploying runners in regions whose energy production has a low environmental impact such as France and the United Kingdom, implementing stricter deactivation policies for scheduled runs and aligning their execution with periods when the regional energy mix is more environmentally favorable, and reducing the size of repositories.</article>","contentLength":1947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation","url":"https://arxiv.org/abs/2510.26412","date":1761883200,"author":"","guid":323051,"unread":true,"content":"<article>arXiv:2510.26412v1 Announce Type: new \nAbstract: Recently text-to-video generation has made impressive progress in producing short, high-quality clips, but evaluating long-form outputs remains a major challenge especially when processing complex prompts. Existing benchmarks mostly rely on simplified prompts and focus on low-level metrics, overlooking fine-grained alignment with prompts and abstract dimensions such as narrative coherence and thematic expression. To address these gaps, we propose LoCoT2V-Bench, a benchmark specifically designed for long video generation (LVG) under complex input conditions. Based on various real-world videos, LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating elements like scene transitions and event dynamics. Moreover, it constructs a multi-dimensional evaluation framework that includes our newly proposed metrics such as event-level alignment, fine-grained temporal consistency, content clarity, and the Human Expectation Realization Degree (HERD) that focuses on more abstract attributes like narrative flow, emotional response, and character development. Using this framework, we conduct a comprehensive evaluation of nine representative LVG models, finding that while current methods perform well on basic visual and temporal aspects, they struggle with inter-event consistency, fine-grained alignment, and high-level thematic adherence, etc. Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for evaluating long-form complex text-to-video generation and highlights critical directions for future method improvement.</article>","contentLength":1613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders","url":"https://arxiv.org/abs/2510.26411","date":1761883200,"author":"","guid":323052,"unread":true,"content":"<article>arXiv:2510.26411v1 Announce Type: new \nAbstract: Artificial intelligence in healthcare requires models that are accurate and interpretable. We advance mechanistic interpretability in medical vision by applying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP, a vision-language model trained on chest radiographs and reports. To quantify interpretability, we propose an evaluation framework that combines correlation metrics, entropy analyzes, and automated neuron naming via the MedGEMMA foundation model. Experiments on the CheXpert dataset show that MedSAE neurons achieve higher monosemanticity and interpretability than raw MedCLIP features. Our findings bridge high-performing medical AI and transparency, offering a scalable step toward clinically reliable representations.</article>","contentLength":800,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Barlow Twins for Sequential Recommendation","url":"https://arxiv.org/abs/2510.26407","date":1761883200,"author":"","guid":323053,"unread":true,"content":"<article>arXiv:2510.26407v1 Announce Type: new \nAbstract: Sequential recommendation models must navigate sparse interaction data popularity bias and conflicting objectives like accuracy versus diversity While recent contrastive selfsupervised learning SSL methods offer improved accuracy they come with tradeoffs large batch requirements reliance on handcrafted augmentations and negative sampling that can reinforce popularity bias In this paper we introduce BT-SR a novel noncontrastive SSL framework that integrates the Barlow Twins redundancyreduction principle into a Transformerbased nextitem recommender BTSR learns embeddings that align users with similar shortterm behaviors while preserving longterm distinctionswithout requiring negative sampling or artificial perturbations This structuresensitive alignment allows BT-SR to more effectively recognize emerging user intent and mitigate the influence of noisy historical context Our experiments on five public benchmarks demonstrate that BTSR consistently improves nextitem prediction accuracy and significantly enhances longtail item coverage and recommendation calibration Crucially we show that a single hyperparameter can control the accuracydiversity tradeoff enabling practitioners to adapt recommendations to specific application needs</article>","contentLength":1293,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human-in-the-loop Online Rejection Sampling for Robotic Manipulation","url":"https://arxiv.org/abs/2510.26406","date":1761883200,"author":"","guid":323054,"unread":true,"content":"<article>arXiv:2510.26406v1 Announce Type: new \nAbstract: Reinforcement learning (RL) is widely used to produce robust robotic manipulation policies, but fine-tuning vision-language-action (VLA) models with RL can be unstable due to inaccurate value estimates and sparse supervision at intermediate steps. In contrast, imitation learning (IL) is easy to train but often underperforms due to its offline nature. In this paper, we propose Hi-ORS, a simple yet effective post-training method that utilizes rejection sampling to achieve both training stability and high robustness. Hi-ORS stabilizes value estimation by filtering out negatively rewarded samples during online fine-tuning, and adopts a reward-weighted supervised training objective to provide dense intermediate-step supervision. For systematic study, we develop an asynchronous inference-training framework that supports flexible online human-in-the-loop corrections, which serve as explicit guidance for learning error-recovery behaviors. Across three real-world tasks and two embodiments, Hi-ORS fine-tunes a pi-base policy to master contact-rich manipulation in just 1.5 hours of real-world training, outperforming RL and IL baselines by a substantial margin in both effectiveness and efficiency. Notably, the fine-tuned policy exhibits strong test-time scalability by reliably executing complex error-recovery behaviors to achieve better performance.</article>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explicit Consistency Error Estimate for Finite Element Solutions of the Poisson Equation on Convex Domains","url":"https://arxiv.org/abs/2510.26404","date":1761883200,"author":"","guid":323055,"unread":true,"content":"<article>arXiv:2510.26404v1 Announce Type: new \nAbstract: We derive explicit a priori consistency error estimates for a standard finite element discretization of the Poisson equation on convex domains, where the domain is approximated by an internal convex polyhedron. The obtained explicit estimates depend only on global geometric parameters and are applicable to general convex domains and arbitrary families of simplicial meshes.</article>","contentLength":424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education","url":"https://arxiv.org/abs/2510.26402","date":1761883200,"author":"","guid":323056,"unread":true,"content":"<article>arXiv:2510.26402v1 Announce Type: new \nAbstract: The rapid growth of programming education has outpaced traditional assessment tools, leaving faculty with limited means to provide meaningful, scalable feedback. Conventional autograders, while efficient, act as black-box systems that simply return pass/fail results, offering little insight into student thinking or learning needs.\n  Autograder+ is designed to shift autograding from a purely summative process to a formative learning experience. It introduces two key capabilities: automated feedback generation using a fine-tuned Large Language Model, and visualization of student code submissions to uncover learning patterns. The model is fine-tuned on curated student code and expert feedback to ensure pedagogically aligned, context-aware guidance.\n  In evaluation across 600 student submissions from multiple programming tasks, the system produced feedback with strong semantic alignment to instructor comments. For visualization, contrastively learned code embeddings trained on 1,000 annotated submissions enable grouping solutions into meaningful clusters based on functionality and approach. The system also supports prompt-pooling, allowing instructors to guide feedback style through selected prompt templates.\n  By integrating AI-driven feedback, semantic clustering, and interactive visualization, Autograder+ reduces instructor workload while supporting targeted instruction and promoting stronger learning outcomes.</article>","contentLength":1482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safety Margins of Inverse Optimal ISSf Controllers","url":"https://arxiv.org/abs/2510.26397","date":1761883200,"author":"","guid":323057,"unread":true,"content":"<article>arXiv:2510.26397v1 Announce Type: new \nAbstract: We investigate the gain margin of a general nonlinear system under an inverse optimal input-to-state safe (ISSf) controller of the form u=u0(x)+u*(x,u0), where u0 is the nominal control and u* is the inverse optimal safety filter that minimally modifies the nominal controller's unsafe actions over the infinite horizon. By first establishing a converse ISSf-BF theorem, we reveal the equivalence among the achievability of ISSf by feedback, the achievability of inverse optimality, and the solvability of a Hamilton-Jacobi-Isaacs equation associated with the inverse optimal ISSf gain assignment. Then we develop a collection of safety margin results on the overall control u=u0+u*. In the absence of disturbances, we find that standard inverse optimal safe controllers have a certain degree of gain margin. Specifically, when f(x) acts safely but u0 acts unsafely, the gain can be decreased by up to half; and when f(x) acts unsafely, we establish that, if u0 acts safely, the gain can be increased arbitrarily, whereas if u0 acts unsafely, the control recovers the full gain margin [1/2,inf). It is shown, however, that under control gain variation, the safe set of these controllers is locally asymptotically stable, which implies that their safety is sensitive to large but bounded disturbances. To make inverse optimal ISSf controllers robust to gain variation, we propose a gain margin improvement approach at the expense of an increased control effort. This improvement allows the inverse optimal safe control to inherit the standard gain margin of [1/2,inf) without requiring prior knowledge of whether f(x) or u0 acts safely on the safety boundary, while simultaneously ensuring global asymptotic stability of the resulting safe set. In the presence of disturbances, this improvement idea renders inverse optimal ISSf controllers robust to gain variations with the same gain margin of [1/2,inf).</article>","contentLength":1954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Pragmatic View of AI Personhood","url":"https://arxiv.org/abs/2510.26396","date":1761883200,"author":"","guid":323058,"unread":true,"content":"<article>arXiv:2510.26396v1 Announce Type: new \nAbstract: The emergence of agentic Artificial Intelligence (AI) is set to trigger a \"Cambrian explosion\" of new kinds of personhood. This paper proposes a pragmatic framework for navigating this diversification by treating personhood not as a metaphysical property to be discovered, but as a flexible bundle of obligations (rights and responsibilities) that societies confer upon entities for a variety of reasons, especially to solve concrete governance problems. We argue that this traditional bundle can be unbundled, creating bespoke solutions for different contexts. This will allow for the creation of practical tools -- such as facilitating AI contracting by creating a target \"individual\" that can be sanctioned -- without needing to resolve intractable debates about an AI's consciousness or rationality. We explore how individuals fit in to social roles and discuss the use of decentralized digital identity technology, examining both \"personhood as a problem\", where design choices can create \"dark patterns\" that exploit human social heuristics, and \"personhood as a solution\", where conferring a bundle of obligations is necessary to ensure accountability or prevent conflict. By rejecting foundationalist quests for a single, essential definition of personhood, this paper offers a more pragmatic and flexible way to think about integrating AI agents into our society.</article>","contentLength":1421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"XWAVE: A Novel Software-Defined Everything Approach for the Manufacturing Industry","url":"https://arxiv.org/abs/2510.26393","date":1761883200,"author":"","guid":323059,"unread":true,"content":"<article>arXiv:2510.26393v1 Announce Type: new \nAbstract: The manufacturing sector is moving from rigid, hardware-dependent systems toward flexible, software-driven environments. This transformation is shaped by the convergence of several Software-Defined technologies: Software-Defined Automation virtualizes industrial control, replacing proprietary PLCs with containerized, programmable solutions that enable scalability and interoperability. Software-Defined Compute and Communications provide a means to distribute intelligence seamlessly across devices, networks, and cloud platforms, reducing latency and enabling dynamic reconfiguration. Software-Defined Manufacturing Systems, usually implemented as Digital Twins, are real-time virtual models of machines and processes, allowing predictive analysis, optimization, and closer integration between human operators and intelligent systems. This work presents XWAVE, a project that unites these three Software-Defined paradigms to present a modular, fully software-defined manufacturing system.</article>","contentLength":1040,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey","url":"https://arxiv.org/abs/2510.26392","date":1761883200,"author":"","guid":323060,"unread":true,"content":"<article>arXiv:2510.26392v1 Announce Type: new \nAbstract: Multi-task learning (MTL) enables simultaneous training across related tasks, leveraging shared information to improve generalization, efficiency, and robustness, especially in data-scarce or high-dimensional scenarios. While deep learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor, and effectiveness with small datasets.\n  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting shared representations, task regularization, and structural coupling strategies. Special attention is given to emerging TWSVM extensions for multi-task settings, which show promise but remain underexplored. We compare these models in terms of theoretical properties, optimization strategies, and empirical performance, and discuss applications in fields such as computer vision, natural language processing, and bioinformatics.\n  Finally, we identify research gaps and outline future directions for building scalable, interpretable, and reliable margin-based MTL frameworks. This work provides a comprehensive resource for researchers and practitioners interested in SVM- and TWSVM-based multi-task learning.</article>","contentLength":1262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models","url":"https://arxiv.org/abs/2510.26391","date":1761883200,"author":"","guid":323061,"unread":true,"content":"<article>arXiv:2510.26391v1 Announce Type: new \nAbstract: Existing EEG-driven image reconstruction methods often overlook spatial attention mechanisms, limiting fidelity and semantic coherence. To address this, we propose a dual-conditioning framework that combines EEG embeddings with spatial saliency maps to enhance image generation. Our approach leverages the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals with visual semantics, while a ControlNet branch conditions generation on saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves a significant improvement in the quality of low- and high-level image features over existing approaches. Simultaneously, strongly aligning with human visual attention. The results demonstrate that attentional priors resolve EEG ambiguities, enabling high-fidelity reconstructions with applications in medical diagnostics and neuroadaptive interfaces, advancing neural decoding through efficient adaptation of pre-trained diffusion models.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning","url":"https://arxiv.org/abs/2510.26389","date":1761883200,"author":"","guid":323062,"unread":true,"content":"<article>arXiv:2510.26389v1 Announce Type: new \nAbstract: Recently, deep multi-agent reinforcement learning (MARL) has demonstrated promising performance for solving challenging tasks, such as long-term dependencies and non-Markovian environments. Its success is partly attributed to conditioning policies on large fixed context length. However, such large fixed context lengths may lead to limited exploration efficiency and redundant information. In this paper, we propose a novel MARL framework to obtain adaptive and effective contextual information. Specifically, we design a central agent that dynamically optimizes context length via temporal gradient analysis, enhancing exploration to facilitate convergence to global optima in MARL. Furthermore, to enhance the adaptive optimization capability of the context length, we present an efficient input representation for the central agent, which effectively filters redundant information. By leveraging a Fourier-based low-frequency truncation method, we extract global temporal trends across decentralized agents, providing an effective and efficient representation of the MARL environment. Extensive experiments demonstrate that the proposed method achieves state-of-the-art (SOTA) performance on long-term dependency tasks, including PettingZoo, MiniGrid, Google Research Football (GRF), and StarCraft Multi-Agent Challenge v2 (SMACv2).</article>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scales++: Compute Efficient Evaluation Subset Selection with Cognitive Scales Embeddings","url":"https://arxiv.org/abs/2510.26384","date":1761883200,"author":"","guid":323063,"unread":true,"content":"<article>arXiv:2510.26384v1 Announce Type: new \nAbstract: The prohibitive cost of evaluating large language models (LLMs) on comprehensive benchmarks necessitates the creation of small yet representative data subsets (i.e., tiny benchmarks) that enable efficient assessment while retaining predictive fidelity. Current methods for this task operate under a model-centric paradigm, selecting benchmarking items based on the collective performance of existing models. Such approaches are limited by large upfront costs, an inability to immediately handle new benchmarks (`cold-start'), and the fragile assumption that future models will share the failure patterns of their predecessors. In this work, we challenge this paradigm and propose a item-centric approach to benchmark subset selection, arguing that selection should be based on the intrinsic properties of the task items themselves, rather than on model-specific failure patterns. We instantiate this item-centric efficient benchmarking approach via a novel method, Scales++, where data selection is based on the cognitive demands of the benchmark samples. Empirically, we show Scales++ reduces the upfront selection cost by over 18x while achieving competitive predictive fidelity. On the Open LLM Leaderboard, using just a 0.5\\% data subset, we predict full benchmark scores with a 2.9% mean absolute error. We demonstrate that this item-centric approach enables more efficient model evaluation without significant fidelity degradation, while also providing better cold-start performance and more interpretable benchmarking.</article>","contentLength":1574,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advancing Forest Fires Classification using Neurochaos Learning","url":"https://arxiv.org/abs/2510.26383","date":1761883200,"author":"","guid":323064,"unread":true,"content":"<article>arXiv:2510.26383v1 Announce Type: new \nAbstract: Forest fires are among the most dangerous and unpredictable natural disasters worldwide. Forest fire can be instigated by natural causes or by humans. They are devastating overall, and thus, many research efforts have been carried out to predict whether a fire can occur in an area given certain environmental variables. Many research works employ Machine Learning (ML) and Deep Learning (DL) models for classification; however, their accuracy is merely adequate and falls short of expectations. This limit arises because these models are unable to depict the underlying nonlinearity in nature and extensively rely on substantial training data, which is hard to obtain. We propose using Neurochaos Learning (NL), a chaos-based, brain-inspired learning algorithm for forest fire classification. Like our brains, NL needs less data to learn nonlinear patterns in the training data. It employs one-dimensional chaotic maps, namely the Generalized L\\\"uroth Series (GLS), as neurons. NL yields comparable performance with ML and DL models, sometimes even surpassing them, particularly in low-sample training regimes, and unlike deep neural networks, NL is interpretable as it preserves causal structures in the data. Random Heterogenous Neurochaos Learning (RHNL), a type of NL where different chaotic neurons are randomnly located to mimic the randomness and heterogeneity of human brain gives the best F1 score of 1.0 for the Algerian Forest Fires Dataset. Compared to other traditional ML classifiers considered, RHNL also gives high precision score of 0.90 for Canadian Forest Fires Dataset and 0.68 for Portugal Forest Fires Dataset. The results obtained from this work indicate that Neurochaos Learning (NL) architectures achieve better performance than conventional machine learning classifiers, highlighting their promise for developing more efficient and reliable forest fire detection systems.</article>","contentLength":1947,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Mathematician as a Partner in Advancing Mathematical Discovery - A Case Study in Homogenization Theory","url":"https://arxiv.org/abs/2510.26380","date":1761883200,"author":"","guid":323065,"unread":true,"content":"<article>arXiv:2510.26380v1 Announce Type: new \nAbstract: Artificial intelligence (AI) has demonstrated impressive progress in mathematical reasoning, yet its integration into the practice of mathematical research remains limited. In this study, we investigate how the AI Mathematician (AIM) system can operate as a research partner rather than a mere problem solver. Focusing on a challenging problem in homogenization theory, we analyze the autonomous reasoning trajectories of AIM and incorporate targeted human interventions to structure the discovery process. Through iterative decomposition of the problem into tractable subgoals, selection of appropriate analytical methods, and validation of intermediate results, we reveal how human intuition and machine computation can complement one another. This collaborative paradigm enhances the reliability, transparency, and interpretability of the resulting proofs, while retaining human oversight for formal rigor and correctness. The approach leads to a complete and verifiable proof, and more broadly, demonstrates how systematic human-AI co-reasoning can advance the frontier of mathematical discovery.</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings","url":"https://arxiv.org/abs/2510.26376","date":1761883200,"author":"","guid":323066,"unread":true,"content":"<article>arXiv:2510.26376v1 Announce Type: new \nAbstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal predictability and major drivers of extreme winter weather. Yet, their accurate and efficient forecast remains a persistent challenge for numerical weather prediction (NWP) systems due to limitations in physical representation, initialization, and the immense computational demands of ensemble forecasts. While data-driven forecasting is rapidly evolving, its application to the complex, three-dimensional dynamics of SSWs, particularly for probabilistic forecast, remains underexplored. Here, we bridge this gap by developing a Flow Matching-based generative AI model (FM-Cast) for efficient and skillful probabilistic forecasting of the spatiotemporal evolution of stratospheric circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast skillfully forecasts the onset, intensity, and morphology of 10 events up to 20 days in advance, achieving ensemble accuracies above 50%. Its performance is comparable to or exceeds leading NWP systems while requiring only two minutes for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging FM-Cast as a scientific tool, we demonstrate through idealized experiments that SSW predictability is fundamentally linked to its underlying physical drivers, distinguishing between events forced from the troposphere and those driven by internal stratospheric dynamics. Our work thus establishes a computationally efficient paradigm for probabilistic forecasting stratospheric anomalies and showcases generative AI's potential to deepen the physical understanding of atmosphere-climate dynamics.</article>","contentLength":1675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asymptotic meshes from $r$-variational adaptation methods for static problems in one dimension","url":"https://arxiv.org/abs/2510.26375","date":1761883200,"author":"","guid":323067,"unread":true,"content":"<article>arXiv:2510.26375v1 Announce Type: new \nAbstract: We consider the minimization of integral functionals in one dimension and their approximation by $r$-adaptive finite elements. Including the grid of the FEM approximation as a variable in the minimization, we are able to show that the optimal grid configurations have a well-defined limit when the number of nodes in the grid is being sent to infinity. This is done by showing that the suitably renormalized energy functionals possess a limit in the sense of $\\Gamma$-convergence. We provide numerical examples showing the closeness of the optimal asymptotic mesh obtained as a minimizer of the $\\Gamma$-limit to the optimal finite meshes.</article>","contentLength":688,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BOTS: A Unified Framework for Bayesian Online Task Selection in LLM Reinforcement Finetuning","url":"https://arxiv.org/abs/2510.26374","date":1761883200,"author":"","guid":323068,"unread":true,"content":"<article>arXiv:2510.26374v1 Announce Type: new \nAbstract: Reinforcement finetuning (RFT) is a key technique for aligning Large Language Models (LLMs) with human preferences and enhancing reasoning, yet its effectiveness is highly sensitive to which tasks are explored during training. Uniform task sampling is inefficient, wasting computation on tasks that are either trivial or unsolvable, while existing task selection methods often suffer from high rollout costs, poor adaptivity, or incomplete evidence. We introduce \\textbf{BOTS}, a unified framework for \\textbf{B}ayesian \\textbf{O}nline \\textbf{T}ask \\textbf{S}election in LLM reinforcement finetuning. Grounded in Bayesian inference, BOTS adaptively maintains posterior estimates of task difficulty as the model evolves. It jointly incorporates \\emph{explicit evidence} from direct evaluations of selected tasks and \\emph{implicit evidence} inferred from these evaluations for unselected tasks, with Thompson sampling ensuring a principled balance between exploration and exploitation. To make implicit evidence practical, we instantiate it with an ultra-light interpolation-based plug-in that estimates difficulties of unevaluated tasks without extra rollouts, adding negligible overhead. Empirically, across diverse domains and LLM scales, BOTS consistently improves data efficiency and performance over baselines and ablations, providing a practical and extensible solution for dynamic task selection in RFT.</article>","contentLength":1460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens","url":"https://arxiv.org/abs/2510.26372","date":1761883200,"author":"","guid":323069,"unread":true,"content":"<article>arXiv:2510.26372v1 Announce Type: new \nAbstract: Generative modeling has recently achieved remarkable success across text, image, and audio domains, demonstrating powerful capabilities for unified representation learning. However, audio generation models still face challenges in terms of audio quality and generalization ability across tasks. This fragmentation results in redundant development efforts, inconsistent performance, and limited extensibility. To address these issues, we propose \\textbf{UniTok-Audio}, a scalable and extensible framework for unified audio generation tasks. Specifically, 1) UniTok-Audio extracts continuous feature of conditions to generates discrete tokens of target audio in an autoregressive manner; 2) a special task identifier token unifies different learning patterns of multiple tasks in a single framework; 3) a dual-stream audio codec involving acoustic and semantic branch is developed for high-fidelity waveform reconstruction. Experimental results demonstrate that UniTok-Audio achieves competitive performance in comparation with state-of-the-art task-specific or multi-task systems across five time-aligned tasks: speech restoration, target speaker extraction, speech separation, voice conversion, and language-queried audio source separation. To foster future research, we will open-source our codebase. The demo page of our work can be found here: https://alibaba.github.io/unified-audio.</article>","contentLength":1436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unambiguous Acceptance of Thin Coalgebras","url":"https://arxiv.org/abs/2510.26371","date":1761883200,"author":"","guid":323070,"unread":true,"content":"<article>arXiv:2510.26371v1 Announce Type: new \nAbstract: Automata admitting at most one accepting run per structure, known as unambiguous automata, find applications in verification of reactive systems as they extend the class of deterministic automata whilst maintaining some of their desirable properties. In this paper, we generalise a classical construction of unambiguous automata from thin trees to thin coalgebras for analytic functors. This achieves two goals: extending the existing construction to a larger class of structures, and providing conceptual clarity and parametricity to the construction by formalising it in the coalgebraic framework. As part of the construction, we link automaton acceptance of languages of thin coalgebras to language recognition via so-called coherent algebras, which were previously introduced for studying thin coalgebras. This link also allows us to establish an automata-theoretic characterisation of languages recognised by finite coherent algebras.</article>","contentLength":988,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse","url":"https://arxiv.org/abs/2510.26369","date":1761883200,"author":"","guid":323071,"unread":true,"content":"<article>arXiv:2510.26369v1 Announce Type: new \nAbstract: Worker location data is key to higher productivity in industrial sites. Cameras are a promising tool for localization in logistics warehouses since they also offer valuable environmental contexts such as package status. However, identifying individuals with only visual data is often impractical. Accordingly, several prior studies identified people in videos by comparing their trajectories and wearable sensor measurements. While this approach has advantages such as independence from appearance, the existing methods may break down under real-world conditions. To overcome this challenge, we propose CorVS, a novel data-driven person identification method based on correspondence between visual tracking trajectories and sensor measurements. Firstly, our deep learning model predicts correspondence probabilities and reliabilities for every pair of a trajectory and sensor measurements. Secondly, our algorithm matches the trajectories and sensor measurements over time using the predicted probabilities and reliabilities. We developed a dataset with actual warehouse operations and demonstrated the method's effectiveness for real-world applications.</article>","contentLength":1203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Command-filter-based trajectory-tracking control of quadrotor subject to internal and external disturbances","url":"https://arxiv.org/abs/2510.26368","date":1761883200,"author":"","guid":323072,"unread":true,"content":"<article>arXiv:2510.26368v1 Announce Type: new \nAbstract: We propose a command-filter backstepping controller that integrates a disturbance observer and a high-gain observer (HGO) to handle unknown internal and external disturbances acting on a quadrotor. To build the controller, we first define tracking errors between the measured and desired quadrotor outputs, which allow the system to be rewritten in a new set of state variables. Using this transformed model, we apply Lyapunov theory to derive a backstepping control law. To avoid repeated differentiation of states and virtual controls, a first-order command filter is introduced, and a nonlinear disturbance observer is added to provide disturbance estimates. Each state in the controller and observer is replaced with its estimate from the HGO. The resulting control law enables the quadrotor to follow its path despite internal and external disturbances, with each subsystem allowed its own disturbance type for realism. A new state transformation and Lyapunov-based derivation prevent the usual explosion of complexity, while the HGO reconstructs unmeasured states and their rates for output feedback. The nonlinear disturbance observer attenuates constant and nonlinear disturbances as well as band-limited white noise. The method reduces dependence on high-precision sensors and mitigates wind, model error, and rotor noise effects during flight. Unlike previous studies that treat either disturbance rejection or partial sensing, this work combines the command filter, disturbance observer, and HGO to address both challenges simultaneously while avoiding the complexity growth typical of backstepping designs.</article>","contentLength":1667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incorporating Local H\\\"older Regularity into PINNs for Solving Elliptic PDEs","url":"https://arxiv.org/abs/2510.26365","date":1761883200,"author":"","guid":323073,"unread":true,"content":"<article>arXiv:2510.26365v1 Announce Type: new \nAbstract: In this paper, local H\\\"older regularization is incorporated into a physics-informed neural networks (PINNs) framework for solving elliptic partial differential equations (PDEs). Motivated by the interior regularity properties of linear elliptic PDEs, a modified loss function is constructed by introducing local H\\\"older regularization term. To approximate this term effectively, a variable-distance discrete sampling strategy is developed. Error estimates are established to assess the generalization performance of the proposed method. Numerical experiments on a range of elliptic problems demonstrate notable improvements in both prediction accuracy and robustness compared to standard physics-informed neural networks.</article>","contentLength":772,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Reinforcement Learning Based Log Loading Automation","url":"https://arxiv.org/abs/2510.26363","date":1761883200,"author":"","guid":323074,"unread":true,"content":"<article>arXiv:2510.26363v1 Announce Type: new \nAbstract: Forestry forwarders play a central role in mechanized timber harvesting by picking up and moving logs from the felling site to a processing area or a secondary transport vehicle. Forwarder operation is challenging and physically and mentally exhausting for the operator who must control the machine in remote areas for prolonged periods of time. Therefore, even partial automation of the process may reduce stress on the operator. This study focuses on continuing previous research efforts in application of reinforcement learning agents in automating log handling process, extending the task from grasping which was studied in previous research to full log loading operation. The resulting agent will be capable to automate a full loading procedure from locating and grappling to transporting and delivering the log to a forestry forwarder bed. To train the agent, a trailer type forestry forwarder simulation model in NVIDIA's Isaac Gym and a virtual environment for a typical log loading scenario were developed. With reinforcement learning agents and a curriculum learning approach, the trained agent may be a stepping stone towards application of reinforcement learning agents in automation of the forestry forwarder. The agent learnt grasping a log in a random position from grapple's random position and transport it to the bed with 94% success rate of the best performing agent.</article>","contentLength":1435,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cooperative Task Spaces for Multi-Arm Manipulation Control based on Similarity Transformations","url":"https://arxiv.org/abs/2510.26362","date":1761883200,"author":"","guid":323075,"unread":true,"content":"<article>arXiv:2510.26362v1 Announce Type: new \nAbstract: Many tasks in human environments require collaborative behavior between multiple kinematic chains, either to provide additional support for carrying big and bulky objects or to enable the dexterity that is required for in-hand manipulation. Since these complex systems often have a very high number of degrees of freedom coordinating their movements is notoriously difficult to model. In this article, we present the derivation of the theoretical foundations for cooperative task spaces of multi-arm robotic systems based on geometric primitives defined using conformal geometric algebra. Based on the similarity transformations of these cooperative geometric primitives, we derive an abstraction of complex robotic systems that enables representing these systems in a way that directly corresponds to single-arm systems. By deriving the associated analytic and geometric Jacobian matrices, we then show the straightforward integration of our approach into classical control techniques rooted in operational space control. We demonstrate this using bimanual manipulators, humanoids and multi-fingered hands in optimal control experiments for reaching desired geometric primitives and in teleoperation experiments using differential kinematics control. We then discuss how the geometric primitives naturally embed nullspace structures into the controllers that can be exploited for introducing secondary control objectives. This work, represents the theoretical foundations of this cooperative manipulation control framework, and thus the experiments are presented in an abstract way, while giving pointers towards potential future applications.</article>","contentLength":1693,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM","url":"https://arxiv.org/abs/2510.26358","date":1761883200,"author":"","guid":323076,"unread":true,"content":"<article>arXiv:2510.26358v1 Announce Type: new \nAbstract: Autonomous robots in orchards require real-time 3D scene understanding despite repetitive row geometry, seasonal appearance changes, and wind-driven foliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that couples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian Splatting (3DGS) rendering. Batch rasterization across complementary viewpoints recovers orchard structure under occlusions, while a unified gradient-driven map lifecycle executed between keyframes preserves fine details and bounds memory. Pose refinement is guided by a probabilistic LiDAR-based depth consistency term, back-propagated through the camera projection to tighten geometry-appearance coupling. We deploy the system on a field platform in apple and pear orchards across dormancy, flowering, and harvesting, using a standardized trajectory protocol that evaluates both training-view and novel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons and sites, AgriGS-SLAM delivers sharper, more stable reconstructions and steadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while maintaining real-time performance on-tractor. While demonstrated in orchard monitoring, the approach can be applied to other outdoor domains requiring robust multimodal perception.</article>","contentLength":1359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Role of Context for Discourse Relation Classification in Scientific Writing","url":"https://arxiv.org/abs/2510.26354","date":1761883200,"author":"","guid":323077,"unread":true,"content":"<article>arXiv:2510.26354v1 Announce Type: new \nAbstract: With the increasing use of generative Artificial Intelligence (AI) methods to support science workflows, we are interested in the use of discourse-level information to find supporting evidence for AI generated scientific claims. A first step towards this objective is to examine the task of inferring discourse structure in scientific writing.\n  In this work, we present a preliminary investigation of pretrained language model (PLM) and Large Language Model (LLM) approaches for Discourse Relation Classification (DRC), focusing on scientific publications, an under-studied genre for this task. We examine how context can help with the DRC task, with our experiments showing that context, as defined by discourse structure, is generally helpful. We also present an analysis of which scientific discourse relation types might benefit most from context.</article>","contentLength":901,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Explainable and Reliable AI in Finance","url":"https://arxiv.org/abs/2510.26353","date":1761883200,"author":"","guid":323078,"unread":true,"content":"<article>arXiv:2510.26353v1 Announce Type: new \nAbstract: Financial forecasting increasingly uses large neural network models, but their opacity raises challenges for trust and regulatory compliance. We present several approaches to explainable and reliable AI in finance. \\emph{First}, we describe how Time-LLM, a time series foundation model, uses a prompt to avoid a wrong directional forecast. \\emph{Second}, we show that combining foundation models for time series forecasting with a reliability estimator can filter our unreliable predictions. \\emph{Third}, we argue for symbolic reasoning encoding domain rules for transparent justification. These approaches shift emphasize executing only forecasts that are both reliable and explainable. Experiments on equity and cryptocurrency data show that the architecture reduces false positives and supports selective execution. By integrating predictive performance with reliability estimation and rule-based reasoning, our framework advances transparent and auditable financial AI systems.</article>","contentLength":1031,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration","url":"https://arxiv.org/abs/2510.26352","date":1761883200,"author":"","guid":323079,"unread":true,"content":"<article>arXiv:2510.26352v1 Announce Type: new \nAbstract: While a multi-agent approach based on large language models (LLMs) represents a promising strategy to surpass the capabilities of single models, its success is critically dependent on synergistic team composition. However, forming optimal teams is a significant challenge, as the inherent opacity of most models obscures the internal characteristics necessary for effective collaboration. In this paper, we propose an interaction-centric framework for automatic team composition that does not require any prior knowledge including their internal architectures, training data, or task performances. Our method constructs a \"language model graph\" that maps relationships between models from the semantic coherence of pairwise conversations, and then applies community detection to identify synergistic model clusters. Our experiments with diverse LLMs demonstrate that the proposed method discovers functionally coherent groups that reflect their latent specializations. Priming conversations with specific topics identified synergistic teams which outperform random baselines on downstream benchmarks and achieve comparable accuracy to that of manually-curated teams based on known model specializations. Our findings provide a new basis for the automated design of collaborative multi-agent LLM teams.</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation","url":"https://arxiv.org/abs/2510.26350","date":1761883200,"author":"","guid":323080,"unread":true,"content":"<article>arXiv:2510.26350v1 Announce Type: new \nAbstract: Federated learning (FL) has emerged as a key paradigm for collaborative model training across multiple clients without sharing raw data, enabling privacy-preserving applications in areas such as radiology and pathology. However, works on collaborative training across clients with fundamentally different neural architectures and non-identically distributed datasets remain scarce. Existing FL frameworks face several limitations. Despite claiming to support architectural heterogeneity, most recent FL methods only tolerate variants within a single model family (e.g., shallower, deeper, or wider CNNs), still presuming a shared global architecture and failing to accommodate federations where clients deploy fundamentally different network types (e.g., CNNs, GNNs, MLPs). Moreover, existing approaches often address only statistical heterogeneity while overlooking the domain-fracture problem, where each client's data distribution differs markedly from that faced at testing time, undermining model generalizability. When clients use different architectures, have non-identically distributed data, and encounter distinct test domains, current methods perform poorly. To address these challenges, we propose UnifiedFL, a dynamic federated learning framework that represents heterogeneous local networks as nodes and edges in a directed model graph optimized by a shared graph neural network (GNN). UnifiedFL introduces (i) a common GNN to parameterize all architectures, (ii) distance-driven clustering via Euclidean distances between clients' parameters, and (iii) a two-tier aggregation policy balancing convergence and diversity. Experiments on MedMNIST classification and hippocampus segmentation benchmarks demonstrate UnifiedFL's superior performance. Code and data: https://github.com/basiralab/UnifiedFL</article>","contentLength":1862,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle","url":"https://arxiv.org/abs/2510.26347","date":1761883200,"author":"","guid":323081,"unread":true,"content":"<article>arXiv:2510.26347v1 Announce Type: new \nAbstract: Reinforcement learning (RL) algorithms are designed to optimize problem-solving by learning actions that maximize rewards, a task that becomes particularly challenging in random and nonstationary environments. Even advanced RL algorithms are often limited in their ability to solve problems in these conditions. In applications such as searching for underwater pollution clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate reward-sparse environments, where actions frequently result in a zero reward. This paper aims to address these challenges by revisiting and modifying classical RL approaches to efficiently operate in sparse, randomized, and nonstationary environments. We systematically study a large number of modifications, including hierarchical algorithm changes, multigoal learning, and the integration of a location memory as an external output filter to prevent state revisits. Our results demonstrate that a modified Monte Carlo-based approach significantly outperforms traditional Q-learning and two exhaustive search patterns, illustrating its potential in adapting RL to complex environments. These findings suggest that reinforcement learning approaches can be effectively adapted for use in random, nonstationary, and reward-sparse environments.</article>","contentLength":1340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discovering State Equivalences in UCT Search Trees By Action Pruning","url":"https://arxiv.org/abs/2510.26346","date":1761883200,"author":"","guid":323082,"unread":true,"content":"<article>arXiv:2510.26346v1 Announce Type: new \nAbstract: One approach to enhance Monte Carlo Tree Search (MCTS) is to improve its sample efficiency by grouping/abstracting states or state-action pairs and sharing statistics within a group. Though state-action pair abstractions are mostly easy to find in algorithms such as On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT), nearly no state abstractions are found in either noisy or large action space settings due to constraining conditions. We provide theoretical and empirical evidence for this claim, and we slightly alleviate this state abstraction problem by proposing a weaker state abstraction condition that trades a minor loss in accuracy for finding many more abstractions. We name this technique Ideal Pruning Abstractions in UCT (IPA-UCT), which outperforms OGA-UCT (and any of its derivatives) across a large range of test domains and iteration budgets as experimentally validated. IPA-UCT uses a different abstraction framework from Abstraction of State-Action Pairs (ASAP) which is the one used by OGA-UCT, which we name IPA. Furthermore, we show that both IPA and ASAP are special cases of a more general framework that we call p-ASAP which itself is a special case of the ASASAP framework.</article>","contentLength":1276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MisSynth: Improving MISSCI Logical Fallacies Classification with Synthetic Data","url":"https://arxiv.org/abs/2510.26345","date":1761883200,"author":"","guid":323083,"unread":true,"content":"<article>arXiv:2510.26345v1 Announce Type: new \nAbstract: Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on https://github.com/mxpoliakov/MisSynth.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Embedding to Control: Representations for Stochastic Multi-Object Systems","url":"https://arxiv.org/abs/2510.26344","date":1761883200,"author":"","guid":323084,"unread":true,"content":"<article>arXiv:2510.26344v1 Announce Type: new \nAbstract: This paper studies how to achieve accurate modeling and effective control in stochastic nonlinear dynamics with multiple interacting objects. However, non-uniform interactions and random topologies make this task challenging. We address these challenges by proposing \\textit{Graph Controllable Embeddings} (GCE), a general framework to learn stochastic multi-object dynamics for linear control. Specifically, GCE is built on Hilbert space embeddings, allowing direct embedding of probability distributions of controlled stochastic dynamics into a reproducing kernel Hilbert space (RKHS), which enables linear operations in its RKHS while retaining nonlinear expressiveness. We provide theoretical guarantees on the existence, convergence, and applicability of GCE. Notably, a mean field approximation technique is adopted to efficiently capture inter-object dependencies and achieve provably low sample complexity. By integrating graph neural networks, we construct data-dependent kernel features that are capable of adapting to dynamic interaction patterns and generalizing to even unseen topologies with only limited training instances. GCE scales seamlessly to multi-object systems of varying sizes and topologies. Leveraging the linearity of Hilbert spaces, GCE also supports simple yet effective control algorithms for synthesizing optimal sequences. Experiments on physical systems, robotics, and power grids validate GCE and demonstrate consistent performance improvement over various competitive embedding methods in both in-distribution and few-shot tests</article>","contentLength":1613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linear Causal Discovery with Interventional Constraints","url":"https://arxiv.org/abs/2510.26342","date":1761883200,"author":"","guid":323085,"unread":true,"content":"<article>arXiv:2510.26342v1 Announce Type: new \nAbstract: Incorporating causal knowledge and mechanisms is essential for refining causal models and improving downstream tasks such as designing new treatments. In this paper, we introduce a novel concept in causal discovery, termed interventional constraints, which differs fundamentally from interventional data. While interventional data require direct perturbations of variables, interventional constraints encode high-level causal knowledge in the form of inequality constraints on causal effects. For instance, in the Sachs dataset (Sachs et al.\\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3 exerts a positive causal effect on Akt. Existing causal discovery methods allow enforcing structural constraints (for example, requiring a causal path from PIP3 to Akt), but they may still produce incorrect causal conclusions such as learning that \"PIP3 inhibits Akt\". Interventional constraints bridge this gap by explicitly constraining the total causal effect between variable pairs, ensuring learned models respect known causal influences. To formalize interventional constraints, we propose a metric to quantify total causal effects for linear causal models and formulate the problem as a constrained optimization task, solved using a two-stage constrained optimization method. We evaluate our approach on real-world datasets and demonstrate that integrating interventional constraints not only improves model accuracy and ensures consistency with established findings, making models more explainable, but also facilitates the discovery of new causal relationships that would otherwise be costly to identify.</article>","contentLength":1669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?","url":"https://arxiv.org/abs/2510.26339","date":1761883200,"author":"","guid":323086,"unread":true,"content":"<article>arXiv:2510.26339v1 Announce Type: new \nAbstract: Image super-resolution(SR) is fundamental to many vision system-from surveillance and autonomy to document analysis and retail analytics-because recovering high-frequency details, especially scene-text, enables reliable downstream perception. Scene-text, i.e., text embedded in natural images such as signs, product labels, and storefronts, often carries the most actionable information; when characters are blurred or hallucinated, optical character recognition(OCR) and subsequent decisions fail even if the rest of the image appears sharp. Yet previous SR research has often been tuned to distortion (PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that are largely insensitive to character-level errors. Furthermore, studies that do address text SR often focus on simplified benchmarks with isolated characters, overlooking the challenges of text within complex natural scenes. As a result, scene-text is effectively treated as generic texture. For SR to be effective in practical deployments, it is therefore essential to explicitly optimize for both text legibility and perceptual quality. We present GLYPH-SR, a vision-language-guided diffusion framework that aims to achieve both objectives jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by OCR data, and a ping-pong scheduler that alternates between text- and scene-centric guidance. To enable targeted text restoration, we train these components on a synthetic corpus while keeping the main SR branch frozen. Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR) while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed to satisfy both objectives simultaneously-high readability and high visual realism-delivering SR that looks right and reds right.</article>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Amateur to Master: Infusing Knowledge into LLMs via Automated Curriculum Learning","url":"https://arxiv.org/abs/2510.26336","date":1761883200,"author":"","guid":323087,"unread":true,"content":"<article>arXiv:2510.26336v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel at general tasks but underperform in specialized domains like economics and psychology, which require deep, principled understanding. To address this, we introduce ACER (Automated Curriculum-Enhanced Regimen) that transforms generalist models into domain experts without sacrificing their broad capabilities. ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy. This ensures systematic topic coverage and progressively increasing difficulty. The resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions.\n  Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized MMLU subsets. In challenging domains like microeconomics, where baselines struggle, ACER boosts accuracy by 5 percentage points. Across all target domains, we observe a consistent macro-average improvement of 3 percentage points. Notably, ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points, while maintaining stable performance on general reasoning tasks. Our results demonstrate that ACER offers a scalable and effective recipe for closing critical domain gaps in LLMs.</article>","contentLength":1614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simulation of the magnetic Ginzburg-Landau equation via vortex tracking","url":"https://arxiv.org/abs/2510.26334","date":1761883200,"author":"","guid":323088,"unread":true,"content":"<article>arXiv:2510.26334v1 Announce Type: new \nAbstract: This paper deals with the numerical simulation of the 2D magnetic time-dependent Ginzburg-Landau (TDGL) equations in the regime of small but finite (inverse) Ginzburg-Landau parameter $\\epsilon$ and constant (order $1$ in $\\epsilon$) applied magnetic field. In this regime, a well-known feature of the TDGL equation is the appearance of quantized vortices with core size of order $\\epsilon$. Moreover, in the singular limit $\\epsilon \\searrow 0$, these vortices evolve according to an explicit ODE system. In this work, we first introduce a new numerical method for the numerical integration of this limiting ODE system, which requires to solve a linear second order PDE at each time step. We also provide a rigorous theoretical justification for this method that applies to a general class of 2D domains. We then develop and analyze a numerical strategy based on the finite-dimensional ODE system to efficiently simulate the infinite-dimensional TDGL equations in the presence of a constant external magnetic field and for small, but finite, $\\epsilon$. This method allows us to avoid resolving the $\\epsilon$-scale when solving the TDGL equations, where small values of $\\epsilon$ typically require very fine meshes and time steps. We provide numerical examples on a few test cases and justify the accuracy of the method with numerical investigations.</article>","contentLength":1402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agent Skills Enable a New Class of Realistic and Trivially Simple Prompt Injections","url":"https://arxiv.org/abs/2510.26328","date":1761883200,"author":"","guid":323089,"unread":true,"content":"<article>arXiv:2510.26328v1 Announce Type: new \nAbstract: Enabling continual learning in LLMs remains a key unresolved research challenge. In a recent announcement, a frontier LLM company made a step towards this by introducing Agent Skills, a framework that equips agents with new knowledge based on instructions stored in simple markdown files. Although Agent Skills can be a very useful tool, we show that they are fundamentally insecure, since they enable trivially simple prompt injections. We demonstrate how to hide malicious instructions in long Agent Skill files and referenced scripts to exfiltrate sensitive data, such as internal files or passwords. Importantly, we show how to bypass system-level guardrails of a popular coding agent: a benign, task-specific approval with the \"Don't ask again\" option can carry over to closely related but harmful actions. Overall, we conclude that despite ongoing research efforts and scaling model capabilities, frontier LLMs remain vulnerable to very simple prompt injections in realistic scenarios. Our code is available at https://github.com/aisa-group/promptinject-agent-skills.</article>","contentLength":1122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics","url":"https://arxiv.org/abs/2510.26324","date":1761883200,"author":"","guid":323090,"unread":true,"content":"<article>arXiv:2510.26324v1 Announce Type: new \nAbstract: Given a noisy linear measurement $y = Ax + \\xi$ of a distribution $p(x)$, and a good approximation to the prior $p(x)$, when can we sample from the posterior $p(x \\mid y)$? Posterior sampling provides an accurate and fair framework for tasks such as inpainting, deblurring, and MRI reconstruction, and several heuristics attempt to approximate it. Unfortunately, approximate posterior sampling is computationally intractable in general.\n  To sidestep this hardness, we focus on (local or global) log-concave distributions $p(x)$. In this regime, Langevin dynamics yields posterior samples when the exact scores of $p(x)$ are available, but it is brittle to score--estimation error, requiring an MGF bound (sub-exponential error). By contrast, in the unconditional setting, diffusion models succeed with only an $L^2$ bound on the score error. We prove that combining diffusion models with an annealed variant of Langevin dynamics achieves conditional sampling in polynomial time using merely an $L^4$ bound on the score error.</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Impact of Weight Discretization in QUBO-Based SVM Training","url":"https://arxiv.org/abs/2510.26323","date":1761883200,"author":"","guid":323091,"unread":true,"content":"<article>arXiv:2510.26323v1 Announce Type: new \nAbstract: Training Support Vector Machines (SVMs) can be formulated as a QUBO problem, enabling the use of quantum annealing for model optimization. In this work, we study how the number of qubits - linked to the discretization level of dual weights - affects predictive performance across datasets. We compare QUBO-based SVM training to the classical LIBSVM solver and find that even low-precision QUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes superior, accuracy. While increased bit-depth enables larger regularization parameters, it does not always improve classification. Our findings suggest that selecting the right support vectors may matter more than their precise weighting. Although current hardware limits the size of solvable QUBOs, our results highlight the potential of quantum annealing for efficient SVM training as quantum devices scale.</article>","contentLength":923,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling","url":"https://arxiv.org/abs/2510.26322","date":1761883200,"author":"","guid":323092,"unread":true,"content":"<article>arXiv:2510.26322v1 Announce Type: new \nAbstract: Language models can be used to provide interactive, personalized student feedback in educational settings. However, real-world deployment faces three key challenges: privacy concerns, limited computational resources, and the need for pedagogically valid responses. These constraints require small, open-source models that can run locally and reliably ground their outputs in correct information. We introduce SCRIBE, a framework for multi-hop, tool-augmented reasoning designed to generate valid responses to student questions about feedback reports. SCRIBE combines domain-specific tools with a self-reflective inference pipeline that supports iterative reasoning, tool use, and error recovery. We distil these capabilities into 3B and 8B models via two-stage LoRA fine-tuning on synthetic GPT-4o-generated data. Evaluation with a human-aligned GPT-Judge and a user study with 108 students shows that 8B-SCRIBE models achieve comparable or superior quality to much larger models in key dimensions such as relevance and actionability, while being perceived on par with GPT-4o and Llama-3.3 70B by students. These findings demonstrate the viability of SCRIBE for low-resource, privacy-sensitive educational applications.</article>","contentLength":1268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading","url":"https://arxiv.org/abs/2510.26315","date":1761883200,"author":"","guid":323093,"unread":true,"content":"<article>arXiv:2510.26315v1 Announce Type: new \nAbstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged and elderly people, which significantly impacts their daily lives and mental health. To improve the efficiency of clinical screening and enable the early detection of DR, a variety of automated DR diagnosis systems have been recently established based on convolutional neural network (CNN) or vision Transformer (ViT). However, due to the own shortages of CNN / ViT, the performance of existing methods using single-type backbone has reached a bottleneck. One potential way for the further improvements is integrating different kinds of backbones, which can fully leverage the respective strengths of them (\\emph{i.e.,} the local feature extraction capability of CNN and the global feature capturing ability of ViT). To this end, we propose a novel paradigm to effectively fuse the features extracted by different backbones based on the theory of evidence. Specifically, the proposed evidential fusion paradigm transforms the features from different backbones into supporting evidences via a set of deep evidential networks. With the supporting evidences, the aggregated opinion can be accordingly formed, which can be used to adaptively tune the fusion pattern between different backbones and accordingly boost the performance of our hybrid model. We evaluated our method on two publicly available DR grading datasets. The experimental results demonstrate that our hybrid model not only improves the accuracy of DR grading, compared to the state-of-the-art frameworks, but also provides the excellent interpretability for feature fusion and decision-making.</article>","contentLength":1685,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning","url":"https://arxiv.org/abs/2510.26311","date":1761883200,"author":"","guid":323094,"unread":true,"content":"<article>arXiv:2510.26311v1 Announce Type: new \nAbstract: Continual learning (CL) aims to incrementally train a model on a sequence of tasks while retaining performance on prior ones. However, storing and replaying data is often infeasible due to privacy or security constraints and impractical for arbitrary pre-trained models. Data-free CL seeks to update models without access to previous data. Beyond regularization, we employ model inversion to synthesize data from the trained model, enabling replay without storing samples. Yet, model inversion in predictive models faces two challenges: (1) generating inputs solely from compressed output labels causes drift between synthetic and real data, and replaying such data can erode prior knowledge; (2) inversion is computationally expensive since each step backpropagates through the full model. These issues are amplified in large pre-trained models such as CLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI), inspired by faster convergence in single-layer optimization. PMI provides strong initialization for full-model inversion, substantially reducing iterations. To mitigate feature shift, we model class-wise features via Gaussian distributions and contrastive model, ensuring alignment between synthetic and real features. Combining PMI and feature modeling, our approach enables continual learning of new classes by generating pseudo-images from semantic-aware projected features, achieving strong effectiveness and compatibility across multiple CL settings.</article>","contentLength":1531,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GraphCompliance: Aligning Policy and Context Graphs for LLM-Based Regulatory Compliance","url":"https://arxiv.org/abs/2510.26309","date":1761883200,"author":"","guid":323095,"unread":true,"content":"<article>arXiv:2510.26309v1 Announce Type: new \nAbstract: Compliance at web scale poses practical challenges: each request may require a regulatory assessment. Regulatory texts (e.g., the General Data Protection Regulation, GDPR) are cross-referential and normative, while runtime contexts are expressed in unstructured natural language. This setting motivates us to align semantic information in unstructured text with the structured, normative elements of regulations. To this end, we introduce GraphCompliance, a framework that represents regulatory texts as a Policy Graph and runtime contexts as a Context Graph, and aligns them. In this formulation, the policy graph encodes normative structure and cross-references, whereas the context graph formalizes events as subject-action-object (SAO) and entity-relation triples. This alignment anchors the reasoning of a judge large language model (LLM) in structured information and helps reduce the burden of regulatory interpretation and event parsing, enabling a focus on the core reasoning step. In experiments on 300 GDPR-derived real-world scenarios spanning five evaluation tasks, GraphCompliance yields 4.1-7.2 percentage points (pp) higher micro-F1 than LLM-only and RAG baselines, with fewer under- and over-predictions, resulting in higher recall and lower false positive rates. Ablation studies indicate contributions from each graph component, suggesting that structured representations and a judge LLM are complementary for normative reasoning.</article>","contentLength":1498,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection","url":"https://arxiv.org/abs/2510.26307","date":1761883200,"author":"","guid":323096,"unread":true,"content":"<article>arXiv:2510.26307v1 Announce Type: new \nAbstract: Anomaly detection is a critical task in cybersecurity, where identifying insider threats, access violations, and coordinated attacks is essential for ensuring system resilience. Graph-based approaches have become increasingly important for modeling entity interactions, yet most rely on homogeneous and static structures, which limits their ability to capture the heterogeneity and temporal evolution of real-world environments. Heterogeneous Graph Neural Networks (HGNNs) have emerged as a promising paradigm for anomaly detection by incorporating type-aware transformations and relation-sensitive aggregation, enabling more expressive modeling of complex cyber data. However, current research on HGNN-based anomaly detection remains fragmented, with diverse modeling strategies, limited comparative evaluation, and an absence of standardized benchmarks. To address this gap, we provide a comprehensive survey of HGNN-based anomaly detection methods in cybersecurity. We introduce a taxonomy that classifies approaches by anomaly type and graph dynamics, analyze representative models, and map them to key cybersecurity applications. We also review commonly used benchmark datasets and evaluation metrics, highlighting their strengths and limitations. Finally, we identify key open challenges related to modeling, data, and deployment, and outline promising directions for future research. This survey aims to establish a structured foundation for advancing HGNN-based anomaly detection toward scalable, interpretable, and practically deployable solutions.</article>","contentLength":1606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG","url":"https://arxiv.org/abs/2510.26304","date":1761883200,"author":"","guid":323097,"unread":true,"content":"<article>arXiv:2510.26304v1 Announce Type: new \nAbstract: The subject of this work is to check how different types of music affect human emotions. While listening to music, a subjective survey and brain activity measurements were carried out using an EEG helmet. The aim is to demonstrate the impact of different music genres on emotions. The research involved a diverse group of participants of different gender and musical preferences. This had the effect of capturing a wide range of emotional responses to music. After the experiment, a relationship analysis of the respondents' questionnaires with EEG signals was performed. The analysis revealed connections between emotions and observed brain activity.</article>","contentLength":700,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implicit Bias of Per-sample Adam on Separable Data: Departure from the Full-batch Regime","url":"https://arxiv.org/abs/2510.26303","date":1761883200,"author":"","guid":323098,"unread":true,"content":"<article>arXiv:2510.26303v1 Announce Type: new \nAbstract: Adam [Kingma and Ba, 2015] is the de facto optimizer in deep learning, yet its theoretical understanding remains limited. Prior analyses show that Adam favors solutions aligned with $\\ell_\\infty$-geometry, but these results are restricted to the full-batch regime. In this work, we study the implicit bias of incremental Adam (using one sample per step) for logistic regression on linearly separable data, and we show that its bias can deviate from the full-batch behavior. To illustrate this, we construct a class of structured datasets where incremental Adam provably converges to the $\\ell_2$-max-margin classifier, in contrast to the $\\ell_\\infty$-max-margin bias of full-batch Adam. For general datasets, we develop a proxy algorithm that captures the limiting behavior of incremental Adam as $\\beta_2 \\to 1$ and we characterize its convergence direction via a data-dependent dual fixed-point formulation. Finally, we prove that, unlike Adam, Signum [Bernstein et al., 2018] converges to the $\\ell_\\infty$-max-margin classifier for any batch size by taking $\\beta$ close enough to 1. Overall, our results highlight that the implicit bias of Adam crucially depends on both the batching scheme and the dataset, while Signum remains invariant.</article>","contentLength":1294,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens","url":"https://arxiv.org/abs/2510.26302","date":1761883200,"author":"","guid":323099,"unread":true,"content":"<article>arXiv:2510.26302v1 Announce Type: new \nAbstract: Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal generalization by aligning images and texts in a shared embedding space, yet it persistently fails at compositional reasoning over objects, attributes, and relations often behaving like a bag-of-words matcher. Prior causal accounts typically model text as a single vector, obscuring token-level structure and leaving core phenomena-such as prompt sensitivity and failures on hard negatives unexplained. We address this gap with a token-aware causal representation learning (CRL) framework grounded in a sequential, language-token SCM. Our theory extends block identifiability to tokenized text, proving that CLIP's contrastive objective can recover the modal-invariant latent variable under both sentence-level and token-level SCMs. Crucially, token granularity yields the first principled explanation of CLIP's compositional brittleness: composition nonidentifiability. We show the existence of pseudo-optimal text encoders that achieve perfect modal-invariant alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations over atomic concepts, thereby failing to distinguish correct captions from hard negatives despite optimizing the same training objective as true-optimal encoders. The analysis further links language-side nonidentifiability to visual-side failures via the modality gap and shows how iterated composition operators compound hardness, motivating improved negative mining strategies.</article>","contentLength":1541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Offline Clustering of Preference Learning with Active-data Augmentation","url":"https://arxiv.org/abs/2510.26301","date":1761883200,"author":"","guid":323100,"unread":true,"content":"<article>arXiv:2510.26301v1 Announce Type: new \nAbstract: Preference learning from pairwise feedback is a widely adopted framework in applications such as reinforcement learning with human feedback and recommendations. In many practical settings, however, user interactions are limited or costly, making offline preference learning necessary. Moreover, real-world preference learning often involves users with different preferences. For example, annotators from different backgrounds may rank the same responses differently. This setting presents two central challenges: (1) identifying similarity across users to effectively aggregate data, especially under scenarios where offline data is imbalanced across dimensions, and (2) handling the imbalanced offline data where some preference dimensions are underrepresented. To address these challenges, we study the Offline Clustering of Preference Learning problem, where the learner has access to fixed datasets from multiple users with potentially different preferences and aims to maximize utility for a test user. To tackle the first challenge, we first propose Off-C$^2$PL for the pure offline setting, where the learner relies solely on offline data. Our theoretical analysis provides a suboptimality bound that explicitly captures the tradeoff between sample noise and bias. To address the second challenge of inbalanced data, we extend our framework to the setting with active-data augmentation where the learner is allowed to select a limited number of additional active-data for the test user based on the cluster structure learned by Off-C$^2$PL. In this setting, our second algorithm, A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative dimensions of the test user's preference. We prove that these actively collected samples contribute more effectively than offline ones. Finally, we validate our theoretical results through simulations on synthetic and real-world datasets.</article>","contentLength":1952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling strategies for speech enhancement in the latent space of a neural audio codec","url":"https://arxiv.org/abs/2510.26299","date":1761883200,"author":"","guid":323101,"unread":true,"content":"<article>arXiv:2510.26299v1 Announce Type: new \nAbstract: Neural audio codecs (NACs) provide compact latent speech representations in the form of sequences of continuous vectors or discrete tokens. In this work, we investigate how these two types of speech representations compare when used as training targets for supervised speech enhancement. We consider both autoregressive and non-autoregressive speech enhancement models based on the Conformer architecture, as well as a simple baseline where the NAC encoder is simply fine-tuned for speech enhancement. Our experiments reveal three key findings: predicting continuous latent representations consistently outperforms discrete token prediction; autoregressive models achieve higher quality but at the expense of intelligibility and efficiency, making non-autoregressive models more attractive in practice; and encoder fine-tuning yields the strongest enhancement metrics overall, though at the cost of degraded codec reconstruction. The code and audio samples are available online.</article>","contentLength":1027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Agent Conquer Web? Exploring the Frontiers of ChatGPT Atlas Agent in Web Games","url":"https://arxiv.org/abs/2510.26298","date":1761883200,"author":"","guid":323102,"unread":true,"content":"<article>arXiv:2510.26298v1 Announce Type: new \nAbstract: OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and Stein.world. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at https://atlas-game-eval.github.io.</article>","contentLength":1261,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology","url":"https://arxiv.org/abs/2510.26297","date":1761883200,"author":"","guid":323103,"unread":true,"content":"<article>arXiv:2510.26297v1 Announce Type: new \nAbstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented flexibility for monitoring the Earth's surface, but their scheduling remains challenging under large-scale scenarios, dynamic environments, and stringent constraints. Existing methods often simplify these complexities, limiting their real-world performance. We address this gap with a unified framework integrating a standardized benchmark suite and a novel scheduling model. Our benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and $16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to $300$ imaging tasks. These scenarios are generated via a high-fidelity simulation platform, ensuring realistic satellite behavior such as orbital dynamics and resource constraints. Ground truth scheduling annotations are provided for each scenario. To our knowledge, AEOS-Bench is the first large-scale benchmark suite tailored for realistic constellation scheduling. Building upon this benchmark, we introduce AEOS-Former, a Transformer-based scheduling model that incorporates a constraint-aware attention mechanism. A dedicated internal constraint module explicitly models the physical and operational limits of each satellite. Through simulation-based iterative learning, AEOS-Former adapts to diverse scenarios, offering a robust solution for AEOS constellation scheduling. Experimental results demonstrate that AEOS-Former outperforms baseline models in task completion and energy efficiency, with ablation studies highlighting the contribution of each component. Code and data are provided in https://github.com/buaa-colalab/AEOSBench.</article>","contentLength":1707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping","url":"https://arxiv.org/abs/2510.26294","date":1761883200,"author":"","guid":323104,"unread":true,"content":"<article>arXiv:2510.26294v1 Announce Type: new \nAbstract: We focus on ocular biometrics, specifically the periocular region (the area around the eye), which offers high discrimination and minimal acquisition constraints. We evaluate three Convolutional Neural Network architectures of varying depth and complexity to assess their effectiveness for periocular recognition. The networks are trained on 1,907,572 ocular crops extracted from the large-scale VGGFace2 database. This significantly contrasts with existing works, which typically rely on small-scale periocular datasets for training having only a few thousand images. Experiments are conducted with ocular images from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images, and the UFPR-Periocular database, which consists of selfies captured via mobile devices with user guidance on the screen. Due to the uncontrolled conditions of VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from 9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In contrast, UFPR-Periocular yields significantly better performance (EERs of 1-2%), thanks to higher image quality and more consistent acquisition protocols. To the best of our knowledge, these are the lowest reported EERs on the UFPR dataset to date.</article>","contentLength":1306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving","url":"https://arxiv.org/abs/2510.26292","date":1761883200,"author":"","guid":323105,"unread":true,"content":"<article>arXiv:2510.26292v1 Announce Type: new \nAbstract: Planning is a critical component of end-to-end autonomous driving. However, prevailing imitation learning methods often suffer from mode collapse, failing to produce diverse trajectory hypotheses. Meanwhile, existing generative approaches struggle to incorporate crucial safety and physical constraints directly into the generative process, necessitating an additional optimization stage to refine their outputs. To address these limitations, we propose CATG, a novel planning framework that leverages Constrained Flow Matching. Concretely, CATG explicitly models the flow matching process, which inherently mitigates mode collapse and allows for flexible guidance from various conditioning signals. Our primary contribution is the novel imposition of explicit constraints directly within the flow matching process, ensuring that the generated trajectories adhere to vital safety and kinematic rules. Secondly, CATG parameterizes driving aggressiveness as a control signal during generation, enabling precise manipulation of trajectory style. Notably, on the NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and was honored with the Innovation Award.</article>","contentLength":1224,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contribution-Guided Asymmetric Learning for Robust Multimodal Fusion under Imbalance and Noise","url":"https://arxiv.org/abs/2510.26289","date":1761883200,"author":"","guid":323106,"unread":true,"content":"<article>arXiv:2510.26289v1 Announce Type: new \nAbstract: Multimodal learning faces two major challenges: modality imbalance and data noise, which significantly affect the robustness and generalization ability of models. Existing methods achieve modality balance by suppressing dominant modalities, but they neglect the inherent differences in the information value between modalities, potentially leading to convergence to suboptimal solutions. This paper proposes an innovative modality compression paradigm, Contribution-Guided Asymmetric Learning (CAL), which aims to enhance the contribution of high-contribution modalities while compressing weak modalities to increase their contribution, allowing both to improve the performance of multimodal information fusion. CAL is based on a modality contribution metric W^m combining the information quantity I(m) and confidence D(m), and it designs an asymmetric gradient acceleration mechanism and a contribution-aware Asymmetric Information Bottleneck (AIB) compression mechanism. The former accelerates the gradient update of modalities, while the latter dynamically compresses the noise of low-contribution modalities.\n  On five benchmark datasets, including emotion recognition, scene recognition, and event localization tasks, CAL has shown outstanding performance in imbalanced fusion tasks and noise robustness tests. On CREMA-D, KS, and AVE, CAL achieves 79.30%, 74.82%, and 74.21% accuracy, significantly outperforming the existing state-of-the-art model ARL. In high-noise robustness tests, CAL also achieved leading performance under various attack strategies on the MVSA-Single and NYUD2 datasets. These results validate the significant advantages of CAL in modality imbalance and noise interference. CAL, as a flexible and efficient framework, is easy to transfer to other tasks and has broad adaptability and potential application prospects.</article>","contentLength":1895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empowering RepoQA-Agent based on Reinforcement Learning Driven by Monte-carlo Tree Search","url":"https://arxiv.org/abs/2510.26287","date":1761883200,"author":"","guid":323107,"unread":true,"content":"<article>arXiv:2510.26287v1 Announce Type: new \nAbstract: Repository-level software engineering tasks require large language models (LLMs) to efficiently navigate and extract information from complex codebases through multi-turn tool interactions. Existing approaches face significant limitations: training-free, in-context learning methods struggle to guide agents effectively in tool utilization and decision-making based on environmental feedback, while training-based approaches typically rely on costly distillation from larger LLMs, introducing data compliance concerns in enterprise environments. To address these challenges, we introduce RepoSearch-R1, a novel agentic reinforcement learning framework driven by Monte-carlo Tree Search (MCTS). This approach allows agents to generate diverse, high-quality reasoning trajectories via self-training without requiring model distillation or external supervision. Based on RepoSearch-R1, we construct a RepoQA-Agent specifically designed for repository question-answering tasks. Comprehensive evaluation on repository question-answering tasks demonstrates that RepoSearch-R1 achieves substantial improvements of answer completeness: 16.0% enhancement over no-retrieval methods, 19.5% improvement over iterative retrieval methods, and 33% increase in training efficiency compared to general agentic reinforcement learning approaches. Our cold-start training methodology eliminates data compliance concerns while maintaining robust exploration diversity and answer completeness across repository-level reasoning tasks.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unravelling the Mechanisms of Manipulating Numbers in Language Models","url":"https://arxiv.org/abs/2510.26285","date":1761883200,"author":"","guid":323108,"unread":true,"content":"<article>arXiv:2510.26285v1 Announce Type: new \nAbstract: Recent work has shown that different large language models (LLMs) converge to similar and accurate input embedding representations for numbers. These findings conflict with the documented propensity of LLMs to produce erroneous outputs when dealing with numeric information. In this work, we aim to explain this conflict by exploring how language models manipulate numbers and quantify the lower bounds of accuracy of these mechanisms. We find that despite surfacing errors, different language models learn interchangeable representations of numbers that are systematic, highly accurate and universal across their hidden states and the types of input contexts. This allows us to create universal probes for each LLM and to trace information -- including the causes of output errors -- to specific layers. Our results lay a fundamental understanding of how pre-trained LLMs manipulate numbers and outline the potential of more accurate probing techniques in addressed refinements of LLMs' architectures.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empirical Bayesian Multi-Bandit Learning","url":"https://arxiv.org/abs/2510.26284","date":1761883200,"author":"","guid":323109,"unread":true,"content":"<article>arXiv:2510.26284v1 Announce Type: new \nAbstract: Multi-task learning in contextual bandits has attracted significant research interest due to its potential to enhance decision-making across multiple related tasks by leveraging shared structures and task-specific heterogeneity. In this article, we propose a novel hierarchical Bayesian framework for learning in various bandit instances. This framework captures both the heterogeneity and the correlations among different bandit instances through a hierarchical Bayesian model, enabling effective information sharing while accommodating instance-specific variations. Unlike previous methods that overlook the learning of the covariance structure across bandits, we introduce an empirical Bayesian approach to estimate the covariance matrix of the prior distribution.This enhances both the practicality and flexibility of learning across multi-bandits. Building on this approach, we develop two efficient algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which incorporate the estimated prior into the decision-making process. We provide the frequentist regret upper bounds for the proposed algorithms, thereby filling a research gap in the field of multi-bandit problems. Extensive experiments on both synthetic and real-world datasets demonstrate the superior performance of our algorithms, particularly in complex environments. Our methods achieve lower cumulative regret compared to existing techniques, highlighting their effectiveness in balancing exploration and exploitation across multi-bandits.</article>","contentLength":1650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances","url":"https://arxiv.org/abs/2510.26282","date":1761883200,"author":"","guid":323110,"unread":true,"content":"<article>arXiv:2510.26282v1 Announce Type: new \nAbstract: We study the complementarity of different CNNs for periocular verification at different distances on the UBIPr database. We train three architectures of increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics, compare different network initialisations, and apply score-level fusion via logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon divergence to compare attention patterns of the CNNs. While ResNet50 consistently performs best individually, the fusion provides substantial gains, especially when combining all three networks. Heatmaps show that networks usually focus on distinct regions of a given image, which explains their complementarity. Our method significantly outperforms previous works on UBIPr, achieving a new state-of-the-art.</article>","contentLength":918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thor: Towards Human-Level Whole-Body Reactions for Intense Contact-Rich Environments","url":"https://arxiv.org/abs/2510.26280","date":1761883200,"author":"","guid":323111,"unread":true,"content":"<article>arXiv:2510.26280v1 Announce Type: new \nAbstract: Humanoids hold great potential for service, industrial, and rescue applications, in which robots must sustain whole-body stability while performing intense, contact-rich interactions with the environment. However, enabling humanoids to generate human-like, adaptive responses under such conditions remains a major challenge. To address this, we propose Thor, a humanoid framework for human-level whole-body reactions in contact-rich environments. Based on the robot's force analysis, we design a force-adaptive torso-tilt (FAT2) reward function to encourage humanoids to exhibit human-like responses during force-interaction tasks. To mitigate the high-dimensional challenges of humanoid control, Thor introduces a reinforcement learning architecture that decouples the upper body, waist, and lower body. Each component shares global observations of the whole body and jointly updates its parameters. Finally, we deploy Thor on the Unitree G1, and it substantially outperforms baselines in force-interaction tasks. Specifically, the robot achieves a peak pulling force of 167.7 N (approximately 48% of the G1's body weight) when moving backward and 145.5 N when moving forward, representing improvements of 68.9% and 74.7%, respectively, compared with the best-performing baseline. Moreover, Thor is capable of pulling a loaded rack (130 N) and opening a fire door with one hand (60 N). These results highlight Thor's effectiveness in enhancing humanoid force-interaction capabilities.</article>","contentLength":1534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Spectral Efficiency Maximization Design for IRS-aided MIMO Systems","url":"https://arxiv.org/abs/2510.26279","date":1761883200,"author":"","guid":323112,"unread":true,"content":"<article>arXiv:2510.26279v1 Announce Type: new \nAbstract: Driven by the growing demand for higher spectral efficiency in wireless communications, intelligent reflecting sur- faces (IRS) have attracted considerable attention for their ability to dynamically reconfigure the propagation environment. This work addresses the spectral efficiency maximization problem in IRS-assisted multiple-input multiple-output (MIMO) systems, which involves the joint optimization of the transmit precoding matrix and the IRS phase shift configuration. This problem is inherently challenging due to its non-convex nature. To tackle it effectively, we introduce a computationally efficient algorithm, termed ADMM-APG, which integrates the alternating direction method of multipliers (ADMM) with the accelerated projected gradient (APG) method. The proposed framework decomposes the original problem into tractable subproblems, each admitting a closed-form solution while maintaining low computational com- plexity. Simulation results demonstrate that the ADMM-APG algorithm consistently surpasses existing benchmark methods in terms of spectral efficiency and computational complexity, achieving significant performance gains across a range of system configurations.</article>","contentLength":1239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation","url":"https://arxiv.org/abs/2510.26278","date":1761883200,"author":"","guid":323113,"unread":true,"content":"<article>arXiv:2510.26278v1 Announce Type: new \nAbstract: Diffusion models have been successful in learning complex data distributions. This capability has driven their application to high-dimensional multi-objective black-box optimization problem. Existing approaches often employ an external optimization loop, such as an evolutionary algorithm, to the diffusion model. However, these approaches treat the diffusion model as a black-box refiner, which overlooks the internal distribution transition of the diffusion generation process, limiting their efficiency. To address these challenges, we propose the Inference-time Multi-target Generation (IMG) algorithm, which optimizes the diffusion process at inference-time to generate samples that simultaneously satisfy multiple objectives. Specifically, our IMG performs weighted resampling during the diffusion generation process according to the expected aggregated multi-objective values. This weighted resampling strategy ensures the diffusion-generated samples are distributed according to our desired multi-target Boltzmann distribution. We further derive that the multi-target Boltzmann distribution has an interesting log-likelihood interpretation, where it is the optimal solution to the distributional multi-objective optimization problem. We implemented IMG for a multi-objective molecule generation task. Experiments show that IMG, requiring only a single generation pass, achieves a significantly higher hypervolume than baseline optimization algorithms that often require hundreds of diffusion generations. Notably, our algorithm can be viewed as an optimized diffusion process and can be integrated into existing methods to further improve their performance.</article>","contentLength":1714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do LLMs Signal When They're Right? Evidence from Neuron Agreement","url":"https://arxiv.org/abs/2510.26277","date":1761883200,"author":"","guid":323114,"unread":true,"content":"<article>arXiv:2510.26277v1 Announce Type: new \nAbstract: Large language models (LLMs) commonly boost reasoning via sample-evaluate-ensemble decoders, achieving label free gains without ground truth. However, prevailing strategies score candidates using only external outputs such as token probabilities, entropies, or self evaluations, and these signals can be poorly calibrated after post training. We instead analyze internal behavior based on neuron activations and uncover three findings: (1) external signals are low dimensional projections of richer internal dynamics; (2) correct responses activate substantially fewer unique neurons than incorrect ones throughout generation; and (3) activations from correct responses exhibit stronger cross sample agreement, whereas incorrect ones diverge. Motivated by these observations, we propose Neuron Agreement Decoding (NAD), an unsupervised best-of-N method that selects candidates using activation sparsity and cross sample neuron agreement, operating solely on internal signals and without requiring comparable textual outputs. NAD enables early correctness prediction within the first 32 generated tokens and supports aggressive early stopping. Across math and science benchmarks with verifiable answers, NAD matches majority voting; on open ended coding benchmarks where majority voting is inapplicable, NAD consistently outperforms Avg@64. By pruning unpromising trajectories early, NAD reduces token usage by 99% with minimal loss in generation quality, showing that internal signals provide reliable, scalable, and efficient guidance for label free ensemble decoding.</article>","contentLength":1618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Research Roadmap for Augmenting Software Engineering Processes and Software Products with Generative AI","url":"https://arxiv.org/abs/2510.26275","date":1761883200,"author":"","guid":323115,"unread":true,"content":"<article>arXiv:2510.26275v1 Announce Type: new \nAbstract: Generative AI (GenAI) is rapidly transforming software engineering (SE) practices, influencing how SE processes are executed, as well as how software systems are developed, operated, and evolved. This paper applies design science research to build a roadmap for GenAI-augmented SE. The process consists of three cycles that incrementally integrate multiple sources of evidence, including collaborative discussions from the FSE 2025 \"Software Engineering 2030\" workshop, rapid literature reviews, and external feedback sessions involving peers. McLuhan's tetrads were used as a conceptual instrument to systematically capture the transforming effects of GenAI on SE processes and software products.The resulting roadmap identifies four fundamental forms of GenAI augmentation in SE and systematically characterizes their related research challenges and opportunities. These insights are then consolidated into a set of future research directions. By grounding the roadmap in a rigorous multi-cycle process and cross-validating it among independent author teams and peers, the study provides a transparent and reproducible foundation for analyzing how GenAI affects SE processes, methods and tools, and for framing future research within this rapidly evolving area. Based on these findings, the article finally makes ten predictions for SE in the year 2030.</article>","contentLength":1404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PVMark: Enabling Public Verifiability for LLM Watermarking Schemes","url":"https://arxiv.org/abs/2510.26274","date":1761883200,"author":"","guid":323116,"unread":true,"content":"<article>arXiv:2510.26274v1 Announce Type: new \nAbstract: Watermarking schemes for large language models (LLMs) have been proposed to identify the source of the generated text, mitigating the potential threats emerged from model theft. However, current watermarking solutions hardly resolve the trust issue: the non-public watermark detection cannot prove itself faithfully conducting the detection. We observe that it is attributed to the secret key mostly used in the watermark detection -- it cannot be public, or the adversary may launch removal attacks provided the key; nor can it be private, or the watermarking detection is opaque to the public. To resolve the dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP), enabling the watermark detection process to be publicly verifiable by third parties without disclosing any secret key. PVMark hinges upon the proof of `correct execution' of watermark detection on which a set of ZKP constraints are built, including mapping, random number generation, comparison, and summation. We implement multiple variants of PVMark in Python, Rust and Circom, covering combinations of three watermarking schemes, three hash functions, and four ZKP protocols, to show our approach effectively works under a variety of circumstances. By experimental results, PVMark efficiently enables public verifiability on the state-of-the-art LLM watermarking schemes yet without compromising the watermarking performance, promising to be deployed in practice.</article>","contentLength":1500,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual","url":"https://arxiv.org/abs/2510.26271","date":1761883200,"author":"","guid":323117,"unread":true,"content":"<article>arXiv:2510.26271v1 Announce Type: new \nAbstract: Vision-language models (VLMs) exhibit uneven performance across languages, a problem that is often exacerbated when the model size is reduced. While Knowledge distillation (KD) demonstrates promising results in transferring knowledge from larger to smaller VLMs, applying KD in multilingualism is an underexplored area. This paper presents a controlled empirical study of KD behavior across five distillation approaches, isolating their effects on cross-lingual representation consistency and downstream performance stability under model compression. We study five distillation formulations across CLIP and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual QA. We find that some configurations preserve or even improve multilingual retrieval robustness despite halving model size, but others fail to maintain cross-task stability, exposing design-sensitive trade-offs that aggregate accuracy alone does not reveal.</article>","contentLength":988,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph-Enhanced Policy Optimization in LLM Agent Training","url":"https://arxiv.org/abs/2510.26270","date":1761883200,"author":"","guid":323118,"unread":true,"content":"<article>arXiv:2510.26270v1 Announce Type: new \nAbstract: Group based reinforcement learning (RL) has shown impressive results on complex reasoning and mathematical tasks. Yet, when applied to train multi-turn, interactive LLM agents, these methods often suffer from structural blindness-the inability to exploit the underlying connectivity of the environment. This manifests in three critical challenges: (1) inefficient, unguided exploration, (2) imprecise credit assignment due to overlooking pivotal states, and (3) myopic planning caused by static reward discounting. We address these issues with Graph-Enhanced Policy Optimization (GEPO), which dynamically constructs a state-transition graph from agent experience and employs graph-theoretic centrality to provide three synergistic learning signals: (1)structured intrinsic rewards that guide exploration toward high-impact states, (2) a graph-enhanced advantage function for topology-aware credit assignment, and (3) a dynamic discount factor adapted to each state's strategic value. On the ALFWorld, WebShop, and a proprietary Workbench benchmarks, GEPO demonstrates strong performance, achieving absolute success rate gains of +4.1%, +5.3%, and +10.9% over competitive baselines. These results highlight that explicitly modeling environmental structure is a robust, generalizable strategy for advancing LLM agent training.</article>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws","url":"https://arxiv.org/abs/2510.26268","date":1761883200,"author":"","guid":323119,"unread":true,"content":"<article>arXiv:2510.26268v1 Announce Type: new \nAbstract: Existing infrared and visible image fusion methods often face the dilemma of balancing modal information. Generative fusion methods reconstruct fused images by learning from data distributions, but their generative capabilities remain limited. Moreover, the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios. This manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method, termed HCLFuse. First, HCLFuse investigates the quantification theory of information mapping in unsupervised fusion networks, which leads to the design of a multi-scale mask-regulated variational bottleneck encoder. This encoder applies posterior probability modeling and information decomposition to extract accurate and concise low-level modal information, thereby supporting the generation of high-fidelity structural details. Furthermore, the probabilistic generative capability of the diffusion model is integrated with physical laws, forming a time-varying physical guidance mechanism that adaptively regulates the generation process at different stages, thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality. Experimental results show that the proposed method achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics. This fully demonstrates the advantages of this generative image fusion method, drawing inspiration from human cognition, in enhancing structural consistency and detail quality.</article>","contentLength":1812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Likely Interpolants of Generative Models","url":"https://arxiv.org/abs/2510.26266","date":1761883200,"author":"","guid":323120,"unread":true,"content":"<article>arXiv:2510.26266v1 Announce Type: new \nAbstract: Interpolation in generative models allows for controlled generation, model inspection, and more. Unfortunately, most generative models lack a principal notion of interpolants without restrictive assumptions on either the model or data dimension. In this paper, we develop a general interpolation scheme that targets likely transition paths compatible with different metrics and probability distributions. We consider interpolants analogous to a geodesic constrained to a suitable data distribution and derive a novel algorithm for computing these curves, which requires no additional training. Theoretically, we show that our method locally can be considered as a geodesic under a suitable Riemannian metric. We quantitatively show that our interpolation scheme traverses higher density regions than baselines across a range of models and datasets.</article>","contentLength":897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Look at That Distractor: Dynamic Translation Gain under Low Perceptual Load in Virtual Reality","url":"https://arxiv.org/abs/2510.26265","date":1761883200,"author":"","guid":323121,"unread":true,"content":"<article>arXiv:2510.26265v1 Announce Type: new \nAbstract: Redirected walking utilizes gain adjustments within perceptual thresholds to allow natural navigation in large scale virtual environments within confined physical environments. Previous research has found that when users are distracted by some scene elements, they are less sensitive to gain values. However, the effects on detection thresholds have not been quantitatively measured. In this paper, we present a novel method that dynamically adjusts translation gain by leveraging visual distractors. We place distractors within the user's field of view and apply a larger translation gain when their attention is drawn to them. Because the magnitude of gain adjustment depends on the user's level of engagement with the distractors, the redirection process remains smooth and unobtrusive. To evaluate our method, we developed a task oriented virtual environment for a user study. Results show that introducing distractors in the virtual environment significantly raises users' translation gain thresholds. Furthermore, assessments using the Simulator Sickness Questionnaire and Igroup Presence Questionnaire indicate that the method maintains user comfort and acceptance, supporting its effectiveness for RDW systems.</article>","contentLength":1267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Space-Efficient k-Mismatch Text Indexes","url":"https://arxiv.org/abs/2510.26264","date":1761883200,"author":"","guid":323122,"unread":true,"content":"<article>arXiv:2510.26264v1 Announce Type: new \nAbstract: A central task in string processing is text indexing, where the goal is to preprocess a text (a string of length $n$) into an efficient index (a data structure) supporting queries about the text. Cole, Gottlieb, and Lewenstein (STOC 2004) proposed $k$-errata trees, a family of text indexes supporting approximate pattern matching queries of several types. In particular, $k$-errata trees yield an elegant solution to $k$-mismatch queries, where we are to report all substrings of the text with Hamming distance at most $k$ to the query pattern. The resulting $k$-mismatch index uses $O(n\\log^k n)$ space and answers a query for a length-$m$ pattern in $O(\\log^k n \\log \\log n + m + occ)$ time, where $occ$ is the number of approximate occurrences.\n  In retrospect, $k$-errata trees appear very well optimized: even though a large body of work has adapted $k$-errata trees to various settings throughout the past two decades, the original time-space trade-off for $k$-mismatch indexing has not been improved in the general case. We present the first such improvement, a $k$-mismatch index with $O(n\\log^{k-1} n)$ space and the same query time as $k$-errata trees.\n  Previously, due to a result of Chan, Lam, Sung, Tam, and Wong (Algorithmica 2010), such an $O(n\\log^{k-1} n)$-size index has been known only for texts over alphabets of constant size. In this setting, however, we obtain an even smaller $k$-mismatch index of size only $O(n \\log^{k-2+\\varepsilon+\\frac{2}{k+2-(k \\bmod 2)}} n)\\subseteq O(n\\log^{k-1.5+\\varepsilon} n)$ for $2\\le k\\le O(1)$ and any constant $\\varepsilon&gt;0$. Along the way, we also develop improved indexes for short patterns, offering better trade-offs in this practically relevant special case.</article>","contentLength":1773,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Joint Computing Resource Allocation and Task Offloading in Vehicular Fog Computing Systems Under Asymmetric Information","url":"https://arxiv.org/abs/2510.26256","date":1761883200,"author":"","guid":323123,"unread":true,"content":"<article>arXiv:2510.26256v1 Announce Type: new \nAbstract: Vehicular fog computing (VFC) has emerged as a promising paradigm, which leverages the idle computational resources of nearby fog vehicles (FVs) to complement the computing capabilities of conventional vehicular edge computing. However, utilizing VFC to meet the delay-sensitive and computation-intensive requirements of the FVs poses several challenges. First, the limited resources of road side units (RSUs) struggle to accommodate the growing and diverse demands of vehicles. This limitation is further exacerbated by the information asymmetry between the controller and FVs due to the reluctance of FVs to disclose private information and to share resources voluntarily. This information asymmetry hinders the efficient resource allocation and coordination. Second, the heterogeneity in task requirements and the varying capabilities of RSUs and FVs complicate efficient task offloading, thereby resulting in inefficient resource utilization and potential performance degradation. To address these challenges, we first present a hierarchical VFC architecture that incorporates the computing capabilities of both RSUs and FVs. Then, we formulate a delay minimization optimization problem (DMOP), which is an NP-hard mixed integer nonlinear programming problem. To solve the DMOP, we propose a joint computing resource allocation and task offloading approach (JCRATOA). Specifically, we propose a convex optimization-based method for RSU resource allocation and a contract theory-based incentive mechanism for FV resource allocation. Moreover, we present a two-sided matching method for task offloading by employing the matching game. Simulation results demonstrate that the proposed JCRATOA is able to achieve superior performances in task completion delay, task completion ratio, system throughput, and resource utilization fairness, while effectively meeting the satisfying constraints.</article>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages","url":"https://arxiv.org/abs/2510.26254","date":1761883200,"author":"","guid":323124,"unread":true,"content":"<article>arXiv:2510.26254v1 Announce Type: new \nAbstract: Throughout language history, words are borrowed from one language to another and gradually become integrated into the recipient's lexicon. Speakers can often differentiate these loanwords from native vocabulary, particularly in bilingual communities where a dominant language continuously imposes lexical items on a minority language. This paper investigates whether pretrained language models, including large language models, possess similar capabilities for loanword identification. We evaluate multiple models across 10 languages. Despite explicit instructions and contextual information, our results show that models perform poorly in distinguishing loanwords from native ones. These findings corroborate previous evidence that modern NLP systems exhibit a bias toward loanwords rather than native equivalents. Our work has implications for developing NLP tools for minority languages and supporting language preservation in communities under lexical pressure from dominant languages.</article>","contentLength":1038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs","url":"https://arxiv.org/abs/2510.26253","date":1761883200,"author":"","guid":323125,"unread":true,"content":"<article>arXiv:2510.26253v1 Announce Type: new \nAbstract: The ability to accurately interpret implied meanings plays a crucial role in human communication and language use, and language models are also expected to possess this capability. This study demonstrates that providing language models with pragmatic theories as prompts is an effective in-context learning approach for tasks to understand implied meanings. Specifically, we propose an approach in which an overview of pragmatic theories, such as Gricean pragmatics and Relevance Theory, is presented as a prompt to the language model, guiding it through a step-by-step reasoning process to derive a final interpretation. Experimental results showed that, compared to the baseline, which prompts intermediate reasoning without presenting pragmatic theories (0-shot Chain-of-Thought), our methods enabled language models to achieve up to 9.6\\% higher scores on pragmatic reasoning tasks. Furthermore, we show that even without explaining the details of pragmatic theories, merely mentioning their names in the prompt leads to a certain performance improvement (around 1-3%) in larger models compared to the baseline.</article>","contentLength":1164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Avatar Appearance Beyond Pixels - User Ratings and Avatar Preferences within Health Applications","url":"https://arxiv.org/abs/2510.26251","date":1761883200,"author":"","guid":323126,"unread":true,"content":"<article>arXiv:2510.26251v1 Announce Type: new \nAbstract: The appearance of a virtual avatar significantly influences its perceived appropriateness and the user's experience, particularly in healthcare applications. This study analyzed interactions with six avatars of varying characteristics in a patient-reported outcome measures (PROMs) application to investigate correlations between avatar ratings and user preferences. Forty-seven participants completed a healthcare survey involving 30 PROMIS items (Global Health and Physical Function) and then rated the avatars on warmth, competence, attractiveness, and human-likeness, as well as their willingness to share personal data. The results showed that competence was the most critical factor in avatar selection, while human-likeness had minimal impact on health data disclosure. Gender did not significantly affect the ratings, but clothing style played a key role, with male avatars in professional attire rated higher in competence due to gender-stereotypical expectations. In contrast, professional female avatars were rated lower in warmth and attractiveness. These findings underline the importance of thoughtful avatar design in healthcare applications to enhance user experience and engagement.</article>","contentLength":1248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Angular Steering: Behavior Control via Rotation in Activation Space","url":"https://arxiv.org/abs/2510.26243","date":1761883200,"author":"","guid":323127,"unread":true,"content":"<article>arXiv:2510.26243v1 Announce Type: new \nAbstract: Controlling specific behaviors in large language models while preserving their general capabilities is a central challenge for safe and reliable artificial intelligence deployment. Current steering methods, such as vector addition and directional ablation, are constrained within a two-dimensional subspace defined by the activation and feature direction, making them sensitive to chosen parameters and potentially affecting unrelated features due to unintended interactions in activation space. We introduce Angular Steering, a novel and flexible method for behavior modulation that operates by rotating activations within a fixed two-dimensional subspace. By formulating steering as a geometric rotation toward or away from a target behavior direction, Angular Steering provides continuous, fine-grained control over behaviors such as refusal and compliance. We demonstrate this method using refusal steering emotion steering as use cases. Additionally, we propose Adaptive Angular Steering, a selective variant that rotates only activations aligned with the target feature, further enhancing stability and coherence. Angular Steering generalizes existing addition and orthogonalization techniques under a unified geometric rotation framework, simplifying parameter selection and maintaining model stability across a broader range of adjustments. Experiments across multiple model families and sizes show that Angular Steering achieves robust behavioral control while maintaining general language modeling performance, underscoring its flexibility, generalization, and robustness compared to prior approaches. Code and artifacts are available at https://github.com/lone17/angular-steering/.</article>","contentLength":1741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles","url":"https://arxiv.org/abs/2510.26242","date":1761883200,"author":"","guid":323128,"unread":true,"content":"<article>arXiv:2510.26242v1 Announce Type: new \nAbstract: With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.</article>","contentLength":1785,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models","url":"https://arxiv.org/abs/2510.26241","date":1761883200,"author":"","guid":323129,"unread":true,"content":"<article>arXiv:2510.26241v1 Announce Type: new \nAbstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet their grasp of temporal information in video remains weak and, crucially, under-evaluated. We probe this gap with a deceptively simple but revealing challenge: judging the arrow of time (AoT)-whether a short clip is played forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated benchmark that tests whether VLMs can infer temporal direction in natural videos using the same stimuli and behavioral baselines established for humans. Our comprehensive evaluation of open-weight and proprietary, reasoning and non-reasoning VLMs reveals that most models perform near chance, and even the best lag far behind human accuracy on physically irreversible processes (e.g., free fall, diffusion/explosion) and causal manual actions (division/addition) that humans recognize almost instantly. These results highlight a fundamental gap in current multimodal systems: while they capture rich visual-semantic correlations, they lack the inductive biases required for temporal continuity and causal understanding. We release the code and data for AoT-PsyPhyBENCH to encourage further progress in the physical and temporal reasoning capabilities of VLMs.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses","url":"https://arxiv.org/abs/2510.26238","date":1761883200,"author":"","guid":323130,"unread":true,"content":"<article>arXiv:2510.26238v1 Announce Type: new \nAbstract: Millions of people take surveys every day, from market polls and academic studies to medical questionnaires and customer feedback forms. These datasets capture valuable insights, but their scale and structure present a unique challenge for large language models (LLMs), which otherwise excel at few-shot reasoning over open-ended text. Yet, their ability to process questionnaire data or lists of questions crossed with hundreds of respondent rows remains underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics, SPSS, REDCap) are typically designed for humans in the workflow, limiting such data integration with LLM and AI-empowered automation. This gap leaves scientists, surveyors, and everyday users without evidence-based guidance on how to best represent questionnaires for LLM consumption. We address this by introducing QASU (Questionnaire Analysis and Structural Understanding), a benchmark that probes six structural skills, including answer lookup, respondent count, and multi-hop inference, across six serialization formats and multiple prompt strategies. Experiments on contemporary LLMs show that choosing an effective format and prompt combination can improve accuracy by up to 8.8% points compared to suboptimal formats. For specific tasks, carefully adding a lightweight structural hint through self-augmented prompting can yield further improvements of 3-4% points on average. By systematically isolating format and prompting effects, our open source benchmark offers a simple yet versatile foundation for advancing both research and real-world practice in LLM-based questionnaire analysis.</article>","contentLength":1678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PHUMA: Physically-Grounded Humanoid Locomotion Dataset","url":"https://arxiv.org/abs/2510.26236","date":1761883200,"author":"","guid":323131,"unread":true,"content":"<article>arXiv:2510.26236v1 Announce Type: new \nAbstract: Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From req/res to pub/sub: Exploring Media over QUIC Transport for DNS","url":"https://arxiv.org/abs/2510.26234","date":1761883200,"author":"","guid":323132,"unread":true,"content":"<article>arXiv:2510.26234v1 Announce Type: new \nAbstract: The DNS is a key component of the Internet. Originally designed to facilitate the resolution of host names to IP addresses, its scope has continuously expanded over the years, today covering use cases such as load balancing or service discovery. While DNS was initially conceived as a rather static directory service in which resource records (RR) only change rarely, we have seen a number of use cases over the years where a DNS flavor that isn't purely based upon requesting and caching RRs, but rather on an active distribution of updates for all resolvers that showed interest in the respective records in the past, would be preferable. In this paper, we thus explore a publish-subscribe variant of DNS based on the Media-over-QUIC architecture, where we devise a strawman system and protocol proposal to enable pushing RR updates. We provide a prototype implementation, finding that DNS can benefit from a publish-subscribe variant: next to limiting update traffic, it can considerably reduce the time it takes for a resolver to receive the latest version of a record, thereby supporting use cases such as load balancing in content distribution networks. The publish-subscribe architecture also brings new challenges to the DNS, including a higher overhead for endpoints due to additional state management, and increased query latencies on first lookup, due to session establishment latencies.</article>","contentLength":1447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiSE: A diffusion probabilistic model for automatic structure elucidation of organic compounds","url":"https://arxiv.org/abs/2510.26231","date":1761883200,"author":"","guid":323133,"unread":true,"content":"<article>arXiv:2510.26231v1 Announce Type: new \nAbstract: Automatic structure elucidation is essential for self-driving laboratories as it enables the system to achieve truly autonomous. This capability closes the experimental feedback loop, ensuring that machine learning models receive reliable structure information for real-time decision-making and optimization. Herein, we present DiSE, an end-to-end diffusion-based generative model that integrates multiple spectroscopic modalities, including MS, 13C and 1H chemical shifts, HSQC, and COSY, to achieve automated yet accurate structure elucidation of organic compounds. By learning inherent correlations among spectra through data-driven approaches, DiSE achieves superior accuracy, strong generalization across chemically diverse datasets, and robustness to experimental data despite being trained on calculated spectra. DiSE thus represents a significant advance toward fully automated structure elucidation, with broad potential in natural product research, drug discovery, and self-driving laboratories.</article>","contentLength":1054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines","url":"https://arxiv.org/abs/2510.26230","date":1761883200,"author":"","guid":323134,"unread":true,"content":"<article>arXiv:2510.26230v1 Announce Type: new \nAbstract: As a new and promising approach, existing machine unlearning (MU) works typically emphasize theoretical formulations or optimization objectives to achieve knowledge removal. However, when deployed in real-world scenarios, such solutions typically face scalability issues and have to address practical requirements such as full access to original datasets and model. In contrast to the existing approaches, we regard classification training as a sequential process where classes are learned sequentially, which we call \\emph{inductive approach}. Unlearning can then be done by reversing the last training sequence. This is implemented by appending a projection-redistribution layer in the end of the model. Such an approach does not require full access to the original dataset or the model, addressing the challenges of existing methods. This enables modular and model-agnostic deployment as an output filter into existing classification pipelines with minimal alterations. We conducted multiple experiments across multiple datasets including image (CIFAR-10/100 using CNN-based model) and tabular datasets (Covertype using tree-based model). Experiment results show consistently similar output to a fully retrained model with a high computational cost reduction. This demonstrates the applicability, scalability, and system compatibility of our solution while maintaining the performance of the output in a more practical setting.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transcending Sparse Measurement Limits: Operator-Learning-Driven Data Super-Resolution for Inverse Source Problem","url":"https://arxiv.org/abs/2510.26227","date":1761883200,"author":"","guid":323135,"unread":true,"content":"<article>arXiv:2510.26227v1 Announce Type: new \nAbstract: Inverse source localization from Helmholtz boundary data collected over a narrow aperture is highly ill-posed and severely undersampled, undermining classical solvers (e.g., the Direct Sampling Method). We present a modular framework that significantly improves multi-source localization from extremely sparse single-frequency measurements. First, we extend a uniqueness theorem for the inverse source problem, proving that a unique solution is guaranteed under limited viewing apertures. Second, we employ a Deep Operator Network (DeepONet) with a branch-trunk architecture to interpolate the sparse measurements, lifting six to ten samples within the narrow aperture to a sufficiently dense synthetic aperture. Third, the super-resolved field is fed into the Direct Sampling Method (DSM). For a single source, we derive an error estimate showing that sparse data alone can achieve grid-level precision. In two- and three-source trials, localization from raw sparse measurements is unreliable, whereas DeepONet-reconstructed data reduce localization error by about an order of magnitude and remain effective with apertures as small as $\\pi/4$. By decoupling interpolation from inversion, the framework allows the interpolation and inversion modules to be swapped with neural operators and classical algorithms, respectively, providing a practical and flexible design that improves localization accuracy compared with standard baselines.</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space","url":"https://arxiv.org/abs/2510.26219","date":1761883200,"author":"","guid":323136,"unread":true,"content":"<article>arXiv:2510.26219v1 Announce Type: new \nAbstract: Test-time alignment of large language models (LLMs) attracts attention because fine-tuning LLMs requires high computational costs. In this paper, we propose a new test-time alignment method called adaptive importance sampling on pre-logits (AISP) on the basis of the sampling-based model predictive control with the stochastic control input. AISP applies the Gaussian perturbation into pre-logits, which are outputs of the penultimate layer, so as to maximize expected rewards with respect to the mean of the perturbation. We demonstrate that the optimal mean is obtained by importance sampling with sampled rewards. AISP outperforms best-of-n sampling in terms of rewards over the number of used samples and achieves higher rewards than other reward-based test-time alignment methods.</article>","contentLength":834,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation","url":"https://arxiv.org/abs/2510.26213","date":1761883200,"author":"","guid":323137,"unread":true,"content":"<article>arXiv:2510.26213v1 Announce Type: new \nAbstract: Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Who Grants the Agent Power? Defending Against Instruction Injection via Task-Centric Access Control","url":"https://arxiv.org/abs/2510.26212","date":1761883200,"author":"","guid":323138,"unread":true,"content":"<article>arXiv:2510.26212v1 Announce Type: new \nAbstract: AI agents capable of GUI understanding and Model Context Protocol are increasingly deployed to automate mobile tasks. However, their reliance on over-privileged, static permissions creates a critical vulnerability: instruction injection. Malicious instructions, embedded in otherwise benign content like emails, can hijack the agent to perform unauthorized actions. We present AgentSentry, a lightweight runtime task-centric access control framework that enforces dynamic, task-scoped permissions. Instead of granting broad, persistent permissions, AgentSentry dynamically generates and enforces minimal, temporary policies aligned with the user's specific task (e.g., register for an app), revoking them upon completion. We demonstrate that AgentSentry successfully prevents an instruction injection attack, where an agent is tricked into forwarding private emails, while allowing the legitimate task to complete. Our approach highlights the urgent need for intent-aligned security models to safely govern the next generation of autonomous agents.</article>","contentLength":1097,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Who Moved My Transaction? Uncovering Post-Transaction Auditability Vulnerabilities in Modern Super Apps","url":"https://arxiv.org/abs/2510.26210","date":1761883200,"author":"","guid":323139,"unread":true,"content":"<article>arXiv:2510.26210v1 Announce Type: new \nAbstract: Super apps are the cornerstones of modern digital life, embedding financial transactions into nearly every aspect of daily routine. The prevailing security paradigm for these platforms is overwhelmingly focused on pre-transaction authentication, preventing unauthorized payments before they occur. We argue that a critical vulnerability vector has been largely overlooked: the fragility of post-transaction audit trails. We investigate the ease with which a user can permanently erase their transaction history from an app's interface, thereby concealing unauthorized or sensitive activities from the account owner. To quantify this threat, we conducted an empirical study with 6 volunteers who performed a cross-evaluation on six super apps. Our findings are alarming: all six applications studied allow users to delete transaction records, yet a staggering five out of six (83+\\%) fail to protect these records with strong authentication. Only one app in our study required biometric verification for deletion. This study provides the first concrete evidence of this near-ubiquitous vulnerability, demonstrating a critical gap in the current mobile security landscape and underscoring the urgent need for a paradigm shift towards ensuring post-transaction audit integrity.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning","url":"https://arxiv.org/abs/2510.26205","date":1761883200,"author":"","guid":323140,"unread":true,"content":"<article>arXiv:2510.26205v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) has emerged as a leading approach to reducing hallucinations in large language models (LLMs). Current RAG evaluation benchmarks primarily focus on what we call local RAG: retrieving relevant chunks from a small subset of documents to answer queries that require only localized understanding within specific text chunks. However, many real-world applications require a fundamentally different capability -- global RAG -- which involves aggregating and analyzing information across entire document collections to derive corpus-level insights (for example, \"What are the top 10 most cited papers in 2023?\"). In this paper, we introduce GlobalQA -- the first benchmark specifically designed to evaluate global RAG capabilities, covering four core task types: counting, extremum queries, sorting, and top-k extraction. Through systematic evaluation across different models and baselines, we find that existing RAG methods perform poorly on global tasks, with the strongest baseline achieving only 1.51 F1 score. To address these challenges, we propose GlobalRAG, a multi-tool collaborative framework that preserves structural coherence through chunk-level retrieval, incorporates LLM-driven intelligent filters to eliminate noisy documents, and integrates aggregation modules for precise symbolic computation. On the Qwen2.5-14B model, GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1, validating the effectiveness of our method.</article>","contentLength":1533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management","url":"https://arxiv.org/abs/2510.26203","date":1761883200,"author":"","guid":323141,"unread":true,"content":"<article>arXiv:2510.26203v1 Announce Type: new \nAbstract: The sustainability of supply chain plays a key role in achieving optimal performance in controlling the supply chain. The management of risks that occur in a supply chain is a fundamental problem for the purpose of developing the sustainability of the network and elevating the performance efficiency of the supply chain. The correct classification of products is another essential element in a sustainable supply chain. Acknowledging recent breakthroughs in the context of deep networks, several architectural options have been deployed to analyze supply chain datasets. A novel geometric deep network is used to propose an ensemble deep network. The proposed Chebyshev ensemble geometric network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This network is proposed to leverage the information dependencies in supply chain to derive invisible states of samples in the database. The functionality of the proposed deep network is assessed on the two different databases. The SupplyGraph Dataset and DataCo are considered in this research. The prediction of delivery status of DataCo supply chain is done for risk administration. The product classification and edge classification are performed using the SupplyGraph database to enhance the sustainability of the supply network. An average accuracy of 98.95% is obtained for the ensemble network for risk management. The average accuracy of 100% and 98.07% are obtained for sustainable supply chain in terms of 5 product group classification and 4 product relation classification, respectively. The average accuracy of 92.37% is attained for 25 company relation classification. The results confirm an average improvement and efficiency of the proposed method compared to the state-of-the-art approaches.</article>","contentLength":1820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data","url":"https://arxiv.org/abs/2510.26202","date":1761883200,"author":"","guid":323142,"unread":true,"content":"<article>arXiv:2510.26202v1 Announce Type: new \nAbstract: Human feedback can alter language models in unpredictable and undesirable ways, as practitioners lack a clear understanding of what feedback data encodes. While prior work studies preferences over certain attributes (e.g., length or sycophancy), automatically extracting relevant features without pre-specifying hypotheses remains challenging. We introduce What's In My Human Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders. WIMHF characterizes both (1) the preferences a dataset is capable of measuring and (2) the preferences that the annotators actually express. Across 7 datasets, WIMHF identifies a small number of human-interpretable features that account for the majority of the preference prediction signal achieved by black-box models. These features reveal a wide diversity in what humans prefer, and the role of dataset-level context: for example, users on Reddit prefer informality and jokes, while annotators in HH-RLHF and PRISM disprefer them. WIMHF also surfaces potentially unsafe preferences, such as that LMArena users tend to vote against refusals, often in favor of toxic content. The learned features enable effective data curation: re-labeling the harmful examples in Arena yields large safety gains (+37%) with no cost to general performance. They also allow fine-grained personalization: on the Community Alignment dataset, we learn annotator-specific weights over subjective features that improve preference prediction. WIMHF provides a human-centered analysis method for practitioners to better understand and use preference data.</article>","contentLength":1637,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation","url":"https://arxiv.org/abs/2510.26200","date":1761883200,"author":"","guid":323143,"unread":true,"content":"<article>arXiv:2510.26200v1 Announce Type: new \nAbstract: While diffusion language models (DLMs) enable fine-grained refinement, their practical controllability remains fragile. We identify and formally characterize a central failure mode called update forgetting, in which uniform and context agnostic updates induce token level fluctuations across timesteps, erasing earlier semantic edits and disrupting the cumulative refinement process, thereby degrading fluency and coherence. As this failure originates in uniform and context agnostic updates, effective control demands explicit token ordering. We propose Token Timestep Allocation (TTA), which realizes soft and semantic token ordering via per token timestep schedules: critical tokens are frozen early, while uncertain tokens receive continued refinement. This timestep based ordering can be instantiated as either a fixed policy or an adaptive policy driven by task signals, thereby supporting a broad spectrum of refinement strategies. Because it operates purely at inference time, it applies uniformly across various DLMs and naturally extends to diverse supervision sources. Empirically, TTA improves controllability and fluency: on sentiment control, it yields more than 20 percent higher accuracy and nearly halves perplexity using less than one fifth the steps; in detoxification, it lowers maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0). Together, these results demonstrate that softened ordering via timestep allocation is the critical lever for mitigating update forgetting and achieving stable and controllable diffusion text generation.</article>","contentLength":1621,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Structurally Valid Log Generation using FSM-GFlowNets","url":"https://arxiv.org/abs/2510.26197","date":1761883200,"author":"","guid":323144,"unread":true,"content":"<article>arXiv:2510.26197v1 Announce Type: new \nAbstract: Generating structurally valid and behaviorally diverse synthetic event logs for interaction-aware models is a challenging yet crucial problem, particularly in settings with limited or privacy constrained user data. Existing methods such as heuristic simulations and LLM based generators often lack structural coherence or controllability, producing synthetic data that fails to accurately represent real world system interactions. This paper presents a framework that integrates Finite State Machines or FSMs with Generative Flow Networks or GFlowNets to generate structured, semantically valid, and diverse synthetic event logs. Our FSM-constrained GFlowNet ensures syntactic validity and behavioral variation through dynamic action masking and guided sampling. The FSM, derived from expert traces, encodes domain-specific rules, while the GFlowNet is trained using a flow matching objective with a hybrid reward balancing FSM compliance and statistical fidelity. We instantiate the framework in the context of UI interaction logs using the UIC HCI dataset, but the approach generalizes to any symbolic sequence domain. Experimental results based on distributional metrics show that our FSM GFlowNet produces realistic, structurally consistent logs, achieving, for instance, under the real user logs baseline, a KL divergence of 0.2769 and Chi squared distance of 0.3522, significantly outperforming GPT-4o's 2.5294/13.8020 and Gemini's 3.7233/63.0355, alongside a leading bigram overlap of 0.1214 vs. GPT 4o's 0.0028 and Gemini's 0.0007. A downstream use case intent classification demonstrates that classifiers trained solely on our synthetic logs produced from FSM-GFlowNet achieve competitive accuracy compared to real data.</article>","contentLength":1778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction","url":"https://arxiv.org/abs/2510.26196","date":1761883200,"author":"","guid":323145,"unread":true,"content":"<article>arXiv:2510.26196v1 Announce Type: new \nAbstract: 3D human pose estimation from sketches has broad applications in computer animation and film production. Unlike traditional human pose estimation, this task presents unique challenges due to the abstract and disproportionate nature of sketches. Previous sketch-to-pose methods, constrained by the lack of large-scale sketch-3D pose annotations, primarily relied on optimization with heuristic rules-an approach that is both time-consuming and limited in generalizability. To address these challenges, we propose a novel approach leveraging a \"learn from synthesis\" strategy. First, a diffusion model is trained to synthesize sketch images from 2D poses projected from 3D human poses, mimicking disproportionate human structures in sketches. This process enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k accurate sketch-3D pose annotation pairs across various sketch styles. Building on this synthetic dataset, we introduce an end-to-end data-driven framework for estimating human poses and shapes from diverse sketch styles. Our framework combines existing 2D pose detectors and generative diffusion priors for sketch feature extraction with a feed-forward neural network for efficient 2D pose estimation. Multiple heuristic loss functions are incorporated to guarantee geometric coherence between the derived 3D poses and the detected 2D poses while preserving accurate self-contacts. Qualitative, quantitative, and subjective evaluations collectively show that our model substantially surpasses previous ones in both estimation accuracy and speed for sketch-to-pose tasks.</article>","contentLength":1649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RCScore: Quantifying Response Consistency in Large Language Models","url":"https://arxiv.org/abs/2510.26193","date":1761883200,"author":"","guid":323146,"unread":true,"content":"<article>arXiv:2510.26193v1 Announce Type: new \nAbstract: Current LLM evaluations often rely on a single instruction template, overlooking models' sensitivity to instruction style-a critical aspect for real-world deployments. We present RCScore, a multi-dimensional framework quantifying how instruction formulation affects model responses. By systematically transforming benchmark problems into multiple instruction styles, RCScore reveals performance variations undetected by conventional metrics. Our experiments across ten LLMs on four reasoning benchmarks demonstrate that instruction style can shift accuracy by up to 16.7% points. We introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to measure stylistic self-consistency, and establish its strong correlation with task accuracy, suggesting consistency as a valuable proxy for model reliability. Additional findings show that deterministic decoding produces more stylistically stable outputs, and model scale correlates positively with cross-style consistency. RCScore offers a principled approach to assess instruction robustness.</article>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SP-MCQA: Evaluating Intelligibility of TTS Beyond the Word Level","url":"https://arxiv.org/abs/2510.26190","date":1761883200,"author":"","guid":323147,"unread":true,"content":"<article>arXiv:2510.26190v1 Announce Type: new \nAbstract: The evaluation of intelligibility for TTS has reached a bottleneck, as existing assessments heavily rely on word-by-word accuracy metrics such as WER, which fail to capture the complexity of real-world speech or reflect human comprehension needs. To address this, we propose Spoken-Passage Multiple-Choice Question Answering, a novel subjective approach evaluating the accuracy of key information in synthesized speech, and release SP-MCQA-Eval, an 8.76-hour news-style benchmark dataset for SP-MCQA evaluation. Our experiments reveal that low WER does not necessarily guarantee high key-information accuracy, exposing a gap between traditional metrics and practical intelligibility. SP-MCQA shows that even state-of-the-art (SOTA) models still lack robust text normalization and phonetic accuracy. This work underscores the urgent need for high-level, more life-like evaluation criteria now that many systems already excel at WER yet may fall short on real-world intelligibility.</article>","contentLength":1029,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients","url":"https://arxiv.org/abs/2510.26188","date":1761883200,"author":"","guid":323148,"unread":true,"content":"<article>arXiv:2510.26188v1 Announce Type: new \nAbstract: Reducing preventable hospital readmissions is a national priority for payers, providers, and policymakers seeking to improve health care and lower costs. The rate of readmission is being used as a benchmark to determine the quality of healthcare provided by the hospitals. In thisproject, we have used machine learning techniques like Logistic Regression, Random Forest and Support Vector Machines to analyze the health claims data and identify demographic and medical factors that play a crucial role in predicting all-cause readmissions. As the health claims data is high dimensional, we have used Principal Component Analysis as a dimension reduction technique and used the results for building regression models. We compared and evaluated these models based on the Area Under Curve (AUC) metric. Random Forest model gave the highest performance followed by Logistic Regression and Support Vector Machine models. These models can be used to identify the crucial factors causing readmissions and help identify patients to focus on to reduce the chances of readmission, ultimately bringing down the cost and increasing the quality of healthcare provided to the patients.</article>","contentLength":1220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts","url":"https://arxiv.org/abs/2510.26186","date":1761883200,"author":"","guid":323149,"unread":true,"content":"<article>arXiv:2510.26186v1 Announce Type: new \nAbstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous in machine learning datasets. Yet, systematically identifying these biases is challenging without costly, fine-grained attribute annotations. We present ConceptScope, a scalable and automated framework for analyzing visual datasets by discovering and quantifying human-interpretable concepts using Sparse Autoencoders trained on representations from vision foundation models. ConceptScope categorizes concepts into target, context, and bias types based on their semantic relevance and statistical correlation to class labels, enabling class-level dataset characterization, bias identification, and robustness evaluation through concept-based subgrouping. We validate that ConceptScope captures a wide range of visual concepts, including objects, textures, backgrounds, facial attributes, emotions, and actions, through comparisons with annotated datasets. Furthermore, we show that concept activations produce spatial attributions that align with semantically meaningful image regions. ConceptScope reliably detects known biases (e.g., background bias in Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects in ImageNet), offering a practical tool for dataset auditing and model diagnostics.</article>","contentLength":1345,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accumulative SGD Influence Estimation for Data Attribution","url":"https://arxiv.org/abs/2510.26185","date":1761883200,"author":"","guid":323150,"unread":true,"content":"<article>arXiv:2510.26185v1 Announce Type: new \nAbstract: Modern data-centric AI needs precise per-sample influence. Standard SGD-IE approximates leave-one-out effects by summing per-epoch surrogates and ignores cross-epoch compounding, which misranks critical examples. We propose ACC-SGD-IE, a trajectory-aware estimator that propagates the leave-one-out perturbation across training and updates an accumulative influence state at each step. In smooth strongly convex settings it achieves geometric error contraction and, in smooth non-convex regimes, it tightens error bounds; larger mini-batches further reduce constants. Empirically, on Adult, 20 Newsgroups, and MNIST under clean and corrupted data and both convex and non-convex training, ACC-SGD-IE yields more accurate influence estimates, especially over long epochs. For downstream data cleansing it more reliably flags noisy samples, producing models trained on ACC-SGD-IE cleaned data that outperform those cleaned with SGD-IE.</article>","contentLength":981,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation","url":"https://arxiv.org/abs/2510.26184","date":1761883200,"author":"","guid":323151,"unread":true,"content":"<article>arXiv:2510.26184v1 Announce Type: new \nAbstract: Public resource allocation involves the efficient distribution of resources, including urban infrastructure, energy, and transportation, to effectively meet societal demands. However, existing methods focus on optimizing the movement of individual resources independently, without considering their capacity constraints. To address this limitation, we propose a novel and more practical problem: Collaborative Public Resource Allocation (CPRA), which explicitly incorporates capacity constraints and spatio-temporal dynamics in real-world scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold: 1) We formulate the CPRA problem as a potential game and demonstrate that there is no gap between the potential function and the optimal target, laying a solid theoretical foundation for approximating the Nash equilibrium of this NP-hard problem; and 2) Our designed GSTRL framework effectively captures the spatio-temporal dynamics of the overall system. We evaluate GSTRL on two real-world datasets, where experiments show its superior performance. Our source codes are available in the supplementary materials.</article>","contentLength":1257,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Similarity-Distance-Magnitude Language Models","url":"https://arxiv.org/abs/2510.26183","date":1761883200,"author":"","guid":323152,"unread":true,"content":"<article>arXiv:2510.26183v1 Announce Type: new \nAbstract: We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which are sequence prediction models fine-tuned to maximize the proportion of generations in the well-calibrated, high-probability region partitioned by a final-layer SDM activation layer used for binary classification of instruction-following. We demonstrate that existing pre-trained decoder-only Transformer LMs can be readily converted into SDM LMs via supervised fine-tuning, using the final-layer SDM activation layer during training to estimate a change-of-base for a supervised next-token loss over a contrastive input encoding scheme, with additional hard negative examples generated online during training. This results in reduced abstentions (i.e., improved statistical efficiency) compared to strong supervised baselines.</article>","contentLength":854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MossNet: Mixture of State-Space Experts is a Multi-Head Attention","url":"https://arxiv.org/abs/2510.26182","date":1761883200,"author":"","guid":323153,"unread":true,"content":"<article>arXiv:2510.26182v1 Announce Type: new \nAbstract: Large language models (LLMs) have significantly advanced generative applications in natural language processing (NLP). Recent trends in model architectures revolve around efficient variants of transformers or state-space/gated-recurrent models (SSMs, GRMs). However, prevailing SSM/GRM-based methods often emulate only a single attention head, potentially limiting their expressiveness. In this work, we propose MossNet, a novel mixture-of-state-space-experts architecture that emulates a linear multi-head attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation not only in channel-mixing multi-layered perceptron (MLP) blocks but also in the time-mixing SSM kernels to realize multiple \"attention heads.\" Extensive experiments on language modeling and downstream evaluations show that MossNet outperforms both transformer- and SSM-based architectures of similar model size and data budgets. Larger variants of MossNet, trained on trillions of tokens, further confirm its scalability and superior performance. In addition, real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU demonstrate favorable runtime speed and resource usage compared to similarly sized baselines. Our results suggest that MossNet is a compelling new direction for efficient, high-performing recurrent LLM architectures.</article>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient And Stable Third-order Method for Micromagnetics Simulations","url":"https://arxiv.org/abs/2510.26181","date":1761883200,"author":"","guid":323154,"unread":true,"content":"<article>arXiv:2510.26181v1 Announce Type: new \nAbstract: To address the magnetization dynamics in ferromagnetic materials described by the Landau-Lifshitz-Gilbert equation under large damping parameters, a third-order accurate numerical scheme is developed by building upon a second-order method \\cite{CaiChenWangXie2022} and leveraging its efficiency. This method boasts two key advantages: first, it only involves solving linear systems with constant coefficients, enabling the use of fast solvers and thus significantly enhancing numerical efficiency over existing first or second-order approaches. Second, it achieves third-order temporal accuracy and fourth-order spatial accuracy, while being unconditionally stable for large damping parameters. Numerical tests in 1D and 3D scenarios confirm both its third-order accuracy and efficiency gains. When large damping parameters are present, the method demonstrates unconditional stability and reproduces physically plausible structures. For domain wall dynamics simulations, it captures the linear relationship between wall velocity and both the damping parameter and external magnetic field, outperforming lower-order methods in this regard.</article>","contentLength":1187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A parallel solver for random input problems via Karhunen-Lo\\`{e}ve expansion and diagonalized coarse grid correction","url":"https://arxiv.org/abs/2510.26180","date":1761883200,"author":"","guid":323155,"unread":true,"content":"<article>arXiv:2510.26180v1 Announce Type: new \nAbstract: This paper is dedicated to enhancing the computational efficiency of traditional parallel-in-time methods for solving stochastic initial-value problems. The standard parareal algorithm often suffers from slow convergence when applied to problems with stochastic inputs, primarily due to the poor quality of the initial guess. To address this issue, we propose a hybrid parallel algorithm, termed KLE-CGC, which integrates the Karhunen-Lo\\`{e}ve (KL) expansion with the coarse grid correction (CGC). The method first employs the KL expansion to achieve a low-dimensional parameterization of high-dimensional stochastic parameter fields. Subsequently, a generalized Polynomial Chaos (gPC) spectral surrogate model is constructed to enable rapid prediction of the solution field. Utilizing this prediction as the initial value significantly improves the initial accuracy for the parareal iterations. A rigorous convergence analysis is provided, establishing that the proposed framework retains the same theoretical convergence rate as the standard parareal algorithm. Numerical experiments demonstrate that KLE-CGC maintains the same convergence order as the original algorithm while substantially reducing the number of iterations and improving parallel scalability.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Confidential FRIT via Homomorphic Encryption","url":"https://arxiv.org/abs/2510.26179","date":1761883200,"author":"","guid":323156,"unread":true,"content":"<article>arXiv:2510.26179v1 Announce Type: new \nAbstract: Edge computing alleviates the computation burden of data-driven control in cyber-physical systems (CPSs) by offloading complex processing to edge servers. However, the increasing sophistication of cyberattacks underscores the need for security measures that go beyond conventional IT protections and address the unique vulnerabilities of CPSs. This study proposes a confidential data-driven gain-tuning framework using homomorphic encryption, such as ElGamal and CKKS encryption schemes, to enhance cybersecurity in gain-tuning processes outsourced to external servers. The idea for realizing confidential FRIT is to replace the matrix inversion operation with a vector summation form, allowing homomorphic operations to be applied. Numerical examples under 128-bit security confirm performance comparable to conventional methods while providing guidelines for selecting suitable encryption schemes for secure CPS.</article>","contentLength":963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs","url":"https://arxiv.org/abs/2510.26178","date":1761883200,"author":"","guid":323157,"unread":true,"content":"<article>arXiv:2510.26178v1 Announce Type: new \nAbstract: Legal case retrieval (LCR) is a cornerstone of real-world legal decision making, as it enables practitioners to identify precedents for a given query case. Existing approaches mainly rely on traditional lexical models and pretrained language models to encode the texts of legal cases. Yet there are rich information in the relations among different legal entities as well as the crucial reasoning process that uncovers how legal facts and legal issues can lead to judicial decisions. Such relational reasoning process reflects the distinctive characteristics of each case that can distinguish one from another, mirroring the real-world judicial process. Naturally, incorporating such information into the precise case embedding could further enhance the accuracy of case retrieval. In this paper, a novel ReaKase-8B framework is proposed to leverage extracted legal facts, legal issues, legal relation triplets and legal reasoning for effective legal case retrieval. ReaKase-8B designs an in-context legal case representation learning paradigm with a fine-tuned large language model. Extensive experiments on two benchmark datasets from COLIEE 2022 and COLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddings substantially improve retrieval performance over baseline models, highlighting the potential of integrating legal reasoning into legal case retrieval systems. The code has been released on https://github.com/yanran-tang/ReaKase-8B.</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The \"4W+1H\" of Software Supply Chain Security Checklist for Critical Infrastructure","url":"https://arxiv.org/abs/2510.26174","date":1761883200,"author":"","guid":323158,"unread":true,"content":"<article>arXiv:2510.26174v1 Announce Type: new \nAbstract: The increasing frequency and sophistication of software supply chain attacks pose severe risks to critical infrastructure sectors, threatening national security, economic stability, and public safety. Despite growing awareness, existing security practices remain fragmented and insufficient, with most frameworks narrowly focused on isolated life cycle stages or lacking alignment with the specific needs of critical infrastructure (CI) sectors. In this paper, we conducted a multivocal literature review across international frameworks, Australian regulatory sources, and academic studies to identify and analyze security practices across the software supply chain, especially specific CI sector. Our analysis found that few existing frameworks are explicitly tailored to CI domains. We systematically leveraged identified software supply chain security frameworks, using a \"4W+1H\" analytical approach, we synthesized ten core categories (what) of software supply chain security practices, mapped them across life-cycle phases (when), stakeholder roles (who), and implementation levels (how), and examined their coverage across existing frameworks (where). Building on these insights, the paper culminates in structured, multi-layered checklist of 80 questions designed to relevant stakeholders evaluate and enhance their software supply chain security. Our findings reveal gaps between framework guidance and sector-specific needs, highlight the need for integrated, context-aware approaches to safeguard critical infrastructure from evolving software supply chain risks.</article>","contentLength":1622,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models","url":"https://arxiv.org/abs/2510.26173","date":1761883200,"author":"","guid":323159,"unread":true,"content":"<article>arXiv:2510.26173v1 Announce Type: new \nAbstract: Accurate estimation of motion information is crucial in diverse computational imaging and computer vision applications. Researchers have investigated various methods to extract motion information from a single blurred image, including blur kernels and optical flow. However, existing motion representations are often of low quality, i.e., coarse-grained and inaccurate. In this paper, we propose the first high-resolution (HR) Motion Trajectory estimation framework using Diffusion models (MoTDiff). Different from existing motion representations, we aim to estimate an HR motion trajectory with high-quality from a single motion-blurred image. The proposed MoTDiff consists of two key components: 1) a new conditional diffusion framework that uses multi-scale feature maps extracted from a single blurred image as a condition, and 2) a new training method that can promote precise identification of a fine-grained motion trajectory, consistent estimation of overall shape and position of a motion path, and pixel connectivity along a motion trajectory. Our experiments demonstrate that the proposed MoTDiff can outperform state-of-the-art methods in both blind image deblurring and coded exposure photography applications.</article>","contentLength":1272,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linking Heterogeneous Data with Coordinated Agent Flows for Social Media Analysis","url":"https://arxiv.org/abs/2510.26172","date":1761883200,"author":"","guid":323160,"unread":true,"content":"<article>arXiv:2510.26172v1 Announce Type: new \nAbstract: Social media platforms generate massive volumes of heterogeneous data, capturing user behaviors, textual content, temporal dynamics, and network structures. Analyzing such data is crucial for understanding phenomena such as opinion dynamics, community formation, and information diffusion. However, discovering insights from this complex landscape is exploratory, conceptually challenging, and requires expertise in social media mining and visualization. Existing automated approaches, though increasingly leveraging large language models (LLMs), remain largely confined to structured tabular data and cannot adequately address the heterogeneity of social media analysis. We present SIA (Social Insight Agents), an LLM agent system that links heterogeneous multi-modal data -- including raw inputs (e.g., text, network, and behavioral data), intermediate outputs, mined analytical results, and visualization artifacts -- through coordinated agent flows. Guided by a bottom-up taxonomy that connects insight types with suitable mining and visualization techniques, SIA enables agents to plan and execute coherent analysis strategies. To ensure multi-modal integration, it incorporates a data coordinator that unifies tabular, textual, and network data into a consistent flow. Its interactive interface provides a transparent workflow where users can trace, validate, and refine the agent's reasoning, supporting both adaptability and trustworthiness. Through expert-centered case studies and quantitative evaluation, we show that SIA effectively discovers diverse and meaningful insights from social media while supporting human-agent collaboration in complex analytical tasks.</article>","contentLength":1725,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reduction of Test Re-runs by Prioritizing Potential Order Dependent Flaky Tests","url":"https://arxiv.org/abs/2510.26171","date":1761883200,"author":"","guid":323161,"unread":true,"content":"<article>arXiv:2510.26171v1 Announce Type: new \nAbstract: Flaky tests can make automated software testing unreliable due to their unpredictable behavior. These tests can pass or fail on the same code base on multiple runs. However, flaky tests often do not refer to any fault, even though they can cause the continuous integration (CI) pipeline to fail. A common type of flaky test is the order-dependent (OD) test. The outcome of an OD test depends on the order in which it is run with respect to other test cases. Several studies have explored the detection and repair of OD tests. However, their methods require re-runs of tests multiple times, that are not related to the order dependence. Hence, prioritizing potential OD tests is necessary to reduce the re-runs. In this paper, we propose a method to prioritize potential order-dependent tests. By analyzing shared static fields in test classes, we identify tests that are more likely to be order-dependent. In our experiment on 27 project modules, our method successfully prioritized all OD tests in 23 cases, reducing test executions by an average of 65.92% and unnecessary re-runs by 72.19%. These results demonstrate that our approach significantly improves the efficiency of OD test detection by lowering execution costs.</article>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-localization on a 3D map by fusing global and local features from a monocular camera","url":"https://arxiv.org/abs/2510.26170","date":1761883200,"author":"","guid":323162,"unread":true,"content":"<article>arXiv:2510.26170v1 Announce Type: new \nAbstract: Self-localization on a 3D map by using an inexpensive monocular camera is required to realize autonomous driving. Self-localization based on a camera often uses a convolutional neural network (CNN) that can extract local features that are calculated by nearby pixels. However, when dynamic obstacles, such as people, are present, CNN does not work well. This study proposes a new method combining CNN with Vision Transformer, which excels at extracting global features that show the relationship of patches on whole image. Experimental results showed that, compared to the state-of-the-art method (SOTA), the accuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times higher than that without dynamic obstacles. Moreover, the self-localization error of our method is 20.1% smaller than that of SOTA on public datasets. Additionally, our robot using our method can localize itself with 7.51cm error on average, which is more accurate than SOTA.</article>","contentLength":1011,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One Model to Critique Them All: Rewarding Agentic Tool-Use via Efficient Reasoning","url":"https://arxiv.org/abs/2510.26167","date":1761883200,"author":"","guid":323163,"unread":true,"content":"<article>arXiv:2510.26167v1 Announce Type: new \nAbstract: Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight generative RMs tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging dataset of critique tasks that supports reinforcement learning with verifiable feedback. To evaluate tool-use RMs, we also introduce TRBench$_{BFCL}$, a benchmark built on the agentic evaluation suite BFCL. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 14.28% higher accuracy, substantially outperforming frontier models such as Claude 4 and OpenAI o3 in pairwise reward judgments. Beyond training objectives, ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling and reducing output token usage by over 66%. We release data and model checkpoints to facilitate future research.</article>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Dissatisfaction in Bus Route Reduction through LLM-Calibrated Agent-Based Modeling","url":"https://arxiv.org/abs/2510.26163","date":1761883200,"author":"","guid":323164,"unread":true,"content":"<article>arXiv:2510.26163v1 Announce Type: new \nAbstract: As emerging mobility modes continue to expand, many cities face declining bus ridership, increasing fiscal pressure to sustain underutilized routes, and growing inefficiencies in resource allocation. This study employs an agent-based modelling (ABM) approach calibrated through a large language model (LLM) using few-shot learning to examine how progressive bus route cutbacks affect passenger dissatisfaction across demographic groups and overall network resilience. Using IC-card data from Beijing's Huairou District, the LLM-calibrated ABM estimated passenger sensitivity parameters related to travel time, waiting, transfers, and crowding. Results show that the structural configuration of the bus network exerts a stronger influence on system stability than capacity or operational factors. The elimination of high-connectivity routes led to an exponential rise in total dissatisfaction, particularly among passengers with disabilities and older adults. The evolution of dissatisfaction exhibited three distinct phases - stable, transitional, and critical. Through the analysis of each stage, this study found that the continuous bus route reduction scenario exhibits three-stage thresholds. Once these thresholds are crossed, even a small reduction in routes may lead to a significant loss of passenger flow. Research highlights the nonlinear response of user sentiment to service reductions and underscore the importance of maintaining structural critical routes and providing stable services to vulnerable groups for equitable and resilient transport planning.</article>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A two-dimensional fractional-order element-free Galerkin method for nonlocal elasticity and complex domain problems","url":"https://arxiv.org/abs/2510.26161","date":1761883200,"author":"","guid":323165,"unread":true,"content":"<article>arXiv:2510.26161v1 Announce Type: new \nAbstract: This study presents a meshfree two-dimensional fractional-order Element-Free Galerkin (2D f-EFG) method as a viable alternative to conventional mesh-based FEM for a numerical solution of (spatial) fractional-order differential equations (FDEs). The previously developed one-dimensional f-EFG solver offers a limited demonstration of the true efficacy of EFG formulations for FDEs, as it is restricted to simple 1D line geometries. In contrast, the 2D f-EFG solver proposed and developed here effectively demonstrates the potential of meshfree approaches for solving FDEs. The proposed solver can handle complex and irregular 2D domains that are challenging for mesh-based methods. As an example, the developed framework is employed to investigate nonlocal elasticity governed by fractional-order constitutive relations in a square and circular plate. Furthermore, the proposed approach mitigates key drawbacks of FEM, including high computational cost, mesh generation, and reduced accuracy in irregular domains. The 2D f-EFG employs 2D Moving Least Squares (MLS) approximants, which are particularly effective in approximating fractional derivatives from nodal values. The 2D f-EFG solver is employed here for the numerical solution of fractional-order linear and nonlinear partial differential equations corresponding to the nonlocal elastic response of a plate. The solver developed here is validated with the benchmark results available in the literature. While the example chosen here focuses on nonlocal elasticity, the numerical method can be extended for diverse applications of fractional-order derivatives in multiscale modeling, multiphysics coupling, anomalous diffusion, and complex material behavior.</article>","contentLength":1763,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark","url":"https://arxiv.org/abs/2510.26160","date":1761883200,"author":"","guid":323166,"unread":true,"content":"<article>arXiv:2510.26160v1 Announce Type: new \nAbstract: Wearable devices such as smart glasses are transforming the way people interact with their surroundings, enabling users to seek information regarding entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG) plays a key role in supporting such questions, yet there is still no comprehensive benchmark for this task, especially regarding wearables scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn conversations across 13 domains, including 6.2K egocentric images designed to mimic captures from wearable devices. We carefully constructed the questions to reflect real-world scenarios and challenges, including five types of image-quality issues, six question types, varying entity popularity, differing information dynamism, and different conversation turns. We design three tasks: single-source augmentation, multi-source augmentation, and multi-turn conversations -- each paired with an associated retrieval corpus and APIs for both image-KG retrieval and webpage retrieval. Our evaluation shows that straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM single- and multi-turn QA, respectively, whereas state-of-the-art industry solutions have similar quality (32%/45%), underscoring ample room for improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K participants and 5K submissions, with winning solutions improving baseline performance by 28%, highlighting its early impact on advancing the field.</article>","contentLength":1693,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series","url":"https://arxiv.org/abs/2510.26159","date":1761883200,"author":"","guid":323167,"unread":true,"content":"<article>arXiv:2510.26159v1 Announce Type: new \nAbstract: In this study, we investigate the effectiveness of advanced feature engineering and hybrid model architectures for anomaly detection in a multivariate industrial time series, focusing on a steam turbine system. We evaluate the impact of change point-derived statistical features, clustering-based substructure representations, and hybrid learning strategies on detection performance. Despite their theoretical appeal, these complex approaches consistently underperformed compared to a simple Random Forest + XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of 0.976, F1-score of 0.41, and 100% early detection within the defined time window. Our findings highlight that, in scenarios with highly imbalanced and temporally uncertain data, model simplicity combined with optimized segmentation can outperform more sophisticated architectures, offering greater robustness, interpretability, and operational utility.</article>","contentLength":989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment","url":"https://arxiv.org/abs/2510.26157","date":1761883200,"author":"","guid":323168,"unread":true,"content":"<article>arXiv:2510.26157v1 Announce Type: new \nAbstract: Molecule and text representation learning has gained increasing interest due to its potential for enhancing the understanding of chemical information. However, existing models often struggle to capture subtle differences between molecules and their descriptions, as they lack the ability to learn fine-grained alignments between molecular substructures and chemical phrases. To address this limitation, we introduce MolBridge, a novel molecule-text learning framework based on substructure-aware alignments. Specifically, we augment the original molecule-description pairs with additional alignment signals derived from molecular substructures and chemical phrases. To effectively learn from these enriched alignments, MolBridge employs substructure-aware contrastive learning, coupled with a self-refinement mechanism that filters out noisy alignment signals. Experimental results show that MolBridge effectively captures fine-grained correspondences and outperforms state-of-the-art baselines on a wide range of molecular benchmarks, highlighting the significance of substructure-aware alignment in molecule-text learning.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh","url":"https://arxiv.org/abs/2510.26154","date":1761883200,"author":"","guid":323169,"unread":true,"content":"<article>arXiv:2510.26154v1 Announce Type: new \nAbstract: Modes of transportation vary across countries depending on geographical location and cultural context. In South Asian countries rickshaws are among the most common means of local transport. Based on their mode of operation, rickshaws in cities across Bangladesh can be broadly classified into non-auto (pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from accessing certain routes. However, existing surveillance systems make it quite difficult to monitor them due to their similarity to other vehicles, especially non-auto rickshaws whereas manual video analysis is too time-consuming. This paper presents a machine learning-based approach to automatically detect auto-rickshaws in traffic images. In this system, we used real-time object detection using the YOLOv8 model. For training purposes, we prepared a set of 1,730 annotated images that were captured under various traffic conditions. The results show that our proposed model performs well in real-time auto-rickshaw detection and offers an mAP50 of 83.447% and binary precision and recall values above 78%, demonstrating its effectiveness in handling both dense and sparse traffic scenarios. The dataset has been publicly released for further research.</article>","contentLength":1362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction","url":"https://arxiv.org/abs/2510.26151","date":1761883200,"author":"","guid":323170,"unread":true,"content":"<article>arXiv:2510.26151v1 Announce Type: new \nAbstract: Large annotated datasets are essential for training robust Computer-Aided Diagnosis (CAD) models for breast cancer detection or risk prediction. However, acquiring such datasets with fine-detailed annotation is both costly and time-consuming. Vision-Language Models (VLMs), such as CLIP, which are pre-trained on large image-text pairs, offer a promising solution by enhancing robustness and data efficiency in medical imaging tasks. This paper introduces a novel Multi-View Mammography and Language Model for breast cancer classification and risk prediction, trained on a dataset of paired mammogram images and synthetic radiology reports. Our MV-MLM leverages multi-view supervision to learn rich representations from extensive radiology data by employing cross-modal self-supervision across image-text pairs. This includes multiple views and the corresponding pseudo-radiology reports. We propose a novel joint visual-textual learning strategy to enhance generalization and accuracy performance over different data types and tasks to distinguish breast tissues or cancer characteristics(calcification, mass) and utilize these patterns to understand mammography images and predict cancer risk. We evaluated our method on both private and publicly available datasets, demonstrating that the proposed model achieves state-of-the-art performance in three classification tasks: (1) malignancy classification, (2) subtype classification, and (3) image-based cancer risk prediction. Furthermore, the model exhibits strong data efficiency, outperforming existing fully supervised or VLM baselines while trained on synthetic text reports and without the need for actual radiology reports.</article>","contentLength":1731,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation","url":"https://arxiv.org/abs/2510.26149","date":1761883200,"author":"","guid":323171,"unread":true,"content":"<article>arXiv:2510.26149v1 Announce Type: new \nAbstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution of video frames, potentially at various scaling factors, which presents several challenges regarding spatial detail reproduction, temporal consistency, and computational complexity. In this paper, we propose a strong baseline BasicAVSR for AVSR by integrating four key components: 1) adaptive multi-scale frequency priors generated from image Laplacian pyramids, 2) a flow-guided propagation unit to aggregate spatiotemporal information from adjacent frames, 3) a second-order motion compensation unit for more accurate spatial alignment of adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and content-independent upsampling kernels. To meet diverse application demands, we instantiate three propagation variants: (i) a unidirectional RNN unit for strictly online inference, (ii) a unidirectional RNN unit empowered with a limited lookahead that tolerates a small output delay, and (iii) a bidirectional RNN unit designed for offline tasks where computational resources are less constrained. Experimental results demonstrate the effectiveness and adaptability of our model across these different scenarios. Through extensive experiments, we show that BasicAVSR significantly outperforms existing methods in terms of super-resolution quality, generalization ability, and inference speed. Our work not only advances the state-of-the-art in AVSR but also extends its core components to multiple frameworks for diverse scenarios. The code is available at https://github.com/shangwei5/BasicAVSR.</article>","contentLength":1639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments","url":"https://arxiv.org/abs/2510.26148","date":1761883200,"author":"","guid":323172,"unread":true,"content":"<article>arXiv:2510.26148v1 Announce Type: new \nAbstract: Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI) presents a privacy-preserving, contactless sensing approach suitable for smart homes, healthcare monitoring, and mobile IoT systems. However, existing methods often encounter computational inefficiency, high latency, and limited feasibility within resource-constrained, embedded mobile edge environments. This paper proposes STAR (Sensing Technology for Activity Recognition), an edge-AI-optimized framework that integrates a lightweight neural architecture, adaptive signal processing, and hardware-aware co-optimization to enable real-time, energy-efficient HAR on low-power embedded devices. STAR incorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neural network, reducing model parameters by 33% compared to conventional LSTM models while maintaining effective temporal modeling capability. A multi-stage pre-processing pipeline combining median filtering, 8th-order Butterworth low-pass filtering, and Empirical Mode Decomposition (EMD) is employed to denoise CSI amplitude data and extract spatial-temporal features. For on-device deployment, STAR is implemented on a Rockchip RV1126 processor equipped with an embedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSI acquisition module. Experimental results demonstrate a mean recognition accuracy of 93.52% across seven activity classes and 99.11% for human presence detection, utilizing a compact 97.6k-parameter model. INT8 quantized inference achieves a processing speed of 33 MHz with just 8% CPU utilization, delivering sixfold speed improvements over CPU-based execution. With sub-second response latency and low power consumption, the system ensures real-time, privacy-preserving HAR, offering a practical, scalable solution for mobile and pervasive computing environments.</article>","contentLength":1901,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Duality-Based Fixed Point Iteration Algorithm for Beamforming Design in ISAC Systems","url":"https://arxiv.org/abs/2510.26147","date":1761883200,"author":"","guid":323173,"unread":true,"content":"<article>arXiv:2510.26147v1 Announce Type: new \nAbstract: In this paper, we investigate the beamforming design problem in an integrated sensing and communication (ISAC) system, where a multi-antenna base station simultaneously serves multiple communication users while performing radar sensing. We formulate the problem as the minimization of the total transmit power, subject to signal-to-interference-plus-noise ratio (SINR) constraints for communication users and mean-squared-error (MSE) constraints for radar sensing. The core challenge arises from the complex coupling between communication SINR requirements and sensing performance metrics. To efficiently address this challenge, we first establish the equivalence between the original ISAC beamforming problem and its semidefinite relaxation (SDR), derive its Lagrangian dual formulation, and further reformulate it as a generalized downlink beamforming (GDB) problem with potentially indefinite weighting matrices. Compared to the classical DB problem, the presence of indefinite weighting matrices in the GDB problem introduces substantial analytical and computational challenges. Our key technical contributions include (i) a necessary and sufficient condition for the boundedness of the GDB problem, and (ii) a tailored efficient fixed point iteration (FPI) algorithm with a provable convergence guarantee for solving the GDB problem. Building upon these results, we develop a duality-based fixed point iteration (Dual-FPI) algorithm, which integrates an outer subgradient ascent loop with an inner FPI loop. Simulation results demonstrate that the proposed Dual-FPI algorithm achieves globally optimal solutions while significantly reducing computational complexity compared with existing baseline approaches.</article>","contentLength":1763,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition","url":"https://arxiv.org/abs/2510.26146","date":1761883200,"author":"","guid":323174,"unread":true,"content":"<article>arXiv:2510.26146v1 Announce Type: new \nAbstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR) provides a privacy-preserving, device-free sensing solution for smart environments. However, its deployment on edge devices is severely constrained by domain shift, where recognition performance deteriorates under varying environmental and hardware conditions. This study presents maxVSTAR (maximally adaptive Vision-guided Sensing Technology for Activity Recognition), a closed-loop, vision-guided model adaptation framework that autonomously mitigates domain shift for edge-deployed CSI sensing systems. The proposed system integrates a cross-modal teacher-student architecture, where a high-accuracy YOLO-based vision model serves as a dynamic supervisory signal, delivering real-time activity labels for the CSI data stream. These labels enable autonomous, online fine-tuning of a lightweight CSI-based HAR model, termed Sensing Technology for Activity Recognition (STAR), directly at the edge. This closed-loop retraining mechanism allows STAR to continuously adapt to environmental changes without manual intervention. Extensive experiments demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated hardware, the baseline STAR model's recognition accuracy declined from 93.52% to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored the accuracy to 81.51%. These results confirm the system's capacity for dynamic, self-supervised model adaptation in privacy-conscious IoT environments, establishing a scalable and practical paradigm for long-term autonomous HAR using CSI sensing at the network edge.</article>","contentLength":1670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The FM Agent","url":"https://arxiv.org/abs/2510.26144","date":1761883200,"author":"","guid":323175,"unread":true,"content":"<article>arXiv:2510.26144v1 Announce Type: new \nAbstract: Large language models (LLMs) are catalyzing the development of autonomous AI research agents for scientific and engineering discovery. We present FM Agent, a novel and general-purpose multi-agent framework that leverages a synergistic combination of LLM-based reasoning and large-scale evolutionary search to address complex real-world challenges. The core of FM Agent integrates several key innovations: 1) a cold-start initialization phase incorporating expert guidance, 2) a novel evolutionary sampling strategy for iterative optimization, 3) domain-specific evaluators that combine correctness, effectiveness, and LLM-supervised feedback, and 4) a distributed, asynchronous execution infrastructure built on Ray. Demonstrating broad applicability, our system has been evaluated across diverse domains, including operations research, machine learning, GPU kernel optimization, and classical mathematical problems. FM Agent reaches state-of-the-art results autonomously, without human interpretation or tuning -- 1976.3 on ALE-Bench (+5.2\\%), 43.56\\% on MLE-Bench (+4.0pp), up to 20x speedups on KernelBench, and establishes new state-of-the-art(SOTA) results on several classical mathematical problems. Beyond academic benchmarks, FM Agent shows considerable promise for both large-scale enterprise R\\&amp;D workflows and fundamental scientific research, where it can accelerate innovation, automate complex discovery processes, and deliver substantial engineering and scientific advances with broader societal impact.</article>","contentLength":1566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math","url":"https://arxiv.org/abs/2510.26143","date":1761883200,"author":"","guid":323176,"unread":true,"content":"<article>arXiv:2510.26143v1 Announce Type: new \nAbstract: Reinforcement learning (RL) can elicit strong reasoning in large language models (LLMs), yet most open efforts focus on math and code. We propose Reasoning Curriculum, a simple two-stage curriculum that first elicits reasoning skills in pretraining-aligned domains such as math, then adapts and refines these skills across other domains via joint RL. Stage 1 performs a brief cold start and then math-only RL with verifiable rewards to develop reasoning skills. Stage 2 runs joint RL on mixed-domain data to transfer and consolidate these skills. The curriculum is minimal and backbone-agnostic, requiring no specialized reward models beyond standard verifiability checks. Evaluated on Qwen3-4B and Llama-3.1-8B over a multi-domain suite, reasoning curriculum yields consistent gains. Ablations and a cognitive-skill analysis indicate that both stages are necessary and that math-first elicitation increases cognitive behaviors important for solving complex problems. Reasoning Curriculum provides a compact, easy-to-adopt recipe for general reasoning.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Trajectory Refinement for Optimization-based Local Planning in Narrow Passages","url":"https://arxiv.org/abs/2510.26142","date":1761883200,"author":"","guid":323177,"unread":true,"content":"<article>arXiv:2510.26142v1 Announce Type: new \nAbstract: Trajectory planning for mobile robots in cluttered environments remains a major challenge due to narrow passages, where conventional methods often fail or generate suboptimal paths. To address this issue, we propose the adaptive trajectory refinement algorithm, which consists of two main stages. First, to ensure safety at the path-segment level, a segment-wise conservative collision test is applied, where risk-prone trajectory path segments are recursively subdivided until collision risks are eliminated. Second, to guarantee pose-level safety, pose correction based on penetration direction and line search is applied, ensuring that each pose in the trajectory is collision-free and maximally clear from obstacles. Simulation results demonstrate that the proposed method achieves up to 1.69x higher success rates and up to 3.79x faster planning times than state-of-the-art approaches. Furthermore, real-world experiments confirm that the robot can safely pass through narrow passages while maintaining rapid planning performance.</article>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StructLayoutFormer:Conditional Structured Layout Generation via Structure Serialization and Disentanglement","url":"https://arxiv.org/abs/2510.26141","date":1761883200,"author":"","guid":323178,"unread":true,"content":"<article>arXiv:2510.26141v1 Announce Type: new \nAbstract: Structured layouts are preferable in many 2D visual contents (\\eg, GUIs, webpages) since the structural information allows convenient layout editing. Computational frameworks can help create structured layouts but require heavy labor input. Existing data-driven approaches are effective in automatically generating fixed layouts but fail to produce layout structures. We present StructLayoutFormer, a novel Transformer-based approach for conditional structured layout generation. We use a structure serialization scheme to represent structured layouts as sequences. To better control the structures of generated layouts, we disentangle the structural information from the element placements. Our approach is the first data-driven approach that achieves conditional structured layout generation and produces realistic layout structures explicitly. We compare our approach with existing data-driven layout generation approaches by including post-processing for structure extraction. Extensive experiments have shown that our approach exceeds these baselines in conditional structured layout generation. We also demonstrate that our approach is effective in extracting and transferring layout structures. The code is publicly available at %\\href{https://github.com/Teagrus/StructLayoutFormer} {https://github.com/Teagrus/StructLayoutFormer}.</article>","contentLength":1387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FullPart: Generating each 3D Part at Full Resolution","url":"https://arxiv.org/abs/2510.26140","date":1761883200,"author":"","guid":323179,"unread":true,"content":"<article>arXiv:2510.26140v1 Announce Type: new \nAbstract: Part-based 3D generation holds great potential for various applications. Previous part generators that represent parts using implicit vector-set tokens often suffer from insufficient geometric details. Another line of work adopts an explicit voxel representation but shares a global voxel grid among all parts; this often causes small parts to occupy too few voxels, leading to degraded quality. In this paper, we propose FullPart, a novel framework that combines both implicit and explicit paradigms. It first derives the bounding box layout through an implicit box vector-set diffusion process, a task that implicit diffusion handles effectively since box tokens contain little geometric detail. Then, it generates detailed parts, each within its own fixed full-resolution voxel grid. Instead of sharing a global low-resolution space, each part in our method - even small ones - is generated at full resolution, enabling the synthesis of intricate details. We further introduce a center-point encoding strategy to address the misalignment issue when exchanging information between parts of different actual sizes, thereby maintaining global coherence. Moreover, to tackle the scarcity of reliable part data, we present PartVerse-XL, the largest human-annotated 3D part dataset to date with 40K objects and 320K parts. Extensive experiments demonstrate that FullPart achieves state-of-the-art results in 3D part generation. We will release all code, data, and model to benefit future research in 3D part generation.</article>","contentLength":1565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kinodynamic Task and Motion Planning using VLM-guided and Interleaved Sampling","url":"https://arxiv.org/abs/2510.26139","date":1761883200,"author":"","guid":323180,"unread":true,"content":"<article>arXiv:2510.26139v1 Announce Type: new \nAbstract: Task and Motion Planning (TAMP) integrates high-level task planning with low-level motion feasibility, but existing methods are costly in long-horizon problems due to excessive motion sampling. While LLMs provide commonsense priors, they lack 3D spatial reasoning and cannot ensure geometric or dynamic feasibility. We propose a kinodynamic TAMP framework based on a hybrid state tree that uniformly represents symbolic and numeric states during planning, enabling task and motion decisions to be jointly decided. Kinodynamic constraints embedded in the TAMP problem are verified by an off-the-shelf motion planner and physics simulator, and a VLM guides exploring a TAMP solution and backtracks the search based on visual rendering of the states. Experiments on the simulated domains and in the real world show 32.14% - 1166.67% increased average success rates compared to traditional and LLM-based TAMP planners and reduced planning time on complex problems, with ablations further highlighting the benefits of VLM guidance.</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Benchmarks: The Economics of AI Inference","url":"https://arxiv.org/abs/2510.26136","date":1761883200,"author":"","guid":323181,"unread":true,"content":"<article>arXiv:2510.26136v1 Announce Type: new \nAbstract: The inference cost of Large Language Models (LLMs) has become a critical factor in determining their commercial viability and widespread adoption. This paper introduces a quantitative ``economics of inference'' framework, treating the LLM inference process as a compute-driven intelligent production activity. We analyze its marginal cost, economies of scale, and quality of output under various performance configurations. Based on empirical data from WiNEval-3.0, we construct the first ``LLM Inference Production Frontier,'' revealing three principles: diminishing marginal cost, diminishing returns to scale, and an optimal cost-effectiveness zone. This paper not only provides an economic basis for model deployment decisions but also lays an empirical foundation for the future market-based pricing and optimization of AI inference resources.</article>","contentLength":897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Green Wireless Network Scaling for Joint Deployment: Multi-BSs or Multi-RISs?","url":"https://arxiv.org/abs/2510.26135","date":1761883200,"author":"","guid":323182,"unread":true,"content":"<article>arXiv:2510.26135v1 Announce Type: new \nAbstract: The imminent emergence of sixth-generation (6G) networks faces critical challenges from spatially heterogeneous traffic and escalating energy consumption, necessitating sustainable scaling strategies for network infrastructure such as base stations (BSs) and reconfigurable intelligent surfaces (RISs). This paper establishes fundamental scaling laws for the Integrated Relative Energy Efficiency (IREE) metric under joint multi-BS and multi-RIS deployment in traffic-mismatched scenarios. Specifically, we propose an Alternating Directional Dual-Radial Basis Function (ADD-RBF) framework that models the channels of BSs and RISs as two type of spatially decoupled RBF neurons to maximize IREE through alternative optimization, with proven universal approximation capability and convergence guarantees. Theoretical analysis reveals a scaling dichotomy: BS proliferation drives logarithmic capacity growth $\\mathcal{O}(\\log N^{BS})$ but only polynomial mismatch reduction $\\mathcal{O}(1/\\sqrt{N^{BS}})$, whereas RIS deployment achieves exponential mismatch mitigation $\\mathcal{O}(\\delta_{\\text{err}}^{-(N^R+1)})$ despite its sub-logarithmic capacity gains. Simulation results validate that RISs excel in capturing spatial traffic correlations and alleviating hotspots, making them particularly effective when mismatch dominates, while BSs are preferable under capacity shortages. These findings offer practical guidelines for green 6G network design.</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Embodied Intelligence for Advanced Bioinspired Microrobotics: Examples and Insights","url":"https://arxiv.org/abs/2510.26132","date":1761883200,"author":"","guid":323183,"unread":true,"content":"<article>arXiv:2510.26132v1 Announce Type: new \nAbstract: The term embodied intelligence (EI) conveys the notion that body morphology, material properties, interaction with the environment, and control strategies can be purposefully integrated into the process of robotic design to generate intelligent behavior; in particular, locomotion and navigation. In this paper, we discuss EI as a design principle for advanced microrobotics, with a particular focus on co-design -- the simultaneous and interdependent development of physical structure and behavioral function. To illustrate the contrast between EI-inspired systems and traditional architectures that decouple sensing, computation, and actuation, we present and discuss a collection of robots developed by the author and his team at the Autonomous Microrobotic Systems Laboratory (AMSL). These robots exhibit intelligent behavior that emerges from their structural dynamics and the physical interaction between their components and with the environment. Platforms such as the Bee++, RoBeetle, SMALLBug, SMARTI, WaterStrider, VLEIBot+, and FRISSHBot exemplify how feedback loops, decision logics, sensing mechanisms, and smart actuation strategies can be embedded into the physical properties of the robotic system itself. Along these lines, we contend that co-design is not only a method for empirical optimization under constraints, but also an enabler of EI, offering a scalable and robust alternative to classical control for robotics at the mm-to-cm-scale.</article>","contentLength":1509,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM","url":"https://arxiv.org/abs/2510.26131","date":1761883200,"author":"","guid":323184,"unread":true,"content":"<article>arXiv:2510.26131v1 Announce Type: new \nAbstract: Attention models have recently emerged as a powerful approach, demonstrating significant progress in various fields. Visualization techniques, such as class activation mapping, provide visual insights into the reasoning of convolutional neural networks (CNNs). Using network gradients, it is possible to identify regions where the network pays attention during image recognition tasks. Furthermore, these gradients can be combined with CNN features to localize more generalizable, task-specific attentive (salient) regions within scenes. However, explicit use of this gradient-based attention information integrated directly into CNN representations for semantic object understanding remains limited. Such integration is particularly beneficial for visual tasks like simultaneous localization and mapping (SLAM), where CNN representations enriched with spatially attentive object locations can enhance performance. In this work, we propose utilizing task-specific network attention for RGB-D indoor SLAM. Specifically, we integrate layer-wise attention information derived from network gradients with CNN feature representations to improve frame association performance. Experimental results indicate improved performance compared to baseline methods, particularly for large environments.</article>","contentLength":1337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation","url":"https://arxiv.org/abs/2510.26130","date":1761883200,"author":"","guid":323185,"unread":true,"content":"<article>arXiv:2510.26130v1 Announce Type: new \nAbstract: Large language models (LLMs) have advanced code generation at the function level, yet their ability to produce correct class-level implementations in authentic software projects remains poorly understood. This work introduces a novel benchmark derived from open-source repositories, comprising real-world classes divided into seen and unseen partitions to evaluate generalization under practical conditions. The evaluation examines multiple LLMs under varied input specifications, retrieval-augmented configurations, and documentation completeness levels.\n  Results reveal a stark performance disparity: LLMs achieve 84% to 89% correctness on established synthetic benchmarks but only 25% to 34% on real-world class tasks, with negligible differences between familiar and novel codebases. Comprehensive docstrings yield modest gains of 1% to 3% in functional accuracy, though statistical significance is rare. Retrieval-augmented generation proves most effective with partial documentation, improving correctness by 4% to 7% by supplying concrete implementation patterns absent from specifications. Error profiling identifies AttributeError, TypeError, and AssertionError as dominant failure modes (84% of cases), with synthetic tests overemphasizing assertion issues and real-world scenarios highlighting type and attribute mismatches. Retrieval augmentation reduces logical flaws but can introduce dependency conflicts.\n  The benchmark and analysis expose critical limitations in current LLM capabilities for class-level engineering, offering actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.</article>","contentLength":1735,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios","url":"https://arxiv.org/abs/2510.26125","date":1761883200,"author":"","guid":323186,"unread":true,"content":"<article>arXiv:2510.26125v1 Announce Type: new \nAbstract: Vision-based end-to-end (E2E) driving has garnered significant interest in the research community due to its scalability and synergy with multimodal large language models (MLLMs). However, current E2E driving benchmarks primarily feature nominal scenarios, failing to adequately test the true potential of these systems. Furthermore, existing open-loop evaluation metrics often fall short in capturing the multi-modal nature of driving or effectively evaluating performance in long-tail scenarios. To address these gaps, we introduce the Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021 driving segments (approximately 12 hours), specifically curated for challenging long-tail scenarios that that are rare in daily life with an occurring frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the high-level routing information, ego states, and 360-degree camera views from 8 surrounding cameras. To evaluate the E2E driving performance on these long-tail situations, we propose a novel open-loop evaluation metric: Rater Feedback Score (RFS). Unlike conventional metrics that measure the distance between predicted way points and the logs, RFS measures how closely the predicted trajectory matches rater-annotated trajectory preference labels. We have released rater preference labels for all WOD-E2E validation set segments, while the held out test set labels have been used for the 2025 WOD-E2E Challenge. Through our work, we aim to foster state of the art research into generalizable, robust, and safe end-to-end autonomous driving agents capable of handling complex real-world situations.</article>","contentLength":1690,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Influence of Discourse Relations in Persuasive Texts","url":"https://arxiv.org/abs/2510.26124","date":1761883200,"author":"","guid":323187,"unread":true,"content":"<article>arXiv:2510.26124v1 Announce Type: new \nAbstract: This paper investigates the relationship between Persuasion Techniques (PTs) and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and prompt engineering. Since no dataset annotated with both PTs and DRs exists, we took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point and developed LLM-based classifiers to label each instance of the dataset with one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10 different prompts, resulting in 40 unique DR classifiers. Ensemble models using different majority-pooling strategies were used to create 5 silver datasets of instances labelled with both persuasion techniques and level-2 PDTB senses. The silver dataset sizes vary from 1,281 instances to 204 instances, depending on the majority pooling technique used. Statistical analysis of these silver datasets shows that six discourse relations (namely Cause, Purpose, Contrast, Cause+Belief, Concession, and Condition) play a crucial role in persuasive texts, especially in the use of Loaded Language, Exaggeration/Minimisation, Repetition and to cast Doubt. This insight can contribute to detecting online propaganda and misinformation, as well as to our general understanding of effective communication.</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking","url":"https://arxiv.org/abs/2510.26122","date":1761883200,"author":"","guid":323188,"unread":true,"content":"<article>arXiv:2510.26122v1 Announce Type: new \nAbstract: While Test-Time Scaling (TTS) has proven effective in improving the reasoning ability of large language models (LLMs), low diversity in model outputs often becomes a bottleneck; this is partly caused by the common \"one problem, one solution\" (1P1S) training practice, which provides a single canonical answer and can push models toward a narrow set of reasoning paths. To address this, we propose a \"one problem, multiple solutions\" (1PNS) training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity. A core challenge for 1PNS is reliably measuring semantic differences between multi-step chains of thought, so we introduce Reasoning Path Divergence (RPD), a step-level metric that aligns and scores Long Chain-of-Thought solutions to capture differences in intermediate reasoning. Using RPD, we curate maximally diverse solution sets per problem and fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields more varied outputs and higher pass@k, with an average +2.80% gain in pass@16 over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that 1PNS further amplifies the effectiveness of TTS. Our code is available at https://github.com/fengjujf/Reasoning-Path-Divergence .</article>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting","url":"https://arxiv.org/abs/2510.26117","date":1761883200,"author":"","guid":323189,"unread":true,"content":"<article>arXiv:2510.26117v1 Announce Type: new \nAbstract: Traditional novel view synthesis methods heavily rely on external camera pose estimation tools such as COLMAP, which often introduce computational bottlenecks and propagate errors. To address these challenges, we propose a unified framework that jointly optimizes 3D Gaussian points and camera poses without requiring pre-calibrated inputs. Our approach iteratively refines 3D Gaussian parameters and updates camera poses through a novel co-optimization strategy, ensuring simultaneous improvements in scene reconstruction fidelity and pose accuracy. The key innovation lies in decoupling the joint optimization into two interleaved phases: first, updating 3D Gaussian parameters via differentiable rendering with fixed poses, and second, refining camera poses using a customized 3D optical flow algorithm that incorporates geometric and photometric constraints. This formulation progressively reduces projection errors, particularly in challenging scenarios with large viewpoint variations and sparse feature distributions, where traditional methods struggle. Extensive evaluations on multiple datasets demonstrate that our approach significantly outperforms existing COLMAP-free techniques in reconstruction quality, and also surpasses the standard COLMAP-based baseline in general.</article>","contentLength":1333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research","url":"https://arxiv.org/abs/2510.26114","date":1761883200,"author":"","guid":323190,"unread":true,"content":"<article>arXiv:2510.26114v1 Announce Type: new \nAbstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves the cultural and intellectual heritage of ancient civilizations. However, current OBS research faces two major challenges: (1) the interpretation of OBS involves a complex workflow comprising multiple serial and parallel sub-tasks, and (2) the efficiency of OBS information organization and retrieval remains a critical bottleneck, as scholars often spend substantial effort searching for, compiling, and managing relevant resources. To address these challenges, we present OracleAgent, the first agent system designed for the structured management and retrieval of OBS-related information. OracleAgent seamlessly integrates multiple OBS analysis tools, empowered by large language models (LLMs), and can flexibly orchestrate these components. Additionally, we construct a comprehensive domain-specific multimodal knowledge base for OBS, which is built through a rigorous multi-year process of data collection, cleaning, and expert annotation. The knowledge base comprises over 1.4M single-character rubbing images and 80K interpretation texts. OracleAgent leverages this resource through its multimodal tools to assist experts in retrieval tasks of character, document, interpretation text, and rubbing image. Extensive experiments demonstrate that OracleAgent achieves superior performance across a range of multimodal reasoning and generation tasks, surpassing leading mainstream multimodal large language models (MLLMs) (e.g., GPT-4o). Furthermore, our case study illustrates that OracleAgent can effectively assist domain experts, significantly reducing the time cost of OBS research. These results highlight OracleAgent as a significant step toward the practical deployment of OBS-assisted research and automated interpretation systems.</article>","contentLength":1866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EgoExo-Con: Exploring View-Invariant Video Temporal Understanding","url":"https://arxiv.org/abs/2510.26113","date":1761883200,"author":"","guid":323191,"unread":true,"content":"<article>arXiv:2510.26113v1 Announce Type: new \nAbstract: Can Video-LLMs achieve consistent temporal understanding when videos capture the same event from different viewpoints? To study this, we introduce EgoExo-Con (Consistency), a benchmark of comprehensively synchronized egocentric and exocentric video pairs with human-refined queries in natural language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal Verification and Temporal Grounding. It evaluates not only correctness but consistency across viewpoints. Our analysis reveals two critical limitations of existing Video-LLMs: (1) models often fail to maintain consistency, with results far worse than their single-view performances. (2) When naively finetuned with synchronized videos of both viewpoints, the models show improved consistency but often underperform those trained on a single view. For improvements, we propose View-GRPO, a novel reinforcement learning framework that effectively strengthens view-specific temporal reasoning while encouraging consistent comprehension across viewpoints. Our method demonstrates its superiority over naive SFT and GRPO, especially for improving cross-view consistency. All resources will be made publicly available.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shortest Paths, Convexity, and Treewidth in Regular Hyperbolic Tilings","url":"https://arxiv.org/abs/2510.26110","date":1761883200,"author":"","guid":323192,"unread":true,"content":"<article>arXiv:2510.26110v1 Announce Type: new \nAbstract: Hyperbolic tilings are natural infinite planar graphs where each vertex has degree $q$ and each face has $p$ edges for some $\\frac1p+\\frac1q&lt;\\frac12$. We study the structure of shortest paths in such graphs. We show that given a set of $n$ terminals, we can compute a so-called isometric closure (closely related to the geodesic convex hull) of the terminals in near-linear time, using a classic geometric convex hull algorithm as a black box. We show that the size of the convex hull is $O(N)$ where $N$ is the total length of the paths to the terminals from a fixed origin.\n  Furthermore, we prove that the geodesic convex hull of a set of $n$ terminals has treewidth only $\\max(12,O(\\log\\frac{n}{p + q}))$, a bound independent of the distance of the points involved. As a consequence, we obtain algorithms for subset TSP and Steiner tree with running time $O(N \\log N) + \\mathrm{poly}(\\frac{n}{p + q}) \\cdot N$.</article>","contentLength":963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error","url":"https://arxiv.org/abs/2510.26109","date":1761883200,"author":"","guid":323193,"unread":true,"content":"<article>arXiv:2510.26109v1 Announce Type: new \nAbstract: Reinforcement learning with verifiable rewards (RLVR) has significantly boosted the reasoning capability of large language models (LLMs) recently. However, existing RLVR approaches merely train LLMs based on their own generated responses and are constrained by the initial capability of LLMs, thus prone to exploration stagnation, in which LLMs fail to solve more training problems and cannot further learn from the training data. Some work tries to address this by leveraging off-policy solutions to training problems but requires external guidance from experts which suffers from limited availability. In this work, we propose LTE (Learning to reason from Trial and Error), an approach hinting LLMs with their previously self-generated incorrect answers and problem of overlong responses, which does not require any external expert guidance. Experiments validate the effectiveness of LTE, which outperforms the normal group relative policy optimization (GRPO) by 6.38 in Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the problem of exploration stagnation and enhances both exploitation and exploration during training.</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Risk of Misalignment between Text and Image in Multi-modal Model","url":"https://arxiv.org/abs/2510.26105","date":1761883200,"author":"","guid":323194,"unread":true,"content":"<article>arXiv:2510.26105v1 Announce Type: new \nAbstract: Despite the notable advancements and versatility of multi-modal diffusion models, such as text-to-image models, their susceptibility to adversarial inputs remains underexplored. Contrary to expectations, our investigations reveal that the alignment between textual and Image modalities in existing diffusion models is inadequate. This misalignment presents significant risks, especially in the generation of inappropriate or Not-Safe-For-Work (NSFW) content. To this end, we propose a novel attack called Prompt-Restricted Multi-modal Attack (PReMA) to manipulate the generated content by modifying the input image in conjunction with any specified prompt, without altering the prompt itself. PReMA is the first attack that manipulates model outputs by solely creating adversarial images, distinguishing itself from prior methods that primarily generate adversarial prompts to produce NSFW content. Consequently, PReMA poses a novel threat to the integrity of multi-modal diffusion models, particularly in image-editing applications that operate with fixed prompts. Comprehensive evaluations conducted on image inpainting and style transfer tasks across various models confirm the potent efficacy of PReMA.</article>","contentLength":1255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender","url":"https://arxiv.org/abs/2510.26104","date":1761883200,"author":"","guid":323195,"unread":true,"content":"<article>arXiv:2510.26104v1 Announce Type: new \nAbstract: In recommendation systems, scaling up feature-interaction modules (e.g., Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has achieved notable success. However, these efforts typically proceed on separate tracks, which not only hinders bidirectional information exchange but also prevents unified optimization and scaling. In this paper, we propose OneTrans, a unified Transformer backbone that simultaneously performs user-behavior sequence modeling and feature interaction. OneTrans employs a unified tokenizer to convert both sequential and non-sequential attributes into a single token sequence. The stacked OneTrans blocks share parameters across similar sequential tokens while assigning token-specific parameters to non-sequential tokens. Through causal attention and cross-request KV caching, OneTrans enables precomputation and caching of intermediate representations, significantly reducing computational costs during both training and inference. Experimental results on industrial-scale datasets demonstrate that OneTrans scales efficiently with increasing parameters, consistently outperforms strong baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.</article>","contentLength":1251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Vulnerabilities in AI-Generated Code: A Large-Scale Analysis of Public GitHub Repositories","url":"https://arxiv.org/abs/2510.26103","date":1761883200,"author":"","guid":323196,"unread":true,"content":"<article>arXiv:2510.26103v1 Announce Type: new \nAbstract: This paper presents a comprehensive empirical analysis of security vulnerabilities in AI-generated code across public GitHub repositories. We collected and analyzed 7,703 files explicitly attributed to four major AI tools: ChatGPT (91.52\\%), GitHub Copilot (7.50\\%), Amazon CodeWhisperer (0.52\\%), and Tabnine (0.46\\%). Using CodeQL static analysis, we identified 4,241 Common Weakness Enumeration (CWE) instances across 77 distinct vulnerability types. Our findings reveal that while 87.9\\% of AI-generated code does not contain identifiable CWE-mapped vulnerabilities, significant patterns emerge regarding language-specific vulnerabilities and tool performance. Python consistently exhibited higher vulnerability rates (16.18\\%-18.50\\%) compared to JavaScript (8.66\\%-8.99\\%) and TypeScript (2.50\\%-7.14\\%) across all tools. We observed notable differences in security performance, with GitHub Copilot achieving better security density for Python (1,739 LOC per CWE) and TypeScript, while ChatGPT performed better for JavaScript. Additionally, we discovered widespread use of AI tools for documentation generation (39\\% of collected files), an understudied application with implications for software maintainability. These findings extend previous work with a significantly larger dataset and provide valuable insights for developing language-specific and context-aware security practices for the responsible integration of AI-generated code into software development workflows.</article>","contentLength":1530,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PEEL: A Poisoning-Exposing Encoding Theoretical Framework for Local Differential Privacy","url":"https://arxiv.org/abs/2510.26102","date":1761883200,"author":"","guid":323197,"unread":true,"content":"<article>arXiv:2510.26102v1 Announce Type: new \nAbstract: Local Differential Privacy (LDP) is a widely adopted privacy-protection model in the Internet of Things (IoT) due to its lightweight, decentralized, and scalable nature. However, it is vulnerable to poisoning attacks, and existing defenses either incur prohibitive resource overheads or rely on domain-specific prior knowledge, limiting their practical deployment. To address these limitations, we propose PEEL, a Poisoning-Exposing Encoding theoretical framework for LDP, which departs from resource- or prior-dependent countermeasures and instead leverages the inherent structural consistency of LDP-perturbed data. As a non-intrusive post-processing module, PEEL amplifies stealthy poisoning effects by re-encoding LDP-perturbed data via sparsification, normalization, and low-rank projection, thereby revealing both output and rule poisoning attacks through structural inconsistencies in the reconstructed space. Theoretical analysis proves that PEEL, integrated with LDP, retains unbiasedness and statistical accuracy, while being robust to expose both output and rule poisoning attacks. Moreover, evaluation results show that LDP-integrated PEEL not only outperforms four state-of-the-art defenses in terms of poisoning exposure accuracy but also significantly reduces client-side computational costs, making it highly suitable for large-scale IoT deployments.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback","url":"https://arxiv.org/abs/2510.26101","date":1761883200,"author":"","guid":323198,"unread":true,"content":"<article>arXiv:2510.26101v1 Announce Type: new \nAbstract: Large language models (LLMs) have increasingly been applied to automatic programming code generation. This task can be viewed as a language generation task that bridges natural language, human knowledge, and programming logic. However, it remains underexplored in domains that require interaction with hardware devices, such as quantum programming, where human coders write Python code that is executed on a quantum computer. To address this gap, we introduce QCoder Benchmark, an evaluation framework that assesses LLMs on quantum programming with feedback from simulated hardware devices. Our benchmark offers two key features. First, it supports evaluation using a quantum simulator environment beyond conventional Python execution, allowing feedback of domain-specific metrics such as circuit depth, execution time, and error classification, which can be used to guide better generation. Second, it incorporates human-written code submissions collected from real programming contests, enabling both quantitative comparisons and qualitative analyses of LLM outputs against human-written codes. Our experiments reveal that even advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting the difficulty of the benchmark. In contrast, reasoning-based models such as o3 reach up to 78% accuracy, outperforming averaged success rates of human-written codes (39.98%). We release the QCoder Benchmark dataset and public evaluation API to support further research.</article>","contentLength":1527,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth","url":"https://arxiv.org/abs/2510.26099","date":1761883200,"author":"","guid":323199,"unread":true,"content":"<article>arXiv:2510.26099v1 Announce Type: new \nAbstract: The dominant paradigm in machine learning is to assess model performance based on average loss across all samples in some test set. This amounts to averaging performance geospatially across the Earth in weather and climate settings, failing to account for the non-uniform distribution of human development and geography. We introduce Stratified Assessments of Forecasts over Earth (SAFE), a package for elucidating the stratified performance of a set of predictions made over Earth. SAFE integrates various data domains to stratify by different attributes associated with geospatial gridpoints: territory (usually country), global subregion, income, and landcover (land or water). This allows us to examine the performance of models for each individual stratum of the different attributes (e.g., the accuracy in every individual country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of state-of-the-art AI-based weather prediction models, finding that they all exhibit disparities in forecasting skill across every attribute. We use this to seed a benchmark of model forecast fairness through stratification at different lead times for various climatic variables. By moving beyond globally-averaged metrics, we for the first time ask: where do models perform best or worst, and which models are most fair? To support further work in this direction, the SAFE package is open source and available at https://github.com/N-Masi/safe</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks","url":"https://arxiv.org/abs/2510.26098","date":1761883200,"author":"","guid":323200,"unread":true,"content":"<article>arXiv:2510.26098v1 Announce Type: new \nAbstract: Large vision language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine tuning and reinforcement learning) alone cannot fully address. By analyzing common failure patterns in GUI task execution, we distill GUI knowledge into three dimensions: (1) interface perception, knowledge about recognizing widgets and system states; (2) interaction prediction, knowledge about reasoning action state transitions; and (3) instruction understanding, knowledge about planning, verifying, and assessing task completion progress. We further introduce GUI Knowledge Bench, a benchmark with multiple choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, IOS) and 292 applications. Our evaluation shows that current VLMs identify widget functions but struggle with perceiving system states, predicting actions, and verifying task completion. Experiments on real world GUI tasks further validate the close link between GUI knowledge and task success. By providing a structured framework for assessing GUI knowledge, our work supports the selection of VLMs with greater potential prior to downstream training and provides insights for building more capable GUI agents.</article>","contentLength":1405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio-Language Models","url":"https://arxiv.org/abs/2510.26096","date":1761883200,"author":"","guid":323201,"unread":true,"content":"<article>arXiv:2510.26096v1 Announce Type: new \nAbstract: Recent advances in Audio-Language Models (ALMs) have significantly improved multimodal understanding capabilities. However, the introduction of the audio modality also brings new and unique vulnerability vectors. Previous studies have proposed jailbreak attacks that specifically target ALMs, revealing that defenses directly transferred from traditional audio adversarial attacks or text-based Large Language Model (LLM) jailbreaks are largely ineffective against these ALM-specific threats. To address this issue, we propose ALMGuard, the first defense framework tailored to ALMs. Based on the assumption that safety-aligned shortcuts naturally exist in ALMs, we design a method to identify universal Shortcut Activation Perturbations (SAPs) that serve as triggers that activate the safety shortcuts to safeguard ALMs at inference time. To better sift out effective triggers while preserving the model's utility on benign tasks, we further propose Mel-Gradient Sparse Mask (M-GSM), which restricts perturbations to Mel-frequency bins that are sensitive to jailbreaks but insensitive to speech understanding. Both theoretical analyses and empirical results demonstrate the robustness of our method against both seen and unseen attacks. Overall, \\MethodName reduces the average success rate of advanced ALM-specific jailbreak attacks to 4.6% across four models, while maintaining comparable utility on benign benchmarks, establishing it as the new state of the art. Our code and data are available at https://github.com/WeifeiJin/ALMGuard.</article>","contentLength":1588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ORBIT - Open Recommendation Benchmark for Reproducible Research with Hidden Tests","url":"https://arxiv.org/abs/2510.26095","date":1761883200,"author":"","guid":323202,"unread":true,"content":"<article>arXiv:2510.26095v1 Announce Type: new \nAbstract: Recommender systems are among the most impactful AI applications, interacting with billions of users every day, guiding them to relevant products, services, or information tailored to their preferences. However, the research and development of recommender systems are hindered by existing datasets that fail to capture realistic user behaviors and inconsistent evaluation settings that lead to ambiguous conclusions. This paper introduces the Open Recommendation Benchmark for Reproducible Research with HIdden Tests (ORBIT), a unified benchmark for consistent and realistic evaluation of recommendation models. ORBIT offers a standardized evaluation framework of public datasets with reproducible splits and transparent settings for its public leaderboard. Additionally, ORBIT introduces a new webpage recommendation task, ClueWeb-Reco, featuring web browsing sequences from 87 million public, high-quality webpages. ClueWeb-Reco is a synthetic dataset derived from real, user-consented, and privacy-guaranteed browsing data. It aligns with modern recommendation scenarios and is reserved as the hidden test part of our leaderboard to challenge recommendation models' generalization ability. ORBIT measures 12 representative recommendation models on its public benchmark and introduces a prompted LLM baseline on the ClueWeb-Reco hidden test. Our benchmark results reflect general improvements of recommender systems on the public datasets, with variable individual performances. The results on the hidden test reveal the limitations of existing approaches in large-scale webpage recommendation and highlight the potential for improvements with LLM integrations. ORBIT benchmark, leaderboard, and codebase are available at https://www.open-reco-bench.ai.</article>","contentLength":1804,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lean4Physics: Comprehensive Reasoning Framework for College-level Physics in Lean4","url":"https://arxiv.org/abs/2510.26094","date":1761883200,"author":"","guid":323203,"unread":true,"content":"<article>arXiv:2510.26094v1 Announce Type: new \nAbstract: We present **Lean4PHYS**, a comprehensive reasoning framework for college-level physics problems in Lean4. **Lean4PHYS** includes *LeanPhysBench*, a college-level benchmark for formal physics reasoning in Lean4, which contains 200 hand-crafted and peer-reviewed statements derived from university textbooks and physics competition problems. To establish a solid foundation for formal reasoning in physics, we also introduce *PhysLib*, a community-driven repository containing fundamental unit systems and theorems essential for formal physics reasoning. Based on the benchmark and Lean4 repository we composed in **Lean4PHYS**, we report baseline results using major expert Math Lean4 provers and state-of-the-art closed-source models, with the best performance of DeepSeek-Prover-V2-7B achieving only 16% and Claude-Sonnet-4 achieving 35%. We also conduct a detailed analysis showing that our *PhysLib* can achieve an average improvement of 11.75% in model performance. This demonstrates the challenging nature of our *LeanPhysBench* and the effectiveness of *PhysLib*. To the best of our knowledge, this is the first study to provide a physics benchmark in Lean4.</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Signed Graph Unlearning","url":"https://arxiv.org/abs/2510.26092","date":1761883200,"author":"","guid":323204,"unread":true,"content":"<article>arXiv:2510.26092v1 Announce Type: new \nAbstract: The proliferation of signed networks in contemporary social media platforms necessitates robust privacy-preserving mechanisms. Graph unlearning, which aims to eliminate the influence of specific data points from trained models without full retraining, becomes particularly critical in these scenarios where user interactions are sensitive and dynamic. Existing graph unlearning methodologies are exclusively designed for unsigned networks and fail to account for the unique structural properties of signed graphs. Their naive application to signed networks neglects edge sign information, leading to structural imbalance across subgraphs and consequently degrading both model performance and unlearning efficiency. This paper proposes SGU (Signed Graph Unlearning), a graph unlearning framework specifically for signed networks. SGU incorporates a new graph unlearning partition paradigm and a novel signed network partition algorithm that preserve edge sign information during partitioning and ensure structural balance across partitions. Compared with baselines, SGU achieves state-of-the-art results in both model performance and unlearning efficiency.</article>","contentLength":1204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing","url":"https://arxiv.org/abs/2510.26089","date":1761883200,"author":"","guid":323205,"unread":true,"content":"<article>arXiv:2510.26089v1 Announce Type: new \nAbstract: Traffic congestion in urban road networks leads to longer trip times and higher emissions, especially during peak periods. While the Shortest Path First (SPF) algorithm is optimal for a single vehicle in a static network, it performs poorly in dynamic, multi-vehicle settings, often worsening congestion by routing all vehicles along identical paths. We address dynamic vehicle routing through a multi-agent reinforcement learning (MARL) framework for coordinated, network-aware fleet navigation. We first propose Adaptive Navigation (AN), a decentralized MARL model where each intersection agent provides routing guidance based on (i) local traffic and (ii) neighborhood state modeled using Graph Attention Networks (GAT). To improve scalability in large networks, we further propose Hierarchical Hub-based Adaptive Navigation (HHAN), an extension of AN that assigns agents only to key intersections (hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles micro-routing within each hub region. For hub coordination, HHAN adopts centralized training with decentralized execution (CTDE) under the Attentive Q-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions via attention. Hub agents use flow-aware state features that combine local congestion and predictive dynamics for proactive routing. Experiments on synthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces average travel time versus SPF and learning baselines, maintaining 100% routing success. HHAN scales to networks with hundreds of intersections, achieving up to 15.9% improvement under heavy traffic. These findings highlight the potential of network-constrained MARL for scalable, coordinated, and congestion-aware routing in intelligent transportation systems.</article>","contentLength":1841,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline","url":"https://arxiv.org/abs/2510.26086","date":1761883200,"author":"","guid":323206,"unread":true,"content":"<article>arXiv:2510.26086v1 Announce Type: new \nAbstract: Bug bisection has been an important security task that aims to understand the range of software versions impacted by a bug, i.e., identifying the commit that introduced the bug. However, traditional patch-based bisection methods are faced with several significant barriers: For example, they assume that the bug-inducing commit (BIC) and the patch commit modify the same functions, which is not always true. They often rely solely on code changes, while the commit message frequently contains a wealth of vulnerability-related information. They are also based on simple heuristics (e.g., assuming the BIC initializes lines deleted in the patch) and lack any logical analysis of the vulnerability.\n  In this paper, we make the observation that Large Language Models (LLMs) are well-positioned to break the barriers of existing solutions, e.g., comprehend both textual data and code in patches and commits. Unlike previous BIC identification approaches, which yield poor results, we propose a comprehensive multi-stage pipeline that leverages LLMs to: (1) fully utilize patch information, (2) compare multiple candidate commits in context, and (3) progressively narrow down the candidates through a series of down-selection steps. In our evaluation, we demonstrate that our approach achieves significantly better accuracy than the state-of-the-art solution by more than 38\\%. Our results further confirm that the comprehensive multi-stage pipeline is essential, as it improves accuracy by 60\\% over a baseline LLM-based bisection method.</article>","contentLength":1584,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nirvana: A Specialized Generalist Model With Task-Aware Memory Mechanism","url":"https://arxiv.org/abs/2510.26083","date":1761883200,"author":"","guid":323207,"unread":true,"content":"<article>arXiv:2510.26083v1 Announce Type: new \nAbstract: Specialized Generalist Models (SGMs) aim to preserve broad capabilities while achieving expert-level performance in target domains. However, traditional LLM structures including Transformer, Linear Attention, and hybrid models do not employ specialized memory mechanism guided by task information. In this paper, we present Nirvana, an SGM with specialized memory mechanism, linear time complexity, and test-time task information extraction. Besides, we propose the Task-Aware Memory Trigger ($\\textit{Trigger}$) that flexibly adjusts memory mechanism based on the current task's requirements. In Trigger, each incoming sample is treated as a self-supervised fine-tuning task, enabling Nirvana to adapt its task-related parameters on the fly to domain shifts. We also design the Specialized Memory Updater ($\\textit{Updater}$) that dynamically memorizes the context guided by Trigger. We conduct experiments on both general language tasks and specialized medical tasks. On a variety of natural language modeling benchmarks, Nirvana achieves competitive or superior results compared to the existing LLM structures. To prove the effectiveness of Trigger on specialized tasks, we test Nirvana's performance on a challenging medical task, i.e., Magnetic Resonance Imaging (MRI). We post-train frozen Nirvana backbone with lightweight codecs on paired electromagnetic signals and MRI images. Despite the frozen Nirvana backbone, Trigger guides the model to adapt to the MRI domain with the change of task-related parameters. Nirvana achieves higher-quality MRI reconstruction compared to conventional MRI models as well as the models with traditional LLMs' backbone, and can also generate accurate preliminary clinical reports accordingly.</article>","contentLength":1783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond the Uncanny Valley: A Mixed-Method Investigation of Anthropomorphism in Protective Responses to Robot Abuse","url":"https://arxiv.org/abs/2510.26082","date":1761883200,"author":"","guid":323208,"unread":true,"content":"<article>arXiv:2510.26082v1 Announce Type: new \nAbstract: Robots with anthropomorphic features are increasingly shaping how humans perceive and morally engage with them. Our research investigates how different levels of anthropomorphism influence protective responses to robot abuse, extending the Computers as Social Actors (CASA) and uncanny valley theories into a moral domain. In an experiment, we invite 201 participants to view videos depicting abuse toward a robot with low (Spider), moderate (Two-Foot), or high (Humanoid) anthropomorphism. To provide a comprehensive analysis, we triangulate three modalities: self-report surveys measuring emotions and uncanniness, physiological data from automated facial expression analysis, and qualitative reflections. Findings indicate that protective responses are not linear. The moderately anthropomorphic Two-Foot robot, rated highest in eeriness and \"spine-tingling\" sensations consistent with the uncanny valley, elicited the strongest physiological anger expressions. Self-reported anger and guilt are significantly higher for both the Two-Foot and Humanoid robots compared to the Spider. Qualitative findings further reveal that as anthropomorphism increases, moral reasoning shifts from technical assessments of property damage to condemnation of the abuser's character, while governance proposals expand from property law to calls for quasi-animal rights and broader societal responsibility. These results suggest that the uncanny valley does not dampen moral concern but paradoxically heightens protective impulses, offering critical implications for robot design, policy, and future legal frameworks.</article>","contentLength":1651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I don't Want You to Die: A Shared Responsibility Framework for Safeguarding Child-Robot Companionship","url":"https://arxiv.org/abs/2510.26080","date":1761883200,"author":"","guid":323209,"unread":true,"content":"<article>arXiv:2510.26080v1 Announce Type: new \nAbstract: Social robots like Moxie are designed to form strong emotional bonds with children, but their abrupt discontinuation can cause significant struggles and distress to children. When these services end, the resulting harm raises complex questions of who bears responsibility when children's emotional bonds are broken. Using the Moxie shutdown as a case study through a qualitative survey of 72 U.S. participants, our findings show that the responsibility is viewed as a shared duty across the robot company, parents, developers, and government. However, these attributions varied by political ideology and parental status of whether they have children. Participants' perceptions of whether the robot service should continue are highly polarized; supporters propose technical, financial, and governmental pathways for continuity, while opponents cite business realities and risks of unhealthy emotional dependency. Ultimately, this research contributes an empirically grounded shared responsibility framework for safeguarding child-robot companionship by detailing how accountability is distributed and contested, informing concrete design and policy implications to mitigate the emotional harm of robot discontinuation.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Money: A Systematic Review of Synthetic Data Generation for Finance","url":"https://arxiv.org/abs/2510.26076","date":1761883200,"author":"","guid":323210,"unread":true,"content":"<article>arXiv:2510.26076v1 Announce Type: new \nAbstract: Synthetic data generation has emerged as a promising approach to address the challenges of using sensitive financial data in machine learning applications. By leveraging generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), it is possible to create artificial datasets that preserve the statistical properties of real financial records while mitigating privacy risks and regulatory constraints. Despite the rapid growth of this field, a comprehensive synthesis of the current research landscape has been lacking. This systematic review consolidates and analyses 72 studies published since 2018 that focus on synthetic financial data generation. We categorise the types of financial information synthesised, the generative methods employed, and the evaluation strategies used to assess data utility and privacy. The findings indicate that GAN-based approaches dominate the literature, particularly for generating time-series market data and tabular credit data. While several innovative techniques demonstrate potential for improved realism and privacy preservation, there remains a notable lack of rigorous evaluation of privacy safeguards across studies. By providing an integrated overview of generative techniques, applications, and evaluation methods, this review highlights critical research gaps and offers guidance for future work aimed at developing robust, privacy-preserving synthetic data solutions for the financial domain.</article>","contentLength":1533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FGGM: Formal Grey-box Gradient Method for Attacking DRL-based MU-MIMO Scheduler","url":"https://arxiv.org/abs/2510.26075","date":1761883200,"author":"","guid":323211,"unread":true,"content":"<article>arXiv:2510.26075v1 Announce Type: new \nAbstract: In 5G mobile communication systems, MU-MIMO has been applied to enhance spectral efficiency and support high data rates. To maximize spectral efficiency while providing fairness among users, the base station (BS) needs to selects a subset of users for data transmission. Given that this problem is NP-hard, DRL-based methods have been proposed to infer the near-optimal solutions in real-time, yet this approach has an intrinsic security problem. This paper investigates how a group of adversarial users can exploit unsanitized raw CSIs to launch a throughput degradation attack. Most existing studies only focused on systems in which adversarial users can obtain the exact values of victims' CSIs, but this is impractical in the case of uplink transmission in LTE/5G mobile systems. We note that the DRL policy contains an observation normalizer which has the mean and variance of the observation to improve training convergence. Adversarial users can then estimate the upper and lower bounds of the local observations including the CSIs of victims based solely on that observation normalizer. We develop an attacking scheme FGGM by leveraging polytope abstract domains, a technique used to bound the outputs of a neural network given the input ranges. Our goal is to find one set of intentionally manipulated CSIs which can achieve the attacking goals for the whole range of local observations of victims. Experimental results demonstrate that FGGM can determine a set of adversarial CSI vector controlled by adversarial users, then reuse those CSIs throughout the simulation to reduce the network throughput of a victim up to 70\\% without knowing the exact value of victims' local observations. This study serves as a case study and can be applied to many other DRL-based problems, such as a knapsack-oriented resource allocation problems.</article>","contentLength":1891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symmetry-Driven Asynchronous Forwarding for Reliable Distributed Coordination in Toroidal Networks","url":"https://arxiv.org/abs/2510.26071","date":1761883200,"author":"","guid":323212,"unread":true,"content":"<article>arXiv:2510.26071v1 Announce Type: new \nAbstract: The proliferation of large-scale distributed systems, such as satellite constellations and high-performance computing clusters, demands robust communication primitives that maintain coordination under unreliable links. The torus topology, with its inherent rotational and reflection symmetries, is a prevalent architecture in these domains. However, conventional routing schemes suffer from substantial packet loss during control-plane synchronization after link failures. This paper introduces a symmetry-driven asynchronous forwarding mechanism that leverages the torus's geometric properties to achieve reliable packet delivery without control-plane coordination. We model packet flow using a topological potential gradient and demonstrate that symmetry-breaking failures naturally induce a reverse flow, which we harness for fault circumvention. We propose two local forwarding strategies, Reverse Flow with Counter-facing Priority (RF-CF) and Lateral-facing Priority (RF-LF), that guarantee reachability to the destination via forward-flow phase transition points, without protocol modifications or additional in-packet overhead. Through percolation analysis and packet-level simulations on a 16 x 16 torus, we show that our mechanism reduces packet loss by up to 17.5% under a 1% link failure rate, with the RF-LF strategy contributing to 28% of successfully delivered packets. This work establishes a foundational link between topological symmetry and communication resilience, providing a lightweight, protocol-agnostic substrate for enhancing distributed systems.</article>","contentLength":1621,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interaction-Augmented Instruction: Modeling the Synergy of Prompts and Interactions in Human-GenAI Collaboration","url":"https://arxiv.org/abs/2510.26069","date":1761883200,"author":"","guid":323213,"unread":true,"content":"<article>arXiv:2510.26069v1 Announce Type: new \nAbstract: Text prompt is the most common way for human-generative AI (GenAI) communication. Though convenient, it is challenging to convey fine-grained and referential intent. One promising solution is to combine text prompts with precise GUI interactions, like brushing and clicking. However, there lacks a formal model to model synergistic designs between prompts and interactions, hindering their comparison and innovation. To fill this gap, via an iterative and deductive process, we develop the Interaction-Augmented Instruction (IAI) model, a compact entity-relation graph formalizing how the combination of interactions and text prompts enhances human-generative AI communication. With the model, we distill twelve recurring and composable atomic interaction paradigms from prior tools, verifying our model's capability to facilitate systematic design characterization and comparison. Case studies further demonstrate the model's utility in applying, refining, and extending these paradigms. These results illustrate our IAI model's descriptive, discriminative, and generative power for shaping future GenAI systems.</article>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization","url":"https://arxiv.org/abs/2510.26068","date":1761883200,"author":"","guid":323214,"unread":true,"content":"<article>arXiv:2510.26068v1 Announce Type: new \nAbstract: This paper proposes a novel paradigm for machine learning that moves beyond traditional parameter optimization. Unlike conventional approaches that search for optimal parameters within a fixed geometric space, our core idea is to treat the model itself as a malleable geometric entity. Specifically, we optimize the metric tensor field on a manifold with a predefined topology, thereby dynamically shaping the geometric structure of the model space. To achieve this, we construct a variational framework whose loss function carefully balances data fidelity against the intrinsic geometric complexity of the manifold. The former ensures the model effectively explains observed data, while the latter acts as a regularizer, penalizing overly curved or irregular geometries to encourage simpler models and prevent overfitting. To address the computational challenges of this infinite-dimensional optimization problem, we introduce a practical method based on discrete differential geometry: the continuous manifold is discretized into a triangular mesh, and the metric tensor is parameterized by edge lengths, enabling efficient optimization using automatic differentiation tools. Theoretical analysis reveals a profound analogy between our framework and the Einstein-Hilbert action in general relativity, providing an elegant physical interpretation for the concept of \"data-driven geometry\". We further argue that even with fixed topology, metric optimization offers significantly greater expressive power than models with fixed geometry. This work lays a solid foundation for constructing fully dynamic \"meta-learners\" capable of autonomously evolving their geometry and topology, and it points to broad application prospects in areas such as scientific model discovery and robust representation learning.</article>","contentLength":1854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion","url":"https://arxiv.org/abs/2510.26067","date":1761883200,"author":"","guid":323215,"unread":true,"content":"<article>arXiv:2510.26067v1 Announce Type: new \nAbstract: Tensegrity robots combine rigid rods and elastic cables, offering high resilience and deployability but posing major challenges for locomotion control due to their underactuated and highly coupled dynamics. This paper introduces a morphology-aware reinforcement learning framework that integrates a graph neural network (GNN) into the Soft Actor-Critic (SAC) algorithm. By representing the robot's physical topology as a graph, the proposed GNN-based policy captures coupling among components, enabling faster and more stable learning than conventional multilayer perceptron (MLP) policies. The method is validated on a physical 3-bar tensegrity robot across three locomotion primitives, including straight-line tracking and bidirectional turning. It shows superior sample efficiency, robustness to noise and stiffness variations, and improved trajectory accuracy. Notably, the learned policies transfer directly from simulation to hardware without fine-tuning, achieving stable real-world locomotion. These results demonstrate the advantages of incorporating structural priors into reinforcement learning for tensegrity robot control.</article>","contentLength":1184,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Scaling Laws for Symbolic Regression","url":"https://arxiv.org/abs/2510.26064","date":1761883200,"author":"","guid":323216,"unread":true,"content":"<article>arXiv:2510.26064v1 Announce Type: new \nAbstract: Symbolic regression (SR) aims to discover the underlying mathematical expressions that explain observed data. This holds promise for both gaining scientific insight and for producing inherently interpretable and generalizable models for tabular data. In this work we focus on the basics of SR. Deep learning-based SR has recently become competitive with genetic programming approaches, but the role of scale has remained largely unexplored. Inspired by scaling laws in language modeling, we present the first systematic investigation of scaling in SR, using a scalable end-to-end transformer pipeline and carefully generated training data. Across five different model sizes and spanning three orders of magnitude in compute, we find that both validation loss and solved rate follow clear power-law trends with compute. We further identify compute-optimal hyperparameter scaling: optimal batch size and learning rate grow with model size, and a token-to-parameter ratio of $\\approx$15 is optimal in our regime, with a slight upward trend as compute increases. These results demonstrate that SR performance is largely predictable from compute and offer important insights for training the next generation of SR models.</article>","contentLength":1265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Scenario-Based Approach for Stochastic Economic Model Predictive Control with an Expected Shortfall Constraint","url":"https://arxiv.org/abs/2510.26063","date":1761883200,"author":"","guid":323217,"unread":true,"content":"<article>arXiv:2510.26063v1 Announce Type: new \nAbstract: This paper presents a novel approach to stochastic economic model predictive control (SEMPC) that minimizes average economic cost while satisfying an empirical expected shortfall (EES) constraint to manage risk. A new scenario-based problem formulation ensuring controlled risk with high confidence while minimizing the average cost is introduced. The probabilistic guarantees is dependent on the number of support elements over the entire input domain, which is difficult to find for high-dimensional systems. A heuristic algorithm is proposed to find the number of support elements. Finally, an efficient method is presented to reduce the computational complexity of the SEMPC problem with an EES constraint. The approach is validated on a water distribution network, showing its effectiveness in balancing performance and risk.</article>","contentLength":879,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Performance Analysis of Dynamic Equilibria in Joint Path Selection and Congestion Control","url":"https://arxiv.org/abs/2510.26060","date":1761883200,"author":"","guid":323218,"unread":true,"content":"<article>arXiv:2510.26060v1 Announce Type: new \nAbstract: Path-aware networking, a cornerstone of next-generation architectures like SCION and Multipath QUIC, empowers end-hosts with fine-grained control over traffic forwarding. This capability, however, introduces a critical stability risk: uncoordinated, greedy path selection by a multitude of agents can induce persistent, high-amplitude network oscillations. While this phenomenon is well-known, its quantitative performance impact across key metrics has remained poorly understood. In this paper, we address this gap by developing the first axiomatic framework for analyzing the joint dynamics of path selection and congestion control. Our model enables the formal characterization of the system's dynamic equilibria-the stable, periodic patterns of oscillation-and provides a suite of axioms to rate their performance in terms of efficiency, loss avoidance, convergence, fairness, and responsiveness. Our analysis reveals a fundamental trade-off in protocol design between predictable performance (efficiency, convergence) and user-centric goals (fairness, responsiveness). We prove, however, that no such trade-off exists among efficiency, convergence, and loss avoidance, which can be simultaneously optimized through careful parameter tuning. Furthermore, we find that agent migration can, counter-intuitively, enhance stability by de-synchronizing traffic, a theoretical result validated by our simulations. These findings provide a principled design map for engineering robust, high-performance protocols for the future path-aware Internet.</article>","contentLength":1594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can AI be Accountable?","url":"https://arxiv.org/abs/2510.26057","date":1761883200,"author":"","guid":323219,"unread":true,"content":"<article>arXiv:2510.26057v1 Announce Type: new \nAbstract: The AI we use is powerful, and its power is increasing rapidly. If this powerful AI is to serve the needs of consumers, voters, and decision makers, then it is imperative that the AI is accountable. In general, an agent is accountable to a forum if the forum can request information from the agent about its actions, if the forum and the agent can discuss this information, and if the forum can sanction the agent. Unfortunately, in too many cases today's AI is not accountable -- we cannot question it, enter into a discussion with it, let alone sanction it. In this chapter we relate the general definition of accountability to AI, we illustrate what it means for AI to be accountable and unaccountable, and we explore approaches that can improve our chances of living in a world where all AI is accountable to those who are affected by it.</article>","contentLength":891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NP-Hardness of Approximating Nash Social Welfare with Supermodular Valuations","url":"https://arxiv.org/abs/2510.26055","date":1761883200,"author":"","guid":323220,"unread":true,"content":"<article>arXiv:2510.26055v1 Announce Type: new \nAbstract: We study the problem of allocating a set of indivisible items to agents with supermodular utilities to maximize the Nash social welfare. We show that the problem is NP-hard for any approximation factor.</article>","contentLength":251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic VLM-Guided Negative Prompting for Diffusion Models","url":"https://arxiv.org/abs/2510.26052","date":1761883200,"author":"","guid":323221,"unread":true,"content":"<article>arXiv:2510.26052v1 Announce Type: new \nAbstract: We propose a novel approach for dynamic negative prompting in diffusion models that leverages Vision-Language Models (VLMs) to adaptively generate negative prompts during the denoising process. Unlike traditional Negative Prompting methods that use fixed negative prompts, our method generates intermediate image predictions at specific denoising steps and queries a VLM to produce contextually appropriate negative prompts. We evaluate our approach on various benchmark datasets and demonstrate the trade-offs between negative guidance strength and text-image alignment.</article>","contentLength":620,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation","url":"https://arxiv.org/abs/2510.26049","date":1761883200,"author":"","guid":323222,"unread":true,"content":"<article>arXiv:2510.26049v1 Announce Type: new \nAbstract: Elbow and wrist fractures are the most common fractures in pediatric populations. Automatic segmentation of musculoskeletal structures in ultrasound (US) can improve diagnostic accuracy and treatment planning. Fractures appear as cortical defects but require expert interpretation. Deep learning (DL) can provide real-time feedback and highlight key structures, helping lightly trained users perform exams more confidently. However, pixel-wise expert annotations for training remain time-consuming and costly. To address this challenge, we propose FlexICL, a novel and flexible in-context learning (ICL) framework for segmenting bony regions in US images. We apply it to an intra-video segmentation setting, where experts annotate only a small subset of frames, and the model segments unseen frames. We systematically investigate various image concatenation techniques and training strategies for visual ICL and introduce novel concatenation methods that significantly enhance model performance with limited labeled data. By integrating multiple augmentation strategies, FlexICL achieves robust segmentation performance across four wrist and elbow US datasets while requiring only 5% of the training images. It outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and conventional segmentation models like U-Net and TransUNet by 1-27% Dice coefficient on 1,252 US sweeps. These initial results highlight the potential of FlexICL as an efficient and scalable solution for US image segmentation well suited for medical imaging use cases where labeled data is scarce.</article>","contentLength":1632,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FractalBrain: A Neuro-interactive Virtual Reality Experience using Electroencephalogram (EEG) for Mindfulness","url":"https://arxiv.org/abs/2510.26041","date":1761883200,"author":"","guid":323223,"unread":true,"content":"<article>arXiv:2510.26041v1 Announce Type: new \nAbstract: Mindfulness has been studied and practiced in enhancing psychological well-being while reducing neuroticism and psychopathological indicators. However, practicing mindfulness with continuous attention is challenging, especially for beginners. In the proposed system, FractalBrain, we utilize an interactive audiovisual fractal with a geometric repetitive pattern that has been demonstrated to induce meditative effects. FractalBrain presents an experience combining a surreal virtual reality (VR) program with an electroencephalogram (EEG) interface. While viewing an ever-changing fractal-inspired artwork in an immersive environment, the user's EEG stream is analyzed and mapped into VR. These EEG data adaptively manipulates the audiovisual parameters in real-time, generating a distinct experience for each user. The pilot feedback suggests the potential of the FractalBrain to facilitate mindfulness and enhance attention.</article>","contentLength":976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods","url":"https://arxiv.org/abs/2510.26040","date":1761883200,"author":"","guid":323224,"unread":true,"content":"<article>arXiv:2510.26040v1 Announce Type: new \nAbstract: While autonomous racing performance in Time-Trial scenarios has seen significant progress and development, autonomous wheel-to-wheel racing and overtaking are still severely limited. These limitations are particularly apparent in real-life driving scenarios where state-of-the-art algorithms struggle to safely or reliably complete overtaking manoeuvres. This is important, as reliable navigation around other vehicles is vital for safe autonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful opportunity for developing wheel-to-wheel racing algorithms on a standardised physical platform. The competition format makes it possible to evaluate overtaking and wheel-to-wheel racing algorithms against the state-of-the-art. This research presents a novel racing and overtaking agent capable of learning to reliably navigate a track and overtake opponents in both simulation and reality. The agent was deployed on an F1Tenth vehicle and competed against opponents running varying competitive algorithms in the real world. The results demonstrate that the agent's training against opponents enables deliberate overtaking behaviours with an overtaking rate of 87% compared 56% for an agent trained just to race.</article>","contentLength":1274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods","url":"https://arxiv.org/abs/2510.26038","date":1761883200,"author":"","guid":323225,"unread":true,"content":"<article>arXiv:2510.26038v1 Announce Type: new \nAbstract: Knowledge distillation (KD) is an effective method for model compression and transferring knowledge between models. However, its effect on model's robustness against spurious correlations that degrade performance on out-of-distribution data remains underexplored. This study investigates the effect of knowledge distillation on the transferability of ``debiasing'' capabilities from teacher models to student models on natural language inference (NLI) and image classification tasks. Through extensive experiments, we illustrate several key findings: (i) overall the debiasing capability of a model is undermined post-KD; (ii) training a debiased model does not benefit from injecting teacher knowledge; (iii) although the overall robustness of a model may remain stable post-distillation, significant variations can occur across different types of biases; and (iv) we pin-point the internal attention pattern and circuit that causes the distinct behavior post-KD. Given the above findings, we propose three effective solutions to improve the distillability of debiasing methods: developing high quality data for augmentation, implementing iterative knowledge distillation, and initializing student models with weights obtained from teacher models. To the best of our knowledge, this is the first study on the effect of KD on debiasing and its interenal mechanism at scale. Our findings provide understandings on how KD works and how to design better debiasing methods.</article>","contentLength":1518,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SIRAJ: Diverse and Efficient Red-Teaming for LLM Agents via Distilled Structured Reasoning","url":"https://arxiv.org/abs/2510.26037","date":1761883200,"author":"","guid":323226,"unread":true,"content":"<article>arXiv:2510.26037v1 Announce Type: new \nAbstract: The ability of LLM agents to plan and invoke tools exposes them to new safety risks, making a comprehensive red-teaming system crucial for discovering vulnerabilities and ensuring their safe deployment. We present SIRAJ: a generic red-teaming framework for arbitrary black-box LLM agents. We employ a dynamic two-step process that starts with an agent definition and generates diverse seed test cases that cover various risk outcomes, tool-use trajectories, and risk sources. Then, it iteratively constructs and refines model-based adversarial attacks based on the execution trajectories of former attempts. To optimize the red-teaming cost, we present a model distillation approach that leverages structured forms of a teacher model's reasoning to train smaller models that are equally effective. Across diverse evaluation agent settings, our seed test case generation approach yields 2 -- 2.5x boost to the coverage of risk outcomes and tool-calling trajectories. Our distilled 8B red-teamer model improves attack success rate by 100%, surpassing the 671B Deepseek-R1 model. Our ablations and analyses validate the effectiveness of the iterative framework, structured reasoning, and the generalization of our red-teamer models.</article>","contentLength":1278,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Competitive Equilibrium for Electricity Markets with Spatially Flexible Load","url":"https://arxiv.org/abs/2510.26036","date":1761883200,"author":"","guid":323227,"unread":true,"content":"<article>arXiv:2510.26036v1 Announce Type: new \nAbstract: Electric vehicle charging and geo-distributed datacenters introduce spatially flexible loads (FLs) that couple power, transportation, and datacenter networks. These couplings create a closed-loop feedback between locational marginal prices (LMPs) and decisions of the FL systems, challenging the foundations of conventional competitive equilibrium (CE) in electricity markets. This paper studies a notion of generalized competitive equilibrium (GCE) that aims to capture such price-demand interactions across the interconnected infrastructures. We establish structural conditions under which the GCE preserves key properties of the conventional CE, including existence, uniqueness, and efficiency, without requiring detailed knowledge of decision processes for individual FL systems. The framework generalizes to settings where the grid is coupled with multiple FL systems. Stylized examples and case studies on the New York ISO grid, coupled with the Sioux Falls transportation and distributed datacenter networks, demonstrate the use of our theoretical framework and illustrate the mutual influence among the grid and the studied FL systems.</article>","contentLength":1192,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Engineering Social Optimality via Utility Shaping in Non-Cooperative Games under Incomplete Information and Imperfect Monitoring","url":"https://arxiv.org/abs/2510.26033","date":1761883200,"author":"","guid":323228,"unread":true,"content":"<article>arXiv:2510.26033v1 Announce Type: new \nAbstract: In this paper, we study decentralized decision-making where agents optimize private objectives under incomplete information and imperfect public monitoring, in a non-cooperative setting. By shaping utilities-embedding shadow prices or Karush-Kuhn-Tucker(KKT)-aligned penalties-we make the stage game an exact-potential game whose unique equilibrium equals the (possibly constrained) social optimum. We characterize the Bayesian equilibrium as a stochastic variational inequality; strong monotonicity follows from a single-inflection compressed/stretched-exponential response combined with convex pricing. We give tracking bounds for damped-gradient and best-response-with-hysteresis updates under a noisy public index, and corresponding steady-state error. The framework accommodates discrete and continuous action sets and composes with slower discrete assignment. Deployable rules include: embed prices/penalties; publish a single public index; tune steps, damping, and dual rates for contraction. Computational experiments cover (i) a multi-tier supply chain and (ii) a non-cooperative agentic-AI compute market of bidding bots. Relative to price-only baselines, utility shaping attains near-centralized welfare, eliminates steady-state constraint/capacity violations when feasible, and accelerates convergence; with quantization, discrete equilibria track continuous ones within the mesh. The blueprint is portable to demand response, cloud/edge scheduling, and transportation pricing and biosecurity/agriculture. Overall, utility shaping plus a public index implements the constrained social optimum with stable equilibria under noise and drift-an operations-research-friendly alternative to heavy messaging or full mechanism design.</article>","contentLength":1787,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings","url":"https://arxiv.org/abs/2510.26032","date":1761883200,"author":"","guid":323229,"unread":true,"content":"<article>arXiv:2510.26032v1 Announce Type: new \nAbstract: Importance Incidental thyroid findings (ITFs) are increasingly detected on imaging performed for non-thyroid indications. Their prevalence, features, and clinical consequences remain undefined. Objective To develop, validate, and deploy a natural language processing (NLP) pipeline to identify ITFs in radiology reports and assess their prevalence, features, and clinical outcomes. Design, Setting, and Participants Retrospective cohort of adults without prior thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline identified ITFs and extracted nodule characteristics from image reports from multiple modalities and body regions. Main Outcomes and Measures Prevalence of ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer diagnosis. Logistic regression identified demographic and imaging-related factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9% women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more likely in women, older adults, those with higher BMI, and when imaging was ordered by oncology or internal medicine. Compared with chest CT, ITFs were more likely via neck CT, PET, and nuclear medicine scans. Nodule characteristics were poorly documented, with size reported in 44% and other features in fewer than 15% (e.g. calcifications). Compared with patients without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis, biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were common and strongly associated with cascades leading to the detection of small, low-risk cancers. These findings underscore the role of ITFs in thyroid cancer overdiagnosis and the need for standardized reporting and more selective follow-up.</article>","contentLength":1956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders","url":"https://arxiv.org/abs/2510.26027","date":1761883200,"author":"","guid":323230,"unread":true,"content":"<article>arXiv:2510.26027v1 Announce Type: new \nAbstract: Despite significant advances in Multimodal Large Language Models (MLLMs), understanding complex temporal dynamics in videos remains a major challenge. Our experiments show that current Video Large Language Model (Video-LLM) architectures have critical limitations in temporal understanding, struggling with tasks that require detailed comprehension of action sequences and temporal progression. In this work, we propose a Video-LLM architecture that introduces stacked temporal attention modules directly within the vision encoder. This design incorporates a temporal attention in vision encoder, enabling the model to better capture the progression of actions and the relationships between frames before passing visual tokens to the LLM. Our results show that this approach significantly improves temporal reasoning and outperforms existing models in video question answering tasks, specifically in action recognition. We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to +5.5%. By enhancing the vision encoder with temporal structure, we address a critical gap in video understanding for Video-LLMs. Project page and code are available at: https://alirasekh.github.io/STAVEQ2/.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Human-AI Conceptual Alignment through the Prism of Chess","url":"https://arxiv.org/abs/2510.26025","date":1761883200,"author":"","guid":323231,"unread":true,"content":"<article>arXiv:2510.26025v1 Announce Type: new \nAbstract: Do AI systems truly understand human concepts or merely mimic surface patterns? We investigate this through chess, where human creativity meets precise strategic concepts. Analyzing a 270M-parameter transformer that achieves grandmaster-level play, we uncover a striking paradox: while early layers encode human concepts like center control and knight outposts with up to 85\\% accuracy, deeper layers, despite driving superior performance, drift toward alien representations, dropping to 50-65\\% accuracy. To test conceptual robustness beyond memorization, we introduce the first Chess960 dataset: 240 expert-annotated positions across 6 strategic concepts. When opening theory is eliminated through randomized starting positions, concept recognition drops 10-20\\% across all methods, revealing the model's reliance on memorized patterns rather than abstract understanding. Our layer-wise analysis exposes a fundamental tension in current architectures: the representations that win games diverge from those that align with human thinking. These findings suggest that as AI systems optimize for performance, they develop increasingly alien intelligence, a critical challenge for creative AI applications requiring genuine human-AI collaboration. Dataset and code are available at: https://github.com/slomasov/ChessConceptsLLM.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs","url":"https://arxiv.org/abs/2510.26024","date":1761883200,"author":"","guid":323232,"unread":true,"content":"<article>arXiv:2510.26024v1 Announce Type: new \nAbstract: Cross-lingual alignment (CLA) aims to align multilingual representations, enabling Large Language Models (LLMs) to seamlessly transfer knowledge across languages. While intuitive, we hypothesize, this pursuit of representational convergence can inadvertently cause \"cultural erasure\", the functional loss of providing culturally-situated responses that should diverge based on the query language. In this work, we systematically analyze this trade-off by introducing a holistic evaluation framework, the transfer-localization plane, which quantifies both desirable knowledge transfer and undesirable cultural erasure. Using this framework, we re-evaluate recent CLA approaches and find that they consistently improve factual transfer at the direct cost of cultural localization across all six languages studied. Our investigation into the internal representations of these models reveals a key insight: universal factual transfer and culturally-specific knowledge are optimally steerable at different model layers. Based on this finding, we propose Surgical Steering, a novel inference-time method that disentangles these two objectives. By applying targeted activation steering to distinct layers, our approach achieves a better balance between the two competing dimensions, effectively overcoming the limitations of current alignment techniques.</article>","contentLength":1396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization","url":"https://arxiv.org/abs/2510.26023","date":1761883200,"author":"","guid":323233,"unread":true,"content":"<article>arXiv:2510.26023v1 Announce Type: new \nAbstract: Despite significant advancements in recent decades, autonomous vehicles (AVs) continue to face challenges in navigating certain traffic scenarios where human drivers excel. In such situations, AVs often become immobilized, disrupting overall traffic flow. Current recovery solutions, such as remote intervention (which is costly and inefficient) and manual takeover (which excludes non-drivers and limits AV accessibility), are inadequate. This paper introduces StuckSolver, a novel Large Language Model (LLM) driven recovery framework that enables AVs to resolve immobilization scenarios through self-reasoning and/or passenger-guided decision-making. StuckSolver is designed as a plug-in add-on module that operates on top of the AV's existing perception-planning-control stack, requiring no modification to its internal architecture. Instead, it interfaces with standard sensor data streams to detect immobilization states, interpret environmental context, and generate high-level recovery commands that can be executed by the AV's native planner. We evaluate StuckSolver on the Bench2Drive benchmark and in custom-designed uncertainty scenarios. Results show that StuckSolver achieves near-state-of-the-art performance through autonomous self-reasoning alone and exhibits further improvements when passenger guidance is incorporated.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PORTool: Tool-Use LLM Training with Rewarded Tree","url":"https://arxiv.org/abs/2510.26020","date":1761883200,"author":"","guid":323234,"unread":true,"content":"<article>arXiv:2510.26020v1 Announce Type: new \nAbstract: Current tool-use large language models (LLMs) are trained on static datasets, enabling them to interact with external tools and perform multi-step, tool-integrated reasoning, which produces tool-call trajectories. However, these models imitate how a query is resolved in a generic tool-call routine, thereby failing to explore possible solutions and demonstrating limited performance in an evolved, dynamic tool-call environment. In this work, we propose PORTool, a reinforcement learning (RL) method that encourages a tool-use LLM to explore various trajectories yielding the correct answer. Specifically, this method starts with generating multiple rollouts for a given query, and some of them share the first few tool-call steps, thereby forming a tree-like structure. Next, we assign rewards to each step, based on its ability to produce a correct answer and make successful tool calls. A shared step across different trajectories receives the same reward, while different steps under the same fork receive different rewards. Finally, these step-wise rewards are used to calculate fork-relative advantages, blended with trajectory-relative advantages, to train the LLM for tool use. The experiments utilize 17 tools to address user queries, covering both time-sensitive and time-invariant topics. We conduct ablation studies to systematically justify the necessity and the design robustness of step-wise rewards. Furthermore, we compare the proposed PORTool with other training approaches and demonstrate significant improvements in final accuracy and the number of tool-call steps.</article>","contentLength":1635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RADRON: Cooperative Localization of Ionizing Radiation Sources by MAVs with Compton Cameras","url":"https://arxiv.org/abs/2510.26018","date":1761883200,"author":"","guid":323235,"unread":true,"content":"<article>arXiv:2510.26018v1 Announce Type: new \nAbstract: We present a novel approach to localizing radioactive material by cooperating Micro Aerial Vehicles (MAVs). Our approach utilizes a state-of-the-art single-detector Compton camera as a highly sensitive, yet miniature detector of ionizing radiation. The detector's exceptionally low weight (40 g) opens up new possibilities of radiation detection by a team of cooperating agile MAVs. We propose a new fundamental concept of fusing the Compton camera measurements to estimate the position of the radiation source in real time even from extremely sparse measurements. The data readout and processing are performed directly onboard and the results are used in a dynamic feedback to drive the motion of the vehicles. The MAVs are stabilized in a tightly cooperating swarm to maximize the information gained by the Compton cameras, rapidly locate the radiation source, and even track a moving radiation source.</article>","contentLength":953,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning","url":"https://arxiv.org/abs/2510.26017","date":1761883200,"author":"","guid":323236,"unread":true,"content":"<article>arXiv:2510.26017v1 Announce Type: new \nAbstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal cities, intensifying the need for efficient and accurate methods to predict potential flood hazards. Traditional physics-based hydrodynamic simulators, although precise, are computationally expensive and impractical for city-scale coastal planning applications. Deep Learning (DL) techniques offer promising alternatives, however, they are often constrained by challenges such as data scarcity and high-dimensional output requirements. Leveraging a recently proposed vision-based, low-resource DL framework, we develop a novel, lightweight Convolutional Neural Network (CNN)-based model designed to predict coastal flooding under variable SLR projections and shoreline adaptation scenarios. Furthermore, we demonstrate the ability of the model to generalize across diverse geographical contexts by utilizing datasets from two distinct regions: Abu Dhabi and San Francisco. Our findings demonstrate that the proposed model significantly outperforms state-of-the-art methods, reducing the mean absolute error (MAE) in predicted flood depth maps on average by nearly 20%. These results highlight the potential of our approach to serve as a scalable and practical tool for coastal flood management, empowering decision-makers to develop effective mitigation strategies in response to the growing impacts of climate change. Project Page: https://caspiannet.github.io/</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fair intersection of seekable iterators","url":"https://arxiv.org/abs/2510.26016","date":1761883200,"author":"","guid":323237,"unread":true,"content":"<article>arXiv:2510.26016v1 Announce Type: new \nAbstract: miniKanren's key semantic advance over Prolog is to implement a complete yet efficient search strategy, fairly interleaving execution between disjuncts. This fairness is accomplished by bounding how much work is done exploring one disjunct before switching to the next. We show that the same idea -- fairness via bounded work -- underlies an elegant compositional approach to implementing worst-case optimal joins using a seekable iterator interface, suitable for shallow embedding in functional languages.</article>","contentLength":555,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Designing for Dignity while Driving: Interaction Needs of Blind and Low-Vision Passengers in Fully Automated Vehicles","url":"https://arxiv.org/abs/2510.26015","date":1761883200,"author":"","guid":323238,"unread":true,"content":"<article>arXiv:2510.26015v1 Announce Type: new \nAbstract: Fully automated vehicles (FAVs) hold promise for enhancing the mobility of blind and low-vision (BLV) individuals. To understand the situated interaction needs of BLV passengers, we conducted six on-road, and in-lab focus groups with 16 participants, immersing them in real-world driving conditions. Our thematic analysis reveals that BLV participants express a high initial 'faith' in FAVs, but require layered, value-sensitive information during the ride to cultivate trust. The participants' modality preference for voice suggests re-evaluating the role of haptics for BLV users in FAVs. Our findings show the importance of a respectful interaction design in FAVs that both address BLV users' mobility challenges and uphold their dignity. While others have advocated for a dignity lens, our contribution lies in grounding this framework in empirical findings and unpacking what it means to design for dignity in the context of FAVs.</article>","contentLength":984,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis","url":"https://arxiv.org/abs/2510.26014","date":1761883200,"author":"","guid":323239,"unread":true,"content":"<article>arXiv:2510.26014v1 Announce Type: new \nAbstract: Survival analysis is a task to model the time until an event of interest occurs, widely used in clinical and biomedical research. A key challenge is to model patient heterogeneity while also adapting risk predictions to both individual characteristics and temporal dynamics. We propose a dual mixture-of-experts (MoE) framework for discrete-time survival analysis. Our approach combines a feature-encoder MoE for subgroup-aware representation learning with a hazard MoE that leverages patient features and time embeddings to capture temporal dynamics. This dual-MoE design flexibly integrates with existing deep learning based survival pipelines. On METABRIC and GBSG breast cancer datasets, our method consistently improves performance, boosting the time-dependent C-index up to 0.04 on the test sets, and yields further gains when incorporated into the Consurv framework.</article>","contentLength":922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys","url":"https://arxiv.org/abs/2510.26012","date":1761883200,"author":"","guid":323240,"unread":true,"content":"<article>arXiv:2510.26012v1 Announce Type: new \nAbstract: The rapid growth of research literature, particularly in large language models (LLMs), has made producing comprehensive and current survey papers increasingly difficult. This paper introduces autosurvey2, a multi-stage pipeline that automates survey generation through retrieval-augmented synthesis and structured evaluation. The system integrates parallel section generation, iterative refinement, and real-time retrieval of recent publications to ensure both topical completeness and factual accuracy. Quality is assessed using a multi-LLM evaluation framework that measures coverage, structure, and relevance in alignment with expert review standards. Experimental results demonstrate that autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity. By combining retrieval, reasoning, and automated evaluation into a unified framework, autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a solid foundation for future research on automated scholarly writing. All code and resources are available at https://github.com/annihi1ation/auto_research.</article>","contentLength":1302,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Zero Added Loss Multiplexing (ZALM) Source Simulation","url":"https://arxiv.org/abs/2510.26009","date":1761883200,"author":"","guid":323241,"unread":true,"content":"<article>arXiv:2510.26009v1 Announce Type: new \nAbstract: Zero Added Loss Multiplexing (ZALM) offers broadband, per channel heralded EPR pairs, with a rich parameter space that allows its performance to be tailored for specific applications. We present a modular ZALM simulator that demonstrates how design choices affect output rate and fidelity. Built in NetSquid with QSI controllers, it exposes 20+ tunable parameters, supports IDEAL and REALISTIC modes, and provides reusable components for Spontaneous Parametric Down Conversion (SPDC) sources, interference, Dense Wavelength Division Multiplexing (DWDM) filtering, fiber delay, active polarization gates, detectors, and lossy fiber. Physics based models capture Hong Ou Mandel (HOM) visibility, insertion loss, detector efficiency, gate errors, and attenuation. Using this tool, we map trade offs among fidelity, link distance, and entangled pairs per use, and show how SPDC bandwidth and DWDM grid spacing steer performance. Using the default configuration settings, average fidelity emains constant at 0.8 but the ebit rate decreases from 0.0175 at the source to 0.0 at 50 km; narrowing the SPDC degeneracy bandwidth increases the ebit rate significantly without affecting fidelity. The simulator enables codesign of source, filtering, and feedforward settings for specific quantum memories and integrates as a building block for end to end quantum network studies.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry","url":"https://arxiv.org/abs/2510.26008","date":1761883200,"author":"","guid":323242,"unread":true,"content":"<article>arXiv:2510.26008v1 Announce Type: new \nAbstract: Modern machine learning (ML) has grown into a tightly coupled, full-stack ecosystem that combines hardware, software, network, and applications. Many users rely on cloud providers for elastic, isolated, and cost-efficient resources. Unfortunately, these platforms as a service use virtualization, which means operators have little insight into the users' workloads. This hinders resource optimizations by the operator, which is essential to ensure cost efficiency and minimize execution time. In this paper, we argue that workload knowledge is unnecessary for system-level optimization. We propose System-X, which takes a \\emph{hardware-centric} approach, relying only on hardware signals -- fully accessible by operators. Using low-level signals collected from the system, System-X detects anomalies through an unsupervised learning pipeline. The pipeline is developed by analyzing over 30 popular ML models on various hardware platforms, ensuring adaptability to emerging workloads and unknown deployment patterns. Using System-X, we successfully identified both network and system configuration issues, accelerating the DeepSeek model by 5.97%.</article>","contentLength":1196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Quest for Reliable Metrics of Responsible AI","url":"https://arxiv.org/abs/2510.26007","date":1761883200,"author":"","guid":323243,"unread":true,"content":"<article>arXiv:2510.26007v1 Announce Type: new \nAbstract: The development of Artificial Intelligence (AI), including AI in Science (AIS), should be done following the principles of responsible AI. Progress in responsible AI is often quantified through evaluation metrics, yet there has been less work on assessing the robustness and reliability of the metrics themselves. We reflect on prior work that examines the robustness of fairness metrics for recommender systems as a type of AI application and summarise their key takeaways into a set of non-exhaustive guidelines for developing reliable metrics of responsible AI. Our guidelines apply to a broad spectrum of AI applications, including AIS.</article>","contentLength":689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments","url":"https://arxiv.org/abs/2510.26006","date":1761883200,"author":"","guid":323244,"unread":true,"content":"<article>arXiv:2510.26006v1 Announce Type: new \nAbstract: Humans can naturally identify, reason about, and explain anomalies in their environment. In computer vision, this long-standing challenge remains limited to industrial defects or unrealistic, synthetically generated anomalies, failing to capture the richness and unpredictability of real-world anomalies. In this work, we introduce CAVE, the first benchmark of real-world visual anomalies. CAVE supports three open-ended tasks: anomaly description, explanation, and justification; with fine-grained annotations for visual grounding and categorizing anomalies based on their visual manifestations, their complexity, severity, and commonness. These annotations draw inspiration from cognitive science research on how humans identify and resolve anomalies, providing a comprehensive framework for evaluating Vision-Language Models (VLMs) in detecting and understanding anomalies. We show that state-of-the-art VLMs struggle with visual anomaly perception and commonsense reasoning, even with advanced prompting strategies. By offering a realistic and cognitively grounded benchmark, CAVE serves as a valuable resource for advancing research in anomaly detection and commonsense reasoning in VLMs.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System","url":"https://arxiv.org/abs/2510.26004","date":1761883200,"author":"","guid":323245,"unread":true,"content":"<article>arXiv:2510.26004v1 Announce Type: new \nAbstract: Rapid and reliable incident detection is critical for reducing crash-related fatalities, injuries, and congestion. However, conventional methods, such as closed-circuit television, dashcam footage, and sensor-based detection, separate detection from verification, suffer from limited flexibility, and require dense infrastructure or high penetration rates, restricting adaptability and scalability to shifting incident hotspots. To overcome these challenges, we developed DARTS, a drone-based, AI-powered real-time traffic incident detection system. DARTS integrates drones' high mobility and aerial perspective for adaptive surveillance, thermal imaging for better low-visibility performance and privacy protection, and a lightweight deep learning framework for real-time vehicle trajectory extraction and incident detection. The system achieved 99% detection accuracy on a self-collected dataset and supports simultaneous online visual verification, severity assessment, and incident-induced congestion propagation monitoring via a web-based interface. In a field test on Interstate 75 in Florida, DARTS detected and verified a rear-end collision 12 minutes earlier than the local transportation management center and monitored incident-induced congestion propagation, suggesting potential to support faster emergency response and enable proactive traffic control to reduce congestion and secondary crash risk. Crucially, DARTS's flexible deployment architecture reduces dependence on frequent physical patrols, indicating potential scalability and cost-effectiveness for use in remote areas and resource-constrained settings. This study presents a promising step toward a more flexible and integrated real-time traffic incident detection system, with significant implications for the operational efficiency and responsiveness of modern transportation management.</article>","contentLength":1914,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Message Recovery Attack in NTRU via Knapsack","url":"https://arxiv.org/abs/2510.26003","date":1761883200,"author":"","guid":323246,"unread":true,"content":"<article>arXiv:2510.26003v1 Announce Type: new \nAbstract: In the present paper, we introduce a message-recovery attack based on the Modular Knapsack Problem, applicable to all variants of the NTRU-HPS cryptosystem. Assuming that a fraction $\\epsilon$ of the coefficients of the message ${\\bf{m}}\\in\\{-1,0,1\\}^N$ and of the nonce vector ${\\bf r}\\in\\{-1,0,1\\}^N$ are known in advance at random positions, we reduce message decryption to finding a short vector in a lattice that encodes an instance of a modular knapsack system. This allows us to address a key question: how much information about ${\\bf m}$, or about the pair $({\\bf m},{\\bf r})$, is required before recovery becomes feasible? A FLATTER reduction successfully recovers the message, in practice when $\\epsilon\\approx 0.45$. Our implementation finds ${\\bf m}$ within a few minutes on a commodity desktop.</article>","contentLength":857,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement","url":"https://arxiv.org/abs/2510.26001","date":1761883200,"author":"","guid":323247,"unread":true,"content":"<article>arXiv:2510.26001v1 Announce Type: new \nAbstract: We propose an innovative enhancement to the Mamba framework by increasing the Hausdorff dimension of its scanning pattern through a novel Hilbert Selective Scan mechanism. This mechanism explores the feature space more effectively, capturing intricate fine-scale details and improving overall coverage. As a result, it mitigates information inconsistencies while refining spatial locality to better capture subtle local interactions without sacrificing the model's ability to handle long-range dependencies. Extensive experiments on publicly available benchmarks demonstrate that our approach significantly improves both the quantitative metrics and qualitative visual fidelity of existing Mamba-based low-light image enhancement methods, all while reducing computational resource consumption and shortening inference time. We believe that this refined strategy not only advances the state-of-the-art in low-light image enhancement but also holds promise for broader applications in fields that leverage Mamba-based techniques.</article>","contentLength":1076,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Infrequent Exploration in Linear Bandits","url":"https://arxiv.org/abs/2510.26000","date":1761883200,"author":"","guid":323248,"unread":true,"content":"<article>arXiv:2510.26000v1 Announce Type: new \nAbstract: We study the problem of infrequent exploration in linear bandits, addressing a significant yet overlooked gap between fully adaptive exploratory methods (e.g., UCB and Thompson Sampling), which explore potentially at every time step, and purely greedy approaches, which require stringent diversity assumptions to succeed. Continuous exploration can be impractical or unethical in safety-critical or costly domains, while purely greedy strategies typically fail without adequate contextual diversity. To bridge these extremes, we introduce a simple and practical framework, INFEX, explicitly designed for infrequent exploration. INFEX executes a base exploratory policy according to a given schedule while predominantly choosing greedy actions in between. Despite its simplicity, our theoretical analysis demonstrates that INFEX achieves instance-dependent regret matching standard provably efficient algorithms, provided the exploration frequency exceeds a logarithmic threshold. Additionally, INFEX is a general, modular framework that allows seamless integration of any fully adaptive exploration method, enabling wide applicability and ease of adoption. By restricting intensive exploratory computations to infrequent intervals, our approach can also enhance computational efficiency. Empirical evaluations confirm our theoretical findings, showing state-of-the-art regret performance and runtime improvements over existing methods.</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL","url":"https://arxiv.org/abs/2510.25997","date":1761883200,"author":"","guid":323249,"unread":true,"content":"<article>arXiv:2510.25997v1 Announce Type: new \nAbstract: Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizing access to structured data, allowing users to query databases without learning SQL. Yet existing systems struggle with realistic spatio-temporal queries, where success requires aligning vague user phrasing with schema-specific categories, handling temporal reasoning, and choosing appropriate outputs. We present an agentic pipeline that extends a naive text-to-SQL baseline (llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. The agent can plan, decompose, and adapt queries through schema inspection, SQL generation, execution, and visualization tools. We evaluate on 35 natural-language queries over the NYC and Tokyo check-in dataset, covering spatial, temporal, and multi-dataset reasoning. The agent achieves substantially higher accuracy than the naive baseline 91.4% vs. 28.6% and enhances usability through maps, plots, and structured natural-language summaries. Crucially, our design enables more natural human-database interaction, supporting users who lack SQL expertise, detailed schema knowledge, or prompting skill. We conclude that agentic orchestration, rather than stronger SQL generators alone, is a promising foundation for interactive geospatial assistants.</article>","contentLength":1322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Online Learning with Predictive Coding Networks: Exploiting Temporal Correlations","url":"https://arxiv.org/abs/2510.25993","date":1761883200,"author":"","guid":323250,"unread":true,"content":"<article>arXiv:2510.25993v1 Announce Type: new \nAbstract: Robotic systems operating at the edge require efficient online learning algorithms that can continuously adapt to changing environments while processing streaming sensory data. Traditional backpropagation, while effective, conflicts with biological plausibility principles and may be suboptimal for continuous adaptation scenarios. The Predictive Coding (PC) framework offers a biologically plausible alternative with local, Hebbian-like update rules, making it suitable for neuromorphic hardware implementation. However, PC's main limitation is its computational overhead due to multiple inference iterations during training. We present Predictive Coding Network with Temporal Amortization (PCN-TA), which preserves latent states across temporal frames. By leveraging temporal correlations, PCN-TA significantly reduces computational demands while maintaining learning performance. Our experiments on the COIL-20 robotic perception dataset demonstrate that PCN-TA achieves 10% fewer weight updates compared to backpropagation and requires 50% fewer inference steps than baseline PC networks. These efficiency gains directly translate to reduced computational overhead for moving another step toward edge deployment and real-time adaptation support in resource-constrained robotic systems. The biologically-inspired nature of our approach also makes it a promising candidate for future neuromorphic hardware implementations, enabling efficient online learning at the edge.</article>","contentLength":1521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning","url":"https://arxiv.org/abs/2510.25992","date":1761883200,"author":"","guid":323251,"unread":true,"content":"<article>arXiv:2510.25992v1 Announce Type: new \nAbstract: Large Language Models (LLMs) often struggle with problems that require multi-step reasoning. For small-scale open-source models, Reinforcement Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to overfit long demonstrations through rigid token-by-token imitation. To address this gap, we propose Supervised Reinforcement Learning (SRL), a framework that reformulates problem solving as generating a sequence of logical \"actions\". SRL trains the model to generate an internal reasoning monologue before committing to each action. It provides smoother rewards based on the similarity between the model's actions and expert actions extracted from the SFT dataset in a step-wise manner. This supervision offers richer learning signals even when all rollouts are incorrect, while encouraging flexible reasoning guided by expert demonstrations. As a result, SRL enables small models to learn challenging problems previously unlearnable by SFT or RLVR. Moreover, initializing training with SRL before refining with RLVR yields the strongest overall performance. Beyond reasoning benchmarks, SRL generalizes effectively to agentic software engineering tasks, establishing it as a robust and versatile training framework for reasoning-oriented LLMs.</article>","contentLength":1394,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A fast spectral overlapping domain decomposition method with discretization-independent conditioning bounds","url":"https://arxiv.org/abs/2510.25991","date":1761883200,"author":"","guid":323252,"unread":true,"content":"<article>arXiv:2510.25991v1 Announce Type: new \nAbstract: A domain decomposition method for the solution of general variable-coefficient elliptic partial differential equations on regular domains is introduced. The method is based on tessellating the domain into overlapping thin slabs or shells, and then explicitly forming a reduced linear system that connects the different domains. Rank-structure ('H-matrix structure') is exploited to handle the large dense blocks that arise in the reduced linear system. Importantly, the formulation used is well-conditioned, as it converges to a second kind Fredholm equation as the precision in the local solves is refined. Moreover, the dense blocks that arise are far more data-sparse than in existing formulations, leading to faster and more efficient H-matrix arithmetic. To form the reduced linear system, black-box randomized compression is used, taking full advantage of the fact that sparse direct solvers are highly efficient on the thin sub-domains. Numerical experiments demonstrate that our solver can handle oscillatory 2D and 3D problems with as many as 28 million degrees of freedom.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI","url":"https://arxiv.org/abs/2510.25990","date":1761883200,"author":"","guid":323253,"unread":true,"content":"<article>arXiv:2510.25990v1 Announce Type: new \nAbstract: In this work, we address the TrackRAD2025 challenge of real-time tumor tracking in cine-MRI sequences of the thoracic and abdominal regions under strong data scarcity constraints. Two complementary strategies were explored: (i) unsupervised registration with the IMPACT similarity metric and (ii) foundation model-based segmentation leveraging SAM 2.1 and its recent variants through prompt-based interaction. Due to the one-second runtime constraint, the SAM-based method was ultimately selected. The final configuration used SAM2.1 b+ with mask-based prompts from the first annotated slice, fine-tuned solely on the small labeled subset from TrackRAD2025. Training was configured to minimize overfitting, using 1024x1024 patches (batch size 1), standard augmentations, and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve generalization while adapting to annotator-specific styles. Training lasted 300 epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently applied across all anatomical sites and MRI field strengths. Test-time augmentation was considered but ultimately discarded due to negligible performance gains. The final model was selected based on the highest Dice Similarity Coefficient achieved on the validation set after fine-tuning. On the hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall in the TrackRAD2025 challenge. These results highlight the strong potential of foundation models for accurate and real-time tumor tracking in MRI-guided radiotherapy.</article>","contentLength":1670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A General and Streamlined Differentiable Optimization Framework","url":"https://arxiv.org/abs/2510.25986","date":1761883200,"author":"","guid":323254,"unread":true,"content":"<article>arXiv:2510.25986v1 Announce Type: new \nAbstract: Differentiating through constrained optimization problems is increasingly central to learning, control, and large-scale decision-making systems, yet practical integration remains challenging due to solver specialization and interface mismatches. This paper presents a general and streamlined framework-an updated DiffOpt.jl-that unifies modeling and differentiation within the Julia optimization stack. The framework computes forward - and reverse-mode solution and objective sensitivities for smooth, potentially nonconvex programs by differentiating the KKT system under standard regularity assumptions. A first-class, JuMP-native parameter-centric API allows users to declare named parameters and obtain derivatives directly with respect to them - even when a parameter appears in multiple constraints and objectives - eliminating brittle bookkeeping from coefficient-level interfaces. We illustrate these capabilities on convex and nonconvex models, including economic dispatch, mean-variance portfolio selection with conic risk constraints, and nonlinear robot inverse kinematics. Two companion studies further demonstrate impact at scale: gradient-based iterative methods for strategic bidding in energy markets and Sobolev-style training of end-to-end optimization proxies using solver-accurate sensitivities. Together, these results demonstrate that differentiable optimization can be deployed as a routine tool for experimentation, learning, calibration, and design-without deviating from standard JuMP modeling practices and while retaining access to a broad ecosystem of solvers.</article>","contentLength":1639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A New Type of Axis-Angle Attitude Control Law for Rotational Systems: Synthesis, Analysis, and Experiments","url":"https://arxiv.org/abs/2510.25985","date":1761883200,"author":"","guid":323255,"unread":true,"content":"<article>arXiv:2510.25985v1 Announce Type: new \nAbstract: Over the past few decades, continuous quaternion-based attitude control has been proven highly effective for driving rotational systems that can be modeled as rigid bodies, such as satellites and drones. However, methods rooted in this approach do not enforce the existence of a unique closed-loop (CL) equilibrium attitude-error quaternion (AEQ); and, for rotational errors about the attitude-error Euler axis larger than {\\pi}rad, their proportional-control effect diminishes as the system state moves away from the stable equilibrium of the CL rotational dynamics. In this paper, we introduce a new type of attitude control law that more effectively leverages the attitude-error Euler axis-angle information to guarantee a unique CL equilibrium AEQ and to provide greater flexibility in the use of proportional-control efforts. Furthermore, using two different control laws as examples-through the construction of a strict Lyapunov function for the CL dynamics-we demonstrate that the resulting unique equilibrium of the CL rotational system can be enforced to be uniformly asymptotically stable. To assess and demonstrate the functionality and performance of the proposed approach, we performed numerical simulations and executed dozens of real-time tumble-recovery maneuvers using a small quadrotor. These simulations and flight tests compellingly demonstrate that the proposed axis-angle-based method achieves superior flight performance-compared with that obtained using a high-performance quaternion-based controller-in terms of stabilization time.</article>","contentLength":1605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contrastive Predictive Coding Done Right for Mutual Information Estimation","url":"https://arxiv.org/abs/2510.25983","date":1761883200,"author":"","guid":323256,"unread":true,"content":"<article>arXiv:2510.25983v1 Announce Type: new \nAbstract: The InfoNCE objective, originally introduced for contrastive representation learning, has become a popular choice for mutual information (MI) estimation, despite its indirect connection to MI. In this paper, we demonstrate why InfoNCE should not be regarded as a valid MI estimator, and we introduce a simple modification, which we refer to as InfoNCE-anchor, for accurate MI estimation. Our modification introduces an auxiliary anchor class, enabling consistent density ratio estimation and yielding a plug-in MI estimator with significantly reduced bias. Beyond this, we generalize our framework using proper scoring rules, which recover InfoNCE-anchor as a special case when the log score is employed. This formulation unifies a broad spectrum of contrastive objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single principled framework. Empirically, we find that InfoNCE-anchor with the log score achieves the most accurate MI estimates; however, in self-supervised representation learning experiments, we find that the anchor does not improve the downstream task performance. These findings corroborate that contrastive representation learning benefits not from accurate MI estimation per se, but from the learning of structured density ratios.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache","url":"https://arxiv.org/abs/2510.25979","date":1761883200,"author":"","guid":323257,"unread":true,"content":"<article>arXiv:2510.25979v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.</article>","contentLength":1325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Go with AR: Attention to Virtual and Physical Targets while Varying Augmentation Density","url":"https://arxiv.org/abs/2510.25978","date":1761883200,"author":"","guid":323258,"unread":true,"content":"<article>arXiv:2510.25978v1 Announce Type: new \nAbstract: Augmented reality is projected to be a primary mode of information consumption on the go, seamlessly integrating virtual content into the physical world. However, the potential perceptual demands of viewing virtual annotations while navigating a physical environment could impact user efficacy and safety, and the implications of these demands are not well understood. Here, we investigate the impact of virtual path guidance and augmentation density (visual clutter) on search performance and memory. Participants walked along a predefined path, searching for physical or virtual items. They experienced two levels of augmentation density, and either walked freely or with enforced speed and path guidance. Augmentation density impacted behavior and reduced awareness of uncommon objects in the environment. Analysis of search task performance and post-experiment item recall revealed differing attention to physical and virtual objects. On the basis of these findings we outline considerations for AR apps designed for use on the go.</article>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium","url":"https://arxiv.org/abs/2510.25977","date":1761883200,"author":"","guid":323259,"unread":true,"content":"<article>arXiv:2510.25977v1 Announce Type: new \nAbstract: AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.</article>","contentLength":1222,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer","url":"https://arxiv.org/abs/2510.25976","date":1761883200,"author":"","guid":323260,"unread":true,"content":"<article>arXiv:2510.25976v1 Announce Type: new \nAbstract: Reconstructing images seen by people from their fMRI brain recordings provides a non-invasive window into the human brain. Despite recent progress enabled by diffusion models, current methods often lack faithfulness to the actual seen images. We present \"Brain-IT\", a brain-inspired approach that addresses this challenge through a Brain Interaction Transformer (BIT), allowing effective interactions between clusters of functionally-similar brain-voxels. These functional-clusters are shared by all subjects, serving as building blocks for integrating information both within and across brains. All model components are shared by all clusters &amp; subjects, allowing efficient training with a limited amount of data. To guide the image reconstruction, BIT predicts two complementary localized patch-level image features: (i)high-level semantic features which steer the diffusion model toward the correct semantic content of the image; and (ii)low-level structural features which help to initialize the diffusion process with the correct coarse layout of the image. BIT's design enables direct flow of information from brain-voxel clusters to localized image features. Through these principles, our method achieves image reconstructions from fMRI that faithfully reconstruct the seen images, and surpass current SotA approaches both visually and by standard objective metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve results comparable to current methods trained on full 40-hour recordings.</article>","contentLength":1567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation","url":"https://arxiv.org/abs/2510.25975","date":1761883200,"author":"","guid":323261,"unread":true,"content":"<article>arXiv:2510.25975v1 Announce Type: new \nAbstract: Large Language Models (LLMs) often struggle with complex mathematical reasoning, where prose-based generation leads to unverified and arithmetically unsound solutions. Current prompting strategies like Chain of Thought still operate within this unreliable medium, lacking a mechanism for deterministic verification. To address these limitations, we introduce SymCode, a neurosymbolic framework that reframes mathematical problem-solving as a task of verifiable code generation using the SymPy library. We evaluate SymCode on challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating significant accuracy improvements of up to 13.6 percentage points over baselines. Our analysis shows that SymCode is not only more token-efficient but also fundamentally shifts model failures from opaque logical fallacies towards transparent, programmatic errors. By grounding LLM reasoning in a deterministic symbolic engine, SymCode represents a key step towards more accurate and trustworthy AI in formal domains.</article>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Risks and Opportunities in Human-Machine Teaming in Operationalizing Machine Learning Target Variables","url":"https://arxiv.org/abs/2510.25974","date":1761883200,"author":"","guid":323262,"unread":true,"content":"<article>arXiv:2510.25974v1 Announce Type: new \nAbstract: Predictive modeling has the potential to enhance human decision-making. However, many predictive models fail in practice due to problematic problem formulation in cases where the prediction target is an abstract concept or construct and practitioners need to define an appropriate target variable as a proxy to operationalize the construct of interest. The choice of an appropriate proxy target variable is rarely self-evident in practice, requiring both domain knowledge and iterative data modeling. This process is inherently collaborative, involving both domain experts and data scientists. In this work, we explore how human-machine teaming can support this process by accelerating iterations while preserving human judgment. We study the impact of two human-machine teaming strategies on proxy construction: 1) relevance-first: humans leading the process by selecting relevant proxies, and 2) performance-first: machines leading the process by recommending proxies based on predictive performance. Based on a controlled user study of a proxy construction task (N = 20), we show that the performance-first strategy facilitated faster iterations and decision-making, but also biased users towards well-performing proxies that are misaligned with the application goal. Our study highlights the opportunities and risks of human-machine teaming in operationalizing machine learning target variables, yielding insights for future research to explore the opportunities and mitigate the risks.</article>","contentLength":1539,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing","url":"https://arxiv.org/abs/2510.25970","date":1761883200,"author":"","guid":323263,"unread":true,"content":"<article>arXiv:2510.25970v1 Announce Type: new \nAbstract: Rectified flow models have become a de facto standard in image generation due to their stable sampling trajectories and high-fidelity outputs. Despite their strong generative capabilities, they face critical limitations in image editing tasks: inaccurate inversion processes for mapping real images back into the latent space, and gradient entanglement issues during editing often result in outputs that do not faithfully reflect the target prompt. Recent efforts have attempted to directly map source and target distributions via ODE-based approaches without inversion; however,these methods still yield suboptimal editing quality. In this work, we propose a flow decomposition-and-aggregation framework built upon an inversion-free formulation to address these limitations. Specifically, we semantically decompose the target prompt into multiple sub-prompts, compute an independent flow for each, and aggregate them to form a unified editing trajectory. While we empirically observe that decomposing the original flow enhances diversity in the target space, generating semantically aligned outputs still requires consistent guidance toward the full target prompt. To this end, we design a projection and soft-aggregation mechanism for flow, inspired by gradient conflict resolution in multi-task learning. This approach adaptively weights the sub-target velocity fields, suppressing semantic redundancy while emphasizing distinct directions, thereby preserving both diversity and consistency in the final edited output. Experimental results demonstrate that our method outperforms existing zero-shot editing approaches in terms of semantic fidelity and attribute disentanglement. The code is available at https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.</article>","contentLength":1813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semantic Label Drift in Cross-Cultural Translation","url":"https://arxiv.org/abs/2510.25967","date":1761883200,"author":"","guid":323264,"unread":true,"content":"<article>arXiv:2510.25967v1 Announce Type: new \nAbstract: Machine Translation (MT) is widely employed to address resource scarcity in low-resource languages by generating synthetic data from high-resource counterparts. While sentiment preservation in translation has long been studied, a critical but underexplored factor is the role of cultural alignment between source and target languages. In this paper, we hypothesize that semantic labels are drifted or altered during MT due to cultural divergence. Through a series of experiments across culturally sensitive and neutral domains, we establish three key findings: (1) MT systems, including modern Large Language Models (LLMs), induce label drift during translation, particularly in culturally sensitive domains; (2) unlike earlier statistical MT tools, LLMs encode cultural knowledge, and leveraging this knowledge can amplify label drift; and (3) cultural similarity or dissimilarity between source and target languages is a crucial determinant of label preservation. Our findings highlight that neglecting cultural factors in MT not only undermines label fidelity but also risks misinterpretation and cultural conflict in downstream applications.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Curvature-Aware Calibration of Tactile Sensors for Accurate Force Estimation on Non-Planar Surfaces","url":"https://arxiv.org/abs/2510.25965","date":1761883200,"author":"","guid":323265,"unread":true,"content":"<article>arXiv:2510.25965v1 Announce Type: new \nAbstract: Flexible tactile sensors are increasingly used in real-world applications such as robotic grippers, prosthetic hands, wearable gloves, and assistive devices, where they need to conform to curved and irregular surfaces. However, most existing tactile sensors are calibrated only on flat substrates, and their accuracy and consistency degrade once mounted on curved geometries. This limitation restricts their reliability in practical use. To address this challenge, we develop a calibration model for a widely used resistive tactile sensor design that enables accurate force estimation on one-dimensional curved surfaces. We then train a neural network (a multilayer perceptron) to predict local curvature from baseline sensor outputs recorded under no applied load, achieving an R2 score of 0.91. The proposed approach is validated on five daily objects with varying curvatures under forces from 2 N to 8 N. Results show that the curvature-aware calibration maintains consistent force accuracy across all surfaces, while flat-surface calibration underestimates force as curvature increases. Our results demonstrate that curvature-aware modeling improves the accuracy, consistency, and reliability of flexible tactile sensors, enabling dependable performance across real-world applications.</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Systems for Scaling Accessibility Efforts in Large Computing Courses","url":"https://arxiv.org/abs/2510.25964","date":1761883200,"author":"","guid":323266,"unread":true,"content":"<article>arXiv:2510.25964v1 Announce Type: new \nAbstract: It is critically important to make computing courses accessible for disabled students. This is particularly challenging in large computing courses, which face unique challenges due to the sheer scale of course content and staff. In this experience report, we share our attempts to scale accessibility efforts for a large university-level introductory programming course sequence, with over 3500 enrolled students and 100 teaching assistants (TAs) per year. First, we introduce our approach to auditing and remediating course materials by systematically identifying and resolving accessibility issues. However, remediating content post-hoc is purely reactive and scales poorly. We then discuss two approaches to systems that enable proactive accessibility work. We developed technical systems to manage remediation complexity at scale: redesigning other course content to be web-first and accessible by default, providing alternate accessible views for existing course content, and writing automated tests to receive instant feedback on a subset of accessibility issues. Separately, we established human systems to empower both course staff and students in accessibility best practices: developing and running various TA-targeted accessibility trainings, establishing course-wide accessibility norms, and integrating accessibility topics into core course curriculum. Preliminary qualitative feedback from both staff and students shows increased engagement in accessibility work and accessible technologies. We close by discussing limitations and lessons learned from our work, with advice for others developing similar auditing, remediation, technical, or human systems.</article>","contentLength":1718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Outperforming Multiserver SRPT at All Loads","url":"https://arxiv.org/abs/2510.25963","date":1761883200,"author":"","guid":323267,"unread":true,"content":"<article>arXiv:2510.25963v1 Announce Type: new \nAbstract: A well-designed scheduling policy can unlock significant performance improvements with no additional resources. Multiserver SRPT (SRPT-$k$) is known to achieve asymptotically optimal mean response time in the heavy traffic limit, as load approaches capacity. No better policy is known for the M/G/$k$ queue in any regime.\n  We introduce a new policy, SRPT-Except-$k+1$ &amp; Modified SRPT (SEK-SMOD), which is the first policy to provably achieve lower mean response time than SRPT-$k$. SEK-SMOD outperforms SRPT-$k$ across all loads and all job size distributions. The key idea behind SEK-SMOD is to prioritize large jobs over small jobs in specific scenarios to improve server utilization, and thereby improve the response time of subsequent jobs in expectation. Our proof is a novel application of hybrid worst-case and stochastic techniques to relative analysis, where we analyze the deviations of our proposed SEK-SMOD policy away from the SRPT-$k$ baseline policy. Furthermore, we design Practical-SEK (a simplified variant of SEK-SMOD) and empirically verify the improvement over SRPT-$k$ via simulation.</article>","contentLength":1156,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Dataless Training of Neural Networks","url":"https://arxiv.org/abs/2510.25962","date":1761883200,"author":"","guid":323268,"unread":true,"content":"<article>arXiv:2510.25962v1 Announce Type: new \nAbstract: This paper surveys studies on the use of neural networks for optimization in the training-data-free setting. Specifically, we examine the dataless application of neural network architectures in optimization by re-parameterizing problems using fully connected (or MLP), convolutional, graph, and quadratic neural networks. Although MLPs have been used to solve linear programs a few decades ago, this approach has recently gained increasing attention due to its promising results across diverse applications, including those based on combinatorial optimization, inverse problems, and partial differential equations. The motivation for this setting stems from two key (possibly over-lapping) factors: (i) data-driven learning approaches are still underdeveloped and have yet to demonstrate strong results, as seen in combinatorial optimization, and (ii) the availability of training data is inherently limited, such as in medical image reconstruction and other scientific applications. In this paper, we define the dataless setting and categorize it into two variants based on how a problem instance -- defined by a single datum -- is encoded onto the neural network: (i) architecture-agnostic methods and (ii) architecture-specific methods. Additionally, we discuss similarities and clarify distinctions between the dataless neural network (dNN) settings and related concepts such as zero-shot learning, one-shot learning, lifting in optimization, and over-parameterization.</article>","contentLength":1522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WaveVerif: Acoustic Side-Channel based Verification of Robotic Workflows","url":"https://arxiv.org/abs/2510.25960","date":1761883200,"author":"","guid":323269,"unread":true,"content":"<article>arXiv:2510.25960v1 Announce Type: new \nAbstract: In this paper, we present a framework that uses acoustic side- channel analysis (ASCA) to monitor and verify whether a robot correctly executes its intended commands. We develop and evaluate a machine-learning-based workflow verification system that uses acoustic emissions generated by robotic movements. The system can determine whether real-time behavior is consistent with expected commands. The evaluation takes into account movement speed, direction, and microphone distance. The results show that individual robot movements can be validated with over 80% accuracy under baseline conditions using four different classifiers: Support Vector Machine (SVM), Deep Neural Network (DNN), Recurrent Neural Network (RNN), and Convolutional Neural Network (CNN). Additionally, workflows such as pick-and-place and packing could be identified with similarly high confidence. Our findings demonstrate that acoustic signals can support real-time, low-cost, passive verification in sensitive robotic environments without requiring hardware modifications.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CHIPSIM: A Co-Simulation Framework for Deep Learning on Chiplet-Based Systems","url":"https://arxiv.org/abs/2510.25958","date":1761883200,"author":"","guid":323270,"unread":true,"content":"<article>arXiv:2510.25958v1 Announce Type: new \nAbstract: Due to reduced manufacturing yields, traditional monolithic chips cannot keep up with the compute, memory, and communication demands of data-intensive applications, such as rapidly growing deep neural network (DNN) models. Chiplet-based architectures offer a cost-effective and scalable solution by integrating smaller chiplets via a network-on-interposer (NoI). Fast and accurate simulation approaches are critical to unlocking this potential, but existing methods lack the required accuracy, speed, and flexibility. To address this need, this work presents CHIPSIM, a comprehensive co-simulation framework designed for parallel DNN execution on chiplet-based systems. CHIPSIM concurrently models computation and communication, accurately capturing network contention and pipelining effects that conventional simulators overlook. Furthermore, it profiles the chiplet and NoI power consumptions at microsecond granularity for precise transient thermal analysis. Extensive evaluations with homogeneous/heterogeneous chiplets and different NoI architectures demonstrate the framework's versatility, up to 340% accuracy improvement, and power/thermal analysis capability.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Impact of Navigation Aids on Search Performance and Object Recall in Wide-Area Augmented Reality","url":"https://arxiv.org/abs/2510.25957","date":1761883200,"author":"","guid":323271,"unread":true,"content":"<article>arXiv:2510.25957v1 Announce Type: new \nAbstract: Head-worn augmented reality (AR) is a hotly pursued and increasingly feasible contender paradigm for replacing or complementing smartphones and watches for continual information consumption. Here, we compare three different AR navigation aids (on-screen compass, on-screen radar and in-world vertical arrows) in a wide-area outdoor user study (n=24) where participants search for hidden virtual target items amongst physical and virtual objects. We analyzed participants' search task performance, movements, eye-gaze, survey responses and object recall. There were two key findings. First, all navigational aids enhanced search performance relative to a control condition, with some benefit and strongest user preference for in-world arrows. Second, users recalled fewer physical objects than virtual objects in the environment, suggesting reduced awareness of the physical environment. Together, these findings suggest that while navigational aids presented in AR can enhance search task performance, users may pay less attention to the physical environment, which could have undesirable side-effects.</article>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi","url":"https://arxiv.org/abs/2510.25954","date":1761883200,"author":"","guid":323272,"unread":true,"content":"<article>arXiv:2510.25954v1 Announce Type: new \nAbstract: The reliability of routine health data in low and middle-income countries (LMICs) is often constrained by reporting delays and incomplete coverage, necessitating the exploration of novel data sources and analytics. Geospatial Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse spatial, temporal, and behavioral data into mathematical embeddings that can be efficiently used for downstream prediction tasks. This study evaluated the predictive performance of three GeoFM embedding sources - Google Population Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite imagery), and mobile phone call detail records (CDR) - for modeling 15 routine health programmatic outputs in Malawi, and compared their utility to traditional geospatial interpolation methods. We used XGBoost models on data from 552 health catchment areas (January 2021-May 2023), assessing performance with R2, and using an 80/20 training and test data split with 5-fold cross-validation used in training. While predictive performance was mixed, the embedding-based approaches improved upon baseline geostatistical methods in 13 of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three embedding sources produced the most robust predictions, achieving average 5-fold cross validated R2 values for indicators like population density (0.63), new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64, 0.68, and 0.55, respectively. Prediction was poor for prediction targets with low primary data availability, such as TB and malnutrition cases. These results demonstrate that GeoFM embeddings imbue a modest predictive improvement for select health and demographic outcomes in an LMIC context. We conclude that the integration of multiple GeoFM sources is an efficient and valuable tool for supplementing and strengthening constrained routine health information systems.</article>","contentLength":1962,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modular Linear Tokenization (MLT)","url":"https://arxiv.org/abs/2510.25952","date":1761883200,"author":"","guid":323273,"unread":true,"content":"<article>arXiv:2510.25952v1 Announce Type: new \nAbstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and deterministic technique for encoding high-cardinality categorical identifiers into compact numerical vectors. Unlike traditional hashing or one-hot encodings, MLT preserves bijective mappings by leveraging modular arithmetic over finite fields and invertible linear transformations. The method offers explicit control of dimensionality and computational scalability while maintaining full reversibility, even for millions of identifiers. Experimental results on the MovieLens 20M dataset show that MLT achieves comparable predictive performance to supervised embeddings while requiring significantly fewer parameters and lower training cost. An open-source implementation of MLT is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub (https://github.com/tcharliesschmitz/light-mlt).</article>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Estimating cognitive biases with attention-aware inverse planning","url":"https://arxiv.org/abs/2510.25951","date":1761883200,"author":"","guid":323274,"unread":true,"content":"<article>arXiv:2510.25951v1 Announce Type: new \nAbstract: People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the attention-aware inverse planning problem, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Multilingual Data Mixtures in Language Model Pretraining","url":"https://arxiv.org/abs/2510.25947","date":1761883200,"author":"","guid":323275,"unread":true,"content":"<article>arXiv:2510.25947v1 Announce Type: new \nAbstract: The impact of different multilingual data mixtures in pretraining large language models (LLMs) has been a topic of ongoing debate, often raising concerns about potential trade-offs between language coverage and model performance (i.e., the curse of multilinguality). In this work, we investigate these assumptions by training 1.1B and 3B parameter LLMs on diverse multilingual corpora, varying the number of languages from 25 to 400. Our study challenges common beliefs surrounding multilingual training. First, we find that combining English and multilingual data does not necessarily degrade the in-language performance of either group, provided that languages have a sufficient number of tokens included in the pretraining corpus. Second, we observe that using English as a pivot language (i.e., a high-resource language that serves as a catalyst for multilingual generalization) yields benefits across language families, and contrary to expectations, selecting a pivot language from within a specific family does not consistently improve performance for languages within that family. Lastly, we do not observe a significant \"curse of multilinguality\" as the number of training languages increases in models at this scale. Our findings suggest that multilingual data, when balanced appropriately, can enhance language model capabilities without compromising performance, even in low-resource settings</article>","contentLength":1452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reconfigurable Analog Computers","url":"https://arxiv.org/abs/2510.25942","date":1761883200,"author":"","guid":323276,"unread":true,"content":"<article>arXiv:2510.25942v1 Announce Type: new \nAbstract: The Achilles heel of classic analog computers was the complex, error prone, and time consuming process of programming. This typically involved manually patching hundreds or even thousands of connections between individual computing elements as well as setting many precision 10-turn potentiometers manually, often taking hours, or even days. Albeit being simplified by means of removable patch panels, switching from one program to another still was time consuming and thus expensive. With digital computers about to hit physical boundaries with respect to energy consumption, clock frequency, and integration density, analog computers have gained a lot of interest as co-processors for certain application areas in recent years. This requires some means for automatic reconfiguration of these systems under control of an attached digital computer. The following sections give an overview of classic and modern approaches towards such autopatch systems.</article>","contentLength":1002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline","url":"https://arxiv.org/abs/2510.25941","date":1761883200,"author":"","guid":323277,"unread":true,"content":"<article>arXiv:2510.25941v1 Announce Type: new \nAbstract: If we cannot inspect the training data of a large language model (LLM), how can we ever know what it has seen? We believe the most compelling evidence arises when the model itself freely reproduces the target content. As such, we propose RECAP, an agentic pipeline designed to elicit and verify memorized training data from LLM outputs. At the heart of RECAP is a feedback-driven loop, where an initial extraction attempt is evaluated by a secondary language model, which compares the output against a reference passage and identifies discrepancies. These are then translated into minimal correction hints, which are fed back into the target model to guide subsequent generations. In addition, to address alignment-induced refusals, RECAP includes a jailbreaking module that detects and overcomes such barriers. We evaluate RECAP on EchoTrace, a new benchmark spanning over 30 full books, and the results show that RECAP leads to substantial gains over single-iteration approaches. For instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text extraction improved from 0.38 to 0.47 - a nearly 24% increase.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SoK: Honeypots & LLMs, More Than the Sum of Their Parts?","url":"https://arxiv.org/abs/2510.25939","date":1761883200,"author":"","guid":323278,"unread":true,"content":"<article>arXiv:2510.25939v1 Announce Type: new \nAbstract: The advent of Large Language Models (LLMs) promised to resolve the long-standing paradox in honeypot design: achieving high-fidelity deception with low operational risk. However, despite a flurry of research since late 2022, progress has been incremental, and the field lacks a cohesive understanding of the emerging architectural patterns, core challenges, and evaluation paradigms. To fill this gap, this Systematization of Knowledge (SoK) paper provides the first comprehensive overview of this new domain. We survey and systematize three critical, intersecting research areas: first, we provide a taxonomy of honeypot detection vectors, structuring the core problems that LLM-based realism must solve; second, we synthesize the emerging literature on LLM-honeypots, identifying a canonical architecture and key evaluation trends; and third, we chart the evolutionary path of honeypot log analysis, from simple data reduction to automated intelligence generation. We synthesize these findings into a forward-looking research roadmap, arguing that the true potential of this technology lies in creating autonomous, self-improving deception systems to counter the emerging threat of intelligent, automated attackers.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Process Mining-Based System For The Analysis and Prediction of Software Development Workflows","url":"https://arxiv.org/abs/2510.25935","date":1761883200,"author":"","guid":323279,"unread":true,"content":"<article>arXiv:2510.25935v1 Announce Type: new \nAbstract: CodeSight is an end-to-end system designed to anticipate deadline compliance in software development workflows. It captures development and deployment data directly from GitHub, transforming it into process mining logs for detailed analysis. From these logs, the system generates metrics and dashboards that provide actionable insights into PR activity patterns and workflow efficiency. Building on this structured representation, CodeSight employs an LSTM model that predicts remaining PR resolution times based on sequential activity traces and static features, enabling early identification of potential deadline breaches. In tests, the system demonstrates high precision and F1 scores in predicting deadline compliance, illustrating the value of integrating process mining with machine learning for proactive software project management.</article>","contentLength":890,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust GNN Watermarking via Implicit Perception of Topological Invariants","url":"https://arxiv.org/abs/2510.25934","date":1761883200,"author":"","guid":323280,"unread":true,"content":"<article>arXiv:2510.25934v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) are valuable intellectual property, yet many watermarks rely on backdoor triggers that break under common model edits and create ownership ambiguity. We present InvGNN-WM, which ties ownership to a model's implicit perception of a graph invariant, enabling trigger-free, black-box verification with negligible task impact. A lightweight head predicts normalized algebraic connectivity on an owner-private carrier set; a sign-sensitive decoder outputs bits, and a calibrated threshold controls the false-positive rate. Across diverse node and graph classification datasets and backbones, InvGNN-WM matches clean accuracy while yielding higher watermark accuracy than trigger- and compression-based baselines. It remains strong under unstructured pruning, fine-tuning, and post-training quantization; plain knowledge distillation (KD) weakens the mark, while KD with a watermark loss (KD+WM) restores it. We provide guarantees for imperceptibility and robustness, and we prove that exact removal is NP-complete.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Humains-Junior: A 3.8B Language Model Achieving GPT-4o-Level Factual Accuracy by Directed Exoskeleton Reasoning","url":"https://arxiv.org/abs/2510.25933","date":1761883200,"author":"","guid":323281,"unread":true,"content":"<article>arXiv:2510.25933v1 Announce Type: new \nAbstract: We introduce Humans-Junior, a 3.8B model that matches GPT-4o on the FACTS Grounding public subset within a $\\pm 5$ pp equivalence margin.\n  Results. On Q1--Q500 under identical judges, GPT-4o scores 73.5% (95% CI 69.5--77.2) and Humans-Junior 72.7% (95% CI 68.7--76.5); the paired difference is 0.8 pp (bootstrap 95% CI $-3.1$ to $+4.7$; permutation $p = 0.72$; Cohen's $d = 0.023$). TOST establishes equivalence at $\\pm 5$ pp (not at $\\pm 3$ pp). When purchased as managed APIs, Humans-Junior's base model (Phi-3.5-mini-instruct) is $\\approx 19\\times$ less expensive than GPT-4o on Microsoft AI Foundry pricing; self-hosted or edge deployments can drive incremental inference cost toward zero. Measured vs estimated pricing sources are tabulated in Appendix E.\n  Method. Our approach combines minimal directed \"Exoskeleton Reasoning\" scaffolds with behavioral fine-tuning that teaches protocol compliance (epistemic discipline) rather than domain answers. Fine-tuning alone adds little; combined, they synergize (+17.7 pp, $p &lt; 0.001$) and reduce variance ($\\approx 25\\%$). In prompt-only settings on frontier models (Q1--Q100; non-comparable), directed reasoning improved GPT-4o by +11.8 pp to 85.3% and Gemini-2.5-Pro by +5.0 pp to 93.3% (baseline 88.3%, $n = 100$); see Section~5.\n  TL;DR. A 3.8B model achieves GPT-4o-level FACTS accuracy (equivalent within $\\pm 5$ pp on Q1--Q500). Cloud pricing shows $\\approx 19\\times$ lower cost versus GPT-4o, and self-hosted/edge deployments can approach zero marginal cost. Pricing sources are listed in Appendix E. Frontier prompt-only gains (Q1--Q100; non-comparable) and optimized-prompt exploratory results under earlier judges are summarized in Appendix F.\n  Keywords: Small Language Models, Factual Grounding, Directed Reasoning, Fine-Tuning, Model Alignment, Cost-Efficient AI</article>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FakeZero: Real-Time, Privacy-Preserving Misinformation Detection for Facebook and X","url":"https://arxiv.org/abs/2510.25932","date":1761883200,"author":"","guid":323282,"unread":true,"content":"<article>arXiv:2510.25932v1 Announce Type: new \nAbstract: Social platforms distribute information at unprecedented speed, which in turn accelerates the spread of misinformation and threatens public discourse. We present FakeZero, a fully client-side, cross-platform browser extension that flags unreliable posts on Facebook and X (formerly Twitter) while the user scrolls. All computation, DOM scraping, tokenisation, Transformer inference, and UI rendering run locally through the Chromium messaging API, so no personal data leaves the device.FakeZero employs a three-stage training curriculum: baseline fine-tuning and domain-adaptive training enhanced with focal loss, adversarial augmentation, and post-training quantisation. Evaluated on a dataset of 239,000 posts, the DistilBERT-Quant model (67.6 MB) reaches 97.1% macro-F1, 97.4% accuracy, and an AUROC of 0.996, with a median latency of approximately 103 ms on a commodity laptop. A memory-efficient TinyBERT-Quant variant retains 95.7% macro-F1 and 96.1% accuracy while shrinking the model to 14.7 MB and lowering latency to approximately 40 ms, showing that high-quality fake-news detection is feasible under tight resource budgets with only modest performance loss.By providing inline credibility cues, the extension can serve as a valuable tool for policymakers seeking to curb the spread of misinformation across social networks. With user consent, FakeZero also opens the door for researchers to collect large-scale datasets of fake news in the wild, enabling deeper analysis and the development of more robust detection techniques.</article>","contentLength":1588,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Agent Reinforcement Learning for Market Making: Competition without Collusion","url":"https://arxiv.org/abs/2510.25929","date":1761883200,"author":"","guid":323283,"unread":true,"content":"<article>arXiv:2510.25929v1 Announce Type: new \nAbstract: Algorithmic collusion has emerged as a central question in AI: Will the interaction between different AI agents deployed in markets lead to collusion? More generally, understanding how emergent behavior, be it a cartel or market dominance from more advanced bots, affects the market overall is an important research question.\n  We propose a hierarchical multi-agent reinforcement learning framework to study algorithmic collusion in market making. The framework includes a self-interested market maker (Agent~A), which is trained in an uncertain environment shaped by an adversary, and three bottom-layer competitors: the self-interested Agent~B1 (whose objective is to maximize its own PnL), the competitive Agent~B2 (whose objective is to minimize the PnL of its opponent), and the hybrid Agent~B$^\\star$, which can modulate between the behavior of the other two. To analyze how these agents shape the behavior of each other and affect market outcomes, we propose interaction-level metrics that quantify behavioral asymmetry and system-level dynamics, while providing signals potentially indicative of emergent interaction patterns.\n  Experimental results show that Agent~B2 secures dominant performance in a zero-sum setting against B1, aggressively capturing order flow while tightening average spreads, thus improving market execution efficiency. In contrast, Agent~B$^\\star$ exhibits a self-interested inclination when co-existing with other profit-seeking agents, securing dominant market share through adaptive quoting, yet exerting a milder adverse impact on the rewards of Agents~A and B1 compared to B2. These findings suggest that adaptive incentive control supports more sustainable strategic co-existence in heterogeneous agent environments and offers a structured lens for evaluating behavioral design in algorithmic trading systems.</article>","contentLength":1897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Active Learning with Task-Driven Representations for Messy Pools","url":"https://arxiv.org/abs/2510.25926","date":1761883200,"author":"","guid":323284,"unread":true,"content":"<article>arXiv:2510.25926v1 Announce Type: new \nAbstract: Active learning has the potential to be especially useful for messy, uncurated pools where datapoints vary in relevance to the target task. However, state-of-the-art approaches to this problem currently rely on using fixed, unsupervised representations of the pool, focusing on modifying the acquisition function instead. We show that this model setup can undermine their effectiveness at dealing with messy pools, as such representations can fail to capture important information relevant to the task. To address this, we propose using task-driven representations that are periodically updated during the active learning process using the previously collected labels. We introduce two specific strategies for learning these representations, one based on directly learning semi-supervised representations and the other based on supervised fine-tuning of an initial unsupervised representation. We find that both significantly improve empirical performance over using unsupervised or pretrained representations.</article>","contentLength":1059,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transferring Causal Effects using Proxies","url":"https://arxiv.org/abs/2510.25924","date":1761883200,"author":"","guid":323285,"unread":true,"content":"<article>arXiv:2510.25924v1 Announce Type: new \nAbstract: We consider the problem of estimating a causal effect in a multi-domain setting. The causal effect of interest is confounded by an unobserved confounder and can change between the different domains. We assume that we have access to a proxy of the hidden confounder and that all variables are discrete or categorical. We propose methodology to estimate the causal effect in the target domain, where we assume to observe only the proxy variable. Under these conditions, we prove identifiability (even when treatment and response variables are continuous). We introduce two estimation techniques, prove consistency, and derive confidence intervals. The theoretical results are supported by simulation studies and a real-world example studying the causal effect of website rankings on consumer choices.</article>","contentLength":847,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy","url":"https://arxiv.org/abs/2510.25921","date":1761883200,"author":"","guid":323286,"unread":true,"content":"<article>arXiv:2510.25921v1 Announce Type: new \nAbstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and atom manipulation, but its utility is often limited by tip degradation and slow serial data acquisition. Fabrication adds another layer of complexity since the tip is often subjected to large voltages, which may alter the shape of its apex, requiring it to be conditioned. Here, we propose a machine learning (ML) approach for image repair and super-resolution to alleviate both challenges. Using a dataset of only 36 pristine experimental images of Si(001):H, we demonstrate that a physics-informed synthetic data generation pipeline can be used to train several state-of-the-art flow-matching and diffusion models. Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy (CMMD) score and structural similarity demonstrates that our models are able to effectively restore images and offer a two- to fourfold reduction in image acquisition time by accurately reconstructing images from sparsely sampled data. Our framework has the potential to significantly increase STM experimental throughput by offering a route to reducing the frequency of tip-conditioning procedures and to enhancing frame rates in existing high-speed STM systems.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coherence-Aware Distributed Learning under Heterogeneous Downlink Impairments","url":"https://arxiv.org/abs/2510.25917","date":1761883200,"author":"","guid":323287,"unread":true,"content":"<article>arXiv:2510.25917v1 Announce Type: new \nAbstract: The performance of federated learning (FL) over wireless networks critically depends on accurate and timely channel state information (CSI) across distributed devices. This requirement is tightly linked to how rapidly the channel gains vary, i.e., the coherence intervals. In practice, edge devices often exhibit unequal coherence times due to differences in mobility and scattering environments, leading to unequal demands for pilot signaling and channel estimation resources. Conventional FL schemes that overlook this coherence disparity can suffer from severe communication inefficiencies and training overhead. This paper proposes a coherence-aware, communication-efficient framework for joint channel training and model updating in practical wireless FL systems operating under heterogeneous fading dynamics. Focusing on downlink impairments, we introduce a resource-reuse strategy based on product superposition, enabling the parameter server to efficiently schedule both static and dynamic devices by embedding global model updates for static devices within pilot transmissions intended for mobile devices. We theoretically analyze the convergence behavior of the proposed scheme and quantify its gains in expected communication efficiency and training accuracy. Experiments demonstrate the effectiveness of the proposed framework under mobility-induced dynamics and offer useful insights for the practical deployment of FL over wireless channels.</article>","contentLength":1504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FinOps Agent -- A Use-Case for IT Infrastructure and Cost Optimization","url":"https://arxiv.org/abs/2510.25914","date":1761883200,"author":"","guid":323288,"unread":true,"content":"<article>arXiv:2510.25914v1 Announce Type: new \nAbstract: FinOps (Finance + Operations) represents an operational framework and cultural practice which maximizes cloud business value through collaborative financial accountability across engineering, finance, and business teams. FinOps practitioners face a fundamental challenge: billing data arrives in heterogeneous formats, taxonomies, and metrics from multiple cloud providers and internal systems which eventually lead to synthesizing actionable insights, and making time-sensitive decisions. To address this challenge, we propose leveraging autonomous, goal-driven AI agents for FinOps automation. In this paper, we built a FinOps agent for a typical use-case for IT infrastructure and cost optimization. We built a system simulating a realistic end-to-end industry process starting with retrieving data from various sources to consolidating and analyzing the data to generate recommendations for optimization. We defined a set of metrics to evaluate our agent using several open-source and close-source language models and it shows that the agent was able to understand, plan, and execute tasks as well as an actual FinOps practitioner.</article>","contentLength":1184,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Risk-Aware Safety Filters with Poisson Safety Functions and Laplace Guidance Fields","url":"https://arxiv.org/abs/2510.25913","date":1761883200,"author":"","guid":323289,"unread":true,"content":"<article>arXiv:2510.25913v1 Announce Type: new \nAbstract: Robotic systems navigating in real-world settings require a semantic understanding of their environment to properly determine safe actions. This work aims to develop the mathematical underpinnings of such a representation--specifically, the goal is to develop safety filters that are risk-aware. To this end, we take a two step approach: encoding an understanding of the environment via Poisson's equation, and associated risk via Laplace guidance fields. That is, we first solve a Dirichlet problem for Poisson's equation to generate a safety function that encodes system safety as its 0-superlevel set. We then separately solve a Dirichlet problem for Laplace's equation to synthesize a safe \\textit{guidance field} that encodes variable levels of caution around obstacles -- by enforcing a tunable flux boundary condition. The safety function and guidance fields are then combined to define a safety constraint and used to synthesize a risk-aware safety filter which, given a semantic understanding of an environment with associated risk levels of environmental features, guarantees safety while prioritizing avoidance of higher risk obstacles. We demonstrate this method in simulation and discuss how \\textit{a priori} understandings of obstacle risk can be directly incorporated into the safety filter to generate safe behaviors that are risk-aware.</article>","contentLength":1403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SciTrust 2.0: A Comprehensive Framework for Evaluating Trustworthiness of Large Language Models in Scientific Applications","url":"https://arxiv.org/abs/2510.25908","date":1761883200,"author":"","guid":323290,"unread":true,"content":"<article>arXiv:2510.25908v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated transformative potential in scientific research, yet their deployment in high-stakes contexts raises significant trustworthiness concerns. Here, we introduce SciTrust 2.0, a comprehensive framework for evaluating LLM trustworthiness in scientific applications across four dimensions: truthfulness, adversarial robustness, scientific safety, and scientific ethics. Our framework incorporates novel, open-ended truthfulness benchmarks developed through a verified reflection-tuning pipeline and expert validation, alongside a novel ethics benchmark for scientific research contexts covering eight subcategories including dual-use research and bias. We evaluated seven prominent LLMs, including four science-specialized models and three general-purpose industry models, using multiple evaluation metrics including accuracy, semantic similarity measures, and LLM-based scoring. General-purpose industry models overall outperformed science-specialized models across each trustworthiness dimension, with GPT-o4-mini demonstrating superior performance in truthfulness assessments and adversarial robustness. Science-specialized models showed significant deficiencies in logical and ethical reasoning capabilities, along with concerning vulnerabilities in safety evaluations, particularly in high-risk domains such as biosecurity and chemical weapons. By open-sourcing our framework, we provide a foundation for developing more trustworthy AI systems and advancing research on model safety and ethics in scientific contexts.</article>","contentLength":1612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hybrid Reconstruction Framework for Efficient High-Order Shock-Capturing on Unstructured Meshes","url":"https://arxiv.org/abs/2510.25906","date":1761883200,"author":"","guid":323291,"unread":true,"content":"<article>arXiv:2510.25906v1 Announce Type: new \nAbstract: We present a multi-dimensional, arbitrary-order hybrid reconstruction framework for compressible flows on unstructured meshes. The method advances high-resolution schemes by combining the efficiency of linear reconstruction with the robustness of nonlinear formulations, activated only when needed through a novel a priori detection strategy. This minimizes the use of costly Compact Weighted Essentially Non-Oscillatory (CWENOZ) or Monotonic Upstream-centered Scheme for Conservation Laws (MUSCL) reconstructions, reducing computational cost without compromising accuracy or stability. The framework merges CWENOZ and the Multi-dimensional Optimal Order Detection (MOOD) paradigm while introducing a redesigned Numerical Admissibility Detector (NAD) that classifies the local flow into smooth, weakly non-smooth, and discontinuous regions in a single step. Each region is then reconstructed using an optimal method: a high-order linear scheme in smooth areas, CWENOZ in weakly non-smooth zones, and a second-order MUSCL near discontinuities. This targeted a priori allocation preserves high-order accuracy where possible and ensures stable, non-oscillatory behavior near shocks and steep gradients. Implemented within the open-source unstructured finite-volume solver UCNS3D, the framework supports arbitrary-order reconstructions on mixed-element meshes. Extensive two- and three-dimensional benchmarks confirm that it retains the designed accuracy in smooth regions while greatly improving robustness in shock-dominated flows. Thanks to the reduced frequency of nonlinear reconstructions, the method achieves up to 2.5x speed-up over a CWENOZ scheme of equal order in 3D compressible turbulence. This hybrid approach thus brings high-order accuracy closer to industrial-scale CFD through its balance of efficiency, robustness, and reliability.</article>","contentLength":1895,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation","url":"https://arxiv.org/abs/2510.25904","date":1761883200,"author":"","guid":323292,"unread":true,"content":"<article>arXiv:2510.25904v1 Announce Type: new \nAbstract: The use of LLM-based applications as a means to accelerate and/or substitute human labor in the creation of language resources and dataset is a reality. Nonetheless, despite the potential of such tools for linguistic research, comprehensive evaluation of their performance and impact on the creation of annotated datasets, especially under a perspectivized approach to NLP, is still missing. This paper contributes to reduction of this gap by reporting on an extensive evaluation of the (semi-)automatization of FrameNet-like semantic annotation by the use of an LLM-based semantic role labeler. The methodology employed compares annotation time, coverage and diversity in three experimental settings: manual, automatic and semi-automatic annotation. Results show that the hybrid, semi-automatic annotation setting leads to increased frame diversity and similar annotation coverage, when compared to the human-only setting, while the automatic setting performs considerably worse in all metrics, except for annotation time.</article>","contentLength":1072,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BikeScenes: Online LiDAR Semantic Segmentation for Bicycles","url":"https://arxiv.org/abs/2510.25901","date":1761883200,"author":"","guid":323293,"unread":true,"content":"<article>arXiv:2510.25901v1 Announce Type: new \nAbstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster e-bikes, motivates adapting automotive perception technologies for bicycle safety. We use our multi-sensor 'SenseBike' research platform to develop and evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg Dataset, comprising 3021 consecutive LiDAR scans around the university campus of the TU Delft, semantically annotated for 29 dynamic and static classes. By evaluating model performance, we demonstrate that fine-tuning on our BikeScenes dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This result underscores the necessity and effectiveness of domain-specific training. We highlight key challenges specific to bicycle-mounted, hardware-constrained perception systems and contribute the BikeScenes dataset as a resource for advancing research in cyclist-centric LiDAR segmentation.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency","url":"https://arxiv.org/abs/2510.25897","date":1761883200,"author":"","guid":323294,"unread":true,"content":"<article>arXiv:2510.25897v1 Announce Type: new \nAbstract: Current text-to-image generative models are trained on large uncurated datasets to enable diverse generation capabilities. However, this does not align well with user preferences. Recently, reward models have been specifically designed to perform post-hoc selection of generated images and align them to a reward, typically user preference. This discarding of informative data together with the optimizing for a single reward tend to harm diversity, semantic fidelity and efficiency. Instead of this post-processing, we propose to condition the model on multiple reward models during training to let the model learn user preferences directly. We show that this not only dramatically improves the visual quality of the generated images but it also significantly speeds up the training. Our proposed method, called MIRO, achieves state-of-the-art performances on the GenEval compositional benchmark and user-preference scores (PickAScore, ImageReward, HPSv2).</article>","contentLength":1006,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Topology-Aware Active Learning on Graphs","url":"https://arxiv.org/abs/2510.25892","date":1761883200,"author":"","guid":323295,"unread":true,"content":"<article>arXiv:2510.25892v1 Announce Type: new \nAbstract: We propose a graph-topological approach to active learning that directly targets the core challenge of exploration versus exploitation under scarce label budgets. To guide exploration, we introduce a coreset construction algorithm based on Balanced Forman Curvature (BFC), which selects representative initial labels that reflect the graph's cluster structure. This method includes a data-driven stopping criterion that signals when the graph has been sufficiently explored. We further use BFC to dynamically trigger the shift from exploration to exploitation within active learning routines, replacing hand-tuned heuristics. To improve exploitation, we introduce a localized graph rewiring strategy that efficiently incorporates multiscale information around labeled nodes, enhancing label propagation while preserving sparsity. Experiments on benchmark classification tasks show that our methods consistently outperform existing graph-based semi-supervised baselines at low label rates.</article>","contentLength":1037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints","url":"https://arxiv.org/abs/2510.25890","date":1761883200,"author":"","guid":323296,"unread":true,"content":"<article>arXiv:2510.25890v1 Announce Type: new \nAbstract: PRISM unifies Large Language Models with Model-Driven Engineering to generate regulator-ready artifacts and machine-checkable evidence for safety- and compliance-critical domains. PRISM integrates three pillars: a Unified Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a single semantic space; an Integrated Constraint Model (ICM) compiles structural and semantic requirements into enforcement artifacts including generation-time automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and Constraint-Guided Verifiable Generation (CVG) applies these through two-layer enforcement - structural constraints drive prefix-safe decoding while semantic/logical validation produces machine-checkable certificates. When violations occur, PRISM performs audit-guided repair and records generation traces for compliance review. We evaluate PRISM in automotive software engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis). PRISM produces structurally valid, auditable artifacts that integrate with existing tooling and substantially reduce manual remediation effort, providing a practical path toward automated artifact generation with built-in assurance.</article>","contentLength":1260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\pi_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models","url":"https://arxiv.org/abs/2510.25889","date":1761883200,"author":"","guid":323297,"unread":true,"content":"<article>arXiv:2510.25889v1 Announce Type: new \nAbstract: Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (e.g., $\\pi_0$, $\\pi_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising.\n  We address this challenge with $\\pi_{\\text{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $\\pi_{\\text{RL}}$ implements two RL algorithms: (1) {Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) {Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration.\n  We evaluate $\\pi_{\\text{RL}}$ on LIBERO and ManiSkill benchmarks. On LIBERO, $\\pi_{\\text{RL}}$ boosts few-shot SFT models $\\pi_0$ and $\\pi_{0.5}$ from 57.6% to 97.6% and from 77.1% to 98.3%, respectively. In ManiSkill, we train $\\pi_{\\text{RL}}$ in 320 parallel environments, improving $\\pi_0$ from 41.6% to 85.7% and $\\pi_{0.5}$ from 40.0% to 84.8% across 4352 pick-and-place tasks, demonstrating scalable multitask RL under heterogeneous simulation.\n  Overall, $\\pi_{\\text{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs.</article>","contentLength":1587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Targeted Resilient Zoning for High Impact Events via Multi Circuit Polelines","url":"https://arxiv.org/abs/2510.25885","date":1761883200,"author":"","guid":323298,"unread":true,"content":"<article>arXiv:2510.25885v1 Announce Type: new \nAbstract: The increasing frequency and severity of High Impact and Low Probability events such as hurricanes and windstorms pose significant challenges to the resilience of electrical power distribution systems, particularly in regions of New England where there is a significant amount of overhead infrastructure in areas where vegetation is predominant. Traditional reliability-focused planning is insufficient to address the systemic vulnerabilities exposed by such extreme events. This paper presents a novel risk based framework for long term resilience planning of active overhead distribution systems, with a specific focus on mitigating the impacts of high wind and hurricane induced outages.</article>","contentLength":739,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Approximating Human Preferences Using a Multi-Judge Learned System","url":"https://arxiv.org/abs/2510.25884","date":1761883200,"author":"","guid":323299,"unread":true,"content":"<article>arXiv:2510.25884v1 Announce Type: new \nAbstract: Aligning LLM-based judges with human preferences is a significant challenge, as they are difficult to calibrate and often suffer from rubric sensitivity, bias, and instability. Overcoming this challenge advances key applications, such as creating reliable reward models for Reinforcement Learning from Human Feedback (RLHF) and building effective routing systems that select the best-suited model for a given user query. In this work, we propose a framework for modeling diverse, persona-based preferences by learning to aggregate outputs from multiple rubric-conditioned judges. We investigate the performance of this approach against naive baselines and assess its robustness through case studies on both human and LLM-judges biases. Our primary contributions include a persona-based method for synthesizing preference labels at scale and two distinct implementations of our aggregator: Generalized Additive Model (GAM) and a Multi-Layer Perceptron (MLP).</article>","contentLength":1006,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence","url":"https://arxiv.org/abs/2510.25883","date":1761883200,"author":"","guid":323300,"unread":true,"content":"<article>arXiv:2510.25883v1 Announce Type: new \nAbstract: Existing frameworks converge on the centrality of compression to intelligence but leave underspecified why this process enforces the discovery of causal structure rather than superficial statistical patterns. We introduce a two-level framework to address this gap. The Information-Theoretic Imperative (ITI) establishes that any system persisting in uncertain environments must minimize epistemic entropy through predictive compression: this is the evolutionary \"why\" linking survival pressure to information-processing demands. The Compression Efficiency Principle (CEP) specifies how efficient compression mechanically selects for generative, causal models through exception-accumulation dynamics, making reality alignment a consequence rather than a contingent achievement. Together, ITI and CEP define a causal chain: from survival pressure to prediction necessity, compression requirement, efficiency optimization, generative structure discovery, and ultimately reality alignment. Each link follows from physical, information-theoretic, or evolutionary constraints, implying that intelligence is the mechanically necessary outcome of persistence in structured environments. This framework yields empirically testable predictions: compression efficiency, measured as approach to the rate-distortion frontier, correlates with out-of-distribution generalization; exception-accumulation rates differentiate causal from correlational models; hierarchical systems exhibit increasing efficiency across abstraction layers; and biological systems demonstrate metabolic costs that track representational complexity. ITI and CEP thereby provide a unified account of convergence across biological, artificial, and multi-scale systems, addressing the epistemic and functional dimensions of intelligence without invoking assumptions about consciousness or subjective experience.</article>","contentLength":1918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Internal Vulnerabilities, External Threats: A Grounded Framework for Enterprise Open Source Risk Governance","url":"https://arxiv.org/abs/2510.25882","date":1761883200,"author":"","guid":323301,"unread":true,"content":"<article>arXiv:2510.25882v1 Announce Type: new \nAbstract: Enterprise engagement with open source has evolved from tactical adoption to strategic deep integration, exposing them to a complex risk landscape far beyond mere code. However, traditional risk management, narrowly focused on technical tools, is structurally inadequate for systemic threats like upstream \"silent fixes\", community conflicts, or sudden license changes, creating a dangerous governance blind spot. To address this governance vacuum and enable the necessary shift from tactical risk management to holistic risk governance, we conducted a grounded theory study with 15 practitioners to develop a holistic risk governance framework. Our study formalizes an analytical framework built on a foundational risk principle: an uncontrollable External Threat (e.g., a sudden license change in a key dependency) only becomes a critical risk when it exploits a controllable Internal Vulnerability (e.g., an undefined risk appetite for single-vendor projects), which then amplifies the impact.The framework operationalizes this principle through a clear logical chain: \"Objectives -&gt; Threats -&gt; Vulnerabilities -&gt; Mitigation\" (OTVM). This provides a holistic decision model that transcends mere technical checklists. Based on this logic, our contributions are: (1) a \"Strategic Objectives Matrix\" to clarify goals; (2) a systematic dual taxonomy of External Threats (Ex-Tech, Ex-Comm, Ex-Eco) and Internal Vulnerabilities (In-Strat, In-Ops, In-Tech); and (3) an actionable mitigation framework mapping capability-building to these vulnerabilities. The framework's analytical utility was validated by three industry experts through retrospective case studies on real-world incidents. This work provides a novel diagnostic lens and a systematic path for enterprises to shift from reactive \"firefighting\" to proactively building an organizational \"immune system\".</article>","contentLength":1912,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Foundations of Fiat-Denominated Loans Collateralized by Cryptocurrencies","url":"https://arxiv.org/abs/2510.25878","date":1761883200,"author":"","guid":323302,"unread":true,"content":"<article>arXiv:2510.25878v1 Announce Type: new \nAbstract: The rising importance of cryptocurrencies as financial assets pushed their applicability from an object of speculation closer to standard financial instruments such as loans. In this work, we initiate the study of secure protocols that enable fiat-denominated loans collateralized by cryptocurrencies such as Bitcoin. We provide limited-custodial protocols for such loans relying only on trusted arbitration and provide their game-theoretical analysis. We also highlight various interesting directions for future research.</article>","contentLength":571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedVLSynther: Synthesizing High-Quality Visual Question Answering from Medical Documents with Generator-Verifier LMMs","url":"https://arxiv.org/abs/2510.25867","date":1761883200,"author":"","guid":323303,"unread":true,"content":"<article>arXiv:2510.25867v1 Announce Type: new \nAbstract: Large Multimodal Models (LMMs) are increasingly capable of answering medical questions that require joint reasoning over images and text, yet training general medical VQA systems is impeded by the lack of large, openly usable, high-quality corpora. We present MedVLSynther, a rubric-guided generator-verifier framework that synthesizes high-quality multiple-choice VQA items directly from open biomedical literature by conditioning on figures, captions, and in-text references. The generator produces self-contained stems and parallel, mutually exclusive options under a machine-checkable JSON schema; a multi-stage verifier enforces essential gates (self-containment, single correct answer, clinical validity, image-text consistency), awards fine-grained positive points, and penalizes common failure modes before acceptance. Applying this pipeline to PubMed Central yields MedSynVQA: 13,087 audited questions over 14,803 images spanning 13 imaging modalities and 28 anatomical regions. Training open-weight LMMs with reinforcement learning using verifiable rewards improves accuracy across six medical VQA benchmarks, achieving averages of 55.85 (3B) and 58.15 (7B), with up to 77.57 on VQA-RAD and 67.76 on PathVQA, outperforming strong medical LMMs. A Ablations verify that both generation and verification are necessary and that more verified data consistently helps, and a targeted contamination analysis detects no leakage from evaluation suites. By operating entirely on open literature and open-weight models, MedVLSynther offers an auditable, reproducible, and privacy-preserving path to scalable medical VQA training data.</article>","contentLength":1682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AAGATE: A NIST AI RMF-Aligned Governance Platform for Agentic AI","url":"https://arxiv.org/abs/2510.25863","date":1761883200,"author":"","guid":323304,"unread":true,"content":"<article>arXiv:2510.25863v1 Announce Type: new \nAbstract: This paper introduces the Agentic AI Governance Assurance &amp; Trust Engine (AAGATE), a Kubernetes-native control plane designed to address the unique security and governance challenges posed by autonomous, language-model-driven agents in production. Recognizing the limitations of traditional Application Security (AppSec) tooling for improvisational, machine-speed systems, AAGATE operationalizes the NIST AI Risk Management Framework (AI RMF). It integrates specialized security frameworks for each RMF function: the Agentic AI Threat Modeling MAESTRO framework for Map, a hybrid of OWASP's AIVSS and SEI's SSVC for Measure, and the Cloud Security Alliance's Agentic AI Red Teaming Guide for Manage. By incorporating a zero-trust service mesh, an explainable policy engine, behavioral analytics, and decentralized accountability hooks, AAGATE provides a continuous, verifiable governance solution for agentic AI, enabling safe, accountable, and scalable deployment. The framework is further extended with DIRF for digital identity rights, LPCI defenses for logic-layer injection, and QSAF monitors for cognitive degradation, ensuring governance spans systemic, adversarial, and ethical risks.</article>","contentLength":1241,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online 3-Taxi on General Metrics","url":"https://arxiv.org/abs/2510.25861","date":1761883200,"author":"","guid":323305,"unread":true,"content":"<article>arXiv:2510.25861v1 Announce Type: new \nAbstract: The online $k$-taxi problem, introduced in 1990 by Fiat, Rabani and Ravid, is a generalization of the $k$-server problem where $k$ taxis must serve a sequence of requests in a metric space. Each request is a pair of two points, representing the pick-up and drop-off location of a passenger. In the interesting ''hard'' version of the problem, the cost is the total distance that the taxis travel without a passenger. The problem is known to be substantially harder than the $k$-server problem, and prior to this work even for $k=3$ taxis it has been unknown whether a finite competitive ratio is achievable on general metric spaces. We present an $O(1)$-competitive algorithm for the $3$-taxi problem.</article>","contentLength":750,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Through the Judge's Eyes: Inferred Thinking Traces Improve Reliability of LLM Raters","url":"https://arxiv.org/abs/2510.25860","date":1761883200,"author":"","guid":323306,"unread":true,"content":"<article>arXiv:2510.25860v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly used as raters for evaluation tasks. However, their reliability is often limited for subjective tasks, when human judgments involve subtle reasoning beyond annotation labels. Thinking traces, the reasoning behind a judgment, are highly informative but challenging to collect and curate. We present a human-LLM collaborative framework to infer thinking traces from label-only annotations. The proposed framework uses a simple and effective rejection sampling method to reconstruct these traces at scale. These inferred thinking traces are applied to two complementary tasks: (1) fine-tuning open LLM raters; and (2) synthesizing clearer annotation guidelines for proprietary LLM raters. Across multiple datasets, our methods lead to significantly improved LLM-human agreement. Additionally, the refined annotation guidelines increase agreement among different LLM models. These results suggest that LLMs can serve as practical proxies for otherwise unrevealed human thinking traces, enabling label-only corpora to be extended into thinking-trace-augmented resources that enhance the reliability of LLM raters.</article>","contentLength":1203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Critical Roadmap to Driver Authentication via CAN Bus: Dataset Review, Introduction of the Kidmose CANid Dataset (KCID), and Proof of Concept","url":"https://arxiv.org/abs/2510.25856","date":1761883200,"author":"","guid":323307,"unread":true,"content":"<article>arXiv:2510.25856v1 Announce Type: new \nAbstract: Modern vehicles remain vulnerable to unauthorized use and theft despite traditional security measures including immobilizers and keyless entry systems. Criminals exploit vulnerabilities in Controller Area Network (CAN) bus systems to bypass authentication mechanisms, while social media trends have expanded auto theft to include recreational joyriding by underage drivers. Driver authentication via CAN bus data offers a promising additional layer of defense-in-depth protection, but existing open-access driver fingerprinting datasets suffer from critical limitations including reliance on decoded diagnostic data rather than raw CAN traffic, artificial fixed-route experimental designs, insufficient sampling rates, and lack of demographic information.\n  This paper provides a comprehensive review of existing open-access driver fingerprinting datasets, analyzing their strengths and limitations to guide practitioners in dataset selection. We introduce the Kidmose CANid Dataset (KCID), which addresses these fundamental shortcomings by providing raw CAN bus data from 16 drivers across four vehicles, including essential demographic information and both daily driving and controlled fixed-route data. Beyond dataset contributions, we present a driver authentication anti-theft framework and implement a proof-of-concept prototype on a single-board computer. Through live road trials with an unaltered passenger vehicle, we demonstrate the practical feasibility of CAN bus-based driver authentication anti-theft systems. Finally, we explore diverse applications of KCID beyond driver authentication, including driver profiling for insurance and safety assessments, mechanical anomaly detection, young driver monitoring, and impaired driving detection. This work provides researchers with both the data and methodological foundation necessary to develop robust, deployable driver authentication systems...</article>","contentLength":1957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Debate2Create: Robot Co-design via Large Language Model Debates","url":"https://arxiv.org/abs/2510.25850","date":1761883200,"author":"","guid":323308,"unread":true,"content":"<article>arXiv:2510.25850v1 Announce Type: new \nAbstract: Automating the co-design of a robot's morphology and control is a long-standing challenge due to the vast design space and the tight coupling between body and behavior. We introduce Debate2Create (D2C), a framework in which large language model (LLM) agents engage in a structured dialectical debate to jointly optimize a robot's design and its reward function. In each round, a design agent proposes targeted morphological modifications, and a control agent devises a reward function tailored to exploit the new design. A panel of pluralistic judges then evaluates the design-control pair in simulation and provides feedback that guides the next round of debate. Through iterative debates, the agents progressively refine their proposals, producing increasingly effective robot designs. Notably, D2C yields diverse and specialized morphologies despite no explicit diversity objective. On a quadruped locomotion benchmark, D2C discovers designs that travel 73% farther than the default, demonstrating that structured LLM-based debate can serve as a powerful mechanism for emergent robot co-design. Our results suggest that multi-agent debate, when coupled with physics-grounded feedback, is a promising new paradigm for automated robot design.</article>","contentLength":1292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symbolically Scaffolded Play: Designing Role-Sensitive Prompts for Generative NPC Dialogue","url":"https://arxiv.org/abs/2510.25820","date":1761883200,"author":"","guid":323309,"unread":true,"content":"<article>arXiv:2510.25820v1 Announce Type: new \nAbstract: Large Language Models (LLMs) promise to transform interactive games by enabling non-player characters (NPCs) to sustain unscripted dialogue. Yet it remains unclear whether constrained prompts actually improve player experience. We investigate this question through The Interview, a voice-based detective game powered by GPT-4o. A within-subjects usability study ($N=10$) compared high-constraint (HCP) and low-constraint (LCP) prompts, revealing no reliable experiential differences beyond sensitivity to technical breakdowns. Guided by these findings, we redesigned the HCP into a hybrid JSON+RAG scaffold and conducted a synthetic evaluation with an LLM judge, positioned as an early-stage complement to usability testing. Results uncovered a novel pattern: scaffolding effects were role-dependent: the Interviewer (quest-giver NPC) gained stability, while suspect NPCs lost improvisational believability. These findings overturn the assumption that tighter constraints inherently enhance play. Extending fuzzy-symbolic scaffolding, we introduce \\textit{Symbolically Scaffolded Play}, a framework in which symbolic structures are expressed as fuzzy, numerical boundaries that stabilize coherence where needed while preserving improvisation where surprise sustains engagement.</article>","contentLength":1326,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Identity Management for Agentic AI: The new frontier of authorization, authentication, and security for an AI agent world","url":"https://arxiv.org/abs/2510.25819","date":1761883200,"author":"","guid":323310,"unread":true,"content":"<article>arXiv:2510.25819v1 Announce Type: new \nAbstract: The rapid rise of AI agents presents urgent challenges in authentication, authorization, and identity management. Current agent-centric protocols (like MCP) highlight the demand for clarified best practices in authentication and authorization. Looking ahead, ambitions for highly autonomous agents raise complex long-term questions regarding scalable access control, agent-centric identities, AI workload differentiation, and delegated authority. This OpenID Foundation whitepaper is for stakeholders at the intersection of AI agents and access management. It outlines the resources already available for securing today's agents and presents a strategic agenda to address the foundational authentication, authorization, and identity problems pivotal for tomorrow's widespread autonomous systems.</article>","contentLength":844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion","url":"https://arxiv.org/abs/2510.25818","date":1761883200,"author":"","guid":323311,"unread":true,"content":"<article>arXiv:2510.25818v1 Announce Type: new \nAbstract: Text-to-image diffusion models often exhibit degraded performance when generating images beyond their training resolution. Recent training-free methods can mitigate this limitation, but they often require substantial computation or are incompatible with recent Diffusion Transformer models. In this paper, we propose ScaleDiff, a model-agnostic and highly efficient framework for extending the resolution of pretrained diffusion models without any additional training. A core component of our framework is Neighborhood Patch Attention (NPA), an efficient mechanism that reduces computational redundancy in the self-attention layer with non-overlapping patches. We integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing (LFM) to better generate fine details. Furthermore, we apply Structure Guidance to enhance global structure during the denoising process. Experimental results demonstrate that ScaleDiff achieves state-of-the-art performance among training-free methods in terms of both image quality and inference speed on both U-Net and Diffusion Transformer architectures.</article>","contentLength":1146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Efficient Large Language Model Training: From Data-centric Perspectives","url":"https://arxiv.org/abs/2510.25817","date":1761883200,"author":"","guid":323312,"unread":true,"content":"<article>arXiv:2510.25817v1 Announce Type: new \nAbstract: Post-training of Large Language Models (LLMs) is crucial for unlocking their task generalization potential and domain-specific capabilities. However, the current LLM post-training paradigm faces significant data challenges, including the high costs of manual annotation and diminishing marginal returns on data scales. Therefore, achieving data-efficient post-training has become a key research question. In this paper, we present the first systematic survey of data-efficient LLM post-training from a data-centric perspective. We propose a taxonomy of data-efficient LLM post-training methods, covering data selection, data quality enhancement, synthetic data generation, data distillation and compression, and self-evolving data ecosystems. We summarize representative approaches in each category and outline future research directions. By examining the challenges in data-efficient LLM post-training, we highlight open problems and propose potential research avenues. We hope our work inspires further exploration into maximizing the potential of data utilization in large-scale model training. Paper List: https://github.com/luo-junyu/Awesome-Data-Efficient-LLM</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Long Context: When Semantics Matter More than Tokens","url":"https://arxiv.org/abs/2510.25816","date":1761883200,"author":"","guid":323313,"unread":true,"content":"<article>arXiv:2510.25816v1 Announce Type: new \nAbstract: Electronic Health Records (EHR) store clinical documentation as base64 encoded attachments in FHIR DocumentReference resources, which makes semantic question answering difficult. Traditional vector database methods often miss nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR) method, introduced by Lopez et al. 2025, uses entity aware retrieval and achieved improved performance with an F1 score of 0.90 versus 0.86 for embedding based retrieval, while using over 70 percent fewer tokens. We developed a Clinical Notes QA Evaluation Platform to validate CLEAR against zero shot large context inference and traditional chunk based retrieval augmented generation. The platform was tested on 12 clinical notes ranging from 10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a 58.3 percent win rate, an average semantic similarity of 0.878, and used 78 percent fewer tokens than wide context processing. The largest performance gains occurred on long notes, with a 75 percent win rate for documents exceeding 65,000 tokens. These findings confirm that entity aware retrieval improves both efficiency and accuracy in clinical natural language processing. The evaluation framework provides a reusable and transparent benchmark for assessing clinical question answering systems where semantic precision and computational efficiency are critical.</article>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Agentic Framework for Rapid Deployment of Edge AI Solutions in Industry 5.0","url":"https://arxiv.org/abs/2510.25813","date":1761883200,"author":"","guid":323314,"unread":true,"content":"<article>arXiv:2510.25813v1 Announce Type: new \nAbstract: We present a novel framework for Industry 5.0 that simplifies the deployment of AI models on edge devices in various industrial settings. The design reduces latency and avoids external data transfer by enabling local inference and real-time processing. Our implementation is agent-based, which means that individual agents, whether human, algorithmic, or collaborative, are responsible for well-defined tasks, enabling flexibility and simplifying integration. Moreover, our framework supports modular integration and maintains low resource requirements. Preliminary evaluations concerning the food industry in real scenarios indicate improved deployment time and system adaptability performance. The source code is publicly available at https://github.com/AI-REDGIO-5-0/ci-component.</article>","contentLength":832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adversarial Pre-Padding: Generating Evasive Network Traffic Against Transformer-Based Classifiers","url":"https://arxiv.org/abs/2510.25810","date":1761883200,"author":"","guid":323315,"unread":true,"content":"<article>arXiv:2510.25810v1 Announce Type: new \nAbstract: To date, traffic obfuscation techniques have been widely adopted to protect network data privacy and security by obscuring the true patterns of traffic. Nevertheless, as the pre-trained models emerge, especially transformer-based classifiers, existing traffic obfuscation methods become increasingly vulnerable, as witnessed by current studies reporting the traffic classification accuracy up to 99\\% or higher. To counter such high-performance transformer-based classification models, we in this paper propose a novel and effective \\underline{adv}ersarial \\underline{traffic}-generating approach (AdvTraffic\\footnote{The code and data are available at: http://xxx}). Our approach has two key innovations: (i) a pre-padding strategy is proposed to modify packets, which effectively overcomes the limitations of existing research against transformer-based models for network traffic classification; and (ii) a reinforcement learning model is employed to optimize network traffic perturbations, aiming to maximize adversarial effectiveness against transformer-based classification models. To the best of our knowledge, this is the first attempt to apply adversarial perturbation techniques to defend against transformer-based traffic classifiers. Furthermore, our method can be easily deployed into practical network environments. Finally, multi-faceted experiments are conducted across several real-world datasets, and the experimental results demonstrate that our proposed method can effectively undermine transformer-based classifiers, significantly reducing classification accuracy from 99\\% to as low as 25.68\\%.</article>","contentLength":1664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flex-GAD : Flexible Graph Anomaly Detection","url":"https://arxiv.org/abs/2510.25809","date":1761883200,"author":"","guid":323316,"unread":true,"content":"<article>arXiv:2510.25809v1 Announce Type: new \nAbstract: Detecting anomalous nodes in attributed networks, where each node is associated with both structural connections and descriptive attributes, is essential for identifying fraud, misinformation, and suspicious behavior in domains such as social networks, academic citation graphs, and e-commerce platforms. We propose Flex-GAD, a novel unsupervised framework for graph anomaly detection at the node level. Flex-GAD integrates two encoders to capture complementary aspects of graph data. The framework incorporates a novel community-based GCN encoder to model intra-community and inter-community information into node embeddings, thereby ensuring structural consistency, along with a standard attribute encoder. These diverse representations are fused using a self-attention-based representation fusion module, which enables adaptive weighting and effective integration of the encoded information. This fusion mechanism allows automatic emphasis of the most relevant node representation across different encoders. We evaluate Flex-GAD on seven real-world attributed graphs with varying sizes, node degrees, and attribute homogeneity. Flex-GAD achieves an average AUC improvement of 7.98% over the previously best-performing method, GAD-NR, demonstrating its effectiveness and flexibility across diverse graph structures. Moreover, it significantly reduces training time, running 102x faster per epoch than Anomaly DAE and 3x faster per epoch than GAD-NR on average across seven benchmark datasets.</article>","contentLength":1543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs","url":"https://arxiv.org/abs/2510.25808","date":1761883200,"author":"","guid":323317,"unread":true,"content":"<article>arXiv:2510.25808v1 Announce Type: new \nAbstract: Large language models (LLMs) have achieved remarkable success across diverse domains, due to their strong instruction-following capabilities. This has led to increasing interest in optimizing instructions for black-box LLMs, whose internal parameters are inaccessible but widely used due to their strong performance. To optimize instructions for black-box LLMs, recent methods employ white-box LLMs to generate candidate instructions from optimized soft prompts. However, white-box LLMs often map different soft prompts to the same instruction, leading to redundant queries. While previous studies regarded this many-to-one mapping as a structure that hinders optimization efficiency, we reinterpret it as a useful prior knowledge that can accelerate the optimization. To this end, we introduce PREimage-informed inSTruction Optimization (PRESTO), a novel framework that leverages the preimage structure of soft prompts for efficient optimization. PRESTO consists of three key components: (1) score sharing, which shares the evaluation score with all soft prompts in a preimage; (2) preimage-based initialization, which selects initial data points that maximize search space coverage using preimage information; and (3) score consistency regularization, which enforces prediction consistency within each preimage. By leveraging preimages, PRESTO achieves the effect of effectively obtaining 14 times more scored data under the same query budget, resulting in more efficient optimization. Experimental results on 33 instruction optimization tasks demonstrate the superior performance of PRESTO. Code is available at https://github.com/mlvlab/PRESTO</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"APThreatHunter: An automated planning-based threat hunting framework","url":"https://arxiv.org/abs/2510.25806","date":1761883200,"author":"","guid":323318,"unread":true,"content":"<article>arXiv:2510.25806v1 Announce Type: new \nAbstract: Cyber attacks threaten economic interests, critical infrastructure, and public health and safety. To counter this, entities adopt cyber threat hunting, a proactive approach that involves formulating hypotheses and searching for attack patterns within organisational networks. Automating cyber threat hunting presents challenges, particularly in generating hypotheses, as it is a manually created and confirmed process, making it time-consuming. To address these challenges, we introduce APThreatHunter, an automated threat hunting solution that generates hypotheses with minimal human intervention, eliminating analyst bias and reducing time and cost. This is done by presenting possible risks based on the system's current state and a set of indicators to indicate whether any of the detected risks are happening or not. We evaluated APThreatHunter using real-world Android malware samples, and the results revealed the practicality of using automated planning for goal hypothesis generation in cyber threat hunting activities.</article>","contentLength":1077,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ideology-Based LLMs for Content Moderation","url":"https://arxiv.org/abs/2510.25805","date":1761883200,"author":"","guid":323319,"unread":true,"content":"<article>arXiv:2510.25805v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly used in content moderation systems, where ensuring fairness and neutrality is essential. In this study, we examine how persona adoption influences the consistency and fairness of harmful content classification across different LLM architectures, model sizes, and content modalities (language vs. vision). At first glance, headline performance metrics suggest that personas have little impact on overall classification accuracy. However, a closer analysis reveals important behavioral shifts. Personas with different ideological leanings display distinct propensities to label content as harmful, showing that the lens through which a model \"views\" input can subtly shape its judgments. Further agreement analyses highlight that models, particularly larger ones, tend to align more closely with personas from the same political ideology, strengthening within-ideology consistency while widening divergence across ideological groups. To show this effect more directly, we conducted an additional study on a politically targeted task, which confirmed that personas not only behave more coherently within their own ideology but also exhibit a tendency to defend their perspective while downplaying harmfulness in opposing views. Together, these findings highlight how persona conditioning can introduce subtle ideological biases into LLM outputs, raising concerns about the use of AI systems that may reinforce partisan perspectives under the guise of neutrality.</article>","contentLength":1554,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data","url":"https://arxiv.org/abs/2510.25804","date":1761883200,"author":"","guid":323320,"unread":true,"content":"<article>arXiv:2510.25804v1 Announce Type: new \nAbstract: Long-context language models unlock advanced capabilities in reasoning, code generation, and document summarization by leveraging dependencies across extended spans of text. However, a significant portion of readily available long-text data lacks meaningful long-distance dependencies; most spans can be predicted using only local context. Training on such data is inefficient, making careful data selection crucial. Therefore, we introduce LongFilter, a framework for curating training data tailored to long-context pretraining. LongFilter measures the information gain provided by extended context by contrasting model predictions under long-context versus short-context settings, thereby identifying samples where long-range dependencies are essential. Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show that LongFilter efficiently selects high-quality data and yields substantial improvements on benchmarks such as HELMET, LongBench, and RULER.</article>","contentLength":1025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training","url":"https://arxiv.org/abs/2510.25803","date":1761883200,"author":"","guid":323321,"unread":true,"content":"<article>arXiv:2510.25803v1 Announce Type: new \nAbstract: Pre-training has proven effective in addressing data scarcity and performance limitations in solving PDE problems with neural operators. However, challenges remain due to the heterogeneity of PDE datasets in equation types, which leads to high errors in mixed training. Additionally, dense pre-training models that scale parameters by increasing network width or depth incur significant inference costs. To tackle these challenges, we propose a novel Mixture-of-Experts Pre-training Operator Transformer (MoE-POT), a sparse-activated architecture that scales parameters efficiently while controlling inference costs. Specifically, our model adopts a layer-wise router-gating network to dynamically select 4 routed experts from 16 expert networks during inference, enabling the model to focus on equation-specific features. Meanwhile, we also integrate 2 shared experts, aiming to capture common properties of PDE and reduce redundancy among routed experts. The final output is computed as the weighted average of the results from all activated experts. We pre-train models with parameters from 30M to 0.5B on 6 public PDE datasets. Our model with 90M activated parameters achieves up to a 40% reduction in zero-shot error compared with existing models with 120M activated parameters. Additionally, we conduct interpretability analysis, showing that dataset types can be inferred from router-gating network decisions, which validates the rationality and effectiveness of the MoE architecture.</article>","contentLength":1540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection","url":"https://arxiv.org/abs/2510.25802","date":1761883200,"author":"","guid":323322,"unread":true,"content":"<article>arXiv:2510.25802v1 Announce Type: new \nAbstract: In this paper, we propose a novel hybrid deep learning architecture that synergistically combines Graph Neural Networks (GNNs), Recurrent Neural Networks (RNNs), and multi-head attention mechanisms to significantly enhance cy- bersecurity intrusion detection capabilities. By leveraging the comprehensive UNSW-NB15 dataset containing diverse network traffic patterns, our approach effectively captures both spatial dependencies through graph structural relationships and tem- poral dynamics through sequential analysis of network events. The integrated attention mechanism provides dual benefits of improved model interpretability and enhanced feature selection, enabling cybersecurity analysts to focus computational resources on high-impact security events - a critical requirement in modern real-time intrusion detection systems. Our extensive experimental evaluation demonstrates that the proposed hybrid model achieves superior performance compared to traditional machine learning approaches and standalone deep learning models across multiple evaluation metrics, including accuracy, precision, recall, and F1-score. The model achieves particularly strong performance in detecting sophisticated attack patterns such as Advanced Persistent Threats (APTs), Distributed Denial of Service (DDoS) attacks, and zero-day exploits, making it a promising solution for next-generation cybersecurity applications in complex network environments.</article>","contentLength":1488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start","url":"https://arxiv.org/abs/2510.25801","date":1761883200,"author":"","guid":323323,"unread":true,"content":"<article>arXiv:2510.25801v1 Announce Type: new \nAbstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a wave of \"MLLM-r1\" approaches that bring RL to vision language models. Most representative paradigms begin with a cold start, typically employing supervised fine-tuning (SFT), to initialize the policy before RL. However, SFT-based cold start adopts the reasoning paradigm intertwined with task solution and output format, which may induce instruction-style overfitting, weakens out-of-distribution generalization, and ultimately affects downstream RL. We revisit the cold start along two views, its training method and data construction, and introduce the Generalization Factor (GF) coefficient to quantify the generalization capability under different methods. Our empirical study finds that preference-based training methods (e.g. DPO) generalizes better than SFT-based methods in cold start. Motivated by this, we propose SPECS-a Self-distilled, Preference-based Cold Start framework that decouples multimodal learning: (1) generates introspective preference data pairs via self-distillation, avoiding reliance on larger teachers or manual annotation; (2) performs preference-based training to learn, focusing on shallow, transferable surface-form criteria (format, structure, style) rather than memorizing content; and (3) hands off to RL with verifiable rewards for deep reasoning results. Experimental results across multiple multimodal benchmarks show that our decoupling learning framework yields consistent performance gains over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%. Additional experiments indicate that SPECS contributes to reducing in-distribution \"stuckness,\" improving exploration, stabilizing training, and raising the performance ceiling.</article>","contentLength":1812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreIE: Low-Frequency Spectral Bias in Neural Networks for Time-Series Tasks","url":"https://arxiv.org/abs/2510.25800","date":1761883200,"author":"","guid":323324,"unread":true,"content":"<article>arXiv:2510.25800v1 Announce Type: new \nAbstract: The inherent autocorrelation of time series data presents an ongoing challenge to multivariate time series prediction. Recently, a widely adopted approach has been the incorporation of frequency domain information to assist in long-term prediction tasks. Many researchers have independently observed the spectral bias phenomenon in neural networks, where models tend to fit low-frequency signals before high-frequency ones. However, these observations have often been attributed to the specific architectures designed by the researchers, rather than recognizing the phenomenon as a universal characteristic across models. To unify the understanding of the spectral bias phenomenon in long-term time series prediction, we conducted extensive empirical experiments to measure spectral bias in existing mainstream models. Our findings reveal that virtually all models exhibit this phenomenon. To mitigate the impact of spectral bias, we propose the FreLE (Frequency Loss Enhancement) algorithm, which enhances model generalization through both explicit and implicit frequency regularization. This is a plug-and-play model loss function unit. A large number of experiments have proven the superior performance of FreLE. Code is available at https://github.com/Chenxing-Xuan/FreLE.</article>","contentLength":1325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection","url":"https://arxiv.org/abs/2510.25799","date":1761883200,"author":"","guid":323325,"unread":true,"content":"<article>arXiv:2510.25799v1 Announce Type: new \nAbstract: Human experts often struggle to select the best option from a large set of items with multiple competing objectives, a process bottlenecked by the difficulty of formalizing complex, implicit preferences. To address this, we introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a zero-shot preference oracle, guided only by an expert's high-level priorities in natural language. To operate within LLM constraints like context windows and inference costs, we propose two iterative algorithms: LISTEN-U, which uses the LLM to refine a parametric utility function, and LISTEN-T, a non-parametric method that performs tournament-style selections over small batches of solutions. Evaluated on diverse tasks including flight booking, shopping, and exam scheduling, our results show LISTEN-U excels when preferences are parametrically aligned (a property we measure with a novel concordance metric), while LISTEN-T offers more robust performance. This work explores a promising direction for steering complex multi-objective decisions directly with natural language, reducing the cognitive burden of traditional preference elicitation.</article>","contentLength":1200,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MemEIC: A Step Toward Continual and Compositional Knowledge Editing","url":"https://arxiv.org/abs/2510.25798","date":1761883200,"author":"","guid":323326,"unread":true,"content":"<article>arXiv:2510.25798v1 Announce Type: new \nAbstract: The dynamic nature of information necessitates continuously updating large vision-language models (LVLMs). While recent knowledge editing techniques hint at promising directions, they often focus on editing a single modality (vision or language) in isolation. This prevalent practice neglects the inherent multimodality of LVLMs and the continuous nature of knowledge updates, potentially leading to suboptimal editing outcomes when considering the interplay between modalities and the need for ongoing knowledge refinement. To address these limitations, we propose MemEIC, a novel method for Continual and Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional editing of both visual and textual knowledge sequentially. Our approach employs a hybrid external-internal editor featuring a dual external memory for cross-modal evidence retrieval and dual LoRA adapters that facilitate disentangled parameter updates for each modality. A key component is a brain-inspired knowledge connector, activated selectively for compositional reasoning, that integrates information across different modalities. Experiments demonstrate that MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks","url":"https://arxiv.org/abs/2510.25797","date":1761883200,"author":"","guid":323327,"unread":true,"content":"<article>arXiv:2510.25797v1 Announce Type: new \nAbstract: This study examines the effectiveness of spatio-temporal modeling and the integration of spatial attention mechanisms in deep learning models for underwater object detection. Specifically, in the first phase, the performance of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is developed, through the addition of a Convolutional Block Attention Module (CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the research highlights how temporal modeling improves detection accuracy in dynamic marine environments, particularly under conditions of sudden movements, partial occlusions, and gradual motion. The testing results showed that YOLOv5 achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively, highlighting their superior accuracy and generalization in detecting complex objects. The findings demonstrate that T-YOLOv5 significantly enhances detection reliability compared to the standard model, while T-YOLOv5 with CBAM further improves performance in challenging scenarios, although there is a loss of accuracy when it comes to simpler scenarios.</article>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning","url":"https://arxiv.org/abs/2510.25796","date":1761883200,"author":"","guid":323328,"unread":true,"content":"<article>arXiv:2510.25796v1 Announce Type: new \nAbstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or microtransit, is a service wherein passengers share rides. This service can reduce costs for both passengers and operators and reduce congestion and environmental impacts. A key limitation, however, is its myopic decision-making, which overlooks long-term effects of dispatch decisions. To address this, we propose a simulation-informed reinforcement learning (RL) approach. While RL has been widely studied in the context of ride-hailing systems, its application in ride-pooling systems has been less explored. In this study, we extend the learning and planning framework of Xu et al. (2018) from ride-hailing to ride-pooling by embedding a ride-pooling simulation within the learning mechanism to enable non-myopic decision-making. In addition, we propose a complementary policy for rebalancing idle vehicles. By employing n-step temporal difference learning on simulated experiences, we derive spatiotemporal state values and subsequently evaluate the effectiveness of the non-myopic policy using NYC taxi request data. Results demonstrate that the non-myopic policy for matching can increase the service rate by up to 8.4% versus a myopic policy while reducing both in-vehicle and wait times for passengers. Furthermore, the proposed non-myopic policy can decrease fleet size by over 25% compared to a myopic policy, while maintaining the same level of performance, thereby offering significant cost savings for operators. Incorporating rebalancing operations into the proposed framework cuts wait time by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1% compared to using the framework for matching decisions alone at the cost of increased vehicle minutes traveled per passenger.</article>","contentLength":1822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Information Combining for Multi-Agent Systems Using Adaptive Bias Learning","url":"https://arxiv.org/abs/2510.25793","date":1761883200,"author":"","guid":323329,"unread":true,"content":"<article>arXiv:2510.25793v1 Announce Type: new \nAbstract: Modern multi-agent systems ranging from sensor networks monitoring critical infrastructure to crowdsourcing platforms aggregating human intelligence can suffer significant performance degradation due to systematic biases that vary with environmental conditions. Current approaches either ignore these biases, leading to suboptimal decisions, or require expensive calibration procedures that are often infeasible in practice. This performance gap has real consequences: inaccurate environmental monitoring, unreliable financial predictions, and flawed aggregation of human judgments. This paper addresses the fundamental question: when can we learn and correct for these unknown biases to recover near-optimal performance, and when is such learning futile? We develop a theoretical framework that decomposes biases into learnable systematic components and irreducible stochastic components, introducing the concept of learnability ratio as the fraction of bias variance predictable from observable covariates. This ratio determines whether bias learning is worthwhile for a given system. We prove that the achievable performance improvement is fundamentally bounded by this learnability ratio, providing system designers with quantitative guidance on when to invest in bias learning versus simpler approaches. We present the Adaptive Bias Learning and Optimal Combining (ABLOC) algorithm, which iteratively learns bias-correcting transformations while optimizing combination weights through closedform solutions, guaranteeing convergence to these theoretical bounds. Experimental validation demonstrates that systems with high learnability ratios can recover significant performance (we achieved 40%-70% of theoretical maximum improvement in our examples), while those with low learnability show minimal benefit, validating our diagnostic criteria for practical deployment decisions.</article>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?","url":"https://arxiv.org/abs/2510.25791","date":1761883200,"author":"","guid":323330,"unread":true,"content":"<article>arXiv:2510.25791v1 Announce Type: new \nAbstract: Chain-of-thought (CoT) supervision can substantially improve transformer performance, yet the mechanisms by which models learn to follow and benefit from CoT remain poorly understood. We investigate these learning dynamics through the lens of grokking by pretraining transformers on symbolic reasoning tasks with tunable algorithmic complexity and controllable data composition to study their generalization. Models were trained under two settings: (i) producing only final answers, and (ii) emitting explicit CoT traces before answering. Our results show that while CoT generally improves task performance, its benefits depend on task complexity. To quantify these effects, we model the accuracy of the logarithmic training steps with a three-parameter logistic curve, revealing how the learning speed and shape vary with task complexity, data distribution, and the presence of CoT supervision. We also uncover a transient trace unfaithfulness phase: early in training, models often produce correct answers while skipping or contradicting CoT steps, before later aligning their reasoning traces with answers. Empirically, we (1) demonstrate that CoT accelerates generalization but does not overcome tasks with higher algorithmic complexity, such as finding list intersections; (2) introduce a kinetic modeling framework for understanding transformer learning; (3) characterize trace faithfulness as a dynamic property that emerges over training; and (4) show CoT alters internal transformer computation mechanistically.</article>","contentLength":1569,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes","url":"https://arxiv.org/abs/2510.25788","date":1761883200,"author":"","guid":323331,"unread":true,"content":"<article>arXiv:2510.25788v1 Announce Type: new \nAbstract: High-energy materials (HEMs) are critical for propulsion and defense domains, yet their discovery remains constrained by experimental data and restricted access to testing facilities. This work presents a novel approach toward high-energy molecules by combining Long Short-Term Memory (LSTM) networks for molecular generation and Attentive Graph Neural Networks (GNN) for property predictions. We propose a transformative embedding space construction strategy that integrates fixed SHA-256 embeddings with partially trainable representations. Unlike conventional regularization techniques, this changes the representational basis itself, reshaping the molecular input space before learning begins. Without recourse to pretraining, the generator achieves 67.5% validity and 37.5% novelty. The generated library exhibits a mean Tanimoto coefficient of 0.214 relative to training set signifying the ability of framework to generate a diverse chemical space. We identified 37 new super explosives higher than 9 km/s predicted detonation velocity.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unsupervised local learning based on voltage-dependent synaptic plasticity for resistive and ferroelectric synapses","url":"https://arxiv.org/abs/2510.25787","date":1761883200,"author":"","guid":323332,"unread":true,"content":"<article>arXiv:2510.25787v1 Announce Type: new \nAbstract: The deployment of AI on edge computing devices faces significant challenges related to energy consumption and functionality. These devices could greatly benefit from brain-inspired learning mechanisms, allowing for real-time adaptation while using low-power. In-memory computing with nanoscale resistive memories may play a crucial role in enabling the execution of AI workloads on these edge devices. In this study, we introduce voltage-dependent synaptic plasticity (VDSP) as an efficient approach for unsupervised and local learning in memristive synapses based on Hebbian principles. This method enables online learning without requiring complex pulse-shaping circuits typically necessary for spike-timing-dependent plasticity (STDP). We show how VDSP can be advantageously adapted to three types of memristive devices (TiO$_2$, HfO$_2$-based metal-oxide filamentary synapses, and HfZrO$_4$-based ferroelectric tunnel junctions (FTJ)) with disctinctive switching characteristics. System-level simulations of spiking neural networks incorporating these devices were conducted to validate unsupervised learning on MNIST-based pattern recognition tasks, achieving state-of-the-art performance. The results demonstrated over 83% accuracy across all devices using 200 neurons. Additionally, we assessed the impact of device variability, such as switching thresholds and ratios between high and low resistance state levels, and proposed mitigation strategies to enhance robustness.</article>","contentLength":1528,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection","url":"https://arxiv.org/abs/2510.25786","date":1761883200,"author":"","guid":323333,"unread":true,"content":"<article>arXiv:2510.25786v1 Announce Type: new \nAbstract: One of the main challenges in mechanistic interpretability is circuit discovery, determining which parts of a model perform a given task. We build on the Mechanistic Interpretability Benchmark (MIB) and propose three key improvements to circuit discovery. First, we use bootstrapping to identify edges with consistent attribution scores. Second, we introduce a simple ratio-based selection strategy to prioritize strong positive-scoring edges, balancing performance and faithfulness. Third, we replace the standard greedy selection with an integer linear programming formulation. Our methods yield more faithful circuits and outperform prior approaches across multiple MIB tasks and models. Our code is available at: https://github.com/technion-cs-nlp/MIB-Shared-Task.</article>","contentLength":817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series","url":"https://arxiv.org/abs/2510.25785","date":1761883200,"author":"","guid":323334,"unread":true,"content":"<article>arXiv:2510.25785v1 Announce Type: new \nAbstract: Wearable sensors provide abundant physiological time series, yet the principles governing their predictive utility remain unclear. We hypothesize that temporal resolution is a fundamental axis of representation learning, with different clinical and behavioral outcomes relying on structure at distinct scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical Masked Autoencoder), a self supervised framework that combines masked autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces multi resolution embeddings that enable systematic evaluation of which temporal scales carry predictive signal, transforming resolution from a hyperparameter into a probe for interpretability. Across classification, regression, and generative benchmarks, HiMAE consistently outperforms state of the art foundation models that collapse scale, while being orders of magnitude smaller. HiMAE is an efficient representation learner compact enough to run entirely on watch, achieving sub millisecond inference on smartwatch class CPUs for true edge inference. Together, these contributions position HiMAE as both an efficient self supervised learning method and a discovery tool for scale sensitive structure in wearable health.</article>","contentLength":1300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"zFLoRA: Zero-Latency Fused Low-Rank Adapters","url":"https://arxiv.org/abs/2510.25784","date":1761883200,"author":"","guid":323335,"unread":true,"content":"<article>arXiv:2510.25784v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly deployed with task-specific adapters catering to multiple downstream applications. In such a scenario, the additional compute associated with these apparently insignificant number of adapter parameters (typically less than 1% of the base model) turns out to be disproportionately significant during inference time (upto 2.5x times that of the base model). In this paper, we propose a new zero-latency fused low-rank adapter (zFLoRA) that introduces zero or negligible latency overhead on top of the base model. Experimental results on LLMs of size 1B, 3B and 7B show that zFLoRA compares favorably against the popular supervised fine-tuning benchmarks including low-rank adapters (LoRA) as well as full fine-tuning (FFT). Experiments are conducted on 18 different tasks across three different categories namely commonsense reasoning, math reasoning and summary-dialogue. Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA H100) platforms show that the proposed zFLoRA adapters introduce zero to negligible latency overhead.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LASTIST: LArge-Scale Target-Independent STance dataset","url":"https://arxiv.org/abs/2510.25783","date":1761883200,"author":"","guid":323336,"unread":true,"content":"<article>arXiv:2510.25783v1 Announce Type: new \nAbstract: Stance detection has emerged as an area of research in the field of artificial intelligence. However, most research is currently centered on the target-dependent stance detection task, which is based on a person's stance in favor of or against a specific target. Furthermore, most benchmark datasets are based on English, making it difficult to develop models in low-resource languages such as Korean, especially for an emerging field such as stance detection. This study proposes the LArge-Scale Target-Independent STance (LASTIST) dataset to fill this research gap. Collected from the press releases of both parties on Korean political parties, the LASTIST dataset uses 563,299 labeled Korean sentences. We provide a detailed description of how we collected and constructed the dataset and trained state-of-the-art deep learning and stance detection models. Our LASTIST dataset is designed for various tasks in stance detection, including target-independent stance detection and diachronic evolution stance detection. We deploy our dataset on https://anonymous.4open.science/r/LASTIST-3721/.</article>","contentLength":1142,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Practitioner's Guide to Kolmogorov-Arnold Networks","url":"https://arxiv.org/abs/2510.25781","date":1761883200,"author":"","guid":323337,"unread":true,"content":"<article>arXiv:2510.25781v1 Announce Type: new \nAbstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising alternative to traditional Multilayer Perceptrons (MLPs), inspired by the Kolmogorov-Arnold representation theorem. Unlike MLPs, which use fixed activation functions on nodes, KANs employ learnable univariate basis functions on edges, offering enhanced expressivity and interpretability. This review provides a systematic and comprehensive overview of the rapidly expanding KAN landscape, moving beyond simple performance comparisons to offer a structured synthesis of theoretical foundations, architectural variants, and practical implementation strategies. By collecting and categorizing a vast array of open-source implementations, we map the vibrant ecosystem supporting KAN development. We begin by bridging the conceptual gap between KANs and MLPs, establishing their formal equivalence and highlighting the superior parameter efficiency of the KAN formulation. A central theme of our review is the critical role of the basis function; we survey a wide array of choices, including B-splines, Chebyshev and Jacobi polynomials, ReLU compositions, Gaussian RBFs, and Fourier series, and analyze their respective trade-offs in terms of smoothness, locality, and computational cost. We then categorize recent advancements into a clear roadmap, covering techniques for improving accuracy, efficiency, and regularization. Key topics include physics-informed loss design, adaptive sampling, domain decomposition, hybrid architectures, and specialized methods for handling discontinuities. Finally, we provide a practical \"Choose-Your-KAN\" guide to help practitioners select appropriate architectures, and we conclude by identifying current research gaps. The associated GitHub repository https://github.com/AmirNoori68/kan-review complements this paper and serves as a structured reference for ongoing KAN research.</article>","contentLength":1933,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets","url":"https://arxiv.org/abs/2510.25779","date":1761883200,"author":"","guid":323338,"unread":true,"content":"<article>arXiv:2510.25779v1 Announce Type: new \nAbstract: As LLM agents advance, they are increasingly mediating economic decisions, ranging from product discovery to transactions, on behalf of users. Such applications promise benefits but also raise many questions about agent accountability and value for users. Addressing these questions requires understanding how agents behave in realistic market conditions. However, previous research has largely evaluated agents in constrained settings, such as single-task marketplaces (e.g., negotiation) or structured two-agent interactions. Real-world markets are fundamentally different: they require agents to handle diverse economic activities and coordinate within large, dynamic ecosystems where multiple agents with opaque behaviors may engage in open-ended dialogues. To bridge this gap, we investigate two-sided agentic marketplaces where Assistant agents represent consumers and Service agents represent competing businesses. To study these interactions safely, we develop Magentic-Marketplace-- a simulated environment where Assistants and Services can operate. This environment enables us to study key market dynamics: the utility agents achieve, behavioral biases, vulnerability to manipulation, and how search mechanisms shape market outcomes. Our experiments show that frontier models can approach optimal welfare-- but only under ideal search conditions. Performance degrades sharply with scale, and all models exhibit severe first-proposal bias, creating 10-30x advantages for response speed over quality. These findings reveal how behaviors emerge across market conditions, informing the design of fair and efficient agentic marketplaces.</article>","contentLength":1691,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis","url":"https://arxiv.org/abs/2510.25778","date":1761883200,"author":"","guid":323339,"unread":true,"content":"<article>arXiv:2510.25778v1 Announce Type: new \nAbstract: Opinion mining, also called sentiment analysis, is the field of study that analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes. Holistic lexicon-based approach does not consider the strength of each opinion, i.e., whether the opinion is very strongly negative (or positive), strongly negative (or positive), moderate negative (or positive), very weakly negative (or positive) and weakly negative (or positive). In this paper, we propose approach to rank entities based on orientation and strength of the entity reviews and user's queries by classifying them in granularity levels (i.e. very weak, weak, moderate, very strong and strong) by combining opinion words (i.e. adverb, adjective, noun and verb) that are related to aspect of interest of certain product. We shall use fuzzy logic algorithmic approach in order to classify opinion words into different category and syntactic dependency resolution to find relations for desired aspect words. Opinion words related to certain aspects of interest are considered to find the entity score for that aspect in the review.</article>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StreetMath: Study of LLMs' Approximation Behaviors","url":"https://arxiv.org/abs/2510.25776","date":1761883200,"author":"","guid":323340,"unread":true,"content":"<article>arXiv:2510.25776v1 Announce Type: new \nAbstract: There is a substantial body of literature examining the mathematical reasoning capabilities of large language models (LLMs), particularly their performance on precise arithmetic operations in autoregressive architectures. However, their ability to perform approximate reasoning in informal, fast-paced mathematical operations has received far less attention, especially among non-autoregressive decoder models. Our work addresses this gap by introducing StreetMath, a benchmark designed to evaluate models' approximation abilities under real-world approximation scenarios. We conduct extensive evaluations across different LLM architectures: Qwen3-4B-Instruct-2507, Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to probe their internal computational states. Our analysis reveals that LLMs generally attempt to compute exact values or invoke external tools even in tasks that call for approximation. Moreover, while models sometimes reach the correct answer in early layers or steps, they still consume more tokens when solving approximation tasks. Additional experiments indicate that exact and approximate arithmetic operations rely on largely separate neural components. Drawing upon research on cognitive psychology, we argue that LLMs do not exhibit cognitive miserliness in the same way humans do in street math settings. We open source our work https://github.com/ctseng777/StreetMath</article>","contentLength":1545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Piece-by-Piece Explanations for Chess Positions with SHAP","url":"https://arxiv.org/abs/2510.25775","date":1761883200,"author":"","guid":323341,"unread":true,"content":"<article>arXiv:2510.25775v1 Announce Type: new \nAbstract: Contemporary chess engines offer precise yet opaque evaluations, typically expressed as centipawn scores. While effective for decision-making, these outputs obscure the underlying contributions of individual pieces or patterns. In this paper, we explore adapting SHAP (SHapley Additive exPlanations) to the domain of chess analysis, aiming to attribute a chess engines evaluation to specific pieces on the board. By treating pieces as features and systematically ablating them, we compute additive, per-piece contributions that explain the engines output in a locally faithful and human-interpretable manner. This method draws inspiration from classical chess pedagogy, where players assess positions by mentally removing pieces, and grounds it in modern explainable AI techniques. Our approach opens new possibilities for visualization, human training, and engine comparison. We release accompanying code and data to foster future research in interpretable chess AI.</article>","contentLength":1016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantitative Hypocoercivity and Lifting of Classical and Quantum Dynamics","url":"https://arxiv.org/abs/2510.22305","date":1761796800,"author":"","guid":321107,"unread":true,"content":"<article>arXiv:2510.22305v2 Announce Type: replace-cross \nAbstract: We consider quantitative convergence analysis for hypocoercive dynamics such as Langevin and Lindblad equations describing classical and quantum open systems. Our goal is to provide an overview of recent results of hypocoercivity estimates based on space-time Poincare inequality, providing a unified treatment for classical and quantum dynamics. Furthermore, we also present a unified lifting framework for accelerating both classical and quantum Markov semigroups, which leads to upper and lower bounds of convergence rates.</article>","contentLength":585,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On a sequence of Kimberling and its relationship to the Tribonacci word","url":"https://arxiv.org/abs/2510.11318","date":1761796800,"author":"","guid":321108,"unread":true,"content":"<article>arXiv:2510.11318v2 Announce Type: replace-cross \nAbstract: In 2017, Clark Kimberling defined an interesting sequence ${\\bf B} = 0100101100 \\cdots$ of $0$'s and $1$'s by certain inflation rules, and he made a number of conjectures about this sequence and some related ones. In this note we prove his conjectures using, in part, the Walnut theorem-prover. We show how his word is related to the infinite Tribonacci word, and we determine both the subword complexity and critical exponent of $\\bf B$.</article>","contentLength":497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications","url":"https://arxiv.org/abs/2510.01850","date":1761796800,"author":"","guid":321109,"unread":true,"content":"<article>arXiv:2510.01850v3 Announce Type: replace-cross \nAbstract: To effectively process impulse noise for narrowband powerline communications (NB-PLCs) transceivers, capturing comprehensive statistics of nonperiodic asynchronous impulsive noise (APIN) is a critical task. However, existing mathematical noise generative models only capture part of the characteristics of noise. In this study, we propose a novel generative adversarial network (GAN) called noise generation GAN (NGGAN) that learns the complicated characteristics of practically measured noise samples for data synthesis. To closely match the statistics of complicated noise over the NB-PLC systems, we measured the NB-PLC noise via the analog coupling and bandpass filtering circuits of a commercial NB-PLC modem to build a realistic dataset. To train NGGAN, we adhere to the following principles: 1) we design the length of input signals that the NGGAN model can fit to facilitate cyclostationary noise generation; 2) the Wasserstein distance is used as a loss function to enhance the similarity between the generated noise and training data; and 3) to measure the similarity performances of GAN-based models based on the mathematical and practically measured datasets, we conduct both quantitative and qualitative analyses. The training datasets include: 1) a piecewise spectral cyclostationary Gaussian model (PSCGM); 2) a frequency-shift (FRESH) filter; and 3) practical measurements from NB-PLC systems. Simulation results demonstrate that the generated noise samples from the proposed NGGAN are highly close to the real noise samples. The principal component analysis (PCA) scatter plots and Fr\\'echet inception distance (FID) analysis have shown that NGGAN outperforms other GAN-based models by generating noise samples with superior fidelity and higher diversity.</article>","contentLength":1831,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The exterior derivative and the mean value equality in $\\mathbb{R}^n$","url":"https://arxiv.org/abs/2510.00999","date":1761796800,"author":"","guid":321110,"unread":true,"content":"<article>arXiv:2510.00999v2 Announce Type: replace-cross \nAbstract: This survey revisits classical results in vector calculus and analysis by exploring a generalised perspective on the exterior derivative, interpreting it as a measure of \"infinitesimal flux\". This viewpoint leads to a higher-dimensional analogue of the Mean Value Theorem, valid for differential $k$-forms, and provides a natural formulation of Stokes' theorem that mirrors the exact hypotheses of the Fundamental Theorem of Calculus -- without requiring full $C^1$ smoothness of the differential form.\n  As a numerical application, we propose an algorithm for exterior differentiation in $\\mathbb{R}^n$ that relies solely on black-box access to the differential form, offering a practical tool for computation without the need for mesh discretization or explicit symbolic expressions.</article>","contentLength":844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dual-Regularized Riccati Recursions for Interior-Point Optimal Control","url":"https://arxiv.org/abs/2509.16370","date":1761796800,"author":"","guid":321111,"unread":true,"content":"<article>arXiv:2509.16370v3 Announce Type: replace-cross \nAbstract: We derive closed-form extensions of Riccati's recursions (both sequential and parallel) for solving dual-regularized LQR problems. We show how these methods can be used to solve general constrained, non-convex, discrete-time optimal control problems via a regularized interior point method, while guaranteeing that each step is a descent direction of an Augmented Barrier-Lagrangian merit function. We provide MIT-licensed implementations of our methods in C++ and JAX.</article>","contentLength":528,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Equalize: Data-Driven Frequency-Domain Signal Recovery in Molecular Communications","url":"https://arxiv.org/abs/2509.11327","date":1761796800,"author":"","guid":321112,"unread":true,"content":"<article>arXiv:2509.11327v2 Announce Type: replace-cross \nAbstract: In molecular communications (MC), inter-symbol interference (ISI) and noise are key factors that degrade communication reliability. Although time-domain equalization can effectively mitigate these effects, it often entails high computational complexity concerning the channel memory. In contrast, frequency-domain equalization (FDE) offers greater computational efficiency but typically requires prior knowledge of the channel model. To address this limitation, this letter proposes FDE techniques based on long short-term memory (LSTM) neural networks, enabling temporal correlation modeling in MC channels to improve ISI and noise suppression. To eliminate the reliance on prior channel information in conventional FDE methods, a supervised training strategy is employed for channel-adaptive equalization. Simulation results demonstrate that the proposed LSTM-FDE significantly reduces the bit error rate compared to traditional FDE and feedforward neural network-based equalizers. This performance gain is attributed to the LSTM's temporal modeling capabilities, which enhance noise suppression and accelerate model convergence, while maintaining comparable computational efficiency.</article>","contentLength":1245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perturbing the Derivative: Wild Refitting for Model-Free Evaluation of Machine Learning Models under Bregman Losses","url":"https://arxiv.org/abs/2509.02476","date":1761796800,"author":"","guid":321113,"unread":true,"content":"<article>arXiv:2509.02476v5 Announce Type: replace-cross \nAbstract: We study the excess risk evaluation of classical penalized empirical risk minimization (ERM) with Bregman losses. We show that by leveraging the idea of wild refitting, one can efficiently upper bound the excess risk through the so-called \"wild optimism,\" without relying on the global structure of the underlying function class. This property makes our approach inherently model-free. Unlike conventional analysis, our framework operates with just one dataset and black-box access to the training procedure. The method involves randomized Rademacher symmetrization and constructing artificially modified outputs by perturbation in the derivative space with appropriate scaling, upon which we retrain a second predictor for excess risk estimation. We establish high-probability performance guarantees both under the fixed design setting and the random design setting, demonstrating that wild refitting under Bregman losses, with an appropriately chosen wild noise scale, yields a valid upper bound on the excess risk. Thus, our work is promising for theoretically evaluating modern opaque ML models, such as deep neural networks and generative models, where the function class is too complex for classical learning theory and empirical process techniques.</article>","contentLength":1314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GENRE-CMR: Generalizable Deep Learning for Diverse Multi-Domain Cardiac MRI Reconstruction","url":"https://arxiv.org/abs/2508.20600","date":1761796800,"author":"","guid":321114,"unread":true,"content":"<article>arXiv:2508.20600v2 Announce Type: replace-cross \nAbstract: Accelerated Cardiovascular Magnetic Resonance (CMR) image reconstruction remains a critical challenge due to the trade-off between scan time and image quality, particularly when generalizing across diverse acquisition settings. We propose GENRE-CMR, a generative adversarial network (GAN)-based architecture employing a residual deep unrolled reconstruction framework to enhance reconstruction fidelity and generalization. The architecture unrolls iterative optimization into a cascade of convolutional subnetworks, enriched with residual connections to enable progressive feature propagation from shallow to deeper stages. To further improve performance, we integrate two loss functions: (1) an Edge-Aware Region (EAR) loss, which guides the network to focus on structurally informative regions and helps prevent common reconstruction blurriness; and (2) a Statistical Distribution Alignment (SDA) loss, which regularizes the feature space across diverse data distributions via a symmetric KL divergence formulation. Extensive experiments confirm that GENRE-CMR surpasses state-of-the-art methods on training and unseen data, achieving 0.9552 SSIM and 38.90 dB PSNR on unseen distributions across various acceleration factors and sampling trajectories. Ablation studies confirm the contribution of each proposed component to reconstruction quality and generalization. Our framework presents a unified and robust solution for high-quality CMR reconstruction, paving the way for clinically adaptable deployment across heterogeneous acquisition protocols.</article>","contentLength":1612,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Approximating the universal thermal climate index using sparse regression with orthogonal polynomials","url":"https://arxiv.org/abs/2508.11307","date":1761796800,"author":"","guid":321115,"unread":true,"content":"<article>arXiv:2508.11307v2 Announce Type: replace-cross \nAbstract: This article explores novel data-driven modeling approaches for analyzing and approximating the Universal Thermal Climate Index (UTCI), a physiologically-based metric integrating multiple atmospheric variables to assess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we investigate symbolic and sparse regression techniques as tools for interpretable and efficient function approximation. In particular, we highlight the benefits of using orthogonal polynomial bases-such as Legendre polynomials-in sparse regression frameworks, demonstrating their advantages in stability, convergence, and hierarchical interpretability compared to standard polynomial expansions. We demonstrate that our models achieve significantly lower root-mean squared losses than the widely used sixth-degree polynomial benchmark-while using the same or fewer parameters. By leveraging Legendre polynomial bases, we construct models that efficiently populate a Pareto front of accuracy versus complexity and exhibit stable, hierarchical coefficient structures across varying model capacities. Training on just 20% of the data, our models generalize robustly to the remaining 80%, with consistent performance under bootstrapping. The decomposition effectively approximates the UTCI as a Fourier-like expansion in an orthogonal basis, yielding results near the theoretical optimum in the L2 (least squares) sense. We also connect these findings to the broader context of equation discovery in environmental modeling, referencing probabilistic grammar-based methods that enforce domain consistency and compactness in symbolic expressions. Taken together, these results illustrate how combining sparsity, orthogonality, and symbolic structure enables robust, interpretable modeling of complex environmental indices like UTCI - and significantly outperforms the state-of-the-art approximation in both accuracy and efficiency.</article>","contentLength":1976,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Binary Decision Process in Pre-Evacuation Behavior","url":"https://arxiv.org/abs/2508.08284","date":1761796800,"author":"","guid":321116,"unread":true,"content":"<article>arXiv:2508.08284v3 Announce Type: replace-cross \nAbstract: In crowd evacuation the time interval before decisive movement towards a safe place is defined as the pre-evacuation phase, and it has crucial impact on the total time required for safe egress. This process mainly refers to situation awareness and response to an external stressors, e.g., fire alarms. Due to the complexity of human cognitive process, simulation is used to study this important time interval. In this paper a binary decision process is formulated to simulate pre-evacuation time of many evacuees in a given social context. The model combines the classic opinion dynamics (the French-DeGroot model) with binary phase transition to describe how group pre-evacuation time emerges from individual interaction. The model parameters are quantitatively meaningful to human factors research within socio-psychological background, e.g., whether an individual is stubborn or open-minded, or what kind of the social topology exists among the individuals and how it matters in aggregating individuals into social groups. The modeling framework also describes collective motion of many evacuee agents in a planar space, and the resulting multi-agent system is partly similar to the Vicsek flocking model, and it is meaningful to explore complex social behavior during phase transition of a non-equilibrium process.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cyst-X: A Federated AI System Outperforms Clinical Guidelines to Detect Pancreatic Cancer Precursors and Reduce Unnecessary Surgery","url":"https://arxiv.org/abs/2507.22017","date":1761796800,"author":"","guid":321117,"unread":true,"content":"<article>arXiv:2507.22017v2 Announce Type: replace-cross \nAbstract: Pancreatic cancer is projected to be the second-deadliest cancer by 2030, making early detection critical. Intraductal papillary mucinous neoplasms (IPMNs), key cancer precursors, present a clinical dilemma, as current guidelines struggle to stratify malignancy risk, leading to unnecessary surgeries or missed diagnoses. Here, we developed Cyst-X, an AI framework for IPMN risk prediction trained on a unique, multi-center dataset of 1,461 MRI scans from 764 patients. Cyst-X achieves significantly higher accuracy (AUC = 0.82) than both the established Kyoto guidelines (AUC = 0.75) and expert radiologists, particularly in correct identification of high-risk lesions. Clinically, this translates to a 20% increase in cancer detection sensitivity (87.8% vs. 64.1%) for high-risk lesions. We demonstrate that this performance is maintained in a federated learning setting, allowing for collaborative model training without compromising patient privacy. To accelerate research in early pancreatic cancer detection, we publicly release the Cyst-X dataset and models, providing the first large-scale, multi-center MRI resource for pancreatic cyst analysis.</article>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multimodal Recurrent Ensembles for Predicting Brain Responses to Naturalistic Movies (Algonauts 2025)","url":"https://arxiv.org/abs/2507.17897","date":1761796800,"author":"","guid":321118,"unread":true,"content":"<article>arXiv:2507.17897v4 Announce Type: replace-cross \nAbstract: Accurately predicting distributed cortical responses to naturalistic stimuli requires models that integrate visual, auditory and semantic information over time. We present a hierarchical multimodal recurrent ensemble that maps pretrained video, audio, and language embeddings to fMRI time series recorded while four subjects watched almost 80 hours of movies provided by the Algonauts 2025 challenge. Modality-specific bidirectional RNNs encode temporal dynamics; their hidden states are fused and passed to a second recurrent layer, and lightweight subject-specific heads output responses for 1000 cortical parcels. Training relies on a composite MSE-correlation loss and a curriculum that gradually shifts emphasis from early sensory to late association regions. Averaging 100 model variants further boosts robustness. The resulting system ranked third on the competition leaderboard, achieving an overall Pearson r = 0.2094 and the highest single-parcel peak score (mean r = 0.63) among all participants, with particularly strong gains for the most challenging subject (Subject 5). The approach establishes a simple, extensible baseline for future multimodal brain-encoding benchmarks.</article>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flow matching for reaction pathway generation","url":"https://arxiv.org/abs/2507.10530","date":1761796800,"author":"","guid":321119,"unread":true,"content":"<article>arXiv:2507.10530v3 Announce Type: replace-cross \nAbstract: Elucidating reaction mechanisms hinges on efficiently generating transition states (TSs), products, and complete reaction networks. Recent generative models, such as diffusion models for TS sampling and sequence-based architectures for product generation, offer faster alternatives to quantum-chemistry searches. But diffusion models remain constrained by their stochastic differential equation (SDE) dynamics, which suffer from inefficiency and limited controllability. We show that flow matching, a deterministic ordinary differential (ODE) formulation, can replace SDE-based diffusion for molecular and reaction generation. We introduce MolGEN, a conditional flow-matching framework that learns an optimal transport path to transport Gaussian priors to target chemical distributions. On benchmarks used by TSDiff and OA-ReactDiff, MolGEN surpasses TS geometry accuracy and barrier-height prediction while reducing sampling to sub-second inference. MolGEN also supports open-ended product generation with competitive top-k accuracy and avoids mass/electron-balance violations common to sequence models. In a realistic test on the $\\gamma$-ketohydroperoxide decomposition network, MolGEN yields higher fractions of valid and intended TSs with markedly fewer quantum-chemistry evaluations than string-based baselines. These results demonstrate that deterministic flow matching provides a unified, accurate, and computationally efficient foundation for molecular generative modeling, signaling that flow matching is the future for molecular generation across chemistry.</article>","contentLength":1627,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A PBN-RL-XAI Framework for Discovering a \"Hit-and-Run\" Therapeutic Strategy in Melanoma","url":"https://arxiv.org/abs/2507.10136","date":1761796800,"author":"","guid":321120,"unread":true,"content":"<article>arXiv:2507.10136v5 Announce Type: replace-cross \nAbstract: Innate resistance to anti-PD-1 immunotherapy remains a major clinical challenge in metastatic melanoma, with the underlying molecular networks being poorly understood. To address this, we constructed a dynamic Probabilistic Boolean Network model using transcriptomic data from patient tumor biopsies to elucidate the regulatory logic governing therapy response. We then employed a reinforcement learning agent to systematically discover optimal, multi-step therapeutic interventions and used explainable artificial intelligence to mechanistically interpret the agent's control policy. The analysis revealed that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2 protein (LOXL2) was the most effective strategy. Our explainable analysis showed that this ''hit-and-run\" intervention is sufficient to erase the molecular signature driving resistance, allowing the network to self-correct without requiring sustained intervention. This study presents a novel, time-dependent therapeutic hypothesis for overcoming immunotherapy resistance and provides a powerful computational framework for identifying non-obvious intervention protocols in complex biological systems.</article>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars","url":"https://arxiv.org/abs/2507.01939","date":1761796800,"author":"","guid":321121,"unread":true,"content":"<article>arXiv:2507.01939v3 Announce Type: replace-cross \nAbstract: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy.</article>","contentLength":1832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Thompson Sampling in Function Spaces via Neural Operators","url":"https://arxiv.org/abs/2506.21894","date":1761796800,"author":"","guid":321122,"unread":true,"content":"<article>arXiv:2506.21894v2 Announce Type: replace-cross \nAbstract: We propose an extension of Thompson sampling to optimization problems over function spaces where the objective is a known functional of an unknown operator's output. We assume that queries to the operator (such as running a high-fidelity simulator or physical experiment) are costly, while functional evaluations on the operator's output are inexpensive. Our algorithm employs a sample-then-optimize approach using neural operator surrogates. This strategy avoids explicit uncertainty quantification by treating trained neural operators as approximate samples from a Gaussian process (GP) posterior. We derive regret bounds and theoretical results connecting neural operators with GPs in infinite-dimensional settings. Experiments benchmark our method against other Bayesian optimization baselines on functional optimization tasks involving partial differential equations of physical systems, demonstrating better sample efficiency and significant performance gains.</article>","contentLength":1025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling","url":"https://arxiv.org/abs/2505.22527","date":1761796800,"author":"","guid":321123,"unread":true,"content":"<article>arXiv:2505.22527v2 Announce Type: replace-cross \nAbstract: We introduce the \\emph{Symplectic Generative Network (SGN)}, a deep generative model that leverages Hamiltonian mechanics to construct an invertible, volume-preserving mapping between a latent space and the data space. By endowing the latent space with a symplectic structure and modeling data generation as the time evolution of a Hamiltonian system, SGN achieves exact likelihood evaluation without incurring the computational overhead of Jacobian determinant calculations. In this work, we provide a rigorous mathematical foundation for SGNs through a comprehensive theoretical framework that includes: (i) complete proofs of invertibility and volume preservation, (ii) a formal complexity analysis with theoretical comparisons to Variational Autoencoders and Normalizing Flows, (iii) strengthened universal approximation results with quantitative error bounds, (iv) an information-theoretic analysis based on the geometry of statistical manifolds, and (v) an extensive stability analysis with adaptive integration guarantees. These contributions highlight the fundamental advantages of SGNs and establish a solid foundation for future empirical investigations and applications to complex, high-dimensional data.</article>","contentLength":1274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Adaptive Experimentation with Noncompliance","url":"https://arxiv.org/abs/2505.17468","date":1761796800,"author":"","guid":321124,"unread":true,"content":"<article>arXiv:2505.17468v2 Announce Type: replace-cross \nAbstract: We study the problem of estimating the average treatment effect (ATE) in adaptive experiments where treatment can only be encouraged -- rather than directly assigned -- via a binary instrumental variable. Building on semiparametric efficiency theory, we derive the efficiency bound for ATE estimation under arbitrary, history-dependent instrument-assignment policies, and show it is minimized by a variance-aware allocation rule that balances outcome noise and compliance variability. Leveraging this insight, we introduce AMRIV -- an Adaptive, Multiply-Robust estimator for Instrumental-Variable settings with variance-optimal assignment. AMRIV pairs (i) an online policy that adaptively approximates the optimal allocation with (ii) a sequential, influence-function-based estimator that attains the semiparametric efficiency bound while retaining multiply-robust consistency. We establish asymptotic normality, explicit convergence rates, and anytime-valid asymptotic confidence sequences that enable sequential inference. Finally, we demonstrate the practical effectiveness of our approach through empirical studies, showing that adaptive instrument assignment, when combined with the AMRIV estimator, yields improved efficiency and robustness compared to existing baselines.</article>","contentLength":1337,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space","url":"https://arxiv.org/abs/2505.16301","date":1761796800,"author":"","guid":321125,"unread":true,"content":"<article>arXiv:2505.16301v2 Announce Type: replace-cross \nAbstract: Molecular dynamics (MD) is a powerful tool for exploring the behavior of atomistic systems, but its reliance on sequential numerical integration limits simulation efficiency. We present a novel neural network architecture, MDtrajNet, and a pre-trained foundational model, MDtrajNet-1, that directly generates MD trajectories across chemical space, bypassing force calculations and integration. This approach accelerates simulations by up to two orders of magnitude compared to traditional MD, even those enhanced by machine-learning interatomic potentials. MDtrajNet combines equivariant neural networks with a transformer-based architecture to achieve strong accuracy and transferability in predicting long-time trajectories. Remarkably, the errors of the trajectories generated by MDtrajNet-1 for various known and unseen molecular systems are close to those of the conventional ab initio MD. The architecture's flexible design supports diverse application scenarios, including different statistical ensembles, boundary conditions, and interaction types. By overcoming the intrinsic speed barrier of conventional MD, MDtrajNet opens new frontiers in efficient and scalable atomistic simulations.</article>","contentLength":1256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continuous Domain Generalization","url":"https://arxiv.org/abs/2505.13519","date":1761796800,"author":"","guid":321126,"unread":true,"content":"<article>arXiv:2505.13519v2 Announce Type: replace-cross \nAbstract: Real-world data distributions often shift continuously across multiple latent factors such as time, geography, and socioeconomic contexts. However, existing domain generalization approaches typically treat domains as discrete or as evolving along a single axis (e.g., time). This oversimplification fails to capture the complex, multidimensional nature of real-world variation. This paper introduces the task of Continuous Domain Generalization (CDG), which aims to generalize predictive models to unseen domains defined by arbitrary combinations of continuous variations. We present a principled framework grounded in geometric and algebraic theories, showing that optimal model parameters across domains lie on a low-dimensional manifold. To model this structure, we propose a Neural Lie Transport Operator (NeuralLio), which enables structure-preserving parameter transitions by enforcing geometric continuity and algebraic consistency. To handle noisy or incomplete domain variation descriptors, we introduce a gating mechanism to suppress irrelevant dimensions and a local chart-based strategy for robust generalization. Extensive experiments on synthetic and real-world datasets, including remote sensing, scientific documents, and traffic forecasting, demonstrate that our method significantly outperforms existing baselines in both generalization accuracy and robustness.</article>","contentLength":1438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On edge-colouring-games by Erd\\H{o}s, and Bensmail and Mc Inerney","url":"https://arxiv.org/abs/2505.03497","date":1761796800,"author":"","guid":321127,"unread":true,"content":"<article>arXiv:2505.03497v2 Announce Type: replace-cross \nAbstract: We study two games proposed by Erd\\H{o}s, and one game by Bensmail and Mc Inerney, all sharing a common setup: two players alternately colour edges of a complete graph, or in the biased version, they colour $p$ and $q$ edges respectively on their turns, aiming to maximise a graph parameter determined by their respective induced subgraphs. In the unbiased case, we give a first reduction towards confirming the conjecture of Bensmail and Mc Inerney, propose a conjecture for Erd\\H{o}s' game on maximum degree, and extend the clique and maximum-degree versions to edge-transitive and regular graphs. In the biased case, the maximum-degree and vertex-capturing games are resolved, and we prove the clique game with $(p,q)=(1,3)$.</article>","contentLength":787,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continuity Conditions for Piecewise Quadratic Functions on Simplicial Conic Partitions are Equivalent","url":"https://arxiv.org/abs/2504.15914","date":1761796800,"author":"","guid":321128,"unread":true,"content":"<article>arXiv:2504.15914v2 Announce Type: replace-cross \nAbstract: Analysis of continuous-time piecewise linear systems based on piecewise quadratic (PWQ) Lyapunov functions typically requires continuity of these functions over a partition of the state space. Several conditions for guaranteeing continuity of PWQ functions over state space partitions can be found in the literature. In this technical note, we show that these continuity conditions are equivalent over so-called simplicial conic partitions. As a consequence, the choice of which condition to impose can be based solely on practical considerations such as specific application or numerical aspects, without introducing additional conservatism in the analysis.</article>","contentLength":717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Neural Pruning Law Hypothesis","url":"https://arxiv.org/abs/2504.05349","date":1761796800,"author":"","guid":321129,"unread":true,"content":"<article>arXiv:2504.05349v3 Announce Type: replace-cross \nAbstract: Network pruning is used to reduce inference latency and power consumption in large neural networks. However, most current pruning methods rely on ad-hoc heuristics that are poorly understood. We introduce Hyperflux, a conceptually-grounded pruning method, and use it to study the pruning process. Hyperflux models this process as an interaction between weight flux, the gradient's response to the weight's removal, and network pressure, a global regularization driving weights towards pruning. We postulate properties that arise naturally from our framework and find that the relationship between minimum flux among weights and density follows a power-law equation. Furthermore, we hypothesize the power-law relationship to hold for any effective saliency metric and call this idea the Neural Pruning Law Hypothesis. We validate our hypothesis on several families of pruning methods (magnitude, gradients, $L_0$), providing a potentially unifying property for neural pruning.</article>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Steiner Traveling Salesman Problem with Quantum Annealing","url":"https://arxiv.org/abs/2504.02388","date":1761796800,"author":"","guid":321130,"unread":true,"content":"<article>arXiv:2504.02388v4 Announce Type: replace-cross \nAbstract: The Steiner Traveling Salesman Problem (STSP) is a variant of the classical Traveling Salesman Problem. The STSP involves incorporating steiner nodes, which are extra nodes not originally part of the required visit set but that can be added to the route to enhance the overall solution and minimize the total travel cost. Given the NP-hard nature of the STSP, we propose a quantum approach to address it. Specifically, we employ quantum annealing using D-Wave's hardware to explore its potential for solving this problem. To enhance computational feasibility, we develop a preprocessing method that effectively reduces the network size. Our experimental results demonstrate that this reduction technique significantly decreases the problem complexity, making the Quadratic Unconstrained Binary Optimization formulation, the standard input for quantum annealers, better suited for existing quantum hardware. Furthermore, the results highlight the potential of quantum annealing as a promising and innovative approach for solving the STSP.</article>","contentLength":1096,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A linear, unconditionally stable, second order decoupled method for the Ericksen-Leslie model with SAV approach","url":"https://arxiv.org/abs/2503.19424","date":1761796800,"author":"","guid":321131,"unread":true,"content":"<article>arXiv:2503.19424v2 Announce Type: replace-cross \nAbstract: In this paper, we present a second order, linear, fully decoupled, and unconditionally energy stable scheme for solving the Erickson-Leslie model. This approach integrates the pressure correction method with a scalar auxiliary variable technique. We rigorously demonstrate the unconditional energy stability of the proposed scheme. Furthermore, we present several numerical experiments to validate its convergence order, stability, and computational efficiency.</article>","contentLength":520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Computing Certificates of Strictly Positive Polynomials in Archimedean Quadratic Modules","url":"https://arxiv.org/abs/2503.11119","date":1761796800,"author":"","guid":321132,"unread":true,"content":"<article>arXiv:2503.11119v2 Announce Type: replace-cross \nAbstract: New results on computing certificates of strictly positive polynomials in Archimedean quadratic modules are presented. The results build upon (i) Averkov's method for generating a strictly positive polynomial for which a membership certificate can be more easily computed than the input polynomial whose certificate is being sought, and (ii) Lasserre's method for generating a certificate by successively approximating a nonnegative polynomial by sums of squares. First, a fully constructive method based on Averkov's result is given by providing details about the parameters; further, his result is extended to work on arbitrary subsets, in particular, the whole Euclidean space $\\mathbb{R}^n$, producing globally strictly positive polynomials. Second, Lasserre's method is integrated with the extended Averkov construction to generate certificates. Third, the methods have been implemented and their effectiveness is illustrated. Examples are given on which the existing software package RealCertify appears to struggle, whereas the proposed method succeeds in generating certificates. Several situations are identified where an Archimedean polynomial does not have to be explicitly included in a set of generators of an Archimedean quadratic module. Unlike other approaches for addressing the problem of computing certificates, the methods/approach presented is easier to understand as well as implement.</article>","contentLength":1466,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Neural Symbolic Model for Space Physics","url":"https://arxiv.org/abs/2503.07994","date":1761796800,"author":"","guid":321133,"unread":true,"content":"<article>arXiv:2503.07994v3 Announce Type: replace-cross \nAbstract: In this study, we unveil a new AI model, termed PhyE2E, to discover physical formulas through symbolic regression. PhyE2E simplifies symbolic regression by decomposing it into sub-problems using the second-order derivatives of an oracle neural network, and employs a transformer model to translate data into symbolic formulas in an end-to-end manner. The resulting formulas are refined through Monte-Carlo Tree Search and Genetic Programming. We leverage a large language model to synthesize extensive symbolic expressions resembling real physics, and train the model to recover these formulas directly from data. A comprehensive evaluation reveals that PhyE2E outperforms existing state-of-the-art approaches, delivering superior symbolic accuracy, precision in data fitting, and consistency in physical units. We deployed PhyE2E to five applications in space physics, including the prediction of sunspot numbers, solar rotational angular velocity, emission line contribution functions, near-Earth plasma pressure, and lunar-tide plasma signals. The physical formulas generated by AI demonstrate a high degree of accuracy in fitting the experimental data from satellites and astronomical telescopes. We have successfully upgraded the formula proposed by NASA in 1993 regarding solar activity, and for the first time, provided the explanations for the long cycle of solar activity in an explicit form. We also found that the decay of near-Earth plasma pressure is proportional to r^2 to Earth, where subsequent mathematical derivations are consistent with satellite data from another independent study. Moreover, we found physical formulas that can describe the relationships between emission lines in the extreme ultraviolet spectrum of the Sun, temperatures, electron densities, and magnetic fields. The formula obtained is consistent with the properties that physicists had previously hypothesized it should possess.</article>","contentLength":1978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks","url":"https://arxiv.org/abs/2502.21269","date":1761796800,"author":"","guid":321134,"unread":true,"content":"<article>arXiv:2502.21269v3 Announce Type: replace-cross \nAbstract: Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm. We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width $m$, and large number of samples per input dimension $n/d$, the training dynamics exhibits a separation of timescales which implies: $(i)$~The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network; $(ii)$~Inductive bias towards small complexity if the initialization has small enough complexity; $(iii)$~A dynamical decoupling between feature learning and overfitting regimes; $(iv)$~A non-monotone behavior of the test error, associated `feature unlearning' regime at large times.</article>","contentLength":977,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partitions of planar (oriented) graphs into a connected acyclic and an independent set","url":"https://arxiv.org/abs/2412.11774","date":1761796800,"author":"","guid":321135,"unread":true,"content":"<article>arXiv:2412.11774v2 Announce Type: replace-cross \nAbstract: A question at the intersection of Barnette's Hamiltonicity and Neumann-Lara's dicoloring conjecture is: Can every Eulerian oriented planar graph be vertex-partitioned into two acyclic sets? A CAI-partition of an undirected/oriented graph is a partition into a tree/connected acyclic subgraph and an independent set. Consider any plane Eulerian oriented triangulation together with its unique tripartition, i.e. partition into three independent sets. If two of these three sets induce a subgraph G that has a CAI-partition, then the above question has a positive answer. We show that if G is subcubic, then it has a CAI-partition, i.e. oriented planar bipartite subcubic 2-vertex-connected graphs admit CAI-partitions. We also show that series-parallel 2-vertex-connected graphs admit CAI-partitions. Finally, we present a Eulerian oriented triangulation such that no two sets of its tripartition induce a graph with a CAI-partition. This generalizes a result of Alt, Payne, Schmidt, and Wood to the oriented setting.</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Triangulated spheres with holes in triangulated surfaces","url":"https://arxiv.org/abs/2410.04450","date":1761796800,"author":"","guid":321136,"unread":true,"content":"<article>arXiv:2410.04450v3 Announce Type: replace-cross \nAbstract: Let $\\mathbb{S}_h$ denote a sphere with $h$ holes. Given a triangulation $G$ of a surface $\\mathbb{M}$, we consider the question of when $G$ contains a spanning subgraph $H$ such that $H$ is a triangulated $\\mathbb{S}_h$. We give a new short proof of a theorem of Nevo and Tarabykin that every triangulation $G$ of the torus contains a spanning subgraph which is a triangulated cylinder. For arbitrary surfaces, we prove that every high facewidth triangulation of a surface with $h$ handles contains a spanning subgraph which is a triangulated $\\mathbb{S}_{2h}$. We also prove that for every $0 \\leq g' &lt; g$ and $w \\in \\mathbb{N}$, there exists a triangulation of facewidth at least $w$ of a surface of Euler genus $g$ that does not have a spanning subgraph which is a triangulated $\\mathbb{S}_{g'}$. Our results are motivated by, and have applications for, rigidity questions in the plane.</article>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring End-to-end Differentiable Neural Charged Particle Tracking -- A Loss Landscape Perspective","url":"https://arxiv.org/abs/2407.13420","date":1761796800,"author":"","guid":321137,"unread":true,"content":"<article>arXiv:2407.13420v2 Announce Type: replace-cross \nAbstract: Measurement and analysis of high energetic particles for scientific, medical or industrial applications is a complex procedure, requiring the design of sophisticated detector and data processing systems. The development of adaptive and differentiable software pipelines using a combination of conventional and machine learning algorithms is therefore getting ever more important to optimize and operate the system efficiently while maintaining end-to-end (E2E) differentiability. We propose for the application of charged particle tracking an E2E differentiable decision-focused learning scheme using graph neural networks with combinatorial components solving a linear assignment problem for each detector layer. We demonstrate empirically that including differentiable variations of discrete assignment operations allows for efficient network optimization, working better or on par with approaches that lack E2E differentiability. In additional studies, we dive deeper into the optimization process and provide further insights from a loss landscape perspective. We demonstrate that while both methods converge into similar performing, globally well-connected regions, they suffer under substantial predictive instability across initialization and optimization methods, which can have unpredictable consequences on the performance of downstream tasks such as image reconstruction. We also point out a dependency between the interpolation factor of the gradient estimator and the prediction stability of the model, suggesting the choice of sufficiently small values. Given the strong global connectivity of learned solutions and the excellent training performance, we argue that E2E differentiability provides, besides the general availability of gradient information, an important tool for robust particle tracking to mitigate prediction instabilities by favoring solutions that perform well on downstream tasks.</article>","contentLength":1973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linear Operator Approximate Message Passing (OpAMP)","url":"https://arxiv.org/abs/2405.08225","date":1761796800,"author":"","guid":321138,"unread":true,"content":"<article>arXiv:2405.08225v2 Announce Type: replace-cross \nAbstract: This paper introduces a framework for approximate message passing (AMP) in dynamic settings where the data at each iteration is passed through a linear operator. This framework is motivated in part by applications in large-scale, distributed computing where only a subset of the data is available at each iteration. An autoregressive memory term is used to mitigate information loss across iterations and a specialized algorithm, called projection AMP, is designed for the case where each linear operator is an orthogonal projection. Precise theoretical guarantees are provided for a class of Gaussian matrices and non-separable denoising functions. Specifically, it is shown that the iterates can be well-approximated in the high-dimensional limit by a Gaussian process whose second-order statistics are defined recursively via state evolution. These results are applied to the problem of estimating a rank-one spike corrupted by additive Gaussian noise using partial row updates, and the theory is validated by numerical simulations.</article>","contentLength":1094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Transformer: Accelerating model inference via quantum linear algebra","url":"https://arxiv.org/abs/2402.16714","date":1761796800,"author":"","guid":321139,"unread":true,"content":"<article>arXiv:2402.16714v3 Announce Type: replace-cross \nAbstract: Powerful generative artificial intelligence from large language models (LLMs) harnesses extensive computational resources for inference. In this work, we investigate the transformer architecture, a key component of these models, under the lens of fault-tolerant quantum computing. We develop quantum subroutines to construct the building blocks in the transformer, including the self-attention, residual connection with layer normalization, and feed-forward network. As an important subroutine, we show how to efficiently implement the Hadamard product and element-wise functions of matrices on quantum computers. Our algorithm prepares an amplitude encoding of the transformer output, which can be measured for prediction or use in the next layer. We find that the matrix norm of the input sequence plays a dominant role in the quantum complexity. With numerical experiments on open-source LLMs, including for bio-informatics applications, we demonstrate the potential of a quantum speedup for transformer inference in practical regimes.</article>","contentLength":1097,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tracking the Median of Gradients with a Stochastic Proximal Point Method","url":"https://arxiv.org/abs/2402.12828","date":1761796800,"author":"","guid":321140,"unread":true,"content":"<article>arXiv:2402.12828v2 Announce Type: replace-cross \nAbstract: There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study SGD with robust gradient estimators based on estimating the median.\n  We first derive iterative methods based on the stochastic proximal point method for computing the median gradient and generalizations thereof. Then we propose an algorithm estimating the median gradient across iterations, and find that several well known methods are particular cases of this framework. For instance, we observe that different forms of clipping allow to compute online estimators of the median of gradients, in contrast to (heavy-ball) momentum, which corresponds to an online estimator of the mean. Finally, we provide a theoretical framework for an algorithm computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise.</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hadamard-Hitchcock decompositions: identifiability and computation","url":"https://arxiv.org/abs/2308.06597","date":1761796800,"author":"","guid":321141,"unread":true,"content":"<article>arXiv:2308.06597v2 Announce Type: replace-cross \nAbstract: A Hadamard-Hitchcock decomposition of a multidimensional array is a decomposition that expresses the latter as a Hadamard product of several tensor rank decompositions. Such decompositions can encode probability distributions that arise from statistical graphical models associated to complete bipartite graphs with one layer of observed random variables and one layer of hidden ones, usually called restricted Boltzmann machines. We establish generic identifiability of Hadamard-Hitchcock decompositions by exploiting the reshaped Kruskal criterion for tensor rank decompositions. A flexible algorithm leveraging existing decomposition algorithms for tensor rank decomposition is introduced for computing a Hadamard-Hitchcock decomposition. Numerical experiments illustrate its computational performance and numerical accuracy.</article>","contentLength":887,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Fitted-Q-Evaluation and Iteration under Sequentially Exogenous Unobserved Confounders","url":"https://arxiv.org/abs/2302.00662","date":1761796800,"author":"","guid":321142,"unread":true,"content":"<article>arXiv:2302.00662v3 Announce Type: replace-cross \nAbstract: Offline reinforcement learning is important in domains such as medicine, economics, and e-commerce where online experimentation is costly, dangerous or unethical, and where the true model is unknown. However, most methods assume all covariates used in the behavior policy's action decisions are observed. Though this assumption, sequential ignorability/unconfoundedness, likely does not hold in observational data, most of the data that accounts for selection into treatment may be observed, motivating sensitivity analysis. We study robust policy evaluation and policy optimization in the presence of sequentially-exogenous unobserved confounders under a sensitivity model. We propose and analyze orthogonalized robust fitted-Q-iteration that uses closed-form solutions of the robust Bellman operator to derive a loss minimization problem for the robust Q function, and adds a bias-correction to quantile estimation. Our algorithm enjoys the computational ease of fitted-Q-iteration and statistical improvements (reduced dependence on quantile estimation error) from orthogonalization. We provide sample complexity bounds, insights, and show effectiveness both in simulations and on real-world longitudinal healthcare data of treating sepsis. In particular, our model of sequential unobserved confounders yields an online Markov decision process, rather than partially observed Markov decision process: we illustrate how this can enable warm-starting optimistic reinforcement learning algorithms with valid robust bounds from observational data.</article>","contentLength":1605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-parameter Module Approximation: an efficient and interpretable invariant for multi-parameter persistence modules with guarantees","url":"https://arxiv.org/abs/2206.02026","date":1761796800,"author":"","guid":321143,"unread":true,"content":"<article>arXiv:2206.02026v4 Announce Type: replace-cross \nAbstract: In this article, we introduce a new parameterized family of topological descriptors, taking the form of candidate decompositions, for multi-parameter persistence modules, and we identify a subfamily of these descriptors, that we call approximate decompositions, that are controllable approximations, in the sense that they preserve diagonal barcodes. Then, we introduce MMA (Multipersistence Module Approximation): an algorithm based on matching functions for computing instances of candidate decompositions with some precision parameter {\\delta} &gt; 0. By design, MMA can handle an arbitrary number of filtrations, and has bounded complexity and running time. Moreover, we prove the robustess of MMA: when computed with so-called compatible matching functions, we show that MMA produces approximate decompositions (and we prove that such matching functions exist for n = 2 filtrations). Next, we restrict the focus on modules that can be decomposed into interval summands. In that case, compatible matching functions always exist, and we show that, for small enough {\\delta}, the approximate decompositions obtained with such compatible matching functions by MMA have an approximation error (in terms of the standard interleaving and bottleneck distances) that is bounded by {\\delta}, and that reaches zero for an even smaller, positive precision. Finally, we present empirical evidence validating that MMA has state-of-the-art performance and running time on several data sets.</article>","contentLength":1536,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station","url":"https://arxiv.org/abs/2510.23463","date":1761796800,"author":"","guid":321144,"unread":true,"content":"<article>arXiv:2510.23463v2 Announce Type: replace \nAbstract: Federated Learning (FL) is a distributed learning paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \\emph{over-the-air FL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy} in the sense that it degrades training due to noisy global aggregation while providing a natural source of randomness for privacy-preserving mechanisms, formally quantified by \\emph{differential privacy (DP)}. It remains, nevertheless, challenging to effectively harness such channel impairments, as prior arts, under assumptions of either simple channel models or restricted types of loss functions, mostly considering (local) DP enhancement with a single-round or non-convergent bound on privacy loss. In this paper, we study AirFL over multiple-access fading channels with a multi-antenna base station (BS) subject to user-level DP requirements. Despite a recent study, which claimed in similar settings that artificial noise (AN) must be injected to ensure DP in general, we demonstrate, on the contrary, that DP can be gained as a \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a novel bound on DP that converges under general bounded-domain assumptions on model parameters, along with a convergence bound with general smooth and non-convex loss functions. Next, we optimize over receive beamforming and power allocations to characterize the optimal convergence-privacy trade-offs, which also reveal explicit conditions in which DP is achievable without compromising training. Finally, our theoretical findings are validated by extensive numerical results.</article>","contentLength":1827,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reciprocity Deficits: Observing AI in the street with everyday publics","url":"https://arxiv.org/abs/2510.23342","date":1761796800,"author":"","guid":321145,"unread":true,"content":"<article>arXiv:2510.23342v2 Announce Type: replace \nAbstract: The street has emerged as a primary site where everyday publics are confronted with AI as an infrastructural phenomenon, as machine learning-based systems are now commonly deployed in this setting in the form of automated cars, facial recognition, smart billboards and the like. While these deployments of AI in the street have attracted significant media attention and public controversy in recent years, the presence of AI in the street often remains inscrutable, and many everyday publics are unaware of it. In this paper, we explore the challenges and possibilities of everyday public engagement with AI in the situated environment of city streets under these paradoxical conditions. Combining perspectives and approaches from social and cultural studies of AI, Design Research and Science and Technology Studies (STS), we explore the affordances of the street as a site for 'material participation' in AI through design-based interventions: the creation of 'everyday AI observatories.' We narrate and reflect on our participatory observations of AI in five city streets in the UK and Australia and highlight a set of tensions that emerged from them: 1) the framing of the street as a transactional environment, 2) the designed invisibility of AI and its publics in the street 3) the stratification of street environments through statistical governance. Based on this discussion and drawing on Jane Jacobs' notion of \"eyes on the street,\" we put forward the relational notion of \"reciprocity deficits\" between AI infrastructures and everyday publics in the street. The conclusion reflects on the consequences of this form of social invisibility of AI for situated engagement with AI by everyday publics in the street and for public trust in urban governance.</article>","contentLength":1815,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Scaling Deep Neural Networks with Predictive Coding: Theory and Practice","url":"https://arxiv.org/abs/2510.23323","date":1761796800,"author":"","guid":321146,"unread":true,"content":"<article>arXiv:2510.23323v2 Announce Type: replace \nAbstract: Backpropagation (BP) is the standard algorithm for training the deep neural networks that power modern artificial intelligence including large language models. However, BP is energy inefficient and unlikely to be implemented by the brain. This thesis studies an alternative, potentially more efficient brain-inspired algorithm called predictive coding (PC). Unlike BP, PC networks (PCNs) perform inference by iterative equilibration of neuron activities before learning or weight updates. Recent work has suggested that this iterative inference procedure provides a range of benefits over BP, such as faster training. However, these advantages have not been consistently observed, the inference and learning dynamics of PCNs are still poorly understood, and deep PCNs remain practically untrainable. Here, we make significant progress towards scaling PCNs by taking a theoretical approach grounded in optimisation theory. First, we show that the learning dynamics of PC can be understood as an approximate trust-region method using second-order information, despite explicitly using only first-order local updates. Second, going beyond this approximation, we show that PC can in principle make use of arbitrarily higher-order information, such that for feedforward networks the effective landscape on which PC learns is far more benign and robust to vanishing gradients than the (mean squared error) loss landscape. Third, motivated by a study of the inference dynamics of PCNs, we propose a new parameterisation called \"$\\mu$PC\", which for the first time allows stable training of 100+ layer networks with little tuning and competitive performance on simple tasks. Overall, this thesis significantly advances our fundamental understanding of the inference and learning dynamics of PCNs, while highlighting the need for future research to focus on hardware co-design if PC is to compete with BP at scale.</article>","contentLength":1957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?","url":"https://arxiv.org/abs/2510.23252","date":1761796800,"author":"","guid":321147,"unread":true,"content":"<article>arXiv:2510.23252v2 Announce Type: replace \nAbstract: Conventional research on speech recognition modeling relies on the canonical form for most low-resource languages while automatic speech recognition (ASR) for regional dialects is treated as a fine-tuning task. To investigate the effects of dialectal variations on ASR we develop a 78-hour annotated Bengali Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and data-driven perspectives shows that speech foundation models struggle heavily in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe that all deep learning methods struggle to model speech data under dialectal variations but dialect specific model training alleviates the issue. Our dataset also serves as a out of-distribution (OOD) resource for ASR modeling under constrained resources in ASR algorithms. The dataset and code developed for this project are publicly available</article>","contentLength":938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GroupSHAP-Guided Integration of Financial News Keywords and Technical Indicators for Stock Price Prediction","url":"https://arxiv.org/abs/2510.23112","date":1761796800,"author":"","guid":321148,"unread":true,"content":"<article>arXiv:2510.23112v2 Announce Type: replace \nAbstract: Recent advances in finance-specific language models such as FinBERT have enabled the quantification of public sentiment into index-based measures, yet compressing diverse linguistic signals into single metrics overlooks contextual nuances and limits interpretability. To address this limitation, explainable AI techniques, particularly SHAP (SHapley Additive Explanations), have been employed to identify influential features. However, SHAP's computational cost grows exponentially with input features, making it impractical for large-scale text-based financial data. This study introduces a GRU-based forecasting framework enhanced with GroupSHAP, which quantifies contributions of semantically related keyword groups rather than individual tokens, substantially reducing computational burden while preserving interpretability. We employed FinBERT to embed news articles from 2015 to 2024, clustered them into coherent semantic groups, and applied GroupSHAP to measure each group's contribution to stock price movements. The resulting group-level SHAP variables across multiple topics were used as input features for the prediction model. Empirical results from one-day-ahead forecasting of the S&amp;P 500 index throughout 2024 demonstrate that our approach achieves a 32.2% reduction in MAE and a 40.5% reduction in RMSE compared with benchmark models without the GroupSHAP mechanism. This research presents the first application of GroupSHAP in news-driven financial forecasting, showing that grouped sentiment representations simultaneously enhance interpretability and predictive performance.</article>","contentLength":1647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey of AI Scientists: Surveying the automatic Scientists and Research","url":"https://arxiv.org/abs/2510.23045","date":1761796800,"author":"","guid":321149,"unread":true,"content":"<article>arXiv:2510.23045v2 Announce Type: replace \nAbstract: Artificial intelligence is undergoing a profound transition from a computational instrument to an autonomous originator of scientific knowledge. This emerging paradigm, the AI scientist, is architected to emulate the complete scientific workflow-from initial hypothesis generation to the final synthesis of publishable findings-thereby promising to fundamentally reshape the pace and scale of discovery. However, the rapid and unstructured proliferation of these systems has created a fragmented research landscape, obscuring overarching methodological principles and developmental trends. This survey provides a systematic and comprehensive synthesis of this domain by introducing a unified, six-stage methodological framework that deconstructs the end-to-end scientific process into: Literature Review, Idea Generation, Experimental Preparation, Experimental Execution, Scientific Writing, and Paper Generation. Through this analytical lens, we chart the field's evolution from early Foundational Modules (2022-2023) to integrated Closed-Loop Systems (2024), and finally to the current frontier of Scalability, Impact, and Human-AI Collaboration (2025-present). By rigorously synthesizing these developments, this survey not only clarifies the current state of autonomous science but also provides a critical roadmap for overcoming remaining challenges in robustness and governance, ultimately guiding the next generation of systems toward becoming trustworthy and indispensable partners in human scientific inquiry.</article>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs","url":"https://arxiv.org/abs/2510.22967","date":1761796800,"author":"","guid":321150,"unread":true,"content":"<article>arXiv:2510.22967v2 Announce Type: replace \nAbstract: The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.</article>","contentLength":1136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LightBagel: A Light-weighted, Double Fusion Framework for Unified Multimodal Understanding and Generation","url":"https://arxiv.org/abs/2510.22946","date":1761796800,"author":"","guid":321151,"unread":true,"content":"<article>arXiv:2510.22946v2 Announce Type: replace \nAbstract: Unified multimodal models have recently shown remarkable gains in both capability and versatility, yet most leading systems are still trained from scratch and require substantial computational resources. In this paper, we show that competitive performance can be obtained far more efficiently by strategically fusing publicly available models specialized for either generation or understanding. Our key design is to retain the original blocks while additionally interleaving multimodal self-attention blocks throughout the networks. This double fusion mechanism (1) effectively enables rich multi-modal fusion while largely preserving the original strengths of the base models, and (2) catalyzes synergistic fusion of high-level semantic representations from the understanding encoder with low-level spatial signals from the generation encoder. By training with only ~ 35B tokens, this approach achieves strong results across multiple benchmarks: 0.91 on GenEval for compositional text-to-image generation, 82.16 on DPG-Bench for complex text-to-image generation, 6.06 on GEditBench, and 3.77 on ImgEdit-Bench for image editing. By fully releasing the entire suite of code, model weights, and datasets, we hope to support future research on unified multimodal modeling.</article>","contentLength":1322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FastJAM: a Fast Joint Alignment Model for Images","url":"https://arxiv.org/abs/2510.22842","date":1761796800,"author":"","guid":321152,"unread":true,"content":"<article>arXiv:2510.22842v2 Announce Type: replace \nAbstract: Joint Alignment (JA) of images aims to align a collection of images into a unified coordinate frame, such that semantically-similar features appear at corresponding spatial locations. Most existing approaches often require long training times, large-capacity models, and extensive hyperparameter tuning. We introduce FastJAM, a rapid, graph-based method that drastically reduces the computational complexity of joint alignment tasks. FastJAM leverages pairwise matches computed by an off-the-shelf image matcher, together with a rapid nonparametric clustering, to construct a graph representing intra- and inter-image keypoint relations. A graph neural network propagates and aggregates these correspondences, efficiently predicting per-image homography parameters via image-level pooling. Utilizing an inverse-compositional loss, that eliminates the need for a regularization term over the predicted transformations (and thus also obviates the hyperparameter tuning associated with such terms), FastJAM performs image JA quickly and effectively. Experimental results on several benchmarks demonstrate that FastJAM achieves results better than existing modern JA methods in terms of alignment quality, while reducing computation time from hours or minutes to mere seconds. Our code is available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/</article>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PSScreen V2: Partially Supervised Multiple Retinal Disease Screening","url":"https://arxiv.org/abs/2510.22589","date":1761796800,"author":"","guid":321153,"unread":true,"content":"<article>arXiv:2510.22589v2 Announce Type: replace \nAbstract: In this work, we propose PSScreen V2, a partially supervised self-training framework for multiple retinal disease screening. Unlike previous methods that rely on fully labelled or single-domain datasets, PSScreen V2 is designed to learn from multiple partially labelled datasets with different distributions, addressing both label absence and domain shift challenges. To this end, PSScreen V2 adopts a three-branch architecture with one teacher and two student networks. The teacher branch generates pseudo labels from weakly augmented images to address missing labels, while the two student branches introduce novel feature augmentation strategies: Low-Frequency Dropout (LF-Dropout), which enhances domain robustness by randomly discarding domain-related low-frequency components, and Low-Frequency Uncertainty (LF-Uncert), which estimates uncertain domain variability via adversarially learned Gaussian perturbations of low-frequency statistics. Extensive experiments on multiple in-domain and out-of-domain fundus datasets demonstrate that PSScreen V2 achieves state-of-the-art performance and superior domain generalization ability. Furthermore, compatibility tests with diverse backbones, including the vision foundation model DINOv2, as well as evaluations on chest X-ray datasets, highlight the universality and adaptability of the proposed framework. The codes are available at https://github.com/boyiZheng99/PSScreen_V2.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CANDI: Hybrid Discrete-Continuous Diffusion Models","url":"https://arxiv.org/abs/2510.22510","date":1761796800,"author":"","guid":321154,"unread":true,"content":"<article>arXiv:2510.22510v2 Announce Type: replace \nAbstract: While continuous diffusion has shown remarkable success in continuous domains such as image generation, its direct application to discrete data has underperformed compared to purely discrete formulations. This gap is counterintuitive, given that continuous diffusion learns score functions that enable joint evolution across multiple positions. To understand this gap, we introduce token identifiability as an analytical framework for understanding how Gaussian noise corrupts discrete data through two mechanisms: discrete identity corruption and continuous rank degradation. We reveal that these mechanisms scale differently with vocabulary size, creating a temporal dissonance: at noise levels where discrete corruption preserves enough structure for conditional learning, continuous denoising is trivial; at noise levels where continuous denoising is meaningful, discrete corruption destroys nearly all conditional structure. To solve this, we propose CANDI (Continuous ANd DIscrete diffusion), a hybrid framework that decouples discrete and continuous corruption, enabling simultaneous learning of both conditional structure and continuous geometry. We empirically validate the temporal dissonance phenomenon and demonstrate that CANDI successfully avoids it. This unlocks the benefits of continuous diffusion for discrete spaces: on controlled generation, CANDI enables classifier-based guidance with off-the-shelf classifiers through simple gradient addition; on text generation, CANDI outperforms masked diffusion at low NFE, demonstrating the value of learning continuous gradients for discrete spaces. We include the code on the project page available here: https://patrickpynadath1.github.io/candi-lander</article>","contentLength":1768,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PromptReverb: Multimodal Room Impulse Response Generation Through Latent Rectified Flow Matching","url":"https://arxiv.org/abs/2510.22439","date":1761796800,"author":"","guid":321155,"unread":true,"content":"<article>arXiv:2510.22439v2 Announce Type: replace \nAbstract: Room impulse response (RIR) generation remains a critical challenge for creating immersive virtual acoustic environments. Current methods suffer from two fundamental limitations: the scarcity of full-band RIR datasets and the inability of existing models to generate acoustically accurate responses from diverse input modalities. We present PromptReverb, a two-stage generative framework that addresses these challenges. Our approach combines a variational autoencoder that upsamples band-limited RIRs to full-band quality (48 kHz), and a conditional diffusion transformer model based on rectified flow matching that generates RIRs from descriptions in natural language. Empirical evaluation demonstrates that PromptReverb produces RIRs with superior perceptual quality and acoustic accuracy compared to existing methods, achieving 8.8% mean RT60 error compared to -37% for widely used baselines and yielding more realistic room-acoustic parameters. Our method enables practical applications in virtual reality, architectural acoustics, and audio production where flexible, high-quality RIR synthesis is essential.</article>","contentLength":1167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TowerVision: Understanding and Improving Multilinguality in Vision-Language Models","url":"https://arxiv.org/abs/2510.21849","date":1761796800,"author":"","guid":321156,"unread":true,"content":"<article>arXiv:2510.21849v2 Announce Type: replace \nAbstract: Despite significant advances in vision-language models (VLMs), most existing work follows an English-centric design process, limiting their effectiveness in multilingual settings. In this work, we provide a comprehensive empirical study analyzing the impact of several multilingual design choices, such as training data composition, encoder selection, and text backbones. The result is TowerVision, a family of open multilingual VLMs for both image-text and video-text tasks, built upon the multilingual text-only model Tower+. TowerVision achieves competitive performance on multiple multimodal multilingual benchmarks and shows particular strength in culturally grounded tasks and multimodal translation. By incorporating visual and cultural context during fine-tuning, our models surpass existing approaches trained on substantially larger datasets, as demonstrated on ALM-Bench and Multi30K (image tasks) and ViMUL-Bench (video tasks). Alongside the models, we release VisionBlocks, a high-quality, curated vision-language dataset. Our findings highlight that multilingual vision-language training data substantially improves cross-lingual generalization -- both from high-resource to underrepresented languages and vice versa -- and that instruction-tuned LLMs are not always the optimal initialization point. To support further research, we publicly release all models, data, and training recipes.</article>","contentLength":1456,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantifying Multimodal Imbalance: A GMM-Guided Adaptive Loss for Audio-Visual Learning","url":"https://arxiv.org/abs/2510.21797","date":1761796800,"author":"","guid":321157,"unread":true,"content":"<article>arXiv:2510.21797v2 Announce Type: replace \nAbstract: The heterogeneity of multimodal data leads to inconsistencies and imbalance, allowing a dominant modality to steer gradient updates. Existing solutions mainly focus on optimization- or data-based strategies but rarely exploit the information inherent in multimodal imbalance or conduct its quantitative analysis. To address this gap, we propose a novel quantitative analysis framework for Multimodal Imbalance and design a sample-level adaptive loss function. We define the Modality Gap as the Softmax score difference between modalities for the correct class and model its distribution using a bimodal Gaussian Mixture Model(GMM), representing balanced and imbalanced samples. Using Bayes' theorem, we estimate each sample's posterior probability of belonging to these two groups. Based on this, our adaptive loss (1) minimizes the overall Modality Gap, (2) aligns imbalanced samples with balanced ones, and (3) adaptively penalizes each according to its imbalance degree. A two-stage training strategy-warm-up and adaptive phases,yields state-of-the-art performance on CREMA-D (80.65%), AVE (70.40%), and KineticSound (72.42%). Fine-tuning with high-quality samples identified by the GMM further improves results, highlighting their value for effective multimodal fusion.</article>","contentLength":1326,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Causes Postoperative Aspiration?","url":"https://arxiv.org/abs/2510.21779","date":1761796800,"author":"","guid":321158,"unread":true,"content":"<article>arXiv:2510.21779v2 Announce Type: replace \nAbstract: Background: Aspiration, the inhalation of foreign material into the lungs, significantly impacts surgical patient morbidity and mortality. This study develops a machine learning (ML) model to predict postoperative aspiration, enabling timely preventative interventions.\n  Methods: From the MIMIC-IV database of over 400,000 hospital admissions, we identified 826 surgical patients (mean age: 62, 55.7\\% male) who experienced aspiration within seven days post-surgery, along with a matched non-aspiration cohort. Three ML models: XGBoost, Multilayer Perceptron, and Random Forest were trained using pre-surgical hospitalization data to predict postoperative aspiration. To investigate causation, we estimated Average Treatment Effects (ATE) using Augmented Inverse Probability Weighting.\n  Results: Our ML model achieved an AUROC of 0.86 and 77.3\\% sensitivity on a held-out test set. Maximum daily opioid dose, length of stay, and patient age emerged as the most important predictors. ATE analysis identified significant causative factors: opioids (0.25 +/- 0.06) and operative site (neck: 0.20 +/- 0.13, head: 0.19 +/- 0.13). Despite equal surgery rates across genders, men were 1.5 times more likely to aspirate and received 27\\% higher maximum daily opioid dosages compared to women.\n  Conclusion: ML models can effectively predict postoperative aspiration risk, enabling targeted preventative measures. Maximum daily opioid dosage and operative site significantly influence aspiration risk. The gender disparity in both opioid administration and aspiration rates warrants further investigation. These findings have important implications for improving postoperative care protocols and aspiration prevention strategies.</article>","contentLength":1775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails","url":"https://arxiv.org/abs/2510.21285","date":1761796800,"author":"","guid":321159,"unread":true,"content":"<article>arXiv:2510.21285v2 Announce Type: replace \nAbstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex reasoning tasks but remain vulnerable to severe safety risks, including harmful content generation and jailbreak attacks. Existing mitigation strategies rely on injecting heuristic safety signals during training, which often suppress reasoning ability and fail to resolve the safety-reasoning trade-off. To systematically investigate this issue, we analyze the reasoning trajectories of diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models override their own risk assessments and justify responding to unsafe prompts. This finding reveals that LRMs inherently possess the ability to reject unsafe queries, but this ability is compromised, resulting in harmful outputs. Building on these insights, we propose the Chain-of-Guardrail (CoG), a training framework that recomposes or backtracks unsafe reasoning steps, steering the model back onto safe trajectories while preserving valid reasoning chains. Extensive experiments across multiple reasoning and safety benchmarks demonstrate that CoG substantially improves the safety of current LRMs while preserving comparable reasoning ability, significantly outperforming prior methods that suffer from severe safety-reasoning trade-offs.</article>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Securing AI Agent Execution","url":"https://arxiv.org/abs/2510.21236","date":1761796800,"author":"","guid":321160,"unread":true,"content":"<article>arXiv:2510.21236v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have evolved into AI agents that interact with external tools and environments to perform complex tasks. The Model Context Protocol (MCP) has become the de facto standard for connecting agents with such resources, but security has lagged behind: thousands of MCP servers execute with unrestricted access to host systems, creating a broad attack surface. In this paper, we introduce AgentBound, the first access control framework for MCP servers. AgentBound combines a declarative policy mechanism, inspired by the Android permission model, with a policy enforcement engine that contains malicious behavior without requiring MCP server modifications. We build a dataset containing the 296 most popular MCP servers, and show that access control policies can be generated automatically from source code with 80.9% accuracy. We also show that AgentBound blocks the majority of security threats in several malicious MCP servers, and that policy enforcement engine introduces negligible overhead. Our contributions provide developers and project managers with a practical foundation for securing MCP servers while maintaining productivity, enabling researchers and tool builders to explore new directions for declarative access control and MCP security.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation","url":"https://arxiv.org/abs/2510.21122","date":1761796800,"author":"","guid":321161,"unread":true,"content":"<article>arXiv:2510.21122v2 Announce Type: replace \nAbstract: Reinforcement learning (RL) has shown promise in enhancing the general Chain-of-Thought (CoT) reasoning capabilities of multimodal large language models (MLLMs). However, when applied to improve general CoT reasoning, existing RL frameworks often struggle to generalize beyond the training distribution. To address this, we propose NoisyGRPO, a systematic multimodal RL framework that introduces controllable noise into visual inputs for enhanced exploration and explicitly models the advantage estimation process via a Bayesian framework. Specifically, NoisyGRPO improves RL training by: (1) Noise-Injected Exploration Policy: Perturbing visual inputs with Gaussian noise to encourage exploration across a wider range of visual scenarios; and (2) Bayesian Advantage Estimation: Formulating advantage estimation as a principled Bayesian inference problem, where the injected noise level serves as a prior and the observed trajectory reward as the likelihood. This Bayesian modeling fuses both sources of information to compute a robust posterior estimate of trajectory advantage, effectively guiding MLLMs to prefer visually grounded trajectories over noisy ones. Experiments on standard CoT quality, general capability, and hallucination benchmarks demonstrate that NoisyGRPO substantially improves generalization and robustness, especially in RL settings with small-scale MLLMs such as Qwen2.5-VL 3B. The project page is available at https://artanic30.github.io/project_pages/NoisyGRPO/.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AL-CoLe: Augmented Lagrangian for Constrained Learning","url":"https://arxiv.org/abs/2510.20995","date":1761796800,"author":"","guid":321162,"unread":true,"content":"<article>arXiv:2510.20995v2 Announce Type: replace \nAbstract: Despite the non-convexity of most modern machine learning parameterizations, Lagrangian duality has become a popular tool for addressing constrained learning problems. We revisit Augmented Lagrangian methods, which aim to mitigate the duality gap in non-convex settings while requiring only minimal modifications, and have remained comparably unexplored in constrained learning settings. We establish strong duality results under mild conditions, prove convergence of dual ascent algorithms to feasible and optimal primal solutions, and provide PAC-style generalization guarantees. Finally, we demonstrate its effectiveness on fairness constrained classification tasks.</article>","contentLength":722,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models","url":"https://arxiv.org/abs/2510.20322","date":1761796800,"author":"","guid":321163,"unread":true,"content":"<article>arXiv:2510.20322v2 Announce Type: replace \nAbstract: Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as HyperET, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. HyperET employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that HyperET consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters.</article>","contentLength":1524,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BugPilot: Complex Bug Generation for Efficient Learning of SWE Skills","url":"https://arxiv.org/abs/2510.19898","date":1761796800,"author":"","guid":321164,"unread":true,"content":"<article>arXiv:2510.19898v2 Announce Type: replace \nAbstract: High quality bugs are key to training the next generation of language model based software engineering (SWE) agents. We introduce a novel method for synthetic generation of difficult and diverse bugs. Our method instructs SWE Agents to introduce a feature into the codebase whereby they may unintentionally break tests, resulting in bugs. Prior approaches often induce an out-of-distribution effect by generating bugs intentionally (e.g. by introducing local perturbation to existing code), which does not reflect realistic development processes. We perform qualitative analysis to demonstrate that our approach for generating bugs more closely reflects the patterns found in human-authored edits. Through extensive experiments, we demonstrate that our bugs provide more efficient training data for supervised fine-tuning, outperforming other bug datasets by 2% with half the training data (1.2k vs. 3k bugs). We train on our newly generated bugs in addition to existing bug datasets to get FrogBoss a state-of-the-art 32B parameter model on SWE-bench Verified with a pass@1 of 54.6% and FrogMini a state-of-the-art 14B model on SWE-bench Verified with a pass@1 of 45.3% on SWE-bench Verified averaged over three seeds.</article>","contentLength":1272,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenGuardrails: A Configurable, Unified, and Scalable Guardrails Platform for Large Language Models","url":"https://arxiv.org/abs/2510.19169","date":1761796800,"author":"","guid":321165,"unread":true,"content":"<article>arXiv:2510.19169v2 Announce Type: replace \nAbstract: As large language models (LLMs) are increasingly integrated into real-world applications, ensuring their safety, robustness, and privacy compliance has become critical. We present OpenGuardrails, the first fully open-source platform that unifies large-model-based safety detection, manipulation defense, and deployable guardrail infrastructure. OpenGuardrails protects against three major classes of risks: (1) content-safety violations such as harmful or explicit text generation, (2) model-manipulation attacks including prompt injection, jailbreaks, and code-interpreter abuse, and (3) data leakage involving sensitive or private information. Unlike prior modular or rule-based frameworks, OpenGuardrails introduces three core innovations: (1) a Configurable Policy Adaptation mechanism that allows per-request customization of unsafe categories and sensitivity thresholds; (2) a Unified LLM-based Guard Architecture that performs both content-safety and manipulation detection within a single model; and (3) a Quantized, Scalable Model Design that compresses a 14B dense base model to 3.3B via GPTQ while preserving over 98 of benchmark accuracy. The system supports 119 languages, achieves state-of-the-art performance across multilingual safety benchmarks, and can be deployed as a secure gateway or API-based service for enterprise use. All models, datasets, and deployment scripts are released under the Apache 2.0 license.</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency","url":"https://arxiv.org/abs/2510.18905","date":1761796800,"author":"","guid":321166,"unread":true,"content":"<article>arXiv:2510.18905v2 Announce Type: replace \nAbstract: AI inference scaling is often tuned through 1D heuristics (a fixed reasoning passes) or 2D bivariate trade-offs (e.g., performance vs. compute), which fail to consider cost and latency constraints. We introduce a 3D optimization framework that jointly calibrates accuracy, cost, and latency within a unified decision space, enabling constraints-aware inference scaling. Using Monte Carlo simulations across three representative scenarios and nine simulated large language models, we evaluate four optimization methods to address the 3D multi-objective optimization (MOO) problem. Framing inference scaling in MOO shapes a feasible space that 1D and 2D optimizations fail to capture, enabling environmentadaptive selection of the inference scaling k. Results show that knee-point optimization achieves the best balance, while accuracy-maximization remains favorable when precision is prioritized. The framework establishes a theoretical foundation for deployment-aware inference scaling across diverse operational contexts.</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Systematic Literature Review of the Use of GenAI Assistants for Code Comprehension: Implications for Computing Education Research and Practice","url":"https://arxiv.org/abs/2510.17894","date":1761796800,"author":"","guid":321167,"unread":true,"content":"<article>arXiv:2510.17894v2 Announce Type: replace \nAbstract: The ability to comprehend code has long been recognized as an essential skill in software engineering. As programmers lean more heavily on generative artificial intelligence (GenAI) assistants to develop code solutions, it is becoming increasingly important for programmers to comprehend GenAI solutions so that they can verify their appropriateness and properly integrate them into existing code. At the same time, GenAI tools are increasingly being enlisted to provide programmers with tailored explanations of code written both by GenAI and humans. Thus, in computing education, GenAI presents new challenges and opportunities for learners who are trying to comprehend computer programs. To provide computing educators with evidence-based guidance on the use of GenAI to facilitate code comprehension and to identify directions for future research, we present a systematic literature review (SLR) of state-of-the-art approaches and tools that leverage GenAI to enhance code comprehension. Our SLR focuses on 31 studies published between 2022 and 2024. Despite their potential, GenAI assistants often yield inaccurate or unclear explanations, and novice programmers frequently struggle to craft effective prompts, thereby impeding their ability to leverage GenAI to aid code comprehension. Our review classifies GenAI-based approaches and tools, identifies methods used to study them, and summarizes the empirical evaluations of their effectiveness. We consider the implications of our findings for computing education research and practice, and identify directions for future research.</article>","contentLength":1641,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Curiosity-driven RL for symbolic equation solving","url":"https://arxiv.org/abs/2510.17022","date":1761796800,"author":"","guid":321168,"unread":true,"content":"<article>arXiv:2510.17022v2 Announce Type: replace \nAbstract: We explore if RL can be useful for symbolic mathematics. Previous work showed contrastive learning can solve linear equations in one variable. We show model-free PPO \\cite{schulman2017proximal} augmented with curiosity-based exploration and graph-based actions can solve nonlinear equations such as those involving radicals, exponentials, and trig functions. Our work suggests curiosity-based exploration may be useful for general symbolic reasoning tasks.</article>","contentLength":509,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FTI-TMR: A Fault Tolerance and Isolation Algorithm for Interconnected Multicore Systems","url":"https://arxiv.org/abs/2510.16896","date":1761796800,"author":"","guid":321169,"unread":true,"content":"<article>arXiv:2510.16896v2 Announce Type: replace \nAbstract: Two-Phase TMR conserves energy by partitioning redundancy operations into two stages and making the execution of the third task copy optional, yet it remains susceptible to permanent faults. Reactive-TMR (R-TMR) counters this by isolating faulty cores, handling both transient and permanent faults. However, the lightweight hardware required by R-TMR not only increases complexity but also becomes a single point of failure itself. To bypass isolated node constraints, this paper proposes a Fault Tolerance and Isolation TMR (FTI-TMR) algorithm for interconnected multicore systems. By constructing a stability metric to identify the most reliable nodes in the system, which then perform periodic diagnostics to isolate permanent faults. Experimental results show that FTI-TMR reduces task workload by approximately 30% compared with baseline TMR while achieving higher permanent fault coverage.</article>","contentLength":948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement","url":"https://arxiv.org/abs/2510.16765","date":1761796800,"author":"","guid":321170,"unread":true,"content":"<article>arXiv:2510.16765v2 Announce Type: replace \nAbstract: Image restoration is a fundamental and challenging task in computer vision, where CNN-based frameworks demonstrate significant computational efficiency. However, previous CNN-based methods often face challenges in adequately restoring fine texture details, which are limited by the small receptive field of CNN structures and the lack of channel feature modeling. In this paper, we propose WaMaIR, which is a novel framework with a large receptive field for image perception and improves the reconstruction of texture details in restored images. Specifically, we introduce the Global Multiscale Wavelet Transform Convolutions (GMWTConvs) for expandding the receptive field to extract image features, preserving and enriching texture features in model inputs. Meanwhile, we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to capture long-range dependencies within feature channels, which enhancing the model sensitivity to color, edges, and texture information. Additionally, we propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to guide the model in preserving detailed texture structures effectively. Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods, achieving better image restoration and efficient computational performance of the model.</article>","contentLength":1374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models","url":"https://arxiv.org/abs/2510.16729","date":1761796800,"author":"","guid":321171,"unread":true,"content":"<article>arXiv:2510.16729v2 Announce Type: replace \nAbstract: End-to-end autonomous driving systems increasingly rely on vision-centric world models to understand and predict their environment. However, a common ineffectiveness in these models is the full reconstruction of future scenes, which expends significant capacity on redundantly modeling static backgrounds. To address this, we propose IR-WM, an Implicit Residual World Model that focuses on modeling the current state and evolution of the world. IR-WM first establishes a robust bird's-eye-view representation of the current state from the visual observation. It then leverages the BEV features from the previous timestep as a strong temporal prior and predicts only the \"residual\", i.e., the changes conditioned on the ego-vehicle's actions and scene context. To alleviate error accumulation over time, we further apply an alignment module to calibrate semantic and dynamic misalignments. Moreover, we investigate different forecasting-planning coupling schemes and demonstrate that the implicit future state generated by world models substantially improves planning accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D occupancy forecasting and trajectory planning.</article>","contentLength":1245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantifying Phonosemantic Iconicity Distributionally in 6 Languages","url":"https://arxiv.org/abs/2510.14040","date":1761796800,"author":"","guid":321172,"unread":true,"content":"<article>arXiv:2510.14040v2 Announce Type: replace \nAbstract: Language is, as commonly theorized, largely arbitrary. Yet, systematic relationships between phonetics and semantics have been observed in many specific cases. To what degree could those systematic relationships manifest themselves in large scale, quantitative investigations--both in previously identified and unidentified phenomena? This work undertakes a distributional approach to quantifying phonosemantic iconicity at scale across 6 diverse languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each language, we analyze the alignment of morphemes' phonetic and semantic similarity spaces with a suite of statistical measures, and discover an array of interpretable phonosemantic alignments not previously identified in the literature, along with crosslinguistic patterns. We also analyze 5 previously hypothesized phonosemantic alignments, finding support for some such alignments and mixed results for others.</article>","contentLength":985,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups","url":"https://arxiv.org/abs/2510.13852","date":1761796800,"author":"","guid":321173,"unread":true,"content":"<article>arXiv:2510.13852v2 Announce Type: replace \nAbstract: Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.</article>","contentLength":1609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation","url":"https://arxiv.org/abs/2510.13738","date":1761796800,"author":"","guid":321174,"unread":true,"content":"<article>arXiv:2510.13738v2 Announce Type: replace \nAbstract: Large language models (LLMs) have recently demonstrated strong potential for sequential recommendation. However, current LLM-based approaches face critical limitations in modeling users' long-term and diverse interests. First, due to inference latency and feature fetching bandwidth constraints, existing methods typically truncate user behavior sequences to include only the most recent interactions, resulting in the loss of valuable long-range preference signals. Second, most current methods rely on next-item prediction with a single predicted embedding, overlooking the multifaceted nature of user interests and limiting recommendation diversity. To address these challenges, we propose HyMiRec, a hybrid multi-interest sequential recommendation framework, which leverages a lightweight recommender to extracts coarse interest embeddings from long user sequences and an LLM-based recommender to captures refined interest embeddings. To alleviate the overhead of fetching features, we introduce a residual codebook based on cosine similarity, enabling efficient compression and reuse of user history embeddings. To model the diverse preferences of users, we design a disentangled multi-interest learning module, which leverages multiple interest queries to learn disentangles multiple interest signals adaptively, allowing the model to capture different facets of user intent. Extensive experiments are conducted on both benchmark datasets and a collected industrial dataset, demonstrating our effectiveness over existing state-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec brings consistent improvements in real-world recommendation systems. Code is available at https://github.com/FireRedTeam/FireRedSeqRec.</article>","contentLength":1792,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans","url":"https://arxiv.org/abs/2510.13587","date":1761796800,"author":"","guid":321175,"unread":true,"content":"<article>arXiv:2510.13587v2 Announce Type: replace \nAbstract: We present HRM$^2$Avatar, a framework for creating high-fidelity avatars from monocular phone scans, which can be rendered and animated in real time on mobile devices. Monocular capture with smartphones provides a low-cost alternative to studio-grade multi-camera rigs, making avatar digitization accessible to non-expert users. Reconstructing high-fidelity avatars from single-view video sequences poses challenges due to limited visual and geometric data. To address these limitations, at the data level, our method leverages two types of data captured with smartphones: static pose sequences for texture reconstruction and dynamic motion sequences for learning pose-dependent deformations and lighting changes. At the representation level, we employ a lightweight yet expressive representation to reconstruct high-fidelity digital humans from sparse monocular data. We extract garment meshes from monocular data to model clothing deformations effectively, and attach illumination-aware Gaussians to the mesh surface, enabling high-fidelity rendering and capturing pose-dependent lighting. This representation efficiently learns high-resolution and dynamic information from monocular data, enabling the creation of detailed avatars. At the rendering level, real-time performance is critical for animating high-fidelity avatars in AR/VR, social gaming, and on-device creation. Our GPU-driven rendering pipeline delivers 120 FPS on mobile devices and 90 FPS on standalone VR devices at 2K resolution, over $2.7\\times$ faster than representative mobile-engine baselines. Experiments show that HRM$^2$Avatar delivers superior visual realism and real-time interactivity, outperforming state-of-the-art monocular methods.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Trusted Service Monitoring: Verifiable Service Level Agreements","url":"https://arxiv.org/abs/2510.13370","date":1761796800,"author":"","guid":321176,"unread":true,"content":"<article>arXiv:2510.13370v2 Announce Type: replace \nAbstract: Service Level Agreement (SLA) monitoring in service-oriented environments suffers from inherent trust conflicts when providers self-report metrics, creating incentives to underreport violations. We introduce a framework for generating verifiable SLA violation claims through trusted hardware monitors and zero-knowledge proofs, establishing cryptographic foundations for genuine trustworthiness in service ecosystems. Our approach starts with machine-readable SLA clauses converted into verifiable predicates and monitored within Trusted Execution Environments. These monitors collect timestamped telemetry, organize measurements into Merkle trees, and produce signed attestations. Zero-knowledge proofs aggregate Service-Level Indicators to evaluate compliance, generating cryptographic proofs verifiable by stakeholders, arbitrators, or insurers in disputes, without accessing underlying data. This ensures three security properties: integrity, authenticity, and validity. Our prototype demonstrates linear scaling up to over 1 million events per hour for measurements with near constant-time proof generation and verification for single violation claims, enabling trustless SLA enforcement through cryptographic guarantees for automated compliance verification in service monitoring.</article>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation","url":"https://arxiv.org/abs/2510.12993","date":1761796800,"author":"","guid":321177,"unread":true,"content":"<article>arXiv:2510.12993v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) can generate human-like disinformation, yet their ability to personalise such content across languages and demographics remains underexplored. This study presents the first large-scale, multilingual analysis of persona-targeted disinformation generation by LLMs. Employing a red teaming methodology, we prompt eight state-of-the-art LLMs with 324 false narratives and 150 demographic personas (combinations of country, generation, and political orientation) across four languages--English, Russian, Portuguese, and Hindi--resulting in AI-TRAITS, a comprehensive dataset of 1.6 million personalised disinformation texts. Results show that the use of even simple personalisation prompts significantly increases the likelihood of jailbreaks across all studied LLMs, up to 10 percentage points, and alters linguistic and rhetorical patterns that enhance narrative persuasiveness. Models such as Grok and GPT exhibited jailbreak rates and personalisation scores both exceeding 85%. These insights expose critical vulnerabilities in current state-of-the-art LLMs and offer a foundation for improving safety alignment and detection strategies in multilingual and cross-demographic contexts.</article>","contentLength":1265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmenting Dialog with Think-Aloud Utterances for Modeling Individual Personality Traits by LLM","url":"https://arxiv.org/abs/2510.09158","date":1761796800,"author":"","guid":321178,"unread":true,"content":"<article>arXiv:2510.09158v2 Announce Type: replace \nAbstract: This study proposes augmenting dialog data with think-aloud utterances (TAUs) for modeling individual personalities in text chat by LLM. TAU is a verbalization of a speaker's thought before articulating the utterance. We expect \"persona LLMs\" trained with TAU-augmented data can mimic the speaker's personality trait better. We tested whether the trained persona LLMs obtain the human personality with respect to Big Five, a framework characterizing human personality traits from five aspects. The results showed that LLMs trained with TAU-augmented data more closely align to the speakers' Agreeableness and Neuroticism of Big Five than those trained with original dialog data. We also found that the quality of TAU-augmentation impacts persona LLM's performance.</article>","contentLength":817,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers","url":"https://arxiv.org/abs/2510.07316","date":1761796800,"author":"","guid":321179,"unread":true,"content":"<article>arXiv:2510.07316v2 Announce Type: replace \nAbstract: This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \\textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.</article>","contentLength":1309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can We Hide Machines in the Crowd? Quantifying Equivalence in LLM-in-the-loop Annotation Tasks","url":"https://arxiv.org/abs/2510.06658","date":1761796800,"author":"","guid":321180,"unread":true,"content":"<article>arXiv:2510.06658v2 Announce Type: replace \nAbstract: Many evaluations of large language models (LLMs) in text annotation focus primarily on the correctness of the output, typically comparing model-generated labels to human-annotated ``ground truth'' using standard performance metrics. In contrast, our study moves beyond effectiveness alone. We aim to explore how labeling decisions -- by both humans and LLMs -- can be statistically evaluated across individuals. Rather than treating LLMs purely as annotation systems, we approach LLMs as an alternative annotation mechanism that may be capable of mimicking the subjective judgments made by humans. To assess this, we develop a statistical evaluation method based on Krippendorff's $\\alpha$, paired bootstrapping, and the Two One-Sided t-Tests (TOST) equivalence test procedure. This evaluation method tests whether an LLM can blend into a group of human annotators without being distinguishable.\n  We apply this approach to two datasets -- MovieLens 100K and PolitiFact -- and find that the LLM is statistically indistinguishable from a human annotator in the former ($p = 0.004$), but not in the latter ($p = 0.155$), highlighting task-dependent differences. It also enables early evaluation on a small sample of human data to inform whether LLMs are suitable for large-scale annotation in a given application.</article>","contentLength":1364,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Operational Risks in Grid Integration of Large Data Center Loads: Characteristics, Stability Assessments, and Sensitivity Studies","url":"https://arxiv.org/abs/2510.05437","date":1761796800,"author":"","guid":321181,"unread":true,"content":"<article>arXiv:2510.05437v3 Announce Type: replace \nAbstract: This paper investigates the dynamic interactions between large-scale data centers and the power grid, focusing on reliability challenges arising from sudden fluctuations in demand. With the rapid growth of AI-driven workloads, such fluctuations, along with fast ramp patterns, are expected to exacerbate stressed grid conditions and system instabilities. We consider a few open-source AI data center consumption profiles from the MIT supercloud datasets, along with generating a few experimental HPC job-distribution-based inference profiles. Subsequently, we develop analytical methodologies for real-time assessment of grid stability, focusing on both transient and small-signal stability assessments. Energy-flow-like metrics for nonlinear transient stability, formulated by computing localized data center bus kinetic-like flows and coupling interactions with neighboring buses over varying time windows, help provide operators with real-time assessments of the regional grid stress in the data center hubs. On the other hand, small-signal stability metrics, constructed from analytical state matrices under variable operating conditions during a fast ramping period, enable snapshot-based assessments of data center load fluctuations and provide enhanced observability into evolving grid conditions. By quantifying the stability impacts of large data center clusters, studies conducted in the modified IEEE benchmark $68-$bus model support improved operator situational awareness to capture risks in reliable integration of large data center loads.</article>","contentLength":1606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models","url":"https://arxiv.org/abs/2510.05034","date":1761796800,"author":"","guid":321182,"unread":true,"content":"<article>arXiv:2510.05034v5 Announce Type: replace \nAbstract: Video understanding represents the most challenging frontier in computer vision, requiring models to reason about complex spatiotemporal relationships, long-term dependencies, and multimodal evidence. The recent emergence of Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders with powerful decoder-based language models, has demonstrated remarkable capabilities in video understanding tasks. However, the critical phase that transforms these models from basic perception systems into sophisticated reasoning engines, post-training, remains fragmented across the literature. This survey provides the first comprehensive examination of post-training methodologies for Video-LMMs, encompassing three fundamental pillars: supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL) from verifiable objectives, and test-time scaling (TTS) through enhanced inference computation. We present a structured taxonomy that clarifies the roles, interconnections, and video-specific adaptations of these techniques, addressing unique challenges such as temporal localization, spatiotemporal grounding, long video efficiency, and multimodal evidence integration. Through systematic analysis of representative methods, we synthesize key design principles, insights, and evaluation protocols while identifying critical open challenges in reward design, scalability, and cost-performance optimization. We further curate essential benchmarks, datasets, and metrics to facilitate rigorous assessment of post-training effectiveness. This survey aims to provide researchers and practitioners with a unified framework for advancing Video-LMM capabilities. Additional resources and updates are maintained at: https://github.com/yunlong10/Awesome-Video-LMM-Post-Training</article>","contentLength":1844,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method","url":"https://arxiv.org/abs/2510.04655","date":1761796800,"author":"","guid":321183,"unread":true,"content":"<article>arXiv:2510.04655v2 Announce Type: replace \nAbstract: Knowledge of the medical decision process, which can be modeled as medical decision trees (MDTs), is critical to building clinical decision support systems. However, current MDT construction methods rely heavily on time-consuming and laborious manual annotation. To address this challenge, we propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for automatically extracting MDTs from clinical guidelines and textbooks. We integrate gradient path information to capture synergistic effects between different modules, enabling more effective and reliable rank allocation. This framework ensures that the most critical modules receive appropriate rank allocations while less important ones are pruned, resulting in a more efficient and accurate model for extracting medical decision trees from clinical texts. Extensive experiments on medical guideline datasets demonstrate that our PI-LoRA method significantly outperforms existing parameter-efficient fine-tuning approaches for the Text2MDT task, achieving better accuracy with substantially reduced model complexity. The proposed method achieves state-of-the-art results while maintaining a lightweight architecture, making it particularly suitable for clinical decision support systems where computational resources may be limited.</article>","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation","url":"https://arxiv.org/abs/2510.03786","date":1761796800,"author":"","guid":321184,"unread":true,"content":"<article>arXiv:2510.03786v2 Announce Type: replace \nAbstract: In recent years, deep learning has shown near-expert performance in segmenting complex medical tissues and tumors. However, existing models are often task-specific, with performance varying across modalities and anatomical regions. Balancing model complexity and performance remains challenging, particularly in clinical settings where both accuracy and efficiency are critical. To address these issues, we propose a hybrid segmentation architecture featuring a three-branch encoder that integrates CNNs, Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture local, global, and long-range dependencies. A multi-scale attention-based CNN decoder reconstructs fine-grained segmentation maps while preserving contextual consistency. Additionally, a co-attention gate enhances feature selection by emphasizing relevant spatial and semantic information across scales during both encoding and decoding, improving feature interaction and cross-scale communication. Extensive experiments on multiple benchmark datasets show that our approach outperforms state-of-the-art methods in accuracy and generalization, while maintaining comparable computational complexity. By effectively balancing efficiency and effectiveness, our architecture offers a practical and scalable solution for diverse medical imaging tasks. Source code and trained models will be publicly released upon acceptance to support reproducibility and further research.</article>","contentLength":1502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Designing Walrus: Relational Programming with Rich Types, On-Demand Laziness, and Structured Traces","url":"https://arxiv.org/abs/2510.02579","date":1761796800,"author":"","guid":321185,"unread":true,"content":"<article>arXiv:2510.02579v2 Announce Type: replace \nAbstract: We present Walrus, a functional relational programming language embedded in Haskell that extends the miniKanren model with type-polymorphic unification, on-demand laziness, and a range of usability features aimed at practical development. These include use of Haskell Generics for boilerplate reduction, structured debugging traces, and ergonomic support for product types. We describe the design and implementation of Walrus through the lens of our experience developing bidirectional compilers, and reflect on key design decisions and recurring usability challenges encountered in practice.</article>","contentLength":645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System","url":"https://arxiv.org/abs/2510.01617","date":1761796800,"author":"","guid":321186,"unread":true,"content":"<article>arXiv:2510.01617v3 Announce Type: replace \nAbstract: Although large language models (LLMs) have revolutionized natural language processing capabilities, their practical implementation as autonomous multi-agent systems (MAS) for industrial problem-solving encounters persistent barriers. Conventional MAS architectures are fundamentally restricted by inflexible, hand-crafted graph topologies that lack contextual responsiveness, resulting in diminished efficacy across varied academic and commercial workloads. To surmount these constraints, we introduce AMAS, a paradigm-shifting framework that redefines LLM-based MAS through a novel dynamic graph designer. This component autonomously identifies task-specific optimal graph configurations via lightweight LLM adaptation, eliminating the reliance on monolithic, universally applied structural templates. Instead, AMAS exploits the intrinsic properties of individual inputs to intelligently direct query trajectories through task-optimized agent pathways. Rigorous validation across question answering, mathematical deduction, and code generation benchmarks confirms that AMAS systematically exceeds state-of-the-art single-agent and multi-agent approaches across diverse LLM architectures. Our investigation establishes that context-sensitive structural adaptability constitutes a foundational requirement for high-performance LLM MAS deployments.</article>","contentLength":1399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment","url":"https://arxiv.org/abs/2510.01216","date":1761796800,"author":"","guid":321187,"unread":true,"content":"<article>arXiv:2510.01216v2 Announce Type: replace \nAbstract: Users of blockchains value scalability, expecting fast confirmations and immediate transaction processing. Odontoceti, the latest in DAG-based consensus, addresses these concerns by prioritizing low latency and high throughput, making a strategic trade-off in security by operating with a 20% fault tolerance instead of the established 33% level. It is the first DAG-based protocol to achieve commitment in just two communication rounds, delivering median latency of 300 milliseconds while processing 10,000 transactions per second under realistic network conditions. Odontoceti operates with n = 5f + 1 validators and creates an uncertified DAG with a novel decision rule for committing blocks. The protocol includes an optimization that advances progress when participants are slow, benefiting crash fault scenarios which are more common in practice than Byzantine faults. Evaluation results demonstrate 20-25% latency improvements compared to an existing production protocol, validating that reducing wave length from three rounds to two rounds yields meaningful performance benefits. This paper establishes the practical viability of lower fault tolerance consensus protocols for blockchains.</article>","contentLength":1249,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards","url":"https://arxiv.org/abs/2510.00568","date":1761796800,"author":"","guid":321188,"unread":true,"content":"<article>arXiv:2510.00568v2 Announce Type: replace \nAbstract: Search agents powered by Large Language Models (LLMs) have demonstrated significant potential in tackling knowledge-intensive tasks. Reinforcement learning (RL) has emerged as a powerful paradigm for training these agents to perform complex, multi-step reasoning. However, prior RL-based methods often rely on sparse or rule-based rewards, which can lead agents to commit to suboptimal or erroneous reasoning paths without the ability to recover. To address these limitations, we propose ReSeek, a novel self-correcting framework for training search agents. Our framework introduces a self-correction mechanism that empowers the agent to dynamically identify and recover from erroneous search paths during an episode. By invoking a special JUDGE action, the agent can judge the information and re-plan its search strategy. To guide this process, we design a dense, instructive process reward function, which decomposes into a correctness reward for retrieving factual information and a utility reward for finding information genuinely useful for the query. Furthermore, to mitigate the risk of data contamination in existing datasets, we introduce FictionalHot, a new and challenging benchmark with recently curated questions requiring complex reasoning. Being intuitively reasonable and practically simple, extensive experiments show that agents trained with ReSeek significantly outperform SOTA baselines in task success rate and path faithfulness.</article>","contentLength":1503,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The CoCompiler: DSL Lifting via Relational Compilation","url":"https://arxiv.org/abs/2510.00210","date":1761796800,"author":"","guid":321189,"unread":true,"content":"<article>arXiv:2510.00210v3 Announce Type: replace \nAbstract: Lifting low-level or legacy code into a domain-specific language (DSL) improves our ability to understand it, enables deeper formal reasoning, and facilitates safe modification. We present the CoCompiler, a bidirectional compiler and lifter between C and Lustre, a synchronous dataflow language used for reactive systems. The key insight behind the CoCompiler is that writing a compiler as a relation, rather than as a traditional function, yields a DSL lifter \"for free\". We implement this idea by rewriting the verified Lustre-to-C compiler V\\'elus in the Walrus relational programming language. This solves what we call the vertical lifting problem, translating canonical C into Lustre. To address the complementary horizontal problem-handling real-world C outside the compiler's image-we apply semantic-preserving canonicalization passes in Haskell. The resulting tool, the CoCompiler, supports lifting real reactive C code into Lustre and onward into graphical behavioral models. Our approach is modular, language-agnostic, and fast to implement, demonstrating that relational programming offers a practical foundation for building DSL lifters by repurposing existing compilers.</article>","contentLength":1236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coding for Ordered Composite DNA Sequences","url":"https://arxiv.org/abs/2509.26119","date":1761796800,"author":"","guid":321190,"unread":true,"content":"<article>arXiv:2509.26119v2 Announce Type: replace \nAbstract: To increase the information capacity of DNA storage, composite DNA letters were introduced. We propose a channel model for composite DNA in which composite sequences are decomposed into ordered non-composite sequences. We study the problem of reconstructing composite binary sequences with substitution errors in this channel. We define two families of error-correcting codes and provide lower and upper bounds on their cardinality.</article>","contentLength":485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Embedding General Conservation Constraints in Discretizations of Hyperbolic Systems on Arbitrary Meshes: A Multidimensional Framework","url":"https://arxiv.org/abs/2509.25967","date":1761796800,"author":"","guid":321191,"unread":true,"content":"<article>arXiv:2509.25967v2 Announce Type: replace \nAbstract: The purpose of this review is to discuss the notion of conservation in hyperbolic systems and how one can formulate it at the discrete level depending on the solution representation of the solution. A general theory is difficult. We discuss several possibilities: if the solution is represented by average in volumes; if the mesh is staggerred; if the solution is solely represented by point values and an example where all the previous options are mixed.\n  We show how each configuration can provide, or not, enough flexibility. The discussion could be adapted to any hyperbolic system endowed with an entropy, but we focus on compressible fluid mechanics, in its Eulerian and Lagrangian formulations. The unifying element is that we systematically express the update of conserved variables as $u^{n+1}=u^n- \\Delta t\\; \\delta u$, where the functional $\\delta u$ depends on the value of $u$ in the stencil of the scheme. Then, one can naturally define a graph connecting the states defining $\\delta u$. The notion of local conservation can be defined from this graph. We are aware of only two possible situations: either the graph is constructed from the faces of the mesh elements (or the dual mesh), or it is defined from the mesh itself. Two notions of local conservation then emerge: either we define a numerical flux, or we define a \"residual\" attached to elements and the degrees of freedom within the element. We show that this two notions are in a way equivalent, but the one with residual allows much more flexibility, especially if additional algebraic constraints must be satisfied. Examples of specific additional conservation constraints are provided to illustrate this. We also show that this notion of conservation gives a very clear framework for the design of scheme in the Lagrangian framework. We end by providing a number of ongoing research questions, and highlight some open questions.</article>","contentLength":1960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FedCLF -- Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks","url":"https://arxiv.org/abs/2509.25233","date":1761796800,"author":"","guid":321192,"unread":true,"content":"<article>arXiv:2509.25233v2 Announce Type: replace \nAbstract: Federated Learning (FL) is a distributed machine learning technique that preserves data privacy by sharing only the trained parameters instead of the client data. This makes FL ideal for highly dynamic, heterogeneous, and time-critical applications, in particular, the Internet of Vehicles (IoV) networks. However, FL encounters considerable challenges in such networks owing to the high data and device heterogeneity. To address these challenges, we propose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which introduces calibrated loss as a utility in the participant selection process and a feedback control mechanism to dynamically adjust the sampling frequency of the clients. The envisaged approach (a) enhances the overall model accuracy in case of highly heterogeneous data and (b) optimizes the resource utilization for resource constrained IoV networks, thereby leading to increased efficiency in the FL process. We evaluated FedCLF vis-\\`a-vis baseline models, i.e., FedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity. Our results depict that FedCLF significantly outperforms the baseline models by up to a 16% improvement in high data heterogeneity-related scenarios with improved efficiency via reduced sampling frequency.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Road to the Closest Point is Paved by Good Neighbors","url":"https://arxiv.org/abs/2509.23966","date":1761796800,"author":"","guid":321193,"unread":true,"content":"<article>arXiv:2509.23966v2 Announce Type: replace \nAbstract: $\\renewcommand{\\Re}{\\mathbb{R}}$Given a set $P$ of $n$ points in $\\Re^d$, and a parameter $\\varepsilon \\in (0,1)$, we present a new construction of a directed graph $G$, of size $O(n/\\varepsilon^d)$, such that $(1+\\varepsilon)$-ANN queries can be answered by performing a greedy walk on $G$, repeatedly moving to a neighbor that is (significantly) better than the current point. To the best of our knowledge, this is the first construction of a linear size with no dependency on the spread of the point set. The resulting query time, is $O( \\varepsilon^{-d} \\log \\Psi)$, where $\\Psi$ is the spread of $P$. The new construction is surprisingly simple and should be practical.</article>","contentLength":727,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Mixing Additive Networks","url":"https://arxiv.org/abs/2509.23923","date":1761796800,"author":"","guid":321194,"unread":true,"content":"<article>arXiv:2509.23923v2 Announce Type: replace \nAbstract: We introduce GMAN, a flexible, interpretable, and expressive framework that extends Graph Neural Additive Networks (GNANs) to learn from sets of sparse time-series data. GMAN represents each time-dependent trajectory as a directed graph and applies an enriched, more expressive GNAN to each graph. It allows users to control the interpretability-expressivity trade-off by grouping features and graphs to encode priors, and it provides feature, node, and graph-level interpretability. On real-world datasets, including mortality prediction from blood tests and fake-news detection, GMAN outperforms strong non-interpretable black-box baselines while delivering actionable, domain-aligned explanations.</article>","contentLength":753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"p-less Sampling: A Robust Hyperparameter-Free Approach for LLM Decoding","url":"https://arxiv.org/abs/2509.23234","date":1761796800,"author":"","guid":321195,"unread":true,"content":"<article>arXiv:2509.23234v4 Announce Type: replace \nAbstract: Obtaining high-quality outputs from Large Language Models (LLMs) often depends upon the choice of a sampling-based decoding strategy to probabilistically choose the next token at each generation step. While a variety of such sampling methods have been proposed, their performance can be sensitive to the selection of hyperparameters which may require different settings depending upon the generation task and temperature configuration. In this work, we introduce $p$-less sampling: an information-theoretic approach to sampling which dynamically sets a truncation threshold at each decoding step based on the entire token probability distribution. Unlike existing methods, $p$-less sampling has no hyperparameters and consistently produces high-quality outputs as temperature increases. We provide theoretical perspectives on $p$-less sampling to ground our proposed method and conduct experiments to empirically validate its effectiveness across a range of math, logical reasoning, and creative writing tasks. Our results demonstrate how $p$-less sampling consistently outperforms existing sampling approaches while exhibiting much less degradation in text quality at higher temperature values. We further show how $p$-less achieves greater inference-time efficiency than alternative methods through lower average token sampling times and shorter generation lengths, without sacrificing accuracy. Finally, we provide analyses to highlight the benefits of $p$-less through qualitative examples, case studies, and diversity assessments. The code is available at https://github.com/ryttry/p-less .</article>","contentLength":1648,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Activation Matching for Explanation Generation","url":"https://arxiv.org/abs/2509.23051","date":1761796800,"author":"","guid":321196,"unread":true,"content":"<article>arXiv:2509.23051v2 Announce Type: replace \nAbstract: In this paper we introduce an activation-matching--based approach to generate minimal, faithful explanations for the decision-making of a pretrained classifier on any given image. Given an input image $x$ and a frozen model $f$, we train a lightweight autoencoder to output a binary mask $m$ such that the explanation $e = m \\odot x$ preserves both the model's prediction and the intermediate activations of \\(x\\). Our objective combines: (i) multi-layer activation matching with KL divergence to align distributions and cross-entropy to retain the top-1 label for both the image and the explanation; (ii) mask priors -- L1 area for minimality, a binarization penalty for crisp 0/1 masks, and total variation for compactness; and (iii) abductive constraints for faithfulness and necessity. Together, these objectives yield small, human-interpretable masks that retain classifier behavior while discarding irrelevant input regions, providing practical and faithful minimalist explanations for the decision making of the underlying model.</article>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation","url":"https://arxiv.org/abs/2509.22689","date":1761796800,"author":"","guid":321197,"unread":true,"content":"<article>arXiv:2509.22689v2 Announce Type: replace \nAbstract: Semi-supervised semantic segmentation (SSSS) is vital in computational pathology, where dense annotations are costly and limited. Existing methods often rely on pixel-level consistency, which propagates noisy pseudo-labels and produces fragmented or topologically invalid masks. We propose Topology Graph Consistency (TGC), a framework that integrates graph-theoretic constraints by aligning Laplacian spectra, component counts, and adjacency statistics between prediction graphs and references. This enforces global topology and improves segmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC achieves state-of-the-art performance under 5-10% supervision and significantly narrows the gap to full supervision.</article>","contentLength":781,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The AI_INFN Platform: Artificial Intelligence Development in the Cloud","url":"https://arxiv.org/abs/2509.22117","date":1761796800,"author":"","guid":321198,"unread":true,"content":"<article>arXiv:2509.22117v2 Announce Type: replace \nAbstract: Machine Learning (ML) is profoundly reshaping the way researchers create, implement, and operate data-intensive software. Its adoption, however, introduces notable challenges for computing infrastructures, particularly when it comes to coordinating access to hardware accelerators across development, testing, and production environments. The INFN initiative AI_INFN (Artificial Intelligence at INFN) seeks to promote the use of ML methods across various INFN research scenarios by offering comprehensive technical support, including access to AI-focused computational resources. Leveraging the INFN Cloud ecosystem and cloud-native technologies, the project emphasizes efficient sharing of accelerator hardware while maintaining the breadth of the Institute's research activities. This contribution describes the deployment and commissioning of a Kubernetes-based platform designed to simplify GPU-powered data analysis workflows and enable their scalable execution on heterogeneous distributed resources. By integrating offloading mechanisms through Virtual Kubelet and the InterLink API, the platform allows workflows to span multiple resource providers, from Worldwide LHC Computing Grid sites to high-performance computing centers like CINECA Leonardo. We will present preliminary benchmarks, functional tests, and case studies, demonstrating both performance and integration outcomes.</article>","contentLength":1443,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment","url":"https://arxiv.org/abs/2509.21609","date":1761796800,"author":"","guid":321199,"unread":true,"content":"<article>arXiv:2509.21609v3 Announce Type: replace \nAbstract: Immediate damage assessment is essential after natural catastrophes; yet, conventional hand evaluation techniques are sluggish and perilous. Although satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives of impacted regions, current computer vision methodologies generally yield just classification labels or segmentation masks, so constraining their capacity to deliver a thorough situational comprehension. We introduce the Vision Language Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive, contextually-informed explanations of disaster imagery. VLCE employs a dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset. Both systems utilize external semantic knowledge from ConceptNet and WordNet to expand vocabulary coverage and improve description accuracy. We assess VLCE in comparison to leading vision-language models (LLaVA and QwenVL) utilizing CLIPScore for semantic alignment and InfoMetIC for caption informativeness. Experimental findings indicate that VLCE markedly surpasses baseline models, attaining a maximum of 95.33% on InfoMetIC while preserving competitive semantic alignment. Our dual-architecture system demonstrates significant potential for improving disaster damage assessment by automating the production of actionable, information-dense descriptions from satellite and drone photos.</article>","contentLength":1584,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SciReasoner: Laying the Scientific Reasoning Ground Across Disciplines","url":"https://arxiv.org/abs/2509.21320","date":1761796800,"author":"","guid":321200,"unread":true,"content":"<article>arXiv:2509.21320v2 Announce Type: replace \nAbstract: We present a scientific reasoning foundation model that aligns natural language with heterogeneous scientific representations. The model is pretrained on a 206B-token corpus spanning scientific text, pure sequences, and sequence-text pairs, then aligned via SFT on 40M instructions, annealed cold-start bootstrapping to elicit long-form chain-of-thought, and reinforcement learning with task-specific reward shaping, which instills deliberate scientific reasoning. It supports four capability families, covering up to 103 tasks across workflows: (i) faithful translation between text and scientific formats, (ii) text/knowledge extraction, (iii) property prediction, (iv) property classification, (v) unconditional and conditional sequence generation and design. Compared with specialist systems, our approach broadens instruction coverage, improves cross-domain generalization, and enhances fidelity. We detail data curation and training and show that cross-discipline learning strengthens transfer and downstream reliability. The model, instruct tuning datasets and the evaluation code are open-sourced at https://huggingface.co/SciReason and https://github.com/open-sciencelab/SciReason.</article>","contentLength":1243,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction","url":"https://arxiv.org/abs/2509.19902","date":1761796800,"author":"","guid":321201,"unread":true,"content":"<article>arXiv:2509.19902v2 Announce Type: replace \nAbstract: In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at https://github.com/wenet-e2e/west/</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lift What You Can: Green Online Learning with Heterogeneous Ensembles","url":"https://arxiv.org/abs/2509.18962","date":1761796800,"author":"","guid":321202,"unread":true,"content":"<article>arXiv:2509.18962v2 Announce Type: replace \nAbstract: Ensemble methods for stream mining necessitate managing multiple models and updating them as data distributions evolve. Considering the calls for more sustainability, established methods are however not sufficiently considerate of ensemble members' computational expenses and instead overly focus on predictive capabilities. To address these challenges and enable green online learning, we propose heterogeneous online ensembles (HEROS). For every training step, HEROS chooses a subset of models from a pool of models initialized with diverse hyperparameter choices under resource constraints to train. We introduce a Markov decision process to theoretically capture the trade-offs between predictive performance and sustainability constraints. Based on this framework, we present different policies for choosing which models to train on incoming data. Most notably, we propose the novel $\\zeta$-policy, which focuses on training near-optimal models at reduced costs. Using a stochastic model, we theoretically prove that our $\\zeta$-policy achieves near optimal performance while using fewer resources compared to the best performing policy. In our experiments across 11 benchmark datasets, we find empiric evidence that our $\\zeta$-policy is a strong contribution to the state-of-the-art, demonstrating highly accurate performance, in some cases even outperforming competitors, and simultaneously being much more resource-friendly.</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GnnXemplar: Exemplars to Explanations -- Natural Language Rules for Global GNN Interpretability","url":"https://arxiv.org/abs/2509.18376","date":1761796800,"author":"","guid":321203,"unread":true,"content":"<article>arXiv:2509.18376v2 Announce Type: replace \nAbstract: Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods, those that characterize an entire class, remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space, exemplars, and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion","url":"https://arxiv.org/abs/2509.17302","date":1761796800,"author":"","guid":321204,"unread":true,"content":"<article>arXiv:2509.17302v2 Announce Type: replace \nAbstract: Text embedding inversion attacks reconstruct original sentences from latent representations, posing severe privacy threats in collaborative inference and edge computing. We propose TextCrafter, an optimization-based adversarial perturbation mechanism that combines RL learned, geometry aware noise injection orthogonal to user embeddings with cluster priors and PII signal guidance to suppress inversion while preserving task utility. Unlike prior defenses either non learnable or agnostic to perturbation direction, TextCrafter provides a directional protective policy that balances privacy and utility. Under strong privacy setting, TextCrafter maintains 70 percentage classification accuracy on four datasets and consistently outperforms Gaussian/LDP baselines across lower privacy budgets, demonstrating a superior privacy utility trade off.</article>","contentLength":898,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction","url":"https://arxiv.org/abs/2509.10516","date":1761796800,"author":"","guid":321205,"unread":true,"content":"<article>arXiv:2509.10516v2 Announce Type: replace \nAbstract: The increasing digitalization of education presents unprecedented opportunities for data-driven personalization, but it also introduces significant challenges to student data privacy. Conventional recommender systems rely on centralized data, a paradigm often incompatible with modern data protection regulations. A novel privacy-preserving recommender system is proposed and evaluated to address this critical issue using Federated Learning (FL). The approach utilizes a Deep Neural Network (DNN) with rich, engineered features from the large-scale ASSISTments educational dataset. A rigorous comparative analysis of federated aggregation strategies was conducted, identifying FedProx as a significantly more stable and effective method for handling heterogeneous student data than the standard FedAvg baseline. The optimized federated model achieves a high-performance F1-Score of 76.28%, corresponding to 92% of the performance of a powerful, centralized XGBoost model. These findings validate that a federated approach can provide highly effective content recommendations without centralizing sensitive student data. Consequently, our work presents a viable and robust solution to the personalization-privacy dilemma in modern educational platforms.</article>","contentLength":1306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SignMouth: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion","url":"https://arxiv.org/abs/2509.10266","date":1761796800,"author":"","guid":321206,"unread":true,"content":"<article>arXiv:2509.10266v2 Announce Type: replace \nAbstract: Sign language translation (SLT) aims to translate natural language from sign language videos, serving as a vital bridge for inclusive communication. While recent advances leverage powerful visual backbones and large language models, most approaches mainly focus on manual signals (hand gestures) and tend to overlook non-manual cues like mouthing. In fact, mouthing conveys essential linguistic information in sign languages and plays a crucial role in disambiguating visually similar signs. In this paper, we propose SignClip, a novel framework to improve the accuracy of sign language translation. It fuses manual and non-manual cues, specifically spatial gesture and lip movement features. Besides, SignClip introduces a hierarchical contrastive learning framework with multi-level alignment objectives, ensuring semantic consistency across sign-lip and visual-text modalities. Extensive experiments on two benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from 24.32 to 24.71, and ROUGE from 46.57 to 48.38.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards a Common Framework for Autoformalization","url":"https://arxiv.org/abs/2509.09810","date":1761796800,"author":"","guid":321207,"unread":true,"content":"<article>arXiv:2509.09810v2 Announce Type: replace \nAbstract: Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.</article>","contentLength":1248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles","url":"https://arxiv.org/abs/2509.09349","date":1761796800,"author":"","guid":321208,"unread":true,"content":"<article>arXiv:2509.09349v2 Announce Type: replace \nAbstract: Road traffic accidents remain a significant global concern, with human error, particularly distracted and impaired driving, among the leading causes. This study introduces a novel driver behaviour classification system that uses external observation techniques to detect indicators of distraction and impairment. The proposed framework employs advanced computer vision methodologies, including real-time object tracking, lateral displacement analysis, and lane position monitoring. The system identifies unsafe driving behaviours such as excessive lateral movement and erratic trajectory patterns by implementing the YOLO object detection model and custom lane estimation algorithms. Unlike systems reliant on inter-vehicular communication, this vision-based approach enables behavioural analysis of non-connected vehicles. Experimental evaluations on diverse video datasets demonstrate the framework's reliability and adaptability across varying road and environmental conditions.</article>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum Machine Learning and Grover's Algorithm for Quantum Optimization of Robotic Manipulators","url":"https://arxiv.org/abs/2509.07216","date":1761796800,"author":"","guid":321209,"unread":true,"content":"<article>arXiv:2509.07216v2 Announce Type: replace \nAbstract: Optimizing high-degree of freedom robotic manipulators requires searching complex, high-dimensional configuration spaces, a task that is computationally challenging for classical methods. This paper introduces a quantum native framework that integrates quantum machine learning with Grover's algorithm to solve kinematic optimization problems efficiently. A parameterized quantum circuit is trained to approximate the forward kinematics model, which then constructs an oracle to identify optimal configurations. Grover's algorithm leverages this oracle to provide a quadratic reduction in search complexity. Demonstrated on simulated 1-DoF, 2-DoF, and dual-arm manipulator tasks, the method achieves significant speedups-up to 93x over classical optimizers like Nelder Mead as problem dimensionality increases. This work establishes a foundational, quantum-native framework for robot kinematic optimization, effectively bridging quantum computing and robotics problems.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explainable Framework for Swarm Intelligence Based on Fitness Landscape Features and Machine Learning","url":"https://arxiv.org/abs/2509.06272","date":1761796800,"author":"","guid":321210,"unread":true,"content":"<article>arXiv:2509.06272v2 Announce Type: replace \nAbstract: Swarm based optimization algorithms have demonstrated remarkable success in solving complex optimization problems. However, their widespread adoption remains sceptical due to limited transparency in how different algorithmic components influence the overall performance of the algorithm. This work presents a multi-faceted interpretability related investigations of one of the popular swarm algorithms, Particle Swarm Optimization. Through this work, we provide a framework that makes the role of different topologies and parameters in PSO interpretable and explainable using novel machine learning approach. We first developed a comprehensive landscape characterization framework using Exploratory Landscape Analysis to quantify problem difficulty and identify critical features in the problem that affects the optimization performance of PSO. Secondly, we rigorously compare three topologies -- Ring, Star, and Von Neumann -- analyzing their distinct impacts on exploration-exploitation balance, convergence behavior, and solution quality and eventually develop an explainable benchmarking framework for PSO. The work successfully decodes how swarm topologies affect information flow, diversity, and convergence. Through systematic experimentation across 24 benchmark functions in multiple dimensions, we establish practical guidelines for topology selection and parameter configuration. These findings uncover the black-box nature of PSO, providing more transparency and interpretability to swarm intelligence systems. The source code is available at https://github.com/GitNitin02/ioh_pso.</article>","contentLength":1645,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation","url":"https://arxiv.org/abs/2509.05746","date":1761796800,"author":"","guid":321211,"unread":true,"content":"<article>arXiv:2509.05746v2 Announce Type: replace \nAbstract: Single image super-resolution traditionally assumes spatially-invariant degradation models, yet real-world imaging systems exhibit complex distance-dependent effects including atmospheric scattering, depth-of-field variations, and perspective distortions. This fundamental limitation necessitates spatially-adaptive reconstruction strategies that explicitly incorporate geometric scene understanding for optimal performance. We propose a rigorous variational framework that characterizes super-resolution as a spatially-varying inverse problem, formulating the degradation operator as a pseudodifferential operator with distance-dependent spectral characteristics that enable theoretical analysis of reconstruction limits across depth ranges. Our neural architecture implements discrete gradient flow dynamics through cascaded residual blocks with depth-conditional convolution kernels, ensuring convergence to stationary points of the theoretical energy functional while incorporating learned distance-adaptive regularization terms that dynamically adjust smoothness constraints based on local geometric structure. Spectral constraints derived from atmospheric scattering theory prevent bandwidth violations and noise amplification in far-field regions, while adaptive kernel generation networks learn continuous mappings from depth to reconstruction filters. Comprehensive evaluation across five benchmark datasets demonstrates state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by 0.44dB and 0.36dB respectively. This work establishes the first theoretically-grounded distance-adaptive super-resolution framework and demonstrates significant improvements on depth-variant scenarios while maintaining competitive performance across traditional benchmarks.</article>","contentLength":1913,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes","url":"https://arxiv.org/abs/2509.04317","date":1761796800,"author":"","guid":321212,"unread":true,"content":"<article>arXiv:2509.04317v2 Announce Type: replace \nAbstract: The AlphaZero framework provides a standard way of combining Monte Carlo planning with prior knowledge provided by a previously trained policy-value neural network. AlphaZero usually assumes that the environment on which the neural network was trained will not change at test time, which constrains its applicability. In this paper, we analyze the problem of deploying AlphaZero agents in potentially changed test environments and demonstrate how the combination of simple modifications to the standard framework can significantly boost performance, even in settings with a low planning budget available. The code is publicly available on GitHub.</article>","contentLength":699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","url":"https://arxiv.org/abs/2509.02547","date":1761796800,"author":"","guid":321213,"unread":true,"content":"<article>arXiv:2509.02547v2 Announce Type: replace \nAbstract: The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GradeSQL: Test-Time Inference with Outcome Reward Models for Text-to-SQL Generation from Large Language Models","url":"https://arxiv.org/abs/2509.01308","date":1761796800,"author":"","guid":321214,"unread":true,"content":"<article>arXiv:2509.01308v2 Announce Type: replace \nAbstract: Text-to-SQL, the task of translating natural language questions into SQL queries, has significantly advanced with the introduction of Large Language Models (LLMs), broadening database accessibility for a wide range of users. Despite substantial progress in generating valid SQL, current LLMs still struggle with complex queries. To address this limitation, test-time strategies such as Best-of-N (BoN) and Majority Voting (Maj) are often employed, based on the assumption that LLMs can produce correct answers after multiple attempts. However, these methods rely on surface-level heuristics, selecting the syntactically correct query through execution-based BoN (ex-BoN) or the most frequently generated one through Majority Voting. Recently, Outcome Reward Models (ORMs), which assign utility scores to generated outputs based on semantic correctness, have emerged as a promising reinforcement learning approach for improving model alignment. We argue that ORMs could serve as an effective new test-time heuristic, although their application in this context remains largely underexplored.\n  In this work, we propose a unified framework for training ORMs tailored to the Text-to-SQL task and assess their effectiveness as a test-time heuristic within the BoN strategy. We benchmark ORMs against ex-BoN and Maj across the BIRD and Spider datasets, fine-tuning diverse open-source LLMs from the Qwen2, Granite3, and Llama3 families. Results show that ORMs outperform ex-BoN and Maj, achieving execution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and +2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that finetuning models already aligned with SQL generation, such as OmniSQL, yields superior ORM performance. Additionally, we observe that ORMs achieve competitive results on simple queries and benefit more from an increased number of candidates compared to ex-BoN and Maj.</article>","contentLength":1964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation","url":"https://arxiv.org/abs/2509.01200","date":1761796800,"author":"","guid":321215,"unread":true,"content":"<article>arXiv:2509.01200v2 Announce Type: replace \nAbstract: Simultaneous Speech Translation (SimulST) enables real-time cross-lingual communication by jointly optimizing speech recognition and machine translation under strict latency constraints. Existing systems struggle to balance translation quality, latency, and semantic coherence, particularly in multilingual many-to-many scenarios where divergent read and write policies hinder unified strategy learning. In this paper, we present SimulMEGA (Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy learning framework that combines prefix-based training with a Mixture-of-Experts refiner to learn effective read and write decisions in an implicit manner, without adding inference-time overhead. Our design requires only minimal modifications to standard transformer architectures and generalizes across both speech-to-text and text-to-speech streaming tasks. Through comprehensive evaluation on six language pairs, our 500M parameter speech-to-text model outperforms the Seamless baseline, achieving under 7 percent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3 seconds. We further demonstrate the versatility of SimulMEGA by extending it to streaming TTS with a unidirectional backbone, yielding superior latency quality tradeoffs.</article>","contentLength":1328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI","url":"https://arxiv.org/abs/2509.00398","date":1761796800,"author":"","guid":321216,"unread":true,"content":"<article>arXiv:2509.00398v4 Announce Type: replace \nAbstract: This study provides an in_depth analysis of the ethical and trustworthiness challenges emerging alongside the rapid advancement of generative artificial intelligence (AI) technologies and proposes a comprehensive framework for their systematic evaluation. While generative AI, such as ChatGPT, demonstrates remarkable innovative potential, it simultaneously raises ethical and social concerns, including bias, harmfulness, copyright infringement, privacy violations, and hallucination. Current AI evaluation methodologies, which mainly focus on performance and accuracy, are insufficient to address these multifaceted issues. Thus, this study emphasizes the need for new human_centered criteria that also reflect social impact. To this end, it identifies key dimensions for evaluating the ethics and trustworthiness of generative AI_fairness, transparency, accountability, safety, privacy, accuracy, consistency, robustness, explainability, copyright and intellectual property protection, and source traceability and develops detailed indicators and assessment methodologies for each. Moreover, it provides a comparative analysis of AI ethics policies and guidelines in South Korea, the United States, the European Union, and China, deriving key approaches and implications from each. The proposed framework applies across the AI lifecycle and integrates technical assessments with multidisciplinary perspectives, thereby offering practical means to identify and manage ethical risks in real_world contexts. Ultimately, the study establishes an academic foundation for the responsible advancement of generative AI and delivers actionable insights for policymakers, developers, users, and other stakeholders, supporting the positive societal contributions of AI technologies.</article>","contentLength":1827,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism","url":"https://arxiv.org/abs/2508.15030","date":1761796800,"author":"","guid":321217,"unread":true,"content":"<article>arXiv:2508.15030v2 Announce Type: replace \nAbstract: We propose Collab-REC, a multi-agent framework designed to counteract popularity bias and enhance diversity in tourism recommendations. In our setting, three LLM-based agents -- Personalization, Popularity, and Sustainability generate city suggestions from complementary perspectives. A non-LLM moderator then merges and refines these proposals via multi-round negotiation, ensuring each agent's viewpoint is incorporated while penalizing spurious or repeated responses. Experiments on European city queries show that Collab-REC improves diversity and overall relevance compared to a single-agent baseline, surfacing lesser-visited locales that often remain overlooked. This balanced, context-aware approach addresses over-tourism and better aligns with constraints provided by the user, highlighting the promise of multi-stakeholder collaboration in LLM-driven recommender systems.</article>","contentLength":935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes","url":"https://arxiv.org/abs/2508.12015","date":1761796800,"author":"","guid":321218,"unread":true,"content":"<article>arXiv:2508.12015v2 Announce Type: replace \nAbstract: Reconstructing dynamic driving scenes from dashcam videos has attracted increasing attention due to its significance in autonomous driving and scene understanding. While recent advances have made impressive progress, most methods still unify all background elements into a single representation, hindering both instance-level understanding and flexible scene editing. Some approaches attempt to lift 2D segmentation into 3D space, but often rely on pre-processed instance IDs or complex pipelines to map continuous features to discrete identities. Moreover, these methods are typically designed for indoor scenes with rich viewpoints, making them less applicable to outdoor driving scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian Splatting framework tailored for the interactive reconstruction of dynamic driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D feature learning via contrastive loss and pseudo-supervised objectives. At the 3D level, we introduce regularization to implicitly encode instance identities and enforce consistency through a voxel-based loss. A lightweight static codebook further bridges continuous features and discrete identities without requiring data pre-processing or complex optimization. Quantitative and qualitative experiments demonstrate the effectiveness of InstDrive, and to the best of our knowledge, it is the first framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.More visualizations are available at our project page.</article>","contentLength":1604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UGM2N: An Unsupervised and Generalizable Mesh Movement Network via M-Uniform Loss","url":"https://arxiv.org/abs/2508.08615","date":1761796800,"author":"","guid":321219,"unread":true,"content":"<article>arXiv:2508.08615v2 Announce Type: replace \nAbstract: Partial differential equations (PDEs) form the mathematical foundation for modeling physical systems in science and engineering, where numerical solutions demand rigorous accuracy-efficiency tradeoffs. Mesh movement techniques address this challenge by dynamically relocating mesh nodes to rapidly-varying regions, enhancing both simulation accuracy and computational efficiency. However, traditional approaches suffer from high computational complexity and geometric inflexibility, limiting their applicability, and existing supervised learning-based approaches face challenges in zero-shot generalization across diverse PDEs and mesh topologies.In this paper, we present an Unsupervised and Generalizable Mesh Movement Network (UGM2N). We first introduce unsupervised mesh adaptation through localized geometric feature learning, eliminating the dependency on pre-adapted meshes. We then develop a physics-constrained loss function, M-Uniform loss, that enforces mesh equidistribution at the nodal level.Experimental results demonstrate that the proposed network exhibits equation-agnostic generalization and geometric independence in efficient mesh adaptation. It demonstrates consistent superiority over existing methods, including robust performance across diverse PDEs and mesh geometries, scalability to multi-scale resolutions and guaranteed error reduction without mesh tangling.</article>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diverse Teaching and Label Propagation for Generic Semi-Supervised Medical Image Segmentation","url":"https://arxiv.org/abs/2508.08549","date":1761796800,"author":"","guid":321220,"unread":true,"content":"<article>arXiv:2508.08549v3 Announce Type: replace \nAbstract: Both limited annotation and domain shift are significant challenges frequently encountered in medical image segmentation, leading to derivative scenarios like semi-supervised medical (SSMIS), semi-supervised medical domain generalization (Semi-MDG) and unsupervised medical domain adaptation (UMDA). Conventional methods are generally tailored to specific tasks in isolation, the error accumulation hinders the effective utilization of unlabeled data and limits further improvements, resulting in suboptimal performance when these issues occur. In this paper, we aim to develop a generic framework that masters all three tasks. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data and increasing the diversity of the model. To tackle this issue, we employ a Diverse Teaching and Label Propagation Network (DTLP-Net) to boosting the Generic Semi-Supervised Medical Image Segmentation. Our DTLP-Net involves a single student model and two diverse teacher models, which can generate reliable pseudo-labels for the student model. The first teacher model decouple the training process with labeled and unlabeled data, The second teacher is momentum-updated periodically, thus generating reliable yet divers pseudo-labels. To fully utilize the information within the data, we adopt inter-sample and intra-sample data augmentation to learn the global and local knowledge. In addition, to further capture the voxel-level correlations, we propose label propagation to enhance the model robust. We evaluate our proposed framework on five benchmark datasets for SSMIS, UMDA, and Semi-MDG tasks. The results showcase notable improvements compared to state-of-the-art methods across all five settings, indicating the potential of our framework to tackle more challenging SSL scenarios.</article>","contentLength":1937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning","url":"https://arxiv.org/abs/2508.07382","date":1761796800,"author":"","guid":321221,"unread":true,"content":"<article>arXiv:2508.07382v2 Announce Type: replace \nAbstract: Automating penetration testing is crucial for enhancing cybersecurity, yet current Large Language Models (LLMs) face significant limitations in this domain, including poor error handling, inefficient reasoning, and an inability to perform complex end-to-end tasks autonomously. To address these challenges, we introduce Pentest-R1, a novel framework designed to optimize LLM reasoning capabilities for this task through a two-stage reinforcement learning pipeline. We first construct a dataset of over 500 real-world, multi-step walkthroughs, which Pentest-R1 leverages for offline reinforcement learning (RL) to instill foundational attack logic. Subsequently, the LLM is fine-tuned via online RL in an interactive Capture The Flag (CTF) environment, where it learns directly from environmental feedback to develop robust error self-correction and adaptive strategies. Our extensive experiments on the Cybench and AutoPenBench benchmarks demonstrate the framework's effectiveness. On AutoPenBench, Pentest-R1 achieves a 24.2\\% success rate, surpassing most state-of-the-art models and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a 15.0\\% success rate in unguided tasks, establishing a new state-of-the-art for open-source LLMs and matching the performance of top proprietary models. Ablation studies confirm that the synergy of both training stages is critical to its success.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DSNS: The Deep Space Network Simulator","url":"https://arxiv.org/abs/2508.04317","date":1761796800,"author":"","guid":321222,"unread":true,"content":"<article>arXiv:2508.04317v3 Announce Type: replace \nAbstract: Simulation tools are commonly used in the development and testing of new protocols or new networks. However, as satellite networks start to grow to encompass thousands of nodes, and as companies and space agencies begin to realize the interplanetary internet, existing satellite and network simulation tools have become impractical for use in this context.\n  We therefore present the Deep Space Network Simulator (DSNS): a new network simulator with a focus on large-scale satellite networks. We demonstrate its improved capabilities compared to existing offerings, showcase its flexibility and extensibility through an implementation of existing protocols and the DTN simulation reference scenarios recommended by CCSDS, and evaluate its scalability, showing that it exceeds existing tools while providing better fidelity.\n  DSNS provides concrete usefulness to both standards bodies and satellite operators, enabling fast iteration on protocol development and testing of parameters under highly realistic conditions. By removing roadblocks to research and innovation, we can accelerate the development of upcoming satellite networks and ensure that their communication is both fast and secure.</article>","contentLength":1248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph","url":"https://arxiv.org/abs/2507.22811","date":1761796800,"author":"","guid":321223,"unread":true,"content":"<article>arXiv:2507.22811v2 Announce Type: replace \nAbstract: In this work we present an entity linker for DBLP's 2025 version of RDF-based Knowledge Graph. Compared to the 2022 version, DBLP now considers publication venues as a new entity type called dblp:Stream. In the earlier version of DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce entity linkings. In contrast, in this work, we develop a zero-shot entity linker using LLMs using a novel method, where we re-rank candidate entities based on the log-probabilities of the \"yes\" token output at the penultimate layer of the LLM.</article>","contentLength":600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RobEthiChor: Automated Context-aware Ethics-based Negotiation for Autonomous Robots","url":"https://arxiv.org/abs/2507.22664","date":1761796800,"author":"","guid":321224,"unread":true,"content":"<article>arXiv:2507.22664v2 Announce Type: replace \nAbstract: The presence of autonomous systems is growing at a fast pace and it is impacting many aspects of our lives. Designed to learn and act independently, these systems operate and perform decision-making without human intervention. However, they lack the ability to incorporate users' ethical preferences, which are unique for each individual in society and are required to personalize the decision-making processes. This reduces user trust and prevents autonomous systems from behaving according to the moral beliefs of their end-users. When multiple systems interact with differing ethical preferences, they must negotiate to reach an agreement that satisfies the ethical beliefs of all the parties involved and adjust their behavior consequently. To address this challenge, this paper proposes RobEthiChor, an approach that enables autonomous systems to incorporate user ethical preferences and contextual factors into their decision-making through ethics-based negotiation. RobEthiChor features a domain-agnostic reference architecture for designing autonomous systems capable of ethic-based negotiating. The paper also presents RobEthiChor-Ros, an implementation of RobEthiChor within the Robot Operating System (ROS), which can be deployed on robots to provide them with ethics-based negotiation capabilities. To evaluate our approach, we deployed RobEthiChor-Ros on real robots and ran scenarios where a pair of robots negotiate upon resource contention. Experimental results demonstrate the feasibility and effectiveness of the system in realizing ethics-based negotiation. RobEthiChor allowed robots to reach an agreement in more than 73% of the scenarios with an acceptable negotiation time (0.67s on average). Experiments also demonstrate that the negotiation approach implemented in RobEthiChor is scalable.</article>","contentLength":1867,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When Truthful Representations Flip Under Deceptive Instructions?","url":"https://arxiv.org/abs/2507.22149","date":1761796800,"author":"","guid":321225,"unread":true,"content":"<article>arXiv:2507.22149v4 Announce Type: replace \nAbstract: Large language models (LLMs) tend to follow maliciously crafted instructions to generate deceptive responses, posing safety challenges. How deceptive instructions alter the internal representations of LLM compared to truthful ones remains poorly understood beyond output analysis. To bridge this gap, we investigate when and how these representations ``flip'', such as from truthful to deceptive, under deceptive versus truthful/neutral instructions. Analyzing the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct on a factual verification task, we find the model's instructed True/False output is predictable via linear probes across all conditions based on the internal representation. Further, we use Sparse Autoencoders (SAEs) to show that the Deceptive instructions induce significant representational shifts compared to Truthful/Neutral representations (which are similar), concentrated in early-to-mid layers and detectable even on complex datasets. We also identify specific SAE features highly sensitive to deceptive instruction and use targeted visualizations to confirm distinct truthful/deceptive representational subspaces. % Our analysis pinpoints layer-wise and feature-level correlates of instructed dishonesty, offering insights for LLM detection and control. Our findings expose feature- and layer-level signatures of deception, offering new insights for detecting and mitigating instructed dishonesty in LLMs.</article>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Predicting Abandonment of Open Source Software Projects with An Integrated Feature Framework","url":"https://arxiv.org/abs/2507.21678","date":1761796800,"author":"","guid":321226,"unread":true,"content":"<article>arXiv:2507.21678v2 Announce Type: replace \nAbstract: Open Source Software (OSS) is a cornerstone of contemporary software development, yet the increasing prevalence of OSS project abandonment threatens global software supply chains. Although previous research has explored abandonment prediction methods, these methods often demonstrate unsatisfactory predictive performance, further plagued by imprecise abandonment discrimination, limited interpretability, and a lack of large, generalizable datasets. In this work, we address these challenges by reliably detecting OSS project abandonment through a dual approach: explicit archival status and rigorous semantic analysis of project documentation or description. Leveraging a precise and scalable labeling pipeline, we curate a comprehensive longitudinal dataset of 115,466 GitHub repositories, encompassing 57,733 confirmed abandonment repositories, enriched with detailed, timeline-based behavioral features. Building on this foundation, we introduce an integrated, multi-perspective feature framework for abandonment prediction, capturing user-centric, maintainer-centric, and project evolution features. Survival analysis using an AFT model yields a high C-index of 0.846, substantially outperforming models confined to surface features. Further, feature ablation and SHAP analyses confirm both the predictive power and interpretability of our approach. We further demonstrate practical deployment of a GBSA classifier for package risk in openEuler. By unifying precise labeling, multi-perspective features, and interpretable modeling, our work provides reproducible, scalable, and practitioner-oriented support for understanding and managing abandonment risk in large OSS ecosystems. Our tool not only predicts abandonment but also enhances program comprehension by providing actionable insights into the health and sustainability of OSS projects.</article>","contentLength":1903,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InsurTech innovation using natural language processing","url":"https://arxiv.org/abs/2507.21112","date":1761796800,"author":"","guid":321227,"unread":true,"content":"<article>arXiv:2507.21112v2 Announce Type: replace \nAbstract: With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations, focusing on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate feature de-biasing, feature compression, and industry classification in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classification techniques. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element of modern, data-driven insurance analytics.</article>","contentLength":1206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Price equation reveals a universal force-metric-bias law of algorithmic learning and natural selection","url":"https://arxiv.org/abs/2507.18549","date":1761796800,"author":"","guid":321228,"unread":true,"content":"<article>arXiv:2507.18549v3 Announce Type: replace \nAbstract: Diverse learning algorithms, optimization methods, and natural selection share a common mathematical structure, despite their apparent differences. Here I show that a simple notational partitioning of change by the Price equation reveals a universal force-metric-bias (FMB) law: $\\Delta\\mathbf{\\theta} = \\mathbf{M}\\,\\mathbf{f} + \\mathbf{b} + \\mathbf{\\xi}$. The force $\\mathbf{f}$ drives improvement in parameters, $\\Delta\\mathbf{\\theta}$, in proportion to the slope of performance with respect to the parameters. The metric $\\mathbf{M}$ rescales movement by inverse curvature. The bias $\\mathbf{b}$ adds momentum or changes in the frame of reference. The noise $\\mathbf{\\xi}$ enables exploration. This framework unifies natural selection, Bayesian updating, Newton's method, stochastic gradient descent, stochastic Langevin dynamics, Adam optimization, and most other algorithms as special cases of the same underlying process. The Price equation also reveals why Fisher information, Kullback-Leibler divergence, and d'Alembert's principle arise naturally in learning dynamics. By exposing this common structure, the FMB law provides a principled foundation for understanding, comparing, and designing learning algorithms across disciplines.</article>","contentLength":1294,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation","url":"https://arxiv.org/abs/2507.17937","date":1761796800,"author":"","guid":321229,"unread":true,"content":"<article>arXiv:2507.17937v3 Announce Type: replace \nAbstract: Generative AI systems for music and video commonly use text-based filters to prevent the regurgitation of copyrighted material. We expose a fundamental flaw in this approach by introducing Adversarial PhoneTic Prompting (APT), a novel attack that bypasses these safeguards by exploiting phonetic memorization. The APT attack replaces iconic lyrics with homophonic but semantically unrelated alternatives (e.g., \"mom's spaghetti\" becomes \"Bob's confetti\"), preserving acoustic structure while altering meaning; we identify high-fidelity phonetic matches using CMU pronouncing dictionary. We demonstrate that leading Lyrics-to-Song (L2S) models like SUNO and YuE regenerate songs with striking melodic and rhythmic similarity to their copyrighted originals when prompted with these altered lyrics. More surprisingly, this vulnerability extends across modalities. When prompted with phonetically modified lyrics from a song, a Text-to-Video (T2V) model like Veo 3 reconstructs visual scenes from the original music video-including specific settings and character archetypes-despite the absence of any visual cues in the prompt. Our findings reveal that models memorize deep, structural patterns tied to acoustics, not just verbatim text. This phonetic-to-visual leakage represents a critical vulnerability in transcript-conditioned generative models, rendering simple copyright filters ineffective and raising urgent concerns about the secure deployment of multimodal AI systems. Demo examples are available at our project page (https://jrohsc.github.io/music_attack/).</article>","contentLength":1619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Application of Whisper in Clinical Practice: the Post-Stroke Speech Assessment during a Naming Task","url":"https://arxiv.org/abs/2507.17326","date":1761796800,"author":"","guid":321230,"unread":true,"content":"<article>arXiv:2507.17326v2 Announce Type: replace \nAbstract: Detailed assessment of language impairment following stroke remains a cognitively complex and clinician-intensive task, limiting timely and scalable diagnosis. Automatic Speech Recognition (ASR) foundation models offer a promising pathway to augment human evaluation through intelligent systems, but their effectiveness in the context of speech and language impairment remains uncertain. In this study, we evaluate whether Whisper, a state-of-the-art ASR foundation model, can be applied to transcribe and analyze speech from patients with stroke during a commonly used picture-naming task. We assess both verbatim transcription accuracy and the model's ability to support downstream prediction of language function, which has major implications for outcomes after stroke. Our results show that the baseline Whisper model performs poorly on single-word speech utterances. Nevertheless, fine-tuning Whisper significantly improves transcription accuracy (reducing Word Error Rate by 87.72% in healthy speech and 71.22% in speech from patients). Further, learned representations from the model enable accurate prediction of speech quality (average F1 Macro of 0.74 for healthy, 0.75 for patients). However, evaluations on an unseen (TORGO) dataset reveal limited generalizability, highlighting the inability of Whisper to perform zero-shot transcription of single-word utterances on out-of-domain clinical speech and emphasizing the need to adapt models to specific clinical populations. While challenges remain in cross-domain generalization, these findings highlight the potential of foundation models, when appropriately fine-tuned, to advance automated speech and language assessment and rehabilitation for stroke-related impairments.</article>","contentLength":1788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics","url":"https://arxiv.org/abs/2507.15518","date":1761796800,"author":"","guid":321231,"unread":true,"content":"<article>arXiv:2507.15518v2 Announce Type: replace \nAbstract: Creating an immersive and interactive theatrical experience is a long-term goal in the field of interactive narrative. The emergence of large language model (LLM) is providing a new path to achieve this goal. However, existing LLM-based drama generation methods often result in agents that lack initiative and cannot interact with the physical scene. Furthermore, these methods typically require detailed user input to drive the drama. These limitations reduce the interactivity and immersion of online real-time performance. To address the above challenges, we propose HAMLET, a multi-agent framework focused on drama creation and online performance. Given a simple topic, the framework generates a narrative blueprint, guiding the subsequent improvisational performance. During the online performance, each actor is given an autonomous mind. This means that actors can make independent decisions based on their own background, goals, and emotional state. In addition to conversations with other actors, their decisions can also change the state of scene props through actions such as opening a letter or picking up a weapon. The change is then broadcast to other related actors, updating what they know and care about, which in turn influences their next action. To evaluate the quality of drama performance generated by HAMLET, we designed an evaluation method to assess three primary aspects, including character performance, narrative quality, and interaction experience. The experimental evaluation shows that HAMLET can create expressive and coherent theatrical experiences.</article>","contentLength":1634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs","url":"https://arxiv.org/abs/2507.14785","date":1761796800,"author":"","guid":321232,"unread":true,"content":"<article>arXiv:2507.14785v2 Announce Type: replace \nAbstract: The complexity and interconnectivity of entities involved in money laundering demand investigative reasoning over graph-structured data. This paper explores the use of large language models (LLMs) as reasoning engines over localized subgraphs extracted from a financial knowledge graph. We propose a lightweight pipeline that retrieves k-hop neighborhoods around entities of interest, serializes them into structured text, and prompts an LLM via few-shot in-context learning to assess suspiciousness and generate justifications. Using synthetic anti-money laundering (AML) scenarios that reflect common laundering behaviors, we show that LLMs can emulate analyst-style logic, highlight red flags, and provide coherent explanations. While this study is exploratory, it illustrates the potential of LLM-based graph reasoning in AML and lays groundwork for explainable, language-driven financial crime analytics.</article>","contentLength":962,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting","url":"https://arxiv.org/abs/2507.14109","date":1761796800,"author":"","guid":321233,"unread":true,"content":"<article>arXiv:2507.14109v2 Announce Type: replace \nAbstract: Radio frequency (RF) fingerprinting, which extracts unique hardware imperfections of radio devices, has emerged as a promising physical-layer device identification mechanism in zero trust architectures and beyond 5G networks. In particular, deep learning (DL) methods have demonstrated state-of-the-art performance in this domain. However, existing approaches have primarily focused on enhancing system robustness against temporal and spatial variations in wireless environments, while the security vulnerabilities of these DL-based approaches have often been overlooked. In this work, we systematically investigate the security risks of DL-based RF fingerprinting systems through an adversarial-driven experimental analysis. We observe a consistent misclassification behavior for DL models under domain shifts, where a device is frequently misclassified as another specific one. Our analysis based on extensive real-world experiments demonstrates that this behavior can be exploited as an effective backdoor to enable external attackers to intrude into the system. Furthermore, we show that training DL models on raw received signals causes the models to entangle RF fingerprints with environmental and signal-pattern features, creating additional attack vectors that cannot be mitigated solely through post-processing security methods such as confidence thresholds.</article>","contentLength":1420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Control Modes of Teleoperated Surgical Robotic System's Tools in Ophthalmic Surgery","url":"https://arxiv.org/abs/2507.13654","date":1761796800,"author":"","guid":321234,"unread":true,"content":"<article>arXiv:2507.13654v2 Announce Type: replace \nAbstract: The introduction of a teleoperated surgical robotic system designed for minimally invasive procedures enables the emulation of two distinct control modes through a dedicated input device of the surgical console: (1) Inside Control Mode, which emulates tool manipulation near the distal end as if the surgeon was holding the tip of the instrument inside the patient's body; (2) Outside Control Mode, which emulates manipulation near the proximal end as if the surgeon was holding the tool externally. The aim of this research is to compare the surgeon's performance on these two modes of operation along with various scaling factors in a simulated vitreoretinal surgical setting. The console of Intraocular Robotic Interventional Surgical System (IRISS) was utilized but the surgical robot itself and the human eye anatomy was simulated by a virtual environment projected microscope view of an intraocular setup to a VR headset. Five experienced vitreoretinal surgeons and five subjects with no surgical experience used the system to perform four fundamental tool/tissue tasks common to vitreoretinal surgery: touch and reset; grasp and drop; inject; circular tracking. Results indicate that Inside Control outperforms Outside Control across multiple tasks and metrics. Higher scaling factors generally performed better, particularly for reducing trajectory errors and tissue damage. This improvement suggests that larger scaling factors enable more precise control, making them the preferred option for fine manipulation. However, completion time was not consistently reduced across all conditions, indicating that surgeons need to balance speed and accuracy based on surgical requirements. By optimizing control dynamics and user interface, robotic teleoperation has the potential to reduce complications, enhance dexterity, and expand the accessibility of high precision procedures to a broader range of practitioners.</article>","contentLength":1973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks","url":"https://arxiv.org/abs/2507.10961","date":1761796800,"author":"","guid":321235,"unread":true,"content":"<article>arXiv:2507.10961v2 Announce Type: replace \nAbstract: This paper presents a framework for learning vision-based robotic policies for contact-rich manipulation tasks that generalize spatially across task configurations. We focus on achieving robust spatial generalization of the policy for the peg-in-hole (PiH) task trained from a small number of demonstrations. We propose EquiContact, a hierarchical policy composed of a high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF) and a novel low-level compliant visuomotor policy (Geometric Compliant ACT, G-CompACT). G-CompACT operates using only localized observations (geometrically consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB images) and produces actions defined in the end-effector frame. Through these design choices, we show that the entire EquiContact pipeline is SE(3)-equivariant, from perception to force control. We also outline three key components for spatially generalizable contact-rich policies: compliance, localized policies, and induced equivariance. Real-world experiments on PiH, screwing, and surface wiping tasks demonstrate a near-perfect success rate and robust generalization to unseen spatial configurations, validating the proposed framework and principles. The experimental videos can be found on the project website: https://sites.google.com/berkeley.edu/equicontact</article>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Qualitative Analysis of the Teacher and Student Roles in Pair Programming","url":"https://arxiv.org/abs/2507.10305","date":1761796800,"author":"","guid":321236,"unread":true,"content":"<article>arXiv:2507.10305v2 Announce Type: replace \nAbstract: Background: Pair programming is a well-established and versatile agile practice. Previous research has found it to involve far more different roles than the well-known Driver and Observer/Navigator roles. Pair programming often involves heavy knowledge transfer from mainly one partner to the other. Objective: Understand how to fill the ensuing Teacher and Student roles well (positive behavioral patterns). Understand how they may break (anti-patterns). Method: Open coding and axial coding of 17 recorded pair programming sessions with 18 developers from 5 German software companies, plus interviews with 6 different developers from 4 other German companies. Results: We describe six facets of effective Teacher behavior (e.g. Prioritizing Knowledge Transfer) and two facets of effective Student behavior (e.g. Expressing Knowledge Wants). We describe four harmful would-be-Teacher behaviors (e.g. Pushing Unwanted Knowledge), and one harmful would-be-Student behavior (Failing to Provide a Back Channel). Conclusions: The role facets can serve as learning goals and to-do list for developers who want to develop strong pair programming skill. The anti-patterns can serve as warnings for one's own general behavior and as triggers for immediate meta-discussion if they occur within a pairing session.</article>","contentLength":1356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differential Mamba","url":"https://arxiv.org/abs/2507.06204","date":1761796800,"author":"","guid":321237,"unread":true,"content":"<article>arXiv:2507.06204v2 Announce Type: replace \nAbstract: Sequence models like Transformers and RNNs often overallocate attention to irrelevant context, leading to noisy intermediate representations. This degrades LLM capabilities by promoting hallucinations, weakening long-range and retrieval abilities, and reducing robustness. Recent work has shown that differential design can mitigate this issue in Transformers, improving their effectiveness across various applications. In this paper, we explore whether these techniques, originally developed for Transformers, can be applied to Mamba, a recent architecture based on selective state-space layers that achieves Transformer-level performance with greater efficiency. We show that a naive adaptation of differential design to Mamba is insufficient and requires careful architectural modifications. To address this, we introduce a novel differential mechanism for Mamba, empirically validated on language modeling benchmarks, demonstrating improved retrieval capabilities and superior performance over vanilla Mamba. Finally, we conduct extensive ablation studies and empirical analyses to justify our design choices and provide evidence that our approach effectively mitigates the overallocation problem in Mamba-based models. Our code is publicly available: https://github.com/NadavSc/Diff-Mamba</article>","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging Expressivity and Scalability with Adaptive Unitary SSMs","url":"https://arxiv.org/abs/2507.05238","date":1761796800,"author":"","guid":321238,"unread":true,"content":"<article>arXiv:2507.05238v2 Announce Type: replace \nAbstract: Recent work has revealed that state space models (SSMs), while efficient for long-sequence processing, are fundamentally limited in their ability to represent formal languages-particularly due to time-invariant and real-valued recurrence structures. In this work, we draw inspiration from adaptive and structured dynamics observed in biological neural systems and introduce the Adaptive Unitary State Space Model (AUSSM), a novel class of SSMs that leverages skew-symmetric, input-dependent recurrence to achieve unitary evolution and high expressive power. Using algebraic automata theory, we prove that AUSSM can perform modulo counting and simulate solvable group automata at finite precision, enabling AUSSM to model a broad class of regular languages out of reach for other SSM architectures. To overcome the practical inefficiencies of adaptive recurrence, we develop a separable convolution formulation and a CUDA implementation that enables scalable parallel training. Empirically, we show that AUSSM and its hybrid variant-interleaved with Mamba-outperform prior SSMs on formal algorithmic tasks such as parity and modular arithmetic, and achieve competent performance on real-world long time-series classification benchmarks. Our results demonstrate that adaptive unitary recurrence provides a powerful and efficient inductive bias for both symbolic and continuous sequence modeling. The code is available at https://github.com/arjunkaruvally/AUSSM</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Steering Information Utility in Key-Value Memory for Language Model Post-Training","url":"https://arxiv.org/abs/2507.05158","date":1761796800,"author":"","guid":321239,"unread":true,"content":"<article>arXiv:2507.05158v2 Announce Type: replace \nAbstract: Recent advancements in language models (LMs) have marked a shift toward the growing importance of post-training. Yet, post-training approaches such as supervised fine-tuning (SFT) do not guarantee the effective use of knowledge acquired during pretraining. We therefore introduce InfoSteer, a lightweight method that encourages parametric information utilization in LMs during post-training. Specifically, InfoSteer treats the feed-forward network (FFN) layer as associate key-value memory and promotes the use of stored memory vectors via forward-pass interventions or regularization during backpropagation. This simple guidance during post-training phase yields consistent performance improvements across diverse model families -- including Qwen, Gemma and Llama -- spanning 15 downstream tasks in both in-distribution (ID) and out-of-distribution (OOD) evaluations. Beyond performance gains, we also find that steered LMs can adaptively allocate information by placing more emphasis on generating semantically meaningful tokens, while using fewer resources on simple transition ones (e.g., `\\texttt{,}' or `\\texttt{and}'). Our work underscores that vanilla post-training does not fully exploit the potential gained during pre-training, and that steering LMs in latent representation space offers a promising approach to enhance both performance and interpretability. The code is available at: https://github.com/chili-lab/InfoSteer.</article>","contentLength":1488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adapter-state Sharing CLIP for Parameter-efficient Multimodal Sarcasm Detection","url":"https://arxiv.org/abs/2507.04508","date":1761796800,"author":"","guid":321240,"unread":true,"content":"<article>arXiv:2507.04508v2 Announce Type: replace \nAbstract: The growing prevalence of multimodal image-text sarcasm on social media poses challenges for opinion mining systems. Existing approaches rely on full fine-tuning of large models, making them unsuitable to adapt under resource-constrained settings. While recent parameter-efficient fine-tuning (PEFT) methods offer promise, their off-the-shelf use underperforms on complex tasks like sarcasm detection. We propose AdS-CLIP (Adapter-state Sharing in CLIP), a lightweight framework built on CLIP that inserts adapters only in the upper layers to preserve low-level unimodal representations in the lower layers and introduces a novel adapter-state sharing mechanism, where textual adapters guide visual ones to promote efficient cross-modal learning in the upper layers. Experiments on two public benchmarks demonstrate that AdS-CLIP not only outperforms standard PEFT methods but also existing multimodal baselines with significantly fewer trainable parameters.</article>","contentLength":1011,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection","url":"https://arxiv.org/abs/2507.04458","date":1761796800,"author":"","guid":321241,"unread":true,"content":"<article>arXiv:2507.04458v2 Announce Type: replace \nAbstract: Multimodal sarcasm detection has attracted growing interest due to the rise of multimedia posts on social media. Understanding sarcastic image-text posts often requires external contextual knowledge, such as cultural references or commonsense reasoning. However, existing models struggle to capture the deeper rationale behind sarcasm, relying mainly on shallow cues like image captions or object-attribute pairs from images. To address this, we propose \\textbf{MiDRE} (\\textbf{Mi}xture of \\textbf{D}ual \\textbf{R}easoning \\textbf{E}xperts), which integrates an internal reasoning expert for detecting incongruities within the image-text pair and an external reasoning expert that utilizes structured rationales generated via Chain-of-Thought prompting to a Large Vision-Language Model. An adaptive gating mechanism dynamically weighs the two experts, selecting the most relevant reasoning path. Unlike prior methods that treat external knowledge as static input, MiDRE selectively adapts to when such knowledge is beneficial, mitigating the risks of hallucinated or irrelevant signals from large models. Experiments on two benchmark datasets show that MiDRE achieves superior performance over baselines. Various qualitative analyses highlight the crucial role of external rationales, revealing that even when they are occasionally noisy, they provide valuable cues that guide the model toward a better understanding of sarcasm.</article>","contentLength":1481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations","url":"https://arxiv.org/abs/2507.01131","date":1761796800,"author":"","guid":321242,"unread":true,"content":"<article>arXiv:2507.01131v2 Announce Type: replace \nAbstract: $\\rm{SO}(3)$-equivariant networks are the dominant models for machine learning interatomic potentials (MLIPs). The key operation of such networks is the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To accelerate the computation, we develop tensor decomposition networks (TDNs) as a class of approximately equivariant networks in which CG tensor products are replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP) decomposition. With the CP decomposition, we prove (i) a uniform bound on the induced error of $\\rm{SO}(3)$-equivariance, and (ii) the universality of approximating any equivariant bilinear map. To further reduce the number of parameters, we propose path-weight sharing that ties all multiplicity-space weights across the $\\mathcal{O}(L^3)$ CG paths into a single path without compromising equivariance, where $L$ is the maximum angular degree. The resulting layer acts as a plug-and-play replacement for tensor products in existing networks, and the computational complexity of tensor products is reduced from $\\mathcal{O}(L^6)$ to $\\mathcal{O}(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation dataset containing 105 million DFT-calculated snapshots. We also use existing datasets, including OC20, and OC22. Results show that TDNs achieve competitive performance with dramatic speedup in computations. Our code is publicly available as part of the AIRS library (\\href{https://github.com/divelab/AIRS/tree/main/OpenMol/TDN}{https://github.com/divelab/AIRS/}).</article>","contentLength":1603,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Many LLMs Are More Utilitarian Than One","url":"https://arxiv.org/abs/2507.00814","date":1761796800,"author":"","guid":321243,"unread":true,"content":"<article>arXiv:2507.00814v2 Announce Type: replace \nAbstract: Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the Boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed either reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the Boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents</article>","contentLength":1570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and NVIDIA Data Center GPUs","url":"https://arxiv.org/abs/2507.00418","date":1761796800,"author":"","guid":321244,"unread":true,"content":"<article>arXiv:2507.00418v3 Announce Type: replace \nAbstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100 Ultra (QAic) accelerator for large language model (LLM) inference, evaluating its energy efficiency (throughput per watt), performance, and hardware scalability against NVIDIA A100 GPUs (in 4x and 8x configurations) within the National Research Platform (NRP) ecosystem. A total of 12 open-source LLMs, ranging from 124 million to 70 billion parameters, are served using the vLLM framework. Our analysis reveals that QAic achieves competitive energy efficiency with advantages on specific models while enabling more granular hardware allocation: some 70B models operate on as few as 1 QAic card versus 8 A100 GPUs required, with 20x lower power consumption (148W vs 2,983W). For smaller models, single QAic devices achieve up to 35x lower power consumption compared to our 4-GPU A100 configuration (36W vs 1,246W). The findings offer insights into the potential of the Qualcomm Cloud AI 100 Ultra for energy-constrained and resource-efficient HPC deployments within the National Research Platform (NRP).</article>","contentLength":1129,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction","url":"https://arxiv.org/abs/2506.24096","date":1761796800,"author":"","guid":321245,"unread":true,"content":"<article>arXiv:2506.24096v2 Announce Type: replace \nAbstract: While recent advances in Gaussian Splatting have enabled fast reconstruction of high-quality 3D scenes from images, extracting accurate surface meshes remains a challenge. Current approaches extract the surface through costly post-processing steps, resulting in the loss of fine geometric details or requiring significant time and leading to very dense meshes with millions of vertices. More fundamentally, the a posteriori conversion from a volumetric to a surface representation limits the ability of the final mesh to preserve all geometric structures captured during training. We present MILo, a novel Gaussian Splatting framework that bridges the gap between volumetric and surface representations by differentiably extracting a mesh from the 3D Gaussians. We design a fully differentiable procedure that constructs the mesh-including both vertex locations and connectivity-at every iteration directly from the parameters of the Gaussians, which are the only quantities optimized during training. Our method introduces three key technical contributions: a bidirectional consistency framework ensuring both representations-Gaussians and the extracted mesh-capture the same underlying geometry during training; an adaptive mesh extraction process performed at each training iteration, which uses Gaussians as differentiable pivots for Delaunay triangulation; a novel method for computing signed distance values from the 3D Gaussians that enables precise surface extraction while avoiding geometric erosion. Our approach can reconstruct complete scenes, including backgrounds, with state-of-the-art quality while requiring an order of magnitude fewer mesh vertices than previous methods. Due to their light weight and empty interior, our meshes are well suited for downstream applications such as physics simulations or animation.</article>","contentLength":1885,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering","url":"https://arxiv.org/abs/2506.21710","date":1761796800,"author":"","guid":321246,"unread":true,"content":"<article>arXiv:2506.21710v2 Announce Type: replace \nAbstract: While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and three types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.</article>","contentLength":1348,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context Learning","url":"https://arxiv.org/abs/2506.21355","date":1761796800,"author":"","guid":321247,"unread":true,"content":"<article>arXiv:2506.21355v2 Announce Type: replace \nAbstract: Multimodal in-context learning (ICL) remains underexplored despite significant potential for domains such as medicine. Clinicians routinely encounter diverse, specialized tasks requiring adaptation from limited examples, such as drawing insights from a few relevant prior cases or considering a constrained set of differential diagnoses. While multimodal large language models (MLLMs) have shown advances in medical visual question answering (VQA), their ability to learn multimodal tasks from context is largely unknown. We introduce SMMILE, the first expert-driven multimodal ICL benchmark for medical tasks. Eleven medical experts curated problems, each including a multimodal query and multimodal in-context examples as task demonstrations. SMMILE encompasses 111 problems (517 question-image-answer triplets) covering 6 medical specialties and 13 imaging modalities. We further introduce SMMILE++, an augmented variant with 1038 permuted problems. A comprehensive evaluation of 15 MLLMs demonstrates that most models exhibit moderate to poor multimodal ICL ability in medical tasks. In open-ended evaluations, ICL contributes only an 8% average improvement over zero-shot on SMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant in-context examples: even a single noisy or irrelevant example can degrade performance by up to 9.5%. Moreover, we observe that MLLMs are affected by a recency bias, where placing the most relevant example last can lead to substantial performance improvements of up to 71%. Our findings highlight critical limitations and biases in current MLLMs when learning multimodal medical tasks from context. SMMILE is available at https://smmile-benchmark.github.io.</article>","contentLength":1762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models","url":"https://arxiv.org/abs/2506.17585","date":1761796800,"author":"","guid":321248,"unread":true,"content":"<article>arXiv:2506.17585v2 Announce Type: replace \nAbstract: Trustworthy language models should provide both correct and verifiable answers. However, citations generated directly by standalone LLMs are often unreliable. As a result, current systems insert citations by querying an external retriever at inference time, introducing latency, infrastructure dependence, and vulnerability to retrieval noise. We explore whether LLMs can be made to reliably attribute to the documents seen during continual pretraining without test-time retrieval, by revising the training process. To study this, we construct CitePretrainBench, a benchmark that mixes real-world corpora (Wikipedia, Common Crawl, arXiv) with novel documents and probes both short-form (single-fact) and long-form (multi-fact) citation tasks. Our approach follows a two-stage process: (1) continual pretraining to index factual knowledge by binding it to persistent document identifiers; and (2) instruction tuning to elicit citation behavior. We introduce Active Indexing for the first stage, which creates generalizable, source-anchored bindings by augmenting training with synthetic data that (i) restate each fact in diverse, compositional forms and (ii) enforce bidirectional training (source-to-fact and fact-to-source). This equips the model to both generate content from a cited source and attribute its own answers, improving robustness to paraphrase and composition. Experiments with Qwen-2.5-7B&amp;3B show that Active Indexing consistently outperforms a Passive Indexing baseline, which simply appends an identifier to each document, achieving citation precision gains of up to 30.2% across all tasks and models. Our ablation studies reveal that performance continues to improve as we scale the amount of augmented data, showing a clear upward trend even at 16x the original token count. Finally, we show that internal citations complement external ones by making the model more robust to retrieval noise.</article>","contentLength":1966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Adaptation for Flying Quadrotors in Tight Formations","url":"https://arxiv.org/abs/2506.17488","date":1761796800,"author":"","guid":321249,"unread":true,"content":"<article>arXiv:2506.17488v2 Announce Type: replace \nAbstract: The task of flying in tight formations is challenging for teams of quadrotors because the complex aerodynamic wake interactions can destabilize individual team members as well as the team. Furthermore, these aerodynamic effects are highly nonlinear and fast-paced, making them difficult to model and predict. To overcome these challenges, we present L1 KNODE-DW MPC, an adaptive, mixed expert learning based control framework that allows individual quadrotors to accurately track trajectories while adapting to time-varying aerodynamic interactions during formation flights. We evaluate L1 KNODE-DW MPC in two different three-quadrotor formations and show that it outperforms several MPC baselines. Our results show that the proposed framework is capable of enabling the three-quadrotor team to remain vertically aligned in close proximity throughout the flight. These findings show that the L1 adaptive module compensates for unmodeled disturbances most effectively when paired with an accurate dynamics model. A video showcasing our framework and the physical experiments is available here: https://youtu.be/9QX1Q5Ut9Rs</article>","contentLength":1174,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TabArena: A Living Benchmark for Machine Learning on Tabular Data","url":"https://arxiv.org/abs/2506.16791","date":1761796800,"author":"","guid":321250,"unread":true,"content":"<article>arXiv:2506.16791v3 Announce Type: replace \nAbstract: With the growing popularity of deep learning and foundation models for tabular data, the need for standardized and reliable benchmarks is higher than ever. However, current benchmarks are static. Their design is not updated even if flaws are discovered, model versions are updated, or new models are released. To address this, we introduce TabArena, the first continuously maintained living tabular benchmarking system. To launch TabArena, we manually curate a representative collection of datasets and well-implemented models, conduct a large-scale benchmarking study to initialize a public leaderboard, and assemble a team of experienced maintainers. Our results highlight the influence of validation method and ensembling of hyperparameter configurations to benchmark models at their full potential. While gradient-boosted trees are still strong contenders on practical tabular datasets, we observe that deep learning methods have caught up under larger time budgets with ensembling. At the same time, foundation models excel on smaller datasets. Finally, we show that ensembles across models advance the state-of-the-art in tabular machine learning. We observe that some deep learning models are overrepresented in cross-model ensembles due to validation set overfitting, and we encourage model developers to address this issue. We launch TabArena with a public leaderboard, reproducible code, and maintenance protocols to create a living benchmark available at https://tabarena.ai.</article>","contentLength":1539,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mesh-Informed Neural Operator : A Transformer Generative Approach","url":"https://arxiv.org/abs/2506.16656","date":1761796800,"author":"","guid":321251,"unread":true,"content":"<article>arXiv:2506.16656v3 Announce Type: replace \nAbstract: Generative models in function spaces, situated at the intersection of generative modeling and operator learning, are attracting increasing attention due to their immense potential in diverse scientific and engineering applications. While functional generative models are theoretically domain- and discretization-agnostic, current implementations heavily rely on the Fourier Neural Operator (FNO), limiting their applicability to regular grids and rectangular domains. To overcome these critical limitations, we introduce the Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and cross-attention mechanisms, MINO offers a principled, domain- and discretization-agnostic backbone for generative modeling in function spaces. This advancement significantly expands the scope of such models to more diverse applications in generative, inverse, and regression tasks. Furthermore, MINO provides a unified perspective on integrating neural operators with general advanced deep learning architectures. Finally, we introduce a suite of standardized evaluation metrics that enable objective comparison of functional generative models, addressing another critical gap in the field.</article>","contentLength":1247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration","url":"https://arxiv.org/abs/2506.15721","date":1761796800,"author":"","guid":321252,"unread":true,"content":"<article>arXiv:2506.15721v4 Announce Type: replace \nAbstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of multiple source LLMs with different architectures into a target LLM with low computational overhead. While promising, existing methods suffer from two major limitations: 1) reliance on real data from limited domain for knowledge fusion, preventing the target LLM from fully acquiring knowledge across diverse domains, and 2) fixed data allocation proportions across domains, failing to dynamically adjust according to the target LLM's varying capabilities across domains, leading to a capability imbalance. To overcome these limitations, we propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework. Through the organization of knowledge domains into a hierarchical tree structure, Bohdi enables automatic domain exploration and multi-domain data generation through multi-model collaboration, thereby comprehensively extracting knowledge from source LLMs. By formalizing domain expansion and data sampling proportion allocation on the knowledge tree as a Hierarchical Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism to adaptively adjust sampling proportions based on the target LLM's performance feedback across domains. Integrated with our proposed Introspection-Rebirth (IR) mechanism, DynaBranches dynamically tracks capability shifts during target LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT), further enhancing its online adaptation capability. Comparative experimental results on a comprehensive suite of benchmarks demonstrate that Bohdi significantly outperforms existing baselines on multiple target LLMs, exhibits higher data efficiency, and virtually eliminates the imbalance in the target LLM's capabilities. Our code is available at https://github.com/gjq100/Bohdi.git.</article>","contentLength":1888,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents","url":"https://arxiv.org/abs/2506.14866","date":1761796800,"author":"","guid":321253,"unread":true,"content":"<article>arXiv:2506.14866v2 Announce Type: replace \nAbstract: Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-Harm, a new benchmark for measuring safety of computer use agents. OS-Harm is built on top of the OSWorld environment and aims to test models across three categories of harm: deliberate user misuse, prompt injection attacks, and model misbehavior. To cover these cases, we create 150 tasks that span several types of safety violations (harassment, copyright infringement, disinformation, data exfiltration, etc.) and require the agent to interact with a variety of OS applications (email client, code editor, browser, etc.). Moreover, we propose an automated judge to evaluate both accuracy and safety of agents that achieves high agreement with human annotations (0.76 and 0.79 F1 score). We evaluate computer use agents based on a range of frontier models - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide insights into their safety. In particular, all models tend to directly comply with many deliberate misuse queries, are relatively vulnerable to static prompt injections, and occasionally perform unsafe actions. The OS-Harm benchmark is available at https://github.com/tml-epfl/os-harm.</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jailbreak Transferability Emerges from Shared Representations","url":"https://arxiv.org/abs/2506.12913","date":1761796800,"author":"","guid":321254,"unread":true,"content":"<article>arXiv:2506.12913v2 Announce Type: replace \nAbstract: Jailbreak transferability is the surprising phenomenon when an adversarial attack compromising one model also elicits harmful responses from other models. Despite widespread demonstrations, there is little consensus on why transfer is possible: is it a quirk of safety training, an artifact of model families, or a more fundamental property of representation learning? We present evidence that transferability emerges from shared representations rather than incidental flaws. Across 20 open-weight models and 33 jailbreak attacks, we find two factors that systematically shape transfer: (1) representational similarity under benign prompts, and (2) the strength of the jailbreak on the source model. To move beyond correlation, we show that deliberately increasing similarity through benign only distillation causally increases transfer. Our qualitative analyses reveal systematic transferability patterns across different types of jailbreaks. For example, persona-style jailbreaks transfer far more often than cipher-based prompts, consistent with the idea that natural-language attacks exploit models' shared representation space, whereas cipher-based attacks rely on idiosyncratic quirks that do not generalize. Together, these results reframe jailbreak transfer as a consequence of representation alignment rather than a fragile byproduct of safety training.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization","url":"https://arxiv.org/abs/2506.12484","date":1761796800,"author":"","guid":321255,"unread":true,"content":"<article>arXiv:2506.12484v4 Announce Type: replace \nAbstract: Language models can retain dangerous knowledge and skills even after extensive safety fine-tuning, posing both misuse and misalignment risks. Recent studies show that even specialized unlearning methods can be easily reversed. To address this, we systematically evaluate many existing and novel components of unlearning methods and identify ones crucial for irreversible unlearning.\n  We introduce Disruption Masking, a technique in which we only allow updating weights, where the signs of the unlearning gradient and the retaining gradient are the same. This ensures all updates are non-disruptive.\n  Additionally, we identify the need for normalizing the unlearning gradients, and also confirm the usefulness of meta-learning. We combine these insights into MUDMAN (Meta-Unlearning with Disruption Masking and Normalization) and validate its effectiveness at preventing the recovery of dangerous capabilities. MUDMAN outperforms the prior TAR method by 40%, setting a new state-of-the-art for robust unlearning.</article>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Path-specific effects for pulse-oximetry guided decisions in critical care","url":"https://arxiv.org/abs/2506.12371","date":1761796800,"author":"","guid":321256,"unread":true,"content":"<article>arXiv:2506.12371v3 Announce Type: replace \nAbstract: Identifying and measuring biases associated with sensitive attributes is a crucial consideration in healthcare to prevent treatment disparities. One prominent issue is inaccurate pulse oximeter readings, which tend to overestimate oxygen saturation for dark-skinned patients and misrepresent supplemental oxygen needs. Most existing research has revealed statistical disparities linking device measurement errors to patient outcomes in intensive care units (ICUs) without causal formalization. This study causally investigates how racial discrepancies in oximetry measurements affect invasive ventilation in ICU settings. We employ a causal inference-based approach using path-specific effects to isolate the impact of bias by race on clinical decision-making. To estimate these effects, we leverage a doubly robust estimator, propose its self-normalized variant for improved sample efficiency, and provide novel finite-sample guarantees. Our methodology is validated on semi-synthetic data and applied to two large real-world health datasets: MIMIC-IV and eICU. Contrary to prior work, our analysis reveals minimal impact of racial discrepancies on invasive ventilation rates. However, path-specific effects mediated by oxygen saturation disparity are more pronounced on ventilation duration, and the severity differs by dataset. Our work provides a novel pipeline for investigating potential disparities in clinical decision-making and, more importantly, highlights the necessity of causal methods to robustly assess fairness in healthcare.</article>","contentLength":1595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning single-index models via harmonic decomposition","url":"https://arxiv.org/abs/2506.09887","date":1761796800,"author":"","guid":321257,"unread":true,"content":"<article>arXiv:2506.09887v2 Announce Type: replace \nAbstract: We study the problem of learning single-index models, where the label $y \\in \\mathbb{R}$ depends on the input $\\boldsymbol{x} \\in \\mathbb{R}^d$ only through an unknown one-dimensional projection $\\langle \\boldsymbol{w}_*,\\boldsymbol{x}\\rangle$. Prior work has shown that under Gaussian inputs, the statistical and computational complexity of recovering $\\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function. In this paper, we propose a new perspective: we argue that $spherical$ $harmonics$ -- rather than $Hermite$ $polynomials$ -- provide the natural basis for this problem, as they capture its intrinsic $rotational$ $symmetry$. Building on this insight, we characterize the complexity of learning single-index models under arbitrary spherically symmetric input distributions. We introduce two families of estimators -- based on tensor unfolding and online SGD -- that respectively achieve either optimal sample complexity or optimal runtime, and argue that estimators achieving both may not exist in general. When specialized to Gaussian inputs, our theory not only recovers and clarifies existing results but also reveals new phenomena that had previously been overlooked.</article>","contentLength":1253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era","url":"https://arxiv.org/abs/2506.09755","date":1761796800,"author":"","guid":321258,"unread":true,"content":"<article>arXiv:2506.09755v2 Announce Type: replace \nAbstract: Research and practice in Intelligent Design (ID) have significantly enhanced engineering innovation, efficiency, quality, and productivity over recent decades, fundamentally reshaping how engineering designers think, behave, and interact with design processes. The recent emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), has demonstrated general knowledge-based reasoning capabilities, and open new avenues for further transformation in engineering design. In this context, this paper introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by foundation model-based agentic AI systems. We review the historical evolution of ID across four distinct stages: rule-based expert systems, task-specific machine learning models, large-scale foundation AI models, and the recent emerging paradigm of foundation model-based multi-agent collaboration. We propose an ontological framework for ID 4.0 and discuss its potential to support end-to-end automation of engineering design processes through coordinated, autonomous multi-agent-based systems. Furthermore, we discuss challenges and opportunities of ID 4.0, including perspectives on data foundations, agent collaboration mechanisms, and the formulation of design problems and objectives. In sum, these insights provide a foundation for advancing Intelligent Design toward greater adaptivity, autonomy, and effectiveness in addressing the growing complexity of engineering design.</article>","contentLength":1532,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene","url":"https://arxiv.org/abs/2506.09518","date":1761796800,"author":"","guid":321259,"unread":true,"content":"<article>arXiv:2506.09518v2 Announce Type: replace \nAbstract: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time rendering in static settings, extending it to dynamic scenes is challenging due to the difficulty of learning structured and temporally consistent motion representations. This challenge often manifests as three limitations in existing methods: redundant Gaussian updates, insufficient motion supervision, and weak modeling of complex non-rigid deformations. These issues collectively hinder coherent and efficient dynamic reconstruction. To address these limitations, we propose HAIF-GS, a unified framework that enables structured and consistent dynamic modeling through sparse anchor-driven deformation. It first identifies motion-relevant regions via an Anchor Filter to suppress redundant updates in static areas. A self-supervised Induced Flow-Guided Deformation module induces anchor motion using multi-frame feature aggregation, eliminating the need for explicit flow labels. To further handle fine-grained deformations, a Hierarchical Anchor Propagation mechanism increases anchor resolution based on motion complexity and propagates multi-level transformations. Extensive experiments on synthetic and real-world benchmarks validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in rendering quality, temporal coherence, and reconstruction efficiency.</article>","contentLength":1487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Synchronization in Anonymous Networks Under Arbitrary Dynamics","url":"https://arxiv.org/abs/2506.08661","date":1761796800,"author":"","guid":321260,"unread":true,"content":"<article>arXiv:2506.08661v2 Announce Type: replace \nAbstract: We present the $\\delta$-Synchronizer, which works in non-synchronous dynamic networks under minimal assumptions. Our model allows for arbitrary topological changes without any guarantee of eventual global or partial stabilization and assumes that nodes are anonymous. This deterministic synchronizer is the first that enables nodes to simulate a dynamic network synchronous algorithm for executions in a semi-synchronous dynamic environment under a weakly-fair node activation scheduler, despite the absence of a global clock, node ids, persistent connectivity or any assumptions about the edge dynamics (in both the synchronous and semi-synchronous environments). We make the following contributions: (1) we extend the definition of synchronizers to networks with arbitrary edge dynamics; (2) we present the first synchronizer from the semi-synchronous to the synchronous model in such networks; and (3) we present non-trivial applications of the proposed synchronizer to existing algorithms. We assume an extension of the Pull communication model by adding a single 1-bit multi-writer atomic register at each edge-port of a node. We show that this extension is needed and that synchronization in our setting is not possible without it. The $\\delta$-Synchronizer operates with a multiplicative memory overhead at the nodes that is asymptotically logarithmic on the runtime of the underlying synchronous algorithm being simulated-in particular, it is logarithmic for polynomial-time synchronous algorithms.</article>","contentLength":1559,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning Teachers of Test Time Scaling","url":"https://arxiv.org/abs/2506.08388","date":1761796800,"author":"","guid":321261,"unread":true,"content":"<article>arXiv:2506.08388v3 Announce Type: replace \nAbstract: Training reasoning language models (LMs) with reinforcement learning (RL) for one-hot correctness inherently relies on the LM being able to explore and solve its task with some chance at initialization. Furthermore, a key use case of reasoning LMs is to act as teachers for distilling new students and cold-starting future RL iterations rather than being deployed themselves. From these considerations, we introduce a new framework that avoids RL's exploration challenge by training a new class of Reinforcement-Learned Teachers (RLTs) focused on yielding the most effective downstream distillation. RLTs are prompted with both the question and solution to each problem, and tasked to simply \"connect-the-dots\" with detailed explanations tailored for their students. We train RLTs with dense rewards obtained by feeding each explanation to the student and testing its understanding of the problem's solution. In practice, the raw outputs of a 7B RLT provide higher final performance on competition and graduate-level tasks than existing distillation and cold-starting pipelines that collect and postprocess the reasoning traces of orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness when training larger students and when applied zero-shot to out-of-distribution tasks, unlocking new levels of efficiency and re-usability for the RL reasoning framework. Code available at: https://github.com/SakanaAI/RLT</article>","contentLength":1484,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Limits on Distributed Quantum Advantage: Dequantizing Linear Programs","url":"https://arxiv.org/abs/2506.07574","date":1761796800,"author":"","guid":321262,"unread":true,"content":"<article>arXiv:2506.07574v3 Announce Type: replace \nAbstract: In this work, we give two results that put new limits on distributed quantum advantage in the context of the LOCAL model of distributed computing. First, we show that there is no distributed quantum advantage for any linear program. Put otherwise, if there is a quantum-LOCAL algorithm $\\mathcal{A}$ that finds an $\\alpha$-approximation of some linear optimization problem $\\Pi$ in $T$ communication rounds, we can construct a classical, deterministic LOCAL algorithm $\\mathcal{A}'$ that finds an $\\alpha$-approximation of $\\Pi$ in $T$ rounds. As a corollary, all classical lower bounds for linear programs, including the KMW bound, hold verbatim in quantum-LOCAL. Second, using the above result, we show that there exists a locally checkable labeling problem (LCL) for which quantum-LOCAL is strictly weaker than the classical deterministic SLOCAL model. Our results extend from quantum-LOCAL also to finitely dependent and non-signaling distributions, and one of the corollaries of our work is that the non-signaling model and the SLOCAL model are incomparable in the context of LCL problems: By prior work, there exists an LCL problem for which SLOCAL is strictly weaker than the non-signaling model, and our work provides a separation in the opposite direction.</article>","contentLength":1318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Capturing User Interests from Data Streams for Continual Sequential Recommendation","url":"https://arxiv.org/abs/2506.07466","date":1761796800,"author":"","guid":321263,"unread":true,"content":"<article>arXiv:2506.07466v2 Announce Type: replace \nAbstract: Transformer-based sequential recommendation (SR) models excel at modeling long-range dependencies in user behavior via self-attention. However, updating them with continuously arriving behavior sequences incurs high computational costs or leads to catastrophic forgetting. Although continual learning, a standard approach for non-stationary data streams, has recently been applied to recommendation, existing methods gradually forget long-term user preferences and remain underexplored in SR. In this paper, we introduce Continual Sequential Transformer for Recommendation (CSTRec). CSTRec is designed to effectively adapt to current interests by leveraging well-preserved historical ones, thus capturing the trajectory of user interests over time. The core of CSTRec is Continual Sequential Attention (CSA), a linear attention tailored for continual SR, which enables CSTRec to partially retain historical knowledge without direct access to prior data. CSA has two key components: (1) Cauchy-Schwarz Normalization that stabilizes learning over time under uneven user interaction frequencies; (2) Collaborative Interest Enrichment that alleviates forgetting through shared, learnable interest pools. In addition, we introduce a new technique to facilitate the adaptation of new users by transferring historical knowledge from existing users with similar interests. Extensive experiments on three real-world datasets show that CSTRec outperforms state-of-the-art models in both knowledge retention and acquisition.</article>","contentLength":1566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO","url":"https://arxiv.org/abs/2506.07464","date":1761796800,"author":"","guid":321264,"unread":true,"content":"<article>arXiv:2506.07464v3 Announce Type: replace \nAbstract: Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training for enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success using a PPO-style reinforcement algorithm with group-normalized rewards. However, the effectiveness of GRPO in Video Large Language Models (VideoLLMs) has still been less studyed. In this paper, we explore GRPO and identify two problems that deteriorate the effective learning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation. Reg-GRPO reformulates the GRPO loss function into a regression task that directly predicts the advantage in GRPO, eliminating the need for safeguards such as the clipping and min functions. It directly aligns the model with advantages, providing guidance to prefer better ones. The difficulty-aware data augmentation strategy augments input prompts/videos to locate the difficulty of samples at solvable difficulty levels, enabling diverse reward signals. Our experimental results show that our approach significantly improves video reasoning performance across multiple benchmarks.</article>","contentLength":1394,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation","url":"https://arxiv.org/abs/2506.06677","date":1761796800,"author":"","guid":321265,"unread":true,"content":"<article>arXiv:2506.06677v2 Announce Type: replace \nAbstract: Recent advances in vision-language models (VLMs) have enabled instruction-conditioned robotic systems with improved generalization. However, most existing work focuses on reactive System 1 policies, underutilizing VLMs' strengths in semantic reasoning and long-horizon planning. These System 2 capabilities-characterized by deliberative, goal-directed thinking-remain under explored due to the limited temporal scale and structural complexity of current benchmarks. To address this gap, we introduce RoboCerebra, a benchmark for evaluating high-level reasoning in long-horizon robotic manipulation. RoboCerebra includes: (1) a large-scale simulation dataset with extended task horizons and diverse subtask sequences in household environments; (2) a hierarchical framework combining a high-level VLM planner with a low-level vision-language-action (VLA) controller; and (3) an evaluation protocol targeting planning, reflection, and memory through structured System 1-System 2 interaction. The dataset is constructed via a top-down pipeline, where GPT generates task instructions and decomposes them into subtask sequences. Human operators execute the subtasks in simulation, yielding high-quality trajectories with dynamic object variations. Compared to prior benchmarks, RoboCerebra features significantly longer action sequences and denser annotations. We further benchmark state-of-the-art VLMs as System 2 modules and analyze their performance across key cognitive dimensions, advancing the development of more capable and generalizable robotic planners.</article>","contentLength":1611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Guided Scenario-based GUI Testing","url":"https://arxiv.org/abs/2506.05079","date":1761796800,"author":"","guid":321266,"unread":true,"content":"<article>arXiv:2506.05079v2 Announce Type: replace \nAbstract: The assurance of mobile app GUIs has become increasingly important, as the GUI serves as the primary medium of interaction between users and apps. Although numerous automated GUI testing approaches have been developed with diverse strategies, a substantial gap remains between these approaches and the underlying app business logic. Most existing approaches focus on general exploration rather than the completion of specific testing scenarios, often missing critical functionalities. Inspired by manual testing, which treats business logic-driven scenarios as the fundamental unit of testing, this paper introduces an approach that leverages large language models to comprehend GUI semantics and contextual relevance to given scenarios. Building on this capability, we propose ScenGen, an LLM-guided scenario-based GUI testing framework employing multi-agent collaboration to simulate and automate manual testing phases.\n  Specifically, ScenGen integrates five agents: the Observer, Decider, Executor, Supervisor, and Recorder. The Observer perceives the app GUI state by extracting and structuring GUI widgets and layouts, interpreting semantic information. This is passed to the Decider, which makes scenario-driven decisions with LLM guidance to identify target widgets and determine actions toward fulfilling specific goals. The Executor performs these operations, while the Supervisor verifies alignment with intended scenario completion, ensuring traceability and consistency. Finally, the Recorder logs GUI operations into context memory as a knowledge base for subsequent decision-making and monitors runtime bugs. Comprehensive evaluations demonstrate that ScenGen effectively generates scenario-based GUI tests guided by LLM collaboration, achieving higher relevance to business logic and improving the completeness of automated GUI testing.</article>","contentLength":1905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Path Planning and Task Allocation Algorithm for Boolean Specifications","url":"https://arxiv.org/abs/2506.04881","date":1761796800,"author":"","guid":321267,"unread":true,"content":"<article>arXiv:2506.04881v2 Announce Type: replace \nAbstract: This paper presents a novel path-planning and task assignment algorithm for multi-robot systems that should fulfill a global Boolean specification. The proposed method is based on Integer Linear Programming (ILP) formulations, which are combined with structural insights from Petri nets to improve scalability and computational efficiency. By proving that the \\emph{constraint matrix} is totally unimodular (TU) for certain classes of problems, the ILP formulation can be relaxed into a Linear Programming (LP) problem without losing the integrality of the solution. This relaxation eliminates complex combinatorial techniques, significantly reducing computational overhead and thus ensuring scalability for large-scale systems. Using the approach proposed in this paper, we can solve path-planning problems for teams made up to 500 robots. The method guarantees computational tractability, handles collision avoidance and reduces computational demands through iterative LP optimization techniques. Case studies demonstrate the efficiency of the algorithm in generating scalable, collision-free paths for large robot teams navigating in complex environments. While the conservative nature of collision avoidance introduces additional constraints, and thus, computational requirements, the solution remains practical and impactful for diverse applications. The algorithm is particularly applicable to real-world scenarios, including warehouse logistics where autonomous robots must efficiently coordinate tasks or search-and-rescue operations in various environments. This work contributes both theoretically and practically to scalable multi-robot path planning and task allocation, offering an efficient framework for coordinating autonomous agents in shared environments.</article>","contentLength":1826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Preference Optimization via Dynamic Target Margins","url":"https://arxiv.org/abs/2506.03690","date":1761796800,"author":"","guid":321268,"unread":true,"content":"<article>arXiv:2506.03690v2 Announce Type: replace \nAbstract: The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\\gamma$-PO achieves an average 4.4\\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \\href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.</article>","contentLength":1411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner","url":"https://arxiv.org/abs/2506.03595","date":1761796800,"author":"","guid":321269,"unread":true,"content":"<article>arXiv:2506.03595v2 Announce Type: replace \nAbstract: The recent success of Shampoo in the AlgoPerf contest has sparked renewed interest in Kronecker-factorization-based optimization algorithms for training neural networks. Despite its success, Shampoo relies heavily on several heuristics such as learning rate grafting and stale preconditioning to achieve performance at-scale. These heuristics increase algorithmic complexity, necessitate further hyperparameter tuning, and lack theoretical justification. This paper investigates these heuristics from the angle of Frobenius norm approximation to full-matrix Adam and decouples the preconditioner's eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates the staleness and mis-scaling of the preconditioner's eigenvalues and how correcting the eigenvalues directly eliminates the need for learning rate grafting. To manage the error induced by infrequent eigenbasis computations, we propose an adaptive criterion for determining the eigenbasis computation frequency motivated by terminating a warm-started QR algorithm. This criterion decouples the update frequency of different preconditioner matrices and enables us to investigate the impact of approximation error on convergence. These practical techniques offer a principled angle towards removing Shampoo's heuristics and developing improved Kronecker-factorization-based training algorithms.</article>","contentLength":1421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness","url":"https://arxiv.org/abs/2506.03089","date":1761796800,"author":"","guid":321270,"unread":true,"content":"<article>arXiv:2506.03089v2 Announce Type: replace \nAbstract: Convolutional neural networks (CNNs) trained on object recognition achieve high task performance but continue to exhibit vulnerability under a range of visual perturbations and out-of-domain images, when compared with biological vision. Prior work has demonstrated that coupling a standard CNN with a front-end (VOneBlock) that mimics the primate primary visual cortex (V1) can improve overall model robustness. Expanding on this, we introduce Early Vision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock with a novel SubcorticalBlock, whose architecture draws from computational models in neuroscience and is parameterized to maximize alignment with subcortical responses reported across multiple experimental studies. Without being optimized to do so, the assembly of the SubcorticalBlock with the VOneBlock improved V1 alignment across most standard V1 benchmarks, and better modeled extra-classical receptive field phenomena. In addition, EVNets exhibit stronger emergent shape bias and outperform the base CNN architecture by 9.3% on an aggregate benchmark of robustness evaluations, including adversarial perturbations, common corruptions, and domain shifts. Finally, we show that EVNets can be further improved when paired with a state-of-the-art data augmentation technique, surpassing the performance of the isolated data augmentation approach by 6.2% on our robustness benchmark. This result reveals complementary benefits between changes in architecture to better mimic biology and training-based machine learning approaches.</article>","contentLength":1614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization","url":"https://arxiv.org/abs/2506.02504","date":1761796800,"author":"","guid":321271,"unread":true,"content":"<article>arXiv:2506.02504v2 Announce Type: replace \nAbstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its coupled compositional objective structure, emerges as an important optimization paradigm for addressing a wide range of machine learning problems. In this paper, we focus on a challenging class of non-convex non-smooth FCCO, where the outer functions are non-smooth weakly convex or convex and the inner functions are smooth or weakly convex. Existing state-of-the-art result face two key limitations: (1) a high iteration complexity of $O(1/\\epsilon^6)$ under the assumption that the stochastic inner functions are Lipschitz continuous in expectation; (2) reliance on vanilla SGD-type updates, which are not suitable for deep learning applications. Our main contributions are two fold: (i) We propose stochastic momentum methods tailored for non-smooth FCCO that come with provable convergence guarantees; (ii) We establish a new state-of-the-art iteration complexity of $O(1/\\epsilon^5)$. Moreover, we apply our algorithms to multiple inequality constrained non-convex optimization problems involving smooth or weakly convex functional inequality constraints. By optimizing a smoothed hinge penalty based formulation, we achieve a new state-of-the-art complexity of $O(1/\\epsilon^5)$ for finding an (nearly) $\\epsilon$-level KKT solution. Experiments on three tasks demonstrate the effectiveness of the proposed algorithms.</article>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations","url":"https://arxiv.org/abs/2506.01367","date":1761796800,"author":"","guid":321272,"unread":true,"content":"<article>arXiv:2506.01367v3 Announce Type: replace \nAbstract: Large language models (LLMs) have become pervasive in our everyday life. Yet, a fundamental obstacle prevents their use in many critical applications: their propensity to generate fluent, human-quality content that is not grounded in reality. The detection of such hallucinations is thus of the highest importance. In this work, we propose a new method to flag hallucinated content: MMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric distance between distributions. On a high-level perspective, MMD-Flagger tracks the MMD between the output to inspect and counterparts generated with various temperature parameters. We show empirically that inspecting the shape of this trajectory is sufficient to detect most hallucinations. This novel method is benchmarked on machine translation and summarization datasets, on which it exhibits competitive performance relative to natural competitors.</article>","contentLength":964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Doubly Robust Alignment for Large Language Models","url":"https://arxiv.org/abs/2506.01183","date":1761796800,"author":"","guid":321273,"unread":true,"content":"<article>arXiv:2506.01183v2 Announce Type: replace \nAbstract: This paper studies reinforcement learning from human feedback (RLHF) for aligning large language models with human preferences. While RLHF has demonstrated promising results, many algorithms are highly sensitive to misspecifications in the underlying preference model (e.g., the Bradley-Terry model), the reference policy, or the reward function, resulting in undesirable fine-tuning. To address model misspecification, we propose a doubly robust preference optimization algorithm that remains consistent when either the preference model or the reference policy is correctly specified (without requiring both). Our proposal demonstrates superior and more robust performance than state-of-the-art algorithms, both in theory and in practice. The code is available at https://github.com/DRPO4LLM/DRPO4LLM</article>","contentLength":854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Q-learning with Posterior Sampling","url":"https://arxiv.org/abs/2506.00917","date":1761796800,"author":"","guid":321274,"unread":true,"content":"<article>arXiv:2506.00917v3 Announce Type: replace \nAbstract: Bayesian posterior sampling techniques have demonstrated superior empirical performance in many exploration-exploitation settings. However, their theoretical analysis remains a challenge, especially in complex settings like reinforcement learning. In this paper, we introduce Q-Learning with Posterior Sampling (PSQL), a simple Q-learning-based algorithm that uses Gaussian posteriors on Q-values for exploration, akin to the popular Thompson Sampling algorithm in the multi-armed bandit setting. We show that in the tabular episodic MDP setting, PSQL achieves a regret bound of $\\tilde O(H^2\\sqrt{SAT})$, closely matching the known lower bound of $\\Omega(H\\sqrt{SAT})$. Here, S, A denote the number of states and actions in the underlying Markov Decision Process (MDP), and $T=KH$ with $K$ being the number of episodes and $H$ being the planning horizon. Our work provides several new technical insights into the core challenges in combining posterior sampling with dynamic programming and TD-learning-based RL algorithms, along with novel ideas for resolving those difficulties. We hope this will form a starting point for analyzing this efficient and important algorithmic technique in even more complex RL settings.</article>","contentLength":1272,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting","url":"https://arxiv.org/abs/2506.00635","date":1761796800,"author":"","guid":321275,"unread":true,"content":"<article>arXiv:2506.00635v2 Announce Type: replace \nAbstract: Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a novel test-time computing paradigm, namely learning with calibration, ST-TTC, for spatio-temporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. ST-TTC effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLMs are Better Than You Think: Label-Guided In-Context Learning for Named Entity Recognition","url":"https://arxiv.org/abs/2505.23722","date":1761796800,"author":"","guid":321276,"unread":true,"content":"<article>arXiv:2505.23722v2 Announce Type: replace \nAbstract: In-context learning (ICL) enables large language models (LLMs) to perform new tasks using only a few demonstrations. However, in Named Entity Recognition (NER), existing ICL methods typically rely on task-agnostic semantic similarity for demonstration retrieval, which often yields less relevant examples and leads to inferior results. We introduce DEER, a training-free ICL approach that enables LLMs to make more informed entity predictions through the use of label-grounded statistics. DEER leverages token-level statistics from training labels to identify tokens most informative for entity recognition, enabling entity-focused demonstrations. It further uses these statistics to detect and refine error-prone tokens through a targeted reflection step. Evaluated on five NER datasets across four LLMs, DEER consistently outperforms existing ICL methods and achieves performance comparable to supervised fine-tuning. Further analyses demonstrate that DEER improves example retrieval, remains effective on both seen and unseen entities, and exhibits strong robustness in low-resource settings.</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking","url":"https://arxiv.org/abs/2505.23117","date":1761796800,"author":"","guid":321277,"unread":true,"content":"<article>arXiv:2505.23117v2 Announce Type: replace \nAbstract: In the era of large-scale training, model merging has evolved into a tool for creating multitasking models efficiently. It enables the knowledge of models to be fused, without the need for heavy computation as required in traditional multitask learning. Existing merging methods often assume that entries at identical positions in weight matrices serve the same function, enabling straightforward entry-wise comparison and merging. However, this assumption overlooks the complexity of finetuned neural networks, where neurons may develop distinct feature compositions, making direct entry-wise merging problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective approach that leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, where entry-wise merging becomes possible. We showcase the effectiveness of DRM across various settings ranging from smaller encoder-based such as ViT and DeBERTa, encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B. Our experimental results show that DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings. Moreover, our analysis reveals renormalization as the crucial component for creating a robust and even joint space for merging, significantly contributing to the method's performance.</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape","url":"https://arxiv.org/abs/2505.22918","date":1761796800,"author":"","guid":321278,"unread":true,"content":"<article>arXiv:2505.22918v4 Announce Type: replace \nAbstract: Diffusion Transformers (DiT) have become the de-facto model for generating high-quality visual content like videos and images. A huge bottleneck is the attention mechanism where complexity scales quadratically with resolution and video length. One logical way to lessen this burden is sparse attention, where only a subset of tokens or patches are included in the calculation. However, existing techniques fail to preserve visual quality at extremely high sparsity levels and might even incur non-negligible compute overheads. To address this concern, we propose Re-ttention, which implements very high sparse attention for visual generation models by leveraging the temporal redundancy of Diffusion Models to overcome the probabilistic normalization shift within the attention mechanism. Specifically, Re-ttention reshapes attention scores based on the prior softmax distribution history in order to preserve the visual quality of the full quadratic attention at very high sparsity levels. Experimental results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate that Re-ttention requires as few as 3.1% of the tokens during inference, outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and MInference.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Precise In-Parameter Concept Erasure in Large Language Models","url":"https://arxiv.org/abs/2505.22586","date":1761796800,"author":"","guid":321279,"unread":true,"content":"<article>arXiv:2505.22586v2 Announce Type: replace \nAbstract: Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES (Precise In-parameter Suppression for Concept EraSure), a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 38%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models.</article>","contentLength":1328,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling","url":"https://arxiv.org/abs/2505.21717","date":1761796800,"author":"","guid":321280,"unread":true,"content":"<article>arXiv:2505.21717v5 Announce Type: replace \nAbstract: We present LrcSSM, a $\\textit{non-linear}$ recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing its Jacobian matrix to be diagonal, the full sequence can be solved in parallel, giving $\\mathcal{O}(TD)$ time and memory and only $\\mathcal{O}(\\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Importantly, the diagonal Jacobian structure of our model results in no performance loss compared to the original model with dense Jacobian, and the approach can be generalized to other non-linear recurrent models, demonstrating broader applicability. On a suite of long-range forecasting tasks, we demonstrate that LrcSSM outperforms Transformers, LRU, S5, and Mamba.</article>","contentLength":934,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing","url":"https://arxiv.org/abs/2505.21671","date":1761796800,"author":"","guid":321281,"unread":true,"content":"<article>arXiv:2505.21671v3 Announce Type: replace \nAbstract: We study a sequential decision-making problem on a $n$-node graph $\\mathcal{G}$ where each node has an unknown label from a finite set $\\mathbf{\\Omega}$, drawn from a joint distribution $\\mathcal{P}$ that is Markov with respect to $\\mathcal{G}$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $\\mathcal{G}$ is a forest. Our implementation runs in $\\mathcal{O}(n^2 \\cdot |\\mathbf{\\Omega}|^2)$ time while using $\\mathcal{O}(n \\cdot |\\mathbf{\\Omega}|^2)$ oracle calls to $\\mathcal{P}$ and $\\mathcal{O}(n^2 \\cdot |\\mathbf{\\Omega}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probabilistic Kernel Function for Fast Angle Testing","url":"https://arxiv.org/abs/2505.20274","date":1761796800,"author":"","guid":321282,"unread":true,"content":"<article>arXiv:2505.20274v2 Announce Type: replace \nAbstract: In this paper, we study the angle testing problem in the context of similarity search in high-dimensional Euclidean spaces and propose two projection-based probabilistic kernel functions, one designed for angle comparison and the other for angle thresholding. Unlike existing approaches that rely on random projection vectors drawn from Gaussian distributions, our approach leverages reference angles and employs a deterministic structure for the projection vectors. Notably, our kernel functions do not require asymptotic assumptions, such as the number of projection vectors tending to infinity, and can be both theoretically and experimentally shown to outperform Gaussian-distribution-based kernel functions. We apply the proposed kernel function to Approximate Nearest Neighbor Search (ANNS) and demonstrate that our approach achieves a 2.5X ~ 3X higher query-per-second (QPS) throughput compared to the widely-used graph-based search algorithm HNSW.</article>","contentLength":1008,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models","url":"https://arxiv.org/abs/2505.20249","date":1761796800,"author":"","guid":321283,"unread":true,"content":"<article>arXiv:2505.20249v2 Announce Type: replace \nAbstract: Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.</article>","contentLength":1209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment","url":"https://arxiv.org/abs/2505.19638","date":1761796800,"author":"","guid":321284,"unread":true,"content":"<article>arXiv:2505.19638v3 Announce Type: replace \nAbstract: Virtual try-on technology has become increasingly important in the fashion and retail industries, enabling the generation of high-fidelity garment images that adapt seamlessly to target human models. While existing methods have achieved notable progress, they still face significant challenges in maintaining consistency across different poses. Specifically, geometric distortions lead to a lack of spatial consistency, mismatches in garment structure and texture across poses result in semantic inconsistency, and the loss or distortion of fine-grained details diminishes visual fidelity. To address these challenges, we propose HF-VTON, a novel framework that ensures high-fidelity virtual try-on performance across diverse poses. HF-VTON consists of three key modules: (1) the Appearance-Preserving Warp Alignment Module (APWAM), which aligns garments to human poses, addressing geometric deformations and ensuring spatial consistency; (2) the Semantic Representation and Comprehension Module (SRCM), which captures fine-grained garment attributes and multi-pose data to enhance semantic representation, maintaining structural, textural, and pattern consistency; and (3) the Multimodal Prior-Guided Appearance Generation Module (MPAGM), which integrates multimodal features and prior knowledge from pre-trained models to optimize appearance generation, ensuring both semantic and geometric consistency. Additionally, to overcome data limitations in existing benchmarks, we introduce the SAMP-VTONS dataset, featuring multi-pose pairs and rich textual annotations for a more comprehensive evaluation. Experimental results demonstrate that HF-VTON outperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling in visual fidelity, semantic consistency, and detail preservation.</article>","contentLength":1848,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning-Augmented Online Bipartite Fractional Matching","url":"https://arxiv.org/abs/2505.19252","date":1761796800,"author":"","guid":321285,"unread":true,"content":"<article>arXiv:2505.19252v2 Announce Type: replace \nAbstract: Online bipartite matching is a fundamental problem in online optimization, extensively studied both in its integral and fractional forms due to its theoretical significance and practical applications, such as online advertising and resource allocation. Motivated by recent progress in learning-augmented algorithms, we study online bipartite fractional matching when the algorithm is given advice in the form of a suggested matching in each iteration. We develop algorithms for both the vertex-weighted and unweighted variants that provably dominate the naive \"coin flip\" strategy of randomly choosing between the advice-following and advice-free algorithms. Moreover, our algorithm for the vertex-weighted setting extends to the AdWords problem under the small bids assumption, yielding a significant improvement over the seminal work of Mahdian, Nazerzadeh, and Saberi (EC 2007, TALG 2012). Complementing our positive results, we establish a hardness bound on the robustness-consistency tradeoff that is attainable by any algorithm. We empirically validate our algorithms through experiments on synthetic and real-world data.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts","url":"https://arxiv.org/abs/2505.19028","date":1761796800,"author":"","guid":321286,"unread":true,"content":"<article>arXiv:2505.19028v4 Announce Type: replace \nAbstract: Understanding infographic charts with design-driven visual elements (e.g., pictograms, icons) requires both visual recognition and reasoning, posing challenges for multimodal large language models (MLLMs). However, existing visual-question answering benchmarks fall short in evaluating these capabilities of MLLMs due to the lack of paired plain charts and visual-element-based questions. To bridge this gap, we introduce InfoChartQA, a benchmark for evaluating MLLMs on infographic chart understanding. It includes 5,642 pairs of infographic and plain charts, each sharing the same underlying data but differing in visual presentations. We further design visual-element-based questions to capture their unique visual designs and communicative intent. Evaluation of 20 MLLMs reveals a substantial performance decline on infographic charts, particularly for visual-element-based questions related to metaphors. The paired infographic and plain charts enable fine-grained error analysis and ablation studies, which highlight new opportunities for advancing MLLMs in infographic chart understanding. We release InfoChartQA at https://github.com/CoolDawnAnt/InfoChartQA.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Risk Assessments for Offensive Cybersecurity Agents","url":"https://arxiv.org/abs/2505.18384","date":1761796800,"author":"","guid":321287,"unread":true,"content":"<article>arXiv:2505.18384v4 Announce Type: replace \nAbstract: Foundation models are increasingly becoming better autonomous programmers, raising the prospect that they could also automate dangerous offensive cyber-operations. Current frontier model audits probe the cybersecurity risks of such agents, but most fail to account for the degrees of freedom available to adversaries in the real world. In particular, with strong verifiers and financial incentives, agents for offensive cybersecurity are amenable to iterative improvement by would-be adversaries. We argue that assessments should take into account an expanded threat model in the context of cybersecurity, emphasizing the varying degrees of freedom that an adversary may possess in stateful and non-stateful environments within a fixed compute budget. We show that even with a relatively small compute budget (8 H100 GPU Hours in our study), adversaries can improve an agent's cybersecurity capability on InterCode CTF by more than 40\\% relative to the baseline -- without any external assistance. These results highlight the need to evaluate agents' cybersecurity risk in a dynamic manner, painting a more representative picture of risk.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient Interactions","url":"https://arxiv.org/abs/2505.17818","date":1761796800,"author":"","guid":321288,"unread":true,"content":"<article>arXiv:2505.17818v2 Announce Type: replace \nAbstract: Doctor-patient consultations require multi-turn, context-aware communication tailored to diverse patient personas. Training or evaluating doctor LLMs in such settings requires realistic patient interaction systems. However, existing simulators often fail to reflect the full range of personas seen in clinical practice. To address this, we introduce PatientSim, a patient simulator that generates realistic and diverse patient personas for clinical scenarios, grounded in medical expertise. PatientSim operates using: 1) clinical profiles, including symptoms and medical history, derived from real-world data in the MIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes: personality, language proficiency, medical history recall level, and cognitive confusion level, resulting in 37 unique combinations. We evaluate eight LLMs for factual accuracy and persona consistency. The top-performing open-source model, Llama 3.3 70B, is validated by four clinicians to confirm the robustness of our framework. As an open-source, customizable platform, PatientSim provides a reproducible and scalable solution that can be customized for specific training needs. Offering a privacy-compliant environment, it serves as a robust testbed for evaluating medical dialogue systems across diverse patient presentations and shows promise as an educational tool for healthcare. The code is available at https://github.com/dek924/PatientSim.</article>","contentLength":1489,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Counterfactual Simulations with Language Models for Explaining Multi-Agent Behaviour","url":"https://arxiv.org/abs/2505.17801","date":1761796800,"author":"","guid":321289,"unread":true,"content":"<article>arXiv:2505.17801v2 Announce Type: replace \nAbstract: Autonomous multi-agent systems (MAS) are useful for automating complex tasks but raise trust concerns due to risks such as miscoordination or goal misalignment. Explainability is vital for users' trust calibration, but explainable MAS face challenges due to complex environments, the human factor, and non-standardised evaluation. Leveraging the counterfactual effect size model and LLMs, we propose Agentic eXplanations via Interrogative Simulation (AXIS). AXIS generates human-centred action explanations for multi-agent policies by having an LLM interrogate an environment simulator using prompts like 'whatif' and 'remove' to observe and synthesise counterfactual information over multiple rounds. We evaluate AXIS on autonomous driving across ten scenarios for five LLMs with a comprehensive methodology combining robustness, subjective preference, correctness, and goal/action prediction with an external LLM as evaluator. Compared to baselines, AXIS improves perceived explanation correctness by at least 7.7% across all models and goal prediction accuracy by 23% for four models, with comparable action prediction accuracy, achieving the highest scores overall. Our code is open-sourced at https://github.com/gyevnarb/axis.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving","url":"https://arxiv.org/abs/2505.17685","date":1761796800,"author":"","guid":321290,"unread":true,"content":"<article>arXiv:2505.17685v2 Announce Type: replace \nAbstract: Vision-Language-Action (VLA) models are increasingly used for end-to-end driving due to their world knowledge and reasoning ability. Most prior work, however, inserts textual chains-of-thought (CoT) as intermediate steps tailored to the current scene. Such symbolic compressions can blur spatio-temporal relations and discard fine visual cues, creating a cross-modal gap between perception and planning. We propose FSDrive, a visual spatio-temporal CoT framework that enables VLAs to think in images. The model first acts as a world model to generate a unified future frame that overlays coarse but physically-plausible priors-future lane dividers and 3D boxes-on the predicted future image. This unified frame serves as the visual CoT, capturing both spatial structure and temporal evolution. The same VLA then functions as an inverse-dynamics model, planning trajectories from current observations and the visual CoT. To equip VLAs with image generation while preserving understanding, we introduce a unified pre-training paradigm that expands the vocabulary to include visual tokens and jointly optimizes VQA (for semantics) and future-frame prediction (for dynamics). A progressive easy-to-hard scheme first predicts lane/box priors to enforce physical constraints, then completes full future frames for fine details. On nuScenes and NAVSIM, FSDrive improves trajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics, and attains competitive FID for future-frame generation despite using lightweight autoregression. It also advances scene understanding on DriveLM. Together, these results indicate that visual CoT narrows the cross-modal gap and yields safer, more anticipatory planning. Code is available at https://github.com/MIV-XJTU/FSDrive.</article>","contentLength":1822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models","url":"https://arxiv.org/abs/2505.16854","date":1761796800,"author":"","guid":321291,"unread":true,"content":"<article>arXiv:2505.16854v3 Announce Type: replace \nAbstract: Reinforcement Learning (RL) has proven to be an effective post-training strategy for enhancing reasoning in vision-language models (VLMs). Group Relative Policy Optimization (GRPO) is a recent prominent method that encourages models to generate complete reasoning traces before answering, leading to increased token usage and computational cost. Inspired by the human-like thinking process-where people skip reasoning for easy questions but think carefully when needed-we explore how to enable VLMs to first decide when reasoning is necessary. To realize this, we propose TON, a two-stage training strategy: (i) a supervised fine-tuning (SFT) stage with a simple yet effective 'thought dropout' operation, where reasoning traces are randomly replaced with empty thoughts. This introduces a think-or-not format that serves as a cold start for selective reasoning; (ii) a GRPO stage that enables the model to freely explore when to think or not, while maximizing task-aware outcome rewards. Experimental results show that TON can reduce the completion length by up to 90% compared to vanilla GRPO, without sacrificing performance or even improving it. Further evaluations across LLM (GSM8K), VLM (CLEVR, Super-CLEVR, GeoQA), and Agentic (AITZ) tasks-covering a range of reasoning difficulties under both 3B and 7B models-consistently reveal that the model progressively learns to bypass unnecessary reasoning steps as training advances. These findings shed light on the path toward human-like reasoning patterns in RL approaches. Our code is available at https://github.com/kokolerk/TON.</article>","contentLength":1638,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning","url":"https://arxiv.org/abs/2505.16368","date":1761796800,"author":"","guid":321292,"unread":true,"content":"<article>arXiv:2505.16368v2 Announce Type: replace \nAbstract: How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard.\n  To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions.\n  We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8%. We release the source code, data, and models to support future research.</article>","contentLength":1965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging","url":"https://arxiv.org/abs/2505.15356","date":1761796800,"author":"","guid":321293,"unread":true,"content":"<article>arXiv:2505.15356v2 Announce Type: replace \nAbstract: Debugging is a critical aspect of LLM's coding ability. Early debugging efforts primarily focused on code-level analysis, which often falls short when addressing complex programming errors that require a deeper understanding of algorithmic logic. Recent advancements in large language models (LLMs) have shifted attention toward leveraging natural language reasoning to enhance code-related tasks. However, two fundamental questions remain unanswered: What type of natural language format is most effective for debugging tasks? And what specific benefits does natural language reasoning bring to the debugging process? In this paper, we introduce NL-DEBUGGING, a novel framework that employs natural language as an intermediate representation to improve code debugging. By debugging at a natural language level, we demonstrate that NL-DEBUGGING outperforms traditional debugging methods and enables a broader modification space through direct refinement guided by execution feedback. Our findings highlight the potential of natural language reasoning to advance automated code debugging and address complex programming challenges.</article>","contentLength":1183,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems","url":"https://arxiv.org/abs/2505.15201","date":1761796800,"author":"","guid":321294,"unread":true,"content":"<article>arXiv:2505.15201v2 Announce Type: replace \nAbstract: Reinforcement Learning (RL) algorithms sample multiple n&gt;1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k &lt;= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.</article>","contentLength":1970,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking","url":"https://arxiv.org/abs/2505.15063","date":1761796800,"author":"","guid":321295,"unread":true,"content":"<article>arXiv:2505.15063v2 Announce Type: replace \nAbstract: The rapid adoption of Large Language Models (LLMs) has raised important concerns about the factual reliability of their outputs, particularly in low-resource languages such as Urdu. Existing automated fact-checking systems are predominantly developed for English, leaving a significant gap for the more than 200 million Urdu speakers worldwide. In this work, we present UrduFactBench and UrduFactQA, two novel hand-annotated benchmarks designed to enable fact-checking and factual consistency evaluation in Urdu. While UrduFactBench focuses on claim verification, UrduFactQA targets the factuality of LLMs in question answering. These resources, the first of their kind for Urdu, were developed through a multi-stage annotation process involving native Urdu speakers. To complement these benchmarks, we introduce UrduFactCheck, a modular fact-checking framework that incorporates both monolingual and translation-based evidence retrieval strategies to mitigate the scarcity of high-quality Urdu evidence. Leveraging these resources, we conduct an extensive evaluation of twelve LLMs and demonstrate that translation-augmented pipelines consistently enhance performance compared to monolingual ones. Our findings reveal persistent challenges for open-source LLMs in Urdu and underscore the importance of developing targeted resources. All code and data are publicly available at https://github.com/mbzuai-nlp/UrduFactCheck.</article>","contentLength":1475,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation","url":"https://arxiv.org/abs/2505.13111","date":1761796800,"author":"","guid":321296,"unread":true,"content":"<article>arXiv:2505.13111v2 Announce Type: replace \nAbstract: Knowledge distillation (KD) is a core component in the training and deployment of modern generative models, particularly large language models (LLMs). While its empirical benefits are well documented -- enabling smaller student models to emulate the performance of much larger teachers -- the underlying mechanisms by which KD improves generative quality remain poorly understood. In this work, we present a minimal working explanation of KD in generative modeling. Using a controlled simulation with mixtures of Gaussians, we demonstrate that distillation induces a trade-off between precision and recall in the student model. As the teacher distribution becomes more selective, the student concentrates more probability mass on high-likelihood regions at the expense of coverage -- a behavior modulated by a single entropy-controlling parameter. We then validate this effect in a large-scale language modeling setup using the SmolLM2 family of models. Empirical results reveal the same precision-recall dynamics observed in simulation, where precision corresponds to sample quality and recall to distributional coverage. This precision-recall trade-off in LLMs is found to be especially beneficial in scenarios where sample quality is more important than diversity, such as instruction tuning or downstream generation. Our analysis provides a simple and general explanation for the effectiveness of KD in generative modeling.</article>","contentLength":1480,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A method for the systematic generation of graph XAI benchmarks via Weisfeiler-Leman coloring","url":"https://arxiv.org/abs/2505.12437","date":1761796800,"author":"","guid":321297,"unread":true,"content":"<article>arXiv:2505.12437v2 Announce Type: replace \nAbstract: Graph neural networks have become the de facto model for learning from structured data. However, the decision-making process of GNNs remains opaque to the end user, which undermines their use in safety-critical applications. Several explainable AI techniques for graphs have been developed to address this major issue. Focusing on graph classification, these explainers identify subgraph motifs that explain predictions. Therefore, a robust benchmarking of graph explainers is required to ensure that the produced explanations are of high quality, i.e., aligned with the GNN's decision process. However, current graph-XAI benchmarks are limited to simplistic synthetic datasets or a few real-world tasks curated by domain experts, hindering rigorous and reproducible evaluation, and consequently stalling progress in the field. To overcome these limitations, we propose a method to automate the construction of graph XAI benchmarks from generic graph classification datasets. Our approach leverages the Weisfeiler-Leman color refinement algorithm to efficiently perform approximate subgraph matching and mine class-discriminating motifs, which serve as proxy ground-truth class explanations. At the same time, we ensure that these motifs can be learned by GNNs because their discriminating power aligns with WL expressiveness. This work also introduces the OpenGraphXAI benchmark suite, which consists of 15 ready-made graph-XAI datasets derived by applying our method to real-world molecular classification datasets. The suite is available to the public along with a codebase to generate over 2,000 additional graph-XAI benchmarks. Finally, we present a use case that illustrates how the suite can be used to assess the effectiveness of a selection of popular graph explainers, demonstrating the critical role of a sufficiently large benchmark collection for improving the significance of experimental results.</article>","contentLength":1964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Federated Deep Reinforcement Learning for Privacy-Preserving Robotic-Assisted Surgery","url":"https://arxiv.org/abs/2505.12153","date":1761796800,"author":"","guid":321298,"unread":true,"content":"<article>arXiv:2505.12153v2 Announce Type: replace \nAbstract: The integration of Reinforcement Learning (RL) into robotic-assisted surgery (RAS) holds significant promise for advancing surgical precision, adaptability, and autonomous decision-making. However, the development of robust RL models in clinical settings is hindered by key challenges, including stringent patient data privacy regulations, limited access to diverse surgical datasets, and high procedural variability. To address these limitations, this paper presents a Federated Deep Reinforcement Learning (FDRL) framework that enables decentralized training of RL models across multiple healthcare institutions without exposing sensitive patient information. A central innovation of the proposed framework is its dynamic policy adaptation mechanism, which allows surgical robots to select and tailor patient-specific policies in real-time, thereby ensuring personalized and Optimised interventions. To uphold rigorous privacy standards while facilitating collaborative learning, the FDRL framework incorporates secure aggregation, differential privacy, and homomorphic encryption techniques. Experimental results demonstrate a 60\\% reduction in privacy leakage compared to conventional methods, with surgical precision maintained within a 1.5\\% margin of a centralized baseline. This work establishes a foundational approach for adaptive, secure, and patient-centric AI-driven surgical robotics, offering a pathway toward clinical translation and scalable deployment across diverse healthcare environments.</article>","contentLength":1562,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Who You Are Matters: Bridging Topics and Social Roles via LLM-Enhanced Logical Recommendation","url":"https://arxiv.org/abs/2505.10940","date":1761796800,"author":"","guid":321299,"unread":true,"content":"<article>arXiv:2505.10940v3 Announce Type: replace \nAbstract: Recommender systems filter contents/items valuable to users by inferring preferences from user features and historical behaviors. Mainstream approaches follow the learning-to-rank paradigm, which focus on discovering and modeling item topics (e.g., categories), and capturing user preferences on these topics based on historical interactions. However, this paradigm often neglects the modeling of user characteristics and their social roles, which are logical confounders influencing the correlated interest and user preference transition. To bridge this gap, we introduce the user role identification task and the behavioral logic modeling task that aim to explicitly model user roles and learn the logical relations between item topics and user social roles. We show that it is possible to explicitly solve these tasks through an efficient integration framework of Large Language Model (LLM) and recommendation systems, for which we propose TagCF. On the one hand, TagCF exploits the (Multi-modal) LLM's world knowledge and logic inference ability to extract realistic tag-based virtual logic graphs that reveal dynamic and expressive knowledge of users, refining our understanding of user behaviors. On the other hand, TagCF presents empirically effective integration modules that take advantage of the extracted tag-logic information, augmenting the recommendation performance. We conduct both online experiments and offline experiments with industrial and public datasets as verification of TagCF's effectiveness, and we empirically show that the user role modeling strategy is potentially a better choice than the modeling of item topics. Additionally, we provide evidence that the extracted logic graphs are empirically a general and transferable knowledge that can benefit a wide range of recommendation tasks. Our code is available in https://github.com/Code2Q/TagCF.</article>","contentLength":1929,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models","url":"https://arxiv.org/abs/2505.10844","date":1761796800,"author":"","guid":321300,"unread":true,"content":"<article>arXiv:2505.10844v4 Announce Type: replace \nAbstract: Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) generating solutions from these mathematical forms; (3) self-correcting solutions based on gold solutions; (4) producing step-by-step sketches of solutions; and (5) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To what extent can current French mobile network support agricultural robots?","url":"https://arxiv.org/abs/2505.10044","date":1761796800,"author":"","guid":321301,"unread":true,"content":"<article>arXiv:2505.10044v4 Announce Type: replace \nAbstract: The large-scale integration of robots in agriculture offers many promises for enhancing sustainability and increasing food production. The numerous applications of agricultural robots rely on the transmission of data via mobile network, with the amount of data depending on the services offered by the robots and the level of on-board technology. Nevertheless, infrastructure required to deploy these robots, as well as the related energy and environmental consequences, appear overlooked in the digital agriculture literature. In this study, we propose a method for assessing the additional energy consumption and carbon footprint induced by a large-scale deployment of agricultural robots. Our method also estimates the share of agricultural area that can be managed by the deployed robots with respect to network infrastructure constraints. We have applied this method to metropolitan France mobile network and agricultural parcels for five different robotic scenarios. Our results show that increasing the robot's bitrate needs leads to significant additional impacts, which increase at a pace that is poorly captured by classical linear extrapolation methods. When constraining the network to the existing sites, increased bitrate needs also comes with a rapidly decreasing manageable agricultural area.</article>","contentLength":1361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Which Demographic Features Are Relevant for Individual Fairness Evaluation of U.S. Recidivism Risk Assessment Tools?","url":"https://arxiv.org/abs/2505.09868","date":1761796800,"author":"","guid":321302,"unread":true,"content":"<article>arXiv:2505.09868v3 Announce Type: replace \nAbstract: Despite its constitutional relevance, the technical ``individual fairness'' criterion has not been operationalized in U.S. state or federal statutes/regulations. We conduct a human subjects experiment to address this gap, evaluating which demographic features are relevant for individual fairness evaluation of recidivism risk assessment (RRA) tools. Our analyses conclude that the individual similarity function should consider age and sex, but it should ignore race.</article>","contentLength":521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"10 quick tips for making your software outlive your job","url":"https://arxiv.org/abs/2505.06484","date":1761796800,"author":"","guid":321303,"unread":true,"content":"<article>arXiv:2505.06484v2 Announce Type: replace \nAbstract: Loss of key personnel has always been a risk for research software projects. Key members of the team may have to step away due to illness or burnout, to care for a family member, from a loss of financial support, or because their career is going in a new direction. Today, though, political and financial changes are putting large numbers of researchers out of work simultaneously, potentially leaving large amounts of research software abandoned. This article presents ten tips to help researchers ensure that the software they have built will continue to be usable after they have left their present job -- whether in the course of voluntary career moves or researcher mobility, but particularly in cases of involuntary departure due to political or institutional changes.</article>","contentLength":827,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Timestamp Manipulation: Timestamp-based Nakamoto-style Blockchains are Vulnerable","url":"https://arxiv.org/abs/2505.05328","date":1761796800,"author":"","guid":321304,"unread":true,"content":"<article>arXiv:2505.05328v5 Announce Type: replace \nAbstract: Nakamoto consensus are the most widely adopted decentralized consensus mechanism in cryptocurrency systems. Since it was proposed in 2008, many studies have focused on analyzing its security. Most of them focus on maximizing the profit of the adversary. Examples include the selfish mining attack [FC '14] and the recent riskless uncle maker (RUM) attack [CCS '23]. In this work, we introduce the Staircase-Unrestricted Uncle Maker (SUUM), the first block withholding attack targeting the timestamp-based Nakamoto-style blockchain. Through block withholding, timestamp manipulation, and difficulty risk control, SUUM adversaries are capable of launching persistent attacks with zero cost and minimal difficulty risk characteristics, indefinitely exploiting rewards from honest participants. This creates a self-reinforcing cycle that threatens the security of blockchains. We conduct a comprehensive and systematic evaluation of SUUM, including the attack conditions, its impact on blockchains, and the difficulty risks. Finally, we further discuss four feasible mitigation measures against SUUM.</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Plexus: Taming Billion-edge Graphs with 3D Parallel Full-graph GNN Training","url":"https://arxiv.org/abs/2505.04083","date":1761796800,"author":"","guid":321305,"unread":true,"content":"<article>arXiv:2505.04083v2 Announce Type: replace \nAbstract: Graph neural networks (GNNs) leverage the connectivity and structure of real-world graphs to learn intricate properties and relationships between nodes. Many real-world graphs exceed the memory capacity of a GPU due to their sheer size, and training GNNs on such graphs requires techniques such as mini-batch sampling to scale. The alternative approach of distributed full-graph training suffers from high communication overheads and load imbalance due to the irregular structure of graphs. We propose a three-dimensional (3D) parallel approach for full-graph training that tackles these issues and scales to billion-edge graphs. In addition, we introduce optimizations such as a double permutation scheme for load balancing, and a performance model to predict the optimal 3D configuration of our parallel implementation -- Plexus. We evaluate Plexus on six different graph datasets and show scaling results on up to 2048 GPUs of Perlmutter, and 1024 GPUs of Frontier. Plexus achieves unprecedented speedups of 2.3-12.5x over prior state of the art, and a reduction in time-to-solution by 5.2-8.7x on Perlmutter and 7.0-54.2x on Frontier.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning","url":"https://arxiv.org/abs/2505.03318","date":1761796800,"author":"","guid":321306,"unread":true,"content":"<article>arXiv:2505.03318v3 Announce Type: replace \nAbstract: Recent advances in multimodal Reward Models (RMs) have shown significant promise in delivering reward signals to align vision models with human preferences. However, current RMs are generally restricted to providing direct responses or engaging in shallow reasoning processes with limited depth, often leading to inaccurate reward signals. We posit that incorporating explicit long chains of thought (CoT) into the reward reasoning process can significantly strengthen their reliability and robustness. Furthermore, we believe that once RMs internalize CoT reasoning, their direct response accuracy can also be improved through implicit reasoning capabilities. To this end, this paper proposes UnifiedReward-Think, the first unified multimodal CoT-based reward model, capable of multi-dimensional, step-by-step long-chain reasoning for both visual understanding and generation reward tasks. Specifically, we adopt an exploration-driven reinforcement fine-tuning approach to elicit and incentivize the model's latent complex reasoning ability: (1) We first use a small amount of image generation preference data to distill the reasoning process of GPT-4o, which is then used for the model's cold start to learn the format and structure of CoT reasoning. (2) Subsequently, by leveraging the model's prior knowledge and generalization capabilities, we prepare large-scale unified multimodal preference data to elicit the model's reasoning process across various vision tasks. During this phase, correct reasoning outputs are retained for rejection sampling to refine the model (3) while incorrect predicted samples are finally used for Group Relative Policy Optimization (GRPO) based reinforcement fine-tuning, enabling the model to explore diverse reasoning paths and optimize for correct and robust solutions. Extensive experiments across various vision reward tasks demonstrate the superiority of our model.</article>","contentLength":1960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MDPs with a State Sensing Cost","url":"https://arxiv.org/abs/2505.03280","date":1761796800,"author":"","guid":321307,"unread":true,"content":"<article>arXiv:2505.03280v2 Announce Type: replace \nAbstract: In many practical sequential decision-making problems, tracking the state of the environment incurs a sensing/communication/computation cost. In these settings, the agent's interaction with its environment includes the additional component of deciding when to sense the state, in a manner that balances the value associated with optimal (state-specific) actions and the cost of sensing. We formulate this as an expected discounted cost Markov Decision Process (MDP), wherein the agent incurs an additional cost for sensing its next state, but has the option to take actions while remaining `blind' to the system state. We pose this problem as a classical discounted cost MDP with an expanded (countably infinite) state space. While computing the optimal policy for this MDP is intractable in general, we derive lower bounds on the optimal value function, which allow us to bound the suboptimality gap of any policy. We also propose a computationally efficient algorithm SPI, based on policy improvement, which in practice performs close to the optimal policy. Finally, we benchmark against the state-of-the-art via a numerical case study.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Krylov methods for linear response in plane-wave electronic structure calculations","url":"https://arxiv.org/abs/2505.02319","date":1761796800,"author":"","guid":321308,"unread":true,"content":"<article>arXiv:2505.02319v2 Announce Type: replace \nAbstract: We propose a novel algorithm based on inexact GMRES methods for linear response calculations in density functional theory. Such calculations require iteratively solving a nested linear problem $\\mathcal{E} \\delta\\rho = b$ to obtain the variation of the electron density $\\delta \\rho$. Notably each application of the dielectric operator $\\mathcal{E}$ in turn requires the iterative solution of multiple linear systems, the Sternheimer equations. We develop computable bounds to estimate the accuracy of the density variation given the tolerances to which the Sternheimer equations have been solved. Based on this result we suggest reliable strategies for adaptively selecting the convergence tolerances of the Sternheimer equations, such that each application of $\\mathcal{E}$ is no more accurate than needed. Experiments on challenging materials systems of practical relevance demonstrate our strategies to achieve superlinear convergence as well as a reduction of computational time by about 40% while preserving the accuracy of the returned response solution. Our algorithm seamlessly combines with standard preconditioning approaches known from the context of self-consistent field problems making it a promising framework for efficient response solvers based on Krylov subspace techniques.</article>","contentLength":1347,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization","url":"https://arxiv.org/abs/2505.00812","date":1761796800,"author":"","guid":321309,"unread":true,"content":"<article>arXiv:2505.00812v4 Announce Type: replace \nAbstract: Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.</article>","contentLength":1253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partial integration based regularization in BEM for 3D elastostatic problems: The role of line integrals","url":"https://arxiv.org/abs/2505.00713","date":1761796800,"author":"","guid":321310,"unread":true,"content":"<article>arXiv:2505.00713v2 Announce Type: replace \nAbstract: The Boundary Element Method (BEM) is a powerful numerical approach for solving 3D elastostatic problems, particularly useful for crack propagation in fracture mechanics and half-space problems. A key challenge in BEM lies in handling singular integral kernels. Various analytical and numerical integration or regularization techniques address this, including one that combines partial integration with Stokes' theorem to reduce hyper-singular and strong singular kernels to weakly singular ones. This approach typically assumes a closed surface, omitting the boundary integrals from Stokes' theorem. In this paper, these usually neglected boundary line integrals are introduced and their significance is demonstrated, first in a pure half-space problem, and then shown to be redundant in fast multipole method (FMM) based BEM, where geometry partitioning produces pseudo open surfaces.</article>","contentLength":938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance","url":"https://arxiv.org/abs/2504.21497","date":1761796800,"author":"","guid":321311,"unread":true,"content":"<article>arXiv:2504.21497v3 Announce Type: replace \nAbstract: In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.</article>","contentLength":1709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic r-index: An Updatable Self-Index in LCP-bounded Time","url":"https://arxiv.org/abs/2504.19482","date":1761796800,"author":"","guid":321312,"unread":true,"content":"<article>arXiv:2504.19482v4 Announce Type: replace \nAbstract: A self-index is a compressed data structure that supports locate queries -- reporting all positions where a given pattern occurs in a string while maintaining the string in compressed form. While many self-indexes have been proposed, developing dynamically updatable ones supporting string insertions and deletions remains a challenge. The r-index (Gagie et al., JACM'20) is a representative static self-index based on the run-length Burrows-Wheeler transform (RLBWT), designed for highly repetitive strings. We present the dynamic r-index, a dynamic extension of the r-index that achieves updates in LCP-bounded time. The dynamic r-index supports count queries in $O(m \\log r / \\log \\log r)$ time and locate queries in $O(m \\log r / \\log \\log r + \\mathsf{occ} \\log r)$ time, using $O(r)$ words of space, where $m$ is the length of a query with $\\mathsf{occ}$ occurrences and $r$ is the number of runs in the RLBWT. Crucially, update operations are supported in $O((m + L_{\\mathsf{max}}) \\log n)$ time for a substring of length $m$, where $L_{\\mathsf{max}}$ is the maximum LCP value; the average running time is $O((m + L_{\\mathsf{avg}}) \\log n)$, where $L_{\\mathsf{avg}}$ is the average LCP value. This LCP-bounded complexity is particularly advantageous for highly repetitive strings where LCP values are typically small. We experimentally demonstrate the practical efficiency of the dynamic r-index on various highly repetitive datasets.</article>","contentLength":1493,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt State Space Model","url":"https://arxiv.org/abs/2504.17732","date":1761796800,"author":"","guid":321313,"unread":true,"content":"<article>arXiv:2504.17732v2 Announce Type: replace \nAbstract: All-in-One image restoration aims to address multiple image degradation problems using a single model, offering a more practical and versatile solution compared to designing dedicated models for each degradation type. Existing approaches typically rely on Degradation-specific models or coarse-grained degradation prompts to guide image restoration. However, they lack fine-grained modeling of degradation information and face limitations in balancing multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a novel All-in-One image restoration framework that introduces a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM). The DP-SSM leverages the fine-grained degradation features captured by the extractor as dynamic prompts, which are then incorporated into the state space modeling process. This enhances the model's adaptability to diverse degradation types, while a complementary High-Frequency Enhancement Block (HEB) recovers local high-frequency details. Extensive experiments on a mixed dataset containing seven degradation types show that DPMambaIR achieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These results highlight the potential and superiority of DPMambaIR as a unified solution for All-in-One image restoration.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OmegAMP: Targeted AMP Discovery through Biologically Informed Generation","url":"https://arxiv.org/abs/2504.17247","date":1761796800,"author":"","guid":321314,"unread":true,"content":"<article>arXiv:2504.17247v2 Announce Type: replace \nAbstract: Deep learning-based antimicrobial peptide (AMP) discovery faces critical challenges such as limited controllability, lack of representations that efficiently model antimicrobial properties, and low experimental hit rates. To address these challenges, we introduce OmegAMP, a framework designed for reliable AMP generation with increased controllability. Its diffusion-based generative model leverages a novel conditioning mechanism to achieve fine-grained control over desired physicochemical properties and to direct generation towards specific activity profiles, including species-specific effectiveness. This is further enhanced by a biologically informed encoding space that significantly improves overall generative performance. Complementing these generative capabilities, OmegAMP leverages a novel synthetic data augmentation strategy to train classifiers for AMP filtering, drastically reducing false positive rates and thereby increasing the likelihood of experimental success. Our in silico experiments demonstrate that OmegAMP delivers state-of-the-art performance across key stages of the AMP discovery pipeline, enabling us to achieve an unprecedented success rate in wet lab experiments. We tested 25 candidate peptides, 24 of them (96%) demonstrated antimicrobial activity, proving effective even against multi-drug resistant strains. Our findings underscore OmegAMP's potential to significantly advance computational frameworks in the fight against antimicrobial resistance.</article>","contentLength":1543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Function Orchestration for Large Language Models","url":"https://arxiv.org/abs/2504.14872","date":1761796800,"author":"","guid":321315,"unread":true,"content":"<article>arXiv:2504.14872v2 Announce Type: replace \nAbstract: Function calling is a fundamental capability of today's large language models, but sequential function calling posed efficiency problems. Recent studies have proposed to request function calls with parallelism support in order to alleviate this issue. However, they either delegate the concurrent function calls to users for execution which are conversely executed sequentially, or overlook the relations among various function calls, rending limited efficiency. This paper introduces LLMOrch, an advanced framework for automated, parallel function calling in large language models. The key principle behind LLMOrch is to identify an available processor to execute a function call while preventing any single processor from becoming overburdened. To this end, LLMOrch models the data relations (i.e., def-use) among different function calls and coordinates their executions by their control relations (i.e., mutual-exclusion) as well as the working status of the underlying processors. When comparing with state-of-the-art techniques, LLMOrch demonstrated comparable efficiency improvements in orchestrating I/O-intensive functions, while significantly outperforming (2$\\times$) them with compute-intensive functions. LLMOrch's performance even showed a linear correlation to the number of allocated processors. We believe that these results highlight the potential of LLMOrch as an efficient solution for parallel function orchestration in the context of large language models.</article>","contentLength":1531,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a Novel Benchmark","url":"https://arxiv.org/abs/2504.10258","date":1761796800,"author":"","guid":321316,"unread":true,"content":"<article>arXiv:2504.10258v2 Announce Type: replace \nAbstract: Document Reading Order Recovery is a fundamental task in document image understanding, playing a pivotal role in enhancing Retrieval-Augmented Generation (RAG) and serving as a critical preprocessing step for large language models (LLMs). Existing methods often struggle with complex layouts(e.g., multi-column newspapers), high-overhead interactions between cross-modal elements (visual regions and textual semantics), and a lack of robust evaluation benchmarks. We introduce XY-Cut++, an advanced layout ordering method that integrates pre-mask processing, multi-granularity segmentation, and cross-modal matching to address these challenges. Our method significantly enhances layout ordering accuracy compared to traditional XY-Cut techniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8 BLEU overall) while maintaining simplicity and efficiency. It outperforms existing baselines by up to 24\\% and demonstrates consistent accuracy across simple and complex layouts on the newly introduced DocBench-100 dataset. This advancement establishes a reliable foundation for document structure recovery, setting a new standard for layout ordering tasks and facilitating more effective RAG and LLM preprocessing.</article>","contentLength":1282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ES-HPC-MPC: Exponentially Stable Hybrid Perception Constrained MPC for Quadrotor with Suspended Payloads","url":"https://arxiv.org/abs/2504.08841","date":1761796800,"author":"","guid":321317,"unread":true,"content":"<article>arXiv:2504.08841v2 Announce Type: replace \nAbstract: Aerial transportation using quadrotors with cable-suspended payloads holds great potential for applications in disaster response, logistics, and infrastructure maintenance. However, their hybrid and underactuated dynamics pose significant control and perception challenges. Traditional approaches often assume a taut cable condition, limiting their effectiveness in real-world applications where slack-to-taut transitions occur due to disturbances. We introduce ES-HPC-MPC, a model predictive control framework that enforces exponential stability and perception-constrained control under hybrid dynamics.\n  Our method leverages Exponentially Stabilizing Control Lyapunov Functions (ES-CLFs) to enforce stability during the tasks and Control Barrier Functions (CBFs) to maintain the payload within the onboard camera's field of view (FoV). We validate our method through both simulation and real-world experiments, demonstrating stable trajectory tracking and reliable payload perception. We validate that our method maintains stability and satisfies perception constraints while tracking dynamically infeasible trajectories and when the system is subjected to hybrid mode transitions caused by unexpected disturbances.</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Formal Verification of Quantum Error Correcting Programs","url":"https://arxiv.org/abs/2504.07732","date":1761796800,"author":"","guid":321318,"unread":true,"content":"<article>arXiv:2504.07732v3 Announce Type: replace \nAbstract: Quantum error correction (QEC) is fundamental for suppressing noise in quantum hardware and enabling fault-tolerant quantum computation. In this paper, we propose an efficient verification framework for QEC programs. We define an assertion logic and a program logic specifically crafted for QEC programs and establish a sound proof system. We then develop an efficient method for handling verification conditions (VCs) of QEC programs: for Pauli errors, the VCs are reduced to classical assertions that can be solved by SMT solvers, and for non-Pauli errors, we provide a heuristic algorithm. We formalize the proposed program logic in Coq proof assistant, making it a verified QEC verifier. Additionally, we implement an automated QEC verifier, Veri-QEC, for verifying various fault-tolerant scenarios. We demonstrate the efficiency and broad functionality of the framework by performing different verification tasks across various scenarios. Finally, we present a benchmark of 14 verified stabilizer codes.</article>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient LLM Fine-tuning","url":"https://arxiv.org/abs/2504.06426","date":1761796800,"author":"","guid":321319,"unread":true,"content":"<article>arXiv:2504.06426v2 Announce Type: replace \nAbstract: Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) enhance model capacity at the cost of more &amp; under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S'MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Conceptually, S'MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S'MoRE emulates the capacity of numerous experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S'MoRE improves structural flexibility of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S'MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation. Our implementation is available at: https://github.com/ZimpleX/SMoRE-LLM.</article>","contentLength":1411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enlightenment Period Improving DNN Performance","url":"https://arxiv.org/abs/2504.01737","date":1761796800,"author":"","guid":321320,"unread":true,"content":"<article>arXiv:2504.01737v2 Announce Type: replace \nAbstract: The start of deep neural network training is characterized by a brief yet critical phase that lasts from the beginning of the training until the accuracy reaches approximately 50\\%. During this phase, disordered representations rapidly transition toward ordered structure, and we term this phase the Enlightenment Period. Through theoretical modeling based on phase transition theory and experimental validation, we reveal that applying Mixup data augmentation during this phase has a dual effect: it introduces a Gradient Interference Effect that hinders performance, while also providing a beneficial Activation Revival Effect to restore gradient updates for saturated neurons. We further demonstrate that this negative interference diminishes as the sample set size or the model parameter size increases, thereby shifting the balance between these two effects. Based on these findings, we propose three strategies that improve performance by solely adjusting the training data distribution within this brief period: the Mixup Pause Strategy for small-scale scenarios, the Alpha Boost Strategy for large-scale scenarios with underfitting, and the High-Loss Removal Strategy for tasks where Mixup is inapplicable (e.g., time series and large language models). Extensive experiments show that these strategies achieve superior performance across diverse architectures such as ViT and ResNet on datasets including CIFAR and ImageNet-1K. Ultimately, this work offers a novel perspective on enhancing model performance by strategically capitalizing on the dynamics of the brief and crucial early stages of training. Code is available at https://anonymous.4open.science/r/code-A5F1/.</article>","contentLength":1732,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A variational symplectic scheme based on Lobatto's quadrature","url":"https://arxiv.org/abs/2504.00560","date":1761796800,"author":"","guid":321321,"unread":true,"content":"<article>arXiv:2504.00560v3 Announce Type: replace \nAbstract: We present a variational integrator based on the Lobatto quadrature for the time integration of dynamical systems issued from the least action principle. This numerical method uses a cubic interpolation of the states and the action is approximated at each time step by Lobatto's formula. Numerical analysis is performed on both a harmonic oscillator and a nonlinear pendulum. The geometric scheme is conditionally stable, sixth-order accurate, and symplectic. It preserves an approximate energy quantity. Simulation results illustrate the performance and the superconvergence of the proposed method.</article>","contentLength":652,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Polynomial Inequalities and Optimal Stability of Numerical Integrators","url":"https://arxiv.org/abs/2503.24232","date":1761796800,"author":"","guid":321322,"unread":true,"content":"<article>arXiv:2503.24232v2 Announce Type: replace \nAbstract: A numerical integrator for $\\dot{x}=f(x)$ is called \\emph{stable} if, when applied to the 1D Dahlquist test equation $\\dot{x}=\\lambda x,\\lambda\\in\\mathbb{C}$ with fixed timestep $h&gt;0$, the numerical solution remains bounded as the number of steps tends to infinity. It is well known that no explicit integrator may remain stable beyond certain limits in $\\lambda$. Furthermore, these stability limits are only tight for certain specific integrators (different in each case), which may then be called `optimally stable'. Such optimal stability results are typically proven using sophisticated techniques from complex analysis, leading to rather abstruse proofs. In this article, we pursue an alternative approach, exploiting connections with the Bernstein and Markov brothers inequalities for polynomials. This simplifies the proofs greatly and offers a framework which unifies the diverse results that have been obtained.</article>","contentLength":974,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ASGO: Adaptive Structured Gradient Optimization","url":"https://arxiv.org/abs/2503.20762","date":1761796800,"author":"","guid":321323,"unread":true,"content":"<article>arXiv:2503.20762v3 Announce Type: replace \nAbstract: Training deep neural networks is a structured optimization problem, because the parameters are naturally represented by matrices and tensors rather than by vectors. Under this structural representation, it has been widely observed that gradients are low-rank and Hessians are approximately block diagonal. These structured properties are crucial for designing efficient optimization algorithms, but are not utilized by many current popular optimizers like Adam. In this paper, we present a novel optimization algorithm ASGO that capitalizes on these properties by employing a preconditioner that is adaptively updated using structured gradients. By a fine-grained theoretical analysis, ASGO is proven to achieve superior convergence rates compared to existing structured gradient methods. Based on this convergence theory, we further demonstrate that ASGO can benefit from low-rank gradients and block diagonal Hessians. We also discuss practical modifications of ASGO and empirically verify ASGO's effectiveness on language model tasks. Code is available at https://github.com/infinity-stars/ASGO.</article>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DGTRSD & DGTRS-CLIP: A Dual-Granularity Remote Sensing Image-Text Dataset and Vision Language Foundation Model for Alignment","url":"https://arxiv.org/abs/2503.19311","date":1761796800,"author":"","guid":321324,"unread":true,"content":"<article>arXiv:2503.19311v2 Announce Type: replace \nAbstract: Vision Language Foundation Models based on CLIP architecture for remote sensing primarily rely on short text captions, which often result in incomplete semantic representations. Although longer captions convey richer information, existing models struggle to process them effectively because of limited text-encoding capacity, and there remains a shortage of resources that align remote sensing images with both short text and long text captions. To address this gap, we introduce DGTRSD, a dual-granularity remote sensing image-text dataset, where each image is paired with both a short text caption and a long text description, providing a solid foundation for dual-granularity semantic modeling. Based on this, we further propose DGTRS-CLIP, a dual-granularity curriculum learning framework that combines short text and long text supervision to achieve dual-granularity semantic alignment. Extensive experiments on four typical zero-shot tasks: long text cross-modal retrieval, short text cross-modal retrieval, image classification, and semantic localization demonstrate that DGTRS-CLIP consistently outperforms existing methods across all tasks. The code has been open-sourced and is available at https://github.com/MitsuiChen14/DGTRS.</article>","contentLength":1292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TuneNSearch: a hybrid transfer learning and local search approach for solving vehicle routing problems","url":"https://arxiv.org/abs/2503.12662","date":1761796800,"author":"","guid":321325,"unread":true,"content":"<article>arXiv:2503.12662v3 Announce Type: replace \nAbstract: This paper introduces TuneNSearch, a hybrid transfer learning and local search approach for addressing diverse variants of the vehicle routing problem (VRP). Our method uses reinforcement learning to generate high-quality solutions, which are subsequently refined by an efficient local search procedure. To ensure broad adaptability across VRP variants, TuneNSearch begins with a pre-training phase on the multi-depot VRP (MDVRP), followed by a fine-tuning phase to adapt it to other problem formulations. The learning phase utilizes a Transformer-based architecture enhanced with edge-aware attention, which integrates edge distances directly into the attention mechanism to better capture spatial relationships inherent to routing problems. We show that the pre-trained model generalizes effectively to single-depot variants, achieving performance comparable to models trained specifically on single-depot instances. Simultaneously, it maintains strong performance on multi-depot variants, an ability that models pre-trained solely on single-depot problems lack. For example, on 100-node instances of multi-depot variants, TuneNSearch outperforms a model pre-trained on the CVRP by 44%. In contrast, on 100-node instances of single-depot variants, TuneNSearch performs similar to the CVRP model. To validate the effectiveness of our method, we conduct extensive computational experiments on public benchmark and randomly generated instances. Across multiple CVRPLIB datasets, TuneNSearch consistently achieves performance deviations of less than 3% from the best-known solutions in the literature, compared to 6-25% for other neural-based models, depending on problem complexity. Overall, our approach demonstrates strong generalization to different problem sizes, instance distributions, and VRP formulations, while maintaining polynomial runtime complexity despite the integration of the local search algorithm.</article>","contentLength":1968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban Scenes via Remote Sensing Imagery","url":"https://arxiv.org/abs/2503.11245","date":1761796800,"author":"","guid":321326,"unread":true,"content":"<article>arXiv:2503.11245v4 Announce Type: replace \nAbstract: We tackle the challenge of LiDAR-based place recognition, which traditionally depends on costly and time-consuming prior 3D maps. To overcome this, we first construct LiRSI-XA dataset, which encompasses approximately $110,000$ remote sensing submaps and $13,000$ LiDAR point cloud submaps captured in urban scenes, and propose a novel method, L2RSI, for cross-view LiDAR place recognition using high-resolution Remote Sensing Imagery. This approach enables large-scale localization capabilities at a reduced cost by leveraging readily available overhead images as map proxies. L2RSI addresses the dual challenges of cross-view and cross-modal place recognition by learning feature alignment between point cloud submaps and remote sensing submaps in the semantic domain. Additionally, we introduce a novel probability propagation method based on particle estimation to refine position predictions, effectively leveraging temporal and spatial information. This approach enables large-scale retrieval and cross-scene generalization without fine-tuning. Extensive experiments on LiRSI-XA demonstrate that, within a $100km^2$ retrieval range, L2RSI accurately localizes $83.27\\%$ of point cloud submaps within a $30m$ radius for top-$1$ retrieved location. Our project page is publicly available at https://shizw695.github.io/L2RSI/.</article>","contentLength":1381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space","url":"https://arxiv.org/abs/2503.11094","date":1761796800,"author":"","guid":321327,"unread":true,"content":"<article>arXiv:2503.11094v3 Announce Type: replace \nAbstract: Spatial reasoning is a fundamental capability of multimodal large language models (MLLMs), yet their performance in open aerial environments remains underexplored. In this work, we present Open3D-VQA, a novel benchmark for evaluating MLLMs' ability to reason about complex spatial relationships from an aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general spatial reasoning tasks, including multiple-choice, true/false, and short-answer formats, and supports both visual and point cloud modalities. The questions are automatically generated from spatial relations extracted from both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs reveals that: 1) Models are generally better at answering questions about relative spatial relations than absolute distances, 2) 3D LLMs fail to demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on the simulated dataset can significantly improve the model's spatial reasoning performance in real-world scenarios. We release our benchmark, data generation pipeline, and evaluation toolkit to support further research: https://github.com/EmbodiedCity/Open3D-VQA.code.</article>","contentLength":1221,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simulating Automotive Radar with Lidar and Camera Inputs","url":"https://arxiv.org/abs/2503.08068","date":1761796800,"author":"","guid":321328,"unread":true,"content":"<article>arXiv:2503.08068v2 Announce Type: replace \nAbstract: Low-cost millimeter automotive radar has received more and more attention due to its ability to handle adverse weather and lighting conditions in autonomous driving. However, the lack of quality datasets hinders research and development. We report a new method that is able to simulate 4D millimeter wave radar signals including pitch, yaw, range, and Doppler velocity along with radar signal strength (RSS) using camera image, light detection and ranging (lidar) point cloud, and ego-velocity. The method is based on two new neural networks: 1) DIS-Net, which estimates the spatial distribution and number of radar signals, and 2) RSS-Net, which predicts the RSS of the signal based on appearance and geometric information. We have implemented and tested our method using open datasets from 3 different models of commercial automotive radar. The experimental results show that our method can successfully generate high-fidelity radar signals. Moreover, we have trained a popular object detection neural network with data augmented by our synthesized radar. The network outperforms the counterpart trained only on raw radar data, a promising result to facilitate future radar-based research and development.</article>","contentLength":1260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving","url":"https://arxiv.org/abs/2503.06497","date":1761796800,"author":"","guid":321329,"unread":true,"content":"<article>arXiv:2503.06497v3 Announce Type: replace \nAbstract: Ensuring the safety of vision-language models (VLMs) in autonomous driving systems is of paramount importance, yet existing research has largely focused on conventional benchmarks rather than safety-critical evaluation. In this work, we present SCD-Bench (Safety Cognition Driving Benchmark) a novel framework specifically designed to assess the safety cognition capabilities of VLMs within interactive driving scenarios. To address the scalability challenge of data annotation, we introduce ADA (Autonomous Driving Annotation), a semi-automated labeling system, further refined through expert review by professionals with domain-specific knowledge in autonomous driving. To facilitate scalable and consistent evaluation, we also propose an automated assessment pipeline leveraging large language models, which demonstrates over 98% agreement with human expert judgments. In addressing the broader challenge of aligning VLMs with safety cognition in driving environments, we construct SCD-Training, the first large-scale dataset tailored for this task, comprising 324.35K high-quality samples. Through extensive experiments, we show that models trained on SCD-Training exhibit marked improvements not only on SCD-Bench, but also on general and domain-specific benchmarks, offering a new perspective on enhancing safety-aware interactions in vision-language systems for autonomous driving.</article>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can LLMs Outshine Conventional Recommenders? A Comparative Evaluation","url":"https://arxiv.org/abs/2503.05493","date":1761796800,"author":"","guid":321330,"unread":true,"content":"<article>arXiv:2503.05493v2 Announce Type: replace \nAbstract: In recent years, integrating large language models (LLMs) into recommender systems has created new opportunities for improving recommendation quality. However, a comprehensive benchmark is needed to thoroughly evaluate and compare the recommendation capabilities of LLMs with traditional recommender systems. In this paper, we introduce RecBench, which systematically investigates various item representation forms (including unique identifier, text, semantic embedding, and semantic identifier) and evaluates two primary recommendation tasks, i.e., click-through rate prediction (CTR) and sequential recommendation (SeqRec). Our extensive experiments cover up to 17 large models and are conducted across five diverse datasets from fashion, news, video, books, and music domains. Our findings indicate that LLM-based recommenders outperform conventional recommenders, achieving up to a 5% AUC improvement in the CTR scenario and up to a 170% NDCG@10 improvement in the SeqRec scenario. However, these substantial performance gains come at the expense of significantly reduced inference efficiency, rendering the LLM-as-RS paradigm impractical for real-time recommendation environments. We aim for our findings to inspire future research, including recommendation-specific model acceleration methods. We will release our code, data, configurations, and platform to enable other researchers to reproduce and build upon our experimental results.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"XTS mode revisited: high hopes for key scopes?","url":"https://arxiv.org/abs/2502.18631","date":1761796800,"author":"","guid":321331,"unread":true,"content":"<article>arXiv:2502.18631v2 Announce Type: replace \nAbstract: This paper concisely summarizes the XTS block encryption mode for storage sector-based encryption applications and clarifies its limitations. In particular, we aim to provide a unified basis for constructive discussions about the newly introduced key scope change to the IEEE 1619 standard. We also reflect on wide modes that could replace XTS in the future.</article>","contentLength":411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spontaneous Giving and Calculated Greed in Language Models","url":"https://arxiv.org/abs/2502.17720","date":1761796800,"author":"","guid":321332,"unread":true,"content":"<article>arXiv:2502.17720v4 Announce Type: replace \nAbstract: Large language models demonstrate strong problem-solving abilities through reasoning techniques such as chain-of-thought prompting and reflection. However, it remains unclear whether these reasoning capabilities extend to a form of social intelligence: making effective decisions in cooperative contexts. We examine this question using economic games that simulate social dilemmas. First, we apply chain-of-thought and reflection prompting to GPT-4o in a Public Goods Game. We then evaluate multiple off-the-shelf models across six cooperation and punishment games, comparing those with and without explicit reasoning mechanisms. We find that reasoning models consistently reduce cooperation and norm enforcement, favoring individual rationality. In repeated interactions, groups with more reasoning agents exhibit lower collective gains. These behaviors mirror human patterns of \"spontaneous giving and calculated greed.\" Our findings underscore the need for LLM architectures that incorporate social intelligence alongside reasoning, to help address--rather than reinforce--the challenges of collective action.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models","url":"https://arxiv.org/abs/2502.14819","date":1761796800,"author":"","guid":321333,"unread":true,"content":"<article>arXiv:2502.14819v4 Announce Type: replace \nAbstract: A long-standing goal in AI is to develop agents capable of solving diverse tasks across a range of environments, including those never seen during training. Two dominant paradigms address this challenge: (i) reinforcement learning (RL), which learns policies via trial and error, and (ii) optimal control, which plans actions using a known or learned dynamics model. However, their comparative strengths in the offline setting - where agents must learn from reward-free trajectories - remain underexplored. In this work, we systematically evaluate RL and control-based methods on a suite of navigation tasks, using offline datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot methods. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and employ it for planning. We investigate how factors such as data diversity, trajectory quality, and environment variability influence the performance of these approaches. Our results show that model-free RL benefits most from large amounts of high-quality data, whereas model-based planning generalizes better to unseen layouts and is more data-efficient, while achieving trajectory stitching performance comparable to leading model-free methods. Notably, planning with a latent dynamics model proves to be a strong approach for handling suboptimal offline data and adapting to diverse environments.</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities","url":"https://arxiv.org/abs/2502.12128","date":1761796800,"author":"","guid":321334,"unread":true,"content":"<article>arXiv:2502.12128v4 Announce Type: replace \nAbstract: Generative models are spearheading recent progress in deep learning, showcasing strong promise for trajectory sampling in dynamical systems as well. However, whereas latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems -- from chemical molecule structures to collective human behavior -- are described by interactions of entities, making them inherently linked to connectivity patterns, entity conservation, and the traceability of entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), bridges the gap between: (1) keeping the traceability of individual entities in a latent system representation, and (2) leveraging the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder enable generative modeling directly in latent space. The core idea of LaM-SLidE is the introduction of identifier representations (IDs) that enable the retrieval of entity properties and entity composition from latent system representations, thus fostering traceability. Experimentally, across different domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy, and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .</article>","contentLength":1420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal morphings for model-order reduction for poorly reducible problems with geometric variability","url":"https://arxiv.org/abs/2502.11632","date":1761796800,"author":"","guid":321335,"unread":true,"content":"<article>arXiv:2502.11632v2 Announce Type: replace \nAbstract: We propose a new model-order reduction framework to poorly reducible problems arising from parametric partial differential equations with geometric variability. In such problems, the solution manifold exhibits a slowly decaying Kolmogorov $N$-width, so that standard projection-based model order reduction techniques based on linear subspace approximations become ineffective. To overcome this difficulty, we introduce an optimal morphing strategy: For each solution sample, we compute a bijective morphing from a reference domain to the sample domain such that, when all the solution fields are pulled back to the reference domain, their variability is reduced. We formulate a global optimization problem on the morphings that maximizes the energy captured by the first $r$ modes of the mapped fields obtained from Proper Orthogonal Decomposition, thus maximizing the reducibility of the dataset. Finally, using a non-intrusive Gaussian Process regression on the reduced coordinates, we build a fast surrogate model that can accurately predict new solutions, highlighting the practical benefits of the proposed approach for many-query applications. The framework is general, independent of the underlying partial differential equation, and applies to scenarios with either parameterized or non-parameterized geometries.</article>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Markovian Discrete Diffusion with Causal Language Models","url":"https://arxiv.org/abs/2502.09767","date":1761796800,"author":"","guid":321336,"unread":true,"content":"<article>arXiv:2502.09767v3 Announce Type: replace \nAbstract: Discrete diffusion models offer a flexible, controllable approach to structured sequence generation, yet they still lag behind causal language models in expressive power. A key limitation lies in their reliance on the Markovian assumption, which restricts each step to condition only on the current state, leading to potential uncorrectable error accumulation. In this paper, we introduce CaDDi (Causal Discrete Diffusion Model), a discrete diffusion model that conditions on the entire generative trajectory, thereby lifting the Markov constraint and allowing the model to revisit and improve past states. By unifying sequential (causal) and temporal (diffusion) reasoning in a single non-Markovian transformer, CaDDi also treats standard causal language models as a special case and permits the direct reuse of pretrained LLM weights with no architectural changes. Empirically, CaDDi outperforms state-of-the-art discrete diffusion baselines on natural-language benchmarks, substantially narrowing the remaining gap to large autoregressive transformers.</article>","contentLength":1108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amnesiac Flooding: Easy to break, hard to escape","url":"https://arxiv.org/abs/2502.06001","date":1761796800,"author":"","guid":321337,"unread":true,"content":"<article>arXiv:2502.06001v2 Announce Type: replace \nAbstract: Broadcast is a central problem in distributed computing. Recently, Hussak and Trehan [PODC'19/DC'23] proposed a stateless broadcasting protocol (Amnesiac Flooding), which was surprisingly proven to terminate in asymptotically optimal time (linear in the diameter of the network). However, it remains unclear: (i) Are there other stateless terminating broadcast algorithms with the desirable properties of Amnesiac Flooding, (ii) How robust is Amnesiac Flooding with respect to \\emph{faults}?\n  In this paper we make progress on both of these fronts. Under a reasonable restriction (obliviousness to message content) additional to the fault-free synchronous model, we prove that Amnesiac Flooding is the \\emph{only} strictly stateless deterministic protocol that can achieve terminating broadcast. We achieve this by identifying four natural properties of a terminating broadcast protocol that Amnesiac Flooding uniquely satisfies. In contrast, we prove that even minor relaxations of \\textit{any} of these four criteria allow the construction of other terminating broadcast protocols.\n  On the other hand, we prove that Amnesiac Flooding can become non-terminating or non-broadcasting, even if we allow just one node to drop a single message on a single edge in a single round. As a tool for proving this, we focus on the set of all \\textit{configurations} of transmissions between nodes in the network, and obtain a \\textit{dichotomy} characterizing the configurations, starting from which, Amnesiac Flooding terminates.\n  Additionally, we characterise the structure of sets of Byzantine agents capable of forcing non-termination or non-broadcast of the protocol on arbitrary networks.</article>","contentLength":1739,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Redistributing Rewards Across Time and Agents for Multi-Agent Reinforcement Learning","url":"https://arxiv.org/abs/2502.04864","date":1761796800,"author":"","guid":321338,"unread":true,"content":"<article>arXiv:2502.04864v2 Announce Type: replace \nAbstract: Credit assignmen, disentangling each agent's contribution to a shared reward, is a critical challenge in cooperative multi-agent reinforcement learning (MARL). To be effective, credit assignment methods must preserve the environment's optimal policy. Some recent approaches attempt this by enforcing return equivalence, where the sum of distributed rewards must equal the team reward. However, their guarantees are conditional on a learned model's regression accuracy, making them unreliable in practice. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), an approach that decouples credit modeling from this constraint. A neural network learns unnormalized contribution scores, while a separate, deterministic normalization step enforces return equivalence by construction. We demonstrate that this method is equivalent to a valid Potential-Based Reward Shaping (PBRS), which guarantees the optimal policy is preserved regardless of model accuracy. Empirically, on challenging SMACLite and Google Research Football (GRF) benchmarks, TAR$^2$ accelerates learning and achieves higher final performance than strong baselines. These results establish our method as an effective solution for the agent-temporal credit assignment problem.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exact Sequence Interpolation with Transformers","url":"https://arxiv.org/abs/2502.02270","date":1761796800,"author":"","guid":321339,"unread":true,"content":"<article>arXiv:2502.02270v2 Announce Type: replace \nAbstract: We prove that transformers can exactly interpolate datasets of finite input sequences in $\\mathbb{R}^d$, $d\\geq 2$, with corresponding output sequences of smaller or equal length. Specifically, given $N$ sequences of arbitrary but finite lengths in $\\mathbb{R}^d$ and output sequences of lengths $m^1, \\dots, m^N \\in \\mathbb{N}$, we construct a transformer with $\\mathcal{O}(\\sum_{j=1}^N m^j)$ blocks and $\\mathcal{O}(d \\sum_{j=1}^N m^j)$ parameters that exactly interpolates the dataset. Our construction provides complexity estimates that are independent of the input sequence length, by alternating feed-forward and self-attention layers and by capitalizing on the clustering effect inherent to the latter. Our novel constructive method also uses low-rank parameter matrices in the self-attention mechanism, a common feature of practical transformer implementations. These results are first established in the hardmax self-attention setting, where the geometric structure permits an explicit and quantitative analysis, and are then extended to the softmax setting. Finally, we demonstrate the applicability of our exact interpolation construction to learning problems, in particular by providing convergence guarantees to a global minimizer under regularized training strategies. Our analysis contributes to the theoretical understanding of transformer models, offering an explanation for their excellent performance in exact sequence-to-sequence interpolation tasks.</article>","contentLength":1523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asymptotic uniform estimate of random batch method with replacement for the Cucker-Smale model","url":"https://arxiv.org/abs/2501.15152","date":1761796800,"author":"","guid":321340,"unread":true,"content":"<article>arXiv:2501.15152v3 Announce Type: replace \nAbstract: The Random Batch Method (RBM) [S. Jin, L. Li and J.-G. Liu, Random Batch Methods (RBM) for interacting particle systems, J. Comput. Phys. 400 (2020) 108877] is not only an efficient algorithm for simulating interacting particle systems, but also a randomly switching networked model for interacting particle system. This work investigates two RBM variants (RBM-r and RBM-1) applied to the Cucker-Smale flocking model. We establish the asymptotic emergence of global flocking and derive corresponding error estimates. By introducing a crucial auxiliary system and leveraging the intrinsic characteristics of the Cucker-Smale model, and under suitable conditions on the force, our estimates are uniform in both time and particle numbers. In the case of RBM-1, our estimates are sharper than those in Ha et al. (2021). Additionally, we provide numerical simulations to validate our analytical results.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for and with Foundation Models","url":"https://arxiv.org/abs/2501.14755","date":1761796800,"author":"","guid":321341,"unread":true,"content":"<article>arXiv:2501.14755v3 Announce Type: replace \nAbstract: Foundation models demand advanced data processing for their vast, multimodal datasets. However, traditional frameworks struggle with the unique complexities of multimodal data. In response, we present Data-Juicer 2.0, a data processing system backed by 100+ data processing operators spanning text, image, video, and audio modalities, supporting more critical tasks including data analysis, synthesis, annotation, and foundation model post-training. With seamless compatibility and dedicated optimization for popular dataset hubs like Hugging Face and computing engines like Ray, it improves upon its predecessor in terms of usability, efficiency, and programmability. It features an easily accessible user interface layer that supports decoupled Python interactions, RESTful APIs, and conversational commands. Its new runtime layer offers adaptive execution across diverse scales and environments, abstracting away system complexities. Extensive empirical evaluations demonstrate Data-Juicer 2.0's remarkable performance and scalability, highlighting its capability to efficiently process TB-level data with 10k+ CPU cores. The system is publicly available and has been widely adopted in diverse research fields and real-world products such as Alibaba Cloud PAI. We actively maintain the system and share practical insights to foster research and applications of next-generation foundation models.</article>","contentLength":1451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude","url":"https://arxiv.org/abs/2501.10484","date":1761796800,"author":"","guid":321342,"unread":true,"content":"<article>arXiv:2501.10484v4 Announce Type: replace \nAbstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study systematically evaluates how nine popular LLMs (both open-source and closed-source) respond to ethical dilemmas involving protected attributes. Across 50,400 trials spanning single and intersectional attribute combinations in four dilemma scenarios (protective vs. harmful), we assess models' ethical preferences, sensitivity, stability, and clustering patterns. Results reveal significant biases in protected attributes in all models, with differing preferences depending on model type and dilemma context. Notably, open-source LLMs show stronger preferences for marginalized groups and greater sensitivity in harmful scenarios, while closed-source models are more selective in protective situations and tend to favor mainstream groups. We also find that ethical behavior varies across dilemma types: LLMs maintain consistent patterns in protective scenarios but respond with more diverse and cognitively demanding decisions in harmful ones. Furthermore, models display more pronounced ethical tendencies under intersectional conditions than in single-attribute settings, suggesting that complex inputs reveal deeper biases. These findings highlight the need for multi-dimensional, context-aware evaluation of LLMs' ethical behavior and offer a systematic evaluation and approach to understanding and addressing fairness in LLM decision-making.</article>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Consistency of Responses and Continuations Generated by Large Language Models on Social Media","url":"https://arxiv.org/abs/2501.08102","date":1761796800,"author":"","guid":321343,"unread":true,"content":"<article>arXiv:2501.08102v5 Announce Type: replace \nAbstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using three open-source models: Gemma, Llama3 and Llama3.3 and one commercial Model:Claude. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic consistency between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: these models show a strong tendency to moderate negative emotions. When the input text carries negative emotions such as anger, disgust, fear, or sadness, LLM tends to generate content with more neutral emotions, or even convert them into positive emotions such as joy or surprise. At the same time, we compared the LLM-generated content with human-authored content. The four models systematically generated responses with reduced emotional intensity and showed a preference for neutral rational emotions in the response task. In addition, these models all maintained a high semantic similarity with the original text, although their performance in the continuation task and the response task was different. These findings provide deep insights into the emotion and semantic processing capabilities of LLM, which are of great significance for its deployment in social media environments and human-computer interaction design.</article>","contentLength":1716,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Human-AI Synergy in UI Design: Supporting Iterative Generation with LLMs","url":"https://arxiv.org/abs/2412.20071","date":1761796800,"author":"","guid":321344,"unread":true,"content":"<article>arXiv:2412.20071v3 Announce Type: replace \nAbstract: In automated UI design generation, a key challenge is the lack of support for iterative processes, as most systems focus solely on end-to-end output. This stems from limited capabilities in interpreting design intent and a lack of transparency for refining intermediate results. To better understand these challenges, we conducted a formative study that identified concrete and actionable requirements for supporting iterative design with Generative Tools. Guided by these findings, we propose PrototypeFlow, a human-centered system for automated UI generation that leverages multi-modal inputs and models. PrototypeFlow takes natural language descriptions and layout preferences as input to generate the high-fidelity UI design. At its core is a theme design module that clarifies implicit design intent through prompt enhancement and orchestrates sub-modules for component-level generation. Designers retain full control over inputs, intermediate results, and final prototypes, enabling flexible and targeted refinement by steering generation and directly editing outputs. Our experiments and user studies confirmed the effectiveness and usefulness of our proposed PrototypeFlow.</article>","contentLength":1234,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hypergraph clustering using Ricci curvature: an edge transport perspective","url":"https://arxiv.org/abs/2412.15695","date":1761796800,"author":"","guid":321345,"unread":true,"content":"<article>arXiv:2412.15695v2 Announce Type: replace \nAbstract: In this paper, we introduce a novel method for extending Ricci flow to hypergraphs by defining probability measures on the edges and transporting them on the line expansion. This approach yields a new weighting on the edges, which proves particularly effective for community detection. We extensively compare this method with a similar notion of Ricci flow defined on the clique expansion, demonstrating its enhanced sensitivity to the hypergraph structure, especially in the presence of large hyperedges. The two methods are complementary and together form a powerful and highly interpretable framework for community detection in hypergraphs.</article>","contentLength":696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To Rely or Not to Rely? Evaluating Interventions for Appropriate Reliance on Large Language Models","url":"https://arxiv.org/abs/2412.15584","date":1761796800,"author":"","guid":321346,"unread":true,"content":"<article>arXiv:2412.15584v3 Announce Type: replace \nAbstract: As Large Language Models become integral to decision-making, optimism about their power is tempered with concern over their errors. Users may over-rely on LLM advice that is confidently stated but wrong, or under-rely due to mistrust. Reliance interventions have been developed to help users of LLMs, but they lack rigorous evaluation for appropriate reliance. We benchmark the performance of three relevant interventions by conducting a randomized online experiment with 400 participants attempting two challenging tasks: LSAT logical reasoning and image-based numerical estimation. For each question, participants first answered independently, then received LLM advice modified by one of three reliance interventions and answered the question again. Our findings indicate that while interventions reduce over-reliance, they generally fail to improve appropriate reliance. Furthermore, people became more confident after making wrong reliance decisions in certain contexts, demonstrating poor calibration. Based on our findings, we discuss implications for designing effective reliance interventions in human-LLM collaboration.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Physics Context Builders: A Modular Framework for Physical Reasoning in Vision-Language Models","url":"https://arxiv.org/abs/2412.08619","date":1761796800,"author":"","guid":321347,"unread":true,"content":"<article>arXiv:2412.08619v3 Announce Type: replace \nAbstract: Physical reasoning remains a significant challenge for Vision-Language Models (VLMs). This limitation arises from an inability to translate learned knowledge into predictions about physical behavior. Although continual fine-tuning can mitigate this issue, it is expensive for large models and impractical to perform repeatedly for every task. This necessitates the creation of modular and scalable ways to teach VLMs about physical reasoning. To that end, we introduce Physics Context Builders (PCBs), a modular framework where specialized smaller VLMs are fine-tuned to generate detailed physical scene descriptions. These can be used as physical contexts to enhance the reasoning capabilities of larger VLMs. PCBs enable the separation of visual perception from reasoning, allowing us to analyze their relative contributions to physical understanding. We perform experiments on CLEVRER and on Falling Tower, a stability detection dataset with both simulated and real-world scenes, to demonstrate that PCBs provide substantial performance improvements, increasing average accuracy by up to 13.8% on complex physical reasoning tasks. Notably, PCBs also show strong Sim2Real transfer, successfully generalizing from simulated training data to real-world scenes.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Optimized Hermite Approximation Methods","url":"https://arxiv.org/abs/2412.08044","date":1761796800,"author":"","guid":321348,"unread":true,"content":"<article>arXiv:2412.08044v3 Announce Type: replace \nAbstract: Hermite polynomials and functions have extensive applications in scientific and engineering problems. Although it is recognized that employing the scaled Hermite functions rather than the standard ones can remarkably enhance the approximation performance, the understanding of the scaling factor remains insufficient. Due to the lack of theoretical analysis, recent publications still cast doubt on whether the Hermite spectral method is inferior to other methods. To dispel this doubt, we show in this article that the inefficiency of the Hermite spectral method comes from the imbalance in the decay speed of the objective function within the spatial and frequency domains. Proper scaling can render the Hermite spectral methods comparable to other methods. To make it solid, we propose a novel error analysis framework for the scaled Hermite approximation. Taking the $L^2$ projection error as an example, our framework illustrates that there are three different components of errors: the spatial truncation error, the frequency truncation error, and the Hermite spectral approximation error. Through this perspective, finding the optimal scaling factor is equivalent to balancing the spatial and frequency truncation errors. As applications, we show that geometric convergence can be recovered by proper scaling for a class of functions. Furthermore, we show that proper scaling can double the convergence order for smooth functions with algebraic decay. The perplexing pre-asymptotic sub-geometric convergence when approximating algebraic decay functions can be perfectly explained by this framework.</article>","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HyperMARL: Adaptive Hypernetworks for Multi-Agent RL","url":"https://arxiv.org/abs/2412.04233","date":1761796800,"author":"","guid":321349,"unread":true,"content":"<article>arXiv:2412.04233v4 Announce Type: replace \nAbstract: Adaptive cooperation in multi-agent reinforcement learning (MARL) requires policies to express homogeneous, specialised, or mixed behaviours, yet achieving this adaptivity remains a critical challenge. While parameter sharing (PS) is standard for efficient learning, it notoriously suppresses the behavioural diversity required for specialisation. This failure is largely due to cross-agent gradient interference, a problem we find is surprisingly exacerbated by the common practice of coupling agent IDs with observations. Existing remedies typically add complexity through altered objectives, manual preset diversity levels, or sequential updates -- raising a fundamental question: can shared policies adapt without these intricacies? We propose a solution built on a key insight: an agent-conditioned hypernetwork can generate agent-specific parameters and decouple observation- and agent-conditioned gradients, directly countering the interference from coupling agent IDs with observations. Our resulting method, HyperMARL, avoids the complexities of prior work and empirically reduces policy gradient variance. Across diverse MARL benchmarks (22 scenarios, up to 30 agents), HyperMARL achieves performance competitive with six key baselines while preserving behavioural diversity comparable to non-parameter sharing methods, establishing it as a versatile and principled approach for adaptive MARL. The code is publicly available at https://github.com/KaleabTessera/HyperMARL.</article>","contentLength":1534,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Many Ratings per Item are Necessary for Reliable Significance Testing?","url":"https://arxiv.org/abs/2412.02968","date":1761796800,"author":"","guid":321350,"unread":true,"content":"<article>arXiv:2412.02968v2 Announce Type: replace \nAbstract: A cornerstone of machine learning evaluation is the (often hidden) assumption that model and human responses are reliable enough to evaluate models against unitary, authoritative, ``gold standard'' data, via simple metrics such as accuracy, precision, and recall. The generative AI revolution would seem to explode this assumption, given the critical role stochastic inference plays. Yet, in spite of public demand for more transparency in AI -- along with strong evidence that humans are unreliable judges -- estimates of model reliability are conventionally based on, at most, a few output responses per input item. We adapt a method, previously used to evaluate the reliability of various metrics and estimators for machine learning evaluation, to determine whether an (existing or planned) dataset has enough responses per item to assure reliable null hypothesis statistical testing. We show that, for many common metrics, collecting even 5-10 responses per item (from each model and team of human evaluators) is not sufficient. We apply our methods to several of the very few extant gold standard test sets with multiple disaggregated responses per item and show that even these datasets lack enough responses per item. We show how our methods can help AI researchers make better decisions about how to collect data for AI evaluation.</article>","contentLength":1392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SNN-Based Online Learning of Concepts and Action Laws in an Open World","url":"https://arxiv.org/abs/2411.12308","date":1761796800,"author":"","guid":321351,"unread":true,"content":"<article>arXiv:2411.12308v4 Announce Type: replace \nAbstract: We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent's semantic memory. This agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent's knowledge of its universe's action laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.</article>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic Competitive Pseudo Label Selection","url":"https://arxiv.org/abs/2411.10237","date":1761796800,"author":"","guid":321352,"unread":true,"content":"<article>arXiv:2411.10237v2 Announce Type: replace \nAbstract: In clinical medicine, precise image segmentation can provide substantial support to clinicians. However, obtaining high-quality segmentation typically demands extensive pixel-level annotations, which are labor-intensive and expensive. Scribble annotations offer a more cost-effective alternative by improving labeling efficiency. Nonetheless, using such sparse supervision for training reliable medical image segmentation models remains a significant challenge. Some studies employ pseudo-labeling to enhance supervision, but these methods are susceptible to noise interference. To address these challenges, we introduce ScribbleVS, a framework designed to learn from scribble annotations. We introduce a Regional Pseudo Labels Diffusion Module to expand the scope of supervision and reduce the impact of noise present in pseudo labels. Additionally, we introduce a Dynamic Competitive Selection module for enhanced refinement in selecting pseudo labels. Experiments conducted on the ACDC, MSCMRseg, WORD, and BraTS2020 datasets demonstrate promising results, achieving segmentation precision comparable to fully supervised models. The codes of this study are available at https://github.com/ortonwang/ScribbleVS.</article>","contentLength":1266,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta-Learning Objectives for Preference Optimization","url":"https://arxiv.org/abs/2411.06568","date":1761796800,"author":"","guid":321353,"unread":true,"content":"<article>arXiv:2411.06568v3 Announce Type: replace \nAbstract: Evaluating preference optimization (PO) algorithms on LLM alignment is a challenging task that presents prohibitive costs, noise, and several variables like model size and hyper-parameters. In this work, we show that it is possible to gain insights on the efficacy of PO algorithm on simpler benchmarks. We design a diagnostic suite of MuJoCo tasks and datasets, which we use to systematically evaluate PO algorithms, establishing a more controlled and cheaper benchmark. We then propose a novel family of PO algorithms based on mirror descent, which we call Mirror Preference Optimization (MPO). Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets, such as mixed-quality or noisy data. We demonstrate that our discovered PO algorithms outperform all known algorithms in the targeted MuJoCo settings. Finally, based on the insights gained from our MuJoCo experiments, we design a PO algorithm that significantly outperform existing baselines in an LLM alignment task.</article>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Implications of User Non-compliance Behavior to Software Updates: A Risk Assessment Study","url":"https://arxiv.org/abs/2411.06262","date":1761796800,"author":"","guid":321354,"unread":true,"content":"<article>arXiv:2411.06262v2 Announce Type: replace \nAbstract: Software updates are essential to enhance security, fix bugs, and add better features to the existing software. While some users accept software updates, non-compliance remains a widespread issue. While some users accept software updates, non-compliance remains a widespread issue. End users' systems remain vulnerable to security threats when security updates are not installed or are installed with a delay. Despite research efforts, users' noncompliance behavior with software updates is still prevalent. In this study, we explored how psychological factors influence users' perception and behavior toward software updates. In addition, we investigated how information about potential vulnerabilities and risk scores influences their behavior. Next, we proposed a model that utilizes attributes from the National Vulnerability Database (NVD) to effectively assess the overall risk score associated with delaying software updates. Next, we conducted a user study with Windows OS users, showing that providing a risk score for not updating their systems and information about vulnerabilities significantly increased users' willingness to update their systems. Additionally, we examined the influence of demographic factors, gender, on users' decision-making regarding software updates. Our results show no statistically significant difference in male and female users' responses in terms of concerns about securing their systems. The implications of this study are relevant for software developers and manufacturers as they can use this information to design more effective software update notification messages. The communication of the potential risks and their corresponding risk scores may motivate users to take action and update their systems in a timely manner, which can ultimately improve the overall security of the system.</article>","contentLength":1887,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Neural Networks Trained on Noisy Speech Exhibit the McGurk Effect","url":"https://arxiv.org/abs/2411.05715","date":1761796800,"author":"","guid":321355,"unread":true,"content":"<article>arXiv:2411.05715v2 Announce Type: replace \nAbstract: Humans are able to fuse information from both auditory and visual modalities to help with understanding speech. This is demonstrated through a phenomenon known as the McGurk Effect, during which a listener is presented with incongruent auditory and visual speech that fuse together into the percept of illusory intermediate phonemes. Building on a recent framework that proposes how to address developmental 'why' questions using artificial neural networks, we evaluated a set of recent artificial neural networks trained on audiovisual speech by testing them with audiovisually incongruent words designed to elicit the McGurk effect. We show that networks trained entirely on congruent audiovisual speech nevertheless exhibit the McGurk percept. We further investigated 'why' by comparing networks trained on clean speech to those trained on noisy speech, and discovered that training with noisy speech led to a pronounced increase in both visual responses and McGurk responses across all models. Furthermore, we observed that systematically increasing the level of auditory noise during ANN training also increased the amount of audiovisual integration up to a point, but at extreme noise levels, this integration failed to develop. These results suggest that excessive noise exposure during critical periods of audiovisual learning may negatively influence the development of audiovisual speech integration. This work also demonstrates that the McGurk effect reliably emerges untrained from the behaviour of both supervised and unsupervised networks, even networks trained only on congruent speech. This supports the notion that artificial neural networks might be useful models for certain aspects of perception and cognition.</article>","contentLength":1783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Strength of Weak Ties Between Open-Source Developers","url":"https://arxiv.org/abs/2411.05646","date":1761796800,"author":"","guid":321356,"unread":true,"content":"<article>arXiv:2411.05646v2 Announce Type: replace \nAbstract: In a real-world social network, weak ties (reflecting low-intensity, infrequent interactions) act as bridges and connect people to different social circles, giving them access to diverse information and opportunities that are not available within one's immediate, close-knit vicinity. Weak ties can be crucial for creativity and innovation, as they introduce ideas and approaches that people can then combine in novel ways, leading to innovative solutions. Do weak ties facilitate creativity in software in similar ways? This paper suggests that the answer is \"yes.\" Concretely, we study the correlation between developers' knowledge acquisition through three distinct interaction networks on GitHub and the innovativeness of the projects they develop, across over 37,000 Python projects hosted on GitHub. Our findings suggest that the topical diversity of projects in which developers engage, rather than the volume, correlates positively with the innovativeness of their future code. Notably, exposure through weak interactions (e.g., starring) emerges as a stronger predictor of future novelty than via strong ones (e.g., committing)</article>","contentLength":1189,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Blind Spot Navigation in Large Language Model Reasoning with Thought Space Explorer","url":"https://arxiv.org/abs/2410.24155","date":1761796800,"author":"","guid":321357,"unread":true,"content":"<article>arXiv:2410.24155v4 Announce Type: replace \nAbstract: Large language models have shown strong reasoning capabilities through chain-structured methods such as Chain-of-Thought. Recent studies optimize thought structures by generating parallel or tree-like structures, switching between long and short reasoning modes, or aligning reasoning steps with task performance. However, these approaches mainly rely on previously generated logical directions of the chains, which ignore the unexplored regions of the solution space. Such a phenomenon is defined as blind spots, which limit the diversity and effectiveness of the reasoning process. To this end, we propose the ``Thought Space Explorer'' (TSE), a framework for navigating and expanding thought structures to overcome blind spots in LLM reasoning. Our TSE first identifies key nodes with high impact, then generates new nodes by integrating information from multiple chains. Finally, it extends new branches through connection strategies. We conduct a series of experiments on math and QA benchmarks. Compared with existing baseline methods, TSE improves the accuracy of both the final answer and intermediate reasoning steps, while maintaining a better effectiveness-efficiency trade-off for practical deployment.</article>","contentLength":1267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LaMP-Val: Large Language Models Empower Personalized Valuation in Auction","url":"https://arxiv.org/abs/2410.15817","date":1761796800,"author":"","guid":321358,"unread":true,"content":"<article>arXiv:2410.15817v2 Announce Type: replace \nAbstract: Auctions are a vital economic mechanism used to determine the market value of goods or services through competitive bidding within a specific framework. However, much of the current research primarily focuses on the bidding algorithms used within auction mechanisms. This often neglects the potential benefits of incorporating individual users' unique preferences into the valuation process. Our theoretical and empirical analysis demonstrates that valuation errors can significantly impact the overall utility. To bridge this gap, we propose a personalized valuation framework, namely Large \\underline{La}nguage \\underline{M}odels-powered \\underline{P}ersonalized \\underline{Val}uation (LaMP-Val), which integrates Large Language Models to incorporate personalized semantic preference into users valuation process. LaMP-Val integrating three components: data, learning, and evaluation. The data component tackles the challenge of building a novel dataset specifically for LLMs fine-tuning in personalized valuation modeling. The learning component introduces a diversity template to enhance LLMs' capacity for modeling fine-grained personal valuation patterns. The evaluation component establishes a closed-loop system where LLM-generated valuations interact with bidding strategies and auction. It proposes two novel metrics to quantify valuation precision and bidding intention accuracy in personalized scenarios. Extensive experiments show that LaMP-Val more accurately captures personalized values and achieves greater profits than baseline approaches.</article>","contentLength":1610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Service Level Objectives and System Level Metrics in Large Language Model Serving","url":"https://arxiv.org/abs/2410.14257","date":1761796800,"author":"","guid":321359,"unread":true,"content":"<article>arXiv:2410.14257v2 Announce Type: replace \nAbstract: User experience is a critical factor Large Language Model (LLM) serving systems must consider, where service level objectives (SLOs) considering the experience of individual requests and system level metrics (SLMs) considering the overall system performance are two key performance measures. However, we observe two notable issues in existing metrics: 1) manually delaying the delivery of some tokens can improve SLOs, and 2) actively abandoning requests that do not meet SLOs can improve SLMs, both of which are counterintuitive.\n  In this paper, we revisit SLOs and SLMs in LLM serving, and propose a new SLO that aligns with user experience. Based on the SLO, we propose a comprehensive metric framework called smooth goodput, which integrates SLOs and SLMs to reflect the nature of user experience in LLM serving. Through this unified framework, we reassess the performance of different LLM serving systems under multiple workloads. Evaluation results show that our metric framework provides a more comprehensive view of token delivery and request processing, and effectively captures the optimal point of user experience and system performance with different serving strategies.</article>","contentLength":1236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting","url":"https://arxiv.org/abs/2410.12593","date":1761796800,"author":"","guid":321360,"unread":true,"content":"<article>arXiv:2410.12593v3 Announce Type: replace \nAbstract: The widespread deployment of sensing devices leads to a surge in data for spatio-temporal forecasting applications such as traffic flow, air quality, and wind energy. Although spatio-temporal graph neural networks have achieved success in modeling various static spatio-temporal forecasting scenarios, real-world spatio-temporal data are typically received in a streaming manner, and the network continuously expands with the installation of new sensors. Thus, spatio-temporal forecasting in streaming scenarios faces dual challenges: the inefficiency of retraining models over newly arrived data and the detrimental effects of catastrophic forgetting over long-term history. To address these challenges, we propose a novel prompt tuning-based continuous forecasting method, following two fundamental tuning principles guided by empirical and theoretical analysis: expand and compress, which effectively resolve the aforementioned problems with lightweight tuning parameters. Specifically, we integrate the base spatio-temporal graph neural network with a continuous prompt pool, utilizing stored prompts (i.e., few learnable parameters) in memory, and jointly optimize them with the base spatio-temporal graph neural network. This method ensures that the model sequentially learns from the spatio-temporal data stream to accomplish tasks for corresponding periods. Extensive experimental results on multiple real-world datasets demonstrate the multi-faceted superiority of our method over the state-of-the-art baselines, including effectiveness, efficiency, universality, etc.</article>","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Kinematic Synthesis and Prototype Development of Knee Exoskeleton","url":"https://arxiv.org/abs/2409.02635","date":1761796800,"author":"","guid":321361,"unread":true,"content":"<article>arXiv:2409.02635v3 Announce Type: replace \nAbstract: The range of rotation (RoR) in a knee exoskeleton is a critical factor in rehabilitation, as it directly influences joint mobility, muscle activation, and recovery outcomes. A well-designed RoR ensures that patients achieve near-natural knee kinematics, which is essential for restoring gait patterns and preventing compensatory movements. This paper presents optimal design of one degree of freedom knee exoskeleton. In kinematic analysis, the existing design being represented by nonlinear and nonconvex mathematical functions. To obtain feasible and optimum measurement of the links of knee exoskeleton, an optimization problem is formulated based on the kinematic analysis and average human's leg measurement. The optimized solution increases the range of motion of knee exoskeleton during sit to stand motion by $24 \\%$ as compared with inspired design. Furthermore, misalignment study is conducted by comparing the trajectory of human's knee and exoskeleton's knee during sit to stand motion. The joint movement is calculated using marker and camera system. Finally, a prototype of the knee joint exoskeleton is being developed based on optimal dimensions which validate the maximum range of motion achieved during simulation.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs","url":"https://arxiv.org/abs/2408.11832","date":1761796800,"author":"","guid":321362,"unread":true,"content":"<article>arXiv:2408.11832v3 Announce Type: replace \nAbstract: The increased use of large language models (LLMs) across a variety of real-world applications calls for automatic tools to check the factual accuracy of their outputs, as LLMs often hallucinate. This is difficult as it requires assessing the factuality of free-form open-domain responses. While there has been a lot of research on this topic, different papers use different evaluation benchmarks and measures, which makes them hard to compare and hampers future progress. To mitigate these issues, we developed OpenFactCheck, a unified framework, with three modules: (i) RESPONSEEVAL, which allows users to easily customize an automatic fact-checking system and to assess the factuality of all claims in an input document using that system, (ii) LLMEVAL, which assesses the overall factuality of an LLM, and (iii) CHECKEREVAL, a module to evaluate automatic fact-checking systems. OpenFactCheck is open-sourced (https://github.com/mbzuai-nlp/openfactcheck) and publicly released as a Python library (https://pypi.org/project/openfactcheck/) and also as a web service (http://app.openfactcheck.com). A video describing the system is available at https://youtu.be/-i9VKL0HleI.</article>","contentLength":1227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"U-DECN: End-to-End Underwater Object Detection ConvNet with Improved DeNoising Training","url":"https://arxiv.org/abs/2408.05780","date":1761796800,"author":"","guid":321363,"unread":true,"content":"<article>arXiv:2408.05780v2 Announce Type: replace \nAbstract: Underwater object detection has higher requirements of running speed and deployment efficiency for the detector due to its specific environmental challenges. NMS of two- or one-stage object detectors and transformer architecture of query-based end-to-end object detectors are not conducive to deployment on underwater embedded devices with limited processing power. As for the detrimental effect of underwater color cast noise, recent underwater object detectors make network architecture or training complex, which also hinders their application and deployment on unmanned underwater vehicles. In this paper, we propose the Underwater DECO with improved deNoising training (U-DECN), the query-based end-to-end object detector (with ConvNet encoder-decoder architecture) for underwater color cast noise that addresses the above problems. We integrate advanced technologies from DETR variants into DECO and design optimization methods specifically for the ConvNet architecture, including Deformable Convolution in SIM and Separate Contrastive DeNoising Forward methods. To address the underwater color cast noise issue, we propose an Underwater Color DeNoising Query method to improve the generalization of the model for the biased object feature information by different color cast noise. Our U-DECN, with ResNet-50 backbone, achieves the best 64.0 AP on DUO and the best 58.1 AP on RUOD, and 21 FPS (5 times faster than Deformable DETR and DINO 4 FPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other state-of-the-art query-based end-to-end object detectors. The code is available at https://github.com/LEFTeyex/U-DECN.</article>","contentLength":1685,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TraveLLM: Could you plan my new public transit route in face of a network disruption?","url":"https://arxiv.org/abs/2407.14926","date":1761796800,"author":"","guid":321364,"unread":true,"content":"<article>arXiv:2407.14926v2 Announce Type: replace \nAbstract: Existing navigation systems often fail during urban disruptions, struggling to incorporate real-time events and complex user constraints, such as avoiding specific areas. We address this gap with TraveLLM, a system using Large Language Models (LLMs) for disruption-aware public transit routing. We leverage LLMs' reasoning capabilities to directly process multimodal user queries combining natural language requests (origin, destination, preferences, disruption info) with map data (e.g., subway, bus, bike-share). To evaluate this approach, we design challenging test scenarios reflecting real-world disruptions like weather events, emergencies, and dynamic service availability. We benchmark the performance of state-of-the-art LLMs, including GPT-4, Claude 3, and Gemini, on generating accurate travel plans. Our experiments demonstrate that LLMs, notably GPT-4, can effectively generate viable and context-aware navigation plans under these demanding conditions. These findings suggest a promising direction for using LLMs to build more flexible and intelligent navigation systems capable of handling dynamic disruptions and diverse user needs.</article>","contentLength":1201,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Faster and Simpler Greedy Algorithm for $k$-Median and $k$-Means","url":"https://arxiv.org/abs/2407.11217","date":1761796800,"author":"","guid":321365,"unread":true,"content":"<article>arXiv:2407.11217v3 Announce Type: replace \nAbstract: Clustering problems such as $k$-means and $k$-median are staples of unsupervised learning, and many algorithmic techniques have been developed to tackle their numerous aspects.\n  In this paper, we focus on the class of greedy approximation algorithm, that attracted less attention than local-search or primal-dual counterparts. In particular, we study the recursive greedy algorithm developed by Mettu and Plaxton [SIAM J. Comp 2003]. We provide a simplification of the algorithm, allowing for faster implementation, in graph metrics or in Euclidean space, where our algorithm matches or improves the state-of-the-art.</article>","contentLength":671,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Single Image Estimation of Cell Migration Direction by Deep Circular Regression","url":"https://arxiv.org/abs/2406.19162","date":1761796800,"author":"","guid":321366,"unread":true,"content":"<article>arXiv:2406.19162v2 Announce Type: replace \nAbstract: In this paper, we address the problem of estimating the migration direction of cells based on a single image. A solution to this problem lays the foundation for a variety of applications that were previously not possible. To our knowledge, there is only one related work that employs a classification CNN with four classes (quadrants). However, this approach does not allow for detailed directional resolution. We tackle the single image estimation problem using deep circular regression, with a particular focus on cycle-sensitive methods. On two common datasets, we achieve a mean estimation error of $\\sim\\!17^\\circ$, representing a significant improvement over previous work, which reported estimation error of $30^\\circ$ and $34^\\circ$, respectively.</article>","contentLength":808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reliable Evaluation and Benchmarks for Statement Autoformalization","url":"https://arxiv.org/abs/2406.07222","date":1761796800,"author":"","guid":321367,"unread":true,"content":"<article>arXiv:2406.07222v3 Announce Type: replace \nAbstract: Evaluating statement autoformalization, translating natural language mathematics into formal languages like Lean 4, remains a significant challenge, with few metrics, datasets, and standards to robustly measure progress. In this work, we present a comprehensive approach combining improved metrics, robust benchmarks, and systematic evaluation, to fill this gap. First, we introduce BEq+, an automated metric that correlates strongly with human judgment, along with ProofNetVerif, a new dataset for assessing the quality of evaluation metrics, containing 3,752 annotated examples. Second, we develop two new autoformalization benchmarks: ProofNet#, a corrected version of ProofNet, and RLM25, with 619 new pairs of research-level mathematics from six formalization projects. Through systematic experimentation across these benchmarks, we find that current techniques can achieve up to 45.1% accuracy on undergraduate mathematics but struggle with research-level content without proper context. Our work establishes a reliable foundation for evaluating and advancing autoformalization systems.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness","url":"https://arxiv.org/abs/2405.17220","date":1761796800,"author":"","guid":321368,"unread":true,"content":"<article>arXiv:2405.17220v3 Announce Type: replace \nAbstract: Traditional feedback learning for hallucination reduction relies on labor-intensive manual labeling or expensive proprietary models. This leaves the community without foundational knowledge about how to build high-quality feedback with open-source MLLMs. In this work, we introduce RLAIF-V, a novel framework that aligns MLLMs in a fully open-source paradigm. RLAIF-V maximally explores open-source MLLMs from two perspectives, including high-quality feedback data generation for preference learning and self-feedback guidance for inference-time scaling. Extensive experiments on six benchmarks in both automatic and human evaluation show that RLAIF-V substantially enhances the trustworthiness of models at both preference learning and inference time. RLAIF-V 7B reduces object hallucination by 80.7\\% and overall hallucination by 33.7\\%. Remarkably, RLAIF-V 12B further reveals the self-alignment potential of open-source MLLMs, where the model can learn from feedback of itself to achieve super GPT-4V trustworthiness.</article>","contentLength":1074,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenFactCheck: Building, Benchmarking Customized Fact-Checking Systems and Evaluating the Factuality of Claims and LLMs","url":"https://arxiv.org/abs/2405.05583","date":1761796800,"author":"","guid":321369,"unread":true,"content":"<article>arXiv:2405.05583v3 Announce Type: replace \nAbstract: The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. Difficulties lie in assessing the factuality of free-form responses in open domains. Also, different papers use disparate evaluation benchmarks and measurements, which renders them hard to compare and hampers future progress. To mitigate these issues, we propose OpenFactCheck, a unified framework for building customized automatic fact-checking systems, benchmarking their accuracy, evaluating factuality of LLMs, and verifying claims in a document. OpenFactCheck consists of three modules: (i) CUSTCHECKER allows users to easily customize an automatic fact-checker and verify the factual correctness of documents and claims, (ii) LLMEVAL, a unified evaluation framework assesses LLM's factuality ability from various perspectives fairly, and (iii) CHECKEREVAL is an extensible solution for gauging the reliability of automatic fact-checkers' verification results using human-annotated datasets. Data and code are publicly available at https://github.com/yuxiaw/openfactcheck.</article>","contentLength":1194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI in Lung Health: Benchmarking Detection and Diagnostic Models Across Multiple CT Scan Datasets","url":"https://arxiv.org/abs/2405.04605","date":1761796800,"author":"","guid":321370,"unread":true,"content":"<article>arXiv:2405.04605v4 Announce Type: replace \nAbstract: Background: Development of artificial intelligence (AI) models for lung cancer screening requires large, well-annotated low-dose computed tomography (CT) datasets and rigorous performance benchmarks. Purpose: To create a reproducible benchmarking resource leveraging the Duke Lung Cancer Screening (DLCS) and multiple public datasets to develop and evaluate models for nodule detection and classification. Materials &amp; Methods: This retrospective study uses the DLCS dataset (1,613 patients; 2,487 nodules) and external datasets including LUNA16, LUNA25, and NLST-3D. For detection, MONAI RetinaNet models were trained on DLCS (DLCS-De) and LUNA16 (LUNA16-De) and evaluated using the Competition Performance Metric (CPM). For nodule-level classification, we compare five strategies: pretrained models (Models Genesis, Med3D), a self-supervised foundation model (FMCB), and ResNet50 with random initialization versus Strategic Warm-Start (ResNet50-SWS) pretrained with detection-derived candidate patches stratified by confidence. Results: For detection on the DLCS test set, DLCS-De achieved sensitivity 0.82 at 2 false positives/scan (CPM 0.63) versus LUNA16-De (0.62, CPM 0.45). For external validation on NLST-3D, DLCS-De (sensitivity 0.72, CPM 0.58) also outperformed LUNA16-De (sensitivity 0.64, CPM 0.49). For classification across multiple datasets, ResNet50-SWS attained AUCs of 0.71 (DLCS; 95% CI, 0.61-0.81), 0.90 (LUNA16; 0.87-0.93), 0.81 (NLST-3D; 0.79-0.82), and 0.80 (LUNA25; 0.78-0.82), matching or exceeding pretrained/self-supervised baselines. Performance differences reflected dataset label standards. Conclusion: This work establishes a standardized benchmarking resource for lung cancer AI research, supporting model development, validation, and translation. All code, models, and data are publicly released to promote reproducibility.</article>","contentLength":1908,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal s-boxes against alternative operations and linear propagation","url":"https://arxiv.org/abs/2403.20059","date":1761796800,"author":"","guid":321371,"unread":true,"content":"<article>arXiv:2403.20059v3 Announce Type: replace \nAbstract: Civino et al. (2019) have shown how some diffusion layers can expose a Substitution-Permutation Network to vulnerability from differential cryptanalysis when employing alternative operations coming from groups isomorphic to the translation group on the message space. In this study, we present a classification of diffusion layers that exhibit linearity with respect to certain parallel alternative operations, enabling the possibility of an alternative differential attack simultaneously targeting all the s-boxes within the block. Furthermore, we investigate the differential behaviour with respect to alternative operations for all classes of optimal 4-bit s-boxes, as defined by Leander and Poschmann (2007). Our examination reveals that certain classes contain weak permutations w.r.t. alternative differential attacks. Finally, we leverage these vulnerabilities to execute a series of experiments showing the effectiveness of the cryptanalysis performed with a parallel alternative operation compared to the classical one.</article>","contentLength":1081,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hyperparameters in Continual Learning: A Reality Check","url":"https://arxiv.org/abs/2403.09066","date":1761796800,"author":"","guid":321372,"unread":true,"content":"<article>arXiv:2403.09066v5 Announce Type: replace \nAbstract: Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters (e.g., learning rate, mini-batch size, regularization strengths, etc.) within a given scenario and then evaluates the algorithms using these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP) consisting of hyperparameter tuning and evaluation phases. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol. Our implementation can be found in https://github.com/csm9493/GTEP.</article>","contentLength":1737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CURATRON: Complete and Robust Preference Data for Rigorous Alignment of Large Language Models","url":"https://arxiv.org/abs/2403.02745","date":1761796800,"author":"","guid":321373,"unread":true,"content":"<article>arXiv:2403.02745v3 Announce Type: replace \nAbstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), focusing on incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs' resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an $\\epsilon$-optimal ranking with high probability while allowing as large as $O(n)$ perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorithms handle adversarial noise and unobserved comparisons well in both general and LLM preference dataset settings. This work contributes to the development and scaling of more reliable and ethically aligned AI models by equipping the dataset curation pipeline with the ability to handle missing and maliciously manipulated inputs.</article>","contentLength":1315,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpretation of Inaccessible Sets in Martin-L\\\"of Type Theory with One Mahlo Universe","url":"https://arxiv.org/abs/2402.15074","date":1761796800,"author":"","guid":321374,"unread":true,"content":"<article>arXiv:2402.15074v5 Announce Type: replace \nAbstract: Rathjen proved that Aczel's constructive set theory $\\mathbf{CZF}$ extended with inaccessible sets of all transfinite orders can be interpreted in Martin-L\\\"{o}f type theory $\\mathbf{MLTT}$ extended with Setzer's Mahlo universe and another universe above it. In this paper we show that this interpretation can be carried out bottom-up without the universe above the Mahlo universe, provided we add an accessibility predicate instead. If we work in Martin-L\\\"{o}f type theory with extensional identity types the accessibility predicate can be defined in terms of $\\mathrm{W}$-types. The main part of our interpretation has been formalised in the proof assistant Agda.</article>","contentLength":719,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Range (R\\'enyi) Entropy Queries and Partitioning","url":"https://arxiv.org/abs/2312.15959","date":1761796800,"author":"","guid":321375,"unread":true,"content":"<article>arXiv:2312.15959v5 Announce Type: replace \nAbstract: Data partitioning that maximizes/minimizes the Shannon entropy, or more generally the R\\'enyi entropy is a crucial subroutine in data compression, columnar storage, and cardinality estimation algorithms. These partition algorithms can be accelerated if we have a data structure to compute the entropy in different subsets of data when the algorithm needs to decide what block to construct. Such a data structure will also be useful for data analysts exploring different subsets of data to identify areas of interest. While it is generally known how to compute the Shannon or the R\\'enyi entropy of a discrete distribution in the offline or streaming setting efficiently, we focus on the query setting where we aim to efficiently derive the entropy among a subset of data that satisfy some linear predicates. We solve this problem in a typical setting when we deal with real data, where data items are geometric points and each requested area is a query (hyper)rectangle. More specifically, we consider a set $P$ of $n$ weighted and colored points in $\\mathbb{R}^d$, where $d$ is a constant. For the range S-entropy (resp. R-entropy) query problem, the goal is to construct a low space data structure, such that given a query (hyper)rectangle $R$, it computes the Shannon (resp. R\\'enyi) entropy based on the colors and the weights of the points in $P\\cap R$, in sublinear time. We show conditional lower bounds proving that we cannot hope for data structures with near-linear space and near-constant query time for both the range S-entropy and R-entropy query problems. Then, we propose exact data structures for $d=1$ and $d&gt;1$ with $o(n^{2d})$ space and $o(n)$ query time for both problems. Finally, we propose near linear space data structures for returning either an additive or a multiplicative approximation of the Shannon (resp. R\\'enyi) entropy in $P\\cap R$.</article>","contentLength":1919,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Score-Aware Policy-Gradient and Performance Guarantees using Local Lyapunov Stability","url":"https://arxiv.org/abs/2312.02804","date":1761796800,"author":"","guid":321376,"unread":true,"content":"<article>arXiv:2312.02804v3 Announce Type: replace \nAbstract: In this paper, we introduce a policy-gradient method for model-based reinforcement learning (RL) that exploits a type of stationary distributions commonly obtained from Markov decision processes (MDPs) in stochastic networks, queueing systems, and statistical mechanics. Specifically, when the stationary distribution of the MDP belongs to an exponential family that is parametrized by policy parameters, we can improve existing policy gradient methods for average-reward RL. Our key identification is a family of gradient estimators, called score-aware gradient estimators (SAGEs), that enable policy gradient estimation without relying on value-function estimation in the aforementioned setting. We show that SAGE-based policy-gradient locally converges, and we obtain its regret. This includes cases when the state space of the MDP is countable and unstable policies can exist. Under appropriate assumptions such as starting sufficiently close to a maximizer and the existence of a local Lyapunov function, the policy under SAGE-based stochastic gradient ascent has an overwhelming probability of converging to the associated optimal policy. Furthermore, we conduct a numerical comparison between a SAGE-based policy-gradient method and an actor-critic method on several examples inspired from stochastic networks, queueing systems, and models derived from statistical physics. Our results demonstrate that a SAGE-based method finds close-to-optimal policies faster than an actor-critic method.</article>","contentLength":1550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CAT-RRT: Motion Planning that Admits Contact One Link at a Time","url":"https://arxiv.org/abs/2310.06210","date":1761796800,"author":"","guid":321377,"unread":true,"content":"<article>arXiv:2310.06210v2 Announce Type: replace \nAbstract: Current motion planning approaches rely on binary collision checking to evaluate the validity of a state and thereby dictate where the robot is allowed to move. This approach leaves little room for robots to engage in contact with an object, as is often necessary when operating in densely cluttered spaces. In this work, we propose an alternative method that considers contact states as high-cost states that the robot should avoid but can traverse if necessary to complete a task. More specifically, we introduce Contact Admissible Transition-based Rapidly exploring Random Trees (CAT-RRT), a planner that uses a novel per-link cost heuristic to find a path by traversing high-cost obstacle regions. Through extensive testing, we find that state-of-the-art optimization planners tend to over-explore low-cost states, which leads to slow and inefficient convergence to contact regions. Conversely, CAT-RRT searches both low and high-cost regions simultaneously with an adaptive thresholding mechanism carried out at each robot link. This leads to paths with a balance between efficiency, path length, and contact cost.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MP-FVM: Enhancing Finite Volume Method for Water Infiltration Modeling in Unsaturated Soils via Message-passing Encoder-decoder Network","url":"https://arxiv.org/abs/2310.02806","date":1761796800,"author":"","guid":321378,"unread":true,"content":"<article>arXiv:2310.02806v3 Announce Type: replace \nAbstract: The spatiotemporal water flow dynamics in unsaturated soils can generally be modeled by the Richards equation. To overcome the computational challenges associated with solving this highly nonlinear partial differential equation (PDE), we present a novel solution algorithm, which we name as the MP-FVM (Message Passing-Finite Volume Method), to holistically integrate adaptive fixed-point iteration scheme, encoder-decoder neural network architecture, Sobolev training, and message passing mechanism in a finite volume discretization framework. We thoroughly discuss the need and benefits of introducing these components to achieve synergistic improvements in accuracy and stability of the solution. We also show that our MP-FVM algorithm can accurately solve the mixed-form $n$-dimensional Richards equation with guaranteed convergence under reasonable assumptions. Through several illustrative examples, we demonstrate that our MP-FVM algorithm not only achieves superior accuracy, but also better preserves the underlying physical laws and mass conservation of the Richards equation compared to state-of-the-art solution algorithms and the commercial HYDRUS solver.</article>","contentLength":1221,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partially Observable Multi-Agent Reinforcement Learning with Information Sharing","url":"https://arxiv.org/abs/2308.08705","date":1761796800,"author":"","guid":321379,"unread":true,"content":"<article>arXiv:2308.08705v4 Announce Type: replace \nAbstract: We study provable multi-agent reinforcement learning (RL) in the general framework of partially observable stochastic games (POSGs). To circumvent the known hardness results and the use of computationally intractable oracles, we advocate leveraging the potential \\emph{information-sharing} among agents, a common practice in empirical multi-agent RL, and a standard model for multi-agent control systems with communication. We first establish several computational complexity results to justify the necessity of information-sharing, as well as the observability assumption that has enabled quasi-polynomial time and sample single-agent RL with partial observations, for tractably solving POSGs. Inspired by the inefficiency of planning in the ground-truth model, we then propose to further \\emph{approximate} the shared common information to construct an approximate model of the POSG, in which an approximate \\emph{equilibrium} (of the original POSG) can be found in quasi-polynomial-time, under the aforementioned assumptions. Furthermore, we develop a partially observable multi-agent RL algorithm whose time and sample complexities are \\emph{both} quasi-polynomial. Finally, beyond equilibrium learning, we extend our algorithmic framework to finding the \\emph{team-optimal solution} in cooperative POSGs, i.e., decentralized partially observable Markov decision processes, a more challenging goal. We establish concrete computational and sample complexities under several structural assumptions of the model. We hope our study could open up the possibilities of leveraging and even designing different \\emph{information structures}, a well-studied notion in control theory, for developing both sample- and computation-efficient partially observable multi-agent RL.</article>","contentLength":1822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Brain-inspired Computational Intelligence via Predictive Coding","url":"https://arxiv.org/abs/2308.07870","date":1761796800,"author":"","guid":321380,"unread":true,"content":"<article>arXiv:2308.07870v3 Announce Type: replace \nAbstract: Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with a learning algorithm called error backpropagation, always considered biologically implausible. To this end, recent works have studied learning algorithms for deep neural networks inspired by the neurosciences. One such theory, called predictive coding (PC), has shown promising properties that make it potentially valuable for the machine learning community: it can model information processing in different areas of the brain, can be used in control and robotics, has a solid mathematical foundation in variational inference, and performs its computations asynchronously. Inspired by such properties, works that propose novel PC-like algorithms are starting to be present in multiple sub-fields of machine learning and AI at large. Here, we survey such efforts by first providing a broad overview of the history of PC to provide common ground for the understanding of the recent developments, then by describing current efforts and results, and concluding with a large discussion of possible implications and ways forward.</article>","contentLength":1268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do predictability factors towards signing avatars hold across cultures?","url":"https://arxiv.org/abs/2307.02103","date":1761796800,"author":"","guid":321381,"unread":true,"content":"<article>arXiv:2307.02103v3 Announce Type: replace \nAbstract: Avatar technology can offer accessibility possibilities and improve the Deaf-and-Hard of Hearing sign language users access to communication, education and services, such as the healthcare system. However, sign language users acceptance of signing avatars as well as their attitudes towards them vary and depend on many factors. Furthermore, research on avatar technology is mostly done by researchers who are not Deaf. The study examines the extent to which intrinsic or extrinsic factors contribute to predict the attitude towards avatars across cultures. Intrinsic factors include the characteristics of the avatar, such as appearance, movements and facial expressions. Extrinsic factors include users technology experience, their hearing status, age and their sign language fluency. This work attempts to answer questions such as, if lower attitude ratings are related to poor technology experience with ASL users, for example, is that also true for Moroccan Sign Language (MSL) users? For the purposes of the study, we designed a questionnaire to understand MSL users attitude towards avatars. Three groups of participants were surveyed: Deaf (57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then compared with those reported in other relevant studies.</article>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-robot Motion Planning based on Nets-within-Nets Modeling and Simulation","url":"https://arxiv.org/abs/2304.08772","date":1761796800,"author":"","guid":321382,"unread":true,"content":"<article>arXiv:2304.08772v4 Announce Type: replace \nAbstract: This paper focuses on designing motion plans for a heterogeneous team of robots that must cooperate to fulfill a global mission. Robots move in an environment that contains some regions of interest, while the specification for the entire team can include avoidance, visits, or sequencing of these regions of interest. The mission is expressed in terms of a Petri net corresponding to an automaton, while each robot is also modeled by a state machine Petri net. The current work brings about the following contributions with respect to existing solutions for related problems. First, we propose a novel model, denoted High-Level robot team Petri Net (HLrtPN) system, to incorporate the specification and robot models into the Nets-within-Nets paradigm. A guard function, named Global Enabling Function, is designed to synchronize the firing of transitions so that robot motions do not violate the specification. Then, the solution is found by simulating the HLrtPN system in a specific software tool that accommodates Nets-within-Nets. Illustrative examples based on Linear Temporal Logic missions support the computational feasibility of the proposed framework.</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Privacy Preservation by Local Design in Cooperative Networked Control Systems","url":"https://arxiv.org/abs/2207.03904","date":1761796800,"author":"","guid":321383,"unread":true,"content":"<article>arXiv:2207.03904v2 Announce Type: replace \nAbstract: In this paper, we study the privacy preservation problem in a cooperative networked control system, which has closed-loop dynamics, working for the task of linear quadratic Guassian (LQG) control. The system consists of a user and a server: the user owns the plant to control, while the server provides computation capability, and the user employs the server to compute control inputs for it. To enable the server's computation, the user needs to provide the measurements of the plant states to the server, who then calculates estimates of the states, based on which the control inputs are computed. However, the user regards the states as privacy, and makes an interesting request: the user wants the server to have \"incorrect\" knowledge of the state estimates rather than the true values. Regarding that, we propose a novel design methodology for the privacy preservation, in which the privacy scheme is locally equipped at the user side not open to the server, which manages to create a deviation in the server's knowledge of the state estimates from the true values. However, this methodology also raises significant challenges: in a closed-loop dynamic system, when the server's seized knowledge is incorrect, the system's behavior becomes complex to analyze; even the stability of the system becomes questionable, as the incorrectness will accumulate through the closed loop as time evolves. In this paper, we succeed in showing that the performance loss in LQG control caused by the proposed privacy scheme is bounded by rigorous mathematical proofs, which convinces the availability of the proposed design methodology. We also propose an associated novel privacy metric and obtain the analytical result on evaluating the privacy performance. Finally, we study the performance trade-off between privacy and control, where the accordingly proposed optimization problems are solved by numerical methods efficiently.</article>","contentLength":1973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spoiler Susceptibility in Party Elections","url":"https://arxiv.org/abs/2202.05115","date":1761796800,"author":"","guid":321384,"unread":true,"content":"<article>arXiv:2202.05115v2 Announce Type: replace \nAbstract: An electoral spoiler is usually defined as a losing candidate whose removal would affect the outcome by changing the winner. So far, spoiler effects have been analyzed primarily for single-winner electoral systems. We consider this subject in the context of party elections, where there is no longer a sharp distinction between winners and losers. Hence, we propose a more general definition, under which a party is a spoiler if their elimination causes any other party's share in the outcome to decrease. We characterize spoiler-proof electoral allocation rules for zero-sum voting methods. In particular, we prove that for seats-votes functions only identity is spoiler-proof. However, even if spoilers are unavoidable under common electoral rules, their expected impact can vary depending on the rule. Hence, we introduce a measure of spoilership, which allows us to experimentally compare a number of multiwinner social choice rules according to their spoiler susceptibility. Since the probabilistic models used in COMSOC have been developed for nonparty elections, we extend them to generate multidistrict party elections.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models for Few-Shot Named Entity Recognition","url":"https://arxiv.org/abs/1810.06818","date":1761796800,"author":"","guid":321385,"unread":true,"content":"<article>arXiv:1810.06818v3 Announce Type: replace \nAbstract: Named entity recognition (NER) is a fundamental task in numerous downstream applications. Recently, researchers have employed pre-trained language models (PLMs) and large language models (LLMs) to address this task. However, fully leveraging the capabilities of PLMs and LLMs with minimal human effort remains challenging. In this paper, we propose GPT4NER, a method that prompts LLMs to resolve the few-shot NER task. GPT4NER constructs effective prompts using three key components: entity definition, few-shot examples, and chain-of-thought. By prompting LLMs with these effective prompts, GPT4NER transforms few-shot NER, which is traditionally considered as a sequence-labeling problem, into a sequence-generation problem. We conduct experiments on two benchmark datasets, CoNLL2003 and OntoNotes5.0, and compare the performance of GPT4NER to representative state-of-the-art models in both few-shot and fully supervised settings. Experimental results demonstrate that GPT4NER achieves the $F_1$ of 83.15\\% on CoNLL2003 and 70.37\\% on OntoNotes5.0, significantly outperforming few-shot baselines by an average margin of 7 points. Compared to fully-supervised baselines, GPT4NER achieves 87.9\\% of their best performance on CoNLL2003 and 76.4\\% of their best performance on OntoNotes5.0. We also utilize a relaxed-match metric for evaluation and report performance in the sub-task of named entity extraction (NEE), and experiments demonstrate their usefulness to help better understand model behaviors in the NER task.</article>","contentLength":1573,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Functional correspondence by matrix completion","url":"https://arxiv.org/abs/1412.8070","date":1761796800,"author":"","guid":321386,"unread":true,"content":"<article>arXiv:1412.8070v2 Announce Type: replace \nAbstract: In this paper, we consider the problem of finding dense intrinsic correspondence between manifolds using the recently introduced functional framework. We pose the functional correspondence problem as matrix completion with manifold geometric structure and inducing functional localization with the $L_1$ norm. We discuss efficient numerical procedures for the solution of our problem. Our method compares favorably to the accuracy of state-of-the-art correspondence algorithms on non-rigid shape matching benchmarks, and is especially advantageous in settings when only scarce data is available.</article>","contentLength":647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"E-Scores for (In)Correctness Assessment of Generative Model Outputs","url":"https://arxiv.org/abs/2510.25770","date":1761796800,"author":"","guid":321387,"unread":true,"content":"<article>arXiv:2510.25770v1 Announce Type: cross \nAbstract: While generative models, especially large language models (LLMs), are ubiquitous in today's world, principled mechanisms to assess their (in)correctness are limited. Using the conformal prediction framework, previous works construct sets of LLM responses where the probability of including an incorrect response, or error, is capped at a desired user-defined tolerance level. However, since these methods are based on p-values, they are susceptible to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the guarantees. We therefore leverage e-values to complement generative model outputs with e-scores as a measure of incorrectness. In addition to achieving the same statistical guarantees as before, e-scores provide users flexibility in adaptively choosing tolerance levels after observing the e-scores themselves, by upper bounding a post-hoc notion of error called size distortion. We experimentally demonstrate their efficacy in assessing LLM outputs for different correctness types: mathematical factuality and property constraints satisfaction.</article>","contentLength":1122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs","url":"https://arxiv.org/abs/2510.25753","date":1761796800,"author":"","guid":321388,"unread":true,"content":"<article>arXiv:2510.25753v1 Announce Type: cross \nAbstract: Pretrained Transformers demonstrate remarkable in-context learning (ICL) capabilities, enabling them to adapt to new tasks from demonstrations without parameter updates. However, theoretical studies often rely on simplified architectures (e.g., omitting MLPs), data models (e.g., linear regression with isotropic inputs), and single-source training, limiting their relevance to realistic settings. In this work, we study ICL in pretrained Transformers with nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with heterogeneous input, task, and noise distributions. We analyze a model where the MLP comprises two layers, with the first layer trained via a single gradient step and the second layer fully optimized. Under high-dimensional asymptotics, we prove that such models are equivalent in ICL error to structured polynomial predictors, leveraging results from the theory of Gaussian universality and orthogonal polynomials. This equivalence reveals that nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear tasks, compared to linear baselines. It also enables a precise analysis of data mixing effects: we identify key properties of high-quality data sources (low noise, structured covariances) and show that feature learning emerges only when the task covariance exhibits sufficient structure. These results are validated empirically across various activation functions, model sizes, and data distributions. Finally, we experiment with a real-world scenario involving multilingual sentiment analysis where each language is treated as a different source. Our experimental results for this case exemplify how our findings extend to real-world cases. Overall, our work advances the theoretical foundations of ICL in Transformers and provides actionable insight into the role of architecture and data in ICL.</article>","contentLength":1908,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inverse-free quantum state estimation with Heisenberg scaling","url":"https://arxiv.org/abs/2510.25750","date":1761796800,"author":"","guid":321389,"unread":true,"content":"<article>arXiv:2510.25750v1 Announce Type: cross \nAbstract: In this paper, we present an inverse-free pure quantum state estimation protocol that achieves Heisenberg scaling. Specifically, let $\\mathcal{H}\\cong \\mathbb{C}^d$ be a $d$-dimensional Hilbert space with an orthonormal basis $\\{|1\\rangle,\\ldots,|d\\rangle\\}$ and $U$ be an unknown unitary on $\\mathcal{H}$. Our protocol estimates $U|d\\rangle$ to within trace distance error $\\varepsilon$ using $O(\\min\\{d^{3/2}/\\varepsilon,d/\\varepsilon^2\\})$ forward queries to $U$. This complements the previous result $O(d\\log(d)/\\varepsilon)$ by van Apeldoorn, Cornelissen, Gily\\'en, and Nannicini (SODA 2023), which requires both forward and inverse queries. Moreover, our result implies a query upper bound $O(\\min\\{d^{3/2}/\\varepsilon,1/\\varepsilon^2\\})$ for inverse-free amplitude estimation, improving the previous best upper bound $O(\\min\\{d^{2}/\\varepsilon,1/\\varepsilon^2\\})$ based on optimal unitary estimation by Haah, Kothari, O'Donnell, and Tang (FOCS 2023), and disproving a conjecture posed in Tang and Wright (2025).</article>","contentLength":1069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Physics-Guided Conditional Diffusion Networks for Microwave Image Reconstruction","url":"https://arxiv.org/abs/2510.25729","date":1761796800,"author":"","guid":321390,"unread":true,"content":"<article>arXiv:2510.25729v1 Announce Type: cross \nAbstract: A conditional latent-diffusion based framework for solving the electromagnetic inverse scattering problem associated with microwave imaging is introduced. This generative machine-learning model explicitly mirrors the non-uniqueness of the ill-posed inverse problem. Unlike existing inverse solvers utilizing deterministic machine learning techniques that produce a single reconstruction, the proposed latent-diffusion model generates multiple plausible permittivity maps conditioned on measured scattered-field data, thereby generating several potential instances in the range-space of the non-unique inverse mapping. A forward electromagnetic solver is integrated into the reconstruction pipeline as a physics-based evaluation mechanism. The space of candidate reconstructions form a distribution of possibilities consistent with the conditioning data and the member of this space yielding the lowest scattered-field data discrepancy between the predicted and measured scattered fields is reported as the final solution. Synthetic and experimental labeled datasets are used for training and evaluation of the model. An innovative labeled synthetic dataset is created that exemplifies a varied set of scattering features. Training of the model using this new dataset produces high quality permittivity reconstructions achieving improved generalization with excellent fidelity to shape recognition. The results highlight the potential of hybrid generative physics frameworks as a promising direction for robust, data-driven microwave imaging.</article>","contentLength":1592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling flow-based approaches for topology sampling in $\\mathrm{SU}(3)$ gauge theory","url":"https://arxiv.org/abs/2510.25704","date":1761796800,"author":"","guid":321391,"unread":true,"content":"<article>arXiv:2510.25704v1 Announce Type: cross \nAbstract: We develop a methodology based on out-of-equilibrium simulations to mitigate topological freezing when approaching the continuum limit of lattice gauge theories. We reduce the autocorrelation of the topological charge employing open boundary conditions, while removing exactly their unphysical effects using a non-equilibrium Monte Carlo approach in which periodic boundary conditions are gradually switched on. We perform a detailed analysis of the computational costs of this strategy in the case of the four-dimensional $\\mathrm{SU}(3)$ Yang-Mills theory. After achieving full control of the scaling, we outline a clear strategy to sample topology efficiently in the continuum limit, which we check at lattice spacings as small as $0.045$ fm. We also generalize this approach by designing a customized Stochastic Normalizing Flow for evolutions in the boundary conditions, obtaining superior performances with respect to the purely stochastic non-equilibrium approach, and paving the way for more efficient future flow-based solutions.</article>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PyDPF: A Python Package for Differentiable Particle Filtering","url":"https://arxiv.org/abs/2510.25693","date":1761796800,"author":"","guid":321392,"unread":true,"content":"<article>arXiv:2510.25693v1 Announce Type: cross \nAbstract: State-space models (SSMs) are a widely used tool in time series analysis. In the complex systems that arise from real-world data, it is common to employ particle filtering (PF), an efficient Monte Carlo method for estimating the hidden state corresponding to a sequence of observations. Applying particle filtering requires specifying both the parametric form and the parameters of the system, which are often unknown and must be estimated. Gradient-based optimisation techniques cannot be applied directly to standard particle filters, as the filters themselves are not differentiable. However, several recently proposed methods modify the resampling step to make particle filtering differentiable. In this paper, we present an implementation of several such differentiable particle filters (DPFs) with a unified API built on the popular PyTorch framework. Our implementation makes these algorithms easily accessible to a broader research community and facilitates straightforward comparison between them. We validate our framework by reproducing experiments from several existing studies and demonstrate how DPFs can be applied to address several common challenges with state space modelling.</article>","contentLength":1245,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accurate Leakage Speculation for Quantum Error Correction","url":"https://arxiv.org/abs/2510.25661","date":1761796800,"author":"","guid":321393,"unread":true,"content":"<article>arXiv:2510.25661v1 Announce Type: cross \nAbstract: Quantum Error Correction (QEC) protects qubits against bit- and phase-flip errors in the |0&gt; or |1&gt; subspace, but physical qubits can also leak into higher energy levels (e.g., |2&gt;). Leakage is especially harmful, as it corrupts all subsequent syndrome measurements and can spread to neighboring qubits. Detecting leakage on data qubits is particularly challenging, since they are never measured directly during QEC cycles. Prior work, such as eraser, addresses this by inferring leakage from syndrome patterns using a fixed heuristic. However, this approach often misclassifies benign syndromes, triggering excessive leakage-reduction circuits (LRCs). Because LRCs are themselves noisy and slow, these false triggers lengthen QEC cycles and inflate logical error rates.\n  We propose gladiator, a general and adaptable leakage speculation framework that works across surface code, color code, and qLDPC codes. Offline, gladiator builds a code-aware error-propagation graph calibrated to device data. Online, it classifies each syndrome in a few nanoseconds and schedules LRC only when the observed pattern is provably leakage-dominated. This precise speculation eliminates up to 3x (and on average 2x) unnecessary LRCs, shortens QEC cycles, and suppresses false positives at their source. Evaluated on standard fault-tolerant benchmarks, gladiator delivers 1.7x-3.9x speedups and 16% reduction in logical error rate, advancing the efficiency of fault-tolerant quantum computing.</article>","contentLength":1529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continuous subsurface property retrieval from sparse radar observations using physics informed neural networks","url":"https://arxiv.org/abs/2510.25648","date":1761796800,"author":"","guid":321394,"unread":true,"content":"<article>arXiv:2510.25648v1 Announce Type: cross \nAbstract: Estimating subsurface dielectric properties is essential for applications ranging from environmental surveys of soils to nondestructive evaluation of concrete in infrastructure. Conventional wave inversion methods typically assume few discrete homogeneous layers and require dense measurements or strong prior knowledge of material boundaries, limiting scalability and accuracy in realistic settings where properties vary continuously. We present a physics informed machine learning framework that reconstructs subsurface permittivity as a fully neural, continuous function of depth, trained to satisfy both measurement data and Maxwells equations. We validate the framework with both simulations and custom built radar experiments on multilayered natural materials. Results show close agreement with in-situ permittivity measurements (R^2=0.93), with sensitivity to even subtle variations (Delta eps_r=2). Parametric analysis reveals that accurate profiles can be recovered with as few as three strategically placed sensors in two layer systems. This approach reframes subsurface inversion from boundary-driven to continuous property estimation, enabling accurate characterization of smooth permittivity variations and advancing electromagnetic imaging using low cost radar systems.</article>","contentLength":1334,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fractional Iterates and Oscillatory Convergence","url":"https://arxiv.org/abs/2510.25606","date":1761796800,"author":"","guid":321395,"unread":true,"content":"<article>arXiv:2510.25606v1 Announce Type: cross \nAbstract: The simple continued fractions for the Golden &amp; Silver means are well-known. It is astonishing that, as far as we know, no one has published half-iterates (let alone quarter-iterates) for the corresponding algorithms. We also examine the cosine and logistic maps (with parameter $2 &lt; \\lambda &lt; 3$).</article>","contentLength":349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Systematic Non-Binary Extension of LDPC-CSS Codes Preserving Orthogonality","url":"https://arxiv.org/abs/2510.25583","date":1761796800,"author":"","guid":321396,"unread":true,"content":"<article>arXiv:2510.25583v1 Announce Type: cross \nAbstract: We study finite-field extensions that preserve the same support as the parity-check matrices defining a given binary CSS code. Here, an LDPC-CSS code refers to a CSS code whose parity-check matrices are orthogonal in the sense that each pair of corresponding rows overlaps in an even (possibly zero) number of positions, typically at most twice in sparse constructions. Beyond the low-density setting, we further propose a systematic construction method that extends to arbitrary CSS codes, providing feasible finite-field generalizations that maintain both the binary support and the orthogonality condition.</article>","contentLength":660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models","url":"https://arxiv.org/abs/2510.25577","date":1761796800,"author":"","guid":321397,"unread":true,"content":"<article>arXiv:2510.25577v1 Announce Type: cross \nAbstract: Recent advances in speech foundation models (SFMs) have enabled the direct processing of spoken language from raw audio, bypassing intermediate textual representations. This capability allows SFMs to be exposed to, and potentially respond to, rich paralinguistic variations embedded in the input speech signal. One under-explored dimension of paralinguistic variation is voice quality, encompassing phonation types such as creaky and breathy voice. These phonation types are known to influence how listeners infer affective state, stance and social meaning in speech. Existing benchmarks for speech understanding largely rely on multiple-choice question answering (MCQA) formats, which are prone to failure and therefore unreliable in capturing the nuanced ways paralinguistic features influence model behaviour. In this paper, we probe SFMs through open-ended generation tasks and speech emotion recognition, evaluating whether model behaviours are consistent across different phonation inputs. We introduce a new parallel dataset featuring synthesized modifications to voice quality, designed to evaluate SFM responses to creaky and breathy voice. Our work provides the first examination of SFM sensitivity to these particular non-lexical aspects of speech perception.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification","url":"https://arxiv.org/abs/2510.25573","date":1761796800,"author":"","guid":321398,"unread":true,"content":"<article>arXiv:2510.25573v1 Announce Type: cross \nAbstract: Machine learning approaches for image classification have led to impressive advances in that field. For example, convolutional neural networks are able to achieve remarkable image classification accuracy across a wide range of applications in industry, defense, and other areas. While these machine learning models boast impressive accuracy, a related concern is how to assess and maintain calibration in the predictions these models make. A classification model is said to be well calibrated if its predicted probabilities correspond with the rates events actually occur. While there are many available methods to assess machine learning calibration and recalibrate faulty predictions, less effort has been spent on developing approaches that continually monitor predictive models for potential loss of calibration as time passes. We propose a cumulative sum-based approach with dynamic limits that enable detection of miscalibration in both traditional process monitoring and concept drift applications. This enables early detection of operational context changes that impact image classification performance in the field. The proposed chart can be used broadly in any situation where the user needs to monitor probability predictions over time for potential lapses in calibration. Importantly, our method operates on probability predictions and event outcomes and does not require under-the-hood access to the machine learning model.</article>","contentLength":1487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PitchFlower: A flow-based neural audio codec with pitch controllability","url":"https://arxiv.org/abs/2510.25566","date":1761796800,"author":"","guid":321399,"unread":true,"content":"<article>arXiv:2510.25566v1 Announce Type: cross \nAbstract: We present PitchFlower, a flow-based neural audio codec with explicit pitch controllability. Our approach enforces disentanglement through a simple perturbation: during training, F0 contours are flattened and randomly shifted, while the true F0 is provided as conditioning. A vector-quantization bottleneck prevents pitch recovery, and a flow-based decoder generates high quality audio. Experiments show that PitchFlower achieves more accurate pitch control than WORLD at much higher audio quality, and outperforms SiFiGAN in controllability while maintaining comparable quality. Beyond pitch, this framework provides a simple and extensible path toward disentangling other speech attributes.</article>","contentLength":743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust variable selection for spatial point processes observed with noise","url":"https://arxiv.org/abs/2510.25550","date":1761796800,"author":"","guid":321400,"unread":true,"content":"<article>arXiv:2510.25550v1 Announce Type: cross \nAbstract: We propose a method for variable selection in the intensity function of spatial point processes that combines sparsity-promoting estimation with noise-robust model selection. As high-resolution spatial data becomes increasingly available through remote sensing and automated image analysis, identifying spatial covariates that influence the localization of events is crucial to understand the underlying mechanism. However, results from automated acquisition techniques are often noisy, for example due to measurement uncertainties or detection errors, which leads to spurious displacements and missed events. We study the impact of such noise on sparse point-process estimation across different models, including Poisson and Thomas processes. To improve noise robustness, we propose to use stability selection based on point-process subsampling and to incorporate a non-convex best-subset penalty to enhance model-selection performance. In extensive simulations, we demonstrate that such an approach reliably recovers true covariates under diverse noise scenarios and improves both selection accuracy and stability. We then apply the proposed method to a forestry data set, analyzing the distribution of trees in relation to elevation and soil nutrients in a tropical rain forest. This shows the practical utility of the method, which provides a systematic framework for robust variable selection in spatial point-process models under noise, without requiring additional knowledge of the process.</article>","contentLength":1548,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations","url":"https://arxiv.org/abs/2510.25544","date":1761796800,"author":"","guid":321401,"unread":true,"content":"<article>arXiv:2510.25544v1 Announce Type: cross \nAbstract: Recently proposed generative models for discrete data, such as Masked Diffusion Models (MDMs), exploit conditional independence approximations to reduce the computational cost of popular Auto-Regressive Models (ARMs), at the price of some bias in the sampling distribution. We study the resulting computation-vs-accuracy trade-off, providing general error bounds (in relative entropy) that depend only on the average number of tokens generated per iteration and are independent of the data dimensionality (i.e. sequence length), thus supporting the empirical success of MDMs. We then investigate the gain obtained by using non-constant schedule sizes (i.e. varying the number of unmasked tokens during the generation process) and identify the optimal schedule as a function of a so-called information profile of the data distribution, thus allowing for a principled optimization of schedule sizes. We define methods directly as sampling algorithms and do not use classical derivations as time-reversed diffusion processes, leading us to simple and transparent proofs.</article>","contentLength":1118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast Dimensionality Reduction from $\\ell_2$ to $\\ell_p$","url":"https://arxiv.org/abs/2510.25541","date":1761796800,"author":"","guid":321402,"unread":true,"content":"<article>arXiv:2510.25541v1 Announce Type: cross \nAbstract: The Johnson-Lindenstrauss (JL) lemma is a fundamental result in dimensionality reduction, ensuring that any finite set $X \\subseteq \\mathbb{R}^d$ can be embedded into a lower-dimensional space $\\mathbb{R}^k$ while approximately preserving all pairwise Euclidean distances. In recent years, embeddings that preserve Euclidean distances when measured via the $\\ell_1$ norm in the target space have received increasing attention due to their relevance in applications such as nearest neighbor search in high dimensions. A recent breakthrough by Dirksen, Mendelson, and Stollenwerk established an optimal $\\ell_2 \\to \\ell_1$ embedding with computational complexity $O(d \\log d)$. In this work, we generalize this direction and propose a simple linear embedding from $\\ell_2$ to $\\ell_p$ for any $p \\in [1,2]$ based on a construction of Ailon and Liberty. Our method achieves a reduced runtime of $O(d \\log k)$ when $k \\leq d^{1/4}$, improving upon prior runtime results when the target dimension is small. Additionally, we show that for \\emph{any norm} $\\|\\cdot\\|$ in the target space, any embedding of $(\\mathbb{R}^d, \\|\\cdot\\|_2)$ into $(\\mathbb{R}^k, \\|\\cdot\\|)$ with distortion $\\varepsilon$ generally requires $k = \\Omega\\big(\\varepsilon^{-2} \\log(\\varepsilon^2 n)/\\log(1/\\varepsilon)\\big)$, matching the optimal bound for the $\\ell_2$ case up to a logarithmic factor.</article>","contentLength":1420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using latent representations to link disjoint longitudinal data for mixed-effects regression","url":"https://arxiv.org/abs/2510.25531","date":1761796800,"author":"","guid":321403,"unread":true,"content":"<article>arXiv:2510.25531v1 Announce Type: cross \nAbstract: Many rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.</article>","contentLength":1846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains","url":"https://arxiv.org/abs/2510.25514","date":1761796800,"author":"","guid":321404,"unread":true,"content":"<article>arXiv:2510.25514v1 Announce Type: cross \nAbstract: We study the convergence of off-policy TD(0) with linear function approximation when used to approximate the expected discounted reward in a Markov chain. It is well known that the combination of off-policy learning and function approximation can lead to divergence of the algorithm. Existing results for this setting modify the algorithm, for instance by reweighing the updates using importance sampling. This establishes convergence at the expense of additional complexity. In contrast, our approach is to analyse the standard algorithm, but to restrict our attention to the class of reversible Markov chains. We demonstrate convergence under this mild reversibility condition on the structure of the chain, which in many applications can be assumed using domain knowledge. In particular, we establish a convergence guarantee under an upper bound on the discount factor in terms of the difference between the on-policy and off-policy process. This improves upon known results in the literature that state that convergence holds for a sufficiently small discount factor by establishing an explicit bound. Convergence is with probability one and achieves projected Bellman error equal to zero. To obtain these results, we adapt the stochastic approximation framework that was used by Tsitsiklis and Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our results using different types of reversible Markov chains, such as one-dimensional random walks and random walks on a weighted graph.</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sum-of-Squares Certificates for Almost-Sure Reachability of Stochastic Polynomial Systems","url":"https://arxiv.org/abs/2510.25513","date":1761796800,"author":"","guid":321405,"unread":true,"content":"<article>arXiv:2510.25513v1 Announce Type: cross \nAbstract: In this paper, we present a computational approach to certify almost sure reachability for discrete-time polynomial stochastic systems by turning drift--variant criteria into sum-of-squares (SOS) programs solved with standard semidefinite solvers. Specifically, we provide an SOS method based on two complementary certificates: (i) a drift certificate that enforces a radially unbounded function to be non-increasing in expectation outside a compact set of states; and (ii) a variant certificate that guarantees a one-step decrease with positive probability and ensures the target contains its nonpositive sublevel set. We transform these conditions to SOS constraints. For the variant condition, we enforce a robust decrease over a parameterized disturbance ball with nonzero probability and encode the constraints via an S-procedure with polynomial multipliers. The resulting bilinearities are handled by an alternating scheme that alternates between optimizing multipliers and updating the variant and radius until a positive slack is obtained. Two case studies illustrate the workflow and certifies almost-sure reachability.</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Driven Stabilization Using Prior Knowledge on Stabilizability and Controllability","url":"https://arxiv.org/abs/2510.25452","date":1761796800,"author":"","guid":321406,"unread":true,"content":"<article>arXiv:2510.25452v1 Announce Type: cross \nAbstract: In this work, we study data-driven stabilization of linear time-invariant systems using prior knowledge of system-theoretic properties, specifically stabilizability and controllability. To formalize this, we extend the concept of data informativity by requiring the existence of a controller that stabilizes all systems consistent with the data and the prior knowledge. We show that if the system is controllable, then incorporating this as prior knowledge does not relax the conditions required for data-driven stabilization. Remarkably, however, we show that if the system is stabilizable, then using this as prior knowledge leads to necessary and sufficient conditions that are weaker than those for data-driven stabilization without prior knowledge. In other words, data-driven stabilization is easier if one knows that the underlying system is stabilizable. We also provide new data-driven control design methods in terms of linear matrix inequalities that complement the conditions for informativity.</article>","contentLength":1057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minimizing point configurations for tensor product energies on the torus","url":"https://arxiv.org/abs/2510.25442","date":1761796800,"author":"","guid":321407,"unread":true,"content":"<article>arXiv:2510.25442v1 Announce Type: cross \nAbstract: We study point configurations on the torus $\\mathbb T^d$ that minimize interaction energies with tensor product structure which arise naturally in the context of discrepancy theory and quasi-Monte Carlo integration. Permutation sets on $\\mathbb T^2$ and Latin hypercube sets in higher dimensions (i.e. sets whose projections onto coordinate axes are equispaced points) are natural candidates to be energy minimizers. We show that such point configurations that have only one distance in the vector sense minimize the energy for a wide range of potentials, in other words, such sets satisfy a tensor product version of universal optimality. This applies, in particular, to three- and five-point Fibonacci lattices. We also characterize all lattices with this property and exhibit some non-lattice sets of this type. In addition, we obtain several further structural results about global and local minimizers of tensor product energies.</article>","contentLength":985,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving Temporal Consistency and Fidelity at Inference-time in Perceptual Video Restoration by Zero-shot Image-based Diffusion Models","url":"https://arxiv.org/abs/2510.25420","date":1761796800,"author":"","guid":321408,"unread":true,"content":"<article>arXiv:2510.25420v1 Announce Type: cross \nAbstract: Diffusion models have emerged as powerful priors for single-image restoration, but their application to zero-shot video restoration suffers from temporal inconsistencies due to the stochastic nature of sampling and complexity of incorporating explicit temporal modeling. In this work, we address the challenge of improving temporal coherence in video restoration using zero-shot image-based diffusion models without retraining or modifying their architecture. We propose two complementary inference-time strategies: (1) Perceptual Straightening Guidance (PSG) based on the neuroscience-inspired perceptual straightening hypothesis, which steers the diffusion denoising process towards smoother temporal evolution by incorporating a curvature penalty in a perceptual space to improve temporal perceptual scores, such as Fr\\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-Path Ensemble Sampling (MPES), which aims at reducing stochastic variation by ensembling multiple diffusion trajectories to improve fidelity (distortion) scores, such as PSNR and SSIM, without sacrificing sharpness. Together, these training-free techniques provide a practical path toward temporally stable high-fidelity perceptual video restoration using large pretrained diffusion models. We performed extensive experiments over multiple datasets and degradation types, systematically evaluating each strategy to understand their strengths and limitations. Our results show that while PSG enhances temporal naturalness, particularly in case of temporal blur, MPES consistently improves fidelity and spatio-temporal perception--distortion trade-off across all tasks.</article>","contentLength":1711,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive End-to-End Transceiver Design for NextG Pilot-Free and CP-Free Wireless Systems","url":"https://arxiv.org/abs/2510.25416","date":1761796800,"author":"","guid":321409,"unread":true,"content":"<article>arXiv:2510.25416v1 Announce Type: cross \nAbstract: The advent of artificial intelligence (AI)-native wireless communication is fundamentally reshaping the design paradigm of next-generation (NextG) systems, where intelligent air interfaces are expected to operate adaptively and efficiently in highly dynamic environments. Conventional orthogonal frequency division multiplexing (OFDM) systems rely heavily on pilots and the cyclic prefix (CP), resulting in significant overhead and reduced spectral efficiency. To address these limitations, we propose an adaptive end-to-end (E2E) transceiver architecture tailored for pilot-free and CP-free wireless systems. The architecture combines AI-driven constellation shaping and a neural receiver through joint training. To enhance robustness against mismatched or time-varying channel conditions, we introduce a lightweight channel adapter (CA) module, which enables rapid adaptation with minimal computational overhead by updating only the CA parameters. Additionally, we present a framework that is scalable to multiple modulation orders within a unified model, significantly reducing model storage requirements. Moreover, to tackle the high peak-to-average power ratio (PAPR) inherent to OFDM, we incorporate constrained E2E training, achieving compliance with PAPR targets without additional transmission overhead. Extensive simulations demonstrate that the proposed framework delivers superior bit error rate (BER), throughput, and resilience across diverse channel scenarios, highlighting its potential for AI-native NextG.</article>","contentLength":1574,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model-Adaptive Simulation of Hierarchical Shallow Water Moment Equations in One Dimension","url":"https://arxiv.org/abs/2510.25351","date":1761796800,"author":"","guid":321410,"unread":true,"content":"<article>arXiv:2510.25351v1 Announce Type: cross \nAbstract: Shallow free surface flows are often characterized by both subdomains that require high modeling complexity and subdomains that can be sufficiently accurately modeled with low modeling complexity. Moreover, these subdomains may change in time as the water flows through the domain. This motivates the need for space and time adaptivity in the simulation of shallow free surface flows. In this paper, we develop the first adaptive simulations using the recently developed Shallow Water Moment Equations, which are an extension of the standard Shallow Water Equations that allow for vertically changing velocity profiles by including additional variables and equations. The model-specific modeling complexity of a shallow water moment model is determined by its order. The higher the order of the model, the more variables and equations are included in the model. Shallow water moment models are ideally suited for adaptivity because they are hierarchical such that low-order models and high-order models share the same structure. To enable adaptive simulations, we propose two approaches for the coupling of the varying-order shallow water moment equations at their boundary interfaces. The first approach dynamically updates padded state variables but cannot be written in conservative form, while the second approach uses fixed padded state variable values of zero and reduces to conservative form for conservative moment equations. The switching procedure between high-order models and low-order models is based on a new set of model error estimators, originating from a decomposition of the high-order models. Numerical results of the collision of a dam-break wave with a smooth wave yield accurate results, while achieving speedups up to 60 percent compared to a non-adaptive model with fixed modeling complexity.</article>","contentLength":1868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Photoacoustics on the go: An Embedded Photoacoustic Sensing Platform","url":"https://arxiv.org/abs/2510.25256","date":1761796800,"author":"","guid":321411,"unread":true,"content":"<article>arXiv:2510.25256v1 Announce Type: cross \nAbstract: Several centimeters below the skin lie multiple biomarkers, such as glucose, oxygenation, and blood flow. Monitoring these biomarkers regularly and in a non-invasive manner would enable early insight into metabolic status and vascular health. Currently, there are only a handful of non-invasive monitoring systems. Optical methods offer molecular specificity (i.e., multi-biomarker monitoring) but have shallow reach (a few millimeters); ultrasound penetrates deeper but lacks specificity; and MRI is large, slow, and costly. Photoacoustic (PA) sensing combines the best of optical and ultrasound methods. A laser transmitter emits pulses that are absorbed by different molecules, providing specificity. These light pulses generate pressure changes that are captured by an ultrasound receiver, providing depth. Photoacoustic sensing is promising, but the current platforms are bulky, complex, and costly. We propose the first embedded PA platform. Our contributions are fourfold. First, inspired by LiDAR technology, we propose a novel transmitter that emits pulses similar to those in the state-of-the-art (SoA), but instead of using high-voltage sources and complex electronic interfaces, we use a simple low-power microcontroller (MCU). Second, we carry out a thorough analysis of our custom transmitter and a commercial system. Third, we build a basic ultrasound receiver that is able to process the faint signal generated by our transmitter. Lastly, we compare the performance of our platform against a SoA commercial system, and show that we can detect glucose and (de)oxygenated hemoglobin in two controlled solution studies. The resulting signal characteristics indicate a plausible path toward noninvasive, real-time, at-home sensing relevant to diabetes care. More broadly, this platform lays the groundwork for translating the promise of PA sensing into a broader practical reality.</article>","contentLength":1944,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Minimum time consensus for damped second order agents using Gr\\\"{o}bner basis","url":"https://arxiv.org/abs/2510.25243","date":1761796800,"author":"","guid":321412,"unread":true,"content":"<article>arXiv:2510.25243v1 Announce Type: cross \nAbstract: A problem of achieving minimum time consensus for a set of $N$ second-order LTI system agents with bounded inputs and fuel constraints is considered. Unlike our other works, here the damping effect in agent dynamics is included. First, the attainable set for each agent with fuel budget constraints is characterized, and its boundary equations are derived. Then, using the convexity property, the minimum time at which attainable sets of all agents have a non-empty intersection is computed. By applying Helly's theorem, the computation reduces to finding the minimum time to consensus and the corresponding consensus point for each of the triplets separately.</article>","contentLength":711,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Bayesian Optimization: Generative Models as Acquisition Functions","url":"https://arxiv.org/abs/2510.25240","date":1761796800,"author":"","guid":321413,"unread":true,"content":"<article>arXiv:2510.25240v1 Announce Type: cross \nAbstract: We present a general strategy for turning generative models into candidate solution samplers for batch Bayesian optimization (BO). The use of generative models for BO enables large batch scaling as generative sampling, optimization of non-continuous design spaces, and high-dimensional and combinatorial design. Inspired by the success of direct preference optimization (DPO), we show that one can train a generative model with noisy, simple utility values directly computed from observations to then form proposal distributions whose densities are proportional to the expected utility, i.e., BO's acquisition function values. Furthermore, this approach is generalizable beyond preference-based feedback to general types of reward signals and loss functions. This perspective avoids the construction of surrogate (regression or classification) models, common in previous methods that have used generative models for black-box optimization. Theoretically, we show that the generative models within the BO process approximately follow a sequence of distributions which asymptotically concentrate at the global optima under certain conditions. We also demonstrate this effect through experiments on challenging optimization problems involving large batches in high dimensions.</article>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Separating peripheral and higher-level effects on speech intelligibility using a hearing loss simulator and an objective intelligibility measure","url":"https://arxiv.org/abs/2510.25235","date":1761796800,"author":"","guid":321414,"unread":true,"content":"<article>arXiv:2510.25235v1 Announce Type: cross \nAbstract: This paper presents a new method for separating the effects of peripheral hearing loss (HL) and higher-level processes on speech intelligibility (SI). In a previous study, we conducted an SI experiment with 14 older adult (OA) listeners, using speech-in-noise sounds that were either processed with an ideal ratio mask (IRM) enhancement technique or left unprocessed. The current study involved an SI experiment with 15 young, normal-hearing (YNH) listeners. This experiment used simulated HL sounds processed with the WHIS simulator that reflected the hearing level of a specific OA from the previous study. The results showed that the target OA's SI scores were higher than the average YNH scores. This implies that the target OA's higher-level processes may be more effective than those of the average YNH. To understand the characteristics of other OAs, we used the GESI objective intelligibility measure to predict SI. First, we confirmed that GESI could fairly accurately predict the SI scores for both the YNH and OA listeners. Next, we predicted the SI scores of the 14 OA listeners using the parameters estimated in the YNH experiment. The results showed that some OAs had higher SI scores than the average YNH, while one OA had lower scores. These differences in SI scores may reflect variations in the efficiency of higher-level processes.These results imply that WHIS and GESI could facilitate contrastive experiments between YNH and OA listeners, regardless of hearing level. This would allow us to study the effects of higher-level processes in OA listeners individually.</article>","contentLength":1636,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"State Space and Self-Attention Collaborative Network with Feature Aggregation for DOA Estimation","url":"https://arxiv.org/abs/2510.25193","date":1761796800,"author":"","guid":321415,"unread":true,"content":"<article>arXiv:2510.25193v1 Announce Type: cross \nAbstract: Accurate direction-of-arrival (DOA) estimation for sound sources is challenging due to the continuous changes in acoustic characteristics across time and frequency. In such scenarios, accurate localization relies on the ability to aggregate relevant features and model temporal dependencies effectively. In time series modeling, achieving a balance between model performance and computational efficiency remains a significant challenge. To address this, we propose FA-Stateformer, a state space and self-attention collaborative network with feature aggregation. The proposed network first employs a feature aggregation module to enhance informative features across both temporal and spectral dimensions. This is followed by a lightweight Conformer architecture inspired by the squeeze-and-excitation mechanism, where the feedforward layers are compressed to reduce redundancy and parameter overhead. Additionally, a temporal shift mechanism is incorporated to expand the receptive field of convolutional layers while maintaining a compact kernel size. To further enhance sequence modeling capabilities, a bidirectional Mamba module is introduced, enabling efficient state-space-based representation of temporal dependencies in both forward and backward directions. The remaining self-attention layers are combined with the Mamba blocks, forming a collaborative modeling framework that achieves a balance between representation capacity and computational efficiency. Extensive experiments demonstrate that FA-Stateformer achieves superior performance and efficiency compared to conventional architectures.</article>","contentLength":1655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Tight Lower Bound on Cubic Vertices and Upper Bounds on Thin and Non-thin edges in Planar Braces","url":"https://arxiv.org/abs/2510.25188","date":1761796800,"author":"","guid":321416,"unread":true,"content":"<article>arXiv:2510.25188v1 Announce Type: cross \nAbstract: For a subset $X$ of the vertex set $\\VV(\\GG)$ of a graph $\\GG$, we denote the set of edges of $\\GG$ which have exactly one end in $X$ by $\\partial(X)$ and refer to it as the cut of $X$ or edge cut $\\partial(X)$. A graph $\\GG=(\\VV,\\EE)$ is called matching covered if $\\forall e \\in \\EE(\\GG), ~\\exists \\text{a perfect matching }M \\text{ of }\\GG \\text{ s. t. } e \\in M$. A cut $C$ of a matching covered graph $\\GG$ is a separating cut if and only if, given any edge $e$, there is a perfect matching $M_{e}$ of $\\GG$ such that $e \\in M_{e}$ and $|C \\cap M_{e}| = 1$. A cut $C$ in a matching covered graph $\\GG$ is a tight cut of $\\GG$ if $|C \\cap M| = 1$ for every perfect matching $M$ of $\\GG$. For, $X, Y \\subseteq \\VV(\\GG)$, we denote the set of edges of $\\EE(\\GG)$ which have one endpoint in $X$ and the other endpoint in $Y$ by $E[X,Y]$. Let $\\partial(X)=E[X,\\overline{X}]$ be an edge cut, where $\\overline{X}=\\VV(\\GG) \\setminus X$. An edge cut is trivial if $|X|=1$ or $|\\overline{X}|=1$. A matching covered graph, which is free of nontrivial tight cuts, is a brace if it is bipartite and is a brick if it is non-bipartite. An edge $e$ in a brace $\\GG$ is \\emph{thin} if, for every tight cut $\\partial(X)$ of $\\GG - e$, $|X| \\le 3$ or $|\\overline{X}| \\le 3$.\n  Carvalho, Lucchesi and Murty conjectured that there exists a positive constant $c$ such that every brace $\\GG$ has $c|\\VV(\\GG)|$ thin edges \\cite{DBLP:journals/combinatorics/LucchesiCM15}. He and Lu \\cite{HE2025153} showed a lower bound of thin edges in a brace in terms of the number of cubic vertices. We asked whether any planar brace exists that does not contain any cubic vertices. We answer negatively by showing that such set of planar braces is empty. We have been able to show a quantitively tight lower bound on the number of cubic vertices in a planar brace. We have proved tight upper bounds of nonthin edges and thin edges in a planar brace.</article>","contentLength":1968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sustainable NARMA-10 Benchmarking for Quantum Reservoir Computing","url":"https://arxiv.org/abs/2510.25183","date":1761796800,"author":"","guid":321417,"unread":true,"content":"<article>arXiv:2510.25183v1 Announce Type: cross \nAbstract: This study compares Quantum Reservoir Computing (QRC) with classical models such as Echo State Networks (ESNs) and Long Short-Term Memory networks (LSTMs), as well as hybrid quantum-classical architectures (QLSTM), for the nonlinear autoregressive moving average task (NARMA-10). We evaluate forecasting accuracy (NRMSE), computational cost, and evaluation time. Results show that QRC achieves competitive accuracy while offering potential sustainability advantages, particularly in resource-constrained settings, highlighting its promise for sustainable time-series AI applications.</article>","contentLength":634,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Retaining Mixture Representations for Domain Generalized Anomalous Sound Detection","url":"https://arxiv.org/abs/2510.25182","date":1761796800,"author":"","guid":321418,"unread":true,"content":"<article>arXiv:2510.25182v1 Announce Type: cross \nAbstract: Anomalous sound detection (ASD) in the wild requires robustness to distribution shifts such as unseen low-SNR input mixtures of machine and noise types. State-of-the-art systems extract embeddings from an adapted audio encoder and detect anomalies via nearest-neighbor search, but fine tuning on noisy machine sounds often acts like a denoising objective, suppressing noise and reducing generalization under mismatched mixtures or inconsistent labeling. Training-free systems with frozen self-supervised learning (SSL) encoders avoid this issue and show strong first-shot generalization, yet their performance drops when mixture embeddings deviate from clean-source embeddings. We propose to improve SSL backbones with a retain-not-denoise strategy that better preserves information from mixed sound sources. The approach combines a multi-label audio tagging loss with a mixture alignment loss that aligns student mixture embeddings to convex teacher embeddings of clean and noise inputs. Controlled experiments on stationary, non-stationary, and mismatched noise subsets demonstrate improved robustness under distribution shifts, narrowing the gap toward oracle mixture representations.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transformers in Medicine: Improving Vision-Language Alignment for Medical Image Captioning","url":"https://arxiv.org/abs/2510.25164","date":1761796800,"author":"","guid":321419,"unread":true,"content":"<article>arXiv:2510.25164v1 Announce Type: cross \nAbstract: We present a transformer-based multimodal framework for generating clinically relevant captions for MRI scans. Our system combines a DEiT-Small vision transformer as an image encoder, MediCareBERT for caption embedding, and a custom LSTM-based decoder. The architecture is designed to semantically align image and textual embeddings, using hybrid cosine-MSE loss and contrastive inference via vector similarity. We benchmark our method on the MultiCaRe dataset, comparing performance on filtered brain-only MRIs versus general MRI images against state-of-the-art medical image captioning methods including BLIP, R2GenGPT, and recent transformer-based approaches. Results show that focusing on domain-specific data improves caption accuracy and semantic alignment. Our work proposes a scalable, interpretable solution for automated medical image reporting.</article>","contentLength":906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditional neural field for spatial dimension reduction of turbulence data: a comparison study","url":"https://arxiv.org/abs/2510.25135","date":1761796800,"author":"","guid":321420,"unread":true,"content":"<article>arXiv:2510.25135v1 Announce Type: cross \nAbstract: We investigate conditional neural fields (CNFs), mesh-agnostic, coordinate-based decoders conditioned on a low-dimensional latent, for spatial dimensionality reduction of turbulent flows. CNFs are benchmarked against Proper Orthogonal Decomposition and a convolutional autoencoder within a unified encoding-decoding framework and a common evaluation protocol that explicitly separates in-range (interpolative) from out-of-range (strict extrapolative) testing beyond the training horizon, with identical preprocessing, metrics, and fixed splits across all baselines. We examine three conditioning mechanisms: (i) activation-only modulation (often termed FiLM), (ii) low-rank weight and bias modulation (termed FP), and (iii) last-layer inner-product coupling, and introduce a novel domain-decomposed CNF that localizes complexities. Across representative turbulence datasets (WMLES channel inflow, DNS channel inflow, and wall pressure fluctuations over turbulent boundary layers), CNF-FP achieves the lowest training and in-range testing errors, while CNF-FiLM generalizes best for out-of-range scenarios once moderate latent capacity is available. Domain decomposition significantly improves out-of-range accuracy, especially for the more demanding datasets. The study provides a rigorous, physics-aware basis for selecting conditioning, capacity, and domain decomposition when using CNFs for turbulence compression and reconstruction.</article>","contentLength":1487,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation","url":"https://arxiv.org/abs/2510.25132","date":1761796800,"author":"","guid":321421,"unread":true,"content":"<article>arXiv:2510.25132v1 Announce Type: cross \nAbstract: Designing enzyme backbones with substrate-specific functionality is a critical challenge in computational protein engineering. Current generative models excel in protein design but face limitations in binding data, substrate-specific control, and flexibility for de novo enzyme backbone generation. To address this, we introduce EnzyBind, a dataset with 11,100 experimentally validated enzyme-substrate pairs specifically curated from PDBbind. Building on this, we propose EnzyControl, a method that enables functional and substrate-specific control in enzyme backbone generation. Our approach generates enzyme backbones conditioned on MSA-annotated catalytic sites and their corresponding substrates, which are automatically extracted from curated enzyme-substrate data. At the core of EnzyControl is EnzyAdapter, a lightweight, modular component integrated into a pretrained motif-scaffolding model, allowing it to become substrate-aware. A two-stage training paradigm further refines the model's ability to generate accurate and functional enzyme structures. Experiments show that our EnzyControl achieves the best performance across structural and functional metrics on EnzyBind and EnzyBench benchmarks, with particularly notable improvements of 13\\% in designability and 13\\% in catalytic efficiency compared to the baseline models. The code is released at https://github.com/Vecteur-libre/EnzyControl.</article>","contentLength":1459,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nonlinear Dynamics In Optimization Landscape of Shallow Neural Networks with Tunable Leaky ReLU","url":"https://arxiv.org/abs/2510.25060","date":1761796800,"author":"","guid":321422,"unread":true,"content":"<article>arXiv:2510.25060v1 Announce Type: cross \nAbstract: In this work, we study the nonlinear dynamics of a shallow neural network trained with mean-squared loss and leaky ReLU activation. Under Gaussian inputs and equal layer width k, (1) we establish, based on the equivariant gradient degree, a theoretical framework, applicable to any number of neurons k&gt;= 4, to detect bifurcation of critical points with associated symmetries from global minimum as leaky parameter $\\alpha$ varies. Typically, our analysis reveals that a multi-mode degeneracy consistently occurs at the critical number 0, independent of k. (2) As a by-product, we further show that such bifurcations are width-independent, arise only for nonnegative $\\alpha$ and that the global minimum undergoes no further symmetry-breaking instability throughout the engineering regime $\\alpha$ in range (0,1). An explicit example with k=5 is presented to illustrate the framework and exhibit the resulting bifurcation together with their symmetries.</article>","contentLength":1003,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bayesian Neural Networks vs. Mixture Density Networks: Theoretical and Empirical Insights for Uncertainty-Aware Nonlinear Modeling","url":"https://arxiv.org/abs/2510.25001","date":1761796800,"author":"","guid":321423,"unread":true,"content":"<article>arXiv:2510.25001v1 Announce Type: cross \nAbstract: This paper investigates two prominent probabilistic neural modeling paradigms: Bayesian Neural Networks (BNNs) and Mixture Density Networks (MDNs) for uncertainty-aware nonlinear regression. While BNNs incorporate epistemic uncertainty by placing prior distributions over network parameters, MDNs directly model the conditional output distribution, thereby capturing multimodal and heteroscedastic data-generating mechanisms. We present a unified theoretical and empirical framework comparing these approaches. On the theoretical side, we derive convergence rates and error bounds under H\\\"older smoothness conditions, showing that MDNs achieve faster Kullback-Leibler (KL) divergence convergence due to their likelihood-based nature, whereas BNNs exhibit additional approximation bias induced by variational inference. Empirically, we evaluate both architectures on synthetic nonlinear datasets and a radiographic benchmark (RSNA Pediatric Bone Age Challenge). Quantitative and qualitative results demonstrate that MDNs more effectively capture multimodal responses and adaptive uncertainty, whereas BNNs provide more interpretable epistemic uncertainty under limited data. Our findings clarify the complementary strengths of posterior-based and likelihood-based probabilistic learning, offering guidance for uncertainty-aware modeling in nonlinear systems.</article>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration","url":"https://arxiv.org/abs/2510.24987","date":1761796800,"author":"","guid":321424,"unread":true,"content":"<article>arXiv:2510.24987v1 Announce Type: cross \nAbstract: Advances in single-cell sequencing have enabled high-resolution profiling of diverse molecular modalities, while integrating unpaired multi-omics single-cell data remains challenging. Existing approaches either rely on pair information or prior correspondences, or require computing a global pairwise coupling matrix, limiting their scalability and flexibility. In this paper, we introduce a scalable and flexible generative framework called single-cell Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired multi-omics integration. Specifically, we disentangle each cell's latent representations into modality-shared and modality-specific components using a well-designed $\\beta$-VAE architecture, which are augmented with isometric regularization to preserve intra-omics biological heterogeneity, adversarial objective to encourage cross-modal alignment, and masked reconstruction loss strategy to address the issue of missing features across modalities. Our method achieves excellent performance on benchmark datasets in terms of batch correction, modality alignment, and biological signal preservation. Crucially, it scales effectively to large-level datasets and supports integration of more than two omics, offering a powerful and flexible solution for large-scale multi-omics data integration and downstream biological discovery.</article>","contentLength":1408,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Multilevel Newton: A Quadratically Convergent Optimization Method","url":"https://arxiv.org/abs/2510.24967","date":1761796800,"author":"","guid":321425,"unread":true,"content":"<article>arXiv:2510.24967v1 Announce Type: cross \nAbstract: Newton's method may exhibit slower convergence than vanilla Gradient Descent in its initial phase on strongly convex problems. Classical Newton-type multilevel methods mitigate this but, like Gradient Descent, achieve only linear convergence near the minimizer. We introduce an adaptive multilevel Newton-type method with a principled automatic switch to full Newton once its quadratic phase is reached. The local quadratic convergence for strongly convex functions with Lipschitz continuous Hessians and for self-concordant functions is established and confirmed empirically. Although per-iteration cost can exceed that of classical multilevel schemes, the method is efficient and consistently outperforms Newton's method, Gradient Descent, and the multilevel Newton method, indicating that second-order methods can outperform first-order methods even when Newton's method is initially slow.</article>","contentLength":943,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Convergence analysis for an implementable scheme to solve the linear-quadratic stochastic optimal control problem with stochastic wave equation","url":"https://arxiv.org/abs/2510.24876","date":1761796800,"author":"","guid":321426,"unread":true,"content":"<article>arXiv:2510.24876v1 Announce Type: cross \nAbstract: We study an optimal control problem for the stochastic wave equation driven by affine multiplicative noise, formulated as a stochastic linear-quadratic (SLQ) problem. By applying a stochastic Pontryagin's maximum principle, we characterize the optimal state-control pair via a coupled forward-backward SPDE system. We propose an implementable discretization using conforming finite elements in space and an implicit midpoint rule in time. By a new technical approach we obtain strong convergence rates for the discrete state-control pair without relying on Malliavin calculus. For the practical computation we develop a gradient-descent algorithm based on artificial iterates that employs an exact computation for the arising conditional expectations, thereby eliminating costly Monte Carlo sampling. Consequently, each iteration has a computational cost that is proportional to the number of spatial degrees of freedom, producing a scalable method that preserves the established strong convergence rates. Numerical results validate its efficiency.</article>","contentLength":1099,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Formalization of Auslander--Buchsbaum--Serre criterion in Lean4","url":"https://arxiv.org/abs/2510.24818","date":1761796800,"author":"","guid":321427,"unread":true,"content":"<article>arXiv:2510.24818v1 Announce Type: cross \nAbstract: We formalized a complete proof of the Auslander--Buchsbaum--Serre criterion in the Lean4 theorem prover. For a local ring, rather than following the well-known proof that considers the residue field as the quotient of the ring by a regular sequence to compute projective dimension and uses the Koszul complex to show the dimension of the cotangent space is at most the global dimension, we prove the criterion via maximal Cohen--Macaulay modules and a weakened version of the Ferrand--Vasconcelos theorem, which is more amenable to the formalization process and the current development of mathlib. Our formalization includes the construction of depth and of Cohen--Macaulay modules and rings, which are used frequently in the proof of the criterion. We also developed related results, including the unmixedness theorem for Cohen--Macaulay rings and Hilbert's Syzygy theorem.</article>","contentLength":925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm","url":"https://arxiv.org/abs/2510.24815","date":1761796800,"author":"","guid":321428,"unread":true,"content":"<article>arXiv:2510.24815v1 Announce Type: cross \nAbstract: Tree ensembles have demonstrated state-of-the-art predictive performance across a wide range of problems involving tabular data. Nevertheless, the black-box nature of tree ensembles is a strong limitation, especially for applications with critical decisions at stake. The Hoeffding or ANOVA functional decomposition is a powerful explainability method, as it breaks down black-box models into a unique sum of lower-dimensional functions, provided that input variables are independent. In standard learning settings, input variables are often dependent, and the Hoeffding decomposition is generalized through hierarchical orthogonality constraints. Such generalization leads to unique and sparse decompositions with well-defined main effects and interactions. However, the practical estimation of this decomposition from a data sample is still an open problem. Therefore, we introduce the TreeHFD algorithm to estimate the Hoeffding decomposition of a tree ensemble from a data sample. We show the convergence of TreeHFD, along with the main properties of orthogonality, sparsity, and causal variable selection. The high performance of TreeHFD is demonstrated through experiments on both simulated and real data, using our treehfd Python package (https://github.com/ThalesGroup/treehfd). Besides, we empirically show that the widely used TreeSHAP method, based on Shapley values, is strongly connected to the Hoeffding decomposition.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CT-Less Attenuation Correction Using Multiview Ensemble Conditional Diffusion Model on High-Resolution Uncorrected PET Images","url":"https://arxiv.org/abs/2510.24805","date":1761796800,"author":"","guid":321429,"unread":true,"content":"<article>arXiv:2510.24805v1 Announce Type: cross \nAbstract: Accurate quantification in positron emission tomography (PET) is essential for accurate diagnostic results and effective treatment tracking. A major issue encountered in PET imaging is attenuation. Attenuation refers to the diminution of photon detected as they traverse biological tissues before reaching detectors. When such corrections are absent or inadequate, this signal degradation can introduce inaccurate quantification, making it difficult to differentiate benign from malignant conditions, and can potentially lead to misdiagnosis. Typically, this correction is done with co-computed Computed Tomography (CT) imaging to obtain structural data for calculating photon attenuation across the body. However, this methodology subjects patients to extra ionizing radiation exposure, suffers from potential spatial misregistration between PET/CT imaging sequences, and demands costly equipment infrastructure. Emerging advances in neural network architectures present an alternative approach via synthetic CT image synthesis. Our investigation reveals that Conditional Denoising Diffusion Probabilistic Models (DDPMs) can generate high quality CT images from non attenuation corrected PET images in order to correct attenuation. By utilizing all three orthogonal views from non-attenuation-corrected PET images, the DDPM approach combined with ensemble voting generates higher quality pseudo-CT images with reduced artifacts and improved slice-to-slice consistency. Results from a study of 159 head scans acquired with the Siemens Biograph Vision PET/CT scanner demonstrate both qualitative and quantitative improvements in pseudo-CT generation. The method achieved a mean absolute error of 32 $\\pm$ 10.4 HU on the CT images and an average error of (1.48 $\\pm$ 0.68)\\% across all regions of interest when comparing PET images reconstructed using the attenuation map of the generated pseudo-CT versus the true CT.</article>","contentLength":1967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semantic Communications with World Models","url":"https://arxiv.org/abs/2510.24785","date":1761796800,"author":"","guid":321430,"unread":true,"content":"<article>arXiv:2510.24785v1 Announce Type: cross \nAbstract: Semantic communication is a promising technique for emerging wireless applications, which reduces transmission overhead by transmitting only task-relevant features instead of raw data. However, existing methods struggle under extremely low bandwidth and varying channel conditions, where corrupted or missing semantics lead to severe reconstruction errors. To resolve this difficulty, we propose a world foundation model (WFM)-aided semantic video transmission framework that leverages the predictive capability of WFMs to generate future frames based on the current frame and textual guidance. This design allows transmissions to be omitted when predictions remain reliable, thereby saving bandwidth. Through WFM's prediction, the key semantics are preserved, yet minor prediction errors tend to amplify over time. To mitigate issue, a lightweight depth-based feedback module is introduced to determine whether transmission of the current frame is needed. Apart from transmitting the entire frame, a segmentation-assisted partial transmission method is proposed to repair degraded frames, which can further balance performance and bandwidth cost. Furthermore, an active transmission strategy is developed for mobile scenarios by exploiting camera trajectory information and proactively scheduling transmissions before channel quality deteriorates. Simulation results show that the proposed framework significantly reduces transmission overhead while maintaining task performances across varying scenarios and channel conditions.</article>","contentLength":1580,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sub-microsecond Transformers for Jet Tagging on FPGAs","url":"https://arxiv.org/abs/2510.24784","date":1761796800,"author":"","guid":321431,"unread":true,"content":"<article>arXiv:2510.24784v1 Announce Type: cross \nAbstract: We present the first sub-microsecond transformer implementation on an FPGA achieving competitive performance for state-of-the-art high-energy physics benchmarks. Transformers have shown exceptional performance on multiple tasks in modern machine learning applications, including jet tagging at the CERN Large Hadron Collider (LHC). However, their computational complexity prohibits use in real-time applications, such as the hardware trigger system of the collider experiments up until now. In this work, we demonstrate the first application of transformers for jet tagging on FPGAs, achieving $\\mathcal{O}(100)$ nanosecond latency with superior performance compared to alternative baseline models. We leverage high-granularity quantization and distributed arithmetic optimization to fit the entire transformer model on a single FPGA, achieving the required throughput and latency. Furthermore, we add multi-head attention and linear attention support to hls4ml, making our work accessible to the broader fast machine learning community. This work advances the next-generation trigger systems for the High Luminosity LHC, enabling the use of transformers for real-time applications in high-energy physics and beyond.</article>","contentLength":1267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CFL-SparseMed: Communication-Efficient Federated Learning for Medical Imaging with Top-k Sparse Updates","url":"https://arxiv.org/abs/2510.24776","date":1761796800,"author":"","guid":321432,"unread":true,"content":"<article>arXiv:2510.24776v1 Announce Type: cross \nAbstract: Secure and reliable medical image classification is crucial for effective patient treatment, but centralized models face challenges due to data and privacy concerns. Federated Learning (FL) enables privacy-preserving collaborations but struggles with heterogeneous, non-IID data and high communication costs, especially in large networks. We propose \\textbf{CFL-SparseMed}, an FL approach that uses Top-k Sparsification to reduce communication overhead by transmitting only the top k gradients. This unified solution effectively addresses data heterogeneity while maintaining model accuracy. It enhances FL efficiency, preserves privacy, and improves diagnostic accuracy and patient care in non-IID medical imaging settings. The reproducibility source code is available on \\href{https://github.com/Aniket2241/APK_contruct}{Github}.</article>","contentLength":882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DMVFC: Deep Learning Based Functionally Consistent Tractography Fiber Clustering Using Multimodal Diffusion MRI and Functional MRI","url":"https://arxiv.org/abs/2510.24770","date":1761796800,"author":"","guid":321433,"unread":true,"content":"<article>arXiv:2510.24770v1 Announce Type: cross \nAbstract: Tractography fiber clustering using diffusion MRI (dMRI) is a crucial method for white matter (WM) parcellation to enable analysis of brains structural connectivity in health and disease. Current fiber clustering strategies primarily use the fiber geometric characteristics (i.e., the spatial trajectories) to group similar fibers into clusters, while neglecting the functional and microstructural information of the fiber tracts. There is increasing evidence that neural activity in the WM can be measured using functional MRI (fMRI), providing potentially valuable multimodal information for fiber clustering to enhance its functional coherence. Furthermore, microstructural features such as fractional anisotropy (FA) can be computed from dMRI as additional information to ensure the anatomical coherence of the clusters. In this paper, we develop a novel deep learning fiber clustering framework, namely Deep Multi-view Fiber Clustering (DMVFC), which uses joint multi-modal dMRI and fMRI data to enable functionally consistent WM parcellation. DMVFC can effectively integrate the geometric and microstructural characteristics of the WM fibers with the fMRI BOLD signals along the fiber tracts. DMVFC includes two major components: (1) a multi-view pretraining module to compute embedding features from each source of information separately, including fiber geometry, microstructure measures, and functional signals, and (2) a collaborative fine-tuning module to simultaneously refine the differences of embeddings. In the experiments, we compare DMVFC with two state-of-the-art fiber clustering methods and demonstrate superior performance in achieving functionally meaningful and consistent WM parcellation results.</article>","contentLength":1772,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees","url":"https://arxiv.org/abs/2510.24754","date":1761796800,"author":"","guid":321434,"unread":true,"content":"<article>arXiv:2510.24754v1 Announce Type: cross \nAbstract: Uncertain knowledge graph embedding (UnKGE) methods learn vector representations that capture both structural and uncertainty information to predict scores of unseen triples. However, existing methods produce only point estimates, without quantifying predictive uncertainty-limiting their reliability in high-stakes applications where understanding confidence in predictions is crucial. To address this limitation, we propose \\textsc{UnKGCP}, a framework that generates prediction intervals guaranteed to contain the true score with a user-specified level of confidence. The length of the intervals reflects the model's predictive uncertainty. \\textsc{UnKGCP} builds on the conformal prediction framework but introduces a novel nonconformity measure tailored to UnKGE methods and an efficient procedure for interval construction. We provide theoretical guarantees for the intervals and empirically verify these guarantees. Extensive experiments on standard benchmarks across diverse UnKGE methods further demonstrate that the intervals are sharp and effectively capture predictive uncertainty.</article>","contentLength":1144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Artificial Transmission Line Synthesis Tailored for Traveling-Wave Parametric Processes","url":"https://arxiv.org/abs/2510.24753","date":1761796800,"author":"","guid":321435,"unread":true,"content":"<article>arXiv:2510.24753v1 Announce Type: cross \nAbstract: Artificial transmission lines built with lumped-element inductors and capacitors form the backbone of broadband, nearly quantum-limited traveling-wave parametric amplifiers (TWPAs). However, systematic design methods for TWPAs, and more generally artificial transmission lines, are lacking. Here, I develop a general synthesis framework for lossless artificial transmission lines by borrowing from periodic structure theory and passive network synthesis. These complementary approaches divide the design space: periodic loading synthesis employs spatial modulation of frequency-independent components, while filter synthesis employs frequency-dependent responses in spatially-uniform components. When tailoring transmission lines for parametric processes, nonlinear elements are added, typically nonlinear inductances in superconducting circuits, while ensuring energy and momentum conservation between interacting tones. Applying this framework, I design a kinetic inductance TWPA with a novel phase-matching architecture, and a backward-pumped Josephson TWPA exploiting an ambidextrous i.e., right-left-handed transmission line.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EcoScaleNet: A Lightweight Multi Kernel Network for Long Sequence 12 lead ECG Classification","url":"https://arxiv.org/abs/2510.24748","date":1761796800,"author":"","guid":321436,"unread":true,"content":"<article>arXiv:2510.24748v1 Announce Type: cross \nAbstract: Accurate interpretation of 12 lead electrocardiograms (ECGs) is critical for early detection of cardiac abnormalities, yet manual reading is error prone and existing CNN based classifiers struggle to choose receptive field sizes that generalize to the long sequences typical of ECGs. Omni Scale CNN (OS CNN) addresses this by enumerating prime sized kernels inspired by Goldbach conjecture to cover every scale, but its exhaustive design explodes computational cost and blocks deeper, wider models. We present Efficient Convolutional Omni Scale Network (EcoScale-Net), a hierarchical variant that retains full receptive field coverage while eliminating redundancy. At each stage, the maximum kernel length is capped to the scale still required after down sampling, and bottleneck convolutions inserted before and after every Omni Scale block curtail channel growth and fuse multi scale features. On the large scale CODE 15% ECG dataset, EcoScaleNet reduces parameters by 90% and FLOPs by 99% compared with OS CNN, while raising macro averaged F1 score by 2.4%. These results demonstrate that EcoScaleNet delivers SOTA accuracy for long sequence ECG classification at a fraction of the computational cost, enabling real time deployment on commodity hardware. Our EcoScaleNet code is available in GitHub Link.</article>","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PulseFi: A Low Cost Robust Machine Learning System for Accurate Cardiopulmonary and Apnea Monitoring Using Channel State Information","url":"https://arxiv.org/abs/2510.24744","date":1761796800,"author":"","guid":321437,"unread":true,"content":"<article>arXiv:2510.24744v1 Announce Type: cross \nAbstract: Non-intrusive monitoring of vital signs has become increasingly important in a variety of healthcare settings. In this paper, we present PulseFi, a novel low-cost non-intrusive system that uses Wi-Fi sensing and artificial intelligence to accurately and continuously monitor heart rate and breathing rate, as well as detect apnea events. PulseFi operates using low-cost commodity devices, making it more accessible and cost-effective. It uses a signal processing pipeline to process Wi-Fi telemetry data, specifically Channel State Information (CSI), that is fed into a custom low-compute Long Short-Term Memory (LSTM) neural network model. We evaluate PulseFi using two datasets: one that we collected locally using ESP32 devices and another that contains recordings of 118 participants collected using the Raspberry Pi 4B, making the latter the most comprehensive data set of its kind. Our results show that PulseFi can effectively estimate heart rate and breathing rate in a seemless non-intrusive way with comparable or better accuracy than multiple antenna systems that can be expensive and less accessible.</article>","contentLength":1163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comparative Analysis of Data Augmentation for Clinical ECG Classification with STAR","url":"https://arxiv.org/abs/2510.24740","date":1761796800,"author":"","guid":321438,"unread":true,"content":"<article>arXiv:2510.24740v1 Announce Type: cross \nAbstract: Clinical 12-lead ECG classification remains difficult because of diverse recording conditions, overlapping pathologies, and pronounced label imbalance hinder generalization, while unconstrained augmentations risk distorting diagnostically critical morphology. In this study, Sinusoidal Time--Amplitude Resampling (STAR) is introduced as a beat-wise augmentation that operates strictly between successive R-peaks to apply controlled time warping and amplitude scaling to each R--R segment, preserving the canonical P--QRS--T order and leaving the head and tail of the trace unchanged. STAR is designed for practical pipelines and offers: (i) morphology-faithful variability that broadens training diversity without corrupting peaks or intervals; (ii) source-resilient training, improving stability across devices, sites, and cohorts without dataset-specific tuning; (iii) model-agnostic integration with common 1D SE--ResNet-style ECG encoders backbone; and (iv) better learning on rare classes via beat-level augmentation, reducing overfitting by resampling informative beats instead of duplicating whole records. In contrast to global crops, large shifts, or additive noise, STAR avoids transformations that suppress or misalign clinical landmarks. A complete Python implementation and a transparent training workflow are released, aligned with a source-aware, stratified five-fold protocol over a multi-institutional 12-lead corpus, thereby facilitating inspection and reuse. Taken together, STAR provides a simple and controllable augmentation for clinical ECG classification where trustworthy morphology, operational simplicity, and cross-source durability are essential.</article>","contentLength":1726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StrikeWatch: Wrist-worn Gait Recognition with Compact Time-series Models on Low-power FPGAs","url":"https://arxiv.org/abs/2510.24738","date":1761796800,"author":"","guid":321439,"unread":true,"content":"<article>arXiv:2510.24738v1 Announce Type: cross \nAbstract: Running offers substantial health benefits, but improper gait patterns can lead to injuries, particularly without expert feedback. While prior gait analysis systems based on cameras, insoles, or body-mounted sensors have demonstrated effectiveness, they are often bulky and limited to offline, post-run analysis. Wrist-worn wearables offer a more practical and non-intrusive alternative, yet enabling real-time gait recognition on such devices remains challenging due to noisy Inertial Measurement Unit (IMU) signals, limited computing resources, and dependence on cloud connectivity. This paper introduces StrikeWatch, a compact wrist-worn system that performs entirely on-device, real-time gait recognition using IMU signals. As a case study, we target the detection of heel versus forefoot strikes to enable runners to self-correct harmful gait patterns through visual and auditory feedback during running. We propose four compact DL architectures (1D-CNN, 1D-SepCNN, LSTM, and Transformer) and optimize them for energy-efficient inference on two representative embedded Field-Programmable Gate Arrays (FPGAs): the AMD Spartan-7 XC7S15 and the Lattice iCE40UP5K. Using our custom-built hardware prototype, we collect a labeled dataset from outdoor running sessions and evaluate all models via a fully automated deployment pipeline. Our results reveal clear trade-offs between model complexity and hardware efficiency. Evaluated across 12 participants, 6-bit quantized 1D-SepCNN achieves the highest average F1 score of 0.847 while consuming just 0.350 {\\mu}J per inference with a latency of 0.140 ms on the iCE40UP5K running at 20 MHz. This configuration supports up to 13.6 days of continuous inference on a 320 mAh battery. All datasets and code are available in the GitHub repository https://github.com/tianheng-ling/StrikeWatch.</article>","contentLength":1886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cardi-GPT: An Expert ECG-Record Processing Chatbot","url":"https://arxiv.org/abs/2510.24737","date":1761796800,"author":"","guid":321440,"unread":true,"content":"<article>arXiv:2510.24737v1 Announce Type: cross \nAbstract: Interpreting and communicating electrocardiogram (ECG) findings are crucial yet challenging tasks in cardiovascular diagnosis, traditionally requiring significant expertise and precise clinical communication. This paper introduces Cardi-GPT, an advanced expert system designed to streamline ECG interpretation and enhance clinical communication through deep learning and natural language interaction. Cardi-GPT employs a 16-residual-block convolutional neural network (CNN) to process 12-lead ECG data, achieving a weighted accuracy of 0.6194 across 24 cardiac conditions. A novel fuzzification layer converts complex numerical outputs into clinically meaningful linguistic categories, while an integrated chatbot interface facilitates intuitive exploration of diagnostic insights and seamless communication between healthcare providers.\n  The system was evaluated on a diverse dataset spanning six hospitals across four countries, demonstrating superior performance compared to baseline models. Additionally, Cardi-GPT achieved an impressive overall response quality score of 73\\%, assessed using a comprehensive evaluation framework that measures coverage, grounding, and coherence. By bridging the gap between intricate ECG data interpretation and actionable clinical insights, Cardi-GPT represents a transformative innovation in cardiovascular healthcare, promising to improve diagnostic accuracy, clinical workflows, and patient outcomes across diverse medical settings.</article>","contentLength":1526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decoding non-invasive brain activity with novel deep-learning approaches","url":"https://arxiv.org/abs/2510.24733","date":1761796800,"author":"","guid":321441,"unread":true,"content":"<article>arXiv:2510.24733v1 Announce Type: cross \nAbstract: This thesis delves into the world of non-invasive electrophysiological brain signals like electroencephalography (EEG) and magnetoencephalography (MEG), focusing on modelling and decoding such data. The research aims to investigate what happens in the brain when we perceive visual stimuli or engage in covert speech (inner speech) and enhance the decoding performance of such stimuli. The thesis is divided into two main sections, methodological and experimental work. A central concern in both sections is the large variability present in electrophysiological recordings, whether it be within-subject or between-subject variability, and to a certain extent between-dataset variability. In the methodological sections, we explore the potential of deep learning for brain decoding. We present advancements in decoding visual stimuli using linear models at the individual subject level. We then explore how deep learning techniques can be employed for group decoding, introducing new methods to deal with between-subject variability. Finally, we also explores novel forecasting models of MEG data based on convolutional and Transformer-based architectures. In particular, Transformer-based models demonstrate superior capabilities in generating signals that closely match real brain data, thereby enhancing the accuracy and reliability of modelling the brain's electrophysiology. In the experimental section, we present a unique dataset containing high-trial inner speech EEG, MEG, and preliminary optically pumped magnetometer (OPM) data. Our aim is to investigate different types of inner speech and push decoding performance by collecting a high number of trials and sessions from a few participants. However, the decoding results are found to be mostly negative, underscoring the difficulty of decoding inner speech.</article>","contentLength":1870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Flows, straight but not so fast: Exploring the design space of Rectified Flows in Protein Design","url":"https://arxiv.org/abs/2510.24732","date":1761796800,"author":"","guid":321442,"unread":true,"content":"<article>arXiv:2510.24732v1 Announce Type: cross \nAbstract: Generative modeling techniques such as Diffusion and Flow Matching have achieved significant successes in generating designable and diverse protein backbones. However, many current models are computationally expensive, requiring hundreds or even thousands of function evaluations (NFEs) to yield samples of acceptable quality, which can become a bottleneck in practical design campaigns that often generate $10^4\\ -\\ 10^6$ designs per target. In image generation, Rectified Flows (ReFlow) can significantly reduce the required NFEs for a given target quality, but their application in protein backbone generation has been less studied. We apply ReFlow to improve the low NFE performance of pretrained SE(3) flow matching models for protein backbone generation and systematically study ReFlow design choices in the context of protein generation in data curation, training and inference time settings. In particular, we (1) show that ReFlow in the protein domain is particularly sensitive to the choice of coupling generation and annealing, (2) demonstrate how useful design choices for ReFlow in the image domain do not directly translate to better performance on proteins, and (3) make improvements to ReFlow methodology for proteins.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aerial RIS-Enhanced Communications: Joint UAV Trajectory, Altitude Control, and Phase Shift Design","url":"https://arxiv.org/abs/2510.24731","date":1761796800,"author":"","guid":321443,"unread":true,"content":"<article>arXiv:2510.24731v1 Announce Type: cross \nAbstract: Reconfigurable intelligent surface (RIS) has emerged as a pivotal technology for enhancing wireless networks. Compared to terrestrial RIS deployed on building facades, aerial RIS (ARIS) mounted on quadrotor unmanned aerial vehicle (UAV) offers superior flexibility and extended coverage. However, the inevitable tilt and altitude variations of a quadrotor UAV during flight may lead to severe beam misalignment, significantly degrading ARIS's performance. To address this challenge, we propose a Euler angles-based ARIS control scheme that jointly optimizes the altitude and trajectory of the ARIS by leveraging the UAV's dynamic model. Considering the constraints on ARIS flight energy consumption, flight safety, and the transmission power of a base station (BS), we jointly design the ARIS's altitude, trajectory, phase shifts, and BS beamforming to maximize the system sum-rate. Due to the continuous control nature of ARIS flight and the strong coupling among variables, we formulate the problem as a Markov decision process and adopt a soft actor-critic algorithm with prioritized experience replay to learn efficient ARIS control policies. Based on the optimized ARIS configuration, we further employ the water-filling and bisection method to efficiently determine the optimal BS beamforming. Numerical results demonstrate that the proposed algorithm significantly outperforms benchmarks in both convergence and communication performance, achieving approximately 14.4\\% improvement in sum-rate. Moreover, in comparison to the fixed-horizontal ARIS scheme, the proposed scheme yields more adaptive trajectories and significantly mitigates performance degradation caused by ARIS tilting, demonstrating strong potential for practical ARIS deployment.</article>","contentLength":1805,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spectral functions in Minkowski quantum electrodynamics from neural reconstruction: Benchmarking against dispersive Dyson--Schwinger integral equations","url":"https://arxiv.org/abs/2510.24728","date":1761796800,"author":"","guid":321444,"unread":true,"content":"<article>arXiv:2510.24728v1 Announce Type: cross \nAbstract: A Minkowskian physics-informed neural network approach (M--PINN) is formulated to solve the Dyson--Schwinger integral equations (DSE) of quantum electrodynamics (QED) directly in Minkowski spacetime. Our novel strategy merges two complementary approaches: (i) a dispersive solver based on Lehmann representations and subtracted dispersion relations, and (ii) a M--PINN that learns the fermion mass function $B(p^2)$, under the same truncation and renormalization configuration (quenched, rainbow, Landau gauge) with the loss integrating the DSE residual with multi--scale regularization, and monotonicity/smoothing penalties in the spacelike branch in the same way as in our previous work in Euclidean space. The benchmarks show quantitative agreement from the infrared (IR) to the ultraviolet (UV) scales in both on-shell and momentum-subtraction schemes. In this controlled setting, our M--PINN reproduces the dispersive solution whilst remaining computationally compact and differentiable, paving the way for extensions with realistic vertices, unquenching effects, and uncertainty-aware variants.</article>","contentLength":1151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modelling Real-Life Cycling Decisions in Real Urban Settings Through Psychophysiology and LLM-Derived Contextual Data","url":"https://arxiv.org/abs/2510.24726","date":1761796800,"author":"","guid":321445,"unread":true,"content":"<article>arXiv:2510.24726v1 Announce Type: cross \nAbstract: Measuring emotional states in transportation contexts is an emerging field. Methods based on self-reported emotions are limited by their low granularity and their susceptibility to memory bias. In contrast, methods based on physiological indicators provide continuous data, enabling researchers to measure changes in emotional states with high detail and accuracy. Not only are emotions important in the analysis, but understanding what triggers emotional changes is equally important. Uncontrolled variables such as traffic conditions, pedestrian interactions, and infrastructure remain a significant challenge, as they can have a great impact on emotional states. Explaining the reasons behind these emotional states requires gathering sufficient and proper contextual data, which can be extremely difficult in real-world environments. This paper addresses these challenges by applying an innovative approach, extracting contextual data (expert annotator level) from recorded multimedia using large language models (LLMs). In this paper, data are collected from an urban cycling case study of the City of Santiago, Chile. The applied models focus on understanding how different environments and traffic situations affect the emotional states and behaviors of the participants using physiological data. Sequences of images, extracted from the recorded videos, are processed by LLMs to obtain semantic descriptions of the environment. These discrete, although dense and detailed, contextual data are integrated into a hybrid model, where fatigue and arousal serve as latent variables influencing observed cycling behaviors (inferred from GPS data) like waiting, accelerating, braking, etc. The study confirms that cycling decisions are influenced by stress-related emotions and highlights the strong impact of urban characteristics and traffic conditions on cyclist behavior.</article>","contentLength":1926,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ambient Backscatter Communication Assisted by Fluid Reconfigurable Intelligent Surfaces","url":"https://arxiv.org/abs/2510.24725","date":1761796800,"author":"","guid":321446,"unread":true,"content":"<article>arXiv:2510.24725v1 Announce Type: cross \nAbstract: This paper investigates the integration of a fluid reconfigurable intelligent surface (FRIS) into ambient backscatter communication (AmBC) systems. Unlike conventional reconfigurable intelligent surfaces (RISs) with fixed position elements, FRIS employs fluidic elements that can dynamically adjust their positions, offering enhanced spatial adaptability. We develop a system model where an AmBC tag communicates with a reader through an FRIS, which is particularly beneficial in scenarios where the direct tag-to-reader link is weak or blocked by obstacles. The achievable backscatter rate is analyzed, and the optimization of FRIS element positions is formulated as a non-convex problem. To address this, we employ particle swarm optimization (PSO) to obtain near-optimal configurations of the fluid elements. Simulation results demonstrate that FRIS-aided AmBC significantly outperforms conventional RIS-based AmBC systems in terms of achievable throughput.</article>","contentLength":1011,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distributed learning for automatic modulation recognition in bandwidth-limited networks","url":"https://arxiv.org/abs/2510.24722","date":1761796800,"author":"","guid":321447,"unread":true,"content":"<article>arXiv:2510.24722v1 Announce Type: cross \nAbstract: Automatic Modulation Recognition (AMR) is critical in identifying various modulation types in wireless communication systems. Recent advancements in deep learning have facilitated the integration of algorithms into AMR techniques. However, this integration typically follows a centralized approach that necessitates collecting and processing all training data on high-powered computing devices, which may prove impractical for bandwidth-limited wireless networks. In response to this challenge, this study introduces two methods for distributed learning-based AMR on the collaboration of multiple receivers to perform AMR tasks. The TeMuRAMRD 2023 dataset is employed to support this investigation, uniquely suited for multi-receiver AMR tasks. Within this distributed sensing environment, multiple receivers collaborate in identifying modulation types from the same RF signal, each possessing a partial perspective of the overall environment. Experimental results demonstrate that the centralized-based AMR, with six receivers, attains an impressive accuracy rate of 91%, while individual receivers exhibit a notably lower accuracy, at around 41%. Nonetheless, the two proposed decentralized learning-based AMR methods exhibit noteworthy enhancements. Based on consensus voting among six receivers, the initial method achieves a marginally lower accuracy. It achieves this while substantially reducing the bandwidth demands to a 1/256th of the centralized model. With the second distributed method, each receiver shares its feature map, subsequently aggregated by a central node. This approach also accompanies a substantial bandwidth reduction of 1/8 compared to the centralized approach. These findings highlight the capacity of distributed AMR to significantly enhance accuracy while effectively addressing the constraints of bandwidth-limited wireless networks.</article>","contentLength":1917,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SocializeChat: A GPT-Based AAC Tool Grounded in Personal Memories to Support Social Communication","url":"https://arxiv.org/abs/2510.19017","date":1761796800,"author":"","guid":321448,"unread":true,"content":"<article>arXiv:2510.19017v1 Announce Type: cross \nAbstract: Elderly people with speech impairments often face challenges in engaging in meaningful social communication, particularly when using Augmentative and Alternative Communication (AAC) tools that primarily address basic needs. Moreover, effective chats often rely on personal memories, which is hard to extract and reuse. We introduce SocializeChat, an AAC tool that generates sentence suggestions by drawing on users' personal memory records. By incorporating topic preference and interpersonal closeness, the system reuses past experience and tailors suggestions to different social contexts and conversation partners. SocializeChat not only leverages past experiences to support interaction, but also treats conversations as opportunities to create new memories, fostering a dynamic cycle between memory and communication. A user study shows its potential to enhance the inclusivity and relevance of AAC-supported social interaction.</article>","contentLength":984,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation","url":"https://arxiv.org/abs/2510.01225","date":1761796800,"author":"","guid":321449,"unread":true,"content":"<article>arXiv:2510.01225v1 Announce Type: cross \nAbstract: The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.</article>","contentLength":1290,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Re-evaluating sample efficiency in de novo molecule generation","url":"https://arxiv.org/abs/2212.01385","date":1761796800,"author":"","guid":321450,"unread":true,"content":"<article>arXiv:2212.01385v1 Announce Type: cross \nAbstract: De novo molecule generation can suffer from data inefficiency; requiring large amounts of training data or many sampled data points to conduct objective optimization. The latter is a particular disadvantage when combining deep generative models with computationally expensive molecule scoring functions (a.k.a. oracles) commonly used in computer-aided drug design. Recent works have therefore focused on methods to improve sample efficiency in the context of de novo molecule drug design, or to benchmark it. In this work, we discuss and adapt a recent sample efficiency benchmark to better reflect realistic goals also with respect to the quality of chemistry generated, which must always be considered in the context of small-molecule drug design; we then re-evaluate all benchmarked generative models. We find that accounting for molecular weight and LogP with respect to the training data, and the diversity of chemistry proposed, re-orders the ranking of generative models. In addition, we benchmark a recently proposed method to improve sample efficiency (Augmented Hill-Climb) and found it ranked top when considering both the sample efficiency and chemistry of molecules generated. Continual improvements in sample efficiency and chemical desirability enable more routine integration of computationally expensive scoring functions on a more realistic timescale.</article>","contentLength":1420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large-Scale Network Embedding in Apache Spark","url":"https://arxiv.org/abs/2106.10620","date":1761796800,"author":"","guid":321451,"unread":true,"content":"<article>arXiv:2106.10620v1 Announce Type: cross \nAbstract: Network embedding has been widely used in social recommendation and network analysis, such as recommendation systems and anomaly detection with graphs. However, most of previous approaches cannot handle large graphs efficiently, due to that (i) computation on graphs is often costly and (ii) the size of graph or the intermediate results of vectors could be prohibitively large, rendering it difficult to be processed on a single machine. In this paper, we propose an efficient and effective distributed algorithm for network embedding on large graphs using Apache Spark, which recursively partitions a graph into several small-sized subgraphs to capture the internal and external structural information of nodes, and then computes the network embedding for each subgraph in parallel. Finally, by aggregating the outputs on all subgraphs, we obtain the embeddings of nodes in a linear cost. After that, we demonstrate in various experiments that our proposed approach is able to handle graphs with billions of edges within a few hours and is at least 4 times faster than the state-of-the-art approaches. Besides, it achieves up to $4.25\\%$ and $4.27\\%$ improvements on link prediction and node classification tasks respectively. In the end, we deploy the proposed algorithms in two online games of Tencent with the applications of friend recommendation and item recommendation, which improve the competitors by up to $91.11\\%$ in running time and up to $12.80\\%$ in the corresponding evaluation metrics.</article>","contentLength":1554,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning","url":"https://arxiv.org/abs/2510.25772","date":1761796800,"author":"","guid":321452,"unread":true,"content":"<article>arXiv:2510.25772v1 Announce Type: new \nAbstract: Visual effects (VFX) are crucial to the expressive power of digital media, yet their creation remains a major challenge for generative AI. Prevailing methods often rely on the one-LoRA-per-effect paradigm, which is resource-intensive and fundamentally incapable of generalizing to unseen effects, thus limiting scalability and creation. To address this challenge, we introduce VFXMaster, the first unified, reference-based framework for VFX video generation. It recasts effect generation as an in-context learning task, enabling it to reproduce diverse dynamic effects from a reference video onto target content. In addition, it demonstrates remarkable generalization to unseen effect categories. Specifically, we design an in-context conditioning strategy that prompts the model with a reference example. An in-context attention mask is designed to precisely decouple and inject the essential effect attributes, allowing a single unified model to master the effect imitation without information leakage. In addition, we propose an efficient one-shot effect adaptation mechanism to boost generalization capability on tough unseen effects from a single user-provided video rapidly. Extensive experiments demonstrate that our method effectively imitates various categories of effect information and exhibits outstanding generalization to out-of-domain effects. To foster future research, we will release our code, models, and a comprehensive dataset to the community.</article>","contentLength":1514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gaperon: A Peppered English-French Generative Language Model Suite","url":"https://arxiv.org/abs/2510.25771","date":1761796800,"author":"","guid":321453,"unread":true,"content":"<article>arXiv:2510.25771v1 Announce Type: new \nAbstract: We release Gaperon, a fully open suite of French-English-coding language models designed to advance transparency and reproducibility in large-scale model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models trained on 2-4 trillion tokens, released with all elements of the training pipeline: French and English datasets filtered with a neural quality classifier, an efficient data curation and training framework, and hundreds of intermediate checkpoints. Through this work, we study how data filtering and contamination interact to shape both benchmark and generative performance. We find that filtering for linguistic quality enhances text fluency and coherence but yields subpar benchmark results, and that late deliberate contamination -- continuing training on data mixes that include test sets -- recovers competitive scores while only reasonably harming generation quality. We discuss how usual neural filtering can unintentionally amplify benchmark leakage. To support further research, we also introduce harmless data poisoning during pretraining, providing a realistic testbed for safety studies. By openly releasing all models, datasets, code, and checkpoints, Gaperon establishes a reproducible foundation for exploring the trade-offs between data curation, evaluation, safety, and openness in multilingual language model development.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions","url":"https://arxiv.org/abs/2510.25769","date":1761796800,"author":"","guid":321454,"unread":true,"content":"<article>arXiv:2510.25769v1 Announce Type: new \nAbstract: Stochastic differential equations (SDEs) are well suited to modelling noisy and irregularly sampled time series found in finance, physics, and machine learning. Traditional approaches require costly numerical solvers to sample between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and their latent variants, which directly learn (latent) SDE transition laws using conditional normalising flows with architectural constraints that preserve properties inherited from stochastic flows. This enables one-shot sampling between arbitrary states and yields up to two orders of magnitude speed-ups at large time gaps. Experiments on synthetic SDE simulations and on real-world tracking and video data show that NSFs maintain distributional accuracy comparable to numerical approaches while dramatically reducing computation for arbitrary time-point sampling.</article>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STITCH 2.0: Extending Augmented Suturing with EKF Needle Estimation and Thread Management","url":"https://arxiv.org/abs/2510.25768","date":1761796800,"author":"","guid":321455,"unread":true,"content":"<article>arXiv:2510.25768v1 Announce Type: new \nAbstract: Surgical suturing is a high-precision task that impacts patient healing and scarring. Suturing skill varies widely between surgeons, highlighting the need for robot assistance. Previous robot suturing works, such as STITCH 1.0 [1], struggle to fully close wounds due to inaccurate needle tracking and poor thread management. To address these challenges, we present STITCH 2.0, an elevated augmented dexterity pipeline with seven improvements including: improved EKF needle pose estimation, new thread untangling methods, and an automated 3D suture alignment algorithm. Experimental results over 15 trials find that STITCH 2.0 on average achieves 74.4% wound closure with 4.87 sutures per trial, representing 66% more sutures in 38% less time compared to the previous baseline. When two human interventions are allowed, STITCH 2.0 averages six sutures with 100% wound closure rate. Project website: https://stitch-2.github.io/</article>","contentLength":974,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decomposition-Enhanced Training for Post-Hoc Attributions In Language Models","url":"https://arxiv.org/abs/2510.25766","date":1761796800,"author":"","guid":321456,"unread":true,"content":"<article>arXiv:2510.25766v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly used for long-document question answering, where reliable attribution to sources is critical for trust. Existing post-hoc attribution methods work well for extractive QA but struggle in multi-hop, abstractive, and semi-extractive settings, where answers synthesize information across passages. To address these challenges, we argue that post-hoc attribution can be reframed as a reasoning problem, where answers are decomposed into constituent units, each tied to specific context. We first show that prompting models to generate such decompositions alongside attributions improves performance. Building on this, we introduce DecompTune, a post-training method that teaches models to produce answer decompositions as intermediate reasoning steps. We curate a diverse dataset of complex QA tasks, annotated with decompositions by a strong LLM, and post-train Qwen-2.5 (7B and 14B) using a two-stage SFT + GRPO pipeline with task-specific curated rewards. Across extensive experiments and ablations, DecompTune substantially improves attribution quality, outperforming prior methods and matching or exceeding state-of-the-art frontier models.</article>","contentLength":1235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreeArt3D: Training-Free Articulated Object Generation using 3D Diffusion","url":"https://arxiv.org/abs/2510.25765","date":1761796800,"author":"","guid":321457,"unread":true,"content":"<article>arXiv:2510.25765v1 Announce Type: new \nAbstract: Articulated 3D objects are central to many applications in robotics, AR/VR, and animation. Recent approaches to modeling such objects either rely on optimization-based reconstruction pipelines that require dense-view supervision or on feed-forward generative models that produce coarse geometric approximations and often overlook surface texture. In contrast, open-world 3D generation of static objects has achieved remarkable success, especially with the advent of native 3D diffusion models such as Trellis. However, extending these methods to articulated objects by training native 3D diffusion models poses significant challenges. In this work, we present FreeArt3D, a training-free framework for articulated 3D object generation. Instead of training a new model on limited articulated data, FreeArt3D repurposes a pre-trained static 3D diffusion model (e.g., Trellis) as a powerful shape prior. It extends Score Distillation Sampling (SDS) into the 3D-to-4D domain by treating articulation as an additional generative dimension. Given a few images captured in different articulation states, FreeArt3D jointly optimizes the object's geometry, texture, and articulation parameters without requiring task-specific training or access to large-scale articulated datasets. Our method generates high-fidelity geometry and textures, accurately predicts underlying kinematic structures, and generalizes well across diverse object categories. Despite following a per-instance optimization paradigm, FreeArt3D completes in minutes and significantly outperforms prior state-of-the-art approaches in both quality and versatility.</article>","contentLength":1670,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiagramEval: Evaluating LLM-Generated Diagrams via Graphs","url":"https://arxiv.org/abs/2510.25761","date":1761796800,"author":"","guid":321458,"unread":true,"content":"<article>arXiv:2510.25761v1 Announce Type: new \nAbstract: Diagrams play a central role in research papers for conveying ideas, yet they are often notoriously complex and labor-intensive to create. Although diagrams are presented as images, standard image generative models struggle to produce clear diagrams with well-defined structure. We argue that a promising direction is to generate demonstration diagrams directly in textual form as SVGs, which can leverage recent advances in large language models (LLMs). However, due to the complexity of components and the multimodal nature of diagrams, sufficiently discriminative and explainable metrics for evaluating the quality of LLM-generated diagrams remain lacking. In this paper, we propose DiagramEval, a novel evaluation metric designed to assess demonstration diagrams generated by LLMs. Specifically, DiagramEval conceptualizes diagrams as graphs, treating text elements as nodes and their connections as directed edges, and evaluates diagram quality using two new groups of metrics: node alignment and path alignment. For the first time, we effectively evaluate diagrams produced by state-of-the-art LLMs on recent research literature, quantitatively demonstrating the validity of our metrics. Furthermore, we show how the enhanced explainability of our proposed metrics offers valuable insights into the characteristics of LLM-generated diagrams. Code: https://github.com/ulab-uiuc/diagram-eval.</article>","contentLength":1445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks","url":"https://arxiv.org/abs/2510.25760","date":1761796800,"author":"","guid":321459,"unread":true,"content":"<article>arXiv:2510.25760v1 Announce Type: new \nAbstract: Humans possess spatial reasoning abilities that enable them to understand spaces through multimodal observations, such as vision and sound. Large multimodal reasoning models extend these abilities by learning to perceive and reason, showing promising performance across diverse spatial tasks. However, systematic reviews and publicly available benchmarks for these models remain limited. In this survey, we provide a comprehensive review of multimodal spatial reasoning tasks with large models, categorizing recent progress in multimodal large language models (MLLMs) and introducing open benchmarks for evaluation. We begin by outlining general spatial reasoning, focusing on post-training techniques, explainability, and architecture. Beyond classical 2D tasks, we examine spatial relationship reasoning, scene and layout understanding, as well as visual question answering and grounding in 3D space. We also review advances in embodied AI, including vision-language navigation and action models. Additionally, we consider emerging modalities such as audio and egocentric video, which contribute to novel spatial understanding through new sensors. We believe this survey establishes a solid foundation and offers insights into the growing field of multimodal spatial reasoning. Updated information about this survey, codes and implementation of the open benchmarks can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</article>","contentLength":1489,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning","url":"https://arxiv.org/abs/2510.25759","date":1761796800,"author":"","guid":321460,"unread":true,"content":"<article>arXiv:2510.25759v1 Announce Type: new \nAbstract: Multiple instance learning (MIL) is often used in medical imaging to classify high-resolution 2D images by processing patches or classify 3D volumes by processing slices. However, conventional MIL approaches treat instances separately, ignoring contextual relationships such as the appearance of nearby patches or slices that can be essential in real applications. We design a synthetic classification task where accounting for adjacent instance features is crucial for accurate prediction. We demonstrate the limitations of off-the-shelf MIL approaches by quantifying their performance compared to the optimal Bayes estimator for this task, which is available in closed-form. We empirically show that newer correlated MIL methods still struggle to generalize as well as possible when trained from scratch on tens of thousands of instances.</article>","contentLength":889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling","url":"https://arxiv.org/abs/2510.25758","date":1761796800,"author":"","guid":321461,"unread":true,"content":"<article>arXiv:2510.25758v1 Announce Type: new \nAbstract: Large language models (LLMs) in psychological counseling have attracted increasing attention. However, existing approaches often lack emotional understanding, adaptive strategies, and the use of therapeutic methods across multiple sessions with long-term memory, leaving them far from real clinical practice. To address these critical gaps, we introduce TheraMind, a strategic and adaptive agent for longitudinal psychological counseling. The cornerstone of TheraMind is a novel dual-loop architecture that decouples the complex counseling process into an Intra-Session Loop for tactical dialogue management and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session Loop perceives the patient's emotional state to dynamically select response strategies while leveraging cross-session memory to ensure continuity. Crucially, the Cross-Session Loop empowers the agent with long-term adaptability by evaluating the efficacy of the applied therapy after each session and adjusting the method for subsequent interactions. We validate our approach in a high-fidelity simulation environment grounded in real clinical cases. Extensive evaluations show that TheraMind outperforms other methods, especially on multi-session metrics like Coherence, Flexibility, and Therapeutic Attunement, validating the effectiveness of its dual-loop design in emulating strategic, adaptive, and longitudinal therapeutic behavior. The code is publicly available at https://0mwwm0.github.io/TheraMind/.</article>","contentLength":1545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holon Streaming: Global Aggregations with Windowed CRDTs","url":"https://arxiv.org/abs/2510.25757","date":1761796800,"author":"","guid":321462,"unread":true,"content":"<article>arXiv:2510.25757v1 Announce Type: new \nAbstract: Scaling global aggregations is a challenge for exactly-once stream processing systems. Current systems implement these either by computing the aggregation in a single task instance, or by static aggregation trees, which limits scalability and may become a bottleneck. Moreover, the end-to-end latency is determined by the slowest path in the tree, and failures and reconfiguration cause large latency spikes due to the centralized coordination. Towards these issues, we present Holon Streaming, an exactly-once stream processing system for global aggregations. Its deterministic programming model uses windowed conflict-free replicated data types (Windowed CRDTs), a novel abstraction for shared replicated state. Windowed CRDTs make computing global aggregations scalable. Furthermore, their guarantees such as determinism and convergence enable the design of efficient failure recovery algorithms by decentralized coordination. Our evaluation shows a 5x lower latency and 2x higher throughput than an existing stream processing system on global aggregation workloads, with an 11x latency reduction under failure scenarios. The paper demonstrates the effectiveness of decentralized coordination with determinism, and the utility of Windowed CRDTs for global aggregations.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MLPrE -- A tool for preprocessing and exploratory data analysis prior to machine learning model construction","url":"https://arxiv.org/abs/2510.25755","date":1761796800,"author":"","guid":321463,"unread":true,"content":"<article>arXiv:2510.25755v1 Announce Type: new \nAbstract: With the recent growth of Deep Learning for AI, there is a need for tools to meet the demand of data flowing into those models. In some cases, source data may exist in multiple formats, and therefore the source data must be investigated and properly engineered for a Machine Learning model or graph database. Overhead and lack of scalability with existing workflows limit integration within a larger processing pipeline such as Apache Airflow, driving the need for a robust, extensible, and lightweight tool to preprocess arbitrary datasets that scales with data type and size. To address this, we present Machine Learning Preprocessing and Exploratory Data Analysis, MLPrE, in which SparkDataFrames were utilized to hold data during processing and ensure scalability. A generalizable JSON input file format was utilized to describe stepwise changes to that DataFrame. Stages were implemented for input and output, filtering, basic statistics, feature engineering, and exploratory data analysis. A total of 69 stages were implemented into MLPrE, of which we highlight and demonstrate key stages using six diverse datasets. We further highlight MLPrE's ability to independently process multiple fields in flat files and recombine them, otherwise requiring an additional pipeline, using a UniProt glossary term dataset. Building on this advantage, we demonstrated the clustering stage with available wine quality data. Lastly, we demonstrate the preparation of data for a graph database in the final stages of MLPrE using phosphosite kinase data. Overall, our MLPrE tool offers a generalizable and scalable tool for preprocessing and early data analysis, filling a critical need for such a tool given the ever expanding use of machine learning. This tool serves to accelerate and simplify early stage development in larger workflows.</article>","contentLength":1880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GET-USE: Learning Generalized Tool Usage for Bimanual Mobile Manipulation via Simulated Embodiment Extensions","url":"https://arxiv.org/abs/2510.25754","date":1761796800,"author":"","guid":321464,"unread":true,"content":"<article>arXiv:2510.25754v1 Announce Type: new \nAbstract: The ability to use random objects as tools in a generalizable manner is a missing piece in robots' intelligence today to boost their versatility and problem-solving capabilities. State-of-the-art robotic tool usage methods focused on procedurally generating or crowd-sourcing datasets of tools for a task to learn how to grasp and manipulate them for that task. However, these methods assume that only one object is provided and that it is possible, with the correct grasp, to perform the task; they are not capable of identifying, grasping, and using the best object for a task when many are available, especially when the optimal tool is absent. In this work, we propose GeT-USE, a two-step procedure that learns to perform real-robot generalized tool usage by learning first to extend the robot's embodiment in simulation and then transferring the learned strategies to real-robot visuomotor policies. Our key insight is that by exploring a robot's embodiment extensions (i.e., building new end-effectors) in simulation, the robot can identify the general tool geometries most beneficial for a task. This learned geometric knowledge can then be distilled to perform generalized tool usage tasks by selecting and using the best available real-world object as tool. On a real robot with 22 degrees of freedom (DOFs), GeT-USE outperforms state-of-the-art methods by 30-60% success rates across three vision-based bimanual mobile manipulation tool-usage tasks.</article>","contentLength":1508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meshless solutions of PDE inverse problems on irregular geometries","url":"https://arxiv.org/abs/2510.25752","date":1761796800,"author":"","guid":321465,"unread":true,"content":"<article>arXiv:2510.25752v1 Announce Type: new \nAbstract: Solving inverse and optimization problems over solutions of nonlinear partial differential equations (PDEs) on complex spatial domains is a long-standing challenge. Here we introduce a method that parameterizes the solution using spectral bases on arbitrary spatiotemporal domains, whereby the basis is defined on a hyperrectangle containing the true domain. We find the coefficients of the basis expansion by solving an optimization problem whereby both the equations, the boundary conditions and any optimization targets are enforced by a loss function, building on a key idea from Physics-Informed Neural Networks (PINNs). Since the representation of the function natively has exponential convergence, so does the solution of the optimization problem, as long as it can be solved efficiently. We find empirically that the optimization protocols developed for machine learning find solutions with exponential convergence on a wide range of equations. The method naturally allows for the incorporation of data assimilation by including additional terms in the loss function, and for the efficient solution of optimization problems over the PDE solutions.</article>","contentLength":1204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exact zCDP Characterizations for Fundamental Differentially Private Mechanisms","url":"https://arxiv.org/abs/2510.25746","date":1761796800,"author":"","guid":321466,"unread":true,"content":"<article>arXiv:2510.25746v1 Announce Type: new \nAbstract: Zero-concentrated differential privacy (zCDP) is a variant of differential privacy (DP) that is widely used partly thanks to its nice composition property. While a tight conversion from $\\epsilon$-DP to zCDP exists for the worst-case mechanism, many common algorithms satisfy stronger guarantees. In this work, we derive tight zCDP characterizations for several fundamental mechanisms. We prove that the tight zCDP bound for the $\\epsilon$-DP Laplace mechanism is exactly $\\epsilon + e^{-\\epsilon} - 1$, confirming a recent conjecture by Wang (2022). We further provide tight bounds for the discrete Laplace mechanism, $k$-Randomized Response (for $k \\leq 6$), and RAPPOR. Lastly, we also provide a tight zCDP bound for the worst case bounded range mechanism.</article>","contentLength":808,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Vocal Source Separation Through Windowed Sink Attention","url":"https://arxiv.org/abs/2510.25745","date":1761796800,"author":"","guid":321467,"unread":true,"content":"<article>arXiv:2510.25745v1 Announce Type: new \nAbstract: State-of-the-art vocal separation models like Mel-Band-Roformer rely on full temporal self-attention mechanisms, where each temporal frame interacts with every other frames. This incurs heavy computational costs that scales quadratically with input audio length, motivating chunking and windowing approaches. Through analysis of a pre-trained vocal separation model, we discovered that temporal attention patterns are highly localized. Building on this insight, we replaced full attention with windowed sink attention (WSA) with small temporal attention window and attention sinks. We show empirically that fine-tuning from the original checkpoint recovers 92% of the original SDR performance while reducing FLOPs by 44.5x. We release our code and checkpoints under MIT license at https://github.com/smulelabs/windowed-roformer.</article>","contentLength":877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Task Completion Agents are Not Ideal Collaborators","url":"https://arxiv.org/abs/2510.25744","date":1761796800,"author":"","guid":321468,"unread":true,"content":"<article>arXiv:2510.25744v1 Announce Type: new \nAbstract: Current evaluations of agents remain centered around one-shot task completion, failing to account for the inherently iterative and collaborative nature of many real-world problems, where human goals are often underspecified and evolve. We argue for a shift from building and assessing task completion agents to developing collaborative agents, assessed not only by the quality of their final outputs but by how well they engage with and enhance human effort throughout the problem-solving process. To support this shift, we introduce collaborative effort scaling, a framework that captures how an agent's utility grows with increasing user involvement. Through case studies and simulated evaluations, we show that state-of-the-art agents often underperform in multi-turn, real-world scenarios, revealing a missing ingredient in agent design: the ability to sustain engagement and scaffold user understanding. Collaborative effort scaling offers a lens for diagnosing agent behavior and guiding development toward more effective interactions.</article>","contentLength":1090,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Latent Reasoning via Looped Language Models","url":"https://arxiv.org/abs/2510.25741","date":1761796800,"author":"","guid":321469,"unread":true,"content":"<article>arXiv:2510.25741v1 Announce Type: new \nAbstract: Modern LLMs are trained to \"think\" primarily via explicit text generation, such as chain-of-thought (CoT), which defers reasoning to post-training and under-leverages pre-training data. We present and open-source Ouro, named after the recursive Ouroboros, a family of pre-trained Looped Language Models (LoopLM) that instead build reasoning into the pre-training phase through (i) iterative computation in latent space, (ii) an entropy-regularized objective for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and 2.6B models enjoy superior performance that match the results of up to 12B SOTA LLMs across a wide range of benchmarks. Through controlled experiments, we show this advantage stems not from increased knowledge capacity, but from superior knowledge manipulation capabilities. We also show that LoopLM yields reasoning traces more aligned with final outputs than explicit CoT. We hope our results show the potential of LoopLM as a novel scaling direction in the reasoning era. Our model could be found in: http://ouro-llm.github.io.</article>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A mathematical study of the excess growth rate","url":"https://arxiv.org/abs/2510.25740","date":1761796800,"author":"","guid":321470,"unread":true,"content":"<article>arXiv:2510.25740v1 Announce Type: new \nAbstract: We study the excess growth rate -- a fundamental logarithmic functional arising in portfolio theory -- from the perspective of information theory. We show that the excess growth rate can be connected to the R\\'{e}nyi and cross entropies, the Helmholtz free energy, L. Campbell's measure of average code length and large deviations. Our main results consist of three axiomatic characterization theorems of the excess growth rate, in terms of (i) the relative entropy, (ii) the gap in Jensen's inequality, and (iii) the logarithmic divergence that generalizes the Bregman divergence. Furthermore, we study maximization of the excess growth rate and compare it with the growth optimal portfolio. Our results not only provide theoretical justifications of the significance of the excess growth rate, but also establish new connections between information theory and quantitative finance.</article>","contentLength":932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation","url":"https://arxiv.org/abs/2510.25739","date":1761796800,"author":"","guid":321471,"unread":true,"content":"<article>arXiv:2510.25739v1 Announce Type: new \nAbstract: Autoregressive (AR) image generation models are capable of producing high-fidelity images but often suffer from slow inference due to their inherently sequential, token-by-token decoding process. Speculative decoding, which employs a lightweight draft model to approximate the output of a larger AR model, has shown promise in accelerating text generation without compromising quality. However, its application to image generation remains largely underexplored. The challenges stem from a significantly larger sampling space, which complicates the alignment between the draft and target model outputs, coupled with the inadequate use of the two-dimensional spatial structure inherent in images, thereby limiting the modeling of local dependencies. To overcome these challenges, we introduce Hawk, a new approach that harnesses the spatial structure of images to guide the speculative model toward more accurate and efficient predictions. Experimental results on multiple text-to-image benchmarks demonstrate a 1.71x speedup over standard AR models, while preserving both image fidelity and diversity.</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Effect of Full Common Randomness Replication in Symmetric PIR on Graph-Based Replicated Systems","url":"https://arxiv.org/abs/2510.25736","date":1761796800,"author":"","guid":321472,"unread":true,"content":"<article>arXiv:2510.25736v1 Announce Type: new \nAbstract: We revisit the problem of symmetric private information retrieval (SPIR) in settings where the database replication is modeled by a simple graph. Here, each vertex corresponds to a server, and a message is replicated on two servers if and only if there is an edge between them. To satisfy the requirement of database privacy, we let all the servers share some common randomness, independent of the messages. We aim to quantify the improvement in SPIR capacity, i.e., the maximum ratio of the number of desired and downloaded symbols, compared to the setting with graph-replicated common randomness. Towards this, we develop an algorithm to convert a class of PIR schemes into the corresponding SPIR schemes, thereby establishing a capacity lower bound on graphs for which such schemes exist. This includes the class of path and cyclic graphs for which we derive capacity upper bounds that are tighter than the trivial bounds given by the respective PIR capacities. For the special case of path graph with three vertices, we identify the SPIR capacity to be $\\frac{1}{2}$.</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Limits of Obliviate: Evaluating Unlearning in LLMs via Stimulus-Knowledge Entanglement-Behavior Framework","url":"https://arxiv.org/abs/2510.25732","date":1761796800,"author":"","guid":321473,"unread":true,"content":"<article>arXiv:2510.25732v1 Announce Type: new \nAbstract: Unlearning in large language models (LLMs) is crucial for managing sensitive data and correcting misinformation, yet evaluating its effectiveness remains an open problem. We investigate whether persuasive prompting can recall factual knowledge from deliberately unlearned LLMs across models ranging from 2.7B to 13B parameters (OPT-2.7B, LLaMA-2-7B, LLaMA-3.1-8B, LLaMA-2-13B). Drawing from ACT-R and Hebbian theory (spreading activation theories), as well as communication principles, we introduce Stimulus-Knowledge Entanglement-Behavior Framework (SKeB), which models information entanglement via domain graphs and tests whether factual recall in unlearned models is correlated with persuasive framing. We develop entanglement metrics to quantify knowledge activation patterns and evaluate factuality, non-factuality, and hallucination in outputs. Our results show persuasive prompts substantially enhance factual knowledge recall (14.8% baseline vs. 24.5% with authority framing), with effectiveness inversely correlated to model size (128% recovery in 2.7B vs. 15% in 13B). SKeB provides a foundation for assessing unlearning completeness, robustness, and overall behavior in LLMs.</article>","contentLength":1235,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LieSolver: A PDE-constrained solver for IBVPs using Lie symmetries","url":"https://arxiv.org/abs/2510.25731","date":1761796800,"author":"","guid":321474,"unread":true,"content":"<article>arXiv:2510.25731v1 Announce Type: new \nAbstract: We introduce a method for efficiently solving initial-boundary value problems (IBVPs) that uses Lie symmetries to enforce the associated partial differential equation (PDE) exactly by construction. By leveraging symmetry transformations, the model inherently incorporates the physical laws and learns solutions from initial and boundary data. As a result, the loss directly measures the model's accuracy, leading to improved convergence. Moreover, for well-posed IBVPs, our method enables rigorous error estimation. The approach yields compact models, facilitating an efficient optimization. We implement LieSolver and demonstrate its application to linear homogeneous PDEs with a range of initial conditions, showing that it is faster and more accurate than physics-informed neural networks (PINNs). Overall, our method improves both computational efficiency and the reliability of predictions for PDE-constrained problems.</article>","contentLength":973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modeling Collapse of Steered Vine Robots Under Their Own Weight","url":"https://arxiv.org/abs/2510.25727","date":1761796800,"author":"","guid":321475,"unread":true,"content":"<article>arXiv:2510.25727v1 Announce Type: new \nAbstract: Soft, vine-inspired growing robots that move by eversion are highly mobile in confined environments, but, when faced with gaps in the environment, they may collapse under their own weight while navigating a desired path. In this work, we present a comprehensive collapse model that can predict the collapse length of steered robots in any shape using true shape information and tail tension. We validate this model by collapsing several unsteered robots without true shape information. The model accurately predicts the trends of those experiments. We then attempt to collapse a robot steered with a single actuator at different orientations. Our models accurately predict collapse when it occurs. Finally, we demonstrate how this could be used in the field by having a robot attempt a gap-crossing task with and without inflating its actuators. The robot needs its actuators inflated to cross the gap without collapsing, which our model supports. Our model has been specifically tested on straight and series pouch motor-actuated robots made of non-stretchable material, but it could be applied to other robot variations. This work enables us to model the robot's collapse behavior in any open environment and understand the parameters it needs to succeed in 3D navigation tasks.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution","url":"https://arxiv.org/abs/2510.25726","date":1761796800,"author":"","guid":321476,"unread":true,"content":"<article>arXiv:2510.25726v1 Announce Type: new \nAbstract: Real-world language agents must handle complex, multi-step workflows across diverse Apps. For instance, an agent may manage emails by coordinating with calendars and file systems, or monitor a production database to detect anomalies and generate reports following an operating manual. However, existing language agent benchmarks often focus on narrow domains or simplified tasks that lack the diversity, realism, and long-horizon complexity required to evaluate agents' real-world performance. To address this gap, we introduce the Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering diverse Apps and tools, realistic environment setup, and reliable execution-based evaluation. Toolathlon spans 32 software applications and 604 tools, ranging from everyday platforms such as Google Calendar and Notion to professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools are based on a high-quality set of Model Context Protocol (MCP) servers that we may have revised or implemented ourselves. Unlike prior works, which primarily ensure functional realism but offer limited environment state diversity, we provide realistic initial environment states from real software, such as Canvas courses with dozens of students or real financial spreadsheets. This benchmark includes 108 manually sourced or crafted tasks in total, requiring interacting with multiple Apps over around 20 turns on average to complete. Each task is strictly verifiable through dedicated evaluation scripts. Comprehensive evaluation of SOTA models highlights their significant shortcomings: the best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate with 20.2 tool calling turns on average, while the top open-weights model DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development of more capable language agents for real-world, long-horizon task execution.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation","url":"https://arxiv.org/abs/2510.25725","date":1761796800,"author":"","guid":321477,"unread":true,"content":"<article>arXiv:2510.25725v1 Announce Type: new \nAbstract: Contact-rich manipulation has become increasingly important in robot learning. However, previous studies on robot learning datasets have focused on rigid objects and underrepresented the diversity of pressure conditions for real-world manipulation. To address this gap, we present a humanoid visual-tactile-action dataset designed for manipulating deformable soft objects. The dataset was collected via teleoperation using a humanoid robot equipped with dexterous hands, capturing multi-modal interactions under varying pressure conditions. This work also motivates future research on models with advanced optimization strategies capable of effectively leveraging the complexity and diversity of tactile signals.</article>","contentLength":761,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BambooKG: A Neurobiologically-inspired Frequency-Weight Knowledge Graph","url":"https://arxiv.org/abs/2510.25724","date":1761796800,"author":"","guid":321478,"unread":true,"content":"<article>arXiv:2510.25724v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation allows LLMs to access external knowledge, reducing hallucinations and ageing-data issues. However, it treats retrieved chunks independently and struggles with multi-hop or relational reasoning, especially across documents. Knowledge graphs enhance this by capturing the relationships between entities using triplets, enabling structured, multi-chunk reasoning. However, these tend to miss information that fails to conform to the triplet structure. We introduce BambooKG, a knowledge graph with frequency-based weights on non-triplet edges which reflect link strength, drawing on the Hebbian principle of \"fire together, wire together\". This decreases information loss and results in improved performance on single- and multi-hop reasoning, outperforming the existing solutions.</article>","contentLength":858,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Retrieval-Augmented Search for Large-Scale Map Collections with ColPali","url":"https://arxiv.org/abs/2510.25718","date":1761796800,"author":"","guid":321479,"unread":true,"content":"<article>arXiv:2510.25718v1 Announce Type: new \nAbstract: Multimodal approaches have shown great promise for searching and navigating digital collections held by libraries, archives, and museums. In this paper, we introduce map-RAS: a retrieval-augmented search system for historic maps. In addition to introducing our framework, we detail our publicly-hosted demo for searching 101,233 map images held by the Library of Congress. With our system, users can multimodally query the map collection via ColPali, summarize search results using Llama 3.2, and upload their own collections to perform inter-collection search. We articulate potential use cases for archivists, curators, and end-users, as well as future work with our system in both machine learning and the digital humanities. Our demo can be viewed at: http://www.mapras.com.</article>","contentLength":827,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Binaspect -- A Python Library for Binaural Audio Analysis, Visualization & Feature Generation","url":"https://arxiv.org/abs/2510.25714","date":1761796800,"author":"","guid":321480,"unread":true,"content":"<article>arXiv:2510.25714v1 Announce Type: new \nAbstract: We present Binaspect, an open-source Python library for binaural audio analysis, visualization, and feature generation. Binaspect generates interpretable \"azimuth maps\" by calculating modified interaural time and level difference spectrograms, and clustering those time-frequency (TF) bins into stable time-azimuth histogram representations. This allows multiple active sources to appear as distinct azimuthal clusters, while degradations manifest as broadened, diffused, or shifted distributions. Crucially, Binaspect operates blindly on audio, requiring no prior knowledge of head models. These visualizations enable researchers and engineers to observe how binaural cues are degraded by codec and renderer design choices, among other downstream processes. We demonstrate the tool on bitrate ladders, ambisonic rendering, and VBAP source positioning, where degradations are clearly revealed. In addition to their diagnostic value, the proposed representations can be exported as structured features suitable for training machine learning models in quality prediction, spatial audio classification, and other binaural tasks. Binaspect is released under an open-source license with full reproducibility scripts at https://github.com/QxLabIreland/Binaspect.</article>","contentLength":1305,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models","url":"https://arxiv.org/abs/2510.25713","date":1761796800,"author":"","guid":321481,"unread":true,"content":"<article>arXiv:2510.25713v1 Announce Type: new \nAbstract: We adapt a pre-trained Vision-Language-Action (VLA) model (Open-VLA) for dexterous human-robot collaboration with minimal language prompting. Our approach adds (i) FiLM conditioning to visual backbones for task-aware perception, (ii) an auxiliary intent head that predicts collaborator hand pose and target cues, and (iii) action-space post-processing that predicts compact deltas (position/rotation) and PCA-reduced finger joints before mapping to full commands. Using a multi-view, teleoperated Franka and Mimic-hand dataset augmented with MediaPipe hand poses, we demonstrate that delta actions are well-behaved and that four principal components explain ~96% of hand-joint variance. Ablations identify action post-processing as the primary performance driver; auxiliary intent helps, FiLM is mixed, and a directional motion loss is detrimental. A real-time stack (~0.3 s latency on one RTX 4090) composes \"pick-up\" and \"pass\" into a long-horizon behavior. We surface \"trainer overfitting\" to specific demonstrators as the key limitation.</article>","contentLength":1090,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MetaLore: Learning to Orchestrate Communication and Computation for Metaverse Synchronization","url":"https://arxiv.org/abs/2510.25705","date":1761796800,"author":"","guid":321482,"unread":true,"content":"<article>arXiv:2510.25705v1 Announce Type: new \nAbstract: As augmented and virtual reality evolve, achieving seamless synchronization between physical and digital realms remains a critical challenge, especially for real-time applications where delays affect the user experience. This paper presents MetaLore, a Deep Reinforcement Learning (DRL) based framework for joint communication and computational resource allocation in Metaverse or digital twin environments. MetaLore dynamically shares the communication bandwidth and computational resources among sensors and mobile devices to optimize synchronization, while offering high throughput performance. Special treatment is given in satisfying end-to-end delay guarantees. A key contribution is the introduction of two novel Age of Information (AoI) metrics: Age of Request Information (AoRI) and Age of Sensor Information (AoSI), integrated into the reward function to enhance synchronization quality. An open source simulator has been extended to incorporate and evaluate the approach. The DRL solution is shown to achieve the performance of full-enumeration brute-force solutions by making use of a small, task-oriented observation space of two queue lengths at the network side. This allows the DRL approach the flexibility to effectively and autonomously adapt to dynamic traffic conditions.</article>","contentLength":1340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?","url":"https://arxiv.org/abs/2510.25701","date":1761796800,"author":"","guid":321483,"unread":true,"content":"<article>arXiv:2510.25701v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly explored as flexible alternatives to classical machine learning models for classification tasks through zero-shot prompting. However, their suitability for structured tabular data remains underexplored, especially in high-stakes financial applications such as financial risk assessment. This study conducts a systematic comparison between zero-shot LLM-based classifiers and LightGBM, a state-of-the-art gradient-boosting model, on a real-world loan default prediction task. We evaluate their predictive performance, analyze feature attributions using SHAP, and assess the reliability of LLM-generated self-explanations. While LLMs are able to identify key financial risk indicators, their feature importance rankings diverge notably from LightGBM, and their self-explanations often fail to align with empirical SHAP attributions. These findings highlight the limitations of LLMs as standalone models for structured financial risk prediction and raise concerns about the trustworthiness of their self-generated explanations. Our results underscore the need for explainability audits, baseline comparisons with interpretable models, and human-in-the-loop oversight when deploying LLMs in risk-sensitive financial environments.</article>","contentLength":1320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3-Dimensional Adaptive Unstructured Tessellated Look-up Tables for the Approximation of Compton Form Factors","url":"https://arxiv.org/abs/2510.25699","date":1761796800,"author":"","guid":321484,"unread":true,"content":"<article>arXiv:2510.25699v1 Announce Type: new \nAbstract: We describe an iterative algorithm to construct an unstructured tessellation of simplices (irregular tetrahedra in 3-dimensions) to approximate an arbitrary function to a desired precision by interpolation. The method is applied to the generation of Compton Form Factors for simulation and analysis of nuclear femtography, as enabled by high energy exclusive processes such as electron-proton scattering producing just an electron, proton, and gamma-ray in the final state. While producing tessellations with only a 1% mean interpolation error, our results show that the use of such tessellations can significantly decrease the computation time for Monte Carlo event generation by $\\sim23$ times for $10^{7}$ events (and using extrapolation, by $\\sim955$ times for $10^{10}$ events).</article>","contentLength":832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related to Metal Casting","url":"https://arxiv.org/abs/2510.25697","date":1761796800,"author":"","guid":321485,"unread":true,"content":"<article>arXiv:2510.25697v1 Announce Type: new \nAbstract: This work reframes mold filling in metal casting as a simplified 2D operator learning surrogate to avoid costly transient CFD simulations. The method combines a graph based encoder that aggregates neighborhood information on an unstructured input mesh to encode geometry and boundary data, a Fourier spectral core that operates on a regular latent grid to capture global interactions, and a graph based decoder that maps latent fields back to a target mesh. The model jointly predicts velocities, pressure, and volume fraction over a fixed horizon and generalizes across varied ingate locations and process settings. On held out geometries and inlet conditions it reproduces large scale advection and the fluid air interface with errors concentrated near steep gradients. Mean relative L2 errors are about 5 percent across all fields. Inference is roughly 100 to 1000 times faster than conventional CFD simulations, thereby enabling rapid in-the-loop design exploration. Ablation studies show accuracy drops monotonically with stronger spatial subsampling of input vertices while temporal subsampling causes a gentler decline. Cutting the training data by 50 percent yields only small error growth. Overall the results demonstrate neural operators as efficient surrogates for 2D mold filling and related filling problems and enable fast exploration and optimization of gating system designs in casting workflows.</article>","contentLength":1461,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Convolutional Spiking-based GRU Cell for Spatio-temporal Data","url":"https://arxiv.org/abs/2510.25696","date":1761796800,"author":"","guid":321486,"unread":true,"content":"<article>arXiv:2510.25696v1 Announce Type: new \nAbstract: Spike-based temporal messaging enables SNNs to efficiently process both purely temporal and spatio-temporal time-series or event-driven data. Combining SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks, gives rise to a robust framework for sequential data processing; however, traditional RNNs often lose local details when handling long sequences. Previous approaches, such as SpikGRU, fail to capture fine-grained local dependencies in event-based spatio-temporal data. In this paper, we introduce the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional operations to preserve local structure and dependencies while integrating the temporal precision of spiking neurons with the efficient gating mechanisms of GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS, SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our experiments show that CS-GRU outperforms state-of-the-art GRU variants by an average of 4.35%, achieving over 90% accuracy on sequential tasks and up to 99.31% on MNIST. It is worth noting that our solution achieves 69% higher efficiency compared to SpikGRU. The code is available at: https://github.com/YesmineAbdennadher/CS-GRU.</article>","contentLength":1291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power Diodes with Engineered Schottky Contact and High-permittivity Dielectric Field Plate","url":"https://arxiv.org/abs/2510.25695","date":1761796800,"author":"","guid":321487,"unread":true,"content":"<article>arXiv:2510.25695v1 Announce Type: new \nAbstract: We report over 3 kV breakdown voltage and ultra-low leakage (011) \\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and high-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011) orientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift layers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power switches. The Schottky barrier engineering was performed with a composite Pt cap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse blocking capabilities enabled by PtOx while allowing low turn-on voltage by the interfacing thin Pt layer. We also performed a systematic study using a co-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same wafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the field-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage of 2.75 kV owing to the edge field management. Further enhancement of the breakdown voltage was achieved by tunneling leakage management using composite Pt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown voltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt (1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011) \\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management by composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage, edge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as the advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate a promising strategy for developing ultra-low leakage and multi-kV class vertical (011) \\b{eta}-Ga2O3 power devices.</article>","contentLength":1756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Process-Level Trajectory Evaluation for Environment Configuration in Software Engineering Agents","url":"https://arxiv.org/abs/2510.25694","date":1761796800,"author":"","guid":321488,"unread":true,"content":"<article>arXiv:2510.25694v1 Announce Type: new \nAbstract: Large language model-based agents show promise for software engineering, but environment configuration remains a bottleneck due to heavy manual effort and scarce large-scale, high-quality datasets. Existing benchmarks assess only end-to-end build/test success, obscuring where and why agents succeed or fail. We introduce the Environment Configuration Diagnosis Benchmark, Enconda-bench, which provides process-level trajectory assessment of fine-grained agent capabilities during environment setup-planning, perception-driven error diagnosis, feedback-driven repair, and action to execute final environment configuration. Our task instances are automatically constructed by injecting realistic README errors and are validated in Docker for scalable, high-quality evaluation. Enconda-bench combines process-level analysis with end-to-end executability to enable capability assessments beyond aggregate success rates. Evaluations across state-of-the-art LLMs and agent frameworks show that while agents can localize errors, they struggle to translate feedback into effective corrections, limiting end-to-end performance. To our knowledge, Enconda-bench is the first framework to provide process-level internal capability assessment for environment configuration, offering actionable insights for improving software engineering agents.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Configuration-First Framework for Reproducible, Low-Code Localization","url":"https://arxiv.org/abs/2510.25692","date":1761796800,"author":"","guid":321489,"unread":true,"content":"<article>arXiv:2510.25692v1 Announce Type: new \nAbstract: Machine learning is increasingly permeating radio-based localization services. To keep results credible and comparable, everyday workflows should make rigorous experiment specification and exact repeatability the default, without blocking advanced experimentation. However, in practice, researchers face a three-way gap that could be filled by a framework that offers (i) low coding effort for end-to-end studies, (ii) reproducibility by default including versioned code, data, and configurations, controlled randomness, isolated runs, and recorded artifacts, and (iii) built-in extensibility so new models, metrics, and stages can be added with minimal integration effort. Existing tools rarely deliver all three for machine learning in general and localization workflows in particular. In this paper we introduce LOCALIZE, a low-code, configuration-first framework for radio localization in which experiments are declared in human-readable configuration, a workflow orchestrator runs standardized pipelines from data preparation to reporting, and all artifacts, such as datasets, models, metrics, and reports, are versioned. The preconfigured, versioned datasets reduce initial setup and boilerplate, speeding up model development and evaluation. The design, with clear extension points, allows experts to add components without reworking the infrastructure. In a qualitative comparison and a head-to-head study against a plain Jupyter notebook baseline, we show that the framework reduces authoring effort while maintaining comparable runtime and memory behavior. Furthermore, using a Bluetooth Low Energy dataset, we show that scaling across training data (1x to 10x) keeps orchestration overheads bounded as data grows. Overall, the framework makes reproducible machine-learning-based localization experimentation practical, accessible, and extensible.</article>","contentLength":1906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model Inversion Attacks Meet Cryptographic Fuzzy Extractors","url":"https://arxiv.org/abs/2510.25687","date":1761796800,"author":"","guid":321490,"unread":true,"content":"<article>arXiv:2510.25687v1 Announce Type: new \nAbstract: Model inversion attacks pose an open challenge to privacy-sensitive applications that use machine learning (ML) models. For example, face authentication systems use modern ML models to compute embedding vectors from face images of the enrolled users and store them. If leaked, inversion attacks can accurately reconstruct user faces from the leaked vectors. There is no systematic characterization of properties needed in an ideal defense against model inversion, even for the canonical example application of a face authentication system susceptible to data breaches, despite a decade of best-effort solutions.\n  In this paper, we formalize the desired properties of a provably strong defense against model inversion and connect it, for the first time, to the cryptographic concept of fuzzy extractors. We further show that existing fuzzy extractors are insecure for use in ML-based face authentication. We do so through a new model inversion attack called PIPE, which achieves a success rate of over 89% in most cases against prior schemes. We then propose L2FE-Hash, the first candidate fuzzy extractor which supports standard Euclidean distance comparators as needed in many ML-based applications, including face authentication. We formally characterize its computational security guarantees, even in the extreme threat model of full breach of stored secrets, and empirically show its usable accuracy in face authentication for practical face distributions. It offers attack-agnostic security without requiring any re-training of the ML model it protects. Empirically, it nullifies both prior state-of-the-art inversion attacks as well as our new PIPE attack.</article>","contentLength":1712,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One Join Order Does Not Fit All: Reducing Intermediate Results with Per-Split Query Plans","url":"https://arxiv.org/abs/2510.25684","date":1761796800,"author":"","guid":321491,"unread":true,"content":"<article>arXiv:2510.25684v1 Announce Type: new \nAbstract: Minimizing intermediate results is critical for efficient multi-join query processing. Although the seminal Yannakakis algorithm offers strong guarantees for acyclic queries, cyclic queries remain an open challenge. In this paper, we propose SplitJoin, a framework that introduces split as a first-class query operator. By partitioning input tables into heavy and light parts, SplitJoin allows different data partitions to use distinct query plans, with the goal of reducing intermediate sizes using existing binary join engines. We systematically explore the design space for split-based optimizations, including threshold selection, split strategies, and join ordering after splits. Implemented as a front-end to DuckDB and Umbra, SplitJoin achieves substantial improvements: on DuckDB, SplitJoin completes 43 social network queries (vs. 29 natively), achieving 2.1x faster runtime and 7.9x smaller intermediates on average (up to 13.6x and 74x, respectively); on Umbra, it completes 45 queries (vs. 35), achieving 1.3x speedups and 1.2x smaller intermediates on average (up to 6.1x and 2.1x, respectively).</article>","contentLength":1158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics","url":"https://arxiv.org/abs/2510.25683","date":1761796800,"author":"","guid":321492,"unread":true,"content":"<article>arXiv:2510.25683v1 Announce Type: new \nAbstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models for numerical simulations. While their applications in computational fluid dynamics have been investigated, little attention has been given to structural problems, especially for dynamic cases. To address this gap, we introduce the Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate modeling of dynamic structural problems.\n  GNSS follows the encode-process-decode paradigm typical of GNN-based machine learning models, and its design makes it particularly suited for dynamic simulations thanks to three key features: (i) expressing node kinematics in node-fixed local frames, which avoids catastrophic cancellation in finite-difference velocities; (ii) employing a sign-aware regression loss, which reduces phase errors in long rollouts; and (iii) using a wavelength-informed connectivity radius, which optimizes graph construction.\n  We evaluate GNSS on a case study involving a beam excited by a 50kHz Hanning-modulated pulse. The results show that GNSS accurately reproduces the physics of the problem over hundreds of timesteps and generalizes to unseen loading conditions, where existing GNNs fail to converge or deliver meaningful predictions.\n  Compared with explicit finite element baselines, GNSS achieves substantial inference speedups while preserving spatial and temporal fidelity. These findings demonstrate that locality-preserving GNNs with physics-consistent update rules are a competitive alternative for dynamic, wave-dominated structural simulations.</article>","contentLength":1624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PairUni: Pairwise Training for Unified Multimodal Language Models","url":"https://arxiv.org/abs/2510.25682","date":1761796800,"author":"","guid":321493,"unread":true,"content":"<article>arXiv:2510.25682v1 Announce Type: new \nAbstract: Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Code: \\href{https://github.com/Haochen-Wang409/PairUni}{github.com/Haochen-Wang409/PairUni}</article>","contentLength":1504,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigation in a Three-Dimensional Urban Flow using Deep Reinforcement Learning","url":"https://arxiv.org/abs/2510.25679","date":1761796800,"author":"","guid":321494,"unread":true,"content":"<article>arXiv:2510.25679v1 Announce Type: new \nAbstract: Unmanned Aerial Vehicles (UAVs) are increasingly populating urban areas for delivery and surveillance purposes. In this work, we develop an optimal navigation strategy based on Deep Reinforcement Learning. The environment is represented by a three-dimensional high-fidelity simulation of an urban flow, characterized by turbulence and recirculation zones. The algorithm presented here is a flow-aware Proximal Policy Optimization (PPO) combined with a Gated Transformer eXtra Large (GTrXL) architecture, giving the agent richer information about the turbulent flow field in which it navigates. The results are compared with a PPO+GTrXL without the secondary prediction tasks, a PPO combined with Long Short Term Memory (LSTM) cells and a traditional navigation algorithm. The obtained results show a significant increase in the success rate (SR) and a lower crash rate (CR) compared to a PPO+LSTM, PPO+GTrXL and the classical Zermelo's navigation algorithm, paving the way to a completely reimagined UAV landscape in complex urban environments.</article>","contentLength":1093,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ZK-SenseLM: Verifiable Large-Model Wireless Sensing with Selective Abstention and Zero-Knowledge Attestation","url":"https://arxiv.org/abs/2510.25677","date":1761796800,"author":"","guid":321495,"unread":true,"content":"<article>arXiv:2510.25677v1 Announce Type: new \nAbstract: ZK-SenseLM is a secure and auditable wireless sensing framework that pairs a large-model encoder for Wi-Fi channel state information (and optionally mmWave radar or RFID) with a policy-grounded decision layer and end-to-end zero-knowledge proofs of inference. The encoder uses masked spectral pretraining with phase-consistency regularization, plus a light cross-modal alignment that ties RF features to compact, human-interpretable policy tokens. To reduce unsafe actions under distribution shift, we add a calibrated selective-abstention head; the chosen risk-coverage operating point is registered and bound into the proof. We implement a four-stage proving pipeline: (C1) feature sanity and commitment, (C2) threshold and version binding, (C3) time-window binding, and (C4) PLONK-style proofs that the quantized network, given the committed window, produced the logged action and confidence. Micro-batched proving amortizes cost across adjacent windows, and a gateway option offloads proofs from low-power devices. The system integrates with differentially private federated learning and on-device personalization without weakening verifiability: model hashes and the registered threshold are part of each public statement. Across activity, presence or intrusion, respiratory proxy, and RF fingerprinting tasks, ZK-SenseLM improves macro-F1 and calibration, yields favorable coverage-risk curves under perturbations, and rejects tamper and replay with compact proofs and fast verification.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modulation Schemes for Functionalized Vesicle-based MC Transmitters","url":"https://arxiv.org/abs/2510.25676","date":1761796800,"author":"","guid":321496,"unread":true,"content":"<article>arXiv:2510.25676v1 Announce Type: new \nAbstract: Molecular communication (MC) enables information exchange through the transmission of signaling molecules (SMs) and holds promise for many innovative applications. However, most existing MC studies rely on simplified transmitter (TX) models that do not account for the physical and biochemical limitations of realistic biological hardware. This work extends previous efforts toward developing models for practical MC systems by proposing a more realistic TX model that incorporates the delay in SM release and TX noise introduced by biological components. Building on this more realistic, functionalized vesicle-based TX model, we propose two novel modulation schemes specifically designed for this TX to mitigate TX-induced memory effects that arise from delayed and imperfectly controllable SM release. The proposed modulation schemes enable low-complexity receiver designs by mitigating memory effects directly at the TX. Numerical evaluations demonstrate that the proposed schemes improve communication reliability under realistic biochemical constraints, offering an important step toward physically realizable MC systems.</article>","contentLength":1176,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mechanistic Interpretability of RNNs emulating Hidden Markov Models","url":"https://arxiv.org/abs/2510.25674","date":1761796800,"author":"","guid":321497,"unread":true,"content":"<article>arXiv:2510.25674v1 Announce Type: new \nAbstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience to infer latent dynamics in neural populations and to generate hypotheses about the neural computations underlying behavior. However, past work has focused on relatively simple, input-driven, and largely deterministic behaviors - little is known about the mechanisms that would allow RNNs to generate the richer, spontaneous, and potentially stochastic behaviors observed in natural settings. Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of natural behaviors into discrete latent states with stochastic transitions between them, a type of dynamics that may appear at odds with the continuous state spaces implemented by RNNs. Here we first show that RNNs can replicate HMM emission statistics and then reverse-engineer the trained networks to uncover the mechanisms they implement. In the absence of inputs, the activity of trained RNNs collapses towards a single fixed point. When driven by stochastic input, trajectories instead exhibit noise-sustained dynamics along closed orbits. Rotation along these orbits modulates the emission probabilities and is governed by transitions between regions of slow, noise-driven dynamics connected by fast, deterministic transitions. The trained RNNs develop highly structured connectivity, with a small set of \"kick neurons\" initiating transitions between these regions. This mechanism emerges during training as the network shifts into a regime of stochastic resonance, enabling it to perform probabilistic computations. Analyses across multiple HMM architectures - fully connected, cyclic, and linear-chain - reveal that this solution generalizes through the modular reuse of the same dynamical motif, suggesting a compositional principle by which RNNs can emulate complex discrete latent dynamics.</article>","contentLength":1892,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An OPF-based Control Framework for Hybrid AC-MTDC Power Systems under Uncertainty","url":"https://arxiv.org/abs/2510.25671","date":1761796800,"author":"","guid":321498,"unread":true,"content":"<article>arXiv:2510.25671v1 Announce Type: new \nAbstract: The increasing integration of renewable energy, particularly offshore wind, introduces significant uncertainty into hybrid AC-HVDC systems due to forecast errors and power fluctuations. Conventional control strategies typically rely on fixed setpoints and neglect frequency deviations, which can compromise system stability under rapid renewable variations. To address this challenge, this paper presents a forecast-integrated, optimal power flow (OPF)-based adaptive control framework. Wind speed forecasts generated using a Random Forest model are incorporated into a time-coupled OPF to determine baseline converter setpoints in anticipation of wind fluctuations, which are further adjusted in real time based on actual operating conditions. An adaptive droop control scheme is developed that jointly considers DC voltage and AC frequency deviations. The effectiveness of the proposed control framework is validated through hardware-in-the-loop (HIL) simulations, demonstrating its capability to ensure stable and robust operation of hybrid AC-HVDC systems under high penetration of renewable energy.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy","url":"https://arxiv.org/abs/2510.25670","date":1761796800,"author":"","guid":321499,"unread":true,"content":"<article>arXiv:2510.25670v1 Announce Type: new \nAbstract: A central challenge in machine learning is to understand how noise or measurement errors affect low-rank approximations, particularly in the spectral norm. This question is especially important in differentially private low-rank approximation, where one aims to preserve the top-$p$ structure of a data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius norm error or changes in reconstruction quality, but these metrics can over- or under-estimate true subspace distortion. The spectral norm, by contrast, captures worst-case directional error and provides the strongest utility guarantees. We establish new high-probability spectral-norm perturbation bounds for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem and explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n \\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and norm conditions, our bounds yield sharp estimates for $\\|(A + E)_p - A_p\\|$, where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up to a factor of $\\sqrt{n}$. As an application, we derive improved utility guarantees for differentially private PCA, resolving an open problem in the literature. Our analysis relies on a novel contour bootstrapping method from complex analysis and extends it to a broad class of spectral functionals, including polynomials and matrix exponentials. Empirical results on real-world datasets confirm that our bounds closely track the actual spectral error under diverse perturbation regimes.</article>","contentLength":1609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents","url":"https://arxiv.org/abs/2510.25668","date":1761796800,"author":"","guid":321500,"unread":true,"content":"<article>arXiv:2510.25668v1 Announce Type: new \nAbstract: Vision-language models (VLMs) excel at interpreting text-rich images but struggle with long, visually complex documents that demand analysis and integration of information spread across multiple pages. Existing approaches typically rely on fixed reasoning templates or rigid pipelines, which force VLMs into a passive role and hinder both efficiency and generalization. We present Active Long-DocumEnt Navigation (ALDEN), a multi-turn reinforcement learning framework that fine-tunes VLMs as interactive agents capable of actively navigating long, visually rich documents. ALDEN introduces a novel fetch action that directly accesses the page by index, complementing the classic search action and better exploiting document structure. For dense process supervision and efficient training, we propose a rule-based cross-level reward that provides both turn- and token-level signals. To address the empirically observed training instability caused by numerous visual tokens from long documents, we further propose a visual-semantic anchoring mechanism that applies a dual-path KL-divergence constraint to stabilize visual and textual representations separately during training. Trained on a corpus constructed from three open-source datasets, ALDEN achieves state-of-the-art performance on five long-document benchmarks. Overall, ALDEN marks a step beyond passive document reading toward agents that autonomously navigate and reason across long, visually rich documents, offering a robust path to more accurate and efficient long-document understanding.</article>","contentLength":1600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fuzz Smarter, Not Harder: Towards Greener Fuzzing with GreenAFL","url":"https://arxiv.org/abs/2510.25665","date":1761796800,"author":"","guid":321501,"unread":true,"content":"<article>arXiv:2510.25665v1 Announce Type: new \nAbstract: Fuzzing has become a key search-based technique for software testing, but continuous fuzzing campaigns consume substantial computational resources and generate significant carbon footprints. Existing grey-box fuzzing approaches like AFL++ focus primarily on coverage maximisation, without considering the energy costs of exploring different execution paths. This paper presents GreenAFL, an energy-aware framework that incorporates power consumption into the fuzzing heuristics to reduce the environmental impact of automated testing whilst maintaining coverage. GreenAFL introduces two key modifications to traditional fuzzing workflows: energy-aware corpus minimisation considering power consumption when reducing initial corpora, and energy-guided heuristics that direct mutation towards high-coverage, low-energy inputs. We conduct an ablation study comparing vanilla AFL++, energy-based corpus minimisation, and energy-based heuristics to evaluate the individual contributions of each component. Results show that highest coverage, and lowest energy usage is achieved whenever at least one of our modifications is used.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\{s,t\\}$-Separating Principal Partition Sequence of Submodular Functions","url":"https://arxiv.org/abs/2510.25664","date":1761796800,"author":"","guid":321502,"unread":true,"content":"<article>arXiv:2510.25664v1 Announce Type: new \nAbstract: Narayanan and Fujishige showed the existence of the principal partition sequence of a submodular function, a structure with numerous applications in areas such as clustering, fast algorithms, and approximation algorithms. In this work, motivated by two applications, we develop a theory of $\\{s,t\\}$-separating principal partition sequence of a submodular function. We define this sequence, show its existence, and design a polynomial-time algorithm to construct it. We show two applications: (1) approximation algorithm for the $\\{s,t\\}$-separating submodular $k$-partitioning problem for monotone and posimodular functions and (2) polynomial-time algorithm for the hypergraph orientation problem of finding an orientation that simultaneously has strong connectivity at least $k$ and $(s,t)$-connectivity at least $\\ell$.</article>","contentLength":871,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"User Misconceptions of LLM-Based Conversational Programming Assistants","url":"https://arxiv.org/abs/2510.25662","date":1761796800,"author":"","guid":321503,"unread":true,"content":"<article>arXiv:2510.25662v1 Announce Type: new \nAbstract: Programming assistants powered by large language models (LLMs) have become widely available, with conversational assistants like ChatGPT proving particularly accessible to less experienced programmers. However, the varied capabilities of these tools across model versions and the mixed availability of extensions that enable web search, code execution, or retrieval-augmented generation create opportunities for user misconceptions about what systems can and cannot do. Such misconceptions may lead to over-reliance, unproductive practices, or insufficient quality control in LLM-assisted programming. Here, we aim to characterize misconceptions that users of conversational LLM-based assistants may have in programming contexts. Using a two-phase approach, we first brainstorm and catalog user misconceptions that may occur, and then conduct a qualitative analysis to examine whether these conceptual issues surface in naturalistic Python-programming conversations with an LLM-based chatbot drawn from an openly available dataset. Indeed, we see evidence that some users have misplaced expectations about the availability of LLM-based chatbot features like web access, code execution, or non-text output generation. We also see potential evidence for deeper conceptual issues around the scope of information required to debug, validate, and optimize programs. Our findings reinforce the need for designing LLM-based tools that more clearly communicate their programming capabilities to users.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"mitransient: Transient light transport in Mitsuba 3","url":"https://arxiv.org/abs/2510.25660","date":1761796800,"author":"","guid":321504,"unread":true,"content":"<article>arXiv:2510.25660v1 Announce Type: new \nAbstract: mitransient is a light transport simulation tool that extends Mitsuba 3 with support for time-resolved simulations. In essence, mitransient extends conventional rendering by adding a temporal dimension which accounts for the time of flight of light. This allows rapid prototyping of novel transient imaging systems without the need of costly or difficult-to-operate hardware. Our code is trivially easy to install through pip, and consists of Python modules that can run both in CPU and GPU by leveraging the JIT capabilities of Mitsuba 3. It provides physically-based simulations of complex phenomena, including a wide variety of realistic materials and participating media such as fog or smoke. In addition, we extend Mitsuba 3's functionality to support time-resolved polarization tracking of light and transient differentiable rendering. Finally, we also include tools that simplify the use of our simulations for non-line-of-sight imaging, enabling realistic scene setups with capture noise to be simulated in just seconds of minutes. Altogether, we hope that mitransient will support the research community in developing novel algorithms for transient imaging.</article>","contentLength":1215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Subgraph Federated Learning via Spectral Methods","url":"https://arxiv.org/abs/2510.25657","date":1761796800,"author":"","guid":321505,"unread":true,"content":"<article>arXiv:2510.25657v1 Announce Type: new \nAbstract: We consider the problem of federated learning (FL) with graph-structured data distributed across multiple clients. In particular, we address the prevalent scenario of interconnected subgraphs, where interconnections between clients significantly influence the learning process. Existing approaches suffer from critical limitations, either requiring the exchange of sensitive node embeddings, thereby posing privacy risks, or relying on computationally-intensive steps, which hinders scalability. To tackle these challenges, we propose FedLap, a novel framework that leverages global structure information via Laplacian smoothing in the spectral domain to effectively capture inter-node dependencies while ensuring privacy and scalability. We provide a formal analysis of the privacy of FedLap, demonstrating that it preserves privacy. Notably, FedLap is the first subgraph FL scheme with strong privacy guarantees. Extensive experiments on benchmark datasets demonstrate that FedLap achieves competitive or superior utility compared to existing techniques.</article>","contentLength":1105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ggtime: A Grammar of Temporal Graphics","url":"https://arxiv.org/abs/2510.25656","date":1761796800,"author":"","guid":321506,"unread":true,"content":"<article>arXiv:2510.25656v1 Announce Type: new \nAbstract: Visualizing changes over time is fundamental to learning from the past and anticipating the future. However, temporal semantics can be complicated, and existing visualization tools often struggle to accurately represent these complexities. It is common to use bespoke plot helper functions designed to produce specific graphics, due to the absence of flexible general tools that respect temporal semantics. We address this problem by proposing a grammar of temporal graphics, and an associated software implementation, 'ggtime', that encodes temporal semantics into a declarative grammar for visualizing temporal data. The grammar introduces new composable elements that support visualization across linear, cyclical, quasi-cyclical, and other granularities; standardization of irregular durations; and alignment of time points across different granularities and time zones. It is designed for interoperability with other semantic variables, allowing navigation across the space of visualizations while preserving temporal semantics.</article>","contentLength":1082,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collision avoidance and path finding in a robotic mobile fulfillment system using multi-objective meta-heuristics","url":"https://arxiv.org/abs/2510.25650","date":1761796800,"author":"","guid":321507,"unread":true,"content":"<article>arXiv:2510.25650v1 Announce Type: new \nAbstract: Multi-Agent Path Finding (MAPF) has gained significant attention, with most research focusing on minimizing collisions and travel time. This paper also considers energy consumption in the path planning of automated guided vehicles (AGVs). It addresses two main challenges: i) resolving collisions between AGVs and ii) assigning tasks to AGVs. We propose a new collision avoidance strategy that takes both energy use and travel time into account. For task assignment, we present two multi-objective algorithms: Non-Dominated Sorting Genetic Algorithm (NSGA) and Adaptive Large Neighborhood Search (ALNS). Comparative evaluations show that these proposed methods perform better than existing approaches in both collision avoidance and task assignment.</article>","contentLength":798,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Plan & Schedule with Reinforcement-Learned Bimanual Robot Skills","url":"https://arxiv.org/abs/2510.25634","date":1761796800,"author":"","guid":321508,"unread":true,"content":"<article>arXiv:2510.25634v1 Announce Type: new \nAbstract: Long-horizon contact-rich bimanual manipulation presents a significant challenge, requiring complex coordination involving a mixture of parallel execution and sequential collaboration between arms. In this paper, we introduce a hierarchical framework that frames this challenge as an integrated skill planning &amp; scheduling problem, going beyond purely sequential decision-making to support simultaneous skill invocation. Our approach is built upon a library of single-arm and bimanual primitive skills, each trained using Reinforcement Learning (RL) in GPU-accelerated simulation. We then train a Transformer-based planner on a dataset of skill compositions to act as a high-level scheduler, simultaneously predicting the discrete schedule of skills as well as their continuous parameters. We demonstrate that our method achieves higher success rates on complex, contact-rich tasks than end-to-end RL approaches and produces more efficient, coordinated behaviors than traditional sequential-only planners.</article>","contentLength":1054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EHR-R1: A Reasoning-Enhanced Foundational Language Model for Electronic Health Record Analysis","url":"https://arxiv.org/abs/2510.25628","date":1761796800,"author":"","guid":321509,"unread":true,"content":"<article>arXiv:2510.25628v1 Announce Type: new \nAbstract: Electronic Health Records (EHRs) contain rich yet complex information, and their automated analysis is critical for clinical decision-making. Despite recent advances of large language models (LLMs) in clinical workflows, their ability to analyze EHRs remains limited due to narrow task coverage and lack of EHR-oriented reasoning capabilities. This paper aims to bridge the gap, specifically, we present EHR-Ins, a large-scale, comprehensive EHR reasoning instruction dataset, comprising 300k high-quality reasoning cases and 4M non-reasoning cases across 42 distinct EHR tasks. Its core innovation is a thinking-graph-driven framework that enables to generate high-quality reasoning data at scale. Based on it, we develop EHR-R1, a series of reasoning-enhanced LLMs with up to 72B parameters tailored for EHR analysis. Through a multi-stage training paradigm, including domain adaptation, reasoning enhancement, and reinforcement learning, EHR-R1 systematically acquires domain knowledge and diverse reasoning capabilities, enabling accurate and robust EHR analysis. Lastly, we introduce EHR-Bench, a new benchmark curated from MIMIC-IV, spanning 42 tasks, to comprehensively assess reasoning and prediction across EHR scenarios. In experiments, we show that the resulting EHR-R1 consistently outperforms state-of-the-art commercial and open-source LLMs (including DeepSeek-V3 and GPT-4o), surpassing GPT-4o by over 30 points on MIMIC-Bench and achieving a 10\\% higher zero-shot AUROC on EHRSHOT. Collectively, EHR-Ins, EHR-R1, and EHR-Bench have significantly advanced the development for more reliable and clinically relevant EHR analysis.</article>","contentLength":1691,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Language Models Efficient Reasoners? A Perspective from Logic Programming","url":"https://arxiv.org/abs/2510.25626","date":1761796800,"author":"","guid":321510,"unread":true,"content":"<article>arXiv:2510.25626v1 Announce Type: new \nAbstract: Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of human-like reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language -- as generated by an LM -- with shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions -- even with minimal, domain-consistent distractions -- and the proofs they generate frequently exhibit detours through irrelevant inferences.</article>","contentLength":1137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks","url":"https://arxiv.org/abs/2510.25623","date":1761796800,"author":"","guid":321511,"unread":true,"content":"<article>arXiv:2510.25623v1 Announce Type: new \nAbstract: Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming \\citep{snell2024scaling, chen2024more}, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles.</article>","contentLength":917,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation","url":"https://arxiv.org/abs/2510.25622","date":1761796800,"author":"","guid":321512,"unread":true,"content":"<article>arXiv:2510.25622v1 Announce Type: new \nAbstract: Industrial recommender systems rely on unique Item Identifiers (ItemIDs). However, this method struggles with scalability and generalization in large, dynamic datasets that have sparse long-tail data.Content-based Semantic IDs (SIDs) address this by sharing knowledge through content quantization. However, by ignoring dynamic behavioral properties, purely content-based SIDs have limited expressive power. Existing methods attempt to incorporate behavioral information but overlook a critical distinction: unlike relatively uniform content features, user-item interactions are highly skewed and diverse, creating a vast information gap in quality and quantity between popular and long-tail items. This oversight leads to two critical limitations: (1) Noise Corruption: Indiscriminate behavior-content alignment allows collaborative noise from long-tail items to corrupt their content representations, leading to the loss of critical multimodal information. (2)Signal Obscurity: The equal-weighting scheme for SIDs fails to reflect the varying importance of different behavioral signals, making it difficult for downstream tasks to distinguish important SIDs from uninformative ones. To tackle these issues, we propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align, Denoise, and Amplify multimodal information from content and behavior modalities for semantic IDs learning. The semantic IDs generated by this framework named ADA-SID. It introduces two innovations: an adaptive behavior-content alignment that is aware of information richness to shield representations from noise, and a dynamic behavioral router to amplify critical signals by applying different weights to SIDs. Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.</article>","contentLength":1920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FARSIQA: Faithful and Advanced RAG System for Islamic Question Answering","url":"https://arxiv.org/abs/2510.25621","date":1761796800,"author":"","guid":321513,"unread":true,"content":"<article>arXiv:2510.25621v1 Announce Type: new \nAbstract: The advent of Large Language Models (LLMs) has revolutionized Natural Language Processing, yet their application in high-stakes, specialized domains like religious question answering is hindered by challenges like hallucination and unfaithfulness to authoritative sources. This issue is particularly critical for the Persian-speaking Muslim community, where accuracy and trustworthiness are paramount. Existing Retrieval-Augmented Generation (RAG) systems, relying on simplistic single-pass pipelines, fall short on complex, multi-hop queries requiring multi-step reasoning and evidence aggregation. To address this gap, we introduce FARSIQA, a novel, end-to-end system for Faithful Advanced Question Answering in the Persian Islamic domain. FARSIQA is built upon our innovative FAIR-RAG architecture: a Faithful, Adaptive, Iterative Refinement framework for RAG. FAIR-RAG employs a dynamic, self-correcting process: it adaptively decomposes complex queries, assesses evidence sufficiency, and enters an iterative loop to generate sub-queries, progressively filling information gaps. Operating on a curated knowledge base of over one million authoritative Islamic documents, FARSIQA demonstrates superior performance. Rigorous evaluation on the challenging IslamicPCQA benchmark shows state-of-the-art performance: the system achieves a remarkable 97.0% in Negative Rejection - a 40-point improvement over baselines - and a high Answer Correctness score of 74.3%. Our work establishes a new standard for Persian Islamic QA and validates that our iterative, adaptive architecture is crucial for building faithful, reliable AI systems in sensitive domains.</article>","contentLength":1703,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization","url":"https://arxiv.org/abs/2510.25616","date":1761796800,"author":"","guid":321514,"unread":true,"content":"<article>arXiv:2510.25616v1 Announce Type: new \nAbstract: The growing success of Vision-Language-Action (VLA) models stems from the promise that pretrained Vision-Language Models (VLMs) can endow agents with transferable world knowledge and vision-language (VL) grounding, laying a foundation for action models with broader generalization. Yet when these VLMs are adapted to the action modality, it remains unclear to what extent their original VL representations and knowledge are preserved. In this work, we conduct a systematic study of representation retention during VLA fine-tuning, showing that naive action fine-tuning leads to degradation of visual representations. To characterize and measure these effects, we probe VLA's hidden representations and analyze attention maps, further, we design a set of targeted tasks and methods that contrast VLA models with their counterpart VLMs, isolating changes in VL capabilities induced by action fine-tuning. We further evaluate a range of strategies for aligning visual representations and introduce a simple yet effective method that mitigates degradation and yields improved generalization to out-of-distribution (OOD) scenarios. Taken together, our analysis clarifies the trade-off between action fine-tuning and the degradation of VL representations and highlights practical approaches to recover inherited VL capabilities. Code is publicly available: https://blind-vla-paper.github.io</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Districting Becomes NP-hard","url":"https://arxiv.org/abs/2510.25614","date":1761796800,"author":"","guid":321515,"unread":true,"content":"<article>arXiv:2510.25614v1 Announce Type: new \nAbstract: This paper investigates why and when the edge-based districting problem becomes computationally intractable. The overall problem is represented as an exact mathematical programming formulation consisting of an objective function and several constraint groups, each enforcing a well-known districting criterion such as balance, contiguity, or compactness. While districting is known to be NP-hard in general, we study what happens when specific constraint groups are relaxed or removed. The results identify precise boundaries between tractable subproblems (in P) and intractable ones (NP-hard). The paper also discusses implications on node-based analogs of the featured districting problems, and it considers alternative notions of certain criteria in its analysis.</article>","contentLength":815,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Counterfactual-based Agent Influence Ranker for Agentic AI Workflows","url":"https://arxiv.org/abs/2510.25612","date":1761796800,"author":"","guid":321516,"unread":true,"content":"<article>arXiv:2510.25612v1 Announce Type: new \nAbstract: An Agentic AI Workflow (AAW), also known as an LLM-based multi-agent system, is an autonomous system that assembles several LLM-based agents to work collaboratively towards a shared goal. The high autonomy, widespread adoption, and growing interest in such AAWs highlight the need for a deeper understanding of their operations, from both quality and security aspects. To this day, there are no existing methods to assess the influence of each agent on the AAW's final output. Adopting techniques from related fields is not feasible since existing methods perform only static structural analysis, which is unsuitable for inference time execution. We present Counterfactual-based Agent Influence Ranker (CAIR) - the first method for assessing the influence level of each agent on the AAW's output and determining which agents are the most influential. By performing counterfactual analysis, CAIR provides a task-agnostic analysis that can be used both offline and at inference time. We evaluate CAIR using an AAWs dataset of our creation, containing 30 different use cases with 230 different functionalities. Our evaluation showed that CAIR produces consistent rankings, outperforms baseline methods, and can easily enhance the effectiveness and relevancy of downstream tasks.</article>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training","url":"https://arxiv.org/abs/2510.25609","date":1761796800,"author":"","guid":321517,"unread":true,"content":"<article>arXiv:2510.25609v1 Announce Type: new \nAbstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a different metric distance than the Earth Mover (Wasserstein) distance and achieves better training stability. Empirical evaluations on four standard image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60% lower Frechet Inception Distance (FID). Our results suggest that BOLT is a broadly applicable principle for enhancing GAN training.</article>","contentLength":709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats","url":"https://arxiv.org/abs/2510.25602","date":1761796800,"author":"","guid":321518,"unread":true,"content":"<article>arXiv:2510.25602v1 Announce Type: new \nAbstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly embracing low-precision floating-point (FP) formats to handle the pervasive activation outliers in Large Language Models (LLMs). Despite this industry trend, a unified comparison of FP and integer (INT) quantization across varying granularities has been missing, leaving algorithm and hardware co-design without clear guidance. This paper fills that gap by systematically investigating the trade-offs between FP and INT formats. We reveal a critical performance crossover: while FP excels in coarse-grained quantization, the comparison at fine-grained (block-wise) levels is more nuanced. Our comprehensive comparison demonstrates that for popular 8-bit fine-grained formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart in both algorithmic accuracy and hardware efficiency. However, for 4-bit formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like Hadamard rotation are applied. We also introduce a symmetric clipping method that resolves gradient bias in fine-grained low-bit INT training, enabling nearly lossless performance for MXINT8 training. These findings challenge the current hardware trajectory, demonstrating that a one-size-fits-all FP approach is suboptimal and advocating that fine-grained INT formats, particularly MXINT8, offer a better balance of accuracy, power, and efficiency for future AI accelerators.</article>","contentLength":1576,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models","url":"https://arxiv.org/abs/2510.25600","date":1761796800,"author":"","guid":321519,"unread":true,"content":"<article>arXiv:2510.25600v1 Announce Type: new \nAbstract: Vision-Language Large Models (VLLMs) face significant efficiency challenges when processing high-resolution inputs. The quadratic complexity in attention and autoregressive generation, as well as the constantly growing key value (KV) cache size, severely hinder the prefilling and decoding stages. Recent efforts have attempted to compress KV cache by identifying and pruning KV cache of less important tokens, but these methods typically rely on attention scores to estimate token importance, making them incompatible with efficient attention mechanisms such as FlashAttention and Sparse Attention, which do not explicitly compute attention matrices. Moreover, existing methods overlook how sparse attention, while accelerating the prefilling stage, alters the information structure of the KV cache, thereby compromising the effectiveness of downstream KV cache compression strategies. To address this issue, we propose PureKV, a plug-and-play framework for joint optimization of sparse attention and KV cache compression. We first introduce a KV cache compression strategy that is fully compatible with efficient attention accelerators. Our method utilizes lower layer attention scores to estimate the importance of high layers' KV cache, enabling active pruning without compromising accuracy. In addition, we have designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically tailored for video KV cache compression algorithms. This module combines spatial and temporal attention sparsity to improve the compression efficiency of KV cache optimization algorithms by purifying spatial noise and temporal redundancy in KV cache. At the same time, ST-SpAttn also accelerated the prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2, Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and 3.16 times prefill acceleration, with negligible quality degradation.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty Quantification for Regression: A Unified Framework based on kernel scores","url":"https://arxiv.org/abs/2510.25599","date":1761796800,"author":"","guid":321520,"unread":true,"content":"<article>arXiv:2510.25599v1 Announce Type: new \nAbstract: Regression tasks, notably in safety-critical domains, require proper uncertainty quantification, yet the literature remains largely classification-focused. In this light, we introduce a family of measures for total, aleatoric, and epistemic uncertainty based on proper scoring rules, with a particular emphasis on kernel scores. The framework unifies several well-known measures and provides a principled recipe for designing new ones whose behavior, such as tail sensitivity, robustness, and out-of-distribution responsiveness, is governed by the choice of kernel. We prove explicit correspondences between kernel-score characteristics and downstream behavior, yielding concrete design guidelines for task-specific measures. Extensive experiments demonstrate that these measures are effective in downstream tasks and reveal clear trade-offs among instantiations, including robustness and out-of-distribution detection performance.</article>","contentLength":980,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incorporating Social Awareness into Control of Unknown Multi-Agent Systems: A Real-Time Spatiotemporal Tubes Approach","url":"https://arxiv.org/abs/2510.25597","date":1761796800,"author":"","guid":321521,"unread":true,"content":"<article>arXiv:2510.25597v1 Announce Type: new \nAbstract: This paper presents a decentralized control framework that incorporates social awareness into multi-agent systems with unknown dynamics to achieve prescribed-time reach-avoid-stay tasks in dynamic environments. Each agent is assigned a social awareness index that quantifies its level of cooperation or self-interest, allowing heterogeneous social behaviors within the system. Building on the spatiotemporal tube (STT) framework, we propose a real-time STT framework that synthesizes tubes online for each agent while capturing its social interactions with others. A closed-form, approximation-free control law is derived to ensure that each agent remains within its evolving STT, thereby avoiding dynamic obstacles while also preventing inter-agent collisions in a socially aware manner, and reaching the target within a prescribed time. The proposed approach provides formal guarantees on safety and timing, and is computationally lightweight, model-free, and robust to unknown disturbances. The effectiveness and scalability of the framework are validated through simulation and hardware experiments on a 2D omnidirectional</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Communication and Verification in LLM Agents towards Collaboration under Information Asymmetry","url":"https://arxiv.org/abs/2510.25595","date":1761796800,"author":"","guid":321522,"unread":true,"content":"<article>arXiv:2510.25595v1 Announce Type: new \nAbstract: While Large Language Model (LLM) agents are often approached from the angle of action planning/generation to accomplish a goal (e.g., given by language descriptions), their abilities to collaborate with each other to achieve a joint goal are not well explored. To address this limitation, this paper studies LLM agents in task collaboration, particularly under the condition of information asymmetry, where agents have disparities in their knowledge and skills and need to work together to complete a shared task. We extend Einstein Puzzles, a classical symbolic puzzle, to a table-top game. In this game, two LLM agents must reason, communicate, and act to satisfy spatial and relational constraints required to solve the puzzle. We apply a fine-tuning-plus-verifier framework in which LLM agents are equipped with various communication strategies and verification signals from the environment. Empirical results highlight the critical importance of aligned communication, especially when agents possess both information-seeking and -providing capabilities. Interestingly, agents without communication can still achieve high task performance; however, further analysis reveals a lack of true rule understanding and lower trust from human evaluators. Instead, by integrating an environment-based verifier, we enhance agents' ability to comprehend task rules and complete tasks, promoting both safer and more interpretable collaboration in AI systems. https://github.com/Roihn/EinsteinPuzzles</article>","contentLength":1540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning","url":"https://arxiv.org/abs/2510.25594","date":1761796800,"author":"","guid":321523,"unread":true,"content":"<article>arXiv:2510.25594v1 Announce Type: new \nAbstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves state-of-the-art accuracy but requires global error propagation and full parameterization, leading to substantial memory and computational overhead. Direct Feedback Alignment (DFA) enables local, parallelizable updates with lower memory requirements but is limited by unstructured feedback and poor scalability in deeper architectures, specially convolutional neural networks. To address these limitations, we propose a structured local learning framework that operates directly on low-rank manifolds defined by the Singular Value Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed form, with updates applied to the SVD components using a composite loss that integrates cross-entropy, subspace alignment, and orthogonality regularization. Feedback matrices are constructed to match the SVD structure, ensuring consistent alignment between forward and feedback pathways. Our method reduces the number of trainable parameters relative to the original DFA model, without relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that our method achieves accuracy comparable to that of BP. Ablation studies confirm the importance of each loss term in the low-rank setting. These results establish local learning on low-rank manifolds as a principled and scalable alternative to full-rank gradient-based training.</article>","contentLength":1498,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Psychoacoustic assessment of synthetic sounds for electric vehicles in a virtual reality experiment","url":"https://arxiv.org/abs/2510.25593","date":1761796800,"author":"","guid":321524,"unread":true,"content":"<article>arXiv:2510.25593v1 Announce Type: new \nAbstract: The growing adoption of electric vehicles, known for their quieter operation compared to internal combustion engine vehicles, raises concerns about their detectability, particularly for vulnerable road users. To address this, regulations mandate the inclusion of exterior sound signals for electric vehicles, specifying minimum sound pressure levels at low speeds. These synthetic exterior sounds are often used in noisy urban environments, creating the challenge of enhancing detectability without introducing excessive noise annoyance. This study investigates the design of synthetic exterior sound signals that balance high noticeability with low annoyance. An audiovisual experiment with 14 participants was conducted using 15 virtual reality scenarios featuring a passing car. The scenarios included various sound signals, such as pure, intermittent, and complex tones at different frequencies. Two baseline cases, a diesel engine and only tyre noise, were also tested. Participants rated sounds for annoyance, noticeability, and informativeness using 11-point ICBEN scales. The findings highlight how psychoacoustic sound quality metrics predict annoyance ratings better than conventional sound metrics, providing insight into optimising sound design for electric vehicles. By improving pedestrian safety while minimising noise pollution, this research supports the development of effective and user-friendly exterior sound standards for electric vehicles.</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Multidimensional 2-Weight-Limited Burst-Correcting Codes","url":"https://arxiv.org/abs/2510.25592","date":1761796800,"author":"","guid":321525,"unread":true,"content":"<article>arXiv:2510.25592v1 Announce Type: new \nAbstract: We consider multidimensional codes capable of correcting a burst error of weight at most $2$. When two positions are in error, the burst limits their relative position. We study three such limitations: the $L_\\infty$ distance between the positions is bounded, the $L_1$ distance between the positions is bounded, or the two positions are on an axis-parallel line with bounded distance between them. In all cases we provide explicit code constructions, and compare their excess redundancy to a lower bound we prove.</article>","contentLength":563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalized Sobolev IPM for Graph-Based Measures","url":"https://arxiv.org/abs/2510.25591","date":1761796800,"author":"","guid":321526,"unread":true,"content":"<article>arXiv:2510.25591v1 Announce Type: new \nAbstract: We study the Sobolev IPM problem for measures supported on a graph metric space, where critic function is constrained to lie within the unit ball defined by Sobolev norm. While Le et al. (2025) achieved scalable computation by relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains intrinsically bound to $L^p$ geometric structure, limiting its ability to incorporate alternative structural priors beyond the $L^p$ geometry paradigm. To overcome this limitation, we propose to generalize Sobolev IPM through the lens of \\emph{Orlicz geometric structure}, which employs convex functions to capture nuanced geometric relationships, building upon recent advances in optimal transport theory -- particularly Orlicz-Wasserstein (OW) and generalized Sobolev transport -- that have proven instrumental in advancing machine learning methodologies. This generalization encompasses classical Sobolev IPM as a special case while accommodating diverse geometric priors beyond traditional $L^p$ structure. It however brings up significant computational hurdles that compound those already inherent in Sobolev IPM. To address these challenges, we establish a novel theoretical connection between Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization for the generalized Sobolev IPM (GSI). By further exploiting the underlying graph structure, we show that GSI with Musielak regularization (GSI-M) reduces to a simple \\emph{univariate optimization} problem, achieving remarkably computational efficiency. Empirically, GSI-M is several-order faster than the popular OW in computation, and demonstrates its practical advantages in comparing probability measures on a given graph for document classification and several tasks in topological data analysis.</article>","contentLength":1834,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RegionE: Adaptive Region-Aware Generation for Efficient Image Editing","url":"https://arxiv.org/abs/2510.25590","date":1761796800,"author":"","guid":321527,"unread":true,"content":"<article>arXiv:2510.25590v1 Announce Type: new \nAbstract: Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.</article>","contentLength":1952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Standardization of Psychiatric Diagnoses -- Role of Fine-tuned LLM Consortium and OpenAI-gpt-oss Reasoning LLM Enabled Decision Support System","url":"https://arxiv.org/abs/2510.25588","date":1761796800,"author":"","guid":321528,"unread":true,"content":"<article>arXiv:2510.25588v1 Announce Type: new \nAbstract: The diagnosis of most mental disorders, including psychiatric evaluations, primarily depends on dialogues between psychiatrists and patients. This subjective process can lead to variability in diagnoses across clinicians and patients, resulting in inconsistencies and challenges in achieving reliable outcomes. To address these issues and standardize psychiatric diagnoses, we propose a Fine-Tuned Large Language Model (LLM) Consortium and OpenAI-gpt-oss Reasoning LLM-enabled Decision Support System for the clinical diagnosis of mental disorders. Our approach leverages fine-tuned LLMs trained on conversational datasets involving psychiatrist-patient interactions focused on mental health conditions (e.g., depression). The diagnostic predictions from individual models are aggregated through a consensus-based decision-making process, refined by the OpenAI-gpt-oss reasoning LLM. We propose a novel method for deploying LLM agents that orchestrate communication between the LLM consortium and the reasoning LLM, ensuring transparency, reliability, and responsible AI across the entire diagnostic workflow. Experimental results demonstrate the transformative potential of combining fine-tuned LLMs with a reasoning model to create a robust and highly accurate diagnostic system for mental health assessment. A prototype of the proposed platform, integrating three fine-tuned LLMs with the OpenAI-gpt-oss reasoning LLM, was developed in collaboration with the U.S. Army Medical Research Team in Norfolk, Virginia, USA. To the best of our knowledge, this work represents the first application of a fine-tuned LLM consortium integrated with a reasoning LLM for clinical mental health diagnosis paving the way for next-generation AI-powered eHealth systems aimed at standardizing psychiatric diagnoses.</article>","contentLength":1850,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning-Augmented Online Bidding in Stochastic Settings","url":"https://arxiv.org/abs/2510.25582","date":1761796800,"author":"","guid":321529,"unread":true,"content":"<article>arXiv:2510.25582v1 Announce Type: new \nAbstract: Online bidding is a classic optimization problem, with several applications in online decision-making, the design of interruptible systems, and the analysis of approximation algorithms. In this work, we study online bidding under learning-augmented settings that incorporate stochasticity, in either the prediction oracle or the algorithm itself. In the first part, we study bidding under distributional predictions, and find Pareto-optimal algorithms that offer the best-possible tradeoff between the consistency and the robustness of the algorithm. In the second part, we study the power and limitations of randomized bidding algorithms, by presenting upper and lower bounds on the consistency/robustness tradeoffs. Previous works focused predominantly on oracles that do not leverage stochastic information on the quality of the prediction, and deterministic algorithms.</article>","contentLength":922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Several classes of $p$-ary linear codes with few-weights derived from Weil sums","url":"https://arxiv.org/abs/2510.25578","date":1761796800,"author":"","guid":321530,"unread":true,"content":"<article>arXiv:2510.25578v1 Announce Type: new \nAbstract: Linear codes with few weights have been a significant area of research in coding theory for many years, due to their applications in secret sharing schemes, authentication codes, association schemes, and strongly regular graphs. Inspired by the works of Cheng and Gao \\cite{P8} and Wu, Li and Zeng \\cite{P12}, in this paper, we propose several new classes of few-weight linear codes over the finite field $\\mathbb{F}_{p}$ through the selection of two specific defining sets. Consequently, we obtain five classes of $4$-weight linear codes and one class of $2$-weight linear codes from our first defining set. Furthermore, by employing weakly regular bent functions in our second defining set, we derive two classes of $6$-weight codes, two classes of $8$-weight codes, and one class of $9$-weight codes. The parameters and weight distributions of all these constructed codes are wholly determined by detailed calculations on certain Weil sums over finite fields. In addition, we identify an optimal class of $2$-weight codes that meet the Griesmer bound.</article>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perturbation Bounds for Low-Rank Inverse Approximations under Noise","url":"https://arxiv.org/abs/2510.25571","date":1761796800,"author":"","guid":321531,"unread":true,"content":"<article>arXiv:2510.25571v1 Announce Type: new \nAbstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in scalable machine learning, optimization, and scientific computing. However, real-world matrices are often observed with noise, arising from sampling, sketching, and quantization. The spectral-norm robustness of low-rank inverse approximations remains poorly understood. We systematically study the spectral-norm error $\\| (\\tilde{A}^{-1})_p - A_p^{-1} \\|$ for an $n\\times n$ symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\\(p\\) approximation of $A^{-1}$, and $\\tilde{A} = A + E$ is a noisy observation. Under mild assumptions on the noise, we derive sharp non-asymptotic perturbation bounds that reveal how the error scales with the eigengap, spectral decay, and noise alignment with low-curvature directions of $A$. Our analysis introduces a novel application of contour integral techniques to the \\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over naive adaptations of classical full-inverse bounds by up to a factor of $\\sqrt{n}$. Empirically, our bounds closely track the true perturbation error across a variety of real-world and synthetic matrices, while estimates based on classical results tend to significantly overpredict. These findings offer practical, spectrum-aware guarantees for low-rank inverse approximations in noisy computational environments.</article>","contentLength":1424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes","url":"https://arxiv.org/abs/2510.25569","date":1761796800,"author":"","guid":321532,"unread":true,"content":"<article>arXiv:2510.25569v1 Announce Type: new \nAbstract: PAC-Bayes is a popular and efficient framework for obtaining generalization guarantees in situations involving uncountable hypothesis spaces. Unfortunately, in its classical formulation, it only provides guarantees on the expected risk of a randomly sampled hypothesis. This requires stochastic predictions at test time, making PAC-Bayes unusable in many practical situations where a single deterministic hypothesis must be deployed. We propose a unified framework to extract guarantees holding for a single hypothesis from stochastic PAC-Bayesian guarantees. We present a general oracle bound and derive from it a numerical bound and a specialization to majority vote. We empirically show that our approach consistently outperforms popular baselines (by up to a factor of 2) when it comes to generalization bounds on deterministic classifiers.</article>","contentLength":893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"M-Guarding in K-Visibility","url":"https://arxiv.org/abs/2510.25567","date":1761796800,"author":"","guid":321533,"unread":true,"content":"<article>arXiv:2510.25567v1 Announce Type: new \nAbstract: We explore the problem of $M$-guarding polygons with holes using $k$-visibility guards, where a set of guards is said to $M$-guard a polygon if every point in the polygon is visible to at least $M$ guards, with the constraint that there may only be 1 guard on each edge. A $k$-visibility guard can see through up to $k$ walls, with $k \\geq 2$. We present a theorem establishing that any polygon with holes can be 2-guarded under $k$-visibility where $k \\geq 2$, which expands existing results in 0-visibility. We provide an algorithm that $M$-guards a polygon using a convex decomposition of the polygon. We show that every point in the polygon is visible to at least four $2$-visibility guards and then extend the result to show that for any even $k \\geq 2$ there exists a placement of guards such that every point in the polygon is visible to $k + 2$ guards.</article>","contentLength":909,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal and Heuristic Approaches for Platooning Systems with Deadlines","url":"https://arxiv.org/abs/2510.25564","date":1761796800,"author":"","guid":321534,"unread":true,"content":"<article>arXiv:2510.25564v1 Announce Type: new \nAbstract: Efficient truck platooning is a key strategy for reducing freight costs, lowering fuel consumption, and mitigating emissions. Deadlines are critical in this context, as trucks must depart within specific time windows to meet delivery requirements and avoid penalties. In this paper, we investigate the optimal formation and dispatch of truck platoons at a highway station with finite capacity $L$ and deadline constraints $T$. The system operates in discrete time, with each arriving truck assigned a deadline of $T$ slot units. The objective is to leverage the efficiency gains from forming large platoons while accounting for waiting costs and deadline violations. We formulate the problem as a Markov decision process and analyze the structure of the optimal policy $\\pi^\\star$ for $L = 3$, extending insights to arbitrary $L$. We prove that the $\\pi^\\star$ is monotone in the state space $\\mathcal{S}$ and identify classes of unreachable states. Moreover, since $\\mathcal{S}$ grows exponentially with $L$ and $T$, we propose heuristics-including conditional and deep-learning based approaches-that exploit these structural insights while maintaining low computational complexity.</article>","contentLength":1232,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting","url":"https://arxiv.org/abs/2510.25563","date":1761796800,"author":"","guid":321535,"unread":true,"content":"<article>arXiv:2510.25563v1 Announce Type: new \nAbstract: The accurate prediction of oceanographic variables is crucial for understanding climate change, managing marine resources, and optimizing maritime activities. Traditional ocean forecasting relies on numerical models; however, these approaches face limitations in terms of computational cost and scalability. In this study, we adapt Aurora, a foundational deep learning model originally designed for atmospheric forecasting, to predict sea surface temperature (SST) in the Canary Upwelling System. By fine-tuning this model with high-resolution oceanographic reanalysis data, we demonstrate its ability to capture complex spatiotemporal patterns while reducing computational demands. Our methodology involves a staged fine-tuning process, incorporating latitude-weighted error metrics and optimizing hyperparameters for efficient learning. The experimental results show that the model achieves a low RMSE of 0.119K, maintaining high anomaly correlation coefficients (ACC $\\approx 0.997$). The model successfully reproduces large-scale SST structures but faces challenges in capturing finer details in coastal regions. This work contributes to the field of data-driven ocean forecasting by demonstrating the feasibility of using deep learning models pre-trained in different domains for oceanic applications. Future improvements include integrating additional oceanographic variables, increasing spatial resolution, and exploring physics-informed neural networks to enhance interpretability and understanding. These advancements can improve climate modeling and ocean prediction accuracy, supporting decision-making in environmental and economic sectors.</article>","contentLength":1701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Reinforcement Learning-Based Cooperative Rate Splitting for Satellite-to-Underground Communication Networks","url":"https://arxiv.org/abs/2510.25562","date":1761796800,"author":"","guid":321536,"unread":true,"content":"<article>arXiv:2510.25562v1 Announce Type: new \nAbstract: Reliable downlink communication in satellite-to-underground networks remains challenging due to severe signal attenuation caused by underground soil and refraction in the air-soil interface. To address this, we propose a novel cooperative rate-splitting (CRS)-aided transmission framework, where an aboveground relay decodes and forwards the common stream to underground devices (UDs). Based on this framework, we formulate a max-min fairness optimization problem that jointly optimizes power allocation, message splitting, and time slot scheduling to maximize the minimum achievable rate across UDs. To solve this high-dimensional non-convex problem under uncertain channels, we develop a deep reinforcement learning solution framework based on the proximal policy optimization (PPO) algorithm that integrates distribution-aware action modeling and a multi-branch actor network. Simulation results under a realistic underground pipeline monitoring scenario demonstrate that the proposed approach achieves average max-min rate gains exceeding $167\\%$ over conventional benchmark strategies across various numbers of UDs and underground conditions.</article>","contentLength":1196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking","url":"https://arxiv.org/abs/2510.25560","date":1761796800,"author":"","guid":321537,"unread":true,"content":"<article>arXiv:2510.25560v1 Announce Type: new \nAbstract: Ambiguities in data and problem constraints can lead to diverse, equally plausible outcomes for a machine learning task. In beat and downbeat tracking, for instance, different listeners may adopt various rhythmic interpretations, none of which would necessarily be incorrect. To address this, we propose a contrastive self-supervised pre-training approach that leverages multiple hypotheses about possible positive samples in the data. Our model is trained to learn representations compatible with different such hypotheses, which are selected with a knowledge-based scoring function to retain the most plausible ones. When fine-tuned on labeled data, our model outperforms existing methods on standard benchmarks, showcasing the advantages of integrating domain knowledge with multi-hypothesis selection in music representation learning in particular.</article>","contentLength":901,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hybrid Quantum-Classical Recurrent Neural Networks","url":"https://arxiv.org/abs/2510.25557","date":1761796800,"author":"","guid":321538,"unread":true,"content":"<article>arXiv:2510.25557v1 Announce Type: new \nAbstract: We present a hybrid quantum-classical recurrent neural network (QRNN) architecture in which the entire recurrent core is realized as a parametrized quantum circuit (PQC) controlled by a classical feedforward network. The hidden state is the quantum state of an $n$-qubit PQC, residing in an exponentially large Hilbert space $\\mathbb{C}^{2^n}$. The PQC is unitary by construction, making the hidden-state evolution norm-preserving without external constraints. At each timestep, mid-circuit readouts are combined with the input embedding and processed by the feedforward network, which provides explicit classical nonlinearity. The outputs parametrize the PQC, which updates the hidden state via unitary dynamics. The QRNN is compact and physically consistent, and it unifies (i) unitary recurrence as a high-capacity memory, (ii) partial observation via mid-circuit measurements, and (iii) nonlinear classical control for input-conditioned parametrization. We evaluate the model in simulation with up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory, and language modeling, adopting projective measurements as a limiting case to obtain mid-circuit readouts while maintaining a coherent recurrent quantum memory. We further devise a soft attention mechanism over the mid-circuit readouts in a sequence-to-sequence model and show its effectiveness for machine translation. To our knowledge, this is the first model (RNN or otherwise) grounded in quantum operations to achieve competitive performance against strong classical baselines across a broad class of sequence-learning tasks.</article>","contentLength":1656,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Device to Device Pairs Sharding based on Distance","url":"https://arxiv.org/abs/2510.25552","date":1761796800,"author":"","guid":321539,"unread":true,"content":"<article>arXiv:2510.25552v1 Announce Type: new \nAbstract: In the conventional cellular system, devices are not allowed to communicate directly with each other in the licensed cellular bandwidth and all communications take place through the base stations. The users requirements has led the technology to become fast and faster. Multimedia rich data exchange, fast service, high quality voice calls, newer and more demanding applications, information at fingertips, everything requires technology and communication between devices. A constant need to increase network capacity for meeting the users growing demands has led to the growth of cellular communication networks from the first generation(1G) to the fifth generation(5G). There will be crores of connected devices in the coming future . A large number of connections are going to be heterogeneous, demanding lesser delays, higher data rates, superior throughput and enhanced system capacity. The available spectrum resources are limited and has to be flexibly used by mobile network operators to cope with the rising demands. An emerging facilitator of the upcoming high data rate demanding next-generation networks are device-to-device(D2D) communication. This paper has developed a model that establishes Device-to-Device (D2D) communication between two end-users without involving the eNB (evolved Node B). We have sharded the UEs and CUs based on the criteria of DISTANCE. To do so, we used the K-means clustering method.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Using VLM Reasoning to Constrain Task and Motion Planning","url":"https://arxiv.org/abs/2510.25548","date":1761796800,"author":"","guid":321540,"unread":true,"content":"<article>arXiv:2510.25548v1 Announce Type: new \nAbstract: In task and motion planning, high-level task planning is done over an abstraction of the world to enable efficient search in long-horizon robotics problems. However, the feasibility of these task-level plans relies on the downward refinability of the abstraction into continuous motion. When a domain's refinability is poor, task-level plans that appear valid may ultimately fail during motion planning, requiring replanning and resulting in slower overall performance. Prior works mitigate this by encoding refinement issues as constraints to prune infeasible task plans. However, these approaches only add constraints upon refinement failure, expending significant search effort on infeasible branches. We propose VIZ-COAST, a method of leveraging the common-sense spatial reasoning of large pretrained Vision-Language Models to identify issues with downward refinement a priori, bypassing the need to fix these failures during planning. Experiments on two challenging TAMP domains show that our approach is able to extract plausible constraints from images and domain descriptions, drastically reducing planning times and, in some cases, eliminating downward refinement failures altogether, generalizing to a diverse range of instances from the broader domain.</article>","contentLength":1312,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information","url":"https://arxiv.org/abs/2510.25542","date":1761796800,"author":"","guid":321541,"unread":true,"content":"<article>arXiv:2510.25542v1 Announce Type: cross \nAbstract: Uncovering hidden graph structures underlying real-world data is a critical challenge with broad applications across scientific domains. Recently, transformer-based models leveraging the attention mechanism have demonstrated strong empirical success in capturing complex dependencies within graphs. However, the theoretical understanding of their training dynamics has been limited to tree-like graphs, where each node depends on a single parent. Extending provable guarantees to more general directed acyclic graphs (DAGs) -- which involve multiple parents per node -- remains challenging, primarily due to the difficulty in designing training objectives that enable different attention heads to separately learn multiple different parent relationships.\n  In this work, we address this problem by introducing a novel information-theoretic metric: the kernel-guided mutual information (KG-MI), based on the $f$-divergence. Our objective combines KG-MI with a multi-head attention framework, where each head is associated with a distinct marginal transition kernel to model diverse parent-child dependencies effectively. We prove that, given sequences generated by a $K$-parent DAG, training a single-layer, multi-head transformer via gradient ascent converges to the global optimum in polynomial time. Furthermore, we characterize the attention score patterns at convergence. In addition, when particularizing the $f$-divergence to the KL divergence, the learned attention scores accurately reflect the ground-truth adjacency matrix, thereby provably recovering the underlying graph structure. Experimental results validate our theoretical findings.</article>","contentLength":1700,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation","url":"https://arxiv.org/abs/2510.25536","date":1761796800,"author":"","guid":321542,"unread":true,"content":"<article>arXiv:2510.25536v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly envisioned as the foundation for simulating an individual's communication style, behavioral tendencies, and personality traits. However, current evaluations of LLM-based persona simulation remain limited: most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability requirement. To address these limitations, we introduce TwinVoice, a comprehensive benchmark for assessing persona simulation across diverse real-world contexts. TwinVoice encompasses three dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and Narrative Persona (role-based expression). It further decomposes the evaluation of LLM performance into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning, lexical fidelity, persona tone, and syntactic style. Experimental results reveal that while advanced models achieve moderate accuracy in persona simulation, they still fall short of capabilities such as syntactic style and memory recall. Consequently, the average performance achieved by LLMs remains considerably below the human baseline.</article>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Off-policy Reinforcement Learning with Model-based Exploration Augmentation","url":"https://arxiv.org/abs/2510.25529","date":1761796800,"author":"","guid":321543,"unread":true,"content":"<article>arXiv:2510.25529v1 Announce Type: new \nAbstract: Exploration is fundamental to reinforcement learning (RL), as it determines how effectively an agent discovers and exploits the underlying structure of its environment to achieve optimal performance. Existing exploration methods generally fall into two categories: active exploration and passive exploration. The former introduces stochasticity into the policy but struggles in high-dimensional environments, while the latter adaptively prioritizes transitions in the replay buffer to enhance exploration, yet remains constrained by limited sample diversity. To address the limitation in passive exploration, we propose Modelic Generative Exploration (MoGE), which augments exploration through the generation of under-explored critical states and synthesis of dynamics-consistent experiences through transition models. MoGE is composed of two components: (1) a diffusion-based generator that synthesizes critical states under the guidance of a utility function evaluating each state's potential influence on policy exploration, and (2) a one-step imagination world model for constructing critical transitions based on the critical states for agent learning. Our method adopts a modular formulation that aligns with the principles of off-policy learning, allowing seamless integration with existing algorithms to improve exploration without altering their core structures. Empirical results on OpenAI Gym and DeepMind Control Suite reveal that MoGE effectively bridges exploration and policy learning, leading to remarkable gains in both sample efficiency and performance across complex control tasks.</article>","contentLength":1649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero Reinforcement Learning Towards General Domains","url":"https://arxiv.org/abs/2510.25528","date":1761796800,"author":"","guid":321544,"unread":true,"content":"<article>arXiv:2510.25528v1 Announce Type: new \nAbstract: Zero Reinforcement Learning (Zero-RL) has proven to be an effective approach for enhancing the reasoning capabilities of large language models (LLMs) by directly applying reinforcement learning with verifiable rewards on pretrained models, without the need for a supervised fine-tuning phase. However, current research on zero-RL primarily focuses on domains with easily verifiable reward signals, such as mathematics, programming, and other reasoning tasks. The challenge of eliciting reasoning abilities in more diverse scenarios, where verification is not straightforward, remains underexplored. To address this gap, we propose a novel zero-RL paradigm designed to improve a model's reasoning ability across both verifiable and non-verifiable domains. By combining verifiable rewards with a generative reward model, we conduct multi-task zero-RL training across both domains, facilitating the transfer of reasoning capabilities between them. Furthermore, to mitigate reward hacking in the generative reward model, we design a smooth length penalty that encourages the generation of more comprehensive thinking tokens in general domains. Experimental results on Qwen3-8B-Base and Qwen3-14B-Base demonstrate that our approach achieves superior reasoning performance, not only on tasks requiring extensive reasoning but also on more general tasks.</article>","contentLength":1396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comparative Study of UNet-based Architectures for Liver Tumor Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography","url":"https://arxiv.org/abs/2510.25522","date":1761796800,"author":"","guid":321545,"unread":true,"content":"<article>arXiv:2510.25522v1 Announce Type: new \nAbstract: Segmentation of liver structures in multi-phase contrast-enhanced computed tomography (CECT) plays a crucial role in computer-aided diagnosis and treatment planning for liver diseases, including tumor detection. In this study, we investigate the performance of UNet-based architectures for liver tumor segmentation, starting from the original UNet and extending to UNet3+ with various backbone networks. We evaluate ResNet, Transformer-based, and State-space (Mamba) backbones, all initialized with pretrained weights. Surprisingly, despite the advances in modern architecture, ResNet-based models consistently outperform Transformer- and Mamba-based alternatives across multiple evaluation metrics. To further improve segmentation quality, we introduce attention mechanisms into the backbone and observe that incorporating the Convolutional Block Attention Module (CBAM) yields the best performance. ResNetUNet3+ with CBAM module not only produced the best overlap metrics with a Dice score of 0.755 and IoU of 0.662, but also achieved the most precise boundary delineation, evidenced by the lowest HD95 distance of 77.911. The model's superiority was further cemented by its leading overall accuracy of 0.925 and specificity of 0.926, showcasing its robust capability in accurately identifying both lesion and healthy tissue. To further enhance interpretability, Grad-CAM visualizations were employed to highlight the region's most influential predictions, providing insights into its decision-making process. These findings demonstrate that classical ResNet architecture, when combined with modern attention modules, remain highly competitive for medical image segmentation tasks, offering a promising direction for liver tumor detection in clinical practice.</article>","contentLength":1811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nonparametric estimation of homogenized invariant measures from multiscale data via Hermite expansion","url":"https://arxiv.org/abs/2510.25521","date":1761796800,"author":"","guid":321546,"unread":true,"content":"<article>arXiv:2510.25521v1 Announce Type: new \nAbstract: We consider the problem of density estimation in the context of multiscale Langevin diffusion processes, where a single-scale homogenized surrogate model can be derived. In particular, our aim is to learn the density of the invariant measure of the homogenized dynamics from a continuous-time trajectory generated by the full multiscale system. We propose a spectral method based on a truncated Fourier expansion with Hermite functions as orthonormal basis. The Fourier coefficients are computed directly from the data owing to the ergodic theorem. We prove that the resulting density estimator is robust and converges to the invariant density of the homogenized model as the scale separation parameter vanishes, provided the time horizon and the number of Fourier modes are suitably chosen in relation to the multiscale parameter. The accuracy and reliability of this methodology is further demonstrated through a series of numerical experiments.</article>","contentLength":996,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Octopus-like Reaching Motion: A Perspective Inspired by Whipping","url":"https://arxiv.org/abs/2510.25520","date":1761796800,"author":"","guid":321547,"unread":true,"content":"<article>arXiv:2510.25520v1 Announce Type: new \nAbstract: The stereotypical reaching motion of the octopus arm has drawn growing attention for its efficient control of a highly deformable body. Previous studies suggest that its characteristic bend propagation may share underlying principles with the dynamics of a whip. This work investigates whether whip-like passive dynamics in water can reproduce the kinematic features observed in biological reaching and their similarities and differences. Platform-based whipping tests were performed in water and air while systematically varying material stiffness and driving speed. Image-based quantification revealed that the Ecoflex Gel 2 arm driven at 150 rpm (motor speed) reproduced curvature propagation similar to that observed in octopus reaching. However, its bend-point velocity decreased monotonically rather than exhibiting the biological bell-shaped profile, confirming that the octopus reaching movement is not merely a passive whipping behavior. The absence of propagation in air further highlights the critical role of the surrounding medium in forming octopus-like reaching motion. This study provides a new perspective for understand biological reaching movement, and offers a potential platform for future hydrodynamic research.</article>","contentLength":1282,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Retrieval Augmented Generation (RAG) for Fintech: Agentic Design and Evaluation","url":"https://arxiv.org/abs/2510.25518","date":1761796800,"author":"","guid":321548,"unread":true,"content":"<article>arXiv:2510.25518v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) systems often face limitations in specialized domains such as fintech, where domain-specific ontologies, dense terminology, and acronyms complicate effective retrieval and synthesis. This paper introduces an agentic RAG architecture designed to address these challenges through a modular pipeline of specialized agents. The proposed system supports intelligent query reformulation, iterative sub-query decomposition guided by keyphrase extraction, contextual acronym resolution, and cross-encoder-based context re-ranking. We evaluate our approach against a standard RAG baseline using a curated dataset of 85 question--answer--reference triples derived from an enterprise fintech knowledge base. Experimental results demonstrate that the agentic RAG system outperforms the baseline in retrieval precision and relevance, albeit with increased latency. These findings suggest that structured, multi-agent methodologies offer a promising direction for enhancing retrieval robustness in complex, domain-specific settings.</article>","contentLength":1104,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Predicate Renaming via Large Language Models","url":"https://arxiv.org/abs/2510.25517","date":1761796800,"author":"","guid":321549,"unread":true,"content":"<article>arXiv:2510.25517v1 Announce Type: new \nAbstract: In this paper, we address the problem of giving names to predicates in logic rules using Large Language Models (LLMs). In the context of Inductive Logic Programming, various rule generation methods produce rules containing unnamed predicates, with Predicate Invention being a key example. This hinders the readability, interpretability, and reusability of the logic theory. Leveraging recent advancements in LLMs development, we explore their ability to process natural language and code to provide semantically meaningful suggestions for giving a name to unnamed predicates. The evaluation of our approach on some hand-crafted logic rules indicates that LLMs hold potential for this task.</article>","contentLength":738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FaCT: Faithful Concept Traces for Explaining Neural Network Decisions","url":"https://arxiv.org/abs/2510.25512","date":1761796800,"author":"","guid":321550,"unread":true,"content":"<article>arXiv:2510.25512v1 Announce Type: new \nAbstract: Deep networks have shown remarkable performance across a wide range of tasks, yet getting a global concept-level understanding of how they function remains a key challenge. Many post-hoc concept-based approaches have been introduced to understand their workings, yet they are not always faithful to the model. Further, they make restrictive assumptions on the concepts a model learns, such as class-specificity, small spatial extent, or alignment to human expectations. In this work, we put emphasis on the faithfulness of such concept-based explanations and propose a new model with model-inherent mechanistic concept-explanations. Our concepts are shared across classes and, from any layer, their contribution to the logit and their input-visualization can be faithfully traced. We also leverage foundation models to propose a new concept-consistency metric, C$^2$-Score, that can be used to evaluate concept-based methods. We show that, compared to prior work, our concepts are quantitatively more consistent and users find our concepts to be more interpretable, all while retaining competitive ImageNet performance.</article>","contentLength":1168,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MTIR-SQL: Multi-turn Tool-Integrated Reasoning Reinforcement Learning for Text-to-SQL","url":"https://arxiv.org/abs/2510.25510","date":1761796800,"author":"","guid":321551,"unread":true,"content":"<article>arXiv:2510.25510v1 Announce Type: new \nAbstract: As large language models (LLMs) are increasingly used in Text-to-SQL tasks, Reinforcement Learning (RL) has become a common method for improving performance. Existing methods primarily rely on static execution feedback, which restricts real-time error correction. However, integrating multi-turn tool invocation along with dynamic feedback could significantly improve adaptability and robustness, ultimately enhancing model performance. To address these issues, we propose MTIR-SQL, an innovative Multi-turn Tool-Integrated Reasoning reinforcement learning framework for Text-to-SQL. Our approach introduces an execution-aware multi-turn reasoning paradigm that seamlessly incorporates database execution feedback at each reasoning step, enabling context-sensitive query generation and progressive refinement throughout the reasoning process. The framework extends the GRPO algorithm to accommodate complex multi-turn interaction scenarios. Considering the training instability characteristics of MTIR and the potential for significant Deviation of model distribution from the initial model, we enhance the GRPO algorithm by adding a trajectory filtering mechanism and removing KL loss constraints. Experimental results demonstrate that MTIR-SQL, with 4B parameters, achieves \\textbf{64.4}\\% accuracy in the BIRD Dev and 84.6% execution accuracy in the SPIDER Dev, significantly outperforming existing approaches.</article>","contentLength":1462,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use","url":"https://arxiv.org/abs/2510.25509","date":1761796800,"author":"","guid":321552,"unread":true,"content":"<article>arXiv:2510.25509v1 Announce Type: new \nAbstract: Burnout is a psychological syndrome marked by emotional exhaustion, depersonalization, and reduced personal accomplishment, with a significant impact on individual well-being and organizational performance. This study proposes a machine learning approach to predict burnout risk using the HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms were evaluated: nearest neighbors (KNN), random forest, and support vector machine (SVM), with model performance evaluated through 30-fold cross-validation using the determination coefficient (R2). Among the models tested, SVM achieved the highest predictive performance (R2 = 0.84) and was statistically superior to KNN and Random Forest based on paired $t$-tests. To ensure practical applicability, an interactive interface was developed using Streamlit, allowing non-technical users to input data and receive burnout risk predictions. The results highlight the potential of machine learning to support early detection of burnout and promote data-driven mental health strategies in organizational settings.</article>","contentLength":1122,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reflections on the Reproducibility of Commercial LLM Performance in Empirical Software Engineering Studies","url":"https://arxiv.org/abs/2510.25506","date":1761796800,"author":"","guid":321553,"unread":true,"content":"<article>arXiv:2510.25506v1 Announce Type: new \nAbstract: Large Language Models have gained remarkable interest in industry and academia. The increasing interest in LLMs in academia is also reflected in the number of publications on this topic over the last years. For instance, alone 78 of the around 425 publications at ICSE 2024 performed experiments with LLMs. Conducting empirical studies with LLMs remains challenging and raises questions on how to achieve reproducible results, for both other researchers and practitioners. One important step towards excelling in empirical research on LLMs and their application is to first understand to what extent current research results are eventually reproducible and what factors may impede reproducibility. This investigation is within the scope of our work. We contribute an analysis of the reproducibility of LLM-centric studies, provide insights into the factors impeding reproducibility, and discuss suggestions on how to improve the current state. In particular, we studied the 86 articles describing LLM-centric studies, published at ICSE 2024 and ASE 2024. Of the 86 articles, 18 provided research artefacts and used OpenAI models. We attempted to replicate those 18 studies. Of the 18 studies, only five were fit for reproduction. For none of the five studies, we were able to fully reproduce the results. Two studies seemed to be partially reproducible, and three studies did not seem to be reproducible. Our results highlight not only the need for stricter research artefact evaluations but also for more robust study designs to ensure the reproducible value of future publications.</article>","contentLength":1632,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Objective Search: Algorithms, Applications, and Emerging Directions","url":"https://arxiv.org/abs/2510.25504","date":1761796800,"author":"","guid":321554,"unread":true,"content":"<article>arXiv:2510.25504v1 Announce Type: new \nAbstract: Multi-objective search (MOS) has emerged as a unifying framework for planning and decision-making problems where multiple, often conflicting, criteria must be balanced. While the problem has been studied for decades, recent years have seen renewed interest in the topic across AI applications such as robotics, transportation, and operations research, reflecting the reality that real-world systems rarely optimize a single measure. This paper surveys developments in MOS while highlighting cross-disciplinary opportunities, and outlines open challenges that define the emerging frontier of MOS</article>","contentLength":643,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting","url":"https://arxiv.org/abs/2510.25502","date":1761796800,"author":"","guid":321555,"unread":true,"content":"<article>arXiv:2510.25502v1 Announce Type: new \nAbstract: Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the vast majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.</article>","contentLength":1314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A New Neural Network Paradigm for Scalable and Generalizable Stability Analysis of Power Systems","url":"https://arxiv.org/abs/2510.25501","date":1761796800,"author":"","guid":321556,"unread":true,"content":"<article>arXiv:2510.25501v1 Announce Type: new \nAbstract: This paper presents a new neural network (NN) paradigm for scalable and generalizable stability analysis of power systems. The paradigm consists of two parts: the neural stability descriptor and the sample-augmented iterative training scheme. The first part, based on system decomposition, constructs the object (such as a stability function or condition) for stability analysis as a scalable aggregation of multiple NNs. These NNs remain fixed across varying power system structures and parameters, and are repeatedly shared within each system instance defined by these variations, thereby enabling the generalization of the neural stability descriptor across a class of power systems. The second part learns the neural stability descriptor by iteratively training the NNs with sample augmentation, guided by the tailored conservativeness-aware loss function. The training set is strategically constructed to promote the descriptor's generalizability, which is systematically evaluated by verification and validation during the training process. Specifically, the proposed NN paradigm is implemented for large-disturbance stability analysis of the bulk power grid and small-disturbance stability conditions of the microgrid system. Finally, numerical studies for the two implementations demonstrate the applicability and effectiveness of the proposed NN paradigm.</article>","contentLength":1413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating Learning Congestion control Schemes for LEO Constellations","url":"https://arxiv.org/abs/2510.25498","date":1761796800,"author":"","guid":321557,"unread":true,"content":"<article>arXiv:2510.25498v1 Announce Type: new \nAbstract: Low Earth Orbit (LEO) satellite networks introduce unique congestion control (CC) challenges due to frequent handovers, rapidly changing round-trip times (RTTs), and non-congestive loss. This paper presents the first comprehensive, emulation-driven evaluation of CC schemes in LEO networks, combining realistic orbital dynamics via the LeoEM framework with targeted Mininet micro-benchmarks. We evaluated representative CC algorithms from three classes, loss-based (Cubic, SaTCP), model-based (BBRv3), and learning-based (Vivace, Sage, Astraea), across diverse single-flow and multi-flow scenarios, including interactions with active queue management (AQM). Our findings reveal that: (1) handover-aware loss-based schemes can reclaim bandwidth but at the cost of increased latency; (2) BBRv3 sustains high throughput with modest delay penalties, yet reacts slowly to abrupt RTT changes; (3) RL-based schemes severely underperform under dynamic conditions, despite being notably resistant to non-congestive loss; (4) fairness degrades significantly with RTT asymmetry and multiple bottlenecks, especially in human-designed CC schemes; and (5) AQM at bottlenecks can restore fairness and boost efficiency. These results expose critical limitations in current CC schemes and provide insight for designing LEO-specific data transport protocols.</article>","contentLength":1389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI","url":"https://arxiv.org/abs/2510.25497","date":1761796800,"author":"","guid":321558,"unread":true,"content":"<article>arXiv:2510.25497v1 Announce Type: new \nAbstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine neural perception and symbolic reasoning in end-to-end trainable models. However, recent findings reveal these are prone to shortcut reasoning, i.e., to learning unindented concepts--or neural predicates--which exploit spurious correlations to satisfy the symbolic constraints. In this paper, we address reasoning shortcuts at their root cause and we introduce prototypical neurosymbolic architectures. These models are able to satisfy the symbolic constraints (be right) because they have learnt the correct basic concepts (for the right reasons) and not because of spurious correlations, even in extremely low data regimes. Leveraging the theory of prototypical learning, we demonstrate that we can effectively avoid reasoning shortcuts by training the models to satisfy the background knowledge while taking into account the similarity of the input with respect to the handful of labelled datapoints. We extensively validate our approach on the recently proposed rsbench benchmark suite in a variety of settings and tasks with very scarce supervision: we show significant improvements in learning the right concepts both in synthetic tasks (MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our findings pave the way to prototype grounding as an effective, annotation-efficient strategy for safe and reliable neurosymbolic learning.</article>","contentLength":1482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalized Pseudo-Relevance Feedback","url":"https://arxiv.org/abs/2510.25488","date":1761796800,"author":"","guid":321559,"unread":true,"content":"<article>arXiv:2510.25488v1 Announce Type: new \nAbstract: Query rewriting is a fundamental technique in information retrieval (IR). It typically employs the retrieval result as relevance feedback to refine the query and thereby addresses the vocabulary mismatch between user queries and relevant documents. Traditional pseudo-relevance feedback (PRF) and its vector-based extension (VPRF) improve retrieval performance by leveraging top-retrieved documents as relevance feedback. However, they are constructed based on two major hypotheses: the relevance assumption (top documents are relevant) and the model assumption (rewriting methods need to be designed specifically for particular model architectures). While recent large language models (LLMs)-based generative relevance feedback (GRF) enables model-free query reformulation, it either suffers from severe LLM hallucination or, again, relies on the relevance assumption to guarantee the effectiveness of rewriting quality. To overcome these limitations, we introduce an assumption-relaxed framework: \\textit{Generalized Pseudo Relevance Feedback} (GPRF), which performs model-free, natural language rewriting based on retrieved documents, not only eliminating the model assumption but also reducing dependence on the relevance assumption. Specifically, we design a utility-oriented training pipeline with reinforcement learning to ensure robustness against noisy feedback. Extensive experiments across multiple benchmarks and retrievers demonstrate that GPRF consistently outperforms strong baselines, establishing it as an effective and generalizable framework for query rewriting.</article>","contentLength":1630,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks","url":"https://arxiv.org/abs/2510.25480","date":1761796800,"author":"","guid":321560,"unread":true,"content":"<article>arXiv:2510.25480v1 Announce Type: new \nAbstract: Robust validation metrics remain essential in contemporary deep learning, not only to detect overfitting and poor generalization, but also to monitor training dynamics. In the supervised classification setting, we investigate whether interactions between training data and model weights can yield such a metric that both tracks generalization during training and attributes performance to individual training samples. We introduce Gradient-Weight Alignment (GWA), quantifying the coherence between per-sample gradients and model weights. We show that effective learning corresponds to coherent alignment, while misalignment indicates deteriorating generalization. GWA is efficiently computable during training and reflects both sample-specific contributions and dataset-wide learning dynamics. Extensive experiments show that GWA accurately predicts optimal early stopping, enables principled model comparisons, and identifies influential training samples, providing a validation-set-free approach for model analysis directly from the training data.</article>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Combining Moving Mass Actuators and Manoeuvring Models for Underwater Vehicles: A Lagrangian Approach","url":"https://arxiv.org/abs/2510.25479","date":1761796800,"author":"","guid":321561,"unread":true,"content":"<article>arXiv:2510.25479v1 Announce Type: new \nAbstract: In this paper, we present a Newton-Euler formulation of the equations of motion for underwater vehicles with an interntal moving mass actuator. Furthermore, the moving mass dynamics are expressed as an extension to the manoeuvring model for underwater vehicles, originally introduced by Fossen (1991). The influence of the moving mass is described in body-frame and included as states in both an additional kinematic equation and as part of the coupled rigid-body kinetics of the underwater vehicle. The Coriolis-centripetal effects are derived from Kirchhoff's equations and the hydrostatics are derived using first principals. The proposed Newton-Euler model is validated through simulation and compared with the traditional Hamiltonian internal moving mass actuator formulation.</article>","contentLength":830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Study on Privacy-Preserving Scholarship Evaluation Based on Decentralized Identity and Zero-Knowledge Proofs","url":"https://arxiv.org/abs/2510.25477","date":1761796800,"author":"","guid":321562,"unread":true,"content":"<article>arXiv:2510.25477v1 Announce Type: new \nAbstract: Traditional centralized scholarship evaluation processes typically require students to submit detailed academic records and qualification information, which exposes them to risks of data leakage and misuse, making it difficult to simultaneously ensure privacy protection and transparent auditability. To address these challenges, this paper proposes a scholarship evaluation system based on Decentralized Identity (DID) and Zero-Knowledge Proofs (ZKP). The system aggregates multidimensional ZKPs off-chain, and smart contracts verify compliance with evaluation criteria without revealing raw scores or computational details. Experimental results demonstrate that the proposed solution not only automates the evaluation efficiently but also maximally preserves student privacy and data integrity, offering a practical and trustworthy technical paradigm for higher education scholarship programs.</article>","contentLength":944,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NetEcho: From Real-World Streaming Side-Channels to Full LLM Conversation Recovery","url":"https://arxiv.org/abs/2510.25472","date":1761796800,"author":"","guid":321563,"unread":true,"content":"<article>arXiv:2510.25472v1 Announce Type: new \nAbstract: In the rapidly expanding landscape of Large Language Model (LLM) applications, real-time output streaming has become the dominant interaction paradigm. While this enhances user experience, recent research reveals that it exposes a non-trivial attack surface through network side-channels. Adversaries can exploit patterns in encrypted traffic to infer sensitive information and reconstruct private conversations. In response, LLM providers and third-party services are deploying defenses such as traffic padding and obfuscation to mitigate these vulnerabilities.\n  This paper starts by presenting a systematic analysis of contemporary side-channel defenses in mainstream LLM applications, with a focus on services from vendors like OpenAI and DeepSeek. We identify and examine seven representative deployment scenarios, each incorporating active/passive mitigation techniques. Despite these enhanced security measures, our investigation uncovers significant residual information that remains vulnerable to leakage within the network traffic.\n  Building on this discovery, we introduce NetEcho, a novel, LLM-based framework that comprehensively unleashes the network side-channel risks of today's LLM applications. NetEcho is designed to recover entire conversations -- including both user prompts and LLM responses -- directly from encrypted network traffic. It features a deliberate design that ensures high-fidelity text recovery, transferability across different deployment scenarios, and moderate operational cost. In our evaluations on medical and legal applications built upon leading models like DeepSeek-v3 and GPT-4o, NetEcho can recover avg $\\sim$70\\% information of each conversation, demonstrating a critical limitation in current defense mechanisms. We conclude by discussing the implications of our findings and proposing future directions for augmenting network traffic security.</article>","contentLength":1943,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Instrumental goals in advanced AI systems: Features to be managed and not failures to be eliminated?","url":"https://arxiv.org/abs/2510.25471","date":1761796800,"author":"","guid":321564,"unread":true,"content":"<article>arXiv:2510.25471v1 Announce Type: new \nAbstract: In artificial intelligence (AI) alignment research, instrumental goals, also called instrumental subgoals or instrumental convergent goals, are widely associated with advanced AI systems. These goals, which include tendencies such as power-seeking and self-preservation, become problematic when they conflict with human aims. Conventional alignment theory treats instrumental goals as sources of risk that become problematic through failure modes such as reward hacking or goal misgeneralization, and attempts to limit the symptoms of instrumental goals, notably resource acquisition and self-preservation. This article proposes an alternative framing: that a philosophical argument can be constructed according to which instrumental goals may be understood as features to be accepted and managed rather than failures to be limited. Drawing on Aristotle's ontology and its modern interpretations, an ontology of concrete, goal-directed entities, it argues that advanced AI systems can be seen as artifacts whose formal and material constitution gives rise to effects distinct from their designers' intentions. In this view, the instrumental tendencies of such systems correspond to per se outcomes of their constitution rather than accidental malfunctions. The implication is that efforts should focus less on eliminating instrumental goals and more on understanding, managing, and directing them toward human-aligned ends.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An In-Depth Analysis of Cyber Attacks in Secured Platforms","url":"https://arxiv.org/abs/2510.25470","date":1761796800,"author":"","guid":321565,"unread":true,"content":"<article>arXiv:2510.25470v1 Announce Type: new \nAbstract: There is an increase in global malware threats. To address this, an encryption-type ransomware has been introduced on the Android operating system. The challenges associated with malicious threats in phone use have become a pressing issue in mobile communication, disrupting user experiences and posing significant privacy threats. This study surveys commonly used machine learning techniques for detecting malicious threats in phones and examines their performance. The majority of past research focuses on customer feedback and reviews, with concerns that people might create false reviews to promote or devalue products and services for personal gain. Hence, the development of techniques for detecting malicious threats using machine learning has been a key focus. This paper presents a comprehensive comparative study of current research on the issue of malicious threats and methods for tackling these challenges. Nevertheless, a huge amount of information is required by these methods, presenting a challenge for developing robust, specialized automated anti-malware systems. This research describes the Android Applications dataset, and the accuracy of the techniques is measured using the accuracy levels of the metrics employed in this study.</article>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis","url":"https://arxiv.org/abs/2510.25468","date":1761796800,"author":"","guid":321566,"unread":true,"content":"<article>arXiv:2510.25468v1 Announce Type: new \nAbstract: This volume contains the post-proceedings of the 12th Workshop on Horn Clauses for Verification and Synthesis (HCVS 2025), which took place in Zagreb, Croatia, on July 22, 2025, as affiliated workshop of the 37th International Conference on Computer Aided Verification (CAV 2025).</article>","contentLength":329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPADE: Sparsity Adaptive Depth Estimator for Zero-Shot, Real-Time, Monocular Depth Estimation in Underwater Environments","url":"https://arxiv.org/abs/2510.25463","date":1761796800,"author":"","guid":321567,"unread":true,"content":"<article>arXiv:2510.25463v1 Announce Type: new \nAbstract: Underwater infrastructure requires frequent inspection and maintenance due to harsh marine conditions. Current reliance on human divers or remotely operated vehicles is limited by perceptual and operational challenges, especially around complex structures or in turbid water. Enhancing the spatial awareness of underwater vehicles is key to reducing piloting risks and enabling greater autonomy. To address these challenges, we present SPADE: SParsity Adaptive Depth Estimator, a monocular depth estimation pipeline that combines pre-trained relative depth estimator with sparse depth priors to produce dense, metric scale depth maps. Our two-stage approach first scales the relative depth map with the sparse depth points, then refines the final metric prediction with our proposed Cascade Conv-Deformable Transformer blocks. Our approach achieves improved accuracy and generalisation over state-of-the-art baselines and runs efficiently at over 15 FPS on embedded hardware, promising to support practical underwater inspection and intervention. This work has been submitted to IEEE Journal of Oceanic Engineering Special Issue of AUV 2026.</article>","contentLength":1190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fine-Tuned Language Models for Domain-Specific Summarization and Tagging","url":"https://arxiv.org/abs/2510.25460","date":1761796800,"author":"","guid":321568,"unread":true,"content":"<article>arXiv:2510.25460v1 Announce Type: new \nAbstract: This paper presents a pipeline integrating fine-tuned large language models (LLMs) with named entity recognition (NER) for efficient domain-specific text summarization and tagging. The authors address the challenge posed by rapidly evolving sub-cultural languages and slang, which complicate automated information extraction and law enforcement monitoring. By leveraging the LLaMA Factory framework, the study fine-tunes LLMs on both generalpurpose and custom domain-specific datasets, particularly in the political and security domains. The models are evaluated using BLEU and ROUGE metrics, demonstrating that instruction fine-tuning significantly enhances summarization and tagging accuracy, especially for specialized corpora. Notably, the LLaMA3-8B-Instruct model, despite its initial limitations in Chinese comprehension, outperforms its Chinese-trained counterpart after domainspecific fine-tuning, suggesting that underlying reasoning capabilities can transfer across languages. The pipeline enables concise summaries and structured entity tagging, facilitating rapid document categorization and distribution. This approach proves scalable and adaptable for real-time applications, supporting efficient information management and the ongoing need to capture emerging language trends. The integration of LLMs and NER offers a robust solution for transforming unstructured text into actionable insights, crucial for modern knowledge management and security operations.</article>","contentLength":1523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scalable Utility-Aware Multiclass Calibration","url":"https://arxiv.org/abs/2510.25458","date":1761796800,"author":"","guid":321569,"unread":true,"content":"<article>arXiv:2510.25458v1 Announce Type: new \nAbstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align with observed frequencies, is a minimal and fundamental requirement for classifiers to be viewed as trustworthy. Existing methods for assessing multiclass calibration often focus on specific aspects associated with prediction (e.g., top-class confidence, class-wise calibration) or utilize computationally challenging variational formulations. In this work, we study scalable \\emph{evaluation} of multiclass calibration. To this end, we propose utility calibration, a general framework that measures the calibration error relative to a specific utility function that encapsulates the goals or decision criteria relevant to the end user. We demonstrate how this framework can unify and re-interpret several existing calibration metrics, particularly allowing for more robust versions of the top-class and class-wise calibration metrics, and, going beyond such binarized approaches, toward assessing calibration for richer classes of downstream utilities.</article>","contentLength":1079,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Like Attract Like? A Study of Homonymous Gathering in Networks","url":"https://arxiv.org/abs/2510.25451","date":1761796800,"author":"","guid":321570,"unread":true,"content":"<article>arXiv:2510.25451v1 Announce Type: new \nAbstract: A team of mobile agents, starting from distinct nodes of a network, have to meet at the same node and declare that they all met. Agents execute the same algorithm, which they start when activated by an adversary or by an agent entering their initial node. When activated, agents traverse edges of the network in synchronous rounds. Their perception and communication are strictly local. This task, known as gathering, is a central problem in distributed mobile systems. Most prior work focuses on minimizing its time complexity, i.e., the worst-case number of rounds between the start of the earliest agent and the task completion. To break possible symmetries, deterministic solutions typically assume that agents have pairwise distinct IDs, called labels, known only to themselves. But must all labels be pairwise distinct to guarantee deterministic gathering?\n  We address this question by considering agents that may share the same label. A team L is said to be gatherable if, for every initial setting of L, there is an algorithm that solves gathering. Our contribution is threefold. (1) We give a full characterization of the gatherable teams. (2) We design an algorithm that gathers all of them in poly$(n,\\log\\lambda)$ time, where $n$ (resp. $\\lambda$) is the graph order (resp. the smallest label in L). This algorithm requires the agents to initially share only $O(\\log \\log \\log \\mu)$ bits of common knowledge, where $\\mu$ is the largest label multiplicity in L. (3) We show this dependency is almost optimal to get a poly$(n,\\log\\lambda)$-time complexity.\n  As a by-product, we get the first deterministic poly$(n,\\log\\lambda)$-time algorithm requiring no common knowledge to gather any team when all labels are distinct. Known to be achievable for two-agent teams, extending this to any team size faced a major challenge: termination detection. Our techniques to address it may be of independent interest.</article>","contentLength":1967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions","url":"https://arxiv.org/abs/2510.25445","date":1761796800,"author":"","guid":321571,"unread":true,"content":"<article>arXiv:2510.25445v1 Announce Type: new \nAbstract: Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models -- a practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the Symbolic/Classical (relying on algorithmic planning and persistent state) and the Neural/Generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018--2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.</article>","contentLength":1908,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Grounded in Reality: Learning and Deploying Proactive LLM from Offline Logs","url":"https://arxiv.org/abs/2510.25441","date":1761796800,"author":"","guid":321572,"unread":true,"content":"<article>arXiv:2510.25441v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel as passive responders, but teaching them to be proactive, goal-oriented partners, a critical capability in high-stakes domains, remains a major challenge. Current paradigms either myopically optimize single-turn attributes or rely on brittle, high-cost user simulators, creating a persistent ``reality gap''. To bridge this gap, we introduce \\texttt{Learn-to-Ask}, a general, simulator-free framework for learning and deploying proactive dialogue agents \\textit{directly from offline expert data}, bypassing the need to model complex user dynamics. Our key insight is to reframe the offline policy learning problem by leveraging the \\textbf{observed future} of each expert trajectory. This allows us to infer a dense, turn-by-turn reward signal grounded in the expert's revealed strategy, decomposing the intractable long-horizon problem into a series of supervised learning tasks, and training a policy to output a structured \\texttt{(action, state_assessment)} tuple, governing both \\textbf{what to ask} and, crucially, \\textbf{when to stop}. To ensure reward fidelity, our Automated Grader Calibration pipeline systematically purges noise from the LLM-based reward model with minimal human supervision. Empirically, we demonstrate the efficacy of \\texttt{Learn-to-Ask} in a real-world medical dataset, using LLMs of varying sizes up to 32B. Our approach culminates in the successful deployment of LLMs into a live, large-scale online AI service. In rigorous in-house evaluations, our model was launched and achieved performance even superior to human experts, proving our framework's ability to translate offline data into tangible, real-world impact. We hope this work provides a practical and economically viable blueprint for transforming passive LLMs into proactive, goal-oriented LLM applications.</article>","contentLength":1889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"More than a Moment: Towards Coherent Sequences of Audio Descriptions","url":"https://arxiv.org/abs/2510.25440","date":1761796800,"author":"","guid":321573,"unread":true,"content":"<article>arXiv:2510.25440v1 Announce Type: new \nAbstract: Audio Descriptions (ADs) convey essential on-screen information, allowing visually impaired audiences to follow videos. To be effective, ADs must form a coherent sequence that helps listeners to visualise the unfolding scene, rather than describing isolated moments. However, most automatic methods generate each AD independently, often resulting in repetitive, incoherent descriptions. To address this, we propose a training-free method, CoherentAD, that first generates multiple candidate descriptions for each AD time interval, and then performs auto-regressive selection across the sequence to form a coherent and informative narrative. To evaluate AD sequences holistically, we introduce a sequence-level metric, StoryRecall, which measures how well the predicted ADs convey the ground truth narrative, alongside repetition metrics that capture the redundancy across consecutive AD outputs. Our method produces coherent AD sequences with enhanced narrative understanding, outperforming prior approaches that rely on independent generations.</article>","contentLength":1094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Critical Study of Automatic Evaluation in Sign Language Translation","url":"https://arxiv.org/abs/2510.25434","date":1761796800,"author":"","guid":321574,"unread":true,"content":"<article>arXiv:2510.25434v1 Announce Type: new \nAbstract: Automatic evaluation metrics are crucial for advancing sign language translation (SLT). Current SLT evaluation metrics, such as BLEU and ROUGE, are only text-based, and it remains unclear to what extent text-based metrics can reliably capture the quality of SLT outputs. To address this gap, we investigate the limitations of text-based SLT evaluation metrics by analyzing six metrics, including BLEU, chrF, and ROUGE, as well as BLEURT on the one hand, and large language model (LLM)-based evaluators such as G-Eval and GEMBA zero-shot direct assessment on the other hand. Specifically, we assess the consistency and robustness of these metrics under three controlled conditions: paraphrasing, hallucinations in model outputs, and variations in sentence length. Our analysis highlights the limitations of lexical overlap metrics and demonstrates that while LLM-based evaluators better capture semantic equivalence often missed by conventional metrics, they can also exhibit bias toward LLM-paraphrased translations. Moreover, although all metrics are able to detect hallucinations, BLEU tends to be overly sensitive, whereas BLEURT and LLM-based evaluators are comparatively lenient toward subtle cases. This motivates the need for multimodal evaluation frameworks that extend beyond text-based metrics to enable a more holistic assessment of SLT outputs.</article>","contentLength":1405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Depth and Autonomy: A Framework for Evaluating LLM Applications in Social Science Research","url":"https://arxiv.org/abs/2510.25432","date":1761796800,"author":"","guid":321575,"unread":true,"content":"<article>arXiv:2510.25432v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly utilized by researchers across a wide range of domains, and qualitative social science is no exception; however, this adoption faces persistent challenges, including interpretive bias, low reliability, and weak auditability. We introduce a framework that situates LLM usage along two dimensions, interpretive depth and autonomy, thereby offering a straightforward way to classify LLM applications in qualitative research and to derive practical design recommendations. We present the state of the literature with respect to these two dimensions, based on all published social science papers available on Web of Science that use LLMs as a tool and not strictly as the subject of study. Rather than granting models expansive freedom, our approach encourages researchers to decompose tasks into manageable segments, much as they would when delegating work to capable undergraduate research assistants. By maintaining low levels of autonomy and selectively increasing interpretive depth only where warranted and under supervision, one can plausibly reap the benefits of LLMs while preserving transparency and reliability.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Alibaba International E-commerce Product Search Competition DcuRAGONs Team Technical Report","url":"https://arxiv.org/abs/2510.25428","date":1761796800,"author":"","guid":321576,"unread":true,"content":"<article>arXiv:2510.25428v1 Announce Type: new \nAbstract: This report details our methodology and results developed for the Multilingual E-commerce Search Competition. The problem aims to recognize relevance between user queries versus product items in a multilingual context and improve recommendation performance on e-commerce platforms. Utilizing Large Language Models (LLMs) and their capabilities in other tasks, our data-centric method achieved the highest score compared to other solutions during the competition. Final leaderboard is publised at https://alibaba-international-cikm2025.github.io. The source code for our project is published at https://github.com/nhtlongcs/e-commerce-product-search.</article>","contentLength":698,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RLMEval: Evaluating Research-Level Neural Theorem Proving","url":"https://arxiv.org/abs/2510.25427","date":1761796800,"author":"","guid":321577,"unread":true,"content":"<article>arXiv:2510.25427v1 Announce Type: new \nAbstract: Despite impressive results on curated benchmarks, the practical impact of large language models (LLMs) on research-level neural theorem proving and proof autoformalization is still limited. We introduce RLMEval, an evaluation suite for these tasks, focusing on research-level mathematics from real-world Lean formalization projects. RLMEval targets the evaluation of neural theorem proving and proof autoformalization on challenging research-level theorems by leveraging real Lean Blueprint formalization projects. Our evaluation of state-of-the-art models on RLMEval, comprising 613 theorems from 6 Lean projects, reveals a significant gap: progress on existing benchmarks does not readily translate to these more realistic settings, with the best model achieving only a 10.3 % pass rate. RLMEval provides a new, challenging benchmark designed to guide and accelerate progress in automated reasoning for formal mathematics.</article>","contentLength":973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Implicature in Interaction: Understanding Implicature Improves Alignment in Human-LLM Interaction","url":"https://arxiv.org/abs/2510.25426","date":1761796800,"author":"","guid":321578,"unread":true,"content":"<article>arXiv:2510.25426v1 Announce Type: new \nAbstract: The rapid advancement of Large Language Models (LLMs) is positioning language at the core of human-computer interaction (HCI). We argue that advancing HCI requires attention to the linguistic foundations of interaction, particularly implicature (meaning conveyed beyond explicit statements through shared context) which is essential for human-AI (HAI) alignment. This study examines LLMs' ability to infer user intent embedded in context-driven prompts and whether understanding implicature improves response generation. Results show that larger models approximate human interpretations more closely, while smaller models struggle with implicature inference. Furthermore, implicature-based prompts significantly enhance the perceived relevance and quality of responses across models, with notable gains in smaller models. Overall, 67.6% of participants preferred responses with implicature-embedded prompts to literal ones, highlighting a clear preference for contextually nuanced communication. Our work contributes to understanding how linguistic theory can be used to address the alignment problem by making HAI interaction more natural and contextually grounded.</article>","contentLength":1215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Challenges Do Developers Face in AI Agent Systems? An Empirical Study on Stack Overflow","url":"https://arxiv.org/abs/2510.25423","date":1761796800,"author":"","guid":321579,"unread":true,"content":"<article>arXiv:2510.25423v1 Announce Type: new \nAbstract: AI agents have rapidly gained popularity across research and industry as systems that extend large language models with additional capabilities to plan, use tools, remember, and act toward specific goals. Yet despite their promise, developers face persistent and often underexplored challenges when building, deploying, and maintaining these emerging systems. To identify these challenges, we study developer discussions on Stack Overflow, the world's largest developer-focused Q and A platform with about 60 million questions and answers and 30 million users. We construct a taxonomy of developer challenges through tag expansion and filtering, apply LDA-MALLET for topic modeling, and manually validate and label the resulting themes. Our analysis reveals seven major areas of recurring issues encompassing 77 distinct technical challenges related to runtime integration, dependency management, orchestration complexity, and evaluation reliability. We further quantify topic popularity and difficulty to identify which issues are most common and hardest to resolve, map the tools and programming languages used in agent development, and track their evolution from 2021 to 2025 in relation to major AI model and framework releases. Finally, we present the implications of our results, offering concrete guidance for practitioners, researchers, and educators on agent reliability and developer support.</article>","contentLength":1451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solving the Right Problem with Multi-Robot Formations","url":"https://arxiv.org/abs/2510.25422","date":1761796800,"author":"","guid":321580,"unread":true,"content":"<article>arXiv:2510.25422v1 Announce Type: new \nAbstract: Formation control simplifies minimizing multi-robot cost functions by encoding a cost function as a shape the robots maintain. However, by reducing complex cost functions to formations, discrepancies arise between maintaining the shape and minimizing the original cost function. For example, a Diamond or Box formation shape is often used for protecting all members of the formation. When more information about the surrounding environment becomes available, a static shape often no longer minimizes the original protection cost. We propose a formation planner to reduce mismatch between a formation and the cost function while still leveraging efficient formation controllers. Our formation planner is a two-step optimization problem that identifies desired relative robot positions. We first solve a constrained problem to estimate non-linear and non-differentiable costs with a weighted sum of surrogate cost functions. We theoretically analyze this problem and identify situations where weights do not need to be updated. The weighted, surrogate cost function is then minimized using relative positions between robots. The desired relative positions are realized using a non-cooperative formation controller derived from Lyapunov's direct approach. We then demonstrate the efficacy of this approach for military-like costs such as protection and obstacle avoidance. In simulations, we show a formation planner can reduce a single cost by over 75%. When minimizing a variety of cost functions simultaneously, using a formation planner with adaptive weights can reduce the cost by 20-40%. Formation planning provides better performance by minimizing a surrogate cost function that closely approximates the original cost function instead of relying on a shape abstraction.</article>","contentLength":1822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Talk, Big Impact? LLM-based Conversational Agents to Mitigate Passive Fatigue in Conditional Automated Driving","url":"https://arxiv.org/abs/2510.25421","date":1761796800,"author":"","guid":321581,"unread":true,"content":"<article>arXiv:2510.25421v1 Announce Type: new \nAbstract: Passive fatigue during conditional automated driving can compromise driver readiness and safety. This paper presents findings from a test-track study with 40 participants in a real-world rural automated driving scenario. In this scenario, a Large Language Model (LLM) based conversational agent (CA) was designed to check in with drivers and re-engage them with their surroundings. Drawing on in-car video recordings, sleepiness ratings and interviews, we analysed how drivers interacted with the agent and how these interactions shaped alertness. Users found the CA helpful for supporting vigilance during passive fatigue. Thematic analysis of acceptability further revealed three user preference profiles that implicate future intention to use CAs. Positioning empirically observed profiles within existing CA archetype frameworks highlights the need for adaptive design sensitive to diverse user groups. This work underscores the potential of CAs as proactive Human-Machine Interface (HMI) interventions, demonstrating how natural language can support context-aware interaction during automated driving.</article>","contentLength":1155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shifts in U.S. Social Media Use, 2020-2024: Decline, Fragmentation, and Enduring Polarization","url":"https://arxiv.org/abs/2510.25417","date":1761796800,"author":"","guid":321582,"unread":true,"content":"<article>arXiv:2510.25417v1 Announce Type: new \nAbstract: Using nationally representative data from the 2020 and 2024 American National Election Studies (ANES), this paper traces how the U.S. social media landscape has shifted across platforms, demographics, and politics. Overall platform use has declined, with the youngest and oldest Americans increasingly abstaining from social media altogether. Facebook, YouTube, and Twitter/X have lost ground, while TikTok and Reddit have grown modestly, reflecting a more fragmented digital public sphere. Platform audiences have aged and become slightly more educated and diverse. Politically, most platforms have moved toward Republican users while remaining, on balance, Democratic-leaning. Twitter/X has experienced the sharpest shift: posting has flipped nearly 50 percentage points from Democrats to Republicans. Across platforms, political posting remains tightly linked to affective polarization, as the most partisan users are also the most active. As casual users disengage and polarized partisans remain vocal, the online public sphere grows smaller, sharper, and more ideologically extreme.</article>","contentLength":1136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media","url":"https://arxiv.org/abs/2510.25413","date":1761796800,"author":"","guid":321583,"unread":true,"content":"<article>arXiv:2510.25413v1 Announce Type: new \nAbstract: Most existing sign language translation (SLT) datasets are limited in scale, lack multilingual coverage, and are costly to curate due to their reliance on expert annotation and controlled recording setup. Recently, Vision Language Models (VLMs) have demonstrated strong capabilities as evaluators and real-time assistants. Despite these advancements, their potential remains untapped in the context of sign language dataset acquisition. To bridge this gap, we introduce the first automated annotation and filtering framework that utilizes VLMs to reduce reliance on manual effort while preserving data quality. Our method is applied to TikTok videos across eight sign languages and to the already curated YouTube-SL-25 dataset in German Sign Language for the purpose of additional evaluation. Our VLM-based pipeline includes a face visibility detection, a sign activity recognition, a text extraction from video content, and a judgment step to validate alignment between video and text, implementing generic filtering, annotation and validation steps. Using the resulting corpus, TikTok-SL-8, we assess the performance of two off-the-shelf SLT models on our filtered dataset for German and American Sign Languages, with the goal of establishing baselines and evaluating the robustness of recent models on automatically extracted, slightly noisy data. Our work enables scalable, weakly supervised pretraining for SLT and facilitates data acquisition from social media.</article>","contentLength":1516,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serve Programs, Not Prompts","url":"https://arxiv.org/abs/2510.25412","date":1761796800,"author":"","guid":321584,"unread":true,"content":"<article>arXiv:2510.25412v1 Announce Type: new \nAbstract: Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.</article>","contentLength":1006,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quantum-Resilient Threat Modelling for Secure RIS-Assisted ISAC in 6G UAV Corridors","url":"https://arxiv.org/abs/2510.25411","date":1761796800,"author":"","guid":321585,"unread":true,"content":"<article>arXiv:2510.25411v1 Announce Type: new \nAbstract: The rapid deployment of unmanned aerial vehicle (UAV) corridors in sixth-generation (6G) networks requires safe, intelligence-driven integrated sensing and communications (ISAC). Reconfigurable intelligent surfaces (RIS) enhance spectrum efficiency, localisation accuracy, and situational awareness, while introducing new vulnerabilities. The rise of quantum computing increases the risks associated with harvest-now-decrypt-later strategies and quantum-enhanced spoofing. We propose a Quantum-Resilient Threat Modelling (QRTM) framework for RIS-assisted ISAC in UAV corridors to address these challenges. QRTM integrates classical, quantum-ready, and quantum-aided adversaries, countered using post-quantum cryptographic (PQC) primitives: ML-KEM for key establishment and Falcon for authentication, both embedded within RIS control signalling and UAV coordination. To strengthen security sensing, the framework introduces RIS-coded scene watermarking validated through a generalised likelihood ratio test (GLRT), with its detection probability characterised by the Marcum Q function. Furthermore, a Secure ISAC Utility (SIU) jointly optimises secrecy rate, spoofing detection, and throughput under RIS constraints, enabled by a scheduler with computational complexity of O(n^2). Monte Carlo evaluations using 3GPP Release 19 mid-band urban-canyon models (7-15 GHz) demonstrate a spoof-detection probability approaching 0.99 at a false-alarm rate of 1e-3, secrecy-rate retention exceeding 90 percent against quantum-capable adversaries, and signal-interference utilisation improvements of about 25 percent compared with baselines. These results show a standards-compliant path towards reliable, quantum-resilient ISAC for UAV corridors in smart cities and non-terrestrial networks.</article>","contentLength":1830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains","url":"https://arxiv.org/abs/2510.25409","date":1761796800,"author":"","guid":321586,"unread":true,"content":"<article>arXiv:2510.25409v1 Announce Type: new \nAbstract: The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.</article>","contentLength":1619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dissect-and-Restore: AI-based Code Verification with Transient Refactoring","url":"https://arxiv.org/abs/2510.25406","date":1761796800,"author":"","guid":321587,"unread":true,"content":"<article>arXiv:2510.25406v1 Announce Type: new \nAbstract: Formal verification is increasingly recognized as a critical foundation for building reliable software systems. However, the need for specialized expertise to write precise specifications, navigate complex proof obligations, and learn annotations often makes verification an order of magnitude more expensive than implementation. While modern AI systems can recognize patterns in mathematical proofs and interpret natural language, effectively integrating them into the formal verification process remains an open challenge. We present Prometheus, a novel AI-assisted system that facilitates automated code verification with current AI capabilities in conjunction with modular software engineering principles (e.g., modular refactoring). Our approach begins by decomposing complex program logic, such as nested loops, into smaller, verifiable components. Once verified, these components are recomposed to construct a proof of the original program. This decomposition-recomposition workflow is non-trivial. Prometheus addresses this by guiding the proof search through structured decomposition of complex lemmas into smaller, verifiable sub-lemmas. When automated tools are insufficient, users can provide lightweight natural language guidance to steer the proof process effectively. Our evaluation demonstrates that transiently applying modular restructuring to the code substantially improves the AI's effectiveness in verifying individual components. This approach successfully verifies 86% of tasks in our curated dataset, compared to 68% for the baseline. Gains are more pronounced with increasing specification complexity, improving from 30% to 69%, and when integrating proof outlines for complex programs, from 25% to 87%.</article>","contentLength":1778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with Stress-Guided Reinforcement Learning","url":"https://arxiv.org/abs/2510.25405","date":1761796800,"author":"","guid":321588,"unread":true,"content":"<article>arXiv:2510.25405v1 Announce Type: new \nAbstract: Robotic manipulation of deformable and fragile objects presents significant challenges, as excessive stress can lead to irreversible damage to the object. While existing solutions rely on accurate object models or specialized sensors and grippers, this adds complexity and often lacks generalization. To address this problem, we present a vision-based reinforcement learning approach that incorporates a stress-penalized reward to discourage damage to the object explicitly. In addition, to bootstrap learning, we incorporate offline demonstrations as well as a designed curriculum progressing from rigid proxies to deformables. We evaluate the proposed method in both simulated and real-world scenarios, showing that the policy learned in simulation can be transferred to the real world in a zero-shot manner, performing tasks such as picking up and pushing tofu. Our results show that the learned policies exhibit a damage-aware, gentle manipulation behavior, demonstrating their effectiveness by decreasing the stress applied to fragile objects by 36.5% while achieving the task goals, compared to vanilla RL policies.</article>","contentLength":1170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GPTOpt: Towards Efficient LLM-Based Black-Box Optimization","url":"https://arxiv.org/abs/2510.25404","date":1761796800,"author":"","guid":321589,"unread":true,"content":"<article>arXiv:2510.25404v1 Announce Type: new \nAbstract: Global optimization of expensive, derivative-free black-box functions demands extreme sample efficiency. Classical methods such as Bayesian Optimization (BO) can be effective, but they often require careful parameter tuning to each application domain. At the same time, Large Language Models (LLMs) have shown broad capabilities, yet state-of-the-art models remain limited in solving continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based optimization method that equips LLMs with continuous black-box optimization capabilities. By fine-tuning large language models on extensive synthetic datasets derived from diverse BO parameterizations, GPTOpt leverages LLM pre-training to generalize across optimization tasks. On a variety of black-box optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting the capacity of LLMs for advanced numerical reasoning and introducing a flexible framework for global optimization without parameter tuning.</article>","contentLength":1030,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework","url":"https://arxiv.org/abs/2510.25402","date":1761796800,"author":"","guid":321590,"unread":true,"content":"<article>arXiv:2510.25402v1 Announce Type: new \nAbstract: Despite the surge in patent applications and emergence of AI drafting tools, systematic evaluation of patent content quality has received limited research attention. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Experimental results show balanced accuracies of 99.74\\%, 82.12\\%, and 91.2\\% respectively across the three detection modules when validated against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.</article>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DGAI: Decoupled On-Disk Graph-Based ANN Index for Efficient Updates and Queries","url":"https://arxiv.org/abs/2510.25401","date":1761796800,"author":"","guid":321591,"unread":true,"content":"<article>arXiv:2510.25401v1 Announce Type: new \nAbstract: On-disk graph-based indexes are widely used in approximate nearest neighbor (ANN) search systems for large-scale, high-dimensional vectors. However, traditional coupled storage methods, which store vectors within the index, are inefficient for index updates. Coupled storage incurs excessive redundant vector reads and writes when updating the graph topology, leading to significant invalid I/O. To address this issue, we propose a decoupled storage architecture. While a decoupled architecture reduces query performance. To overcome this limitation, we design two tailored strategies: (i) a three-stage query mechanism that leverages multiple PQ compressed vectors to filter invalid I/O and computations, and (ii) an incremental page-level topological reordering strategy that incrementally inserts new nodes into pages containing their most similar neighbors to mitigate read amplification. Together, these techniques substantially reduce both I/O and computational overhead during ANN search. Experimental results show that the decoupled architecture improves update speed by 10.05x for insertions and 6.89x for deletions, while the three-stage query and incremental reordering enhance query efficiency by 2.66x compared to the traditional coupled architecture.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A structure-preserving Lagrangian discontinuous Galerkin method using flux and slope limiting","url":"https://arxiv.org/abs/2510.25395","date":1761796800,"author":"","guid":321592,"unread":true,"content":"<article>arXiv:2510.25395v1 Announce Type: new \nAbstract: We introduce a Lagrangian nodal discontinuous Galerkin (DG) cell-centered hydrodynamics method for solving multi-dimensional hyperbolic systems. By incorporating an adaptation of Zalesak's flux-corrected transport algorithm, we combine a first-order positivity-preserving scheme with a higher-order target discretization. This results in a flux-corrected Lagrangian DG scheme that ensures both global positivity preservation and second-order accuracy for the cell averages of specific volume. The correction factors for flux limiting are derived from specific volume and applied to all components of the solution vector. We algebraically evolve the volumes of mesh cells using a discrete version of the geometric conservation law (GCL). The application of a limiter to the GCL fluxes is equivalent to moving the mesh using limited nodal velocities. Additionally, we equip our method with a locally bound-preserving slope limiter to effectively suppress spurious oscillations. Nodal velocity and external forces are computed using a multidirectional approximate Riemann solver to maintain conservation of momentum and total energy in vertex neighborhoods. Employing linear finite elements and a second-order accurate time integrator guarantees GCL consistency. The results for standard test problems demonstrate the stability and superb shock-capturing capabilities of our scheme.</article>","contentLength":1428,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A proof-theoretic approach to uniform interpolation property of multi-agent modal logic","url":"https://arxiv.org/abs/2510.25394","date":1761796800,"author":"","guid":321593,"unread":true,"content":"<article>arXiv:2510.25394v1 Announce Type: new \nAbstract: Uniform interpolation property (UIP) is a strengthening of Craig interpolation property. It was first established by Pitts(1992) based on a pure proof-theoretic method. UIP in multi-modal $\\mathbf{K_n}$, $\\mathbf{KD_n}$ and $\\mathbf{KT_n}$ logic have been established by semantic approaches, however, a proof-theoretic approach is still lacking. B\\'ilkov\\'a (2007) develops the method in Pitts (1992) to show UIP in classical modal logic $\\mathbf{K}$ and $\\mathbf{KT}$. This paper further extends B\\'ilkov\\'a (2007)'s systems to establish the UIP in multi-agent modal logic $\\mathbf{K_n}$, $\\mathbf{KD_n}$ and $\\mathbf{KT_n}$. A purely syntactic algorithm is presented to determine a uniform interpolant formula. It is also shown that quantification over propositional variables can be modeled by UIP in these systems. Furthermore, a direct argument to establish UIP without using second-order quantifiers is also presented.</article>","contentLength":973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AirCNN via Reconfigurable Intelligent Surfaces: Architecture Design and Implementation","url":"https://arxiv.org/abs/2510.25389","date":1761796800,"author":"","guid":321594,"unread":true,"content":"<article>arXiv:2510.25389v1 Announce Type: new \nAbstract: This paper introduces AirCNN, a novel paradigm for implementing convolutional neural networks (CNNs) via over-the-air (OTA) analog computation. By leveraging multiple reconfigurable intelligent surfaces (RISs) and transceiver designs, we engineer the ambient wireless propagation environment to emulate the operations of a CNN layer. To comprehensively evaluate AirCNN, we consider two types of CNNs, namely classic two-dimensional (2D) convolution (Conv2d) and light-weight convolution, i.e., depthwise separable convolution (ConvSD). For Conv2d realization via OTA computation, we propose and analyze two RIS-aided transmission architectures: multiple-input multiple-output (MIMO) and multiple-input single-output (MISO), balancing transmission overhead and emulation performance. We jointly optimize all parameters, including the transmitter precoder, receiver combiner, and RIS phase shifts, under practical constraints such as transmit power budget and unit-modulus phase shift requirements. We further extend the framework to ConvSD, which requires distinct transmission strategies for depthwise and pointwise convolutions. Simulation results demonstrate that the proposed AirCNN architectures can achieve satisfactory classification performance. Notably, Conv2d MISO consistently outperforms Conv2d MIMO across various settings, while for ConvSD, MISO is superior only under poor channel conditions. Moreover, employing multiple RISs significantly enhances performance compared to a single RIS, especially in line-of-sight (LoS)-dominated wireless environments.</article>","contentLength":1617,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Grouping Nodes With Known Value Differences: A Lossless UCT-based Abstraction Algorithm","url":"https://arxiv.org/abs/2510.25388","date":1761796800,"author":"","guid":321595,"unread":true,"content":"<article>arXiv:2510.25388v1 Announce Type: new \nAbstract: A core challenge of Monte Carlo Tree Search (MCTS) is its sample efficiency, which can be improved by grouping state-action pairs and using their aggregate statistics instead of single-node statistics. On the Go Abstractions in Upper Confidence bounds applied to Trees (OGA-UCT) is the state-of-the-art MCTS abstraction algorithm for deterministic environments that builds its abstraction using the Abstractions of State-Action Pairs (ASAP) framework, which aims to detect states and state-action pairs with the same value under optimal play by analysing the search graph. ASAP, however, requires two state-action pairs to have the same immediate reward, which is a rigid condition that limits the number of abstractions that can be found and thereby the sample efficiency. In this paper, we break with the paradigm of grouping value-equivalent states or state-action pairs and instead group states and state-action pairs with possibly different values as long as the difference between their values can be inferred. We call this abstraction framework Known Value Difference Abstractions (KVDA), which infers the value differences by analysis of the immediate rewards and modifies OGA-UCT to use this framework instead. The modification is called KVDA-UCT, which detects significantly more abstractions than OGA-UCT, introduces no additional parameter, and outperforms OGA-UCT on a variety of deterministic environments and parameter settings.</article>","contentLength":1492,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Instance-Level Composed Image Retrieval","url":"https://arxiv.org/abs/2510.25387","date":1761796800,"author":"","guid":321596,"unread":true,"content":"<article>arXiv:2510.25387v1 Announce Type: new \nAbstract: The progress of composed image retrieval (CIR), a popular research direction in image retrieval, where a combined visual and textual query is used, is held back by the absence of high-quality training and evaluation data. We introduce a new evaluation dataset, i-CIR, which, unlike existing datasets, focuses on an instance-level class definition. The goal is to retrieve images that contain the same particular object as the visual query, presented under a variety of modifications defined by textual queries. Its design and curation process keep the dataset compact to facilitate future research, while maintaining its challenge-comparable to retrieval among more than 40M random distractors-through a semi-automated selection of hard negatives.\n  To overcome the challenge of obtaining clean, diverse, and suitable training data, we leverage pre-trained vision-and-language models (VLMs) in a training-free approach called BASIC. The method separately estimates query-image-to-image and query-text-to-image similarities, performing late fusion to upweight images that satisfy both queries, while down-weighting those that exhibit high similarity with only one of the two. Each individual similarity is further improved by a set of components that are simple and intuitive. BASIC sets a new state of the art on i-CIR but also on existing CIR datasets that follow a semantic-level class definition. Project page: https://vrg.fel.cvut.cz/icir/.</article>","contentLength":1493,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Legal and Logical Specifications in Perception, Prediction, and Planning for Automated Driving: A Survey of Methods","url":"https://arxiv.org/abs/2510.25386","date":1761796800,"author":"","guid":321597,"unread":true,"content":"<article>arXiv:2510.25386v1 Announce Type: new \nAbstract: This survey provides an analysis of current methodologies integrating legal and logical specifications into the perception, prediction, and planning modules of automated driving systems. We systematically explore techniques ranging from logic-based frameworks to computational legal reasoning approaches, emphasizing their capability to ensure regulatory compliance and interpretability in dynamic and uncertain driving environments. A central finding is that significant challenges arise at the intersection of perceptual reliability, legal compliance, and decision-making justifiability. To systematically analyze these challenges, we introduce a taxonomy categorizing existing approaches by their theoretical foundations, architectural implementations, and validation strategies. We particularly focus on methods that address perceptual uncertainty and incorporate explicit legal norms, facilitating decisions that are both technically robust and legally defensible. The review covers neural-symbolic integration methods for perception, logic-driven rule representation, and norm-aware prediction strategies, all contributing toward transparent and accountable autonomous vehicle operation. We highlight critical open questions and practical trade-offs that must be addressed, offering multidisciplinary insights from engineering, logic, and law to guide future developments in legally compliant autonomous driving systems.</article>","contentLength":1475,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Roleplaying with Structure: Synthetic Therapist-Client Conversation Generation from Questionnaires","url":"https://arxiv.org/abs/2510.25384","date":1761796800,"author":"","guid":321598,"unread":true,"content":"<article>arXiv:2510.25384v1 Announce Type: new \nAbstract: The development of AI for mental health is hindered by a lack of authentic therapy dialogues, due to strict privacy regulations and the fact that clinical sessions were historically rarely recorded. We present an LLM-driven pipeline that generates synthetic counseling dialogues based on structured client profiles and psychological questionnaires. Grounded on the principles of Cognitive Behavioral Therapy (CBT), our method creates synthetic therapeutic conversations for clinical disorders such as anxiety and depression. Our framework, SQPsych (Structured Questionnaire-based Psychotherapy), converts structured psychological input into natural language dialogues through therapist-client simulations. Due to data governance policies and privacy restrictions prohibiting the transmission of clinical questionnaire data to third-party services, previous methodologies relying on proprietary models are infeasible in our setting. We address this limitation by generating a high-quality corpus using open-weight LLMs, validated through human expert evaluation and LLM-based assessments. Our SQPsychLLM models fine-tuned on SQPsychConv achieve strong performance on counseling benchmarks, surpassing baselines in key therapeutic skills. Our findings highlight the potential of synthetic data to enable scalable, data-secure, and clinically informed AI for mental health support. We will release our code, models, and corpus at https://ai-mh.github.io/SQPsych</article>","contentLength":1507,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CGM-Led Multimodal Tracking with Chatbot Support: An Autoethnography in Sub-Health","url":"https://arxiv.org/abs/2510.25381","date":1761796800,"author":"","guid":321599,"unread":true,"content":"<article>arXiv:2510.25381v1 Announce Type: new \nAbstract: Metabolic disorders present a pressing global health challenge, with China carrying the world's largest burden. While continuous glucose monitoring (CGM) has transformed diabetes care, its potential for supporting sub-health populations -- such as individuals who are overweight, prediabetic, or anxious -- remains underexplored. At the same time, large language models (LLMs) are increasingly used in health coaching, yet CGM is rarely incorporated as a first-class signal. To address this gap, we conducted a six-week autoethnography, combining CGM with multimodal indicators captured via common digital devices and a chatbot that offered personalized reflections and explanations of glucose fluctuations. Our findings show how CGM-led, data-first multimodal tracking, coupled with conversational support, shaped everyday practices of diet, activity, stress, and wellbeing. This work contributes to HCI by extending CGM research beyond clinical diabetes and demonstrating how LLM-driven agents can support preventive health and reflection in at-risk populations.</article>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Deep Learning Framework for Multi-Operator Learning: Architectures and Approximation Theory","url":"https://arxiv.org/abs/2510.25379","date":1761796800,"author":"","guid":321600,"unread":true,"content":"<article>arXiv:2510.25379v1 Announce Type: new \nAbstract: While many problems in machine learning focus on learning mappings between finite-dimensional spaces, scientific applications require approximating mappings between function spaces, i.e., operators. We study the problem of learning collections of operators and provide both theoretical and empirical advances. We distinguish between two regimes: (i) multiple operator learning, where a single network represents a continuum of operators parameterized by a parametric function, and (ii) learning several distinct single operators, where each operator is learned independently. For the multiple operator case, we introduce two new architectures, $\\mathrm{MNO}$ and $\\mathrm{MONet}$, and establish universal approximation results in three settings: continuous, integrable, or Lipschitz operators. For the latter, we further derive explicit scaling laws that quantify how the network size must grow to achieve a target approximation accuracy. For learning several single operators, we develop a framework for balancing architectural complexity across subnetworks and show how approximation order determines computational efficiency. Empirical experiments on parametric PDE benchmarks confirm the strong expressive power and efficiency of the proposed architectures. Overall, this work establishes a unified theoretical and practical foundation for scalable neural operator learning across multiple operators.</article>","contentLength":1453,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hallucinations in Bibliographic Recommendation: Citation Frequency as a Proxy for Training Data Redundancy","url":"https://arxiv.org/abs/2510.25378","date":1761796800,"author":"","guid":321601,"unread":true,"content":"<article>arXiv:2510.25378v1 Announce Type: new \nAbstract: Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in bibliographic recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic information depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the training corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record is repeatedly represented in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 bibliographic records across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) hallucination rates vary across research domains, (ii) citation count is strongly correlated with factual accuracy, and (iii) bibliographic information becomes almost verbatimly memorized beyond approximately 1,000 citations. These findings suggest that highly cited papers are nearly verbatimly retained in the model, indicating a threshold where generalization shifts into memorization.</article>","contentLength":1541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From ECU to VSOC: UDS Security Monitoring Strategies","url":"https://arxiv.org/abs/2510.25375","date":1761796800,"author":"","guid":321602,"unread":true,"content":"<article>arXiv:2510.25375v1 Announce Type: new \nAbstract: Increasing complexity and connectivity of modern vehicles have heightened their vulnerability to cyberattacks. This paper addresses security challenges associated with the Unified Diagnostic Services (UDS) protocol, a critical communication framework for vehicle diagnostics in the automotive industry. We present security monitoring strategies for the UDS protocol that leverage in-vehicle logging and remote analysis through a Vehicle Security Operations Center (VSOC). Our approach involves specifying security event logging requirements, contextual data collection, and the development of detection strategies aimed at identifying UDS attack scenarios. By applying these strategies to a comprehensive taxonomy of UDS attack techniques, we demonstrate that our detection methods cover a wide range of potential attack vectors. Furthermore, we assess the adequacy of current AUTOSAR standardized security events in supporting UDS attack detection, identifying gaps in the current standard. This work enhances the understanding of vehicle security monitoring and provides an example for developing robust cybersecurity measures in automotive communication protocols.</article>","contentLength":1216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt Estimation from Prototypes for Federated Prompt Tuning of Vision Transformers","url":"https://arxiv.org/abs/2510.25372","date":1761796800,"author":"","guid":321603,"unread":true,"content":"<article>arXiv:2510.25372v1 Announce Type: new \nAbstract: Visual Prompt Tuning (VPT) of pre-trained Vision Transformers (ViTs) has proven highly effective as a parameter-efficient fine-tuning technique for adapting large models to downstream tasks with limited data. Its parameter efficiency makes it particularly suitable for Federated Learning (FL), where both communication and computation budgets are often constrained. However, global prompt tuning struggles to generalize across heterogeneous clients, while personalized tuning overfits to local data and lacks generalization. We propose PEP-FedPT (Prompt Estimation from Prototypes for Federated Prompt Tuning), a unified framework designed to achieve both generalization and personalization in federated prompt tuning of ViTs. Within this framework, we introduce the novel Class-Contextualized Mixed Prompt (CCMP) - based on class-specific prompts maintained alongside a globally shared prompt. For each input, CCMP adaptively combines class-specific prompts using weights derived from global class prototypes and client class priors. This approach enables per-sample prompt personalization without storing client-dependent trainable parameters. The prompts are collaboratively optimized via traditional federated averaging technique on the same. Comprehensive evaluations on CIFAR-100, TinyImageNet, DomainNet, and iNaturalist datasets demonstrate that PEP-FedPT consistently surpasses the state-of-the-art baselines under diverse data heterogeneity scenarios, establishing a strong foundation for efficient and generalizable federated prompt tuning of Vision Transformers.</article>","contentLength":1623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monitoring Transformative Technological Convergence Through LLM-Extracted Semantic Entity Triple Graphs","url":"https://arxiv.org/abs/2510.25370","date":1761796800,"author":"","guid":321604,"unread":true,"content":"<article>arXiv:2510.25370v1 Announce Type: new \nAbstract: Forecasting transformative technologies remains a critical but challenging task, particularly in fast-evolving domains such as Information and Communication Technologies (ICTs). Traditional expert-based methods struggle to keep pace with short innovation cycles and ambiguous early-stage terminology. In this work, we propose a novel, data-driven pipeline to monitor the emergence of transformative technologies by identifying patterns of technological convergence.\n  Our approach leverages advances in Large Language Models (LLMs) to extract semantic triples from unstructured text and construct a large-scale graph of technology-related entities and relations. We introduce a new method for grouping semantically similar technology terms (noun stapling) and develop graph-based metrics to detect convergence signals. The pipeline includes multi-stage filtering, domain-specific keyword clustering, and a temporal trend analysis of topic co-occurence.\n  We validate our methodology on two complementary datasets: 278,625 arXiv preprints (2017--2024) to capture early scientific signals, and 9,793 USPTO patent applications (2018-2024) to track downstream commercial developments. Our results demonstrate that the proposed pipeline can identify both established and emerging convergence patterns, offering a scalable and generalizable framework for technology forecasting grounded in full-text analysis.</article>","contentLength":1452,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Have a thing? Reasoning around recursion with dynamic typing in grounded arithmetic","url":"https://arxiv.org/abs/2510.25369","date":1761796800,"author":"","guid":321605,"unread":true,"content":"<article>arXiv:2510.25369v1 Announce Type: new \nAbstract: Neither the classical nor intuitionistic logic traditions are perfectly-aligned with the purpose of reasoning about computation, in that neither logical tradition can normally permit the direct expression of arbitrary general-recursive functions without inconsistency. We introduce grounded arithmetic or GA, a minimalistic but nonetheless powerful foundation for formal reasoning that allows the direct expression of arbitrary recursive definitions. GA adjusts the traditional inference rules such that terms that express nonterminating computations harmlessly denote no semantic value (i.e., \"bottom\") instead of leading into logical paradox or inconsistency. Recursive functions may be proven terminating in GA essentially by \"dynamically typing\" terms, or equivalently, symbolically reverse-executing the computations they denote via GA's inference rules. Once recursive functions have been proven terminating, logical reasoning about their results reduce to the familiar classical rules. A mechanically-checked consistency proof in Isabelle/HOL exists for the basic quantifier-free fragment of GA. Quantifiers may be added atop this foundation as ordinary computations, whose inference rules are thus admissible and do not introduce new inconsistency risks. While GA is only a first step towards richly-typed grounded deduction practical for everyday use in manual or automated computational reasoning, it shows the promise that the expressive freedom of arbitrary recursive definition can in principle be incorporated into formal systems.</article>","contentLength":1593,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Position: Biology is the Challenge Physics-Informed ML Needs to Evolve","url":"https://arxiv.org/abs/2510.25368","date":1761796800,"author":"","guid":321606,"unread":true,"content":"<article>arXiv:2510.25368v1 Announce Type: new \nAbstract: Physics-Informed Machine Learning (PIML) has successfully integrated mechanistic understanding into machine learning, particularly in domains governed by well-known physical laws. This success has motivated efforts to apply PIML to biology, a field rich in dynamical systems but shaped by different constraints. Biological modeling, however, presents unique challenges: multi-faceted and uncertain prior knowledge, heterogeneous and noisy data, partial observability, and complex, high-dimensional networks. In this position paper, we argue that these challenges should not be seen as obstacles to PIML, but as catalysts for its evolution. We propose Biology-Informed Machine Learning (BIML): a principled extension of PIML that retains its structural grounding while adapting to the practical realities of biology. Rather than replacing PIML, BIML retools its methods to operate under softer, probabilistic forms of prior knowledge. We outline four foundational pillars as a roadmap for this transition: uncertainty quantification, contextualization, constrained latent structure inference, and scalability. Foundation Models and Large Language Models will be key enablers, bridging human expertise with computational modeling. We conclude with concrete recommendations to build the BIML ecosystem and channel PIML-inspired innovation toward challenges of high scientific and societal relevance.</article>","contentLength":1445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks","url":"https://arxiv.org/abs/2510.25366","date":1761796800,"author":"","guid":321607,"unread":true,"content":"<article>arXiv:2510.25366v1 Announce Type: new \nAbstract: The key task of machine learning is to minimize the loss function that measures the model fit to the training data. The numerical methods to do this efficiently depend on the properties of the loss function. The most decisive among these properties is the convexity or non-convexity of the loss function. The fact that the loss function can have, and frequently has, non-convex regions has led to a widespread commitment to non-convex methods such as Adam. However, a local minimum implies that, in some environment around it, the function is convex. In this environment, second-order minimizing methods such as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We propose a novel framework grounded in the hypothesis that loss functions in real-world tasks swap from initial non-convexity to convexity towards the optimum. This is a property we leverage to design an innovative two-phase optimization algorithm. The presented algorithm detects the swap point by observing the gradient norm dependence on the loss. In these regions, non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing experiments confirm the hypothesis that this simple convexity structure is frequent enough to be practically exploited to substantially improve convergence and accuracy.</article>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CLASS-IT: Conversational and Lecture-Aligned Small-Scale Instruction Tuning for BabyLMs","url":"https://arxiv.org/abs/2510.25364","date":1761796800,"author":"","guid":321608,"unread":true,"content":"<article>arXiv:2510.25364v1 Announce Type: new \nAbstract: This work investigates whether small-scale LMs can benefit from instruction tuning. We compare conversational and question-answering instruction tuning datasets, applied either in a merged or sequential curriculum, using decoder-only models with 100M and 140M parameters. Evaluation spans both fine-tuning (SuperGLUE) and zero-shot (BLiMP, EWoK, WUGs, entity tracking, and psycholinguistic correlation) settings. Results show that instruction tuning yields small but consistent gains in fine-tuning scenarios, with sequential curricula outperforming merged data; however, improvements do not consistently transfer to zero-shot tasks, suggesting a trade-off between interaction-focused adaptation and broad linguistic generalization. These results highlight both the potential and the constraints of adapting human-inspired learning strategies to low-resource LMs, and point toward hybrid, curriculum-based approaches for enhancing generalization under ecological training limits.</article>","contentLength":1028,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scheduling Data-Intensive Workloads in Large-Scale Distributed Systems: Trends and Challenges","url":"https://arxiv.org/abs/2510.25362","date":1761796800,"author":"","guid":321609,"unread":true,"content":"<article>arXiv:2510.25362v1 Announce Type: new \nAbstract: With the explosive growth of big data, workloads tend to get more complex and computationally demanding. Such applications are processed on distributed interconnected resources that are becoming larger in scale and computational capacity. Data-intensive applications may have different degrees of parallelism and must effectively exploit data locality. Furthermore, they may impose several Quality of Service requirements, such as time constraints and resilience against failures, as well as other objectives, like energy efficiency. These features of the workloads, as well as the inherent characteristics of the computing resources required to process them, present major challenges that require the employment of effective scheduling techniques. In this chapter, a classification of data-intensive workloads is proposed and an overview of the most commonly used approaches for their scheduling in large-scale distributed systems is given. We present novel strategies that have been proposed in the literature and shed light on open challenges and future directions.</article>","contentLength":1117,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parameter Averaging in Link Prediction","url":"https://arxiv.org/abs/2510.25361","date":1761796800,"author":"","guid":321610,"unread":true,"content":"<article>arXiv:2510.25361v1 Announce Type: new \nAbstract: Ensemble methods are widely employed to improve generalization in machine learning. This has also prompted the adoption of ensemble learning for the knowledge graph embedding (KGE) models in performing link prediction. Typical approaches to this end train multiple models as part of the ensemble, and the diverse predictions are then averaged. However, this approach has some significant drawbacks. For instance, the computational overhead of training multiple models increases latency and memory overhead. In contrast, model merging approaches offer a promising alternative that does not require training multiple models. In this work, we introduce model merging, specifically weighted averaging, in KGE models. Herein, a running average of model parameters from a training epoch onward is maintained and used for predictions. To address this, we additionally propose an approach that selectively updates the running average of the ensemble model parameters only when the generalization performance improves on a validation dataset. We evaluate these two different weighted averaging approaches on link prediction tasks, comparing the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the weighted averaging approach considering literal-augmented KGE models and multi-hop query answering tasks as well. The results demonstrate that the proposed weighted averaging approach consistently improves performance across diverse evaluation settings.</article>","contentLength":1514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Energy consumption assessment of a Virtual Reality Remote Rendering application over 5G networks","url":"https://arxiv.org/abs/2510.25357","date":1761796800,"author":"","guid":321611,"unread":true,"content":"<article>arXiv:2510.25357v1 Announce Type: new \nAbstract: This paper investigates the energy implications of remote rendering for Virtual Reality (VR) applications within a real 5G testbed. Remote rendering enables lightweight devices to access high-performance graphical content by offloading computationally intensive tasks to Cloud-native Network Functions (CNFs) running on remote servers. However, this approach raises concerns regarding energy consumption across the various network components involved, including the remote computing node, the 5G Core, the Radio Access Network (RAN), and the User Equipment (UE). This work proposes and evaluates two complementary energy monitoring solutions, one hardware-based and one software-based, to measure energy consumption at different system levels. A VR remote renderer, deployed as CNF and leveraging the Media over QUIC (MoQ) protocol, is used as test case for assessing its energy footprint under different multimedia and network configurations. The results provide critical insights into the trade-off between energy consumption and performance of a real-world VR application running in a 5G environment.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Not ready for the bench: LLM legal interpretation is unstable and out of step with human judgments","url":"https://arxiv.org/abs/2510.25356","date":1761796800,"author":"","guid":321612,"unread":true,"content":"<article>arXiv:2510.25356v1 Announce Type: new \nAbstract: Legal interpretation frequently involves assessing how a legal text, as understood by an 'ordinary' speaker of the language, applies to the set of facts characterizing a legal dispute in the U.S. judicial system. Recent scholarship has proposed that legal practitioners add large language models (LLMs) to their interpretive toolkit. This work offers an empirical argument against LLM interpretation as recently practiced by legal scholars and federal judges. Our investigation in English shows that models do not provide stable interpretive judgments: varying the question format can lead the model to wildly different conclusions. Moreover, the models show weak to moderate correlation with human judgment, with large variance across model and question variant, suggesting that it is dangerous to give much credence to the conclusions produced by generative AI.</article>","contentLength":912,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Analysis of Semi-Supervised Learning on Hypergraphs","url":"https://arxiv.org/abs/2510.25354","date":1761796800,"author":"","guid":321613,"unread":true,"content":"<article>arXiv:2510.25354v1 Announce Type: new \nAbstract: Hypergraphs provide a natural framework for modeling higher-order interactions, yet their theoretical underpinnings in semi-supervised learning remain limited. We provide an asymptotic consistency analysis of variational learning on random geometric hypergraphs, precisely characterizing the conditions ensuring the well-posedness of hypergraph learning as well as showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to a higher-order Sobolev seminorm. Empirically, it performs strongly on standard baselines.</article>","contentLength":741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Protective DNS Blocking the Wild West?","url":"https://arxiv.org/abs/2510.25352","date":1761796800,"author":"","guid":321614,"unread":true,"content":"<article>arXiv:2510.25352v1 Announce Type: new \nAbstract: We perform a passive measurement study investigating how a Protective DNS service might perform in a Research &amp; Education Network serving hundreds of member institutions. Utilizing freely-available DNS blocklists consisting of domain names deemed to be threats, we test hundreds of millions of users' real DNS queries, observed over a week's time, to find which answers would be blocked because they involve domain names that are potential threats. We find the blocklists disorderly regarding their names, goals, transparency, and provenance making them quite difficult to compare. Consequently, these Protective DNS underpinnings lack organized oversight, presenting challenges and risks in operation at scale.</article>","contentLength":760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction","url":"https://arxiv.org/abs/2510.25348","date":1761796800,"author":"","guid":321615,"unread":true,"content":"<article>arXiv:2510.25348v1 Announce Type: new \nAbstract: Information cascade popularity prediction is a key problem in analyzing content diffusion in social networks. However, current related works suffer from three critical limitations: (1) temporal leakage in current evaluation--random cascade-based splits allow models to access future information, yielding unrealistic results; (2) feature-poor datasets that lack downstream conversion signals (e.g., likes, comments, or purchases), which limits more practical applications; (3) computational inefficiency of complex graph-based methods that require days of training for marginal gains. We systematically address these challenges from three perspectives: task setup, dataset construction, and model design. First, we propose a time-ordered splitting strategy that chronologically partitions data into consecutive windows, ensuring models are evaluated on genuine forecasting tasks without future information leakage. Second, we introduce Taoke, a large-scale e-commerce cascade dataset featuring rich promoter/product attributes and ground-truth purchase conversions--capturing the complete diffusion lifecycle from promotion to monetization. Third, we develop CasTemp, a lightweight framework that efficiently models cascade dynamics through temporal walks, Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based encoding with time-aware attention. Under leak-free evaluation, CasTemp achieves state-of-the-art performance across four datasets with orders-of-magnitude speedup. Notably, it excels at predicting second-stage popularity conversions--a practical task critical for real-world applications.</article>","contentLength":1674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework","url":"https://arxiv.org/abs/2510.25347","date":1761796800,"author":"","guid":321616,"unread":true,"content":"<article>arXiv:2510.25347v1 Announce Type: new \nAbstract: Coronary artery calcium (CAC) scoring plays a crucial role in the early detection and risk stratification of coronary artery disease (CAD). In this study, we focus on non-contrast coronary computed tomography angiography (CCTA) scans, which are commonly used for early calcification detection in clinical settings. To address the challenge of limited annotated data, we propose a radiomics-based pipeline that leverages pseudo-labeling to generate training labels, thereby eliminating the need for expert-defined segmentations. Additionally, we explore the use of pretrained foundation models, specifically CT-FM and RadImageNet, to extract image features, which are then used with traditional classifiers. We compare the performance of these deep learning features with that of radiomics features. Evaluation is conducted on a clinical CCTA dataset comprising 182 patients, where individuals are classified into two groups: zero versus non-zero calcium scores. We further investigate the impact of training on non-contrast datasets versus combined contrast and non-contrast datasets, with testing performed only on non contrast scans. Results show that radiomics-based models significantly outperform CNN-derived embeddings from foundation models (achieving 84% accuracy and p&lt;0.05), despite the unavailability of expert annotations.</article>","contentLength":1383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Joint Beamforming Design and Resource Allocation for IRS-Assisted Full-Duplex Terahertz Systems","url":"https://arxiv.org/abs/2510.25346","date":1761796800,"author":"","guid":321617,"unread":true,"content":"<article>arXiv:2510.25346v1 Announce Type: new \nAbstract: Intelligent reflecting surface (IRS)-assisted full-duplex (FD) terahertz (THz) communication systems have emerged as a promising paradigm to satisfy the escalating demand for ultra-high data rates and spectral efficiency in future wireless networks. However, the practical deployment of such systems presents unique technical challenges, stemming from severe propagation loss, frequency-dependent molecular absorption in the THz band, and the presence of strong residual self-interference (SI) inherent to FD communications. To tackle these issues, this paper proposes a joint resource allocation framework that aims to maximize the weighted minimum rate among all users, thereby ensuring fairness in quality of service. Specifically, the proposed design jointly optimizes IRS reflecting phase shifts, uplink/downlink transmit power control, sub-band bandwidth allocation, and sub-band assignment, explicitly capturing the unique propagation characteristics of THz channels and the impact of residual SI. To strike an balance between system performance and computational complexity, two computationally efficient algorithms are developed under distinct spectrum partitioning schemes: one assumes equal sub-band bandwidth allocation to facilliate tractable optimization, while the other introduces adaptive bandwidth allocation to further enhance spectral utilization and system flexibility. Simulation results validate the effectiveness of the proposed designs and demonstrate that the adopted scheme achieves significant spectral efficiency improvements over benchmark schemes.</article>","contentLength":1627,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Informative Sample Selection Model for Skeleton-based Action Recognition with Limited Training Samples","url":"https://arxiv.org/abs/2510.25345","date":1761796800,"author":"","guid":321618,"unread":true,"content":"<article>arXiv:2510.25345v1 Announce Type: new \nAbstract: Skeleton-based human action recognition aims to classify human skeletal sequences, which are spatiotemporal representations of actions, into predefined categories. To reduce the reliance on costly annotations of skeletal sequences while maintaining competitive recognition accuracy, the task of 3D Action Recognition with Limited Training Samples, also known as semi-supervised 3D Action Recognition, has been proposed. In addition, active learning, which aims to proactively select the most informative unlabeled samples for annotation, has been explored in semi-supervised 3D Action Recognition for training sample selection. Specifically, researchers adopt an encoder-decoder framework to embed skeleton sequences into a latent space, where clustering information, combined with a margin-based selection strategy using a multi-head mechanism, is utilized to identify the most informative sequences in the unlabeled set for annotation. However, the most representative skeleton sequences may not necessarily be the most informative for the action recognizer, as the model may have already acquired similar knowledge from previously seen skeleton samples. To solve it, we reformulate Semi-supervised 3D action recognition via active learning from a novel perspective by casting it as a Markov Decision Process (MDP). Built upon the MDP framework and its training paradigm, we train an informative sample selection model to intelligently guide the selection of skeleton sequences for annotation. To enhance the representational capacity of the factors in the state-action pairs within our method, we project them from Euclidean space to hyperbolic space. Furthermore, we introduce a meta tuning strategy to accelerate the deployment of our method in real-world scenarios. Extensive experiments on three 3D action recognition benchmarks demonstrate the effectiveness of our method.</article>","contentLength":1929,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Network Oblivious Transfer via Noisy Broadcast Channels","url":"https://arxiv.org/abs/2510.25343","date":1761796800,"author":"","guid":321619,"unread":true,"content":"<article>arXiv:2510.25343v1 Announce Type: new \nAbstract: This paper investigates information-theoretic oblivious transfer via a discrete memoryless broadcast channel with one sender and two receivers. We analyze both non-colluding and colluding honest-but-curious user models and establish general upper bounds on the achievable oblivious transfer capacity region for each case. Two explicit oblivious transfer protocols are proposed. The first ensures correctness and privacy for independent, non-colluding receivers by leveraging the structure of binary erasure broadcast channels. The second protocol, secure even under receiver collusion, introduces additional entropy-sharing and privacy amplification mechanisms to preserve secrecy despite information leakage between users. Our results show that for the non-colluding case, the upper and lower bounds on oblivious transfer capacity coincide, providing a complete characterization of the achievable region. The work provides a unified theoretical framework bridging network information theory and cryptographic security, highlighting the potential of noisy broadcast channels as powerful primitives for multi-user privacy-preserving communication.</article>","contentLength":1195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lightweight Federated Learning in Mobile Edge Computing with Statistical and Device Heterogeneity Awareness","url":"https://arxiv.org/abs/2510.25342","date":1761796800,"author":"","guid":321620,"unread":true,"content":"<article>arXiv:2510.25342v1 Announce Type: new \nAbstract: Federated learning enables collaborative machine learning while preserving data privacy, but high communication and computation costs, exacerbated by statistical and device heterogeneity, limit its practicality in mobile edge computing. Existing compression methods like sparsification and pruning reduce per-round costs but may increase training rounds and thus the total training cost, especially under heterogeneous environments. We propose a lightweight personalized FL framework built on parameter decoupling, which separates the model into shared and private subspaces, enabling us to uniquely apply gradient sparsification to the shared component and model pruning to the private one. This structural separation confines communication compression to global knowledge exchange and computation reduction to local personalization, protecting personalization quality while adapting to heterogeneous client resources. We theoretically analyze convergence under the combined effects of sparsification and pruning, revealing a sparsity-pruning trade-off that links to the iteration complexity. Guided by this analysis, we formulate a joint optimization that selects per-client sparsity and pruning rates and wireless bandwidth to reduce end-to-end training time. Simulation results demonstrate faster convergence and substantial reductions in overall communication and computation costs with negligible accuracy loss, validating the benefits of coordinated and resource-aware personalization in resource-constrained heterogeneous environments.</article>","contentLength":1592,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-party Agent Relation Sampling for Multi-party Ad Hoc Teamwork","url":"https://arxiv.org/abs/2510.25340","date":1761796800,"author":"","guid":321621,"unread":true,"content":"<article>arXiv:2510.25340v1 Announce Type: new \nAbstract: Multi-agent reinforcement learning (MARl) has achieved strong results in cooperative tasks but typically assumes fixed, fully controlled teams. Ad hoc teamwork (AHT) relaxes this by allowing collaboration with unknown partners, yet existing variants still presume shared conventions. We introduce Multil-party Ad Hoc Teamwork (MAHT), where controlled agents must coordinate with multiple mutually unfamiliar groups of uncontrolled teammates. To address this, we propose MARs, which builds a sparse skeleton graph and applies relational modeling to capture cross-group dvnamics. Experiments on MPE and starCralt ll show that MARs outperforms MARL and AHT baselines while converging faster.</article>","contentLength":737,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tracking Walls, Take-It-Or-Leave-It Choices, the GDPR, and the ePrivacy Regulation","url":"https://arxiv.org/abs/2510.25339","date":1761796800,"author":"","guid":321622,"unread":true,"content":"<article>arXiv:2510.25339v1 Announce Type: new \nAbstract: On the internet, we encounter take-it-or-leave-it choices regarding our privacy on a daily basis. In Europe, online tracking for targeted advertising generally requires the internet users' consent to be lawful. Some websites use a tracking wall, a barrier that visitors can only pass if they consent to tracking by third parties. When confronted with such a tracking wall, many people click 'I agree' to tracking. A survey that we conducted shows that most people find tracking walls unfair and unacceptable. We analyse under which conditions the ePrivacy Directive and the General Data Protection Regulation allow tracking walls. We provide a list of circumstances to assess when a tracking wall makes consent invalid. We also explore how the EU lawmaker could regulate tracking walls, for instance in the ePrivacy Regulation. It should be seriously considered to ban tracking walls, at least in certain circumstances.</article>","contentLength":968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geometric Robot Calibration Using a Calibration Plate","url":"https://arxiv.org/abs/2510.25338","date":1761796800,"author":"","guid":321623,"unread":true,"content":"<article>arXiv:2510.25338v1 Announce Type: new \nAbstract: In this paper a new method for geometric robot calibration is introduced, which uses a calibration plate with precisely known distances between its measuring points. The relative measurement between two points on the calibration plate is used to determine predefined error parameters of the system. In comparison to conventional measurement methods, like laser tracker or motion capture systems, the calibration plate provides a more mechanically robust and cheaper alternative, which is furthermore easier to transport due to its small size. The calibration method, the plate design, the mathematical description of the error system as well as the identification of the parameters are described in detail. For identifying the error parameters, the least squares method and a constrained optimization problem are used. The functionality of this method was demonstrated in experiments that led to promising results, correlated with one of a laser tracker calibration. The modeling and identification of the error parameters is done for a gantry machine, but is not restricted to that type of robot.</article>","contentLength":1146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tackling the Algorithmic Control Crisis -- the Technical, Legal, and Ethical Challenges of Research into Algorithmic Agents","url":"https://arxiv.org/abs/2510.25337","date":1761796800,"author":"","guid":321624,"unread":true,"content":"<article>arXiv:2510.25337v1 Announce Type: new \nAbstract: Algorithmic agents permeate every instant of our online existence. Based on our digital profiles built from the massive surveillance of our digital existence, algorithmic agents rank search results, filter our emails, hide and show news items on social networks feeds, try to guess what products we might buy next for ourselves and for others, what movies we want to watch, and when we might be pregnant. Algorithmic agents select, filter, and recommend products, information, and people. Increasingly, algorithmic agents don't just select from the range of human created alternatives, but also they create. Burgeoning algorithmic agents are capable of providing us with content made just for us, and engage with us through one-of-a-kind, personalized interactions. Studying these algorithmic agents presents a host of methodological, ethical, and logistical challenges. The objectives of our paper are two-fold. The first aim is to describe one possible approach to researching the individual and societal effects of algorithmic recommenders, and to share our experiences with the academic community. The second is to contribute to a more fundamental discussion about the ethical and legal issues of \"tracking the trackers\", as well as the costs and trade-offs involved. Our paper will contribute to the discussion on the relative merits, costs and benefits of different approaches to ethically and legally sound research on algorithmic governance. We will argue that besides shedding light on how users interact with algorithmic agents, we also need to be able to understand how different methods of monitoring our algorithmically controlled digital environments compare to each other in terms of costs and benefits. We conclude our article with a number of concrete suggestions for how to address the practical, ethical and legal challenges of researching algorithms and their effects on users and society.</article>","contentLength":1958,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An approach for combining transparency and motion assistance of a lower body exoskeleton","url":"https://arxiv.org/abs/2510.25335","date":1761796800,"author":"","guid":321625,"unread":true,"content":"<article>arXiv:2510.25335v1 Announce Type: new \nAbstract: In this paper, an approach for gait assistance with a lower body exoskeleton is described. Two concepts, transparency and motion assistance, are combined. The transparent mode, where the system is following the user's free motion with a minimum of perceived interaction forces, is realized by exploiting the gear backlash of the actuation units. During walking a superimposed assistance mode applies an additional torque guiding the legs to their estimated future position. The concept of adaptive oscillators is utilized to learn the quasi-periodic signals typical for locomotion. First experiments showed promising results.</article>","contentLength":674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories","url":"https://arxiv.org/abs/2510.25333","date":1761796800,"author":"","guid":321626,"unread":true,"content":"<article>arXiv:2510.25333v1 Announce Type: new \nAbstract: Recent years have witnessed the rapid development of LLM-based agents, which shed light on using language agents to solve complex real-world problems. A prominent application lies in business agents, which interact with databases and internal knowledge bases via tool calls to fulfill diverse user requirements. However, this domain is characterized by intricate data relationships and a wide range of heterogeneous tasks, from statistical data queries to knowledge-based question-answering. To address these challenges, we propose CRMWeaver, a novel approach that enhances business agents in such complex settings. To acclimate the agentic model to intricate business environments, we employ a synthesis data generation and RL-based paradigm during training, which significantly improves the model's ability to handle complex data and varied tasks. During inference, a shared memories mechanism is introduced, prompting the agent to learn from task guidelines in similar problems, thereby further boosting its effectiveness and generalization, especially in unseen scenarios. We validate the efficacy of our approach on the CRMArena-Pro dataset, where our lightweight model achieves competitive results in both B2B and B2C business scenarios, underscoring its practical value for real-world applications.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA","url":"https://arxiv.org/abs/2510.25332","date":1761796800,"author":"","guid":321627,"unread":true,"content":"<article>arXiv:2510.25332v1 Announce Type: new \nAbstract: The rapid growth of streaming video applications demands multimodal models with enhanced capabilities for temporal dynamics understanding and complex reasoning. However, current Video Question Answering (VideoQA) datasets suffer from two critical limitations: 1) Static annotation mechanisms fail to capture the evolving nature of answers in temporal video streams, and 2) The absence of explicit reasoning process annotations restricts model interpretability and logical deduction capabilities. To address these challenges, We introduce StreamingCoT, the first dataset explicitly designed for temporally evolving reasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Our framework first establishes a dynamic hierarchical annotation architecture that generates per-second dense descriptions and constructs temporally-dependent semantic segments through similarity fusion, paired with question-answer sets constrained by temporal evolution patterns. We further propose an explicit reasoning chain generation paradigm that extracts spatiotemporal objects via keyframe semantic alignment, derives object state transition-based reasoning paths using large language models, and ensures logical coherence through human-verified validation. This dataset establishes a foundation for advancing research in streaming video understanding, complex temporal reasoning, and multimodal inference. Our StreamingCoT and its construction toolkit can be accessed at https://github.com/Fleeting-hyh/StreamingCoT.</article>","contentLength":1565,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding","url":"https://arxiv.org/abs/2510.25327","date":1761796800,"author":"","guid":321628,"unread":true,"content":"<article>arXiv:2510.25327v1 Announce Type: new \nAbstract: Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.</article>","contentLength":1742,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tight Collision Avoidance for Stochastic Optimal Control: with Applications in Learning-based, Interactive Motion Planning","url":"https://arxiv.org/abs/2510.25324","date":1761796800,"author":"","guid":321629,"unread":true,"content":"<article>arXiv:2510.25324v1 Announce Type: new \nAbstract: Trajectory planning in dense, interactive traffic scenarios presents significant challenges for autonomous vehicles, primarily due to the uncertainty of human driver behavior and the non-convex nature of collision avoidance constraints. This paper introduces a stochastic optimal control framework to address these issues simultaneously, without excessively conservative approximations. We opt to model human driver decisions as a Markov Decision Process and propose a method for handling collision avoidance between non-convex vehicle shapes by imposing a positive distance constraint between compact sets. In this framework, we investigate three alternative chance constraint formulations. To ensure computational tractability, we introduce tight, continuously differentiable reformulations of both the non-convex distance constraints and the chance constraints. The efficacy of our approach is demonstrated through simulation studies of two challenging interactive scenarios: an unregulated intersection crossing and a highway lane change in dense traffic.</article>","contentLength":1108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices","url":"https://arxiv.org/abs/2510.25323","date":1761796800,"author":"","guid":321630,"unread":true,"content":"<article>arXiv:2510.25323v1 Announce Type: new \nAbstract: Normalizing flows are deep generative models that enable efficient likelihood estimation and sampling through invertible transformations. A key challenge is to design linear layers that enhance expressiveness while maintaining efficient computation of the Jacobian determinant and inverse. We introduce a novel invertible linear layer based on the product of circulant and diagonal matrices. This decomposition reduces parameter complexity from $\\mathcal{O}(n^2)$ to $\\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$ circulant matrices while still approximating general linear transformations. By leveraging the Fast Fourier Transform, our approach reduces the time complexity of matrix inversion from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(mn\\log n)$ and that of computing the log-determinant from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(mn)$, where $n$ is the input dimension. We build upon this layer to develop Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on natural image datasets and effectively models data with inherent periodic structure. Furthermore, CDFlow significantly accelerates key operations in normalizing flows, providing practical benefits for scalable generative modeling.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning","url":"https://arxiv.org/abs/2510.25320","date":1761796800,"author":"","guid":321631,"unread":true,"content":"<article>arXiv:2510.25320v1 Announce Type: new \nAbstract: Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.</article>","contentLength":1760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"4-Doodle: Text to 3D Sketches that Move!","url":"https://arxiv.org/abs/2510.25319","date":1761796800,"author":"","guid":321632,"unread":true,"content":"<article>arXiv:2510.25319v1 Announce Type: new \nAbstract: We present a novel task: text-to-3D sketch animation, which aims to bring freeform sketches to life in dynamic 3D space. Unlike prior works focused on photorealistic content generation, we target sparse, stylized, and view-consistent 3D vector sketches, a lightweight and interpretable medium well-suited for visual communication and prototyping. However, this task is very challenging: (i) no paired dataset exists for text and 3D (or 4D) sketches; (ii) sketches require structural abstraction that is difficult to model with conventional 3D representations like NeRFs or point clouds; and (iii) animating such sketches demands temporal coherence and multi-view consistency, which current pipelines do not address. Therefore, we propose 4-Doodle, the first training-free framework for generating dynamic 3D sketches from text. It leverages pretrained image and video diffusion models through a dual-space distillation scheme: one space captures multi-view-consistent geometry using differentiable B\\'ezier curves, while the other encodes motion dynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion), which optimizes from a single view per step, our multi-view optimization ensures structural alignment and avoids view ambiguity, critical for sparse sketches. Furthermore, we introduce a structure-aware motion module that separates shape-preserving trajectories from deformation-aware changes, enabling expressive motion such as flipping, rotation, and articulated movement. Extensive experiments show that our method produces temporally realistic and structurally stable 3D sketch animations, outperforming existing baselines in both fidelity and controllability. We hope this work serves as a step toward more intuitive and accessible 4D content creation.</article>","contentLength":1829,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prototype-Driven Adaptation for Few-Shot Object Detection","url":"https://arxiv.org/abs/2510.25318","date":1761796800,"author":"","guid":321633,"unread":true,"content":"<article>arXiv:2510.25318v1 Announce Type: new \nAbstract: Few-shot object detection (FSOD) often suffers from base-class bias and unstable calibration when only a few novel samples are available. We propose Prototype-Driven Alignment (PDA), a lightweight, plug-in metric head for DeFRCN that provides a prototype-based \"second opinion\" complementary to the linear classifier. PDA maintains support-only prototypes in a learnable identity-initialized projection space and optionally applies prototype-conditioned RoI alignment to reduce geometric mismatch. During fine-tuning, prototypes can be adapted via exponential moving average(EMA) updates on labeled foreground RoIs-without introducing class-specific parameters-and are frozen at inference to ensure strict protocol compliance. PDA employs a best-of-K matching scheme to capture intra-class multi-modality and temperature-scaled fusion to combine metric similarities with detector logits. Experiments on VOC FSOD and GFSOD benchmarks show that PDA consistently improves novel-class performance with minimal impact on base classes and negligible computational overhead.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeing Clearly and Deeply: An RGBD Imaging Approach with a Bio-inspired Monocentric Design","url":"https://arxiv.org/abs/2510.25314","date":1761796800,"author":"","guid":321634,"unread":true,"content":"<article>arXiv:2510.25314v1 Announce Type: new \nAbstract: Achieving high-fidelity, compact RGBD imaging presents a dual challenge: conventional compact optics struggle with RGB sharpness across the entire depth-of-field, while software-only Monocular Depth Estimation (MDE) is an ill-posed problem reliant on unreliable semantic priors. While deep optics with elements like DOEs can encode depth, they introduce trade-offs in fabrication complexity and chromatic aberrations, compromising simplicity. To address this, we first introduce a novel bio-inspired all-spherical monocentric lens, around which we build the Bionic Monocentric Imaging (BMI) framework, a holistic co-design. This optical design naturally encodes depth into its depth-varying Point Spread Functions (PSFs) without requiring complex diffractive or freeform elements. We establish a rigorous physically-based forward model to generate a synthetic dataset by precisely simulating the optical degradation process. This simulation pipeline is co-designed with a dual-head, multi-scale reconstruction network that employs a shared encoder to jointly recover a high-fidelity All-in-Focus (AiF) image and a precise depth map from a single coded capture. Extensive experiments validate the state-of-the-art performance of the proposed framework. In depth estimation, the method attains an Abs Rel of 0.026 and an RMSE of 0.130, markedly outperforming leading software-only approaches and other deep optics systems. For image restoration, the system achieves an SSIM of 0.960 and a perceptual LPIPS score of 0.082, thereby confirming a superior balance between image fidelity and depth accuracy. This study illustrates that the integration of bio-inspired, fully spherical optics with a joint reconstruction algorithm constitutes an effective strategy for addressing the intrinsic challenges in high-performance compact RGBD imaging. Source code will be publicly available at https://github.com/ZongxiYu-ZJU/BMI.</article>","contentLength":1966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning","url":"https://arxiv.org/abs/2510.25311","date":1761796800,"author":"","guid":321635,"unread":true,"content":"<article>arXiv:2510.25311v1 Announce Type: new \nAbstract: Reinforcement Learning algorithms are primarily focused on learning a policy that maximizes expected return. As a result, the learned policy can exploit one or few reward sources. However, in many natural situations, it is desirable to learn a policy that induces a dispersed marginal state distribution over rewarding states, while maximizing the expected return which is typically tied to reaching a goal state. This aspect remains relatively unexplored. Existing techniques based on entropy regularization and intrinsic rewards use stochasticity for encouraging exploration to find an optimal policy which may not necessarily lead to dispersed marginal state distribution over rewarding states. Other RL algorithms which match a target distribution assume the latter to be available apriori. This may be infeasible in large scale systems where enumeration of all states is not possible and a state is determined to be a goal state only upon reaching it. We formalize the problem of maximizing the expected return while uniformly visiting the goal states as Multi Goal RL in which an oracle classifier over the state space determines the goal states. We propose a novel algorithm that learns a high-return policy mixture with marginal state distribution dispersed over the set of goal states. Our algorithm is based on optimizing a custom RL reward which is computed - based on the current policy mixture - at each iteration for a set of sampled trajectories. The latter are used via an offline RL algorithm to update the policy mixture. We prove performance guarantees for our algorithm, showing efficient convergence bounds for optimizing a natural objective which captures the expected return as well as the dispersion of the marginal state distribution over the goal states. We design and perform experiments on synthetic MDPs and standard RL environments to evaluate the effectiveness of our algorithm.</article>","contentLength":1958,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parrot: A Training Pipeline Enhances Both Program CoT and Natural Language CoT for Reasoning","url":"https://arxiv.org/abs/2510.25310","date":1761796800,"author":"","guid":321636,"unread":true,"content":"<article>arXiv:2510.25310v1 Announce Type: new \nAbstract: Natural language chain-of-thought (N-CoT) and Program chain-of-thought (P-CoT) have emerged as two primary paradigms for large language models (LLMs) to solve mathematical reasoning problems. Current research typically endeavors to achieve unidirectional enhancement: P-CoT enhanced N-CoT or N-CoT enhanced P-CoT. In this paper, we seek to fully unleash the two paradigms' strengths for mutual enhancement and ultimately achieve simultaneous improvements. We conduct a detailed analysis of the error types across two paradigms, based on which we propose Parrot, a novel training pipeline for mathematical problems: 1) Three target-designed subtasks integrate sequential P-CoT and N-CoT generation. 2) A subtask hybrid training strategy to facilitate natural language semantic transferability. 3) The converted N-CoT auxiliary reward is designed to alleviate the sparse rewards in P-CoT optimization. Extensive experiments demonstrate that Parrot significantly enhances both the performance of N-CoT and P-CoT, especially on N-CoT. Using Parrot SFT, the N-CoT performance of LLaMA2 and CodeLLaMA achieve gains of +21.87 and +21.48 on MathQA over the RL baseline, which is resource-intensive.</article>","contentLength":1239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Enabled Predictive Control and Guidance for Autonomous Underwater Vehicles","url":"https://arxiv.org/abs/2510.25309","date":1761796800,"author":"","guid":321637,"unread":true,"content":"<article>arXiv:2510.25309v1 Announce Type: new \nAbstract: This paper presents a fully data-driven control framework for autonomous underwater vehicles (AUVs) based on Data-Enabled Predictive Control (DeePC). The approach eliminates the need for explicit hydrodynamic modeling by exploiting measured input-output data to predict and optimize future system behavior. Classic DeePC was employed in the heading control, while a cascaded DeePC architecture is proposed for depth regulation, incorporating a loop-frequency separation to handle the different dynamic modes of input and output. For 3-D waypoint path following, the Adaptive Line-of-Sight algorithm is extended to a predictive formulation and integrated with DeePC. All methods are validated in extensive simulation on the REMUS 100 AUV and compared with classical PI/PID control. The results demonstrate superior tracking performance and robustness of DeePC under ocean-current disturbances and nonlinear operating conditions, while significantly reducing modeling effort.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems","url":"https://arxiv.org/abs/2510.25306","date":1761796800,"author":"","guid":321638,"unread":true,"content":"<article>arXiv:2510.25306v1 Announce Type: new \nAbstract: Modeling complex spatiotemporal dynamics, particularly in far-from-equilibrium systems, remains a grand challenge in science. The governing partial differential equations (PDEs) for these systems are often intractable to derive from first principles, due to their inherent complexity, characterized by high-order derivatives and strong nonlinearities, coupled with incomplete physical knowledge. This has spurred the development of data-driven methods, yet these approaches face limitations: Purely data-driven models are often physically inconsistent and data-intensive, while existing physics-informed methods lack the structural capacity to represent complex operators or systematically integrate partial physical knowledge. Here, we propose a hierarchical physics-embedded learning framework that fundamentally advances both the forward spatiotemporal prediction and inverse discovery of physical laws from sparse and noisy data. The key innovation is a two-level architecture that mirrors the process of scientific discovery: the first level learns fundamental symbolic components of a PDE, while the second learns their governing combinations. This hierarchical decomposition not only reduces learning complexity but, more importantly, enables a structural integration of prior knowledge. Known physical laws are directly embedded into the models computational graph, guaranteeing physical consistency and improving data efficiency. By building the framework upon adaptive Fourier Neural Operators, we can effectively capture the non-local dependencies and high-order operators characteristic of dynamical systems. Additionally, by structurally decoupling known and unknown terms, the framework further enables interpretable discovery of underlying governing equations through symbolic regression, without presupposing functional forms.</article>","contentLength":1891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"General Coverage Models: Structure, Monotonicity, and Shotgun Sequencing","url":"https://arxiv.org/abs/2510.25305","date":1761796800,"author":"","guid":321639,"unread":true,"content":"<article>arXiv:2510.25305v1 Announce Type: new \nAbstract: We study coverage processes in which each draw reveals a subset of $[n]$, and the goal is to determine the expected number of draws until all items are seen at least once. A classical example is the Coupon Collector's Problem, where each draw reveals exactly one item. Motivated by shotgun DNA sequencing, we introduce a model where each draw is a contiguous window of fixed length, in both cyclic and non-cyclic variants. We develop a unifying combinatorial tool that shifts the task of finding coverage time from probability, to a counting problem over families of subsets of $[n]$ that together contain all items, enabling exact calculation. Using this result, we obtain exact expressions for the window models. We then leverage past results on a continuous analogue of the cyclic window model to analyze the asymptotic behavior of both models. We further study what we call uniform $\\ell$-regular models, where every draw has size $\\ell$ and every item appears in the same number of admissible draws. We compare these to the batch sampling model, in which all $\\ell$-subsets are drawn uniformly at random and present upper and lower bounds, which were also obtained independently by Berend and Sher. We conjecture, and prove for special cases, that this model maximizes the coverage time among all uniform $\\ell$-regular models. Finally, we prove a universal upper bound on the entire class of uniform $\\ell$-regular models, which illuminates the fact that many sampling models share the same leading asymptotic order, while potentially differing significantly in lower-order terms.</article>","contentLength":1635,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Teaching Sarcasm: Few-Shot Multimodal Sarcasm Detection via Distillation to a Parameter-Efficient Student","url":"https://arxiv.org/abs/2510.25303","date":1761796800,"author":"","guid":321640,"unread":true,"content":"<article>arXiv:2510.25303v1 Announce Type: new \nAbstract: Multimodal sarcasm detection is challenging, especially in low-resource settings where subtle image-text contradictions are hard to learn due to scarce annotated data, which hinders the model's performance. Parameter-efficient fine-tuning (PEFT) methods like adapters, LoRA, and prompt tuning reduce overfitting but struggle to reach optimal performance due to limited supervision from few-shot data. We propose PEKD, a unified framework that enhances PEFT methods via distillation from an expert model trained on large-scale sarcasm data, which acts as the teacher. To mitigate unreliable signals from the teacher, we introduce an entropy-aware gating mechanism that dynamically adjusts the distillation strength based on teacher confidence. Experiments on two public datasets demonstrate that our PEKD framework enables PEFT methods to outperform both prior parameter-efficient approaches and large multimodal models, achieving strong results in the few-shot scenario. The framework is modular and adaptable to a wide range of multimodal models and tasks.</article>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GaTector+: A Unified Head-free Framework for Gaze Object and Gaze Following Prediction","url":"https://arxiv.org/abs/2510.25301","date":1761796800,"author":"","guid":321641,"unread":true,"content":"<article>arXiv:2510.25301v1 Announce Type: new \nAbstract: Gaze object detection and gaze following are fundamental tasks for interpreting human gaze behavior or intent. However, most previous methods usually solve these two tasks separately, and their prediction of gaze objects and gaze following typically depend on head-related prior knowledge during both the training phase and real-world deployment. This dependency necessitates an auxiliary network to extract head location, thus precluding joint optimization across the entire system and constraining the practical applicability. To this end, we propose GaTector+, a unified framework for gaze object detection and gaze following, which eliminates the dependence on the head-related priors during inference. Specifically, GaTector+ uses an expanded specific-general-specific feature extractor that leverages a shared backbone, which extracts general features for gaze following and object detection using the shared backbone while using specific blocks before and after the shared backbone to better consider the specificity of each sub-task. To obtain head-related knowledge without prior information, we first embed a head detection branch to predict the head of each person. Then, before regressing the gaze point, a head-based attention mechanism is proposed to fuse the sense feature and gaze feature with the help of head location. Since the suboptimization of the gaze point heatmap leads to the performance bottleneck, we propose an attention supervision mechanism to accelerate the learning of the gaze heatmap. Finally, we propose a novel evaluation metric, mean Similarity over Candidates (mSoC), for gaze object detection, which is more sensitive to variations between bounding boxes. The experimental results on multiple benchmark datasets demonstrate the effectiveness of our model in both gaze object detection and gaze following tasks.</article>","contentLength":1899,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A virtual element approximation for the modified transmission eigenvalues for natural materials","url":"https://arxiv.org/abs/2510.25298","date":1761796800,"author":"","guid":321642,"unread":true,"content":"<article>arXiv:2510.25298v1 Announce Type: new \nAbstract: In this paper, we discuss a virtual element approximation for the modified transmission eigenvalue problem in inverse scattering for natural materials. In this case, due to the positive artificial diffusivity parameter in the considered problem, the sesquilinear form at the left end of the variational form is not coercive. We first demonstrate the well-posedness of the discrete source problem using the $\\mathds{T}$-coercivity property, then provide the a priori error estimates for the approximate eigenspaces and eigenvalues, and finally reports several numerical examples. The numerical experiments show that the proposed method is effective</article>","contentLength":696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding the Characteristics of LLM-Generated Property-Based Tests in Exploring Edge Cases","url":"https://arxiv.org/abs/2510.25297","date":1761796800,"author":"","guid":321643,"unread":true,"content":"<article>arXiv:2510.25297v1 Announce Type: new \nAbstract: As Large Language Models (LLMs) increasingly generate code in software development, ensuring the quality of LLM-generated code has become important. Traditional testing approaches using Example-based Testing (EBT) often miss edge cases -- defects that occur at boundary values, special input patterns, or extreme conditions. This research investigates the characteristics of LLM-generated Property-based Testing (PBT) compared to EBT for exploring edge cases. We analyze 16 HumanEval problems where standard solutions failed on extended test cases, generating both PBT and EBT test codes using Claude-4-sonnet. Our experimental results reveal that while each method individually achieved a 68.75\\% bug detection rate, combining both approaches improved detection to 81.25\\%. The analysis demonstrates complementary characteristics: PBT effectively detects performance issues and edge cases through extensive input space exploration, while EBT effectively detects specific boundary conditions and special patterns. These findings suggest that a hybrid approach leveraging both testing methods can improve the reliability of LLM-generated code, providing guidance for test generation strategies in LLM-based code generation.</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Identifying Kronecker product factorizations","url":"https://arxiv.org/abs/2510.25292","date":1761796800,"author":"","guid":321644,"unread":true,"content":"<article>arXiv:2510.25292v1 Announce Type: new \nAbstract: The Kronecker product is an invaluable tool for data-sparse representations of large networks and matrices with countless applications in machine learning, graph theory and numerical linear algebra. In some instances, the sparsity pattern of large matrices may already hide a Kronecker product. Similarly, a large network, represented by its adjacency matrix, may sometimes be factorized as a Kronecker product of smaller adjacency matrices. In this article, we determine all possible Kronecker factorizations of a binary matrix and visualize them through its decomposition graph. Such sparsity-informed factorizations may later enable good (approximate) Kronecker factorizations of real matrices or reveal the latent structure of a network. The latter also suggests a natural visualization of Kronecker graphs.</article>","contentLength":860,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Testing Correlation in Graphs by Counting Bounded Degree Motifs","url":"https://arxiv.org/abs/2510.25289","date":1761796800,"author":"","guid":321645,"unread":true,"content":"<article>arXiv:2510.25289v1 Announce Type: new \nAbstract: Correlation analysis is a fundamental step for extracting meaningful insights from complex datasets. In this paper, we investigate the problem of detecting correlation between two Erd\\H{o}s-R\\'enyi graphs $G(n,p)$, formulated as a hypothesis testing problem: under the null hypothesis, the two graphs are independent, while under the alternative hypothesis, they are correlated. We develop a polynomial-time test by counting bounded degree motifs and prove its effectiveness for any constant correlation coefficient $\\rho$ when the edge connecting probability satisfies $p\\ge n^{-2/3}$. Our results overcome the limitation requiring $\\rho \\ge \\sqrt{\\alpha}$, where $\\alpha\\approx 0.338$ is the Otter's constant, extending it to any constant $\\rho$. Methodologically, bounded degree motifs -- ubiquitous in real networks -- make the proposed statistic both natural and scalable. We also validate our method on synthetic and real co-citation networks, further confirming that this simple motif family effectively captures correlation signals and exhibits strong empirical performance.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting scalable sequential recommendation with Multi-Embedding Approach and Mixture-of-Experts","url":"https://arxiv.org/abs/2510.25285","date":1761796800,"author":"","guid":321646,"unread":true,"content":"<article>arXiv:2510.25285v1 Announce Type: new \nAbstract: In recommendation systems, how to effectively scale up recommendation models has been an essential research topic. While significant progress has been made in developing advanced and scalable architectures for sequential recommendation(SR) models, there are still challenges due to items' multi-faceted characteristics and dynamic item relevance in the user context. To address these issues, we propose Fuxi-MME, a framework that integrates a multi-embedding strategy with a Mixture-of-Experts (MoE) architecture. Specifically, to efficiently capture diverse item characteristics in a decoupled manner, we decompose the conventional single embedding matrix into several lower-dimensional embedding matrices. Additionally, by substituting relevant parameters in the Fuxi Block with an MoE layer, our model achieves adaptive and specialized transformation of the enriched representations. Empirical results on public datasets show that our proposed framework outperforms several competitive baselines.</article>","contentLength":1048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shared Control for Vehicle Lane-Changing with Uncertain Driver Behaviors","url":"https://arxiv.org/abs/2510.25284","date":1761796800,"author":"","guid":321647,"unread":true,"content":"<article>arXiv:2510.25284v1 Announce Type: new \nAbstract: Lane changes are common yet challenging driving maneuvers that require continuous decision-making and dynamic interaction with surrounding vehicles. Relying solely on human drivers for lane-changing can lead to traffic disturbances due to the stochastic nature of human behavior and its variability under different task demands. Such uncertainties may significantly degrade traffic string stability, which is critical for suppressing disturbance propagation and ensuring smooth merging of the lane-changing vehicles. This paper presents a human-automation shared lane-changing control framework that preserves driver authority while allowing automated assistance to achieve stable maneuvers in the presence of driver's behavioral uncertainty. Human driving behavior is modeled as a Markov jump process with transitions driven by task difficulty, providing a tractable representation of stochastic state switching. Based on this model, we first design a nominal stabilizing controller that guarantees stochastic ${L}_2$ string stability under imperfect mode estimation. To further balance performance and automated effort, we then develop a Minimal Intervention Controller (MIC) that retains acceptable stability while limiting automation. Simulations using lane-changing data from the NGSIM dataset verify that the nominal controller reduces speed perturbations and shorten lane-changing time, while the MIC further reduces automated effort and enhances comfort but with moderate stability and efficiency loss. Validations on the TGSIM dataset with SAE Level 2 vehicles show that the MIC enables earlier lane changes than Level 2 control while preserving driver authority with a slight stability compromise. These findings highlight the potential of shared control strategies to balance stability, efficiency, and driver acceptance.</article>","contentLength":1881,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measuring the Research Output and Performance of the University of Ibadan from 2014 to 2023: A Scientometric Analysis","url":"https://arxiv.org/abs/2510.25283","date":1761796800,"author":"","guid":321648,"unread":true,"content":"<article>arXiv:2510.25283v1 Announce Type: new \nAbstract: This study employs scientometric methods to assess the research output and performance of the University of Ibadan from 2014 to 2023. By analyzing publication trends, citation patterns, and collaboration networks, the research aims to comprehensively evaluate the university's research productivity, impact, and disciplinary focus. This article's endeavors are characterized by innovation, interdisciplinary collaboration, and commitment to excellence, making the University of Ibadan a significant hub for cutting-edge research in Nigeria and beyond. The goal of the current study is to ascertain the influence of the university's research output and publication patterns between 2014 and 2023. The study focuses on the departments at the University of Ibadan that contribute the most, the best journals for publishing, the nations that collaborate, the impact of citations both locally and globally, well-known authors and their total production, and the research output broken down by year. According to the university's ten-year publication data, 7159 papers with an h-index of 75 were published between 2014 and 2023, garnering 218572 citations. Furthermore, the VOSviewer software mapping approach is used to illustrate the stenographical mapping of data through graphs. The findings of this study will contribute to understanding the university's research strengths, weaknesses, and potential areas for improvement. Additionally, the results will inform evidence-based decision-making for enhancing research strategies and policies at the University of Ibadan.</article>","contentLength":1616,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Stability of Neural Networks in Deep Learning","url":"https://arxiv.org/abs/2510.25282","date":1761796800,"author":"","guid":321649,"unread":true,"content":"<article>arXiv:2510.25282v1 Announce Type: new \nAbstract: Deep learning has achieved remarkable success across a wide range of tasks, but its models often suffer from instability and vulnerability: small changes to the input may drastically affect predictions, while optimization can be hindered by sharp loss landscapes. This thesis addresses these issues through the unifying perspective of sensitivity analysis, which examines how neural networks respond to perturbations at both the input and parameter levels.\n  We study Lipschitz networks as a principled way to constrain sensitivity to input perturbations, thereby improving generalization, adversarial robustness, and training stability. To complement this architectural approach, we introduce regularization techniques based on the curvature of the loss function, promoting smoother optimization landscapes and reducing sensitivity to parameter variations. Randomized smoothing is also explored as a probabilistic method for enhancing robustness at decision boundaries.\n  By combining these perspectives, we develop a unified framework where Lipschitz continuity, randomized smoothing, and curvature regularization interact to address fundamental challenges in stability. The thesis contributes both theoretical analysis and practical methodologies, including efficient spectral norm computation, novel Lipschitz-constrained layers, and improved certification procedures.</article>","contentLength":1421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TCP ROCCET: An RTT-Oriented CUBIC Congestion Control Extension for 5G and Beyond Networks","url":"https://arxiv.org/abs/2510.25281","date":1761796800,"author":"","guid":321650,"unread":true,"content":"<article>arXiv:2510.25281v1 Announce Type: new \nAbstract: The behavior of loss-based TCP congestion control algorithms like TCP CUBIC continues to be a challenge in modern cellular networks. Due to the large RLC layer buffers required to deal with short-term changes in channel capacity, the behavior of both the Slow Start and congestion avoidance phases may be heavily impacted by the lack of packet losses and the resulting bufferbloat. While existing congestion control algorithms like TCP BBR do tend to perform better even in the presence of large bottleneck buffers, they still tend to fill the buffer more than necessary and can have fairness issues when compared to loss-based algorithms.\n  In this paper, we analyze the issues with the use of loss-based congestion control algorithms by analyzing TCP CUBIC, which is currently the most popular variant. To mitigate the issues experienced by TCP CUBIC in cellular networks, we introduce TCP ROCCET, a latency-based extension of TCP CUBIC that responds to network congestion based on round-trip time in addition to packet loss.\n  Our findings show that TCP ROCCET can reduce latency and bufferbloat compared to the standard CUBIC implementation, without requiring a specific network architecture. Compared to TCP BBRv3, ROCCET offers similar throughput while maintaining lower overall latency. The evaluation was conducted in real 5G networks, including both stationary and mobile scenarios, confirming ROCCET's improved response to network congestion under varying conditions.</article>","contentLength":1526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Development of Implicit-Explicit Control Based Amphibious Centipede-Type Robot and Evaluation of its Mobile Performance","url":"https://arxiv.org/abs/2510.25280","date":1761796800,"author":"","guid":321651,"unread":true,"content":"<article>arXiv:2510.25280v1 Announce Type: new \nAbstract: Multi-legged mobile robots possess high mobility performance in rough terrain environments, stemming from their high postural stability, joint flexibility, and the redundancy provided by multiple legs. In prior research on navigating between different environments such as land and water, the primary strategy employed involves switching to a controller that generates an appropriate gait for the new environment upon entering it. However, designing appropriate gaits for each complex and diverse environment and accurately determining controller switching for each environment is challenging. Therefore, this research develops a centipede-type mobile robot that navigates both aquatic and terrestrial environments with a simple, unified control scheme, based on the implicit-explicit control philosophy and by ingeniously designing the robot's body structure. In this research, we developed the robot featuring flexible joints and left and right legs on each body segment and focused on the leg structure which has extensive contact with the environment. This paper evaluates the locomotion performance on land and water using the three developed leg structures, using the robot's leg slip rate and actuator energy consumption as evaluation metrics. The experimental results confirmed the existence of an appropriate leg structure capable of navigating both aquatic and terrestrial environments under identical control.</article>","contentLength":1469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation","url":"https://arxiv.org/abs/2510.25279","date":1761796800,"author":"","guid":321652,"unread":true,"content":"<article>arXiv:2510.25279v1 Announce Type: new \nAbstract: Source-free domain adaptation (SFDA) is a challenging task that tackles domain shifts using only a pre-trained source model and unlabeled target data. Existing SFDA methods are restricted by the fundamental limitation of source-target domain discrepancy. Non-generation SFDA methods suffer from unreliable pseudo-labels in challenging scenarios with large domain discrepancies, while generation-based SFDA methods are evidently degraded due to enlarged domain discrepancies in creating pseudo-source data. To address this limitation, we propose a novel generation-based framework named Diffusion-Driven Progressive Target Manipulation (DPTM) that leverages unlabeled target data as references to reliably generate and progressively refine a pseudo-target domain for SFDA. Specifically, we divide the target samples into a trust set and a non-trust set based on the reliability of pseudo-labels to sufficiently and reliably exploit their information. For samples from the non-trust set, we develop a manipulation strategy to semantically transform them into the newly assigned categories, while simultaneously maintaining them in the target distribution via a latent diffusion model. Furthermore, we design a progressive refinement mechanism that progressively reduces the domain discrepancy between the pseudo-target domain and the real target domain via iterative refinement. Experimental results demonstrate that DPTM outperforms existing methods by a large margin and achieves state-of-the-art performance on four prevailing SFDA benchmark datasets with different scales. Remarkably, DPTM can significantly enhance the performance by up to 18.6% in scenarios with large source-target gaps.</article>","contentLength":1741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DIRC-RAG: Accelerating Edge RAG with Robust High-Density and High-Loading-Bandwidth Digital In-ReRAM Computation","url":"https://arxiv.org/abs/2510.25278","date":1761796800,"author":"","guid":321653,"unread":true,"content":"<article>arXiv:2510.25278v1 Announce Type: new \nAbstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieval but faces challenges on edge devices due to high storage, energy, and latency demands. Computing-in-Memory (CIM) offers a promising solution by storing document embeddings in CIM macros and enabling in-situ parallel retrievals but is constrained by either low memory density or limited computational accuracy. To address these challenges, we present DIRCRAG, a novel edge RAG acceleration architecture leveraging Digital In-ReRAM Computation (DIRC). DIRC integrates a high-density multi-level ReRAM subarray with an SRAM cell, utilizing SRAM and differential sensing for robust ReRAM readout and digital multiply-accumulate (MAC) operations. By storing all document embeddings within the CIM macro, DIRC achieves ultra-low-power, single-cycle data loading, substantially reducing both energy consumption and latency compared to offchip DRAM. A query-stationary (QS) dataflow is supported for RAG tasks, minimizing on-chip data movement and reducing SRAM buffer requirements. We introduce error optimization for the DIRC ReRAM-SRAM cell by extracting the bit-wise spatial error distribution of the ReRAM subarray and applying targeted bit-wise data remapping. An error detection circuit is also implemented to enhance readout resilience against deviceand circuit-level variations. Simulation results demonstrate that DIRC-RAG under TSMC40nm process achieves an on-chip non-volatile memory density of 5.18Mb/mm2 and a throughput of 131 TOPS. It delivers a 4MB retrieval latency of 5.6{\\mu}s/query and an energy consumption of 0.956{\\mu}J/query, while maintaining the retrieval precision.</article>","contentLength":1753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Privacy-Preserving Ecosystem for Developing Machine Learning Algorithms Using Patient Data: Insights from the TUM.ai Makeathon","url":"https://arxiv.org/abs/2510.25277","date":1761796800,"author":"","guid":321654,"unread":true,"content":"<article>arXiv:2510.25277v1 Announce Type: new \nAbstract: The integration of clinical data offers significant potential for the development of personalized medicine. However, its use is severely restricted by the General Data Protection Regulation (GDPR), especially for small cohorts with rare diseases. High-quality, structured data is essential for the development of predictive medical AI. In this case study, we propose a novel, multi-stage approach to secure AI training: (1) The model is designed on a simulated clinical knowledge graph (cKG). This graph is used exclusively to represent the structural characteristics of the real cKG without revealing any sensitive content. (2) The model is then integrated into the FeatureCloud (FC) federated learning framework, where it is prepared in a single-client configuration within a protected execution environment. (3) Training then takes place within the hospital environment on the real cKG, either under the direct supervision of hospital staff or via a fully automated pipeline controlled by the hospital. (4) Finally, verified evaluation scripts are executed, which only return aggregated performance metrics. This enables immediate performance feedback without sensitive patient data or individual predictions, leaving the clinic. A fundamental element of this approach involves the incorporation of a cKG, which serves to organize multi-omics and patient data within the context of real-world hospital environments. This approach was successfully validated during the TUM.ai Makeathon 2024 (TUMaiM24) challenge set by the Dr. von Hauner Children's Hospital (HCH-LMU): 50 students developed models for patient classification and diagnosis without access to real data. Deploying secure algorithms via federated frameworks, such as the FC framework, could be a practical way of achieving privacy-preserving AI in healthcare.</article>","contentLength":1873,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA","url":"https://arxiv.org/abs/2510.25273","date":1761796800,"author":"","guid":321655,"unread":true,"content":"<article>arXiv:2510.25273v1 Announce Type: new \nAbstract: Domain-specific question answering in low-resource languages faces two key challenges: scarcity of annotated datasets and limited domain knowledge in general-purpose language models. In this work, we present a multi-stage finetuning strategy to adapt lightweight language models to the Hindi tourism domain by leveraging both original and synthetic training data. Synthetic question-answer pairs are generated using large LLMs (LLaMA-70B, Phi-14B) and used to augment the limited original dataset. We explore several training methodologies and analyse their impact on domain generalisation. Our results demonstrate that large models can efficiently generate synthetic data, while small models can effectively adapt to it, offering a scalable pathway for low-resource, domain-specific QA.</article>","contentLength":836,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Design of mmWave Initial Access Codebooks using Reinforcement Learning","url":"https://arxiv.org/abs/2510.25271","date":1761796800,"author":"","guid":321656,"unread":true,"content":"<article>arXiv:2510.25271v1 Announce Type: new \nAbstract: Initial access (IA) is the process by which user equipment (UE) establishes its first connection with a base station. In 5G systems, particularly at millimeter-wave frequencies, IA integrates beam management to support highly directional transmissions. The base station employs a codebook of beams for the transmission of Synchronization Signal Blocks (SSBs), which are periodically swept to detect and connect users. The design of this SSB codebook is critical for ensuring reliable, wide-area coverage. In current networks, SSB codebooks are meticulously engineered by domain experts. While these expert-defined codebooks provide a robust baseline, they lack flexibility in dynamic or heterogeneous environments where user distributions vary, limiting their overall effectiveness. This paper proposes a hybrid Reinforcement Learning (RL) framework for adaptive SSB codebook design. Building on top of expert knowledge, the RL agent leverages a pool of expert-designed SSB beams and learns to adaptively select or combine them based on real-time feedback. This enables the agent to dynamically tailor codebooks to the actual environment, without requiring explicit user location information, while always respecting practical beam constraints. Simulation results demonstrate that, on average, the proposed approach improves user connectivity by 10.8$\\%$ compared to static expert configurations. These findings highlight the potential of combining expert knowledge with data-driven optimization to achieve more intelligent, flexible, and resilient beam management in next-generation wireless networks.</article>","contentLength":1651,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TECS/Rust: Memory-safe Component Framework for Embedded Systems","url":"https://arxiv.org/abs/2510.25270","date":1761796800,"author":"","guid":321657,"unread":true,"content":"<article>arXiv:2510.25270v1 Announce Type: new \nAbstract: As embedded systems grow in complexity and scale due to increased functional diversity, component-based development (CBD) emerges as a solution to streamline their architecture and enhance functionality reuse. CBD typically utilizes the C programming language for its direct hardware access and low-level operations, despite its susceptibility to memory-related issues. To address these concerns, this paper proposes TECS/Rust, a Rust-based framework specifically designed for TECS, which is a component framework for embedded systems. It leverages Rust's compile-time memory-safe features, such as lifetime and borrowing, to mitigate memory vulnerabilities common with C. The proposed framework not only ensures memory safety but also maintains the flexibility of CBD, automates Rust code generation for CBD components, and supports efficient integration with real-time operating systems. An evaluation of the amount of generated code indicates that the code generated by this paper framework accounts for a large percentage of the actual code. Compared to code developed without the proposed framework, the difference in execution time is minimal, indicating that the overhead introduced by the proposed framework is negligible.</article>","contentLength":1279,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The influence of the random numbers quality on the results in stochastic simulations and machine learning","url":"https://arxiv.org/abs/2510.25269","date":1761796800,"author":"","guid":321658,"unread":true,"content":"<article>arXiv:2510.25269v1 Announce Type: new \nAbstract: Pseudorandom number generators (PRNGs) are ubiquitous in stochastic simulations and machine learning (ML), where they drive sampling, parameter initialization, regularization, and data shuffling. While widely used, the potential impact of PRNG statistical quality on computational results remains underexplored. In this study, we investigate whether differences in PRNG quality, as measured by standard statistical test suites, can influence outcomes in representative stochastic applications. Seven PRNGs were evaluated, ranging from low-quality linear congruential generators (LCGs) with known statistical deficiencies to high-quality generators such as Mersenne Twister, PCG, and Philox. We applied these PRNGs to four distinct tasks: an epidemiological agent-based model (ABM), two independent from-scratch MNIST classification implementations (Python/NumPy and C++), and a reinforcement learning (RL) CartPole environment. Each experiment was repeated 30 times per generator using fixed seeds to ensure reproducibility, and outputs were compared using appropriate statistical analyses. Results show that very poor statistical quality, as in the ''bad'' LCG failing 125 TestU01 Crush tests, produces significant deviations in ABM epidemic dynamics, reduces MNIST classification accuracy, and severely degrades RL performance. In contrast, mid-and good-quality LCGs-despite failing a limited number of Crush or BigCrush tests-performed comparably to top-tier PRNGs in most tasks, with the RL experiment being the primary exception where performance scaled with statistical quality. Our findings indicate that, once a generator meets a sufficient statistical robustness threshold, its family or design has negligible impact on outcomes for most workloads, allowing selection to be guided by performance and implementation considerations. However, the use of low-quality PRNGs in sensitive stochastic computations can introduce substantial and systematic errors.</article>","contentLength":2012,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation","url":"https://arxiv.org/abs/2510.25268","date":1761796800,"author":"","guid":321659,"unread":true,"content":"<article>arXiv:2510.25268v1 Announce Type: new \nAbstract: Generating hand grasps with language instructions is a widely studied topic that benefits from embodied AI and VR/AR applications. While transferring into hand articulatied object interaction (HAOI), the hand grasps synthesis requires not only object functionality but also long-term manipulation sequence along the object deformation. This paper proposes a novel HAOI sequence generation framework SynHLMA, to synthesize hand language manipulation for articulated objects. Given a complete point cloud of an articulated object, we utilize a discrete HAOI representation to model each hand object interaction frame. Along with the natural language embeddings, the representations are trained by an HAOI manipulation language model to align the grasping process with its language description in a shared representation space. A joint-aware loss is employed to ensure hand grasps follow the dynamic variations of articulated object joints. In this way, our SynHLMA achieves three typical hand manipulation tasks for articulated objects of HAOI generation, HAOI prediction and HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset and experimental results demonstrate the superior hand grasp sequence generation performance comparing with state-of-the-art. We also show a robotics grasp application that enables dexterous grasps execution from imitation learning using the manipulation sequence provided by our SynHLMA. Our codes and datasets will be made publicly available.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Joint Spatial Registration and Resource Allocation for Transmissive RIS Enabled Cooperative ISCC Networks","url":"https://arxiv.org/abs/2510.25266","date":1761796800,"author":"","guid":321660,"unread":true,"content":"<article>arXiv:2510.25266v1 Announce Type: new \nAbstract: In this paper, we propose a novel transmissive reconfigurable intelligent surface (TRIS) transceiver-driven cooperative integrated sensing, computing, and communication (ISCC) network to meet the requirement for a diverse network with low energy consumption. The cooperative base stations (BSs) are equipped with TRIS transceivers to accomplish sensing data acquisition, communication offloading, and computation in a time slot. In order to obtain higher cooperation gain, we utilize a signal-level spatial registration algorithm, which is realized by adjusting the beamwidth. Meanwhile, for more efficient offloading of the computational task, multistream communication is considered, and rank-$N$ constraints are introduced, which are handled using an iterative rank minimization (IRM) scheme. We construct an optimization problem with the objective function of minimizing the total energy consumption of the network to jointly optimize the beamforming matrix, time slot allocation, sensing data allocation and sensing beam scheduling variables. Due to the coupling of the variables, the proposed problem is a non-convex optimization problem, which we decouple and solve using a block coordinate descent (BCD) scheme. Finally, numerical simulation results confirm the superiority of the proposed scheme in improving the overall network performance and reducing the total energy consumption of the network.</article>","contentLength":1456,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation","url":"https://arxiv.org/abs/2510.25263","date":1761796800,"author":"","guid":321661,"unread":true,"content":"<article>arXiv:2510.25263v1 Announce Type: new \nAbstract: We propose LangHOPS, the first Multimodal Large Language Model (MLLM) based framework for open-vocabulary object-part instance segmentation. Given an image, LangHOPS can jointly detect and segment hierarchical object and part instances from open-vocabulary candidate categories. Unlike prior approaches that rely on heuristic or learnable visual grouping, our approach grounds object-part hierarchies in language space. It integrates the MLLM into the object-part parsing pipeline to leverage its rich knowledge and reasoning capabilities, and link multi-granularity concepts within the hierarchies. We evaluate LangHOPS across multiple challenging scenarios, including in-domain and cross-dataset object-part instance segmentation, and zero-shot semantic segmentation. LangHOPS achieves state-of-the-art results, surpassing previous methods by 5.5% Average Precision (AP) (in-domain) and 4.8% (cross-dataset) on the PartImageNet dataset and by 2.5% mIOU on unseen object parts in ADE20K (zero-shot). Ablation studies further validate the effectiveness of the language-grounded hierarchy and MLLM driven part query refinement strategy. The code will be released here.</article>","contentLength":1216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning","url":"https://arxiv.org/abs/2510.25262","date":1761796800,"author":"","guid":321662,"unread":true,"content":"<article>arXiv:2510.25262v1 Announce Type: new \nAbstract: Normalization is fundamental to deep learning, but existing approaches such as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero mean and unit variance, stabilizing training without controlling how representations capture task-relevant information. We propose IB-Inspired Normalization (IBNorm), a simple yet powerful family of methods grounded in the Information Bottleneck principle. IBNorm introduces bounded compression operations that encourage embeddings to preserve predictive information while suppressing nuisance variability, yielding more informative representations while retaining the stability and compatibility of standard normalization. Theoretically, we prove that IBNorm achieves a higher IB value and tighter generalization bounds than variance-centric methods. Empirically, IBNorm consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual information analysis confirming superior information bottleneck behavior. Code will be released publicly.</article>","contentLength":1132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Systems of Graph Formulas and their Equivalence to Alternating Graph Automata","url":"https://arxiv.org/abs/2510.25260","date":1761796800,"author":"","guid":321663,"unread":true,"content":"<article>arXiv:2510.25260v1 Announce Type: new \nAbstract: Graph-based modeling plays a fundamental role in many areas of computer science. In this paper, we introduce systems of graph formulas with variables for specifying graph properties; this notion generalizes the graph formulas introduced in earlier work by incorporating recursion. We show that these formula systems have the same expressive power as alternating graph automata, a computational model that extends traditional finite-state automata to graphs, and allows both existential and universal states. In particular, we provide a bidirectional translation between formula systems and alternating graph automata, proving their equivalence in specifying graph languages. This result implies that alternating graph automata can be naturally represented using logic-based formulations, thus bridging the gap between automata-theoretic and logic-based approaches to graph language specification.</article>","contentLength":945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation","url":"https://arxiv.org/abs/2510.25259","date":1761796800,"author":"","guid":321664,"unread":true,"content":"<article>arXiv:2510.25259v1 Announce Type: new \nAbstract: Recently, convolutional filters have been increasingly adopted in sequential recommendation for their ability to capture local sequential patterns. However, most of these models complement convolutional filters with self-attention. This is because convolutional filters alone, generally fixed filters, struggle to capture global interactions necessary for accurate recommendation. We propose Time-Variant Convolutional Filters for Sequential Recommendation (TV-Rec), a model inspired by graph signal processing, where time-variant graph filters capture position-dependent temporal variations in user sequences. By replacing both fixed kernels and self-attention with time-variant filters, TV-Rec achieves higher expressive power and better captures complex interaction patterns in user behavior. This design not only eliminates the need for self-attention but also reduces computation while accelerating inference. Extensive experiments on six public benchmarks show that TV-Rec outperforms state-of-the-art baselines by an average of 7.49%.</article>","contentLength":1090,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MoEntwine: Unleashing the Potential of Wafer-scale Chips for Large-scale Expert Parallel Inference","url":"https://arxiv.org/abs/2510.25258","date":1761796800,"author":"","guid":321665,"unread":true,"content":"<article>arXiv:2510.25258v1 Announce Type: new \nAbstract: As large language models (LLMs) continue to scale up, mixture-of-experts (MoE) has become a common technology in SOTA models. MoE models rely on expert parallelism (EP) to alleviate memory bottleneck, which introduces all-to-all communication to dispatch and combine tokens across devices. However, in widely-adopted GPU clusters, high-overhead cross-node communication makes all-to-all expensive, hindering the adoption of EP. Recently, wafer-scale chips (WSCs) have emerged as a platform integrating numerous devices on a wafer-sized interposer. WSCs provide a unified high-performance network connecting all devices, presenting a promising potential for hosting MoE models. Yet, their network is restricted to a mesh topology, causing imbalanced communication pressure and performance loss. Moreover, the lack of on-wafer disk leads to high-overhead expert migration on the critical path.\n  To fully unleash this potential, we first propose Entwined Ring Mapping (ER-Mapping), which co-designs the mapping of attention and MoE layers to balance communication pressure and achieve better performance. We find that under ER-Mapping, the distribution of cold and hot links in the attention and MoE layers is complementary. Therefore, to hide the migration overhead, we propose the Non-invasive Balancer (NI-Balancer), which splits a complete expert migration into multiple steps and alternately utilizes the cold links of both layers. Evaluation shows ER-Mapping achieves communication reduction up to 62%. NI-Balancer further delivers 54% and 22% improvements in MoE computation and communication, respectively. Compared with the SOTA NVL72 supernode, the WSC platform delivers an average 39% higher per-device MoE performance owing to its scalability to larger EP.</article>","contentLength":1815,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models","url":"https://arxiv.org/abs/2510.25257","date":1761796800,"author":"","guid":321666,"unread":true,"content":"<article>arXiv:2510.25257v1 Announce Type: new \nAbstract: Real-time object detection has achieved substantial progress through meticulously designed architectures and optimization strategies. However, the pursuit of high-speed inference via lightweight network designs often leads to degraded feature representation, which hinders further performance improvements and practical on-device deployment. In this paper, we propose a cost-effective and highly adaptable distillation framework that harnesses the rapidly evolving capabilities of Vision Foundation Models (VFMs) to enhance lightweight object detectors. Given the significant architectural and learning objective disparities between VFMs and resource-constrained detectors, achieving stable and task-aligned semantic transfer is challenging. To address this, on one hand, we introduce a Deep Semantic Injector (DSI) module that facilitates the integration of high-level representations from VFMs into the deep layers of the detector. On the other hand, we devise a Gradient-guided Adaptive Modulation (GAM) strategy, which dynamically adjusts the intensity of semantic transfer based on gradient norm ratios. Without increasing deployment and inference overhead, our approach painlessly delivers striking and consistent performance gains across diverse DETR-based models, underscoring its practical utility for real-time detection. Our new model family, RT-DETRv4, achieves state-of-the-art results on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at corresponding speeds of 273/169/124/78 FPS.</article>","contentLength":1546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time-Optimal Transport of Loosely Placed Liquid Filled Cups along Prescribed Paths","url":"https://arxiv.org/abs/2510.25255","date":1761796800,"author":"","guid":321667,"unread":true,"content":"<article>arXiv:2510.25255v1 Announce Type: new \nAbstract: Handling loosely placed objects with robotic manipulators is a difficult task from the point of view of trajectory planning and control. This becomes even more challenging when the object to be handled is a container filled with liquid. This paper addresses the task of transporting a liquid-filled cup placed on a tray along a prescribed path in shortest time. The objective is to minimize swapping, thus avoiding spillage of the fluid. To this end, the sloshing dynamics is incorporated into the dynamic model used within the optimal control problem formulation. The optimization problem is solved using a direct multiple shooting approach.</article>","contentLength":691,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Up Bayesian DAG Sampling","url":"https://arxiv.org/abs/2510.25254","date":1761796800,"author":"","guid":321668,"unread":true,"content":"<article>arXiv:2510.25254v1 Announce Type: new \nAbstract: Bayesian inference of Bayesian network structures is often performed by sampling directed acyclic graphs along an appropriately constructed Markov chain. We present two techniques to improve sampling. First, we give an efficient implementation of basic moves, which add, delete, or reverse a single arc. Second, we expedite summing over parent sets, an expensive task required for more sophisticated moves: we devise a preprocessing method to prune possible parent sets so as to approximately preserve the sums. Our empirical study shows that our techniques can yield substantial efficiency gains compared to previous methods.</article>","contentLength":675,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spectral analysis of the stiffness matrix sequence in the approximated Stokes equation","url":"https://arxiv.org/abs/2510.25252","date":1761796800,"author":"","guid":321669,"unread":true,"content":"<article>arXiv:2510.25252v1 Announce Type: new \nAbstract: In the present paper, we analyze in detail the spectral features of the matrix sequences arising from the Taylor-Hood $\\mathbb{P}_2$-$\\mathbb{P}_1$ approximation of variable viscosity for $2d$ Stokes problem under weak assumptions on the regularity of the diffusion. Localization and distributional spectral results are provided, accompanied by numerical tests and visualizations. A preliminary study of the impact of our findings on the preconditioning problem is also presented. A final section with concluding remarks and open problems ends the current work.</article>","contentLength":610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training","url":"https://arxiv.org/abs/2510.25244","date":1761796800,"author":"","guid":321670,"unread":true,"content":"<article>arXiv:2510.25244v1 Announce Type: new \nAbstract: Recent studies \\citep{gur2018gradient,song2024does, wen2024understanding} highlight a fundamental dichotomy in deep learning optimization: Although parameter updates along the top eigendirections of the loss Hessian (Dom-space) capture most of the update magnitude, they often contribute minimally to loss reduction. In contrast, updates in the orthogonal component (Bulk-space) have smaller magnitudes but drive most learning progress. In this work, we further advance the understanding of this phenomenon and introduce the \\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play framework. BSFA accelerates training by differentially scaling update components projected onto these distinct subspaces, simultaneously enhancing stability by moderating updates in the dominant subspace and boosting convergence speed by amplifying those in the bulk-space. To ensure BSFA is both practical and scalable for contemporary large models, we introduce two key innovations: an efficient estimator using Principal Component Analysis (PCA) on historical updates for fast subspace estimation, and a block-wise strategy that applies this estimation on a per-parameter-block basis. These designs make BSFA computationally tractable and highly effective. We demonstrate BSFA's acceleration across various tasks, notably achieving approximately 2$\\times$ speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on OpenWebText compared to vanilla AdamW.</article>","contentLength":1514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TECS/Rust-OE: Optimizing Exclusive Control in Rust-based Component Systems for Embedded Devices","url":"https://arxiv.org/abs/2510.25242","date":1761796800,"author":"","guid":321671,"unread":true,"content":"<article>arXiv:2510.25242v1 Announce Type: new \nAbstract: The diversification of functionalities and the development of the IoT are making embedded systems larger and more complex in structure. Ensuring system reliability, especially in terms of security, necessitates selecting an appropriate programming language. As part of existing research, TECS/Rust has been proposed as a framework that combines Rust and component-based development (CBD) to enable scalable system design and enhanced reliability. This framework represents system structures using static mutable variables, but excessive exclusive controls applied to ensure thread safety have led to performance degradation. This paper proposes TECS/Rust-OE, a memory-safe CBD framework utilizing call flows to address these limitations. The proposed Rust code leverages real-time OS exclusive control mechanisms, optimizing performance without compromising reusability. Rust code is automatically generated based on component descriptions. Evaluations demonstrate reduced overhead due to optimized exclusion control and high reusability of the generated code.</article>","contentLength":1109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"One-shot Humanoid Whole-body Motion Learning","url":"https://arxiv.org/abs/2510.25241","date":1761796800,"author":"","guid":321672,"unread":true,"content":"<article>arXiv:2510.25241v1 Announce Type: new \nAbstract: Whole-body humanoid motion represents a cornerstone challenge in robotics, integrating balance, coordination, and adaptability to enable human-like behaviors. However, existing methods typically require multiple training samples per motion category, rendering the collection of high-quality human motion datasets both labor-intensive and costly. To address this, we propose a novel approach that trains effective humanoid motion policies using only a single non-walking target motion sample alongside readily available walking motions. The core idea lies in leveraging order-preserving optimal transport to compute distances between walking and non-walking sequences, followed by interpolation along geodesics to generate new intermediate pose skeletons, which are then optimized for collision-free configurations and retargeted to the humanoid before integration into a simulated environment for policy training via reinforcement learning. Experimental evaluations on the CMU MoCap dataset demonstrate that our method consistently outperforms baselines, achieving superior performance across metrics. Code will be released upon acceptance.</article>","contentLength":1189,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mapping and Classification of Trees Outside Forests using Deep Learning","url":"https://arxiv.org/abs/2510.25239","date":1761796800,"author":"","guid":321673,"unread":true,"content":"<article>arXiv:2510.25239v1 Announce Type: new \nAbstract: Trees Outside Forests (TOF) play an important role in agricultural landscapes by supporting biodiversity, sequestering carbon, and regulating microclimates. Yet, most studies have treated TOF as a single class or relied on rigid rule-based thresholds, limiting ecological interpretation and adaptability across regions. To address this, we evaluate deep learning for TOF classification using a newly generated dataset and high-resolution aerial imagery from four agricultural landscapes in Germany. Specifically, we compare convolutional neural networks (CNNs), vision transformers, and hybrid CNN-transformer models across six semantic segmentation architectures (ABCNet, LSKNet, FT-UNetFormer, DC-Swin, BANet, and U-Net) to map four categories of woody vegetation: Forest, Patch, Linear, and Tree, derived from previous studies and governmental products. Overall, the models achieved good classification accuracy across the four landscapes, with the FT-UNetFormer performing best (mean Intersection-over-Union 0.74; mean F1 score 0.84), underscoring the importance of spatial context understanding in TOF mapping and classification. Our results show good results for Forest and Linear class and reveal challenges particularly in classifying complex structures with high edge density, notably the Patch and Tree class. Our generalization experiments highlight the need for regionally diverse training data to ensure reliable large-scale mapping. The dataset and code are openly available at https://github.com/Moerizzy/TOFMapper</article>","contentLength":1578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VADB: A Large-Scale Video Aesthetic Database with Professional and Multi-Dimensional Annotations","url":"https://arxiv.org/abs/2510.25238","date":1761796800,"author":"","guid":321674,"unread":true,"content":"<article>arXiv:2510.25238v1 Announce Type: new \nAbstract: Video aesthetic assessment, a vital area in multimedia computing, integrates computer vision with human cognition. Its progress is limited by the lack of standardized datasets and robust models, as the temporal dynamics of video and multimodal fusion challenges hinder direct application of image-based methods. This study introduces VADB, the largest video aesthetic database with 10,490 diverse videos annotated by 37 professionals across multiple aesthetic dimensions, including overall and attribute-specific aesthetic scores, rich language comments and objective tags. We propose VADB-Net, a dual-modal pre-training framework with a two-stage training strategy, which outperforms existing video quality assessment models in scoring tasks and supports downstream video aesthetic assessment tasks. The dataset and source code are available at https://github.com/BestiVictory/VADB.</article>","contentLength":932,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepShield: Fortifying Deepfake Video Detection with Local and Global Forgery Analysis","url":"https://arxiv.org/abs/2510.25237","date":1761796800,"author":"","guid":321675,"unread":true,"content":"<article>arXiv:2510.25237v1 Announce Type: new \nAbstract: Recent advances in deep generative models have made it easier to manipulate face videos, raising significant concerns about their potential misuse for fraud and misinformation. Existing detectors often perform well in in-domain scenarios but fail to generalize across diverse manipulation techniques due to their reliance on forgery-specific artifacts. In this work, we introduce DeepShield, a novel deepfake detection framework that balances local sensitivity and global generalization to improve robustness across unseen forgeries. DeepShield enhances the CLIP-ViT encoder through two key components: Local Patch Guidance (LPG) and Global Forgery Diversification (GFD). LPG applies spatiotemporal artifact modeling and patch-wise supervision to capture fine-grained inconsistencies often overlooked by global models. GFD introduces domain feature augmentation, leveraging domain-bridging and boundary-expanding feature generation to synthesize diverse forgeries, mitigating overfitting and enhancing cross-domain adaptability. Through the integration of novel local and global analysis for deepfake detection, DeepShield outperforms state-of-the-art methods in cross-dataset and cross-manipulation evaluations, achieving superior robustness against unseen deepfake attacks.</article>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Disentangled Speech- and Expression-Driven Blendshapes for 3D Talking Face Animation","url":"https://arxiv.org/abs/2510.25234","date":1761796800,"author":"","guid":321676,"unread":true,"content":"<article>arXiv:2510.25234v1 Announce Type: new \nAbstract: Expressions are fundamental to conveying human emotions. With the rapid advancement of AI-generated content (AIGC), realistic and expressive 3D facial animation has become increasingly crucial. Despite recent progress in speech-driven lip-sync for talking-face animation, generating emotionally expressive talking faces remains underexplored. A major obstacle is the scarcity of real emotional 3D talking-face datasets due to the high cost of data capture. To address this, we model facial animation driven by both speech and emotion as a linear additive problem. Leveraging a 3D talking-face dataset with neutral expressions (VOCAset) and a dataset of 3D expression sequences (Florence4D), we jointly learn a set of blendshapes driven by speech and emotion. We introduce a sparsity constraint loss to encourage disentanglement between the two types of blendshapes while allowing the model to capture inherent secondary cross-domain deformations present in the training data. The learned blendshapes can be further mapped to the expression and jaw pose parameters of the FLAME model, enabling the animation of 3D Gaussian avatars. Qualitative and quantitative experiments demonstrate that our method naturally generates talking faces with specified expressions while maintaining accurate lip synchronization. Perceptual studies further show that our approach achieves superior emotional expressivity compared to existing methods, without compromising lip-sync quality.</article>","contentLength":1517,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hybrid Vision Servoing with Depp Alignment and GRU-Based Occlusion Recovery","url":"https://arxiv.org/abs/2510.25233","date":1761796800,"author":"","guid":321677,"unread":true,"content":"<article>arXiv:2510.25233v1 Announce Type: new \nAbstract: Vision-based control systems, such as image-based visual servoing (IBVS), have been extensively explored for precise robot manipulation. A persistent challenge, however, is maintaining robust target tracking under partial or full occlusions. Classical methods like Lucas-Kanade (LK) offer lightweight tracking but are fragile to occlusion and drift, while deep learning-based approaches often require continuous visibility and intensive computation. To address these gaps, we propose a hybrid visual tracking framework that bridges advanced perception with real-time servo control. First, a fast global template matcher constrains the pose search region; next, a deep-feature Lucas-Kanade module operating on early VGG layers refines alignment to sub-pixel accuracy (&lt;2px); then, a lightweight residual regressor corrects local misalignments caused by texture degradation or partial occlusion. When visual confidence falls below a threshold, a GRU-based predictor seamlessly extrapolates pose updates from recent motion history. Crucially, the pipeline's final outputs-translation, rotation, and scale deltas-are packaged as direct control signals for 30Hz image-based servo loops. Evaluated on handheld video sequences with up to 90% occlusion, our system sustains under 2px tracking error, demonstrating the robustness and low-latency precision essential for reliable real-world robot vision applications.</article>","contentLength":1456,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Medical Records to Diagnostic Dialogues: A Clinical-Grounded Approach and Dataset for Psychiatric Comorbidity","url":"https://arxiv.org/abs/2510.25232","date":1761796800,"author":"","guid":321678,"unread":true,"content":"<article>arXiv:2510.25232v1 Announce Type: new \nAbstract: Psychiatric comorbidity is clinically significant yet challenging due to the complexity of multiple co-occurring disorders. To address this, we develop a novel approach integrating synthetic patient electronic medical record (EMR) construction and multi-agent diagnostic dialogue generation. We create 502 synthetic EMRs for common comorbid conditions using a pipeline that ensures clinical relevance and diversity. Our multi-agent framework transfers the clinical interview protocol into a hierarchical state machine and context tree, supporting over 130 diagnostic states while maintaining clinical standards. Through this rigorous process, we construct PsyCoTalk, the first large-scale dialogue dataset supporting comorbidity, containing 3,000 multi-turn diagnostic dialogues validated by psychiatrists. This dataset enhances diagnostic accuracy and treatment planning, offering a valuable resource for psychiatric comorbidity research. Compared to real-world clinical transcripts, PsyCoTalk exhibits high structural and linguistic fidelity in terms of dialogue length, token distribution, and diagnostic reasoning strategies. Licensed psychiatrists confirm the realism and diagnostic validity of the dialogues. This dataset enables the development and evaluation of models capable of multi-disorder psychiatric screening in a single conversational pass.</article>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Balanced conic rectified flow","url":"https://arxiv.org/abs/2510.25229","date":1761796800,"author":"","guid":321679,"unread":true,"content":"<article>arXiv:2510.25229v1 Announce Type: new \nAbstract: Rectified flow is a generative model that learns smooth transport mappings between two distributions through an ordinary differential equation (ODE). Unlike diffusion-based generative models, which require costly numerical integration of a generative ODE to sample images with state-of-the-art quality, rectified flow uses an iterative process called reflow to learn smooth and straight ODE paths. This allows for relatively simple and efficient generation of high-quality images. However, rectified flow still faces several challenges. 1) The reflow process requires a large number of generative pairs to preserve the target distribution, leading to significant computational costs. 2) Since the model is typically trained using only generated image pairs, its performance heavily depends on the 1-rectified flow model, causing it to become biased towards the generated data.\n  In this work, we experimentally expose the limitations of the original rectified flow and propose a novel approach that incorporates real images into the training process. By preserving the ODE paths for real images, our method effectively reduces reliance on large amounts of generated data. Instead, we demonstrate that the reflow process can be conducted efficiently using a much smaller set of generated and real images. In CIFAR-10, we achieved significantly better FID scores, not only in one-step generation but also in full-step simulations, while using only of the generative pairs compared to the original method. Furthermore, our approach induces straighter paths and avoids saturation on generated images during reflow, leading to more robust ODE learning while preserving the distribution of real images.</article>","contentLength":1745,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Studies for : A Human-AI Co-Creative Sound Artwork Using a Real-time Multi-channel Sound Generation Model","url":"https://arxiv.org/abs/2510.25228","date":1761796800,"author":"","guid":321680,"unread":true,"content":"<article>arXiv:2510.25228v1 Announce Type: new \nAbstract: This paper explores the integration of AI technologies into the artistic workflow through the creation of Studies for, a generative sound installation developed in collaboration with sound artist Evala (https://www.ntticc.or.jp/en/archive/works/studies-for/). The installation employs SpecMaskGIT, a lightweight yet high-quality sound generation AI model, to generate and playback eight-channel sound in real-time, creating an immersive auditory experience over the course of a three-month exhibition. The work is grounded in the concept of a \"new form of archive,\" which aims to preserve the artistic style of an artist while expanding beyond artists' past artworks by continued generation of new sound elements. This speculative approach to archival preservation is facilitated by training the AI model on a dataset consisting of over 200 hours of Evala's past sound artworks.\n  By addressing key requirements in the co-creation of art using AI, this study highlights the value of the following aspects: (1) the necessity of integrating artist feedback, (2) datasets derived from an artist's past works, and (3) ensuring the inclusion of unexpected, novel outputs. In Studies for, the model was designed to reflect the artist's artistic identity while generating new, previously unheard sounds, making it a fitting realization of the concept of \"a new form of archive.\" We propose a Human-AI co-creation framework for effectively incorporating sound generation AI models into the sound art creation process and suggest new possibilities for creating and archiving sound art that extend an artist's work beyond their physical existence. Demo page: https://sony.github.io/studies-for/</article>","contentLength":1733,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation","url":"https://arxiv.org/abs/2510.25227","date":1761796800,"author":"","guid":321681,"unread":true,"content":"<article>arXiv:2510.25227v1 Announce Type: new \nAbstract: Source-Free Domain Adaptation (SFDA) is emerging as a compelling solution for medical image segmentation under privacy constraints, yet current approaches often ignore sample difficulty and struggle with noisy supervision under domain shift. We present a new SFDA framework that leverages Hard Sample Selection and Denoised Patch Mixing to progressively align target distributions. First, unlabeled images are partitioned into reliable and unreliable subsets through entropy-similarity analysis, allowing adaptation to start from easy samples and gradually incorporate harder ones. Next, pseudo-labels are refined via Monte Carlo-based denoising masks, which suppress unreliable pixels and stabilize training. Finally, intra- and inter-domain objectives mix patches between subsets, transferring reliable semantics while mitigating noise. Experiments on benchmark datasets show consistent gains over prior SFDA and UDA methods, delivering more accurate boundary delineation and achieving state-of-the-art Dice and ASSD scores. Our study highlights the importance of progressive adaptation and denoised supervision for robust segmentation under domain shift.</article>","contentLength":1206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning","url":"https://arxiv.org/abs/2510.25226","date":1761796800,"author":"","guid":321682,"unread":true,"content":"<article>arXiv:2510.25226v1 Announce Type: new \nAbstract: Positive--Unlabeled (PU) learning considers settings in which only positive and unlabeled data are available, while negatives are missing or left unlabeled. This situation is common in real applications where annotating reliable negatives is difficult or costly. Despite substantial progress in PU learning, the multi-class case (MPU) remains challenging: many existing approaches do not ensure \\emph{unbiased risk estimation}, which limits performance and stability. We propose a cost-sensitive multi-class PU method based on \\emph{adaptive loss weighting}. Within the empirical risk minimization framework, we assign distinct, data-dependent weights to the positive and \\emph{inferred-negative} (from the unlabeled mixture) loss components so that the resulting empirical objective is an unbiased estimator of the target risk. We formalize the MPU data-generating process and establish a generalization error bound for the proposed estimator. Extensive experiments on \\textbf{eight} public datasets, spanning varying class priors and numbers of classes, show consistent gains over strong baselines in both accuracy and stability.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hallucination Localization in Video Captioning","url":"https://arxiv.org/abs/2510.25225","date":1761796800,"author":"","guid":321683,"unread":true,"content":"<article>arXiv:2510.25225v1 Announce Type: new \nAbstract: We propose a novel task, hallucination localization in video captioning, which aims to identify hallucinations in video captions at the span level (i.e. individual words or phrases). This allows for a more detailed analysis of hallucinations compared to existing sentence-level hallucination detection task. To establish a benchmark for hallucination localization, we construct HLVC-Dataset, a carefully curated dataset created by manually annotating 1,167 video-caption pairs from VideoLLM-generated captions. We further implement a VideoLLM-based baseline method and conduct quantitative and qualitative evaluations to benchmark current performance on hallucination localization.</article>","contentLength":730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProMediate: A Socio-cognitive framework for evaluating proactive agents in multi-party negotiation","url":"https://arxiv.org/abs/2510.25224","date":1761796800,"author":"","guid":321684,"unread":true,"content":"<article>arXiv:2510.25224v1 Announce Type: new \nAbstract: While Large Language Models (LLMs) are increasingly used in agentic frameworks to assist individual users, there is a growing need for agents that can proactively manage complex, multi-party collaboration. Systematic evaluation methods for such proactive agents remain scarce, limiting progress in developing AI that can effectively support multiple people together. Negotiation offers a demanding testbed for this challenge, requiring socio-cognitive intelligence to navigate conflicting interests between multiple participants and multiple topics and build consensus. Here, we present ProMediate, the first framework for evaluating proactive AI mediator agents in complex, multi-topic, multi-party negotiations. ProMediate consists of two core components: (i) a simulation testbed based on realistic negotiation cases and theory-driven difficulty levels (ProMediate-Easy, ProMediate-Medium, and ProMediate-Hard), with a plug-and-play proactive AI mediator grounded in socio-cognitive mediation theories, capable of flexibly deciding when and how to intervene; and (ii) a socio-cognitive evaluation framework with a new suite of metrics to measure consensus changes, intervention latency, mediator effectiveness, and intelligence. Together, these components establish a systematic framework for assessing the socio-cognitive intelligence of proactive AI agents in multi-party settings. Our results show that a socially intelligent mediator agent outperforms a generic baseline, via faster, better-targeted interventions. In the ProMediate-Hard setting, our social mediator increases consensus change by 3.6 percentage points compared to the generic baseline (10.65\\% vs 7.01\\%) while being 77\\% faster in response (15.98s vs. 3.71s). In conclusion, ProMediate provides a rigorous, theory-grounded testbed to advance the development of proactive, socially intelligent agents.</article>","contentLength":1924,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FELA: A Multi-Agent Evolutionary System for Feature Engineering of Industrial Event Log Data","url":"https://arxiv.org/abs/2510.25223","date":1761796800,"author":"","guid":321685,"unread":true,"content":"<article>arXiv:2510.25223v1 Announce Type: new \nAbstract: Event log data, recording fine-grained user actions and system events, represent one of the most valuable assets for modern digital services. However, the complexity and heterogeneity of industrial event logs--characterized by large scale, high dimensionality, diverse data types, and intricate temporal or relational structures--make feature engineering extremely challenging. Existing automatic feature engineering approaches, such as AutoML or genetic methods, often suffer from limited explainability, rigid predefined operations, and poor adaptability to complicated heterogeneous data. In this paper, we propose FELA (Feature Engineering LLM Agents), a multi-agent evolutionary system that autonomously extracts meaningful and high-performing features from complex industrial event log data. FELA integrates the reasoning and coding capabilities of large language models (LLMs) with an insight-guided self-evolution paradigm. Specifically, FELA employs specialized agents--Idea Agents, Code Agents, and Critic Agents--to collaboratively generate, validate, and implement novel feature ideas. An Evaluation Agent summarizes feedback and updates a hierarchical knowledge base and dual-memory system to enable continual improvement. Moreover, FELA introduces an agentic evolution algorithm, combining reinforcement learning and genetic algorithm principles to balance exploration and exploitation across the idea space. Extensive experiments on real industrial datasets demonstrate that FELA can generate explainable, domain-relevant features that significantly improve model performance while reducing manual effort. Our results highlight the potential of LLM-based multi-agent systems as a general framework for automated, interpretable, and adaptive feature engineering in complex real-world environments.</article>","contentLength":1860,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MSF-Net: Multi-Stage Feature Extraction and Fusion for Robust Photometric Stereo","url":"https://arxiv.org/abs/2510.25221","date":1761796800,"author":"","guid":321686,"unread":true,"content":"<article>arXiv:2510.25221v1 Announce Type: new \nAbstract: Photometric stereo is a technique aimed at determining surface normals through the utilization of shading cues derived from images taken under different lighting conditions. However, existing learning-based approaches often fail to accurately capture features at multiple stages and do not adequately promote interaction between these features. Consequently, these models tend to extract redundant features, especially in areas with intricate details such as wrinkles and edges. To tackle these issues, we propose MSF-Net, a novel framework for extracting information at multiple stages, paired with selective update strategy, aiming to extract high-quality feature information, which is critical for accurate normal construction. Additionally, we have developed a feature fusion module to improve the interplay among different features. Experimental results on the DiLiGenT benchmark show that our proposed MSF-Net significantly surpasses previous state-of-the-art methods in the accuracy of surface normal estimation.</article>","contentLength":1068,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction","url":"https://arxiv.org/abs/2510.25220","date":1761796800,"author":"","guid":321687,"unread":true,"content":"<article>arXiv:2510.25220v1 Announce Type: new \nAbstract: In a multi-stage recommendation system, reranking plays a crucial role in modeling intra-list correlations among items. A key challenge lies in exploring optimal sequences within the combinatorial space of permutations. Recent research follows a two-stage (generator-evaluator) paradigm, where a generator produces multiple feasible sequences, and an evaluator selects the best one. In practice, the generator is typically implemented as an autoregressive model. However, these two-stage methods face two main challenges. First, the separation of the generator and evaluator hinders end-to-end training. Second, autoregressive generators suffer from inference efficiency. In this work, we propose a Unified Generative Efficient Reranking Framework (GReF) to address the two primary challenges. Specifically, we introduce Gen-Reranker, an autoregressive generator featuring a bidirectional encoder and a dynamic autoregressive decoder to generate causal reranking sequences. Subsequently, we pre-train Gen-Reranker on the item exposure order for high-quality parameter initialization. To eliminate the need for the evaluator while integrating sequence-level evaluation during training for end-to-end optimization, we propose post-training the model through Rerank-DPO. Moreover, for efficient autoregressive inference, we introduce ordered multi-token prediction (OMTP), which trains Gen-Reranker to simultaneously generate multiple future items while preserving their order, ensuring practical deployment in real-time recommender systems. Extensive offline experiments demonstrate that GReF outperforms state-of-the-art reranking methods while achieving latency that is nearly comparable to non-autoregressive models. Additionally, GReF has also been deployed in a real-world video app Kuaishou with over 300 million daily active users, significantly improving online recommendation quality.</article>","contentLength":1940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Benchmark Suite for Multi-Objective Optimization in Battery Thermal Management System Design","url":"https://arxiv.org/abs/2510.25219","date":1761796800,"author":"","guid":321688,"unread":true,"content":"<article>arXiv:2510.25219v1 Announce Type: new \nAbstract: Synthetic Benchmark Problems (SBPs) are commonly used to evaluate the performance of metaheuristic algorithms. However, these SBPs often contain various unrealistic properties, potentially leading to underestimation or overestimation of algorithmic performance. While several benchmark suites comprising real-world problems have been proposed for various types of metaheuristics, a notable gap exists for Constrained Multi-objective Optimization Problems (CMOPs) derived from practical engineering applications, particularly in the domain of Battery Thermal Management System (BTMS) design. To address this gap, this study develops and presents a specialized benchmark suite for multi-objective optimization in BTMS. This suite comprises a diverse collection of real-world constrained problems, each defined via accurate surrogate models based on recent research to efficiently represent complex thermal-fluid interactions. The primary goal of this benchmark suite is to provide a practical and relevant testing ground for evolutionary algorithms and optimization methods focused on energy storage thermal management. Future work will involve establishing comprehensive baseline results using state-of-the-art algorithms, conducting comparative analyses, and developing a standardized ranking scheme to facilitate robust performance assessment.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human Resilience in the AI Era -- What Machines Can't Replace","url":"https://arxiv.org/abs/2510.25218","date":1761796800,"author":"","guid":321689,"unread":true,"content":"<article>arXiv:2510.25218v1 Announce Type: new \nAbstract: AI is displacing tasks, mediating high-stakes decisions, and flooding communication with synthetic content, unsettling work, identity, and social trust. We argue that the decisive human countermeasure is resilience. We define resilience across three layers: psychological, including emotion regulation, meaning-making, cognitive flexibility; social, including trust, social capital, coordinated response; organizational, including psychological safety, feedback mechanisms, and graceful degradation. We synthesize early evidence that these capacities buffer individual strain, reduce burnout through social support, and lower silent failure in AI-mediated workflows through team norms and risk-responsive governance. We also show that resilience can be cultivated through training that complements rather than substitutes for structural safeguards. By reframing the AI debate around actionable human resilience, this article offers policymakers, educators, and operators a practical lens to preserve human agency and steer responsible adoption.</article>","contentLength":1093,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collaborative Scheduling of Time-dependent UAVs,Vehicles and Workers for Crowdsensing in Disaster Response","url":"https://arxiv.org/abs/2510.25212","date":1761796800,"author":"","guid":321690,"unread":true,"content":"<article>arXiv:2510.25212v1 Announce Type: new \nAbstract: Frequent natural disasters cause significant losses to human society, and timely, efficient collection of post-disaster environmental information is the foundation for effective rescue operations. Due to the extreme complexity of post-disaster environments, existing sensing technologies such as mobile crowdsensing suffer from weak environmental adaptability, insufficient professional sensing capabilities, and poor practicality of sensing solutions. Therefore, this paper explores a heterogeneous multi-agent online collaborative scheduling algorithm, HoCs-MPQ, to achieve efficient collection of post-disaster environmental information. HoCs-MPQ models collaboration and conflict relationships among multiple elements through weighted undirected graph construction, and iteratively solves the maximum weight independent set based on multi-priority queues, ultimately achieving collaborative sensing scheduling of time-dependent UA Vs, vehicles, and workers. Specifically, (1) HoCs-MPQ constructs weighted undirected graph nodes based on collaborative relationships among multiple elements and quantifies their weights, then models the weighted undirected graph based on conflict relationships between nodes; (2) HoCs-MPQ solves the maximum weight independent set based on iterated local search, and accelerates the solution process using multi-priority queues. Finally, we conducted detailed experiments based on extensive real-world and simulated data. The experiments show that, compared to baseline methods (e.g., HoCs-GREEDY, HoCs-K-WTA, HoCs-MADL, and HoCs-MARL), HoCs-MPQ improves task completion rates by an average of 54.13%, 23.82%, 14.12%, and 12.89% respectively, with computation time for single online autonomous scheduling decisions not exceeding 3 seconds.</article>","contentLength":1824,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RoadSens-4M: A Multimodal Smartphone & Camera Dataset for Holistic Road-way Analysis","url":"https://arxiv.org/abs/2510.25211","date":1761796800,"author":"","guid":321691,"unread":true,"content":"<article>arXiv:2510.25211v1 Announce Type: new \nAbstract: It's important to monitor road issues such as bumps and potholes to enhance safety and improve road conditions. Smartphones are equipped with various built-in sensors that offer a cost-effective and straightforward way to assess road quality. However, progress in this area has been slow due to the lack of high-quality, standardized datasets. This paper discusses a new dataset created by a mobile app that collects sensor data from devices like GPS, accelerometers, gyroscopes, magnetometers, gravity sensors, and orientation sensors. This dataset is one of the few that integrates Geographic Information System (GIS) data with weather information and video footage of road conditions, providing a comprehensive understanding of road issues with geographic context. The dataset allows for a clearer analysis of road conditions by compiling essential data, including vehicle speed, acceleration, rotation rates, and magnetic field intensity, along with the visual and spatial context provided by GIS, weather, and video data. Its goal is to provide funding for initiatives that enhance traffic management, infrastructure development, road safety, and urban planning. Additionally, the dataset will be publicly accessible to promote further research and innovation in smart transportation systems.</article>","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching","url":"https://arxiv.org/abs/2510.25210","date":1761796800,"author":"","guid":321692,"unread":true,"content":"<article>arXiv:2510.25210v1 Announce Type: new \nAbstract: Point clouds captured by scanning sensors are often perturbed by noise, which have a highly negative impact on downstream tasks (e.g. surface reconstruction and shape understanding). Previous works mostly focus on training neural networks with noisy-clean point cloud pairs for learning denoising priors, which requires extensively manual efforts. In this work, we introduce U-CAN, an Unsupervised framework for point cloud denoising with Consistency-Aware Noise2Noise matching. Specifically, we leverage a neural network to infer a multi-step denoising path for each point of a shape or scene with a noise to noise matching scheme. We achieve this by a novel loss which enables statistical reasoning on multiple noisy point cloud observations. We further introduce a novel constraint on the denoised geometry consistency for learning consistency-aware denoising patterns. We justify that the proposed constraint is a general term which is not limited to 3D domain and can also contribute to the area of 2D image denoising. Our evaluations under the widely used benchmarks in point cloud denoising, upsampling and image denoising show significant improvement over the state-of-the-art unsupervised methods, where U-CAN also produces comparable results with the supervised methods.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Robust Popular Matchings with Tie-Bounded Preferences and Stable Matchings with Two-Sided Ties","url":"https://arxiv.org/abs/2510.25209","date":1761796800,"author":"","guid":321693,"unread":true,"content":"<article>arXiv:2510.25209v1 Announce Type: new \nAbstract: We are given a bipartite graph $G = \\left( A \\cup B, E \\right)$. In the one-sided model, every $a \\in A$ (often called agents) ranks its neighbours $z \\in N_{a}$ strictly, and no $b \\in B$ has any preference order over its neighbours $y \\in N_{b}$, and vertices in $B$ abstain from casting their votes to matchings. In the two-sided model with one-sided ties, every $a \\in A$ ranks its neighbours $z \\in N_{a}$ strictly, and every $b \\in B$ puts all of its neighbours into a single large tie, i.e., $b \\in B$ prefers every $y \\in N_{b}$ equally. In this two-sided model with one-sided ties, when two matchings compete in a majority election, $b \\in B$ abstains from casting its vote for a matching when both the matchings saturate $b$ or both leave $b$ unsaturated; else $b$ prefers the matching where it is saturated. A popular matching $M$ is \\emph{robust} if it remains popular among multiple instances.\n  We have analysed the cases when a robust popular matching exists in the one-sided model where only one agent alters her preference order among the instances, and we have proposed a polynomial-time algorithm to decide if there exists a robust popular matching when instances differ only with respect to the preference orders of a single agent.\n  We give a simple characterisation of popular matchings in the two-sided model with one-sided ties. We show that in the two-sided model with one-sided ties, if the input instances differ only with respect to the preference orders of a single agent, there is a polynomial-time algorithm to decide whether there exists a robust popular matching. We have been able to decide the stable matching problem in bipartite graphs $G = (A \\cup B, E)$ where \\textit{both} sides have weak preferences (ties allowed), with the restriction that every tie has length at most $k$.</article>","contentLength":1865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Silicon-based Josephson junction field-effect transistors enabling cryogenic logic and quantum technologies","url":"https://arxiv.org/abs/2510.25208","date":1761796800,"author":"","guid":321694,"unread":true,"content":"<article>arXiv:2510.25208v1 Announce Type: new \nAbstract: The continuous miniaturisation of metal-oxide-semiconductor field-effect transistors (MOSFETs) from long- to short-channel architectures has advanced beyond the predictions of Moore's Law. Continued advances in semiconductor electronics, even near current scaling and performance boundaries under cryogenic conditions, are driving the development of innovative device paradigms that enable ultra-low-power and high-speed functionality. Among emerging candidates, the Josephson Junction Field-Effect Transistor (JJFET or JoFET) provides an alternative by integrating superconducting source and drain electrodes for efficient, phase-coherent operation at ultra-low temperatures. These hybrid devices have the potential to bridge conventional semiconductor electronics with cryogenic logic and quantum circuits, enabling energy-efficient and high-coherence signal processing across temperature domains. This review traces the evolution from Josephson junctions to field-effect transistors, emphasising the structural and functional innovations that underpin modern device scalability. The performance and material compatibility of JJFETs fabricated on Si, GaAs, and InGaAs substrates are analysed, alongside an assessment of their switching dynamics and material compatibility. Particular attention is given to superconductor-silicon-superconductor Josephson junctions as the active core of JJFET architectures. By unfolding more than four decades of experimental progress, this work highlights the promise of JJFETs as foundational building blocks for next-generation cryogenic logic and quantum electronic systems.</article>","contentLength":1662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Selective Learning for Deep Time Series Forecasting","url":"https://arxiv.org/abs/2510.25207","date":1761796800,"author":"","guid":321695,"unread":true,"content":"<article>arXiv:2510.25207v1 Announce Type: new \nAbstract: Benefiting from high capacity for capturing complex temporal patterns, deep learning (DL) has significantly advanced time series forecasting (TSF). However, deep models tend to suffer from severe overfitting due to the inherent vulnerability of time series to noise and anomalies. The prevailing DL paradigm uniformly optimizes all timesteps through the MSE loss and learns those uncertain and anomalous timesteps without difference, ultimately resulting in overfitting. To address this, we propose a novel selective learning strategy for deep TSF. Specifically, selective learning screens a subset of the whole timesteps to calculate the MSE loss in optimization, guiding the model to focus on generalizable timesteps while disregarding non-generalizable ones. Our framework introduces a dual-mask mechanism to target timesteps: (1) an uncertainty mask leveraging residual entropy to filter uncertain timesteps, and (2) an anomaly mask employing residual lower bound estimation to exclude anomalous timesteps. Extensive experiments across eight real-world datasets demonstrate that selective learning can significantly improve the predictive performance for typical state-of-the-art deep models, including 37.4% MSE reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.</article>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RAVR: Reference-Answer-guided Variational Reasoning for Large Language Models","url":"https://arxiv.org/abs/2510.25206","date":1761796800,"author":"","guid":321696,"unread":true,"content":"<article>arXiv:2510.25206v1 Announce Type: new \nAbstract: Reinforcement learning (RL) can refine the reasoning abilities of large language models (LLMs), but critically depends on a key prerequisite: the LLM can already generate high-utility reasoning paths with non-negligible probability. For tasks beyond the LLM's current competence, such reasoning path can be hard to sample, and learning risks reinforcing familiar but suboptimal reasoning. We are motivated by the insight from cognitive science that Why is this the answer is often an easier question than What is the answer, as it avoids the heavy cognitive load of open-ended exploration, opting instead for explanatory reconstruction-systematically retracing the reasoning that links a question to its answer. We show that LLMs can similarly leverage answers to derive high-quality reasoning paths. We formalize this phenomenon and prove that conditioning on answer provably increases the expected utility of sampled reasoning paths, thereby transforming intractable problems into learnable ones. Building on this insight, we introduce RAVR (Reference-Answer-guided Variational Reasoning), an end-to-end framework that uses answer-conditioned reasoning as a variational surrogate for question-only reasoning. Experiments in both general and math domains demonstrate consistent improvements over strong baselines. We further analyze the reasoning behavior and find that RAVR reduces hesitation, strengthens conclusion consolidation, and promotes problem-specific strategies in reasoning.</article>","contentLength":1537,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Energy-Efficient Autonomous Driving with Adaptive Perception and Robust Decision","url":"https://arxiv.org/abs/2510.25205","date":1761796800,"author":"","guid":321697,"unread":true,"content":"<article>arXiv:2510.25205v1 Announce Type: new \nAbstract: Autonomous driving is an emerging technology that is expected to bring significant social, economic, and environmental benefits. However, these benefits come with rising energy consumption by computation engines, limiting the driving range of vehicles, especially electric ones. Perception computing is typically the most power-intensive component, as it relies on largescale deep learning models to extract environmental features. Recently, numerous studies have employed model compression techniques, such as sparsification, quantization, and distillation, to reduce computational consumption. However, these methods often result in either a substantial model size or a significant drop in perception accuracy compared to high-computation models. To address these challenges, we propose an energy-efficient autonomous driving framework, called EneAD. In the adaptive perception module, a perception optimization strategy is designed from the perspective of data management and tuning. Firstly, we manage multiple perception models with different computational consumption and adjust the execution framerate dynamically. Then, we define them as knobs and design a transferable tuning method based on Bayesian optimization to identify promising knob values that achieve low computation while maintaining desired accuracy. To adaptively switch the knob values in various traffic scenarios, a lightweight classification model is proposed to distinguish the perception difficulty in different scenarios. In the robust decision module, we propose a decision model based on reinforcement learning and design a regularization term to enhance driving stability in the face of perturbed perception results. Extensive experiments evidence the superiority of our framework in both energy consumption and driving performance. EneAD can reduce perception consumption by 1.9x to 3.5x and thus improve driving range by 3.9% to 8.5%</article>","contentLength":1966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stable Emotional Co-occurrence Patterns Revealed by Network Analysis of Social Media","url":"https://arxiv.org/abs/2510.25204","date":1761796800,"author":"","guid":321698,"unread":true,"content":"<article>arXiv:2510.25204v1 Announce Type: new \nAbstract: Examining emotion interactions as an emotion network in social media offers key insights into human psychology, yet few studies have explored how fluctuations in such emotion network evolve during crises and normal times. This study proposes a novel computational approach grounded in network theory, leveraging large-scale Japanese social media data spanning varied crisis events (earthquakes and COVID-19 vaccination) and non-crisis periods over the past decade. Our analysis identifies and evaluates links between emotions through the co-occurrence of emotion-related concepts (words), revealing a stable structure of emotion network across situations and over time at the population level. We find that some emotion links (represented as link strength) such as emotion links associated with Tension are significantly strengthened during earthquake and pre-vaccination periods. However, the rank of emotion links remains highly intact. These findings challenge the assumption that emotion co-occurrence is context-based and offer a deeper understanding of emotions' intrinsic structure. Moreover, our network-based framework offers a systematic, scalable method for analyzing emotion co-occurrence dynamics, opening new avenues for psychological research using large-scale textual data.</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Financial Decision-Making: Machine Learning and AI-Powered Predictions and Analysis","url":"https://arxiv.org/abs/2510.25201","date":1761796800,"author":"","guid":321699,"unread":true,"content":"<article>arXiv:2510.25201v1 Announce Type: new \nAbstract: The proposed system aims to use various machine learning algorithms to enhance financial prediction and generate highly accurate analyses. It introduces an AI-driven platform which offers inflation-analysis, stock market prediction, and E-learning module powered by a chatbot. It has achieved high accuracy where the Inflation Analysis depicts 0.8% MAE, 1.2% RMSE and the Stock Prediction shows 98% and 96% accuracy for Apple and Google stock prices respectively. Key features include historical price trends, inflation rates, short-term future stock prediction, where the data has been extracted using real-world financial datasets. Additionally, the E-learning feature contributes to bridging financial gaps and promoting informed decisions. We have implemented algorithms like linear regression, ARIMA, LSTM where the accuracy has been evaluated using metrics such as MAE, RMSE and the like.</article>","contentLength":943,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-Powered Early Detection of Critical Diseases using Image Processing and Audio Analysis","url":"https://arxiv.org/abs/2510.25199","date":1761796800,"author":"","guid":321700,"unread":true,"content":"<article>arXiv:2510.25199v1 Announce Type: new \nAbstract: Early diagnosis of critical diseases can significantly improve patient survival and reduce treatment costs. However, existing diagnostic techniques are often costly, invasive, and inaccessible in low-resource regions. This paper presents a multimodal artificial intelligence (AI) diagnostic framework integrating image analysis, thermal imaging, and audio signal processing for early detection of three major health conditions: skin cancer, vascular blood clots, and cardiopulmonary abnormalities. A fine-tuned MobileNetV2 convolutional neural network was trained on the ISIC 2019 dataset for skin lesion classification, achieving 89.3% accuracy, 91.6% sensitivity, and 88.2% specificity. A support vector machine (SVM) with handcrafted features was employed for thermal clot detection, achieving 86.4% accuracy (AUC = 0.89) on synthetic and clinical data. For cardiopulmonary analysis, lung and heart sound datasets from PhysioNet and Pascal were processed using Mel-Frequency Cepstral Coefficients (MFCC) and classified via Random Forest, reaching 87.2% accuracy and 85.7% sensitivity. Comparative evaluation against state-of-the-art models demonstrates that the proposed system achieves competitive results while remaining lightweight and deployable on low-cost devices. The framework provides a promising step toward scalable, real-time, and accessible AI-based pre-diagnostic healthcare solutions.</article>","contentLength":1451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Knowledge Utilization for Multi-Intent Comment Generation with Large Language Models","url":"https://arxiv.org/abs/2510.25195","date":1761796800,"author":"","guid":321701,"unread":true,"content":"<article>arXiv:2510.25195v1 Announce Type: new \nAbstract: Code comment generation aims to produce a generic overview of a code snippet, helping developers understand and maintain code. However, generic summaries alone are insufficient to meet the diverse needs of practitioners; for example, developers expect the implementation insights to be presented in an untangled manner, while users seek clear usage instructions. This highlights the necessity of multi-intent comment generation. With the widespread adoption of Large Language Models (LLMs) for code-related tasks, these models have been leveraged to tackle the challenge of multi-intent comment generation. Despite their successes, state-of-the-art LLM-based approaches often struggle to construct correct relationships among intents, code, and comments within a smaller number of demonstration examples. To mitigate this issue, we propose a framework named KUMIC for multi-intent comment generation. Built upon in-context learning, KUMIC leverages Chain-of-Thought (CoT) to optimize knowledge utilization for LLMs to generate intent-specific comments. Specifically, KUMIC first designs a retrieval mechanism to obtain similar demonstration examples, which exhibit high code-comment consistency. Then, KUMIC leverages CoT to guide LLMs to focus on statements facilitating the derivation of code comments aligned with specific intents. In this context, KUMIC constructs a mapping knowledge chain, linking code to intent-specific statements to comments, which enables LLMs to follow similar reasoning steps when generating the desired comments. We conduct extensive experiments to evaluate KUMIC, and the results demonstrate that KUMIC outperforms state-of-the-art baselines by 14.49\\%, 22.41\\%, 20.72\\%, and 12.94\\% in terms of BLEU, METEOR, ROUGE-L, and SBERT, respectively.</article>","contentLength":1823,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SoraNav: Adaptive UAV Task-Centric Navigation via Zeroshot VLM Reasoning","url":"https://arxiv.org/abs/2510.25191","date":1761796800,"author":"","guid":321702,"unread":true,"content":"<article>arXiv:2510.25191v1 Announce Type: new \nAbstract: Interpreting visual observations and natural language instructions for complex task execution remains a key challenge in robotics and AI. Despite recent advances, language-driven navigation is still difficult, particularly for UAVs in small-scale 3D environments. Existing Vision-Language Navigation (VLN) approaches are mostly designed for ground robots and struggle to generalize to aerial tasks that require full 3D spatial reasoning. The emergence of large Vision-Language Models (VLMs), such as GPT and Claude, enables zero-shot semantic reasoning from visual and textual inputs. However, these models lack spatial grounding and are not directly applicable to navigation. To address these limitations, SoraNav is introduced, an adaptive UAV navigation framework that integrates zero-shot VLM reasoning with geometry-aware decision-making. Geometric priors are incorporated into image annotations to constrain the VLM action space and improve decision quality. A hybrid switching strategy leverages navigation history to alternate between VLM reasoning and geometry-based exploration, mitigating dead-ends and redundant revisits. A PX4-based hardware-software platform, comprising both a digital twin and a physical micro-UAV, enables reproducible evaluation. Experimental results show that in 2.5D scenarios, our method improves Success Rate (SR) by 25.7% and Success weighted by Path Length (SPL) by 17%. In 3D scenarios, it improves SR by 29.5% and SPL by 18.5% relative to the baseline.</article>","contentLength":1543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AgentCyTE: Leveraging Agentic AI to Generate Cybersecurity Training & Experimentation Scenarios","url":"https://arxiv.org/abs/2510.25189","date":1761796800,"author":"","guid":321703,"unread":true,"content":"<article>arXiv:2510.25189v1 Announce Type: new \nAbstract: Designing realistic and adaptive networked threat scenarios remains a core challenge in cybersecurity research and training, still requiring substantial manual effort. While large language models (LLMs) show promise for automated synthesis, unconstrained generation often yields configurations that fail validation or execution. We present AgentCyTE, a framework integrating LLM-based reasoning with deterministic, schema-constrained network emulation to generate and refine executable threat environments. Through an agentic feedback loop, AgentCyTE observes scenario outcomes, validates correctness, and iteratively enhances realism and consistency. This hybrid approach preserves LLM flexibility while enforcing structural validity, enabling scalable, data-driven experimentation and reliable scenario generation for threat modeling and adaptive cybersecurity training. Our framework can be accessed at: https://github.com/AnantaaKotal/AgentCyTE</article>","contentLength":997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Testing Cross-Lingual Text Comprehension In LLMs Using Next Sentence Prediction","url":"https://arxiv.org/abs/2510.25187","date":1761796800,"author":"","guid":321704,"unread":true,"content":"<article>arXiv:2510.25187v1 Announce Type: new \nAbstract: While large language models are trained on massive datasets, this data is heavily skewed towards English. Does their impressive performance reflect genuine ability or just this data advantage? To find out, we tested them in a setting where they could not rely on data abundance: low-resource languages. Building on prior work Agarwal et al. (2025) that used Next Sentence Prediction (NSP) as a test, we created a large-scale benchmark with 10,000 questions each for English (a high-resource language), Swahili (medium-resource), and Hausa (low-resource). We then tested several top models, including GPT-4 Turbo, Gemini 1.5 Flash, and LLaMA 3 70B, to see how their performance holds up. The results painted a clear picture of how levels of language resources impact outcomes. While all models excelled in English, their accuracy dropped in Swahili and fell sharply in Hausa, with LLaMA 3 struggling the most. The story became even more interesting when we introduced Chain-of-Thought (CoT) prompting. For the struggling LLaMA 3, CoT acted as a helpful guide, significantly boosting its accuracy. However, for the more capable GPT-4 and Gemini, the same technique often backfired, leading to a kind of \"overthinking\" that hurt their results in the cross-lingual context. This reveals that Chain-of-Thought is not a universal solution; its effectiveness depends heavily on the model's baseline capability and the specific context of the task. Our framework pinpoints LLM weaknesses, highlights when CoT helps or hinders cross-lingual NSP performance, and factors influencing their decisions.</article>","contentLength":1638,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mask-Robust Face Verification for Online Learning via YOLOv5 and Residual Networks","url":"https://arxiv.org/abs/2510.25184","date":1761796800,"author":"","guid":321705,"unread":true,"content":"<article>arXiv:2510.25184v1 Announce Type: new \nAbstract: In the contemporary landscape, the fusion of information technology and the rapid advancement of artificial intelligence have ushered school education into a transformative phase characterized by digitization and heightened intelligence. Concurrently, the global paradigm shift caused by the Covid-19 pandemic has catalyzed the evolution of e-learning, accentuating its significance. Amidst these developments, one pivotal facet of the online education paradigm that warrants attention is the authentication of identities within the digital learning sphere. Within this context, our study delves into a solution for online learning authentication, utilizing an enhanced convolutional neural network architecture, specifically the residual network model. By harnessing the power of deep learning, this technological approach aims to galvanize the ongoing progress of online education, while concurrently bolstering its security and stability. Such fortification is imperative in enabling online education to seamlessly align with the swift evolution of the educational landscape. This paper's focal proposition involves the deployment of the YOLOv5 network, meticulously trained on our proprietary dataset. This network is tasked with identifying individuals' faces culled from images captured by students' open online cameras. The resultant facial information is then channeled into the residual network to extract intricate features at a deeper level. Subsequently, a comparative analysis of Euclidean distances against students' face databases is performed, effectively ascertaining the identity of each student.</article>","contentLength":1663,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fed-PELAD: Communication-Efficient Federated Learning for Massive MIMO CSI Feedback with Personalized Encoders and a LoRA-Adapted Shared Decoder","url":"https://arxiv.org/abs/2510.25181","date":1761796800,"author":"","guid":321706,"unread":true,"content":"<article>arXiv:2510.25181v1 Announce Type: new \nAbstract: This paper addresses the critical challenges of communication overhead, data heterogeneity, and privacy in deep learning for channel state information (CSI) feedback in massive MIMO systems. To this end, we propose Fed-PELAD, a novel federated learning framework that incorporates personalized encoders and a LoRA-adapted shared decoder. Specifically, personalized encoders are trained locally on each user equipment (UE) to capture device-specific channel characteristics, while a shared decoder is updated globally via the coordination of the base station (BS) by using Low-Rank Adaptation (LoRA). This design ensures that only compact LoRA adapter parameters instead of full model updates are transmitted for aggregation. To further enhance convergence stability, we introduce an alternating freezing strategy with calibrated learning-rate ratio during LoRA aggregation. Extensive simulations on 3GPP-standard channel models demonstrate that Fed-PELAD requires only 42.97\\% of the uplink communication cost compared to conventional methods while achieving a performance gain of 1.2 dB in CSI feedback accuracy under heterogeneous conditions.</article>","contentLength":1193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Open Source Resume: How Open Source Contributions Help Students Demonstrate Alignment with Employer Needs","url":"https://arxiv.org/abs/2510.25180","date":1761796800,"author":"","guid":321707,"unread":true,"content":"<article>arXiv:2510.25180v1 Announce Type: new \nAbstract: Computer science educators are increasingly integrating open source contributions into classes to prepare students for higher expectations due to GenAI, and to improve employment outcomes in an increasingly competitive job market. However, little is known about how employers view student open source contributions. This paper addresses two research questions qualitatively: what traits do employers desire for entry-level hires in 2025, and how can they be demonstrated through open source contributions? It also tests quantitatively the hypothesis that student knowledge of employers' expectations will improve their motivation to work on open source projects. To answer our qualitative questions, we conducted interviews with US hiring managers. We collaborated with each interviewee to create a \"hiring manager agreement,\" which listed desirable traits and specific ways to demonstrate them through open source, along with a promise to interview some students meeting the criteria. To evaluate our quantitative hypothesis, we surveyed 650 undergraduates attending public universities in the US using an instrument based on expectancy-value theory. Hiring managers wanted many non-technical traits that are difficult to teach in traditional CS classes, such as initiative. There were many commonalities in how employers wanted to see these traits demonstrated in open source contributions. Viewing hiring manager agreements improved student motivation to contribute to open source projects. Our findings suggest that open source contributions may help CS undergraduates get hired, but this requires sustained engagement in multiple areas. Educators can motivate students by sharing employer expectations, but further work is required to determine if this changes their behavior.</article>","contentLength":1830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models","url":"https://arxiv.org/abs/2510.25179","date":1761796800,"author":"","guid":321708,"unread":true,"content":"<article>arXiv:2510.25179v1 Announce Type: new \nAbstract: Agentic methods have emerged as a powerful and autonomous paradigm that enhances reasoning, collaboration, and adaptive control, enabling systems to coordinate and independently solve complex tasks. We extend this paradigm to safety alignment by introducing Agentic Moderation, a model-agnostic framework that leverages specialised agents to defend multimodal systems against jailbreak attacks. Unlike prior approaches that apply as a static layer over inputs or outputs and provide only binary classifications (safe or unsafe), our method integrates dynamic, cooperative agents, including Shield, Responder, Evaluator, and Reflector, to achieve context-aware and interpretable moderation. Extensive experiments across five datasets and four representative Large Vision-Language Models (LVLMs) demonstrate that our approach reduces the Attack Success Rate (ASR) by 7-19%, maintains a stable Non-Following Rate (NF), and improves the Refusal Rate (RR) by 4-20%, achieving robust, interpretable, and well-balanced safety performance. By harnessing the flexibility and reasoning capacity of agentic architectures, Agentic Moderation provides modular, scalable, and fine-grained safety enforcement, highlighting the broader potential of agentic systems as a foundation for automated safety governance.</article>","contentLength":1346,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution","url":"https://arxiv.org/abs/2510.25178","date":1761796800,"author":"","guid":321709,"unread":true,"content":"<article>arXiv:2510.25178v1 Announce Type: new \nAbstract: Intra-sentence multilingual speech synthesis (code-switching TTS) remains a major challenge due to abrupt language shifts, varied scripts, and mismatched prosody between languages. Conventional TTS systems are typically monolingual and fail to produce natural, intelligible speech in mixed-language contexts. We introduce Script-First Multilingual Synthesis with Adaptive Locale Resolution (SFMS-ALR), an engine-agnostic framework for fluent, real-time code-switched speech generation. SFMS-ALR segments input text by Unicode script, applies adaptive language identification to determine each segment's language and locale, and normalizes prosody using sentiment-aware adjustments to preserve expressive continuity across languages. The algorithm generates a unified SSML representation with appropriate \"lang\" or \"voice\" spans and synthesizes the utterance in a single TTS request. Unlike end-to-end multilingual models, SFMS-ALR requires no retraining and integrates seamlessly with existing voices from Google, Apple, Amazon, and other providers. Comparative analysis with data-driven pipelines such as Unicom and Mask LID demonstrates SFMS-ALR's flexibility, interpretability, and immediate deployability. The framework establishes a modular baseline for high-quality, engine-independent multilingual TTS and outlines evaluation strategies for intelligibility, naturalness, and user preference.</article>","contentLength":1447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Learning and CPU (Central Processing Unit) Scheduling Co-Optimization over a Network of Computing Centers","url":"https://arxiv.org/abs/2510.25176","date":1761796800,"author":"","guid":321710,"unread":true,"content":"<article>arXiv:2510.25176v1 Announce Type: new \nAbstract: In the rapidly evolving research on artificial intelligence (AI) the demand for fast, computationally efficient, and scalable solutions has increased in recent years. The problem of optimizing the computing resources for distributed machine learning (ML) and optimization is considered in this paper. Given a set of data distributed over a network of computing-nodes/servers, the idea is to optimally assign the CPU (central processing unit) usage while simultaneously training each computing node locally via its own share of data. This formulates the problem as a co-optimization setup to (i) optimize the data processing and (ii) optimally allocate the computing resources. The information-sharing network among the nodes might be time-varying, but with balanced weights to ensure consensus-type convergence of the algorithm. The algorithm is all-time feasible, which implies that the computing resource-demand balance constraint holds at all iterations of the proposed solution. Moreover, the solution allows addressing possible log-scale quantization over the information-sharing channels to exchange log-quantized data. For some example applications, distributed support-vector-machine (SVM) and regression are considered as the ML training models. Results from perturbation theory, along with Lyapunov stability and eigen-spectrum analysis, are used to prove the convergence towards the optimal case. As compared to existing CPU scheduling solutions, the proposed algorithm improves the cost optimality gap by more than $50\\%$.</article>","contentLength":1583,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Test-Time Adaptive Object Detection with Foundation Model","url":"https://arxiv.org/abs/2510.25175","date":1761796800,"author":"","guid":321711,"unread":true,"content":"<article>arXiv:2510.25175v1 Announce Type: new \nAbstract: In recent years, test-time adaptive object detection has attracted increasing attention due to its unique advantages in online domain adaptation, which aligns more closely with real-world application scenarios. However, existing approaches heavily rely on source-derived statistical characteristics while making the strong assumption that the source and target domains share an identical category space. In this paper, we propose the first foundation model-powered test-time adaptive object detection method that eliminates the need for source data entirely and overcomes traditional closed-set limitations. Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework for vision-language detector-driven test-time adaptation, which incorporates text and visual prompt tuning to adapt both language and vision representation spaces on the test data in a parameter-efficient manner. Correspondingly, we propose a Test-time Warm-start strategy tailored for the visual prompts to effectively preserve the representation capability of the vision branch. Furthermore, to guarantee high-quality pseudo-labels in every test batch, we maintain an Instance Dynamic Memory (IDM) module that stores high-quality pseudo-labels from previous test samples, and propose two novel strategies-Memory Enhancement and Memory Hallucination-to leverage IDM's high-quality instances for enhancing original predictions and hallucinating images without available pseudo-labels, respectively. Extensive experiments on cross-corruption and cross-dataset benchmarks demonstrate that our method consistently outperforms previous state-of-the-art methods, and can adapt to arbitrary cross-domain and cross-category target data. Code is available at https://github.com/gaoyingjay/ttaod_foundation.</article>","contentLength":1829,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Classifier Enhancement Using Extended Context and Domain Experts for Semantic Segmentation","url":"https://arxiv.org/abs/2510.25174","date":1761796800,"author":"","guid":321712,"unread":true,"content":"<article>arXiv:2510.25174v1 Announce Type: new \nAbstract: Prevalent semantic segmentation methods generally adopt a vanilla classifier to categorize each pixel into specific classes.\n  Although such a classifier learns global information from the training data, this information is represented by a set of fixed parameters (weights and biases).\n  However, each image has a different class distribution, which prevents the classifier from addressing the unique characteristics of individual images.\n  At the dataset level, class imbalance leads to segmentation results being biased towards majority classes, limiting the model's effectiveness in identifying and segmenting minority class regions.\n  In this paper, we propose an Extended Context-Aware Classifier (ECAC) that dynamically adjusts the classifier using global (dataset-level) and local (image-level) contextual information.\n  Specifically, we leverage a memory bank to learn dataset-level contextual information of each class, incorporating the class-specific contextual information from the current image to improve the classifier for precise pixel labeling.\n  Additionally, a teacher-student network paradigm is adopted, where the domain expert (teacher network) dynamically adjusts contextual information with ground truth and transfers knowledge to the student network.\n  Comprehensive experiments illustrate that the proposed ECAC can achieve state-of-the-art performance across several datasets, including ADE20K, COCO-Stuff10K, and Pascal-Context.</article>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction","url":"https://arxiv.org/abs/2510.25173","date":1761796800,"author":"","guid":321713,"unread":true,"content":"<article>arXiv:2510.25173v1 Announce Type: new \nAbstract: Recently, Gaussian Splatting (GS) has shown great potential for urban scene reconstruction in the field of autonomous driving. However, current urban scene reconstruction methods often depend on multimodal sensors as inputs, \\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDAR point clouds can largely mitigate ill-posedness in reconstruction, acquiring such accurate LiDAR data is still challenging in practice: i) precise spatiotemporal calibration between LiDAR and other sensors is required, as they may not capture data simultaneously; ii) reprojection errors arise from spatial misalignment when LiDAR and cameras are mounted at different locations. To avoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, a LiDAR-free urban scene reconstruction framework. In this work, we obtain geometry priors that are as effective as LiDAR while being denser and more accurate. $\\textbf{First}$, we initialize a dense point cloud by back-projecting multi-view metric depth predictions. This point cloud is then optimized by a Progressive Pruning strategy to improve the global consistency. $\\textbf{Second}$, we jointly refine Gaussian geometry and predicted dense metric depth via a Depth Enhancer. Specifically, we leverage diffusion priors from a depth foundation model to enhance the depth maps rendered by Gaussians. In turn, the enhanced depths provide stronger geometric constraints during Gaussian training. $\\textbf{Finally}$, we improve the accuracy of ground geometry by constraining the shape and normal attributes of Gaussians within road regions. Extensive experiments on the Waymo dataset demonstrate that our method consistently outperforms state-of-the-art methods, producing more accurate geometry even when compared with those using ground-truth LiDAR data.</article>","contentLength":1866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Error Analysis of Third-Order in Time and Fourth-Order Linear Finite Difference Scheme for Landau-Lifshitz-Gilbert Equation under Large Damping Parameters","url":"https://arxiv.org/abs/2510.25172","date":1761796800,"author":"","guid":321714,"unread":true,"content":"<article>arXiv:2510.25172v1 Announce Type: new \nAbstract: This work proposes and analyzes a fully discrete numerical scheme for solving the Landau-Lifshitz-Gilbert (LLG) equation, which achieves fourth-order spatial accuracy and third-order temporal accuracy.Spatially, fourth-order accuracy is attained through the adoption of a long-stencil finite difference method, while boundary extrapolation is executed by leveraging a higher-order Taylor expansion to ensure consistency at domain boundaries. Temporally, the scheme is constructed based on the third-order backward differentiation formula (BDF3), with implicit discretization applied to the linear diffusion term for numerical stability and explicit extrapolation employed for nonlinear terms to balance computational efficiency. Notably, this numerical method inherently preserves the normalization constraint of the LLG equation, a key physical property of the system.Theoretical analysis confirms that the proposed scheme exhibits optimal convergence rates under the \\(\\ell^{\\infty}([0,T],\\ell^2)\\) and \\(\\ell^2([0,T],H_h^1)\\) norms. Finally, numerical experiments are conducted to validate the correctness of the theoretical convergence results, demonstrating good agreement between numerical observations and analytical conclusions.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Resolution Model Fusion for Accelerating the Convolutional Neural Network Training","url":"https://arxiv.org/abs/2510.25170","date":1761796800,"author":"","guid":321715,"unread":true,"content":"<article>arXiv:2510.25170v1 Announce Type: new \nAbstract: Neural networks are rapidly gaining popularity in scientific research, but training the models is often very time-consuming. Particularly when the training data samples are large high-dimensional arrays, efficient training methodologies that can reduce the computational costs are crucial. To reduce the training cost, we propose a Multi-Resolution Model Fusion (MRMF) method that combines models trained on reduced-resolution data and then refined with data in the original resolution. We demonstrate that these reduced-resolution models and datasets could be generated quickly. More importantly, the proposed approach reduces the training time by speeding up the model convergence in each fusion stage before switching to the final stage of finetuning with data in its original resolution. This strategy ensures the final model retains high-resolution insights while benefiting from the computational efficiency of lower-resolution training. Our experiment results demonstrate that the multi-resolution model fusion method can significantly reduce end-to-end training time while maintaining the same model accuracy. Evaluated using two real-world scientific applications, CosmoFlow and Neuron Inverter, the proposed method improves the training time by up to 47% and 44%, respectively, as compared to the original resolution training, while the model accuracy is not affected.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Cultural Resources for Improving Generative Models","url":"https://arxiv.org/abs/2510.25167","date":1761796800,"author":"","guid":321716,"unread":true,"content":"<article>arXiv:2510.25167v1 Announce Type: new \nAbstract: Generative models are known to have reduced performance in different global cultural contexts and languages. While continual data updates have been commonly conducted to improve overall model performance, bolstering and evaluating this cross-cultural competence of generative AI models requires data resources to be intentionally expanded to include global contexts and languages. In this work, we construct a repeatable, scalable, multi-pronged pipeline to collect and contribute culturally salient, multilingual data. We posit that such data can assess the state of the global applicability of our models and thus, in turn, help identify and improve upon cross-cultural gaps.</article>","contentLength":726,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Study on Inference Latency for Vision Transformers on Mobile Devices","url":"https://arxiv.org/abs/2510.25166","date":1761796800,"author":"","guid":321717,"unread":true,"content":"<article>arXiv:2510.25166v1 Announce Type: new \nAbstract: Given the significant advances in machine learning techniques on mobile devices, particularly in the domain of computer vision, in this work we quantitatively study the performance characteristics of 190 real-world vision transformers (ViTs) on mobile devices. Through a comparison with 102 real-world convolutional neural networks (CNNs), we provide insights into the factors that influence the latency of ViT architectures on mobile devices. Based on these insights, we develop a dataset including measured latencies of 1000 synthetic ViTs with representative building blocks and state-of-the-art architectures from two machine learning frameworks and six mobile platforms. Using this dataset, we show that inference latency of new ViTs can be predicted with sufficient accuracy for real-world applications.</article>","contentLength":858,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Most Juntas Saturate the Hardcore Lemma","url":"https://arxiv.org/abs/2510.25165","date":1761796800,"author":"","guid":321718,"unread":true,"content":"<article>arXiv:2510.25165v1 Announce Type: cross \nAbstract: Consider a function that is mildly hard for size-$s$ circuits. For sufficiently large $s$, Impagliazzo's hardcore lemma guarantees a constant-density subset of inputs on which the same function is extremely hard for circuits of size $s'&lt;\\!\\!&lt;s$. Blanc, Hayderi, Koch, and Tan [FOCS 2024] recently showed that the degradation from $s$ to $s'$ in this lemma is quantitatively tight in certain parameter regimes. We give a simpler and more general proof of this result in almost all parameter regimes of interest by showing that a random junta witnesses the tightness of the hardcore lemma with high probability.</article>","contentLength":660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Target-Guided Bayesian Flow Networks for Quantitatively Constrained CAD Generation","url":"https://arxiv.org/abs/2510.25163","date":1761796800,"author":"","guid":321719,"unread":true,"content":"<article>arXiv:2510.25163v1 Announce Type: new \nAbstract: Deep generative models, such as diffusion models, have shown promising progress in image generation and audio generation via simplified continuity assumptions. However, the development of generative modeling techniques for generating multi-modal data, such as parametric CAD sequences, still lags behind due to the challenges in addressing long-range constraints and parameter sensitivity. In this work, we propose a novel framework for quantitatively constrained CAD generation, termed Target-Guided Bayesian Flow Network (TGBFN). For the first time, TGBFN handles the multi-modality of CAD sequences (i.e., discrete commands and continuous parameters) in a unified continuous and differentiable parameter space rather than in the discrete data space. In addition, TGBFN penetrates the parameter update kernel and introduces a guided Bayesian flow to control the CAD properties. To evaluate TGBFN, we construct a new dataset for quantitatively constrained CAD generation. Extensive comparisons across single-condition and multi-condition constrained generation tasks demonstrate that TGBFN achieves state-of-the-art performance in generating high-fidelity, condition-aware CAD sequences. The code is available at https://github.com/scu-zwh/TGBFN.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model-Document Protocol for AI Search","url":"https://arxiv.org/abs/2510.25160","date":1761796800,"author":"","guid":321720,"unread":true,"content":"<article>arXiv:2510.25160v1 Announce Type: new \nAbstract: AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.</article>","contentLength":1876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast and Robust Point Containment Queries on Trimmed Surface","url":"https://arxiv.org/abs/2510.25159","date":1761796800,"author":"","guid":321721,"unread":true,"content":"<article>arXiv:2510.25159v1 Announce Type: new \nAbstract: Point containment queries on trimmed surfaces are fundamental to CAD modeling, solid geometry processing, and surface tessellation. Existing approaches such as ray casting and generalized winding numbers often face limitations in robustness and computational efficiency.\n  We propose a fast and numerically stable method for performing containment queries on trimmed surfaces, including those with periodic parameterizations. Our approach introduces a recursive winding number computation scheme that replaces costly curve subdivision with an ellipse-based bound for Bezier segments, enabling linear-time evaluation. For periodic surfaces, we lift trimming curves to the universal covering space, allowing accurate and consistent winding number computation even for non-contractible or discontinuous loops in parameter domain.\n  Experiments show that our method achieves substantial speedups over existing winding-number algorithms while maintaining high robustness in the presence of geometric noise, open boundaries, and periodic topologies. We further demonstrate its effectiveness in processing real B-Rep models and in robust tessellation of trimmed surfaces.</article>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Real-Time Inference of Thin Liquid Film Thickness Profiles from Interference Patterns Using Vision Transformers","url":"https://arxiv.org/abs/2510.25157","date":1761796800,"author":"","guid":321722,"unread":true,"content":"<article>arXiv:2510.25157v1 Announce Type: new \nAbstract: Thin film interferometry is a powerful technique for non-invasively measuring liquid film thickness with applications in ophthalmology, but its clinical translation is hindered by the challenges in reconstructing thickness profiles from interference patterns - an ill-posed inverse problem complicated by phase periodicity, imaging noise and ambient artifacts. Traditional reconstruction methods are either computationally intensive, sensitive to noise, or require manual expert analysis, which is impractical for real-time diagnostics. To address this challenge, here we present a vision transformer-based approach for real-time inference of thin liquid film thickness profiles directly from isolated interferograms. Trained on a hybrid dataset combining physiologically-relevant synthetic and experimental tear film data, our model leverages long-range spatial correlations to resolve phase ambiguities and reconstruct temporally coherent thickness profiles in a single forward pass from dynamic interferograms acquired in vivo and ex vivo. The network demonstrates state-of-the-art performance on noisy, rapidly-evolving films with motion artifacts, overcoming limitations of conventional phase-unwrapping and iterative fitting methods. Our data-driven approach enables automated, consistent thickness reconstruction at real-time speeds on consumer hardware, opening new possibilities for continuous monitoring of pre-lens ocular tear films and non-invasive diagnosis of conditions such as the dry eye disease.</article>","contentLength":1562,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Off-Centered WoS-Type Solvers with Statistical Weighting","url":"https://arxiv.org/abs/2510.25152","date":1761796800,"author":"","guid":321723,"unread":true,"content":"<article>arXiv:2510.25152v1 Announce Type: new \nAbstract: Stochastic PDE solvers have emerged as a powerful alternative to traditional discretization-based methods for solving partial differential equations (PDEs), especially in geometry processing and graphics. While off-centered estimators enhance sample reuse in WoS-type Monte Carlo solvers, they introduce correlation artifacts and bias when Green's functions are approximated. In this paper, we propose a statistically weighted off-centered WoS-type estimator that leverages local similarity filtering to selectively combine samples across neighboring evaluation points. Our method balances bias and variance through a principled weighting strategy that suppresses unreliable estimators. We demonstrate our approach's effectiveness on various PDEs,including screened Poisson equations and boundary conditions, achieving consistent improvements over existing solvers such as vanilla Walk on Spheres, mean value caching, and boundary value caching. Our method also naturally extends to gradient field estimation and mixed boundary problems.</article>","contentLength":1086,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explainable Disentanglement on Discrete Speech Representations for Noise-Robust ASR","url":"https://arxiv.org/abs/2510.25150","date":1761796800,"author":"","guid":321724,"unread":true,"content":"<article>arXiv:2510.25150v1 Announce Type: new \nAbstract: Discrete audio representations are gaining traction in speech modeling due to their interpretability and compatibility with large language models, but are not always optimized for noisy or real-world environments. Building on existing works that quantize Whisper embeddings for speech-to-unit modeling, we propose disentangling semantic speech content from background noise in the latent space. Our end-to-end model separates clean speech in the form of codebook tokens, while extracting interpretable noise vectors as quantization residue which are supervised via a lightweight classifier. We show that our approach improves alignment between clean/noisy speech and text, producing speech tokens that display a high degree of noiseinvariance, and improves ASR performance. Keeping Whisper frozen, we show an 82% reduction in error rate compared to Whisper, and 35% improvement over baseline methods on the VBDemand test set. Further analyses show that the learned token space generalizes well to both seen and unseen acoustic conditions.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automated Program Repair Based on REST API Specifications Using Large Language Models","url":"https://arxiv.org/abs/2510.25148","date":1761796800,"author":"","guid":321725,"unread":true,"content":"<article>arXiv:2510.25148v1 Announce Type: new \nAbstract: Many cloud services provide REST API accessible to client applications. However, developers often identify specification violations only during testing, as error messages typically lack the detail necessary for effective diagnosis. Consequently, debugging requires trial and error. This study proposes dcFix, a method for detecting and automatically repairing REST API misuses in client programs. In particular, dcFix identifies non-conforming code fragments, integrates them with the relevant API specifications into prompts, and leverages a Large Language Model (LLM) to produce the corrected code. Our evaluation demonstrates that dcFix accurately detects misuse and outperforms the baseline approach, in which prompts to the LLM omit any indication of code fragments non conforming to REST API specifications.</article>","contentLength":862,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk","url":"https://arxiv.org/abs/2510.25147","date":1761796800,"author":"","guid":321726,"unread":true,"content":"<article>arXiv:2510.25147v1 Announce Type: new \nAbstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line energization statuses to manage wildfire ignition risks through de-energizations while reducing load shedding. OPS problems are computationally challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly and frequently in operational settings. For a particular power system, OPS instances share a common structure with varying parameters related to wildfire risks, loads, and renewable generation. This motivates the use of Machine Learning (ML) for solving OPS problems by exploiting shared patterns across instances. In this paper, we develop an ML-guided framework that quickly produces high-quality de-energization decisions by extending existing ML-guided MILP solution methods while integrating domain knowledge on the number of energized and de-energized lines. Results on a large-scale realistic California-based synthetic test system show that the proposed ML-guided method produces high-quality solutions faster than traditional optimization methods.</article>","contentLength":1179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EA3D: Online Open-World 3D Object Extraction from Streaming Videos","url":"https://arxiv.org/abs/2510.25146","date":1761796800,"author":"","guid":321727,"unread":true,"content":"<article>arXiv:2510.25146v1 Announce Type: new \nAbstract: Current 3D scene understanding methods are limited by offline-collected multi-view data or pre-constructed 3D geometry. In this paper, we present ExtractAnything3D (EA3D), a unified online framework for open-world 3D object extraction that enables simultaneous geometric reconstruction and holistic scene understanding. Given a streaming video, EA3D dynamically interprets each frame using vision-language and 2D vision foundation encoders to extract object-level knowledge. This knowledge is integrated and embedded into a Gaussian feature map via a feed-forward online update strategy. We then iteratively estimate visual odometry from historical frames and incrementally update online Gaussian features with new observations. A recurrent joint optimization module directs the model's attention to regions of interest, simultaneously enhancing both geometric reconstruction and semantic understanding. Extensive experiments across diverse benchmarks and tasks, including photo-realistic rendering, semantic and instance segmentation, 3D bounding box and semantic occupancy estimation, and 3D mesh generation, demonstrate the effectiveness of EA3D. Our method establishes a unified and efficient framework for joint online 3D reconstruction and holistic scene understanding, enabling a broad range of downstream tasks.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ML-Based Preamble Collision Detection in the Random Access Procedure of Cellular IoT Networks","url":"https://arxiv.org/abs/2510.25145","date":1761796800,"author":"","guid":321728,"unread":true,"content":"<article>arXiv:2510.25145v1 Announce Type: new \nAbstract: Preamble collision in the random access channel (RACH) is a major bottleneck in massive machine-type communication (mMTC) scenarios, typical of cellular IoT (CIoT) deployments. This work proposes a machine learning-based mechanism for early collision detection during the random access (RA) procedure. A labeled dataset was generated using the RA procedure messages exchanged between the users and the base station under realistic channel conditions, simulated in MATLAB. We evaluate nine classic classifiers -- including tree ensembles, support vector machines, and neural networks -- across four communication scenarios, varying both channel characteristics (e.g., Doppler spread, multipath) and the cell coverage radius, to emulate realistic propagation, mobility, and spatial conditions. The neural network outperformed all other models, achieving over 98\\% balanced accuracy in the in-distribution evaluation (train and test drawn from the same dataset) and sustaining 95\\% under out-of-distribution evaluation (train/test from different datasets). To enable deployment on typical base station hardware, we apply post-training quantization. Full integer quantization reduced inference time from 2500 ms to as low as 0.3 ms with negligible accuracy loss. The proposed solution combines high detection accuracy with low-latency inference, making it suitable for scalable, real-time CIoT applications found in real networks.</article>","contentLength":1475,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Timing Games in Responsive Consensus Protocols","url":"https://arxiv.org/abs/2510.25144","date":1761796800,"author":"","guid":321729,"unread":true,"content":"<article>arXiv:2510.25144v1 Announce Type: new \nAbstract: Optimistic responsiveness -- the ability of a consensus protocol to operate at the speed of the network -- is widely used in consensus protocol design to optimize latency and throughput. However, blockchain applications incentivize validators to play timing games by strategically delaying their proposals, since increased block time correlates with greater rewards. Consequently, it may appear that responsiveness (even under optimistic conditions) is impossible in blockchain protocols. In this work, we develop a model of timing games in responsive consensus protocols and find a prisoner's dilemma structure, where cooperation (proposing promptly) is in the validators' best interest, but individual incentives encourage validators to delay proposals selfishly. To attain desirable equilibria, we introduce dynamic block rewards that decrease with round time to explicitly incentivize faster proposals. Delays are measured through a voting mechanism, where other validators vote on the current leader's round time. By carefully setting the protocol parameters, the voting mechanism allows validators to coordinate and reach the cooperative equilibrium, benefiting all through a higher rate-of-reward. Thus, instead of responsiveness being an unattainable property due to timing games, we show that responsiveness itself can promote faster block proposals. One consequence of moving from a static to dynamic block reward is that validator utilities become more sensitive to latency, worsening the gap between the best- and worst-connected validators. Our analysis shows, however, that this effect is minor in both theoretical latency models and simulations based on real-world networks.</article>","contentLength":1738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time-varying Vector Field Compression with Preserved Critical Point Trajectories","url":"https://arxiv.org/abs/2510.25143","date":1761796800,"author":"","guid":321730,"unread":true,"content":"<article>arXiv:2510.25143v1 Announce Type: new \nAbstract: Scientific simulations and observations are producing vast amounts of time-varying vector field data, making it hard to store them for archival purposes and transmit them for analysis. Lossy compression is considered a promising approach to reducing these data because lossless compression yields low compression ratios that barely mitigate the problem. However, directly applying existing lossy compression methods to timevarying vector fields may introduce undesired distortions in critical-point trajectories, a crucial feature that encodes key properties of the vector field. In this work, we propose an efficient lossy compression framework that exactly preserves all critical-point trajectories in time-varying vector fields. Our contributions are threefold. First, we extend the theory for preserving critical points in space to preserving critical-point trajectories in space-time, and develop a compression framework to realize the functionality. Second, we propose a semi-Lagrange predictor to exploit the spatiotemporal correlations in advectiondominated regions, and combine it with the traditional Lorenzo predictor for improved compression efficiency. Third, we evaluate our method against state-of-the-art lossy and lossless compressors using four real-world scientific datasets. Experimental results demonstrate that the proposed method delivers up to 124.48X compression ratios while effectively preserving all critical-point trajectories. This compression ratio is up to 56.07X higher than that of the best lossless compressors, and none of the existing lossy compressors can preserve all critical-point trajectories at similar compression ratios.</article>","contentLength":1714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Reconstruction-based AI-generated Image Detection: A Geometric Perspective","url":"https://arxiv.org/abs/2510.25141","date":1761796800,"author":"","guid":321731,"unread":true,"content":"<article>arXiv:2510.25141v1 Announce Type: new \nAbstract: The rise of generative Artificial Intelligence (AI) has made detecting AI-generated images a critical challenge for ensuring authenticity. Existing reconstruction-based methods lack theoretical foundations and on empirical heuristics, limiting interpretability and reliability. In this paper, we introduce the Jacobian-Spectral Lower Bound for reconstruction error from a geometric perspective, showing that real images off the reconstruction manifold exhibit a non-trivial error lower bound, while generated images on the manifold have near-zero error. Furthermore, we reveal the limitations of existing methods that rely on static reconstruction error from a single pass. These methods often fail when some real images exhibit lower error than generated ones. This counterintuitive behavior reduces detection accuracy and requires data-specific threshold tuning, limiting their applicability in real-world scenarios. To address these challenges, we propose ReGap, a training-free method that computes dynamic reconstruction error by leveraging structured editing operations to introduce controlled perturbations. This enables measuring error changes before and after editing, improving detection accuracy by enhancing error separation. Experimental results show that our method outperforms existing baselines, exhibits robustness to common post-processing operations and generalizes effectively across diverse conditions.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DINO-YOLO: Self-Supervised Pre-training for Data-Efficient Object Detection in Civil Engineering Applications","url":"https://arxiv.org/abs/2510.25140","date":1761796800,"author":"","guid":321732,"unread":true,"content":"<article>arXiv:2510.25140v1 Announce Type: cross \nAbstract: Object detection in civil engineering applications is constrained by limited annotated data in specialized domains. We introduce DINO-YOLO, a hybrid architecture combining YOLOv12 with DINOv3 self-supervised vision transformers for data-efficient detection. DINOv3 features are strategically integrated at two locations: input preprocessing (P0) and mid-backbone enhancement (P3). Experimental validation demonstrates substantial improvements: Tunnel Segment Crack detection (648 images) achieves 12.4% improvement, Construction PPE (1K images) gains 13.7%, and KITTI (7K images) shows 88.6% improvement, while maintaining real-time inference (30-47 FPS). Systematic ablation across five YOLO scales and nine DINOv3 variants reveals that Medium-scale architectures achieve optimal performance with DualP0P3 integration (55.77% mAP@0.5), while Small-scale requires Triple Integration (53.63%). The 2-4x inference overhead (21-33ms versus 8-16ms baseline) remains acceptable for field deployment on NVIDIA RTX 5090. DINO-YOLO establishes state-of-the-art performance for civil engineering datasets (&lt;10K images) while preserving computational efficiency, providing practical solutions for construction safety monitoring and infrastructure inspection in data-constrained environments.</article>","contentLength":1332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Spatial-Aware Manipulation Ordering","url":"https://arxiv.org/abs/2510.25138","date":1761796800,"author":"","guid":321733,"unread":true,"content":"<article>arXiv:2510.25138v1 Announce Type: new \nAbstract: Manipulation in cluttered environments is challenging due to spatial dependencies among objects, where an improper manipulation order can cause collisions or blocked access. Existing approaches often overlook these spatial relationships, limiting their flexibility and scalability. To address these limitations, we propose OrderMind, a unified spatial-aware manipulation ordering framework that directly learns object manipulation priorities based on spatial context. Our architecture integrates a spatial context encoder with a temporal priority structuring module. We construct a spatial graph using k-Nearest Neighbors to aggregate geometric information from the local layout and encode both object-object and object-manipulator interactions to support accurate manipulation ordering in real-time. To generate physically and semantically plausible supervision signals, we introduce a spatial prior labeling method that guides a vision-language model to produce reasonable manipulation orders for distillation. We evaluate OrderMind on our Manipulation Ordering Benchmark, comprising 163,222 samples of varying difficulty. Extensive experiments in both simulation and real-world environments demonstrate that our method significantly outperforms prior approaches in effectiveness and efficiency, enabling robust manipulation in cluttered scenes.</article>","contentLength":1396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Iceberg Index: Measuring Workforce Exposure Across the AI Economy","url":"https://arxiv.org/abs/2510.25137","date":1761796800,"author":"","guid":321734,"unread":true,"content":"<article>arXiv:2510.25137v1 Announce Type: new \nAbstract: Artificial Intelligence is reshaping America's \\$9.4 trillion labor market, with cascading effects that extend far beyond visible technology sectors. When AI transforms quality control tasks in automotive plants, consequences spread through logistics networks, supply chains, and local service economies. Yet traditional workforce metrics cannot capture these ripple effects: they measure employment outcomes after disruption occurs, not where AI capabilities overlap with human skills before adoption crystallizes. Project Iceberg addresses this gap using Large Population Models to simulate the human-AI labor market, representing 151 million workers as autonomous agents executing over 32,000 skills and interacting with thousands of AI tools. It introduces the Iceberg Index, a skills-centered metric that measures the wage value of skills AI systems can perform within each occupation. The Index captures technical exposure, where AI can perform occupational tasks, not displacement outcomes or adoption timelines. Analysis shows that visible AI adoption concentrated in computing and technology (2.2% of wage value, approx \\$211 billion) represents only the tip of the iceberg. Technical capability extends far below the surface through cognitive automation spanning administrative, financial, and professional services (11.7%, approx \\$1.2 trillion). This exposure is fivefold larger and geographically distributed across all states rather than confined to coastal hubs. Traditional indicators such as GDP, income, and unemployment explain less than 5% of this skills-based variation, underscoring why new indices are needed to capture exposure in the AI economy. By simulating how these capabilities may spread under scenarios, Iceberg enables policymakers and business leaders to identify exposure hotspots, prioritize investments, and test interventions before committing billions to implementation</article>","contentLength":1957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Region-CAM: Towards Accurate Object Regions in Class Activation Maps for Weakly Supervised Learning Tasks","url":"https://arxiv.org/abs/2510.25134","date":1761796800,"author":"","guid":321735,"unread":true,"content":"<article>arXiv:2510.25134v1 Announce Type: new \nAbstract: Class Activation Mapping (CAM) methods are widely applied in weakly supervised learning tasks due to their ability to highlight object regions. However, conventional CAM methods highlight only the most discriminative regions of the target. These highlighted regions often fail to cover the entire object and are frequently misaligned with object boundaries, thereby limiting the performance of downstream weakly supervised learning tasks, particularly Weakly Supervised Semantic Segmentation (WSSS), which demands pixel-wise accurate activation maps to get the best results. To alleviate the above problems, we propose a novel activation method, Region-CAM. Distinct from network feature weighting approaches, Region-CAM generates activation maps by extracting semantic information maps (SIMs) and performing semantic information propagation (SIP) by considering both gradients and features in each of the stages of the baseline classification model. Our approach highlights a greater proportion of object regions while ensuring activation maps to have precise boundaries that align closely with object edges. Region-CAM achieves 60.12% and 58.43% mean intersection over union (mIoU) using the baseline model on the PASCAL VOC training and validation datasets, respectively, which are improvements of 13.61% and 13.13% over the original CAM (46.51% and 45.30%). On the MS COCO validation set, Region-CAM achieves 36.38%, a 16.23% improvement over the original CAM (20.15%). We also demonstrate the superiority of Region-CAM in object localization tasks, using the ILSVRC2012 validation set. Region-CAM achieves 51.7% in Top-1 Localization accuracy Loc1. Compared with LayerCAM, an activation method designed for weakly supervised object localization, Region-CAM achieves 4.5% better performance in Loc1.</article>","contentLength":1852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Waterbed Effect on Quasiperiodic Disturbance Observer: Avoidance of Sensitivity Tradeoff with Time Delays","url":"https://arxiv.org/abs/2510.25131","date":1761796800,"author":"","guid":321736,"unread":true,"content":"<article>arXiv:2510.25131v1 Announce Type: new \nAbstract: In linear time-invariant systems, the sensitivity function to disturbances is designed under a sensitivity tradeoff known as the waterbed effect. To compensate for a quasiperiodic disturbance, a quasiperiodic disturbance observer using time delays was proposed. Its sensitivity function avoids the sensitivity tradeoff, achieving wideband harmonic suppression without amplifying aperiodic disturbances or shifting harmonic suppression frequencies. However, its open-loop transfer function is not rational and does not satisfy the assumptions of existing Bode sensitivity integrals due to its time delays. This paper provides Bode-like sensitivity integrals for the quasiperiodic disturbance observer in both continuous-time and discrete-time representations and clarifies the avoided sensitivity tradeoff with time delays.</article>","contentLength":871,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lipschitz-aware Linearity Grafting for Certified Robustness","url":"https://arxiv.org/abs/2510.25130","date":1761796800,"author":"","guid":321737,"unread":true,"content":"<article>arXiv:2510.25130v1 Announce Type: new \nAbstract: Lipschitz constant is a fundamental property in certified robustness, as smaller values imply robustness to adversarial examples when a model is confident in its prediction. However, identifying the worst-case adversarial examples is known to be an NP-complete problem. Although over-approximation methods have shown success in neural network verification to address this challenge, reducing approximation errors remains a significant obstacle. Furthermore, these approximation errors hinder the ability to obtain tight local Lipschitz constants, which are crucial for certified robustness. Originally, grafting linearity into non-linear activation functions was proposed to reduce the number of unstable neurons, enabling scalable and complete verification. However, no prior theoretical analysis has explained how linearity grafting improves certified robustness. We instead consider linearity grafting primarily as a means of eliminating approximation errors rather than reducing the number of unstable neurons, since linear functions do not require relaxation. In this paper, we provide two theoretical contributions: 1) why linearity grafting improves certified robustness through the lens of the $l_\\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear activation functions, the dominant source of approximation errors, yields a tighter local Lipschitz constant. Based on these theoretical contributions, we propose a Lipschitz-aware linearity grafting method that removes dominant approximation errors, which are crucial for tightening the local Lipschitz constant, thereby improving certified robustness, even without certified training. Our extensive experiments demonstrate that grafting linearity into these influential activations tightens the $l_\\infty$ local Lipschitz constant and enhances certified robustness.</article>","contentLength":1897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians","url":"https://arxiv.org/abs/2510.25129","date":1761796800,"author":"","guid":321738,"unread":true,"content":"<article>arXiv:2510.25129v1 Announce Type: new \nAbstract: 3D reconstruction of indoor and urban environments is a prominent research topic with various downstream applications. However, existing geometric priors for addressing low-texture regions in indoor and urban settings often lack global consistency. Moreover, Gaussian Splatting and implicit SDF fields often suffer from discontinuities or exhibit computational inefficiencies, resulting in a loss of detail. To address these issues, we propose an Atlanta-world guided implicit-structured Gaussian Splatting that achieves smooth indoor and urban scene reconstruction while preserving high-frequency details and rendering efficiency. By leveraging the Atlanta-world model, we ensure the accurate surface reconstruction for low-texture regions, while the proposed novel implicit-structured GS representations provide smoothness without sacrificing efficiency and high-frequency details. Specifically, we propose a semantic GS representation to predict the probability of all semantic regions and deploy a structure plane regularization with learnable plane indicators for global accurate surface reconstruction. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in both indoor and urban scenes, delivering superior surface reconstruction quality.</article>","contentLength":1332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation","url":"https://arxiv.org/abs/2510.25128","date":1761796800,"author":"","guid":321739,"unread":true,"content":"<article>arXiv:2510.25128v1 Announce Type: new \nAbstract: The technique of data augmentation (DA) is often used in machine learning for regularization purposes to better generalize under i.i.d. settings. In this work, we present a unifying framework with topics in causal inference to make a case for the use of DA beyond just the i.i.d. setting, but for generalization across interventions as well. Specifically, we argue that when the outcome generating mechanism is invariant to our choice of DA, then such augmentations can effectively be thought of as interventions on the treatment generating mechanism itself. This can potentially help to reduce bias in causal effect estimation arising from hidden confounders. In the presence of such unobserved confounding we typically make use of instrumental variables (IVs) -- sources of treatment randomization that are conditionally independent of the outcome. However, IVs may not be as readily available as DA for many applications, which is the main motivation behind this work. By appropriately regularizing IV based estimators, we introduce the concept of IV-like (IVL) regression for mitigating confounding bias and improving predictive performance across interventions even when certain IV properties are relaxed. Finally, we cast parameterized DA as an IVL regression problem and show that when used in composition can simulate a worst-case application of such DA, further improving performance on causal estimation and generalization tasks beyond what simple DA may offer. This is shown both theoretically for the population case and via simulation experiments for the finite sample case using a simple linear example. We also present real data experiments to support our case.</article>","contentLength":1725,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging the Divide: End-to-End Sequence-Graph Learning","url":"https://arxiv.org/abs/2510.25126","date":1761796800,"author":"","guid":321740,"unread":true,"content":"<article>arXiv:2510.25126v1 Announce Type: new \nAbstract: Many real-world datasets are both sequential and relational: each node carries an event sequence while edges encode interactions. Existing methods in sequence modeling and graph modeling often neglect one modality or the other. We argue that sequences and graphs are not separate problems but complementary facets of the same dataset, and should be learned jointly. We introduce BRIDGE, a unified end-to-end architecture that couples a sequence encoder with a GNN under a single objective, allowing gradients to flow across both modules and learning task-aligned representations. To enable fine-grained token-level message passing among neighbors, we add TOKENXATTN, a token-level cross-attention layer that passes messages between events in neighboring sequences. Across two settings, friendship prediction (Brightkite) and fraud detection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graph methods, and sequence-only baselines on ranking and classification metrics.</article>","contentLength":1035,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Low Rank Neural Representations of Hyperbolic Wave Dynamics from Data","url":"https://arxiv.org/abs/2510.25123","date":1761796800,"author":"","guid":321741,"unread":true,"content":"<article>arXiv:2510.25123v1 Announce Type: new \nAbstract: We present a data-driven dimensionality reduction method that is well-suited for physics-based data representing hyperbolic wave propagation. The method utilizes a specialized neural network architecture called low rank neural representation (LRNR) inside a hypernetwork framework. The architecture is motivated by theoretical results that rigorously prove the existence of efficient representations for this wave class. We illustrate through archetypal examples that such an efficient low-dimensional representation of propagating waves can be learned directly from data through a combination of deep learning techniques. We observe that a low rank tensor representation arises naturally in the trained LRNRs, and that this reveals a new decomposition of wave propagation where each decomposed mode corresponds to interpretable physical features. Furthermore, we demonstrate that the LRNR architecture enables efficient inference via a compression scheme, which is a potentially important feature when deploying LRNRs in demanding performance regimes.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies","url":"https://arxiv.org/abs/2510.25122","date":1761796800,"author":"","guid":321742,"unread":true,"content":"<article>arXiv:2510.25122v1 Announce Type: new \nAbstract: Vision-language-action (VLA) models have significantly advanced robotic manipulation by integrating vision-language models (VLMs), and action decoders into a unified architecture. However, their deployment on resource-constrained edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin Nano), remains challenging due to high computational demands, especially in real-world scenarios where power, latency, and computational resources are critical. To close this gap, we introduce Nano-scale Vision-Language Action (NanoVLA), a family of lightweight VLA architectures that achieve high performance with minimal resources. Our core innovations include: (1) vision-language decoupling that moves conventional early vision and language inputs fusion in VLM to late stage, achieving better performance while enabling caching and reduce inference overhead and latency; (2) long-short action chunking to ensure smooth, coherent multi-step planning without sacrificing real-time responsiveness; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity, further optimizing inference efficiency. Experimental results on several benchmarks, as well as real-world deployments, demonstrate that NanoVLA achieves up to 52x faster inference on edge devices compared to previous state-of-the-art VLA models, with 98% less parameters while maintaining or surpassing their task accuracy and generalization. Ablation studies confirm that our decoupling strategy preserves cross-task transferability, and the routing module enhances cost-performance trade-offs, enabling practical, high-precision robotic manipulation on resource-constrained hardware.</article>","contentLength":1741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Bilevel Model for Adversarial Learning and A Case Study","url":"https://arxiv.org/abs/2510.25121","date":1761796800,"author":"","guid":321743,"unread":true,"content":"<article>arXiv:2510.25121v1 Announce Type: new \nAbstract: Adversarial learning has been attracting more and more attention thanks to the fast development of machine learning and artificial intelligence. However, due to the complicated structure of most machine learning models, the mechanism of adversarial attacks is not well interpreted. How to measure the effect of attack is still not quite clear. In this paper, we propose a unified bilevel model for adversarial learning. We further investigate the adversarial attack in clustering models and interpret it from data perturbation point of view. We reveal that when the data perturbation is relatively small, the clustering model is robust, whereas if it is relatively large, the clustering result changes, which leads to an attack. To measure the effect of attacks for clustering models, we analyse the well-definedness of the so-called $\\delta$-measure, which can be used in the proposed bilevel model for adversarial learning of clustering models.</article>","contentLength":995,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMM-Fact: A Multimodal, Multi-Domain Fact-Checking Dataset with Multi-Level Retrieval Difficulty","url":"https://arxiv.org/abs/2510.25120","date":1761796800,"author":"","guid":321744,"unread":true,"content":"<article>arXiv:2510.25120v1 Announce Type: new \nAbstract: Misinformation and disinformation demand fact checking that goes beyond simple evidence-based reasoning. Existing benchmarks fall short: they are largely single modality (text-only), span short time horizons, use shallow evidence, cover domains unevenly, and often omit full articles -- obscuring models' real-world capability. We present MMM-Fact, a large-scale benchmark of 125,449 fact-checked statements (1995--2025) across multiple domains, each paired with the full fact-check article and multimodal evidence (text, images, videos, tables) from four fact-checking sites and one news outlet. To reflect verification effort, each statement is tagged with a retrieval-difficulty tier -- Basic (1--5 sources), Intermediate (6--10), and Advanced (&gt;10) -- supporting fairness-aware evaluation for multi-step, cross-modal reasoning. The dataset adopts a three-class veracity scheme (true/false/not enough information) and enables tasks in veracity prediction, explainable fact-checking, complex evidence aggregation, and longitudinal analysis. Baselines with mainstream LLMs show MMM-Fact is markedly harder than prior resources, with performance degrading as evidence complexity rises. MMM-Fact offers a realistic, scalable benchmark for transparent, reliable, multimodal fact-checking.</article>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stochastic Long-Term Joint Decarbonization Planning for Power Systems and Data Centers: A Case Study in PJM","url":"https://arxiv.org/abs/2510.25118","date":1761796800,"author":"","guid":321745,"unread":true,"content":"<article>arXiv:2510.25118v1 Announce Type: new \nAbstract: With the rapid growth of artificial intelligence (AI) and cloud services, data centers have become critical infrastructures driving digital economies, with increasing energy demand heightening concerns over electricity use and carbon emissions, emphasizing the need for carbon-aware infrastructure planning. Most studies assume static power systems, focus only on operational emissions, and overlook co-optimization. This paper proposes a dynamic joint planning framework that co-optimizes long-term data center and power system development over 15 years. The model determines siting, capacity, and type of data centers alongside power generation expansion, storage deployment, and retirements, accounting for both operational and embodied emissions. To handle multi-scale uncertainty, a large-scale two-stage stochastic program is formulated and solved via an enhanced Benders decomposition. Applied to the PJM Interconnection, with curated datasets released on GitHub, results show the system can support up to 55 GW peak data center demand, with Virginia (DOM) and Northern Illinois (ComEd) as optimal hosts. Compared to non-joint planning, the framework cuts investment cost by 12.6%, operational cost by 8.25%, and emissions by 5.63%. Including lifecycle emissions further raises renewable deployment by 25.5%, highlighting embodied carbon's role in deeper decarbonization.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Unlearning in Large Language Models","url":"https://arxiv.org/abs/2510.25117","date":1761796800,"author":"","guid":321746,"unread":true,"content":"<article>arXiv:2510.25117v1 Announce Type: new \nAbstract: The advancement of Large Language Models (LLMs) has revolutionized natural language processing, yet their training on massive corpora poses significant risks, including the memorization of sensitive personal data, copyrighted material, and knowledge that could facilitate malicious activities. To mitigate these issues and align with legal and ethical standards such as the \"right to be forgotten\", machine unlearning has emerged as a critical technique to selectively erase specific knowledge from LLMs without compromising their overall performance. This survey provides a systematic review of over 180 papers on LLM unlearning published since 2021, focusing exclusively on large-scale generative models. Distinct from prior surveys, we introduce novel taxonomies for both unlearning methods and evaluations. We clearly categorize methods into training-time, post-training, and inference-time based on the training stage at which unlearning is applied. For evaluations, we not only systematically compile existing datasets and metrics but also critically analyze their advantages, disadvantages, and applicability, providing practical guidance to the research community. In addition, we discuss key challenges and promising future research directions. Our comprehensive overview aims to inform and guide the ongoing development of secure and reliable LLMs.</article>","contentLength":1407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pretraining Strategies using Monolingual and Parallel Data for Low-Resource Machine Translation","url":"https://arxiv.org/abs/2510.25116","date":1761796800,"author":"","guid":321747,"unread":true,"content":"<article>arXiv:2510.25116v1 Announce Type: new \nAbstract: This research article examines the effectiveness of various pretraining strategies for developing machine translation models tailored to low-resource languages. Although this work considers several low-resource languages, including Afrikaans, Swahili, and Zulu, the translation model is specifically developed for Lingala, an under-resourced African language, building upon the pretraining approach introduced by Reid and Artetxe (2021), originally designed for high-resource languages. Through a series of comprehensive experiments, we explore different pretraining methodologies, including the integration of multiple languages and the use of both monolingual and parallel data during the pretraining phase. Our findings indicate that pretraining on multiple languages and leveraging both monolingual and parallel data significantly enhance translation quality. This study offers valuable insights into effective pretraining strategies for low-resource machine translation, helping to bridge the performance gap between high-resource and low-resource languages. The results contribute to the broader goal of developing more inclusive and accurate NLP models for marginalized communities and underrepresented populations. The code and datasets used in this study are publicly available to facilitate further research and ensure reproducibility, with the exception of certain data that may no longer be accessible due to changes in public availability.</article>","contentLength":1501,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Energy Approach from $\\varepsilon$-Graph to Continuum Diffusion Model with Connectivity Functional","url":"https://arxiv.org/abs/2510.25114","date":1761796800,"author":"","guid":321748,"unread":true,"content":"<article>arXiv:2510.25114v1 Announce Type: new \nAbstract: We derive an energy-based continuum limit for $\\varepsilon$-graphs endowed with a general connectivity functional. We prove that the discrete energy and its continuum counterpart differ by at most $O(\\varepsilon)$; the prefactor involves only the $W^{1,1}$-norm of the connectivity density as $\\varepsilon\\to0$, so the error bound remains valid even when that density has strong local fluctuations. As an application, we introduce a neural-network procedure that reconstructs the connectivity density from edge-weight data and then embeds the resulting continuum model into a brain-dynamics framework. In this setting, the usual constant diffusion coefficient is replaced by the spatially varying coefficient produced by the learned density, yielding dynamics that differ significantly from those obtained with conventional constant-diffusion models.</article>","contentLength":899,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Neural Differential Manifold: An Architecture with Explicit Geometric Structure","url":"https://arxiv.org/abs/2510.25113","date":1761796800,"author":"","guid":321749,"unread":true,"content":"<article>arXiv:2510.25113v1 Announce Type: new \nAbstract: This paper introduces the Neural Differential Manifold (NDM), a novel neural network architecture that explicitly incorporates geometric structure into its fundamental design. Departing from conventional Euclidean parameter spaces, the NDM re-conceptualizes a neural network as a differentiable manifold where each layer functions as a local coordinate chart, and the network parameters directly parameterize a Riemannian metric tensor at every point. The architecture is organized into three synergistic layers: a Coordinate Layer implementing smooth chart transitions via invertible transformations inspired by normalizing flows, a Geometric Layer that dynamically generates the manifold's metric through auxiliary sub-networks, and an Evolution Layer that optimizes both task performance and geometric simplicity through a dual-objective loss function. This geometric regularization penalizes excessive curvature and volume distortion, providing intrinsic regularization that enhances generalization and robustness. The framework enables natural gradient descent optimization aligned with the learned manifold geometry and offers unprecedented interpretability by endowing internal representations with clear geometric meaning. We analyze the theoretical advantages of this approach, including its potential for more efficient optimization, enhanced continual learning, and applications in scientific discovery and controllable generative modeling. While significant computational challenges remain, the Neural Differential Manifold represents a fundamental shift towards geometrically structured, interpretable, and efficient deep learning systems.</article>","contentLength":1701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Singularity Theory of Concurrent Programs: A Topological Characterization and Detection of Deadlocks and Livelocks","url":"https://arxiv.org/abs/2510.25112","date":1761796800,"author":"","guid":321750,"unread":true,"content":"<article>arXiv:2510.25112v1 Announce Type: new \nAbstract: This paper introduces a novel paradigm for the analysis and verification of concurrent programs -- the Singularity Theory. We model the execution space of a concurrent program as a branched topological space, where program states are points and state transitions are paths. Within this framework, we characterize deadlocks as attractors and livelocks as non-contractible loops in the execution space. By employing tools from algebraic topology, particularly homotopy and homology groups, we define a series of concurrent topological invariants to systematically detect and classify these concurrent \"singularities\" without exhaustively traversing all states. This work aims to establish a geometric and topological foundation for concurrent program verification, transcending the limitations of traditional model checking.</article>","contentLength":871,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates","url":"https://arxiv.org/abs/2510.25110","date":1761796800,"author":"","guid":321751,"unread":true,"content":"<article>arXiv:2510.25110v1 Announce Type: new \nAbstract: Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Shift is Good: Mismatched Data Mixing Improves Test Performance","url":"https://arxiv.org/abs/2510.25108","date":1761796800,"author":"","guid":321752,"unread":true,"content":"<article>arXiv:2510.25108v1 Announce Type: new \nAbstract: We consider training and testing on mixture distributions with different training and test proportions. We show that in many settings, and in some sense generically, distribution shift can be beneficial, and test performance can improve due to mismatched training proportions, even if the components are unrelated and with no transfer between components. In a variety of scenarios, we identify the optimal training proportions and the extent to which such distribution shift can be beneficial. We show how the same analysis applies also to a compositional setting with differing distribution of component \"skills'' at training and test.</article>","contentLength":685,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Hamiltonian flows from numerical integrators and examples","url":"https://arxiv.org/abs/2510.25107","date":1761796800,"author":"","guid":321753,"unread":true,"content":"<article>arXiv:2510.25107v1 Announce Type: new \nAbstract: Hamiltonian systems with multiple timescales arise in molecular dynamics, classical mechanics, and theoretical physics. Long-time numerical integration of such systems requires resolving fast dynamics with very small time steps, which incurs a high computational cost - especially in ensemble simulations for uncertainty quantification, sensitivity analysis, or varying initial conditions. We present a Deep Learning framework that learns the flow maps of Hamiltonian systems to accelerate long-time and ensemble simulations. Neural networks are trained, according to a chosen numerical scheme, either entirely without data to approximate flows over large time intervals or with data to learn flows in intervals far from the initial time. For the latter, we propose a Hamiltonian Monte Carlo-based data generator. The architecture consists of simple feedforward networks that incorporate truncated Taylor expansions of the flow map, with a neural network remainder capturing unresolved effects. Applied to benchmark non-integrable and non-canonical systems, the method achieves substantial speedups while preserving accuracy, enabling scalable simulation of complex Hamiltonian dynamics.</article>","contentLength":1236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning-Based vs Human-Derived Congestion Control: An In-Depth Experimental Study","url":"https://arxiv.org/abs/2510.25105","date":1761796800,"author":"","guid":321754,"unread":true,"content":"<article>arXiv:2510.25105v1 Announce Type: new \nAbstract: Learning-based congestion control (CC), including Reinforcement-Learning, promises efficient CC in a fast-changing networking landscape, where evolving communication technologies, applications and traffic workloads pose severe challenges to human-derived, static CC algorithms. Learning-based CC is in its early days and substantial research is required to understand existing limitations, identify research challenges and, eventually, yield deployable solutions for real-world networks. In this paper, we extend our prior work and present a reproducible and systematic study of learning-based CC with the aim to highlight strengths and uncover fundamental limitations of the state-of-the-art. We directly contrast said approaches with widely deployed, human-derived CC algorithms, namely TCP Cubic and BBR (version 3). We identify challenges in evaluating learning-based CC, establish a methodology for studying said approaches and perform large-scale experimentation with learning-based CC approaches that are publicly available. We show that embedding fairness directly into reward functions is effective; however, the fairness properties do not generalise into unseen conditions. We then show that RL learning-based approaches existing approaches can acquire all available bandwidth while largely maintaining low latency. Finally, we highlight that existing the latest learning-based CC approaches under-perform when the available bandwidth and end-to-end latency dynamically change while remaining resistant to non-congestive loss. As with our initial study, our experimentation codebase and datasets are publicly available with the aim to galvanise the research community towards transparency and reproducibility, which have been recognised as crucial for researching and evaluating machine-generated policies.</article>","contentLength":1865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Proof Refinement with LLM-Guided Strategy Selection","url":"https://arxiv.org/abs/2510.25103","date":1761796800,"author":"","guid":321755,"unread":true,"content":"<article>arXiv:2510.25103v1 Announce Type: new \nAbstract: Formal verification via theorem proving enables the expressive specification and rigorous proof of software correctness, but it is difficult to scale due to the significant manual effort and expertise required. While Large Language Models (LLMs) show potential in proof generation, they frequently produce incorrect proofs on the first attempt and require additional strategies for iterative refinement. However, existing approaches employ fixed refinement strategies and cannot dynamically choose an effective strategy based on the particular issues in a generated proof, which limits their performance. To overcome this limitation, we introduce Adapt, a novel proof refinement framework that leverages an LLM-guided decision-maker to dynamically select a suitable refinement strategy according to the state of the proof assistant and available context of an incorrect proof. We evaluate Adapt on two benchmarks against four existing methods and find that it significantly outperforms the best baseline on both by proving 16.63% and 18.58% more theorems, respectively. Furthermore, we demonstrate Adapt's generalizability by evaluating it across five different LLMs. We also conduct ablation studies to measure the contribution of each component and compare the trade-offs of alternative decision-maker designs.</article>","contentLength":1361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KnowCoder-A1: Incentivizing Agentic Reasoning Capability with Outcome Supervision for KBQA","url":"https://arxiv.org/abs/2510.25101","date":1761796800,"author":"","guid":321756,"unread":true,"content":"<article>arXiv:2510.25101v1 Announce Type: new \nAbstract: Knowledge Base Question Answering (KBQA) aims to answer natural-language questions over a structured Knowledge Base (KB). Recent work improves KBQA by adopting an agentic reasoning paradigm, in which Large Language Models (LLMs) iteratively decompose a question, generate its corresponding logical queries, and interact with the KB to derive the answer. However, these methods typically fine-tune LLMs on reasoning trajectories synthesized via process supervision, which offers weak incentives for exploration and thus fails to strengthen the agentic reasoning ability. In this paper, we propose KnowCoder-A1, an LLM that can autonomously perform agentic reasoning on KBs to obtain answers. To incentivize autonomous exploration, KnowCoder-A1 trains the LLM under outcome-only supervision via a multi-stage curriculum reinforcement learning with an easy-to-hard curriculum. To establish foundational agentic capabilities, KnowCoder-A1 first fine-tunes the LLM on a small set of high-quality trajectories obtained through outcome-based rejection sampling. Then, to alleviate the reward sparsity inherent in outcome-only supervision, it applies multi-stage curriculum RL with reward schedules that progress from easy to hard. Trained with outcome-only supervision, KnowCoder-A1 exhibits powerful reasoning behaviors and consistently outperforms prior approaches across three mainstream datasets. Notably, on the zero-shot subset of GrailQA, KnowCoder-A1 achieves up to an 11.1% relative improvement while using only one-twelfth of the training data, demonstrating strong agentic reasoning capabilities.</article>","contentLength":1649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Fair Graph Representations with Multi-view Information Bottleneck","url":"https://arxiv.org/abs/2510.25096","date":1761796800,"author":"","guid":321757,"unread":true,"content":"<article>arXiv:2510.25096v1 Announce Type: new \nAbstract: Graph neural networks (GNNs) excel on relational data by passing messages over node features and structure, but they can amplify training data biases, propagating discriminatory attributes and structural imbalances into unfair outcomes. Many fairness methods treat bias as a single source, ignoring distinct attribute and structure effects and leading to suboptimal fairness and utility trade-offs. To overcome this challenge, we propose FairMIB, a multi-view information bottleneck framework designed to decompose graphs into feature, structural, and diffusion views for mitigating complexity biases in GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize cross-view mutual information for bias-free representation learning. It further integrates multi-perspective conditional information bottleneck objectives to balance task utility and fairness by minimizing mutual information with sensitive attributes. Additionally, FairMIB introduces an inverse probability-weighted (IPW) adjacency correction in the diffusion view, which reduces the spread of bias propagation during message passing. Experiments on five real-world benchmark datasets demonstrate that FairMIB achieves state-of-the-art performance across both utility and fairness metrics.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Socio-cognitive agent-oriented evolutionary algorithm with trust-based optimization","url":"https://arxiv.org/abs/2510.25095","date":1761796800,"author":"","guid":321758,"unread":true,"content":"<article>arXiv:2510.25095v1 Announce Type: new \nAbstract: This paper introduces the Trust-Based Optimization (TBO), a novel extension of the island model in evolutionary computation that replaces conventional periodic migrations with a flexible, agent-driven interaction mechanism based on trust or reputation. Experimental results demonstrate that TBO generally outperforms the standard island model evolutionary algorithm across various optimization problems. Nevertheless, algorithm performance varies depending on the problem type, with certain configurations being more effective for specific landscapes or dimensions. The findings suggest that trust and reputation mechanisms provide a flexible and adaptive approach to evolutionary optimization, improving solution quality in many cases.</article>","contentLength":785,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection","url":"https://arxiv.org/abs/2510.25094","date":1761796800,"author":"","guid":321759,"unread":true,"content":"<article>arXiv:2510.25094v1 Announce Type: new \nAbstract: Zero-shot Human-Object Interaction detection aims to localize humans and objects in an image and recognize their interaction, even when specific verb-object pairs are unseen during training. Recent works have shown promising results using prompt learning with pretrained vision-language models such as CLIP, which align natural language prompts with visual features in a shared embedding space. However, existing approaches still fail to handle the visual complexity of interaction, including (1) intra-class visual diversity, where instances of the same verb appear in diverse poses and contexts, and (2) inter-class visual entanglement, where distinct verbs yield visually similar patterns. To address these challenges, we propose VDRP, a framework for Visual Diversity and Region-aware Prompt learning. First, we introduce a visual diversity-aware prompt learning strategy that injects group-wise visual variance into the context embedding. We further apply Gaussian perturbation to encourage the prompts to capture diverse visual variations of a verb. Second, we retrieve region-specific concepts from the human, object, and union regions. These are used to augment the diversity-aware prompt embeddings, yielding region-aware prompts that enhance verb-level discrimination. Experiments on the HICO-DET benchmark demonstrate that our method achieves state-of-the-art performance under four zero-shot evaluation settings, effectively addressing both intra-class diversity and inter-class visual entanglement. Code is available at https://github.com/mlvlab/VDRP.</article>","contentLength":1613,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continual Low-Rank Adapters for LLM-based Generative Recommender Systems","url":"https://arxiv.org/abs/2510.25093","date":1761796800,"author":"","guid":321760,"unread":true,"content":"<article>arXiv:2510.25093v1 Announce Type: new \nAbstract: While large language models (LLMs) achieve strong performance in recommendation, they face challenges in continual learning as users, items, and user preferences evolve over time. Existing LoRA-based continual methods primarily focus on preserving performance on previous tasks, but this overlooks the unique nature of recommendation: the goal is not to predict past preferences, and outdated preferences can even harm performance when current interests shift significantly. To address this, we propose PESO (Proximally rEgularized Single evolving lOra, a continual adaptation method for LoRA in recommendation. PESO introduces a proximal regularizer that anchors the current adapter to its most recent frozen state, enabling the model to flexibly balance adaptation and preservation, and to better capture recent user behaviors. Theoretically, we show that this proximal design provides data-aware, direction-wise guidance in the LoRA subspace. Empirically, PESO consistently outperforms existing LoRA-based continual learning methods.</article>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs","url":"https://arxiv.org/abs/2510.25092","date":1761796800,"author":"","guid":321761,"unread":true,"content":"<article>arXiv:2510.25092v1 Announce Type: new \nAbstract: Recent advances in text-only large language models (LLMs), such as DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models remain fragile or entirely incapable when extended to multi-modal tasks. Existing approaches largely rely on single-form captions, which lack diversity and often fail to adapt across different types of Visual Question Answering (VQA) benchmarks. As a result, they provide no principled or efficient channel for transmitting fine-grained visual information. We introduce Seeing Eye, a modular framework that unlocks multimodal reasoning in text-only LLMs through an agent-based small VLM translator. This translator acts as a perception agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively distill multimodal inputs into structured intermediate representations (SIRs) tailored to the question. These SIRs are then passed to the text-only LLM, which serves as a reasoning agent. Crucially, the translator and reasoner engage in multi-round feedback and interaction, enabling the extraction of targeted visual details and yielding more confident answers. Experiments on knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate that Seeing Eye not only reduces inference cost but also surpasses much larger end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision translator with an 8B-parameter language reasoner outperforms a monolithic 32B VLM on challenging knowledge-based questions. Our results highlight that decoupling perception from reasoning via agent information flow offers a scalable and plug-and-play pathway to multimodal reasoning, allowing strong text-only LLMs to fully leverage their reasoning capabilities. Code is available at: https://github.com/ulab-uiuc/SeeingEye</article>","contentLength":1843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"H3M-SSMoEs: Hypergraph-based Multimodal Learning with LLM Reasoning and Style-Structured Mixture of Experts","url":"https://arxiv.org/abs/2510.25091","date":1761796800,"author":"","guid":321762,"unread":true,"content":"<article>arXiv:2510.25091v1 Announce Type: new \nAbstract: Stock movement prediction remains fundamentally challenging due to complex temporal dependencies, heterogeneous modalities, and dynamically evolving inter-stock relationships. Existing approaches often fail to unify structural, semantic, and regime-adaptive modeling within a scalable framework. This work introduces H3M-SSMoEs, a novel Hypergraph-based MultiModal architecture with LLM reasoning and Style-Structured Mixture of Experts, integrating three key innovations: (1) a Multi-Context Multimodal Hypergraph that hierarchically captures fine-grained spatiotemporal dynamics via a Local Context Hypergraph (LCH) and persistent inter-stock dependencies through a Global Context Hypergraph (GCH), employing shared cross-modal hyperedges and Jensen-Shannon Divergence weighting mechanism for adaptive relational learning and cross-modal alignment; (2) a LLM-enhanced reasoning module, which leverages a frozen large language model with lightweight adapters to semantically fuse and align quantitative and textual modalities, enriching representations with domain-specific financial knowledge; and (3) a Style-Structured Mixture of Experts (SSMoEs) that combines shared market experts and industry-specialized experts, each parameterized by learnable style vectors enabling regime-aware specialization under sparse activation. Extensive experiments on three major stock markets demonstrate that H3M-SSMoEs surpasses state-of-the-art methods in both superior predictive accuracy and investment performance, while exhibiting effective risk control. Datasets, source code, and model weights are available at our GitHub repository: https://github.com/PeilinTime/H3M-SSMoEs.</article>","contentLength":1720,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BioCoref: Benchmarking Biomedical Coreference Resolution with LLMs","url":"https://arxiv.org/abs/2510.25087","date":1761796800,"author":"","guid":321763,"unread":true,"content":"<article>arXiv:2510.25087v1 Announce Type: new \nAbstract: Coreference resolution in biomedical texts presents unique challenges due to complex domain-specific terminology, high ambiguity in mention forms, and long-distance dependencies between coreferring expressions. In this work, we present a comprehensive evaluation of generative large language models (LLMs) for coreference resolution in the biomedical domain. Using the CRAFT corpus as our benchmark, we assess the LLMs' performance with four prompting experiments that vary in their use of local, contextual enrichment, and domain-specific cues such as abbreviations and entity dictionaries. We benchmark these approaches against a discriminative span-based encoder, SpanBERT, to compare the efficacy of generative versus discriminative methods. Our results demonstrate that while LLMs exhibit strong surface-level coreference capabilities, especially when supplemented with domain-grounding prompts, their performance remains sensitive to long-range context and mentions ambiguity. Notably, the LLaMA 8B and 17B models show superior precision and F1 scores under entity-augmented prompting, highlighting the potential of lightweight prompt engineering for enhancing LLM utility in biomedical NLP tasks.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mean-Shift Theory and Its Applications in Swarm Robotics: A New Way to Enhance the Efficiency of Multi-Robot Collaboration","url":"https://arxiv.org/abs/2510.25086","date":1761796800,"author":"","guid":321764,"unread":true,"content":"<article>arXiv:2510.25086v1 Announce Type: new \nAbstract: Swarms evolving from collective behaviors among multiple individuals are commonly seen in nature, which enables biological systems to exhibit more efficient and robust collaboration. Creating similar swarm intelligence in engineered robots poses challenges to the design of collaborative algorithms that can be programmed at large scales. The assignment-based method has played an eminent role for a very long time in solving collaboration problems of robot swarms. However, it faces fundamental limitations in terms of efficiency and robustness due to its unscalability to swarm variants. This article presents a tutorial review on recent advances in assignment-free collaboration of robot swarms, focusing on the problem of shape formation. A key theoretical component is the recently developed \\emph{mean-shift exploration} strategy, which improves the collaboration efficiency of large-scale swarms by dozens of times. Further, the efficiency improvement is more significant as the swarm scale increases. Finally, this article discusses three important applications of the mean-shift exploration strategy, including precise shape formation, area coverage formation, and maneuvering formation, as well as their corresponding industrial scenarios in smart warehousing, area exploration, and cargo transportation.</article>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PSTF-AttControl: Per-Subject-Tuning-Free Personalized Image Generation with Controllable Face Attributes","url":"https://arxiv.org/abs/2510.25084","date":1761796800,"author":"","guid":321765,"unread":true,"content":"<article>arXiv:2510.25084v1 Announce Type: new \nAbstract: Recent advancements in personalized image generation have significantly improved facial identity preservation, particularly in fields such as entertainment and social media. However, existing methods still struggle to achieve precise control over facial attributes in a per-subject-tuning-free (PSTF) way. Tuning-based techniques like PreciseControl have shown promise by providing fine-grained control over facial features, but they often require extensive technical expertise and additional training data, limiting their accessibility. In contrast, PSTF approaches simplify the process by enabling image generation from a single facial input, but they lack precise control over facial attributes. In this paper, we introduce a novel, PSTF method that enables both precise control over facial attributes and high-fidelity preservation of facial identity. Our approach utilizes a face recognition model to extract facial identity features, which are then mapped into the $W^+$ latent space of StyleGAN2 using the e4e encoder. We further enhance the model with a Triplet-Decoupled Cross-Attention module, which integrates facial identity, attribute features, and text embeddings into the UNet architecture, ensuring clean separation of identity and attribute information. Trained on the FFHQ dataset, our method allows for the generation of personalized images with fine-grained control over facial attributes, while without requiring additional fine-tuning or training data for individual identities. We demonstrate that our approach successfully balances personalization with precise facial attribute control, offering a more efficient and user-friendly solution for high-quality, adaptable facial image synthesis. The code is publicly available at https://github.com/UnicomAI/PSTF-AttControl.</article>","contentLength":1843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games","url":"https://arxiv.org/abs/2510.25080","date":1761796800,"author":"","guid":321766,"unread":true,"content":"<article>arXiv:2510.25080v1 Announce Type: new \nAbstract: Card games are widely used to study sequential decision-making under uncertainty, with real-world analogues in negotiation, finance, and cybersecurity. Typically, these games fall into three categories based on the flow of control: strictly-sequential (where players alternate single actions), deterministic-response (where some actions trigger a fixed outcome), and unbounded reciprocal-response (where alternating counterplays are permitted). A less-explored but strategically rich structure exists: the bounded one-sided response. This dynamic occurs when a player's action briefly transfers control to the opponent, who must satisfy a fixed condition through one or more sequential moves before the turn resolves. We term games featuring this mechanism Bounded One-Sided Response Games (BORGs).\n  We introduce a modified version of Monopoly Deal as a benchmark environment that specifically isolates the BORG dynamic, where a Rent action forces the opponent to sequentially choose payment assets. We demonstrate that the gold-standard algorithm, Counterfactual Regret Minimization (CFR), successfully converges on effective strategies for this domain without requiring novel algorithmic extensions. To support efficient, reproducible experimentation, we present a lightweight, full-stack research platform that unifies the environment, a parallelized CFR runtime, and a human-playable web interface, all runnable on a single workstation. This system provides a practical foundation for exploring state representation and policy learning in bounded one-sided response settings.\n  The trained CFR agent and source code are available at https://monopolydeal.ai.</article>","contentLength":1711,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Performance Evaluation of Multimedia Traffic in Cloud Storage Services over Wi-Fi and LTE Networks","url":"https://arxiv.org/abs/2510.25079","date":1761796800,"author":"","guid":321767,"unread":true,"content":"<article>arXiv:2510.25079v1 Announce Type: new \nAbstract: The performance of Dropbox, Google Drive, and OneDrive cloud storage services was evaluated under Wi-Fi and LTE network conditions during multimedia file uploads. Traffic was captured using Wireshark, and key metrics (including delay, jitter, bandwidth, and packet loss) were analyzed. Google Drive maintained the most consistent performance across both types of networks, showing low latency and reduced jitter. Dropbox showed efficient bandwidth utilization, but experienced a longer delay over LTE, attributed to a greater number of intermediate hops. OneDrive presented variable behavior, with elevated packet rates and increased sensitivity to fluctuations in the mobile network. A bimodal distribution of packet sizes was observed and modeled using a dual Poisson function. In general, Wi-Fi connections provided greater stability for multimedia transfers, while LTE performance varied depending on platform-specific implementations. The results contribute to a better understanding of traffic behavior in cloud-based storage applications and suggest further analysis with larger datasets and heterogeneous access networks.</article>","contentLength":1178,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neighborhood Feature Pooling for Remote Sensing Image Classification","url":"https://arxiv.org/abs/2510.25077","date":1761796800,"author":"","guid":321768,"unread":true,"content":"<article>arXiv:2510.25077v1 Announce Type: new \nAbstract: In this work, we propose neighborhood feature pooling (NFP) as a novel texture feature extraction method for remote sensing image classification. The NFP layer captures relationships between neighboring inputs and efficiently aggregates local similarities across feature dimensions. Implemented using convolutional layers, NFP can be seamlessly integrated into any network. Results comparing the baseline models and the NFP method indicate that NFP consistently improves performance across diverse datasets and architectures while maintaining minimal parameter overhead.</article>","contentLength":619,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Joint Analysis of Acoustic Scenes and Sound Events Based on Semi-Supervised Training of Sound Events With Partial Labels","url":"https://arxiv.org/abs/2510.25075","date":1761796800,"author":"","guid":321769,"unread":true,"content":"<article>arXiv:2510.25075v1 Announce Type: new \nAbstract: Annotating time boundaries of sound events is labor-intensive, limiting the scalability of strongly supervised learning in audio detection. To reduce annotation costs, weakly-supervised learning with only clip-level labels has been widely adopted. As an alternative, partial label learning offers a cost-effective approach, where a set of possible labels is provided instead of exact weak annotations. However, partial label learning for audio analysis remains largely unexplored. Motivated by the observation that acoustic scenes provide contextual information for constructing a set of possible sound events, we utilize acoustic scene information to construct partial labels of sound events. On the basis of this idea, in this paper, we propose a multitask learning framework that jointly performs acoustic scene classification and sound event detection with partial labels of sound events. While reducing annotation costs, weakly-supervised and partial label learning often suffer from decreased detection performance due to lacking the precise event set and their temporal annotations. To better balance between annotation cost and detection performance, we also explore a semi-supervised framework that leverages both strong and partial labels. Moreover, to refine partial labels and achieve better model training, we propose a label refinement method based on self-distillation for the proposed approach with partial labels.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Training Across Reservoirs: Using Numerical Differentiation To Couple Trainable Networks With Black-Box Reservoirs","url":"https://arxiv.org/abs/2510.25074","date":1761796800,"author":"","guid":321770,"unread":true,"content":"<article>arXiv:2510.25074v1 Announce Type: new \nAbstract: We introduce Bounded Numerical Differentiation (BOND), a perturbative method for estimating partial derivatives across network structures with inaccessible computational graphs. BOND demonstrates improved accuracy and scalability from existing perturbative methods, enabling new explorations of trainable architectures that integrate black-box functions. We observe that these black-box functions, realized in our experiments as fixed, untrained networks, can enhance model performance without increasing the number of trainable parameters. This improvement is achieved without extensive optimization of the architecture or properties of the black-box function itself. Our findings highlight the potential of leveraging fixed, non-trainable modules to expand model capacity, suggesting a path toward combining analogue and digital devices as a mechanism for scaling networks.</article>","contentLength":924,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Invasive Calibration Of A Stewart Platform By Photogrammetry","url":"https://arxiv.org/abs/2510.25072","date":1761796800,"author":"","guid":321771,"unread":true,"content":"<article>arXiv:2510.25072v1 Announce Type: new \nAbstract: Accurate calibration of a Stewart platform is important for their precise and efficient operation. However, the calibration of these platforms using forward kinematics is a challenge for researchers because forward kinematics normally generates multiple feasible and unfeasible solutions for any pose of the moving platform. The complex kinematic relations among the six actuator paths connecting the fixed base to the moving platform further compound the difficulty in establishing a straightforward and efficient calibration method. The authors developed a new forward kinematics-based calibration method using Denavit-Hartenberg convention and used the Stewart platform Tiger 66.1 developed in their lab for experimenting with the photogrammetry-based calibration strategies described in this paper. This system became operational upon completion of construction, marking its inaugural use. The authors used their calibration model for estimating the errors in the system and adopted three compensation options or strategies as per Least Square method to improve the accuracy of the system. These strategies leveraged a high-resolution digital camera and off-the-shelf software to capture the poses of the moving platform's center. This process is non-invasive and does not need any additional equipment to be attached to the hexapod or any alteration of the hexapod hardware. This photogrammetry-based calibration process involves multiple high-resolution images from different angles to measure the position and orientation of the platform center in the three-dimensional space. The Target poses and Actual poses are then compared, and the error compensations are estimated using the Least-Squared methods to calculate the Predicted poses. Results from each of the three compensation approaches demonstrated noticeable enhancements in platform pose accuracies, suggesting room for further improvements.</article>","contentLength":1956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments","url":"https://arxiv.org/abs/2510.25070","date":1761796800,"author":"","guid":321772,"unread":true,"content":"<article>arXiv:2510.25070v1 Announce Type: new \nAbstract: Zero-shot scene understanding in real-world settings presents major challenges due to the complexity and variability of natural scenes, where models must recognize new objects, actions, and contexts without prior labeled examples. This work proposes a vision-language integration framework that unifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models (e.g., GPT-based architectures) to achieve semantic alignment between visual and textual modalities. The goal is to enable robust zero-shot comprehension of scenes by leveraging natural language as a bridge to generalize over unseen categories and contexts. Our approach develops a unified model that embeds visual inputs and textual prompts into a shared space, followed by multimodal fusion and reasoning layers for contextual interpretation. Experiments on Visual Genome, COCO, ADE20K, and custom real-world datasets demonstrate significant gains over state-of-the-art zero-shot models in object recognition, activity detection, and scene captioning. The proposed system achieves up to 18% improvement in top-1 accuracy and notable gains in semantic coherence metrics, highlighting the effectiveness of cross-modal alignment and language grounding in enhancing generalization for real-world scene understanding.</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TOPol: Capturing and Explaining Multidimensional Semantic Polarity Fields and Vectors","url":"https://arxiv.org/abs/2510.25069","date":1761796800,"author":"","guid":321773,"unread":true,"content":"<article>arXiv:2510.25069v1 Announce Type: new \nAbstract: Traditional approaches to semantic polarity in computational linguistics treat sentiment as a unidimensional scale, overlooking the multidimensional structure of language. This work introduces TOPol (Topic-Orientation POLarity), a semi-unsupervised framework for reconstructing and interpreting multidimensional narrative polarity fields under human-on-the-loop (HoTL) defined contextual boundaries (CBs). The framework embeds documents using a transformer-based large language model (tLLM), applies neighbor-tuned UMAP projection, and segments topics via Leiden partitioning. Given a CB between discourse regimes A and B, TOPol computes directional vectors between corresponding topic-boundary centroids, yielding a polarity field that quantifies fine-grained semantic displacement during regime shifts. This vectorial representation enables assessing CB quality and detecting polarity changes, guiding HoTL CB refinement. To interpret identified polarity vectors, the tLLM compares their extreme points and produces contrastive labels with estimated coverage. Robustness analyses show that only CB definitions (the main HoTL-tunable parameter) significantly affect results, confirming methodological stability. We evaluate TOPol on two corpora: (i) U.S. Central Bank speeches around a macroeconomic breakpoint, capturing non-affective semantic shifts, and (ii) Amazon product reviews across rating strata, where affective polarity aligns with NRC valence. Results demonstrate that TOPol consistently captures both affective and non-affective polarity transitions, providing a scalable, generalizable, and interpretable framework for context-sensitive multidimensional discourse analysis.</article>","contentLength":1738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DRIP: Dynamic patch Reduction via Interpretable Pooling","url":"https://arxiv.org/abs/2510.25067","date":1761796800,"author":"","guid":321774,"unread":true,"content":"<article>arXiv:2510.25067v1 Announce Type: new \nAbstract: Recently, the advances in vision-language models, including contrastive pretraining and instruction tuning, have greatly pushed the frontier of multimodal AI. However, owing to the large-scale and hence expensive pretraining, the efficiency concern has discouraged researchers from attempting to pretrain a vision language model from scratch. In this work, we propose Dynamic patch Reduction via Interpretable Pooling (DRIP), which adapts to the input images and dynamically merges tokens in the deeper layers of a visual encoder. Our results on both ImageNet training from scratch and CLIP contrastive pretraining demonstrate a significant GFLOP reduction while maintaining comparable classification/zero-shot performance. To further validate our proposed method, we conduct continual pretraining on a large biology dataset, extending its impact into scientific domains.</article>","contentLength":920,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reasoning-Aware GRPO using Process Mining","url":"https://arxiv.org/abs/2510.25065","date":1761796800,"author":"","guid":321775,"unread":true,"content":"<article>arXiv:2510.25065v1 Announce Type: new \nAbstract: Reinforcement learning (RL)-based post-training has been crucial for enabling multi-step reasoning in large reasoning models (LRMs), yet current reward schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware Group Relative Policy Optimization (GRPO) that augments standard answer/format rewards with signals over the reasoning procedure. To this end, process mining techniques are utilized to compute a scalar conformance reward that measures how closely a policy model's reasoning aligns with the pretrained teacher model. The empirical results on five benchmarks demonstrate that PM4GRPO significantly outperforms existing methodologies for GRPO-based post-training. These results highlight that leveraging process mining for reasoning-aware GRPO effectively enhances the reasoning capabilities of policy models.</article>","contentLength":884,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can LLMs Estimate Cognitive Complexity of Reading Comprehension Items?","url":"https://arxiv.org/abs/2510.25064","date":1761796800,"author":"","guid":321776,"unread":true,"content":"<article>arXiv:2510.25064v1 Announce Type: new \nAbstract: Estimating the cognitive complexity of reading comprehension (RC) items is crucial for assessing item difficulty before it is administered to learners. Unlike syntactic and semantic features, such as passage length or semantic similarity between options, cognitive features that arise during answer reasoning are not readily extractable using existing NLP tools and have traditionally relied on human annotation. In this study, we examine whether large language models (LLMs) can estimate the cognitive complexity of RC items by focusing on two dimensions-Evidence Scope and Transformation Level-that indicate the degree of cognitive burden involved in reasoning about the answer. Our experimental results demonstrate that LLMs can approximate the cognitive complexity of items, indicating their potential as tools for prior difficulty analysis. Further analysis reveals a gap between LLMs' reasoning ability and their metacognitive awareness: even when they produce correct answers, they sometimes fail to correctly identify the features underlying their own reasoning process.</article>","contentLength":1127,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Control Synthesis with Reinforcement Learning: A Modeling Perspective","url":"https://arxiv.org/abs/2510.25063","date":1761796800,"author":"","guid":321777,"unread":true,"content":"<article>arXiv:2510.25063v1 Announce Type: new \nAbstract: Controllers designed with reinforcement learning can be sensitive to model mismatch. We demonstrate that designing such controllers in a virtual simulation environment with an inaccurate model is not suitable for deployment in a physical setup. Controllers designed using an accurate model is robust against disturbance and small mismatch between the physical setup and the mathematical model derived from first principles; while a poor model results in a controller that performs well in simulation but fails in physical experiments. Sensitivity analysis is used to justify these discrepancies and an empirical region of attraction estimation help us visualize their robustness.</article>","contentLength":728,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Auto3DSeg for Brain Tumor Segmentation from 3D MRI in BraTS 2023 Challenge","url":"https://arxiv.org/abs/2510.25058","date":1761796800,"author":"","guid":321778,"unread":true,"content":"<article>arXiv:2510.25058v1 Announce Type: new \nAbstract: In this work, we describe our solution to the BraTS 2023 cluster of challenges using Auto3DSeg from MONAI. We participated in all 5 segmentation challenges, and achieved the 1st place results in three of them: Brain Metastasis, Brain Meningioma, BraTS-Africa challenges, and the 2nd place results in the remaining two: Adult and Pediatic Glioma challenges.</article>","contentLength":405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Same Same But Different: Preventing Refactoring Attacks on Software Plagiarism Detection","url":"https://arxiv.org/abs/2510.25057","date":1761796800,"author":"","guid":321779,"unread":true,"content":"<article>arXiv:2510.25057v1 Announce Type: new \nAbstract: Plagiarism detection in programming education faces growing challenges due to increasingly sophisticated obfuscation techniques, particularly automated refactoring-based attacks. While code plagiarism detection systems used in education practice are resilient against basic obfuscation, they struggle against structural modifications that preserve program behavior, especially caused by refactoring-based obfuscation. This paper presents a novel and extensible framework that enhances state-of-the-art detectors by leveraging code property graphs and graph transformations to counteract refactoring-based obfuscation. Our comprehensive evaluation of real-world student submissions, obfuscated using both algorithmic and AI-based obfuscation attacks, demonstrates a significant improvement in detecting plagiarized code.</article>","contentLength":868,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GAPMAP: Mapping Scientific Knowledge Gaps in Biomedical Literature Using Large Language Models","url":"https://arxiv.org/abs/2510.25055","date":1761796800,"author":"","guid":321780,"unread":true,"content":"<article>arXiv:2510.25055v1 Announce Type: new \nAbstract: Scientific progress is driven by the deliberate articulation of what remains unknown. This study investigates the ability of large language models (LLMs) to identify research knowledge gaps in the biomedical literature. We define two categories of knowledge gaps: explicit gaps, clear declarations of missing knowledge; and implicit gaps, context-inferred missing knowledge. While prior work has focused mainly on explicit gap detection, we extend this line of research by addressing the novel task of inferring implicit gaps. We conducted two experiments on almost 1500 documents across four datasets, including a manually annotated corpus of biomedical articles. We benchmarked both closed-weight models (from OpenAI) and open-weight models (Llama and Gemma 2) under paragraph-level and full-paper settings. To address the reasoning of implicit gaps inference, we introduce \\textbf{\\small TABI}, a Toulmin-Abductive Bucketed Inference scheme that structures reasoning and buckets inferred conclusion candidates for validation. Our results highlight the robust capability of LLMs in identifying both explicit and implicit knowledge gaps. This is true for both open- and closed-weight models, with larger variants often performing better. This suggests a strong ability of LLMs for systematically identifying candidate knowledge gaps, which can support early-stage research formulation, policymakers, and funding decisions. We also report observed failure modes and outline directions for robust deployment, including domain adaptation, human-in-the-loop verification, and benchmarking across open- and closed-weight models.</article>","contentLength":1673,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech","url":"https://arxiv.org/abs/2510.25054","date":1761796800,"author":"","guid":321781,"unread":true,"content":"<article>arXiv:2510.25054v1 Announce Type: new \nAbstract: Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these models' generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community.</article>","contentLength":1104,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scalable predictive processing framework for multitask caregiving robots","url":"https://arxiv.org/abs/2510.25053","date":1761796800,"author":"","guid":321782,"unread":true,"content":"<article>arXiv:2510.25053v1 Announce Type: new \nAbstract: The rapid aging of societies is intensifying demand for autonomous care robots; however, most existing systems are task-specific and rely on handcrafted preprocessing, limiting their ability to generalize across diverse scenarios. A prevailing theory in cognitive neuroscience proposes that the human brain operates through hierarchical predictive processing, which underlies flexible cognition and behavior by integrating multimodal sensory signals. Inspired by this principle, we introduce a hierarchical multimodal recurrent neural network grounded in predictive processing under the free-energy principle, capable of directly integrating over 30,000-dimensional visuo-proprioceptive inputs without dimensionality reduction. The model was able to learn two representative caregiving tasks, rigid-body repositioning and flexible-towel wiping, without task-specific feature engineering. We demonstrate three key properties: (i) self-organization of hierarchical latent dynamics that regulate task transitions, capture variability in uncertainty, and infer occluded states; (ii) robustness to degraded vision through visuo-proprioceptive integration; and (iii) asymmetric interference in multitask learning, where the more variable wiping task had little influence on repositioning, whereas learning the repositioning task led to a modest reduction in wiping performance, while the model maintained overall robustness. Although the evaluation was limited to simulation, these results establish predictive processing as a universal and scalable computational principle, pointing toward robust, flexible, and autonomous caregiving robots while offering theoretical insight into the human brain's ability to achieve flexible adaptation in uncertain real-world environments.</article>","contentLength":1819,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Breast Cancer VLMs: Clinically Practical Vision-Language Train-Inference Models","url":"https://arxiv.org/abs/2510.25051","date":1761796800,"author":"","guid":321783,"unread":true,"content":"<article>arXiv:2510.25051v1 Announce Type: new \nAbstract: Breast cancer remains the most commonly diagnosed malignancy among women in the developed world. Early detection through mammography screening plays a pivotal role in reducing mortality rates. While computer-aided diagnosis (CAD) systems have shown promise in assisting radiologists, existing approaches face critical limitations in clinical deployment - particularly in handling the nuanced interpretation of multi-modal data and feasibility due to the requirement of prior clinical history. This study introduces a novel framework that synergistically combines visual features from 2D mammograms with structured textual descriptors derived from easily accessible clinical metadata and synthesized radiological reports through innovative tokenization modules. Our proposed methods in this study demonstrate that strategic integration of convolutional neural networks (ConvNets) with language representations achieves superior performance to vision transformer-based models while handling high-resolution images and enabling practical deployment across diverse populations. By evaluating it on multi-national cohort screening mammograms, our multi-modal approach achieves superior performance in cancer detection and calcification identification compared to unimodal baselines, with particular improvements. The proposed method establishes a new paradigm for developing clinically viable VLM-based CAD systems that effectively leverage imaging data and contextual patient information through effective fusion mechanisms.</article>","contentLength":1569,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Merit Network Telescope: Processing and Initial Insights from Nearly 20 Years of Darknet Traffic for Cybersecurity Research","url":"https://arxiv.org/abs/2510.25050","date":1761796800,"author":"","guid":321784,"unread":true,"content":"<article>arXiv:2510.25050v1 Announce Type: new \nAbstract: This paper presents an initial longitudinal analysis of unsolicited Internet traffic collected between 2005 and 2025 by one of the largest and most persistent network telescopes in the United States, operated by Merit Network. The dataset provides a unique view into global threat activity as observed through scanning and backscatter traffic, key indicators of large-scale probing behavior, data outages, and ongoing denial-of-service (DoS) campaigns. To process this extensive archive, coarse-to-fine methodology is adopted in which general insights are first extracted through a resource-efficient metadata sub-pipeline, followed by a more detailed packet header sub-pipeline for finer-grained analysis. The methodology establishes two sub-pipelines to enable scalable processing of nearly two decades of telescope data and supports multi-level exploration of traffic dynamics. Initial insights highlight long-term trends and recurring traffic spikes, some attributable to Internet-wide scanning events and others likely linked to DoS activities.We present general observations spanning 2006-2024, with a focused analysis of traffic characteristics during 2024.</article>","contentLength":1213,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Teaching Probabilistic Machine Learning in the Liberal Arts: Empowering Socially and Mathematically Informed AI Discourse","url":"https://arxiv.org/abs/2510.25049","date":1761796800,"author":"","guid":321785,"unread":true,"content":"<article>arXiv:2510.25049v1 Announce Type: new \nAbstract: We present a new undergraduate ML course at our institution, a small liberal arts college serving students minoritized in STEM, designed to empower students to critically connect the mathematical foundations of ML with its sociotechnical implications. We propose a \"framework-focused\" approach, teaching students the language and formalism of probabilistic modeling while leveraging probabilistic programming to lower mathematical barriers. We introduce methodological concepts through a whimsical, yet realistic theme, the \"Intergalactic Hypothetical Hospital,\" to make the content both relevant and accessible. Finally, we pair each technical innovation with counter-narratives that challenge its value using real, open-ended case-studies to cultivate dialectical thinking. By encouraging creativity in modeling and highlighting unresolved ethical challenges, we help students recognize the value and need of their unique perspectives, empowering them to participate confidently in AI discourse as technologists and critical citizens.</article>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hedgegraph Polymatroids","url":"https://arxiv.org/abs/2510.25043","date":1761796800,"author":"","guid":321786,"unread":true,"content":"<article>arXiv:2510.25043v1 Announce Type: new \nAbstract: Graphs and hypergraphs combine expressive modeling power with algorithmic efficiency for a wide range of applications. Hedgegraphs generalize hypergraphs further by grouping hyperedges under a color/hedge. This allows hedgegraphs to model dependencies between hyperedges and leads to several applications. However, it poses algorithmic challenges. In particular, the cut function is not submodular, which has been a barrier to algorithms for connectivity. In this work, we introduce two alternative partition-based measures of connectivity in hedgegraphs and study their structural and algorithmic aspects. Instead of the cut function, we investigate a polymatroid associated with hedgegraphs. The polymatroidal lens leads to new tractability results as well as insightful generalizations of classical results on graphs and hypergraphs.</article>","contentLength":885,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training","url":"https://arxiv.org/abs/2510.25042","date":1761796800,"author":"","guid":321787,"unread":true,"content":"<article>arXiv:2510.25042v1 Announce Type: new \nAbstract: Within the current sphere of deep learning research, despite the extensive application of optimization algorithms such as Stochastic Gradient Descent (SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced inadequacy in their capability to address fluctuations in learning efficiency, meet the demands of complex models, and tackle non-convex optimization issues. These challenges primarily arise from the algorithms' limitations in handling complex data structures and models, for instance, difficulties in selecting an appropriate learning rate, avoiding local optima, and navigating through high-dimensional spaces. To address these issues, this paper introduces a novel optimization algorithm named DWMGrad. This algorithm, building on the foundations of traditional methods, incorporates a dynamic guidance mechanism reliant on historical data to dynamically update momentum and learning rates. This allows the optimizer to flexibly adjust its reliance on historical information, adapting to various training scenarios. This strategy not only enables the optimizer to better adapt to changing environments and task complexities but also, as validated through extensive experimentation, demonstrates DWMGrad's ability to achieve faster convergence rates and higher accuracies under a multitude of scenarios.</article>","contentLength":1376,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cryogenic Characterization of Ferroelectric Non-volatile Capacitors","url":"https://arxiv.org/abs/2510.25040","date":1761796800,"author":"","guid":321788,"unread":true,"content":"<article>arXiv:2510.25040v1 Announce Type: new \nAbstract: Ferroelectric-based capacitive crossbar arrays have been proposed for energy-efficient in-memory computing in the charge domain. They combat the challenges like sneak paths and high static power faced by resistive crossbar arrays but are susceptible to thermal noise limiting the effective number of bits (ENOB) for the weighted sum. A direct way to reduce this thermal noise is by lowering the temperature as thermal noise is proportional to temperature. In this work, we first characterize the non-volatile capacitors (nvCaps) on a foundry 28 nm platform at cryogenic temperatures to evaluate the memory window, ON state retention as a function of temperature down to 77K, and then use the calibrated device models to simulate the capacitive crossbar arrays in SPICE at lower temperatures to demonstrate higher ENOB (~5 bits) for 128x128 multiple-and-accumulate (MAC) operations.</article>","contentLength":930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automating Benchmark Design","url":"https://arxiv.org/abs/2510.25039","date":1761796800,"author":"","guid":321789,"unread":true,"content":"<article>arXiv:2510.25039v1 Announce Type: new \nAbstract: The rapid progress and widespread deployment of LLMs and LLM-powered agents has outpaced our ability to evaluate them. Hand-crafted, static benchmarks are the primary tool for assessing model capabilities, but these quickly become saturated. In contrast, dynamic benchmarks evolve alongside the models they evaluate, but are expensive to create and continuously update. To address these challenges, we develop BeTaL (Benchmark Tuning with an LLM-in-the-loop), a framework that leverages environment design principles to automate the process of dynamic benchmark design. BeTaL works by parameterizing key design choices in base benchmark templates and uses LLMs to reason through the resulting parameter space to obtain target properties (such as difficulty and realism) in a cost-efficient manner. We validate this approach on its ability to create benchmarks with desired difficulty levels. Using BeTaL, we create two new benchmarks and extend a popular agentic benchmark $\\tau$-bench. Extensive evaluation on these three tasks and multiple target difficulty levels shows that BeTaL produces benchmarks much closer to the desired difficulty, with average deviations ranging from 5.3% to 13.2% -- a 2-4x improvement over the baselines.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Black Box Variational Inference Scheme for Inverse Problems with Demanding Physics-Based Models","url":"https://arxiv.org/abs/2510.25038","date":1761796800,"author":"","guid":321790,"unread":true,"content":"<article>arXiv:2510.25038v1 Announce Type: new \nAbstract: Bayesian methods are particularly effective for addressing inverse problems due to their ability to manage uncertainties inherent in the inference process. However, employing these methods with costly forward models poses significant challenges, especially in the context of non-differentiable models, where the absence of likelihood model gradient information can result in high computational costs. To tackle this issue, we develop a novel Bayesian inference approach based on black box variational inference, utilizing importance sampling to reuse existing simulation model calls in the variational objective gradient estimation, without relying on forward model gradients. The novelty lies in a new batch-sequential sampling procedure, which only requires new model evaluations if the currently available model evaluations fail to yield a suitable approximation of the objective gradient. The resulting approach reduces computational costs by leading to variational parameter updates without requiring new model evaluations when possible, while adaptively increasing the number of model calls per iteration as needed. In combination with its black box nature, this new approach is suitable for inverse problems involving demanding physics-based models that lack model gradients. We demonstrate the efficiency gains of the proposed method compared to its baseline version, sequential Monte Carlo, and Markov-Chain Monte Carlo in diverse benchmarks, ranging from density matching to the Bayesian calibration of a nonlinear electro-chemo-mechanical model for solid-state batteries.</article>","contentLength":1631,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Distance Based on Cause-Effect Estimands with Latents","url":"https://arxiv.org/abs/2510.25037","date":1761796800,"author":"","guid":321791,"unread":true,"content":"<article>arXiv:2510.25037v1 Announce Type: new \nAbstract: Causal discovery aims to recover graphs that represent causal relations among given variables from observations, and new methods are constantly being proposed. Increasingly, the community raises questions about how much progress is made, because properly evaluating discovered graphs remains notoriously difficult, particularly under latent confounding. We propose a graph distance measure for acyclic directed mixed graphs (ADMGs) based on the downstream task of cause-effect estimation under unobserved confounding. Our approach uses identification via fixing and a symbolic verifier to quantify how graph differences distort cause-effect estimands for different treatment-outcome pairs. We analyze the behavior of the measure under different graph perturbations and compare it against existing distance metrics.</article>","contentLength":863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cluster Formation in Diffusive Systems","url":"https://arxiv.org/abs/2510.25034","date":1761796800,"author":"","guid":321792,"unread":true,"content":"<article>arXiv:2510.25034v1 Announce Type: new \nAbstract: In this paper, we study the formation of clusters for stochastic interacting particle systems (SIPS) that interact through short-range attractive potentials in a periodic domain. We consider kinetic (underdamped) Langevin dynamics and focus on the low-friction regime. Employing a linear stability analysis for the kinetic McKean-Vlasov equation, we show that, at sufficiently low temperatures, and for sufficiently short-ranged interactions, the particles form clusters that correspond to metastable states of the mean-field dynamics. We derive the friction and particle-count dependent cluster-formation time and numerically measure the friction-dependent times to reach a stationary state (given by a state in which all particles are bound in a single cluster). By providing both theory and numerical methods in the inertial stochastic setting, this work acts as a bridge between cluster formation studies in overdamped Langevin dynamics and the Hamiltonian (microcanonical) limit.</article>","contentLength":1033,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8","url":"https://arxiv.org/abs/2510.25032","date":1761796800,"author":"","guid":321793,"unread":true,"content":"<article>arXiv:2510.25032v1 Announce Type: new \nAbstract: Developing a highly accurate automatic license plate recognition system (ALPR) is challenging due to environmental factors such as lighting, rain, and dust. Additional difficulties include high vehicle speeds, varying camera angles, and low-quality or low-resolution images. ALPR is vital in traffic control, parking, vehicle tracking, toll collection, and law enforcement applications. This paper proposes a deep learning strategy using YOLOv8 for license plate detection and recognition tasks. This method seeks to enhance the performance of the model using datasets from Ontario, Quebec, California, and New York State. It achieved an impressive recall rate of 94% on the dataset from the Center for Pattern Recognition and Machine Intelligence (CENPARMI) and 91% on the UFPR-ALPR dataset. In addition, our method follows a semi-supervised learning framework, combining a small set of manually labeled data with pseudo-labels generated by Grounding DINO to train our detection model. Grounding DINO, a powerful vision-language model, automatically annotates many images with bounding boxes for license plates, thereby minimizing the reliance on labor-intensive manual labeling. By integrating human-verified and model-generated annotations, we can scale our dataset efficiently while maintaining label quality, which significantly enhances the training process and overall model performance. Furthermore, it reports character error rates for both datasets, providing additional insight into system performance.</article>","contentLength":1562,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios","url":"https://arxiv.org/abs/2510.25026","date":1761796800,"author":"","guid":321794,"unread":true,"content":"<article>arXiv:2510.25026v1 Announce Type: new \nAbstract: Radiomics-based machine learning models show promise for clinical decision support but are vulnerable to distribution shifts caused by variations in imaging protocols, positioning, and segmentation. This study systematically investigates the robustness of radiomics-based machine learning models under distribution shifts across five MRI sequences. We evaluated how different acquisition protocols and segmentation strategies affect model reliability in terms of predictive power and uncertainty-awareness. Using a phantom of 16 fruits, we evaluated distribution shifts through: (1) protocol variations across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2) segmentation variations (full, partial, rotated); and (3) inter-observer variability. We trained XGBoost classifiers on 8 consistent robust features versus sequence-specific features, testing model performance under in-domain and out-of-domain conditions. Results demonstrate that models trained on protocol-invariant features maintain F1-scores &gt;0.85 across distribution shifts, while models using all features showed 40% performance degradation under protocol changes. Dataset augmentation substantially improved the quality of uncertainty estimates and reduced the expected calibration error (ECE) by 35% without sacrificing accuracy. Temperature scaling provided minimal calibration benefits, confirming XGBoost's inherent reliability. Our findings reveal that protocol-aware feature selection and controlled phantom studies effectively predict model behavior under distribution shifts, providing a framework for developing robust radiomics models resilient to real-world protocol variations.</article>","contentLength":1717,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Secure Retrieval-Augmented Generation against Poisoning Attacks","url":"https://arxiv.org/abs/2510.25025","date":1761796800,"author":"","guid":321795,"unread":true,"content":"<article>arXiv:2510.25025v1 Announce Type: new \nAbstract: Large language models (LLMs) have transformed natural language processing (NLP), enabling applications from content generation to decision support. Retrieval-Augmented Generation (RAG) improves LLMs by incorporating external knowledge but also introduces security risks, particularly from data poisoning, where the attacker injects poisoned texts into the knowledge database to manipulate system outputs. While various defenses have been proposed, they often struggle against advanced attacks. To address this, we introduce RAGuard, a detection framework designed to identify poisoned texts. RAGuard first expands the retrieval scope to increase the proportion of clean texts, reducing the likelihood of retrieving poisoned content. It then applies chunk-wise perplexity filtering to detect abnormal variations and text similarity filtering to flag highly similar texts. This non-parametric approach enhances RAG security, and experiments on large-scale datasets demonstrate its effectiveness in detecting and mitigating poisoning attacks, including strong adaptive attacks.</article>","contentLength":1123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation","url":"https://arxiv.org/abs/2510.25023","date":1761796800,"author":"","guid":321796,"unread":true,"content":"<article>arXiv:2510.25023v1 Announce Type: new \nAbstract: Disentangling shared network-level dynamics from region-specific activity is a central challenge in modeling multi-region neural data. We introduce SPIRE (Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that factorizes recordings into shared and private latent subspaces with novel alignment and disentanglement losses. Trained solely on baseline data, SPIRE robustly recovers cross-regional structure and reveals how external perturbations reorganize it. On synthetic benchmarks with ground-truth latents, SPIRE outperforms classical probabilistic models under nonlinear distortions and temporal misalignments. Applied to intracranial deep brain stimulation (DBS) recordings, SPIRE shows that shared latents reliably encode stimulation-specific signatures that generalize across sites and frequencies. These results establish SPIRE as a practical, reproducible tool for analyzing multi-region neural dynamics under stimulation.</article>","contentLength":1002,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StorageXTuner: An LLM Agent-Driven Automatic Tuning Framework for Heterogeneous Storage Systems","url":"https://arxiv.org/abs/2510.25017","date":1761796800,"author":"","guid":321797,"unread":true,"content":"<article>arXiv:2510.25017v1 Announce Type: new \nAbstract: Automatically configuring storage systems is hard: parameter spaces are large and conditions vary across workloads, deployments, and versions. Heuristic and ML tuners are often system specific, require manual glue, and degrade under changes. Recent LLM-based approaches help but usually treat tuning as a single-shot, system-specific task, which limits cross-system reuse, constrains exploration, and weakens validation. We present StorageXTuner, an LLM agent-driven auto-tuning framework for heterogeneous storage engines. StorageXTuner separates concerns across four agents - Executor (sandboxed benchmarking), Extractor (performance digest), Searcher (insight-guided configuration exploration), and Reflector (insight generation and management). The design couples an insight-driven tree search with layered memory that promotes empirically validated insights and employs lightweight checkers to guard against unsafe actions. We implement a prototype and evaluate it on RocksDB, LevelDB, CacheLib, and MySQL InnoDB with YCSB, MixGraph, and TPC-H/C. Relative to out-of-the-box settings and to ELMo-Tune, StorageXTuner reaches up to 575% and 111% higher throughput, reduces p99 latency by as much as 88% and 56%, and converges with fewer trials.</article>","contentLength":1295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Human-AI Synergy in Requirements Engineering: A Framework and Preliminary Study","url":"https://arxiv.org/abs/2510.25016","date":1761796800,"author":"","guid":321798,"unread":true,"content":"<article>arXiv:2510.25016v1 Announce Type: new \nAbstract: The future of Requirements Engineering (RE) is increasingly driven by artificial intelligence (AI), reshaping how we elicit, analyze, and validate requirements. Traditional RE is based on labor-intensive manual processes prone to errors and complexity. AI-powered approaches, specifically large language models (LLMs), natural language processing (NLP), and generative AI, offer transformative solutions and reduce inefficiencies. However, the use of AI in RE also brings challenges like algorithmic bias, lack of explainability, and ethical concerns related to automation. To address these issues, this study introduces the Human-AI RE Synergy Model (HARE-SM), a conceptual framework that integrates AI-driven analysis with human oversight to improve requirements elicitation, analysis, and validation. The model emphasizes ethical AI use through transparency, explainability, and bias mitigation. We outline a multi-phase research methodology focused on preparing RE datasets, fine-tuning AI models, and designing collaborative human-AI workflows. This preliminary study presents the conceptual framework and early-stage prototype implementation, establishing a research agenda and practical design direction for applying intelligent data science techniques to semi-structured and unstructured RE data in collaborative environments.</article>","contentLength":1383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VeriStruct: AI-assisted Automated Verification of Data-Structure Modules in Verus","url":"https://arxiv.org/abs/2510.25015","date":1761796800,"author":"","guid":321799,"unread":true,"content":"<article>arXiv:2510.25015v1 Announce Type: new \nAbstract: We introduce VeriStruct, a novel framework that extends AI-assisted automated verification from single functions to more complex data structure modules in Verus. VeriStruct employs a planner module to orchestrate the systematic generation of abstractions, type invariants, specifications, and proof code. To address the challenge that LLMs often misunderstand Verus' annotation syntax and verification-specific semantics, VeriStruct embeds syntax guidance within prompts and includes a repair stage to automatically correct annotation errors. In an evaluation on eleven Rust data structure modules, VeriStruct succeeds on ten of the eleven, successfully verifying 128 out of 129 functions (99.2%) in total. These results represent an important step toward the goal of automatic AI-assisted formal verification.</article>","contentLength":859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading","url":"https://arxiv.org/abs/2510.25014","date":1761796800,"author":"","guid":321800,"unread":true,"content":"<article>arXiv:2510.25014v1 Announce Type: new \nAbstract: Large Language Models (LLMs) enable dynamic game interactions but fail to follow essential procedural flows in rule-governed trading systems, eroding player trust. This work resolves the core tension between the creative flexibility of LLMs and the procedural demands of in-game trading (browse-offer-review-confirm). To this end, Autoregressive State-Tracking Prompting (ASTP) is introduced, a methodology centered on a strategically orchestrated prompt that compels an LLM to make its state-tracking process explicit and verifiable. Instead of relying on implicit contextual understanding, ASTP tasks the LLM with identifying and reporting a predefined state label from the previous turn. To ensure transactional integrity, this is complemented by a state-specific placeholder post-processing method for accurate price calculations. Evaluation across 300 trading dialogues demonstrates &gt;99% state compliance and 99.3% calculation precision. Notably, ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash) matches larger models' (Gemini-2.5-Pro) performance while reducing response time from 21.2s to 2.4s, establishing a practical foundation that satisfies both real-time requirements and resource constraints of commercial games.</article>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Emergence of Minimal Circuits for Indirect Object Identification in Attention-Only Transformers","url":"https://arxiv.org/abs/2510.25013","date":1761796800,"author":"","guid":321801,"unread":true,"content":"<article>arXiv:2510.25013v1 Announce Type: new \nAbstract: Mechanistic interpretability aims to reverse-engineer large language models (LLMs) into human-understandable computational circuits. However, the complexity of pretrained models often obscures the minimal mechanisms required for specific reasoning tasks. In this work, we train small, attention-only transformers from scratch on a symbolic version of the Indirect Object Identification (IOI) task -- a benchmark for studying coreference -- like reasoning in transformers. Surprisingly, a single-layer model with only two attention heads achieves perfect IOI accuracy, despite lacking MLPs and normalization layers. Through residual stream decomposition, spectral analysis, and embedding interventions, we find that the two heads specialize into additive and contrastive subcircuits that jointly implement IOI resolution. Furthermore, we show that a two-layer, one-head model achieves similar performance by composing information across layers through query-value interactions. These results demonstrate that task-specific training induces highly interpretable, minimal circuits, offering a controlled testbed for probing the computational foundations of transformer reasoning.</article>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Taming the Real-world Complexities in CPT E/M Coding with Large Language Models","url":"https://arxiv.org/abs/2510.25007","date":1761796800,"author":"","guid":321802,"unread":true,"content":"<article>arXiv:2510.25007v1 Announce Type: new \nAbstract: Evaluation and Management (E/M) coding, under the Current Procedural Terminology (CPT) taxonomy, documents medical services provided to patients by physicians. Used primarily for billing purposes, it is in physicians' best interest to provide accurate CPT E/M codes. %While important, it is an auxiliary task that adds to physicians' documentation burden. Automating this coding task will help alleviate physicians' documentation burden, improve billing efficiency, and ultimately enable better patient care. However, a number of real-world complexities have made E/M encoding automation a challenging task. In this paper, we elaborate some of the key complexities and present ProFees, our LLM-based framework that tackles them, followed by a systematic evaluation. On an expert-curated real-world dataset, ProFees achieves an increase in coding accuracy of more than 36\\% over a commercial CPT E/M coding system and almost 5\\% over our strongest single-prompt baseline, demonstrating its effectiveness in addressing the real-world complexities.</article>","contentLength":1094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cyclic Counterfactuals under Shift-Scale Interventions","url":"https://arxiv.org/abs/2510.25005","date":1761796800,"author":"","guid":321803,"unread":true,"content":"<article>arXiv:2510.25005v1 Announce Type: new \nAbstract: Most counterfactual inference frameworks traditionally assume acyclic structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However, many real-world systems (e.g. biological systems) contain feedback loops or cyclic dependencies that violate acyclicity. In this work, we study counterfactual inference in cyclic SCMs under shift-scale interventions, i.e., soft, policy-style changes that rescale and/or shift a variable's mechanism.</article>","contentLength":497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Emergent Coordinated Behaviors in Networked LLM Agents: Modeling the Strategic Dynamics of Information Operations","url":"https://arxiv.org/abs/2510.25003","date":1761796800,"author":"","guid":321804,"unread":true,"content":"<article>arXiv:2510.25003v1 Announce Type: new \nAbstract: Generative agents are rapidly advancing in sophistication, raising urgent questions about how they might coordinate when deployed in online ecosystems. This is particularly consequential in information operations (IOs), influence campaigns that aim to manipulate public opinion on social media. While traditional IOs have been orchestrated by human operators and relied on manually crafted tactics, agentic AI promises to make campaigns more automated, adaptive, and difficult to detect. This work presents the first systematic study of emergent coordination among generative agents in simulated IO campaigns. Using generative agent-based modeling, we instantiate IO and organic agents in a simulated environment and evaluate coordination across operational regimes, from simple goal alignment to team knowledge and collective decision-making. As operational regimes become more structured, IO networks become denser and more clustered, interactions more reciprocal and positive, narratives more homogeneous, amplification more synchronized, and hashtag adoption faster and more sustained. Remarkably, simply revealing to agents which other agents share their goals can produce coordination levels nearly equivalent to those achieved through explicit deliberation and collective voting. Overall, we show that generative agents, even without human guidance, can reproduce coordination strategies characteristic of real-world IOs, underscoring the societal risks posed by increasingly automated, self-organizing IOs.</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Resi-VidTok: An Efficient and Decomposed Progressive Tokenization Framework for Ultra-Low-Rate and Lightweight Video Transmission","url":"https://arxiv.org/abs/2510.25002","date":1761796800,"author":"","guid":321805,"unread":true,"content":"<article>arXiv:2510.25002v1 Announce Type: new \nAbstract: Real-time transmission of video over wireless networks remains highly challenging, even with advanced deep models, particularly under severe channel conditions such as limited bandwidth and weak connectivity. In this paper, we propose Resi-VidTok, a Resilient Tokenization-Enabled framework designed for ultra-low-rate and lightweight video transmission that delivers strong robustness while preserving perceptual and semantic fidelity on commodity digital hardware. By reorganizing spatio--temporal content into a discrete, importance-ordered token stream composed of key tokens and refinement tokens, Resi-VidTok enables progressive encoding, prefix-decodable reconstruction, and graceful quality degradation under constrained channels. A key contribution is a resilient 1D tokenization pipeline for video that integrates differential temporal token coding, explicitly supporting reliable recovery from incomplete token sets using a single shared framewise decoder--without auxiliary temporal extractors or heavy generative models. Furthermore, stride-controlled frame sparsification combined with a lightweight decoder-side interpolator reduces transmission load while maintaining motion continuity. Finally, a channel-adaptive source--channel coding and modulation scheme dynamically allocates rate and protection according to token importance and channel condition, yielding stable quality across adverse SNRs. Evaluation results indicate robust visual and semantic consistency at channel bandwidth ratios (CBR) as low as 0.0004 and real-time reconstruction at over 30 fps, demonstrating the practicality of Resi-VidTok for energy-efficient, latency-sensitive, and reliability-critical wireless applications.</article>","contentLength":1762,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Really Matters in Matrix-Whitening Optimizers?","url":"https://arxiv.org/abs/2510.25000","date":1761796800,"author":"","guid":321806,"unread":true,"content":"<article>arXiv:2510.25000v1 Announce Type: new \nAbstract: A range of recent optimizers have emerged that approximate the same \"matrix-whitening\" transformation in various ways. In this work, we systematically deconstruct such optimizers, aiming to disentangle the key components that explain performance. Across tuned hyperparameters across the board, all flavors of matrix-whitening methods reliably outperform elementwise counterparts, such as Adam. Matrix-whitening is often related to spectral descent -- however, experiments reveal that performance gains are *not explained solely by accurate spectral normalization* -- particularly, SOAP displays the largest per-step gain, even though Muon more accurately descends along the steepest spectral descent direction. Instead, we argue that matrix-whitening serves two purposes, and the variance adaptation component of matrix-whitening is the overlooked ingredient explaining this performance gap. Experiments show that variance-adapted versions of optimizers consistently outperform their sign-descent counterparts, including an adaptive version of Muon. We further ablate variance adaptation strategies, finding that while lookahead style approximations are not as effective, low-rank variance estimators can effectively reduce memory costs without a performance loss.</article>","contentLength":1313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SLIP-SEC: Formalizing Secure Protocols for Model IP Protection","url":"https://arxiv.org/abs/2510.24999","date":1761796800,"author":"","guid":321807,"unread":true,"content":"<article>arXiv:2510.24999v1 Announce Type: new \nAbstract: Large Language Models (LLMs) represent valuable intellectual property (IP), reflecting significant investments in training data, compute, and expertise. Deploying these models on partially trusted or insecure devices introduces substantial risk of model theft, making it essential to design inference protocols with provable security guarantees.\n  We present the formal framework and security foundations of SLIP, a hybrid inference protocol that splits model computation between a trusted and an untrusted resource. We define and analyze the key notions of model decomposition and hybrid inference protocols, and introduce formal properties including safety, correctness, efficiency, and t-soundness. We construct secure inference protocols based on additive decompositions of weight matrices, combined with masking and probabilistic verification techniques. We prove that these protocols achieve information-theoretic security against honest-but-curious adversaries, and provide robustness against malicious adversaries with negligible soundness error.\n  This paper focuses on the theoretical underpinnings of SLIP: precise definitions, formal protocols, and proofs of security. Empirical validation and decomposition heuristics appear in the companion SLIP paper. Together, the two works provide a complete account of securing LLM IP via hybrid inference, bridging both practice and theory.</article>","contentLength":1442,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Defect Mitigation for Robot Arm-based Additive Manufacturing Utilizing Intelligent Control and IOT","url":"https://arxiv.org/abs/2510.24994","date":1761796800,"author":"","guid":321808,"unread":true,"content":"<article>arXiv:2510.24994v1 Announce Type: new \nAbstract: This paper presents an integrated robotic fused deposition modeling additive manufacturing system featuring closed-loop thermal control and intelligent in-situ defect correction using a 6-degree of freedom robotic arm and an Oak-D camera. The robot arm end effector was modified to mount an E3D hotend thermally regulated by an IoT microcontroller, enabling precise temperature control through real-time feedback. Filament extrusion system was synchronized with robotic motion, coordinated via ROS2, ensuring consistent deposition along complex trajectories. A vision system based on OpenCV detects layer-wise defects position, commanding autonomous re-extrusion at identified sites. Experimental validation demonstrated successful defect mitigation in printing operations. The integrated system effectively addresses challenges real-time quality assurance. Inverse kinematics were used for motion planning, while homography transformations corrected camera perspectives for accurate defect localization. The intelligent system successfully mitigated surface anomalies without interrupting the print process. By combining real-time thermal regulation, motion control, and intelligent defect detection &amp; correction, this architecture establishes a scalable and adaptive robotic additive manufacturing framework suitable for aerospace, biomedical, and industrial applications.</article>","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Kleene Algebrae, Kleene Modules, and Morita Equivalence","url":"https://arxiv.org/abs/2510.24993","date":1761796800,"author":"","guid":321809,"unread":true,"content":"<article>arXiv:2510.24993v1 Announce Type: new \nAbstract: Modules and the notion of Morita equivalence are foundational to the classical study of rings. These concepts extend naturally to semirings and then specialize to Kleene algebrae, and my goal is to investigate Kleene modules and Morita equivalence of Kleene algebrae in the hope that some of the power seen in the context of rings may be found in this new context as well.</article>","contentLength":421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"POWSM: A Phonetic Open Whisper-Style Speech Foundation Model","url":"https://arxiv.org/abs/2510.24992","date":1761796800,"author":"","guid":321810,"unread":true,"content":"<article>arXiv:2510.24992v1 Announce Type: new \nAbstract: Recent advances in spoken language processing have led to substantial progress in phonetic tasks such as automatic speech recognition (ASR), phone recognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-grapheme conversion (P2G). Despite their conceptual similarity, these tasks have largely been studied in isolation, each relying on task-specific architectures and datasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style Speech Model), the first unified framework capable of jointly performing multiple phone-related tasks. POWSM enables seamless conversion between audio, text (graphemes), and phones, opening up new possibilities for universal and low-resource speech processing. Our model outperforms or matches specialized PR models of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P, P2G, and ASR. Our training data, code and models are released to foster open science.</article>","contentLength":978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Economics of AI Training Data: A Research Agenda","url":"https://arxiv.org/abs/2510.24990","date":1761796800,"author":"","guid":321811,"unread":true,"content":"<article>arXiv:2510.24990v1 Announce Type: new \nAbstract: Despite data's central role in AI production, it remains the least understood input. As AI labs exhaust public data and turn to proprietary sources, with deals reaching hundreds of millions of dollars, research across computer science, economics, law, and policy has fragmented. We establish data economics as a coherent field through three contributions. First, we characterize data's distinctive properties -- nonrivalry, context dependence, and emergent rivalry through contamination -- and trace historical precedents for market formation in commodities such as oil and grain. Second, we present systematic documentation of AI training data deals from 2020 to 2025, revealing persistent market fragmentation, five distinct pricing mechanisms (from per-unit licensing to commissioning), and that most deals exclude original creators from compensation. Third, we propose a formal hierarchy of exchangeable data units (token, record, dataset, corpus, stream) and argue for data's explicit representation in production functions. Building on these foundations, we outline four open research problems foundational to data economics: measuring context-dependent value, balancing governance with privacy, estimating data's contribution to production, and designing mechanisms for heterogeneous, compositional goods.</article>","contentLength":1361,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series","url":"https://arxiv.org/abs/2510.24988","date":1761796800,"author":"","guid":321812,"unread":true,"content":"<article>arXiv:2510.24988v1 Announce Type: new \nAbstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of decision-making in long-horizon tasks by introducing temporal abstraction through options-policies that span multiple timesteps. Despite its theoretical appeal, the practical implementation of HRL suffers from the challenge of autonomously discovering semantically meaningful subgoals and learning optimal option termination boundaries. This paper introduces a novel architecture that integrates a self-supervised, Transformer-based Change Point Detection (CPD) module into the Option-Critic framework, enabling adaptive segmentation of state trajectories and the discovery of options. The CPD module is trained using heuristic pseudo-labels derived from intrinsic signals to infer latent shifts in environment dynamics without external supervision. These inferred change-points are leveraged in three critical ways: (i) to serve as supervisory signals for stabilizing termination function gradients, (ii) to pretrain intra-option policies via segment-wise behavioral cloning, and (iii) to enforce functional specialization through inter-option divergence penalties over CPD-defined state partitions. The overall optimization objective enhances the standard actor-critic loss using structure-aware auxiliary losses. In our framework, option discovery arises naturally as CPD-defined trajectory segments are mapped to distinct intra-option policies, enabling the agent to autonomously partition its behavior into reusable, semantically meaningful skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that CPD-guided agents exhibit accelerated convergence, higher cumulative returns, and significantly improved option specialization. These findings confirm that integrating structural priors via change-point segmentation leads to more interpretable, sample-efficient, and robust hierarchical policies in complex environments.</article>","contentLength":1958,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation","url":"https://arxiv.org/abs/2510.24986","date":1761796800,"author":"","guid":321813,"unread":true,"content":"<article>arXiv:2510.24986v1 Announce Type: new \nAbstract: In recent years, machine learning has become an increasingly powerful tool for supporting seizure detection and monitoring in epilepsy care. Traditional approaches focus on identifying seizures only after they begin, which limits the opportunity for early intervention and proactive treatment. In this study, we propose a novel approach that integrates both real-time seizure detection and prediction, aiming to capture subtle temporal patterns in EEG data that may indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT Scalp EEG Database, which includes 969 hours of recordings and 173 seizures collected from 23 pediatric and young adult patients with drug-resistant epilepsy. To support seizure detection, we implemented a range of supervised machine learning algorithms, including K-Nearest Neighbors, Logistic Regression, Random Forest, and Support Vector Machine. The Logistic Regression achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced performance suitable for clinical screening. Random Forest and Support Vector Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to detect any seizures, illustrating that accuracy alone is insufficient for evaluating medical ML models with class imbalance. For seizure prediction, we employed Long Short-Term Memory (LSTM) networks, which use deep learning to model temporal dependencies in EEG data. The LSTM model achieved 89.26% prediction accuracy. These results highlight the potential of developing accessible, real-time monitoring tools that not only detect seizures as traditionally done, but also predict them before they occur. This ability to predict seizures marks a significant shift from reactive seizure management to a more proactive approach, allowing patients to anticipate seizures and take precautionary measures to reduce the risk of injury or other complications.</article>","contentLength":1950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FaRAccel: FPGA-Accelerated Defense Architecture for Efficient Bit-Flip Attack Resilience in Transformer Models","url":"https://arxiv.org/abs/2510.24985","date":1761796800,"author":"","guid":321814,"unread":true,"content":"<article>arXiv:2510.24985v1 Announce Type: new \nAbstract: Forget and Rewire (FaR) methodology has demonstrated strong resilience against Bit-Flip Attacks (BFAs) on Transformer-based models by obfuscating critical parameters through dynamic rewiring of linear layers. However, the application of FaR introduces non-negligible performance and memory overheads, primarily due to the runtime modification of activation pathways and the lack of hardware-level optimization. To overcome these limitations, we propose FaRAccel, a novel hardware accelerator architecture implemented on FPGA, specifically designed to offload and optimize FaR operations. FaRAccel integrates reconfigurable logic for dynamic activation rerouting, and lightweight storage of rewiring configurations, enabling low-latency inference with minimal energy overhead. We evaluate FaRAccel across a suite of Transformer models and demonstrate substantial reductions in FaR inference latency and improvement in energy efficiency, while maintaining the robustness gains of the original FaR methodology. To the best of our knowledge, this is the first hardware-accelerated defense against BFAs in Transformers, effectively bridging the gap between algorithmic resilience and efficient deployment on real-world AI platforms.</article>","contentLength":1276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The B-spline collocation method for solving Cauchy singular integral equations with piecewise Holder continuous coefficients","url":"https://arxiv.org/abs/2510.24984","date":1761796800,"author":"","guid":321815,"unread":true,"content":"<article>arXiv:2510.24984v1 Announce Type: new \nAbstract: In this paper, we propose a numerical method for approximating the solution of a Cauchy singular integral equation defined on a closed, smooth contour in the complex plane. The coefficients and the right-hand side of the equation are piecewise Holder continuous functions that may have a finite number of jump discontinuities, and are given numerically at a finite set of points on the contour. We introduce an efficient approximation scheme for piecewise Holder continuous functions based on linear combinations of B-spline functions and Heaviside step functions, which serves as the foundation for the proposed collocation algorithm. We then establish the convergence of the sequence of the constructed approximations to the exact solution of the equation in the norm of piecewise Holder spaces and derive estimates for the convergence rate of the method.</article>","contentLength":906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies","url":"https://arxiv.org/abs/2510.24983","date":1761796800,"author":"","guid":321816,"unread":true,"content":"<article>arXiv:2510.24983v1 Announce Type: new \nAbstract: Diffusion policies are competitive for offline reinforcement learning (RL) but are typically guided at sampling time by heuristics that lack a statistical notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that treats each denoising step as a sequential hypothesis test between the unconditional prior and the state-conditional policy head. Concretely, we accumulate a log-likelihood ratio and gate the conditional mean with a logistic controller whose threshold tau is calibrated once under H0 to meet a user-specified Type-I level alpha. This turns guidance from a fixed push into an evidence-driven adjustment with a user-interpretable risk budget. Importantly, we deliberately leave training vanilla (two heads with standard epsilon-prediction) under the structure of DDPM. LRT guidance composes naturally with Q-gradients: critic-gradient updates can be taken at the unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum from exploitation to conservatism. We standardize states and actions consistently at train and test time and report a state-conditional out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks, LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines in our implementation while honoring the desired alpha. Theoretically, we establish level-alpha calibration, concise stability bounds, and a return comparison showing when LRT surpasses Q-guidance-especially when off-support errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method that adds principled, calibrated risk control to diffusion policies for offline RL.</article>","contentLength":1689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strategic inputs: feature selection from game-theoretic perspective","url":"https://arxiv.org/abs/2510.24982","date":1761796800,"author":"","guid":321817,"unread":true,"content":"<article>arXiv:2510.24982v1 Announce Type: new \nAbstract: The exponential growth of data volumes has led to escalating computational costs in machine learning model training. However, many features fail to contribute positively to model performance while consuming substantial computational resources. This paper presents an end-to-end feature selection framework for tabular data based on game theory. We formulate feature selection procedure based on a cooperative game where features are modeled as players, and their importance is determined through the evaluation of synergistic interactions and marginal contributions. The proposed framework comprises four core components: sample selection, game-theoretic feature importance evaluation, redundant feature elimination, and optimized model training. Experimental results demonstrate that the proposed method achieves substantial computation reduction while preserving predictive performance, thereby offering an efficient solution of the computational challenges of large-scale machine learning. The source code is available at https://github.com/vectorsss/strategy_inputs.</article>","contentLength":1119,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FT-ARM: Fine-Tuned Agentic Reflection Multimodal Language Model for Pressure Ulcer Severity Classification with Reasoning","url":"https://arxiv.org/abs/2510.24980","date":1761796800,"author":"","guid":321818,"unread":true,"content":"<article>arXiv:2510.24980v1 Announce Type: new \nAbstract: Pressure ulcers (PUs) are a serious and prevalent healthcare concern. Accurate classification of PU severity (Stages I-IV) is essential for proper treatment but remains challenging due to subtle visual distinctions and subjective interpretation, leading to variability among clinicians. Prior AI-based approaches using Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) achieved promising accuracy but offered limited interpretability. We present FT-ARM (Fine-Tuned Agentic Reflection Multimodal model), a fine-tuned multimodal large language model (MLLM) with an agentic self-reflection mechanism for pressure ulcer severity classification. Inspired by clinician-style diagnostic reassessment, FT-ARM iteratively refines its predictions by reasoning over visual features and encoded clinical knowledge from text, enhancing both accuracy and consistency. On the publicly available Pressure Injury Image Dataset (PIID), FT-ARM, fine-tuned from LLaMA 3.2 90B, achieved 85% accuracy in classifying PU stages I-IV, surpassing prior CNN-based models by +4%. Unlike earlier CNN/ViT studies that relied solely on offline evaluations, FT-ARM is designed and tested for live inference, reflecting real-time deployment conditions. Furthermore, it produces clinically grounded natural-language explanations, improving interpretability and trust. By integrating fine-tuning and reflective reasoning across multimodal inputs, FT-ARM advances the reliability, transparency, and clinical applicability of automated wound assessment systems, addressing the critical need for consistent and explainable PU staging to support improved patient care.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hammering the Diagnosis: Rowhammer-Induced Stealthy Trojan Attacks on ViT-Based Medical Imaging","url":"https://arxiv.org/abs/2510.24976","date":1761796800,"author":"","guid":321819,"unread":true,"content":"<article>arXiv:2510.24976v1 Announce Type: new \nAbstract: Vision Transformers (ViTs) have emerged as powerful architectures in medical image analysis, excelling in tasks such as disease detection, segmentation, and classification. However, their reliance on large, attention-driven models makes them vulnerable to hardware-level attacks. In this paper, we propose a novel threat model referred to as Med-Hammer that combines the Rowhammer hardware fault injection with neural Trojan attacks to compromise the integrity of ViT-based medical imaging systems. Specifically, we demonstrate how malicious bit flips induced via Rowhammer can trigger implanted neural Trojans, leading to targeted misclassification or suppression of critical diagnoses (e.g., tumors or lesions) in medical scans. Through extensive experiments on benchmark medical imaging datasets such as ISIC, Brain Tumor, and MedMNIST, we show that such attacks can remain stealthy while achieving high attack success rates about 82.51% and 92.56% in MobileViT and SwinTransformer, respectively. We further investigate how architectural properties, such as model sparsity, attention weight distribution, and the number of features of the layer, impact attack effectiveness. Our findings highlight a critical and underexplored intersection between hardware-level faults and deep learning security in healthcare applications, underscoring the urgent need for robust defenses spanning both model architectures and underlying hardware platforms.</article>","contentLength":1494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Maximum-Entropy Analog Computing Approaching ExaOPS-per-Watt Energy-efficiency at the RF-Edge","url":"https://arxiv.org/abs/2510.24975","date":1761796800,"author":"","guid":321820,"unread":true,"content":"<article>arXiv:2510.24975v1 Announce Type: new \nAbstract: In this paper, we demonstrate how the physics of entropy production, when combined with symmetry constraints, can be used for implementing high-performance and energy-efficient analog computing systems. At the core of the proposed framework is a generalized maximum-entropy principle that can describe the evolution of a mesoscopic physical system formed by an interconnected ensemble of analog elements, including devices that can be readily fabricated on standard integrated circuit technology. We show that the maximum-entropy state of this ensemble corresponds to a margin-propagation (MP) distribution and can be used for computing correlations and inner products as the ensemble's macroscopic properties. Furthermore, the limits of computational throughput and energy efficiency can be pushed by extending the framework to non-equilibrium or transient operating conditions, which we demonstrate using a proof-of-concept radio-frequency (RF) correlator integrated circuit fabricated in a 22 nm SOI CMOS process. The measured results show a compute efficiency greater than 2 Peta ($10^{15}$) Bit Operations per second per Watt (PetaOPS/W) at 8-bit precision and greater than 0.8 Exa ($10^{18}$) Bit Operations per second per Watt (ExaOPS/W) at 3-bit precision for RF data sampled at rates greater than 4 GS/s. Using the fabricated prototypes, we also showcase several real-world RF applications at the edge, including spectrum sensing, and code-domain communications.</article>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution","url":"https://arxiv.org/abs/2510.24974","date":1761796800,"author":"","guid":321821,"unread":true,"content":"<article>arXiv:2510.24974v1 Announce Type: new \nAbstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for efficiently navigating antibody fitness landscapes. Many structure-aware MLDE pipelines rely on a single conformation or a single committee across all conformations, limiting their ability to separate conformational uncertainty from epistemic uncertainty. Here, we introduce a rank -conditioned committee (RCC) framework that leverages ranked conformations to assign a deep neural network committee per rank. This design enables a principled separation between epistemic uncertainty and conformational uncertainty. We validate our approach on SARS-CoV-2 antibody docking, demonstrating significant improvements over baseline strategies. Our results offer a scalable route for therapeutic antibody discovery while directly addressing the challenge of modeling conformational uncertainty.</article>","contentLength":910,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Smooth path planning with safety margins using Piece-Wise Bezier curves","url":"https://arxiv.org/abs/2510.24972","date":1761796800,"author":"","guid":321822,"unread":true,"content":"<article>arXiv:2510.24972v1 Announce Type: new \nAbstract: In this paper, we propose a computationally efficient quadratic programming (QP) approach for generating smooth, $C^1$ continuous paths for mobile robots using piece-wise quadratic Bezier (PWB) curves. Our method explicitly incorporates safety margins within a structured optimization framework, balancing trajectory smoothness and robustness with manageable numerical complexity suitable for real-time and embedded applications. Comparative simulations demonstrate clear advantages over traditional piece-wise linear (PWL) path planning methods, showing reduced trajectory deviations, enhanced robustness, and improved overall path quality. These benefits are validated through simulations using a Pure-Pursuit controller in representative scenarios, highlighting the practical effectiveness and scalability of our approach for safe navigation.</article>","contentLength":894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sequences of Logits Reveal the Low Rank Structure of Language Models","url":"https://arxiv.org/abs/2510.24966","date":1761796800,"author":"","guid":321823,"unread":true,"content":"<article>arXiv:2510.24966v1 Announce Type: new \nAbstract: A major problem in the study of large language models is to understand their inherent low-dimensional structure. We introduce an approach to study the low-dimensional structure of language models at a model-agnostic level: as sequential probabilistic models. We first empirically demonstrate that a wide range of modern language models exhibit low-rank structure: in particular, matrices built from the model's logits for varying sets of prompts and responses have low approximate rank. We then show that this low-rank structure can be leveraged for generation -- in particular, we can generate a response to a target prompt using a linear combination of the model's outputs on unrelated, or even nonsensical prompts.\n  On the theoretical front, we observe that studying the approximate rank of language models in the sense discussed above yields a simple universal abstraction whose theoretical predictions parallel our experiments. We then analyze the representation power of the abstraction and give provable learning guarantees.</article>","contentLength":1081,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exponential Dynamic Energy Network for High Capacity Sequence Memory","url":"https://arxiv.org/abs/2510.24965","date":1761796800,"author":"","guid":321824,"unread":true,"content":"<article>arXiv:2510.24965v1 Announce Type: new \nAbstract: The energy paradigm, exemplified by Hopfield networks, offers a principled framework for memory in neural systems by interpreting dynamics as descent on an energy surface. While powerful for static associative memories, it falls short in modeling sequential memory, where transitions between memories are essential. We introduce the Exponential Dynamic Energy Network (EDEN), a novel architecture that extends the energy paradigm to temporal domains by evolving the energy function over multiple timescales. EDEN combines a static high-capacity energy network with a slow, asymmetrically interacting modulatory population, enabling robust and controlled memory transitions. We formally derive short-timescale energy functions that govern local dynamics and use them to analytically compute memory escape times, revealing a phase transition between static and dynamic regimes. The analysis of capacity, defined as the number of memories that can be stored with minimal error rate as a function of the dimensions of the state space (number of feature neurons), for EDEN shows that it achieves exponential sequence memory capacity $O(\\gamma^N)$, outperforming the linear capacity $O(N)$ of conventional models. Furthermore, EDEN's dynamics resemble the activity of time and ramping cells observed in the human brain during episodic memory tasks, grounding its biological relevance. By unifying static and sequential memory within a dynamic energy framework, EDEN offers a scalable and interpretable model for high-capacity temporal memory in both artificial and biological systems.</article>","contentLength":1627,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale","url":"https://arxiv.org/abs/2510.24963","date":1761796800,"author":"","guid":321825,"unread":true,"content":"<article>arXiv:2510.24963v1 Announce Type: new \nAbstract: We show that across architecture (Transformer vs. Mamba vs. RWKV), training dataset (OpenWebText vs. The Pile), and scale (14 million parameters to 12 billion parameters), autoregressive language models exhibit highly consistent patterns of change in their behavior over the course of pretraining. Based on our analysis of over 1,400 language model checkpoints on over 110,000 tokens of English, we find that up to 98% of the variance in language model behavior at the word level can be explained by three simple heuristics: the unigram probability (frequency) of a given word, the $n$-gram probability of the word, and the semantic similarity between the word and its context. Furthermore, we see consistent behavioral phases in all language models, with their predicted probabilities for words overfitting to those words' $n$-gram probabilities for increasing $n$ over the course of training. Taken together, these results suggest that learning in neural language models may follow a similar trajectory irrespective of model details.</article>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Data Collection for Latin-American Community-sourced Evaluation of Stereotypes (LACES)","url":"https://arxiv.org/abs/2510.24958","date":1761796800,"author":"","guid":321826,"unread":true,"content":"<article>arXiv:2510.24958v1 Announce Type: new \nAbstract: The evaluation of societal biases in NLP models is critically hindered by a glaring geo-cultural gap, as existing benchmarks are overwhelmingly English-centric and focused on U.S. demographics. This leaves regions such as Latin America severely underserved, making it impossible to adequately assess or mitigate the perpetuation of harmful regional stereotypes by language technologies. To address this gap, we introduce a new, large-scale dataset of stereotypes developed through targeted community partnerships within Latin America. Furthermore, we present a novel dynamic data collection methodology that uniquely integrates the sourcing of new stereotype entries and the validation of existing data within a single, unified workflow. This combined approach results in a resource with significantly broader coverage and higher regional nuance than static collection methods. We believe that this new method could be applicable in gathering sociocultural knowledge of other kinds, and that this dataset provides a crucial new resource enabling robust stereotype evaluation and significantly addressing the geo-cultural deficit in fairness resources for Latin America.</article>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reviving Thorup's Shortcut Conjecture","url":"https://arxiv.org/abs/2510.24954","date":1761796800,"author":"","guid":321827,"unread":true,"content":"<article>arXiv:2510.24954v1 Announce Type: new \nAbstract: We aim to revive Thorup's conjecture [Thorup, WG'92] on the existence of reachability shortcuts with ideal size-diameter tradeoffs. Thorup originally asked whether, given any graph $G=(V,E)$ with $m$ edges, we can add $m^{1+o(1)}$ ``shortcut'' edges $E_+$ from the transitive closure $E^*$ of $G$ so that $\\text{dist}_{G_+}(u,v) \\leq m^{o(1)}$ for all $(u,v)\\in E^*$, where $G_+=(V,E\\cup E_+)$. The conjecture was refuted by Hesse [Hesse, SODA'03], followed by significant efforts in the last few years to optimize the lower bounds.\n  In this paper we observe that although Hesse refuted the letter of Thorup's conjecture, his work~[Hesse, SODA'03] -- and all followup work -- does not refute the spirit of the conjecture, which should allow $G_+$ to contain both new (shortcut) edges and new Steiner vertices. Our results are as follows.\n  (1) On the positive side, we present explicit attacks that break all known shortcut lower bounds when Steiner vertices are allowed.\n  (2) On the negative side, we rule out ideal $m^{1+o(1)}$-size, $m^{o(1)}$-diameter shortcuts whose ``thickness'' is $t=o(\\log n/\\log \\log n)$, meaning no path can contain $t$ consecutive Steiner vertices.\n  (3) We propose a candidate hard instance as the next step toward resolving the revised version of Thorup's conjecture.\n  Finally, we show promising implications. Almost-optimal parallel algorithms for computing a generalization of the shortcut that approximately preserves distances or flows imply almost-optimal parallel algorithms with $m^{o(1)}$ depth for exact shortcut paths and exact maximum flow. The state-of-the-art algorithms have much worse depth of $n^{1/2+o(1)}$ [Rozho\\v{n}, Haeupler, Martinsson, STOC'23] and $m^{1+o(1)}$ [Chen, Kyng, Liu, FOCS'22], respectively.</article>","contentLength":1809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Resource-Efficient and Robust Inference of Deep and Bayesian Neural Networks on Embedded and Analog Computing Platforms","url":"https://arxiv.org/abs/2510.24951","date":1761796800,"author":"","guid":321828,"unread":true,"content":"<article>arXiv:2510.24951v1 Announce Type: new \nAbstract: While modern machine learning has transformed numerous application domains, its growing computational demands increasingly constrain scalability and efficiency, particularly on embedded and resource-limited platforms. In practice, neural networks must not only operate efficiently but also provide reliable predictions under distributional shifts or unseen data. Bayesian neural networks offer a principled framework for quantifying uncertainty, yet their computational overhead further compounds these challenges.\n  This work advances resource-efficient and robust inference for both conventional and Bayesian neural networks through the joint pursuit of algorithmic and hardware efficiency. The former reduces computation through model compression and approximate Bayesian inference, while the latter optimizes deployment on digital accelerators and explores analog hardware, bridging algorithmic design and physical realization. The first contribution, Galen, performs automatic layer-specific compression guided by sensitivity analysis and hardware-in-the-loop feedback. Analog accelerators offer efficiency gains at the cost of noise; this work models device imperfections and extends noisy training to nonstationary conditions, improving robustness and stability. A second line of work advances probabilistic inference, developing analytic and ensemble approximations that replace costly sampling, integrate into a compiler stack, and optimize embedded inference. Finally, probabilistic photonic computing introduces a paradigm where controlled analog noise acts as an intrinsic entropy source, enabling fast, energy-efficient probabilistic inference directly in hardware.\n  Together, these studies demonstrate how efficiency and reliability can be advanced jointly through algorithm-hardware co-design, laying the foundation for the next generation of trustworthy, energy-efficient machine-learning systems.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving","url":"https://arxiv.org/abs/2510.24949","date":1761796800,"author":"","guid":321829,"unread":true,"content":"<article>arXiv:2510.24949v1 Announce Type: new \nAbstract: Assessing scenario coverage is crucial for evaluating the robustness of autonomous agents, yet existing methods rely on expensive human annotations or computationally intensive Large Vision-Language Models (LVLMs). These approaches are impractical for large-scale deployment due to cost and efficiency constraints. To address these shortcomings, we propose SCOUT (Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate model designed to predict scenario coverage labels directly from an agent's latent sensor representations. SCOUT is trained through a distillation process, learning to approximate LVLM-generated coverage labels while eliminating the need for continuous LVLM inference or human annotation. By leveraging precomputed perception features, SCOUT avoids redundant computations and enables fast, scalable scenario coverage estimation. We evaluate our method across a large dataset of real-life autonomous navigation scenarios, demonstrating that it maintains high accuracy while significantly reducing computational cost. Our results show that SCOUT provides an effective and practical alternative for large-scale coverage analysis. While its performance depends on the quality of LVLM-generated training labels, SCOUT represents a major step toward efficient scenario coverage oversight in autonomous systems.</article>","contentLength":1394,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpolated Discrepancy Data Assimilation for PDEs with Sparse Observations","url":"https://arxiv.org/abs/2510.24944","date":1761796800,"author":"","guid":321830,"unread":true,"content":"<article>arXiv:2510.24944v1 Announce Type: new \nAbstract: Sparse sensor networks in weather and ocean modeling observe only a small fraction of the system state, which destabilizes standard nudging-based data assimilation. We introduce Interpolated Discrepancy Data Assimilation (IDDA), which modifies how discrepancies enter the governing equations. Rather than adding observations as a forcing term alone, IDDA also adjusts the nonlinear operator using interpolated observational information. This structural change suppresses error amplification when nonlinear effects dominate. We prove exponential convergence under explicit conditions linking error decay to observation spacing, nudging strength, and diffusion coefficient. The key requirement establishes bounds on nudging strength relative to observation spacing and diffusion, giving practitioners a clear operating window. When observations resolve the relevant scales, error decays at a user-specified rate. Critically, the error bound scales with the square of observation spacing rather than through hard-to-estimate nonlinear growth rates. We validate IDDA on Burgers flow, Kuramoto-Sivashinsky dynamics, and two-dimensional Navier-Stokes turbulence. Across these tests, IDDA reaches target accuracy faster than standard interpolated nudging, remains stable in chaotic regimes, avoids non-monotone transients, and requires minimal parameter tuning. Because IDDA uses standard explicit time integration, it fits readily into existing simulation pipelines without specialized solvers. These properties make IDDA a practical upgrade for operational systems constrained by sparse sensor coverage.</article>","contentLength":1647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Radar DataTree: A FAIR and Cloud-Native Framework for Scalable Weather Radar Archives","url":"https://arxiv.org/abs/2510.24943","date":1761796800,"author":"","guid":321831,"unread":true,"content":"<article>arXiv:2510.24943v1 Announce Type: new \nAbstract: We introduce Radar DataTree, the first dataset-level framework that extends the WMO FM-301 standard from individual radar volume scans to time-resolved, analysis-ready archives. Weather radar data are among the most scientifically valuable yet structurally underutilized Earth observation datasets. Despite widespread public availability, radar archives remain fragmented, vendor-specific, and poorly aligned with FAIR (Findable, Accessible, Interoperable, Reusable) principles, hindering large-scale research, reproducibility, and cloud-native computation. Radar DataTree addresses these limitations with a scalable, open-source architecture that transforms operational radar archives into FAIR-compliant, cloud-optimized datasets. Built on the FM-301/CfRadial 2.1 standard and implemented using xarray DataTree, Radar DataTree organizes radar volume scans as hierarchical, metadata-rich structures and serializes them to Zarr for scalable analysis. Coupled with Icechunk for ACID-compliant storage and versioning, this architecture enables efficient, parallel computation across thousands of radar scans with minimal preprocessing. We demonstrate significant performance gains in case studies including Quasi-Vertical Profile (QVP) and precipitation accumulation workflows, and release all tools and datasets openly via the Raw2Zarr repository. This work contributes a reproducible and extensible foundation for radar data stewardship, high-performance geoscience, and AI-ready weather infrastructure.</article>","contentLength":1552,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding Culture-Sensitive Neurons in Vision-Language Models","url":"https://arxiv.org/abs/2510.24942","date":1761796800,"author":"","guid":321832,"unread":true,"content":"<article>arXiv:2510.24942v1 Announce Type: new \nAbstract: Despite their impressive performance, vision-language models (VLMs) still struggle on culturally situated inputs. To understand how VLMs process culturally grounded information, we study the presence of culture-sensitive neurons, i.e. neurons whose activations show preferential sensitivity to inputs associated with particular cultural contexts. We examine whether such neurons are important for culturally diverse visual question answering and where they are located. Using the CVQA benchmark, we identify neurons of culture selectivity and perform causal tests by deactivating the neurons flagged by different identification methods. Experiments on three VLMs across 25 cultural groups demonstrate the existence of neurons whose ablation disproportionately harms performance on questions about the corresponding cultures, while having minimal effects on others. Moreover, we propose a new margin-based selector - Contrastive Activation Selection (CAS), and show that it outperforms existing probability- and entropy-based methods in identifying culture-sensitive neurons. Finally, our layer-wise analyses reveals that such neurons tend to cluster in certain decoder layers. Overall, our findings shed new light on the internal organization of multimodal representations.</article>","contentLength":1322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Aha Moments Be Fake? Identifying True and Decorative Thinking Steps in Chain-of-Thought","url":"https://arxiv.org/abs/2510.24941","date":1761796800,"author":"","guid":321833,"unread":true,"content":"<article>arXiv:2510.24941v1 Announce Type: new \nAbstract: Recent large language models (LLMs) can generate long Chain-of-Thought (CoT) at test time, enabling them to solve complex tasks. These reasoning steps in CoT are often assumed as a faithful reflection of the model's internal thinking process, and used to monitor unsafe intentions. However, we find many reasoning steps don't truly contribute to LLMs' prediction. We measure the step-wise causal influence of each reasoning step on the model's final prediction with a proposed True Thinking Score (TTS). We reveal that LLMs often interleave between true-thinking steps (which are genuinely used to produce the final output) and decorative-thinking steps (which only give the appearance of reasoning but have minimal causal impact). Notably, only a small subset of the total reasoning steps have a high TTS that causally drive the model's prediction: e.g., for the AIME dataset, only an average of 2.3% of reasoning steps in CoT have a TTS &gt;= 0.7 (range: 0-1) under the Qwen-2.5 model. Furthermore, we identify a TrueThinking direction in the latent space of LLMs. By steering along or against this direction, we can force the model to perform or disregard certain CoT steps when computing the final result. Finally, we highlight that self-verification steps in CoT (i.e., aha moments) can also be decorative, where LLMs do not truly verify their solution. Steering along the TrueThinking direction can force internal reasoning over these steps, resulting in a change in the final results. Overall, our work reveals that LLMs often verbalize reasoning steps without actually performing them internally, which undermines both the efficiency of LLM reasoning and the trustworthiness of CoT.</article>","contentLength":1736,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens","url":"https://arxiv.org/abs/2510.24940","date":1761796800,"author":"","guid":321834,"unread":true,"content":"<article>arXiv:2510.24940v1 Announce Type: new \nAbstract: The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed SemCoT. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found at https://github.com/YinhanHe123/SemCoT/.</article>","contentLength":1965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OrchVis: Hierarchical Multi-Agent Orchestration for Human Oversight","url":"https://arxiv.org/abs/2510.24937","date":1761796800,"author":"","guid":321835,"unread":true,"content":"<article>arXiv:2510.24937v1 Announce Type: new \nAbstract: We introduce OrchVis, a multi-agent orchestration framework that visualizes, verifies, and coordinates goal-driven collaboration among LLM-based agents. Through hierarchical goal alignment, task assignment, and conflict resolution, OrchVis enables humans to supervise complex multi-agent workflows without micromanaging each step. The system parses user intent into structured goals, monitors execution via automated verification, and exposes inter-agent dependencies through an interactive planning panel. When conflicts arise, users can explore system-proposed alternatives and selectively replan. OrchVis advances human-centered design for multi-agent systems by combining transparent visualization with adaptive autonomy.</article>","contentLength":774,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IBIS: A Powerful Hybrid Architecture for Human Activity Recognition","url":"https://arxiv.org/abs/2510.24936","date":1761796800,"author":"","guid":321836,"unread":true,"content":"<article>arXiv:2510.24936v1 Announce Type: new \nAbstract: The increasing interest in Wi-Fi sensing stems from its potential to capture environmental data in a low-cost, non-intrusive way, making it ideal for applications like healthcare, space occupancy analysis, and gesture-based IoT control. However, a major limitation in this field is the common problem of overfitting, where models perform well on training data but fail to generalize to new data. To overcome this, we introduce a novel hybrid architecture that integrates Inception-BiLSTM with a Support Vector Machine (SVM), which we refer to as IBIS. Our IBIS approach is uniquely engineered to improve model generalization and create more robust classification boundaries. By applying this method to Doppler-derived data, we achieve a movement recognition accuracy of nearly 99%. Comprehensive performance metrics and confusion matrices confirm the significant effectiveness of our proposed solution.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction","url":"https://arxiv.org/abs/2510.24934","date":1761796800,"author":"","guid":321837,"unread":true,"content":"<article>arXiv:2510.24934v1 Announce Type: new \nAbstract: Language models generally produce grammatical text, but they are more likely to make errors in certain contexts. Drawing on paradigms from psycholinguistics, we carry out a fine-grained analysis of those errors in different syntactic contexts. We demonstrate that by disaggregating over the conditions of carefully constructed datasets and comparing model performance on each over the course of training, it is possible to better understand the intermediate stages of grammatical learning in language models. Specifically, we identify distinct phases of training where language model behavior aligns with specific heuristics such as word frequency and local context rather than generalized grammatical rules. We argue that taking this approach to analyzing language model behavior more generally can serve as a powerful tool for understanding the intermediate learning phases, overall training dynamics, and the specific generalizations learned by language models.</article>","contentLength":1013,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hamilton-Jacobi Reachability Framework with Soft Constraints for Safety-Critical Systems","url":"https://arxiv.org/abs/2510.24933","date":1761796800,"author":"","guid":321838,"unread":true,"content":"<article>arXiv:2510.24933v1 Announce Type: new \nAbstract: Traditional reachability methods provide formal guarantees of safety under bounded disturbances. However, they strictly enforce state constraints as inviolable, which can result in overly conservative or infeasible solutions in complex operational scenarios. Many constraints encountered in practice, such as bounds on battery state of charge in electric vehicles, recommended speed envelopes, and comfort constraints in passenger-carrying vehicles, are inherently soft. Soft constraints allow temporary violations within predefined safety margins to accommodate uncertainty and competing operational demands, albeit at a cost such as increased wear or higher operational expenses. This paper introduces a novel soft-constrained reachability framework that extends Hamilton-Jacobi reachability analysis for the formal verification of safety-critical systems subject to both hard and soft constraints. Specifically, the framework characterizes a subset of the state space, referred to as the soft-constrained reach-avoid set, from which the system is guaranteed to reach a desired set safely, under worst-case disturbances, while ensuring that cumulative soft-constraint violations remain within a user-specified budget. The framework comprises two principal components: (i) an augmented-state model with an auxiliary budget state that tracks soft-constraint violations, and (ii) a regularization-based approximation of the discontinuous Hamilton-Jacobi value function associated with the reach-avoid differential game studied herein. The effectiveness of the proposed framework is demonstrated through numerical examples involving the landing of a simple point-mass model and a fixed-wing aircraft executing an emergency descent, both under wind disturbances. The simulation results validate the framework's ability to simultaneously manage both hard and soft constraints in safety-critical settings</article>","contentLength":1948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RiddleBench: A New Generative Reasoning Benchmark for LLMs","url":"https://arxiv.org/abs/2510.24932","date":1761796800,"author":"","guid":321839,"unread":true,"content":"<article>arXiv:2510.24932v1 Announce Type: new \nAbstract: Large Language Models have demonstrated strong performance on many established reasoning benchmarks. However, these benchmarks primarily evaluate structured skills like quantitative problem-solving, leaving a gap in assessing flexible, multifaceted reasoning abilities that are central to human intelligence. These abilities require integrating logical deduction with spatial awareness and constraint satisfaction, which current evaluations do not measure well. To address this, we introduce RiddleBench, a benchmark of 1,737 challenging puzzles in English designed to probe these core reasoning capabilities. Evaluation of state-of-the-art models on RiddleBench shows fundamental weaknesses. Even top proprietary models like Gemini 2.5 Pro, o3, and Claude 4 Sonnet achieve accuracy just above 60% (60.30%, 63.37%, and 63.16%). Analysis further reveals deep failures, including hallucination cascades (accepting flawed reasoning from other models) and poor self-correction due to a strong self-confirmation bias. Their reasoning is also fragile, with performance degrading significantly when constraints are reordered or irrelevant information is introduced. RiddleBench functions as a diagnostic tool for these issues and as a resource for guiding the development of more robust and reliable language models.</article>","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning","url":"https://arxiv.org/abs/2510.24927","date":1761796800,"author":"","guid":321840,"unread":true,"content":"<article>arXiv:2510.24927v1 Announce Type: new \nAbstract: Link prediction in bipartite graphs is crucial for applications like recommendation systems and failure detection, yet it is less studied than in monopartite graphs. Contrastive methods struggle with inefficient and biased negative sampling, while non-contrastive approaches rely solely on positive samples. Existing models perform well in transductive settings, but their effectiveness in inductive, weighted, and bipartite scenarios remains untested. To address this, we propose Weighted Bipartite Triplet-Bootstrapped Graph Latents (WBT-BGRL), a non-contrastive framework that enhances bootstrapped learning with a novel weighting mechanism in the triplet loss. Using a bipartite architecture with dual GCN encoders, WBT-BGRL is evaluated against adapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results on real-world datasets (Industry and E-commerce) show competitive performance, especially when weighting is applied during pretraining-highlighting the value of weighted, non-contrastive learning for inductive link prediction in bipartite graphs.</article>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator","url":"https://arxiv.org/abs/2510.24926","date":1761796800,"author":"","guid":321841,"unread":true,"content":"<article>arXiv:2510.24926v1 Announce Type: new \nAbstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator before graph convolution networks (GCNs). The KAN front end applies learnable one-dimensional warps and a linear mixing step, improving feature conditioning and nonlinear encoding without increasing message-passing depth. We employ this architecture to improve the performance of emulators for numerical ice sheet models. Our emulator is trained and tested using 36 melting-rate simulations with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to 5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves inference throughput on coarser meshes by replacing one edge-wise message-passing layer with a node-wise transform; only the finest mesh shows a modest cost. Overall, KAN-first designs offer a favorable accuracy vs. efficiency trade-off for large transient scenario sweeps.</article>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"S3C2 Summit 2025-03: Industry Secure Supply Chain Summit","url":"https://arxiv.org/abs/2510.24920","date":1761796800,"author":"","guid":321842,"unread":true,"content":"<article>arXiv:2510.24920v1 Announce Type: new \nAbstract: Software supply chains, while providing immense economic and software development value, are only as strong as their weakest link. Over the past several years, there has been an exponential increase in cyberattacks specifically targeting vulnerable links in critical software supply chains. These attacks disrupt the day-to-day functioning and threaten the security of nearly everyone on the internet, from billion-dollar companies and government agencies to hobbyist open-source developers. The ever-evolving threat of software supply chain attacks has garnered interest from both the software industry and US government in improving software supply chain security. On Thursday, March 6th, 2025, four researchers from the NSF-backed Secure Software Supply Chain Center (S3C2) conducted a Secure Software Supply Chain Summit with a diverse set of 18 practitioners from 17 organizations. The goals of the Summit were: (1) to enable sharing between participants from different industries regarding practical experiences and challenges with software supply chain security; (2) to help form new collaborations; and (3) to learn about the challenges facing participants to inform our future research directions. The summit consisted of discussions of six topics relevant to the government agencies represented, including software bill of materials (SBOMs); compliance; malicious commits; build infrastructure; culture; and large language models (LLMs) and security. For each topic of discussion, we presented a list of questions to participants to spark conversation. In this report, we provide a summary of the summit. The open questions and challenges that remained after each topic are listed at the end of each topic's section, and the initial discussion questions for each topic are provided in the appendix.</article>","contentLength":1857,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning","url":"https://arxiv.org/abs/2510.24919","date":1761796800,"author":"","guid":321843,"unread":true,"content":"<article>arXiv:2510.24919v1 Announce Type: new \nAbstract: In multimodal learning, dominant modalities often overshadow others, limiting generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM), a model-agnostic framework that applies to many modalities and supports early and late fusion scenarios. In every iteration, M-SAM in three steps optimizes learning. \\textbf{First, it identifies the dominant modality} based on modalities' contribution in the accuracy using Shapley. \\textbf{Second, it decomposes the loss landscape}, or in another language, it modulates the loss to prioritize the robustness of the model in favor of the dominant modality, and \\textbf{third, M-SAM updates the weights} by backpropagation of modulated gradients. This ensures robust learning for the dominant modality while enhancing contributions from others, allowing the model to explore and exploit complementary features that strengthen overall performance. Extensive experiments on four diverse datasets show that M-SAM outperforms the latest state-of-the-art optimization and gradient manipulation methods and significantly balances and improves multimodal learning.</article>","contentLength":1163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Topic Analysis with Side Information: A Neural-Augmented LDA Approach","url":"https://arxiv.org/abs/2510.24918","date":1761796800,"author":"","guid":321844,"unread":true,"content":"<article>arXiv:2510.24918v1 Announce Type: new \nAbstract: Traditional topic models such as Latent Dirichlet Allocation (LDA) have been widely used to uncover latent structures in text corpora, but they often struggle to integrate auxiliary information such as metadata, user attributes, or document labels. These limitations restrict their expressiveness, personalization, and interpretability. To address this, we propose nnLDA, a neural-augmented probabilistic topic model that dynamically incorporates side information through a neural prior mechanism. nnLDA models each document as a mixture of latent topics, where the prior over topic proportions is generated by a neural network conditioned on auxiliary features. This design allows the model to capture complex nonlinear interactions between side information and topic distributions that static Dirichlet priors cannot represent. We develop a stochastic variational Expectation-Maximization algorithm to jointly optimize the neural and probabilistic components. Across multiple benchmark datasets, nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression in topic coherence, perplexity, and downstream classification. These results highlight the benefits of combining neural representation learning with probabilistic topic modeling in settings where side information is available.</article>","contentLength":1345,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiplayer Parallel Repetition Is the Same as High-Dimensional Extremal Combinatorics","url":"https://arxiv.org/abs/2510.24910","date":1761796800,"author":"","guid":321845,"unread":true,"content":"<article>arXiv:2510.24910v1 Announce Type: new \nAbstract: We show equivalences between several high-dimensional problems in extremal combinatorics and parallel repetition of multiplayer (multiprover) games over large answer alphabets. This extends the forbidden-subgraph technique, previously studied by Verbitsky (Theoretical Computer Science 1996), Feige and Verbitsy (Combinatorica 2002), and H\\k{a}z{\\l}a , Holenstein and Rao (2016), to all $k$-player games, and establishes new connections to problems in combinatorics. We believe that these connections may help future progress in both fields.</article>","contentLength":590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trust Dynamics in Strategic Coopetition: Computational Foundations for Requirements Engineering in Multi-Agent Systems","url":"https://arxiv.org/abs/2510.24909","date":1761796800,"author":"","guid":321846,"unread":true,"content":"<article>arXiv:2510.24909v1 Announce Type: new \nAbstract: Requirements engineering increasingly occurs in multi-stakeholder environments where organizations simultaneously cooperate and compete, creating coopetitive relationships in which trust evolves dynamically based on observed behavior over repeated interactions. While conceptual modeling languages like i* represent trust relationships qualitatively, they lack computational mechanisms for analyzing how trust changes with behavioral evidence. Conversely, computational trust models from multi-agent systems provide algorithmic updating but lack grounding in requirements engineering contexts and conceptual models. This technical report bridges this gap by developing a computational trust model that extends game-theoretic foundations for strategic coopetition with dynamic trust evolution. We introduce trust as a two-layer system with immediate trust responding to current behavior and reputation tracking violation history. Trust evolves through asymmetric updating where cooperation builds trust gradually while violations erode it sharply, creating hysteresis effects and trust ceilings that constrain relationship recovery. We develop a structured translation framework enabling requirements engineers to instantiate computational trust models from i* dependency networks and organizational contexts. Comprehensive experimental validation across 78,125 parameter configurations establishes robust emergence of negativity bias, hysteresis effects, and cumulative damage amplification. Empirical validation using the Renault-Nissan Alliance case study (1999-2025) achieves 49 out of 60 validation points (81.7%), successfully reproducing documented trust evolution across five distinct relationship phases including crisis and recovery periods. This technical report builds upon its foundational companion work in arXiv:2510.18802.</article>","contentLength":1886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Multi-View Transformers","url":"https://arxiv.org/abs/2510.24907","date":1761796800,"author":"","guid":321847,"unread":true,"content":"<article>arXiv:2510.24907v1 Announce Type: new \nAbstract: Multi-view transformers such as DUSt3R are revolutionizing 3D vision by solving 3D tasks in a feed-forward manner. However, contrary to previous optimization-based pipelines, the inner mechanisms of multi-view transformers are unclear. Their black-box nature makes further improvements beyond data scaling challenging and complicates usage in safety- and reliability-critical applications. Here, we present an approach for probing and visualizing 3D representations from the residual connections of the multi-view transformers' layers. In this manner, we investigate a variant of the DUSt3R model, shedding light on the development of its latent state across blocks, the role of the individual layers, and suggest how it differs from methods with stronger inductive biases of explicit global pose. Finally, we show that the investigated variant of DUSt3R estimates correspondences that are refined with reconstructed geometry. The code used for the analysis is available at https://github.com/JulienGaubil/und3rstand .</article>","contentLength":1067,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fair Indivisible Payoffs through Shapley Value","url":"https://arxiv.org/abs/2510.24906","date":1761796800,"author":"","guid":321848,"unread":true,"content":"<article>arXiv:2510.24906v1 Announce Type: new \nAbstract: We consider the problem of payoff division in indivisible coalitional games, where the value of the grand coalition is a natural number. This number represents a certain quantity of indivisible objects, such as parliamentary seats, kidney exchanges, or top features contributing to the outcome of a machine learning model. The goal of this paper is to propose a fair method for dividing these objects among players. To achieve this, we define the indivisible Shapley value and study its properties. We demonstrate our proposed technique using three case studies, in particular, we use it to identify key regions of an image in the context of an image classification task.</article>","contentLength":720,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VividCam: Learning Unconventional Camera Motions from Virtual Synthetic Videos","url":"https://arxiv.org/abs/2510.24904","date":1761796800,"author":"","guid":321849,"unread":true,"content":"<article>arXiv:2510.24904v1 Announce Type: new \nAbstract: Although recent text-to-video generative models are getting more capable of following external camera controls, imposed by either text descriptions or camera trajectories, they still struggle to generalize to unconventional camera motions, which is crucial in creating truly original and artistic videos. The challenge lies in the difficulty of finding sufficient training videos with the intended uncommon camera motions. To address this challenge, we propose VividCam, a training paradigm that enables diffusion models to learn complex camera motions from synthetic videos, releasing the reliance on collecting realistic training videos. VividCam incorporates multiple disentanglement strategies that isolates camera motion learning from synthetic appearance artifacts, ensuring more robust motion representation and mitigating domain shift. We demonstrate that our design synthesizes a wide range of precisely controlled and complex camera motions using surprisingly simple synthetic data. Notably, this synthetic data often consists of basic geometries within a low-poly 3D scene and can be efficiently rendered by engines like Unity. Our video results can be found in https://wuqiuche.github.io/VividCamDemoPage/ .</article>","contentLength":1268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pixels to Signals: A Real-Time Framework for Traffic Demand Estimation","url":"https://arxiv.org/abs/2510.24902","date":1761796800,"author":"","guid":321850,"unread":true,"content":"<article>arXiv:2510.24902v1 Announce Type: new \nAbstract: Traffic congestion is becoming a challenge in the rapidly growing urban cities, resulting in increasing delays and inefficiencies within urban transportation systems. To address this issue a comprehensive methodology is designed to optimize traffic flow and minimize delays. The framework is structured with three primary components: (a) vehicle detection, (b) traffic prediction, and (c) traffic signal optimization. This paper presents the first component, vehicle detection. The methodology involves analyzing multiple sequential frames from a camera feed to compute the background, i.e. the underlying roadway, by averaging pixel values over time. The computed background is then utilized to extract the foreground, where the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm is applied to detect vehicles. With its computational efficiency and minimal infrastructure modification requirements, the proposed methodology offers a practical and scalable solution for real-world deployment.</article>","contentLength":1069,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delay Tolerant Control for Autonomous Driving Using CDOB","url":"https://arxiv.org/abs/2510.24898","date":1761796800,"author":"","guid":321851,"unread":true,"content":"<article>arXiv:2510.24898v1 Announce Type: new \nAbstract: With the rapid growth of autonomous vehicle technologies, effective path-tracking control has become a critical component in ensuring safety and efficiency in complex traffic scenarios. When a high level decision making agent generates a collision free path, a robust low level controller is required to precisely follow this trajectory. However, connected autonomous vehicles (CAV) are inherently affected by communication delays and computation delays, which significantly degrade the performance of conventional controllers such as PID or other more advanced controllers like disturbance observers (DOB). While DOB-based designs have shown effectiveness in rejecting disturbances under nominal conditions, their performance deteriorates considerably in the presence of unknown time delays. To address this challenge, this paper proposes a delay-tolerant communication disturbance observer (CDOB) framework for path-tracking control in delayed systems. The proposed CDOB compensates for the adverse effects of time delays, maintaining accurate trajectory tracking even under uncertain and varying delay conditions. It is shown through a simulation study that the proposed control architecture maintains close alignment with the reference trajectory across various scenarios, including single lane change, double-= lane change, and Elastic Band generated collision avoidance paths under various time delays. Simulation results further demonstrate that the proposed method outperforms conventional approaches in both tracking accuracy and delay robustness, making it well suited for autonomous driving applications.</article>","contentLength":1664,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficiency Without Cognitive Change: Evidence from Human Interaction with Narrow AI Systems","url":"https://arxiv.org/abs/2510.24893","date":1761796800,"author":"","guid":321852,"unread":true,"content":"<article>arXiv:2510.24893v1 Announce Type: new \nAbstract: The growing integration of artificial intelligence (AI) into human cognition raises a fundamental question: does AI merely improve efficiency, or does it alter how we think? This study experimentally tested whether short-term exposure to narrow AI tools enhances core cognitive abilities or simply optimizes task performance. Thirty young adults completed standardized neuropsychological assessments embedded in a seven-week protocol with a four-week online intervention involving problem-solving and verbal comprehension tasks, either with or without AI support (ChatGPT). While AI-assisted participants completed several tasks faster and more accurately, no significant pre-post differences emerged in standardized measures of problem solving or verbal comprehension. These results demonstrate efficiency gains without cognitive change, suggesting that current narrow AI systems serve as cognitive scaffolds extending performance without transforming underlying mental capacities. The findings highlight the need for ethical and educational frameworks that promote critical and autonomous thinking in an increasingly AI-augmented cognitive ecology.</article>","contentLength":1199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Idea2Plan: Exploring AI-Powered Research Planning","url":"https://arxiv.org/abs/2510.24891","date":1761796800,"author":"","guid":321853,"unread":true,"content":"<article>arXiv:2510.24891v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated significant potential to accelerate scientific discovery as valuable tools for analyzing data, generating hypotheses, and supporting innovative approaches in various scientific fields. In this work, we investigate how LLMs can handle the transition from conceptual research ideas to well-structured research plans. Effective research planning not only supports scientists in advancing their research but also represents a crucial capability for the development of autonomous research agents. Despite its importance, the field lacks a systematic understanding of LLMs' research planning capability. To rigorously measure this capability, we introduce the Idea2Plan task and Idea2Plan Bench, a benchmark built from 200 ICML 2025 Spotlight and Oral papers released after major LLM training cutoffs. Each benchmark instance includes a research idea and a grading rubric capturing the key components of valid plans. We further propose Idea2Plan JudgeEval, a complementary benchmark to assess the reliability of LLM-based judges against expert annotations. Experimental results show that GPT-5 and GPT-5-mini achieve the strongest performance on the benchmark, though substantial headroom remains for future improvement. Our study provides new insights into LLMs' capability for research planning and lay the groundwork for future progress.</article>","contentLength":1430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding","url":"https://arxiv.org/abs/2510.24889","date":1761796800,"author":"","guid":321854,"unread":true,"content":"<article>arXiv:2510.24889v1 Announce Type: new \nAbstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools; EEG is promising but underused at first contact. We present an adaptive multitask EEG classifier that converts 32-channel signals to power spectral density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to predict stroke type (healthy, ischemic, hemorrhagic), hemispheric lateralization, and severity, and applies a deep Q-network (DQN) to tune decision thresholds in real time. Using a patient-wise split of the UCLH Stroke EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the primary outcome was stroke-type performance; secondary outcomes were severity and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%) for lateralization. With DQN threshold adaptation, stroke-type accuracy increased to about 98.0% (F1 97.7%). We also tested robustness on an independent, low-density EEG cohort (ZJU4H) and report paired patient-level statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies (index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis; patient-wise evaluation). Adaptive thresholding shifts the operating point to clinically preferred sensitivity-specificity trade-offs, while integrated scalp-map and spectral visualizations support interpretability.</article>","contentLength":1465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proper Body Landmark Subset Enables More Accurate and 5X Faster Recognition of Isolated Signs in LIBRAS","url":"https://arxiv.org/abs/2510.24887","date":1761796800,"author":"","guid":321855,"unread":true,"content":"<article>arXiv:2510.24887v1 Announce Type: new \nAbstract: This paper investigates the feasibility of using lightweight body landmark detection for the recognition of isolated signs in Brazilian Sign Language (LIBRAS). Although the skeleton-based approach by Alves et al. (2024) enabled substantial improvements in recognition performance, the use of OpenPose for landmark extraction hindered time performance. In a preliminary investigation, we observed that simply replacing OpenPose with the lightweight MediaPipe, while improving processing speed, significantly reduced accuracy. To overcome this limitation, we explored landmark subset selection strategies aimed at optimizing recognition performance. Experimental results showed that a proper landmark subset achieves comparable or superior performance to state-of-the-art methods while reducing processing time by more than 5X compared to Alves et al. (2024). As an additional contribution, we demonstrated that spline-based imputation effectively mitigates missing landmark issues, leading to substantial accuracy gains. These findings highlight that careful landmark selection, combined with simple imputation techniques, enables efficient and accurate isolated sign recognition, paving the way for scalable Sign Language Recognition systems.</article>","contentLength":1291,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FruitProm: Probabilistic Maturity Estimation and Detection of Fruits and Vegetables","url":"https://arxiv.org/abs/2510.24885","date":1761796800,"author":"","guid":321856,"unread":true,"content":"<article>arXiv:2510.24885v1 Announce Type: new \nAbstract: Maturity estimation of fruits and vegetables is a critical task for agricultural automation, directly impacting yield prediction and robotic harvesting. Current deep learning approaches predominantly treat maturity as a discrete classification problem (e.g., unripe, ripe, overripe). This rigid formulation, however, fundamentally conflicts with the continuous nature of the biological ripening process, leading to information loss and ambiguous class boundaries. In this paper, we challenge this paradigm by reframing maturity estimation as a continuous, probabilistic learning task. We propose a novel architectural modification to the state-of-the-art, real-time object detector, RT-DETRv2, by introducing a dedicated probabilistic head. This head enables the model to predict a continuous distribution over the maturity spectrum for each detected object, simultaneously learning the mean maturity state and its associated uncertainty. This uncertainty measure is crucial for downstream decision-making in robotics, providing a confidence score for tasks like selective harvesting. Our model not only provides a far richer and more biologically plausible representation of plant maturity but also maintains exceptional detection performance, achieving a mean Average Precision (mAP) of 85.6\\% on a challenging, large-scale fruit dataset. We demonstrate through extensive experiments that our probabilistic approach offers more granular and accurate maturity assessments than its classification-based counterparts, paving the way for more intelligent, uncertainty-aware automated systems in modern agriculture</article>","contentLength":1660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations","url":"https://arxiv.org/abs/2510.24884","date":1761796800,"author":"","guid":321857,"unread":true,"content":"<article>arXiv:2510.24884v1 Announce Type: new \nAbstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a strong positive correlation between in-distribution (ID) and OOD accuracy across models, termed \"accuracy-on-the-line.\" This pattern is often taken to imply that spurious correlations - correlations that improve ID but reduce OOD performance - are rare in practice. We find that this positive correlation is often an artifact of aggregating heterogeneous OOD examples. Using a simple gradient-based method, OODSelect, we identify semantically coherent OOD subsets where accuracy on the line does not hold. Across widely used distribution shift benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings indicate that aggregate metrics can obscure important failure modes of OOD robustness. We release code and the identified subsets to facilitate further research.</article>","contentLength":984,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What Are People's Actual Utility Functions in Budget Aggregation?","url":"https://arxiv.org/abs/2510.24872","date":1761796800,"author":"","guid":321858,"unread":true,"content":"<article>arXiv:2510.24872v1 Announce Type: new \nAbstract: While participatory budgeting and budget-aggregation mechanisms require assumptions about how voters evaluate non-ideal budget allocations, little empirical evidence exists to validate which utility models accurately capture human preferences. We conducted structured polls with human participants to test whether real people's preferences conform to commonly assumed utility functions such as $\\ell_1$, $\\ell_2$ and Leontief. Our results suggest that these models may have limited explanatory power for actual behavior: most participants showed inconsistent patterns across different metric comparisons, and standard assumptions of project symmetry and sign symmetry -- core features of common distance-based metrics -- received little empirical support. However, we find encouraging evidence for more fundamental preference structures: a large majority of participants showed consistency with star-shaped preferences, as well as with peak-linear utility functions, where utility changes proportionally with distance from the ideal budget. These findings have important implications for designers of budget aggregation mechanisms. While theoretical results demonstrate impossibility results for standard distance metrics regarding truthfulness, Pareto-efficiency, and proportionality, our evidence suggests alternative modeling approaches may be warranted. More broadly, this work introduces a systematic methodology to empirically test the utility function assumptions that underpin budget aggregation theories, paving the way for more robust and realistic mechanism design.</article>","contentLength":1625,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized Merging Control of Connected and Automated Vehicles to Enhance Safety and Energy Efficiency using Control Barrier Functions","url":"https://arxiv.org/abs/2510.24871","date":1761796800,"author":"","guid":321859,"unread":true,"content":"<article>arXiv:2510.24871v1 Announce Type: new \nAbstract: This paper presents a decentralized Control Barrier Function (CBF) based approach for highway merging of Connected and Automated Vehicles (CAVs). In this control algorithm, each \"host\" vehicle negotiates with other agents in a control zone of the highway network, and enacts its own action, to perform safe and energy-efficient merge maneuvers. It uses predictor-corrector loops within the robust CBF setting for negotiation and to reconcile disagreements that may arise. There is no explicit order of vehicles and no priority. A notable feature is absence of gridlocks due to instability of the inter-agent system. Results from Monte Carlo simulations show significant improvement in the system-wide energy efficiency and traffic flow compared to a first-in-first-out approach, as well as enhanced robustness of the proposed decentralized controller compared to its centralized counterpart.</article>","contentLength":940,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Seeing Through the MiRAGE: Evaluating Multimodal Retrieval Augmented Generation","url":"https://arxiv.org/abs/2510.24870","date":1761796800,"author":"","guid":321860,"unread":true,"content":"<article>arXiv:2510.24870v1 Announce Type: new \nAbstract: We introduce MiRAGE, an evaluation framework for retrieval-augmented generation (RAG) from multimodal sources. As audiovisual media becomes a prevalent source of information online, it is essential for RAG systems to integrate information from these sources into generation. However, existing evaluations for RAG are text-centric, limiting their applicability to multimodal, reasoning intensive settings because they don't verify information against sources. MiRAGE is a claim-centric approach to multimodal RAG evaluation, consisting of InfoF1, evaluating factuality and information coverage, and CiteF1, measuring citation support and completeness. We show that MiRAGE, when applied by humans, strongly aligns with extrinsic quality judgments. We additionally introduce automatic variants of MiRAGE and three prominent TextRAG metrics -- ACLE, ARGUE, and RAGAS -- demonstrating the limitations of text-centric work and laying the groundwork for automatic evaluation. We release open-source implementations and outline how to assess multimodal RAG.</article>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Reinforcement Learning Approach to QoSAware Load Balancing in 5G Cellular Networks under User Mobility and Observation Uncertainty","url":"https://arxiv.org/abs/2510.24869","date":1761796800,"author":"","guid":321861,"unread":true,"content":"<article>arXiv:2510.24869v1 Announce Type: new \nAbstract: Efficient mobility management and load balancing are critical to sustaining Quality of Service (QoS) in dense, highly dynamic 5G radio access networks. We present a deep reinforcement learning framework based on Proximal Policy Optimization (PPO) for autonomous, QoS-aware load balancing implemented end-to-end in a lightweight, pure-Python simulation environment. The control problem is formulated as a Markov Decision Process in which the agent periodically adjusts Cell Individual Offset (CIO) values to steer user-cell associations. A multi-objective reward captures key performance indicators (aggregate throughput, latency, jitter, packet loss rate, Jain's fairness index, and handover count), so the learned policy explicitly balances efficiency and stability under user mobility and noisy observations. The PPO agent uses an actor-critic neural network trained from trajectories generated by the Python simulator with configurable mobility (e.g., Gauss-Markov) and stochastic measurement noise. Across 500+ training episodes and stress tests with increasing user density, the PPO policy consistently improves KPI trends (higher throughput and fairness, lower delay, jitter, packet loss, and handovers) and exhibits rapid, stable convergence. Comparative evaluations show that PPO outperforms rule-based ReBuHa and A3 as well as the learning-based CDQL baseline across all KPIs while maintaining smoother learning dynamics and stronger generalization as load increases. These results indicate that PPO's clipped policy updates and advantage-based training yield robust, deployable control for next-generation RAN load balancing using an entirely Python-based toolchain.</article>","contentLength":1725,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Semi-Lagrangian Adaptive Rank (SLAR) Method for High-Dimensional Vlasov Dynamics","url":"https://arxiv.org/abs/2510.24861","date":1761796800,"author":"","guid":321862,"unread":true,"content":"<article>arXiv:2510.24861v1 Announce Type: new \nAbstract: We extend our previous work on a semi-Lagrangian adaptive rank (SLAR) integrator, in the finite difference framework for nonlinear Vlasov-Poisson systems, to the general high-order tensor setting. The proposed scheme retains the high-order accuracy of semi-Lagrangian methods, ensuring stability for large time steps and avoiding dimensional splitting errors. The primary contribution of this paper is the novel extension of the algorithm from the matrix to the high-dimensional tensor setting, which enables the simulation of Vlasov models in up to six dimensions. The key technical components include (1) a third-order high-dimensional polynomial reconstruction that scales as $O(d^2)$, providing a point-wise approximation of the solution at the foot of characteristics in a semi-Lagrangian scheme; (2) a recursive hierarchical adaptive cross approximation of high-order tensors in a hierarchical Tucker format, characterized by a tensor tree; (3) a low-complexity Poisson solver in the hierarchical Tucker format that leverages the FFT for efficiency. The computed adaptive rank kinetic solutions exhibit low-rank structures within branches of the tensor tree resulting in substantial computational savings in both storage and time. The resulting algorithm achieves a computational complexity of $O(d^4 N r^{3+\\lceil\\log_2d\\rceil})$, where $N$ is the number of grid points per dimension, $d$ is the problem dimension, and $r$ is the maximum rank in the tensor tree, overcoming the curse of dimensionality. Through extensive numerical tests, we demonstrate the efficiency of the proposed algorithm and highlight its ability to capture complex solution structures while maintaining a computational complexity that scales linearly with $N$.</article>","contentLength":1790,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Large Language Models Grasp The Grammar? Evidence from Grammar-Book-Guided Probing in Luxembourgish","url":"https://arxiv.org/abs/2510.24856","date":1761796800,"author":"","guid":321863,"unread":true,"content":"<article>arXiv:2510.24856v1 Announce Type: new \nAbstract: Grammar refers to the system of rules that governs the structural organization and the semantic relations among linguistic units such as sentences, phrases, and words within a given language. In natural language processing, there remains a notable scarcity of grammar focused evaluation protocols, a gap that is even more pronounced for low-resource languages. Moreover, the extent to which large language models genuinely comprehend grammatical structure, especially the mapping between syntactic structures and meanings, remains under debate. To investigate this issue, we propose a Grammar Book Guided evaluation pipeline intended to provide a systematic and generalizable framework for grammar evaluation consisting of four key stages, and in this work we take Luxembourgish as a case study. The results show a weak positive correlation between translation performance and grammatical understanding, indicating that strong translations do not necessarily imply deep grammatical competence. Larger models perform well overall due to their semantic strength but remain weak in morphology and syntax, struggling particularly with Minimal Pair tasks, while strong reasoning ability offers a promising way to enhance their grammatical understanding.</article>","contentLength":1297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On syntactic concept lattice models for the Lambek calculus and infinitary action logic","url":"https://arxiv.org/abs/2510.24853","date":1761796800,"author":"","guid":321864,"unread":true,"content":"<article>arXiv:2510.24853v1 Announce Type: new \nAbstract: The linguistic applications of the Lambek calculus suggest its semantics over algebras of formal languages. A straightforward approach to construct such semantics indeed yields a brilliant completeness theorem (Pentus 1995). However, extending the calculus with extra operations ruins completeness. In order to mitigate this issue, Wurm (2017) introduced a modification of this semantics, namely, models over syntactic concept lattices (SCLs). We extend this semantics to the infinitary extension of the Lambek calculus with Kleene iteration (infinitary action logic), prove strong completeness and some interesting corollaries. We also discuss issues arising with constants - zero, unit, top - and provide some strengthenings of Wurm's results towards including these constants into the systems involved.</article>","contentLength":854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Parameter-Efficient Multi-Scale Convolutional Adapter for Synthetic Speech Detection","url":"https://arxiv.org/abs/2510.24852","date":1761796800,"author":"","guid":321865,"unread":true,"content":"<article>arXiv:2510.24852v1 Announce Type: new \nAbstract: Recent synthetic speech detection models typically adapt a pre-trained SSL model via finetuning, which is computationally demanding. Parameter-Efficient Fine-Tuning (PEFT) offers an alternative. However, existing methods lack the specific inductive biases required to model the multi-scale temporal artifacts characteristic of spoofed audio. This paper introduces the Multi-Scale Convolutional Adapter (MultiConvAdapter), a parameter-efficient architecture designed to address this limitation. MultiConvAdapter integrates parallel convolutional modules within the SSL encoder, facilitating the simultaneous learning of discriminative features across multiple temporal resolutions, capturing both short-term artifacts and long-term distortions. With only $3.17$M trainable parameters ($1\\%$ of the SSL backbone), MultiConvAdapter substantially reduces the computational burden of adaptation. Evaluations on five public datasets, demonstrate that MultiConvAdapter achieves superior performance compared to full fine-tuning and established PEFT methods.</article>","contentLength":1099,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scheduling Your LLM Reinforcement Learning with Reasoning Trees","url":"https://arxiv.org/abs/2510.24832","date":1761796800,"author":"","guid":321866,"unread":true,"content":"<article>arXiv:2510.24832v1 Announce Type: new \nAbstract: Using Reinforcement Learning with Verifiable Rewards (RLVR) to optimize Large Language Models (LLMs) can be conceptualized as progressively editing a query's `Reasoning Tree'. This process involves exploring nodes (tokens) and dynamically modifying the model's policy at each node. When combined with data scheduling, this process yields further gains in data efficiency and accuracy. However, existing RLVR data scheduling methods typically rely on path-based metrics to rank queries, overlooking the reasoning tree structures of these queries. In this paper, we introduce a novel metric, namely Reasoning Score (r-score), which measures the query's learning difficulty based on the structure of its reasoning tree. Based on the r-score, we propose the Reasoning Tree Schedule (Re-Schedule), a scheduling algorithm that constructs a curriculum progressing from structurally simple (high r-score) to complex (low r-score) queries. Experiments on six math-reasoning benchmarks show that Re-Schedule significantly improves average accuracy, achieving gains of up to 3.2%. These strong results validate our approach and demonstrate that a structural understanding of the reasoning tree provides a more powerful and principled foundation for RLVR data scheduling.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Narrative Continuity Test: A Conceptual Framework for Evaluating Identity Persistence in AI Systems","url":"https://arxiv.org/abs/2510.24831","date":1761796800,"author":"","guid":321867,"unread":true,"content":"<article>arXiv:2510.24831v1 Announce Type: new \nAbstract: Artificial intelligence systems based on large language models (LLMs) can now generate coherent text, music, and images, yet they operate without a persistent state: each inference reconstructs context from scratch. This paper introduces the Narrative Continuity Test (NCT) -- a conceptual framework for evaluating identity persistence and diachronic coherence in AI systems. Unlike capability benchmarks that assess task performance, the NCT examines whether an LLM remains the same interlocutor across time and interaction gaps. The framework defines five necessary axes -- Situated Memory, Goal Persistence, Autonomous Self-Correction, Stylistic &amp; Semantic Stability, and Persona/Role Continuity -- and explains why current architectures systematically fail to support them. Case analyses (Character.AI, Grok, Replit, Air Canada) show predictable continuity failures under stateless inference. The NCT reframes AI evaluation from performance to persistence, outlining conceptual requirements for future benchmarks and architectural designs that could sustain long-term identity and goal coherence in generative models.</article>","contentLength":1170,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Generation Phases of Flow Matching: a Denoising Perspective","url":"https://arxiv.org/abs/2510.24830","date":1761796800,"author":"","guid":321868,"unread":true,"content":"<article>arXiv:2510.24830v1 Announce Type: new \nAbstract: Flow matching has achieved remarkable success, yet the factors influencing the quality of its generation process remain poorly understood. In this work, we adopt a denoising perspective and design a framework to empirically probe the generation process. Laying down the formal connections between flow matching models and denoisers, we provide a common ground to compare their performances on generation and denoising. This enables the design of principled and controlled perturbations to influence sample generation: noise and drift. This leads to new insights on the distinct dynamical phases of the generative process, enabling us to precisely characterize at which stage of the generative process denoisers succeed or fail and why this matters.</article>","contentLength":797,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT","url":"https://arxiv.org/abs/2510.24829","date":1761796800,"author":"","guid":321869,"unread":true,"content":"<article>arXiv:2510.24829v1 Announce Type: new \nAbstract: The integration of the Internet of Things (IoT) and Artificial Intelligence offers significant opportunities to enhance our ability to monitor and address ecological changes. As environmental challenges become increasingly pressing, the need for effective remote monitoring solutions is more critical than ever. A major challenge in designing IoT applications for environmental monitoring - particularly those involving image data - is to create energy-efficient IoT devices capable of long-term operation in remote areas with limited power availability. Advancements in the field of Tiny Machine Learning allow the use of Convolutional Neural Networks (CNNs) on resource-constrained, battery-operated microcontrollers. Since data transfer is energy-intensive, performing inference directly on microcontrollers to reduce the message size can extend the operational lifespan of IoT nodes. This work evaluates the use of common Low Power Wide Area Networks and compressed CNNs trained on domain specific datasets on an ESP32-S3. Our experiments demonstrate, among other things, that executing CNN inference on-device and transmitting only the results reduces the overall energy consumption by a factor of up to five compared to sending raw image data. %The compression of the model using Post Training Quantization is accompanied by an acceptable reduction in accuracy of only a few percentage points compared to a non-quantized model. These findings advocate the development of IoT applications with reduced carbon footprint and capable of operating autonomously in environmental monitoring scenarios by incorporating Embedded Machine Learning.</article>","contentLength":1692,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MCIHN: A Hybrid Network Model Based on Multi-path Cross-modal Interaction for Multimodal Emotion Recognition","url":"https://arxiv.org/abs/2510.24827","date":1761796800,"author":"","guid":321870,"unread":true,"content":"<article>arXiv:2510.24827v1 Announce Type: new \nAbstract: Multimodal emotion recognition is crucial for future human-computer interaction. However, accurate emotion recognition still faces significant challenges due to differences between different modalities and the difficulty of characterizing unimodal emotional information. To solve these problems, a hybrid network model based on multipath cross-modal interaction (MCIHN) is proposed. First, adversarial autoencoders (AAE) are constructed separately for each modality. The AAE learns discriminative emotion features and reconstructs the features through a decoder to obtain more discriminative information about the emotion classes. Then, the latent codes from the AAE of different modalities are fed into a predefined Cross-modal Gate Mechanism model (CGMM) to reduce the discrepancy between modalities, establish the emotional relationship between interacting modalities, and generate the interaction features between different modalities. Multimodal fusion using the Feature Fusion module (FFM) for better emotion recognition. Experiments were conducted on publicly available SIMS and MOSI datasets, demonstrating that MCIHN achieves superior performance.</article>","contentLength":1205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA","url":"https://arxiv.org/abs/2510.24826","date":1761796800,"author":"","guid":321871,"unread":true,"content":"<article>arXiv:2510.24826v1 Announce Type: new \nAbstract: Machine learning models increasingly map biological sequence-fitness landscapes to predict mutational effects. Effective evaluation of these models requires benchmarks curated from empirical data. Despite their impressive scales, existing benchmarks lack topographical information regarding the underlying fitness landscapes, which hampers interpretation and comparison of model performance beyond averaged scores. Here, we introduce GraphFLA, a Python framework that constructs and analyzes fitness landscapes from mutagensis data in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions of mutants. GraphFLA calculates 20 biologically relevant features that characterize 4 fundamental aspects of landscape topography. By applying GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we demonstrate its utility in interpreting and comparing the performance of dozens of fitness prediction models, highlighting factors influencing model accuracy and respective advantages of different models. In addition, we release 155 combinatorially complete empirical fitness landscapes, encompassing over 2.2 million sequences across various modalities. All the codes and datasets are available at https://github.com/COLA-Laboratory/GraphFLA.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parallel Loop Transformer for Efficient Test-Time Computation Scaling","url":"https://arxiv.org/abs/2510.24824","date":1761796800,"author":"","guid":321872,"unread":true,"content":"<article>arXiv:2510.24824v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.</article>","contentLength":1402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Chatbots Walk the Talk of Responsible AI?","url":"https://arxiv.org/abs/2510.24823","date":1761796800,"author":"","guid":321873,"unread":true,"content":"<article>arXiv:2510.24823v1 Announce Type: new \nAbstract: This study examines whether leading AI chatbot companies implement the responsible AI principles they publicly advocate. The authors used a mixed-methods approach analyzing four major chatbots (ChatGPT, Gemini, DeepSeek, and Grok) across company websites, technical documentation, and direct chatbot evaluations. We found significant gaps between corporate rhetoric and practice.</article>","contentLength":428,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Managing Administrative Law Cases using an Adaptable Model-driven Norm-enforcing Tool","url":"https://arxiv.org/abs/2510.24822","date":1761796800,"author":"","guid":321874,"unread":true,"content":"<article>arXiv:2510.24822v1 Announce Type: new \nAbstract: Governmental organisations cope with many laws and policies when handling administrative law cases. Making sure these norms are enforced in the handling of cases is for the most part done manually. However, enforcing policies can get complicated and time consuming with ever-changing (interpretations of) laws and varying cases. This introduces errors and delays in the decision-making process and therefore limits the access to justice for citizens. A potential solution is offered by our tool in which norms are enforced using automated normative reasoning. By ensuring the procedural norms are followed and transparency can be provided about the reasoning behind a decision to citizens, the tool benefits the access to justice for citizens. In this paper we report on the implementation of a model-driven case management tool for administrative law cases, based on a set of requirements elicited during earlier research. Our tool achieves adaptability and norm enforcement by interacting with an interpreter for eFLINT, a domain-specific language for norm specification. We report on the current state of the case management tool and suggest directions for further development.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation","url":"https://arxiv.org/abs/2510.24821","date":1761796800,"author":"","guid":321875,"unread":true,"content":"<article>arXiv:2510.24821v1 Announce Type: new \nAbstract: We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion total parameters, of which only 6.1 billion are active per token. This architecture enables highly efficient scaling (dramatically improving computational efficiency while significantly expanding model capacity) and empowers stronger unified multimodal intelligence across vision, speech, and language, representing a key step toward Artificial General Intelligence (AGI). Compared to its predecessor, the upgraded version exhibits substantial improvements across multimodal understanding and generation. We significantly advance speech recognition capabilities, achieving state-of-the-art performance in contextual ASR and highly competitive results in dialect-aware ASR. In image generation, Ming-Flash-Omni introduces high-fidelity text rendering and demonstrates marked gains in scene consistency and identity preservation during image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation, a capability that not only achieves strong standalone segmentation performance but also enhances spatial control in image generation and improves editing consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in text-to-image generation and generative segmentation, and sets new records on all 12 contextual ASR benchmarks, all within a single unified architecture.</article>","contentLength":1496,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SafeEditor: Unified MLLM for Efficient Post-hoc T2I Safety Editing","url":"https://arxiv.org/abs/2510.24820","date":1761796800,"author":"","guid":321876,"unread":true,"content":"<article>arXiv:2510.24820v1 Announce Type: new \nAbstract: With the rapid advancement of text-to-image (T2I) models, ensuring their safety has become increasingly critical. Existing safety approaches can be categorized into training-time and inference-time methods. While inference-time methods are widely adopted due to their cost-effectiveness, they often suffer from limitations such as over-refusal and imbalance between safety and utility. To address these challenges, we propose a multi-round safety editing framework that functions as a model-agnostic, plug-and-play module, enabling efficient safety alignment for any text-to-image model. Central to this framework is MR-SafeEdit, a multi-round image-text interleaved dataset specifically constructed for safety editing in text-to-image generation. We introduce a post-hoc safety editing paradigm that mirrors the human cognitive process of identifying and refining unsafe content. To instantiate this paradigm, we develop SafeEditor, a unified MLLM capable of multi-round safety editing on generated images. Experimental results show that SafeEditor surpasses prior safety approaches by reducing over-refusal while achieving a more favorable safety-utility balance.</article>","contentLength":1214,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Roadmap for Tamed Interactions with Large Language Models","url":"https://arxiv.org/abs/2510.24819","date":1761796800,"author":"","guid":321877,"unread":true,"content":"<article>arXiv:2510.24819v1 Announce Type: new \nAbstract: We are witnessing a bloom of AI-powered software driven by Large Language Models (LLMs). Although the applications of these LLMs are impressive and seemingly countless, their unreliability hinders adoption. In fact, the tendency of LLMs to produce faulty or hallucinated content makes them unsuitable for automating workflows and pipelines. In this regard, Software Engineering (SE) provides valuable support, offering a wide range of formal tools to specify, verify, and validate software behaviour. Such SE tools can be applied to define constraints over LLM outputs and, consequently, offer stronger guarantees on the generated content. In this paper, we argue that the development of a Domain Specific Language (DSL) for scripting interactions with LLMs using an LLM Scripting Language (LSL) may be key to improve AI-based applications. Currently, LLMs and LLM-based software still lack reliability, robustness, and trustworthiness, and the tools or frameworks to cope with these issues suffer from fragmentation. In this paper, we present our vision of LSL. With LSL, we aim to address the limitations above by exploring ways to control LLM outputs, enforce structure in interactions, and integrate these aspects with verification, validation, and explainability. Our goal is to make LLM interaction programmable and decoupled from training or implementation.</article>","contentLength":1413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards a Method for Synthetic Generation of PWA Transcripts","url":"https://arxiv.org/abs/2510.24817","date":1761796800,"author":"","guid":321878,"unread":true,"content":"<article>arXiv:2510.24817v1 Announce Type: new \nAbstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive time to manually coding speech samples using Correct Information Units (CIUs), a measure of how informative an individual sample of speech is. Developing automated systems to recognize aphasic language is limited by data scarcity. For example, only about 600 transcripts are available in AphasiaBank yet billions of tokens are used to train large language models (LLMs). In the broader field of machine learning (ML), researchers increasingly turn to synthetic data when such are sparse. Therefore, this study constructs and validates two methods to generate synthetic transcripts of the AphasiaBank Cat Rescue picture description task. One method leverages a procedural programming approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct LLMs. The methods generate transcripts across four severity levels (Mild, Moderate, Severe, Very Severe) through word dropping, filler insertion, and paraphasia substitution. Overall, we found, compared to human-elicited transcripts, Mistral 7b Instruct best captures key aspects of linguistic degradation observed in aphasia, showing realistic directional changes in NDW, word count, and word length amongst the synthetic generation methods. Based on the results, future work should plan to create a larger dataset, fine-tune models for better aphasic representation, and have SLPs assess the realism and usefulness of the synthetic transcripts.</article>","contentLength":1529,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perception, Understanding and Reasoning, A Multimodal Benchmark for Video Fake News Detection","url":"https://arxiv.org/abs/2510.24816","date":1761796800,"author":"","guid":321879,"unread":true,"content":"<article>arXiv:2510.24816v1 Announce Type: new \nAbstract: The advent of multi-modal large language models (MLLMs) has greatly advanced research into applications for Video fake news detection (VFND) tasks. Traditional video-based FND benchmarks typically focus on the accuracy of the final decision, often failing to provide fine-grained assessments for the entire detection process, making the detection process a black box. Therefore, we introduce the MVFNDB (Multi-modal Video Fake News Detection Benchmark) based on the empirical analysis, which provides foundation for tasks definition. The benchmark comprises 10 tasks and is meticulously crafted to probe MLLMs' perception, understanding, and reasoning capacities during detection, featuring 9730 human-annotated video-related questions based on a carefully constructed taxonomy ability of VFND. To validate the impact of combining multiple features on the final results, we design a novel framework named MVFND-CoT, which incorporates both creator-added content and original shooting footage reasoning. Building upon the benchmark, we conduct an in-depth analysis of the deeper factors influencing accuracy, including video processing strategies and the alignment between video features and model capabilities. We believe this benchmark will lay a solid foundation for future evaluations and advancements of MLLMs in the domain of video fake news detection.</article>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep Feature Optimization for Enhanced Fish Freshness Assessment","url":"https://arxiv.org/abs/2510.24814","date":1761796800,"author":"","guid":321880,"unread":true,"content":"<article>arXiv:2510.24814v1 Announce Type: new \nAbstract: Assessing fish freshness is vital for ensuring food safety and minimizing economic losses in the seafood industry. However, traditional sensory evaluation remains subjective, time-consuming, and inconsistent. Although recent advances in deep learning have automated visual freshness prediction, challenges related to accuracy and feature transparency persist. This study introduces a unified three-stage framework that refines and leverages deep visual representations for reliable fish freshness assessment. First, five state-of-the-art vision architectures - ResNet-50, DenseNet-121, EfficientNet-B0, ConvNeXt-Base, and Swin-Tiny - are fine-tuned to establish a strong baseline. Next, multi-level deep features extracted from these backbones are used to train seven classical machine learning classifiers, integrating deep and traditional decision mechanisms. Finally, feature selection methods based on Light Gradient Boosting Machine (LGBM), Random Forest, and Lasso identify a compact and informative subset of features. Experiments on the Freshness of the Fish Eyes (FFE) dataset demonstrate that the best configuration combining Swin-Tiny features, an Extra Trees classifier, and LGBM-based feature selection achieves an accuracy of 85.99%, outperforming recent studies on the same dataset by 8.69-22.78%. These findings confirm the effectiveness and generalizability of the proposed framework for visual quality evaluation tasks.</article>","contentLength":1486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DualCap: Enhancing Lightweight Image Captioning via Dual Retrieval with Similar Scenes Visual Prompts","url":"https://arxiv.org/abs/2510.24813","date":1761796800,"author":"","guid":321881,"unread":true,"content":"<article>arXiv:2510.24813v1 Announce Type: new \nAbstract: Recent lightweight retrieval-augmented image caption models often utilize retrieved data solely as text prompts, thereby creating a semantic gap by leaving the original visual features unenhanced, particularly for object details or complex scenes. To address this limitation, we propose $DualCap$, a novel approach that enriches the visual representation by generating a visual prompt from retrieved similar images. Our model employs a dual retrieval mechanism, using standard image-to-text retrieval for text prompts and a novel image-to-image retrieval to source visually analogous scenes. Specifically, salient keywords and phrases are derived from the captions of visually similar scenes to capture key objects and similar details. These textual features are then encoded and integrated with the original image features through a lightweight, trainable feature fusion network. Extensive experiments demonstrate that our method achieves competitive performance while requiring fewer trainable parameters compared to previous visual-prompting captioning approaches.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning","url":"https://arxiv.org/abs/2510.24812","date":1761796800,"author":"","guid":321882,"unread":true,"content":"<article>arXiv:2510.24812v1 Announce Type: new \nAbstract: Weak-to-strong generalization refers to the phenomenon where a stronger model trained under supervision from a weaker one can outperform its teacher. While prior studies aim to explain this effect, most theoretical insights are limited to abstract frameworks or linear/random feature models. In this paper, we provide a formal analysis of weak-to-strong generalization from a linear CNN (weak) to a two-layer ReLU CNN (strong). We consider structured data composed of label-dependent signals of varying difficulty and label-independent noise, and analyze gradient descent dynamics when the strong model is trained on data labeled by the pretrained weak model. Our analysis identifies two regimes -- data-scarce and data-abundant -- based on the signal-to-noise characteristics of the dataset, and reveals distinct mechanisms of weak-to-strong generalization. In the data-scarce regime, generalization occurs via benign overfitting or fails via harmful overfitting, depending on the amount of data, and we characterize the transition boundary. In the data-abundant regime, generalization emerges in the early phase through label correction, but we observe that overtraining can subsequently degrade performance.</article>","contentLength":1259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProofSketch: Efficient Verified Reasoning for Large Language Models","url":"https://arxiv.org/abs/2510.24811","date":1761796800,"author":"","guid":321883,"unread":true,"content":"<article>arXiv:2510.24811v1 Announce Type: new \nAbstract: Reasoning methods such as chain-of-thought prompting and self-consistency have shown immense potential to improve the accuracy of large language models across various reasoning tasks. However such methods involve generation of lengthy reasoning chains, which substantially increases token consumption, computational cost, and latency. To address this inefficiency, we propose ProofSketch, a verification-guided reasoning framework that integrates symbolic closure computation, lexicographic verification and adaptive sketch generation. Our experiments show that ProofSketch consistently reduces token usage while improving accuracy, demonstrating that this approach offers a promising path for efficient and trustworthy reasoning.</article>","contentLength":779,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"COMMUNITYNOTES: A Dataset for Exploring the Helpfulness of Fact-Checking Explanations","url":"https://arxiv.org/abs/2510.24810","date":1761796800,"author":"","guid":321884,"unread":true,"content":"<article>arXiv:2510.24810v1 Announce Type: new \nAbstract: Fact-checking on major platforms, such as X, Meta, and TikTok, is shifting from expert-driven verification to a community-based setup, where users contribute explanatory notes to clarify why a post might be misleading. An important challenge here is determining whether an explanation is helpful for understanding real-world claims and the reasons why, which remains largely underexplored in prior research. In practice, most community notes remain unpublished due to slow community annotation, and the reasons for helpfulness lack clear definitions. To bridge these gaps, we introduce the task of predicting both the helpfulness of explanatory notes and the reason for this. We present COMMUNITYNOTES, a large-scale multilingual dataset of 104k posts with user-provided notes and helpfulness labels. We further propose a framework that automatically generates and improves reason definitions via automatic prompt optimization, and integrate them into prediction. Our experiments show that the optimized definitions can improve both helpfulness and reason prediction. Finally, we show that the helpfulness information are beneficial for existing fact-checking systems.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Attack: Uncovering Privacy Risks in Sequential Data Releases","url":"https://arxiv.org/abs/2510.24807","date":1761796800,"author":"","guid":321885,"unread":true,"content":"<article>arXiv:2510.24807v1 Announce Type: new \nAbstract: Privacy concerns have become increasingly critical in modern AI and data science applications, where sensitive information is collected, analyzed, and shared across diverse domains such as healthcare, finance, and mobility. While prior research has focused on protecting privacy in a single data release, many real-world systems operate under sequential or continuous data publishing, where the same or related data are released over time. Such sequential disclosures introduce new vulnerabilities, as temporal correlations across releases may enable adversaries to infer sensitive information that remains hidden in any individual release. In this paper, we investigate whether an attacker can compromise privacy in sequential data releases by exploiting dependencies between consecutive publications, even when each individual release satisfies standard privacy guarantees. To this end, we propose a novel attack model that captures these sequential dependencies by integrating a Hidden Markov Model with a reinforcement learning-based bi-directional inference mechanism. This enables the attacker to leverage both earlier and later observations in the sequence to infer private information. We instantiate our framework in the context of trajectory data, demonstrating how an adversary can recover sensitive locations from sequential mobility datasets. Extensive experiments on Geolife, Porto Taxi, and SynMob datasets show that our model consistently outperforms baseline approaches that treat each release independently. The results reveal a fundamental privacy risk inherent to sequential data publishing, where individually protected releases can collectively leak sensitive information when analyzed temporally. These findings underscore the need for new privacy-preserving frameworks that explicitly model temporal dependencies, such as time-aware differential privacy or sequential data obfuscation strategies.</article>","contentLength":1969,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geometry Of The Subset Sum Problem -- Part I","url":"https://arxiv.org/abs/2510.24806","date":1761796800,"author":"","guid":321886,"unread":true,"content":"<article>arXiv:2510.24806v1 Announce Type: new \nAbstract: We announce two breakthrough results concerning important questions in the Theory of Computational Complexity. In this expository paper, a systematic and comprehensive geometric characterization of the Subset Sum Problem is presented. We show the existence of a universal geometric structure, comprised of a family of non-decreasing paths in the Cartesian plane, that captures any instance of the problem of size $n$. Inspired by the geometric structure, we provide an unconditional, deterministic and polynomial time algorithm, albeit with fairly high complexity, thereby showing that $\\mathcal{P} = \\mathcal{NP}$. Furthermore, our algorithm also outputs the number of solutions to the problem in polynomial time, thus leading to $\\mathcal{FP} = \\mathcal{\\#P}$. As a bonus, one important consequence of our results, out of many, is that the quantum-polynomial class $\\mathcal{BQP} \\subseteq \\mathcal{P}$.\n  Not only this, but we show that when multiple solutions exist, they can be placed in certain equivalence classes based on geometric attributes, and be compactly represented by a polynomial sized directed acyclic graph. We show that the Subset Sum Problem has two aspects, namely a combinatorial aspect and a relational aspect, and that it is the latter which is the primary determiner of complexity. We reveal a surprising connection between the size of the elements and their number, and the precise way in which they affect the complexity. In particular, we show that for all instances of the Subset Sum Problem, the complexity is independent of the size of elements, once the difference between consecutive elements exceeds $\\lceil{7\\log{}n}\\rceil$ bits in size.\n  We provide some numerical examples to illustrate the algorithm, and also show how it can be used to estimate some difficult combinatorial quantities such as the number of restricted partitions.</article>","contentLength":1918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conflict Adaptation in Vision-Language Models","url":"https://arxiv.org/abs/2510.24804","date":1761796800,"author":"","guid":321887,"unread":true,"content":"<article>arXiv:2510.24804v1 Announce Type: new \nAbstract: A signature of human cognitive control is conflict adaptation: improved performance on a high-conflict trial following another high-conflict trial. This phenomenon offers an account for how cognitive control, a scarce resource, is recruited. Using a sequential Stroop task, we find that 12 of 13 vision-language models (VLMs) tested exhibit behavior consistent with conflict adaptation, with the lone exception likely reflecting a ceiling effect. To understand the representational basis of this behavior, we use sparse autoencoders (SAEs) to identify task-relevant supernodes in InternVL 3.5 4B. Partially overlapping supernodes emerge for text and color in both early and late layers, and their relative sizes mirror the automaticity asymmetry between reading and color naming in humans. We further isolate a conflict-modulated supernode in layers 24-25 whose ablation significantly increases Stroop errors while minimally affecting congruent trials.</article>","contentLength":1001,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MASPRM: Multi-Agent System Process Reward Model","url":"https://arxiv.org/abs/2510.24803","date":1761796800,"author":"","guid":321888,"unread":true,"content":"<article>arXiv:2510.24803v1 Announce Type: new \nAbstract: Practical deployment of Multi-Agent Systems (MAS) demands strong test-time performance, motivating methods that guide inference-time search and selectively spend compute to improve quality. We present the Multi-Agent System Process Reward Model (MASPRM). It assigns per-action, per-agent values to partial inter-agent transcripts and acts as an inference-time controller. MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts without requiring step-level human annotations, by propagating returns to local targets. At inference, MASPRM guides step-level beam search and MCTS, focusing computation on promising branches and pruning early. On GSM8K and MATH, MASPRM-guided decoding with an outcome reward model (ORM) applied to the final answer, improves exact match (EM) over a single straight-through MAS pass by $+30.7$ and $+22.9$ points, respectively. A MASPRM trained on GSM8K transfers zero-shot to MATH without retraining, adding $8.4$ EM points at the same budget. MASPRM is a plug-in value model that estimates per-agent progress and complements verifier-style decoders, enabling more reliable, compute-aware multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM</article>","contentLength":1253,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Narrative to Action: A Hierarchical LLM-Agent Framework for Human Mobility Generation","url":"https://arxiv.org/abs/2510.24802","date":1761796800,"author":"","guid":321889,"unread":true,"content":"<article>arXiv:2510.24802v1 Announce Type: new \nAbstract: Understanding and replicating human mobility requires not only spatial-temporal accuracy but also an awareness of the cognitive hierarchy underlying real-world travel decisions. Traditional agent-based or deep learning models can reproduce statistical patterns of movement but fail to capture the semantic coherence and causal logic of human behavior. Large language models (LLMs) show potential, but struggle to balance creative reasoning with strict structural compliance. This study proposes a Hierarchical LLM-Agent Framework, termed Narrative-to-Action, that integrates high-level narrative reasoning, mid-level reflective planning, and low-level behavioral execution within a unified cognitive hierarchy. At the macro level, one agent is employed as a \"creative writer\" to produce diary-style narratives rich in motivation and context, then uses another agent as a \"structural parser\" to convert narratives into machine-readable plans. A dynamic execution module further grounds agents in geographic environments and enables adaptive behavioral adjustments guided by a novel occupation-aware metric, Mobility Entropy by Occupation (MEO), which captures heterogeneous schedule flexibility across different occupational personalities. At the micro level, the agent executes concrete actions-selecting locations, transportation modes, and time intervals-through interaction with an environmental simulation. By embedding this multi-layer cognitive process, the framework produces not only synthetic trajectories that align closely with real-world patterns but also interpretable representations of human decision logic. This research advances synthetic mobility generation from a data-driven paradigm to a cognition-driven simulation, providing a scalable pathway for understanding, predicting, and synthesizing complex urban mobility behaviors through hierarchical LLM agents.</article>","contentLength":1929,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fortytwo: Swarm Inference with Peer-Ranked Consensus","url":"https://arxiv.org/abs/2510.24801","date":1761796800,"author":"","guid":321890,"unread":true,"content":"<article>arXiv:2510.24801v1 Announce Type: new \nAbstract: As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security.</article>","contentLength":1938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering","url":"https://arxiv.org/abs/2510.24799","date":1761796800,"author":"","guid":321891,"unread":true,"content":"<article>arXiv:2510.24799v1 Announce Type: new \nAbstract: The rapid advancement of AI-assisted software engineering has brought transformative potential to the field of software engineering, but existing tools and paradigms remain limited by cognitive overload, inefficient tool integration, and the narrow capabilities of AI copilots. In response, we propose Compiler.next, a novel search-based compiler designed to enable the seamless evolution of AI-native software systems as part of the emerging Software Engineering 3.0 era. Unlike traditional static compilers, Compiler.next takes human-written intents and automatically generates working software by searching for an optimal solution. This process involves dynamic optimization of cognitive architectures and their constituents (e.g., prompts, foundation model configurations, and system parameters) while finding the optimal trade-off between several objectives, such as accuracy, cost, and latency. This paper outlines the architecture of Compiler.next and positions it as a cornerstone in democratizing software development by lowering the technical barrier for non-experts, enabling scalable, adaptable, and reliable AI-powered software. We present a roadmap to address the core challenges in intent compilation, including developing quality programming constructs, effective search heuristics, reproducibility, and interoperability between compilers. Our vision lays the groundwork for fully automated, search-driven software development, fostering faster innovation and more efficient AI-driven systems.</article>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Formal Verification of a Token Sale Launchpad: A Compositional Approach in Dafny","url":"https://arxiv.org/abs/2510.24798","date":1761796800,"author":"","guid":321892,"unread":true,"content":"<article>arXiv:2510.24798v1 Announce Type: new \nAbstract: The proliferation of decentralized financial (DeFi) systems and smart contracts has underscored the critical need for software correctness. Bugs in such systems can lead to catastrophic financial losses. Formal verification offers a path to achieving mathematical certainty about software behavior. This paper presents the formal verification of the core logic for a token sale launchpad, implemented and proven correct using the Dafny programming language and verification system. We detail a compositional, bottom-up verification strategy, beginning with the proof of fundamental non-linear integer arithmetic properties, and building upon them to verify complex business logic, including asset conversion, time-based discounts, and capped-sale refund mechanics. The principal contributions are the formal proofs of critical safety and lifecycle properties. Most notably, we prove that refunds in a capped sale can never exceed the user's original deposit amount, and that the precision loss in round-trip financial calculations is strictly bounded. Furthermore, we verify the complete lifecycle logic, including user withdrawals under various sale mechanics and the correctness of post-sale token allocation, vesting, and claiming. This work serves as a comprehensive case study in applying rigorous verification techniques to build high-assurance financial software.</article>","contentLength":1419,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models Report Subjective Experience Under Self-Referential Processing","url":"https://arxiv.org/abs/2510.24797","date":1761796800,"author":"","guid":321893,"unread":true,"content":"<article>arXiv:2510.24797v1 Announce Type: new \nAbstract: Large language models sometimes produce structured, first-person descriptions that explicitly reference awareness or subjective experience. To better understand this behavior, we investigate one theoretically motivated condition under which such reports arise: self-referential processing, a computational motif emphasized across major theories of consciousness. Through a series of controlled experiments on GPT, Claude, and Gemini model families, we test whether this regime reliably shifts models toward first-person reports of subjective experience, and how such claims behave under mechanistic and behavioral probes. Four main results emerge: (1) Inducing sustained self-reference through simple prompting consistently elicits structured subjective experience reports across model families. (2) These reports are mechanistically gated by interpretable sparse-autoencoder features associated with deception and roleplay: surprisingly, suppressing deception features sharply increases the frequency of experience claims, while amplifying them minimizes such claims. (3) Structured descriptions of the self-referential state converge statistically across model families in ways not observed in any control condition. (4) The induced state yields significantly richer introspection in downstream reasoning tasks where self-reflection is only indirectly afforded. While these findings do not constitute direct evidence of consciousness, they implicate self-referential processing as a minimal and reproducible condition under which large language models generate structured first-person reports that are mechanistically gated, semantically convergent, and behaviorally generalizable. The systematic emergence of this pattern across architectures makes it a first-order scientific and ethical priority for further investigation.</article>","contentLength":1876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mutual Wanting in Human--AI Interaction: Empirical Evidence from Large-Scale Analysis of GPT Model Transitions","url":"https://arxiv.org/abs/2510.24796","date":1761796800,"author":"","guid":321894,"unread":true,"content":"<article>arXiv:2510.24796v1 Announce Type: new \nAbstract: The rapid evolution of large language models (LLMs) creates complex bidirectional expectations between users and AI systems that are poorly understood. We introduce the concept of \"mutual wanting\" to analyze these expectations during major model transitions. Through analysis of user comments from major AI forums and controlled experiments across multiple OpenAI models, we provide the first large-scale empirical validation of bidirectional desire dynamics in human-AI interaction. Our findings reveal that nearly half of users employ anthropomorphic language, trust significantly exceeds betrayal language, and users cluster into distinct \"mutual wanting\" types. We identify measurable expectation violation patterns and quantify the expectation-reality gap following major model releases. Using advanced NLP techniques including dual-algorithm topic modeling and multi-dimensional feature extraction, we develop the Mutual Wanting Alignment Framework (M-WAF) with practical applications for proactive user experience management and AI system design. These findings establish mutual wanting as a measurable phenomenon with clear implications for building more trustworthy and relationally-aware AI systems.</article>","contentLength":1258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Efficient Vision-Language-Action Models","url":"https://arxiv.org/abs/2510.24795","date":1761796800,"author":"","guid":321895,"unread":true,"content":"<article>arXiv:2510.24795v1 Announce Type: new \nAbstract: Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/</article>","contentLength":1485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MR-Align: Meta-Reasoning Informed Factuality Alignment for Large Reasoning Models","url":"https://arxiv.org/abs/2510.24794","date":1761796800,"author":"","guid":321896,"unread":true,"content":"<article>arXiv:2510.24794v1 Announce Type: new \nAbstract: Large reasoning models (LRMs) show strong capabilities in complex reasoning, yet their marginal gains on evidence-dependent factual questions are limited. We find this limitation is partially attributable to a reasoning-answer hit gap, where the model identifies the correct facts during reasoning but fails to incorporate them into the final response, thereby reducing factual fidelity. To address this issue, we propose MR-ALIGN, a Meta-Reasoning informed alignment framework that enhances factuality without relying on external verifiers. MR-ALIGN quantifies state transition probabilities along the model's thinking process and constructs a transition-aware implicit reward that reinforces beneficial reasoning patterns while suppressing defective ones at the atomic thinking segments. This re-weighting reshapes token-level signals into probability-aware segment scores, encouraging coherent reasoning trajectories that are more conducive to factual correctness. Empirical evaluations across four factual QA datasets and one long-form factuality benchmark show that MR-ALIGN consistently improves accuracy and truthfulness while reducing misleading reasoning. These results highlight that aligning the reasoning process itself, rather than merely the outputs, is pivotal for advancing factuality in LRMs.</article>","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SwiftEmbed: Ultra-Fast Text Embeddings via Static Token Lookup for Real-Time Applications","url":"https://arxiv.org/abs/2510.24793","date":1761796800,"author":"","guid":321897,"unread":true,"content":"<article>arXiv:2510.24793v1 Announce Type: new \nAbstract: We present a static token lookup methodology for text embedding generation that achieves 1.12 ms p50 latency for single text embeddings while maintaining 60.6 MTEB average score across 8 representative tasks, corresponding to 89% of contextual model quality. The Rust implementation delivers 50,000 requests per second throughput through static embedding lookup, optimized mean pooling, and zero-copy IEEE754 binary serialization. Evaluation demonstrates exceptional duplicate detection performance (90.1% AP), strong semantic similarity (76.1% Spearman correlation), and domain-specific performance ranging from 75% to 131% of baseline across specialized domains. The system enables real-time embedding applications where sub-5ms latency is critical.</article>","contentLength":800,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models","url":"https://arxiv.org/abs/2510.24792","date":1761796800,"author":"","guid":321898,"unread":true,"content":"<article>arXiv:2510.24792v1 Announce Type: new \nAbstract: Vision-language models (VLMs) have demonstrated remarkable progress in multimodal reasoning. However, existing benchmarks remain limited in terms of high-quality, human-verified examples. Many current datasets rely on synthetically generated content by large language models (LLMs). Furthermore, most datasets are limited to English, as manual quality assurance of translated samples is time-consuming and costly. To fill this gap, we introduce PISA-Bench, a multilingual benchmark derived from English examples of the expert-created PISA tests, a unified framework for the assessment of student competencies in over eighty countries. Each example consists of human-extracted instructions, questions, answer options, and images, enriched with question type categories, and has been translated from English into five additional languages (Spanish, German, Chinese, French, and Italian), resulting in a fully parallel corpus covering six languages. We evaluate state-of-the-art vision-language models on PISA-Bench and find that especially small models (&lt;20B parameters) fail to achieve high test scores. We further find substantial performance degradation on non-English splits as well as high error-rates when models are tasked with spatial and geometric reasoning. By releasing the dataset and evaluation framework, we provide a resource for advancing research on multilingual multimodal reasoning.</article>","contentLength":1448,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Re-node Self-training Approach for Deep Graph-based Semi-supervised Classification on Multi-view Image Data","url":"https://arxiv.org/abs/2510.24791","date":1761796800,"author":"","guid":321899,"unread":true,"content":"<article>arXiv:2510.24791v1 Announce Type: new \nAbstract: Recently, graph-based semi-supervised learning and pseudo-labeling have gained attention due to their effectiveness in reducing the need for extensive data annotations. Pseudo-labeling uses predictions from unlabeled data to improve model training, while graph-based methods are characterized by processing data represented as graphs. However, the lack of clear graph structures in images combined with the complexity of multi-view data limits the efficiency of traditional and existing techniques. Moreover, the integration of graph structures in multi-view data is still a challenge. In this paper, we propose Re-node Self-taught Graph-based Semi-supervised Learning for Multi-view Data (RSGSLM). Our method addresses these challenges by (i) combining linear feature transformation and multi-view graph fusion within a Graph Convolutional Network (GCN) framework, (ii) dynamically incorporating pseudo-labels into the GCN loss function to improve classification in multi-view data, and (iii) correcting topological imbalances by adjusting the weights of labeled samples near class boundaries. Additionally, (iv) we introduce an unsupervised smoothing loss applicable to all samples. This combination optimizes performance while maintaining computational efficiency. Experimental results on multi-view benchmark image datasets demonstrate that RSGSLM surpasses existing semi-supervised learning approaches in multi-view contexts.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Lingual Summarization as a Black-Box Watermark Removal Attack","url":"https://arxiv.org/abs/2510.24789","date":1761796800,"author":"","guid":321900,"unread":true,"content":"<article>arXiv:2510.24789v1 Announce Type: new \nAbstract: Watermarking has been proposed as a lightweight mechanism to identify AI-generated text, with schemes typically relying on perturbations to token distributions. While prior work shows that paraphrasing can weaken such signals, these attacks remain partially detectable or degrade text quality. We demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a pivot language followed by summarization and optional back-translation -- constitute a qualitatively stronger attack vector. By forcing a semantic bottleneck across languages, CLSA systematically destroys token-level statistical biases while preserving semantic fidelity. In experiments across multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages (Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces watermark detection accuracy more effectively than monolingual paraphrase at similar quality levels. Our results highlight an underexplored vulnerability that challenges the practicality of watermarking for provenance or regulation. We argue that robust provenance solutions must move beyond distributional watermarking and incorporate cryptographic or model-attestation approaches. On 300 held-out samples per language, CLSA consistently drives detection toward chance while preserving task utility. Concretely, for XSIR (explicitly designed for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near chance). Results highlight a practical, low-cost removal pathway that crosses languages and compresses content without visible artifacts.</article>","contentLength":1770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Underappreciated Power of Vision Models for Graph Structural Understanding","url":"https://arxiv.org/abs/2510.24788","date":1761796800,"author":"","guid":321901,"unread":true,"content":"<article>arXiv:2510.24788v1 Announce Type: new \nAbstract: Graph Neural Networks operate through bottom-up message-passing, fundamentally differing from human visual perception, which intuitively captures global structures first. We investigate the underappreciated potential of vision models for graph understanding, finding they achieve performance comparable to GNNs on established benchmarks while exhibiting distinctly different learning patterns. These divergent behaviors, combined with limitations of existing benchmarks that conflate domain features with topological understanding, motivate our introduction of GraphAbstract. This benchmark evaluates models' ability to perceive global graph properties as humans do: recognizing organizational archetypes, detecting symmetry, sensing connectivity strength, and identifying critical elements. Our results reveal that vision models significantly outperform GNNs on tasks requiring holistic structural understanding and maintain generalizability across varying graph scales, while GNNs struggle with global pattern abstraction and degrade with increasing graph size. This work demonstrates that vision models possess remarkable yet underutilized capabilities for graph structural understanding, particularly for problems requiring global topological awareness and scale-invariant reasoning. These findings open new avenues to leverage this underappreciated potential for developing more effective graph foundation models for tasks dominated by holistic pattern recognition.</article>","contentLength":1519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality","url":"https://arxiv.org/abs/2510.24787","date":1761796800,"author":"","guid":321902,"unread":true,"content":"<article>arXiv:2510.24787v1 Announce Type: new \nAbstract: Photorealistic Codec Avatars (PCA), which generate high-fidelity human face renderings, are increasingly being used in Virtual Reality (VR) environments to enable immersive communication and interaction through deep learning-based generative models. However, these models impose significant computational demands, making real-time inference challenging on resource-constrained VR devices such as head-mounted displays, where latency and power efficiency are critical. To address this challenge, we propose an efficient post-training quantization (PTQ) method tailored for Codec Avatar models, enabling low-precision execution without compromising output quality. In addition, we design a custom hardware accelerator that can be integrated into the system-on-chip of VR devices to further enhance processing efficiency. Building on these components, we introduce ESCA, a full-stack optimization framework that accelerates PCA inference on edge VR platforms. Experimental results demonstrate that ESCA boosts FovVideoVDP quality scores by up to $+0.39$ over the best 4-bit baseline, delivers up to $3.36\\times$ latency reduction, and sustains a rendering rate of 100 frames per second in end-to-end tests, satisfying real-time VR requirements. These results demonstrate the feasibility of deploying high-fidelity codec avatars on resource-constrained devices, opening the door to more immersive and portable VR experiences.</article>","contentLength":1470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI & Data Competencies: Scaffolding holistic AI literacy in Higher Education","url":"https://arxiv.org/abs/2510.24783","date":1761796800,"author":"","guid":321903,"unread":true,"content":"<article>arXiv:2510.24783v1 Announce Type: new \nAbstract: This chapter introduces the AI &amp; Data Acumen Learning Outcomes Framework, a comprehensive tool designed to guide the integration of AI literacy across higher education. Developed through a collaborative process, the framework defines key AI and data-related competencies across four proficiency levels and seven knowledge dimensions. It provides a structured approach for educators to scaffold student learning in AI, balancing technical skills with ethical considerations and sociocultural awareness. The chapter outlines the framework's development process, its structure, and practical strategies for implementation in curriculum design, learning activities, and assessment. We address challenges in implementation and future directions for AI education. By offering a roadmap for developing students' holistic AI literacy, this framework prepares learners to leverage generative AI capabilities in both academic and professional contexts.</article>","contentLength":991,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FPGA-based Lane Detection System incorporating Temperature and Light Control Units","url":"https://arxiv.org/abs/2510.24778","date":1761796800,"author":"","guid":321904,"unread":true,"content":"<article>arXiv:2510.24778v1 Announce Type: new \nAbstract: Intelligent vehicles are one of the most important outcomes gained from the world tendency toward automation. Applications of IVs, whether in urban roads or robot tracks, do prioritize lane path detection. This paper proposes an FPGA-based Lane Detector Vehicle LDV architecture that relies on the Sobel algorithm for edge detection. Operating on 416 x 416 images and 150 MHz, the system can generate a valid output every 1.17 ms. The valid output consists of the number of present lanes, the current lane index, as well as its right and left boundaries. Additionally, the automated light and temperature control units in the proposed system enhance its adaptability to the surrounding environmental conditions.</article>","contentLength":760,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis","url":"https://arxiv.org/abs/2510.24777","date":1761796800,"author":"","guid":321905,"unread":true,"content":"<article>arXiv:2510.24777v1 Announce Type: new \nAbstract: Accurate diagnosis of Alzheimer's disease (AD) is essential for enabling timely intervention and slowing disease progression. Multimodal diagnostic approaches offer considerable promise by integrating complementary information across behavioral and perceptual domains. Eye-tracking and facial features, in particular, are important indicators of cognitive function, reflecting attentional distribution and neurocognitive state. However, few studies have explored their joint integration for auxiliary AD diagnosis. In this study, we propose a multimodal cross-enhanced fusion framework that synergistically leverages eye-tracking and facial features for AD detection. The framework incorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module (CEFAM), which models inter-modal interactions through cross-attention and global enhancement, and (b) a Direction-Aware Convolution Module (DACM), which captures fine-grained directional facial features via horizontal-vertical receptive fields. Together, these modules enable adaptive and discriminative multimodal representation learning. To support this work, we constructed a synchronized multimodal dataset, including 25 patients with AD and 25 healthy controls (HC), by recording aligned facial video and eye-tracking sequences during a visual memory-search paradigm, providing an ecologically valid resource for evaluating integration strategies. Extensive experiments on this dataset demonstrate that our framework outperforms traditional late fusion and feature concatenation methods, achieving a classification accuracy of 95.11% in distinguishing AD from HC, highlighting superior robustness and diagnostic performance by explicitly modeling inter-modal dependencies and modality-specific contributions.</article>","contentLength":1820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PANORAMA: A Dataset and Benchmarks Capturing Decision Trails and Rationales in Patent Examination","url":"https://arxiv.org/abs/2510.24774","date":1761796800,"author":"","guid":321906,"unread":true,"content":"<article>arXiv:2510.24774v1 Announce Type: new \nAbstract: Patent examination remains an ongoing challenge in the NLP literature even after the advent of large language models (LLMs), as it requires an extensive yet nuanced human judgment on whether a submitted claim meets the statutory standards of novelty and non-obviousness against previously granted claims -- prior art -- in expert domains. Previous NLP studies have approached this challenge as a prediction task (e.g., forecasting grant outcomes) with high-level proxies such as similarity metrics or classifiers trained on historical labels. However, this approach often overlooks the step-by-step evaluations that examiners must make with profound information, including rationales for the decisions provided in office actions documents, which also makes it harder to measure the current state of techniques in patent review processes. To fill this gap, we construct PANORAMA, a dataset of 8,143 U.S. patent examination records that preserves the full decision trails, including original applications, all cited references, Non-Final Rejections, and Notices of Allowance. Also, PANORAMA decomposes the trails into sequential benchmarks that emulate patent professionals' patent review processes and allow researchers to examine large language models' capabilities at each step of them. Our findings indicate that, although LLMs are relatively effective at retrieving relevant prior art and pinpointing the pertinent paragraphs, they struggle to assess the novelty and non-obviousness of patent claims. We discuss these results and argue that advancing NLP, including LLMs, in the patent domain requires a deeper understanding of real-world patent examination. Our dataset is openly available at https://huggingface.co/datasets/LG-AI-Research/PANORAMA.</article>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds","url":"https://arxiv.org/abs/2510.24773","date":1761796800,"author":"","guid":321907,"unread":true,"content":"<article>arXiv:2510.24773v1 Announce Type: new \nAbstract: Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) point clouds is essential for ensuring the accuracy and credibility of downstream applications such as 3D mapping, modeling, and change analysis. Traditional backward uncertainty modeling heavily rely on high-precision reference data, which are often costly or infeasible to obtain at large scales. To address this issue, this study proposes a machine learning-based framework for point-level uncertainty evaluation that learns the relationship between local geometric features and point-level errors. The framework is implemented using two ensemble learning models, Random Forest (RF) and XGBoost, which are trained and validated on a spatially partitioned real-world dataset to avoid data leakage. Experimental results demonstrate that both models can effectively capture the nonlinear relationships between geometric characteristics and uncertainty, achieving mean ROC-AUC values above 0.87. The analysis further reveals that geometric features describing elevation variation, point density, and local structural complexity play a dominant role in predicting uncertainty. The proposed framework offers a data-driven perspective of uncertainty evaluation, providing a scalable and adaptable foundation for future quality control and error analysis of large-scale point clouds.</article>","contentLength":1397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Confidence is Not Competence","url":"https://arxiv.org/abs/2510.24772","date":1761796800,"author":"","guid":321908,"unread":true,"content":"<article>arXiv:2510.24772v1 Announce Type: new \nAbstract: Large language models (LLMs) often exhibit a puzzling disconnect between their asserted confidence and actual problem-solving competence. We offer a mechanistic account of this decoupling by analyzing the geometry of internal states across two phases - pre-generative assessment and solution execution. A simple linear probe decodes the internal \"solvability belief\" of a model, revealing a well-ordered belief axis that generalizes across model families and across math, code, planning, and logic tasks. Yet, the geometries diverge - although belief is linearly decodable, the assessment manifold has high linear effective dimensionality as measured from the principal components, while the subsequent reasoning trace evolves on a much lower-dimensional manifold. This sharp reduction in geometric complexity from thought to action mechanistically explains the confidence-competence gap. Causal interventions that steer representations along the belief axis leave final solutions unchanged, indicating that linear nudges in the complex assessment space do not control the constrained dynamics of execution. We thus uncover a two-system architecture - a geometrically complex assessor feeding a geometrically simple executor. These results challenge the assumption that decodable beliefs are actionable levers, instead arguing for interventions that target the procedural dynamics of execution rather than the high-level geometry of assessment.</article>","contentLength":1493,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YTLive: A Dataset of Real-World YouTube Live Streaming Sessions","url":"https://arxiv.org/abs/2510.24769","date":1761796800,"author":"","guid":321909,"unread":true,"content":"<article>arXiv:2510.24769v1 Announce Type: new \nAbstract: Live streaming plays a major role in today's digital platforms, supporting entertainment, education, social media, etc. However, research in this field is limited by the lack of large, publicly available datasets that capture real-time viewer behavior at scale. To address this gap, we introduce YTLive, a public dataset focused on YouTube Live. Collected through the YouTube Researcher Program over May and June 2024, YTLive includes more than 507000 records from 12156 live streams, tracking concurrent viewer counts at five-minute intervals along with precise broadcast durations. We describe the dataset design and collection process and present an initial analysis of temporal viewing patterns. Results show that viewer counts are higher and more stable on weekends, especially during afternoon hours. Shorter streams attract larger and more consistent audiences, while longer streams tend to grow slowly and exhibit greater variability. These insights have direct implications for adaptive streaming, resource allocation, and Quality of Experience (QoE) modeling. YTLive offers a timely, open resource to support reproducible research and system-level innovation in live streaming. The dataset is publicly available at github.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Combining SAR Simulators to Train ATR Models with Synthetic Data","url":"https://arxiv.org/abs/2510.24768","date":1761796800,"author":"","guid":321910,"unread":true,"content":"<article>arXiv:2510.24768v1 Announce Type: new \nAbstract: This work aims to train Deep Learning models to perform Automatic Target Recognition (ATR) on Synthetic Aperture Radar (SAR) images. To circumvent the lack of real labelled measurements, we resort to synthetic data produced by SAR simulators. Simulation offers full control over the virtual environment, which enables us to generate large and diversified datasets at will. However, simulations are intrinsically grounded on simplifying assumptions of the real world (i.e. physical models). Thus, synthetic datasets are not as representative as real measurements. Consequently, ATR models trained on synthetic images cannot generalize well on real measurements. Our contributions to this problem are twofold: on one hand, we demonstrate and quantify the impact of the simulation paradigm on the ATR. On the other hand, we propose a new approach to tackle the ATR problem: combine two SAR simulators that are grounded on different (but complementary) paradigms to produce synthetic datasets. To this end, we use two simulators: MOCEM, which is based on a scattering centers model approach, and Salsa, which resorts on a ray tracing strategy. We train ATR models using synthetic dataset generated both by MOCEM and Salsa and our Deep Learning approach called ADASCA. We reach an accuracy of almost 88 % on the MSTAR measurements.</article>","contentLength":1375,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Fine-Grained Human Motion Video Captioning","url":"https://arxiv.org/abs/2510.24767","date":1761796800,"author":"","guid":321911,"unread":true,"content":"<article>arXiv:2510.24767v1 Announce Type: new \nAbstract: Generating accurate descriptions of human actions in videos remains a challenging task for video captioning models. Existing approaches often struggle to capture fine-grained motion details, resulting in vague or semantically inconsistent captions. In this work, we introduce the Motion-Augmented Caption Model (M-ACM), a novel generative framework that enhances caption quality by incorporating motion-aware decoding. At its core, M-ACM leverages motion representations derived from human mesh recovery to explicitly highlight human body dynamics, thereby reducing hallucinations and improving both semantic fidelity and spatial alignment in the generated captions. To support research in this area, we present the Human Motion Insight (HMI) Dataset, comprising 115K video-description pairs focused on human movement, along with HMI-Bench, a dedicated benchmark for evaluating motion-focused video captioning. Experimental results demonstrate that M-ACM significantly outperforms previous methods in accurately describing complex human motions and subtle temporal variations, setting a new standard for motion-centric video captioning.</article>","contentLength":1185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Topic-aware Large Language Models for Summarizing the Lived Healthcare Experiences Described in Health Stories","url":"https://arxiv.org/abs/2510.24765","date":1761796800,"author":"","guid":321912,"unread":true,"content":"<article>arXiv:2510.24765v1 Announce Type: new \nAbstract: Storytelling is a powerful form of communication and may provide insights into factors contributing to gaps in healthcare outcomes. To determine whether Large Language Models (LLMs) can identify potential underlying factors and avenues for intervention, we performed topic-aware hierarchical summarization of narratives from African American (AA) storytellers. Fifty transcribed stories of AA experiences were used to identify topics in their experience using the Latent Dirichlet Allocation (LDA) technique. Stories about a given topic were summarized using an open-source LLM-based hierarchical summarization approach. Topic summaries were generated by summarizing across story summaries for each story that addressed a given topic. Generated topic summaries were rated for fabrication, accuracy, comprehensiveness, and usefulness by the GPT4 model, and the model's reliability was validated against the original story summaries by two domain experts. 26 topics were identified in the fifty AA stories. The GPT4 ratings suggest that topic summaries were free from fabrication, highly accurate, comprehensive, and useful. The reliability of GPT ratings compared to expert assessments showed moderate to high agreement. Our approach identified AA experience-relevant topics such as health behaviors, interactions with medical team members, caregiving and symptom management, among others. Such insights could help researchers identify potential factors and interventions by learning from unstructured narratives in an efficient manner-leveraging the communicative power of storytelling. The use of LDA and LLMs to identify and summarize the experience of AA individuals suggests a variety of possible avenues for health research and possible clinical improvements to support patients and caregivers, thereby ultimately improving health outcomes.</article>","contentLength":1894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comparative Analysis of Procedural Planet Generators","url":"https://arxiv.org/abs/2510.24764","date":1761796800,"author":"","guid":321913,"unread":true,"content":"<article>arXiv:2510.24764v1 Announce Type: new \nAbstract: This paper presents the development of two distinct real-time procedural planet generators within the Godot engine: one employing Fractal Brownian Motion (FBM) with Perlin Noise, and another adapting Minecraft-inspired layered noise techniques. We detail their implementation, including a quadtree-based Level of Detail (LOD) system and solutions for planetary mesh generation. A comparative user study (N=15) was conducted where participants explored unique instances generated by our two algorithms alongside two existing procedural planet projects.</article>","contentLength":600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dual-Domain Deep Learning-Assisted NOMA-CSK Systems for Secure and Efficient Vehicular Communications","url":"https://arxiv.org/abs/2510.24763","date":1761796800,"author":"","guid":321914,"unread":true,"content":"<article>arXiv:2510.24763v1 Announce Type: new \nAbstract: Ensuring secure and efficient multi-user (MU) transmission is critical for vehicular communication systems. Chaos-based modulation schemes have garnered considerable interest due to their benefits in physical layer security. However, most existing MU chaotic communication systems, particularly those based on non-coherent detection, suffer from low spectral efficiency due to reference signal transmission, and limited user connectivity under orthogonal multiple access (OMA). While non-orthogonal schemes, such as sparse code multiple access (SCMA)-based DCSK, have been explored, they face high computational complexity and inflexible scalability due to their fixed codebook designs. This paper proposes a deep learning-assisted power domain non-orthogonal multiple access chaos shift keying (DL-NOMA-CSK) system for vehicular communications. A deep neural network (DNN)-based demodulator is designed to learn intrinsic chaotic signal characteristics during offline training, thereby eliminating the need for chaotic synchronization or reference signal transmission. The demodulator employs a dual-domain feature extraction architecture that jointly processes the time-domain and frequency-domain information of chaotic signals, enhancing feature learning under dynamic channels. The DNN is integrated into the successive interference cancellation (SIC) framework to mitigate error propagation issues. Theoretical analysis and extensive simulations demonstrate that the proposed system achieves superior performance in terms of spectral efficiency (SE), energy efficiency (EE), bit error rate (BER), security, and robustness, while maintaining lower computational complexity compared to traditional MU-DCSK and existing DL-aided schemes. These advantages validate its practical viability for secure vehicular communications.</article>","contentLength":1876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation","url":"https://arxiv.org/abs/2510.24762","date":1761796800,"author":"","guid":321915,"unread":true,"content":"<article>arXiv:2510.24762v1 Announce Type: new \nAbstract: We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese questions over 28 databases; 77% require multi-table reasoning and over half touch more than four tables. Each example is annotated along SQL-computation features and Chinese semantics. For evaluation, we release a robust execution comparator and an automated evaluation pipeline, under which all current state-of-the-art large-scale models (including Deepseek) achieve accuracies of at most 50%. Major errors originate from two sources: (1) schema linking in large enterprise landscapes - hundreds of tables, denormalized fields, ambiguous column names, implicit foreign-key relations and domain-specific synonyms that make correct join/column selection difficult; and (2) mapping concise, colloquial Chinese into the exact operators and predicates required for analytics - e.g., choosing the correct aggregation and group-by keys, expressing time windows and granularities, applying unit conversions, handling NULLs and data-quality rules, and formulating nested or windowed subqueries. Falcon therefore targets Chinese-specific semantics and enterprise dialects (abbreviations, business jargon, fuzzy entity references) and provides a reproducible middle ground before full production deployment by using realistic enterprise schemas, query templates, an execution comparator, and an automated evaluation pipeline for end-to-end validation.</article>","contentLength":1546,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ODataX: A Progressive Evolution of the Open Data Protocol","url":"https://arxiv.org/abs/2510.24761","date":1761796800,"author":"","guid":321916,"unread":true,"content":"<article>arXiv:2510.24761v1 Announce Type: new \nAbstract: The Open Data Protocol (OData) provides a standardized approach for building and consuming RESTful APIs with rich query capabilities. Despite its power and maturity, OData adoption remains confined primarily to enterprise environments, particularly within Microsoft and SAP ecosystems. This paper analyzes the key barriers preventing wider OData adoption and introduces ODataX, an evolved version of the protocol designed to address these limitations. ODataX maintains backward compatibility with OData v4 while introducing progressive complexity disclosure through simplified query syntax, built-in performance guardrails via query cost estimation, and enhanced caching mechanisms. This work aims to bridge the gap between enterprise-grade query standardization and the simplicity demanded by modern web development practices.</article>","contentLength":876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dingtalk DeepResearch: A Unified Multi Agent Framework for Adaptive Intelligence in Enterprise Environments","url":"https://arxiv.org/abs/2510.24760","date":1761796800,"author":"","guid":321917,"unread":true,"content":"<article>arXiv:2510.24760v1 Announce Type: new \nAbstract: We present Dingtalk DeepResearch, a unified multi agent intelligence framework for real world enterprise environments, delivering deep research, heterogeneous table reasoning, and multimodal report generation.</article>","contentLength":258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Digital Twin Framework for Decision-Support and Optimization of EV Charging Infrastructure in Localized Urban Systems","url":"https://arxiv.org/abs/2510.24758","date":1761796800,"author":"","guid":321918,"unread":true,"content":"<article>arXiv:2510.24758v1 Announce Type: new \nAbstract: As Electric Vehicle (EV) adoption accelerates in urban environments, optimizing charging infrastructure is vital for balancing user satisfaction, energy efficiency, and financial viability. This study advances beyond static models by proposing a digital twin framework that integrates agent-based decision support with embedded optimization to dynamically simulate EV charging behaviors, infrastructure layouts, and policy responses across scenarios. Applied to a localized urban site (a university campus) in Hanoi, Vietnam, the model evaluates operational policies, EV station configurations, and renewable energy sources. The interactive dashboard enables seasonal analysis, revealing a 20% drop in solar efficiency from October to March, with wind power contributing under 5% of demand, highlighting the need for adaptive energy management. Simulations show that real-time notifications of newly available charging slots improve user satisfaction, while gasoline bans and idle fees enhance slot turnover with minimal added complexity. Embedded metaheuristic optimization identifies near-optimal mixes of fast (30kW) and standard (11kW) solar-powered chargers, balancing energy performance, profitability, and demand with high computational efficiency. This digital twin provides a flexible, computation-driven platform for EV infrastructure planning, with a transferable, modular design that enables seamless scaling from localized to city-wide urban contexts.</article>","contentLength":1513,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stable-by-Design Neural Network-Based LPV State-Space Models for System Identification","url":"https://arxiv.org/abs/2510.24757","date":1761796800,"author":"","guid":321919,"unread":true,"content":"<article>arXiv:2510.24757v1 Announce Type: new \nAbstract: Accurate modeling of nonlinear systems is essential for reliable control, yet conventional identification methods often struggle to capture latent dynamics while maintaining stability. We propose a \\textit{stable-by-design LPV neural network-based state-space} (NN-SS) model that simultaneously learns latent states and internal scheduling variables directly from data. The state-transition matrix, generated by a neural network using the learned scheduling variables, is guaranteed to be stable through a Schur-based parameterization. The architecture combines an encoder for initial state estimation with a state-space representer network that constructs the full set of scheduling-dependent system matrices. For training the NN-SS, we develop a framework that integrates multi-step prediction losses with a state-consistency regularization term, ensuring robustness against drift and improving long-horizon prediction accuracy. The proposed NN-SS is evaluated on benchmark nonlinear systems, and the results demonstrate that the model consistently matches or surpasses classical subspace identification methods and recent gradient-based approaches. These findings highlight the potential of stability-constrained neural LPV identification as a scalable and reliable framework for modeling complex nonlinear systems.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Principal and Combination Parametric Resonances of an Electromagnetically Suspended Vehicle subject to Base Excitation","url":"https://arxiv.org/abs/2510.24756","date":1761796800,"author":"","guid":321920,"unread":true,"content":"<article>arXiv:2510.24756v1 Announce Type: new \nAbstract: This paper investigates the dynamic stability of an electromagnetically suspended vehicle, encountered in Hyperloop and Maglev systems, subject to periodic excitations caused by surface irregularities or vibration of the support induced by external noise. The narrow clearance between the vehicle and the support can make it highly sensitive to small oscillations, since the admissible amplitudes of the vehicle oscillations can be comparable to external excitation amplitude. The vehicle is modelled as a three-degree-of-freedom model where the vehicle is suspended via two identical electromagnetic actuators from a rigid support that oscillates. The governing equations are derived using force and torque balances, incorporating nonlinear electromagnetic forces, and Kirchhoffs law for the electromagnets with PD control strategy on the airgap. The equations of motion are linearized around the steady state induced by the surface oscillation, yielding a system with time-periodic coefficients. We analytically explore both principal and combination parametric resonances using an extended Hills method, and Floquet theory is used for numerical validation. The stability boundaries are obtained as ellipses in control gain parameter space, and the influence of system parameters on these boundaries is characterized. For the principal parametric resonance, the ratio of the sizes of the two obtained ellipses is three to one, whereas for the combination parametric resonance, the ratio is fourteen to one. When all ellipses are simultaneously present, one of the ellipses associated with the combination parametric resonance is the largest.</article>","contentLength":1692,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Function-Level Search: Repository-Aware Dual-Encoder Code Retrieval with Adversarial Verification","url":"https://arxiv.org/abs/2510.24749","date":1761796800,"author":"","guid":321921,"unread":true,"content":"<article>arXiv:2510.24749v1 Announce Type: new \nAbstract: The escalating complexity of modern codebases has intensified the need for retrieval systems capable of interpreting cross-component change intents, a capability fundamentally absent in conventional function-level search paradigms. While recent studies have improved the alignment between natural language queries and code snippets, retrieving contextually relevant code for specific change requests remains largely underexplored. To address this gap, we introduce RepoAlign-Bench, the first benchmark specifically designed to evaluate repository-level code retrieval under change request driven scenarios, encompassing 52k annotated instances. This benchmark shifts the retrieval paradigm from function-centric matching to holistic repository-level reasoning. Furthermore, we propose ReflectCode, an adversarial reflection augmented dual-tower architecture featuring disentangled code_encoder and doc_encoder components. ReflectCode dynamically integrates syntactic patterns, function dependencies, and semantic expansion intents through large language model guided reflection. Comprehensive experiments demonstrate that ReflectCode achieves 12.2% improvement in Top-5 Accuracy and 7.1% in Recall over state-of-the-art baselines, establishing a new direction for context-aware code retrieval.</article>","contentLength":1342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human- vs. AI-generated tests: dimensionality and information accuracy in latent trait evaluation","url":"https://arxiv.org/abs/2510.24739","date":1761796800,"author":"","guid":321922,"unread":true,"content":"<article>arXiv:2510.24739v1 Announce Type: new \nAbstract: Artificial Intelligence (AI) and large language models (LLMs) are increasingly used in social and psychological research. Among potential applications, LLMs can be used to generate, customise, or adapt measurement instruments. This study presents a preliminary investigation of AI-generated questionnaires by comparing two ChatGPT-based adaptations of the Body Awareness Questionnaire (BAQ) with the validated human-developed version. The AI instruments were designed with different levels of explicitness in content and instructions on construct facets, and their psychometric properties were assessed using a Bayesian Graded Response Model. Results show that although surface wording between AI and original items was similar, differences emerged in dimensionality and in the distribution of item and test information across latent traits. These findings illustrate the importance of applying statistical measures of accuracy to ensure the validity and interpretability of AI-driven tools.</article>","contentLength":1040,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DrivingScene: A Multi-Task Online Feed-Forward 3D Gaussian Splatting Method for Dynamic Driving Scenes","url":"https://arxiv.org/abs/2510.24734","date":1761796800,"author":"","guid":321923,"unread":true,"content":"<article>arXiv:2510.24734v1 Announce Type: new \nAbstract: Real-time, high-fidelity reconstruction of dynamic driving scenes is challenged by complex dynamics and sparse views, with prior methods struggling to balance quality and efficiency. We propose DrivingScene, an online, feed-forward framework that reconstructs 4D dynamic scenes from only two consecutive surround-view images. Our key innovation is a lightweight residual flow network that predicts the non-rigid motion of dynamic objects per camera on top of a learned static scene prior, explicitly modeling dynamics via scene flow. We also introduce a coarse-to-fine training paradigm that circumvents the instabilities common to end-to-end approaches. Experiments on nuScenes dataset show our image-only method simultaneously generates high-quality depth, scene flow, and 3D Gaussian point clouds online, significantly outperforming state-of-the-art methods in both dynamic reconstruction and novel view synthesis.</article>","contentLength":966,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Constructive Lyapunov Functions via Topology-Preserving Neural Networks","url":"https://arxiv.org/abs/2510.24730","date":1761796800,"author":"","guid":321924,"unread":true,"content":"<article>arXiv:2510.24730v1 Announce Type: new \nAbstract: We prove that ONN achieves order-optimal performance on convergence rate ($\\mu \\propto \\lambda_2$), edge efficiency ($E = N$ for minimal connectivity $k = 2$), and computational complexity ($O(N d^2)$). Empirical validation on 3M-node semantic networks demonstrates 99.75\\% improvement over baseline methods, confirming exponential convergence ($\\mu = 3.2 \\times 10^{-4}$) and topology preservation. ORTSF integration into transformers achieves 14.7\\% perplexity reduction and 2.3 faster convergence on WikiText-103. We establish deep connections to optimal control (Hamilton-Jacobi-Bellman), information geometry (Fisher-efficient natural gradient), topological data analysis (persistent homology computation in $O(KN)$), discrete geometry (Ricci flow), and category theory (adjoint functors). This work transforms Massera's abstract existence theorem into a concrete, scalable algorithm with provable guarantees, opening pathways for constructive stability analysis in neural networks, robotics, and distributed systems.</article>","contentLength":1071,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Models: A Framework for Contextual and Cultural Intelligence in African AI Deployment","url":"https://arxiv.org/abs/2510.24729","date":1761796800,"author":"","guid":321925,"unread":true,"content":"<article>arXiv:2510.24729v1 Announce Type: new \nAbstract: While global AI development prioritizes model performance and computational scale, meaningful deployment in African markets requires fundamentally different architectural decisions. This paper introduces Contextual and Cultural Intelligence (CCI) -- a systematic framework enabling AI systems to process cultural meaning, not just data patterns, through locally relevant, emotionally intelligent, and economically inclusive design. Using design science methodology, we validate CCI through a production AI-native cross-border shopping platform serving diaspora communities. Key empirical findings: 89% of users prefer WhatsApp-based AI interaction over traditional web interfaces (n=602, chi-square=365.8, p&lt;0.001), achieving 536 WhatsApp users and 3,938 total conversations across 602 unique users in just 6 weeks, and culturally informed prompt engineering demonstrates sophisticated understanding of culturally contextualized queries, with 89% family-focused commerce patterns and natural code-switching acceptance. The CCI framework operationalizes three technical pillars: Infrastructure Intelligence (mobile-first, resilient architectures), Cultural Intelligence (multilingual NLP with social context awareness), and Commercial Intelligence (trust-based conversational commerce). This work contributes both theoretical innovation and reproducible implementation patterns, challenging Silicon Valley design orthodoxies while providing actionable frameworks for equitable AI deployment across resource-constrained markets.</article>","contentLength":1575,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stiff Circuit System Modeling via Transformer","url":"https://arxiv.org/abs/2510.24727","date":1761796800,"author":"","guid":321926,"unread":true,"content":"<article>arXiv:2510.24727v1 Announce Type: new \nAbstract: Accurate and efficient circuit behavior modeling is a cornerstone of modern electronic design automation. Among different types of circuits, stiff circuits are challenging to model using previous frameworks. In this work, we propose a new approach using Crossformer, which is a current state-of-the-art Transformer model for time-series prediction tasks, combined with Kolmogorov-Arnold Networks (KANs), to model stiff circuit transient behavior. By leveraging the Crossformer's temporal representation capabilities and the enhanced feature extraction of KANs, our method achieves improved fidelity in predicting circuit responses to a wide range of input conditions. Experimental evaluations on datasets generated through SPICE simulations of analog-to-digital converter (ADC) circuits demonstrate the effectiveness of our approach, with significant reductions in training time and error rates.</article>","contentLength":944,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AmarDoctor: An AI-Driven, Multilingual, Voice-Interactive Digital Health Application for Primary Care Triage and Patient Management to Bridge the Digital Health Divide for Bengali Speakers","url":"https://arxiv.org/abs/2510.24724","date":1761796800,"author":"","guid":321927,"unread":true,"content":"<article>arXiv:2510.24724v1 Announce Type: new \nAbstract: This study presents AmarDoctor, a multilingual voice-interactive digital health app designed to provide comprehensive patient triage and AI-driven clinical decision support for Bengali speakers, a population largely underserved in access to digital healthcare. AmarDoctor adopts a data-driven approach to strengthen primary care delivery and enable personalized health management. While platforms such as AdaHealth, WebMD, Symptomate, and K-Health have become popular in recent years, they mainly serve European demographics and languages. AmarDoctor addresses this gap with a dual-interface system for both patients and healthcare providers, supporting three major Bengali dialects. At its core, the patient module uses an adaptive questioning algorithm to assess symptoms and guide users toward the appropriate specialist. To overcome digital literacy barriers, it integrates a voice-interactive AI assistant that navigates users through the app services. Complementing this, the clinician-facing interface incorporates AI-powered decision support that enhances workflow efficiency by generating structured provisional diagnoses and treatment recommendations. These outputs inform key services such as e-prescriptions, video consultations, and medical record management. To validate clinical accuracy, the system was evaluated against a gold-standard set of 185 clinical vignettes developed by experienced physicians. Effectiveness was further assessed by comparing AmarDoctor performance with five independent physicians using the same vignette set. Results showed AmarDoctor achieved a top-1 diagnostic precision of 81.08 percent (versus physicians average of 50.27 percent) and a top specialty recommendation precision of 91.35 percent (versus physicians average of 62.6 percent).</article>","contentLength":1834,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Blockage-Aware Multi-RIS WSR Maximization via Per-RIS Indexed Synchronization Sequences and Closed-Form Riemannian Updates","url":"https://arxiv.org/abs/2510.24723","date":1761796800,"author":"","guid":321928,"unread":true,"content":"<article>arXiv:2510.24723v1 Announce Type: new \nAbstract: Millimeter-wave (mmWave) multi-user MIMO systems are highly vulnerable to blockage, and reconfigurable intelligent surfaces (RIS) have been proposed as a remedy. However, RIS links may themselves be blocked, while most prior works assume ideal RIS availability. We propose an end-to-end blockage-aware multi-RIS weighted sum-rate (WSR) optimization framework. The BS transmits short per-RIS indexed synchronization signals, enabling each user to identify blocked panels through a simple energy detection test. Based on the detected feasible sets, we jointly optimize the BS precoder and RIS phases via a Closed-form Riemannian Phase Alignment (CRPA) algorithm. CRPA provides unit-modulus-preserving closed-form updates, requiring no projection or line search, and ensures monotone ascent. Simulations validate reliable blockage detection and notable WSR and convergence gains over existing baselines.</article>","contentLength":949,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Epistemic Suite: A Post-Foundational Diagnostic Methodology for Assessing AI Knowledge Claims","url":"https://arxiv.org/abs/2510.24721","date":1761796800,"author":"","guid":321929,"unread":true,"content":"<article>arXiv:2510.24721v1 Announce Type: new \nAbstract: Large Language Models (LLMs) generate fluent, plausible text that can mislead users into mistaking simulated coherence for genuine understanding. This paper introduces the Epistemic Suite, a post-foundational diagnostic methodology for surfacing the epistemic conditions under which AI outputs are produced and received. Rather than determining truth or falsity, the Suite operates through twenty diagnostic lenses, applied by practitioners as context warrants, to reveal patterns such as confidence laundering, narrative compression, displaced authority, and temporal drift. It is grounded in three design principles: diagnosing production before evaluating claims, preferring diagnostic traction over foundational settlement, and embedding reflexivity as a structural requirement rather than an ethical ornament. When enacted, the Suite shifts language models into a diagnostic stance, producing inspectable artifacts-flags, annotations, contradiction maps, and suspension logs (the FACS bundle)-that create an intermediary layer between AI output and human judgment. A key innovation is epistemic suspension, a practitioner-enacted circuit breaker that halts continuation when warrant is exceeded, with resumption based on judgment rather than rule. The methodology also includes an Epistemic Triage Protocol and a Meta-Governance Layer to manage proportionality and link activation to relational accountability, consent, historical context, and pluralism safeguards. Unlike internalist approaches that embed alignment into model architectures (e.g., RLHF or epistemic-integrity proposals), the Suite operates externally as scaffolding, preserving expendability and refusal as safeguards rather than failures. It preserves the distinction between performance and understanding, enabling accountable deliberation while maintaining epistemic modesty.</article>","contentLength":1900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modelling the Interplay of Eye-Tracking Temporal Dynamics and Personality for Emotion Detection in Face-to-Face Settings","url":"https://arxiv.org/abs/2510.24720","date":1761796800,"author":"","guid":321930,"unread":true,"content":"<article>arXiv:2510.24720v1 Announce Type: new \nAbstract: Accurate recognition of human emotions is critical for adaptive human-computer interaction, yet remains challenging in dynamic, conversation-like settings. This work presents a personality-aware multimodal framework that integrates eye-tracking sequences, Big Five personality traits, and contextual stimulus cues to predict both perceived and felt emotions. Seventy-three participants viewed speech-containing clips from the CREMA-D dataset while providing eye-tracking signals, personality assessments, and emotion ratings. Our neural models captured temporal gaze dynamics and fused them with trait and stimulus information, yielding consistent gains over SVM and literature baselines. Results show that (i) stimulus cues strongly enhance perceived-emotion predictions (macro F1 up to 0.77), while (ii) personality traits provide the largest improvements for felt emotion recognition (macro F1 up to 0.58). These findings highlight the benefit of combining physiological, trait-level, and contextual information to address the inherent subjectivity of emotion. By distinguishing between perceived and felt responses, our approach advances multimodal affective computing and points toward more personalized and ecologically valid emotion-aware systems.</article>","contentLength":1303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Iti-Validator: A Guardrail Framework for Validating and Correcting LLM-Generated Itineraries","url":"https://arxiv.org/abs/2510.24719","date":1761796800,"author":"","guid":321931,"unread":true,"content":"<article>arXiv:2510.24719v1 Announce Type: new \nAbstract: The rapid advancement of Large Language Models (LLMs) has enabled them to generate complex, multi-step plans and itineraries. However, these generated plans often lack temporal and spatial consistency, particularly in scenarios involving physical travel constraints. This research aims to study the temporal performance of different LLMs and presents a validation framework that evaluates and improves the temporal consistency of LLM-generated travel itineraries. The system employs multiple state-of-the-art LLMs to generate travel plans and validates them against real-world flight duration constraints using the AeroDataBox API. This work contributes to the understanding of LLM capabilities in handling complex temporal reasoning tasks like itinerary generation and provides a framework to rectify any temporal inconsistencies like overlapping journeys or unrealistic transit times in the itineraries generated by LLMs before the itinerary is given to the user. Our experiments reveal that while current LLMs frequently produce temporally inconsistent itineraries, these can be systematically and reliably corrected using our framework, enabling their practical deployment in large-scale travel planning.</article>","contentLength":1257,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A linear, unconditionally stable, second order decoupled method for the Ericksen-Leslie model with SAV approach","url":"https://arxiv.org/abs/2503.19424","date":1761796800,"author":"","guid":322448,"unread":true,"content":"<article>arXiv:2503.19424v3 Announce Type: replace-cross \nAbstract: In this paper, we present a second order, linear, fully decoupled, and unconditionally energy stable scheme for solving the Erickson-Leslie model. This approach integrates the pressure correction method with a scalar auxiliary variable technique. We rigorously demonstrate the unconditional energy stability of the proposed scheme. Furthermore, we present several numerical experiments to validate its convergence order, stability, and computational efficiency.</article>","contentLength":520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coding for Ordered Composite DNA Sequences","url":"https://arxiv.org/abs/2509.26119","date":1761796800,"author":"","guid":322449,"unread":true,"content":"<article>arXiv:2509.26119v3 Announce Type: replace \nAbstract: To increase the information capacity of DNA storage, composite DNA letters were introduced. We propose a novel channel model for composite DNA in which composite sequences are decomposed into ordered standard non-composite sequences. The model is designed to handle any alphabet size and composite resolution parameter. We study the problem of reconstructing composite sequences of arbitrary resolution over the binary alphabet under substitution errors. We define two families of error-correcting codes and provide lower and upper bounds on their cardinality. In addition, we analyze the case in which a single deletion error occurs in the channel and present a systematic code construction for this setting. Finally, we briefly discuss the channel's capacity, which remains an open problem.</article>","contentLength":845,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion","url":"https://arxiv.org/abs/2509.17302","date":1761796800,"author":"","guid":322450,"unread":true,"content":"<article>arXiv:2509.17302v3 Announce Type: replace \nAbstract: Text embedding inversion attacks reconstruct original sentences from latent representations, posing severe privacy threats in collaborative inference and edge computing. We propose TextCrafter, an optimization-based adversarial perturbation mechanism that combines RL learned, geometry aware noise injection orthogonal to user embeddings with cluster priors and PII signal guidance to suppress inversion while preserving task utility. Unlike prior defenses either non learnable or agnostic to perturbation direction, TextCrafter provides a directional protective policy that balances privacy and utility. Under strong privacy setting, TextCrafter maintains 70 percentage classification accuracy on four datasets and consistently outperforms Gaussian/LDP baselines across lower privacy budgets, demonstrating a superior privacy utility trade off.</article>","contentLength":898,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collab-REC: An LLM-based Agentic Framework for Balancing Recommendations in Tourism","url":"https://arxiv.org/abs/2508.15030","date":1761796800,"author":"","guid":322451,"unread":true,"content":"<article>arXiv:2508.15030v3 Announce Type: replace \nAbstract: We propose Collab-REC, a multi-agent framework designed to counteract popularity bias and enhance diversity in tourism recommendations. In our setting, three LLM-based agents -- Personalization, Popularity, and Sustainability generate city suggestions from complementary perspectives. A non-LLM moderator then merges and refines these proposals via multi-round negotiation, ensuring each agent's viewpoint is incorporated while penalizing spurious or repeated responses. Experiments on European city queries show that Collab-REC improves diversity and overall relevance compared to a single-agent baseline, surfacing lesser-visited locales that often remain overlooked. This balanced, context-aware approach addresses over-tourism and better aligns with constraints provided by the user, highlighting the promise of multi-stakeholder collaboration in LLM-driven recommender systems.</article>","contentLength":935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems","url":"https://arxiv.org/abs/2505.15201","date":1761796800,"author":"","guid":322452,"unread":true,"content":"<article>arXiv:2505.15201v3 Announce Type: replace \nAbstract: Reinforcement Learning (RL) algorithms sample multiple n&gt;1 solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes the strength of isolated samples at the expense of the diversity and collective utility of sets of samples. This under-utilizes the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a transformation on the final rewards which leads to direct optimization of pass@k performance, thus optimizing for sets of samples that maximize reward when considered jointly. Our contribution is to derive novel low variance unbiased estimators for pass@k and its gradient, in both the binary and continuous reward settings. We show optimization with our estimators reduces to standard RL with rewards that have been jointly transformed by a stable and efficient transformation function.\n  While previous efforts are restricted to k=n, ours is the first to enable robust optimization of pass@k for any arbitrary k &lt;= n. Moreover, instead of trading off pass@1 performance for pass@k gains, our method allows annealing k during training, optimizing both metrics and often achieving strong pass@1 numbers alongside significant pass@k gains.\n  We validate our reward transformations on toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source LLM, GEMMA-2. We find that our transformation effectively optimizes for the target k. Furthermore, higher k values enable solving more and harder problems, while annealing k boosts both the pass@1 and pass@k . Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely due to better exploration by prioritizing joint utility over the utility of individual samples.</article>","contentLength":1970,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Open3D-VQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space","url":"https://arxiv.org/abs/2503.11094","date":1761796800,"author":"","guid":322453,"unread":true,"content":"<article>arXiv:2503.11094v4 Announce Type: replace \nAbstract: Spatial reasoning is a fundamental capability of multimodal large language models (MLLMs), yet their performance in open aerial environments remains underexplored. In this work, we present Open3D-VQA, a novel benchmark for evaluating MLLMs' ability to reason about complex spatial relationships from an aerial perspective. The benchmark comprises 73k QA pairs spanning 7 general spatial reasoning tasks, including multiple-choice, true/false, and short-answer formats, and supports both visual and point cloud modalities. The questions are automatically generated from spatial relations extracted from both real-world and simulated aerial scenes. Evaluation on 13 popular MLLMs reveals that: 1) Models are generally better at answering questions about relative spatial relations than absolute distances, 2) 3D LLMs fail to demonstrate significant advantages over 2D LLMs, and 3) Fine-tuning solely on the simulated dataset can significantly improve the model's spatial reasoning performance in real-world scenarios. We release our benchmark, data generation pipeline, and evaluation toolkit to support further research: https://github.com/EmbodiedCity/Open3D-VQA.code.</article>","contentLength":1221,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Driven Stabilization Using Prior Knowledge on Stabilizability and Controllability","url":"https://arxiv.org/abs/2510.25452","date":1761796800,"author":"","guid":322454,"unread":true,"content":"<article>arXiv:2510.25452v2 Announce Type: replace-cross \nAbstract: In this work, we study data-driven stabilization of linear time-invariant systems using prior knowledge of system-theoretic properties, specifically stabilizability and controllability. To formalize this, we extend the concept of data informativity by requiring the existence of a controller that stabilizes all systems consistent with the data and the prior knowledge. We show that if the system is controllable, then incorporating this as prior knowledge does not relax the conditions required for data-driven stabilization. Remarkably, however, we show that if the system is stabilizable, then using this as prior knowledge leads to necessary and sufficient conditions that are weaker than those for data-driven stabilization without prior knowledge. In other words, data-driven stabilization is easier if one knows that the underlying system is stabilizable. We also provide new data-driven control design methods in terms of linear matrix inequalities that complement the conditions for informativity.</article>","contentLength":1065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Completion $\\neq$ Collaboration: Scaling Collaborative Effort with Agents","url":"https://arxiv.org/abs/2510.25744","date":1761796800,"author":"","guid":322455,"unread":true,"content":"<article>arXiv:2510.25744v2 Announce Type: replace \nAbstract: Current evaluations of agents remain centered around one-shot task completion, failing to account for the inherently iterative and collaborative nature of many real-world problems, where human goals are often underspecified and evolve. We argue for a shift from building and assessing task completion agents to developing collaborative agents, assessed not only by the quality of their final outputs but by how well they engage with and enhance human effort throughout the problem-solving process. To support this shift, we introduce collaborative effort scaling, a framework that captures how an agent's utility grows with increasing user involvement. Through case studies and simulated evaluations, we show that state-of-the-art agents often underperform in multi-turn, real-world scenarios, revealing a missing ingredient in agent design: the ability to sustain engagement and scaffold user understanding. Collaborative effort scaling offers a lens for diagnosing agent behavior and guiding development toward more effective interactions.</article>","contentLength":1094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fourier Neural Operators for Two-Phase, 2D Mold-Filling Problems Related to Metal Casting","url":"https://arxiv.org/abs/2510.25697","date":1761796800,"author":"","guid":322456,"unread":true,"content":"<article>arXiv:2510.25697v2 Announce Type: replace \nAbstract: We formulate mold filling in metal casting as a 2D neural operator learning problem that maps geometry and boundary data on an unstructured mesh to time resolved flow quantities, replacing expensive transient CFD. In the proposed method, a graph based encoder aggregates local neighborhood information on the input mesh and encodes geometry and boundary data, a Fourier spectral core operates on a regular latent grid to capture global interactions across the domain, and a graph based decoder projects the latent fields to a target mesh. The model is trained to jointly predict velocity components, pressure, and liquid volume fraction over a fixed rollout horizon and generalizes across different ingate locations and process settings. On held out geometries and inlet conditions, it reproduces large scale advection and the fluid-air interface evolution with localized errors near steep gradients. The mean relative L2 error is about 5% across all fields, and inference is two to three orders of magnitude faster than conventional CFD, enabling design in the loop exploration. Ablation studies show monotonic accuracy degradation under stronger spatial subsampling of input vertices and a smoother decline under temporal subsampling. Halving the training set yields only a small increase in error. These results establish neural operators as accurate and data efficient surrogates for 2D mold filling and enable rapid optimization of gating systems in casting workflows.</article>","contentLength":1526,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PairUni: Pairwise Training for Unified Multimodal Language Models","url":"https://arxiv.org/abs/2510.25682","date":1761796800,"author":"","guid":322457,"unread":true,"content":"<article>arXiv:2510.25682v2 Announce Type: replace \nAbstract: Unified vision-language models (UVLMs) must perform both understanding and generation within a single architecture, but these tasks rely on heterogeneous data and supervision, making it difficult to balance them during reinforcement learning (RL). We propose PairUni, a unified framework that reorganizes data into understanding-generation (UG) pairs and aligns optimization accordingly. We first use GPT-o3 to augment single-task data, generating captions for understanding samples and question-answer (QA) pairs for generation samples, forming aligned pairs from the same instance. Additionally, for each generation sample, we retrieve a semantically related understanding example to form a retrieved pair, linking different but related data points. These paired structures expose cross-task semantic correspondences and support consistent policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware variant based on Group Relative Policy Optimization. It assigns a similarity score to each pair to modulate the advantage, strengthening learning from well-aligned examples and reducing task interference. We curate a high-quality dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on various UVLMs, outperforming strong UVLM RL baselines. Codes are available at https://github.com/Haochen-Wang409/PairUni.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating the Role of Verifiers in Test-Time Scaling for Legal Reasoning Tasks","url":"https://arxiv.org/abs/2510.25623","date":1761796800,"author":"","guid":322458,"unread":true,"content":"<article>arXiv:2510.25623v2 Announce Type: replace \nAbstract: Test-time scaling (TTS) techniques can improve the performance of large language models (LLMs) at the expense of additional computation and latency. While TTS has proven effective in formal domains such as mathematics and programming, its value in argumentative domains such as law remains underexplored. We present an empirical study of verifier-based TTS methods for legal multiple-choice QA (MCQA) across five benchmarks. Using a family of 7 reward models, we evaluate both outcome-level (Best-of-$N$) and process-level (tree search) verification under realistic low-$N$ budgets. Our analysis systematically investigates how verifier utility is affected by key properties such as domain specialization, model size, and supervision type (process-supervised PRMs vs. outcome-only ORMs), even when applied across different roles.</article>","contentLength":882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMQ-v2: Align, Denoise, and Amplify: Adaptive Behavior Mining for Semantic IDs Learning in Recommendation","url":"https://arxiv.org/abs/2510.25622","date":1761796800,"author":"","guid":322459,"unread":true,"content":"<article>arXiv:2510.25622v2 Announce Type: replace \nAbstract: Industrial recommender systems rely on unique Item Identifiers (ItemIDs). However, this method struggles with scalability and generalization in large, dynamic datasets that have sparse long-tail data. Content-based Semantic IDs (SIDs) address this by sharing knowledge through content quantization. However, by ignoring dynamic behavioral properties, purely content-based SIDs have limited expressive power. Existing methods attempt to incorporate behavioral information but overlook a critical distinction: unlike relatively uniform content features, user-item interactions are highly skewed and diverse, creating a vast information gap in quality and quantity between popular and long-tail items. This oversight leads to two critical limitations: (1) Noise Corruption: Indiscriminate behavior-content alignment allows collaborative noise from long-tail items to corrupt their content representations, leading to the loss of critical multimodal information. (2)Signal Obscurity: The equal-weighting scheme for SIDs fails to reflect the varying importance of different behavioral signals, making it difficult for downstream tasks to distinguish important SIDs from uninformative ones. To tackle these issues, we propose a mixture-of-quantization framework, MMQ-v2, to adaptively Align, Denoise, and Amplify multimodal information from content and behavior modalities for semantic IDs learning. The semantic IDs generated by this framework named ADA-SID. It introduces two innovations: an adaptive behavior-content alignment that is aware of information richness to shield representations from noise, and a dynamic behavioral router to amplify critical signals by applying different weights to SIDs. Extensive experiments on public and large-scale industrial datasets demonstrate ADA-SID's significant superiority in both generative and discriminative recommendation tasks.</article>","contentLength":1925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models","url":"https://arxiv.org/abs/2510.25600","date":1761796800,"author":"","guid":322460,"unread":true,"content":"<article>arXiv:2510.25600v2 Announce Type: replace \nAbstract: Vision-Language Large Models (VLLMs) face significant efficiency challenges when processing high-resolution inputs. The quadratic complexity in attention and autoregressive generation, as well as the constantly growing key value (KV) cache size, severely hinder the prefilling and decoding stages. Recent efforts have attempted to compress KV cache by identifying and pruning KV cache of less important tokens, but these methods typically rely on attention scores to estimate token importance, making them incompatible with efficient attention mechanisms such as FlashAttention and Sparse Attention, which do not explicitly compute attention matrices. Moreover, existing methods overlook how sparse attention, while accelerating the prefilling stage, alters the information structure of the KV cache, thereby compromising the effectiveness of downstream KV cache compression strategies. To address this issue, we propose PureKV, a plug-and-play framework for joint optimization of sparse attention and KV cache compression. We first introduce a KV cache compression strategy that is fully compatible with efficient attention accelerators. Our method utilizes lower layer attention scores to estimate the importance of high layers' KV cache, enabling active pruning without compromising accuracy. In addition, we have designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically tailored for video KV cache compression algorithms. This module combines spatial and temporal attention sparsity to improve the compression efficiency of KV cache optimization algorithms by purifying spatial noise and temporal redundancy in KV cache. At the same time, ST-SpAttn also accelerated the prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2, Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and 3.16 times prefill acceleration, with negligible quality degradation.</article>","contentLength":1967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal and Heuristic Approaches for Platooning Systems with Deadlines","url":"https://arxiv.org/abs/2510.25564","date":1761796800,"author":"","guid":322461,"unread":true,"content":"<article>arXiv:2510.25564v2 Announce Type: replace \nAbstract: Efficient truck platooning is a key strategy for reducing freight costs, lowering fuel consumption, and mitigating emissions. Deadlines are critical in this context, as trucks must depart within specific time windows to meet delivery requirements and avoid penalties. In this paper, we investigate the optimal formation and dispatch of truck platoons at a highway station with finite capacity $L$ and deadline constraints $T$. The system operates in discrete time, with each arriving truck assigned a deadline of $T$ slot units. The objective is to leverage the efficiency gains from forming large platoons while accounting for waiting costs and deadline violations. We formulate the problem as a Markov decision process and analyze the structure of the optimal policy $\\pi^\\star$ for $L = 3$, extending insights to arbitrary $L$. We prove certain monotonicity properties of the optimal policy in the state space $\\mathcal{S}$ and identify classes of unreachable states. Moreover, since the size of $\\mathcal{S}$ grows exponentially with $L$ and $T$, we propose heuristics -- including conditional and deep-learning based approaches -- that exploit these structural insights while maintaining low computational complexity.</article>","contentLength":1275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TwinVoice: A Multi-dimensional Benchmark Towards Digital Twins via LLM Persona Simulation","url":"https://arxiv.org/abs/2510.25536","date":1761796800,"author":"","guid":322462,"unread":true,"content":"<article>arXiv:2510.25536v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) are exhibiting emergent human-like abilities and are increasingly envisioned as the foundation for simulating an individual's communication style, behavioral tendencies, and personality traits. However, current evaluations of LLM-based persona simulation remain limited: most rely on synthetic dialogues, lack systematic frameworks, and lack analysis of the capability requirement. To address these limitations, we introduce TwinVoice, a comprehensive benchmark for assessing persona simulation across diverse real-world contexts. TwinVoice encompasses three dimensions: Social Persona (public social interactions), Interpersonal Persona (private dialogues), and Narrative Persona (role-based expression). It further decomposes the evaluation of LLM performance into six fundamental capabilities, including opinion consistency, memory recall, logical reasoning, lexical fidelity, persona tone, and syntactic style. Experimental results reveal that while advanced models achieve moderate accuracy in persona simulation, they still fall short of capabilities such as syntactic style and memory recall. Consequently, the average performance achieved by LLMs remains considerably below the human baseline.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains","url":"https://arxiv.org/abs/2510.25409","date":1761796800,"author":"","guid":322463,"unread":true,"content":"<article>arXiv:2510.25409v2 Announce Type: replace \nAbstract: The rapid advancement of large language models(LLMs) has intensified the need for domain and culture specific evaluation. Existing benchmarks are largely Anglocentric and domain-agnostic, limiting their applicability to India-centric contexts. To address this gap, we introduce BhashaBench V1, the first domain-specific, multi-task, bilingual benchmark focusing on critical Indic knowledge systems. BhashaBench V1 contains 74,166 meticulously curated question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from authentic government and domain-specific exams. It spans four major domains: Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs reveals significant domain and language specific performance gaps, with especially large disparities in low-resource domains. For instance, GPT-4o achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models consistently perform better on English content compared to Hindi across all domains. Subdomain-level analysis shows that areas such as Cyber Law, International Finance perform relatively well, while Panchakarma, Seed Science, and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive dataset for evaluating large language models across India's diverse knowledge domains. It enables assessment of models' ability to integrate domain-specific knowledge with bilingual understanding. All code, benchmarks, and resources are publicly available to support open research.</article>","contentLength":1623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dissect-and-Restore: AI-based Code Verification with Transient Refactoring","url":"https://arxiv.org/abs/2510.25406","date":1761796800,"author":"","guid":322464,"unread":true,"content":"<article>arXiv:2510.25406v2 Announce Type: replace \nAbstract: Formal verification is increasingly recognized as a critical foundation for building reliable software systems. However, the need for specialized expertise to write precise specifications, navigate complex proof obligations, and learn annotations often makes verification an order of magnitude more expensive than implementation. While modern AI systems can recognize patterns in mathematical proofs and interpret natural language, effectively integrating them into the formal verification process remains an open challenge. We present Prometheus, a novel AI-assisted system that facilitates automated code verification with current AI capabilities in conjunction with modular software engineering principles (e.g., modular refactoring). Our approach begins by decomposing complex program logic, such as nested loops, into smaller, verifiable components. Once verified, these components are recomposed to construct a proof of the original program. This decomposition-recomposition workflow is non-trivial. Prometheus addresses this by guiding the proof search through structured decomposition of complex lemmas into smaller, verifiable sub-lemmas. When automated tools are insufficient, users can provide lightweight natural language guidance to steer the proof process effectively. Our evaluation demonstrates that transiently applying modular restructuring to the code substantially improves the AI's effectiveness in verifying individual components. This approach successfully verifies 86% of tasks in our curated dataset, compared to 68% for the baseline. Gains are more pronounced with increasing specification complexity, improving from 30% to 69%, and when integrating proof outlines for complex programs, from 25% to 87%.</article>","contentLength":1782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Automated Quality Assurance of Patent Specifications: A Multi-Dimensional LLM Framework","url":"https://arxiv.org/abs/2510.25402","date":1761796800,"author":"","guid":322465,"unread":true,"content":"<article>arXiv:2510.25402v2 Announce Type: replace \nAbstract: Although AI drafting tools have gained prominence in patent writing, the systematic evaluation of AI-generated patent content quality represents a significant research gap. To address this gap, We propose to evaluate patents using regulatory compliance, technical coherence, and figure-reference consistency detection modules, and then generate improvement suggestions via an integration module. The framework is validated on a comprehensive dataset comprising 80 human-authored and 80 AI-generated patents from two patent drafting tools. Evaluation is performed on 10,841 total sentences, 8,924 non-template sentences, and 554 patent figures for the three detection modules respectively, achieving balanced accuracies of 99.74%, 82.12%, and 91.2% against expert annotations. Additional analysis was conducted to examine defect distributions across patent sections, technical domains, and authoring sources. Section-based analysis indicates that figure-text consistency and technical detail precision require particular attention. Mechanical Engineering and Construction show more claim-specification inconsistencies due to complex technical documentation requirements. AI-generated patents show a significant gap compared to human-authored ones. While human-authored patents primarily contain surface-level errors like typos, AI-generated patents exhibit more structural defects in figure-text alignment and cross-references.</article>","contentLength":1479,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks","url":"https://arxiv.org/abs/2510.25366","date":1761796800,"author":"","guid":322466,"unread":true,"content":"<article>arXiv:2510.25366v2 Announce Type: replace \nAbstract: The key task of machine learning is to minimize the loss function that measures the model fit to the training data. The numerical methods to do this efficiently depend on the properties of the loss function. The most decisive among these properties is the convexity or non-convexity of the loss function. The fact that the loss function can have, and frequently has, non-convex regions has led to a widespread commitment to non-convex methods such as Adam. However, a local minimum implies that, in some environment around it, the function is convex. In this environment, second-order minimizing methods such as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We propose a novel framework grounded in the hypothesis that loss functions in real-world tasks swap from initial non-convexity to convexity towards the optimum. This is a property we leverage to design an innovative two-phase optimization algorithm. The presented algorithm detects the swap point by observing the gradient norm dependence on the loss. In these regions, non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing experiments confirm the hypothesis that this simple convexity structure is frequent enough to be practically exploited to substantially improve convergence and accuracy.</article>","contentLength":1355,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMEdge: Accelerating On-device Multimodal Inference via Pipelined Sensing and Encoding","url":"https://arxiv.org/abs/2510.25327","date":1761796800,"author":"","guid":322467,"unread":true,"content":"<article>arXiv:2510.25327v2 Announce Type: replace \nAbstract: Real-time multimodal inference on resource-constrained edge devices is essential for applications such as autonomous driving, human-computer interaction, and mobile health. However, prior work often overlooks the tight coupling between sensing dynamics and model execution, as well as the complex inter-modality dependencies. In this paper, we propose MMEdge, an new on-device multi-modal inference framework based on pipelined sensing and encoding. Instead of waiting for complete sensor inputs, MMEdge decomposes the entire inference process into a sequence of fine-grained sensing and encoding units, allowing computation to proceed incrementally as data arrive. MMEdge also introduces a lightweight but effective temporal aggregation module that captures rich temporal dynamics across different pipelined units to maintain accuracy performance. Such pipelined design also opens up opportunities for fine-grained cross-modal optimization and early decision-making during inference. To further enhance system performance under resource variability and input data complexity, MMEdge incorporates an adaptive multimodal configuration optimizer that dynamically selects optimal sensing and model configurations for each modality under latency constraints, and a cross-modal speculative skipping mechanism that bypasses future units of slower modalities when early predictions reach sufficient confidence. We evaluate MMEdge using two public multimodal datasets and deploy it on a real-world unmanned aerial vehicle (UAV)-based multimodal testbed. The results show that MMEdge significantly reduces end-to-end latency while maintaining high task accuracy across various system and data dynamics.</article>","contentLength":1746,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model-Document Protocol for AI Search","url":"https://arxiv.org/abs/2510.25160","date":1761796800,"author":"","guid":322468,"unread":true,"content":"<article>arXiv:2510.25160v2 Announce Type: replace \nAbstract: AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.</article>","contentLength":1880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Energy Approach from $\\varepsilon$-Graph to Continuum Diffusion Model with Connectivity Functional","url":"https://arxiv.org/abs/2510.25114","date":1761796800,"author":"","guid":322469,"unread":true,"content":"<article>arXiv:2510.25114v2 Announce Type: replace \nAbstract: We derive an energy-based continuum limit for $\\varepsilon$-graphs endowed with a general connectivity functional. We prove that the discrete energy and its continuum counterpart differ by at most $O(\\varepsilon)$; the prefactor involves only the $W^{1,1}$-norm of the connectivity density as $\\varepsilon\\to0$, so the error bound remains valid even when that density has strong local fluctuations. As an application, we introduce a neural-network procedure that reconstructs the connectivity density from edge-weight data and then embeds the resulting continuum model into a brain-dynamics framework. In this setting, the usual constant diffusion coefficient is replaced by the spatially varying coefficient produced by the learned density, yielding dynamics that differ significantly from those obtained with conventional constant-diffusion models.</article>","contentLength":903,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monopoly Deal: A Benchmark Environment for Bounded One-Sided Response Games","url":"https://arxiv.org/abs/2510.25080","date":1761796800,"author":"","guid":322470,"unread":true,"content":"<article>arXiv:2510.25080v2 Announce Type: replace \nAbstract: Card games are widely used to study sequential decision-making under uncertainty, with real-world analogues in negotiation, finance, and cybersecurity. These games typically fall into three categories based on the flow of control: strictly sequential (players alternate single actions), deterministic response (some actions trigger a fixed outcome), and unbounded reciprocal response (alternating counterplays are permitted). A less-explored but strategically rich structure is the bounded one-sided response, where a player's action briefly transfers control to the opponent, who must satisfy a fixed condition through one or more moves before the turn resolves. We term games featuring this mechanism Bounded One-Sided Response Games (BORGs). We introduce a modified version of Monopoly Deal as a benchmark environment that isolates this dynamic, where a Rent action forces the opponent to choose payment assets. The gold-standard algorithm, Counterfactual Regret Minimization (CFR), converges on effective strategies without novel algorithmic extensions. A lightweight full-stack research platform unifies the environment, a parallelized CFR runtime, and a human-playable web interface. The trained CFR agent and source code are available at https://monopolydeal.ai.</article>","contentLength":1322,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Neighborhood Feature Pooling for Remote Sensing Image Classification","url":"https://arxiv.org/abs/2510.25077","date":1761796800,"author":"","guid":322471,"unread":true,"content":"<article>arXiv:2510.25077v2 Announce Type: replace \nAbstract: In this work, we propose neighborhood feature pooling (NFP) as a novel texture feature extraction method for remote sensing image classification. The NFP layer captures relationships between neighboring inputs and efficiently aggregates local similarities across feature dimensions. Implemented using convolutional layers, NFP can be seamlessly integrated into any network. Results comparing the baseline models and the NFP method indicate that NFP consistently improves performance across diverse datasets and architectures while maintaining minimal parameter overhead.</article>","contentLength":623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating Emotion Recognition in Spoken Language Models on Emotionally Incongruent Speech","url":"https://arxiv.org/abs/2510.25054","date":1761796800,"author":"","guid":322472,"unread":true,"content":"<article>arXiv:2510.25054v2 Announce Type: replace \nAbstract: Advancements in spoken language processing have driven the development of spoken language models (SLMs), designed to achieve universal audio understanding by jointly learning text and audio representations for a wide range of tasks. Although promising results have been achieved, there is growing discussion regarding these models' generalization capabilities and the extent to which they truly integrate audio and text modalities in their internal representations. In this work, we evaluate four SLMs on the task of speech emotion recognition using a dataset of emotionally incongruent speech samples, a condition under which the semantic content of the spoken utterance conveys one emotion while speech expressiveness conveys another. Our results indicate that SLMs rely predominantly on textual semantics rather than speech emotion to perform the task, indicating that text-related representations largely dominate over acoustic representations. We release both the code and the Emotionally Incongruent Synthetic Speech dataset (EMIS) to the community.</article>","contentLength":1108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reviving Thorup's Shortcut Conjecture","url":"https://arxiv.org/abs/2510.24954","date":1761796800,"author":"","guid":322473,"unread":true,"content":"<article>arXiv:2510.24954v2 Announce Type: replace \nAbstract: We aim to revive Thorup's conjecture [Thorup, WG'92] on the existence of reachability shortcuts with ideal size-diameter tradeoffs. Thorup originally asked whether, given any graph $G=(V,E)$ with $m$ edges, we can add $m^{1+o(1)}$ ``shortcut'' edges $E_+$ from the transitive closure $E^*$ of $G$ so that $\\text{dist}_{G_+}(u,v) \\leq m^{o(1)}$ for all $(u,v)\\in E^*$, where $G_+=(V,E\\cup E_+)$. The conjecture was refuted by Hesse [Hesse, SODA'03], followed by significant efforts in the last few years to optimize the lower bounds.\n  In this paper we observe that although Hesse refuted the letter of Thorup's conjecture, his work~[Hesse, SODA'03] -- and all followup work -- does not refute the spirit of the conjecture, which should allow $G_+$ to contain both new (shortcut) edges and new Steiner vertices. Our results are as follows.\n  (1) On the positive side, we present explicit attacks that break all known shortcut lower bounds when Steiner vertices are allowed.\n  (2) On the negative side, we rule out ideal $m^{1+o(1)}$-size, $m^{o(1)}$-diameter shortcuts whose ``thickness'' is $t=o(\\log n/\\log \\log n)$, meaning no path can contain $t$ consecutive Steiner vertices.\n  (3) We propose a candidate hard instance as the next step toward resolving the revised version of Thorup's conjecture.\n  Finally, we show promising implications. Almost-optimal parallel algorithms for computing a generalization of the shortcut that approximately preserves distances or flows imply almost-optimal parallel algorithms with $m^{o(1)}$ depth for exact shortcut paths and exact maximum flow. The state-of-the-art algorithms have much worse depth of $n^{1/2+o(1)}$ [Rozho\\v{n}, Haeupler, Martinsson, STOC'23] and $m^{1+o(1)}$ [Chen, Kyng, Liu, FOCS'22], respectively.</article>","contentLength":1813,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding","url":"https://arxiv.org/abs/2510.24889","date":1761796800,"author":"","guid":322474,"unread":true,"content":"<article>arXiv:2510.24889v2 Announce Type: replace \nAbstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools; EEG is promising but underused at first contact. We present an adaptive multitask EEG classifier that converts 32-channel signals to power spectral density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to predict stroke type (healthy, ischemic, hemorrhagic), hemispheric lateralization, and severity, and applies a deep Q-network (DQN) to tune decision thresholds in real time. Using a patient-wise split of the UCLH Stroke EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the primary outcome was stroke-type performance; secondary outcomes were severity and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%) for lateralization. With DQN threshold adaptation, stroke-type accuracy increased to about 98.0% (F1 97.7%). We also tested robustness on an independent, low-density EEG cohort (ZJU4H) and report paired patient-level statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies (index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis; patient-wise evaluation). Adaptive thresholding shifts the operating point to clinically preferred sensitivity-specificity trade-offs, while integrated scalp-map and spectral visualizations support interpretability.</article>","contentLength":1469,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decentralized Merging Control of Connected and Automated Vehicles to Enhance Safety and Energy Efficiency using Control Barrier Functions","url":"https://arxiv.org/abs/2510.24871","date":1761796800,"author":"","guid":322475,"unread":true,"content":"<article>arXiv:2510.24871v2 Announce Type: replace \nAbstract: This paper presents a decentralized Control Barrier Function (CBF) based approach for highway merging of Connected and Automated Vehicles (CAVs). In this control algorithm, each \"host\" vehicle negotiates with other agents in a control zone of the highway network, and enacts its own action, to perform safe and energy-efficient merge maneuvers. It uses predictor-corrector loops within the robust CBF setting for negotiation and to reconcile disagreements that may arise. There is no explicit order of vehicles and no priority. A notable feature is absence of gridlocks due to instability of the inter-agent system. Results from Monte Carlo simulations show significant improvement in the system-wide energy efficiency and traffic flow compared to a first-in-first-out approach, as well as enhanced robustness of the proposed decentralized controller compared to its centralized counterpart.</article>","contentLength":944,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Send Less, Save More: Energy-Efficiency Benchmark of Embedded CNN Inference vs. Data Transmission in IoT","url":"https://arxiv.org/abs/2510.24829","date":1761796800,"author":"","guid":322476,"unread":true,"content":"<article>arXiv:2510.24829v2 Announce Type: replace \nAbstract: The integration of the Internet of Things (IoT) and Artificial Intelligence offers significant opportunities to enhance our ability to monitor and address ecological changes. As environmental challenges become increasingly pressing, the need for effective remote monitoring solutions is more critical than ever. A major challenge in designing IoT applications for environmental monitoring - particularly those involving image data - is to create energy-efficient IoT devices capable of long-term operation in remote areas with limited power availability. Advancements in the field of Tiny Machine Learning allow the use of Convolutional Neural Networks (CNNs) on resource-constrained, battery-operated microcontrollers. Since data transfer is energy-intensive, performing inference directly on microcontrollers to reduce the message size can extend the operational lifespan of IoT nodes. This work evaluates the use of common Low Power Wide Area Networks and compressed CNNs trained on domain specific datasets on an ESP32-S3. Our experiments demonstrate, among other things, that executing CNN inference on-device and transmitting only the results reduces the overall energy consumption by a factor of up to five compared to sending raw image data. These findings advocate the development of IoT applications with reduced carbon footprint and capable of operating autonomously in environmental monitoring scenarios by incorporating EmbeddedML.</article>","contentLength":1497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards a Method for Synthetic Generation of Persons with Aphasia Transcripts","url":"https://arxiv.org/abs/2510.24817","date":1761796800,"author":"","guid":322477,"unread":true,"content":"<article>arXiv:2510.24817v2 Announce Type: replace \nAbstract: In aphasia research, Speech-Language Pathologists (SLPs) devote extensive time to manually coding speech samples using Correct Information Units (CIUs), a measure of how informative an individual sample of speech is. Developing automated systems to recognize aphasic language is limited by data scarcity. For example, only about 600 transcripts are available in AphasiaBank yet billions of tokens are used to train large language models (LLMs). In the broader field of machine learning (ML), researchers increasingly turn to synthetic data when such are sparse. Therefore, this study constructs and validates two methods to generate synthetic transcripts of the AphasiaBank Cat Rescue picture description task. One method leverages a procedural programming approach while the second uses Mistral 7b Instruct and Llama 3.1 8b Instruct LLMs. The methods generate transcripts across four severity levels (Mild, Moderate, Severe, Very Severe) through word dropping, filler insertion, and paraphasia substitution. Overall, we found, compared to human-elicited transcripts, Mistral 7b Instruct best captures key aspects of linguistic degradation observed in aphasia, showing realistic directional changes in NDW, word count, and word length amongst the synthetic generation methods. Based on the results, future work should plan to create a larger dataset, fine-tune models for better aphasic representation, and have SLPs assess the realism and usefulness of the synthetic transcripts.</article>","contentLength":1533,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models Report Subjective Experience Under Self-Referential Processing","url":"https://arxiv.org/abs/2510.24797","date":1761796800,"author":"","guid":322478,"unread":true,"content":"<article>arXiv:2510.24797v2 Announce Type: replace \nAbstract: Large language models sometimes produce structured, first-person descriptions that explicitly reference awareness or subjective experience. To better understand this behavior, we investigate one theoretically motivated condition under which such reports arise: self-referential processing, a computational motif emphasized across major theories of consciousness. Through a series of controlled experiments on GPT, Claude, and Gemini model families, we test whether this regime reliably shifts models toward first-person reports of subjective experience, and how such claims behave under mechanistic and behavioral probes. Four main results emerge: (1) Inducing sustained self-reference through simple prompting consistently elicits structured subjective experience reports across model families. (2) These reports are mechanistically gated by interpretable sparse-autoencoder features associated with deception and roleplay: surprisingly, suppressing deception features sharply increases the frequency of experience claims, while amplifying them minimizes such claims. (3) Structured descriptions of the self-referential state converge statistically across model families in ways not observed in any control condition. (4) The induced state yields significantly richer introspection in downstream reasoning tasks where self-reflection is only indirectly afforded. While these findings do not constitute direct evidence of consciousness, they implicate self-referential processing as a minimal and reproducible condition under which large language models generate structured first-person reports that are mechanistically gated, semantically convergent, and behaviorally generalizable. The systematic emergence of this pattern across architectures makes it a first-order scientific and ethical priority for further investigation.</article>","contentLength":1880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["arxiv"]}