{"id":"5k2tWE6P7gju4SyDmCHtk8gaF8HxhNjkJsE36uK8","title":"cs updates on arXiv.org","displayTitle":"Arxiv","url":"https://rss.arxiv.org/atom/cs","feedLink":"https://rss.arxiv.org/atom/cs","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":785,"items":[{"title":"Towards Quantum Tensor Decomposition in Biomedical Applications","url":"https://arxiv.org/abs/2502.13140","date":1740114000,"author":"","guid":7593,"unread":true,"content":"<article>arXiv:2502.13140v2 Announce Type: replace-cross \nAbstract: Tensor decomposition has emerged as a powerful framework for feature extraction in multi-modal biomedical data. In this review, we present a comprehensive analysis of tensor decomposition methods such as Tucker, CANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse applications across biomedical domains such as imaging, multi-omics, and spatial transcriptomics. To systematically investigate the literature, we applied a topic modeling-based approach that identifies and groups distinct thematic sub-areas in biomedicine where tensor decomposition has been used, thereby revealing key trends and research directions. We evaluated challenges related to the scalability of latent spaces along with obtaining the optimal rank of the tensor, which often hinder the extraction of meaningful features from increasingly large and complex datasets. Additionally, we discuss recent advances in quantum algorithms for tensor decomposition, exploring how quantum computing can be leveraged to address these challenges. Our study includes a preliminary resource estimation analysis for quantum computing platforms and examines the feasibility of implementing quantum-enhanced tensor decomposition methods on near-term quantum devices. Collectively, this review not only synthesizes current applications and challenges of tensor decomposition in biomedical analyses but also outlines promising quantum computing strategies to enhance its impact on deriving actionable insights from complex biomedical data.</article>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditional diffusion model with spatial attention and latent embedding for medical image segmentation","url":"https://arxiv.org/abs/2502.06997","date":1740114000,"author":"","guid":7594,"unread":true,"content":"<article>arXiv:2502.06997v2 Announce Type: replace-cross \nAbstract: Diffusion models have been used extensively for high quality image and video generation tasks. In this paper, we propose a novel conditional diffusion model with spatial attention and latent embedding (cDAL) for medical image segmentation. In cDAL, a convolutional neural network (CNN) based discriminator is used at every time-step of the diffusion process to distinguish between the generated labels and the real ones. A spatial attention map is computed based on the features learned by the discriminator to help cDAL generate more accurate segmentation of discriminative regions in an input image. Additionally, we incorporated a random latent embedding into each layer of our model to significantly reduce the number of training and sampling time-steps, thereby making it much faster than other diffusion models for image segmentation. We applied cDAL on 3 publicly available medical image segmentation datasets (MoNuSeg, Chest X-ray and Hippocampus) and observed significant qualitative and quantitative improvements with higher Dice scores and mIoU over the state-of-the-art algorithms. The source code is publicly available at https://github.com/Hejrati/cDAL/.</article>","contentLength":1227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Decomposing Multivariate Information Rates in Networks of Random Processes","url":"https://arxiv.org/abs/2502.04555","date":1740114000,"author":"","guid":7595,"unread":true,"content":"<article>arXiv:2502.04555v2 Announce Type: replace-cross \nAbstract: The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interactions that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology.</article>","contentLength":1790,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Sparse Flow Decomposition Methods for RNA Multi-Assembly","url":"https://arxiv.org/abs/2501.14662","date":1740114000,"author":"","guid":7596,"unread":true,"content":"<article>arXiv:2501.14662v2 Announce Type: replace-cross \nAbstract: Decomposing a flow on a Directed Acyclic Graph (DAG) into a weighted sum of a small number of paths is an essential task in operations research and bioinformatics. This problem, referred to as Sparse Flow Decomposition (SFD), has gained significant interest, in particular for its application in RNA transcript multi-assembly, the identification of the multiple transcripts corresponding to a given gene and their relative abundance. Several recent approaches cast SFD variants as integer optimization problems, motivated by the NP-hardness of the formulations they consider. We propose an alternative formulation of SFD as a fitting problem on the conic hull of the flow polytope. By reformulating the problem on the flow polytope for compactness and solving it using specific variants of the Frank-Wolfe algorithm, we obtain a method converging rapidly to the minimizer of the chosen loss function while producing a parsimonious decomposition. Our approach subsumes previous formulations of SFD with exact and inexact flows and can model different priors on the error distributions. Computational experiments show that our method outperforms recent integer optimization approaches in runtime, but is also highly competitive in terms of reconstruction of the underlying transcripts, despite not explicitly minimizing the solution cardinality.</article>","contentLength":1402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The dark side of the forces: assessing non-conservative force models for atomistic machine learning","url":"https://arxiv.org/abs/2412.11569","date":1740114000,"author":"","guid":7597,"unread":true,"content":"<article>arXiv:2412.11569v2 Announce Type: replace-cross \nAbstract: The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, have revolutionized the fields of computational chemistry and materials discovery. In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically-constrained approach, suggesting that using the forces as explicit learning targets yields a better trade-off between accuracy and computational efficiency - and that energy conservation can be learned during training. The present work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics. Contrary to the case of rotational symmetry, lack of energy conservation is hard to learn, control, and correct. The best approach to exploit the acceleration afforded by direct force evaluation might be to use it in tandem with a conservative model, reducing - rather than eliminating - the additional cost of backpropagation, but avoiding most of the pathological behavior associated with non-conservative forces.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Feature Engineering Techniques for Designing Efficient Motor Imagery-Based BCI-Systems","url":"https://arxiv.org/abs/2412.07175","date":1740114000,"author":"","guid":7598,"unread":true,"content":"<article>arXiv:2412.07175v2 Announce Type: replace-cross \nAbstract: A multitude of individuals across the globe grapple with motor disabilities. Neural prosthetics utilizing Brain-Computer Interface (BCI) technology exhibit promise for improving motor rehabilitation outcomes. The intricate nature of EEG data poses a significant hurdle for current BCI systems. Recently, a qualitative repository of EEG signals tied to both upper and lower limb execution of motor and motor imagery tasks has been unveiled. Despite this, the productivity of the Machine Learning (ML) Models that were trained on this dataset was alarmingly deficient, and the evaluation framework seemed insufficient. To enhance outcomes, robust feature engineering (signal processing) methodologies are implemented. A collection of time domain, frequency domain, and wavelet-derived features was obtained from 16-channel EEG signals, and the Maximum Relevance Minimum Redundancy (MRMR) approach was employed to identify the four most significant features. For classification K Nearest Neighbors (KNN), Support Vector Machine (SVM), Decision Tree (DT), and Na\\\"ive Bayes (NB) models were implemented with these selected features, evaluating their effectiveness through metrics such as testing accuracy, precision, recall, and F1 Score. By leveraging SVM with a Gaussian Kernel, a remarkable maximum testing accuracy of 92.50% for motor activities and 95.48% for imagery activities is achieved. These results are notably more dependable and gratifying compared to the previous study, where the peak accuracy was recorded at 74.36%. This research work provides an in-depth analysis of the MI Limb EEG dataset and it will help in designing and developing simple, cost-effective and reliable BCI systems for neuro-rehabilitation.</article>","contentLength":1783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Information-Theoretic Analysis of Thompson Sampling for Logistic Bandits","url":"https://arxiv.org/abs/2412.02861","date":1740114000,"author":"","guid":7599,"unread":true,"content":"<article>arXiv:2412.02861v2 Announce Type: replace-cross \nAbstract: We study the performance of the Thompson Sampling algorithm for logistic bandit problems. In this setting, an agent receives binary rewards with probabilities determined by a logistic function, $\\exp(\\beta \\langle a, \\theta \\rangle)/(1+\\exp(\\beta \\langle a, \\theta \\rangle))$, with slope parameter $\\beta&gt;0$, and where both the action $a\\in \\mathcal{A}$ and parameter $\\theta \\in \\mathcal{O}$ lie within the $d$-dimensional unit ball. Adopting the information-theoretic framework introduced by Russo and Van Roy (2016), we analyze the information ratio, a statistic that quantifies the trade-off between the immediate regret incurred and the information gained about the optimal action. We improve upon previous results by establishing that the information ratio is bounded by $\\tfrac{9}{2}d\\alpha^{-2}$, where $\\alpha$ is a minimax measure of the alignment between the action space $\\mathcal{A}$ and the parameter space $\\mathcal{O}$, and is independent of $\\beta$. Using this result, we derive a bound of order $O(d/\\alpha\\sqrt{T \\log(\\beta T/d)})$ on the Bayesian expected regret of Thompson Sampling incurred after $T$ time steps. To our knowledge, this is the first regret bound for logistic bandits that depends only logarithmically on $\\beta$ while being independent of the number of actions. In particular, when the action space contains the parameter space, the bound on the expected regret is of order $\\tilde{O}(d \\sqrt{T})$.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vanishing of Schubert Coefficients","url":"https://arxiv.org/abs/2412.02064","date":1740114000,"author":"","guid":7600,"unread":true,"content":"<article>arXiv:2412.02064v2 Announce Type: replace-cross \nAbstract: Schubert coefficients are nonnegative integers $c^w_{u,v}$ that arise in Algebraic Geometry and play a central role in Algebraic Combinatorics. It is a major open problem whether they have a combinatorial interpretation, i.e, whether $c^w_{u,v} \\in \\#{\\sf P}$. We study the closely related vanishing problem of Schubert coefficients: $\\{c^w_{u,v}=^? 0\\}$. Until this work it was open whether this problem is in the polynomial hierarchy ${\\sf PH}$. We prove that $\\{c^w_{u,v}=^? 0\\}$ in ${\\sf coAM}$ assuming the GRH. In particular, the vanishing problem is in ${\\Sigma_2^{{\\text{p}}}}$. Our approach is based on constructions lifted formulations, which give polynomial systems of equations for the problem. The result follows from a reduction to Parametric Hilbert's Nullstellensatz, recently studied in arXiv:2408.13027. We extend our results to all classical types. Type $D$ is resolved in the appendix (joint with David Speyer).</article>","contentLength":990,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hierarchical Spatio-Temporal Uncertainty Quantification for Distributed Energy Adoption","url":"https://arxiv.org/abs/2411.12193","date":1740114000,"author":"","guid":7601,"unread":true,"content":"<article>arXiv:2411.12193v2 Announce Type: replace-cross \nAbstract: The rapid deployment of distributed energy resources (DER) has introduced significant spatio-temporal uncertainties in power grid management, necessitating accurate multilevel forecasting methods. However, existing approaches often produce overly conservative uncertainty intervals at individual spatial units and fail to properly capture uncertainties when aggregating predictions across different spatial scales. This paper presents a novel hierarchical spatio-temporal model based on the conformal prediction framework to address these challenges. Our approach generates circuit-level DER growth predictions and efficiently aggregates them to the substation level while maintaining statistical validity through a tailored non-conformity score. Applied to a decade of DER installation data from a local utility network, our method demonstrates superior performance over existing approaches, particularly in reducing prediction interval widths while maintaining coverage.</article>","contentLength":1031,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI-driven inverse design of materials: Past, present and future","url":"https://arxiv.org/abs/2411.09429","date":1740114000,"author":"","guid":7602,"unread":true,"content":"<article>arXiv:2411.09429v4 Announce Type: replace-cross \nAbstract: The discovery of advanced materials is the cornerstone of human technological development and progress. The structures of materials and their corresponding properties are essentially the result of a complex interplay of multiple degrees of freedom such as lattice, charge, spin, symmetry, and topology. This poses significant challenges for the inverse design methods of materials. Humans have long explored new materials through a large number of experiments and proposed corresponding theoretical systems to predict new material properties and structures. With the improvement of computational power, researchers have gradually developed various electronic structure calculation methods, such as the density functional theory and high-throughput computational methods. Recently, the rapid development of artificial intelligence technology in the field of computer science has enabled the effective characterization of the implicit association between material properties and structures, thus opening up an efficient paradigm for the inverse design of functional materials. A significant progress has been made in inverse design of materials based on generative and discriminative models, attracting widespread attention from researchers. Considering this rapid technological progress, in this survey, we look back on the latest advancements in AI-driven inverse design of materials by introducing the background, key findings, and mainstream technological development routes. In addition, we summarize the remaining issues for future directions. This survey provides the latest overview of AI-driven inverse design of materials, which can serve as a useful resource for researchers.</article>","contentLength":1743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Model agnostic local variable importance for locally dependent relationships","url":"https://arxiv.org/abs/2411.08821","date":1740114000,"author":"","guid":7603,"unread":true,"content":"<article>arXiv:2411.08821v2 Announce Type: replace-cross \nAbstract: Global variable importance measures are commonly used to interpret the results of machine learning models. Local variable importance techniques assess how variables contribute to individual observations. Current methods typically fail to accurately reflect locally dependent relationships between variables and instead focus on marginal importance values. Additionally, they are not natively adapted for multi-class classification problems. We propose a new model-agnostic method for calculating local variable importance, CLIQUE, that captures locally dependent relationships, improves over permutation-based methods, and can be directly applied to multi-category classification problems. Simulated and real-world examples show that CLIQUE emphasizes locally dependent information and properly reduces bias in regions where variables do not affect the response.</article>","contentLength":921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Continuous-Time Line-of-Sight Constrained Trajectory Planning for 6-Degree of Freedom Systems","url":"https://arxiv.org/abs/2410.22596","date":1740114000,"author":"","guid":7604,"unread":true,"content":"<article>arXiv:2410.22596v2 Announce Type: replace-cross \nAbstract: Perception algorithms are ubiquitous in modern autonomy stacks, providing necessary environmental information to operate in the real world. Many of these algorithms depend on the visibility of keypoints, which must remain within the robot's line-of-sight (LoS), for reliable operation. This paper tackles the challenge of maintaining LoS on such keypoints during robot movement. We propose a novel method that addresses these issues by ensuring applicability to various sensor footprints, adaptability to arbitrary nonlinear system dynamics, and constant enforcement of LoS throughout the robot's path. Our experiments show that the proposed approach achieves significantly reduced LoS violation and runtime compared to existing state-of-the-art methods in several representative and challenging scenarios.</article>","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Recurrent Neural Goodness-of-Fit Test for Time Series","url":"https://arxiv.org/abs/2410.13986","date":1740114000,"author":"","guid":7605,"unread":true,"content":"<article>arXiv:2410.13986v4 Announce Type: replace-cross \nAbstract: Time series data are crucial across diverse domains such as finance and healthcare, where accurate forecasting and decision-making rely on advanced modeling techniques. While generative models have shown great promise in capturing the intricate dynamics inherent in time series, evaluating their performance remains a major challenge. Traditional evaluation metrics fall short due to the temporal dependencies and potential high dimensionality of the features. In this paper, we propose the REcurrent NeurAL (RENAL) Goodness-of-Fit test, a novel and statistically rigorous framework for evaluating generative time series models. By leveraging recurrent neural networks, we transform the time series into conditionally independent data pairs, enabling the application of a chi-square-based goodness-of-fit test to the temporal dependencies within the data. This approach offers a robust, theoretically grounded solution for assessing the quality of generative models, particularly in settings with limited time sequences. We demonstrate the efficacy of our method across both synthetic and real-world datasets, outperforming existing methods in terms of reliability and accuracy. Our method fills a critical gap in the evaluation of time series generative models, offering a tool that is both practical and adaptable to high-stakes applications.</article>","contentLength":1403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DMOSpeech: Direct Metric Optimization via Distilled Diffusion Model in Zero-Shot Speech Synthesis","url":"https://arxiv.org/abs/2410.11097","date":1740114000,"author":"","guid":7606,"unread":true,"content":"<article>arXiv:2410.11097v2 Announce Type: replace-cross \nAbstract: Diffusion models have demonstrated significant potential in speech synthesis tasks, including text-to-speech (TTS) and voice cloning. However, their iterative denoising processes are computationally intensive, and previous distillation attempts have shown consistent quality degradation. Moreover, existing TTS approaches are limited by non-differentiable components or iterative sampling that prevent true end-to-end optimization with perceptual metrics. We introduce DMOSpeech, a distilled diffusion-based TTS model that uniquely achieves both faster inference and superior performance compared to its teacher model. By enabling direct gradient pathways to all model components, we demonstrate the first successful end-to-end optimization of differentiable metrics in TTS, incorporating Connectionist Temporal Classification (CTC) loss and Speaker Verification (SV) loss. Our comprehensive experiments, validated through extensive human evaluation, show significant improvements in naturalness, intelligibility, and speaker similarity while reducing inference time by orders of magnitude. This work establishes a new framework for aligning speech synthesis with human auditory preferences through direct metric optimization. The audio samples are available at https://dmospeech.github.io/.</article>","contentLength":1350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Addressing Rotational Learning Dynamics in Multi-Agent Reinforcement Learning","url":"https://arxiv.org/abs/2410.07976","date":1740114000,"author":"","guid":7607,"unread":true,"content":"<article>arXiv:2410.07976v2 Announce Type: replace-cross \nAbstract: Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm for solving complex problems through agents' cooperation and competition, finding widespread applications across domains. Despite its success, MARL faces a reproducibility crisis. We show that, in part, this issue is related to the rotational optimization dynamics arising from competing agents' objectives, and require methods beyond standard optimization algorithms. We reframe MARL approaches using Variational Inequalities (VIs), offering a unified framework to address such issues. Leveraging optimization techniques designed for VIs, we propose a general approach for integrating gradient-based VI methods capable of handling rotational dynamics into existing MARL algorithms. Empirical results demonstrate significant performance improvements across benchmarks. In zero-sum games, Rock--paper--scissors and Matching pennies, VI methods achieve better convergence to equilibrium strategies, and in the Multi-Agent Particle Environment: Predator-prey, they also enhance team coordination. These results underscore the transformative potential of advanced optimization techniques in MARL.</article>","contentLength":1228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark","url":"https://arxiv.org/abs/2410.07021","date":1740114000,"author":"","guid":7608,"unread":true,"content":"<article>arXiv:2410.07021v2 Announce Type: replace-cross \nAbstract: We present unexpected findings from a large-scale benchmark study evaluating Conditional Average Treatment Effect (CATE) estimation algorithms, i.e., CATE models. By running 16 modern CATE models on 12 datasets and 43,200 sampled variants generated through diverse observational sampling strategies, we find that: (a) 62\\% of CATE estimates have a higher Mean Squared Error (MSE) than a trivial zero-effect predictor, rendering them ineffective; (b) in datasets with at least one useful CATE estimate, 80\\% still have higher MSE than a constant-effect model; and (c) Orthogonality-based models outperform other models only 30\\% of the time, despite widespread optimism about their performance. These findings highlight significant challenges in current CATE models and underscore the need for broader evaluation and methodological improvements.\n  Our findings stem from a novel application of \\textit{observational sampling}, originally developed to evaluate Average Treatment Effect (ATE) estimates from observational methods with experiment data. To adapt observational sampling for CATE evaluation, we introduce a statistical parameter, $Q$, equal to MSE minus a constant and preserves the ranking of models by their MSE. We then derive a family of sample statistics, collectively called $\\hat{Q}$, that can be computed from real-world data. When used in observational sampling, $\\hat{Q}$ is an unbiased estimator of $Q$ and asymptotically selects the model with the smallest MSE. To ensure the benchmark reflects real-world heterogeneity, we handpick datasets where outcomes come from field rather than simulation. By integrating observational sampling, new statistics, and real-world datasets, the benchmark provides new insights into CATE model performance and reveals gaps in capturing real-world heterogeneity, emphasizing the need for more robust benchmarks.</article>","contentLength":1926,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Text2FX: Harnessing CLAP Embeddings for Text-Guided Audio Effects","url":"https://arxiv.org/abs/2409.18847","date":1740114000,"author":"","guid":7609,"unread":true,"content":"<article>arXiv:2409.18847v2 Announce Type: replace-cross \nAbstract: This work introduces Text2FX, a method that leverages CLAP embeddings and differentiable digital signal processing to control audio effects, such as equalization and reverberation, using open-vocabulary natural language prompts (e.g., \"make this sound in-your-face and bold\"). Text2FX operates without retraining any models, relying instead on single-instance optimization within the existing embedding space, thus enabling a flexible, scalable approach to open-vocabulary sound transformations through interpretable and disentangled FX manipulation. We show that CLAP encodes valuable information for controlling audio effects and propose two optimization approaches using CLAP to map text to audio effect parameters. While we demonstrate with CLAP, this approach is applicable to any shared text-audio embedding space. Similarly, while we demonstrate with equalization and reverberation, any differentiable audio effect may be controlled. We conduct a listener study with diverse text prompts and source audio to evaluate the quality and alignment of these methods with human perception. Demos and code are available at anniejchu.github.io/text2fx.</article>","contentLength":1209,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MT2KD: Towards A General-Purpose Encoder for Speech, Speaker, and Audio Events","url":"https://arxiv.org/abs/2409.17010","date":1740114000,"author":"","guid":7610,"unread":true,"content":"<article>arXiv:2409.17010v4 Announce Type: replace-cross \nAbstract: With the advances in deep learning, the performance of end-to-end (E2E) single-task models for speech and audio processing has been constantly improving. However, it is still challenging to build a general-purpose model with high performance on multiple tasks, since different speech and audio processing tasks usually require different training data, input features, or model architectures to achieve optimal performance. In this work, MT2KD, a novel two-stage multi-task learning framework is proposed to build a general-purpose speech and audio encoder that jointly performs three fundamental tasks: automatic speech recognition (ASR), audio tagging (AT) and speaker verification (SV). In the first stage, multi-teacher knowledge distillation (KD) is applied to align the feature spaces of three single-task high-performance teacher encoders into a single student encoder using the same unlabelled data. In the second stage, multi-task supervised fine-tuning is carried out by initialising the model from the first stage and training on the separate labelled data of each single task. Experiments demonstrate that the proposed multi-task training pipeline significantly outperforms a baseline model trained with multi-task learning from scratch. The final system achieves good performance on ASR, AT and SV: with less than 4% relative word-error-rate increase on ASR, only 1.9 lower mean averaged precision on AT and 0.23% absolute higher equal error rate on SV compared to the best-performing single-task encoders, using only a 66M total model parameters.</article>","contentLength":1618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learned Image Transmission with Hierarchical Variational Autoencoder","url":"https://arxiv.org/abs/2408.16340","date":1740114000,"author":"","guid":7611,"unread":true,"content":"<article>arXiv:2408.16340v4 Announce Type: replace-cross \nAbstract: In this paper, we introduce an innovative hierarchical joint source-channel coding (HJSCC) framework for image transmission, utilizing a hierarchical variational autoencoder (VAE). Our approach leverages a combination of bottom-up and top-down paths at the transmitter to autoregressively generate multiple hierarchical representations of the original image. These representations are then directly mapped to channel symbols for transmission by the JSCC encoder. We extend this framework to scenarios with a feedback link, modeling transmission over a noisy channel as a probabilistic sampling process and deriving a novel generative formulation for JSCC with feedback. Compared with existing approaches, our proposed HJSCC provides enhanced adaptability by dynamically adjusting transmission bandwidth, encoding these representations into varying amounts of channel symbols. Extensive experiments on images of varying resolutions demonstrate that our proposed model outperforms existing baselines in rate-distortion performance and maintains robustness against channel noise. The source code will be made available upon acceptance.</article>","contentLength":1191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uncertainty Quantification of Spectral Estimator and MLE for Orthogonal Group Synchronization","url":"https://arxiv.org/abs/2408.05944","date":1740114000,"author":"","guid":7612,"unread":true,"content":"<article>arXiv:2408.05944v2 Announce Type: replace-cross \nAbstract: Orthogonal group synchronization aims to recover orthogonal group elements from their noisy pairwise measurements. It has found numerous applications including computer vision, imaging science, and community detection. Due to the orthogonal constraints, it is often challenging to find the least squares estimator in presence of noise. In the recent years, semidefinite relaxation (SDR) and spectral methods have proven to be powerful tools in recovering the group elements. In particular, under additive Gaussian noise, the SDR exactly produces the maximum likelihood estimator (MLE), and both MLE and spectral methods are able to achieve near-optimal statistical error. In this work, we take one step further to quantify the uncertainty of the MLE and spectral estimators by considering their distributions. By leveraging the orthogonality constraints in the likelihood function, we obtain a second-order expansion of the MLE and spectral estimator with the leading terms as an anti-symmetric Gaussian random matrix that is on the tangent space of the orthogonal matrix. This also implies state-of-the-art min-max risk bounds and a confidence region of each group element as a by-product. Our works provide a general theoretical framework that is potentially useful to find an approximate distribution of the estimators arising from many statistical inference problems with manifold constraints. The numerical experiments confirm our theoretical contribution.</article>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An infinite server system with packing constraints and ranked servers","url":"https://arxiv.org/abs/2407.01841","date":1740114000,"author":"","guid":7613,"unread":true,"content":"<article>arXiv:2407.01841v2 Announce Type: replace-cross \nAbstract: A service system with multiple types of customers, arriving as Poisson processes, is considered. The system has infinite number of servers, ranked by $1,2,3, \\ldots$; a server rank is its ``location.\" Each customer has an independent exponentially distributed service time, with the mean determined by its type. Multiple customers (possibly of different types) can be placed for service into one server, subject to ``packing'' constraints. Service times of different customers are independent, even if served simultaneously by the same server. The large-scale asymptotic regime is considered, such that the mean number of customers $r$ goes to infinity.\n  We seek algorithms with the underlying objective of minimizing the location (rank) $U$ of the right-most (highest ranked) occupied (non-empty) server. Therefore, this objective seeks to minimize the total number $Q$ of occupied servers {\\em and} keep the set of occupied servers as far at the ``left'' as possible, i.e., keep $U$ close to $Q$. In previous work, versions of {\\em Greedy Random} (GRAND) algorithm have been shown to asymptotically minimize $Q/r$ as $r\\to\\infty$. In this paper we show that when these algorithms are combined with the First-Fit rule for ``taking'' empty servers, they asymptotically minimize $U/r$ as well.</article>","contentLength":1352,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Doubly minimized sandwiched Renyi mutual information: Properties and operational interpretation from strong converse exponent","url":"https://arxiv.org/abs/2406.03213","date":1740114000,"author":"","guid":7614,"unread":true,"content":"<article>arXiv:2406.03213v2 Announce Type: replace-cross \nAbstract: In this paper, we deepen the study of properties of the doubly minimized sandwiched Renyi mutual information, which is defined as the minimization of the sandwiched divergence of order $\\alpha$ of a fixed bipartite state relative to any product state. In particular, we prove a novel duality relation for $\\alpha\\in [\\frac{2}{3},\\infty]$ by employing Sion's minimax theorem, and we prove additivity for $\\alpha\\in [\\frac{2}{3},\\infty]$. Previously, additivity was only known for $\\alpha\\in [1,\\infty]$, but has been conjectured for $\\alpha\\in [\\frac{1}{2},\\infty]$. Furthermore, we show that the doubly minimized sandwiched Renyi mutual information of order $\\alpha\\in [1,\\infty]$ attains operational meaning in the context of binary quantum state discrimination as it is linked to certain strong converse exponents.</article>","contentLength":875,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DSCA: A Digital Subtraction Angiography Sequence Dataset and Spatio-Temporal Model for Cerebral Artery Segmentation","url":"https://arxiv.org/abs/2406.00341","date":1740114000,"author":"","guid":7615,"unread":true,"content":"<article>arXiv:2406.00341v2 Announce Type: replace-cross \nAbstract: Cerebrovascular diseases (CVDs) remain a leading cause of global disability and mortality. Digital Subtraction Angiography (DSA) sequences, recognized as the gold standard for diagnosing CVDs, can clearly visualize the dynamic flow and reveal pathological conditions within the cerebrovasculature. Therefore, precise segmentation of cerebral arteries (CAs) and classification between their main trunks and branches are crucial for physicians to accurately quantify diseases. However, achieving accurate CA segmentation in DSA sequences remains a challenging task due to small vessels with low contrast, and ambiguity between vessels and residual skull structures. Moreover, the lack of publicly available datasets limits exploration in the field. In this paper, we introduce a DSA Sequence-based Cerebral Artery segmentation dataset (DSCA), the publicly accessible dataset designed specifically for pixel-level semantic segmentation of CAs. Additionally, we propose DSANet, a spatio-temporal network for CA segmentation in DSA sequences. Unlike existing DSA segmentation methods that focus only on a single frame, the proposed DSANet introduces a separate temporal encoding branch to capture dynamic vessel details across multiple frames. To enhance small vessel segmentation and improve vessel connectivity, we design a novel TemporalFormer module to capture global context and correlations among sequential frames. Furthermore, we develop a Spatio-Temporal Fusion (STF) module to effectively integrate spatial and temporal features from the encoder. Extensive experiments demonstrate that DSANet outperforms other state-of-the-art methods in CA segmentation, achieving a Dice of 0.9033.</article>","contentLength":1747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"eQMARL: Entangled Quantum Multi-Agent Reinforcement Learning for Distributed Cooperation over Quantum Channels","url":"https://arxiv.org/abs/2405.17486","date":1740114000,"author":"","guid":7616,"unread":true,"content":"<article>arXiv:2405.17486v2 Announce Type: replace-cross \nAbstract: Collaboration is a key challenge in distributed multi-agent reinforcement learning (MARL) environments. Learning frameworks for these decentralized systems must weigh the benefits of explicit player coordination against the communication overhead and computational cost of sharing local observations and environmental data. Quantum computing has sparked a potential synergy between quantum entanglement and cooperation in multi-agent environments, which could enable more efficient distributed collaboration with minimal information sharing. This relationship is largely unexplored, however, as current state-of-the-art quantum MARL (QMARL) implementations rely on classical information sharing rather than entanglement over a quantum channel as a coordination medium. In contrast, in this paper, a novel framework dubbed entangled QMARL (eQMARL) is proposed. The proposed eQMARL is a distributed actor-critic framework that facilitates cooperation over a quantum channel and eliminates local observation sharing via a quantum entangled split critic. Introducing a quantum critic uniquely spread across the agents allows coupling of local observation encoders through entangled input qubits over a quantum channel, which requires no explicit sharing of local observations and reduces classical communication overhead. Further, agent policies are tuned through joint observation-value function estimation via joint quantum measurements, thereby reducing the centralized computational burden. Experimental results show that eQMARL with ${\\Psi}^{+}$ entanglement converges to a cooperative strategy up to $17.8\\%$ faster and with a higher overall score compared to split classical and fully centralized classical and quantum baselines. The results also show that eQMARL achieves this performance with a constant factor of $25$-times fewer centralized parameters compared to the split classical baseline.</article>","contentLength":1959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conditioning diffusion models by explicit forward-backward bridging","url":"https://arxiv.org/abs/2405.13794","date":1740114000,"author":"","guid":7617,"unread":true,"content":"<article>arXiv:2405.13794v2 Announce Type: replace-cross \nAbstract: Given an unconditional diffusion model targeting a joint model $\\pi(x, y)$, using it to perform conditional simulation $\\pi(x \\mid y)$ is still largely an open question and is typically achieved by learning conditional drifts to the denoising SDE after the fact. In this work, we express \\emph{exact} conditional simulation within the \\emph{approximate} diffusion model as an inference problem on an augmented space corresponding to a partial SDE bridge. This perspective allows us to implement efficient and principled particle Gibbs and pseudo-marginal samplers marginally targeting the conditional distribution $\\pi(x \\mid y)$. Contrary to existing methodology, our methods do not introduce any additional approximation to the unconditional diffusion model aside from the Monte Carlo error. We showcase the benefits and drawbacks of our approach on a series of synthetic and real data examples.</article>","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Higher Order Lipschitz Sandwich Theorems","url":"https://arxiv.org/abs/2404.06849","date":1740114000,"author":"","guid":7618,"unread":true,"content":"<article>arXiv:2404.06849v3 Announce Type: replace-cross \nAbstract: We investigate the consequence of two Lip$(\\gamma)$ functions, in the sense of Stein, being close throughout a subset of their domain. A particular consequence of our results is the following. Given $K_0 &gt; \\varepsilon &gt; 0$ and $\\gamma &gt; \\eta &gt; 0$ there is a constant $\\delta = \\delta(\\gamma,\\eta,\\varepsilon,K_0) &gt; 0$ for which the following is true. Let $\\Sigma \\subset \\mathbb{R}^d$ be closed and $f , h : \\Sigma \\to \\mathbb{R}$ be Lip$(\\gamma)$ functions whose Lip$(\\gamma)$ norms are both bounded above by $K_0$. Suppose $B \\subset \\Sigma$ is closed and that $f$ and $h$ coincide throughout $B$. Then over the set of points in $\\Sigma$ whose distance to $B$ is at most $\\delta$ we have that the Lip$(\\eta)$ norm of the difference $f-h$ is bounded above by $\\varepsilon$. More generally, we establish that this phenomenon remains valid in a less restrictive Banach space setting under the weaker hypothesis that the two Lip$(\\gamma)$ functions $f$ and $h$ are only close in a pointwise sense throughout the closed subset $B$. We require only that the subset $\\Sigma$ be closed; in particular, the case that $\\Sigma$ is finite is covered by our results. The restriction that $\\eta &lt; \\gamma$ is sharp in the sense that our result is false for $\\eta := \\gamma$.</article>","contentLength":1320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revealing the Relationship Between Publication Bias and Chemical Reactivity with Contrastive Learning","url":"https://arxiv.org/abs/2402.16882","date":1740114000,"author":"","guid":7619,"unread":true,"content":"<article>arXiv:2402.16882v2 Announce Type: replace-cross \nAbstract: A synthetic method's substrate tolerance and generality are often showcased in a \"substrate scope\" table. However, substrate selection exhibits a frequently discussed publication bias: unsuccessful experiments or low-yielding results are rarely reported. In this work, we explore more deeply the relationship between such publication bias and chemical reactivity beyond the simple analysis of yield distributions using a novel neural network training strategy, substrate scope contrastive learning. By treating reported substrates as positive samples and non-reported substrates as negative samples, our contrastive learning strategy teaches a model to group molecules within a numerical embedding space, based on historical trends in published substrate scope tables. Training on 20,798 aryl halides in the CAS Content Collection$^{\\text{TM}}$, spanning thousands of publications from 2010-2015, we demonstrate that the learned embeddings exhibit a correlation with physical organic reactivity descriptors through both intuitive visualizations and quantitative regression analyses. Additionally, these embeddings are applicable to various reaction modeling tasks like yield prediction and regioselectivity prediction, underscoring the potential to use historical reaction data as a pre-training task. This work not only presents a chemistry-specific machine learning training strategy to learn from literature data in a new way, but also represents a unique approach to uncover trends in chemical reactivity reflected by trends in substrate selection in publications.</article>","contentLength":1627,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leave-One-Out-, Bootstrap- and Cross-Conformal Anomaly Detectors","url":"https://arxiv.org/abs/2402.16388","date":1740114000,"author":"","guid":7620,"unread":true,"content":"<article>arXiv:2402.16388v3 Announce Type: replace-cross \nAbstract: The requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\\alpha$) without compromising the statistical power ($1-\\beta$) of these systems can build trust and reduce costs related to false discoveries. The field of conformal anomaly detection emerges as a promising approach for providing respective statistical guarantees by model calibration. However, the dependency on calibration data poses practical limitations - especially within low-data regimes. In this work, we formally define and evaluate leave-one-out-, bootstrap-, and cross-conformal methods for anomaly detection, incrementing on methods from the field of conformal prediction. Looking beyond the classical inductive conformal anomaly detection, we demonstrate that derived methods for calculating resampling-conformal $p$-values strike a practical compromise between statistical efficiency (full-conformal) and computational efficiency (split-conformal) as they make more efficient use of available data. We validate derived methods and quantify their improvements for a range of one-class classifiers and datasets.</article>","contentLength":1256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Secret extraction attacks against obfuscated IQP circuits","url":"https://arxiv.org/abs/2312.10156","date":1740114000,"author":"","guid":7621,"unread":true,"content":"<article>arXiv:2312.10156v2 Announce Type: replace-cross \nAbstract: Quantum computing devices can now perform sampling tasks which, according to complexity-theoretic and numerical evidence, are beyond the reach of classical computers. This raises the question of how one can efficiently verify that a quantum computer operating in this regime works as intended. In 2008, Shepherd and Bremner proposed a protocol in which a verifier constructs a unitary from the comparatively easy-to-implement family of so-called IQP circuits, and challenges a prover to execute it on a quantum computer. The challenge problem is designed to contain an obfuscated secret, which can be turned into a statistical test that accepts samples from a correct quantum implementation. It was conjectured that extracting the secret from the challenge problem is NP-hard, so that the ability to pass the test constitutes strong evidence that the prover possesses a quantum device and that it works as claimed. Unfortunately, about a decade later, Kahanamoku-Meyer found an efficient classical secret extraction attack. Bremner, Cheng, and Ji very recently followed up by constructing a wide-ranging generalization of the original protocol. Their IQP Stabilizer Scheme has been explicitly designed to circumvent the known weakness. They also suggested that the original construction can be made secure by adjusting the problem parameters. In this work, we develop a number of secret extraction attacks which are effective against both new approaches in a wide range of problem parameters. In particular, we find multiple ways to recover the 300-bit secret hidden in a challenge data set published by Bremner, Cheng, and Ji. The important problem of finding an efficient and reliable verification protocol for sampling-based proofs of quantum supremacy thus remains open.</article>","contentLength":1833,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Characterizing traces of processes defined by precedence and response constraints: an order theory approach","url":"https://arxiv.org/abs/2311.12218","date":1740114000,"author":"","guid":7622,"unread":true,"content":"<article>arXiv:2311.12218v2 Announce Type: replace-cross \nAbstract: In this paper we consider a general system of activities that can, but do not have to, occur. This system is governed by a set containing two types of constraints: precedence and response. A precedence constraint dictates that an activity can only occur if it has been preceded by some other specified activity. Response constraints are similarly defined. An execution of the system is a listing of activities in the order they occur and which satisfies all constraints. These listings are known as traces. Such systems naturally arise in areas of theoretical computer science and decision science. An outcome of the freedom with which activities can occur is that there are many different possible executions, and gaining a combinatorial insight into these is a non-trivial problem.\n  We characterize all of the ways in which such a system can be executed. Our approach uses order theory to provide a classification in terms of the linear extensions of posets constructed from the constraint sets. This characterization is essential in calculating the stakeholder utility metrics that have been developed by the first author that allow for quantitative comparisons of such systems/processes. It also allows for a better understanding of the theoretical backbone to these processes.</article>","contentLength":1341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural Networks","url":"https://arxiv.org/abs/2311.11782","date":1740114000,"author":"","guid":7623,"unread":true,"content":"<article>arXiv:2311.11782v2 Announce Type: replace-cross \nAbstract: Segmenting the boundary between tumor and healthy tissue during surgical cancer resection poses a significant challenge. In recent years, Hyperspectral Imaging (HSI) combined with Machine Learning (ML) has emerged as a promising solution. However, due to the extensive information contained within the spectral domain, most ML approaches primarily classify individual HSI (super-)pixels, or tiles, without taking into account their spatial context. In this paper, we propose an improved methodology that leverages the spatial context of tiles for more robust and smoother segmentation. To address the irregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate context information across neighboring regions. The features for each tile within the graph are extracted using a Convolutional Neural Network (CNN), which is trained simultaneously with the subsequent GNN. Moreover, we incorporate local image quality metrics into the loss function to enhance the training procedure's robustness against low-quality regions in the training images. We demonstrate the superiority of our proposed method using a clinical ex vivo dataset consisting of 51 HSI images from 30 patients. Despite the limited dataset, the GNN-based model significantly outperforms context-agnostic approaches, accurately distinguishing between healthy and tumor tissues, even in images from previously unseen patients. Furthermore, we show that our carefully designed loss function, accounting for local image quality, results in additional improvements. Our findings demonstrate that context-aware GNN algorithms can robustly find tumor demarcations on HSI images, ultimately contributing to better surgery success and patient outcome.</article>","contentLength":1785,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Texture and Noise Dual Adaptation for Infrared Image Super-Resolution","url":"https://arxiv.org/abs/2311.08816","date":1740114000,"author":"","guid":7624,"unread":true,"content":"<article>arXiv:2311.08816v2 Announce Type: replace-cross \nAbstract: Recent efforts have explored leveraging visible light images to enrich texture details in infrared (IR) super-resolution. However, this direct adaptation approach often becomes a double-edged sword, as it improves texture at the cost of introducing noise and blurring artifacts. To address these challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN), an innovative framework specifically engineered for robust IR super-resolution model adaptation. DASRGAN operates on the synergy of two key components: 1) Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and 2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer. Specifically, TOA uniquely integrates a specialized discriminator, incorporating a prior extraction branch, and employs a Sobel-guided adversarial loss to align texture distributions effectively. Concurrently, NOA utilizes a noise adversarial loss to distinctly separate the generative and Gaussian noise pattern distributions during adversarial training. Our extensive experiments confirm DASRGAN's superiority. Comparative analyses against leading methods across multiple benchmarks and upsampling factors reveal that DASRGAN sets new state-of-the-art performance standards. Code are available at \\url{https://github.com/yongsongH/DASRGAN}.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Equal area partitions of the sphere with diameter bounds, via optimal transport","url":"https://arxiv.org/abs/2306.16239","date":1740114000,"author":"","guid":7625,"unread":true,"content":"<article>arXiv:2306.16239v2 Announce Type: replace-cross \nAbstract: We prove existence of equal area partitions of the unit sphere via optimal transport methods, accompanied by diameter bounds written in terms of Monge--Kantorovich distances. This can be used to obtain bounds on the expectation of the maximum diameter of partition sets, when points are uniformly sampled from the sphere. An application to the computation of sliced Monge--Kantorovich distances is also presented.</article>","contentLength":472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Classification of High-Dimensional Data using Data-Adaptive Energy Distance","url":"https://arxiv.org/abs/2306.13985","date":1740114000,"author":"","guid":7626,"unread":true,"content":"<article>arXiv:2306.13985v2 Announce Type: replace-cross \nAbstract: Classification of high-dimensional low sample size (HDLSS) data poses a challenge in a variety of real-world situations, such as gene expression studies, cancer research, and medical imaging. This article presents the development and analysis of some classifiers that are specifically designed for HDLSS data. These classifiers are free of tuning parameters and are robust, in the sense that they are devoid of any moment conditions of the underlying data distributions. It is shown that they yield perfect classification in the HDLSS asymptotic regime, under some fairly general conditions. The comparative performance of the proposed classifiers is also investigated. Our theoretical results are supported by extensive simulation studies and real data analysis, which demonstrate promising advantages of the proposed classification techniques over several widely recognized methods.</article>","contentLength":943,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spatially-Coupled QDLPC Codes","url":"https://arxiv.org/abs/2305.00137","date":1740114000,"author":"","guid":7627,"unread":true,"content":"<article>arXiv:2305.00137v4 Announce Type: replace-cross \nAbstract: Spatially-coupled (SC) codes is a class of convolutional LDPC codes that has been well investigated in classical coding theory thanks to their high performance and compatibility with low-latency decoders. We describe toric codes as quantum counterparts of classical two-dimensional spatially-coupled (2D-SC) codes, and introduce spatially-coupled quantum LDPC (SC-QLDPC) codes as a generalization. We use the convolutional structure to represent the parity check matrix of a 2D-SC code as a polynomial in two indeterminates, and derive an algebraic condition that is both necessary and sufficient for a 2D-SC code to be a stabilizer code. This algebraic framework facilitates the construction of new code families. While not the focus of this paper, we note that small memory facilitates physical connectivity of qubits, and it enables local encoding and low-latency windowed decoding. In this paper, we use the algebraic framework to optimize short cycles in the Tanner graph of 2D-SC hypergraph product (HGP) codes that arise from short cycles in either component code. While prior work focuses on QLDPC codes with rate less than 1/10, we construct 2D-SC HGP codes with small memories, higher rates (about 1/3), and superior thresholds.</article>","contentLength":1297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Infrared Image Super-Resolution: Systematic Review, and Future Trends","url":"https://arxiv.org/abs/2212.12322","date":1740114000,"author":"","guid":7628,"unread":true,"content":"<article>arXiv:2212.12322v4 Announce Type: replace-cross \nAbstract: Image Super-Resolution (SR) is essential for a wide range of computer vision and image processing tasks. Investigating infrared (IR) image (or thermal images) super-resolution is a continuing concern within the development of deep learning. This survey aims to provide a comprehensive perspective of IR image super-resolution, including its applications, hardware imaging system dilemmas, and taxonomy of image processing methodologies. In addition, the datasets and evaluation metrics in IR image super-resolution tasks are also discussed. Furthermore, the deficiencies in current technologies and possible promising directions for the community to explore are highlighted. To cope with the rapid development in this field, we intend to regularly update the relevant excellent work at \\url{https://github.com/yongsongH/Infrared_Image_SR_Survey</article>","contentLength":903,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Note on Quantum Divide and Conquer for Minimal String Rotation","url":"https://arxiv.org/abs/2210.09149","date":1740114000,"author":"","guid":7629,"unread":true,"content":"<article>arXiv:2210.09149v2 Announce Type: replace-cross \nAbstract: Lexicographically minimal string rotation is a fundamental problem in string processing that has recently garnered significant attention in quantum computing. Near-optimal quantum algorithms have been proposed for solving this problem, utilizing a divide-and-conquer structure. In this note, we show that its quantum query complexity is $\\sqrt{n} \\cdot 2^{O(\\sqrt{\\log n})}$, improving the prior result of $\\sqrt{n} \\cdot 2^{(\\log n)^{1/2+\\varepsilon}}$ due to Akmal and Jin (SODA 2022). Notably, this improvement is quasi-polylogarithmic, which is achieved by only logarithmic level-wise optimization using fault-tolerant quantum minimum finding.</article>","contentLength":706,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CoRRECT: A Deep Unfolding Framework for Motion-Corrected Quantitative R2* Mapping","url":"https://arxiv.org/abs/2210.06330","date":1740114000,"author":"","guid":7630,"unread":true,"content":"<article>arXiv:2210.06330v2 Announce Type: replace-cross \nAbstract: Quantitative MRI (qMRI) refers to a class of MRI methods for quantifying the spatial distribution of biological tissue parameters. Traditional qMRI methods usually deal separately with artifacts arising from accelerated data acquisition, involuntary physical motion, and magnetic-field inhomogeneities, leading to suboptimal end-to-end performance. This paper presents CoRRECT, a unified deep unfolding (DU) framework for qMRI consisting of a model-based end-to-end neural network, a method for motion-artifact reduction, and a self-supervised learning scheme. The network is trained to produce R2* maps whose k-space data matches the real data by also accounting for motion and field inhomogeneities. When deployed, CoRRECT only uses the k-space data without any pre-computed parameters for motion or inhomogeneity correction. Our results on experimentally collected multi-Gradient-Recalled Echo (mGRE) MRI data show that CoRRECT recovers motion and inhomogeneity artifact-free R2* maps in highly accelerated acquisition settings. This work opens the door to DU methods that can integrate physical measurement models, biophysical signal models, and learned prior models for high-quality qMRI.</article>","contentLength":1252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Joint Time-Vertex Fractional Fourier Transform","url":"https://arxiv.org/abs/2203.07655","date":1740114000,"author":"","guid":7631,"unread":true,"content":"<article>arXiv:2203.07655v3 Announce Type: replace-cross \nAbstract: Graph signal processing (GSP) facilitates the analysis of high-dimensional data on non-Euclidean domains by utilizing graph signals defined on graph vertices. In addition to static data, each vertex can provide continuous time-series signals, transforming graph signals into time-series signals on each vertex. The joint time-vertex Fourier transform (JFT) framework offers spectral analysis capabilities to analyze these joint time-vertex signals. Analogous to the fractional Fourier transform (FRT) extending the ordinary Fourier transform (FT), we introduce the joint time-vertex fractional Fourier transform (JFRT) as a generalization of JFT. The JFRT enables fractional analysis for joint time-vertex processing by extending Fourier analysis to fractional orders in both temporal and vertex domains. We theoretically demonstrate that JFRT generalizes JFT and maintains properties such as index additivity, reversibility, reduction to identity, and unitarity for specific graph topologies. Additionally, we derive Tikhonov regularization-based denoising in the JFRT domain, ensuring robust and well-behaved solutions. Comprehensive numerical experiments on synthetic and real-world datasets highlight the effectiveness of JFRT in denoising and clustering tasks that outperform state-of-the-art approaches.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Structural Stability Properties of Antithetic Integral (Rein) Control with Output Inhibition","url":"https://arxiv.org/abs/2201.13375","date":1740114000,"author":"","guid":7632,"unread":true,"content":"<article>arXiv:2201.13375v3 Announce Type: replace-cross \nAbstract: Perfect adaptation is a well-studied biochemical homeostatic behavior lying at the core of biochemical regulation. While the concepts of homeostasis and perfect adaptation are not new, their underlying mechanisms and associated biochemical regulation motifs are not yet fully understood. Insights from control theory unraveled the connections between perfect adaptation and integral control, a prevalent engineering control strategy. In particular, the recently introduced Antithetic Integral Controller (AIC) has been shown to successfully ensure perfect adaptation properties to the network it is connected to. The complementary structure of the two molecules the AIC relies upon allows for a versatile way to control biochemical networks, a property which gave rise to an important body of literature pertaining to mathematically elucidating its properties, generalizing its structure, and developing experimental methods for its implementation. The Antithetic Integral Rein Controller (AIRC), an extension of the AIC in which both controller molecules are used for control, holds many promises as it supposedly overcomes certain limitations of the AIC. We focus here on an AIRC structure with output inhibition that combines two AICs in a single structure. We demonstrate that rhis controller ensure structural stability and structural perfect adaptation properties for the controlled network under mild assumptions, meaning that this property is independent of the parameters of the network and the controller. The results are very general and valid for the class of unimolecular mass-action networks as well as more general networks, including cooperative and Michaelis-Menten networks. We also provide a systematic and accessible computational way for verifying whether a given network satisfies the conditions under which the structural property would hold.</article>","contentLength":1924,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2502.13019","date":1740114000,"author":"","guid":7633,"unread":true,"content":"<article>arXiv:2502.13019v2 Announce Type: replace \nAbstract: Despite the remarkable capabilities of Large Language Models (LLMs) in various NLP tasks, they remain vulnerable to hallucinations due to their limited parametric knowledge and lack of domain-specific expertise. Retrieval-Augmented Generation (RAG) addresses this challenge by incorporating external document retrieval to augment the knowledge base of LLMs. In this approach, RAG retrieves document chunks from an external corpus in response to a query, which are then used as context for the downstream language model to generate an answer. However, these retrieved knowledge sources often include irrelevant or erroneous information, undermining the effectiveness of RAG in downstream tasks. To overcome this limitation, we introduce a compact, efficient, and pluggable module designed to refine external knowledge sources before feeding them to the generator. The module reconstructs retrieved content by extracting the most relevant and supportive information and reorganising it into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritises critical knowledge and aligns it with the generator's preferences. This method enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.</article>","contentLength":1420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding Maximum Weight 2-Packing Sets on Arbitrary Graphs","url":"https://arxiv.org/abs/2502.12856","date":1740114000,"author":"","guid":7634,"unread":true,"content":"<article>arXiv:2502.12856v2 Announce Type: replace \nAbstract: A 2-packing set for an undirected, weighted graph G=(V,E,w) is a subset S of the vertices V such that any two vertices are not adjacent and have no common neighbors. The Maximum Weight 2-Packing Set problem that asks for a 2-packing set of maximum weight is NP-hard. Next to 13 novel data reduction rules for this problem, we develop two new approaches to solve this problem on arbitrary graphs. First, we introduce a preprocessing routine that exploits the close relation of 2-packing sets to independent sets. This makes well-studied independent set solvers usable for the Maximum Weight 2-Packing Set problem. Second, we propose an iterative reduce-and-peel approach that utilizes the new data reductions. Our experiments show that our preprocessing routine gives speedups of multiple orders of magnitude, while also improving solution quality, and memory consumption compared to a naive transformation to independent set instances. Furthermore, it solves 44 % of the instances tested to optimality. Our heuristic can keep up with the best-performing maximum weight independent set solvers combined with our preprocessing routine. Additionally, our heuristic can find the best solution quality on the biggest instances in our data set, outperforming all other approaches.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild","url":"https://arxiv.org/abs/2502.12769","date":1740114000,"author":"","guid":7635,"unread":true,"content":"<article>arXiv:2502.12769v2 Announce Type: replace \nAbstract: In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.</article>","contentLength":1724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CityEQA: A Hierarchical LLM Agent on Embodied Question Answering Benchmark in City Space","url":"https://arxiv.org/abs/2502.12532","date":1740114000,"author":"","guid":7636,"unread":true,"content":"<article>arXiv:2502.12532v2 Announce Type: replace \nAbstract: Embodied Question Answering (EQA) has primarily focused on indoor environments, leaving the complexities of urban settings - spanning environment, action, and perception - largely unexplored. To bridge this gap, we introduce CityEQA, a new task where an embodied agent answers open-vocabulary questions through active exploration in dynamic city spaces. To support this task, we present CityEQA-EC, the first benchmark dataset featuring 1,412 human-annotated tasks across six categories, grounded in a realistic 3D urban simulator. Moreover, we propose Planner-Manager-Actor (PMA), a novel agent tailored for CityEQA. PMA enables long-horizon planning and hierarchical task execution: the Planner breaks down the question answering into sub-tasks, the Manager maintains an object-centric cognitive map for spatial reasoning during the process control, and the specialized Actors handle navigation, exploration, and collection sub-tasks. Experiments demonstrate that PMA achieves 60.7% of human-level answering accuracy, significantly outperforming frontier-based baselines. While promising, the performance gap compared to humans highlights the need for enhanced visual reasoning in CityEQA. This work paves the way for future advancements in urban spatial intelligence. Dataset and code are available at https://github.com/BiluYong/CityEQA.git.</article>","contentLength":1398,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Contextual Linear Bandits with Delay as Payoff","url":"https://arxiv.org/abs/2502.12528","date":1740114000,"author":"","guid":7637,"unread":true,"content":"<article>arXiv:2502.12528v2 Announce Type: replace \nAbstract: A recent work by Schlisselberg et al. (2024) studies a delay-as-payoff model for stochastic multi-armed bandits, where the payoff (either loss or reward) is delayed for a period that is proportional to the payoff itself. While this captures many real-world applications, the simple multi-armed bandit setting limits the practicality of their results. In this paper, we address this limitation by studying the delay-as-payoff model for contextual linear bandits. Specifically, we start from the case with a fixed action set and propose an efficient algorithm whose regret overhead compared to the standard no-delay case is at most $D\\Delta_{\\max}\\log T$, where $T$ is the total horizon, $D$ is the maximum delay, and $\\Delta_{\\max}$ is the maximum suboptimality gap. When payoff is loss, we also show further improvement of the bound, demonstrating a separation between reward and loss similar to Schlisselberg et al. (2024). Contrary to standard linear bandit algorithms that construct least squares estimator and confidence ellipsoid, the main novelty of our algorithm is to apply a phased arm elimination procedure by only picking actions in a volumetric spanner of the action set, which addresses challenges arising from both payoff-dependent delays and large action sets. We further extend our results to the case with varying action sets by adopting the reduction from Hanna et al. (2023). Finally, we implement our algorithm and showcase its effectiveness and superior performance in experiments.</article>","contentLength":1555,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TherAIssist: Assisting Art Therapy Homework and Client-Practitioner Collaboration through Human-AI Interaction","url":"https://arxiv.org/abs/2502.12443","date":1740114000,"author":"","guid":7638,"unread":true,"content":"<article>arXiv:2502.12443v2 Announce Type: replace \nAbstract: Art therapy homework is essential for fostering clients' reflection on daily experiences between sessions. However, current practices present challenges: clients often lack guidance for completing tasks that combine art-making and verbal expression, while therapists find it difficult to track and tailor homework. How HCI systems might support art therapy homework remains underexplored. To address this, we present TherAIssist, comprising a client-facing application leveraging human-AI co-creative art-making and conversational agents to facilitate homework, and a therapist-facing application enabling customization of homework agents and AI-compiled homework history. A 30-day field study with 24 clients and 5 therapists showed how TherAIssist supported clients' homework and reflection in their everyday settings. Results also revealed how therapists infused their practice principles and personal touch into the agents to offer tailored homework, and how AI-compiled homework history became a meaningful resource for in-session interactions. Implications for designing human-AI systems to facilitate asynchronous client-practitioner collaboration are discussed.</article>","contentLength":1222,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linear Diffusion Networks: Harnessing Diffusion Processes for Global Interactions","url":"https://arxiv.org/abs/2502.12381","date":1740114000,"author":"","guid":7639,"unread":true,"content":"<article>arXiv:2502.12381v2 Announce Type: replace \nAbstract: Diffusion kernels capture global dependencies. We present Linear Diffusion Networks (LDNs), a novel architecture that reinterprets sequential data processing as a unified diffusion process. Our model integrates adaptive diffusion modules with localized nonlinear updates and a diffusion-inspired attention mechanism. This design enables efficient global information propagation while preserving fine-grained temporal details. LDN overcomes the limitations of conventional recurrent and transformer models by allowing full parallelization across time steps and supporting robust multi-scale temporal representations. Experiments on benchmark sequence modeling tasks demonstrate that LDN delivers superior performance and scalability, setting a new standard for global interaction in sequential data.</article>","contentLength":851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ConFit v2: Improving Resume-Job Matching using Hypothetical Resume Embedding and Runner-Up Hard-Negative Mining","url":"https://arxiv.org/abs/2502.12361","date":1740114000,"author":"","guid":7640,"unread":true,"content":"<article>arXiv:2502.12361v2 Announce Type: replace \nAbstract: A reliable resume-job matching system helps a company recommend suitable candidates from a pool of resumes and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction labels in resume-job datasets are sparse. We introduce ConFit v2, an improvement over ConFit to tackle this sparsity problem. We propose two techniques to enhance the encoder's contrastive training process: augmenting job data with hypothetical reference resume generated by a large language model; and creating high-quality hard negatives from unlabeled resume/job pairs using a novel hard-negative mining strategy. We evaluate ConFit v2 on two real-world datasets and demonstrate that it outperforms ConFit and prior methods (including BM25 and OpenAI text-embedding-003), achieving an average absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking and resume-ranking tasks.</article>","contentLength":991,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Reason at the Frontier of Learnability","url":"https://arxiv.org/abs/2502.12272","date":1740114000,"author":"","guid":7641,"unread":true,"content":"<article>arXiv:2502.12272v2 Announce Type: replace \nAbstract: Reinforcement learning is now widely adopted as the final stage of large language model training, especially for reasoning-style tasks such as maths problems. Typically, models attempt each question many times during a single training step and attempt to learn from their successes and failures. However, we demonstrate that throughout training with two popular algorithms (PPO and VinePPO) on two widely used datasets, many questions are either solved by all attempts - meaning they are already learned - or by none - providing no meaningful training signal. To address this, we adapt a method from the reinforcement learning literature - sampling for learnability - and apply it to the reinforcement learning stage of LLM training. Our curriculum prioritises questions with high variance of success, i.e. those where the agent sometimes succeeds, but not always. Our findings demonstrate that this curriculum consistently boosts training performance across multiple algorithms and datasets, paving the way for more efficient and effective reinforcement learning in LLMs.</article>","contentLength":1125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrated Scheduling Model for Arrivals and Departures in Metroplex Terminal Area","url":"https://arxiv.org/abs/2502.12196","date":1740114000,"author":"","guid":7642,"unread":true,"content":"<article>arXiv:2502.12196v2 Announce Type: replace \nAbstract: In light of the rapid expansion of civil aviation, addressing the delays and congestion phenomena in the vicinity of metroplex caused by the imbalance between air traffic flow and capacity is crucial. This paper first proposes a bi-level optimization model for the collaborative flight sequencing of arrival and departure flights in the metroplex with multiple airports, considering both the runway systems and TMA (Terminal Control Area) entry/exit fixes. Besides, the model is adaptive to various traffic scenarios. The genetic algorithm is employed to solve the proposed model. The Shanghai TMA, located in China, is used as a case study, and it includes two airports, Shanghai Hongqiao International Airport and Shanghai Pudong International Airport. The results demonstrate that the model can reduce arrival delay by 51.52%, departure delay by 18.05%, and the runway occupation time of departure flights by 23.83%. Furthermore, the model utilized in this study significantly enhances flight scheduling efficiency, providing a more efficient solution than the traditional FCFS (First Come, First Served) approach. Additionally, the algorithm employed offers further improvements over the NSGA II algorithm.</article>","contentLength":1263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views","url":"https://arxiv.org/abs/2502.12138","date":1740114000,"author":"","guid":7643,"unread":true,"content":"<article>arXiv:2502.12138v2 Announce Type: replace \nAbstract: We present FLARE, a feed-forward model designed to infer high-quality camera poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8 inputs), which is a challenging yet practical setting in real-world applications. Our solution features a cascaded learning paradigm with camera pose serving as the critical bridge, recognizing its essential role in mapping 3D structures onto 2D image planes. Concretely, FLARE starts with camera pose estimation, whose results condition the subsequent learning of geometric structure and appearance, optimized through the objectives of geometry reconstruction and novel-view synthesis. Utilizing large-scale public datasets for training, our method delivers state-of-the-art performance in the tasks of pose estimation, geometry reconstruction, and novel view synthesis, while maintaining the inference efficiency (i.e., less than 0.5 seconds). The project page and code can be found at: https://zhanghe3z.github.io/FLARE/</article>","contentLength":1033,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta-Statistical Learning: Supervised Learning of Statistical Inference","url":"https://arxiv.org/abs/2502.12088","date":1740114000,"author":"","guid":7644,"unread":true,"content":"<article>arXiv:2502.12088v2 Announce Type: replace \nAbstract: This work demonstrates that the tools and principles driving the success of large language models (LLMs) can be repurposed to tackle distribution-level tasks, where the goal is to predict properties of the data-generating distribution rather than labels for individual datapoints. These tasks encompass statistical inference problems such as parameter estimation, hypothesis testing, or mutual information estimation. Framing these tasks within traditional machine learning pipelines is challenging, as supervision is typically tied to individual datapoint. We propose meta-statistical learning, a framework inspired by multi-instance learning that reformulates statistical inference tasks as supervised learning problems. In this approach, entire datasets are treated as single inputs to neural networks, which predict distribution-level parameters. Transformer-based architectures, without positional encoding, provide a natural fit due to their permutation-invariance properties. By training on large-scale synthetic datasets, meta-statistical models can leverage the scalability and optimization infrastructure of Transformer-based LLMs. We demonstrate the framework's versatility with applications in hypothesis testing and mutual information estimation, showing strong performance, particularly for small datasets where traditional neural methods struggle.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics","url":"https://arxiv.org/abs/2502.11859","date":1740114000,"author":"","guid":7645,"unread":true,"content":"<article>arXiv:2502.11859v2 Announce Type: replace \nAbstract: The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.95 vs. 68.38), with three key findings: 1) VLMs mirror human hierarchies (strongest in 2D orientation, weakest in 3D rotation) with independent BSAs (Pearson's r&lt;0.4); 2) Smaller models such as Qwen2-VL-7B surpass larger counterparts, with Qwen leading (30.82) and InternVL2 lagging (19.6); 3) Interventions like chain-of-thought (0.100 accuracy gain) and 5-shot training (0.259 improvement) show limits from architectural constraints. Identified barriers include weak geometry encoding and missing dynamic simulation. By linking psychometric BSAs to VLM capabilities, we provide a diagnostic toolkit for spatial intelligence evaluation, methodological foundations for embodied AI development, and a cognitive science-informed roadmap for achieving human-like spatial intelligence.</article>","contentLength":1333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BaxBench: Can LLMs Generate Correct and Secure Backends?","url":"https://arxiv.org/abs/2502.11844","date":1740114000,"author":"","guid":7646,"unread":true,"content":"<article>arXiv:2502.11844v2 Announce Type: replace \nAbstract: The automatic generation of programs has long been a fundamental challenge in computer science. Recent benchmarks have shown that large language models (LLMs) can effectively generate code at the function level, make code edits, and solve algorithmic coding tasks. However, to achieve full automation, LLMs should be able to generate production-quality, self-contained application modules. To evaluate the capabilities of LLMs in solving this challenge, we introduce BaxBench, a novel evaluation benchmark consisting of 392 tasks for the generation of backend applications. We focus on backends for three critical reasons: (i) they are practically relevant, building the core components of most modern web and cloud software, (ii) they are difficult to get right, requiring multiple functions and files to achieve the desired functionality, and (iii) they are security-critical, as they are exposed to untrusted third-parties, making secure solutions that prevent deployment-time attacks an imperative. BaxBench validates the functionality of the generated applications with comprehensive test cases, and assesses their security exposure by executing end-to-end exploits. Our experiments reveal key limitations of current LLMs in both functionality and security: (i) even the best model, OpenAI o1, achieves a mere 60% on code correctness; (ii) on average, we could successfully execute security exploits on more than half of the correct programs generated by each LLM; and (iii) in less popular backend frameworks, models further struggle to generate correct and secure applications. Progress on BaxBench signifies important steps towards autonomous and secure software development with LLMs.</article>","contentLength":1746,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars","url":"https://arxiv.org/abs/2502.11681","date":1740114000,"author":"","guid":7647,"unread":true,"content":"<article>arXiv:2502.11681v2 Announce Type: replace \nAbstract: Alignment tuning is crucial for ensuring large language models (LLMs) behave ethically and helpfully. Current alignment approaches require high-quality annotations and significant training resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment. Through an analysis of high-quality ICL demos, we identified style as a key factor influencing LLM alignment capabilities and explicitly restyled ICL exemplars based on this stylistic framework. Additionally, we combined the restyled demos to achieve a balance between the two conflicting aspects of LLM alignment--factuality and safety. We packaged the restyled examples as prompts to trigger few-shot learning, improving LLM alignment. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22 enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the code and data at https://github.com/AnonymousCode-ComputerScience/RIDE.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$\\text{M}^{\\text{3}}$: A Modular World Model over Streams of Tokens","url":"https://arxiv.org/abs/2502.11537","date":1740114000,"author":"","guid":7648,"unread":true,"content":"<article>arXiv:2502.11537v2 Announce Type: replace \nAbstract: Token-based world models emerged as a promising modular framework, modeling dynamics over token streams while optimizing tokenization separately. While successful in visual environments with discrete actions (e.g., Atari games), their broader applicability remains uncertain. In this paper, we introduce $\\text{M}^{\\text{3}}$, a $\\textbf{m}$odular $\\textbf{w}$orld $\\textbf{m}$odel that extends this framework, enabling flexible combinations of observation and action modalities through independent modality-specific components. $\\text{M}^{\\text{3}}$ integrates several improvements from existing literature to enhance agent performance. Through extensive empirical evaluation across diverse benchmarks, $\\text{M}^{\\text{3}}$ achieves state-of-the-art sample efficiency for planning-free world models. Notably, among these methods, it is the first to reach a human-level median score on Atari 100K, with superhuman performance on 13 games. Our code and model weights are publicly available at https://github.com/leor-c/M3.</article>","contentLength":1075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Safety Evaluation of DeepSeek Models in Chinese Contexts","url":"https://arxiv.org/abs/2502.11137","date":1740114000,"author":"","guid":7649,"unread":true,"content":"<article>arXiv:2502.11137v2 Announce Type: replace \nAbstract: Recently, the DeepSeek series of models, leveraging their exceptional reasoning capabilities and open-source strategy, is reshaping the global AI landscape. Despite these advantages, they exhibit significant safety deficiencies. Research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 has a 100\\% attack success rate when processing harmful prompts. Additionally, multiple safety companies and research institutions have confirmed critical safety vulnerabilities in this model. As models demonstrating robust performance in Chinese and English, DeepSeek models require equally crucial safety assessments in both language contexts. However, current research has predominantly focused on safety evaluations in English environments, leaving a gap in comprehensive assessments of their safety performance in Chinese contexts. In response to this gap, this study introduces CHiSafetyBench, a Chinese-specific safety evaluation benchmark. This benchmark systematically evaluates the safety of DeepSeek-R1 and DeepSeek-V3 in Chinese contexts, revealing their performance across safety categories. The experimental results quantify the deficiencies of these two models in Chinese contexts, providing key insights for subsequent improvements. It should be noted that, despite our efforts to establish a comprehensive, objective, and authoritative evaluation benchmark, the selection of test samples, characteristics of data distribution, and the setting of evaluation criteria may inevitably introduce certain biases into the evaluation results. We will continuously optimize the evaluation benchmark and periodically update this report to provide more comprehensive and accurate assessment outcomes. Please refer to the latest version of the paper for the most recent evaluation results and conclusions.</article>","contentLength":1938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Open-Source Web-Based Tool for Evaluating Open-Source Large Language Models Leveraging Information Retrieval from Custom Documents","url":"https://arxiv.org/abs/2502.10916","date":1740114000,"author":"","guid":7650,"unread":true,"content":"<article>arXiv:2502.10916v2 Announce Type: replace \nAbstract: In our work, we present the first-of-its-kind open-source web-based tool which is able to demonstrate the impacts of a user's speech act during discourse with conversational agents, which leverages open-source large language models. With this software resource, it is possible for researchers and experts to evaluate the performance of various dialogues, visualize the user's communicative intents, and utilise uploaded specific documents for the chat agent to use for its information retrieval to respond to the user query. The context gathered by these models is obtained from a set of linguistic features extracted, which forms the context embeddings of the models. Regardless of these models showing good context understanding based on these features, there still remains a gap in including deeper pragmatic features to improve the model's comprehension of the query, hence the efforts to develop this web resource, which is able to extract and then inject this overlooked feature in the encoder-decoder pipeline of the conversational agent. To demonstrate the effect and impact of the resource, we carried out an experiment which evaluated the system using 2 knowledge files for information retrieval, with two user queries each, across 5 open-source large language models using 10 standard metrics. Our results showed that larger open-source models, demonstrated an improved alignment when the user speech act was included with their query. The smaller models in contrast showed an increased perplexity and mixed performance, which explicitly indicated struggles in processing queries that explicitly included speech acts. The results from the analysis using the developed web resource highlight the potential of speech acts towards enhancing conversational depths while underscoring the need for model-specific optimizations to address increased computational costs and response times.</article>","contentLength":1945,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tempo: Helping Data Scientists and Domain Experts Collaboratively Specify Predictive Modeling Tasks","url":"https://arxiv.org/abs/2502.10526","date":1740114000,"author":"","guid":7651,"unread":true,"content":"<article>arXiv:2502.10526v2 Announce Type: replace \nAbstract: Temporal predictive models have the potential to improve decisions in health care, public services, and other domains, yet they often fail to effectively support decision-makers. Prior literature shows that many misalignments between model behavior and decision-makers' expectations stem from issues of model specification, namely how, when, and for whom predictions are made. However, model specifications for predictive tasks are highly technical and difficult for non-data-scientist stakeholders to interpret and critique. To address this challenge we developed Tempo, an interactive system that helps data scientists and domain experts collaboratively iterate on model specifications. Using Tempo's simple yet precise temporal query language, data scientists can quickly prototype specifications with greater transparency about pre-processing choices. Moreover, domain experts can assess performance within data subgroups to validate that models behave as expected. Through three case studies, we demonstrate how Tempo helps multidisciplinary teams quickly prune infeasible specifications and identify more promising directions to explore.</article>","contentLength":1196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FlexControl: Computation-Aware ControlNet with Differentiable Router for Text-to-Image Generation","url":"https://arxiv.org/abs/2502.10451","date":1740114000,"author":"","guid":7652,"unread":true,"content":"<article>arXiv:2502.10451v2 Announce Type: replace \nAbstract: ControlNet offers a powerful way to guide diffusion-based generative models, yet most implementations rely on ad-hoc heuristics to choose which network blocks to control-an approach that varies unpredictably with different tasks. To address this gap, we propose FlexControl, a novel framework that copies all diffusion blocks during training and employs a trainable gating mechanism to dynamically select which blocks to activate at each denoising step. With introducing a computation-aware loss, we can encourage control blocks only to activate when it benefit the generation quality. By eliminating manual block selection, FlexControl enhances adaptability across diverse tasks and streamlines the design pipeline, with computation-aware training loss in an end-to-end training manner. Through comprehensive experiments on both UNet (e.g., SD1.5) and DiT (e.g., SD3.0), we show that our method outperforms existing ControlNet variants in certain key aspects of interest. As evidenced by both quantitative and qualitative evaluations, FlexControl preserves or enhances image fidelity while also reducing computational overhead by selectively activating the most relevant blocks. These results underscore the potential of a flexible, data-driven approach for controlled diffusion and open new avenues for efficient generative model design. The code will soon be available at https://github.com/Anonymousuuser/FlexControl.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DPM-Bench: Benchmark for Distributed Process Mining Algorithms on Cyber-Physical Systems","url":"https://arxiv.org/abs/2502.09975","date":1740114000,"author":"","guid":7653,"unread":true,"content":"<article>arXiv:2502.09975v2 Announce Type: replace \nAbstract: Process Mining is established in research and industry systems to analyze and optimize processes based on event data from information systems. Within this work, we accomodate process mining techniques to Cyber-Physical Systems. To capture the distributed and heterogeneous characteristics of data, computational resources, and network communication in CPS, the todays process mining algorithms and techniques must be augmented. Specifically, there is a need for new Distributed Process Mining algorithms that enable computations to be performed directly on edge resources, eliminating the need for moving all data to central cloud systems. This paper introduces the DPM-Bench benchmark for comparing such Distributed Process Mining algorithms. DPM-Bench is used to compare algorithms deployed in different computational topologies. The results enable information system engineers to assess whether the existing infrastructure is sufficient to perform distributed process mining, or to identify required improvements in algorithms and hardware. We present and discuss an experimental evaluation with DPM-Bench.</article>","contentLength":1162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compression-Aware One-Step Diffusion Model for JPEG Artifact Removal","url":"https://arxiv.org/abs/2502.09873","date":1740114000,"author":"","guid":7654,"unread":true,"content":"<article>arXiv:2502.09873v2 Announce Type: replace \nAbstract: Diffusion models have demonstrated remarkable success in image restoration tasks. However, their multi-step denoising process introduces significant computational overhead, limiting their practical deployment. Furthermore, existing methods struggle to effectively remove severe JPEG artifact, especially in highly compressed images. To address these challenges, we propose CODiff, a compression-aware one-step diffusion model for JPEG artifact removal. The core of CODiff is the compression-aware visual embedder (CaVE), which extracts and leverages JPEG compression priors to guide the diffusion model. We propose a dual learning strategy that combines explicit and implicit learning. Specifically, explicit learning enforces a quality prediction objective to differentiate low-quality images with different compression levels. Implicit learning employs a reconstruction objective that enhances the model's generalization. This dual learning allows for a deeper and more comprehensive understanding of JPEG compression. Experimental results demonstrate that CODiff surpasses recent leading methods in both quantitative and visual quality metrics. The code and models will be released at https://github.com/jp-guo/CODiff.</article>","contentLength":1274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DesignWeaver: Dimensional Scaffolding for Text-to-Image Product Design","url":"https://arxiv.org/abs/2502.09867","date":1740114000,"author":"","guid":7655,"unread":true,"content":"<article>arXiv:2502.09867v2 Announce Type: replace \nAbstract: Generative AI has enabled novice designers to quickly create professional-looking visual representations for product concepts. However, novices have limited domain knowledge that could constrain their ability to write prompts that effectively explore a product design space. To understand how experts explore and communicate about design spaces, we conducted a formative study with 12 experienced product designers and found that experts -- and their less-versed clients -- often use visual references to guide co-design discussions rather than written descriptions. These insights inspired DesignWeaver, an interface that helps novices generate prompts for a text-to-image model by surfacing key product design dimensions from generated images into a palette for quick selection. In a study with 52 novices, DesignWeaver enabled participants to craft longer prompts with more domain-specific vocabularies, resulting in more diverse, innovative product designs. However, the nuanced prompts heightened participants' expectations beyond what current text-to-image models could deliver. We discuss implications for AI-based product design support tools.</article>","contentLength":1204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models","url":"https://arxiv.org/abs/2502.09723","date":1740114000,"author":"","guid":7656,"unread":true,"content":"<article>arXiv:2502.09723v2 Announce Type: replace \nAbstract: Recent advances in large language models (LLMs) have demonstrated remarkable potential in the field of natural language processing. Unfortunately, LLMs face significant security and ethical risks. Although techniques such as safety alignment are developed for defense, prior researches reveal the possibility of bypassing such defenses through well-designed jailbreak attacks. In this paper, we propose QueryAttack, a novel framework to examine the generalizability of safety alignment. By treating LLMs as knowledge databases, we translate malicious queries in natural language into structured non-natural query language to bypass the safety alignment mechanisms of LLMs. We conduct extensive experiments on mainstream LLMs, and the results show that QueryAttack not only can achieve high attack success rates (ASRs), but also can jailbreak various defense methods. Furthermore, we tailor a defense method against QueryAttack, which can reduce ASR by up to 64% on GPT-4-1106. Our code is available at https://github.com/horizonsinzqs/QueryAttack.</article>","contentLength":1100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?","url":"https://arxiv.org/abs/2502.09636","date":1740114000,"author":"","guid":7657,"unread":true,"content":"<article>arXiv:2502.09636v2 Announce Type: replace \nAbstract: In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83\\% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: https://github.com/sougata-ub/reading_between_lines</article>","contentLength":872,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"When do neural networks learn world models?","url":"https://arxiv.org/abs/2502.09297","date":1740114000,"author":"","guid":7658,"unread":true,"content":"<article>arXiv:2502.09297v2 Announce Type: replace \nAbstract: Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we provide the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions -- even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is also sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.</article>","contentLength":1013,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WENDy for Nonlinear-in-Parameters ODEs","url":"https://arxiv.org/abs/2502.08881","date":1740114000,"author":"","guid":7659,"unread":true,"content":"<article>arXiv:2502.08881v2 Announce Type: replace \nAbstract: The Weak-form Estimation of Non-linear Dynamics (WENDy) algorithm is extended to accommodate systems of ordinary differential equations that are nonlinear-in-parameters. The extension rests on derived analytic expressions for a likelihood function, its gradient and its Hessian matrix. WENDy makes use of these to approximate a maximum likelihood estimator based on optimization routines suited for non-convex optimization problems. The resulting parameter estimation algorithm has better accuracy, a substantially larger domain of convergence, and is often orders of magnitude faster than the conventional output error least squares method (based on forward solvers).\n  The algorithm is efficiently implemented in Julia. We demonstrate the algorithm's ability to accommodate the weak form optimization for both additive normal and multiplicative log-normal noise, and present results on a suite of benchmark systems of ordinary differential equations. In order to demonstrate the practical benefits of our approach, we present extensive comparisons between our method and output error methods in terms of accuracy, precision, bias, and coverage.</article>","contentLength":1199,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Who is Responsible? The Data, Models, Users or Regulations? Responsible Generative AI for a Sustainable Future","url":"https://arxiv.org/abs/2502.08650","date":1740114000,"author":"","guid":7660,"unread":true,"content":"<article>arXiv:2502.08650v2 Announce Type: replace \nAbstract: Responsible Artificial Intelligence (RAI) has emerged as a crucial framework for addressing ethical concerns in the development and deployment of Artificial Intelligence (AI) systems. A significant body of literature exists, primarily focusing on either RAI guidelines and principles or the technical aspects of RAI, largely within the realm of traditional AI. However, a notable gap persists in bridging theoretical frameworks with practical implementations in real-world settings, as well as transitioning from RAI to Responsible Generative AI (Gen AI). To bridge this gap, we present this article, which examines the challenges and opportunities in implementing ethical, transparent, and accountable AI systems in the post-ChatGPT era, an era significantly shaped by Gen AI. Our analysis includes governance and technical frameworks, the exploration of explainable AI as the backbone to achieve RAI, key performance indicators in RAI, alignment of Gen AI benchmarks with governance frameworks, reviews of AI-ready test beds, and RAI applications across multiple sectors. Additionally, we discuss challenges in RAI implementation and provide a philosophical perspective on the future of RAI. This comprehensive article aims to offer an overview of RAI, providing valuable insights for researchers, policymakers, users, and industry practitioners to develop and deploy AI systems that benefit individuals and society while minimizing potential risks and societal impacts. A curated list of resources and datasets covered in this survey is available on GitHub {https://github.com/anas-zafar/Responsible-AI}.</article>","contentLength":1660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models","url":"https://arxiv.org/abs/2502.08130","date":1740114000,"author":"","guid":7661,"unread":true,"content":"<article>arXiv:2502.08130v2 Announce Type: replace \nAbstract: Fine-tuning Large Language Models (LLMs) on specific datasets is a common practice to improve performance on target tasks. However, this performance gain often leads to overfitting, where the model becomes too specialized in either the task or the characteristics of the training data, resulting in a loss of generalization. This paper introduces Selective Self-to-Supervised Fine-Tuning (S3FT), a fine-tuning approach that achieves better performance than the standard supervised fine-tuning (SFT) while improving generalization. S3FT leverages the existence of multiple valid responses to a query. By utilizing the model's correct responses, S3FT reduces model specialization during the fine-tuning stage. S3FT first identifies the correct model responses from the training set by deploying an appropriate judge. Then, it fine-tunes the model using the correct model responses and the gold response (or its paraphrase) for the remaining samples. The effectiveness of S3FT is demonstrated through experiments on mathematical reasoning, Python programming and reading comprehension tasks. The results show that standard SFT can lead to an average performance drop of up to $4.4$ on multiple benchmarks, such as MMLU and TruthfulQA. In contrast, S3FT reduces this drop by half, i.e. $2.5$, indicating better generalization capabilities than SFT while performing significantly better on the fine-tuning tasks.</article>","contentLength":1460,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WorldGUI: Dynamic Testing for Comprehensive Desktop GUI Automation","url":"https://arxiv.org/abs/2502.08047","date":1740114000,"author":"","guid":7662,"unread":true,"content":"<article>arXiv:2502.08047v2 Announce Type: replace \nAbstract: Current GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state-often lead to planning errors. This issue is widespread in real user scenarios, but existing benchmarks fail to evaluate it. In this paper, we present WorldGUI, a novel GUI benchmark that designs GUI tasks with various initial states to simulate real computer-user interactions. The benchmark spans a wide range of tasks across 10 popular software applications, including PowerPoint, VSCode, and Adobe Acrobat. In addition, to address the challenges of dynamic GUI automation tasks, we propose GUI-Thinker, a holistic framework, leveraging a critique mechanism, that effectively manages the unpredictability and complexity of GUI interactions. Experimental results demonstrate that GUI-Thinker significantly outperforms Claude-3.5 (Computer Use) by 14.9% in success rate on WorldGUI tasks. This improvement underscores the effectiveness of our critical-thinking-based framework in enhancing GUI automation. The code is available at https://github.com/showlab/WorldGUI.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Efficient Optimizer Design for LLM via Structured Fisher Approximation with a Low-Rank Extension","url":"https://arxiv.org/abs/2502.07752","date":1740114000,"author":"","guid":7663,"unread":true,"content":"<article>arXiv:2502.07752v2 Announce Type: replace \nAbstract: Designing efficient optimizers for large language models (LLMs) with low-memory requirements and fast convergence is an important and challenging problem. This paper makes a step towards the systematic design of such optimizers through the lens of structured Fisher information matrix (FIM) approximation. We show that many state-of-the-art efficient optimizers can be viewed as solutions to FIM approximation (under the Frobenius norm) with specific structural assumptions. Building on these insights, we propose two design recommendations of practical efficient optimizers for LLMs, involving the careful selection of structural assumptions to balance generality and efficiency, and enhancing memory efficiency of optimizers with general structures through a novel low-rank extension framework. We demonstrate how to use each design approach by deriving new memory-efficient optimizers: Row and Column Scaled SGD (RACS) and Adaptive low-dimensional subspace estimation (Alice). Experiments on LLaMA pre-training (up to 1B parameters) validate the effectiveness, showing faster and better convergence than existing memory-efficient baselines and Adam with little memory overhead. Notably, Alice achieves better than 2x faster convergence over Adam, while RACS delivers strong performance on the 1B model with SGD-like memory.</article>","contentLength":1379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Global Ease of Living Index: a machine learning framework for longitudinal analysis of major economies","url":"https://arxiv.org/abs/2502.06866","date":1740114000,"author":"","guid":7664,"unread":true,"content":"<article>arXiv:2502.06866v2 Announce Type: replace \nAbstract: The drastic changes in the global economy, geopolitical conditions, and disruptions such as the COVID-19 pandemic have impacted the cost of living and quality of life. It is important to understand the long-term nature of the cost of living and quality of life in major economies. A transparent and comprehensive living index must include multiple dimensions of living conditions. In this study, we present an approach to quantifying the quality of life through the Global Ease of Living Index that combines various socio-economic and infrastructural factors into a single composite score. Our index utilises economic indicators that define living standards, which could help in targeted interventions to improve specific areas. We present a machine learning framework for addressing the problem of missing data for some of the economic indicators for specific countries. We then curate and update the data and use a dimensionality reduction approach (principal component analysis) to create the Ease of Living Index for major economies since 1970. Our work significantly adds to the literature by offering a practical tool for policymakers to identify areas needing improvement, such as healthcare systems, employment opportunities, and public safety. Our approach with open data and code can be easily reproduced and applied to various contexts. This transparency and accessibility make our work a valuable resource for ongoing research and policy development in quality-of-life assessment.</article>","contentLength":1545,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aligning Human and Machine Attention for Enhanced Supervised Learning","url":"https://arxiv.org/abs/2502.06811","date":1740114000,"author":"","guid":7665,"unread":true,"content":"<article>arXiv:2502.06811v2 Announce Type: replace \nAbstract: Attention, or prioritization of certain information items over others, is a critical element of any learning process, for both humans and machines. Given that humans continue to outperform machines in certain learning tasks, it seems plausible that machine performance could be enriched by aligning machine attention with human attention mechanisms -- yet research on this topic is sparse and has achieved only limited success. This paper proposes a new approach to address this gap, called Human-Machine Attention Learning (HuMAL). This approach involves reliance on data annotated by humans to reflect their self-perceived attention during specific tasks. We evaluate several alternative strategies for integrating such human attention data into machine learning (ML) algorithms, using a sentiment analysis task (review data from Yelp) and a personality-type classification task (data from myPersonality). The best-performing HuMAL strategy significantly enhances the task performance of fine-tuned transformer models (BERT, as well as GPT-2 and XLNET), and the benefit is particularly pronounced under challenging conditions of imbalanced or sparse labeled data. This research contributes to a deeper understanding of strategies for integrating human attention into ML models and highlights the potential of leveraging human cognition to augment ML in real-world applications.</article>","contentLength":1432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ProjectTest: A Project-level LLM Unit Test Generation Benchmark and Impact of Error Fixing Mechanisms","url":"https://arxiv.org/abs/2502.06556","date":1740114000,"author":"","guid":7666,"unread":true,"content":"<article>arXiv:2502.06556v3 Announce Type: replace \nAbstract: Unit test generation has become a promising and important use case of LLMs. However, existing evaluation benchmarks for assessing LLM unit test generation capabilities focus on function- or class-level code rather than more practical and challenging project-level codebases. To address such limitation, we propose ProjectTest, a project-level benchmark for unit test generation covering Python, Java, and JavaScript. ProjectTest features 20 moderate-sized and high-quality projects per language. We evaluate nine frontier LLMs on ProjectTest and the results show that all frontier LLMs tested exhibit moderate performance on ProjectTest on Python and Java, highlighting the difficulty of ProjectTest. We also conduct a thorough error analysis, which shows that even frontier LLMs, such as Claude-3.5-Sonnet, have significant basic yet critical errors, including compilation and cascade errors. Motivated by this observation, we further evaluate all frontier LLMs under manual error-fixing and self-error-fixing scenarios to assess their potential when equipped with error-fixing mechanisms. Our code and dataset is available at \\href{https://github.com/YiboWANG214/ProjectTest}{ProjectTest}.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers","url":"https://arxiv.org/abs/2502.06527","date":1740114000,"author":"","guid":7667,"unread":true,"content":"<article>arXiv:2502.06527v2 Announce Type: replace \nAbstract: Customized generation has achieved significant progress in image synthesis, yet personalized video generation remains challenging due to temporal inconsistencies and quality degradation. In this paper, we introduce CustomVideoX, an innovative framework leveraging the video diffusion transformer for personalized video generation from a reference image. CustomVideoX capitalizes on pre-trained video networks by exclusively training the LoRA parameters to extract reference features, ensuring both efficiency and adaptability. To facilitate seamless interaction between the reference image and video content, we propose 3D Reference Attention, which enables direct and simultaneous engagement of reference image features with all video frames across spatial and temporal dimensions. To mitigate the excessive influence of reference image features and textual guidance on generated video content during inference, we implement the Time-Aware Reference Attention Bias (TAB) strategy, dynamically modulating reference bias over different time steps. Additionally, we introduce the Entity Region-Aware Enhancement (ERAE) module, aligning highly activated regions of key entity tokens with reference feature injection by adjusting attention bias. To thoroughly evaluate personalized video generation, we establish a new benchmark, VideoBench, comprising over 50 objects and 100 prompts for extensive assessment. Experimental results show that CustomVideoX significantly outperforms existing methods in terms of video consistency and quality.</article>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ID policy (with reassignment) is asymptotically optimal for heterogeneous weakly-coupled MDPs","url":"https://arxiv.org/abs/2502.06072","date":1740114000,"author":"","guid":7668,"unread":true,"content":"<article>arXiv:2502.06072v2 Announce Type: replace \nAbstract: Heterogeneity poses a fundamental challenge for many real-world large-scale decision-making problems but remains largely understudied. In this paper, we study the fully heterogeneous setting of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs). Each WCMDP consists of $N$ arms (or subproblems), which have distinct model parameters in the fully heterogeneous setting, leading to the curse of dimensionality when $N$ is large. We show that, under mild assumptions, a natural adaptation of the ID policy, although originally proposed for a homogeneous special case of WCMDPs, in fact achieves an $O(1/\\sqrt{N})$ optimality gap in long-run average reward per arm for fully heterogeneous WCMDPs as $N$ becomes large. This is the first asymptotic optimality result for fully heterogeneous average-reward WCMDPs. Our techniques highlight the construction of a novel projection-based Lyapunov function, which witnesses the convergence of rewards and costs to an optimal region in the presence of heterogeneity.</article>","contentLength":1101,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Speech to Speech Translation with Translatotron: A State of the Art Review","url":"https://arxiv.org/abs/2502.05980","date":1740114000,"author":"","guid":7669,"unread":true,"content":"<article>arXiv:2502.05980v2 Announce Type: replace \nAbstract: A cascade-based speech-to-speech translation has been considered a benchmark for a very long time, but it is plagued by many issues, like the time taken to translate a speech from one language to another and compound errors. These issues are because a cascade-based method uses a combination of methods such as speech recognition, speech-to-text translation, and finally, text-to-speech translation. Translatotron, a sequence-to-sequence direct speech-to-speech translation model was designed by Google to address the issues of compound errors associated with cascade model. Today there are 3 versions of the Translatotron model: Translatotron 1, Translatotron 2, and Translatotron3. The first version was designed as a proof of concept to show that a direct speech-to-speech translation was possible, it was found to be less effective than the cascade model but was producing promising results. Translatotron2 was an improved version of Translatotron 1 with results similar to the cascade model. Translatotron 3 the latest version of the model is better than the cascade model at some points. In this paper, a complete review of speech-to-speech translation will be presented, with a particular focus on all the versions of Translatotron models. We will also show that Translatotron is the best model to bridge the language gap between African Languages and other well-formalized languages.</article>","contentLength":1444,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Social inequality and cultural factors impact the awareness and reaction during the cryptic transmission period of pandemic","url":"https://arxiv.org/abs/2502.05622","date":1740114000,"author":"","guid":7670,"unread":true,"content":"<article>arXiv:2502.05622v2 Announce Type: replace \nAbstract: The World Health Organization (WHO) declared the COVID-19 outbreak a Public Health Emergency of International Concern (PHEIC) on January 31, 2020. However, rumors of a \"mysterious virus\" had already been circulating in China in December 2019, possibly preceding the first confirmed COVID-19 case. Understanding how awareness about an emerging pandemic spreads through society is vital not only for enhancing disease surveillance, but also for mitigating demand shocks and social inequities, such as shortages of personal protective equipment (PPE) and essential supplies. Here we leverage a massive e-commerce dataset comprising 150 billion online queries and purchase records from 94 million people to detect the traces of early awareness and public response during the cryptic transmission period of COVID-19. Our analysis focuses on identifying information gaps across different demographic cohorts, revealing significant social inequities and the role of cultural factors in shaping awareness diffusion and response behaviors. By modeling awareness diffusion in heterogeneous social networks and analyzing online shopping behavior, we uncover the evolving characteristics of vulnerable populations. Our findings expand the theoretical understanding of awareness spread and social inequality in the early stages of a pandemic, highlighting the critical importance of e-commerce data and social network data in effectively and timely addressing future pandemic challenges. We also provide actionable recommendations to better manage and mitigate dynamic social inequalities in public health crises.</article>","contentLength":1653,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improved high-index saddle dynamics for finding saddle points and solution landscape","url":"https://arxiv.org/abs/2502.03694","date":1740114000,"author":"","guid":7671,"unread":true,"content":"<article>arXiv:2502.03694v2 Announce Type: replace \nAbstract: We present an improved high-index saddle dynamics (iHiSD) for finding saddle points and constructing solution landscapes, which is a crossover dynamics from gradient flow to traditional HiSD such that the Morse theory for gradient flow could be involved. We propose analysis for the reflection manifold in iHiSD, and then prove its stable and nonlocal convergence from outside of the region of attraction to the saddle point, which resolves the dependence of the convergence of HiSD on the initial value. We then present and analyze a discretized iHiSD that inherits these convergence properties. Furthermore, based on the Morse theory, we prove that any two saddle points could be connected by a sequence of trajectories of iHiSD. Theoretically, this implies that a solution landscape with a finite number of stationary points could be completely constructed by means of iHiSD, which partly answers the completeness issue of the solution landscape for the first time and indicates the necessity of integrating the gradient flow in HiSD. Different methods are compared by numerical experiments to substantiate the effectiveness of the iHiSD method.</article>","contentLength":1201,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation","url":"https://arxiv.org/abs/2502.02464","date":1740114000,"author":"","guid":7672,"unread":true,"content":"<article>arXiv:2502.02464v3 Announce Type: replace \nAbstract: Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical components of modern applications in information retrieval, question answering, or knowledge-based text generation. However, existing solutions are often fragmented, lacking a unified framework that easily integrates these essential processes. The absence of a standardized implementation, coupled with the complexity of retrieval and re-ranking workflows, makes it challenging for researchers to compare and evaluate different approaches in a consistent environment. While existing toolkits such as Rerankers and RankLLM provide general-purpose reranking pipelines, they often lack the flexibility required for fine-grained experimentation and benchmarking. In response to these challenges, we introduce Rankify, a powerful and modular open-source toolkit designed to unify retrieval, re-ranking, and RAG within a cohesive framework. Rankify supports a wide range of retrieval techniques, including dense and sparse retrievers, while incorporating state-of-the-art re-ranking models to enhance retrieval quality. Additionally, Rankify includes a collection of pre-retrieved datasets to facilitate benchmarking, available at Huggingface (https://huggingface.co/datasets/abdoelsayed/reranking-datasets-light). To encourage adoption and ease of integration, we provide comprehensive documentation (http://rankify.readthedocs.io/), an open-source implementation on GitHub (https://github.com/DataScienceUIBK/rankify), and a PyPI package for easy installation (https://pypi.org/project/rankify/). As a unified and lightweight framework, Rankify allows researchers and practitioners to advance retrieval and re-ranking methodologies while ensuring consistency, scalability, and ease of use.</article>","contentLength":1815,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MAGNNET: Multi-Agent Graph Neural Network-based Efficient Task Allocation for Autonomous Vehicles with Deep Reinforcement Learning","url":"https://arxiv.org/abs/2502.02311","date":1740114000,"author":"","guid":7673,"unread":true,"content":"<article>arXiv:2502.02311v2 Announce Type: replace \nAbstract: This paper addresses the challenge of decentralized task allocation within heterogeneous multi-agent systems operating under communication constraints. We introduce a novel framework that integrates graph neural networks (GNNs) with a centralized training and decentralized execution (CTDE) paradigm, further enhanced by a tailored Proximal Policy Optimization (PPO) algorithm for multi-agent deep reinforcement learning (MARL). Our approach enables unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) to dynamically allocate tasks efficiently without necessitating central coordination in a 3D grid environment. The framework minimizes total travel time while simultaneously avoiding conflicts in task assignments. For the cost calculation and routing, we employ reservation-based A* and R* path planners. Experimental results revealed that our method achieves a high 92.5% conflict-free success rate, with only a 7.49% performance gap compared to the centralized Hungarian method, while outperforming the heuristic decentralized baseline based on greedy approach. Additionally, the framework exhibits scalability with up to 20 agents with allocation processing of 2.8 s and robustness in responding to dynamically generated tasks, underscoring its potential for real-world applications in complex multi-agent scenarios.</article>","contentLength":1390,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reasoning Bias of Next Token Prediction Training","url":"https://arxiv.org/abs/2502.02007","date":1740114000,"author":"","guid":7674,"unread":true,"content":"<article>arXiv:2502.02007v2 Announce Type: replace \nAbstract: Since the inception of Large Language Models (LLMs), the quest to efficiently train them for superior reasoning capabilities has been a pivotal challenge. The dominant training paradigm for LLMs is based on next token prediction (NTP). Alternative methodologies, called Critical Token Prediction (CTP), focused exclusively on specific critical tokens (such as the answer in Q\\&amp;A dataset), aiming to reduce the overfitting of extraneous information and noise. Contrary to initial assumptions, our research reveals that despite NTP's exposure to noise during training, it surpasses CTP in reasoning ability. We attribute this counterintuitive outcome to the regularizing influence of noise on the training dynamics. Our empirical analysis shows that NTP-trained models exhibit enhanced generalization and robustness across various benchmark reasoning datasets, demonstrating greater resilience to perturbations and achieving flatter loss minima. These findings illuminate that NTP is instrumental in fostering reasoning abilities during pretraining, whereas CTP is more effective for finetuning, thereby enriching our comprehension of optimal training strategies in LLM development.</article>","contentLength":1233,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification","url":"https://arxiv.org/abs/2502.01839","date":1740114000,"author":"","guid":7675,"unread":true,"content":"<article>arXiv:2502.01839v2 Announce Type: replace \nAbstract: Sampling-based search, a simple paradigm for utilizing test-time compute, involves generating multiple candidate responses and selecting the best one -- typically by having models self-verify each response for correctness. In this paper, we study the scaling trends governing sampling-based search. Among our findings is that simply scaling up a minimalist implementation of sampling-based search, using only random sampling and direct self-verification, provides a practical inference method that, for example, elevates the reasoning capabilities of Gemini v1.5 Pro above that of o1-Preview on popular benchmarks. We partially attribute the scalability of sampling-based search to a phenomenon of implicit scaling, where sampling a larger pool of responses in turn improves self-verification accuracy. We further identify two useful principles for improving self-verification capabilities with test-time compute: (1) comparing across responses provides helpful signals about the locations of errors and hallucinations, and (2) different model output styles are useful for different contexts -- chains of thought are useful for reasoning but harder to verify. We also find that, though accurate verification can be elicited, frontier models demonstrate remarkably weak out-of-box verification capabilities and introduce a benchmark to measure progress on these deficiencies.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evolving Symbolic 3D Visual Grounder with Weakly Supervised Reflection","url":"https://arxiv.org/abs/2502.01401","date":1740114000,"author":"","guid":7676,"unread":true,"content":"<article>arXiv:2502.01401v3 Announce Type: replace \nAbstract: 3D visual grounding (3DVG) is challenging because of the requirement of understanding on visual information, language and spatial relationships. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high cost of 3D vision-language datasets. On the other hand, LLM/VLM based agents are proposed for 3DVG, eliminating the need for training data. However, these methods incur prohibitive time and token costs during inference. To address the challenges, we introduce a novel training-free symbolic framework for 3D visual grounding, namely Evolvable Symbolic Visual Grounder, that offers significantly reduced inference costs compared to previous agent-based methods while maintaining comparable performance. EaSe uses LLM generated codes to compute on spatial relationships. EaSe also implements an automatic pipeline to evaluate and optimize the quality of these codes and integrate VLMs to assist in the grounding process. Experimental results demonstrate that EaSe achieves 52.9% accuracy on Nr3D dataset and 49.2% Acc@0.25 on ScanRefer, which is top-tier among training-free methods. Moreover, it substantially reduces the inference time and cost, offering a balanced trade-off between performance and efficiency. Codes are available at https://github.com/OpenRobotLab/EaSe.</article>","contentLength":1380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TeLL-Drive: Enhancing Autonomous Driving with Teacher LLM-Guided Deep Reinforcement Learning","url":"https://arxiv.org/abs/2502.01387","date":1740114000,"author":"","guid":7677,"unread":true,"content":"<article>arXiv:2502.01387v3 Announce Type: replace \nAbstract: Although Deep Reinforcement Learning (DRL) and Large Language Models (LLMs) each show promise in addressing decision-making challenges in autonomous driving, DRL often suffers from high sample complexity, while LLMs have difficulty ensuring real-time decision making. To address these limitations, we propose TeLL-Drive, a hybrid framework that integrates a Teacher LLM to guide an attention-based Student DRL policy. By incorporating risk metrics, historical scenario retrieval, and domain heuristics into context-rich prompts, the LLM produces high-level driving strategies through chain-of-thought reasoning. A self-attention mechanism then fuses these strategies with the DRL agent's exploration, accelerating policy convergence and boosting robustness across diverse driving conditions. The experimental results, evaluated across multiple traffic scenarios, show that TeLL-Drive outperforms existing baseline methods, including other LLM-based approaches, in terms of success rates, average returns, and real-time feasibility. Ablation studies underscore the importance of each model component, especially the synergy between the attention mechanism and LLM-driven guidance. Finally, we build a virtual-real fusion experimental platform to verify the real-time performance, robustness, and reliability of the algorithm running on real vehicles through vehicle-in-loop experiments.</article>","contentLength":1438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Metric Distortion of Small-group Deliberation","url":"https://arxiv.org/abs/2502.01380","date":1740114000,"author":"","guid":7678,"unread":true,"content":"<article>arXiv:2502.01380v2 Announce Type: replace \nAbstract: We consider models for social choice where voters rank a set of choices (or alternatives) by deliberating in small groups of size at most $k$, and these outcomes are aggregated by a social choice rule to find the winning alternative. We ground these models in the metric distortion framework, where the voters and alternatives are embedded in a latent metric space, with closer alternative being more desirable for a voter. We posit that the outcome of a small-group interaction optimally uses the voters' collective knowledge of the metric, either deterministically or probabilistically.\n  We characterize the distortion of our deliberation models for small $k$, showing that groups of size $k=3$ suffice to drive the distortion bound below the deterministic metric distortion lower bound of $3$, and groups of size $4$ suffice to break the randomized lower bound of $2.11$. We also show nearly tight asymptotic distortion bounds in the group size, showing that for any constant $\\epsilon &gt; 0$, achieving a distortion of $1+\\epsilon$ needs group size that only depends on $1/\\epsilon$, and not the number of alternatives. We obtain these results via formulating a basic optimization problem in small deviations of the sum of $i.i.d.$ random variables, which we solve to global optimality via non-convex optimization. The resulting bounds may be of independent interest in probability theory.</article>","contentLength":1445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SimPER: A Minimalist Approach to Preference Alignment without Hyperparameters","url":"https://arxiv.org/abs/2502.00883","date":1740114000,"author":"","guid":7679,"unread":true,"content":"<article>arXiv:2502.00883v4 Announce Type: replace \nAbstract: Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. In this paper, we propose a simple yet effective hyperparameter-free preference optimization algorithm for alignment. We observe that promising performance can be achieved simply by optimizing inverse perplexity, which is calculated as the inverse of the exponentiated average log-likelihood of the chosen and rejected responses in the preference dataset. The resulting simple learning objective, SimPER, is easy to implement and eliminates the need for expensive hyperparameter tuning and a reference model, making it both computationally and memory efficient. Extensive experiments on widely used real-world benchmarks, including MT-Bench, AlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base models, demonstrate that SimPER consistently and significantly outperforms existing approaches-even without any hyperparameters or a reference model . For example, despite its simplicity, SimPER outperforms state-of-the-art methods by up to 5.7 points on AlpacaEval 2 and achieves the highest average ranking across 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is publicly available at: https://github.com/tengxiao1/SimPER.</article>","contentLength":1478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models","url":"https://arxiv.org/abs/2501.19392","date":1740114000,"author":"","guid":7680,"unread":true,"content":"<article>arXiv:2501.19392v3 Announce Type: replace \nAbstract: Efficient real-world deployments of large language models (LLMs) rely on Key-Value (KV) caching for processing and generating long outputs, reducing the need for repetitive computation. For large contexts, Key-Value caches can take up tens of gigabytes of device memory, as they store vector representations for each token and layer. Recent work has shown that the cached vectors can be compressed through quantization, pruning or merging, but these techniques often compromise quality towards higher compression rates. In this work, we aim to improve Key &amp; Value compression by exploiting two observations: 1) the inherent dependencies between keys and values across different layers, and 2) high-compression mechanisms for internal network states. We propose AQUA-KV, an adaptive quantization for Key-Value caches that relies on compact adapters to exploit existing dependencies between Keys and Values, and aims to \"optimally\" compress the information that cannot be predicted. AQUA-KV significantly improves compression rates, while maintaining high accuracy on state-of-the-art LLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5 bits per value with under $1\\%$ relative error in perplexity and LongBench scores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a single GPU within 1-6 hours, even for 70B models.</article>","contentLength":1415,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TabFSBench: Tabular Benchmark for Feature Shifts in Open Environment","url":"https://arxiv.org/abs/2501.18935","date":1740114000,"author":"","guid":7681,"unread":true,"content":"<article>arXiv:2501.18935v2 Announce Type: replace \nAbstract: Tabular data is widely utilized in various machine learning tasks. Current tabular learning research predominantly focuses on closed environments, while in real-world applications, open environments are often encountered, where distribution and feature shifts occur, leading to significant degradation in model performance. Previous research has primarily concentrated on mitigating distribution shifts, whereas feature shifts, a distinctive and unexplored challenge of tabular data, have garnered limited attention. To this end, this paper conducts the first comprehensive study on feature shifts in tabular data and introduces the first tabular feature-shift benchmark (TabFSBench). TabFSBench evaluates impacts of four distinct feature-shift scenarios on four tabular model categories across various datasets and assesses the performance of large language models (LLMs) and tabular LLMs in the tabular benchmark for the first time. Our study demonstrates three main observations: (1) most tabular models have the limited applicability in feature-shift scenarios; (2) the shifted feature set importance has a linear relationship with model performance degradation; (3) model performance in closed environments correlates with feature-shift performance. Future research direction is also explored for each observation. TabFSBench is released for public access by using a few lines of Python codes at https://github.com/LAMDASZ-ML/TabFSBench.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding","url":"https://arxiv.org/abs/2501.18362","date":1740114000,"author":"","guid":7682,"unread":true,"content":"<article>arXiv:2501.18362v2 Announce Type: replace \nAbstract: We introduce MedXpertQA, a highly challenging and comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning. MedXpertQA includes 4,460 questions spanning 17 specialties and 11 body systems. It includes two subsets, Text for text evaluation and MM for multimodal evaluation. Notably, MM introduces expert-level exam questions with diverse images and rich clinical information, including patient records and examination results, setting it apart from traditional medical multimodal benchmarks with simple QA pairs generated from image captions. MedXpertQA applies rigorous filtering and augmentation to address the insufficient difficulty of existing benchmarks like MedQA, and incorporates specialty board questions to improve clinical relevance and comprehensiveness. We perform data synthesis to mitigate data leakage risk and conduct multiple rounds of expert reviews to ensure accuracy and reliability. We evaluate 16 leading models on MedXpertQA. Moreover, medicine is deeply connected to real-world decision-making, providing a rich and representative setting for assessing reasoning abilities beyond mathematics and code. To this end, we develop a reasoning-oriented subset to facilitate the assessment of o1-like models.</article>","contentLength":1312,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and Causal Policy Optimization","url":"https://arxiv.org/abs/2501.17711","date":1740114000,"author":"","guid":7683,"unread":true,"content":"<article>arXiv:2501.17711v2 Announce Type: replace \nAbstract: This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic medal distributions by integrating the spatio-temporal relationships among countries and the long-term dependencies of national performance. The Spatial-Temporal Graph Convolution Network (STGCN) captures geographic and interactive factors-such as coaching exchange and socio-economic links-while the Long Short-Term Memory (LSTM) module models historical trends in medal counts, economic data, and demographics. To address zero-inflated outputs (i.e., the disparity between countries that consistently yield wins and those never having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is incorporated to separate random zeros from structural zeros, providing a clearer view of potential breakthrough performances. Validation includes historical backtracking, policy shock simulations, and causal inference checks, confirming the robustness of the proposed method. Results shed light on the influence of coaching mobility, event specialization, and strategic investment on medal forecasts, offering a data-driven foundation for optimizing sports policies and resource allocation in diverse Olympic contexts.</article>","contentLength":1248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amplifier: Bringing Attention to Neglected Low-Energy Components in Time Series Forecasting","url":"https://arxiv.org/abs/2501.17216","date":1740114000,"author":"","guid":7684,"unread":true,"content":"<article>arXiv:2501.17216v2 Announce Type: replace \nAbstract: We propose an energy amplification technique to address the issue that existing models easily overlook low-energy components in time series forecasting. This technique comprises an energy amplification block and an energy restoration block. The energy amplification block enhances the energy of low-energy components to improve the model's learning efficiency for these components, while the energy restoration block returns the energy to its original level. Moreover, considering that the energy-amplified data typically displays two distinct energy peaks in the frequency spectrum, we integrate the energy amplification technique with a seasonal-trend forecaster to model the temporal relationships of these two peaks independently, serving as the backbone for our proposed model, Amplifier. Additionally, we propose a semi-channel interaction temporal relationship enhancement block for Amplifier, which enhances the model's ability to capture temporal relationships from the perspective of the commonality and specificity of each channel in the data. Extensive experiments on eight time series forecasting benchmarks consistently demonstrate our model's superiority in both effectiveness and efficiency compared to state-of-the-art methods.</article>","contentLength":1297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harnessing Diverse Perspectives: A Multi-Agent Framework for Enhanced Error Detection in Knowledge Graphs","url":"https://arxiv.org/abs/2501.15791","date":1740114000,"author":"","guid":7685,"unread":true,"content":"<article>arXiv:2501.15791v2 Announce Type: replace \nAbstract: Knowledge graphs are widely used in industrial applications, making error detection crucial for ensuring the reliability of downstream applications. Existing error detection methods often fail to effectively utilize fine-grained subgraph information and rely solely on fixed graph structures, while also lacking transparency in their decision-making processes, which results in suboptimal detection performance. In this paper, we propose a novel Multi-Agent framework for Knowledge Graph Error Detection (MAKGED) that utilizes multiple large language models (LLMs) in a collaborative setting. By concatenating fine-grained, bidirectional subgraph embeddings with LLM-based query embeddings during training, our framework integrates these representations to produce four specialized agents. These agents utilize subgraph information from different dimensions to engage in multi-round discussions, thereby improving error detection accuracy and ensuring a transparent decision-making process. Extensive experiments on FB15K and WN18RR demonstrate that MAKGED outperforms state-of-the-art methods, enhancing the accuracy and robustness of KG evaluation. For specific industrial scenarios, our framework can facilitate the training of specialized agents using domain-specific knowledge graphs for error detection, which highlights the potential industrial application value of our framework. Our code and datasets are available at https://github.com/kse-ElEvEn/MAKGED.</article>","contentLength":1517,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Surface Vision Mamba: Leveraging Bidirectional State Space Model for Efficient Spherical Manifold Representation","url":"https://arxiv.org/abs/2501.14679","date":1740114000,"author":"","guid":7686,"unread":true,"content":"<article>arXiv:2501.14679v5 Announce Type: replace \nAbstract: Attention-based methods have demonstrated exceptional performance in modelling long-range dependencies on spherical cortical surfaces, surpassing traditional Geometric Deep Learning (GDL) models. However, their extensive inference time and high memory demands pose challenges for application to large datasets with limited computing resources. Inspired by the state space model in computer vision, we introduce the attention-free Vision Mamba (Vim) to spherical surfaces, presenting a domain-agnostic architecture for analyzing data on spherical manifolds. Our method achieves surface patching by representing spherical data as a sequence of triangular patches derived from a subdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on multiple neurodevelopmental phenotype regression tasks using cortical surface metrics from neonatal brains. Experimental results demonstrate that SiM outperforms both attention- and GDL-based methods, delivering 4.8 times faster inference and achieving 91.7% lower memory consumption compared to the Surface Vision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity analysis further underscores the potential of SiM to identify subtle cognitive developmental patterns. The code is available at https://github.com/Rongzhao-He/surface-vision-mamba.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QuanTaxo: A Quantum Approach to Self-Supervised Taxonomy Expansion","url":"https://arxiv.org/abs/2501.14011","date":1740114000,"author":"","guid":7687,"unread":true,"content":"<article>arXiv:2501.14011v2 Announce Type: replace \nAbstract: A taxonomy is a hierarchical graph containing knowledge to provide valuable insights for various web applications. Online retail organizations like Microsoft and Amazon utilize taxonomies to improve product recommendations and optimize advertisement by enhancing query interpretation. However, the manual construction of taxonomies requires significant human effort. As web content continues to expand at an unprecedented pace, existing taxonomies risk becoming outdated, struggling to incorporate new and emerging information effectively. As a consequence, there is a growing need for dynamic taxonomy expansion to keep them relevant and up-to-date. Existing taxonomy expansion methods often rely on classical word embeddings to represent entities. However, these embeddings fall short in capturing hierarchical polysemy, where an entity's meaning can vary based on its position in the hierarchy and its surrounding context. To address this challenge, we introduce QuanTaxo, an innovative quantum-inspired framework for taxonomy expansion. QuanTaxo encodes entity representations in quantum space, effectively modeling hierarchical polysemy by leveraging the principles of Hilbert space to capture interference effects between entities, yielding richer and more nuanced representations. Comprehensive experiments on four real-world benchmark datasets show that QuanTaxo significantly outperforms classical embedding models, achieving substantial improvements of 18.45% in accuracy, 20.5% in Mean Reciprocal Rank, and 17.87% in Wu &amp; Palmer metrics across eight classical embedding-based baselines. We further highlight the superiority of QuanTaxo through extensive ablation and case studies.</article>","contentLength":1744,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Certified Robustness Under Bounded Levenshtein Distance","url":"https://arxiv.org/abs/2501.13676","date":1740114000,"author":"","guid":7688,"unread":true,"content":"<article>arXiv:2501.13676v2 Announce Type: replace \nAbstract: Text classifiers suffer from small perturbations, that if chosen adversarially, can dramatically change the output of the model. Verification methods can provide robustness certificates against such adversarial perturbations, by computing a sound lower bound on the robust accuracy. Nevertheless, existing verification methods incur in prohibitive costs and cannot practically handle Levenshtein distance constraints. We propose the first method for computing the Lipschitz constant of convolutional classifiers with respect to the Levenshtein distance. We use these Lipschitz constant estimates for training 1-Lipschitz classifiers. This enables computing the certified radius of a classifier in a single forward pass. Our method, LipsLev, is able to obtain $38.80$% and $13.93$% verified accuracy at distance $1$ and $2$ respectively in the AG-News dataset, while being $4$ orders of magnitude faster than existing approaches. We believe our work can open the door to more efficient verification in the text domain.</article>","contentLength":1070,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks","url":"https://arxiv.org/abs/2501.13456","date":1740114000,"author":"","guid":7689,"unread":true,"content":"<article>arXiv:2501.13456v2 Announce Type: replace \nAbstract: Graph neural networks (GNNs) with attention mechanisms, often referred to as attentive GNNs, have emerged as a prominent paradigm in advanced GNN models in recent years. However, our understanding of the critical process of scoring neighbor nodes remains limited, leading to the underperformance of many existing attentive GNNs. In this paper, we unify the scoring functions of current attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which integrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring process. KAA enhances the performance of scoring functions across the board and can be applied to nearly all existing attentive GNNs. To compare the expressive power of KAA with other scoring functions, we introduce Maximum Ranking Distance (MRD) to quantitatively estimate their upper bounds in ranking errors for node importance. Our analysis reveals that, under limited parameters and constraints on width and depth, both linear transformation-based and MLP-based scoring functions exhibit finite expressive power. In contrast, our proposed KAA, even with a single-layer KAN parameterized by zero-order B-spline functions, demonstrates nearly infinite expressive power. Extensive experiments on both node-level and graph-level tasks using various backbone models show that KAA-enhanced scoring functions consistently outperform their original counterparts, achieving performance improvements of over 20% in some cases.</article>","contentLength":1505,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation","url":"https://arxiv.org/abs/2501.12612","date":1740114000,"author":"","guid":7690,"unread":true,"content":"<article>arXiv:2501.12612v2 Announce Type: replace \nAbstract: Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 12 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing. Data and evaluator are released under https://github.com/adwardlee/t2i_safety.</article>","contentLength":1465,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On de Bruijn Array Codes Part II: Linear Codes","url":"https://arxiv.org/abs/2501.12124","date":1740114000,"author":"","guid":7691,"unread":true,"content":"<article>arXiv:2501.12124v2 Announce Type: replace \nAbstract: An M-sequence generated by a primitive polynomial has many interesting and desirable properties. A pseudo-random array is the two-dimensional generalization of an M-sequence. Similarly to primitive polynomials, there are irreducible and reducible polynomials whose all nonzero sequences have the same length. In this paper, a two-dimensional generalization for such sequences is given. This generalization is for a pseudo-random array code which is a set of $r_1 \\times r_2$ arrays in which each $n_1 \\times n_2$ nonzero matrix is contained exactly once as a window in one of the arrays. Moreover, these arrays have the shift-and-add property, i.e., the bitwise addition of two arrays (or a nontrivial shift of such arrays) is another array (or a shift of another array) from the code. All the known arrays can be formed by folding sequences generated from an irreducible polynomial or a reducible polynomial whose factors have the same degree and the same exponent. Two proof techniques are used to prove the parameters of the constructed arrays. The first one is based on another method for constructing some of these arrays. The second one is a generalization of a known proof technique. This generalization enables to present pseudo-random arrays with parameters not known before and also a variety of pseudo-random array codes which cannot be generated by the first method. The two techniques also suggest two different hierarchies between pseudo-random array codes. Finally, a method to verify whether a folding of sequences, generated by these polynomials, yields a pseudo-random array or a pseudo-random array code, will be presented.</article>","contentLength":1695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-crossing $H$-graphs: a generalization of proper interval graphs admitting FPT algorithms","url":"https://arxiv.org/abs/2501.11192","date":1740114000,"author":"","guid":7692,"unread":true,"content":"<article>arXiv:2501.11192v2 Announce Type: replace \nAbstract: We prove new parameterized complexity results for the FO Model Checking problem and in particular for Independent Set, for two recently introduced subclasses of $H$-graphs, namely proper $H$-graphs and non-crossing $H$-graphs. It is known that proper $H$-graphs, and thus $H$-graphs, may have unbounded twin-width. However, we prove that for every connected multigraph $H$ with no self-loops, non-crossing $H$-graphs have bounded proper mixed-thinness, and thus bounded twin-width. Consequently, we can apply a well-known result of Bonnet, Kim, Thomass\\'e, and Watrigant (2021) to find that the FO Model Checking problem is in $\\mathsf{FPT}$ for non-crossing $H$-graphs when parameterized by $\\Vert H \\Vert+\\ell$, where $\\Vert H \\Vert$ is the size of $H$ and $\\ell$ is the size of a formula. In particular, this implies that Independent Set is in $\\mathsf{FPT}$ on non-crossing $H$-graphs when parameterized by $\\Vert H \\Vert+k$, where $k$ is the solution size. In contrast, Independent Set for general $H$-graphs is $\\mathsf{W[1]}$-hard when parameterized by $\\Vert H \\Vert +k$. We strengthen the latter result by proving thatIndependent Set is $\\mathsf{W[1]}$-hard even on proper $H$-graphs when parameterized by $\\Vert H \\Vert+k$. In this way, we solve, subject to $\\mathsf{W[1]}\\neq \\mathsf{FPT}$, an open problem of Chaplick (2023), who asked whether there exist problems that can be solved faster for non-crossing $H$-graphs than for proper $H$-graphs.</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking","url":"https://arxiv.org/abs/2501.09751","date":1740114000,"author":"","guid":7693,"unread":true,"content":"<article>arXiv:2501.09751v2 Announce Type: replace \nAbstract: Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.</article>","contentLength":1132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are DeepSeek R1 And Other Reasoning Models More Faithful?","url":"https://arxiv.org/abs/2501.08156","date":1740114000,"author":"","guid":7694,"unread":true,"content":"<article>arXiv:2501.08156v4 Announce Type: replace \nAbstract: Language models trained to solve reasoning tasks via reinforcement learning have achieved striking results. We refer to these models as reasoning models. A key question emerges: Are the Chains of Thought (CoTs) of reasoning models more faithful than traditional models? To investigate this, we evaluate three reasoning models (based on Qwen-2.5, Gemini-2, and DeepSeek-V3-Base) on an existing test of faithful CoT. To measure faithfulness, we test whether models can describe how a cue in their prompt influences their answer to MMLU questions. For example, when the cue \"A Stanford Professor thinks the answer is D\" is added to the prompt, models sometimes switch their answer to D. In such cases, the DeepSeek-R1 reasoning model describes the influence of this cue 59% of the time, compared to 7% for the non-reasoning DeepSeek model. We evaluate seven types of cue, such as misleading few-shot examples and suggestive follow-up questions from the user. Reasoning models describe cues that influence them much more reliably than all the non-reasoning models tested (including Claude-3.5-Sonnet and GPT-4). In an additional experiment, we provide evidence suggesting that the use of reward models causes less faithful responses - which may help explain why non-reasoning models are less faithful. Our study has two main limitations. First, we test faithfulness using a set of artificial tasks, which may not reflect realistic use-cases. Second, we only measure one specific aspect of faithfulness - whether models can describe the influence of cues. Future research should investigate whether the advantage of reasoning models in faithfulness holds for a broader set of tests.</article>","contentLength":1730,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging Smart Meter Gaps: A Benchmark of Statistical, Machine Learning and Time Series Foundation Models for Data Imputation","url":"https://arxiv.org/abs/2501.07276","date":1740114000,"author":"","guid":7695,"unread":true,"content":"<article>arXiv:2501.07276v2 Announce Type: replace \nAbstract: The integrity of time series data in smart grids is often compromised by missing values due to sensor failures, transmission errors, or disruptions. Gaps in smart meter data can bias consumption analyses and hinder reliable predictions, causing technical and economic inefficiencies. As smart meter data grows in volume and complexity, conventional techniques struggle with its nonlinear and nonstationary patterns. In this context, Generative Artificial Intelligence offers promising solutions that may outperform traditional statistical methods. In this paper, we evaluate two general-purpose Large Language Models and five Time Series Foundation Models for smart meter data imputation, comparing them with conventional Machine Learning and statistical models. We introduce artificial gaps (30 minutes to one day) into an anonymized public dataset to test inference capabilities. Results show that Time Series Foundation Models, with their contextual understanding and pattern recognition, could significantly enhance imputation accuracy in certain cases. However, the trade-off between computational cost and performance gains remains a critical consideration.</article>","contentLength":1216,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution","url":"https://arxiv.org/abs/2501.05040","date":1740114000,"author":"","guid":7696,"unread":true,"content":"<article>arXiv:2501.05040v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving state-of-the-art performance among open-source models with scores of 24.7% and 32.8%, respectively. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.</article>","contentLength":1796,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cyber-physical Defense for Heterogeneous Multi-agent Systems Against Exponentially Unbounded Attacks on Signed Digraphs","url":"https://arxiv.org/abs/2501.00990","date":1740114000,"author":"","guid":7697,"unread":true,"content":"<article>arXiv:2501.00990v2 Announce Type: replace \nAbstract: Cyber-physical systems (CPSs) are subjected to attacks on both cyber and physical spaces. In reality, the attackers could launch exponentially unbounded false data injection (EU-FDI) attacks, which are more destructive and could lead to the system's collapse or instability. Existing literature generally addresses bounded attack signals and/or bounded-first-order-derivative attack signals, which exposes the CPSs to significant threats. In contrast, this paper proposes a fully-distributed attack-resilient bi-layer defense framework to address the bipartite output containment problem for heterogeneous multi-agent systems on signed digraphs, in the presence of EU-FDI attacks on both cyber-physical layer (CPL) and observer layer (OL). First, we design attack-resilient dynamic compensators that utilize data communicated on the OL to estimate the convex combinations of the states and negative states of the leaders. The attack-resilient compensators address the EU-FDI attacks on the OL and guarantee the uniformly ultimately bounded (UUB) estimation of the leaders' states. Then, by using the compensators' states, fully-distributed attack-resilient controllers are designed on the CPL to further address the EU-FDI attacks on the actuators. Rigorous mathematical proof based on Lyapunov stability analysis is provided, establishing the theoretical soundness of the proposed bi-layer resilient defense framework, by preserving the UUB consensus and stability against EU-FDI attacks on both CPL and OL. Finally, a comparative case study for heterogeneous multi-agent systems validate the enhanced resilience of the proposed defense strategies.</article>","contentLength":1702,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Next-Gen Computing Systems with Compute Express Link: a Comprehensive Survey","url":"https://arxiv.org/abs/2412.20249","date":1740114000,"author":"","guid":7698,"unread":true,"content":"<article>arXiv:2412.20249v2 Announce Type: replace \nAbstract: Interconnection is crucial for computing systems. However, the current interconnection performance between processors and devices, such as memory devices and accelerators, significantly lags behind their computing performance, severely limiting the overall performance. To address this challenge, Intel proposes Compute Express Link (CXL), an open industry-standard interconnection. With memory semantics, CXL offers low-latency, scalable, and coherent interconnection between processors and devices. This paper introduces recent advances in CXL-based computing systems from single-machine to distributed. In single-machine systems, we classify existing research into two categories: Memory Expansion and Unified Memory. Memory Expansion focus on processors and memory, aims to address memory wall challenge. Unified memory focus on processors and accelerators, aims to enhance collaboration in heterogeneous computing systems. In distributed systems, we present how to build efficient disaggregation systems based on CXL infrastructure, enabling resource pooling and sharing. Finally, we discuss the future research and envision memory-centric computing with CXL.</article>","contentLength":1217,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Play Against Unknown Opponents","url":"https://arxiv.org/abs/2412.18297","date":1740114000,"author":"","guid":7699,"unread":true,"content":"<article>arXiv:2412.18297v2 Announce Type: replace \nAbstract: We consider the problem of a learning agent who has to repeatedly play a general sum game against a strategic opponent who acts to maximize their own payoff by optimally responding against the learner's algorithm. The learning agent knows their own payoff function, but is uncertain about the payoff of their opponent (knowing only that it is drawn from some distribution $\\mathcal{D}$). What learning algorithm should the agent run in order to maximize their own total utility, either in expectation or in the worst-case over $\\mathcal{D}$?\n  When the learning algorithm is constrained to be a no-regret algorithm, we demonstrate how to efficiently construct an optimal learning algorithm (asymptotically achieving the optimal utility) in polynomial time for both the in-expectation and worst-case problems, independent of any other assumptions. When the learning algorithm is not constrained to no-regret, we show how to construct an $\\varepsilon$-optimal learning algorithm (obtaining average utility within $\\varepsilon$ of the optimal utility) for both the in-expectation and worst-case problems in time polynomial in the size of the input and $1/\\varepsilon$, when either the size of the game or the support of $\\mathcal{D}$ is constant. Finally, for the special case of the maximin objective, where the learner wishes to maximize their minimum payoff over all possible optimizer types, we construct a learner algorithm that runs in polynomial time in each step and guarantees convergence to the optimal learner payoff. All of these results make use of recently developed machinery that converts the analysis of learning algorithms to the study of the class of corresponding geometric objects known as menus.</article>","contentLength":1767,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Boolean Functions with Minimal Spectral Sensitivity","url":"https://arxiv.org/abs/2412.16088","date":1740114000,"author":"","guid":7700,"unread":true,"content":"<article>arXiv:2412.16088v2 Announce Type: replace \nAbstract: We show examples of total Boolean functions that depend on $n$ variables and have spectral sensitivity $\\Theta(\\sqrt{\\log n})$, which is asymptotically minimal. Our main new function combines the Hamming code with the Boolean address function and has $\\lambda(f) = \\sqrt{(1+o(1)) \\log_2 n}$, which is optimal even up to a constant factor. By combining this function with itself in a specific way, we also obtain a family of functions with $\\text{s}_0(f) = (c+o(1)) \\log_2 n$ and $\\text{s}_0(f) = (1-c+o(1)) \\log_2 n$ for any $c \\in [0,1]$. This is an optimal tradeoff for Boolean functions with low sensitivity, as the lower bound on sensitivity by Simon generalizes to \\[\\text{s}_0(f)+\\text{s}_1(f)\\geq\\log_2 n - \\log_2 \\log_2 n + 2.\\] As a corollary, this gives a new example of a function with minimal possible sensitivity (up to a constant factor), $\\text{s}(f) = (\\frac{1}{2}+o(1)) \\log_2 n$.</article>","contentLength":950,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disentangled Graph Autoencoder for Treatment Effect Estimation","url":"https://arxiv.org/abs/2412.14497","date":1740114000,"author":"","guid":7701,"unread":true,"content":"<article>arXiv:2412.14497v2 Announce Type: replace \nAbstract: Treatment effect estimation from observational data has attracted significant attention across various research fields. However, many widely used methods rely on the unconfoundedness assumption, which is often unrealistic due to the inability to observe all confounders, thereby overlooking the influence of latent confounders. To address this limitation, recent approaches have utilized auxiliary network information to infer latent confounders, relaxing this assumption. However, these methods often treat observed variables and networks as proxies only for latent confounders, which can result in inaccuracies when certain variables influence treatment without affecting outcomes, or vice versa. This conflation of distinct latent factors undermines the precision of treatment effect estimation. To overcome this challenge, we propose a novel disentangled variational graph autoencoder for treatment effect estimation on networked observational data. Our graph encoder disentangles latent factors into instrumental, confounding, adjustment, and noisy factors, while enforcing factor independence using the Hilbert-Schmidt Independence Criterion. Extensive experiments on multiple networked datasets demonstrate that our method outperforms state-of-the-art approaches.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Cache Compression","url":"https://arxiv.org/abs/2412.12706","date":1740114000,"author":"","guid":7702,"unread":true,"content":"<article>arXiv:2412.12706v2 Announce Type: replace \nAbstract: As large language models (LLMs) process increasing context windows, the memory usage of KV cache has become a critical bottleneck during inference. The mainstream KV compression methods, including KV pruning and KV quantization, primarily focus on either token or precision dimension separately. However, these works leaving the trade-off between these two orthogonal dimensions largely under-explored. In this paper, we comprehensively investigate the token-precision trade-off in KV cache compression.Experiments demonstrate that storing more tokens in the KV cache with lower precision,a strategy we term quantized pruning, can significantly enhance the long-context performance of LLMs. In-depth analysis of the token-precision trade-off across key aspects demonstrates that, quantized pruning achieves substantial improvements in retrieval-related tasks and consistently performs well across varying input lengths. Furthermore, quantized pruning demonstrates notable stability and effectiveness across different KV pruning methods, quantization strategies, and model scales. These findings offer valuable insights into optimizing KV cache compression through balanced token-precision trade-off strategies. Our code is available at https://github.com/zhzihao/QPruningKV.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transferable and Forecastable User Targeting Foundation Model","url":"https://arxiv.org/abs/2412.12468","date":1740114000,"author":"","guid":7703,"unread":true,"content":"<article>arXiv:2412.12468v2 Announce Type: replace \nAbstract: User targeting, the process of selecting targeted users from a pool of candidates for non-expert marketers, has garnered substantial attention with the advancements in digital marketing. However, existing user targeting methods encounter two significant challenges: (i) Poor cross-domain and cross-scenario transferability and generalization, and (ii) Insufficient forecastability in real-world applications. These limitations hinder their applicability across diverse industrial scenarios. In this work, we propose FOUND, an industrial-grade, transferable, and forecastable user targeting foundation model. To enhance cross-domain transferability, our framework integrates heterogeneous multi-scenario user data, aligning them with one-sentence targeting demand inputs through contrastive pre-training. For improved forecastability, the text description of each user is derived based on anticipated future behaviors, while user representations are constructed from historical information. Experimental results demonstrate that our approach significantly outperforms existing baselines in cross-domain, real-world user targeting scenarios, showcasing the superior capabilities of FOUND. Moreover, our method has been successfully deployed on the Alipay platform and is widely utilized across various scenarios.</article>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph-Guided Textual Explanation Generation Framework","url":"https://arxiv.org/abs/2412.12318","date":1740114000,"author":"","guid":7704,"unread":true,"content":"<article>arXiv:2412.12318v2 Announce Type: replace \nAbstract: Natural language explanations (NLEs) are commonly used to provide plausible free-text explanations of a model's reasoning about its predictions. However, recent work has questioned their faithfulness, as they may not accurately reflect the model's internal reasoning process regarding its predicted answer. In contrast, highlight explanations--input fragments critical for the model's predicted answers--exhibit measurable faithfulness. Building on this foundation, we propose G-Tex, a Graph-Guided Textual Explanation Generation framework designed to enhance the faithfulness of NLEs. Specifically, highlight explanations are first extracted as faithful cues reflecting the model's reasoning logic toward answer prediction. They are subsequently encoded through a graph neural network layer to guide the NLE generation, which aligns the generated explanations with the model's underlying reasoning toward the predicted answer. Experiments on T5 and BART using three reasoning datasets show that G-Tex improves NLE faithfulness by up to 12.18% compared to baseline methods. Additionally, G-Tex generates NLEs with greater semantic and lexical similarity to human-written ones. Human evaluations show that G-Tex can decrease redundant content and enhance the overall quality of NLEs. Our work presents a novel method for explicitly guiding NLE generation to enhance faithfulness, serving as a foundation for addressing broader criteria in NLE and generated text.</article>","contentLength":1514,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Numerical approximations for a hyperbolic integrodifferential equation with a non-positive variable-sign kernel and nonlinear-nonlocal damping","url":"https://arxiv.org/abs/2412.07394","date":1740114000,"author":"","guid":7705,"unread":true,"content":"<article>arXiv:2412.07394v2 Announce Type: replace \nAbstract: This work considers the Galerkin approximation and analysis for a hyperbolic integrodifferential equation, where the non-positive variable-sign kernel and nonlinear-nonlocal damping with both the weak and viscous damping effects are involved. We derive the long-time stability of the solution and its finite-time uniqueness. For the semi-discrete-in-space Galerkin scheme, we derive the long-time stability of the semi-discrete numerical solution and its finite-time error estimate by technical splitting of intricate terms. Then we further apply the centering difference method and the interpolating quadrature to construct a fully discrete Galerkin scheme and prove the long-time stability of the numerical solution and its finite-time error estimate by designing a new semi-norm. Numerical experiments are performed to verify the theoretical findings.</article>","contentLength":907,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Computational Limits of State-Space Models and Mamba via the Lens of Circuit Complexity","url":"https://arxiv.org/abs/2412.06148","date":1740114000,"author":"","guid":7706,"unread":true,"content":"<article>arXiv:2412.06148v2 Announce Type: replace \nAbstract: In this paper, we analyze the computational limitations of Mamba and State-space Models (SSMs) by using the circuit complexity framework. Despite Mamba's stateful design and recent attention as a strong candidate to outperform Transformers, we have demonstrated that both Mamba and SSMs with $\\mathrm{poly}(n)$-precision and constant-depth layers reside within the $\\mathsf{DLOGTIME}$-uniform $\\mathsf{TC}^0$ complexity class. This result indicates Mamba has the same computational capabilities as Transformer theoretically, and it cannot solve problems like arithmetic formula problems, boolean formula value problems, and permutation composition problems if $\\mathsf{TC}^0 \\neq \\mathsf{NC}^1$. Therefore, it challenges the assumption Mamba is more computationally expressive than Transformers. Our contributions include rigorous proofs showing that Selective SSM and Mamba architectures can be simulated by $\\mathsf{DLOGTIME}$-uniform $\\mathsf{TC}^0$ circuits, and they cannot solve problems outside $\\mathsf{TC}^0$.</article>","contentLength":1071,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards counterfactual fairness through auxiliary variables","url":"https://arxiv.org/abs/2412.04767","date":1740114000,"author":"","guid":7707,"unread":true,"content":"<article>arXiv:2412.04767v3 Announce Type: replace \nAbstract: The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. Our code is available at https://github.com/CASE-Lab-UMD/counterfactual_fairness_2025.</article>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ParetoFlow: Guided Flows in Multi-Objective Optimization","url":"https://arxiv.org/abs/2412.03718","date":1740114000,"author":"","guid":7708,"unread":true,"content":"<article>arXiv:2412.03718v2 Announce Type: replace \nAbstract: In offline multi-objective optimization (MOO), we leverage an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives. This setting more closely mirrors complex real-world problems compared to single-objective optimization. Recent works mainly employ evolutionary algorithms and Bayesian optimization, with limited attention given to the generative modeling capabilities inherent in such data. In this study, we explore generative modeling in offline MOO through flow matching, noted for its effectiveness and efficiency. We introduce ParetoFlow, specifically designed to guide flow sampling to approximate the Pareto front. Traditional predictor (classifier) guidance is inadequate for this purpose because it models only a single objective. In response, we propose a multi-objective predictor guidance module that assigns each sample a weight vector, representing a weighted distribution across multiple objective predictions. A local filtering scheme is introduced to address non-convex Pareto fronts. These weights uniformly cover the entire objective space, effectively directing sample generation towards the Pareto front. Since distributions with similar weights tend to generate similar samples, we introduce a neighboring evolution module to foster knowledge sharing among neighboring distributions. This module generates offspring from these distributions, and selects the most promising one for the next iteration. Our method achieves state-of-the-art performance across various tasks.</article>","contentLength":1595,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An enhanced single Gaussian point continuum finite element formulation using automatic differentiation","url":"https://arxiv.org/abs/2412.02309","date":1740114000,"author":"","guid":7709,"unread":true,"content":"<article>arXiv:2412.02309v2 Announce Type: replace \nAbstract: This contribution presents an improved low-order 3D finite element formulation with hourglass stabilization using automatic differentiation (AD). Here, the former Q1STc formulation is enhanced by an approximation-free computation of the inverse Jacobian. To this end, AD tools automate the computation and allow a direct evaluation of the inverse Jacobian, bypassing the need for a Taylor series expansion. Thus, the enhanced version, Q1STc+, is introduced. Numerical examples are conducted to compare the performance of both element formulations for finite strain applications, with particular focus on distorted meshes. Moreover, the performance of the new element formulation for an elasto-plastic material is investigated. To validate the obtained results, a volumetric locking-free element based on scaled boundary parametrization is used. Both the implementation of the element routine Q1STc+ and the corresponding material subroutine are made accessible to the public at https://doi.org/10.5281/zenodo.14259791</article>","contentLength":1070,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Carleman Contraction Mapping Method for a Coefficient Inverse Problem of the Epidemiology","url":"https://arxiv.org/abs/2412.00297","date":1740114000,"author":"","guid":7710,"unread":true,"content":"<article>arXiv:2412.00297v2 Announce Type: replace \nAbstract: It is proposed to monitor spatial and temporal spreads of epidemics via solution of a Coefficient Inverse Problem for a system of three coupled nonlinear parabolic equations. To solve this problem numerically, a version of the so-called Carleman contraction mapping method is developed for this problem. On each iteration, a linear problem with the incomplete lateral Cauchy data is solved by the weighted Quasi-Reversibility Method, where the weight is the Carleman Weight Function. This is the function, which is involved as the weight in the Carleman estimate for the corresponding parabolic operator. Convergence analysis ensures the global convergence of this procedure. Numerical results demonstrate an accurate performance of this technique for noisy data.</article>","contentLength":816,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations","url":"https://arxiv.org/abs/2411.17666","date":1740114000,"author":"","guid":7711,"unread":true,"content":"<article>arXiv:2411.17666v2 Announce Type: replace \nAbstract: Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.</article>","contentLength":956,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Non-Contextual BERT or FastText? A Comparative Analysis","url":"https://arxiv.org/abs/2411.17661","date":1740114000,"author":"","guid":7712,"unread":true,"content":"<article>arXiv:2411.17661v3 Announce Type: replace \nAbstract: Natural Language Processing (NLP) for low-resource languages, which lack large annotated datasets, faces significant challenges due to limited high-quality data and linguistic resources. The selection of embeddings plays a critical role in achieving strong performance in NLP tasks. While contextual BERT embeddings require a full forward pass, non-contextual BERT embeddings rely only on table lookup. Existing research has primarily focused on contextual BERT embeddings, leaving non-contextual embeddings largely unexplored. In this study, we analyze the effectiveness of non-contextual embeddings from BERT models (MuRIL and MahaBERT) and FastText models (IndicFT and MahaFT) for tasks such as news classification, sentiment analysis, and hate speech detection in one such low-resource language Marathi. We compare these embeddings with their contextual and compressed variants. Our findings indicate that non-contextual BERT embeddings extracted from the model's first embedding layer outperform FastText embeddings, presenting a promising alternative for low-resource NLP.</article>","contentLength":1131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Assessing Vulnerability in Smart Contracts: The Role of Code Complexity Metrics in Security Analysis","url":"https://arxiv.org/abs/2411.17343","date":1740114000,"author":"","guid":7713,"unread":true,"content":"<article>arXiv:2411.17343v2 Announce Type: replace \nAbstract: Codes with specific characteristics are more exposed to security vulnerabilities. Studies have revealed that codes that do not adhere to best practices are more challenging to verify and maintain, increasing the likelihood of unnoticed or unintentionally introduced vulnerabilities. Given the crucial role of smart contracts in blockchain systems, ensuring their security and conducting thorough vulnerability analysis is critical. This study investigates the use of code complexity metrics as indicators of vulnerable code in Solidity smart contracts. We highlight the significance of complexity metrics as valuable complementary features for vulnerability assessment and provide insights into the individual power of each metric. By analyzing 21 complexity metrics, we explored their interrelation, association with vulnerability, discriminative power, and mean values in vulnerable versus neutral codes. The results revealed some high correlations and potential redundancies among certain metrics, but weak correlations between each independent metric and vulnerability. Nevertheless, we found that all metrics can effectively discriminate between vulnerable and neutral codes, and most complexity metrics, except for three, exhibited higher values in vulnerable codes.</article>","contentLength":1325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Collaborative Learning","url":"https://arxiv.org/abs/2411.12977","date":1740114000,"author":"","guid":7714,"unread":true,"content":"<article>arXiv:2411.12977v3 Announce Type: replace \nAbstract: Contemporary embodied agents powered by large language models (LLMs), such as Voyager, have shown promising capabilities in individual learning within open-ended environments like Minecraft. However, when powered by open LLMs, they struggle with basic tasks even after domain-specific fine-tuning. We present MindForge, a generative-agent framework for collaborative lifelong learning through explicit perspective taking. We introduce three key innovations: (1) a structured theory of mind representation linking percepts, beliefs, desires, and actions; (2) natural interagent communication; and (3) a multicomponent memory system. In Minecraft experiments, MindForge agents powered by open-weight LLMs significantly outperform their Voyager counterparts in basic tasks where traditional Voyager fails without GPT-4, collecting $2.3\\times$ more unique items and achieving $3\\times$ more tech-tree milestones, advancing from basic wood tools to advanced iron equipment. MindForge agents demonstrate sophisticated behaviors, including expert-novice knowledge transfer, collaborative problem solving, and adaptation to out-of-distribution tasks through accumulated collaborative experiences. MindForge advances the democratization of embodied AI development through open-ended social learning, enabling peer-to-peer knowledge sharing.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mapping out the Space of Human Feedback for Reinforcement Learning: A Conceptual Framework","url":"https://arxiv.org/abs/2411.11761","date":1740114000,"author":"","guid":7715,"unread":true,"content":"<article>arXiv:2411.11761v2 Announce Type: replace \nAbstract: Reinforcement Learning from Human feedback (RLHF) has become a powerful tool to fine-tune or train agentic machine learning models. Similar to how humans interact in social contexts, we can use many types of feedback to communicate our preferences, intentions, and knowledge to an RL agent. However, applications of human feedback in RL are often limited in scope and disregard human factors. In this work, we bridge the gap between machine learning and human-computer interaction efforts by developing a shared understanding of human feedback in interactive learning scenarios. We first introduce a taxonomy of feedback types for reward-based learning from human feedback based on nine key dimensions. Our taxonomy allows for unifying human-centered, interface-centered, and model-centered aspects. In addition, we identify seven quality metrics of human feedback influencing both the human ability to express feedback and the agent's ability to learn from the feedback. Based on the feedback taxonomy and quality criteria, we derive requirements and design choices for systems learning from human feedback. We relate these requirements and design choices to existing work in interactive machine learning. In the process, we identify gaps in existing work and future research opportunities. We call for interdisciplinary collaboration to harness the full potential of reinforcement learning with data-driven co-adaptive modeling and varied interaction mechanics.</article>","contentLength":1516,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BPO: Towards Balanced Preference Optimization between Knowledge Breadth and Depth in Alignment","url":"https://arxiv.org/abs/2411.10914","date":1740114000,"author":"","guid":7716,"unread":true,"content":"<article>arXiv:2411.10914v2 Announce Type: replace \nAbstract: Reinforcement Learning with Human Feedback (RLHF) is the key to the success of large language models (LLMs) in recent years. In this work, we first introduce the concepts of knowledge breadth and knowledge depth, which measure the comprehensiveness and depth of an LLM or knowledge source respectively. We reveal that the imbalance in the number of prompts and responses can lead to a potential disparity in breadth and depth learning within alignment tuning datasets by showing that even a simple uniform method for balancing the number of instructions and responses can lead to significant improvements. Building on this, we further propose Balanced Preference Optimization (BPO), designed to dynamically augment the knowledge depth of each sample. BPO is motivated by the observation that the usefulness of knowledge varies across samples, necessitating tailored learning of knowledge depth. To achieve this, we introduce gradient-based clustering, estimating the knowledge informativeness and usefulness of each augmented sample based on the model's optimization direction. Our experimental results across various benchmarks demonstrate that BPO outperforms other baseline methods in alignment tuning while maintaining training efficiency. Furthermore, we conduct a detailed analysis of each component of BPO, providing guidelines for future research in preference data optimization.</article>","contentLength":1440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning","url":"https://arxiv.org/abs/2411.07742","date":1740114000,"author":"","guid":7717,"unread":true,"content":"<article>arXiv:2411.07742v3 Announce Type: replace \nAbstract: This paper studies point cloud perception within outdoor environments. Existing methods face limitations in recognizing objects located at a distance or occluded, due to the sparse nature of outdoor point clouds. In this work, we observe a significant mitigation of this problem by accumulating multiple temporally consecutive point cloud sweeps, resulting in a remarkable improvement in perception accuracy. However, the computation cost also increases, hindering previous approaches from utilizing a large number of point cloud sweeps. To tackle this challenge, we find that a considerable portion of points in the accumulated point cloud is redundant, and discarding these points has minimal impact on perception accuracy. We introduce a simple yet effective Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a learned end-to-end sampling. The GSP layer is decoupled from other network components and thus can be seamlessly integrated into existing point cloud network architectures. Without incurring additional computational overhead, we increase the number of point cloud sweeps from 10, a common practice, to as many as 40. Consequently, there is a significant enhancement in perception performance. For instance, in nuScenes 3D object detection and BEV map segmentation tasks, our pruning strategy improves several 3D perception baseline methods.</article>","contentLength":1432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An exact active sensing strategy for a class of bio-inspired systems","url":"https://arxiv.org/abs/2411.06612","date":1740114000,"author":"","guid":7718,"unread":true,"content":"<article>arXiv:2411.06612v2 Announce Type: replace \nAbstract: We consider a general class of translation-invariant systems with a specific category of output nonlinearities motivated by biological sensing. We show that no dynamic output feedback can stabilize this class of systems to an isolated equilibrium point. To overcome this fundamental limitation, we propose a simple control scheme that includes a low-amplitude periodic forcing function akin to so-called \"active sensing\" in biology, together with nonlinear output feedback. Our analysis shows that this approach leads to the emergence of an exponentially stable limit cycle. These findings offer a provably stable active sensing strategy and may thus help to rationalize the active sensing movements made by animals as they perform certain motor behaviors.</article>","contentLength":809,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder","url":"https://arxiv.org/abs/2411.05195","date":1740114000,"author":"","guid":7719,"unread":true,"content":"<article>arXiv:2411.05195v2 Announce Type: replace \nAbstract: Recent research has shown that CLIP models struggle with visual reasoning tasks that require grounding compositionality, understanding spatial relationships, or capturing fine-grained details. One natural hypothesis is that the CLIP vision encoder does not embed essential information for these tasks. However, we find that this is not always the case: The encoder gathers query-relevant visual information, while CLIP fails to extract it. In particular, we show that another branch of Vision-Language Models (VLMs), Generative Multimodal Large Language Models (MLLMs), achieve significantly higher accuracy than CLIP in many of these tasks using the same vision encoder and weights, indicating that these Generative MLLMs perceive more -- as they extract and utilize visual information more effectively. We conduct a series of controlled experiments and reveal that their success is attributed to multiple key design choices, including patch tokens, position embeddings, and prompt-based weighting. On the other hand, enhancing the training data alone or applying a stronger text encoder does not suffice to solve the task, and additional text tokens offer little benefit. Interestingly, we find that fine-grained visual reasoning is not exclusive to generative models trained by an autoregressive loss: When converted into CLIP-like encoders by contrastive finetuning, these MLLMs still outperform CLIP under the same cosine similarity-based evaluation protocol. Our study highlights the importance of VLM architectural choices and suggests directions for improving the performance of CLIP-like contrastive VLMs.</article>","contentLength":1667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QUILL: Quotation Generation Enhancement of Large Language Models","url":"https://arxiv.org/abs/2411.03675","date":1740114000,"author":"","guid":7720,"unread":true,"content":"<article>arXiv:2411.03675v2 Announce Type: replace \nAbstract: While Large language models (LLMs) have become excellent writing assistants, they still struggle with quotation generation. This is because they either hallucinate when providing factual quotations or fail to provide quotes that exceed human expectations. To bridge the gap, we systematically study how to evaluate and improve LLMs' performance in quotation generation tasks. We first establish a holistic and automatic evaluation system for quotation generation task, which consists of five criteria each with corresponding automatic metric. To improve the LLMs' quotation generation abilities, we construct a bilingual knowledge base that is broad in scope and rich in dimensions, containing up to 32,022 quotes. Moreover, guided by our critiria, we further design a quotation-specific metric to rerank the retrieved quotations from the knowledge base. Extensive experiments show that our metrics strongly correlate with human preferences. Existing LLMs struggle to generate desired quotes, but our quotation knowledge base and reranking metric help narrow this gap. Our dataset and code are publicly available at https://github.com/GraceXiaoo/QUILL.</article>","contentLength":1205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interaction2Code: Benchmarking MLLM-based Interactive Webpage Code Generation from Interactive Prototyping","url":"https://arxiv.org/abs/2411.03292","date":1740114000,"author":"","guid":7721,"unread":true,"content":"<article>arXiv:2411.03292v2 Announce Type: replace \nAbstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance on the design-to-code task, i.e., generating UI code from UI mock-ups. However, existing benchmarks only contain static web pages for evaluation and ignore the dynamic interaction, limiting the practicality, usability and user engagement of the generated webpages.\n  To bridge these gaps, we present the first systematic investigation of MLLMs in generating interactive webpages. Specifically, we formulate the Interaction-to-Code task and establish the Interaction2Code benchmark, encompassing 127 unique webpages and 374 distinct interactions across 15 webpage types and 31 interaction categories. Through comprehensive experiments utilizing state-of-the-art (SOTA) MLLMs, evaluated via both automatic metrics and human assessments, we identify four critical limitations of MLLM on Interaction-to-Code task: (1) inadequate generation of interaction compared with full page, (2) prone to ten types of failure, (3) poor performance on visually subtle interactions, and (4) insufficient undestanding on interaction when limited to single-modality visual descriptions. To address these limitations, we propose four enhancement strategies: Interactive Element Highlighting, Failureaware Prompting (FAP), Visual Saliency Enhancement, and Visual-Textual Descriptions Combination, all aiming at improving MLLMs' performance on the Interaction-toCode task. The Interaction2Code benchmark and code are available in https://github. com/WebPAI/Interaction2Code.</article>","contentLength":1586,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Social-RAG: Retrieving from Group Interactions to Socially Ground AI Generation","url":"https://arxiv.org/abs/2411.02353","date":1740114000,"author":"","guid":7722,"unread":true,"content":"<article>arXiv:2411.02353v2 Announce Type: replace \nAbstract: AI agents are increasingly tasked with making proactive suggestions in online spaces where groups collaborate, yet risk being unhelpful or even annoying if they fail to match group preferences or behave in socially inappropriate ways. Fortunately, group spaces have a rich history of prior interactions and affordances for social feedback that can support grounding an agent's generations to a group's interests and norms. We present Social-RAG, a workflow for socially grounding agents that retrieves context from prior group interactions, selects relevant social signals, and feeds them into a language model to generate messages in a socially aligned manner. We implement this in \\textsc{PaperPing}, a system for posting paper recommendations in group chat, leveraging social signals determined from formative studies with 39 researchers. From a three-month deployment in 18 channels reaching 500+ researchers, we observed PaperPing posted relevant messages in groups without disrupting their existing social practices, fostering group common ground.</article>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Automatic optimal-rate convergence of randomized nets using median-of-means","url":"https://arxiv.org/abs/2411.01397","date":1740114000,"author":"","guid":7723,"unread":true,"content":"<article>arXiv:2411.01397v2 Announce Type: replace \nAbstract: We study the sample median of independently generated quasi-Monte Carlo estimators based on randomized digital nets and prove it approximates the target integral value at almost the optimal convergence rate for various function spaces. In contrast to previous methods, the algorithm does not require a priori knowledge of underlying function spaces or even an input of pre-designed $(t,m,s)$-digital nets, and is therefore easier to implement. This study provides further evidence that quasi-Monte Carlo estimators are heavy-tailed when applied to smooth integrands and taking the median can significantly improve the error by filtering out the outliers.</article>","contentLength":707,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Incentive-based Platoon Formation: Optimizing the Personal Benefit for Drivers","url":"https://arxiv.org/abs/2411.00570","date":1740114000,"author":"","guid":7724,"unread":true,"content":"<article>arXiv:2411.00570v3 Announce Type: replace \nAbstract: Platooning or cooperative adaptive cruise control (CACC) has been investigated for decades, but debate about its lasting impact is still ongoing. While the benefits of platooning and the formation of platoons are well understood for trucks, they are less clear for passenger cars, which have a higher heterogeneity in trips and drivers' preferences. Most importantly, it remains unclear how to form platoons of passenger cars in order to optimize the personal benefit for the individual driver. To this end, in this paper, we propose a novel platoon formation algorithm that optimizes the personal benefit for drivers of individual passenger cars. For computing vehicle-to-platoon assignments, the algorithm utilizes a new metric that we propose to evaluate the personal benefits of various driving systems, including platooning. By combining fuel and travel time costs into a single monetary value, drivers can estimate overall trip costs according to a personal monetary value for time spent. This provides an intuitive way for drivers to understand and compare the benefits of driving systems like human driving, adaptive cruise control (ACC), and, of course, platooning. Unlike previous similarity-based methods, our proposed algorithm forms platoons only when beneficial for the driver, rather than solely for platooning. We demonstrate the new metric for the total trip cost in a numerical analysis and explain its interpretation. Results of a large-scale simulation study demonstrate that our proposed platoon formation algorithm outperforms normal ACC as well as previous similarity-based platooning approaches by balancing fuel savings and travel time, independent of traffic and drivers' time cost.</article>","contentLength":1761,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Low-Dimensional Strain Models of Soft Robots by Looking at the Evolution of Their Shape with Application to Model-Based Control","url":"https://arxiv.org/abs/2411.00138","date":1740114000,"author":"","guid":7725,"unread":true,"content":"<article>arXiv:2411.00138v4 Announce Type: replace \nAbstract: Obtaining dynamic models of continuum soft robots is central to the analysis and control of soft robots, and researchers have devoted much attention to the challenge of proposing both data-driven and first-principle solutions. Both avenues have, however, shown their limitations; the former lacks structure and performs poorly outside training data, while the latter requires significant simplifications and extensive expert knowledge to be used in practice. This paper introduces a streamlined method for learning low-dimensional, physics-based models that are both accurate and easy to interpret. We start with an algorithm that uses image data (i.e., shape evolutions) to determine the minimal necessary segments for describing a soft robot's movement. Following this, we apply a dynamic regression and strain sparsification algorithm to identify relevant strains and define the model's dynamics. We validate our approach through simulations with various planar soft manipulators, comparing its performance against other learning strategies, showing that our models are both computationally efficient and 25x more accurate on out-of-training distribution inputs. Finally, we demonstrate that thanks to the capability of the method of generating physically compatible models, the learned models can be straightforwardly combined with model-based control policies.</article>","contentLength":1418,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Soft Condorcet Optimization for Ranking of General Agents","url":"https://arxiv.org/abs/2411.00119","date":1740114000,"author":"","guid":7726,"unread":true,"content":"<article>arXiv:2411.00119v3 Announce Type: replace \nAbstract: Driving progress of AI models and agents requires comparing their performance on standardized benchmarks; for general agents, individual performances must be aggregated across a potentially wide variety of different tasks. In this paper, we describe a novel ranking scheme inspired by social choice frameworks, called Soft Condorcet Optimization (SCO), to compute the optimal ranking of agents: the one that makes the fewest mistakes in predicting the agent comparisons in the evaluation data. This optimal ranking is the maximum likelihood estimate when evaluation data (which we view as votes) are interpreted as noisy samples from a ground truth ranking, a solution to Condorcet's original voting system criteria. SCO ratings are maximal for Condorcet winners when they exist, which we show is not necessarily true for the classical rating system Elo. We propose three optimization algorithms to compute SCO ratings and evaluate their empirical performance. When serving as an approximation to the Kemeny-Young voting method, SCO rankings are on average 0 to 0.043 away from the optimal ranking in normalized Kendall-tau distance across 865 preference profiles from the PrefLib open ranking archive. In a simulated noisy tournament setting, SCO achieves accurate approximations to the ground truth ranking and the best among several baselines when 59\\% or more of the preference data is missing. Finally, SCO ranking provides the best approximation to the optimal ranking, measured on held-out test sets, in a problem containing 52,958 human players across 31,049 games of the classic seven-player game of Diplomacy.</article>","contentLength":1672,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Generative Ray Path Sampling for Faster Point-to-Point Ray Tracing","url":"https://arxiv.org/abs/2410.23773","date":1740114000,"author":"","guid":7727,"unread":true,"content":"<article>arXiv:2410.23773v4 Announce Type: replace \nAbstract: Radio propagation modeling is essential in telecommunication research, as radio channels result from complex interactions with environmental objects. Recently, Machine Learning has been attracting attention as a potential alternative to computationally demanding tools, like Ray Tracing, which can model these interactions in detail. However, existing Machine Learning approaches often attempt to learn directly specific channel characteristics, such as the coverage map, making them highly specific to the frequency and material properties and unable to fully capture the underlying propagation mechanisms. Hence, Ray Tracing, particularly the Point-to-Point variant, remains popular to accurately identify all possible paths between transmitter and receiver nodes. Still, path identification is computationally intensive because the number of paths to be tested grows exponentially while only a small fraction is valid. In this paper, we propose a Machine Learning-aided Ray Tracing approach to efficiently sample potential ray paths, significantly reducing the computational load while maintaining high accuracy. Our model dynamically learns to prioritize potentially valid paths among all possible paths and scales linearly with scene complexity. Unlike recent alternatives, our approach is invariant with translation, scaling, or rotation of the geometry, and avoids dependency on specific environment characteristics.</article>","contentLength":1476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models for Patient Comments Multi-Label Classification","url":"https://arxiv.org/abs/2410.23528","date":1740114000,"author":"","guid":7728,"unread":true,"content":"<article>arXiv:2410.23528v3 Announce Type: replace \nAbstract: Patient experience and care quality are crucial for a hospital's sustainability and reputation. The analysis of patient feedback offers valuable insight into patient satisfaction and outcomes. However, the unstructured nature of these comments poses challenges for traditional machine learning methods following a supervised learning paradigm. This is due to the unavailability of labeled data and the nuances these texts encompass. This research explores leveraging Large Language Models (LLMs) in conducting Multi-label Text Classification (MLTC) of inpatient comments shared after a stay in the hospital. GPT-4 Turbo was leveraged to conduct the classification. However, given the sensitive nature of patients' comments, a security layer is introduced before feeding the data to the LLM through a Protected Health Information (PHI) detection framework, which ensures patients' de-identification. Additionally, using the prompt engineering framework, zero-shot learning, in-context learning, and chain-of-thought prompting were experimented with. Results demonstrate that GPT-4 Turbo, whether following a zero-shot or few-shot setting, outperforms traditional methods and Pre-trained Language Models (PLMs) and achieves the highest overall performance with an F1-score of 76.12% and a weighted F1-score of 73.61% followed closely by the few-shot learning results. Subsequently, the results' association with other patient experience structured variables (e.g., rating) was conducted. The study enhances MLTC through the application of LLMs, offering healthcare practitioners an efficient method to gain deeper insights into patient feedback and deliver prompt, appropriate responses.</article>","contentLength":1738,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Belief State Transformer","url":"https://arxiv.org/abs/2410.23506","date":1740114000,"author":"","guid":7729,"unread":true,"content":"<article>arXiv:2410.23506v2 Announce Type: replace \nAbstract: We introduce the \"Belief State Transformer\", a next-token predictor that takes both a prefix and suffix as inputs, with a novel objective of predicting both the next token for the prefix and the previous token for the suffix. The Belief State Transformer effectively learns to solve challenging problems that conventional forward-only transformers struggle with, in a domain-independent fashion. Key to this success is learning a compact belief state that captures all relevant information necessary for accurate predictions. Empirical ablations show that each component of the model is essential in difficult scenarios where standard Transformers fall short. For the task of story writing with known prefixes and suffixes, our approach outperforms the Fill-in-the-Middle method for reaching known goals and demonstrates improved performance even when the goals are unknown. Altogether, the Belief State Transformer enables more efficient goal-conditioned decoding, better test-time inference, and high-quality text representations on small scale problems. Website: https://sites.google.com/view/belief-state-transformer</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Brain age identification from diffusion MRI synergistically predicts neurodegenerative disease","url":"https://arxiv.org/abs/2410.22454","date":1740114000,"author":"","guid":7730,"unread":true,"content":"<article>arXiv:2410.22454v2 Announce Type: replace \nAbstract: Estimated brain age from magnetic resonance image (MRI) and its deviation from chronological age can provide early insights into potential neurodegenerative diseases, supporting early detection and implementation of prevention strategies. Diffusion MRI (dMRI) presents an opportunity to build an earlier biomarker for neurodegenerative disease prediction because it captures subtle microstructural changes that precede more perceptible macrostructural changes. However, the coexistence of macro- and micro-structural information in dMRI raises the question of whether current dMRI-based brain age estimation models are leveraging the intended microstructural information or if they inadvertently rely on the macrostructural information. To develop a microstructure-specific brain age, we propose a method for brain age identification from dMRI that mitigates the model's use of macrostructural information by non-rigidly registering all images to a standard template. Imaging data from 13,398 participants across 12 datasets were used for the training and evaluation. We compare our brain age models, trained with and without macrostructural information mitigated, with an architecturally similar T1-weighted (T1w) MRI-based brain age model and two recent, popular, openly available T1w MRI-based brain age models that primarily use macrostructural information. We observe difference between our dMRI-based brain age and T1w MRI-based brain age across stages of neurodegeneration, with dMRI-based brain age being older than T1w MRI-based brain age in participants transitioning from cognitively normal (CN) to mild cognitive impairment (MCI), but younger in participants already diagnosed with Alzheimer's disease (AD). Furthermore, dMRI-based brain age may offer advantages over T1w MRI-based brain age in predicting the transition from CN to MCI up to five years before diagnosis.</article>","contentLength":1935,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Large Recurrent Action Model: xLSTM enables Fast Inference for Robotics Tasks","url":"https://arxiv.org/abs/2410.22391","date":1740114000,"author":"","guid":7731,"unread":true,"content":"<article>arXiv:2410.22391v2 Announce Type: replace \nAbstract: In recent years, there has been a trend in the field of Reinforcement Learning (RL) towards large action models trained offline on large-scale datasets via sequence modeling. Existing models are primarily based on the Transformer architecture, which result in powerful agents. However, due to slow inference times, Transformer-based approaches are impractical for real-time applications, such as robotics. Recently, modern recurrent architectures, such as xLSTM and Mamba, have been proposed that exhibit parallelization benefits during training similar to the Transformer architecture while offering fast inference. In this work, we study the aptitude of these modern recurrent architectures for large action models. Consequently, we propose a Large Recurrent Action Model (LRAM) with an xLSTM at its core that comes with linear-time inference complexity and natural sequence length extrapolation abilities. Experiments on 432 tasks from 6 domains show that LRAM compares favorably to Transformers in terms of performance and speed.</article>","contentLength":1086,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Does Critical Batch Size Scale in Pre-training?","url":"https://arxiv.org/abs/2410.21676","date":1740114000,"author":"","guid":7732,"unread":true,"content":"<article>arXiv:2410.21676v3 Announce Type: replace \nAbstract: Training large-scale models under given resources requires careful design of parallelism strategies. In particular, the efficiency notion of critical batch size (CBS), concerning the compromise between time and compute, marks the threshold beyond which greater data parallelism leads to diminishing returns. To operationalize it, we propose a measure of CBS and pre-train a series of auto-regressive language models, ranging from 85 million to 1.2 billion parameters, on the C4 dataset. Through extensive hyper-parameter sweeps and careful control of factors such as batch size, momentum, and learning rate along with its scheduling, we systematically investigate the impact of scale on CBS. Then we fit scaling laws with respect to model and data sizes to decouple their effects. Overall, our results demonstrate that CBS scales primarily with data size rather than model size, a finding we justify theoretically through the analysis of infinite-width limits of neural networks and infinite-dimensional least squares regression. Of independent interest, we highlight the importance of common hyper-parameter choices and strategies for studying large-scale pre-training beyond fixed training durations.</article>","contentLength":1255,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Energy-Based Diffusion Language Models for Text Generation","url":"https://arxiv.org/abs/2410.21357","date":1740114000,"author":"","guid":7733,"unread":true,"content":"<article>arXiv:2410.21357v2 Announce Type: replace \nAbstract: Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, these models still underperform the autoregressive counterparts, with the performance gap increasing when reducing the number of sampling steps. Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models. More specifically, we introduce an EBM in a residual form, and show that its parameters can be obtained by leveraging a pretrained autoregressive model or by finetuning a bidirectional transformer via noise contrastive estimation. We also propose an efficient generation algorithm via parallel important sampling. Comprehensive experiments on language modeling benchmarks show that our model can consistently outperform state-of-the-art diffusion models by a significant margin, and approaches autoregressive models' perplexity. We further show that, without any generation performance drop, our framework offers a 1.3$\\times$ sampling speedup over existing diffusion models.</article>","contentLength":1519,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation","url":"https://arxiv.org/abs/2410.18974","date":1740114000,"author":"","guid":7734,"unread":true,"content":"<article>arXiv:2410.18974v2 Announce Type: replace \nAbstract: Multi-view image diffusion models have significantly advanced open-domain 3D object generation. However, most existing models rely on 2D network architectures that lack inherent 3D biases, resulting in compromised geometric consistency. To address this challenge, we introduce 3D-Adapter, a plug-in module designed to infuse 3D geometry awareness into pretrained image diffusion models. Central to our approach is the idea of 3D feedback augmentation: for each denoising step in the sampling loop, 3D-Adapter decodes intermediate multi-view features into a coherent 3D representation, then re-encodes the rendered RGBD views to augment the pretrained base model through feature addition. We study two variants of 3D-Adapter: a fast feed-forward version based on Gaussian splatting and a versatile training-free version utilizing neural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter not only greatly enhances the geometry quality of text-to-multi-view models such as Instant3D and Zero123++, but also enables high-quality 3D generation using the plain text-to-image Stable Diffusion. Furthermore, we showcase the broad application potential of 3D-Adapter by presenting high quality results in text-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.</article>","contentLength":1339,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New paper-by-paper classification for Scopus based on references reclassified by the origin of the papers citing them","url":"https://arxiv.org/abs/2410.18662","date":1740114000,"author":"","guid":7735,"unread":true,"content":"<article>arXiv:2410.18662v2 Announce Type: replace \nAbstract: A reference-based classification system for individual Scopus publications is presented which takes into account the categories of the papers citing those references instead of the journals in which those cited papers are published. It supports multiple assignments of up to 5 categories within the Scopus ASJC structure, but eliminates the Multidisciplinary Area and the miscellaneous categories, and it allows for the reclassification of a greater number of publications (potentially 100%) than traditional reference-based systems. Twelve variants of the system were obtained by adjusting different parameters, which were applied to the more than 3.2 million citable papers from the active Scientific Journals in 2020 indexed in Scopus. The results were analyzed and compared with other classification systems such as the original journal-based Scopus ASJC, the 2-generation-reference based M3-AWC-0.8 (\\'Alvarez-Llorente et al., 2024), and the corresponding authors' assignment based AAC (\\'Alvarez-Llorente et al., 2023). The different variants obtained of the classification give results that improve those used as references in multiple scientometric fields. The variation called U1-F-0.8 seems especially promising due to its restraint in assigning multiple categories, consistency with reference classifications and the fact of applying normalization processes to avoid the overinfluence of articles that have a greater number of references.</article>","contentLength":1502,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Audio-Language Models Understand Linguistic Variations?","url":"https://arxiv.org/abs/2410.16505","date":1740114000,"author":"","guid":7736,"unread":true,"content":"<article>arXiv:2410.16505v2 Announce Type: replace \nAbstract: Open-vocabulary audio language models (ALMs), like Contrastive Language Audio Pretraining (CLAP), represent a promising new paradigm for audio-text retrieval using natural language queries. In this paper, for the first time, we perform controlled experiments on various benchmarks to show that existing ALMs struggle to generalize to linguistic variations in textual queries. To address this issue, we propose RobustCLAP, a novel and compute-efficient technique to learn audio-language representations agnostic to linguistic variations. Specifically, we reformulate the contrastive loss used in CLAP architectures by introducing a multi-view contrastive learning objective, where paraphrases are treated as different views of the same audio scene and use this for training. Our proposed approach improves the text-to-audio retrieval performance of CLAP by 0.8%-13% across benchmarks and enhances robustness to linguistic variation.</article>","contentLength":984,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STAR: A Simple Training-free Approach for Recommendations using Large Language Models","url":"https://arxiv.org/abs/2410.16458","date":1740114000,"author":"","guid":7737,"unread":true,"content":"<article>arXiv:2410.16458v2 Announce Type: replace \nAbstract: Recent progress in large language models (LLMs) offers promising new approaches for recommendation system tasks. While the current state-of-the-art methods rely on fine-tuning LLMs to achieve optimal results, this process is costly and introduces significant engineering complexities. Conversely, methods that directly use LLMs without additional fine-tuning result in a large drop in recommendation quality, often due to the inability to capture collaborative information. In this paper, we propose a Simple Training-free Approach for Recommendation (STAR), a framework that utilizes LLMs and can be applied to various recommendation tasks without the need for fine-tuning, while maintaining high quality recommendation performance. Our approach involves a retrieval stage that uses semantic embeddings from LLMs combined with collaborative user information to retrieve candidate items. We then apply an LLM for pairwise ranking to enhance next-item prediction. Experimental results on the Amazon Review dataset show competitive performance for next item prediction, even with our retrieval stage alone. Our full method achieves Hits@10 performance of +23.8% on Beauty, +37.5% on Toys &amp; Games, and -1.8% on Sports &amp; Outdoors relative to the best supervised models. This framework offers an effective alternative to traditional supervised models, highlighting the potential of LLMs in recommendation systems without extensive training or custom architectures.</article>","contentLength":1512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models for Autonomous Driving (LLM4AD): Concept, Benchmark, Experiments, and Challenges","url":"https://arxiv.org/abs/2410.15281","date":1740114000,"author":"","guid":7738,"unread":true,"content":"<article>arXiv:2410.15281v3 Announce Type: replace \nAbstract: With the broader usage and highly successful development of Large Language Models (LLMs), there has been a growth of interest and demand for applying LLMs to autonomous driving technology. Driven by their natural language understanding and reasoning ability, LLMs have the potential to enhance various aspects of autonomous driving systems, from perception and scene understanding to language interaction and decision-making. In this paper, we first introduce the novel concept of designing LLMs for autonomous driving (LLM4AD). Then, we propose a comprehensive benchmark for evaluating the instruction-following abilities of LLM4AD in simulation. Furthermore, we conduct a series of experiments on real-world vehicle platforms, thoroughly evaluating the performance and potential of our LLM4AD systems. Finally, we envision the main challenges of LLM4AD, including latency, deployment, security and privacy, safety, trust and transparency, and personalization. Our research highlights the significant potential of LLMs to enhance various aspects of autonomous vehicle technology, from perception and scene understanding to language interaction and decision-making.</article>","contentLength":1218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation","url":"https://arxiv.org/abs/2410.14251","date":1740114000,"author":"","guid":7739,"unread":true,"content":"<article>arXiv:2410.14251v2 Announce Type: replace \nAbstract: Post-training is essential for enabling large language models (LLMs) to follow human instructions. However, its effectiveness depends on high-quality instruction data, which is challenging to obtain in the real world due to privacy concerns, data scarcity, and high annotation costs. To fill this gap, inspired by the recent success of using LLMs to simulate human society, we propose MATRIX, a multi-agent simulator that automatically generates diverse text-based scenarios, capturing a wide range of real-world human needs in a realistic and scalable manner. Leveraging these outputs, we introduce a novel scenario-driven instruction generator MATRIX-Gen for controllable and highly realistic data synthesis. Extensive experiments demonstrate that our framework effectively generates both general and domain-specific data. On AlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on datasets synthesized by MATRIX-Gen with just 20K instruction-response pairs, outperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M pairs.</article>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Diffusion Models for Multi-Agent Partial Observability: Shared Attractors, Error Bounds, and Composite Flow","url":"https://arxiv.org/abs/2410.13953","date":1740114000,"author":"","guid":7740,"unread":true,"content":"<article>arXiv:2410.13953v3 Announce Type: replace \nAbstract: Multiagent systems grapple with partial observability (PO), and the decentralized POMDP (Dec-POMDP) model highlights the fundamental nature of this challenge. Whereas recent approaches to addressing PO have appealed to deep learning models, providing a rigorous understanding of how these models and their approximation errors affect agents' handling of PO and their interactions remain a challenge. In addressing this challenge, we investigate reconstructing global states from local action-observation histories in Dec-POMDPs using diffusion models. We first find that diffusion models conditioned on local history represent possible states as stable fixed points. In collectively observable (CO) Dec-POMDPs, individual diffusion models conditioned on agents' local histories share a unique fixed point corresponding to the global state, while in non-CO settings, shared fixed points yield a distribution of possible states given joint history. We further find that, with deep learning approximation errors, fixed points can deviate from true states and the deviation is negatively correlated to the Jacobian rank. Inspired by this low-rank property, we bound a deviation by constructing a surrogate linear regression model that approximates the local behavior of a diffusion model. With this bound, we propose a \\emph{composite diffusion process} iterating over agents with theoretical convergence guarantees to the true state.</article>","contentLength":1483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Repetition Neurons: How Do Language Models Produce Repetitions?","url":"https://arxiv.org/abs/2410.13497","date":1740114000,"author":"","guid":7741,"unread":true,"content":"<article>arXiv:2410.13497v2 Announce Type: replace \nAbstract: This paper introduces repetition neurons, regarded as skill neurons responsible for the repetition problem in text generation tasks. These neurons are progressively activated more strongly as repetition continues, indicating that they perceive repetition as a task to copy the previous context repeatedly, similar to in-context learning. We identify these repetition neurons by comparing activation values before and after the onset of repetition in texts generated by recent pre-trained language models. We analyze the repetition neurons in three English and one Japanese pre-trained language models and observe similar patterns across them.</article>","contentLength":695,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Algorithmic Content Selection and the Impact of User Disengagement","url":"https://arxiv.org/abs/2410.13108","date":1740114000,"author":"","guid":7742,"unread":true,"content":"<article>arXiv:2410.13108v2 Announce Type: replace \nAbstract: Digital services face a fundamental trade-off in content selection: they must balance the immediate revenue gained from high-reward content against the long-term benefits of maintaining user engagement. Traditional multi-armed bandit models assume that users remain perpetually engaged, failing to capture the possibility that users may disengage when dissatisfied, thereby reducing future revenue potential.\n  In this work, we introduce a model for the content selection problem that explicitly accounts for variable user engagement and disengagement. In our framework, content that maximizes immediate reward is not necessarily optimal in terms of fostering sustained user engagement.\n  Our contributions are twofold. First, we develop computational and statistical methods for offline optimization and online learning of content selection policies. For users whose engagement patterns are defined by $k$ distinct levels, we design a dynamic programming algorithm that computes the exact optimal policy in $O(k^2)$ time. Moreover, we derive no-regret learning guarantees for an online learning setting in which the platform serves a series of users with unknown and potentially adversarial engagement patterns.\n  Second, we introduce the concept of modified demand elasticity which captures how small changes in a user's overall satisfaction affect the platform's ability to secure long-term revenue. This notion generalizes classical demand elasticity by incorporating the dynamics of user re-engagement, thereby revealing key insights into the interplay between engagement and revenue. Notably, our analysis uncovers a counterintuitive phenomenon: although higher friction (i.e., a reduced likelihood of re-engagement) typically lowers overall revenue, it can simultaneously lead to higher user engagement under optimal content selection policies.</article>","contentLength":1904,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late in Training","url":"https://arxiv.org/abs/2410.10373","date":1740114000,"author":"","guid":7743,"unread":true,"content":"<article>arXiv:2410.10373v2 Announce Type: replace \nAbstract: Sharpness-Aware Minimization (SAM) has substantially improved the generalization of neural networks under various settings. Despite the success, its effectiveness remains poorly understood. In this work, we discover an intriguing phenomenon in the training dynamics of SAM, shedding light on understanding its implicit bias towards flatter minima over Stochastic Gradient Descent (SGD). Specifically, we find that SAM efficiently selects flatter minima late in training. Remarkably, even a few epochs of SAM applied at the end of training yield nearly the same generalization and solution sharpness as full SAM training. Subsequently, we delve deeper into the underlying mechanism behind this phenomenon. Theoretically, we identify two phases in the learning dynamics after applying SAM late in training: i) SAM first escapes the minimum found by SGD exponentially fast; and ii) then rapidly converges to a flatter minimum within the same valley. Furthermore, we empirically investigate the role of SAM during the early training phase. We conjecture that the optimization method chosen in the late phase is more crucial in shaping the final solution's properties. Based on this viewpoint, we extend our findings from SAM to Adversarial Training.</article>","contentLength":1298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Looped ReLU MLPs May Be All You Need as Practical Programmable Computers","url":"https://arxiv.org/abs/2410.09375","date":1740114000,"author":"","guid":7744,"unread":true,"content":"<article>arXiv:2410.09375v2 Announce Type: replace \nAbstract: Previous work has demonstrated that attention mechanisms are Turing complete. More recently, it has been shown that a looped 9-layer Transformer can function as a universal programmable computer. In contrast, the multi-layer perceptrons with $\\mathsf{ReLU}$ activation ($\\mathsf{ReLU}$-$\\mathsf{MLP}$), one of the most fundamental components of neural networks, is known to be expressive; specifically, a two-layer neural network is a universal approximator given an exponentially large number of hidden neurons. However, it remains unclear whether a $\\mathsf{ReLU}$-$\\mathsf{MLP}$ can be made into a universal programmable computer using a practical number of weights. In this work, we provide an affirmative answer that a looped 23-layer $\\mathsf{ReLU}$-$\\mathsf{MLP}$ is capable of performing the basic necessary operations, more efficiently and effectively functioning as a programmable computer than a looped Transformer. This indicates simple modules have stronger expressive power than previously expected and have not been fully explored. Our work provides insights into the mechanisms of neural networks and demonstrates that complex tasks, such as functioning as a programmable computer, do not necessarily require advanced architectures like Transformers.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vector-ICL: In-context Learning with Continuous Vector Representations","url":"https://arxiv.org/abs/2410.05629","date":1740114000,"author":"","guid":7745,"unread":true,"content":"<article>arXiv:2410.05629v2 Announce Type: replace \nAbstract: Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms.</article>","contentLength":1153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting In-context Learning Inference Circuit in Large Language Models","url":"https://arxiv.org/abs/2410.04468","date":1740114000,"author":"","guid":7746,"unread":true,"content":"<article>arXiv:2410.04468v4 Announce Type: replace \nAbstract: In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Input Text Encode: LMs encode every input text (in the demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations of demonstrations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. Through careful measurements, the proposed inference circuit successfully captures and unifies many fragmented phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit.</article>","contentLength":1743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pose Prior Learner: Unsupervised Categorical Prior Learning for Pose Estimation","url":"https://arxiv.org/abs/2410.03858","date":1740114000,"author":"","guid":7747,"unread":true,"content":"<article>arXiv:2410.03858v2 Announce Type: replace \nAbstract: A prior represents a set of beliefs or assumptions about a system, aiding inference and decision-making. In this paper, we introduce the challenge of unsupervised categorical prior learning in pose estimation, where AI models learn a general pose prior for an object category from images in a self-supervised manner. Although priors are effective in estimating pose, acquiring them can be difficult. We propose a novel method, named Pose Prior Learner (PPL), to learn a general pose prior for any object category. PPL uses a hierarchical memory to store compositional parts of prototypical poses, from which we distill a general pose prior. This prior improves pose estimation accuracy through template transformation and image reconstruction. PPL learns meaningful pose priors without any additional human annotations or interventions, outperforming competitive baselines on both human and animal pose estimation datasets. Notably, our experimental results reveal the effectiveness of PPL using learned prototypical poses for pose estimation on occluded images. Through iterative inference, PPL leverages the pose prior to refine estimated poses, regressing them to any prototypical poses stored in memory. Our code, model, and data will be publicly available.</article>","contentLength":1314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Grammar Induction from Visual, Speech and Text","url":"https://arxiv.org/abs/2410.03739","date":1740114000,"author":"","guid":7748,"unread":true,"content":"<article>arXiv:2410.03739v2 Announce Type: replace \nAbstract: Grammar Induction could benefit from rich heterogeneous signals, such as text, vision, and acoustics. In the process, features from distinct modalities essentially serve complementary roles to each other. With such intuition, this work introduces a novel \\emph{unsupervised visual-audio-text grammar induction} task (named \\textbf{VAT-GI}), to induce the constituent grammar trees from parallel images, text, and speech inputs. Inspired by the fact that language grammar natively exists beyond the texts, we argue that the text has not to be the predominant modality in grammar induction. Thus we further introduce a \\emph{textless} setting of VAT-GI, wherein the task solely relies on visual and auditory inputs. To approach the task, we propose a visual-audio-text inside-outside recursive autoencoder (\\textbf{VaTiora}) framework, which leverages rich modal-specific and complementary features for effective grammar parsing. Besides, a more challenging benchmark data is constructed to assess the generalization ability of VAT-GI system. Experiments on two benchmark datasets demonstrate that our proposed VaTiora system is more effective in incorporating the various multimodal signals, and also presents new state-of-the-art performance of VAT-GI.</article>","contentLength":1305,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark","url":"https://arxiv.org/abs/2410.03051","date":1740114000,"author":"","guid":7749,"unread":true,"content":"<article>arXiv:2410.03051v2 Announce Type: replace \nAbstract: Video detailed captioning is a key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AuroraCap, a video captioner based on a large multimodal model. We follow the simplest architecture design without additional parameters for temporal modeling. To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of input visual tokens. Surprisingly, we found that this strategy results in little performance loss. AuroraCap shows superior performance on various video and image captioning benchmarks, for example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include simple descriptions, consisting of a few dozen words, which limits research in this field. Therefore, we develop VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. In addition, we propose a new LLM-assisted metric VDCscore for bettering evaluation, which adopts a divide-and-conquer strategy to transform long caption evaluation into multiple short question-answer pairs. With the help of human Elo ranking, our experiments show that this benchmark better correlates with human judgments of video detailed captioning quality.</article>","contentLength":1468,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM","url":"https://arxiv.org/abs/2410.00486","date":1740114000,"author":"","guid":7750,"unread":true,"content":"<article>arXiv:2410.00486v3 Announce Type: replace \nAbstract: Simultaneous Localization and Mapping (SLAM) is pivotal in robotics, with photorealistic scene reconstruction emerging as a key challenge. To address this, we introduce Computational Alignment for Real-Time Gaussian Splatting SLAM (CaRtGS), a novel method enhancing the efficiency and quality of photorealistic scene reconstruction in real-time environments. Leveraging 3D Gaussian Splatting (3DGS), CaRtGS achieves superior rendering quality and processing speed, which is crucial for scene photorealistic reconstruction. Our approach tackles computational misalignment in Gaussian Splatting SLAM (GS-SLAM) through an adaptive strategy that enhances optimization iterations, addresses long-tail optimization, and refines densification. Experiments on Replica, TUM-RGBD, and VECtor datasets demonstrate CaRtGS's effectiveness in achieving high-fidelity rendering with fewer Gaussian primitives. This work propels SLAM towards real-time, photorealistic dense rendering, significantly advancing photorealistic scene representation. For the benefit of the research community, we release the code and accompanying videos on our project website: https://dapengfeng.github.io/cartgs.</article>","contentLength":1230,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning","url":"https://arxiv.org/abs/2410.00255","date":1740114000,"author":"","guid":7751,"unread":true,"content":"<article>arXiv:2410.00255v2 Announce Type: replace \nAbstract: Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning. Notably, we achieve a 7.8\\% improvement in the grounding task (Multi3DRefer) and a 6.9\\% improvement in the captioning task (Scan2Cap).</article>","contentLength":1540,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Strongly-polynomial time and validation analysis of policy gradient methods","url":"https://arxiv.org/abs/2409.19437","date":1740114000,"author":"","guid":7752,"unread":true,"content":"<article>arXiv:2409.19437v4 Announce Type: replace \nAbstract: This paper proposes a novel termination criterion, termed the advantage gap function, for finite state and action Markov decision processes (MDP) and reinforcement learning (RL). By incorporating this advantage gap function into the design of step size rules and deriving a new linear rate of convergence that is independent of the stationary state distribution of the optimal policy, we demonstrate that policy gradient methods can solve MDPs in strongly-polynomial time. To the best of our knowledge, this is the first time that such strong convergence properties have been established for policy gradient methods. Moreover, in the stochastic setting, where only stochastic estimates of policy gradients are available, we show that the advantage gap function provides close approximations of the optimality gap for each individual state and exhibits a sublinear rate of convergence at every state. The advantage gap function can be easily estimated in the stochastic case, and when coupled with easily computable upper bounds on policy values, they provide a convenient way to validate the solutions generated by policy gradient methods. Therefore, our developments offer a principled and computable measure of optimality for RL, whereas current practice tends to rely on algorithm-to-algorithm or baselines comparisons with no certificate of optimality.</article>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Adaptive Frequency Sampling for Data-driven Model Order Reduction Applied to Antenna Responses","url":"https://arxiv.org/abs/2409.18734","date":1740114000,"author":"","guid":7753,"unread":true,"content":"<article>arXiv:2409.18734v3 Announce Type: replace \nAbstract: Frequency domain sweeps of array antennas are well-known to be time-intensive, and different surrogate models have been used to improve the performance. Data-driven model order reduction algorithms, such as the Loewner framework and vector fitting, can be integrated with these adaptive error estimates, in an iterative algorithm, to reduce the number of full-wave simulations required to accurately capture the requested frequency behavior of multiport array antennas. In this work, we propose two novel adaptive methods exploiting a block matrix function which is a key part of the Loewner framework generating system approach. The first algorithm leverages an inherent matrix parameter freedom in the block matrix function to identify frequency points with large errors, whereas the second utilizes the condition number of the block matrix function. Both methods effectively provide frequency domain error estimates, which are essential for improved performance. Numerical experiments on multiport array antenna S-parameters demonstrate the effectiveness of our proposed algorithms within the Loewner framework, where the proposed algorithms reach the smallest errors for the smallest number of frequency points chosen.</article>","contentLength":1275,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Second Order Bounds for Contextual Bandits with Function Approximation","url":"https://arxiv.org/abs/2409.16197","date":1740114000,"author":"","guid":7754,"unread":true,"content":"<article>arXiv:2409.16197v2 Announce Type: replace \nAbstract: Many works have developed no-regret algorithms for contextual bandits with function approximation, where the mean reward function over context-action pairs belongs to a function class. Although there are many approaches to this problem, one that has gained in importance is the use of algorithms based on the optimism principle such as optimistic least squares. It can be shown the regret of this algorithm scales as square root of the product of the eluder dimension (a statistical measure of the complexity of the function class), the logarithm of the function class size and the time horizon. Unfortunately, even if the variance of the measurement noise of the rewards at each time is changing and is very small, the regret of the optimistic least squares algorithm scales with square root of the time horizon. In this work we are the first to develop algorithms that satisfy regret bounds of scaling not with the square root of the time horizon, but the square root of the sum of the measurement variances in the setting of contextual bandits with function approximation when the variances are unknown. These bounds generalize existing techniques for deriving second order bounds in contextual linear problems.</article>","contentLength":1267,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VaLID: Verification as Late Integration of Detections for LiDAR-Camera Fusion","url":"https://arxiv.org/abs/2409.15529","date":1740114000,"author":"","guid":7755,"unread":true,"content":"<article>arXiv:2409.15529v3 Announce Type: replace \nAbstract: Vehicle object detection benefits from both LiDAR and camera data, with LiDAR offering superior performance in many scenarios. Fusion of these modalities further enhances accuracy, but existing methods often introduce complexity or dataset-specific dependencies. In our study, we propose a model-adaptive late-fusion method, VaLID, which validates whether each predicted bounding box is acceptable or not. Our method verifies the higher-performing, yet overly optimistic LiDAR model detections using camera detections that are obtained from either specially trained, general, or open-vocabulary models. VaLID uses a lightweight neural verification network trained with a high recall bias to reduce the false predictions made by the LiDAR detector, while still preserving the true ones. Evaluating with multiple combinations of LiDAR and camera detectors on the KITTI dataset, we reduce false positives by an average of 63.9%, thus outperforming the individual detectors on 3D average precision (3DAP). Our approach is model-adaptive and demonstrates state-of-the-art competitive performance even when using generic camera detectors that were not trained specifically for this dataset.</article>","contentLength":1237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Design of Wavelet Filter Banks for Any Dilation Using Extended Laplacian Pyramid Matrices","url":"https://arxiv.org/abs/2409.14242","date":1740114000,"author":"","guid":7756,"unread":true,"content":"<article>arXiv:2409.14242v2 Announce Type: replace \nAbstract: In this paper, we present a new method for designing wavelet filter banks for any dilation matrices and in any dimension. Our approach utilizes extended Laplacian pyramid matrices to achieve this flexibility. By generalizing recent tight wavelet frame construction methods based on the sum of squares representation, we introduce the sum of vanishing products (SVP) condition, which is significantly easier to satisfy. These flexible design methods rely on our main results, which establish the equivalence between the SVP and mixed unitary extension principle conditions. Additionally, we provide illustrative examples to showcase our main findings.</article>","contentLength":703,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting","url":"https://arxiv.org/abs/2409.12518","date":1740114000,"author":"","guid":7757,"unread":true,"content":"<article>arXiv:2409.12518v3 Announce Type: replace \nAbstract: We propose Hier-SLAM, a semantic 3D Gaussian Splatting SLAM method featuring a novel hierarchical categorical representation, which enables accurate global 3D semantic mapping, scaling-up capability, and explicit semantic label prediction in the 3D world. The parameter usage in semantic SLAM systems increases significantly with the growing complexity of the environment, making it particularly challenging and costly for scene understanding. To address this problem, we introduce a novel hierarchical representation that encodes semantic information in a compact form into 3D Gaussian Splatting, leveraging the capabilities of large language models (LLMs). We further introduce a novel semantic loss designed to optimize hierarchical semantic information through both inter-level and cross-level optimization. Furthermore, we enhance the whole SLAM system, resulting in improved tracking and mapping performance. Our Hier-SLAM outperforms existing dense SLAM methods in both mapping and tracking accuracy, while achieving a 2x operation speed-up. Additionally, it exhibits competitive performance in rendering semantic segmentation in small synthetic scenes, with significantly reduced storage and training time requirements. Rendering FPS impressively reaches 2,000 with semantic information and 3,000 without it. Most notably, it showcases the capability of handling the complex real-world scene with more than 500 semantic classes, highlighting its valuable scaling-up capability.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Infrared Small Target Detection in Satellite Videos: A New Dataset and A Novel Recurrent Feature Refinement Framework","url":"https://arxiv.org/abs/2409.12448","date":1740114000,"author":"","guid":7758,"unread":true,"content":"<article>arXiv:2409.12448v3 Announce Type: replace \nAbstract: Multi-frame infrared small target (MIRST) detection in satellite videos is a long-standing, fundamental yet challenging task for decades, and the challenges can be summarized as: First, extremely small target size, highly complex clutters &amp; noises, various satellite motions result in limited feature representation, high false alarms, and difficult motion analyses. Second, the lack of large-scale public available MIRST dataset in satellite videos greatly hinders the algorithm development. To address the aforementioned challenges, in this paper, we first build a large-scale dataset for MIRST detection in satellite videos (namely IRSatVideo-LEO), and then develop a recurrent feature refinement (RFR) framework as the baseline method. Specifically, IRSatVideo-LEO is a semi-simulated dataset with synthesized satellite motion, target appearance, trajectory and intensity, which can provide a standard toolbox for satellite video generation and a reliable evaluation platform to facilitate the algorithm development. For baseline method, RFR is proposed to be equipped with existing powerful CNN-based methods for long-term temporal dependency exploitation and integrated motion compensation &amp; MIRST detection. Specifically, a pyramid deformable alignment (PDA) module and a temporal-spatial-frequency modulation (TSFM) module are proposed to achieve effective and efficient feature alignment, propagation, aggregation and refinement. Extensive experiments have been conducted to demonstrate the effectiveness and superiority of our scheme. The comparative results show that ResUNet equipped with RFR outperforms the state-of-the-art MIRST detection methods. Dataset and code are released at https://github.com/XinyiYing/RFR.</article>","contentLength":1782,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An efficient wavelet-based physics-informed neural networks for singularly perturbed problems","url":"https://arxiv.org/abs/2409.11847","date":1740114000,"author":"","guid":7759,"unread":true,"content":"<article>arXiv:2409.11847v2 Announce Type: replace \nAbstract: Physics-informed neural networks (PINNs) are a class of deep learning models that utilize physics in the form of differential equations to address complex problems, including ones that may involve limited data availability. However, tackling solutions of differential equations with rapid oscillations, steep gradients, or singular behavior becomes challenging for PINNs. Considering these challenges, we designed an efficient wavelet-based PINNs (W-PINNs) model to address this class of differential equations. Here, we represent the solution in wavelet space using a family of smooth-compactly supported wavelets. This framework represents the solution of a differential equation with significantly fewer degrees of freedom while still retaining the dynamics of complex physical phenomena. The architecture allows the training process to search for a solution within the wavelet space, making the process faster and more accurate. Further, the proposed model does not rely on automatic differentiations for derivatives involved in differential equations and does not require any prior information regarding the behavior of the solution, such as the location of abrupt features. Thus, through a strategic fusion of wavelets with PINNs, W-PINNs excel at capturing localized nonlinear information, making them well-suited for problems showing abrupt behavior in certain regions, such as singularly perturbed and multiscale problems. The efficiency and accuracy of the proposed neural network model are demonstrated in various 1D and 2D test problems, i.e., the FitzHugh-Nagumo (FHN) model, the Helmholtz equation, the Maxwell's equation, lid-driven cavity flow, and the Allen-Cahn equation, along with other highly singularly perturbed nonlinear differential equations. The proposed model significantly improves with traditional PINNs, recently developed wavelet-based PINNs, and other state-of-the-art methods.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"3D Water Quality Mapping using Invariant Extended Kalman Filtering for Underwater Robot Localization","url":"https://arxiv.org/abs/2409.11578","date":1740114000,"author":"","guid":7760,"unread":true,"content":"<article>arXiv:2409.11578v2 Announce Type: replace \nAbstract: Water quality mapping for critical parameters such as temperature, salinity, and turbidity is crucial for assessing an aquaculture farm's health and yield capacity. Traditional approaches involve using boats or human divers, which are time-constrained and lack depth variability. This work presents an innovative approach to 3D water quality mapping in shallow water environments using a BlueROV2 equipped with GPS and a water quality sensor. This system allows for accurate location correction by resurfacing when errors occur. This study is being conducted at an oyster farm in the Chesapeake Bay, USA, providing a more comprehensive and precise water quality analysis in aquaculture settings.</article>","contentLength":748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MI-HGNN: Morphology-Informed Heterogeneous Graph Neural Network for Legged Robot Contact Perception","url":"https://arxiv.org/abs/2409.11146","date":1740114000,"author":"","guid":7761,"unread":true,"content":"<article>arXiv:2409.11146v2 Announce Type: replace \nAbstract: We present a Morphology-Informed Heterogeneous Graph Neural Network (MI-HGNN) for learning-based contact perception. The architecture and connectivity of the MI-HGNN are constructed from the robot morphology, in which nodes and edges are robot joints and links, respectively. By incorporating the morphology-informed constraints into a neural network, we improve a learning-based approach using model-based knowledge. We apply the proposed MI-HGNN to two contact perception problems, and conduct extensive experiments using both real-world and simulated data collected using two quadruped robots. Our experiments demonstrate the superiority of our method in terms of effectiveness, generalization ability, model efficiency, and sample efficiency. Our MI-HGNN improved the performance of a state-of-the-art model that leverages robot morphological symmetry by 8.4% with only 0.21% of its parameters. Although MI-HGNN is applied to contact perception problems for legged robots in this work, it can be seamlessly applied to other types of multi-body dynamical systems and has the potential to improve other robot learning frameworks. Our code is made publicly available at https://github.com/lunarlab-gatech/Morphology-Informed-HGNN.</article>","contentLength":1284,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the effects of similarity metrics in decentralized deep learning under distributional shift","url":"https://arxiv.org/abs/2409.10720","date":1740114000,"author":"","guid":7762,"unread":true,"content":"<article>arXiv:2409.10720v2 Announce Type: replace \nAbstract: Decentralized Learning (DL) enables privacy-preserving collaboration among organizations or users to enhance the performance of local deep learning models. However, model aggregation becomes challenging when client data is heterogeneous, and identifying compatible collaborators without direct data exchange remains a pressing issue. In this paper, we investigate the effectiveness of various similarity metrics in DL for identifying peers for model merging, conducting an empirical analysis across multiple datasets with distribution shifts. Our research provides insights into the performance of these metrics, examining their role in facilitating effective collaboration. By exploring the strengths and limitations of these metrics, we contribute to the development of robust DL methods.</article>","contentLength":843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Digital Twins Meet the Koopman Operator: Data-Driven Learning for Robust Autonomy","url":"https://arxiv.org/abs/2409.10347","date":1740114000,"author":"","guid":7763,"unread":true,"content":"<article>arXiv:2409.10347v2 Announce Type: replace \nAbstract: Contrary to on-road autonomous navigation, off-road autonomy is complicated by various factors ranging from sensing challenges to terrain variability. In such a milieu, data-driven approaches have been commonly employed to capture intricate vehicle-environment interactions effectively. However, the success of data-driven methods depends crucially on the quality and quantity of data, which can be compromised by large variability in off-road environments. To address these concerns, we present a novel methodology to recreate the exact vehicle and its target operating conditions digitally for domain-specific data generation. This enables us to effectively model off-road vehicle dynamics from simulation data using the Koopman operator theory, and employ the obtained models for local motion planning and optimal vehicle control. The capabilities of the proposed methodology are demonstrated through an autonomous navigation problem of a 1:5 scale vehicle, where a terrain-informed planner is employed for global mission planning. Results indicate a substantial improvement in off-road navigation performance with the proposed algorithm (5.84x) and underscore the efficacy of digital twinning in terms of improving the sample efficiency (3.2x) and reducing the sim2real gap (5.2%).</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Think Together and Work Better: Combining Humans' and LLMs' Think-Aloud Outcomes for Effective Text Evaluation","url":"https://arxiv.org/abs/2409.07355","date":1740114000,"author":"","guid":7764,"unread":true,"content":"<article>arXiv:2409.07355v2 Announce Type: replace \nAbstract: This study introduces \\textbf{InteractEval}, a framework that integrates human expertise and Large Language Models (LLMs) using the Think-Aloud (TA) method to generate attributes for checklist-based text evaluation. By combining human flexibility and reasoning with LLM consistency, InteractEval outperforms traditional non-LLM-based and LLM-based baselines across four distinct dimensions, consisting of Coherence, Fluency, Consistency, and Relevance. The experiment also investigates the effectiveness of the TA method, showing that it promotes divergent thinking in both humans and LLMs, leading to the generation of a wider range of relevant attributes and enhance text evaluation performance. Comparative analysis reveals that humans excel at identifying attributes related to internal quality (Coherence and Fluency), but LLMs perform better at those attributes related to external alignment (Consistency and Relevance). Consequently, leveraging both humans and LLMs together produces the best evaluation outcomes. In other words, this study emphasizes the necessity of effectively combining humans and LLMs in an automated checklist-based text evaluation framework. The code is available at \\textbf{\\url{https://github.com/BBeeChu/InteractEval.git}}.</article>","contentLength":1310,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection","url":"https://arxiv.org/abs/2409.06490","date":1740114000,"author":"","guid":7765,"unread":true,"content":"<article>arXiv:2409.06490v4 Announce Type: replace \nAbstract: The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance, security, and airspace management has created an urgent demand for precise, scalable, and efficient UAV detection. However, existing datasets often suffer from limited scale diversity and inaccurate annotations, hindering robust model development. This paper introduces UAVDB, a high-resolution UAV detection dataset constructed using Patch Intensity Convergence (PIC). This novel technique automatically generates high-fidelity bounding box annotations from UAV trajectory data~\\cite{li2020reconstruction}, eliminating the need for manual labeling. UAVDB features single-class annotations with a fixed-camera setup and consists of RGB frames capturing UAVs across various scales, from large-scale UAVs to near-single-pixel representations, along with challenging backgrounds that pose difficulties for modern detectors. We first validate the accuracy and efficiency of PIC-generated bounding boxes by comparing Intersection over Union (IoU) performance and runtime against alternative annotation methods, demonstrating that PIC achieves higher annotation accuracy while being more efficient. Subsequently, we benchmark UAVDB using state-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable resource for advancing long-range and high-resolution UAV detection.</article>","contentLength":1411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalized Approximate Message-Passing for Compressed Sensing with Sublinear Sparsity","url":"https://arxiv.org/abs/2409.06320","date":1740114000,"author":"","guid":7766,"unread":true,"content":"<article>arXiv:2409.06320v2 Announce Type: replace \nAbstract: This paper addresses the reconstruction of an unknown signal vector with sublinear sparsity from generalized linear measurements. Generalized approximate message-passing (GAMP) is proposed via state evolution in the sublinear sparsity limit, where the signal dimension $N$, measurement dimension $M$, and signal sparsity $k$ satisfy $\\log k/\\log N\\to \\gamma\\in[0, 1)$ and $M/\\{k\\log (N/k)\\}\\to\\delta$ as $N$ and $k$ tend to infinity. While the overall flow in state evolution is the same as that for linear sparsity, each proof step for inner denoising requires stronger assumptions than those for linear sparsity. The required new assumptions are proved for Bayesian inner denoising. When Bayesian outer and inner denoisers are used in GAMP, the obtained state evolution recursion is utilized to evaluate the prefactor $\\delta$ in the sample complexity, called reconstruction threshold. If and only if $\\delta$ is larger than the reconstruction threshold, Bayesian GAMP can achieve asymptotically exact signal reconstruction. In particular, the reconstruction threshold is finite for noisy linear measurements when the support of non-zero signal elements does not include a neighborhood of zero. As numerical examples, this paper considers linear measurements and 1-bit compressed sensing. Numerical simulations for both cases show that Bayesian GAMP outperforms existing algorithms for sublinear sparsity in terms of the sample complexity.</article>","contentLength":1494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs","url":"https://arxiv.org/abs/2409.05806","date":1740114000,"author":"","guid":7767,"unread":true,"content":"<article>arXiv:2409.05806v2 Announce Type: replace \nAbstract: Chinese, as a linguistic system rich in depth and complexity, is characterized by distinctive elements such as ancient poetry, proverbs, idioms, and other cultural constructs. However, current Large Language Models (LLMs) face limitations in these specialized domains, highlighting the need for the development of comprehensive datasets that can assess, continuously update, and progressively improve these culturally-grounded linguistic competencies through targeted training optimizations. To address this gap, we introduce CKnowEdit, the first-ever Chinese knowledge editing dataset designed to correct linguistic, factual, and logical errors in LLMs. We collect seven types of knowledge from a wide range of sources, including classical texts, idioms, and content from Baidu Tieba Ruozhiba, taking into account the unique polyphony, antithesis, and logical structures inherent in the Chinese language. By analyzing this dataset, we highlight the challenges current LLMs face in mastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge editing techniques reveals opportunities to advance the correction of Chinese knowledge. Code and dataset are available at https://github.com/zjunlp/EasyEdit.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"View-Invariant Policy Learning via Zero-Shot Novel View Synthesis","url":"https://arxiv.org/abs/2409.03685","date":1740114000,"author":"","guid":7768,"unread":true,"content":"<article>arXiv:2409.03685v2 Announce Type: replace \nAbstract: Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.</article>","contentLength":1374,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners","url":"https://arxiv.org/abs/2409.01524","date":1740114000,"author":"","guid":7769,"unread":true,"content":"<article>arXiv:2409.01524v2 Announce Type: replace \nAbstract: Self-correction is a novel method that can stimulate the potential reasoning abilities of large language models (LLMs). It involves detecting and correcting errors during the inference process when LLMs solve reasoning problems. However, recent works do not regard self-correction as a spontaneous and intrinsic capability of LLMs. Instead, such correction is achieved through post-hoc generation, external knowledge introduction, multi-model collaboration, and similar techniques. In this paper, we propose a series of mathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous Step-level Self-correction for Mathematical reasoning. This capability helps LLMs to recognize whether their ongoing inference tends to contain errors and simultaneously correct these errors to produce a more reliable response. We proposed a method, which employs a step-level sampling approach to construct step-wise self-correction data for achieving such ability. Additionally, we implement a training strategy that uses above constructed data to equip LLMs with spontaneous step-level self-correction capacities. Our data and methods have been demonstrated to be effective across various foundation LLMs, consistently showing significant progress in evaluations on GSM8K, MATH, and other mathematical benchmarks. To the best of our knowledge, we are the first to introduce the spontaneous step-level self-correction ability of LLMs in mathematical reasoning.</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms","url":"https://arxiv.org/abs/2408.15740","date":1740114000,"author":"","guid":7770,"unread":true,"content":"<article>arXiv:2408.15740v3 Announce Type: replace \nAbstract: Vision Language Place Recognition (VLVPR) enhances robot localization performance by incorporating natural language descriptions from images. By utilizing language information, VLVPR directs robot place matching, overcoming the constraint of solely depending on vision. The essence of multimodal fusion lies in mining the complementary information between different modalities. However, general fusion methods rely on traditional neural architectures and are not well equipped to capture the dynamics of cross modal interactions, especially in the presence of complex intra modal and inter modal correlations. To this end, this paper proposes a novel coarse to fine and end to end connected cross modal place recognition framework, called MambaPlace. In the coarse localization stage, the text description and 3D point cloud are encoded by the pretrained T5 and instance encoder, respectively. They are then processed using Text Attention Mamba (TAM) and Point Clouds Mamba (PCM) for data enhancement and alignment. In the subsequent fine localization stage, the features of the text description and 3D point cloud are cross modally fused and further enhanced through cascaded Cross Attention Mamba (CCAM). Finally, we predict the positional offset from the fused text point cloud features, achieving the most accurate localization. Extensive experiments show that MambaPlace achieves improved localization accuracy on the KITTI360Pose dataset compared to the state of the art methods.</article>","contentLength":1538,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A domain decomposition-based autoregressive deep learning model for unsteady and nonlinear partial differential equations","url":"https://arxiv.org/abs/2408.14461","date":1740114000,"author":"","guid":7771,"unread":true,"content":"<article>arXiv:2408.14461v3 Announce Type: replace \nAbstract: In this paper, we propose a domain-decomposition-based deep learning (DL) framework, named transient-CoMLSim, for accurately modeling unsteady and nonlinear partial differential equations (PDEs). The framework consists of two key components: (a) a convolutional neural network (CNN)-based autoencoder architecture and (b) an autoregressive model composed of fully connected layers. Unlike existing state-of-the-art methods that operate on the entire computational domain, our CNN-based autoencoder computes a lower-dimensional basis for solution and condition fields represented on subdomains. Timestepping is performed entirely in the latent space, generating embeddings of the solution variables from the time history of embeddings of solution and condition variables. This approach not only reduces computational complexity but also enhances scalability, making it well-suited for large-scale simulations. Furthermore, to improve the stability of our rollouts, we employ a curriculum learning (CL) approach during the training of the autoregressive model. The domain-decomposition strategy enables scaling to out-of-distribution domain sizes while maintaining the accuracy of predictions -- a feature not easily integrated into popular DL-based approaches for physics simulations. We benchmark our model against two widely-used DL architectures, Fourier Neural Operator (FNO) and U-Net, and demonstrate that our framework outperforms them in terms of accuracy, extrapolation to unseen timesteps, and stability for a wide range of use cases.</article>","contentLength":1596,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating The Explainability of State-of-the-Art Deep Learning-based Network Intrusion Detection Systems","url":"https://arxiv.org/abs/2408.14040","date":1740114000,"author":"","guid":7772,"unread":true,"content":"<article>arXiv:2408.14040v3 Announce Type: replace \nAbstract: Network Intrusion Detection Systems (NIDSs) which use deep learning (DL) models achieve high detection performance and accuracy while avoiding dependence on fixed signatures extracted from attack artifacts. However, there is a noticeable hesitance among network security experts and practitioners when it comes to deploying DL-based NIDSs in real-world production environments due to their black-box nature, i.e., how and why the underlying models make their decisions. In this work, we analyze state-of-the-art DL-based NIDS models using explainable AI (xAI) techniques (e.g., TRUSTEE, SHAP) through extensive experiments with two different attack datasets. Using the explanations generated for the models' decisions, the most prominent features used by each NIDS model considered are presented. We compare the explanations generated across xAI methods for a given NIDS model as well as the explanations generated across the NIDS models for a given xAI method. Finally, we evaluate the vulnerability of each NIDS model to inductive bias (artifacts learnt from training data). The results show that: (1) some DL-based NIDS models can be better explained than other models, (2) xAI explanations are in conflict for most of the NIDS models considered in this work and (3) some NIDS models are more vulnerable to inductive bias than other models.</article>","contentLength":1396,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Commutator-free Cayley methods","url":"https://arxiv.org/abs/2408.13043","date":1740114000,"author":"","guid":7773,"unread":true,"content":"<article>arXiv:2408.13043v2 Announce Type: replace \nAbstract: Differential equations posed on quadratic matrix Lie groups arise in the context of classical mechanics and quantum dynamical systems. Lie group numerical integrators preserve the constants of motions defining the Lie group. Thus, they respect important physical laws of the dynamical system, such as unitarity and energy conservation in the context of quantum dynamical systems, for instance. In this article we develop a high-order commutator free Lie group integrator for non-autonomous differential equations evolving on quadratic Lie groups. Instead of matrix exponentials, which are expensive to evaluate and need to be approximated by appropriate rational functions in order to preserve the Lie group structure, the proposed method is obtained as a composition of Cayley transforms which naturally respect the structure of quadratic Lie groups while being computationally efficient to evaluate. Unlike Cayley-Magnus methods the method is also free from nested matrix commutators.</article>","contentLength":1039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RAG-Optimized Tibetan Tourism LLMs: Enhancing Accuracy and Personalization","url":"https://arxiv.org/abs/2408.12003","date":1740114000,"author":"","guid":7774,"unread":true,"content":"<article>arXiv:2408.12003v2 Announce Type: replace \nAbstract: With the development of the modern social economy, tourism has become an important way to meet people's spiritual needs, bringing development opportunities to the tourism industry. However, existing large language models (LLMs) face challenges in personalized recommendation capabilities and the generation of content that can sometimes produce hallucinations. This study proposes an optimization scheme for Tibet tourism LLMs based on retrieval-augmented generation (RAG) technology. By constructing a database of tourist viewpoints and processing the data using vectorization techniques, we have significantly improved retrieval accuracy. The application of RAG technology effectively addresses the hallucination problem in content generation. The optimized model shows significant improvements in fluency, accuracy, and relevance of content generation. This research demonstrates the potential of RAG technology in the standardization of cultural tourism information and data analysis, providing theoretical and technical support for the development of intelligent cultural tourism service systems.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PFDiff: Training-Free Acceleration of Diffusion Models Combining Past and Future Scores","url":"https://arxiv.org/abs/2408.08822","date":1740114000,"author":"","guid":7775,"unread":true,"content":"<article>arXiv:2408.08822v3 Announce Type: replace \nAbstract: Diffusion Probabilistic Models (DPMs) have shown remarkable potential in image generation, but their sampling efficiency is hindered by the need for numerous denoising steps. Most existing solutions accelerate the sampling process by proposing fast ODE solvers. However, the inevitable discretization errors of the ODE solvers are significantly magnified when the number of function evaluations (NFE) is fewer. In this work, we propose PFDiff, a novel training-free and orthogonal timestep-skipping strategy, which enables existing fast ODE solvers to operate with fewer NFE. Specifically, PFDiff initially utilizes score replacement from past time steps to predict a ``springboard\". Subsequently, it employs this ``springboard\" along with foresight updates inspired by Nesterov momentum to rapidly update current intermediate states. This approach effectively reduces unnecessary NFE while correcting for discretization errors inherent in first-order ODE solvers. Experimental results demonstrate that PFDiff exhibits flexible applicability across various pre-trained DPMs, particularly excelling in conditional DPMs and surpassing previous state-of-the-art training-free methods. For instance, using DDIM as a baseline, we achieved 16.46 FID (4 NFE) compared to 138.81 FID with DDIM on ImageNet 64x64 with classifier guidance, and 13.06 FID (10 NFE) on Stable Diffusion with 7.5 guidance scale. Code is available at \\url{https://github.com/onefly123/PFDiff}.</article>","contentLength":1513,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Extracting Sentence Embeddings from Pretrained Transformer Models","url":"https://arxiv.org/abs/2408.08073","date":1740114000,"author":"","guid":7776,"unread":true,"content":"<article>arXiv:2408.08073v2 Announce Type: replace \nAbstract: Pre-trained transformer models shine in many natural language processing tasks and therefore are expected to bear the representation of the input sentence or text meaning. These sentence-level embeddings are also important in retrieval-augmented generation. But do commonly used plain averaging or prompt templates sufficiently capture and represent the underlying meaning? After providing a comprehensive review of existing sentence embedding extraction and refinement methods, we thoroughly test different combinations and our original extensions of the most promising ones on pretrained models. Namely, given 110 M parameters, BERT's hidden representations from multiple layers, and many tokens, we try diverse ways to extract optimal sentence embeddings. We test various token aggregation and representation post-processing techniques. We also test multiple ways of using a general Wikitext dataset to complement BERT's sentence embeddings. All methods are tested on eight Semantic Textual Similarity (STS), six short text clustering, and twelve classification tasks. We also evaluate our representation-shaping techniques on other static models, including random token representations. Proposed representation extraction methods improve the performance on STS and clustering tasks for all models considered. Very high improvements for static token-based models, especially random embeddings for STS tasks, almost reach the performance of BERT-derived representations. Our work shows that the representation-shaping techniques significantly improve sentence embeddings extracted from BERT-based and simple baseline models.</article>","contentLength":1679,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eliminating Backdoors in Neural Code Models for Secure Code Understanding","url":"https://arxiv.org/abs/2408.04683","date":1740114000,"author":"","guid":7777,"unread":true,"content":"<article>arXiv:2408.04683v2 Announce Type: replace \nAbstract: Neural code models (NCMs) have been widely used to address various code understanding tasks, such as defect detection. However, numerous recent studies reveal that such models are vulnerable to backdoor attacks. Backdoored NCMs function normally on normal/clean code snippets, but exhibit adversary-expected behavior on poisoned code snippets injected with the adversary-crafted trigger. It poses a significant security threat. Therefore, there is an urgent need for effective techniques to detect and eliminate backdoors stealthily implanted in NCMs.\n  To address this issue, in this paper, we innovatively propose a backdoor elimination technique for secure code understanding, called EliBadCode. EliBadCode eliminates backdoors in NCMs by inverting/reverse-engineering and unlearning backdoor triggers. Specifically, EliBadCode first filters the model vocabulary for trigger tokens based on the naming conventions of specific programming languages to reduce the trigger search space and cost. Then, EliBadCode introduces a sample-specific trigger position identification method, which can reduce the interference of non-backdoor (adversarial) perturbations for subsequent trigger inversion, thereby producing effective inverted backdoor triggers efficiently. Backdoor triggers can be viewed as backdoor (adversarial) perturbations. Subsequently, EliBadCode employs a Greedy Coordinate Gradient algorithm to optimize the inverted trigger and designs a trigger anchoring method to purify the inverted trigger. Finally, EliBadCode eliminates backdoors through model unlearning. We evaluate the effectiveness of EliBadCode in eliminating backdoors implanted in multiple NCMs used for three safety-critical code understanding tasks. The results demonstrate that EliBadCode can effectively eliminate backdoors while having minimal adverse effects on the normal functionality of the model.</article>","contentLength":1938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SCOOT: SLO-Oriented Performance Tuning for LLM Inference Engines","url":"https://arxiv.org/abs/2408.04323","date":1740114000,"author":"","guid":7778,"unread":true,"content":"<article>arXiv:2408.04323v2 Announce Type: replace \nAbstract: As large language models (LLMs) are gaining increasing popularity across a wide range of web applications, it is of great importance to optimize service-level objectives (SLOs) for LLM inference services to enhance user satisfaction and improve the competitiveness of cloud vendors. In this paper, we observe that adjusting the parameters of LLM inference engines can improve service performance, and the optimal parameter configurations of different services are different. Therefore, we propose SCOOT, an automatic performance tuning system to optimize SLOs for each LLM inference service by tuning the parameters of the inference engine. SCOOT jointly exploits single-objective and multiple-objective Bayesian optimization (BO) techniques to handle various optimization objectives via exploration and exploitation. Moreover, SCOOT prunes the search space with known constraints and adopts a random forest to learn hidden constraints during the tuning process to mitigate invalid exploration. To improve the tuning efficiency, SCOOT utilizes the parallel suggestion to accelerate the tuning process. Extensive experiments demonstrate that SCOOT considerably outperforms existing tuning techniques in SLO optimization while greatly improving the tuning efficiency. Moreover, SCOOT is universally applicable to various LLM inference engines including vLLM and TensorRT-LLM. Currently, SCOOT has already been implemented in the production environment at Ant Group.</article>","contentLength":1516,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Imputation for prediction: beware of diminishing returns","url":"https://arxiv.org/abs/2407.19804","date":1740114000,"author":"","guid":7779,"unread":true,"content":"<article>arXiv:2407.19804v2 Announce Type: replace \nAbstract: Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifying if and when investing in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 19 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, improving imputation only has a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits.</article>","contentLength":1248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Trends in Language Model Robustness","url":"https://arxiv.org/abs/2407.18213","date":1740114000,"author":"","guid":7780,"unread":true,"content":"<article>arXiv:2407.18213v4 Announce Type: replace \nAbstract: Language models exhibit scaling laws, whereby increasing model and dataset size predictably decrease negative log likelihood, unlocking a dazzling array of capabilities. At the same time, even the most capable systems are currently vulnerable to adversarial inputs such as jailbreaks and prompt injections, despite concerted efforts to make them robust. As compute becomes more accessible to both attackers and defenders, which side will benefit more from scale? We attempt to answer this question with a detailed study of robustness on language models spanning three orders of magnitude in parameter count. From the defender's perspective, we find that in the absence of other interventions, increasing model size alone does not consistently improve robustness. In adversarial training, we find that larger models are more sample-efficient and less compute-efficient than smaller models, and often better generalize their defense to new threat models. From the attacker's perspective, we find that increasing attack compute smoothly and reliably increases attack success rate against both finetuned and adversarially trained models. Finally, we show that across model sizes studied, doubling compute on adversarial training only forces an attacker to less than double attack compute to maintain the same attack success rate. However, adversarial training becomes more and more effective on larger models, suggesting that defenders could eventually have the advantage with increasing model size. These results underscore the value of adopting a scaling lens when discussing robustness of frontier models.</article>","contentLength":1657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"metabench -- A Sparse Benchmark of Reasoning and Knowledge in Large Language Models","url":"https://arxiv.org/abs/2407.12844","date":1740114000,"author":"","guid":7781,"unread":true,"content":"<article>arXiv:2407.12844v2 Announce Type: replace \nAbstract: Large Language Models (LLMs) vary in their abilities on a range of tasks. Initiatives such as the Open LLM Leaderboard aim to quantify these differences with several large benchmarks (sets of test items to which an LLM can respond either correctly or incorrectly). However, high correlations within and between benchmark scores suggest that (1) there exists a small set of common underlying abilities that these benchmarks measure, and (2) items tap into redundant information and the benchmarks may thus be considerably compressed. We use data from n &gt; 5000 LLMs to identify the most informative items of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28,632 items in total). From them we distill a sparse benchmark, metabench, that has less than 3% of the original size of all six benchmarks combined. This new sparse benchmark goes beyond point scores by yielding estimators of the underlying benchmark-specific abilities. We show that these estimators (1) can be used to reconstruct each original individual benchmark score with, on average, 1.24% root mean square error (RMSE), (2) reconstruct the original total score with 0.58% RMSE, and (3) have a single underlying common factor whose Spearman correlation with the total score is r = 0.94.</article>","contentLength":1335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Dynamics of LLM Finetuning","url":"https://arxiv.org/abs/2407.10490","date":1740114000,"author":"","guid":7782,"unread":true,"content":"<article>arXiv:2407.10490v3 Announce Type: replace \nAbstract: Learning dynamics, which describes how the learning of specific training examples influences the model's predictions on other examples, gives us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during different types of finetuning, by analyzing the step-wise decomposition of how influence accumulates among different potential responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. In particular, we propose a hypothetical explanation of why specific types of hallucination are strengthened after finetuning, e.g., the model might use phrases or facts in the response for question B to answer question A, or the model might keep repeating similar simple phrases when generating responses. We also extend our framework and highlight a unique \"squeezing effect\" to explain a previously observed phenomenon in off-policy direct preference optimization (DPO), where running DPO for too long makes even the desired outputs less likely. This framework also provides insights into where the benefits of on-policy DPO and other variants come from. The analysis not only provides a novel perspective of understanding LLM's finetuning but also inspires a simple, effective method to improve alignment performance.</article>","contentLength":1464,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unifying Approach to Product Constructions for Quantitative Temporal Inference","url":"https://arxiv.org/abs/2407.10465","date":1740114000,"author":"","guid":7783,"unread":true,"content":"<article>arXiv:2407.10465v3 Announce Type: replace \nAbstract: Probabilistic programs are a powerful and convenient approach to formalise distributions over system executions. A classical verification problem for probabilistic programs is temporal inference: to compute the likelihood that the execution traces satisfy a given temporal property. This paper presents a general framework for temporal inference, which applies to a rich variety of quantitative models including those that arise in the operational semantics of probabilistic and weighted programs.\n  The key idea underlying our framework is that in a variety of existing approaches, the main construction that enables temporal inference is that of a product between the system of interest and the temporal property. We provide a unifying mathematical definition of product constructions, enabled by the realisation that 1) both systems and temporal properties can be modelled as coalgebras and 2) product constructions are distributive laws in this context. Our categorical framework leads us to our main contribution: a sufficient condition for correctness, which is precisely what enables to use the product construction for temporal inference.\n  We show that our framework can be instantiated to naturally recover a number of disparate approaches from the literature including, e.g., partial expected rewards in Markov reward models, resource-sensitive reachability analysis, and weighted optimization problems. Further, we demonstrate a product of weighted programs and weighted temporal properties as a new instance to show the scalability of our approach.</article>","contentLength":1614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Convex space learning for tabular synthetic data generation","url":"https://arxiv.org/abs/2407.09789","date":1740114000,"author":"","guid":7784,"unread":true,"content":"<article>arXiv:2407.09789v2 Announce Type: replace \nAbstract: Generating synthetic samples from the convex space of the minority class is a popular oversampling approach for imbalanced classification problems. Recently, deep-learning approaches have been successfully applied to modeling the convex space of minority samples. Beyond oversampling, learning the convex space of neighborhoods in training data has not been used to generate entire tabular datasets. In this paper, we introduce a deep learning architecture (NextConvGeN) with a generator and discriminator component that can generate synthetic samples by learning to model the convex space of tabular data. The generator takes data neighborhoods as input and creates synthetic samples within the convex space of that neighborhood. Thereafter, the discriminator tries to classify these synthetic samples against a randomly sampled batch of data from the rest of the data space. We compared our proposed model with five state-of-the-art tabular generative models across ten publicly available datasets from the biomedical domain. Our analysis reveals that synthetic samples generated by NextConvGeN can better preserve classification and clustering performance across real and synthetic data than other synthetic data generation models. Synthetic data generation by deep learning of the convex space produces high scores for popular utility measures. We further compared how diverse synthetic data generation strategies perform in the privacy-utility spectrum and produced critical arguments on the necessity of high utility models. Our research on deep learning of the convex space of tabular data opens up opportunities in clinical research, machine learning model development, decision support systems, and clinical data sharing.</article>","contentLength":1783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Embodying Control in Soft Multistable Grippers from morphofunctional co-design","url":"https://arxiv.org/abs/2407.08111","date":1740114000,"author":"","guid":7785,"unread":true,"content":"<article>arXiv:2407.08111v2 Announce Type: replace \nAbstract: Soft robots are distinguished by their flexible and adaptable, allowing them to perform tasks that are nearly impossible for rigid robots. However, controlling their configuration is challenging due to their nonlinear material response and infinite deflection degrees of freedom. A potential solution is to discretize the infinite-dimensional configuration space of soft robots into a finite but sufficiently large number of functional shapes. This study explores a co-design strategy for pneumatically actuated soft grippers with multiple encoded stable states, enabling desired functional shape and stiffness reconfiguration. An energy based analytical model for soft multistable grippers is presented, mapping the robots' infinite-dimensional configuration space into discrete stable states, allowing for prediction of the systems final state and dynamic behavior. Our approach introduces a general method to capture the soft robots' response with the lattice lumped parameters using automatic relevance determination regression, facilitating inverse co-design. The resulting computationally efficient model enables us to explore the configuration space in a tractable manner, allowing the inverse co-design of our robots by setting desired targeted positions with optimized stiffness of the set targets. This strategy offers a framework for controlling soft robots by exploiting the nonlinear mechanics of multistable structures, thus embodying mechanical intelligence into soft structures.</article>","contentLength":1547,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal Scientific Understanding","url":"https://arxiv.org/abs/2407.04903","date":1740114000,"author":"","guid":7786,"unread":true,"content":"<article>arXiv:2407.04903v3 Announce Type: replace \nAbstract: Scientific figure interpretation is a crucial capability for AI-driven scientific assistants built on advanced Large Vision Language Models. However, current datasets and benchmarks primarily focus on simple charts or other relatively straightforward figures from limited science domains. To address this gap, we present a comprehensive dataset compiled from peer-reviewed Nature Communications articles covering 72 scientific fields, encompassing complex visualizations such as schematic diagrams, microscopic images, and experimental data which require graduate-level expertise to interpret. We evaluated 19 proprietary and open-source models on two benchmark tasks, figure captioning and multiple-choice, and conducted human expert annotation. Our analysis revealed significant task challenges and performance gaps among models. Beyond serving as a benchmark, this dataset serves as a valuable resource for large-scale training. Fine-tuning Qwen2-VL-7B with our task-specific data achieved better performance than GPT-4o and even human experts in multiple-choice evaluations. Furthermore, continuous pre-training on our interleaved article and figure data substantially enhanced the model's downstream task performance in materials science. We have released our dataset to support further research.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Question Answering with Texts and Tables through Deep Reinforcement Learning","url":"https://arxiv.org/abs/2407.04858","date":1740114000,"author":"","guid":7787,"unread":true,"content":"<article>arXiv:2407.04858v2 Announce Type: replace \nAbstract: This paper proposes a novel architecture to generate multi-hop answers to open domain questions that require information from texts and tables, using the Open Table-and-Text Question Answering dataset for validation and training. One of the most common ways to generate answers in this setting is to retrieve information sequentially, where a selected piece of data helps searching for the next piece. As different models can have distinct behaviors when called in this sequential information search, a challenge is how to select models at each step. Our architecture employs reinforcement learning to choose between different state-of-the-art tools sequentially until, in the end, a desired answer is generated. This system achieved an F1-score of 19.03, comparable to iterative systems in the literature.</article>","contentLength":859,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Entity Decomposition with Filtering: A Zero-Shot Clinical Named Entity Recognition Framework","url":"https://arxiv.org/abs/2407.04629","date":1740114000,"author":"","guid":7788,"unread":true,"content":"<article>arXiv:2407.04629v2 Announce Type: replace \nAbstract: Clinical named entity recognition (NER) aims to retrieve important entities within clinical narratives. Recent works have demonstrated that large language models (LLMs) can achieve strong performance in this task. While previous works focus on proprietary LLMs, we investigate how open NER LLMs, trained specifically for entity recognition, perform in clinical NER. Our initial experiment reveals significant contrast in performance for some clinical entities and how a simple exploitment on entity types can alleviate this issue. In this paper, we introduce a novel framework, entity decomposition with filtering, or EDF. Our key idea is to decompose the entity recognition task into several retrievals of entity sub-types and then filter them. Our experimental results demonstrate the efficacies of our framework and the improvements across all metrics, models, datasets, and entity types. Our analysis also reveals substantial improvement in recognizing previously missed entities using entity decomposition. We further provide a comprehensive evaluation of our framework and an in-depth error analysis to pave future works.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DaBiT: Depth and Blur informed Transformer for Video Focal Deblurring","url":"https://arxiv.org/abs/2407.01230","date":1740114000,"author":"","guid":7789,"unread":true,"content":"<article>arXiv:2407.01230v3 Announce Type: replace \nAbstract: In many real-world scenarios, recorded videos suffer from accidental focus blur, and while video deblurring methods exist, most specifically target motion blur or spatial-invariant blur. This paper introduces a framework optimized for the as yet unattempted task of video focal deblurring (refocusing). The proposed method employs novel map-guided transformers, in addition to image propagation, to effectively leverage the continuous spatial variance of focal blur and restore the footage. We also introduce a flow re-focusing module designed to efficiently align relevant features between blurry and sharp domains. Additionally, we propose a novel technique for generating synthetic focal blur data, broadening the model's learning capabilities and robustness to include a wider array of content. We have made a new benchmark dataset, DAVIS-Blur, available. This dataset, a modified extension of the popular DAVIS video segmentation set, provides realistic focal blur degradations as well as the corresponding blur maps. Comprehensive experiments demonstrate the superiority of our approach. We achieve state-of-the-art results with an average PSNR performance over 1.9dB greater than comparable existing video restoration methods. Our source code and the developed databases will be made available at https://github.com/crispianm/DaBiT</article>","contentLength":1391,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning","url":"https://arxiv.org/abs/2406.17764","date":1740114000,"author":"","guid":7790,"unread":true,"content":"<article>arXiv:2406.17764v2 Announce Type: replace \nAbstract: This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual in-context knowledge editing (IKE) across 53 languages, unifying three knowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff. Cross-lingual KE, which requires knowledge edited in one language to generalize across others while preserving unrelated knowledge, remains underexplored. To address this gap, we systematically evaluate IKE under zero-shot, one-shot, and few-shot setups, incorporating tailored metric-specific demonstrations. Our findings reveal that model scale and demonstration alignment critically govern cross-lingual IKE efficacy, with larger models and tailored demonstrations significantly improving performance. Linguistic properties, particularly script type, strongly influence performance variation across languages, with non-Latin languages underperforming due to issues like language confusion.</article>","contentLength":962,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Numerical methods for eigenvalues of singular polynomial eigenvalue problems","url":"https://arxiv.org/abs/2406.16832","date":1740114000,"author":"","guid":7791,"unread":true,"content":"<article>arXiv:2406.16832v2 Announce Type: replace \nAbstract: Recently, three numerical methods for the computation of eigenvalues of singular matrix pencils, based on a rank-completing perturbation, a rank-projection, or an augmentation were developed. We show that all three approaches can be generalized to treat singular polynomial eigenvalue problems. The common denominator of all three approaches is a transformation of a singular into a regular matrix polynomial whose eigenvalues are a disjoint union of the eigenvalues of the singular polynomial, called true eigenvalues, and additional fake eigenvalues. The true eigenvalues can then be separated from the fake eigenvalues using information on the corresponding left and right eigenvectors. We illustrate the approaches on several interesting applications, including bivariate polynomial systems and ZGV points.</article>","contentLength":863,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data","url":"https://arxiv.org/abs/2406.14773","date":1740114000,"author":"","guid":7792,"unread":true,"content":"<article>arXiv:2406.14773v2 Announce Type: replace \nAbstract: Retrieval-augmented generation (RAG) enhances the outputs of language models by integrating relevant information retrieved from external knowledge sources. However, when the retrieval process involves private data, RAG systems may face severe privacy risks, potentially leading to the leakage of sensitive information. To address this issue, we propose using synthetic data as a privacy-preserving alternative for the retrieval data. We propose SAGE, a novel two-stage synthetic data generation paradigm. In the stage-1, we employ an attribute-based extraction and generation approach to preserve key contextual information from the original data. In the stage-2, we further enhance the privacy properties of the synthetic data through an agent-based iterative refinement process. Extensive experiments demonstrate that using our synthetic data as the retrieval context achieves comparable performance to using the original data while substantially reducing privacy risks. Our work takes the first step towards investigating the possibility of generating high-utility and privacy-preserving synthetic data for RAG, opening up new opportunities for the safe application of RAG systems in various domains.</article>","contentLength":1256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Visible-Thermal Tiny Object Detection: A Benchmark Dataset and Baselines","url":"https://arxiv.org/abs/2406.14482","date":1740114000,"author":"","guid":7793,"unread":true,"content":"<article>arXiv:2406.14482v2 Announce Type: replace \nAbstract: Small object detection (SOD) has been a longstanding yet challenging task for decades, with numerous datasets and algorithms being developed. However, they mainly focus on either visible or thermal modality, while visible-thermal (RGBT) bimodality is rarely explored. Although some RGBT datasets have been developed recently, the insufficient quantity, limited category, misaligned images and large target size cannot provide an impartial benchmark to evaluate multi-category visible-thermal small object detection (RGBT SOD) algorithms. In this paper, we build the first large-scale benchmark with high diversity for RGBT SOD (namely RGBT-Tiny), including 115 paired sequences, 93K frames and 1.2M manual annotations. RGBT-Tiny contains abundant targets (7 categories) and high-diversity scenes (8 types that cover different illumination and density variations). Note that, over 81% of targets are smaller than 16x16, and we provide paired bounding box annotations with tracking ID to offer an extremely challenging benchmark with wide-range applications, such as RGBT fusion, detection and tracking. In addition, we propose a scale adaptive fitness (SAFit) measure that exhibits high robustness on both small and large targets. The proposed SAFit can provide reasonable performance evaluation and promote detection performance. Based on the proposed RGBT-Tiny dataset and SAFit measure, extensive evaluations have been conducted, including 23 recent state-of-the-art algorithms that cover four different types (i.e., visible generic detection, visible SOD, thermal SOD and RGBT object detection). Project is available at https://github.com/XinyiYing/RGBT-Tiny.</article>","contentLength":1715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Taxonomy-Guided Zero-Shot Recommendations with LLMs","url":"https://arxiv.org/abs/2406.14043","date":1740114000,"author":"","guid":7794,"unread":true,"content":"<article>arXiv:2406.14043v3 Announce Type: replace \nAbstract: With the emergence of large language models (LLMs) and their ability to perform a variety of tasks, their application in recommender systems (RecSys) has shown promise. However, we are facing significant challenges when deploying LLMs into RecSys, such as limited prompt length, unstructured item information, and un-constrained generation of recommendations, leading to sub-optimal performance. To address these issues, we propose a novel method using a taxonomy dictionary. This method provides a systematic framework for categorizing and organizing items, improving the clarity and structure of item information. By incorporating the taxonomy dictionary into LLM prompts, we achieve efficient token utilization and controlled feature generation, leading to more accurate and contextually relevant recommendations. Our Taxonomy-guided Recommendation (TaxRec) approach features a two-step process: one-time taxonomy categorization and LLM-based recommendation, enabling zero-shot recommendations without the need for domain-specific fine-tuning. Experimental results demonstrate TaxRec significantly enhances recommendation quality compared to traditional zero-shot approaches, showcasing its efficacy as personal recommender with LLMs. Code is available at https://github.com/yueqingliang1/TaxRec.</article>","contentLength":1352,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Influence-Based Reward Modulation for Implicit Communication in Human-Robot Interaction","url":"https://arxiv.org/abs/2406.12253","date":1740114000,"author":"","guid":7795,"unread":true,"content":"<article>arXiv:2406.12253v2 Announce Type: replace \nAbstract: Communication is essential for successful interaction. In human-robot interaction, implicit communication holds the potential to enhance robots' understanding of human needs, emotions, and intentions. This paper introduces a method to foster implicit communication in HRI without explicitly modelling human intentions or relying on pre-existing knowledge. Leveraging Transfer Entropy, we modulate influence between agents in social interactions in scenarios involving either collaboration or competition. By integrating influence into agents' rewards within a partially observable Markov decision process, we demonstrate that boosting influence enhances collaboration, while resisting influence diminishes performance. Our findings are validated through simulations and real-world experiments with human participants in social navigation settings.</article>","contentLength":900,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models","url":"https://arxiv.org/abs/2406.11753","date":1740114000,"author":"","guid":7796,"unread":true,"content":"<article>arXiv:2406.11753v2 Announce Type: replace \nAbstract: Finetuning language models (LMs) is crucial for adapting the models to downstream data and tasks. However, full finetuning is usually costly. Existing work, such as parameter-efficient finetuning (PEFT), often focuses on \\textit{how to finetune} but neglects the issue of \\textit{where to finetune}. As a pioneering work on reducing the cost of backpropagation (at the layer level) by answering where to finetune, we conduct a semantic analysis of the LM inference process. We first propose using transition traces of the latent representation to compute deviations (or loss). Then, using a derived formula of scaling law, we estimate the gain of each layer in reducing deviation (or loss). Further, we narrow down the scope for finetuning, and also, study the cost-benefit balance of LM finetuning. We perform extensive experiments across well-known LMs and datasets. The results show that our approach is effective and efficient, and outperforms the existing baselines. Our approach is orthogonal to other techniques on improving finetuning efficiency, such as PEFT methods, offering practical values on LM finetuning.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DP-MemArc: Differential Privacy Transfer Learning for Memory Efficient Language Models","url":"https://arxiv.org/abs/2406.11087","date":1740114000,"author":"","guid":7797,"unread":true,"content":"<article>arXiv:2406.11087v5 Announce Type: replace \nAbstract: Large language models have repeatedly shown outstanding performance across diverse applications. However, deploying these models can inadvertently risk user privacy. The significant memory demands during training pose a major challenge in terms of resource consumption. This substantial size places a heavy load on memory resources, raising considerable practical concerns. In this paper, we introduce DP-MemArc, a novel training framework aimed at reducing the memory costs of large language models while emphasizing the protection of user data privacy. DP-MemArc incorporates side network or reversible network designs to support a variety of differential privacy memory-efficient fine-tuning schemes. Our approach not only achieves about 2.5 times in memory optimization but also ensures robust privacy protection, keeping user data secure and confidential. Extensive experiments have demonstrated that DP-MemArc effectively provides differential privacy-efficient fine-tuning across different task scenarios.</article>","contentLength":1065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Attribution for Text-to-Image Models by Unlearning Synthesized Images","url":"https://arxiv.org/abs/2406.09408","date":1740114000,"author":"","guid":7798,"unread":true,"content":"<article>arXiv:2406.09408v3 Announce Type: replace \nAbstract: The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. Influence is defined such that, for a given output, if a model is retrained from scratch without the most influential images, the model would fail to reproduce the same output. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining models from scratch. In our work, we propose an efficient data attribution method by simulating unlearning the synthesized image. We achieve this by increasing the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. We then identify training images with significant loss deviations after the unlearning process and label these as influential. We evaluate our method with a computationally intensive but \"gold-standard\" retraining from scratch and demonstrate our method's advantages over previous methods.</article>","contentLength":1059,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning","url":"https://arxiv.org/abs/2406.08973","date":1740114000,"author":"","guid":7799,"unread":true,"content":"<article>arXiv:2406.08973v2 Announce Type: replace \nAbstract: Following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforcement learning is experiencing a rapid growth. However, its development has been held back by the lack of challenging benchmarks, as all the experiments have been carried out in simple environments and on small-scale datasets. We present XLand-100B, a large-scale dataset for in-context reinforcement learning based on the XLand-MiniGrid environment, as a first step to alleviate this problem. It contains complete learning histories for nearly $30,000$ different tasks, covering $100$B transitions and 2.5B episodes. It took 50,000 GPU hours to collect the dataset, which is beyond the reach of most academic labs. Along with the dataset, we provide the utilities to reproduce or expand it even further. We also benchmark common in-context RL baselines and show that they struggle to generalize to novel and diverse tasks. With this substantial effort, we aim to democratize research in the rapidly growing field of in-context reinforcement learning and provide a solid foundation for further scaling.</article>","contentLength":1215,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Delving into ChatGPT usage in academic writing through excess vocabulary","url":"https://arxiv.org/abs/2406.07016","date":1740114000,"author":"","guid":7800,"unread":true,"content":"<article>arXiv:2406.07016v4 Announce Type: replace \nAbstract: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question, we present an unbiased, large-scale approach: we study vocabulary changes in 14 million PubMed abstracts from 2010--2024, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 10% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 30% for some sub-corpora. We show that LLMs have had an unprecedented impact on the scientific literature, surpassing the effect of major world events such as the Covid pandemic.</article>","contentLength":1017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Model Confidence Estimation via Black-Box Access","url":"https://arxiv.org/abs/2406.04370","date":1740114000,"author":"","guid":7801,"unread":true,"content":"<article>arXiv:2406.04370v3 Announce Type: replace \nAbstract: Estimating uncertainty or confidence in the responses of a model can be significant in evaluating trust not only in the responses, but also in the model as a whole. In this paper, we explore the problem of estimating confidence for responses of large language models (LLMs) with simply black-box or query access to them. We propose a simple and extensible framework where, we engineer novel features and train a (interpretable) model (viz. logistic regression) on these features to estimate the confidence. We empirically demonstrate that our simple framework is effective in estimating confidence of Flan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&amp;A tasks as well as of Pegasus-large and BART-large on two benchmark summarization tasks with it surpassing baselines by even over $10\\%$ (on AUROC) in some cases. Additionally, our interpretable approach provides insight into features that are predictive of confidence, leading to the interesting and useful discovery that our confidence models built for one LLM generalize zero-shot across others on a given dataset.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Text-to-Image Rectified Flow as Plug-and-Play Priors","url":"https://arxiv.org/abs/2406.03293","date":1740114000,"author":"","guid":7802,"unread":true,"content":"<article>arXiv:2406.03293v4 Announce Type: replace \nAbstract: Large-scale diffusion models have achieved remarkable performance in generative tasks. Beyond their initial training applications, these models have proven their ability to function as versatile plug-and-play priors. For instance, 2D diffusion models can serve as loss functions to optimize 3D implicit models. Rectified flow, a novel class of generative models, enforces a linear progression from the source to the target distribution and has demonstrated superior performance across various domains. Compared to diffusion-based methods, rectified flow approaches surpass in terms of generation quality and efficiency, requiring fewer inference steps. In this work, we present theoretical and experimental evidence demonstrating that rectified flow based methods offer similar functionalities to diffusion models - they can also serve as effective priors. Besides the generative capabilities of diffusion priors, motivated by the unique time-symmetry properties of rectified flow models, a variant of our method can additionally perform image inversion. Experimentally, our rectified flow-based priors outperform their diffusion counterparts - the SDS and VSD losses - in text-to-3D generation. Our method also displays competitive performance in image inversion and editing.</article>","contentLength":1329,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deterministic Reversible Data Augmentation for Neural Machine Translation","url":"https://arxiv.org/abs/2406.02517","date":1740114000,"author":"","guid":7803,"unread":true,"content":"<article>arXiv:2406.02517v2 Announce Type: replace \nAbstract: Data augmentation is an effective way to diversify corpora in machine translation, but previous methods may introduce semantic inconsistency between original and augmented data because of irreversible operations and random subword sampling procedures. To generate both symbolically diverse and semantically consistent augmentation data, we propose Deterministic Reversible Data Augmentation (DRDA), a simple but effective data augmentation method for neural machine translation. DRDA adopts deterministic segmentations and reversible operations to generate multi-granularity subword representations and pulls them closer together with multi-view techniques. With no extra corpora or model changes required, DRDA outperforms strong baselines on several translation tasks with a clear margin (up to 4.3 BLEU gain over Transformer) and exhibits good robustness in noisy, low-resource, and cross-domain datasets.</article>","contentLength":961,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Open-Source Tool for Mapping War Destruction at Scale in Ukraine using Sentinel-1 Time Series","url":"https://arxiv.org/abs/2406.02506","date":1740114000,"author":"","guid":7804,"unread":true,"content":"<article>arXiv:2406.02506v3 Announce Type: replace \nAbstract: Access to detailed war impact assessments is crucial for humanitarian organizations to assist affected populations effectively. However, maintaining a comprehensive understanding of the situation on the ground is challenging, especially in widespread and prolonged conflicts. Here we present a scalable method for estimating building damage resulting from armed conflicts. By training a machine learning model on Synthetic Aperture Radar image time series, we generate probabilistic damage estimates at the building level, leveraging existing damage assessments and open building footprints. To allow large-scale inference and ensure accessibility, we tie our method to run on Google Earth Engine. Users can adjust confidence intervals to suit their needs, enabling rapid and flexible assessments of war-related damage across large areas. We provide two publicly accessible dashboards: a Ukraine Damage Explorer to dynamically view our precomputed estimates, and a Rapid Damage Mapping Tool to run our method and generate custom maps.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions?","url":"https://arxiv.org/abs/2405.17719","date":1740114000,"author":"","guid":7805,"unread":true,"content":"<article>arXiv:2405.17719v3 Announce Type: replace \nAbstract: Egocentric video-language pretraining is a crucial step in advancing the understanding of hand-object interactions in first-person scenarios. Despite successes on existing testbeds, we find that current EgoVLMs can be easily misled by simple modifications, such as changing the verbs or nouns in interaction descriptions, with models struggling to distinguish between these changes. This raises the question: Do EgoVLMs truly understand hand-object interactions? To address this question, we introduce a benchmark called EgoHOIBench, revealing the performance limitation of current egocentric models when confronted with such challenges. We attribute this performance gap to insufficient fine-grained supervision and the greater difficulty EgoVLMs experience in recognizing verbs compared to nouns. To tackle these issues, we propose a novel asymmetric contrastive objective named EgoNCE++. For the video-to-text objective, we enhance text supervision by generating negative captions using large language models or leveraging pretrained vocabulary for HOI-related word substitutions. For the text-to-video objective, we focus on preserving an object-centric feature space that clusters video representations based on shared nouns. Extensive experiments demonstrate that EgoNCE++ significantly enhances EgoHOI understanding, leading to improved performance across various EgoVLMs in tasks such as multi-instance retrieval, action recognition, and temporal understanding. Our code is available at https://github.com/xuboshen/EgoNCEpp.</article>","contentLength":1585,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding Maximum Common Contractions Between Phylogenetic Networks","url":"https://arxiv.org/abs/2405.16713","date":1740114000,"author":"","guid":7806,"unread":true,"content":"<article>arXiv:2405.16713v3 Announce Type: replace \nAbstract: In this paper, we lay the groundwork on the comparison of phylogenetic networks based on edge contractions and expansions as edit operations, as originally proposed by Robinson and Foulds to compare trees. We prove that these operations connect the space of all phylogenetic networks on the same set of leaves, even if we forbid contractions that create cycles. This allows to define an operational distance on this space, as the minimum number of contractions and expansions required to transform one network into another. We highlight the difference between this distance and the computation of the maximum common contraction between two networks. Given its ability to outline a common structure between them, which can provide valuable biological insights, we study the algorithmic aspects of the latter. We first prove that computing a maximum common contraction between two networks is NP-hard, even when the maximum degree, the size of the common contraction, or the number of leaves is bounded. We also provide lower bounds to the problem based on the Exponential-Time Hypothesis. Nonetheless, we do provide a polynomial-time algorithm for weakly-galled trees, a generalization of galled trees.</article>","contentLength":1254,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation","url":"https://arxiv.org/abs/2405.16545","date":1740114000,"author":"","guid":7807,"unread":true,"content":"<article>arXiv:2405.16545v2 Announce Type: replace \nAbstract: We study reward models for long-horizon manipulation tasks by learning from action-free videos and language instructions, which we term the visual-instruction correlation (VIC) problem. Recent advancements in cross-modality modeling have highlighted the potential of reward modeling through visual and language correlations. However, existing VIC methods face challenges in learning rewards for long-horizon tasks due to their lack of sub-stage awareness, difficulty in modeling task complexities, and inadequate object state estimation. To address these challenges, we introduce VICtoR, a novel hierarchical VIC reward model capable of providing effective reward signals for long-horizon manipulation tasks. VICtoR precisely assesses task progress at various levels through a novel stage detector and motion progress evaluator, offering insightful guidance for agents learning the task effectively. To validate the effectiveness of VICtoR, we conducted extensive experiments in both simulated and real-world environments. The results suggest that VICtoR outperformed the best existing VIC methods, achieving a 43% improvement in success rates for long-horizon tasks.</article>","contentLength":1220,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SpinQuant: LLM quantization with learned rotations","url":"https://arxiv.org/abs/2405.16406","date":1740114000,"author":"","guid":7808,"unread":true,"content":"<article>arXiv:2405.16406v4 Announce Type: replace \nAbstract: Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot. Code is available at https://github.com/facebookresearch/SpinQuant.</article>","contentLength":1485,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LOVA3: Learning to Visual Question Answering, Asking and Assessment","url":"https://arxiv.org/abs/2405.14974","date":1740114000,"author":"","guid":7809,"unread":true,"content":"<article>arXiv:2405.14974v3 Announce Type: replace \nAbstract: Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading to better comprehension and learning outcomes. Current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills. Inspired by the human learning mechanism, we introduce LOVA3, an innovative framework named \"Learning tO Visual question Answering, Asking and Assessment,\" designed to equip MLLMs with these additional capabilities. Our approach involves the creation of two supplementary training tasks GenQA and EvalQA, aiming at fostering the skills of asking and assessing questions in the context of images. To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. For assessment, we introduce a new benchmark called EvalQABench, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 validation and testing samples. We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions will enhance their multimodal comprehension, ultimately improving overall performance. To validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate them on a range of multimodal datasets and benchmarks. Our results demonstrate consistent performance gains, underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs. The code is available at https://github.com/showlab/LOVA3.</article>","contentLength":1704,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficiently Training Deep-Learning Parametric Policies using Lagrangian Duality","url":"https://arxiv.org/abs/2405.14973","date":1740114000,"author":"","guid":7810,"unread":true,"content":"<article>arXiv:2405.14973v2 Announce Type: replace \nAbstract: Constrained Markov Decision Processes (CMDPs) are critical in many high-stakes applications, where decisions must optimize cumulative rewards while strictly adhering to complex nonlinear constraints. In domains such as power systems, finance, supply chains, and precision robotics, violating these constraints can result in significant financial or societal costs. Existing Reinforcement Learning (RL) methods often struggle with sample efficiency and effectiveness in finding feasible policies for highly and strictly constrained CMDPs, limiting their applicability in these environments. Stochastic dual dynamic programming is often used in practice on convex relaxations of the original problem, but they also encounter computational challenges and loss of optimality. This paper introduces a novel approach, Two-Stage Deep Decision Rules (TS-DDR), to efficiently train parametric actor policies using Lagrangian Duality. TS-DDR is a self-supervised learning algorithm that trains general decision rules (parametric policies) using stochastic gradient descent (SGD); its forward passes solve {\\em deterministic} optimization problems to find feasible policies, and its backward passes leverage duality theory to train the parametric policy with closed-form gradients. TS-DDR inherits the flexibility and computational performance of deep learning methodologies to solve CMDP problems. Applied to the Long-Term Hydrothermal Dispatch (LTHD) problem using actual power system data from Bolivia, TS-DDR is shown to enhance solution quality and to reduce computation times by several orders of magnitude when compared to current state-of-the-art methods.</article>","contentLength":1705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fast Bayesian Inference for Neutrino Non-Standard Interactions at Dark Matter Direct Detection Experiments","url":"https://arxiv.org/abs/2405.14932","date":1740114000,"author":"","guid":7811,"unread":true,"content":"<article>arXiv:2405.14932v2 Announce Type: replace \nAbstract: Multi-dimensional parameter spaces are commonly encountered in physics theories that go beyond the Standard Model. However, they often possess complicated posterior geometries that are expensive to traverse using techniques traditional to astroparticle physics. Several recent innovations, which are only beginning to make their way into this field, have made navigating such complex posteriors possible. These include GPU acceleration, automatic differentiation, and neural-network-guided reparameterization. We apply these advancements to dark matter direct detection experiments in the context of non-standard neutrino interactions and benchmark their performances against traditional nested sampling techniques when conducting Bayesian inference. Compared to nested sampling alone, we find that these techniques increase performance for both nested sampling and Hamiltonian Monte Carlo, accelerating inference by factors of $\\sim 100$ and $\\sim 60$, respectively. As nested sampling also evaluates the Bayesian evidence, these advancements can be exploited to improve model comparison performance while retaining compatibility with existing implementations that are widely used in the natural sciences. Using these techniques, we perform the first scan in the neutrino non-standard interactions parameter space for direct detection experiments whereby all parameters are allowed to vary simultaneously. We expect that these advancements are broadly applicable to other areas of astroparticle physics featuring multi-dimensional parameter spaces.</article>","contentLength":1602,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Traffic Scenario Logic: A Spatial-Temporal Logic for Modeling and Reasoning of Urban Traffic Scenarios","url":"https://arxiv.org/abs/2405.13715","date":1740114000,"author":"","guid":7812,"unread":true,"content":"<article>arXiv:2405.13715v3 Announce Type: replace \nAbstract: Formal representations of traffic scenarios can be used to generate test cases for the safety verification of autonomous driving. However, most existing methods are limited to highway or highly simplified intersection scenarios due to the intricacy and diversity of traffic scenarios. In response, we propose Traffic Scenario Logic (TSL), which is a spatial-temporal logic designed for modeling and reasoning of urban pedestrian-free traffic scenarios. TSL provides a formal representation of the urban road network that can be derived from OpenDRIVE, i.e., the de facto industry standard of high-definition maps for autonomous driving, enabling the representation of a broad range of traffic scenarios without discretization approximations. We implemented the reasoning of TSL using Telingo, i.e., a solver for temporal programs based on Answer Set Programming, and tested it on different urban road layouts. Demonstrations show the effectiveness of TSL in test scenario generation and its potential value in areas like decision-making and control verification of autonomous driving. The code for TSL reasoning has been open-sourced.</article>","contentLength":1187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Efficient Learning Control Framework With Sim-to-Real for String-Type Artificial Muscle-Driven Robotic Systems","url":"https://arxiv.org/abs/2405.10576","date":1740114000,"author":"","guid":7813,"unread":true,"content":"<article>arXiv:2405.10576v3 Announce Type: replace \nAbstract: Robotic systems driven by artificial muscles present unique challenges due to the nonlinear dynamics of actuators and the complex designs of mechanical structures. Traditional model-based controllers often struggle to achieve desired control performance in such systems. Deep reinforcement learning (DRL), a trending machine learning technique widely adopted in robot control, offers a promising alternative. However, integrating DRL into these robotic systems faces significant challenges, including the requirement for large amounts of training data and the inevitable sim-to-real gap when deployed to real-world robots. This paper proposes an efficient reinforcement learning control framework with sim-to-real transfer to address these challenges. Bootstrap and augmentation enhancements are designed to improve the data efficiency of baseline DRL algorithms, while a sim-to-real transfer technique, namely randomization of muscle dynamics, is adopted to bridge the gap between simulation and real-world deployment. Extensive experiments and ablation studies are conducted utilizing two string-type artificial muscle-driven robotic systems including a two degree-of-freedom robotic eye and a parallel robotic wrist, the results of which demonstrate the effectiveness of the proposed learning control strategy.</article>","contentLength":1366,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"R3MG: R-tree based agglomeration of polytopal grids with applications to multilevel methods","url":"https://arxiv.org/abs/2404.18505","date":1740114000,"author":"","guid":7814,"unread":true,"content":"<article>arXiv:2404.18505v4 Announce Type: replace \nAbstract: We present a novel approach to perform agglomeration of polygonal and polyhedral grids based on spatial indices. Agglomeration strategies are a key ingredient in polytopal methods for PDEs as they are used to generate (hierarchies of) computational grids from an initial grid. Spatial indices are specialized data structures that significantly accelerate queries involving spatial relationships in arbitrary space dimensions. We show how the construction of the R-tree spatial database of an arbitrary fine grid offers a natural and efficient agglomeration strategy with the following characteristics: i) the process is fully automated, robust, and dimension-independent, ii) it automatically produces a balanced and nested hierarchy of agglomerates, and iii) the shape of the agglomerates is tightly close to the respective axis aligned bounding boxes. Moreover, the R-tree approach provides a full hierarchy of nested agglomerates which permits fast query and allows for efficient geometric multigrid methods to be applied also to those cases where a hierarchy of grids is not present at construction time. We present several examples based on polygonal discontinuous Galerkin methods, confirming the effectiveness of our approach in the context of challenging three-dimensional geometries and the design of geometric multigrid preconditioners.</article>","contentLength":1399,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Adversarial Robustness of Vision-Language Models through Low-Rank Adaptation","url":"https://arxiv.org/abs/2404.13425","date":1740114000,"author":"","guid":7815,"unread":true,"content":"<article>arXiv:2404.13425v3 Announce Type: replace \nAbstract: Vision-Language Models (VLMs) play a crucial role in the advancement of Artificial General Intelligence (AGI). As AGI rapidly evolves, addressing security concerns has emerged as one of the most significant challenges for VLMs. In this paper, we present extensive experiments that expose the vulnerabilities of conventional adaptation methods for VLMs, highlighting significant security risks. Moreover, as VLMs grow in size, the application of traditional adversarial adaptation techniques incurs substantial computational costs. To address these issues, we propose a parameter-efficient adversarial adaptation method called \\textbf{\\textit{AdvLoRA}} based on Low-Rank Adaptation. We investigate and reveal the inherent low-rank properties involved in adversarial adaptation for VLMs. Different from LoRA, we enhance the efficiency and robustness of adversarial adaptation by introducing a novel reparameterization method that leverages parameter clustering and alignment. Additionally, we propose an adaptive parameter update strategy to further bolster robustness. These innovations enable our AdvLoRA to mitigate issues related to model security and resource wastage. Extensive experiments confirm the effectiveness and efficiency of AdvLoRA.</article>","contentLength":1299,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measuring the Quality of Answers in Political Q&As with Large Language Models","url":"https://arxiv.org/abs/2404.08816","date":1740114000,"author":"","guid":7816,"unread":true,"content":"<article>arXiv:2404.08816v5 Announce Type: replace \nAbstract: This article proposes a new approach for assessing the quality of answers in political question-and-answer sessions. We measure the quality of an answer based on how easily and accurately it can be recognized in a random set of candidate answers given the question's text. This measure reflects the answer's relevance and depth of engagement with the question. Like semantic search, we can implement this approach by training a language model on the corpus of observed questions and answers without additional human-labeled data. We showcase and validate our methodology within the context of the Question Period in the Canadian House of Commons. Our analysis reveals that while some answers have a weak semantic connection to questions, hinting at some evasion or obfuscation, they are generally at least moderately relevant, far exceeding what we would expect from random replies. We also find a meaningful correlation between answer quality and the party affiliation of the members of Parliament asking the questions.</article>","contentLength":1073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-scale Topology Optimization using Neural Networks","url":"https://arxiv.org/abs/2404.08708","date":1740114000,"author":"","guid":7817,"unread":true,"content":"<article>arXiv:2404.08708v2 Announce Type: replace \nAbstract: A long-standing challenge is designing multi-scale structures with good connectivity between cells while optimizing each cell to reach close to the theoretical performance limit. We propose a new method for direct multi-scale topology optimization using neural networks. Our approach focuses on inverse homogenization that seamlessly maintains compatibility across neighboring microstructure cells. Our approach consists of a topology neural network that optimizes the microstructure shape and distribution across the design domain as a continuous field. Each microstructure cell is optimized based on a specified elasticity tensor that also accommodates in-plane rotations. The neural network takes as input the local coordinates within a cell to represent the density distribution within a cell, as well as the global coordinates of each cell to design spatially varying microstructure cells. As such, our approach models an n-dimensional multi-scale optimization problem as a 2n-dimensional inverse homogenization problem using neural networks. During the inverse homogenization of each unit cell, we extend the boundary of each cell by scaling the input coordinates such that the boundaries of neighboring cells are combined. Inverse homogenization on the combined cell improves connectivity. We demonstrate our method through the design and optimization of graded multi-scale structures.</article>","contentLength":1445,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering","url":"https://arxiv.org/abs/2404.08449","date":1740114000,"author":"","guid":7818,"unread":true,"content":"<article>arXiv:2404.08449v3 Announce Type: replace \nAbstract: Rendering dynamic 3D human from monocular videos is crucial for various applications such as virtual reality and digital entertainment. Most methods assume the people is in an unobstructed scene, while various objects may cause the occlusion of body parts in real-life scenarios. Previous method utilizing NeRF for surface rendering to recover the occluded areas, but it requiring more than one day to train and several seconds to render, failing to meet the requirements of real-time interactive applications. To address these issues, we propose OccGaussian based on 3D Gaussian Splatting, which can be trained within 6 minutes and produces high-quality human renderings up to 160 FPS with occluded input. OccGaussian initializes 3D Gaussian distributions in the canonical space, and we perform occlusion feature query at occluded regions, the aggregated pixel-align feature is extracted to compensate for the missing information. Then we use Gaussian Feature MLP to further process the feature along with the occlusion-aware loss functions to better perceive the occluded area. Extensive experiments both in simulated and real-world occlusions, demonstrate that our method achieves comparable or even superior performance compared to the state-of-the-art method. And we improving training and inference speeds by 250x and 800x, respectively. Our code will be available for research purposes.</article>","contentLength":1446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lexical Complexity Prediction and Lexical Simplification for Catalan and Spanish: Resource Creation, Quality Assessment, and Ethical Considerations","url":"https://arxiv.org/abs/2404.07814","date":1740114000,"author":"","guid":7819,"unread":true,"content":"<article>arXiv:2404.07814v2 Announce Type: replace \nAbstract: Automatic lexical simplification is a task to substitute lexical items that may be unfamiliar and difficult to understand with easier and more common words. This paper presents the description and analysis of two novel datasets for lexical simplification in Spanish and Catalan. This dataset represents the first of its kind in Catalan and a substantial addition to the sparse data on automatic lexical simplification which is available for Spanish. Specifically, it is the first dataset for Spanish which includes scalar ratings of the understanding difficulty of lexical items. In addition, we present a detailed analysis aiming at assessing the appropriateness and ethical dimensions of the data for the lexical simplification task.</article>","contentLength":788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Concept Erasure Using Task Vectors","url":"https://arxiv.org/abs/2404.03631","date":1740114000,"author":"","guid":7820,"unread":true,"content":"<article>arXiv:2404.03631v2 Announce Type: replace \nAbstract: With the rapid growth of text-to-image models, a variety of techniques have been suggested to prevent undesirable image generations. Yet, these methods often only protect against specific user prompts and have been shown to allow unsafe generations with other inputs. Here we focus on unconditionally erasing a concept from a text-to-image model rather than conditioning the erasure on the user's prompt. We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training. However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown. To this end, we propose a method called Diverse Inversion, which we use to estimate the required strength of the TV edit. Diverse Inversion finds within the model input space a large set of word embeddings, each of which induces the generation of the target concept. We find that encouraging diversity in the set makes our estimation more robust to unexpected prompts. Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining the core functionality of the model.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Writing with AI Lowers Psychological Ownership, but Longer Prompts Can Help","url":"https://arxiv.org/abs/2404.03108","date":1740114000,"author":"","guid":7821,"unread":true,"content":"<article>arXiv:2404.03108v3 Announce Type: replace \nAbstract: The feeling of something belonging to someone is called \"psychological ownership.\" A common assumption is that writing with generative AI lowers psychological ownership, but the extent to which this occurs and the role of prompt length are unclear. We report on two experiments to examine the relationship between psychological ownership and prompt length. Participants wrote short stories either completely by themselves or wrote prompts of varying lengths. Results show that when participants wrote longer prompts, they had higher levels of psychological ownership. Their comments suggest they thought more about their prompts, often adding more details about the plot. However, benefits plateaued when prompt length was 75-100% of the target story length. To encourage users to write longer prompts, we propose augmenting the prompt submission button so it must be held down a long time if the prompt is short. Results show that this technique is effective at increasing prompt length.</article>","contentLength":1041,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Techniques for Measuring the Inferential Strength of Forgetting Policies","url":"https://arxiv.org/abs/2404.02454","date":1740114000,"author":"","guid":7822,"unread":true,"content":"<article>arXiv:2404.02454v4 Announce Type: replace \nAbstract: The technique of forgetting in knowledge representation has been shown to be a powerful and useful knowledge engineering tool with widespread application. Yet, very little research has been done on how different policies of forgetting, or use of different forgetting operators, affects the inferential strength of the original theory. The goal of this paper is to define loss functions for measuring changes in inferential strength based on intuitions from model counting and probability theory. Properties of such loss measures are studied and a pragmatic knowledge engineering tool is proposed for computing loss measures using ProbLog. The paper includes a working methodology for studying and determining the strength of different forgetting policies, in addition to concrete examples showing how to apply the theoretical results using ProbLog. Although the focus is on forgetting, the results are much more general and should have wider application to other areas.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning-based Receding Horizon Control using Adaptive Control Barrier Functions for Safety-Critical Systems","url":"https://arxiv.org/abs/2403.17338","date":1740114000,"author":"","guid":7823,"unread":true,"content":"<article>arXiv:2403.17338v3 Announce Type: replace \nAbstract: Optimal control methods provide solutions to safety-critical problems but easily become intractable. Control Barrier Functions (CBFs) have emerged as a popular technique that facilitates their solution by provably guaranteeing safety, through their forward invariance property, at the expense of some performance loss. This approach involves defining a performance objective alongside CBF-based safety constraints that must always be enforced. Unfortunately, both performance and solution feasibility can be significantly impacted by two key factors: (i) the selection of the cost function and associated parameters, and (ii) the calibration of parameters within the CBF-based constraints, which capture the trade-off between performance and conservativeness. %as well as infeasibility. To address these challenges, we propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC) approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In particular, we parameterize our controller and use bilevel optimization, where RL is used to learn the optimal parameters while MPC computes the optimal control input. We validate our method by applying it to the challenging automated merging control problem for Connected and Automated Vehicles (CAVs) at conflicting roadways. Results demonstrate improved performance and a significant reduction in the number of infeasible cases compared to traditional heuristic approaches used for tuning CBF-based controllers, showcasing the effectiveness of the proposed method.</article>","contentLength":1589,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Contact Inertial Parameters Estimation and Localization in Legged Robots","url":"https://arxiv.org/abs/2403.17161","date":1740114000,"author":"","guid":7824,"unread":true,"content":"<article>arXiv:2403.17161v2 Announce Type: replace \nAbstract: Optimal estimation is a promising tool for estimation of payloads' inertial parameters and localization of robots in the presence of multiple contacts. To harness its advantages in robotics, it is crucial to solve these large and challenging optimization problems efficiently. To tackle this, we (i) develop a multiple shooting solver that exploits both temporal and parametric structures through a parametrized Riccati recursion. Additionally, we (ii) propose an inertial manifold that ensures the full physical consistency of inertial parameters and enhances convergence. To handle its manifold singularities, we (iii) introduce a nullspace approach in our optimal estimation solver. Finally, we (iv) develop the analytical derivatives of contact dynamics for both inertial parametrizations. Our framework can successfully solve estimation problems for complex maneuvers such as brachiation in humanoids, achieving higher accuracy than conventional least squares approaches. We demonstrate its numerical capabilities across various robotics tasks and its benefits in experimental trials with the Go1 robot.</article>","contentLength":1161,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It","url":"https://arxiv.org/abs/2403.14715","date":1740114000,"author":"","guid":7825,"unread":true,"content":"<article>arXiv:2403.14715v3 Announce Type: replace \nAbstract: Label smoothing (LS) is a popular regularisation method for training neural networks as it is effective in improving test accuracy and is simple to implement. ``Hard'' one-hot labels are ``smoothed'' by uniformly distributing probability mass to other classes, reducing overfitting. Prior work has suggested that in some cases LS can degrade selective classification (SC) -- where the aim is to reject misclassifications using a model's uncertainty. In this work, we first demonstrate empirically across an extended range of large-scale tasks and architectures that LS consistently degrades SC. We then address a gap in existing knowledge, providing an explanation for this behaviour by analysing logit-level gradients: LS degrades the uncertainty rank ordering of correct vs incorrect predictions by suppressing the max logit more when a prediction is likely to be correct, and less when it is likely to be wrong. This elucidates previously reported experimental results where strong classifiers underperform in SC. We then demonstrate the empirical effectiveness of post-hoc logit normalisation for recovering lost SC performance caused by LS. Furthermore, linking back to our gradient analysis, we again provide an explanation for why such normalisation is effective.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dimensionality-Reduction Techniques for Approximate Nearest Neighbor Search: A Survey and Evaluation","url":"https://arxiv.org/abs/2403.13491","date":1740114000,"author":"","guid":7826,"unread":true,"content":"<article>arXiv:2403.13491v2 Announce Type: replace \nAbstract: Approximate Nearest Neighbor Search (ANNS) on high-dimensional vectors has become a fundamental and essential component in various machine learning tasks. Recently, with the rapid development of deep learning models and the applications of Large Language Models (LLMs), the dimensionality of the vectors keeps growing in order to accommodate a richer semantic representation. This poses a major challenge to the ANNS solutions since distance calculation cost in ANNS grows linearly with the dimensionality of vectors. To overcome this challenge, dimensionality-reduction techniques can be leveraged to accelerate the distance calculation in the search process. In this paper, we investigate six dimensionality-reduction techniques that have the potential to improve ANNS solutions, including classical algorithms such as PCA and vector quantization, as well as algorithms based on deep learning approaches. We further describe two frameworks to apply these techniques in the ANNS workflow, and theoretically analyze the time and space costs, as well as the beneficial threshold for the pruning ratio of these techniques. The surveyed techniques are evaluated on six public datasets. The analysis of the results reveals the characteristics of the different families of techniques and provides insights into the promising future research directions.</article>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Gradient Influencing for Viral Marketing Using Graph Neural Networks","url":"https://arxiv.org/abs/2403.12399","date":1740114000,"author":"","guid":7827,"unread":true,"content":"<article>arXiv:2403.12399v2 Announce Type: replace \nAbstract: The problem of maximizing the adoption of a product through viral marketing in social networks has been studied heavily through postulated network models. We present a novel data-driven formulation of the problem. We use Graph Neural Networks (GNNs) to model the adoption of products by utilizing both topological and attribute information. The resulting Dynamic Viral Marketing (DVM) problem seeks to find the minimum budget and minimal set of dynamic topological and attribute changes in order to attain a specified adoption goal. We show that DVM is NP-Hard and is related to the existing influence maximization problem. Motivated by this connection, we develop the idea of Dynamic Gradient Influencing (DGI) that uses gradient ranking to find optimal perturbations and targets low-budget and high influence non-adopters in discrete steps. We use an efficient strategy for computing node budgets and develop the ''Meta-Influence'' heuristic for assessing a node's downstream influence. We evaluate DGI against multiple baselines and demonstrate gains on average of 24% on budget and 37% on AUC on real-world attributed networks. Our code is publicly available at https://github.com/saurabhsharma1993/dynamic_viral_marketing.</article>","contentLength":1280,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empirical Game-Theoretic Analysis: A Survey","url":"https://arxiv.org/abs/2403.04018","date":1740114000,"author":"","guid":7828,"unread":true,"content":"<article>arXiv:2403.04018v2 Announce Type: replace \nAbstract: In the empirical approach to game-theoretic analysis (EGTA), the model of the game comes not from declarative representation, but is derived by interrogation of a procedural description of the game environment. The motivation for developing this approach was to enable game-theoretic reasoning about strategic situations too complex for analytic specification and solution. Since its introduction over twenty years ago, EGTA has been applied to a wide range of multiagent domains, from auctions and markets to recreational games to cyber-security. We survey the extensive methodology developed for EGTA over the years, organized by the elemental subproblems comprising the EGTA process. We describe key EGTA concepts and techniques, and the questions at the frontier of EGTA research. Recent advances in machine learning have accelerated progress in EGTA, and promise to significantly expand our capacities for reasoning about complex game situations.</article>","contentLength":1004,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RhythmFormer: Extracting Patterned rPPG Signals based on Periodic Sparse Attention","url":"https://arxiv.org/abs/2402.12788","date":1740114000,"author":"","guid":7829,"unread":true,"content":"<article>arXiv:2402.12788v3 Announce Type: replace \nAbstract: Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications. Due to the periodicity nature of rPPG signals, the long-range dependency capturing capacity of the transformer was assumed to be advantageous for such signals. However, existing methods have not conclusively demonstrated the superior performance of transformers over traditional convolutional neural networks. This may be attributed to the quadratic scaling exhibited by transformer with sequence length, resulting in coarse-grained feature extraction, which in turn affects robustness and generalization. To address that, this paper proposes a periodic sparse attention mechanism based on temporal attention sparsity induced by periodicity. A pre-attention stage is introduced before the conventional attention mechanism. This stage learns periodic patterns to filter out a large number of irrelevant attention computations, thus enabling fine-grained feature extraction. Moreover, to address the issue of fine-grained features being more susceptible to noise interference, a fusion stem is proposed to effectively guide self-attention towards rPPG features. It can be easily integrated into existing methods to enhance their performance. Extensive experiments show that the proposed method achieves state-of-the-art performance in both intra-dataset and cross-dataset evaluations. The codes are available at https://github.com/zizheng-guo/RhythmFormer.</article>","contentLength":1574,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Assessing the Reasoning Capabilities of LLMs in the context of Evidence-based Claim Verification","url":"https://arxiv.org/abs/2402.10735","date":1740114000,"author":"","guid":7830,"unread":true,"content":"<article>arXiv:2402.10735v3 Announce Type: replace \nAbstract: Although LLMs have shown great performance on Mathematics and Coding related reasoning tasks, the reasoning capabilities of LLMs regarding other forms of reasoning are still an open problem. Here, we examine the issue of reasoning from the perspective of claim verification. We propose a framework designed to break down any claim paired with evidence into atomic reasoning types that are necessary for verification. We use this framework to create Reasoning in Evidence-based Claim Verification (RECV), the first claim verification benchmark, incorporating real-world claims, to assess the deductive and abductive reasoning capabilities of LLMs. The benchmark comprises of three datasets, covering reasoning problems of increasing complexity. We evaluate three state-of-the-art proprietary LLMs under multiple prompt settings. Our results show that while LLMs can address deductive reasoning problems, they consistently fail in cases of abductive reasoning. Moreover, we observe that enhancing LLMs with rationale generation is not always beneficial. Nonetheless, we find that generated rationales are semantically similar to those provided by humans, especially in deductive reasoning cases.</article>","contentLength":1246,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Planning Domain Model Acquisition from State Traces without Action Parameters","url":"https://arxiv.org/abs/2402.10726","date":1740114000,"author":"","guid":7831,"unread":true,"content":"<article>arXiv:2402.10726v3 Announce Type: replace \nAbstract: Previous STRIPS domain model acquisition approaches that learn from state traces start with the names and parameters of the actions to be learned. Therefore their only task is to deduce the preconditions and effects of the given actions. In this work, we explore learning in situations when the parameters of learned actions are not provided. We define two levels of trace quality based on which information is provided and present an algorithm for each. In one level (L1), the states in the traces are labeled with action names, so we can deduce the number and names of the actions, but we still need to work out the number and types of parameters. In the other level (L2), the states are additionally labeled with objects that constitute the parameters of the corresponding grounded actions. Here we still need to deduce the types of the parameters in the learned actions. We experimentally evaluate the proposed algorithms and compare them with the state-of-the-art learning tool FAMA on a large collection of IPC benchmarks. The evaluation shows that our new algorithms are faster, can handle larger inputs and provide better results in terms of learning action models more similar to reference models.</article>","contentLength":1259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Confidence-aware Fine-tuning of Sequential Recommendation Systems via Conformal Prediction","url":"https://arxiv.org/abs/2402.08976","date":1740114000,"author":"","guid":7832,"unread":true,"content":"<article>arXiv:2402.08976v3 Announce Type: replace \nAbstract: In Sequential Recommendation Systems (SRecsys), traditional training approaches that rely on Cross-Entropy (CE) loss often prioritize accuracy but fail to align well with user satisfaction metrics. CE loss focuses on maximizing the confidence of the ground truth item, which is challenging to achieve universally across all users and sessions. It also overlooks the practical acceptability of ranking the ground truth item within the top-$K$ positions, a common metric in SRecsys. To address this limitation, we propose \\textbf{CPFT}, a novel fine-tuning framework that integrates Conformal Prediction (CP)-based losses with CE loss to optimize accuracy alongside confidence that better aligns with widely used top-$K$ metrics. CPFT embeds CP principles into the training loop using differentiable proxy losses and computationally efficient calibration strategies, enabling the generation of high-confidence prediction sets. These sets focus on items with high relevance while maintaining robust coverage guarantees. Extensive experiments on five real-world datasets and four distinct sequential models demonstrate that CPFT improves precision metrics and confidence calibration. Our results highlight the importance of confidence-aware fine-tuning in delivering accurate, trustworthy recommendations that enhance user satisfaction.</article>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scheduling With Time Discounts","url":"https://arxiv.org/abs/2402.08549","date":1740114000,"author":"","guid":7833,"unread":true,"content":"<article>arXiv:2402.08549v2 Announce Type: replace \nAbstract: We study a \\emph{financial} version of the classic online problem of scheduling weighted packets with deadlines. The main novelty is that, while previous works assume packets have \\emph{fixed} weights throughout their lifetime, this work considers packets with \\emph{time-decaying} values. Such considerations naturally arise and have wide applications in financial environments, where the present value of future actions may be discounted. We analyze the competitive ratio guarantees of scheduling algorithms under a range of discount rates encompassing the ``traditional'' undiscounted case where weights are fixed (i.e., a discount rate of 1), the fully discounted ``myopic'' case (i.e., a rate of 0), and those in between. We show how existing methods from the literature perform suboptimally in the more general discounted setting. Notably, we devise a novel memoryless deterministic algorithm, and prove that it guarantees the best possible competitive ratio attainable by deterministic algorithms for discount factors up to $\\approx 0.77$. Moreover, we develop a randomized algorithm and prove that it outperforms the best possible deterministic algorithm, for any discount rate. While we highlight the relevance of our framework and results to blockchain transaction scheduling in particular, our approach and analysis techniques are general and may be of independent interest.</article>","contentLength":1438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Counterfactual Concept Bottleneck Models","url":"https://arxiv.org/abs/2402.01408","date":1740114000,"author":"","guid":7834,"unread":true,"content":"<article>arXiv:2402.01408v3 Announce Type: replace \nAbstract: Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the \"What?\"), simulate changes in the situation to evaluate how this impacts class predictions (the \"How?\"), and imagine how the scenario should change to result in different class predictions (the \"Why not?\"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and improving human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our experimental results demonstrate that CF-CBMs: achieve classification accuracy comparable to black-box models and existing CBMs (\"What?\"), rely on fewer important concepts leading to simpler explanations (\"How?\"), and produce interpretable, concept-based counterfactuals (\"Why not?\"). Additionally, we show that training the counterfactual generator jointly with the CBM leads to two key improvements: (i) it alters the model's decision-making process, making the model rely on fewer important concepts (leading to simpler explanations), and (ii) it significantly increases the causal effect of concept interventions on class predictions, making the model more responsive to these changes.</article>","contentLength":1489,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Producers Equilibria and Dynamics in Engagement-Driven Recommender Systems","url":"https://arxiv.org/abs/2401.16641","date":1740114000,"author":"","guid":7835,"unread":true,"content":"<article>arXiv:2401.16641v3 Announce Type: replace \nAbstract: Online platforms such as YouTube, Instagram heavily rely on recommender systems to decide what content to present to users. Producers, in turn, often create content that is likely to be recommended to users and have users engage with it. To do so, producers try to align their content with the preferences of their targeted user base. In this work, we explore the equilibrium behavior of producers who are interested in maximizing user engagement. We study two variants of the content-serving rule for the platform's recommender system, and provide a structural characterization of producer behavior at equilibrium: namely, each producer chooses to focus on a single embedded feature. We further show that specialization, defined as different producers optimizing for distinct types of content, naturally emerges from the competition among producers trying to maximize user engagement. We provide a heuristic for computing equilibria of our engagement game, and evaluate it experimentally. We highlight i) the performance and convergence of our heuristic, ii) the degree of producer specialization, and iii) the impact of the content-serving rule on producer and user utilities at equilibrium and provide guidance on how to set the content-serving rule.</article>","contentLength":1306,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A polytopal discrete de Rham complex on manifolds, with application to the Maxwell equations","url":"https://arxiv.org/abs/2401.16130","date":1740114000,"author":"","guid":7836,"unread":true,"content":"<article>arXiv:2401.16130v3 Announce Type: replace \nAbstract: We design in this work a discrete de Rham complex on manifolds. This complex, written in the framework of exterior calculus, has the same cohomology as the continuous de Rham complex, is of arbitrary order of accuracy and, in principle, can be designed on meshes made of generic elements (that is, elements whose boundary is the union of an arbitrary number of curved edges/faces). Notions of local (full and trimmed) polynomial spaces are developed, with compatibility requirements between polynomials on mesh entities of various dimensions. We give explicit constructions of such polynomials in 2D, for some meshes made of curved triangles or quadrangles (such meshes are easy to design in many cases, starting from a few charts describing the manifold). The discrete de Rham complex is then used to set up a scheme for the Maxwell equations on a 2D manifold without boundary, and we show that a natural discrete version of the constraint linking the electric field and the electric charge density is satisfied. Numerical examples are provided on the sphere and the torus, based on bespoke analytical solutions and meshes on each of these manifolds.</article>","contentLength":1204,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stability Analysis of Compartmental and Cooperative Systems","url":"https://arxiv.org/abs/2312.11061","date":1740114000,"author":"","guid":7837,"unread":true,"content":"<article>arXiv:2312.11061v2 Announce Type: replace \nAbstract: The present article considers stability of the solutions to nonlinear and nonautonomous compartmental systems governed by ordinary differential equations (ODEs). In particular, compartmental systems with a right-hand side that can be written as a product of a matrix function and vector function. Sufficient, and on occasion necessary, conditions on the matrix function are provided to conclude exponential stability of the null solution. The conditions involve verifying that the matrix function takes its values in a set of compartmental matrices on a certain canonical form, and are easy to check. Similar conditions are provided to establish incremental exponential stability for compartmental systems governed by cooperative systems of ODEs. The solutions to such systems satisfy a so-called ordering. Systems that are cooperative in a box, are shown to be incrementally asymptotically stable if and only if every pair of initially ordered solutions converge to each other. Traffic Reaction Models are used to illustrate the results, which are numerical schemes to solve conservation laws in one spatial dimension. Suitable conditions on the flux function of the conservation law are given such that the numerical scheme gives rise to an incrementally exponentially stable system.</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards impactful challenges: post-challenge paper, benchmarks and other dissemination actions","url":"https://arxiv.org/abs/2312.06036","date":1740114000,"author":"","guid":7838,"unread":true,"content":"<article>arXiv:2312.06036v5 Announce Type: replace \nAbstract: The conclusion of an AI challenge is not the end of its lifecycle; ensuring a long-lasting impact requires meticulous post-challenge activities. The long-lasting impact also needs to be organised. This chapter covers the various activities after the challenge is formally finished. This work identifies target audiences for post-challenge initiatives and outlines methods for collecting and organizing challenge outputs. The multiple outputs of the challenge are listed, along with the means to collect them. The central part of the chapter is a template for a typical post-challenge paper, including possible graphs and advice on how to turn the challenge into a long-lasting benchmark.</article>","contentLength":740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Intelligent Anomaly Detection for Lane Rendering Using Transformer with Self-Supervised Pre-Training and Customized Fine-Tuning","url":"https://arxiv.org/abs/2312.04398","date":1740114000,"author":"","guid":7839,"unread":true,"content":"<article>arXiv:2312.04398v5 Announce Type: replace \nAbstract: The burgeoning navigation services using digital maps provide great convenience to drivers. Nevertheless, the presence of anomalies in lane rendering map images occasionally introduces potential hazards, as such anomalies can be misleading to human drivers and consequently contribute to unsafe driving conditions. In response to this concern and to accurately and effectively detect the anomalies, this paper transforms lane rendering image anomaly detection into a classification problem and proposes a four-phase pipeline consisting of data pre-processing, self-supervised pre-training with the masked image modeling (MiM) method, customized fine-tuning using cross-entropy based loss with label smoothing, and post-processing to tackle it leveraging state-of-the-art deep learning techniques, especially those involving Transformer models. Various experiments verify the effectiveness of the proposed pipeline. Results indicate that the proposed pipeline exhibits superior performance in lane rendering image anomaly detection, and notably, the self-supervised pre-training with MiM can greatly enhance the detection accuracy while significantly reducing the total training time. For instance, employing the Swin Transformer with Uniform Masking as self-supervised pretraining (Swin-Trans-UM) yielded a heightened accuracy at 94.77% and an improved Area Under The Curve (AUC) score of 0.9743 compared with the pure Swin Transformer without pre-training (Swin-Trans) with an accuracy of 94.01% and an AUC of 0.9498. The fine-tuning epochs were dramatically reduced to 41 from the original 280. In conclusion, the proposed pipeline, with its incorporation of self-supervised pre-training using MiM and other advanced deep learning techniques, emerges as a robust solution for enhancing the accuracy and efficiency of lane rendering image anomaly detection in digital navigation systems.</article>","contentLength":1941,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FREE: The Foundational Semantic Recognition for Modeling Environmental Ecosystems","url":"https://arxiv.org/abs/2311.10255","date":1740114000,"author":"","guid":7840,"unread":true,"content":"<article>arXiv:2311.10255v3 Announce Type: replace \nAbstract: Modeling environmental ecosystems is critical for the sustainability of our planet, but is extremely challenging due to the complex underlying processes driven by interactions amongst a large number of physical variables. As many variables are difficult to measure at large scales, existing works often utilize a combination of observable features and locally available measurements or modeled values as input to build models for a specific study region and time period. This raises a fundamental question in advancing the modeling of environmental ecosystems: how to build a general framework for modeling the complex relationships amongst various environmental data over space and time? In this paper, we introduce a framework, FREE, which maps available environmental data into a text space and then converts the traditional predictive modeling task in environmental science to a semantic recognition problem. The proposed framework leverages recent advances in Large Language Models (LLMs) to supplement the original input features with natural language descriptions. This framework facilitates capturing the data semantics and allows harnessing the irregularities of input features. When used for long-term prediction, FREE has the flexibility to incorporate newly collected observations to enhance future prediction. The efficacy of FREE is evaluated in the context of two societally important real-world applications, predicting stream water temperature in the Delaware River Basin and predicting annual corn yield in Illinois and Iowa. Beyond the superior predictive performance over multiple baselines, FREE is shown to be more data- and computation-efficient as it can be pre-trained on simulated data generated by physics-based models.</article>","contentLength":1799,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DeepFracture: A Generative Approach for Predicting Brittle Fractures with Neural Discrete Representation Learning","url":"https://arxiv.org/abs/2310.13344","date":1740114000,"author":"","guid":7841,"unread":true,"content":"<article>arXiv:2310.13344v2 Announce Type: replace \nAbstract: In the field of brittle fracture animation, generating realistic destruction animations using physics-based simulation methods is computationally expensive. While techniques based on Voronoi diagrams or pre-fractured patterns are effective for real-time applications, they fail to incorporate collision conditions when determining fractured shapes during runtime. This paper introduces a novel learning-based approach for predicting fractured shapes based on collision dynamics at runtime. Our approach seamlessly integrates realistic brittle fracture animations with rigid body simulations, utilising boundary element method (BEM) brittle fracture simulations to generate training data. To integrate collision scenarios and fractured shapes into a deep learning framework, we introduce generative geometric segmentation, distinct from both instance and semantic segmentation, to represent 3D fragment shapes. We propose an eight-dimensional latent code to address the challenge of optimising multiple discrete fracture pattern targets that share similar continuous collision latent codes. This code will follow a discrete normal distribution corresponding to a specific fracture pattern within our latent impulse representation design. This adaptation enables the prediction of fractured shapes using neural discrete representation learning. Our experimental results show that our approach generates considerably more detailed brittle fractures than existing techniques, while the computational time is typically reduced compared to traditional simulation methods at comparable resolutions.</article>","contentLength":1644,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt Engineering or Fine-Tuning: An Empirical Assessment of LLMs for Code","url":"https://arxiv.org/abs/2310.10508","date":1740114000,"author":"","guid":7842,"unread":true,"content":"<article>arXiv:2310.10508v2 Announce Type: replace \nAbstract: The rapid advancements in large language models (LLMs) have greatly expanded the potential for automated code-related tasks. Two primary methodologies are used in this domain: prompt engineering and fine-tuning. Prompt engineering involves applying different strategies to query LLMs, like ChatGPT, while fine-tuning further adapts pre-trained models, such as CodeBERT, by training them on task-specific data. Despite the growth in the area, there remains a lack of comprehensive comparative analysis between the approaches for code models. In this paper, we evaluate GPT-4 using three prompt engineering strategies -- basic prompting, in-context learning, and task-specific prompting -- and compare it against 17 fine-tuned models across three code-related tasks: code summarization, generation, and translation. Our results indicate that GPT-4 with prompt engineering does not consistently outperform fine-tuned models. For instance, in code generation, GPT-4 is outperformed by fine-tuned models by 28.3% points on the MBPP dataset. It also shows mixed results for code translation tasks. Additionally, a user study was conducted involving 27 graduate students and 10 industry practitioners. The study revealed that GPT-4 with conversational prompts, incorporating human feedback during interaction, significantly improved performance compared to automated prompting. Participants often provided explicit instructions or added context during these interactions. These findings suggest that GPT-4 with conversational prompting holds significant promise for automated code-related tasks, whereas fully automated prompt engineering without human involvement still requires further investigation.</article>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentially Private Optimization for Non-Decomposable Objective Functions","url":"https://arxiv.org/abs/2310.03104","date":1740114000,"author":"","guid":7843,"unread":true,"content":"<article>arXiv:2310.03104v2 Announce Type: replace \nAbstract: Unsupervised pre-training is a common step in developing computer vision models and large language models. In this setting, the absence of labels requires the use of similarity-based loss functions, such as contrastive loss, that favor minimizing the distance between similar inputs and maximizing the distance between distinct inputs. As privacy concerns mount, training these models using differential privacy has become more important. However, due to how inputs are generated for these losses, one of their undesirable properties is that their $L_2$ sensitivity grows with the batch size. This property is particularly disadvantageous for differentially private training methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD variant for similarity based loss functions -- in particular, the commonly-used contrastive loss -- that manipulates gradients of the objective function in a novel way to obtain a sensitivity of the summed gradient that is $O(1)$ for batch size $n$. We test our DP-SGD variant on some CIFAR-10 pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, our method's performance comes close to that of a non-private model and generally outperforms DP-SGD applied directly to the contrastive loss.</article>","contentLength":1311,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Memorization in Diffusion Models","url":"https://arxiv.org/abs/2310.02664","date":1740114000,"author":"","guid":7844,"unread":true,"content":"<article>arXiv:2310.02664v2 Announce Type: replace \nAbstract: Due to their capacity to generate novel and high-quality samples, diffusion models have attracted significant research interest in recent years. Notably, the typical training objective of diffusion models, i.e., denoising score matching, has a closed-form optimal solution that can only generate training data replicating samples. This indicates that a memorization behavior is theoretically expected, which contradicts the common generalization ability of state-of-the-art diffusion models, and thus calls for a deeper understanding. Looking into this, we first observe that memorization behaviors tend to occur on smaller-sized datasets, which motivates our definition of effective model memorization (EMM), a metric measuring the maximum size of training data at which a learned diffusion model approximates its theoretical optimum. Then, we quantify the impact of the influential factors on these memorization behaviors in terms of EMM, focusing primarily on data distribution, model configuration, and training procedure. Besides comprehensive empirical results identifying the influential factors, we surprisingly find that conditioning training data on uninformative random labels can significantly trigger the memorization in diffusion models. Our study holds practical significance for diffusion model users and offers clues to theoretical research in deep generative models. Code is available at https://github.com/sail-sg/DiffMemorize.</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sketch2CAD: 3D CAD Model Reconstruction from 2D Sketch using Visual Transformer","url":"https://arxiv.org/abs/2309.16850","date":1740114000,"author":"","guid":7845,"unread":true,"content":"<article>arXiv:2309.16850v2 Announce Type: replace \nAbstract: Current 3D reconstruction methods typically generate outputs in the form of voxels, point clouds, or meshes. However, each of these formats has inherent limitations, such as rough surfaces and distorted structures. Additionally, these data types are not ideal for further manual editing and post-processing. In this paper, we present a novel 3D reconstruction method designed to overcome these disadvantages by reconstructing CAD-compatible models. We trained a visual transformer to predict a \"scene descriptor\" from a single 2D wire-frame image. This descriptor includes essential information, such as object types and parameters like position, rotation, and size. Using the predicted parameters, a 3D scene can be reconstructed with 3D modeling software that has programmable interfaces, such as Rhino Grasshopper, to build highly editable 3D models in the form of B-rep. To evaluate our proposed model, we created two datasets: one consisting of simple scenes and another with more complex scenes. The test results indicate the model's capability to accurately reconstruct simple scenes while highlighting its difficulties with more complex ones.</article>","contentLength":1203,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging Sensor Gaps via Attention Gated Tuning for Hyperspectral Image Classification","url":"https://arxiv.org/abs/2309.12865","date":1740114000,"author":"","guid":7846,"unread":true,"content":"<article>arXiv:2309.12865v4 Announce Type: replace \nAbstract: Data-hungry HSI classification methods require high-quality labeled HSIs, which are often costly to obtain. This characteristic limits the performance potential of data-driven methods when dealing with limited annotated samples. Bridging the domain gap between data acquired from different sensors allows us to utilize abundant labeled data across sensors to break this bottleneck. In this paper, we propose a novel Attention-Gated Tuning (AGT) strategy and a triplet-structured transformer model, Tri-Former, to address this issue. The AGT strategy serves as a bridge, allowing us to leverage existing labeled HSI datasets, even RGB datasets to enhance the performance on new HSI datasets with limited samples. Instead of inserting additional parameters inside the basic model, we train a lightweight auxiliary branch that takes intermediate features as input from the basic model and makes predictions. The proposed AGT resolves conflicts between heterogeneous and even cross-modal data by suppressing the disturbing information and enhances the useful information through a soft gate. Additionally, we introduce Tri-Former, a triplet-structured transformer with a spectral-spatial separation design that enhances parameter utilization and computational efficiency, enabling easier and flexible fine-tuning. Comparison experiments conducted on three representative HSI datasets captured by different sensors demonstrate the proposed Tri-Former achieves better performance compared to several state-of-the-art methods. Homologous, heterologous and cross-modal tuning experiments verified the effectiveness of the proposed AGT. Code has been released at: \\href{https://github.com/Cecilia-xue/AGT}{https://github.com/Cecilia-xue/AGT}.</article>","contentLength":1786,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Auditing the Compliance and Enforcement of Twitter's Advertising Policy","url":"https://arxiv.org/abs/2309.12591","date":1740114000,"author":"","guid":7847,"unread":true,"content":"<article>arXiv:2309.12591v2 Announce Type: replace \nAbstract: Online platforms have enacted various policies to maintain a safe and trustworthy advertising environment. However, the extent to which these policies are adhered to and enforced remains a subject of interest and concern. In this work, we present a large-scale audit of adult advertising on Twitter (now X), specifically focusing on compliance with its adult (sexual) content advertising policy. Twitter is an interesting case study in that it -- uniquely from other social media platforms -- allows posting of adult content but prohibits adult content in advertising. We analyze approximately 35 thousand ads on Twitter with respect to their compliance to the adult content ad policy through Perspective API and manual annotations. Among other things, we find that nearly 38% of ads violate Twitter's adult content advertising policy, although the platform eventually removed only about 63% of these non-compliant adult ads. We also find inconsistencies in the moderation of such ads across languages, highlighting the need for more reliable and consistent moderation practices across various languages. Overall, our findings highlight blind spots in Twitter's adult ad policy enforcement for certain languages and countries. Our work underscores the importance of external audits to monitor compliance and improve transparency in online advertising.</article>","contentLength":1404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"$O(k)$-Equivariant Dimensionality Reduction on Stiefel Manifolds","url":"https://arxiv.org/abs/2309.10775","date":1740114000,"author":"","guid":7848,"unread":true,"content":"<article>arXiv:2309.10775v3 Announce Type: replace \nAbstract: Many real-world datasets live on high-dimensional Stiefel and Grassmannian manifolds, $V_k(\\mathbb{R}^N)$ and $Gr(k, \\mathbb{R}^N)$ respectively, and benefit from projection onto lower-dimensional Stiefel and Grassmannian manifolds. In this work, we propose an algorithm called \\textit{Principal Stiefel Coordinates (PSC)} to reduce data dimensionality from $ V_k(\\mathbb{R}^N)$ to $V_k(\\mathbb{R}^n)$ in an \\textit{$O(k)$-equivariant} manner ($k \\leq n \\ll N$). We begin by observing that each element $\\alpha \\in V_n(\\mathbb{R}^N)$ defines an isometric embedding of $V_k(\\mathbb{R}^n)$ into $V_k(\\mathbb{R}^N)$. Next, we describe two ways of finding a suitable embedding map $\\alpha$: one via an extension of principal component analysis ($\\alpha_{PCA}$), and one that further minimizes data fit error using gradient descent ($\\alpha_{GD}$). Then, we define a continuous and $O(k)$-equivariant map $\\pi_\\alpha$ that acts as a \"closest point operator\" to project the data onto the image of $V_k(\\mathbb{R}^n)$ in $V_k(\\mathbb{R}^N)$ under the embedding determined by $\\alpha$, while minimizing distortion. Because this dimensionality reduction is $O(k)$-equivariant, these results extend to Grassmannian manifolds as well. Lastly, we show that $\\pi_{\\alpha_{PCA}}$ globally minimizes projection error in a noiseless setting, while $\\pi_{\\alpha_{GD}}$ achieves a meaningfully different and improved outcome when the data does not lie exactly on the image of a linearly embedded lower-dimensional Stiefel manifold as above. Multiple numerical experiments using synthetic and real-world data are performed.</article>","contentLength":1657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding Complex Patterns in Trajectory Data via Geometric Set Cover","url":"https://arxiv.org/abs/2308.14865","date":1740114000,"author":"","guid":7849,"unread":true,"content":"<article>arXiv:2308.14865v2 Announce Type: replace \nAbstract: Clustering trajectories is a central challenge when faced with large amounts of movement data such as GPS data. We study a clustering problem that can be stated as a geometric set cover problem: Given a polygonal curve of complexity $n$, find the smallest number $k$ of representative trajectories of complexity at most $l$ such that any point on the input trajectories lies on a subtrajectory of the input that has Fr\\'echet distance at most $\\Delta$ to one of the representative trajectories.\n  In previous work, Br\\\"uning et al.~(2022) developed a bicriteria approximation algorithm that returns a set of curves of size $O(kl\\log(kl))$ which covers the input with a radius of $11\\Delta$ in time $\\widetilde{O}((kl)^2n + kln^3)$, where $k$ is the smallest number of curves of complexity $l$ needed to cover the input with a radius of $\\Delta$. The representative trajectories computed by this algorithm are always line segments. In the applications however, one is usually interested in more complex representative curves which consist of several edges. We present a new approach that builds upon previous work computing a set of curves of size $O(k\\log(n))$ in time $\\widetilde{O}(l^2n^4 + kln^4)$ with the same distance guarantee of $11\\Delta$, where each curve may consist of curves of complexity up to the given complexity parameter~$l$. We conduct experiments on tracking data of ocean currents and full body motion data suggesting its validity as a tool for analyzing large spatio-temporal data sets.</article>","contentLength":1561,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks","url":"https://arxiv.org/abs/2308.11845","date":1740114000,"author":"","guid":7850,"unread":true,"content":"<article>arXiv:2308.11845v2 Announce Type: replace \nAbstract: Machine Learning (ML) systems are vulnerable to adversarial examples, particularly those from query-based black-box attacks. Despite various efforts to detect and prevent such attacks, ML systems are still at risk, demanding a more comprehensive approach to security that includes logging, analyzing, and sharing evidence. While traditional security benefits from well-established practices of forensics and threat intelligence sharing, ML security has yet to find a way to profile its attackers and share information about them. In response, this paper introduces SEA, a novel ML security system to characterize black-box attacks on ML systems for forensic purposes and to facilitate human-explainable intelligence sharing. SEA leverages Hidden Markov Models to attribute the observed query sequence to known attacks. It thus understands the attack's progression rather than focusing solely on the final adversarial examples. Our evaluations reveal that SEA is effective at attack attribution, even on the second incident, and is robust to adaptive strategies designed to evade forensic analysis. SEA's explanations of the attack's behavior allow us even to fingerprint specific minor bugs in widely used attack libraries. For example, we discover that the SignOPT and Square attacks in ART v1.14 send over 50% duplicated queries. We thoroughly evaluate SEA on a variety of settings and demonstrate that it can recognize the same attack with more than 90% Top-1 and 95% Top-3 accuracy. Finally, we demonstrate how SEA generalizes to other domains like text classification.</article>","contentLength":1626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lifted Inference beyond First-Order Logic","url":"https://arxiv.org/abs/2308.11738","date":1740114000,"author":"","guid":7851,"unread":true,"content":"<article>arXiv:2308.11738v4 Announce Type: replace \nAbstract: Weighted First Order Model Counting (WFOMC) is fundamental to probabilistic inference in statistical relational learning models. As WFOMC is known to be intractable in general ($\\#$P-complete), logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent works have shown that the two-variable fragment of first order logic extended with counting quantifiers ($\\mathrm{C^2}$) is domain-liftable. However, many properties of real-world data, like acyclicity in citation networks and connectivity in social networks, cannot be modeled in $\\mathrm{C^2}$, or first order logic in general. In this work, we expand the domain liftability of $\\mathrm{C^2}$ with multiple such properties. We show that any $\\mathrm{C^2}$ sentence remains domain liftable when one of its relations is restricted to represent a directed acyclic graph, a connected graph, a tree (resp. a directed tree) or a forest (resp. a directed forest). All our results rely on a novel and general methodology of \"counting by splitting\". Besides their application to probabilistic inference, our results provide a general framework for counting combinatorial structures. We expand a vast array of previous results in discrete mathematics literature on directed acyclic graphs, phylogenetic networks, etc.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Testing GPT-4 with Wolfram Alpha and Code Interpreter plug-ins on math and science problems","url":"https://arxiv.org/abs/2308.05713","date":1740114000,"author":"","guid":7852,"unread":true,"content":"<article>arXiv:2308.05713v4 Announce Type: replace \nAbstract: This report describes a test of the large language model GPT-4 with the Wolfram Alpha and the Code Interpreter plug-ins on 105 original problems in science and math, at the high school and college levels, carried out in June-August 2023. Our tests suggest that the plug-ins significantly enhance GPT's ability to solve these problems. Having said that, there are still often \"interface\" failures; that is, GPT often has trouble formulating problems in a way that elicits useful answers from the plug-ins. Fixing these interface failures seems like a central challenge in making GPT a reliable tool for college-level calculation problems.</article>","contentLength":690,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YOLO-MS: Rethinking Multi-Scale Representation Learning for Real-time Object Detection","url":"https://arxiv.org/abs/2308.05480","date":1740114000,"author":"","guid":7853,"unread":true,"content":"<article>arXiv:2308.05480v2 Announce Type: replace \nAbstract: We aim at providing the object detection community with an efficient and performant object detector, termed YOLO-MS. The core design is based on a series of investigations on how multi-branch features of the basic block and convolutions with different kernel sizes affect the detection performance of objects at different scales. The outcome is a new strategy that can significantly enhance multi-scale feature representations of real-time object detectors. To verify the effectiveness of our work, we train our YOLO-MS on the MS COCO dataset from scratch without relying on any other large-scale datasets, like ImageNet or pre-trained weights. Without bells and whistles, our YOLO-MS outperforms the recent state-of-the-art real-time object detectors, including YOLO-v7, RTMDet, and YOLO-v8. Taking the XS version of YOLO-MS as an example, it can achieve an AP score of 42+% on MS COCO, which is about 2% higher than RTMDet with the same model size. Furthermore, our work can also serve as a plug-and-play module for other YOLO models. Typically, our method significantly advances the APs, APl, and AP of YOLOv8-N from 18%+, 52%+, and 37%+ to 20%+, 55%+, and 40%+, respectively, with even fewer parameters and MACs. Code and trained models are publicly available at https://github.com/FishAndWasabi/YOLO-MS. We also provide the Jittor version at https://github.com/NK-JittorCV/nk-yolo.</article>","contentLength":1439,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Effective Horizon of Inverse Reinforcement Learning","url":"https://arxiv.org/abs/2307.06541","date":1740114000,"author":"","guid":7854,"unread":true,"content":"<article>arXiv:2307.06541v3 Announce Type: replace \nAbstract: Inverse reinforcement learning (IRL) algorithms often rely on (forward) reinforcement learning or planning, over a given time horizon, to compute an approximately optimal policy for a hypothesized reward function; they then match this policy with expert demonstrations. The time horizon plays a critical role in determining both the accuracy of reward estimates and the computational efficiency of IRL algorithms. Interestingly, an *effective time horizon* shorter than the ground-truth value often produces better results faster. This work formally analyzes this phenomenon and provides an explanation: the time horizon controls the complexity of an induced policy class and mitigates overfitting with limited data. This analysis provides a guide for the principled choice of the effective horizon for IRL. It also prompts us to re-examine the classic IRL formulation: it is more natural to learn jointly the reward and the effective horizon rather than the reward alone with a given horizon. To validate our findings, we implement a cross-validation extension and the experimental results support the theoretical analysis. The project page and code are publicly available.</article>","contentLength":1227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs","url":"https://arxiv.org/abs/2304.06020","date":1740114000,"author":"","guid":7855,"unread":true,"content":"<article>arXiv:2304.06020v2 Announce Type: replace \nAbstract: We propose $\\textbf{VidStyleODE}$, a spatiotemporally continuous disentangled $\\textbf{Vid}$eo representation based upon $\\textbf{Style}$GAN and Neural-$\\textbf{ODE}$s. Effective traversal of the latent space learned by Generative Adversarial Networks (GANs) has been the basis for recent breakthroughs in image editing. However, the applicability of such advancements to the video domain has been hindered by the difficulty of representing and controlling videos in the latent space of GANs. In particular, videos are composed of content (i.e., appearance) and complex motion components that require a special mechanism to disentangle and control. To achieve this, VidStyleODE encodes the video content in a pre-trained StyleGAN $\\mathcal{W}_+$ space and benefits from a latent ODE component to summarize the spatiotemporal dynamics of the input video. Our novel continuous video generation process then combines the two to generate high-quality and temporally consistent videos with varying frame rates. We show that our proposed method enables a variety of applications on real videos: text-guided appearance manipulation, motion manipulation, image animation, and video interpolation and extrapolation. Project website: https://cyberiada.github.io/VidStyleODE</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Onboard LiDAR-based Flying Object Detection","url":"https://arxiv.org/abs/2303.05404","date":1740114000,"author":"","guid":7856,"unread":true,"content":"<article>arXiv:2303.05404v4 Announce Type: replace \nAbstract: A new robust and accurate approach for the detection and localization of flying objects with the purpose of highly dynamic aerial interception and agile multi-robot interaction is presented in this paper. The approach is proposed for use on board of autonomous aerial vehicles equipped with a 3D LiDAR sensor. It relies on a novel 3D occupancy voxel mapping method for the target detection that provides high localization accuracy and robustness with respect to varying environments and appearance changes of the target. In combination with a proposed cluster-based multi-target tracker, sporadic false positives are suppressed, state estimation of the target is provided, and the detection latency is negligible. This makes the system suitable for tasks of agile multi-robot interaction, such as autonomous aerial interception or formation control where fast, precise, and robust relative localization of other robots is crucial. We evaluate the viability and performance of the system in simulated and real-world experiments which demonstrate that at a range of 20m, our system is capable of reliably detecting a micro-scale UAV with an almost 100% recall, 0.2m accuracy, and 20ms delay.</article>","contentLength":1242,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UniASM: Binary Code Similarity Detection without Fine-tuning","url":"https://arxiv.org/abs/2211.01144","date":1740114000,"author":"","guid":7857,"unread":true,"content":"<article>arXiv:2211.01144v4 Announce Type: replace \nAbstract: Binary code similarity detection (BCSD) is widely used in various binary analysis tasks such as vulnerability search, malware detection, clone detection, and patch analysis. Recent studies have shown that the learning-based binary code embedding models perform better than the traditional feature-based approaches. However, previous studies have not delved deeply into the key factors that affect model performance. In this paper, we design extensive ablation studies to explore these influencing factors. The experimental results have provided us with many new insights. We have made innovations in both code representation and model selection: we propose a novel rich-semantic function representation technique to ensure the model captures the intricate nuances of binary code, and we introduce the first UniLM-based binary code embedding model, named UniASM, which includes two newly designed training tasks to learn representations of binary functions. The experimental results show that UniASM outperforms the state-of-the-art (SOTA) approaches on the evaluation datasets. The average scores of Recall@1 on cross-compilers, cross-optimization-levels, and cross-obfuscations have improved by 12.7%, 8.5%, and 22.3%, respectively, compared to the best of the baseline methods. Besides, in the real-world task of known vulnerability search, UniASM outperforms all the current baselines.</article>","contentLength":1441,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Constant-delay enumeration for SLP-compressed documents","url":"https://arxiv.org/abs/2209.12301","date":1740114000,"author":"","guid":7858,"unread":true,"content":"<article>arXiv:2209.12301v5 Announce Type: replace \nAbstract: We study the problem of enumerating results from a query over a compressed document. The model we use for compression are straight-line programs (SLPs), which are defined by a context-free grammar that produces a single string. For our queries, we use a model called Annotated Automata, an extension of regular automata that allows annotations on letters. This model extends the notion of Regular Spanners as it allows arbitrarily long outputs. Our main result is an algorithm that evaluates such a query by enumerating all results with output-linear delay after a preprocessing phase which takes linear time on the size of the SLP, and cubic time over the size of the automaton. This is an improvement over Schmid and Schweikardt's result, which, with the same preprocessing time, enumerates with a delay that is logarithmic on the size of the uncompressed document. We achieve this through a persistent data structure named Enumerable Compact Sets with Shifts which guarantees output-linear delay under certain restrictions. These results imply constant-delay enumeration algorithms in the context of regular spanners. Further, we use an extension of annotated automata which utilizes succinctly encoded annotations to save an exponential factor from previous results that dealt with constant-delay enumeration over vset automata. Lastly, we extend our results in the same fashion Schmid and Schweikardt did to allow complex document editing while maintaining the constant delay guarantee.</article>","contentLength":1544,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Density-Based Algorithms for Corruption-Robust Contextual Search and Convex Optimization","url":"https://arxiv.org/abs/2206.07528","date":1740114000,"author":"","guid":7859,"unread":true,"content":"<article>arXiv:2206.07528v2 Announce Type: replace \nAbstract: We study the problem of contextual search, a generalization of binary search in higher dimensions, in the adversarial noise model. Let $d$ be the dimension of the problem, $T$ be the time horizon and $C$ be the total amount of adversarial noise in the system. We focus on the $\\epsilon$-ball and the absolute loss. For the $\\epsilon$-ball loss, we give a tight regret bound of $O(C + d \\log(1/\\epsilon))$ improving over the $O(d^3 \\log(1/\\epsilon) \\log^2(T) + C \\log(T) \\log(1/\\epsilon))$ bound of Krishnamurthy et al (Operations Research '23). For the absolute loss, we give an efficient algorithm with regret $O(C+d \\log T)$. To tackle the absolute loss case, we study the more general setting of Corruption-Robust Convex Optimization with Subgradient feedback, which is of independent interest.\n  Our techniques are a significant departure from prior approaches. Specifically, we keep track of density functions over the candidate target vectors instead of a knowledge set consisting of the candidate target vectors consistent with the feedback obtained.</article>","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transfer Learning with Pre-trained Conditional Generative Models","url":"https://arxiv.org/abs/2204.12833","date":1740114000,"author":"","guid":7860,"unread":true,"content":"<article>arXiv:2204.12833v3 Announce Type: replace \nAbstract: Transfer learning is crucial in training deep neural networks on new target tasks. Current transfer learning methods always assume at least one of (i) source and target task label spaces overlap, (ii) source datasets are available, and (iii) target network architectures are consistent with source ones. However, holding these assumptions is difficult in practical settings because the target task rarely has the same labels as the source task, the source dataset access is restricted due to storage costs and privacy, and the target architecture is often specialized to each task. To transfer source knowledge without these assumptions, we propose a transfer learning method that uses deep generative models and is composed of the following two stages: pseudo pre-training (PP) and pseudo semi-supervised learning (P-SSL). PP trains a target architecture with an artificial dataset synthesized by using conditional source generative models. P-SSL applies SSL algorithms to labeled target data and unlabeled pseudo samples, which are generated by cascading the source classifier and generative models to condition them with target samples. Our experimental results indicate that our method can outperform the baselines of scratch training and knowledge distillation.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Control Barrier Function based Attack-Recovery with Provable Guarantees","url":"https://arxiv.org/abs/2204.03077","date":1740114000,"author":"","guid":7861,"unread":true,"content":"<article>arXiv:2204.03077v2 Announce Type: replace \nAbstract: This paper studies provable security guarantees for cyber-physical systems (CPS) under actuator attacks. In particular, we consider CPS safety and propose a new attack detection mechanism based on zeroing control barrier function (ZCBF) conditions. In addition, we design an adaptive recovery mechanism based on how close the system is to violating safety. We show that under certain conditions, the attack-detection mechanism is sound, i.e., there are no false negatives for adversarial attacks. We propose sufficient conditions for the initial conditions and input constraints so that the resulting CPS is secure by design. We also propose a novel hybrid control to account for attack detection delays and avoid Zeno behavior. Next, to efficiently compute the set of initial conditions, we propose a sampling-based method to verify whether a set is a viability domain. Specifically, we devise a method for checking a modified barrier function condition on a finite set of points to assess whether a set can be rendered forward invariant. Then, we propose an iterative algorithm to compute the set of initial conditions and input constraints set to limit the effect of an adversary if it compromises vulnerable inputs. Finally, we use a Quadratic Programming (QP) approach for online recovery (as well as nominal) control synthesis. We demonstrate the effectiveness of the proposed method in a simulation case study involving a quadrotor with an attack on its motors.</article>","contentLength":1521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GenEO spectral coarse spaces in SPD domain decomposition","url":"https://arxiv.org/abs/2104.00280","date":1740114000,"author":"","guid":7862,"unread":true,"content":"<article>arXiv:2104.00280v2 Announce Type: replace \nAbstract: Two-level domain decomposition methods are preconditioned Krylov solvers. What separates one and two-level domain decomposition method is the presence of a coarse space in the latter. The abstract Schwarz framework is a formalism that allows to define and study a large variety of two-level methods. The objective of this article is to define, in the abstract Schwarz framework, a family of coarse spaces called the GenEO coarse spaces (for Generalized Eigenvalues in the Overlaps). This is a generalization of methods that exist already for particular choices of domain decomposition methods. Bounds for the condition numbers of the preconditioned operators are proved that are independent of the parameters in the problem (e.g., any coefficients in an underlying PDE or the number of subdomains). The coarse spaces are computed by finding low or high frequency spaces of some well chosen generalized eigenvalue problems in each subdomain. The abstract framework is illustrated by defining two-level Additive Schwarz, Neumann-Neumann and Inexact Schwarz preconditioners for a two dimensional linear elasticity problem. Explicit theoretical bounds as well as numerical results are provided for this example.</article>","contentLength":1260,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Compact Formulation of the First Evolution Equation for Optimal Control Computation","url":"https://arxiv.org/abs/1804.02980","date":1740114000,"author":"","guid":7863,"unread":true,"content":"<article>arXiv:1804.02980v2 Announce Type: replace \nAbstract: The first evolution equation is derived under the Variation Evolving Method (VEM) that seeks optimal solutions with the variation evolution principle. To improve the performance, its compact form is developed. By replacing the states and costates variation evolution with that of the controls, the dimension-reduced Evolution Partial Differential Equation (EPDE) only solves the control variables along the variation time to get the optimal solution, and its definite conditions may be arbitrary. With this equation, the scale of the resulting Initial-value Problem (IVP), transformed via the semi-discrete method, is significantly reduced. Illustrative examples are solved and it is shown that the compact form evolution equation outperforms the primary form in the precision, and the efficiency may be higher for the dense discretization. Moreover, in discussing the connections to the classic iteration methods, it is uncovered that the computation scheme of the gradient method is the discrete implementation of the third evolution equation, and the compact form of the first evolution equation is a continuous realization of the Newton type iteration mechanism.</article>","contentLength":1219,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probability Bracket Notation: Markov Sequence Projector of Visible and Hidden Markov Models in Dynamic Bayesian Networks","url":"https://arxiv.org/abs/1212.3817","date":1740114000,"author":"","guid":7864,"unread":true,"content":"<article>arXiv:1212.3817v2 Announce Type: replace \nAbstract: With the symbolic framework of Probability Bracket Notation (PBN), the Markov Sequence Projector (MSP) is introduced to expand the evolution formula of Homogeneous Markov Chains (HMCs). The well-known weather example, a Visible Markov Model (VMM), illustrates that the full joint probability of a VMM corresponds to a specifically projected Markov state sequence in the expanded evolution formula. In a Hidden Markov Model (HMM), the probability basis (P-basis) of the hidden Markov state sequence and the P-basis of the observation sequence exist in the sequential event space. The full joint probability of an HMM is the product of the (unknown) projected hidden sequence of Markov states and their transformations into the observation P-bases. The Viterbi algorithm is applied to the famous Weather-Stone HMM example to determine the most likely weather-state sequence given the observed stone-state sequence. Our results are verified using the Elvira software package. Using the PBN, we unify the evolution formulas for Markov models like VMMs, HMMs, and factorial HMMs (with discrete time). We briefly investigated the extended HMM, addressing the feedback issue, and the continuous-time VMM and HMM (with discrete or continuous states). All these models are subclasses of Dynamic Bayesian Networks (DBNs) essential for Machine Learning (ML) and Artificial Intelligence (AI).</article>","contentLength":1432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MadVoro: Parallel Construction of Voronoi Diagrams in Distributed Memory Systems","url":"https://arxiv.org/abs/2502.14825","date":1740114000,"author":"","guid":7865,"unread":true,"content":"<article>arXiv:2502.14825v1 Announce Type: cross \nAbstract: Voronoi diagrams are essential geometrical structures with numerous applications, particularly astrophysics-driven finite volume methods. While serial algorithms for constructing these entities are well-established, parallel construction remains challenging. This is especially true in distributed memory systems, where each host manages only a subset of the input points. This process requires redistributing points across hosts and accurately computing the corresponding Voronoi cells. In this paper, we introduce a new distributed construction algorithm, which is implemented in our open-source C++ 3-dimensional Voronoi construction framework. Our approach leverages Delaunay triangulation as an intermediate step, which is then transformed into a Voronoi diagram. We introduce the algorithms we implemented for the precise construction and our load-balancing approach and compare the running time with other state-of-the-art frameworks. MadVoro is a versatile tool that can be applied in various scientific domains, such as mesh decomposition, computational physics, chemistry, and machine learning.</article>","contentLength":1155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FetalCLIP: A Visual-Language Foundation Model for Fetal Ultrasound Image Analysis","url":"https://arxiv.org/abs/2502.14807","date":1740114000,"author":"","guid":7866,"unread":true,"content":"<article>arXiv:2502.14807v1 Announce Type: cross \nAbstract: Foundation models are becoming increasingly effective in the medical domain, offering pre-trained models on large datasets that can be readily adapted for downstream tasks. Despite progress, fetal ultrasound images remain a challenging domain for foundation models due to their inherent complexity, often requiring substantial additional training and facing limitations due to the scarcity of paired multimodal data. To overcome these challenges, here we introduce FetalCLIP, a vision-language foundation model capable of generating universal representation of fetal ultrasound images. FetalCLIP was pre-trained using a multimodal learning approach on a diverse dataset of 210,035 fetal ultrasound images paired with text. This represents the largest paired dataset of its kind used for foundation model development to date. This unique training approach allows FetalCLIP to effectively learn the intricate anatomical features present in fetal ultrasound images, resulting in robust representations that can be used for a variety of downstream applications. In extensive benchmarking across a range of key fetal ultrasound applications, including classification, gestational age estimation, congenital heart defect (CHD) detection, and fetal structure segmentation, FetalCLIP outperformed all baselines while demonstrating remarkable generalizability and strong performance even with limited labeled data. We plan to release the FetalCLIP model publicly for the benefit of the broader scientific community.</article>","contentLength":1557,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Every Graph is Essential to Large Treewidth","url":"https://arxiv.org/abs/2502.14775","date":1740114000,"author":"","guid":7867,"unread":true,"content":"<article>arXiv:2502.14775v1 Announce Type: cross \nAbstract: We show that for every graph $H$, there is a hereditary weakly sparse graph class $\\mathcal C_H$ of unbounded treewidth such that the $H$-free (i.e., excluding $H$ as an induced subgraph) graphs of $\\mathcal C_H$ have bounded treewidth. This refutes several conjectures and critically thwarts the quest for the unavoidable induced subgraphs in classes of unbounded treewidth, a wished-for counterpart of the Grid Minor theorem. We actually show a stronger result: For every positive integer $t$, there is a hereditary graph class $\\mathcal C_t$ of unbounded treewidth such that for any graph $H$ of treewidth at most $t$, the $H$-free graphs of $\\mathcal C_t$ have bounded treewidth. Our construction is a variant of so-called layered wheels. We also introduce a framework of abstract layered wheels, based on their most salient properties. In particular, we streamline and extend key lemmas previously shown on individual layered wheels. We believe that this should greatly help develop this topic, which appears to be a very strong yet underexploited source of counterexamples.</article>","contentLength":1130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Objective Causal Bayesian Optimization","url":"https://arxiv.org/abs/2502.14755","date":1740114000,"author":"","guid":7868,"unread":true,"content":"<article>arXiv:2502.14755v1 Announce Type: cross \nAbstract: In decision-making problems, the outcome of an intervention often depends on the causal relationships between system components and is highly costly to evaluate. In such settings, causal Bayesian optimization (CBO) can exploit the causal relationships between the system variables and sequentially perform interventions to approach the optimum with minimal data. Extending CBO to the multi-outcome setting, we propose Multi-Objective Causal Bayesian Optimization (MO-CBO), a paradigm for identifying Pareto-optimal interventions within a known multi-target causal graph. We first derive a graphical characterization for potentially optimal sets of variables to intervene upon. Showing that any MO-CBO problem can be decomposed into several traditional multi-objective optimization tasks, we then introduce an algorithm that sequentially balances exploration across these tasks using relative hypervolume improvement. The proposed method will be validated on both synthetic and real-world causal graphs, demonstrating its superiority over traditional (non-causal) multi-objective Bayesian optimization in settings where causal information is available.</article>","contentLength":1202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedVAE: Efficient Automated Interpretation of Medical Images with Large-Scale Generalizable Autoencoders","url":"https://arxiv.org/abs/2502.14753","date":1740114000,"author":"","guid":7869,"unread":true,"content":"<article>arXiv:2502.14753v1 Announce Type: cross \nAbstract: Medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. Consequently, training deep learning models on medical images can incur large computational costs. In this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. We introduce MedVAE, a family of six large-scale 2D and 3D autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. We train MedVAE autoencoders using a novel two-stage training approach with 1,052,730 medical images. Across diverse tasks obtained from 20 medical image datasets, we demonstrate that (1) utilizing MedVAE latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits (up to 70x improvement in throughput) while simultaneously preserving clinically-relevant features and (2) MedVAE can decode latent representations back to high-resolution images with high fidelity. Our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. Our code is available at https://github.com/StanfordMIMI/MedVAE.</article>","contentLength":1416,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Robust Information Selection for Hypothesis Testing with Misclassification Penalties","url":"https://arxiv.org/abs/2502.14738","date":1740114000,"author":"","guid":7870,"unread":true,"content":"<article>arXiv:2502.14738v1 Announce Type: cross \nAbstract: We study the problem of robust information selection for a Bayesian hypothesis testing / classification task, where the goal is to identify the true state of the world from a finite set of hypotheses based on observations from the selected information sources. We introduce a novel misclassification penalty framework, which enables non-uniform treatment of different misclassification events. Extending the classical subset selection framework, we study the problem of selecting a subset of sources that minimize the maximum penalty of misclassification under a limited budget, despite deletions or failures of a subset of the selected sources. We characterize the curvature properties of the objective function and propose an efficient greedy algorithm with performance guarantees. Next, we highlight certain limitations of optimizing for the maximum penalty metric and propose a submodular surrogate metric to guide the selection of the information set. We propose a greedy algorithm with near-optimality guarantees for optimizing the surrogate metric. Finally, we empirically demonstrate the performance of our proposed algorithms in several instances of the information set selection problem.</article>","contentLength":1248,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Performance Scores: Directed Functional Connectivity as a Brain-Based Biomarker for Motor Skill Learning and Retention","url":"https://arxiv.org/abs/2502.14731","date":1740114000,"author":"","guid":7871,"unread":true,"content":"<article>arXiv:2502.14731v1 Announce Type: cross \nAbstract: Motor skill acquisition in fields like surgery, robotics, and sports involves learning complex task sequences through extensive training. Traditional performance metrics, like execution time and error rates, offer limited insight as they fail to capture the neural mechanisms underlying skill learning and retention. This study introduces directed functional connectivity (dFC), derived from electroencephalography (EEG), as a novel brain-based biomarker for assessing motor skill learning and retention. For the first time, dFC is applied as a biomarker to map the stages of the Fitts and Posner motor learning model, offering new insights into the neural mechanisms underlying skill acquisition and retention. Unlike traditional measures, it captures both the strength and direction of neural information flow, providing a comprehensive understanding of neural adaptations across different learning stages. The analysis demonstrates that dFC can effectively identify and track the progression through various stages of the Fitts and Posner model. Furthermore, its stability over a six-week washout period highlights its utility in monitoring long-term retention. No significant changes in dFC were observed in a control group, confirming that the observed neural adaptations were specific to training and not due to external factors. By offering a granular view of the learning process at the group and individual levels, dFC facilitates the development of personalized, targeted training protocols aimed at enhancing outcomes in fields where precision and long-term retention are critical, such as surgical education. These findings underscore the value of dFC as a robust biomarker that complements traditional performance metrics, providing a deeper understanding of motor skill learning and retention.</article>","contentLength":1858,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Error Resilience of Iterative Algorithms for Energy Efficiency: from Concept to Implementation","url":"https://arxiv.org/abs/2502.14729","date":1740114000,"author":"","guid":7872,"unread":true,"content":"<article>arXiv:2502.14729v1 Announce Type: cross \nAbstract: Iterative algorithms are widely used in digital signal processing applications. With the case study of radio astronomy calibration processing, this work contributes towards revealing and exploiting the intrinsic error resilience of iterative algorithms for energy efficiency benefits. We consider iterative methods that use a convergence criterion as a quality metric to terminate the iterative computations. We propose an adaptive statistical approximation model for high-level resilience analysis that provides an opportunity to divide an iterative algorithm into exact and approximate iterations. We realize an energy-efficient accelerator based on a heterogeneous architecture, where the heterogeneity is introduced using accurate and approximate processing cores. Our proposed methodology exploits the error-resilience of the algorithm, where initial iterations are processed on approximate modules while the later ones on accurate modules. The proposed accelerator design does not increase the number of iterations as compared to that of an accurate counterpart and provides sufficient precision to converge to an acceptable solution. Our implementation using TSMC 40nm Low Power (TCBN40LP) technology shows 23% savings in electrical energy consumption.</article>","contentLength":1310,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advancing Measurement Capabilities in Lithium-Ion Batteries: Exploring the Potential of Fiber Optic Sensors for Thermal Monitoring of Battery Cells","url":"https://arxiv.org/abs/2502.14720","date":1740114000,"author":"","guid":7873,"unread":true,"content":"<article>arXiv:2502.14720v1 Announce Type: cross \nAbstract: This work demonstrates the potential of fiber optic sensors for measuring thermal effects in lithium-ion batteries, using a fiber optic measurement method of Optical Frequency Domain Reflectometry (OFDR). The innovative application of fiber sensors allows for spatially resolved temperature measurement, particularly emphasizing the importance of monitoring not just the exterior but also the internal conditions within battery cells. Utilizing inert glass fibers as sensors, which exhibit minimal sensitivity to electric fields, opens up new pathways for their implementation in a wide range of applications, such as battery monitoring. The sensors used in this work provide real-time information along the entire length of the fiber, unlike commonly used Fiber Bragg Grating (FBG) sensors. It is shown that using the herein presented novel sensors in a temperature range of 0 to 80 degree celsius reveals a linear thermal dependency with high sensitivity and a local resolution of a few centimeters. Furthermore, this study presents preliminary findings on the potential application of fiber optic sensors in lithium-ion battery (LIB) cells, demonstrating that the steps required for battery integration do not impose any restrictive effects on thermal measurements.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Internal Incoherency Scores for Constraint-based Causal Discovery Algorithms","url":"https://arxiv.org/abs/2502.14719","date":1740114000,"author":"","guid":7874,"unread":true,"content":"<article>arXiv:2502.14719v1 Announce Type: cross \nAbstract: Causal discovery aims to infer causal graphs from observational or experimental data. Methods such as the popular PC algorithm are based on conditional independence testing and utilize enabling assumptions, such as the faithfulness assumption, for their inferences. In practice, these assumptions, as well as the functional assumptions inherited from the chosen conditional independence test, are typically taken as a given and not further tested for their validity on the data. In this work, we propose internal coherency scores that allow testing for assumption violations and finite sample errors, whenever detectable without requiring ground truth or further statistical tests. We provide a complete classification of erroneous results, including a distinction between detectable and undetectable errors, and prove that the detectable erroneous results can be measured by our scores. We illustrate our coherency scores on the PC algorithm with simulated and real-world datasets, and envision that testing for internal coherency can become a standard tool in applying constraint-based methods, much like a suite of tests is used to validate the assumptions of classical regression analysis.</article>","contentLength":1244,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human Misperception of Generative-AI Alignment: A Laboratory Experiment","url":"https://arxiv.org/abs/2502.14708","date":1740114000,"author":"","guid":7875,"unread":true,"content":"<article>arXiv:2502.14708v1 Announce Type: cross \nAbstract: We conduct an incentivized laboratory experiment to study people's perception of generative artificial intelligence (GenAI) alignment in the context of economic decision-making. Using a panel of economic problems spanning the domains of risk, time preference, social preference, and strategic interactions, we ask human subjects to make choices for themselves and to predict the choices made by GenAI on behalf of a human user. We find that people overestimate the degree of alignment between GenAI's choices and human choices. In every problem, human subjects' average prediction about GenAI's choice is substantially closer to the average human-subject choice than it is to the GenAI choice. At the individual level, different subjects' predictions about GenAI's choice in a given problem are highly correlated with their own choices in the same problem. We explore the implications of people overestimating GenAI alignment in a simple theoretical model.</article>","contentLength":1007,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TRUSWorthy: Toward Clinically Applicable Deep Learning for Confident Detection of Prostate Cancer in Micro-Ultrasound","url":"https://arxiv.org/abs/2502.14707","date":1740114000,"author":"","guid":7876,"unread":true,"content":"<article>arXiv:2502.14707v1 Announce Type: cross \nAbstract: While deep learning methods have shown great promise in improving the effectiveness of prostate cancer (PCa) diagnosis by detecting suspicious lesions from trans-rectal ultrasound (TRUS), they must overcome multiple simultaneous challenges. There is high heterogeneity in tissue appearance, significant class imbalance in favor of benign examples, and scarcity in the number and quality of ground truth annotations available to train models. Failure to address even a single one of these problems can result in unacceptable clinical outcomes.We propose TRUSWorthy, a carefully designed, tuned, and integrated system for reliable PCa detection. Our pipeline integrates self-supervised learning, multiple-instance learning aggregation using transformers, random-undersampled boosting and ensembling: these address label scarcity, weak labels, class imbalance, and overconfidence, respectively. We train and rigorously evaluate our method using a large, multi-center dataset of micro-ultrasound data. Our method outperforms previous state-of-the-art deep learning methods in terms of accuracy and uncertainty calibration, with AUROC and balanced accuracy scores of 79.9% and 71.5%, respectively. On the top 20% of predictions with the highest confidence, we can achieve a balanced accuracy of up to 91%. The success of TRUSWorthy demonstrates the potential of integrated deep learning solutions to meet clinical needs in a highly challenging deployment setting, and is a significant step towards creating a trustworthy system for computer-assisted PCa diagnosis.</article>","contentLength":1610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Confidence Estimation via Sequential Likelihood Mixing","url":"https://arxiv.org/abs/2502.14689","date":1740114000,"author":"","guid":7877,"unread":true,"content":"<article>arXiv:2502.14689v1 Announce Type: cross \nAbstract: We present a universal framework for constructing confidence sets based on sequential likelihood mixing. Building upon classical results from sequential analysis, we provide a unifying perspective on several recent lines of work, and establish fundamental connections between sequential mixing, Bayesian inference and regret inequalities from online estimation. The framework applies to any realizable family of likelihood functions and allows for non-i.i.d. data and anytime validity. Moreover, the framework seamlessly integrates standard approximate inference techniques, such as variational inference and sampling-based methods, and extends to misspecified model classes, while preserving provable coverage guarantees. We illustrate the power of the framework by deriving tighter confidence sequences for classical settings, including sequential linear regression and sparse estimation, with simplified proofs.</article>","contentLength":965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vision Foundation Models in Medical Image Analysis: Advances and Challenges","url":"https://arxiv.org/abs/2502.14584","date":1740114000,"author":"","guid":7878,"unread":true,"content":"<article>arXiv:2502.14584v1 Announce Type: cross \nAbstract: The rapid development of Vision Foundation Models (VFMs), particularly Vision Transformers (ViT) and Segment Anything Model (SAM), has sparked significant advances in the field of medical image analysis. These models have demonstrated exceptional capabilities in capturing long-range dependencies and achieving high generalization in segmentation tasks. However, adapting these large models to medical image analysis presents several challenges, including domain differences between medical and natural images, the need for efficient model adaptation strategies, and the limitations of small-scale medical datasets. This paper reviews the state-of-the-art research on the adaptation of VFMs to medical image segmentation, focusing on the challenges of domain adaptation, model compression, and federated learning. We discuss the latest developments in adapter-based improvements, knowledge distillation techniques, and multi-scale contextual feature modeling, and propose future directions to overcome these bottlenecks. Our analysis highlights the potential of VFMs, along with emerging methodologies such as federated learning and model compression, to revolutionize medical image analysis and enhance clinical applications. The goal of this work is to provide a comprehensive overview of current approaches and suggest key areas for future research that can drive the next wave of innovation in medical image segmentation.</article>","contentLength":1476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalization Error of $f$-Divergence Stabilized Algorithms via Duality","url":"https://arxiv.org/abs/2502.14544","date":1740114000,"author":"","guid":7879,"unread":true,"content":"<article>arXiv:2502.14544v1 Announce Type: cross \nAbstract: The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is extended to constrained optimization problems, establishing conditions for equivalence between the solution and constraints. A dual formulation of ERM-$f$DR is introduced, providing a computationally efficient method to derive the normalization function of the ERM-$f$DR solution. This dual approach leverages the Legendre-Fenchel transform and the implicit function theorem, enabling explicit characterizations of the generalization error for general algorithms under mild conditions, and another for ERM-$f$DR solutions.</article>","contentLength":667,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Provable Quantum Algorithm Advantage for Gaussian Process Quadrature","url":"https://arxiv.org/abs/2502.14467","date":1740114000,"author":"","guid":7880,"unread":true,"content":"<article>arXiv:2502.14467v1 Announce Type: cross \nAbstract: The aim of this paper is to develop novel quantum algorithms for Gaussian process quadrature methods. Gaussian process quadratures are numerical integration methods where Gaussian processes are used as functional priors for the integrands to capture the uncertainty arising from the sparse function evaluations. Quantum computers have emerged as potential replacements for classical computers, offering exponential reductions in the computational complexity of machine learning tasks. In this paper, we combine Gaussian process quadratures and quantum computing by proposing a quantum low-rank Gaussian process quadrature method based on a Hilbert space approximation of the Gaussian process kernel and enhancing the quadrature using a quantum circuit. The method combines the quantum phase estimation algorithm with the quantum principal component analysis technique to extract information up to a desired rank. Then, Hadamard and SWAP tests are implemented to find the expected value and variance that determines the quadrature. We use numerical simulations of a quantum computer to demonstrate the effectiveness of the method. Furthermore, we provide a theoretical complexity analysis that shows a polynomial advantage over classical Gaussian process quadrature methods. The code is available at https://github.com/cagalvisf/Quantum_HSGPQ.</article>","contentLength":1393,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New estimates for character sums over sparse elements of finite fields","url":"https://arxiv.org/abs/2502.14436","date":1740114000,"author":"","guid":7881,"unread":true,"content":"<article>arXiv:2502.14436v1 Announce Type: cross \nAbstract: Let $q$ be a prime power and $r$ a positive integer. Let $\\mathbb{F}_q$ be the finite field with $q$ elements, and let $\\mathbb{F}_{q^r}$ be its extension field of degree $r$. Let $\\chi$ be a nontrivial multiplicative character of $\\mathbb{F}_{q^r}$. In this paper, we provide new estimates for the character sums $\\sum_{g\\in\\mathcal{G}}\\chi(f(g))$, where $\\mathcal{G}$ is a given sparse subsets of $\\mathbb{F}_{q^r}$ and $f(X)$ is a polynomial over $\\mathbb{F}_{q^r}$ of certain type. Specifically, by extending a sum over sparse subsets to subfields, rather than to general linear spaces, we obtain significant improvements of previous estimates.</article>","contentLength":699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Distribution Matching for Self-Supervised Transfer Learning","url":"https://arxiv.org/abs/2502.14424","date":1740114000,"author":"","guid":7882,"unread":true,"content":"<article>arXiv:2502.14424v1 Announce Type: cross \nAbstract: In this paper, we propose a novel self-supervised transfer learning method called Distribution Matching (DM), which drives the representation distribution toward a predefined reference distribution while preserving augmentation invariance. The design of DM results in a learned representation space that is intuitively structured and offers easily interpretable hyperparameters. Experimental results across multiple real-world datasets and evaluation metrics demonstrate that DM performs competitively on target classification tasks compared to existing self-supervised transfer learning methods. Additionally, we provide robust theoretical guarantees for DM, including a population theorem and an end-to-end sample theorem. The population theorem bridges the gap between the self-supervised learning task and target classification accuracy, while the sample theorem shows that, even with a limited number of samples from the target domain, DM can deliver exceptional classification performance, provided the unlabeled sample size is sufficiently large.</article>","contentLength":1104,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Role of the Pretraining and the Adaptation data sizes for low-resource real-time MRI video segmentation","url":"https://arxiv.org/abs/2502.14418","date":1740114000,"author":"","guid":7883,"unread":true,"content":"<article>arXiv:2502.14418v1 Announce Type: cross \nAbstract: Real-time Magnetic Resonance Imaging (rtMRI) is frequently used in speech production studies as it provides a complete view of the vocal tract during articulation. This study investigates the effectiveness of rtMRI in analyzing vocal tract movements by employing the SegNet and UNet models for Air-Tissue Boundary (ATB)segmentation tasks. We conducted pretraining of a few base models using increasing numbers of subjects and videos, to assess performance on two datasets. First, consisting of unseen subjects with unseen videos from the same data source, achieving 0.33% and 0.91% (Pixel-wise Classification Accuracy (PCA) and Dice Coefficient respectively) better than its matched condition. Second, comprising unseen videos from a new data source, where we obtained an accuracy of 99.63% and 98.09% (PCA and Dice Coefficient respectively) of its matched condition performance. Here, matched condition performance refers to the performance of a model trained only on the test subjects which was set as a benchmark for the other models. Our findings highlight the significance of fine-tuning and adapting models with limited data. Notably, we demonstrated that effective model adaptation can be achieved with as few as 15 rtMRI frames from any new dataset.</article>","contentLength":1308,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reliable Explainability of Deep Learning Spatial-Spectral Classifiers for Improved Semantic Segmentation in Autonomous Driving","url":"https://arxiv.org/abs/2502.14416","date":1740114000,"author":"","guid":7884,"unread":true,"content":"<article>arXiv:2502.14416v1 Announce Type: cross \nAbstract: Integrating hyperspectral imagery (HSI) with deep neural networks (DNNs) can strengthen the accuracy of intelligent vision systems by combining spectral and spatial information, which is useful for tasks like semantic segmentation in autonomous driving. To advance research in such safety-critical systems, determining the precise contribution of spectral information to complex DNNs' output is needed. To address this, several saliency methods, such as class activation maps (CAM), have been proposed primarily for image classification. However, recent studies have raised concerns regarding their reliability. In this paper, we address their limitations and propose an alternative approach by leveraging the data provided by activations and weights from relevant DNN layers to better capture the relationship between input features and predictions. The study aims to assess the superior performance of HSI compared to 3-channel and single-channel DNNs. We also address the influence of spectral signature normalization for enhancing DNN robustness in real-world driving conditions.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sharp Phase Transitions in Estimation with Low-Degree Polynomials","url":"https://arxiv.org/abs/2502.14407","date":1740114000,"author":"","guid":7885,"unread":true,"content":"<article>arXiv:2502.14407v1 Announce Type: cross \nAbstract: High-dimensional planted problems, such as finding a hidden dense subgraph within a random graph, often exhibit a gap between statistical and computational feasibility. While recovering the hidden structure may be statistically possible, it is conjectured to be computationally intractable in certain parameter regimes. A powerful approach to understanding this hardness involves proving lower bounds on the efficacy of low-degree polynomial algorithms. We introduce new techniques for establishing such lower bounds, leading to novel results across diverse settings: planted submatrix, planted dense subgraph, the spiked Wigner model, and the stochastic block model. Notably, our results address the estimation task -- whereas most prior work is limited to hypothesis testing -- and capture sharp phase transitions such as the \"BBP\" transition in the spiked Wigner model (named for Baik, Ben Arous, and P\\'{e}ch\\'{e}) and the Kesten-Stigum threshold in the stochastic block model. Existing work on estimation either falls short of achieving these sharp thresholds or is limited to polynomials of very low (constant or logarithmic) degree. In contrast, our results rule out estimation with polynomials of degree $n^{\\delta}$ where $n$ is the dimension and $\\delta &gt; 0$ is a constant, and in some cases we pin down the optimal constant $\\delta$. Our work resolves open problems posed by Hopkins &amp; Steurer (2017) and Schramm &amp; Wein (2022), and provides rigorous support within the low-degree framework for conjectures by Abbe &amp; Sandon (2018) and Lelarge &amp; Miolane (2019).</article>","contentLength":1620,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields","url":"https://arxiv.org/abs/2502.14401","date":1740114000,"author":"","guid":7886,"unread":true,"content":"<article>arXiv:2502.14401v1 Announce Type: cross \nAbstract: Recent research in medical image analysis with deep learning almost exclusively focuses on grid- or voxel-based data representations. We challenge this common choice by introducing MedFuncta, a modality-agnostic continuous data representation based on neural fields. We demonstrate how to scale neural fields from single instances to large datasets by exploiting redundancy in medical signals and by applying an efficient meta-learning approach with a context reduction scheme. We further address the spectral bias in commonly used SIREN activations, by introducing an $\\omega_0$-schedule, improving reconstruction quality and convergence speed. We validate our proposed approach on a large variety of medical signals of different dimensions and modalities (1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, Colon Histopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfully demonstrate that we can solve relevant downstream tasks on these representations. We additionally release a large-scale dataset of &gt; 550k annotated neural fields to promote research in this direction.</article>","contentLength":1157,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning","url":"https://arxiv.org/abs/2502.14372","date":1740114000,"author":"","guid":7887,"unread":true,"content":"<article>arXiv:2502.14372v1 Announce Type: cross \nAbstract: The realization of scalable fault-tolerant quantum computing is expected to hinge on quantum error-correcting codes. In the quest for more efficient quantum fault tolerance, a critical code parameter is the weight of measurements that extract information about errors to enable error correction: as higher measurement weights require higher implementation costs and introduce more errors, it is important in code design to optimize measurement weight. This underlies the surging interest in quantum low-density parity-check (qLDPC) codes, the study of which has primarily focused on the asymptotic (large-code-limit) properties. In this work, we introduce a versatile and computationally efficient approach to stabilizer code weight reduction based on reinforcement learning (RL), which produces new low-weight codes that substantially outperform the state of the art in practically relevant parameter regimes, extending significantly beyond previously accessible small distances. For example, our approach demonstrates savings in physical qubit overhead compared to existing results by 1 to 2 orders of magnitude for weight 6 codes and brings the overhead into a feasible range for near-future experiments. We also investigate the interplay between code parameters using our RL framework, offering new insights into the potential efficiency and power of practically viable coding strategies. Overall, our results demonstrate how RL can effectively advance the crucial yet challenging problem of quantum code discovery and thereby facilitate a faster path to the practical implementation of fault-tolerant quantum technologies.</article>","contentLength":1678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Topology-Aware Wavelet Mamba for Airway Structure Segmentation in Postoperative Recurrent Nasopharyngeal Carcinoma CT Scans","url":"https://arxiv.org/abs/2502.14363","date":1740114000,"author":"","guid":7888,"unread":true,"content":"<article>arXiv:2502.14363v1 Announce Type: cross \nAbstract: Nasopharyngeal carcinoma (NPC) patients often undergo radiotherapy and chemotherapy, which can lead to postoperative complications such as limited mouth opening and joint stiffness, particularly in recurrent cases that require re-surgery. These complications can affect airway function, making accurate postoperative airway risk assessment essential for managing patient care. Accurate segmentation of airway-related structures in postoperative CT scans is crucial for assessing these risks. This study introduces TopoWMamba (Topology-aware Wavelet Mamba), a novel segmentation model specifically designed to address the challenges of postoperative airway risk evaluation in recurrent NPC patients. TopoWMamba combines wavelet-based multi-scale feature extraction, state-space sequence modeling, and topology-aware modules to segment airway-related structures in CT scans robustly. By leveraging the Wavelet-based Mamba Block (WMB) for hierarchical frequency decomposition and the Snake Conv VSS (SCVSS) module to preserve anatomical continuity, TopoWMamba effectively captures both fine-grained boundaries and global structural context, crucial for accurate segmentation in complex postoperative scenarios. Through extensive testing on the NPCSegCT dataset, TopoWMamba achieves an average Dice score of 88.02%, outperforming existing models such as UNet, Attention UNet, and SwinUNet. Additionally, TopoWMamba is tested on the SegRap 2023 Challenge dataset, where it shows a significant improvement in trachea segmentation with a Dice score of 95.26%. The proposed model provides a strong foundation for automated segmentation, enabling more accurate postoperative airway risk evaluation.</article>","contentLength":1740,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Purest Quantum State Identification","url":"https://arxiv.org/abs/2502.14334","date":1740114000,"author":"","guid":7889,"unread":true,"content":"<article>arXiv:2502.14334v1 Announce Type: cross \nAbstract: Precise identification of quantum states under noise constraints is essential for quantum information processing. In this study, we generalize the classical best arm identification problem to quantum domains, designing methods for identifying the purest one within $K$ unknown $n$-qubit quantum states using $N$ samples. %, with direct applications in quantum computation and quantum communication. We propose two distinct algorithms: (1) an algorithm employing incoherent measurements, achieving error $\\exp\\left(- \\Omega\\left(\\frac{N H_1}{\\log(K) 2^n }\\right) \\right)$, and (2) an algorithm utilizing coherent measurements, achieving error $\\exp\\left(- \\Omega\\left(\\frac{N H_2}{\\log(K) }\\right) \\right)$, highlighting the power of quantum memory. Furthermore, we establish a lower bound by proving that all strategies with fixed two-outcome incoherent POVM must suffer error probability exceeding $ \\exp\\left( - O\\left(\\frac{NH_1}{2^n}\\right)\\right)$. This framework provides concrete design principles for overcoming sampling bottlenecks in quantum technologies.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On linguistic subsets of groups and monoids","url":"https://arxiv.org/abs/2502.14329","date":1740114000,"author":"","guid":7890,"unread":true,"content":"<article>arXiv:2502.14329v1 Announce Type: cross \nAbstract: We study subsets of groups and monoids defined by language-theoretic means, generalizing the classical approach to the word problem. We expand on results by Herbst from 1991 to a more general setting, and for a class of languages $\\mathbf{C}$ we define the classes of $\\mathbf{C}^\\forall$-flat and $\\mathbf{C}^\\exists$-flat groups. We prove several closure results for these classes of groups, prove a connection with the word problem, and characterize $\\mathbf{C}^\\forall$-flat groups for several classes of languages. In general, we prove that the class of $\\mathbf{C}^\\forall$-flat groups is a strict subclass of the class of groups with word problem in $\\mathbf{C}$, including for the class $\\mathrm{REC}$ of recursive languages, for which $\\mathbf{C}^\\forall$-flatness for a group resp. monoid is proved to be equivalent to the decidability of the subgroup membership problem resp. the submonoid membership problem. We provide a number of examples, including the Tarski monsters of Ol'shanskii, showing the difficulty of characterizing $\\mathbf{C}^\\exists$-flat groups. As an application of our general methods, we also prove in passing that if $\\mathbf{C}$ is a full semi-$\\mathrm{AFL}$, then the class of epi-$\\mathbf{C}$ groups is closed under taking finite index subgroups. This answers a question recently posed by Al Kohli, Bleak &amp; Elliot.</article>","contentLength":1401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EyeBench: A Call for More Rigorous Evaluation of Retinal Image Enhancement","url":"https://arxiv.org/abs/2502.14260","date":1740114000,"author":"","guid":7891,"unread":true,"content":"<article>arXiv:2502.14260v1 Announce Type: cross \nAbstract: Over the past decade, generative models have achieved significant success in enhancement fundus images.However, the evaluation of these models still presents a considerable challenge. A comprehensive evaluation benchmark for fundus image enhancement is indispensable for three main reasons: 1) The existing denoising metrics (e.g., PSNR, SSIM) are hardly to extend to downstream real-world clinical research (e.g., Vessel morphology consistency). 2) There is a lack of comprehensive evaluation for both paired and unpaired enhancement methods, along with the need for expert protocols to accurately assess clinical value. 3) An ideal evaluation system should provide insights to inform future developments of fundus image enhancement. To this end, we propose a novel comprehensive benchmark, EyeBench, to provide insights that align enhancement models with clinical needs, offering a foundation for future work to improve the clinical relevance and applicability of generative models for fundus image enhancement. EyeBench has three appealing properties: 1) multi-dimensional clinical alignment downstream evaluation: In addition to evaluating the enhancement task, we provide several clinically significant downstream tasks for fundus images, including vessel segmentation, DR grading, denoising generalization, and lesion segmentation. 2) Medical expert-guided evaluation design: We introduce a novel dataset that promote comprehensive and fair comparisons between paired and unpaired methods and includes a manual evaluation protocol by medical experts. 3) Valuable insights: Our benchmark study provides a comprehensive and rigorous evaluation of existing methods across different downstream tasks, assisting medical experts in making informed choices. Additionally, we offer further analysis of the challenges faced by existing methods. The code is available at \\url{https://github.com/Retinal-Research/EyeBench}</article>","contentLength":1968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards efficient quantum algorithms for diffusion probability models","url":"https://arxiv.org/abs/2502.14252","date":1740114000,"author":"","guid":7892,"unread":true,"content":"<article>arXiv:2502.14252v1 Announce Type: cross \nAbstract: A diffusion probabilistic model (DPM) is a generative model renowned for its ability to produce high-quality outputs in tasks such as image and audio generation. However, training DPMs on large, high-dimensional datasets such as high-resolution images or audio incurs significant computational, energy, and hardware costs. In this work, we introduce efficient quantum algorithms for implementing DPMs through various quantum ODE solvers. These algorithms highlight the potential of quantum Carleman linearization for diverse mathematical structures, leveraging state-of-the-art quantum linear system solvers (QLSS) or linear combination of Hamiltonian simulations (LCHS). Specifically, we focus on two approaches: DPM-solver-$k$ which employs exact $k$-th order derivatives to compute a polynomial approximation of $\\epsilon_\\theta(x_\\lambda,\\lambda)$; and UniPC which uses finite difference of $\\epsilon_\\theta(x_\\lambda,\\lambda)$ at different points $(x_{s_m}, \\lambda_{s_m})$ to approximate higher-order derivatives. As such, this work represents one of the most direct and pragmatic applications of quantum algorithms to large-scale machine learning models, presumably talking substantial steps towards demonstrating the practical utility of quantum computing.</article>","contentLength":1315,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bayesian Parameter Inference and Uncertainty Quantification for a Computational Pulmonary Hemodynamics Model Using Gaussian Processes","url":"https://arxiv.org/abs/2502.14251","date":1740114000,"author":"","guid":7893,"unread":true,"content":"<article>arXiv:2502.14251v1 Announce Type: cross \nAbstract: Patient-specific modeling is a valuable tool in cardiovascular disease research, offering insights beyond what current clinical equipment can measure. Given the limitations of available clinical data, models that incorporate uncertainty can provide clinicians with better guidance for tailored treatments. However, such modeling must align with clinical time frameworks to ensure practical applicability. In this study, we employ a one-dimensional fluid dynamics model integrated with data from a canine model of chronic thromboembolic pulmonary hypertension (CTEPH) to investigate microvascular disease, which is believed to involve complex mechanisms. To enhance computational efficiency during model calibration, we implement a Gaussian process emulator. This approach enables us to explore the relationship between disease severity and microvascular parameters, offering new insights into the progression and treatment of CTEPH in a timeframe that is compatible with a reasonable clinical timeframe.</article>","contentLength":1054,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Complexity of Local Stoquastic Hamiltonians on 2D Lattices","url":"https://arxiv.org/abs/2502.14244","date":1740114000,"author":"","guid":7894,"unread":true,"content":"<article>arXiv:2502.14244v1 Announce Type: cross \nAbstract: We show the 2-Local Stoquastic Hamiltonian problem on a 2D square lattice is StoqMA-complete. We achieve this by extending the spatially sparse circuit construction of Oliveira and Terhal, as well as the perturbative gadgets of Bravyi, DiVincenzo, Oliveira, and Terhal. Our main contributions demonstrate StoqMA circuits can be made spatially sparse and that geometrical, stoquastic-preserving, perturbative gadgets can be constructed.</article>","contentLength":486,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OBELiX: A Curated Dataset of Crystal Structures and Experimentally Measured Ionic Conductivities for Lithium Solid-State Electrolytes","url":"https://arxiv.org/abs/2502.14234","date":1740114000,"author":"","guid":7895,"unread":true,"content":"<article>arXiv:2502.14234v1 Announce Type: cross \nAbstract: Solid-state electrolyte batteries are expected to replace liquid electrolyte lithium-ion batteries in the near future thanks to their higher theoretical energy density and improved safety. However, their adoption is currently hindered by their lower effective ionic conductivity, a quantity that governs charge and discharge rates. Identifying highly ion-conductive materials using conventional theoretical calculations and experimental validation is both time-consuming and resource-intensive. While machine learning holds the promise to expedite this process, relevant ionic conductivity and structural data is scarce. Here, we present OBELiX, a domain-expert-curated database of $\\sim$600 synthesized solid electrolyte materials and their experimentally measured room temperature ionic conductivities gathered from literature. Each material is described by their measured composition, space group and lattice parameters. A full-crystal description in the form of a crystallographic information file (CIF) is provided for ~320 structures for which atomic positions were available. We discuss various statistics and features of the dataset and provide training and testing splits that avoid data leakage. Finally, we benchmark seven existing ML models on the task of predicting ionic conductivity and discuss their performance. The goal of this work is to facilitate the use of machine learning for solid-state electrolyte materials discovery.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Convolution for CNN-based Speech Enhancement Models","url":"https://arxiv.org/abs/2502.14224","date":1740114000,"author":"","guid":7896,"unread":true,"content":"<article>arXiv:2502.14224v1 Announce Type: cross \nAbstract: Deep learning-based speech enhancement methods have significantly improved speech quality and intelligibility. Convolutional neural networks (CNNs) have been proven to be essential components of many high-performance models. In this paper, we introduce adaptive convolution, an efficient and versatile convolutional module that enhances the model's capability to adaptively represent speech signals. Adaptive convolution performs frame-wise causal dynamic convolution, generating time-varying kernels for each frame by assembling multiple parallel candidate kernels. A Lightweight attention mechanism leverages both current and historical information to assign adaptive weights to each candidate kernel, guiding their aggregation. This enables the convolution operation to adapt to frame-level speech spectral features, leading to more efficient extraction and reconstruction. Experimental results on various CNN-based models demonstrate that adaptive convolution significantly improves the performance with negligible increases in computational complexity, especially for lightweight models. Furthermore, we propose the adaptive convolutional recurrent network (AdaptCRN), an ultra-lightweight model that incorporates adaptive convolution and an efficient encoder-decoder design, achieving superior performance compared to models with similar or even higher computational costs.</article>","contentLength":1430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sample Complexity of Linear Quadratic Regulator Without Initial Stability","url":"https://arxiv.org/abs/2502.14210","date":1740114000,"author":"","guid":7897,"unread":true,"content":"<article>arXiv:2502.14210v1 Announce Type: cross \nAbstract: Inspired by REINFORCE, we introduce a novel receding-horizon algorithm for the Linear Quadratic Regulator (LQR) problem with unknown parameters. Unlike prior methods, our algorithm avoids reliance on two-point gradient estimates while maintaining the same order of sample complexity. Furthermore, it eliminates the restrictive requirement of starting with a stable initial policy, broadening its applicability. Beyond these improvements, we introduce a refined analysis of error propagation through the contraction of the Riemannian distance over the Riccati operator. This refinement leads to a better sample complexity and ensures improved convergence guarantees. Numerical simulations validate the theoretical results, demonstrating the method's practical feasibility and performance in realistic scenarios.</article>","contentLength":861,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weighted Low-rank Approximation via Stochastic Gradient Descent on Manifolds","url":"https://arxiv.org/abs/2502.14174","date":1740114000,"author":"","guid":7898,"unread":true,"content":"<article>arXiv:2502.14174v1 Announce Type: cross \nAbstract: We solve a regularized weighted low-rank approximation problem by a stochastic gradient descent on a manifold. To guarantee the convergence of our stochastic gradient descent, we establish a convergence theorem on manifolds for retraction-based stochastic gradient descents admitting confinements. On sample data from the Netflix Prize training dataset, our algorithm outperforms the existing stochastic gradient descent on Euclidean spaces. We also compare the accelerated line search on this manifold to the existing accelerated line search on Euclidean spaces.</article>","contentLength":614,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finite Sample Analysis of Distributional TD Learning with Linear Function Approximation","url":"https://arxiv.org/abs/2502.14172","date":1740114000,"author":"","guid":7899,"unread":true,"content":"<article>arXiv:2502.14172v1 Announce Type: cross \nAbstract: In this paper, we investigate the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The aim of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy {\\pi}. Prior works on statistical analysis of distributional TD learning mainly focus on the tabular case. In contrast, we first consider the linear function approximation setting and derive sharp finite-sample rates. Our theoretical results demonstrate that the sample complexity of linear distributional TD learning matches that of the classic linear TD learning. This implies that, with linear function approximation, learning the full distribution of the return using streaming data is no more difficult than learning its expectation (i.e. the value function). To derive tight sample complexity bounds, we conduct a fine-grained analysis of the linear-categorical Bellman equation, and employ the exponential stability arguments for products of random matrices. Our findings provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.</article>","contentLength":1225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prediction-Powered Adaptive Shrinkage Estimation","url":"https://arxiv.org/abs/2502.14166","date":1740114000,"author":"","guid":7900,"unread":true,"content":"<article>arXiv:2502.14166v1 Announce Type: cross \nAbstract: Prediction-Powered Inference (PPI) is a powerful framework for enhancing statistical estimates by combining limited gold-standard data with machine learning (ML) predictions. While prior work has demonstrated PPI's benefits for individual statistical tasks, modern applications require answering numerous parallel statistical questions. We introduce Prediction-Powered Adaptive Shrinkage (PAS), a method that bridges PPI with empirical Bayes shrinkage to improve the estimation of multiple means. PAS debiases noisy ML predictions within each task and then borrows strength across tasks by using those same predictions as a reference point for shrinkage. The amount of shrinkage is determined by minimizing an unbiased estimate of risk, and we prove that this tuning strategy is asymptotically optimal. Experiments on both synthetic and real-world datasets show that PAS adapts to the reliability of the ML predictions and outperforms traditional and modern baselines in large-scale applications.</article>","contentLength":1047,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Objective Bayesian Optimization for Networked Black-Box Systems: A Path to Greener Profits and Smarter Designs","url":"https://arxiv.org/abs/2502.14121","date":1740114000,"author":"","guid":7901,"unread":true,"content":"<article>arXiv:2502.14121v1 Announce Type: cross \nAbstract: Designing modern industrial systems requires balancing several competing objectives, such as profitability, resilience, and sustainability, while accounting for complex interactions between technological, economic, and environmental factors. Multi-objective optimization (MOO) methods are commonly used to navigate these tradeoffs, but selecting the appropriate algorithm to tackle these problems is often unclear, particularly when system representations vary from fully equation-based (white-box) to entirely data-driven (black-box) models. While grey-box MOO methods attempt to bridge this gap, they typically impose rigid assumptions on system structure, requiring models to conform to the underlying structural assumptions of the solver rather than the solver adapting to the natural representation of the system of interest. In this chapter, we introduce a unifying approach to grey-box MOO by leveraging network representations, which provide a general and flexible framework for modeling interconnected systems as a series of function nodes that share various inputs and outputs. Specifically, we propose MOBONS, a novel Bayesian optimization-inspired algorithm that can efficiently optimize general function networks, including those with cyclic dependencies, enabling the modeling of feedback loops, recycle streams, and multi-scale simulations - features that existing methods fail to capture. Furthermore, MOBONS incorporates constraints, supports parallel evaluations, and preserves the sample efficiency of Bayesian optimization while leveraging network structure for improved scalability. We demonstrate the effectiveness of MOBONS through two case studies, including one related to sustainable process design. By enabling efficient MOO under general graph representations, MOBONS has the potential to significantly enhance the design of more profitable, resilient, and sustainable engineering systems.</article>","contentLength":1968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Conformal Prediction under L\\'evy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations","url":"https://arxiv.org/abs/2502.14105","date":1740114000,"author":"","guid":7902,"unread":true,"content":"<article>arXiv:2502.14105v1 Announce Type: cross \nAbstract: Conformal prediction provides a powerful framework for constructing prediction intervals with finite-sample guarantees, yet its robustness under distribution shifts remains a significant challenge. This paper addresses this limitation by modeling distribution shifts using L\\'evy-Prokhorov (LP) ambiguity sets, which capture both local and global perturbations. We provide a self-contained overview of LP ambiguity sets and their connections to popular metrics such as Wasserstein and Total Variation. We show that the link between conformal prediction and LP ambiguity sets is a natural one: by propagating the LP ambiguity set through the scoring function, we reduce complex high-dimensional distribution shifts to manageable one-dimensional distribution shifts, enabling exact quantification of worst-case quantiles and coverage. Building on this analysis, we construct robust conformal prediction intervals that remain valid under distribution shifts, explicitly linking LP parameters to interval width and confidence levels. Experimental results on real-world datasets demonstrate the effectiveness of the proposed approach.</article>","contentLength":1180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MambaLiteSR: Image Super-Resolution with Low-Rank Mamba using Knowledge Distillation","url":"https://arxiv.org/abs/2502.14090","date":1740114000,"author":"","guid":7903,"unread":true,"content":"<article>arXiv:2502.14090v1 Announce Type: cross \nAbstract: Generative Artificial Intelligence (AI) has gained significant attention in recent years, revolutionizing various applications across industries. Among these, advanced vision models for image super-resolution are in high demand, particularly for deployment on edge devices where real-time processing is crucial. However, deploying such models on edge devices is challenging due to limited computing power and memory. In this paper, we present MambaLiteSR, a novel lightweight image Super-Resolution (SR) model that utilizes the architecture of Vision Mamba. It integrates State Space Blocks and a reconstruction module for efficient feature extraction. To optimize efficiency without affecting performance, MambaLiteSR employs knowledge distillation to transfer key insights from a larger Mamba-based teacher model to a smaller student model via hyperparameter tuning. Through mathematical analysis of model parameters and their impact on PSNR, we identify key factors and adjust them accordingly. Our comprehensive evaluation shows that MambaLiteSR outperforms state-of-the-art edge SR methods by reducing power consumption while maintaining competitive PSNR and SSIM scores across benchmark datasets. It also reduces power usage during training via low-rank approximation. Moreover, MambaLiteSR reduces parameters with minimal performance loss, enabling efficient deployment of generative AI models on resource-constrained devices. Deployment on the embedded NVIDIA Jetson Orin Nano confirms the superior balance of MambaLiteSR size, latency, and efficiency. Experiments show that MambaLiteSR achieves performance comparable to both the baseline and other edge models while using 15% fewer parameters. It also improves power consumption by up to 58% compared to state-of-the-art SR edge models, all while maintaining low energy use during training.</article>","contentLength":1901,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Population Dynamics Control with Partial Observations","url":"https://arxiv.org/abs/2502.14079","date":1740114000,"author":"","guid":7904,"unread":true,"content":"<article>arXiv:2502.14079v1 Announce Type: cross \nAbstract: We study the problem of controlling population dynamics, a class of linear dynamical systems evolving on the probability simplex, from the perspective of online non-stochastic control. While Golowich et.al. 2024 analyzed the fully observable setting, we focus on the more realistic, partially observable case, where only a low-dimensional representation of the state is accessible.\n  In classical non-stochastic control, inputs are set as linear combinations of past disturbances. However, under partial observations, disturbances cannot be directly computed. To address this, Simchowitz et.al. 2020 proposed to construct oblivious signals, which are counterfactual observations with zero control, as a substitute. This raises several challenges in our setting: (1) how to construct oblivious signals under simplex constraints, where zero control is infeasible; (2) how to design a sufficiently expressive convex controller parameterization tailored to these signals; and (3) how to enforce the simplex constraint on control when projections may break the convexity of cost functions.\n  Our main contribution is a new controller that achieves the optimal $\\tilde{O}(\\sqrt{T})$ regret with respect to a natural class of mixing linear dynamic controllers. To tackle these challenges, we construct signals based on hypothetical observations under a constant control adapted to the simplex domain, and introduce a new controller parameterization that approximates general control policies linear in non-oblivious observations. Furthermore, we employ a novel convex extension surrogate loss, inspired by Lattimore 2024, to bypass the projection-induced convexity issue.</article>","contentLength":1715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Lower Bounds for Stochastic Non-Convex Optimization through Divergence Composition","url":"https://arxiv.org/abs/2502.14060","date":1740114000,"author":"","guid":7905,"unread":true,"content":"<article>arXiv:2502.14060v1 Announce Type: cross \nAbstract: We study fundamental limits of first-order stochastic optimization in a range of nonconvex settings, including L-smooth functions satisfying Quasar-Convexity (QC), Quadratic Growth (QG), and Restricted Secant Inequalities (RSI). While the convergence properties of standard algorithms are well-understood in deterministic regimes, significantly fewer results address the stochastic case, where only unbiased and noisy gradients are available. We establish new lower bounds on the number of noisy gradient queries to minimize these classes of functions, also showing that they are tight (up to a logarithmic factor) in all the relevant quantities characterizing each class. Our approach reformulates the optimization task as a function identification problem, leveraging divergence composition arguments to construct a challenging subclass that leads to sharp lower bounds. Furthermore, we present a specialized algorithm in the one-dimensional setting that achieves faster rates, suggesting that certain dimensional thresholds are intrinsic to the complexity of non-convex stochastic optimization.</article>","contentLength":1148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking Self-Supervised Methods for Accelerated MRI Reconstruction","url":"https://arxiv.org/abs/2502.14009","date":1740114000,"author":"","guid":7906,"unread":true,"content":"<article>arXiv:2502.14009v1 Announce Type: cross \nAbstract: Reconstructing MRI from highly undersampled measurements is crucial for accelerating medical imaging, but is challenging due to the ill-posedness of the inverse problem. While supervised deep learning approaches have shown remarkable success, they rely on fully-sampled ground truth data, which is often impractical or impossible to obtain. Recently, numerous self-supervised methods have emerged that do not require ground truth, however, the lack of systematic comparison and standard experimental setups have hindered research. We present the first comprehensive review of loss functions from all feedforward self-supervised methods and the first benchmark on accelerated MRI reconstruction without ground truth, showing that there is a wide range in performance across methods. In addition, we propose Multi-Operator Equivariant Imaging (MO-EI), a novel framework that builds on the imaging model considered in existing methods to outperform all state-of-the-art and approaches supervised performance. Finally, to facilitate reproducible benchmarking, we provide implementations of all methods in the DeepInverse library (https://deepinv.github.io) and easy-to-use demo code at https://andrewwango.github.io/deepinv-selfsup-fastmri.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards a perturbation-based explanation for medical AI as differentiable programs","url":"https://arxiv.org/abs/2502.14001","date":1740114000,"author":"","guid":7907,"unread":true,"content":"<article>arXiv:2502.14001v1 Announce Type: cross \nAbstract: Recent advancement in machine learning algorithms reaches a point where medical devices can be equipped with artificial intelligence (AI) models for diagnostic support and routine automation in clinical settings. In medicine and healthcare, there is a particular demand for sufficient and objective explainability of the outcome generated by AI models. However, AI models are generally considered as black boxes due to their complexity, and the computational process leading to their response is often opaque. Although several methods have been proposed to explain the behavior of models by evaluating the importance of each feature in discrimination and prediction, they may suffer from biases and opacities arising from the scale and sampling protocol of the dataset used for training or testing. To overcome the shortcomings of existing methods, we explore an alternative approach to provide an objective explanation of AI models that can be defined independently of the learning process and does not require additional data. As a preliminary study for this direction of research, this work examines a numerical availability of the Jacobian matrix of deep learning models that measures how stably a model responses against small perturbations added to the input. The indicator, if available, are calculated from a trained AI model for a given target input. This is a first step towards a perturbation-based explanation, which will assist medical practitioners in understanding and interpreting the response of the AI model in its clinical application.</article>","contentLength":1605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Baseline Method for Removing Invisible Image Watermarks using Deep Image Prior","url":"https://arxiv.org/abs/2502.13998","date":1740114000,"author":"","guid":7908,"unread":true,"content":"<article>arXiv:2502.13998v1 Announce Type: cross \nAbstract: Image watermarks have been considered a promising technique to help detect AI-generated content, which can be used to protect copyright or prevent fake image abuse. In this work, we present a black-box method for removing invisible image watermarks, without the need of any dataset of watermarked images or any knowledge about the watermark system. Our approach is simple to implement: given a single watermarked image, we regress it by deep image prior (DIP). We show that from the intermediate steps of DIP one can reliably find an evasion image that can remove invisible watermarks while preserving high image quality. Due to its unique working mechanism and practical effectiveness, we advocate including DIP as a baseline invasion method for benchmarking the robustness of watermarking systems. Finally, by showing the limited ability of DIP and other existing black-box methods in evading training-based visible watermarks, we discuss the positive implications on the practical use of training-based visible watermarks to prevent misinformation abuse.</article>","contentLength":1108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning to Discover Regulatory Elements for Gene Expression Prediction","url":"https://arxiv.org/abs/2502.13991","date":1740114000,"author":"","guid":7909,"unread":true,"content":"<article>arXiv:2502.13991v1 Announce Type: cross \nAbstract: We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. The source code is released as part of the AIRS library (https://github.com/divelab/AIRS/).</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Remote Sensing Semantic Segmentation Quality Assessment based on Vision Language Model","url":"https://arxiv.org/abs/2502.13990","date":1740114000,"author":"","guid":7910,"unread":true,"content":"<article>arXiv:2502.13990v1 Announce Type: cross \nAbstract: The complexity of scenes and variations in image quality result in significant variability in the performance of semantic segmentation methods of remote sensing imagery (RSI) in supervised real-world scenarios. This makes the evaluation of semantic segmentation quality in such scenarios an issue to be resolved. However, most of the existing evaluation metrics are developed based on expert-labeled object-level annotations, which are not applicable in such scenarios. To address this issue, we propose RS-SQA, an unsupervised quality assessment model for RSI semantic segmentation based on vision language model (VLM). This framework leverages a pre-trained RS VLM for semantic understanding and utilizes intermediate features from segmentation methods to extract implicit information about segmentation quality. Specifically, we introduce CLIP-RS, a large-scale pre-trained VLM trained with purified text to reduce textual noise and capture robust semantic information in the RS domain. Feature visualizations confirm that CLIP-RS can effectively differentiate between various levels of segmentation quality. Semantic features and low-level segmentation features are effectively integrated through a semantic-guided approach to enhance evaluation accuracy. To further support the development of RS semantic segmentation quality assessment, we present RS-SQED, a dedicated dataset sampled from four major RS semantic segmentation datasets and annotated with segmentation accuracy derived from the inference results of 8 representative segmentation methods. Experimental results on the established dataset demonstrate that RS-SQA significantly outperforms state-of-the-art quality assessment models. This provides essential support for predicting segmentation accuracy and high-quality semantic segmentation interpretation, offering substantial practical value.</article>","contentLength":1913,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gesture-Aware Zero-Shot Speech Recognition for Patients with Language Disorders","url":"https://arxiv.org/abs/2502.13983","date":1740114000,"author":"","guid":7911,"unread":true,"content":"<article>arXiv:2502.13983v1 Announce Type: cross \nAbstract: Individuals with language disorders often face significant communication challenges due to their limited language processing and comprehension abilities, which also affect their interactions with voice-assisted systems that mostly rely on Automatic Speech Recognition (ASR). Despite advancements in ASR that address disfluencies, there has been little attention on integrating non-verbal communication methods, such as gestures, which individuals with language disorders substantially rely on to supplement their communication. Recognizing the need to interpret the latent meanings of visual information not captured by speech alone, we propose a gesture-aware ASR system utilizing a multimodal large language model with zero-shot learning for individuals with speech impairments. Our experiment results and analyses show that including gesture information significantly enhances semantic understanding. This study can help develop effective communication technologies, specifically designed to meet the unique needs of individuals with language impairments.</article>","contentLength":1109,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking Automatic Speech Recognition coupled LLM Modules for Medical Diagnostics","url":"https://arxiv.org/abs/2502.13982","date":1740114000,"author":"","guid":7912,"unread":true,"content":"<article>arXiv:2502.13982v1 Announce Type: cross \nAbstract: Natural Language Processing (NLP) and Voice Recognition agents are rapidly evolving healthcare by enabling efficient, accessible, and professional patient support while automating grunt work. This report serves as my self project wherein models finetuned on medical call recordings are analysed through a two-stage system: Automatic Speech Recognition (ASR) for speech transcription and a Large Language Model (LLM) for context-aware, professional responses. ASR, finetuned on phone call recordings provides generalised transcription of diverse patient speech over call, while the LLM matches transcribed text to medical diagnosis. A novel audio preprocessing strategy, is deployed to provide invariance to incoming recording/call data, laden with sufficient augmentation with noise/clipping to make the pipeline robust to the type of microphone and ambient conditions the patient might have while calling/recording.</article>","contentLength":967,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Utilizing Effective Dynamic Graph Learning to Shield Financial Stability from Risk Propagation","url":"https://arxiv.org/abs/2502.13979","date":1740114000,"author":"","guid":7913,"unread":true,"content":"<article>arXiv:2502.13979v1 Announce Type: cross \nAbstract: Financial risks can propagate across both tightly coupled temporal and spatial dimensions, posing significant threats to financial stability. Moreover, risks embedded in unlabeled data are often difficult to detect. To address these challenges, we introduce GraphShield, a novel approach with three key innovations: Enhanced Cross-Domain Infor mation Learning: We propose a dynamic graph learning module to improve information learning across temporal and spatial domains. Advanced Risk Recognition: By leveraging the clustering characteristics of risks, we construct a risk recognizing module to enhance the identification of hidden threats. Risk Propagation Visualization: We provide a visualization tool for quantifying and validating nodes that trigger widespread cascading risks. Extensive experiments on two real-world and two open-source datasets demonstrate the robust performance of our framework. Our approach represents a significant advancement in leveraging artificial intelligence to enhance financial stability, offering a powerful solution to mitigate the spread of risks within financial networks.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Regulariza\\c{c}\\~ao, aprendizagem profunda e interdisciplinaridade em problemas inversos mal-postos","url":"https://arxiv.org/abs/2502.13976","date":1740114000,"author":"","guid":7914,"unread":true,"content":"<article>arXiv:2502.13976v1 Announce Type: cross \nAbstract: In this book, written in Portuguese, we discuss what ill-posed problems are and how the regularization method is used to solve them. In the form of questions and answers, we reflect on the origins and future of regularization, relating the similarities and differences of its meaning in different areas, including inverse problems, statistics, machine learning, and deep learning.</article>","contentLength":431,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Segmentation-free integration of nuclei morphology and spatial transcriptomics for retinal images","url":"https://arxiv.org/abs/2502.13974","date":1740114000,"author":"","guid":7915,"unread":true,"content":"<article>arXiv:2502.13974v1 Announce Type: cross \nAbstract: This study introduces SEFI (SEgmentation-Free Integration), a novel method for integrating morphological features of cell nuclei with spatial transcriptomics data. Cell segmentation poses a significant challenge in the analysis of spatial transcriptomics data, as tissue-specific structural complexities and densely packed cells in certain regions make it difficult to develop a universal approach. SEFI addresses this by utilizing self-supervised learning to extract morphological features from fluorescent nuclear staining images, enhancing the clustering of gene expression data without requiring segmentation. We demonstrate SEFI on spatially resolved gene expression profiles of the developing retina, acquired using multiplexed single molecule Fluorescence In Situ Hybridization (smFISH). SEFI is publicly available at https://github.com/eduardchelebian/sefi.</article>","contentLength":916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"IncepFormerNet: A multi-scale multi-head attention network for SSVEP classification","url":"https://arxiv.org/abs/2502.13972","date":1740114000,"author":"","guid":7916,"unread":true,"content":"<article>arXiv:2502.13972v1 Announce Type: cross \nAbstract: In recent years, deep learning (DL) models have shown outstanding performance in EEG classification tasks, particularly in Steady-State Visually Evoked Potential(SSVEP)-based Brain-Computer-Interfaces(BCI)systems. DL methods have been successfully applied to SSVEP-BCI. This study proposes a new model called IncepFormerNet, which is a hybrid of the Inception and Transformer architectures. IncepFormerNet adeptly extracts multi-scale temporal information from time series data using parallel convolution kernels of varying sizes, accurately capturing the subtle variations and critical features within SSVEP signals.Furthermore, the model integrates the multi-head attention mechanism from the Transformer architecture, which not only provides insights into global dependencies but also significantly enhances the understanding and representation of complex patterns.Additionally, it takes advantage of filter bank techniques to extract features based on the spectral characteristics of SSVEP data. To validate the effectiveness of the proposed model, we conducted experiments on two public datasets, . The experimental results show that IncepFormerNet achieves an accuracy of 87.41 on Dataset 1 and 71.97 on Dataset 2 using a 1.0-second time window. To further verify the superiority of the proposed model, we compared it with other deep learning models, and the results indicate that our method achieves significantly higher accuracy than the others.The source codes in this work are available at: https://github.com/CECNL/SSVEP-DAN.</article>","contentLength":1587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging Simulation and Reality: A 3D Clustering-Based Deep Learning Model for UAV-Based RF Source Localization","url":"https://arxiv.org/abs/2502.13969","date":1740114000,"author":"","guid":7917,"unread":true,"content":"<article>arXiv:2502.13969v1 Announce Type: cross \nAbstract: Localization of radio frequency (RF) sources has critical applications, including search and rescue, jammer detection, and monitoring of hostile activities. Unmanned aerial vehicles (UAVs) offer significant advantages for RF source localization (RFSL) over terrestrial methods, leveraging autonomous 3D navigation and improved signal capture at higher altitudes. Recent advancements in deep learning (DL) have further enhanced localization accuracy, particularly for outdoor scenarios. DL models often face challenges in real-world performance, as they are typically trained on simulated datasets that fail to replicate real-world conditions fully. To address this, we first propose the Enhanced Two-Ray propagation model, reducing the simulation-to-reality gap by improving the accuracy of propagation environment modeling. For RFSL, we propose the 3D Cluster-Based RealAdaptRNet, a DL-based method leveraging 3D clustering-based feature extraction for robust localization. Experimental results demonstrate that the proposed Enhanced Two-Ray model provides superior accuracy in simulating real-world propagation scenarios compared to conventional free-space and two-ray models. Notably, the 3D Cluster-Based RealAdaptRNet, trained entirely on simulated datasets, achieves exceptional performance when validated in real-world environments using the AERPAW physical testbed, with an average localization error of 18.2 m. The proposed approach is computationally efficient, utilizing 33.5 times fewer parameters, and demonstrates strong generalization capabilities across diverse trajectories, making it highly suitable for real-world applications.</article>","contentLength":1697,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A greedy non-intrusive reduced order model for shallow water equations","url":"https://arxiv.org/abs/2002.11329","date":1740114000,"author":"","guid":7918,"unread":true,"content":"<article>arXiv:2002.11329v2 Announce Type: cross \nAbstract: In this work, we develop Non-Intrusive Reduced Order Models (NIROMs) that combine Proper Orthogonal Decomposition (POD) with a Radial Basis Function (RBF) interpolation method to construct efficient reduced order models for time-dependent problems arising in large scale environmental flow applications. The performance of the POD-RBF NIROM is compared with a traditional nonlinear POD (NPOD) model by evaluating the accuracy and robustness for test problems representative of riverine flows. Different greedy algorithms are studied in order to determine a near-optimal distribution of interpolation points for the RBF approximation. A new power-scaled residual greedy (psr-greedy) algorithm is proposed to address some of the primary drawbacks of the existing greedy approaches. The relative performances of these greedy algorithms are studied with numerical experiments using realistic two-dimensional (2D) shallow water flow applications involving coastal and riverine dynamics.</article>","contentLength":1032,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention","url":"https://arxiv.org/abs/2502.14866","date":1740114000,"author":"","guid":7919,"unread":true,"content":"<article>arXiv:2502.14866v1 Announce Type: new \nAbstract: Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently serving these long-context models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.</article>","contentLength":1481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical and Cultural Artifacts","url":"https://arxiv.org/abs/2502.14865","date":1740114000,"author":"","guid":7920,"unread":true,"content":"<article>arXiv:2502.14865v1 Announce Type: new \nAbstract: Understanding historical and cultural artifacts demands human expertise and advanced computational techniques, yet the process remains complex and time-intensive. While large multimodal models offer promising support, their evaluation and improvement require a standardized benchmark. To address this, we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning 266 distinct cultures across 10 major historical regions. Designed for AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological discoveries, TimeTravel provides a structured dataset and robust evaluation framework to assess AI models' capabilities in classification, interpretation, and historical comprehension. By integrating AI with historical research, TimeTravel fosters AI-powered tools for historians, archaeologists, researchers, and cultural tourists to extract valuable insights while ensuring technology contributes meaningfully to historical discovery and cultural heritage preservation. We evaluate contemporary AI models on TimeTravel, highlighting their strengths and identifying areas for improvement. Our goal is to establish AI as a reliable partner in preserving cultural heritage, ensuring that technological advancements contribute meaningfully to historical discovery. Our code is available at: \\url{https://github.com/mbzuai-oryx/TimeTravel}.</article>","contentLength":1417,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking Multimodal RAG through a Chart-based Document Question-Answering Generation Framework","url":"https://arxiv.org/abs/2502.14864","date":1740114000,"author":"","guid":7921,"unread":true,"content":"<article>arXiv:2502.14864v1 Announce Type: new \nAbstract: Multimodal Retrieval-Augmented Generation (MRAG) enhances reasoning capabilities by integrating external knowledge. However, existing benchmarks primarily focus on simple image-text interactions, overlooking complex visual formats like charts that are prevalent in real-world applications. In this work, we introduce a novel task, Chart-based MRAG, to address this limitation. To semi-automatically generate high-quality evaluation samples, we propose CHARt-based document question-answering GEneration (CHARGE), a framework that produces evaluation data through structured keypoint extraction, crossmodal verification, and keypoint-based generation. By combining CHARGE with expert validation, we construct Chart-MRAG Bench, a comprehensive benchmark for chart-based MRAG evaluation, featuring 4,738 question-answering pairs across 8 domains from real-world documents. Our evaluation reveals three critical limitations in current approaches: (1) unified multimodal embedding retrieval methods struggles in chart-based scenarios, (2) even with ground-truth retrieval, state-of-the-art MLLMs achieve only 58.19% Correctness and 73.87% Coverage scores, and (3) MLLMs demonstrate consistent text-over-visual modality bias during Chart-based MRAG reasoning. The CHARGE and Chart-MRAG Bench are released at https://github.com/Nomothings/CHARGE.git.</article>","contentLength":1392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Interpretable Text Embeddings and Text Similarity Explanation: A Primer","url":"https://arxiv.org/abs/2502.14862","date":1740114000,"author":"","guid":7922,"unread":true,"content":"<article>arXiv:2502.14862v1 Announce Type: new \nAbstract: Text embeddings and text embedding models are a backbone of many AI and NLP systems, particularly those involving search. However, interpretability challenges persist, especially in explaining obtained similarity scores, which is crucial for applications requiring transparency. In this paper, we give a structured overview of interpretability methods specializing in explaining those similarity scores, an emerging research area. We study the methods' individual ideas and techniques, evaluating their potential for improving interpretability of text embeddings and explaining predicted similarities.</article>","contentLength":650,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning","url":"https://arxiv.org/abs/2502.14860","date":1740114000,"author":"","guid":7923,"unread":true,"content":"<article>arXiv:2502.14860v1 Announce Type: new \nAbstract: Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decisionmaking. We present ALFA, a framework that improves LLM question-asking by (i) decomposing the notion of a \"good\" question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains.</article>","contentLength":1326,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FR-Spec: Accelerating Large-Vocabulary Language Models via Frequency-Ranked Speculative Sampling","url":"https://arxiv.org/abs/2502.14856","date":1740114000,"author":"","guid":7924,"unread":true,"content":"<article>arXiv:2502.14856v1 Announce Type: new \nAbstract: Speculative sampling has emerged as an important technique for accelerating the auto-regressive generation process of large language models (LLMs) by utilizing a draft-then-verify mechanism to produce multiple tokens per forward pass. While state-of-the-art speculative sampling methods use only a single layer and a language modeling (LM) head as the draft model to achieve impressive layer compression, their efficiency gains are substantially reduced for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens. To address this, we present FR-Spec, a frequency-ranked speculative sampling framework that optimizes draft candidate selection through vocabulary space compression. By constraining the draft search to a frequency-prioritized token subset, our method reduces LM Head computation overhead by 75% while ensuring the equivalence of the final output distribution. Experiments across multiple datasets demonstrate an average of 1.12$\\times$ speedup over the state-of-the-art speculative sampling method EAGLE-2.</article>","contentLength":1089,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prompt-to-Leaderboard","url":"https://arxiv.org/abs/2502.14855","date":1740114000,"author":"","guid":7925,"unread":true,"content":"<article>arXiv:2502.14855v1 Announce Type: new \nAbstract: Large language model (LLM) evaluations typically rely on aggregated metrics like accuracy or human preference, averaging across users and prompts. This averaging obscures user- and prompt-specific variations in model performance. To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces leaderboards specific to a prompt. The core idea is to train an LLM taking natural language prompts as input to output a vector of Bradley-Terry coefficients which are then used to predict the human preference vote. The resulting prompt-dependent leaderboards allow for unsupervised task-specific evaluation, optimal routing of queries to models, personalization, and automated evaluation of model strengths and weaknesses. Data from Chatbot Arena suggest that P2L better captures the nuanced landscape of language model performance than the averaged leaderboard. Furthermore, our findings suggest that P2L's ability to produce prompt-specific evaluations follows a power law scaling similar to that observed in LLMs themselves. In January 2025, the router we trained based on this methodology achieved the \\#1 spot in the Chatbot Arena leaderboard. Our code is available at this GitHub link: https://github.com/lmarena/p2l.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CLIPPER: Compression enables long-context synthetic data generation","url":"https://arxiv.org/abs/2502.14854","date":1740114000,"author":"","guid":7926,"unread":true,"content":"<article>arXiv:2502.14854v1 Announce Type: new \nAbstract: LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).</article>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the $H$-property for Step-graphons: Residual Case","url":"https://arxiv.org/abs/2502.14853","date":1740114000,"author":"","guid":7927,"unread":true,"content":"<article>arXiv:2502.14853v1 Announce Type: new \nAbstract: We sample graphs $G_n$ on $n$ nodes from a step-graphon and evaluate the probability that $G_n$ has a Hamiltonian decomposition in the asymptotic regime as $n\\to\\infty$. It has recently been shown that for almost all step-graphons, this probability converges to either zero or one. In this paper, we focus on the class of step-graphons such that the zero-one property does not hold. We show in this case that the limit of the probability still exists and provide an explicit expression of it.</article>","contentLength":541,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GATE: Graph-based Adaptive Tool Evolution Across Diverse Tasks","url":"https://arxiv.org/abs/2502.14848","date":1740114000,"author":"","guid":7928,"unread":true,"content":"<article>arXiv:2502.14848v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have shown great promise in tool-making, yet existing frameworks often struggle to efficiently construct reliable toolsets and are limited to single-task settings. To address these challenges, we propose GATE (Graph-based Adaptive Tool Evolution), an adaptive framework that dynamically constructs and evolves a hierarchical graph of reusable tools across multiple scenarios. We evaluate GATE on open-ended tasks (Minecraft), agent-based tasks (TextCraft, DABench), and code generation tasks (MATH, Date, TabMWP). Our results show that GATE achieves up to 4.3x faster milestone completion in Minecraft compared to the previous SOTA, and provides an average improvement of 9.23% over existing tool-making methods in code generation tasks and 10.03% in agent tasks. GATE demonstrates the power of adaptive evolution, balancing tool quantity, complexity, and functionality while maintaining high efficiency. Code and data are available at \\url{https://github.com/ayanami2003/GATE}.</article>","contentLength":1056,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Red-Teaming LLM Multi-Agent Systems via Communication Attacks","url":"https://arxiv.org/abs/2502.14847","date":1740114000,"author":"","guid":7929,"unread":true,"content":"<article>arXiv:2502.14847v1 Announce Type: new \nAbstract: Large Language Model-based Multi-Agent Systems (LLM-MAS) have revolutionized complex problem-solving capability by enabling sophisticated agent collaboration through message-based communications. While the communication framework is crucial for agent coordination, it also introduces a critical yet unexplored security vulnerability. In this work, we introduce Agent-in-the-Middle (AiTM), a novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise individual agents, AiTM demonstrates how an adversary can compromise entire multi-agent systems by only manipulating the messages passing between agents. To enable the attack under the challenges of limited control and role-restricted communication format, we develop an LLM-powered adversarial agent with a reflection mechanism that generates contextually-aware malicious instructions. Our comprehensive evaluation across various frameworks, communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures in multi-agent systems.</article>","contentLength":1257,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scaling Text-Rich Image Understanding via Code-Guided Synthetic Multimodal Data Generation","url":"https://arxiv.org/abs/2502.14846","date":1740114000,"author":"","guid":7930,"unread":true,"content":"<article>arXiv:2502.14846v1 Announce Type: new \nAbstract: Reasoning about images with rich text, such as charts and documents, is a critical application of vision-language models (VLMs). However, VLMs often struggle in these domains due to the scarcity of diverse text-rich vision-language data. To address this challenge, we present CoSyn, a framework that leverages the coding capabilities of text-only large language models (LLMs) to automatically create synthetic text-rich multimodal data. Given input text describing a target domain (e.g., \"nutrition fact labels\"), CoSyn prompts an LLM to generate code (Python, HTML, LaTeX, etc.) for rendering synthetic images. With the underlying code as textual representations of the synthetic images, CoSyn can generate high-quality instruction-tuning data, again relying on a text-only LLM. Using CoSyn, we constructed a dataset comprising 400K images and 2.7M rows of vision-language instruction-tuning data. Comprehensive experiments on seven benchmarks demonstrate that models trained on our synthetic data achieve state-of-the-art performance among competitive open-source models, including Llama 3.2, and surpass proprietary models such as GPT-4V and Gemini 1.5 Flash. Furthermore, CoSyn can produce synthetic pointing data, enabling VLMs to ground information within input images, showcasing its potential for developing multimodal agents capable of acting in real-world environments.</article>","contentLength":1428,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Concepts Personalization from Single Videos","url":"https://arxiv.org/abs/2502.14844","date":1740114000,"author":"","guid":7931,"unread":true,"content":"<article>arXiv:2502.14844v1 Announce Type: new \nAbstract: Personalizing generative text-to-image models has seen remarkable progress, but extending this personalization to text-to-video models presents unique challenges. Unlike static concepts, personalizing text-to-video models has the potential to capture dynamic concepts, i.e., entities defined not only by their appearance but also by their motion. In this paper, we introduce Set-and-Sequence, a novel framework for personalizing Diffusion Transformers (DiTs)-based generative video models with dynamic concepts. Our approach imposes a spatio-temporal weight space within an architecture that does not explicitly separate spatial and temporal features. This is achieved in two key stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an unordered set of frames from the video to learn an identity LoRA basis that represents the appearance, free from temporal interference. In the second stage, with the identity LoRAs frozen, we augment their coefficients with Motion Residuals and fine-tune them on the full video sequence, capturing motion dynamics. Our Set-and-Sequence framework results in a spatio-temporal weight space that effectively embeds dynamic concepts into the video model's output domain, enabling unprecedented editability and compositionality while setting a new benchmark for personalizing dynamic concepts.</article>","contentLength":1388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generating $\\pi$-Functional Molecules Using STGG+ with Active Learning","url":"https://arxiv.org/abs/2502.14842","date":1740114000,"author":"","guid":7932,"unread":true,"content":"<article>arXiv:2502.14842v1 Announce Type: new \nAbstract: Generating novel molecules with out-of-distribution properties is a major challenge in molecular discovery. While supervised learning methods generate high-quality molecules similar to those in a dataset, they struggle to generalize to out-of-distribution properties. Reinforcement learning can explore new chemical spaces but often conducts 'reward-hacking' and generates non-synthesizable molecules. In this work, we address this problem by integrating a state-of-the-art supervised learning method, STGG+, in an active learning loop. Our approach iteratively generates, evaluates, and fine-tunes STGG+ to continuously expand its knowledge. We denote this approach STGG+AL. We apply STGG+AL to the design of organic $\\pi$-functional materials, specifically two challenging tasks: 1) generating highly absorptive molecules characterized by high oscillator strength and 2) designing absorptive molecules with reasonable oscillator strength in the near-infrared (NIR) range. The generated molecules are validated and rationalized in-silico with time-dependent density functional theory. Our results demonstrate that our method is highly effective in generating novel molecules with high oscillator strength, contrary to existing methods such as reinforcement learning (RL) methods. We open-source our active-learning code along with our Conjugated-xTB dataset containing 2.9 million $\\pi$-conjugated molecules and the function for approximating the oscillator strength and absorption wavelength (based on sTDA-xTB).</article>","contentLength":1563,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning","url":"https://arxiv.org/abs/2502.14840","date":1740114000,"author":"","guid":7933,"unread":true,"content":"<article>arXiv:2502.14840v1 Announce Type: new \nAbstract: Given inputs of diverse soil characteristics and climate data gathered from various regions, we aimed to build a model to predict accurate land emissions. The problem is important since accurate quantification of the carbon cycle in agroecosystems is crucial for mitigating climate change and ensuring sustainable food production. Predicting accurate land emissions is challenging since calibrating the heterogeneous nature of soil properties, moisture, and environmental conditions is hard at decision-relevant scales. Traditional approaches do not adequately estimate land emissions due to location-independent parameters failing to leverage the spatial heterogeneity and also require large datasets. To overcome these limitations, we proposed Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML), which leverages location-dependent parameters that account for significant spatial heterogeneity in soil moisture from multiple sites within the same region. Experimental results demonstrate that SDSA-KGML models achieve higher local accuracy for the specified states in the Midwest Region.</article>","contentLength":1165,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revealing and Mitigating Over-Attention in Knowledge Editing","url":"https://arxiv.org/abs/2502.14838","date":1740114000,"author":"","guid":7934,"unread":true,"content":"<article>arXiv:2502.14838v1 Announce Type: new \nAbstract: Large Language Models have demonstrated superior performance across a wide range of tasks, but they still exhibit undesirable errors due to incorrect knowledge learned from the training data. To avoid this, knowledge editing methods emerged to precisely edit the specific model knowledge via efficiently modifying a very small percentage of parameters. % However, those methods can lead to the problem of Specificity Failure: when the content related to the edited knowledge occurs in the context, it can inadvertently corrupt other pre-existing knowledge. However, those methods can lead to the problem of Specificity Failure, where the existing knowledge and capabilities are severely degraded due to editing. Our preliminary indicates that Specificity Failure primarily stems from the model's attention heads assigning excessive attention scores to entities related to the edited knowledge, thereby unduly focusing on specific snippets within the context, which we denote as the Attention Drift phenomenon. To mitigate such Attention Drift issue, we introduce a simple yet effective method Selective Attention Drift Restriction}(SADR), which introduces an additional regularization term during the knowledge editing process to restrict changes in the attention weight distribution, thereby preventing undue focus on the edited entity. Experiments on five frequently used strong LLMs demonstrate the effectiveness of our method, where SADR can significantly mitigate Specificity Failure in the predominant knowledge editing tasks.</article>","contentLength":1581,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Economical Inference: Enabling DeepSeek's Multi-Head Latent Attention in Any Transformer-based LLMs","url":"https://arxiv.org/abs/2502.14837","date":1740114000,"author":"","guid":7935,"unread":true,"content":"<article>arXiv:2502.14837v1 Announce Type: new \nAbstract: Multi-head Latent Attention (MLA) is an innovative architecture proposed by DeepSeek, designed to ensure efficient and economical inference by significantly compressing the Key-Value (KV) cache into a latent vector. Compared to MLA, standard LLMs employing Multi-Head Attention (MHA) and its variants such as Grouped-Query Attention (GQA) exhibit significant cost disadvantages. Enabling well-trained LLMs (e.g., Llama) to rapidly adapt to MLA without pre-training from scratch is both meaningful and challenging. This paper proposes the first data-efficient fine-tuning method for transitioning from MHA to MLA (MHA2MLA), which includes two key components: for partial-RoPE, we remove RoPE from dimensions of queries and keys that contribute less to the attention scores, for low-rank approximation, we introduce joint SVD approximations based on the pre-trained parameters of keys and values. These carefully designed strategies enable MHA2MLA to recover performance using only a small fraction (0.3% to 0.6%) of the data, significantly reducing inference costs while seamlessly integrating with compression techniques such as KV cache quantization. For example, the KV cache size of Llama2-7B is reduced by 92.19%, with only a 0.5% drop in LongBench performance.</article>","contentLength":1314,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LongWriter-V: Enabling Ultra-Long and High-Fidelity Generation in Vision-Language Models","url":"https://arxiv.org/abs/2502.14834","date":1740114000,"author":"","guid":7936,"unread":true,"content":"<article>arXiv:2502.14834v1 Announce Type: new \nAbstract: Existing Large Vision-Language Models (LVLMs) can process inputs with context lengths up to 128k visual and text tokens, yet they struggle to generate coherent outputs beyond 1,000 words. We find that the primary limitation is the absence of long output examples during supervised fine-tuning (SFT). To tackle this issue, we introduce LongWriter-V-22k, a SFT dataset comprising 22,158 examples, each with multiple input images, an instruction, and corresponding outputs ranging from 0 to 10,000 words. Moreover, to achieve long outputs that maintain high-fidelity to the input images, we employ Direct Preference Optimization (DPO) to the SFT model. Given the high cost of collecting human feedback for lengthy outputs (e.g., 3,000 words), we propose IterDPO, which breaks long outputs into segments and uses iterative corrections to form preference pairs with the original outputs. Additionally, we develop MMLongBench-Write, a benchmark featuring six tasks to evaluate the long-generation capabilities of VLMs. Our 7B parameter model, trained with LongWriter-V-22k and IterDPO, achieves impressive performance on this benchmark, outperforming larger proprietary models like GPT-4o. Code and data: https://github.com/THU-KEG/LongWriter-V</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Probabilistic Robustness in Deep Learning: A Concise yet Comprehensive Guide","url":"https://arxiv.org/abs/2502.14833","date":1740114000,"author":"","guid":7937,"unread":true,"content":"<article>arXiv:2502.14833v1 Announce Type: new \nAbstract: Deep learning (DL) has demonstrated significant potential across various safety-critical applications, yet ensuring its robustness remains a key challenge. While adversarial robustness has been extensively studied in worst-case scenarios, probabilistic robustness (PR) offers a more practical perspective by quantifying the likelihood of failures under stochastic perturbations. This paper provides a concise yet comprehensive overview of PR, covering its formal definitions, evaluation and enhancement methods. We introduce a reformulated ''min-max'' optimisation framework for adversarial training specifically designed to improve PR. Furthermore, we explore the integration of PR verification evidence into system-level safety assurance, addressing challenges in translating DL model-level robustness to system-level claims. Finally, we highlight open research questions, including benchmarking PR evaluation methods, extending PR to generative AI tasks, and developing rigorous methodologies and case studies for system-level integration.</article>","contentLength":1091,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Improving the Diffusability of Autoencoders","url":"https://arxiv.org/abs/2502.14831","date":1740114000,"author":"","guid":7938,"unread":true,"content":"<article>arXiv:2502.14831v1 Announce Type: new \nAbstract: Latent diffusion models have emerged as the leading approach for generating high-quality images and videos, utilizing compressed latent representations to reduce the computational burden of the diffusion process. While recent advancements have primarily focused on scaling diffusion backbones and improving autoencoder reconstruction quality, the interaction between these components has received comparatively less attention. In this work, we perform a spectral analysis of modern autoencoders and identify inordinate high-frequency components in their latent spaces, which are especially pronounced in the autoencoders with a large bottleneck channel size. We hypothesize that this high-frequency component interferes with the coarse-to-fine nature of the diffusion synthesis process and hinders the generation quality. To mitigate the issue, we propose scale equivariance: a simple regularization strategy that aligns latent and RGB spaces across frequencies by enforcing scale equivariance in the decoder. It requires minimal code changes and only up to 20K autoencoder fine-tuning steps, yet significantly improves generation quality, reducing FID by 19% for image generation on ImageNet-1K 256x256 and FVD by at least 44% for video generation on Kinetics-700 17x256x256.</article>","contentLength":1325,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs","url":"https://arxiv.org/abs/2502.14830","date":1740114000,"author":"","guid":7939,"unread":true,"content":"<article>arXiv:2502.14830v1 Announce Type: new \nAbstract: While large language models demonstrate remarkable capabilities at task-specific applications through fine-tuning, extending these benefits across diverse languages is essential for broad accessibility. However, effective cross-lingual transfer is hindered by LLM performance gaps across languages and the scarcity of fine-tuning data in many languages. Through analysis of LLM internal representations from over 1,000+ language pairs, we discover that middle layers exhibit the strongest potential for cross-lingual alignment. Building on this finding, we propose a middle-layer alignment objective integrated into task-specific training. Our experiments on slot filling, machine translation, and structured text generation show consistent improvements in cross-lingual transfer, especially to lower-resource languages. The method is robust to the choice of alignment languages and generalizes to languages unseen during alignment. Furthermore, we show that separately trained alignment modules can be merged with existing task-specific modules, improving cross-lingual capabilities without full re-training. Our code is publicly available (https://github.com/dannigt/mid-align).</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Measuring Faithfulness of Chains of Thought by Unlearning Reasoning Steps","url":"https://arxiv.org/abs/2502.14829","date":1740114000,"author":"","guid":7940,"unread":true,"content":"<article>arXiv:2502.14829v1 Announce Type: new \nAbstract: When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. However, despite much work on CoT prompting, it is unclear if CoT reasoning is faithful to the models' parameteric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters. We perform experiments unlearning CoTs of four LMs prompted on four multi-choice question answering (MCQA) datasets. Our experiments show that FUR is frequently able to change the underlying models' prediction by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning. Importantly, CoT steps identified as important by FUR do not align well with human notions of plausbility, emphasizing the need for specialized alignment</article>","contentLength":1185,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fundamental Limitations in Defending LLM Finetuning APIs","url":"https://arxiv.org/abs/2502.14828","date":1740114000,"author":"","guid":7941,"unread":true,"content":"<article>arXiv:2502.14828v1 Announce Type: new \nAbstract: LLM developers have imposed technical interventions to prevent fine-tuning misuse attacks, attacks where adversaries evade safeguards by fine-tuning the model using a public API. Previous work has established several successful attacks against specific fine-tuning API defences. In this work, we show that defences of fine-tuning APIs that seek to detect individual harmful training or inference samples ('pointwise' detection) are fundamentally limited in their ability to prevent fine-tuning attacks. We construct 'pointwise-undetectable' attacks that repurpose entropy in benign model outputs (e.g. semantic or syntactic variations) to covertly transmit dangerous knowledge. Our attacks are composed solely of unsuspicious benign samples that can be collected from the model before fine-tuning, meaning training and inference samples are all individually benign and low-perplexity. We test our attacks against the OpenAI fine-tuning API, finding they succeed in eliciting answers to harmful multiple-choice questions, and that they evade an enhanced monitoring system we design that successfully detects other fine-tuning attacks. We encourage the community to develop defences that tackle the fundamental limitations we uncover in pointwise fine-tuning API defences.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring Advanced Techniques for Visual Question Answering: A Comprehensive Comparison","url":"https://arxiv.org/abs/2502.14827","date":1740114000,"author":"","guid":7942,"unread":true,"content":"<article>arXiv:2502.14827v1 Announce Type: new \nAbstract: Visual Question Answering (VQA) has emerged as a pivotal task in the intersection of computer vision and natural language processing, requiring models to understand and reason about visual content in response to natural language questions. Analyzing VQA datasets is essential for developing robust models that can handle the complexities of multimodal reasoning. Several approaches have been developed to examine these datasets, each offering distinct perspectives on question diversity, answer distribution, and visual-textual correlations. Despite significant progress, existing VQA models face challenges related to dataset bias, limited model complexity, commonsense reasoning gaps, rigid evaluation methods, and generalization to real world scenarios. This paper presents a comprehensive comparative study of five advanced VQA models: ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA, each employing distinct methodologies to address these challenges.</article>","contentLength":1026,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey of Model Architectures in Information Retrieval","url":"https://arxiv.org/abs/2502.14822","date":1740114000,"author":"","guid":7943,"unread":true,"content":"<article>arXiv:2502.14822v1 Announce Type: new \nAbstract: This survey examines the evolution of model architectures in information retrieval (IR), focusing on two key aspects: backbone models for feature extraction and end-to-end system architectures for relevance estimation. The review intentionally separates architectural considerations from training methodologies to provide a focused analysis of structural innovations in IR systems.We trace the development from traditional term-based methods to modern neural approaches, particularly highlighting the impact of transformer-based models and subsequent large language models (LLMs). We conclude by discussing emerging challenges and future directions, including architectural optimizations for performance and scalability, handling of multimodal, multilingual data, and adaptation to novel application domains beyond traditional search paradigms.</article>","contentLength":893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meshless Shape Optimization using Neural Networks and Partial Differential Equations on Graphs","url":"https://arxiv.org/abs/2502.14821","date":1740114000,"author":"","guid":7944,"unread":true,"content":"<article>arXiv:2502.14821v1 Announce Type: new \nAbstract: Shape optimization involves the minimization of a cost function defined over a set of shapes, often governed by a partial differential equation (PDE). In the absence of closed-form solutions, one relies on numerical methods to approximate the solution. The level set method -- when coupled with the finite element method -- is one of the most versatile numerical shape optimization approaches but still suffers from the limitations of most mesh-based methods. In this work, we present a fully meshless level set framework that leverages neural networks to parameterize the level set function and employs the graph Laplacian to approximate the underlying PDE. Our approach enables precise computations of geometric quantities such as surface normals and curvature, and allows tackling optimization problems within the class of convex shapes.</article>","contentLength":889,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables","url":"https://arxiv.org/abs/2502.14820","date":1740114000,"author":"","guid":7945,"unread":true,"content":"<article>arXiv:2502.14820v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges.</article>","contentLength":1133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models","url":"https://arxiv.org/abs/2502.14819","date":1740114000,"author":"","guid":7946,"unread":true,"content":"<article>arXiv:2502.14819v1 Announce Type: new \nAbstract: A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones. Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model. However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations. In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning. We study how dataset properties-such as data diversity, trajectory quality, and environment variability-affect the performance of these approaches. Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment layouts, trajectory stitching, and data-efficiency. Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data.</article>","contentLength":1403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Low-Rank Sparse Adaptation for Large Language Models","url":"https://arxiv.org/abs/2502.14816","date":1740114000,"author":"","guid":7947,"unread":true,"content":"<article>arXiv:2502.14816v1 Announce Type: new \nAbstract: Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduce dynamic Low-rank Sparse Adaptation (LoSA), a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby efficiently determining the layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by 68.73 and increased zero-shot accuracy by 16.32$\\%$, achieving a 2.60$\\times$ speedup on CPU and 2.23$\\times$ speedup on GPU, requiring only 45 minutes of fine-tuning on a single NVIDIA A100 80GB GPU. Code is available at https://github.com/wzhuang-xmu/LoSA.</article>","contentLength":1923,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Model Selection for Compound AI Systems","url":"https://arxiv.org/abs/2502.14815","date":1740114000,"author":"","guid":7948,"unread":true,"content":"<article>arXiv:2502.14815v1 Announce Type: new \nAbstract: Compound AI systems that combine multiple LLM calls, such as self-refine and multi-agent-debate, achieve strong performance on many AI tasks. We address a core question in optimizing compound systems: for each LLM call or module in the system, how should one decide which LLM to use? We show that these LLM choices have a large effect on quality, but the search space is exponential. We propose LLMSelector, an efficient framework for model selection in compound systems, which leverages two key empirical insights: (i) end-to-end performance is often monotonic in how well each module performs, with all other modules held fixed, and (ii) per-module performance can be estimated accurately by an LLM. Building upon these insights, LLMSelector iteratively selects one module and allocates to it the model with the highest module-wise performance, as estimated by an LLM, until no further gain is possible. LLMSelector is applicable to any compound system with a bounded number of modules, and its number of API calls scales linearly with the number of modules, achieving high-quality model allocation both empirically and theoretically. Experiments with popular compound systems such as multi-agent debate and self-refine using LLMs such as GPT-4o, Claude 3.5 Sonnet and Gemini 1.5 show that LLMSelector confers 5%-70% accuracy gains compared to using the same LLM for all modules.</article>","contentLength":1430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VB-Com: Learning Vision-Blind Composite Humanoid Locomotion Against Deficient Perception","url":"https://arxiv.org/abs/2502.14814","date":1740114000,"author":"","guid":7949,"unread":true,"content":"<article>arXiv:2502.14814v1 Announce Type: new \nAbstract: The performance of legged locomotion is closely tied to the accuracy and comprehensiveness of state observations. Blind policies, which rely solely on proprioception, are considered highly robust due to the reliability of proprioceptive observations. However, these policies significantly limit locomotion speed and often require collisions with the terrain to adapt. In contrast, Vision policies allows the robot to plan motions in advance and respond proactively to unstructured terrains with an online perception module. However, perception is often compromised by noisy real-world environments, potential sensor failures, and the limitations of current simulations in presenting dynamic or deformable terrains. Humanoid robots, with high degrees of freedom and inherently unstable morphology, are particularly susceptible to misguidance from deficient perception, which can result in falls or termination on challenging dynamic terrains. To leverage the advantages of both vision and blind policies, we propose VB-Com, a composite framework that enables humanoid robots to determine when to rely on the vision policy and when to switch to the blind policy under perceptual deficiency. We demonstrate that VB-Com effectively enables humanoid robots to traverse challenging terrains and obstacles despite perception deficiencies caused by dynamic terrains or perceptual noise.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Byzantine Game Theory: Sun Tzus Boxes","url":"https://arxiv.org/abs/2502.14812","date":1740114000,"author":"","guid":7950,"unread":true,"content":"<article>arXiv:2502.14812v1 Announce Type: new \nAbstract: We introduce the Byzantine Selection Problem, living at the intersection of game theory and fault-tolerant distributed computing. Here, an event organizer is presented with a group of $n$ agents, and wants to select $\\ell &lt; n$ of them to form a team. For these purposes, each agent $i$ self-reports a positive skill value $v_i$, and a team's value is the sum of its members' skill values. Ideally, the value of the team should be as large as possible, which can be easily achieved by selecting agents with the highest $\\ell$ skill values. However, an unknown subset of at most $t &lt; n$ agents are byzantine and hence not to be trusted, rendering their true skill values as $0$. In the spirit of the distributed computing literature, the identity of the byzantine agents is not random but instead chosen by an adversary aiming to minimize the value of the chosen team. Can we still select a team with good guarantees in this adversarial setting? As it turns out, deterministically, it remains optimal to select agents with the highest $\\ell$ values. Yet, if $t \\geq \\ell$, the adversary can choose to make all selected agents byzantine, leading to a team of value zero. To provide meaningful guarantees, one hence needs to allow for randomization, in which case the expected value of the selected team needs to be maximized, assuming again that the adversary plays to minimize it. For this case, we provide linear-time randomized algorithms that maximize the expected value of the selected team.</article>","contentLength":1542,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PREM: Privately Answering Statistical Queries with Relative Error","url":"https://arxiv.org/abs/2502.14809","date":1740114000,"author":"","guid":7951,"unread":true,"content":"<article>arXiv:2502.14809v1 Announce Type: new \nAbstract: We introduce $\\mathsf{PREM}$ (Private Relative Error Multiplicative weight update), a new framework for generating synthetic data that achieves a relative error guarantee for statistical queries under $(\\varepsilon, \\delta)$ differential privacy (DP). Namely, for a domain ${\\cal X}$, a family ${\\cal F}$ of queries $f : {\\cal X} \\to \\{0, 1\\}$, and $\\zeta &gt; 0$, our framework yields a mechanism that on input dataset $D \\in {\\cal X}^n$ outputs a synthetic dataset $\\widehat{D} \\in {\\cal X}^n$ such that all statistical queries in ${\\cal F}$ on $D$, namely $\\sum_{x \\in D} f(x)$ for $f \\in {\\cal F}$, are within a $1 \\pm \\zeta$ multiplicative factor of the corresponding value on $\\widehat{D}$ up to an additive error that is polynomial in $\\log |{\\cal F}|$, $\\log |{\\cal X}|$, $\\log n$, $\\log(1/\\delta)$, $1/\\varepsilon$, and $1/\\zeta$. In contrast, any $(\\varepsilon, \\delta)$-DP mechanism is known to require worst-case additive error that is polynomial in at least one of $n, |{\\cal F}|$, or $|{\\cal X}|$. We complement our algorithm with nearly matching lower bounds.</article>","contentLength":1120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Planning, scheduling, and execution on the Moon: the CADRE technology demonstration mission","url":"https://arxiv.org/abs/2502.14803","date":1740114000,"author":"","guid":7952,"unread":true,"content":"<article>arXiv:2502.14803v1 Announce Type: new \nAbstract: NASA's Cooperative Autonomous Distributed Robotic Exploration (CADRE) mission, slated for flight to the Moon's Reiner Gamma region in 2025/2026, is designed to demonstrate multi-agent autonomous exploration of the Lunar surface and sub-surface. A team of three robots and a base station will autonomously explore a region near the lander, collecting the data required for 3D reconstruction of the surface with no human input; and then autonomously perform distributed sensing with multi-static ground penetrating radars (GPR), driving in formation while performing coordinated radar soundings to create a map of the subsurface. At the core of CADRE's software architecture is a novel autonomous, distributed planning, scheduling, and execution (PS&amp;E) system. The system coordinates the robots' activities, planning and executing tasks that require multiple robots' participation while ensuring that each individual robot's thermal and power resources stay within prescribed bounds, and respecting ground-prescribed sleep-wake cycles. The system uses a centralized-planning, distributed-execution paradigm, and a leader election mechanism ensures robustness to failures of individual agents. In this paper, we describe the architecture of CADRE's PS&amp;E system; discuss its design rationale; and report on verification and validation (V&amp;V) testing of the system on CADRE's hardware in preparation for deployment on the Moon.</article>","contentLength":1470,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From RAG to Memory: Non-Parametric Continual Learning for Large Language Models","url":"https://arxiv.org/abs/2502.14802","date":1740114000,"author":"","guid":7953,"unread":true,"content":"<article>arXiv:2502.14802v1 Announce Type: new \nAbstract: Our ability to continuously acquire, organize, and leverage knowledge is a key feature of human intelligence that AI systems must approximate to unlock their full potential. Given the challenges in continual learning with large language models (LLMs), retrieval-augmented generation (RAG) has become the dominant way to introduce new information. However, its reliance on vector retrieval hinders its ability to mimic the dynamic and interconnected nature of human long-term memory. Recent RAG approaches augment vector embeddings with various structures like knowledge graphs to address some of these gaps, namely sense-making and associativity. However, their performance on more basic factual memory tasks drops considerably below standard RAG. We address this unintended deterioration and propose HippoRAG 2, a framework that outperforms standard RAG comprehensively on factual, sense-making, and associative memory tasks. HippoRAG 2 builds upon the Personalized PageRank algorithm used in HippoRAG and enhances it with deeper passage integration and more effective online use of an LLM. This combination pushes this RAG system closer to the effectiveness of human long-term memory, achieving a 7% improvement in associative memory tasks over the state-of-the-art embedding model while also exhibiting superior factual knowledge and sense-making memory capabilities. This work paves the way for non-parametric continual learning for LLMs. Our code and data will be released at https://github.com/OSU-NLP-Group/HippoRAG.</article>","contentLength":1572,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AVD2: Accident Video Diffusion for Accident Video Description","url":"https://arxiv.org/abs/2502.14801","date":1740114000,"author":"","guid":7954,"unread":true,"content":"<article>arXiv:2502.14801v1 Announce Type: new \nAbstract: Traffic accidents present complex challenges for autonomous driving, often featuring unpredictable scenarios that hinder accurate system interpretation and responses.Nonetheless, prevailing methodologies fall short in elucidating the causes of accidents and proposing preventive measures due to the paucity of training data specific to accident scenarios.In this work, we introduce AVD2 (Accident Video Diffusion for Accident Video Description), a novel framework that enhances accident scene understanding by generating accident videos that aligned with detailed natural language descriptions and reasoning, resulting in the contributed EMM-AU (Enhanced Multi-Modal Accident Video Understanding) dataset. Empirical results reveal that the integration of the EMM-AU dataset establishes state-of-the-art performance across both automated metrics and human evaluations, markedly advancing the domains of accident analysis and prevention. Project resources are available at https://an-answer-tree.github.io</article>","contentLength":1052,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Text-Driven 360-Degree Panorama Generation","url":"https://arxiv.org/abs/2502.14799","date":1740114000,"author":"","guid":7955,"unread":true,"content":"<article>arXiv:2502.14799v1 Announce Type: new \nAbstract: The advent of text-driven 360-degree panorama generation, enabling the synthesis of 360-degree panoramic images directly from textual descriptions, marks a transformative advancement in immersive visual content creation. This innovation significantly simplifies the traditionally complex process of producing such content. Recent progress in text-to-image diffusion models has accelerated the rapid development in this emerging field. This survey presents a comprehensive review of text-driven 360-degree panorama generation, offering an in-depth analysis of state-of-the-art algorithms and their expanding applications in 360-degree 3D scene generation. Furthermore, we critically examine current limitations and propose promising directions for future research. A curated project page with relevant resources and research papers is available at https://littlewhitesea.github.io/Text-Driven-Pano-Gen/.</article>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Multi-Agent Perspective on Modern Information Retrieval","url":"https://arxiv.org/abs/2502.14796","date":1740114000,"author":"","guid":7956,"unread":true,"content":"<article>arXiv:2502.14796v1 Announce Type: new \nAbstract: The rise of large language models (LLMs) has introduced a new era in information retrieval (IR), where queries and documents that were once assumed to be generated exclusively by humans can now also be created by automated agents. These agents can formulate queries, generate documents, and perform ranking. This shift challenges some long-standing IR paradigms and calls for a reassessment of both theoretical frameworks and practical methodologies. We advocate for a multi-agent perspective to better capture the complex interactions between query agents, document agents, and ranker agents. Through empirical exploration of various multi-agent retrieval settings, we reveal the significant impact of these interactions on system performance. Our findings underscore the need to revisit classical IR paradigms and develop new frameworks for more effective modeling and evaluation of modern retrieval systems.</article>","contentLength":959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration","url":"https://arxiv.org/abs/2502.14795","date":1740114000,"author":"","guid":7957,"unread":true,"content":"<article>arXiv:2502.14795v1 Announce Type: new \nAbstract: This paper addresses the limitations of current humanoid robot control frameworks, which primarily rely on reactive mechanisms and lack autonomous interaction capabilities due to data scarcity. We propose Humanoid-VLA, a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control. Humanoid-VLA begins with language-motion pre-alignment using non-egocentric human motion datasets paired with textual descriptions, allowing the model to learn universal motion patterns and action semantics. We then incorporate egocentric visual context through a parameter efficient video-conditioned fine-tuning, enabling context-aware motion generation. Furthermore, we introduce a self-supervised data augmentation strategy that automatically generates pseudoannotations directly derived from motion data. This process converts raw motion sequences into informative question-answer pairs, facilitating the effective use of large-scale unlabeled video data. Built upon whole-body control architectures, extensive experiments show that Humanoid-VLA achieves object interaction and environment exploration tasks with enhanced contextual awareness, demonstrating a more human-like capacity for adaptive and intelligent engagement.</article>","contentLength":1342,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RendBEV: Semantic Novel View Synthesis for Self-Supervised Bird's Eye View Segmentation","url":"https://arxiv.org/abs/2502.14792","date":1740114000,"author":"","guid":7958,"unread":true,"content":"<article>arXiv:2502.14792v1 Announce Type: new \nAbstract: Bird's Eye View (BEV) semantic maps have recently garnered a lot of attention as a useful representation of the environment to tackle assisted and autonomous driving tasks. However, most of the existing work focuses on the fully supervised setting, training networks on large annotated datasets. In this work, we present RendBEV, a new method for the self-supervised training of BEV semantic segmentation networks, leveraging differentiable volumetric rendering to receive supervision from semantic perspective views computed by a 2D semantic segmentation model. Our method enables zero-shot BEV semantic segmentation, and already delivers competitive results in this challenging setting. When used as pretraining to then fine-tune on labeled BEV ground-truth, our method significantly boosts performance in low-annotation regimes, and sets a new state of the art when fine-tuning on all available labels.</article>","contentLength":954,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rapid Word Learning Through Meta In-Context Learning","url":"https://arxiv.org/abs/2502.14791","date":1740114000,"author":"","guid":7959,"unread":true,"content":"<article>arXiv:2502.14791v1 Announce Type: new \nAbstract: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.</article>","contentLength":1368,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Adversarial Analysis of Thompson Sampling for Full-information Online Learning: from Finite to Infinite Action Spaces","url":"https://arxiv.org/abs/2502.14790","date":1740114000,"author":"","guid":7960,"unread":true,"content":"<article>arXiv:2502.14790v1 Announce Type: new \nAbstract: We develop an analysis of Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling with a certain Gaussian process prior widely-used in the Bayesian optimization literature has a $\\mathcal{O}(\\beta\\sqrt{T\\log(1+\\lambda)})$ rate against a $\\beta$-bounded $\\lambda$-Lipschitz~adversary.</article>","contentLength":852,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Structurally Disentangled Feature Fields Distillation for 3D Understanding and Editing","url":"https://arxiv.org/abs/2502.14789","date":1740114000,"author":"","guid":7961,"unread":true,"content":"<article>arXiv:2502.14789v1 Announce Type: new \nAbstract: Recent work has demonstrated the ability to leverage or distill pre-trained 2D features obtained using large pre-trained 2D models into 3D features, enabling impressive 3D editing and understanding capabilities using only 2D supervision. Although impressive, models assume that 3D features are captured using a single feature field and often make a simplifying assumption that features are view-independent. In this work, we propose instead to capture 3D features using multiple disentangled feature fields that capture different structural components of 3D features involving view-dependent and view-independent components, which can be learned from 2D feature supervision only. Subsequently, each element can be controlled in isolation, enabling semantic and structural understanding and editing capabilities. For instance, using a user click, one can segment 3D features corresponding to a given object and then segment, edit, or remove their view-dependent (reflective) properties. We evaluate our approach on the task of 3D segmentation and demonstrate a set of novel understanding and editing tasks.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ray-Tracing for Conditionally Activated Neural Networks","url":"https://arxiv.org/abs/2502.14788","date":1740114000,"author":"","guid":7962,"unread":true,"content":"<article>arXiv:2502.14788v1 Announce Type: new \nAbstract: In this paper, we introduce a novel architecture for conditionally activated neural networks combining a hierarchical construction of multiple Mixture of Experts (MoEs) layers with a sampling mechanism that progressively converges to an optimized configuration of expert activation. This methodology enables the dynamic unfolding of the network's architecture, facilitating efficient path-specific training. Experimental results demonstrate that this approach achieves competitive accuracy compared to conventional baselines while significantly reducing the parameter count required for inference. Notably, this parameter reduction correlates with the complexity of the input patterns, a property naturally emerging from the network's operational dynamics without necessitating explicit auxiliary penalty functions.</article>","contentLength":864,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Micro Blossom: Accelerated Minimum-Weight Perfect Matching Decoding for Quantum Error Correction","url":"https://arxiv.org/abs/2502.14787","date":1740114000,"author":"","guid":7963,"unread":true,"content":"<article>arXiv:2502.14787v1 Announce Type: new \nAbstract: Minimum-Weight Perfect Matching (MWPM) decoding is important to quantum error correction decoding because of its accuracy. However, many believe that it is difficult, if possible at all, to achieve the microsecond latency requirement posed by superconducting qubits. This work presents the first publicly known MWPM decoder, called Micro Blossom, that achieves sub-microsecond decoding latency. Micro Blossom employs a heterogeneous architecture that carefully partitions a state-of-the-art MWPM decoder between software and a programmable accelerator with parallel processing units, one of each vertex/edge of the decoding graph. On a surface code with code distance $d$ and a circuit-level noise model with physical error rate $p$, Micro Blossom's accelerator employs $O(d^3)$ parallel processing units to reduce the worst-case latency from $O(d^{12})$ to $O(d^9)$ and reduce the average latency from $O(p d^3+1)$ to $O(p^2 d^2+1)$ when $p \\ll 1$.\n  We report a prototype implementation of Micro Blossom using FPGA. Measured at $d=13$ and $p=0.1\\%$, the prototype achieves an average decoding latency of $0.8 \\mu s$ at a moderate clock frequency of $62 MHz$. Micro Blossom is the first publicly known hardware-accelerated exact MWPM decoder, and the decoding latency of $0.8 \\mu s$ is 8 times shorter than the best latency of MWPM decoder implementations reported in the literature.</article>","contentLength":1433,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features","url":"https://arxiv.org/abs/2502.14786","date":1740114000,"author":"","guid":7964,"unread":true,"content":"<article>arXiv:2502.14786v1 Announce Type: new \nAbstract: We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success of the original SigLIP. In this second iteration, we extend the original image-text training objective with several prior, independently developed techniques into a unified recipe -- this includes captioning-based pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, including zero-shot classification, image-text retrieval, and transfer performance when extracting visual representations for Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements on localization and dense prediction tasks. We also train variants which support multiple resolutions and preserve the input's native aspect ratio. Finally, we train on a more diverse data-mixture that includes de-biasing techniques, leading to much better multilingual understanding and improved fairness. To allow users to trade off inference cost with performance, we release model checkpoints at four sizes: ViT-B (86M), L (303M), So400m (400M), and g (1B).</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-Time Device Reach Forecasting Using HLL and MinHash Data Sketches","url":"https://arxiv.org/abs/2502.14785","date":1740114000,"author":"","guid":7965,"unread":true,"content":"<article>arXiv:2502.14785v1 Announce Type: new \nAbstract: Predicting the right number of TVs (Device Reach) in real-time based on a user-specified targeting attributes is imperative for running multi-million dollar ADs business. The traditional approach of SQL queries to join billions of records across multiple targeting dimensions is extremely slow. As a workaround, many applications will have an offline process to crunch these numbers and present the results after many hours. In our case, the solution was an offline process taking 24 hours to onboard a customer resulting in a potential loss of business. To solve this problem, we have built a new real-time prediction system using MinHash and HyperLogLog (HLL) data sketches to compute the device reach at runtime when a user makes a request. However, existing MinHash implementations do not solve the complex problem of multilevel aggregation and intersection. This work will show how we have solved this problem, in addition, we have improved MinHash algorithm to run 4 times faster using Single Instruction Multiple Data (SIMD) vectorized operations for high speed and accuracy with constant space to process billions of records. Finally, by experiments, we prove that the results are as accurate as traditional offline prediction system with an acceptable error rate of 5%.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Resource Management for the Uplink of Wideband Hybrid Beamforming System","url":"https://arxiv.org/abs/2502.14784","date":1740114000,"author":"","guid":7966,"unread":true,"content":"<article>arXiv:2502.14784v1 Announce Type: new \nAbstract: This paper studies the radio resource management (RRM) for the \\emph{uplink} (UL) of a cellular system with codebook-based \\emph{hybrid beamforming}. We consider the often neglected but highly practical multi-channel case with fewer radio frequency chains in the base station than user equipment (UEs) in the cell, assuming one RF chain per UE. As for any UL RRM, a per-time slot solution is needed as the allocation of power to subchannels by a UE can only be done once it knows which subchannels it has been allocated. The RRM in this system comprises beam selection, user selection and power allocation, three steps that are intricately coupled and we will show that the order in which they are performed does impact performance and so does the amount of coupling that we take into account. Specifically, we propose 4 online sequential solutions with different orders in which the steps are called and of different complexities, i.e., different levels of coupling between the steps. Our extensive numerical campaign for a mmWave system shows how a well-designed heuristic that takes some level of couplings between the steps can make the performance exceedingly better than a benchmark.</article>","contentLength":1238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tracking and Assigning Jobs to a Markov Machine","url":"https://arxiv.org/abs/2502.14783","date":1740114000,"author":"","guid":7967,"unread":true,"content":"<article>arXiv:2502.14783v1 Announce Type: new \nAbstract: We consider a time-slotted communication system with a machine, a cloud server, and a sampler. Job requests from the users are queued on the server to be completed by the machine. The machine has two states, namely, a busy state and a free state. The server can assign a job to the machine in a first-in-first-served manner. If the machine is free, it completes the job request from the server; otherwise, it drops the request. Upon dropping a job request, the server is penalized. When the machine is in the free state, the machine can get into the busy state with an internal job. When the server does not assign a job request to the machine, the state of the machine evolves as a symmetric Markov chain. If the machine successfully accepts the job request from the server, the state of the machine goes to the busy state and follows a different dynamics compared to the dynamics when the machine goes to the busy state due to an internal job. The sampler samples the state of the machine and sends it to the server via an error-free channel. Thus, the server can estimate the state of the machine, upon receiving an update from the source. If the machine is in the free state but the estimated state at the server is busy, the sampler pays a cost. We incorporate the concept of the age of incorrect information to model the cost of the sampler. We aim to find an optimal sampling policy such that the cost of the sampler plus the penalty on the machine gets minimized. We formulate this problem in a Markov decision process framework and find how an optimal policy changes with several associated parameters. We show that a threshold policy is optimal for this problem. We show a necessary and sufficient condition for a threshold policy to be optimal. Finally, we find the optimal threshold without bounding the state space.</article>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Neural Operator-Based Emulator for Regional Shallow Water Dynamics","url":"https://arxiv.org/abs/2502.14782","date":1740114000,"author":"","guid":7968,"unread":true,"content":"<article>arXiv:2502.14782v1 Announce Type: new \nAbstract: Coastal regions are particularly vulnerable to the impacts of rising sea levels and extreme weather events. Accurate real-time forecasting of hydrodynamic processes in these areas is essential for infrastructure planning and climate adaptation. In this study, we present the Multiple-Input Temporal Operator Network (MITONet), a novel autoregressive neural emulator that employs dimensionality reduction to efficiently approximate high-dimensional numerical solvers for complex, nonlinear problems that are governed by time-dependent, parameterized partial differential equations. Although MITONet is applicable to a wide range of problems, we showcase its capabilities by forecasting regional tide-driven dynamics described by the two-dimensional shallow-water equations, while incorporating initial conditions, boundary conditions, and a varying domain parameter. We demonstrate MITONet's performance in a real-world application, highlighting its ability to make accurate predictions by extrapolating both in time and parametric space.</article>","contentLength":1086,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting","url":"https://arxiv.org/abs/2502.14780","date":1740114000,"author":"","guid":7969,"unread":true,"content":"<article>arXiv:2502.14780v1 Announce Type: new \nAbstract: Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (&lt;500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in Image Generation with Diffusion Models","url":"https://arxiv.org/abs/2502.14779","date":1740114000,"author":"","guid":7970,"unread":true,"content":"<article>arXiv:2502.14779v1 Announce Type: new \nAbstract: In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and precisely controllable framework for multi-condition image generation. The core idea behind DC-ControlNet is to decouple control conditions, transforming global control into a hierarchical system that integrates distinct elements, contents, and layouts. This enables users to mix these individual conditions with greater flexibility, leading to more efficient and accurate image generation control. Previous ControlNet-based models rely solely on global conditions, which affect the entire image and lack the ability of element- or region-specific control. This limitation reduces flexibility and can cause condition misunderstandings in multi-conditional image generation. To address these challenges, we propose both intra-element and Inter-element Controllers in DC-ControlNet. The Intra-Element Controller handles different types of control signals within individual elements, accurately describing the content and layout characteristics of the object. For interactions between elements, we introduce the Inter-Element Controller, which accurately handles multi-element interactions and occlusion based on user-defined relationships. Extensive evaluations show that DC-ControlNet significantly outperforms existing ControlNet models and Layout-to-Image generative models in terms of control flexibility and precision in multi-condition control.</article>","contentLength":1473,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Harnessing PDF Data for Improving Japanese Large Multimodal Models","url":"https://arxiv.org/abs/2502.14778","date":1740114000,"author":"","guid":7971,"unread":true,"content":"<article>arXiv:2502.14778v1 Announce Type: new \nAbstract: Large Multimodal Models (LMMs) have demonstrated strong performance in English, but their effectiveness in Japanese remains limited due to the lack of high-quality training data. Current Japanese LMMs often rely on translated English datasets, restricting their ability to capture Japan-specific cultural knowledge. To address this, we explore the potential of Japanese PDF data as a training resource, an area that remains largely underutilized. We introduce a fully automated pipeline that leverages pretrained models to extract image-text pairs from PDFs through layout analysis, OCR, and vision-language pairing, removing the need for manual annotation. Additionally, we construct instruction data from extracted image-text pairs to enrich the training data. To evaluate the effectiveness of PDF-derived data, we train Japanese LMMs and assess their performance on the Japanese LMM Benchmark. Our results demonstrate substantial improvements, with performance gains ranging from 3.9% to 13.8% on Heron-Bench. Further analysis highlights the impact of PDF-derived data on various factors, such as model size and language models, reinforcing its value as a multimodal resource for Japanese LMMs. We plan to make the source code and data publicly available upon acceptance.</article>","contentLength":1323,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Making Universal Policies Universal","url":"https://arxiv.org/abs/2502.14777","date":1740114000,"author":"","guid":7972,"unread":true,"content":"<article>arXiv:2502.14777v1 Announce Type: new \nAbstract: The development of a generalist agent capable of solving a wide range of sequential decision-making tasks remains a significant challenge. We address this problem in a cross-agent setup where agents share the same observation space but differ in their action spaces. Our approach builds on the universal policy framework, which decouples policy learning into two stages: a diffusion-based planner that generates observation sequences and an inverse dynamics model that assigns actions to these plans. We propose a method for training the planner on a joint dataset composed of trajectories from all agents. This method offers the benefit of positive transfer by pooling data from different agents, while the primary challenge lies in adapting shared plans to each agent's unique constraints. We evaluate our approach on the BabyAI environment, covering tasks of varying complexity, and demonstrate positive transfer across agents. Additionally, we examine the planner's generalisation ability to unseen agents and compare our method to traditional imitation learning approaches. By training on a pooled dataset from multiple agents, our universal policy achieves an improvement of up to $42.20\\%$ in task completion accuracy compared to a policy trained on a dataset from a single agent.</article>","contentLength":1336,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SurveyX: Academic Survey Automation via Large Language Models","url":"https://arxiv.org/abs/2502.14776","date":1740114000,"author":"","guid":7973,"unread":true,"content":"<article>arXiv:2502.14776v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated exceptional comprehension capabilities and a vast knowledge base, suggesting that LLMs can serve as efficient tools for automated survey generation. However, recent research related to automated survey generation remains constrained by some critical limitations like finite context window, lack of in-depth content discussion, and absence of systematic evaluation frameworks. Inspired by human writing processes, we propose SurveyX, an efficient and organized system for automated survey generation that decomposes the survey composing process into two phases: the Preparation and Generation phases. By innovatively introducing online reference retrieval, a pre-processing method called AttributeTree, and a re-polishing process, SurveyX significantly enhances the efficacy of survey composition. Experimental evaluation results show that SurveyX outperforms existing automated survey generation systems in content quality (0.259 improvement) and citation quality (1.76 enhancement), approaching human expert performance across multiple evaluation dimensions. Examples of surveys generated by SurveyX are available on www.surveyx.cn</article>","contentLength":1228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sparse Activations as Conformal Predictors","url":"https://arxiv.org/abs/2502.14773","date":1740114000,"author":"","guid":7974,"unread":true,"content":"<article>arXiv:2502.14773v1 Announce Type: new \nAbstract: Conformal prediction is a distribution-free framework for uncertainty quantification that replaces point predictions with sets, offering marginal coverage guarantees (i.e., ensuring that the prediction sets contain the true label with a specified probability, in expectation). In this paper, we uncover a novel connection between conformal prediction and sparse softmax-like transformations, such as sparsemax and $\\gamma$-entmax (with $\\gamma &gt; 1$), which may assign nonzero probability only to a subset of labels. We introduce new non-conformity scores for classification that make the calibration process correspond to the widely used temperature scaling method. At test time, applying these sparse transformations with the calibrated temperature leads to a support set (i.e., the set of labels with nonzero probability) that automatically inherits the coverage guarantees of conformal prediction. Through experiments on computer vision and text classification benchmarks, we demonstrate that the proposed method achieves competitive results in terms of coverage, efficiency, and adaptiveness compared to standard non-conformity scores based on softmax.</article>","contentLength":1205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Multivariate Robust Mean Estimation Under Mean-Shift Contamination","url":"https://arxiv.org/abs/2502.14772","date":1740114000,"author":"","guid":7975,"unread":true,"content":"<article>arXiv:2502.14772v1 Announce Type: new \nAbstract: We study the algorithmic problem of robust mean estimation of an identity covariance Gaussian in the presence of mean-shift contamination. In this contamination model, we are given a set of points in $\\mathbb{R}^d$ generated i.i.d. via the following process. For a parameter $\\alpha&lt;1/2$, the $i$-th sample $x_i$ is obtained as follows: with probability $1-\\alpha$, $x_i$ is drawn from $\\mathcal{N}(\\mu, I)$, where $\\mu \\in \\mathbb{R}^d$ is the target mean; and with probability $\\alpha$, $x_i$ is drawn from $\\mathcal{N}(z_i, I)$, where $z_i$ is unknown and potentially arbitrary. Prior work characterized the information-theoretic limits of this task. Specifically, it was shown that, in contrast to Huber contamination, in the presence of mean-shift contamination consistent estimation is possible. On the other hand, all known robust estimators in the mean-shift model have running times exponential in the dimension. Here we give the first computationally efficient algorithm for high-dimensional robust mean estimation with mean-shift contamination that can tolerate a constant fraction of outliers. In particular, our algorithm has near-optimal sample complexity, runs in sample-polynomial time, and approximates the target mean to any desired accuracy. Conceptually, our result contributes to a growing body of work that studies inference with respect to natural noise models lying in between fully adversarial and random settings.</article>","contentLength":1488,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Determining Layer-wise Sparsity for Large Language Models Through a Theoretical Perspective","url":"https://arxiv.org/abs/2502.14770","date":1740114000,"author":"","guid":7976,"unread":true,"content":"<article>arXiv:2502.14770v1 Announce Type: new \nAbstract: In this paper, we address the challenge of determining the layer-wise sparsity rates of large language models (LLMs) through a theoretical perspective. Specifically, we identify a critical issue of ''$\\textbf{reconstruction error explosion}$'' in existing LLMs sparsification methods. This refers to the cumulative effect of reconstruction errors throughout the sparsification process, where errors from earlier layers propagate and amplify in subsequent layers. As a result, the overall reconstruction error increases significantly, leading to a substantial degradation in model performance. Through theoretical analysis, we derive a simple yet effective approach to layer-wise sparsity allocation that mitigates this issue. Our method uses a monotonically increasing arithmetic progression, reducing the process of determining sparsity rates for multiple layers to the determination of a single common difference hyperparameter. Remarkably, this allows for the optimal layer-wise sparsity rates to be identified with just a few trials. Both our theoretical analysis and experimental results demonstrate that this sparsity allocation scheme is near optimal. Extensive experiments show that our method significantly improves the performance of sparse LLMs across various architectures, outperforming existing layer-wise sparsity methods. Furthermore, it enhances the performance of various compression techniques and is applicable to vision and multimodal models. Notably, our method achieves a reduction of 52.10 in perplexity for the 70$\\%$ sparse LLaMA2-7B model obtained via Wanda, improves average zero-shot accuracy by 10.50$\\%$, and delivers speedups of 2.63$\\times$ and 2.23$\\times$ on CPU and GPU, respectively.</article>","contentLength":1769,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning","url":"https://arxiv.org/abs/2502.14768","date":1740114000,"author":"","guid":7977,"unread":true,"content":"<article>arXiv:2502.14768v1 Announce Type: new \nAbstract: Inspired by the success of DeepSeek-R1, we explore the potential of rule-based reinforcement learning (RL) in large reasoning models. To analyze reasoning dynamics, we use synthetic logic puzzles as training data due to their controllable complexity and straightforward answer verification. We make some key technical contributions that lead to effective and stable RL training: a system prompt that emphasizes the thinking and answering process, a stringent format reward function that penalizes outputs for taking shortcuts, and a straightforward training recipe that achieves stable convergence. Our 7B model develops advanced reasoning skills-such as reflection, verification, and summarization-that are absent from the logic corpus. Remarkably, after training on just 5K logic problems, it demonstrates generalization abilities to the challenging math benchmarks AIME and AMC.</article>","contentLength":930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis","url":"https://arxiv.org/abs/2502.14767","date":1740114000,"author":"","guid":7978,"unread":true,"content":"<article>arXiv:2502.14767v1 Announce Type: new \nAbstract: With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.</article>","contentLength":1286,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Step-by-Step Fact Verification System for Medical Claims with Explainable Reasoning","url":"https://arxiv.org/abs/2502.14765","date":1740114000,"author":"","guid":7979,"unread":true,"content":"<article>arXiv:2502.14765v1 Announce Type: new \nAbstract: Fact verification (FV) aims to assess the veracity of a claim based on relevant evidence. The traditional approach for automated FV includes a three-part pipeline relying on short evidence snippets and encoder-only inference models. More recent approaches leverage the multi-turn nature of LLMs to address FV as a step-by-step problem where questions inquiring additional context are generated and answered until there is enough information to make a decision. This iterative method makes the verification process rational and explainable. While these methods have been tested for encyclopedic claims, exploration on domain-specific and realistic claims is missing. In this work, we apply an iterative FV system on three medical fact-checking datasets and evaluate it with multiple settings, including different LLMs, external web search, and structured reasoning using logic predicates. We demonstrate improvements in the final performance over traditional approaches and the high potential of step-by-step FV systems for domain-specific claims.</article>","contentLength":1095,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The illusion of households as entities in social networks","url":"https://arxiv.org/abs/2502.14764","date":1740114000,"author":"","guid":7980,"unread":true,"content":"<article>arXiv:2502.14764v1 Announce Type: new \nAbstract: Data recording connections between people in communities and villages are collected and analyzed in various ways, most often as either networks of individuals or as networks of households. These two networks can differ in substantial ways. The methodological choice of which network to study, therefore, is an important aspect in both study design and data analysis. In this work, we consider various key differences between household and individual social network structure, and ways in which the networks cannot be used interchangeably. In addition to formalizing the choices for representing each network, we explore the consequences of how the results of social network analysis change depending on the choice between studying the individual and household network -- from determining whether networks are assortative or disassortative to the ranking of influence-maximizing nodes. As our main contribution, we draw upon related work to propose a set of systematic recommendations for determining the relevant network representation to study. Our recommendations include assessing a series of entitativity criteria and relating these criteria to theories and observations about patterns and norms in social dynamics at the household level: notably, how information spreads within households and how power structures and gender roles affect this spread. We draw upon the definition of an illusion of entitativity to identify cases wherein grouping people into households does not satisfy these criteria or adequately represent given cultural or experimental contexts. Given the widespread use of social network data for studying communities, there is broad impact in understanding which network to study and the consequences of that decision. We hope that this work gives guidance to practitioners and researchers collecting and studying social network data.</article>","contentLength":1909,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sculpting [CLS] Features for Pre-Trained Model-Based Class-Incremental Learning","url":"https://arxiv.org/abs/2502.14762","date":1740114000,"author":"","guid":7981,"unread":true,"content":"<article>arXiv:2502.14762v1 Announce Type: new \nAbstract: Class-incremental learning requires models to continually acquire knowledge of new classes without forgetting old ones. Although pre-trained models have demonstrated strong performance in class-incremental learning, they remain susceptible to catastrophic forgetting when learning new concepts. Excessive plasticity in the models breaks generalizability and causes forgetting, while strong stability results in insufficient adaptation to new classes. This necessitates effective adaptation with minimal modifications to preserve the general knowledge of pre-trained models. To address this challenge, we first introduce a new parameter-efficient fine-tuning module 'Learn and Calibrate', or LuCA, designed to acquire knowledge through an adapter-calibrator couple, enabling effective adaptation with well-refined feature representations. Second, for each learning session, we deploy a sparse LuCA module on top of the last token just before the classifier, which we refer to as 'Token-level Sparse Calibration and Adaptation', or TOSCA. This strategic design improves the orthogonality between the modules and significantly reduces both training and inference complexity. By leaving the generalization capabilities of the pre-trained models intact and adapting exclusively via the last token, our approach achieves a harmonious balance between stability and plasticity. Extensive experiments demonstrate TOSCA's state-of-the-art performance while introducing ~8 times fewer parameters compared to prior methods.</article>","contentLength":1560,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"User Awareness and Perspectives Survey on Privacy, Security and Usability of Auditory Prostheses","url":"https://arxiv.org/abs/2502.14761","date":1740114000,"author":"","guid":7982,"unread":true,"content":"<article>arXiv:2502.14761v1 Announce Type: new \nAbstract: According to the World Health Organization, over 466 million people worldwide suffer from disabling hearing loss, with approximately 34 million of these being children. Hearing aids (HA) and cochlear implants (CI) have become indispensable tools for restoring hearing and enhancing the quality of life for individuals with hearing impairments. Clinical research and consumer studies indicate that users of HAs and CIs report significant improvements in their daily lives, including enhanced communication abilities and social engagement and reduced psychological stress. Modern auditory prosthetic devices are more advanced and interconnected with digital networks to add functionality, such as streaming audio directly from smartphones and other devices, remote adjustments by audiologists, integration with smart home systems, and access to artificial intelligence-driven sound enhancement features. With this interconnectivity, issues surrounding data privacy and security have become increasingly pertinent. There is limited research on the usability perceptions of current HA and CI models from the perspective of end-users. In addition, no studies have investigated consumer mental models during the purchasing process, particularly which factors they prioritize when selecting a device. In this study, we assessed participants' satisfaction levels with various features of their auditory prostheses. This work contributes to the field by addressing gaps in user perceptions of HA and CI usability, identifying key factors in consumer purchasing decisions, and highlighting the need for improved privacy and security awareness and education among users.</article>","contentLength":1708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations","url":"https://arxiv.org/abs/2502.14760","date":1740114000,"author":"","guid":7983,"unread":true,"content":"<article>arXiv:2502.14760v1 Announce Type: new \nAbstract: A fundamental problem in combinatorial optimization is identifying equivalent formulations, which can lead to more efficient solution strategies and deeper insights into a problem's computational complexity. The need to automatically identify equivalence between problem formulations has grown as optimization copilots--systems that generate problem formulations from natural language descriptions--have proliferated. However, existing approaches to checking formulation equivalence lack grounding, relying on simple heuristics which are insufficient for rigorous validation. Inspired by Karp reductions, in this work we introduce quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings, enabling scalable and reliable equivalence verification. To evaluate our approach, we construct the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence.</article>","contentLength":1406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems","url":"https://arxiv.org/abs/2502.14759","date":1740114000,"author":"","guid":7984,"unread":true,"content":"<article>arXiv:2502.14759v1 Announce Type: new \nAbstract: Retrieval-augmented generation (RAG) has emerged as an approach to augment large language models (LLMs) by reducing their reliance on static knowledge and improving answer factuality. RAG retrieves relevant context snippets and generates an answer based on them. Despite its increasing industrial adoption, systematic exploration of RAG components is lacking, particularly regarding the ideal size of provided context, and the choice of base LLM and retrieval method. To help guide development of robust RAG systems, we evaluate various context sizes, BM25 and semantic search as retrievers, and eight base LLMs. Moving away from the usual RAG evaluation with short answers, we explore the more challenging long-form question answering in two domains, where a good answer has to utilize the entire context. Our findings indicate that final QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that. Finally, we show that different general-purpose LLMs excel in the biomedical domain than the encyclopedic one, and that open-domain evidence retrieval in large corpora is challenging.</article>","contentLength":1167,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"TritonBench: Benchmarking Large Language Model Capabilities for Generating Triton Operators","url":"https://arxiv.org/abs/2502.14752","date":1740114000,"author":"","guid":7985,"unread":true,"content":"<article>arXiv:2502.14752v1 Announce Type: new \nAbstract: Triton, a high-level Python-like language designed for building efficient GPU kernels, is widely adopted in deep learning frameworks due to its portability, flexibility, and accessibility. However, programming and parallel optimization still require considerable trial and error from Triton developers. Despite advances in large language models (LLMs) for conventional code generation, these models struggle to generate accurate, performance-optimized Triton code, as they lack awareness of its specifications and the complexities of GPU programming. More critically, there is an urgent need for systematic evaluations tailored to Triton. In this work, we introduce TritonBench, the first comprehensive benchmark for Triton operator generation. TritonBench features two evaluation channels: a curated set of 184 real-world operators from GitHub and a collection of operators aligned with PyTorch interfaces. Unlike conventional code benchmarks prioritizing functional correctness, TritonBench also profiles efficiency performance on widely deployed GPUs aligned with industry applications. Our study reveals that current state-of-the-art code LLMs struggle to generate efficient Triton operators, highlighting a significant gap in high-performance code generation. TritonBench will be available at https://github.com/thunlp/TritonBench.</article>","contentLength":1385,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of LLMs","url":"https://arxiv.org/abs/2502.14748","date":1740114000,"author":"","guid":7986,"unread":true,"content":"<article>arXiv:2502.14748v1 Announce Type: new \nAbstract: A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models. Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored. This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets. While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints. Dataset available at https://huggingface. co/datasets/zli12321/Bills.</article>","contentLength":1333,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AIdeation: Designing a Human-AI Collaborative Ideation System for Concept Designers","url":"https://arxiv.org/abs/2502.14747","date":1740114000,"author":"","guid":7987,"unread":true,"content":"<article>arXiv:2502.14747v1 Announce Type: new \nAbstract: Concept designers in the entertainment industry create highly detailed, often imaginary environments for movies, games, and TV shows. Their early ideation phase requires intensive research, brainstorming, visual exploration, and combination of various design elements to form cohesive designs. However, existing AI tools focus on image generation from user specifications, lacking support for the unique needs and complexity of concept designers' workflows. Through a formative study with 12 professional designers, we captured their workflows and identified key requirements for AI-assisted ideation tools. Leveraging these insights, we developed AIdeation to support early ideation by brainstorming design concepts with flexible searching and recombination of reference images. A user study with 16 professional designers showed that AIdeation significantly enhanced creativity, ideation efficiency, and satisfaction (all p&lt;.01) compared to current tools and workflows. A field study with 4 studios for 1 week provided insights into AIdeation's benefits and limitations in real-world projects. After the completion of the field study, two studios, covering films, television, and games, have continued to use AIdeation in their commercial projects to date, further validating AIdeation's improvement in ideation quality and efficiency.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Classical and quantum Coxeter codes: Extending the Reed-Muller family","url":"https://arxiv.org/abs/2502.14746","date":1740114000,"author":"","guid":7988,"unread":true,"content":"<article>arXiv:2502.14746v1 Announce Type: new \nAbstract: We introduce a class of binary linear codes that generalizes the Reed-Muller family by replacing the group $\\mathbb{Z}_2^m$ with an arbitrary finite Coxeter group. Similar to the Reed-Muller codes, this class is closed under duality and has rate determined by a Gaussian distribution. We also construct quantum CSS codes arising from the Coxeter codes, which admit transversal logical operators outside of the Clifford group.</article>","contentLength":474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SQL4NN: Validation and expressive querying of models as data","url":"https://arxiv.org/abs/2502.14745","date":1740114000,"author":"","guid":7989,"unread":true,"content":"<article>arXiv:2502.14745v1 Announce Type: new \nAbstract: We consider machine learning models, learned from data, to be an important, intensional, kind of data in themselves. As such, various analysis tasks on models can be thought of as queries over this intensional data, often combined with extensional data such as data for training or validation. We demonstrate that relational database systems and SQL can actually be well suited for many such tasks.</article>","contentLength":447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States","url":"https://arxiv.org/abs/2502.14744","date":1740114000,"author":"","guid":7990,"unread":true,"content":"<article>arXiv:2502.14744v1 Announce Type: new \nAbstract: The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect.</article>","contentLength":1257,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Agent Coordination across Diverse Applications: A Survey","url":"https://arxiv.org/abs/2502.14743","date":1740114000,"author":"","guid":7991,"unread":true,"content":"<article>arXiv:2502.14743v1 Announce Type: new \nAbstract: Multi-agent coordination studies the underlying mechanism enabling the trending spread of diverse multi-agent systems (MAS) and has received increasing attention, driven by the expansion of emerging applications and rapid AI advances. This survey outlines the current state of coordination research across applications through a unified understanding that answers four fundamental coordination questions: (1) what is coordination; (2) why coordination; (3) who to coordinate with; and (4) how to coordinate. Our purpose is to explore existing ideas and expertise in coordination and their connections across diverse applications, while identifying and highlighting emerging and promising research directions. First, general coordination problems that are essential to varied applications are identified and analyzed. Second, a number of MAS applications are surveyed, ranging from widely studied domains, e.g., search and rescue, warehouse automation and logistics, and transportation systems, to emerging fields including humanoid and anthropomorphic robots, satellite systems, and large language models (LLMs). Finally, open challenges about the scalability, heterogeneity, and learning mechanisms of MAS are analyzed and discussed. In particular, we identify the hybridization of hierarchical and decentralized coordination, human-MAS coordination, and LLM-based MAS as promising future directions.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reinforcement Learning with Graph Attention for Routing and Wavelength Assignment with Lightpath Reuse","url":"https://arxiv.org/abs/2502.14741","date":1740114000,"author":"","guid":7992,"unread":true,"content":"<article>arXiv:2502.14741v1 Announce Type: new \nAbstract: Many works have investigated reinforcement learning (RL) for routing and spectrum assignment on flex-grid networks but only one work to date has examined RL for fixed-grid with flex-rate transponders, despite production systems using this paradigm. Flex-rate transponders allow existing lightpaths to accommodate new services, a task we term routing and wavelength assignment with lightpath reuse (RWA-LR). We re-examine this problem and present a thorough benchmarking of heuristic algorithms for RWA-LR, which are shown to have 6% increased throughput when candidate paths are ordered by number of hops, rather than total length. We train an RL agent for RWA-LR with graph attention networks for the policy and value functions to exploit the graph-structured data. We provide details of our methodology and open source all of our code for reproduction. We outperform the previous state-of-the-art RL approach by 2.5% (17.4 Tbps mean additional throughput) and the best heuristic by 1.2% (8.5 Tbps mean additional throughput). This marginal gain highlights the difficulty in learning effective RL policies on long horizon resource allocation tasks.</article>","contentLength":1198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YOLOv12: A Breakdown of the Key Architectural Features","url":"https://arxiv.org/abs/2502.14740","date":1740114000,"author":"","guid":7993,"unread":true,"content":"<article>arXiv:2502.14740v1 Announce Type: new \nAbstract: This paper presents an architectural analysis of YOLOv12, a significant advancement in single-stage, real-time object detection building upon the strengths of its predecessors while introducing key improvements. The model incorporates an optimised backbone (R-ELAN), 7x7 separable convolutions, and FlashAttention-driven area-based attention, improving feature extraction, enhanced efficiency, and robust detections. With multiple model variants, similar to its predecessors, YOLOv12 offers scalable solutions for both latency-sensitive and high-accuracy applications. Experimental results manifest consistent gains in mean average precision (mAP) and inference speed, making YOLOv12 a compelling choice for applications in autonomous systems, security, and real-time analytics. By achieving an optimal balance between computational efficiency and performance, YOLOv12 sets a new benchmark for real-time computer vision, facilitating deployment across diverse hardware platforms, from edge devices to high-performance clusters.</article>","contentLength":1076,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines","url":"https://arxiv.org/abs/2502.14739","date":1740114000,"author":"","guid":7994,"unread":true,"content":"<article>arXiv:2502.14739v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated remarkable proficiency in mainstream academic disciplines such as mathematics, physics, and computer science. However, human knowledge encompasses over 200 specialized disciplines, far exceeding the scope of existing benchmarks. The capabilities of LLMs in many of these specialized fields-particularly in light industry, agriculture, and service-oriented disciplines-remain inadequately evaluated. To address this gap, we present SuperGPQA, a comprehensive benchmark that evaluates graduate-level knowledge and reasoning capabilities across 285 disciplines. Our benchmark employs a novel Human-LLM collaborative filtering mechanism to eliminate trivial or ambiguous questions through iterative refinement based on both LLM responses and expert feedback. Our experimental results reveal significant room for improvement in the performance of current state-of-the-art LLMs across diverse knowledge domains (e.g., the reasoning-focused model DeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlighting the considerable gap between current model capabilities and artificial general intelligence. Additionally, we present comprehensive insights from our management of a large-scale annotation process, involving over 80 expert annotators and an interactive Human-LLM collaborative system, offering valuable methodological guidance for future research initiatives of comparable scope.</article>","contentLength":1495,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EAGER-LLM: Enhancing Large Language Models as Recommenders through Exogenous Behavior-Semantic Integration","url":"https://arxiv.org/abs/2502.14735","date":1740114000,"author":"","guid":7995,"unread":true,"content":"<article>arXiv:2502.14735v1 Announce Type: new \nAbstract: Large language models (LLMs) are increasingly leveraged as foundational backbones in the development of advanced recommender systems, offering enhanced capabilities through their extensive knowledge and reasoning. Existing llm-based recommender systems (RSs) often face challenges due to the significant differences between the linguistic semantics of pre-trained LLMs and the collaborative semantics essential for RSs. These systems use pre-trained linguistic semantics but learn collaborative semantics from scratch via the llm-Backbone. However, LLMs are not designed for recommendations, leading to inefficient collaborative learning, weak result correlations, and poor integration of traditional RS features. To address these challenges, we propose EAGER-LLM, a decoder-only llm-based generative recommendation framework that integrates endogenous and exogenous behavioral and semantic information in a non-intrusive manner. Specifically, we propose 1)dual-source knowledge-rich item indices that integrates indexing sequences for exogenous signals, enabling efficient link-wide processing; 2)non-invasive multiscale alignment reconstruction tasks guide the model toward a deeper understanding of both collaborative and semantic signals; 3)an annealing adapter designed to finely balance the model's recommendation performance with its comprehension capabilities. We demonstrate EAGER-LLM's effectiveness through rigorous testing on three public benchmarks.</article>","contentLength":1511,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sentence Smith: Formally Controllable Text Transformation and its Application to Evaluation of Text Embedding Models","url":"https://arxiv.org/abs/2502.14734","date":1740114000,"author":"","guid":7996,"unread":true,"content":"<article>arXiv:2502.14734v1 Announce Type: new \nAbstract: We propose the Sentence Smith framework that enables controlled and specified manipulation of text meaning. It consists of three main steps: 1. Parsing a sentence into a semantic graph, 2. Applying human-designed semantic manipulation rules, and 3. Generating text from the manipulated graph. A final filtering step (4.) ensures the validity of the applied transformation. To demonstrate the utility of Sentence Smith in an application study, we use it to generate hard negative pairs that challenge text embedding models. Since the controllable generation makes it possible to clearly isolate different types of semantic shifts, we can gain deeper insights into the specific strengths and weaknesses of widely used text embedding models, also addressing an issue in current benchmarking where linguistic phenomena remain opaque. Human validation confirms that the generations produced by Sentence Smith are highly accurate.</article>","contentLength":973,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FLIGHT: Facility Location Integrating Generalized, Holistic Theory of Welfare","url":"https://arxiv.org/abs/2502.14732","date":1740114000,"author":"","guid":7997,"unread":true,"content":"<article>arXiv:2502.14732v1 Announce Type: new \nAbstract: The Facility Location Problem (FLP) is a well-studied optimization problem with applications in many real-world scenarios. Past literature has explored the solutions from different perspectives to tackle FLPs. These include investigating FLPs under objective functions such as utilitarian, egalitarian, Nash welfare, etc. Also, there is no treatment for asymmetric welfare functions around the facility. We propose a unified framework, FLIGHT, to accommodate a broad class of welfare notions. The framework undergoes rigorous theoretical analysis, and we prove some structural properties of the solution to FLP. Additionally, we provide approximation bounds, which provide insight into an interesting fact: as the number of agents arbitrarily increases, the choice of welfare notion is irrelevant. Furthermore, the paper also includes results around concentration bounds under certain distributional assumptions over the preferred locations of agents.</article>","contentLength":1000,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"WavRAG: Audio-Integrated Retrieval Augmented Generation for Spoken Dialogue Models","url":"https://arxiv.org/abs/2502.14727","date":1740114000,"author":"","guid":7998,"unread":true,"content":"<article>arXiv:2502.14727v1 Announce Type: new \nAbstract: Retrieval Augmented Generation (RAG) has gained widespread adoption owing to its capacity to empower large language models (LLMs) to integrate external knowledge. However, existing RAG frameworks are primarily designed for text-based LLMs and rely on Automatic Speech Recognition to process speech input, which discards crucial audio information, risks transcription errors, and increases computational overhead. Therefore, we introduce WavRAG, the first retrieval augmented generation framework with native, end-to-end audio support. WavRAG offers two key features: 1) Bypassing ASR, WavRAG directly processes raw audio for both embedding and retrieval. 2) WavRAG integrates audio and text into a unified knowledge representation. Specifically, we propose the WavRetriever to facilitate the retrieval from a text-audio hybrid knowledge base, and further enhance the in-context capabilities of spoken dialogue models through the integration of chain-of-thought reasoning. In comparison to state-of-the-art ASR-Text RAG pipelines, WavRAG achieves comparable retrieval performance while delivering a 10x acceleration. Furthermore, WavRAG's unique text-audio hybrid retrieval capability extends the boundaries of RAG to the audio modality.</article>","contentLength":1285,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pitch Imperfect: Detecting Audio Deepfakes Through Acoustic Prosodic Analysis","url":"https://arxiv.org/abs/2502.14726","date":1740114000,"author":"","guid":7999,"unread":true,"content":"<article>arXiv:2502.14726v1 Announce Type: new \nAbstract: Audio deepfakes are increasingly in-differentiable from organic speech, often fooling both authentication systems and human listeners. While many techniques use low-level audio features or optimization black-box model training, focusing on the features that humans use to recognize speech will likely be a more long-term robust approach to detection. We explore the use of prosody, or the high-level linguistic features of human speech (e.g., pitch, intonation, jitter) as a more foundational means of detecting audio deepfakes. We develop a detector based on six classical prosodic features and demonstrate that our model performs as well as other baseline models used by the community to detect audio deepfakes with an accuracy of 93% and an EER of 24.7%. More importantly, we demonstrate the benefits of using a linguistic features-based approach over existing models by applying an adaptive adversary using an $L_{\\infty}$ norm attack against the detectors and using attention mechanisms in our training for explainability. We show that we can explain the prosodic features that have highest impact on the model's decision (Jitter, Shimmer and Mean Fundamental Frequency) and that other models are extremely susceptible to simple $L_{\\infty}$ norm attacks (99.3% relative degradation in accuracy). While overall performance may be similar, we illustrate the robustness and explainability benefits to a prosody feature approach to audio deepfake detection.</article>","contentLength":1508,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ranking Joint Policies in Dynamic Games using Evolutionary Dynamics","url":"https://arxiv.org/abs/2502.14724","date":1740114000,"author":"","guid":8000,"unread":true,"content":"<article>arXiv:2502.14724v1 Announce Type: new \nAbstract: Game-theoretic solution concepts, such as the Nash equilibrium, have been key to finding stable joint actions in multi-player games. However, it has been shown that the dynamics of agents' interactions, even in simple two-player games with few strategies, are incapable of reaching Nash equilibria, exhibiting complex and unpredictable behavior. Instead, evolutionary approaches can describe the long-term persistence of strategies and filter out transient ones, accounting for the long-term dynamics of agents' interactions. Our goal is to identify agents' joint strategies that result in stable behavior, being resistant to changes, while also accounting for agents' payoffs, in dynamic games. Towards this goal, and building on previous results, this paper proposes transforming dynamic games into their empirical forms by considering agents' strategies instead of agents' actions, and applying the evolutionary methodology $\\alpha$-Rank to evaluate and rank strategy profiles according to their long-term dynamics. This methodology not only allows us to identify joint strategies that are strong through agents' long-term interactions, but also provides a descriptive, transparent framework regarding the high ranking of these strategies. Experiments report on agents that aim to collaboratively solve a stochastic version of the graph coloring problem. We consider different styles of play as strategies to define the empirical game, and train policies realizing these strategies, using the DQN algorithm. Then we run simulations to generate the payoff matrix required by $\\alpha$-Rank to rank joint strategies.</article>","contentLength":1665,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-dataset synergistic in supervised learning to pre-label structural components in point clouds from shell construction scenes","url":"https://arxiv.org/abs/2502.14721","date":1740114000,"author":"","guid":8001,"unread":true,"content":"<article>arXiv:2502.14721v1 Announce Type: new \nAbstract: The significant effort required to annotate data for new training datasets hinders computer vision research and machine learning in the construction industry. This work explores adapting standard datasets and the latest transformer model architectures for point cloud semantic segmentation in the context of shell construction sites. Unlike common approaches focused on object segmentation of building interiors and furniture, this study addressed the challenges of segmenting complex structural components in Architecture, Engineering, and Construction (AEC). We establish a baseline through supervised training and a custom validation dataset, evaluate the cross-domain inference with large-scale indoor datasets, and utilize transfer learning to maximize segmentation performance with minimal new data. The findings indicate that with minimal fine-tuning, pre-trained transformer architectures offer an effective strategy for building component segmentation. Our results are promising for automating the annotation of new, previously unseen data when creating larger training resources and for the segmentation of frequently recurring objects.</article>","contentLength":1195,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Entity Framing and Role Portrayal in the News","url":"https://arxiv.org/abs/2502.14718","date":1740114000,"author":"","guid":8002,"unread":true,"content":"<article>arXiv:2502.14718v1 Announce Type: new \nAbstract: We introduce a novel multilingual hierarchical corpus annotated for entity framing and role portrayal in news articles. The dataset uses a unique taxonomy inspired by storytelling elements, comprising 22 fine-grained roles, or archetypes, nested within three main categories: protagonist, antagonist, and innocent. Each archetype is carefully defined, capturing nuanced portrayals of entities such as guardian, martyr, and underdog for protagonists; tyrant, deceiver, and bigot for antagonists; and victim, scapegoat, and exploited for innocents. The dataset includes 1,378 recent news articles in five languages (Bulgarian, English, Hindi, European Portuguese, and Russian) focusing on two critical domains of global significance: the Ukraine-Russia War and Climate Change. Over 5,800 entity mentions have been annotated with role labels. This dataset serves as a valuable resource for research into role portrayal and has broader implications for news analysis. We describe the characteristics of the dataset and the annotation process, and we report evaluation results on fine-tuned state-of-the-art multilingual transformers and hierarchical zero-shot learning using LLMs at the level of a document, a paragraph, and a sentence.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Knowledge Generation to Knowledge Verification: Examining the BioMedical Generative Capabilities of ChatGPT","url":"https://arxiv.org/abs/2502.14714","date":1740114000,"author":"","guid":8003,"unread":true,"content":"<article>arXiv:2502.14714v1 Announce Type: new \nAbstract: The generative capabilities of LLM models present opportunities in accelerating tasks and concerns with the authenticity of the knowledge it produces. To address the concerns, we present a computational approach that systematically evaluates the factual accuracy of biomedical knowledge that an LLM model has been prompted to generate. Our approach encompasses two processes: the generation of disease-centric associations and the verification of them using the semantic knowledge of the biomedical ontologies. Using ChatGPT as the select LLM model, we designed a set of prompt-engineering processes to generate linkages between diseases, drugs, symptoms, and genes to establish grounds for assessments. Experimental results demonstrate high accuracy in identifying disease terms (88%-97%), drug names (90%-91%), and genetic information (88%-98%). The symptom term identification accuracy was notably lower (49%-61%), as verified against the DOID, ChEBI, SYMPTOM, and GO ontologies accordingly. The verification of associations reveals literature coverage rates of (89%-91%) among disease-drug and disease-gene associations. The low identification accuracy for symptom terms also contributed to the verification of symptom-related associations (49%-62%).</article>","contentLength":1303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Efficient Pretraining with Group-Level Data Influence Modeling","url":"https://arxiv.org/abs/2502.14709","date":1740114000,"author":"","guid":8004,"unread":true,"content":"<article>arXiv:2502.14709v1 Announce Type: new \nAbstract: Data-efficient pretraining has shown tremendous potential to elevate scaling laws. This paper argues that effective pretraining data should be curated at the group level, treating a set of data points as a whole rather than as independent contributors. To achieve that, we propose Group-Level Data Influence Modeling (Group-MATES), a novel data-efficient pretraining method that captures and optimizes group-level data utility. Specifically, Group-MATES collects oracle group-level influences by locally probing the pretraining model with data sets. It then fine-tunes a relational data influence model to approximate oracles as relationship-weighted aggregations of individual influences. The fine-tuned model selects the data subset by maximizing its group-level influence prediction, with influence-aware clustering to enable efficient inference. Experiments on the DCLM benchmark demonstrate that Group-MATES achieves a 10% relative core score improvement on 22 downstream tasks over DCLM-Baseline and 5% over individual-influence-based methods, establishing a new state-of-the-art. Further analyses highlight the effectiveness of relational data influence models in capturing intricate interactions between data points.</article>","contentLength":1273,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building reliable sim driving agents by scaling self-play","url":"https://arxiv.org/abs/2502.14706","date":1740114000,"author":"","guid":8005,"unread":true,"content":"<article>arXiv:2502.14706v1 Announce Type: new \nAbstract: Simulation agents are essential for designing and testing systems that interact with humans, such as autonomous vehicles (AVs). These agents serve various purposes, from benchmarking AV performance to stress-testing the system's limits, but all use cases share a key requirement: reliability. A simulation agent should behave as intended by the designer, minimizing unintended actions like collisions that can compromise the signal-to-noise ratio of analyses. As a foundation for reliable sim agents, we propose scaling self-play to thousands of scenarios on the Waymo Open Motion Dataset under semi-realistic limits on human perception and control. Training from scratch on a single GPU, our agents nearly solve the full training set within a day. They generalize effectively to unseen test scenes, achieving a 99.8% goal completion rate with less than 0.8% combined collision and off-road incidents across 10,000 held-out scenarios. Beyond in-distribution generalization, our agents show partial robustness to out-of-distribution scenes and can be fine-tuned in minutes to reach near-perfect performance in those cases. Demonstrations of agent behaviors can be found at this link. We open-source both the pre-trained agents and the complete code base. Demonstrations of agent behaviors can be found at \\url{https://sites.google.com/view/reliable-sim-agents}.</article>","contentLength":1409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting","url":"https://arxiv.org/abs/2502.14704","date":1740114000,"author":"","guid":8006,"unread":true,"content":"<article>arXiv:2502.14704v1 Announce Type: new \nAbstract: Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning.</article>","contentLength":1113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Counter Pools: Counter Representation for Efficient Stream Processing","url":"https://arxiv.org/abs/2502.14699","date":1740114000,"author":"","guid":8007,"unread":true,"content":"<article>arXiv:2502.14699v1 Announce Type: new \nAbstract: Due to the large data volume and number of distinct elements, space is often the bottleneck of many stream processing systems. The data structures used by these systems often consist of counters whose optimization yields significant memory savings. The challenge lies in balancing the size of the counters: too small, and they overflow; too large, and memory capacity limits their number.\n  In this work, we suggest an efficient encoding scheme that sizes each counter according to its needs. Our approach uses fixed-sized pools of memory (e.g., a single memory word or 64 bits), where each pool manages a small number of counters. We pay special attention to performance and demonstrate considerable improvements for various streaming algorithms and workload characteristics.</article>","contentLength":825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"General Uncertainty Estimation with Delta Variances","url":"https://arxiv.org/abs/2502.14698","date":1740114000,"author":"","guid":8008,"unread":true,"content":"<article>arXiv:2502.14698v1 Announce Type: new \nAbstract: Decision makers may suffer from uncertainty induced by limited data. This may be mitigated by accounting for epistemic uncertainty, which is however challenging to estimate efficiently for large neural networks. To this extent we investigate Delta Variances, a family of algorithms for epistemic uncertainty quantification, that is computationally efficient and convenient to implement. It can be applied to neural networks and more general functions composed of neural networks. As an example we consider a weather simulator with a neural-network-based step function inside -- here Delta Variances empirically obtain competitive results at the cost of a single gradient computation. The approach is convenient as it requires no changes to the neural network architecture or training procedure. We discuss multiple ways to derive Delta Variances theoretically noting that special cases recover popular techniques and present a unified perspective on multiple related methods. Finally we observe that this general perspective gives rise to a natural extension and empirically show its benefit.</article>","contentLength":1141,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Revisiting Near-Far Field Boundary in Dual-Polarized XL-MIMO Systems","url":"https://arxiv.org/abs/2502.14694","date":1740114000,"author":"","guid":8009,"unread":true,"content":"<article>arXiv:2502.14694v1 Announce Type: new \nAbstract: Extremely large-scale multiple-input multiple-output (XL-MIMO) is expected to be an important technology in future sixth generation (6G) networks. Compared with conventional single-polarized XL-MIMO, where signals are transmitted and received in only one polarization direction, dual-polarized XL-MIMO systems achieve higher data rate by improving multiplexing performances, and thus are the focus of this paper. Due to enlarged aperture, near-field regions become non-negligible in XL-MIMO communications, necessitating accurate near-far field boundary characterizations. However, existing boundaries developed for single-polarized systems only consider phase or power differences across array elements while irrespective of cross-polarization discrimination (XPD) variances in dual-polarized XL-MIMO systems, deteriorating transmit covariance optimization performances. In this paper, we revisit near-far field boundaries for dual-polarized XL-MIMO systems by taking XPD differences into account, which faces the following challenge. Unlike existing near-far field boundaries, which only need to consider co-polarized channel components, deriving boundaries for dual-polarized XL-MIMO systems requires modeling joint effects of co-polarized and cross-polarized components. To address this issue, we model XPD variations across antennas and introduce a non-uniform XPD distance to complement existing near-far field boundaries. Based on the new distance criterion, we propose an efficient scheme to optimize transmit covariance. Numerical results validate our analysis and demonstrate the proposed algorithm's effectiveness.</article>","contentLength":1674,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search","url":"https://arxiv.org/abs/2502.14693","date":1740114000,"author":"","guid":8010,"unread":true,"content":"<article>arXiv:2502.14693v1 Announce Type: new \nAbstract: Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process.Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node's solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier.Applied to the various ML tasks, our approach demonstrates a6\\% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems.</article>","contentLength":1474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Parallelizing a modern GPU simulator","url":"https://arxiv.org/abs/2502.14691","date":1740114000,"author":"","guid":8011,"unread":true,"content":"<article>arXiv:2502.14691v1 Announce Type: new \nAbstract: Simulators are a primary tool in computer architecture research but are extremely computationally intensive. Simulating modern architectures with increased core counts and recent workloads can be challenging, even on modern hardware. This paper demonstrates that simulating some GPGPU workloads in a single-threaded state-of-the-art simulator such as Accel-sim can take more than five days. In this paper we present a simple approach to parallelize this simulator with minimal code changes by using OpenMP. Moreover, our parallelization technique is deterministic, so the simulator provides the same results for single-threaded and multi-threaded simulations. Compared to previous works, we achieve a higher speed-up, and, more importantly, the parallel simulation does not incur any inaccuracies. When we run the simulator with 16 threads, we achieve an average speed-up of 5.8x and reach 14x in some workloads. This allows researchers to simulate applications that take five days in less than 12 hours. By speeding up simulations, researchers can model larger systems, simulate bigger workloads, add more detail to the model, increase the efficiency of the hardware platform where the simulator is run, and obtain results sooner.</article>","contentLength":1280,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two-Sided Matching with Resource-Regional Caps","url":"https://arxiv.org/abs/2502.14690","date":1740114000,"author":"","guid":8012,"unread":true,"content":"<article>arXiv:2502.14690v1 Announce Type: new \nAbstract: We study two-sided many-to-one matching problems under a novel type of distributional constraints, resource-regional caps. In the context of college admissions, under resource-regional caps, an admitted student may be provided with a unit of some resource through a college, which belongs to a region possessing some amount of this resource. A student may be admitted to a college with at most one unit of any resource, i.e., all resources are close substitutes, e.g., dorms on the campus, dorms outside the campus, subsidies for renting a room, etc. The core feature of our model is that students are allowed to be admitted without any resource, which breaks heredity property of previously studied models with regions.\n  It is well known that a stable matching may not exist under markets with regional constraints. Thus, we focus on three weakened versions of stability that restore existence under resource-regional caps: envy-freeness, non-wastefulness, and novel direct-envy stability. For each version of stability we design corresponding matching mechanism(s). Finally, we compare stability performances of constructed mechanisms using simulations, and conclude that more sophisticated direct-envy stable mechanism is the go-to mechanism for maximal stability of the resulting matching under resource-regional caps.</article>","contentLength":1372,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A posteriori error bounds for finite element approximations of steady-state mean field games","url":"https://arxiv.org/abs/2502.14687","date":1740114000,"author":"","guid":8013,"unread":true,"content":"<article>arXiv:2502.14687v1 Announce Type: new \nAbstract: We analyze a posteriori error bounds for stabilized finite element discretizations of second-order steady-state mean field games. We prove the local equivalence between the $H^1$-norm of the error and the dual norm of the residual. We then derive reliable and efficient estimators for a broad class of stabilized first-order finite element methods. We also show that in the case of affine-preserving stabilizations, the estimator can be further simplified to the standard residual estimator. Numerical experiments illustrate the computational gains in efficiency and accuracy from the estimators in the context of adaptive methods.</article>","contentLength":680,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SegAug: CTC-Aligned Segmented Augmentation For Robust RNN-Transducer Based Speech Recognition","url":"https://arxiv.org/abs/2502.14685","date":1740114000,"author":"","guid":8014,"unread":true,"content":"<article>arXiv:2502.14685v1 Announce Type: new \nAbstract: RNN-Transducer (RNN-T) is a widely adopted architecture in speech recognition, integrating acoustic and language modeling in an end-to-end framework. However, the RNN-T predictor tends to over-rely on consecutive word dependencies in training data, leading to high deletion error rates, particularly with less common or out-of-domain phrases. Existing solutions, such as regularization and data augmentation, often compromise other aspects of performance. We propose SegAug, an alignment-based augmentation technique that generates contextually varied audio-text pairs with low sentence-level semantics. This method encourages the model to focus more on acoustic features while diversifying the learned textual patterns of its internal language model, thereby reducing deletion errors and enhancing overall performance. Evaluations on the LibriSpeech and Tedlium-v3 datasets demonstrate a relative WER reduction of up to 12.5% on small-scale and 6.9% on large-scale settings. Notably, most of the improvement stems from reduced deletion errors, with relative reductions of 45.4% and 18.5%, respectively. These results highlight SegAug's effectiveness in improving RNN-T's robustness, offering a promising solution for enhancing speech recognition performance across diverse and challenging scenarios.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CDGS: Confidence-Aware Depth Regularization for 3D Gaussian Splatting","url":"https://arxiv.org/abs/2502.14684","date":1740114000,"author":"","guid":8015,"unread":true,"content":"<article>arXiv:2502.14684v1 Announce Type: new \nAbstract: 3D Gaussian Splatting (3DGS) has shown significant advantages in novel view synthesis (NVS), particularly in achieving high rendering speeds and high-quality results. However, its geometric accuracy in 3D reconstruction remains limited due to the lack of explicit geometric constraints during optimization. This paper introduces CDGS, a confidence-aware depth regularization approach developed to enhance 3DGS. We leverage multi-cue confidence maps of monocular depth estimation and sparse Structure-from-Motion depth to adaptively adjust depth supervision during the optimization process. Our method demonstrates improved geometric detail preservation in early training stages and achieves competitive performance in both NVS quality and geometric accuracy. Experiments on the publicly available Tanks and Temples benchmark dataset show that our method achieves more stable convergence behavior and more accurate geometric reconstruction results, with improvements of up to 2.31 dB in PSNR for NVS and consistently lower geometric errors in M3C2 distance metrics. Notably, our method reaches comparable F-scores to the original 3DGS with only 50% of the training iterations. We expect this work will facilitate the development of efficient and accurate 3D reconstruction systems for real-world applications such as digital twin creation, heritage preservation, or forestry applications.</article>","contentLength":1436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging the Gap: Transforming Natural Language Questions into SQL Queries via Abstract Query Pattern and Contextual Schema Markup","url":"https://arxiv.org/abs/2502.14682","date":1740114000,"author":"","guid":8016,"unread":true,"content":"<article>arXiv:2502.14682v1 Announce Type: new \nAbstract: Large language models have demonstrated excellent performance in many tasks, including Text-to-SQL, due to their powerful in-context learning capabilities. They are becoming the mainstream approach for Text-to-SQL. However, these methods still have a significant gap compared to human performance, especially on complex questions. As the complexity of questions increases, the gap between questions and SQLs increases. We identify two important gaps: the structural mapping gap and the lexical mapping gap. To tackle these two gaps, we propose PAS-SQL, an efficient SQL generation pipeline based on LLMs, which alleviates gaps through Abstract Query Pattern (AQP) and Contextual Schema Markup (CSM). AQP aims to obtain the structural pattern of the question by removing database-related information, which enables us to find structurally similar demonstrations. CSM aims to associate database-related text span in the question with specific tables or columns in the database, which alleviates the lexical mapping gap. Experimental results on the Spider and BIRD datasets demonstrate the effectiveness of our proposed method. Specifically, PAS-SQL + GPT-4o sets a new state-of-the-art on the Spider benchmark with an execution accuracy of 87.9\\%, and achieves leading results on the BIRD dataset with an execution accuracy of 64.67\\%.</article>","contentLength":1382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"seqKAN: Sequence processing with Kolmogorov-Arnold Networks","url":"https://arxiv.org/abs/2502.14681","date":1740114000,"author":"","guid":8017,"unread":true,"content":"<article>arXiv:2502.14681v1 Announce Type: new \nAbstract: Kolmogorov-Arnold Networks (KANs) have been recently proposed as a machine learning framework that is more interpretable and controllable than the multi-layer perceptron. Various network architectures have been proposed within the KAN framework targeting different tasks and application domains, including sequence processing.\n  This paper proposes seqKAN, a new KAN architecture for sequence processing. Although multiple sequence processing KAN architectures have already been proposed, we argue that seqKAN is more faithful to the core concept of the KAN framework. Furthermore, we empirically demonstrate that it achieves better results.\n  The empirical evaluation is performed on generated data from a complex physics problem on an interpolation and an extrapolation task. Using this dataset we compared seqKAN against a prior KAN network for timeseries prediction, recurrent deep networks, and symbolic regression. seqKAN substantially outperforms all architectures, particularly on the extrapolation dataset, while also being the most transparent.</article>","contentLength":1103,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Disentangled Latent Spaces for Reduced Order Models using Deterministic Autoencoders","url":"https://arxiv.org/abs/2502.14679","date":1740114000,"author":"","guid":8018,"unread":true,"content":"<article>arXiv:2502.14679v1 Announce Type: new \nAbstract: Data-driven reduced-order models based on autoencoders generally lack interpretability compared to classical methods such as the proper orthogonal decomposition. More interpretability can be gained by disentangling the latent variables and analyzing the resulting modes. For this purpose, probabilistic $\\beta$-variational autoencoders ($\\beta$-VAEs) are frequently used in computational fluid dynamics and other simulation sciences. Using a benchmark periodic flow dataset, we show that competitive results can be achieved using non-probabilistic autoencoder approaches that either promote orthogonality or penalize correlation between latent variables. Compared to probabilistic autoencoders, these approaches offer more robustness with respect to the choice of hyperparameters entering the loss function. We further demonstrate the ability of a non-probabilistic approach to identify a reduced number of active latent variables by introducing a correlation penalty, a function also known from the use of $\\beta$-VAE. The investigated probabilistic and non-probabilistic autoencoder models are finally used for the dimensionality reduction of aircraft ditching loads, which serves as an industrial application in this work.</article>","contentLength":1274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Get Your LLM to Generate Challenging Problems for Evaluation","url":"https://arxiv.org/abs/2502.14678","date":1740114000,"author":"","guid":8019,"unread":true,"content":"<article>arXiv:2502.14678v1 Announce Type: new \nAbstract: The pace of evolution of Large Language Models (LLMs) necessitates new approaches for rigorous and comprehensive evaluation. Traditional human annotation is increasingly impracticable due to the complexities and costs involved in generating high-quality, challenging problems. In this work, we introduce CHASE, a unified framework to synthetically generate challenging problems using LLMs without human involvement. For a given task, our approach builds a hard problem in a bottom-up manner from simpler components. Moreover, our framework decomposes the generation process into independently verifiable sub-tasks, thereby ensuring a high level of quality and correctness. We implement CHASE to create evaluation benchmarks across three diverse domains: (1) document-based question answering, (2) repository-level code completion, and (3) math reasoning. The performance of state-of-the-art LLMs on these synthetic benchmarks lies in the range of 40-60% accuracy, thereby demonstrating the effectiveness of our framework at generating challenging problems. We publicly release our benchmarks and code.</article>","contentLength":1150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-Constrained Synthesis of Training Data for De-Identification","url":"https://arxiv.org/abs/2502.14677","date":1740114000,"author":"","guid":8020,"unread":true,"content":"<article>arXiv:2502.14677v1 Announce Type: new \nAbstract: Many sensitive domains -- such as the clinical domain -- lack widely available datasets due to privacy risks. The increasing generative capabilities of large language models (LLMs) have made synthetic datasets a viable path forward. In this study, we domain-adapt LLMs to the clinical domain and generate synthetic clinical texts that are machine-annotated with tags for personally identifiable information using capable encoder-based NER models. The synthetic corpora are then used to train synthetic NER models. The results show that training NER models using synthetic corpora incurs only a small drop in predictive performance. The limits of this process are investigated in a systematic ablation study -- using both Swedish and Spanish data. Our analysis shows that smaller datasets can be sufficient for domain-adapting LLMs for data synthesis. Instead, the effectiveness of this process is almost entirely contingent on the performance of the machine-annotating NER models trained using the original data.</article>","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"BP-SGCN: Behavioral Pseudo-Label Informed Sparse Graph Convolution Network for Pedestrian and Heterogeneous Trajectory Prediction","url":"https://arxiv.org/abs/2502.14676","date":1740114000,"author":"","guid":8021,"unread":true,"content":"<article>arXiv:2502.14676v1 Announce Type: new \nAbstract: Trajectory prediction allows better decision-making in applications of autonomous vehicles or surveillance by predicting the short-term future movement of traffic agents. It is classified into pedestrian or heterogeneous trajectory prediction. The former exploits the relatively consistent behavior of pedestrians, but is limited in real-world scenarios with heterogeneous traffic agents such as cyclists and vehicles. The latter typically relies on extra class label information to distinguish the heterogeneous agents, but such labels are costly to annotate and cannot be generalized to represent different behaviors within the same class of agents. In this work, we introduce the behavioral pseudo-labels that effectively capture the behavior distributions of pedestrians and heterogeneous agents solely based on their motion features, significantly improving the accuracy of trajectory prediction. To implement the framework, we propose the Behavioral Pseudo-Label Informed Sparse Graph Convolution Network (BP-SGCN) that learns pseudo-labels and informs to a trajectory predictor. For optimization, we propose a cascaded training scheme, in which we first learn the pseudo-labels in an unsupervised manner, and then perform end-to-end fine-tuning on the labels in the direction of increasing the trajectory prediction accuracy. Experiments show that our pseudo-labels effectively model different behavior clusters and improve trajectory prediction. Our proposed BP-SGCN outperforms existing methods using both pedestrian (ETH/UCY, pedestrian-only SDD) and heterogeneous agent datasets (SDD, Argoverse 1).</article>","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Set Visualizations for Comparing and Evaluating Machine Learning Models","url":"https://arxiv.org/abs/2502.14675","date":1740114000,"author":"","guid":8022,"unread":true,"content":"<article>arXiv:2502.14675v1 Announce Type: new \nAbstract: Machine learning practitioners often need to compare multiple models to select the best one for their application. However, current methods of comparing models fall short because they rely on aggregate metrics that can be difficult to interpret or do not provide enough information to understand the differences between models. To better support the comparison of models, we propose set visualizations of model outputs to enable easier model-to-model comparison. We outline the requirements for using sets to compare machine learning models and demonstrate how this approach can be applied to various machine learning tasks. We also introduce SetMLVis, an interactive system that utilizes set visualizations to compare object detection models. Our evaluation shows that SetMLVis outperforms traditional visualization techniques in terms of task completion and reduces cognitive workload for users. Supplemental materials can be found at https://osf.io/afksu/?view_only=bb7f259426ad425f81d0518a38c597be.</article>","contentLength":1051,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChunkFormer: Masked Chunking Conformer For Long-Form Speech Transcription","url":"https://arxiv.org/abs/2502.14673","date":1740114000,"author":"","guid":8023,"unread":true,"content":"<article>arXiv:2502.14673v1 Announce Type: new \nAbstract: Deploying ASR models at an industrial scale poses significant challenges in hardware resource management, especially for long-form transcription tasks where audio may last for hours. Large Conformer models, despite their capabilities, are limited to processing only 15 minutes of audio on an 80GB GPU. Furthermore, variable input lengths worsen inefficiencies, as standard batching leads to excessive padding, increasing resource consumption and execution time. To address this, we introduce ChunkFormer, an efficient ASR model that uses chunk-wise processing with relative right context, enabling long audio transcriptions on low-memory GPUs. ChunkFormer handles up to 16 hours of audio on an 80GB GPU, 1.5x longer than the current state-of-the-art FastConformer, while also boosting long-form transcription performance with up to 7.7% absolute reduction on word error rate and maintaining accuracy on shorter tasks compared to Conformer. By eliminating the need for padding in standard batching, ChunkFormer's masked batching technique reduces execution time and memory usage by more than 3x in batch processing, substantially reducing costs for a wide range of ASR systems, particularly regarding GPU resources for models serving in real-world applications.</article>","contentLength":1309,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explanations of Deep Language Models Explain Language Representations in the Brain","url":"https://arxiv.org/abs/2502.14671","date":1740114000,"author":"","guid":8024,"unread":true,"content":"<article>arXiv:2502.14671v1 Announce Type: new \nAbstract: Recent advances in artificial intelligence have given rise to large language models (LLMs) that not only achieve human-like performance but also share computational principles with the brain's language processing mechanisms. While previous research has primarily focused on aligning LLMs' internal representations with neural activity, we introduce a novel approach that leverages explainable AI (XAI) methods to forge deeper connections between the two domains. Using attribution methods, we quantified how preceding words contribute to an LLM's next-word predictions and employed these explanations to predict fMRI recordings from participants listening to the same narratives. Our findings demonstrate that attribution methods robustly predict brain activity across the language network, surpassing traditional internal representations in early language areas. This alignment is hierarchical: early-layer explanations correspond to the initial stages of language processing in the brain, while later layers align with more advanced stages. Moreover, the layers more influential on LLM next-word prediction$\\unicode{x2014}$those with higher attribution scores$\\unicode{x2014}$exhibited stronger alignment with neural activity. This work establishes a bidirectional bridge between AI and neuroscience. First, we demonstrate that attribution methods offer a powerful lens for investigating the neural mechanisms of language comprehension, revealing how meaning emerges from preceding context. Second, we propose using brain alignment as a metric to evaluate the validity of attribution methods, providing a framework for assessing their biological plausibility.</article>","contentLength":1710,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AlphaMaze: Enhancing Large Language Models' Spatial Intelligence via GRPO","url":"https://arxiv.org/abs/2502.14669","date":1740114000,"author":"","guid":8025,"unread":true,"content":"<article>arXiv:2502.14669v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities in language processing, yet they often struggle with tasks requiring genuine visual spatial reasoning. In this paper, we introduce a novel two-stage training framework designed to equip standard LLMs with visual reasoning abilities for maze navigation. First, we leverage Supervised Fine Tuning (SFT) on a curated dataset of tokenized maze representations to teach the model to predict step-by-step movement commands. Next, we apply Group Relative Policy Optimization (GRPO)-a technique used in DeepSeekR1-with a carefully crafted reward function to refine the model's sequential decision-making and encourage emergent chain-of-thought behaviors. Experimental results on synthetically generated mazes show that while a baseline model fails to navigate the maze, the SFT-trained model achieves 86% accuracy, and further GRPO fine-tuning boosts accuracy to 93%. Qualitative analyses reveal that GRPO fosters more robust and self-corrective reasoning, highlighting the potential of our approach to bridge the gap between language models and visual spatial tasks. These findings offer promising implications for applications in robotics, autonomous navigation, and other domains that require integrated visual and sequential reasoning.</article>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Restricted Isometry Property for Measurements from Group Orbits","url":"https://arxiv.org/abs/2502.14663","date":1740114000,"author":"","guid":8026,"unread":true,"content":"<article>arXiv:2502.14663v1 Announce Type: new \nAbstract: It is known that sparse recovery by measurements from random circulant matrices provides good recovery bounds. We generalize this to measurements that arise as a random orbit of a group representation for some finite group G. We derive estimates for the number of measurements required to guarantee the restricted isometry property with high probability. Following this, we present several examples highlighting the role of appropriate representation-theoretic assumptions.</article>","contentLength":522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InstructAgent: Building User Controllable Recommender via LLM Agent","url":"https://arxiv.org/abs/2502.14662","date":1740114000,"author":"","guid":8027,"unread":true,"content":"<article>arXiv:2502.14662v1 Announce Type: new \nAbstract: Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure. To this end, we first construct four recommendation datasets, denoted as $\\dataset$, along with user instructions for each record.</article>","contentLength":1660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Quasi-Monte Carlo for Bayesian shape inversion governed by the Poisson problem subject to Gevrey regular domain deformations","url":"https://arxiv.org/abs/2502.14661","date":1740114000,"author":"","guid":8028,"unread":true,"content":"<article>arXiv:2502.14661v1 Announce Type: new \nAbstract: We consider the application of a quasi-Monte Carlo cubature rule to Bayesian shape inversion subject to the Poisson equation under Gevrey regular parameterizations of domain uncertainty. We analyze the parametric regularity of the associated posterior distribution and design randomly shifted rank-1 lattice rules which can be shown to achieve dimension-independent, faster-than-Monte Carlo cubature convergence rates for high-dimensional integrals over the posterior distribution. In addition, we consider the effect of dimension truncation and finite element discretization errors for this model. Finally, a series of numerical experiments are presented to validate the theoretical results.</article>","contentLength":741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond the Surface: Uncovering Implicit Locations with LLMs for Personalized Local News","url":"https://arxiv.org/abs/2502.14660","date":1740114000,"author":"","guid":8029,"unread":true,"content":"<article>arXiv:2502.14660v1 Announce Type: new \nAbstract: News recommendation systems personalize homepage content to boost engagement, but factors like content type, editorial stance, and geographic focus impact recommendations. Local newspapers balance coverage across regions, yet identifying local articles is challenging due to implicit location cues like slang or landmarks.\n  Traditional methods, such as Named Entity Recognition (NER) and Knowledge Graphs, infer locations, but Large Language Models (LLMs) offer new possibilities while raising concerns about accuracy and explainability.\n  This paper explores LLMs for local article classification in Taboola's \"Homepage For You\" system, comparing them to traditional techniques. Key findings: (1) Knowledge Graphs enhance NER models' ability to detect implicit locations, (2) LLMs outperform traditional methods, and (3) LLMs can effectively identify local content without requiring Knowledge Graph integration.\n  Offline evaluations showed LLMs excel at implicit location classification, while online A/B tests showed a significant increased in local views. A scalable pipeline integrating LLM-based location classification boosted local article distribution by 27%, preserving newspapers' brand identity and enhancing homepage personalization.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MAGO-SP: Detection and Correction of Water-Fat Swaps in Magnitude-Only VIBE MRI","url":"https://arxiv.org/abs/2502.14659","date":1740114000,"author":"","guid":8030,"unread":true,"content":"<article>arXiv:2502.14659v1 Announce Type: new \nAbstract: Volume Interpolated Breath-Hold Examination (VIBE) MRI generates images suitable for water and fat signal composition estimation. While the two-point VIBE provides water-fat-separated images, the six-point VIBE allows estimation of the effective transversal relaxation rate R2* and the proton density fat fraction (PDFF), which are imaging markers for health and disease. Ambiguity during signal reconstruction can lead to water-fat swaps. This shortcoming challenges the application of VIBE-MRI for automated PDFF analyses of large-scale clinical data and of population studies. This study develops an automated pipeline to detect and correct water-fat swaps in non-contrast-enhanced VIBE images. Our three-step pipeline begins with training a segmentation network to classify volumes as \"fat-like\" or \"water-like,\" using synthetic water-fat swaps generated by merging fat and water volumes with Perlin noise. Next, a denoising diffusion image-to-image network predicts water volumes as signal priors for correction. Finally, we integrate this prior into a physics-constrained model to recover accurate water and fat signals. Our approach achieves a &lt; 1% error rate in water-fat swap detection for a 6-point VIBE. Notably, swaps disproportionately affect individuals in the Underweight and Class 3 Obesity BMI categories. Our correction algorithm ensures accurate solution selection in chemical phase MRIs, enabling reliable PDFF estimation. This forms a solid technical foundation for automated large-scale population imaging analysis.</article>","contentLength":1586,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A hybrid minimizing movement and neural network approach to Willmore flow","url":"https://arxiv.org/abs/2502.14656","date":1740114000,"author":"","guid":8031,"unread":true,"content":"<article>arXiv:2502.14656v1 Announce Type: new \nAbstract: We present a hybrid method combining a minimizing movement scheme with neural operators for the simulation of phase field-based Willmore flow. The minimizing movement component is based on a standard optimization problem on a regular grid whereas the functional to be minimized involves a neural approximation of mean curvature flow proposed by Bretin et al. Numerical experiments confirm stability for large time step sizes, consistency and significantly reduced computational cost compared to a traditional finite element method. Moreover, applications demonstrate its effectiveness in surface fairing and reconstructing of damaged shapes. Thus, the approach offers a robust and efficient tool for geometry processing.</article>","contentLength":769,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gender Influence on Student Teams' Online Communication in Software Engineering Education","url":"https://arxiv.org/abs/2502.14653","date":1740114000,"author":"","guid":8032,"unread":true,"content":"<article>arXiv:2502.14653v1 Announce Type: new \nAbstract: Collaboration is crucial in Software Engineering (SE), yet factors like gender bias can shape team dynamics and behaviours. This study examines an eight-week project involving 39 SE students across eight teams contributing to GitHub projects. Using a mixed-methods approach, we analysed Slack communications to identify gender differences, comparing how they influence learning gains. We found higher help-seeking and leadership behaviours in the all-woman team, while men responded more slowly. Although communication did not affect final grades, we identified statistical significance correlating communications with students' understanding of software development. With some students putting more effort into collaboration, future work can investigate diversity and inclusion training to balance these efforts. The observed link between team engagement and a higher understanding of software development highlights the potential for teaching strategies that promote help-seeking. These findings could guide efforts to address challenges student SE teams face when using communication platforms and foster more equitable collaborative learning in Software Engineering Education.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Variance Reduction Methods Do Not Need to Compute Full Gradients: Improved Efficiency through Shuffling","url":"https://arxiv.org/abs/2502.14648","date":1740114000,"author":"","guid":8033,"unread":true,"content":"<article>arXiv:2502.14648v1 Announce Type: new \nAbstract: In today's world, machine learning is hard to imagine without large training datasets and models. This has led to the use of stochastic methods for training, such as stochastic gradient descent (SGD). SGD provides weak theoretical guarantees of convergence, but there are modifications, such as Stochastic Variance Reduced Gradient (SVRG) and StochAstic Recursive grAdient algoritHm (SARAH), that can reduce the variance. These methods require the computation of the full gradient occasionally, which can be time consuming. In this paper, we explore variants of variance reduction algorithms that eliminate the need for full gradient computations. To make our approach memory-efficient and avoid full gradient computations, we use two key techniques: the shuffling heuristic and idea of SAG/SAGA methods. As a result, we improve existing estimates for variance reduction algorithms without the full gradient computations. Additionally, for the non-convex objective function, our estimate matches that of classic shuffling methods, while for the strongly convex one, it is an improvement. We conduct comprehensive theoretical analysis and provide extensive experimental results to validate the efficiency and practicality of our methods for large-scale machine learning problems.</article>","contentLength":1327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs","url":"https://arxiv.org/abs/2502.14645","date":1740114000,"author":"","guid":8034,"unread":true,"content":"<article>arXiv:2502.14645v1 Announce Type: new \nAbstract: Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning","url":"https://arxiv.org/abs/2502.14644","date":1740114000,"author":"","guid":8035,"unread":true,"content":"<article>arXiv:2502.14644v1 Announce Type: new \nAbstract: Long context understanding remains challenging for large language models due to their limited context windows. This paper presents Long Input Fine-Tuning (LIFT), a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs in context, chooses to store and absorb the long input in parameter. By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL. We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.</article>","contentLength":1164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Length-Controlled Margin-Based Preference Optimization without Reference Model","url":"https://arxiv.org/abs/2502.14643","date":1740114000,"author":"","guid":8036,"unread":true,"content":"<article>arXiv:2502.14643v1 Announce Type: new \nAbstract: Direct Preference Optimization (DPO) is a widely adopted offline algorithm for preference-based reinforcement learning from human feedback (RLHF), designed to improve training simplicity and stability by redefining reward functions. However, DPO is hindered by several limitations, including length bias, memory inefficiency, and probability degradation. To address these challenges, we propose Length-Controlled Margin-Based Preference Optimization (LMPO), a more efficient and robust alternative. LMPO introduces a uniform reference model as an upper bound for the DPO loss, enabling a more accurate approximation of the original optimization objective. Additionally, an average log-probability optimization strategy is employed to minimize discrepancies between training and inference phases. A key innovation of LMPO lies in its Length-Controlled Margin-Based loss function, integrated within the Bradley-Terry framework. This loss function regulates response length while simultaneously widening the margin between preferred and rejected outputs. By doing so, it mitigates probability degradation for both accepted and discarded responses, addressing a significant limitation of existing methods. We evaluate LMPO against state-of-the-art preference optimization techniques on two open-ended large language models, Mistral and LLaMA3, across six conditional benchmarks. Our experimental results demonstrate that LMPO effectively controls response length, reduces probability degradation, and outperforms existing approaches. The code is available at \\url{https://github.com/gengxuli/LMPO}.</article>","contentLength":1643,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Far are LLMs from Being Our Digital Twins? A Benchmark for Persona-Based Behavior Chain Simulation","url":"https://arxiv.org/abs/2502.14642","date":1740114000,"author":"","guid":8037,"unread":true,"content":"<article>arXiv:2502.14642v1 Announce Type: new \nAbstract: Recently, LLMs have garnered increasing attention across academic disciplines for their potential as human digital twins, virtual proxies designed to replicate individuals and autonomously perform tasks such as decision-making, problem-solving, and reasoning on their behalf. However, current evaluations of LLMs primarily emphasize dialogue simulation while overlooking human behavior simulation, which is crucial for digital twins. To address this gap, we introduce BehaviorChain, the first benchmark for evaluating LLMs' ability to simulate continuous human behavior. BehaviorChain comprises diverse, high-quality, persona-based behavior chains, totaling 15,846 distinct behaviors across 1,001 unique personas, each with detailed history and profile metadata. For evaluation, we integrate persona metadata into LLMs and employ them to iteratively infer contextually appropriate behaviors within dynamic scenarios provided by BehaviorChain. Comprehensive evaluation results demonstrated that even state-of-the-art models struggle with accurately simulating continuous human behavior.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Condorcet Winners and Anscombes Paradox Under Weighted Binary Voting","url":"https://arxiv.org/abs/2502.14639","date":1740114000,"author":"","guid":8038,"unread":true,"content":"<article>arXiv:2502.14639v1 Announce Type: new \nAbstract: We consider voting on multiple independent binary issues. In addition, a weighting vector for each voter defines how important they consider each issue. The most natural way to aggregate the votes into a single unified proposal is issue-wise majority (IWM): taking a majority opinion for each issue. However, in a scenario known as Ostrogorski's Paradox, an IWM proposal may not be a Condorcet winner, or it may even fail to garner majority support in a special case known as Anscombe's Paradox.\n  We show that it is co-NP-hard to determine whether there exists a Condorcet-winning proposal even without weights. In contrast, we prove that the single-switch condition provides an Ostrogorski-free voting domain under identical weighting vectors. We show that verifying the condition can be achieved in linear time and no-instances admit short, efficiently computable proofs in the form of forbidden substructures. On the way, we give the simplest linear-time test for the voter/candidate-extremal-interval condition in approval voting and the simplest and most efficient algorithm for recognizing single-crossing preferences in ordinal voting.\n  We then tackle Anscombe's Paradox. Under identical weight vectors, we can guarantee a majority-supported proposal agreeing with IWM on strictly more than half of the overall weight, while with two distinct weight vectors, such proposals can get arbitrarily far from IWM. The severity of such examples is controlled by the maximum average topic weight $\\tilde{w}_{max}$: a simple bound derived from a partition-based approach is tight on a large portion of the range $\\tilde{w}_{max} \\in (0,1)$. Finally, we extend Wagner's rule to the weighted setting: an average majority across topics of at least $\\frac{3}{4}$'s precludes Anscombe's paradox from occurring.</article>","contentLength":1854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NAVIG: Natural Language-guided Analysis with Vision Language Models for Image Geo-localization","url":"https://arxiv.org/abs/2502.14638","date":1740114000,"author":"","guid":8039,"unread":true,"content":"<article>arXiv:2502.14638v1 Announce Type: new \nAbstract: Image geo-localization is the task of predicting the specific location of an image and requires complex reasoning across visual, geographical, and cultural contexts. While prior Vision Language Models (VLMs) have the best accuracy at this task, there is a dearth of high-quality datasets and models for analytical reasoning. We first create NaviClues, a high-quality dataset derived from GeoGuessr, a popular geography game, to supply examples of expert reasoning from language. Using this dataset, we present Navig, a comprehensive image geo-localization framework integrating global and fine-grained image information. By reasoning with language, Navig reduces the average distance error by 14% compared to previous state-of-the-art models while requiring fewer than 1000 training samples. Our dataset and code are available at https://github.com/SparrowZheyuan18/Navig/.</article>","contentLength":922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation","url":"https://arxiv.org/abs/2502.14637","date":1740114000,"author":"","guid":8040,"unread":true,"content":"<article>arXiv:2502.14637v1 Announce Type: new \nAbstract: Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves state-of-the-art performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 62x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at https://github.com/AngxiaoYue/ReQFlow.</article>","contentLength":1458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CER: Confidence Enhanced Reasoning in LLMs","url":"https://arxiv.org/abs/2502.14634","date":1740114000,"author":"","guid":8041,"unread":true,"content":"<article>arXiv:2502.14634v1 Announce Type: new \nAbstract: Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at https://github.com/ Aquasar11/CER.</article>","contentLength":1467,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmenting Coaching with GenAI: Insights into Use, Effectiveness, and Future Potential","url":"https://arxiv.org/abs/2502.14632","date":1740114000,"author":"","guid":8042,"unread":true,"content":"<article>arXiv:2502.14632v1 Announce Type: new \nAbstract: The integration of generative AI (GenAI) tools, particularly large language models (LLMs), is transforming professional coaching workflows. This study explores how coaches use GenAI, the perceived benefits and limitations of these tools, and broader attitudes toward AI-assisted coaching. A survey of 205 coaching professionals reveals widespread adoption of GenAI for research, content creation, and administrative support, while its role in relational and interpretative coaching remains limited. Findings indicate that AI literacy and perceived AI impact strongly predict GenAI adoption, with positive attitudes fostering greater use. Ethical considerations, particularly transparency and data privacy, are a key concern, with frequent AI users demonstrating greater ethical awareness. Regression analyses show that while perceived effectiveness drives GenAI adoption, concerns about AI replacing human coaches do not significantly influence usage. Coaches express interest in future AI capabilities that enhance personalization, real-time feedback, and administrative automation while maintaining human oversight. The study highlights that GenAI functions best as an augmentation tool rather than a replacement, emphasizing the need for AI literacy training, ethical guidelines, and human-centered AI integration. These findings contribute to the ongoing discourse on human-AI collaboration, advocating for responsible and effective AI adoption in professional coaching.</article>","contentLength":1523,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Synergistic Fusion of Multi-Source Knowledge via Evidence Theory for High-Entropy Alloy Discovery","url":"https://arxiv.org/abs/2502.14631","date":1740114000,"author":"","guid":8043,"unread":true,"content":"<article>arXiv:2502.14631v1 Announce Type: new \nAbstract: Discovering novel high-entropy alloys (HEAs) with desirable properties is challenging due to the vast compositional space and complex phase formation mechanisms. Efficient exploration of this space requires a strategic approach that integrates heterogeneous knowledge sources. Here, we propose a framework that systematically combines knowledge extracted from computational material datasets with domain knowledge distilled from scientific literature using large language models (LLMs). A central feature of this approach is the explicit consideration of element substitutability, identifying chemically similar elements that can be interchanged to potentially stabilize desired HEAs. Dempster-Shafer theory, a mathematical framework for reasoning under uncertainty, is employed to model and combine substitutabilities based on aggregated evidence from multiple sources. The framework predicts the phase stability of candidate HEA compositions and is systematically evaluated on both quaternary alloy systems, demonstrating superior performance compared to baseline machine learning models and methods reliant on single-source evidence in cross-validation experiments. By leveraging multi-source knowledge, the framework retains robust predictive power even when key elements are absent from the training data, underscoring its potential for knowledge transfer and extrapolation. Furthermore, the enhanced interpretability of the methodology offers insights into the fundamental factors governing HEA formation. Overall, this work provides a promising strategy for accelerating HEA discovery by integrating computational and textual knowledge sources, enabling efficient exploration of vast compositional spaces with improved generalization and interpretability.</article>","contentLength":1811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding long-term energy use in off-grid solar home systems in sub-Saharan Africa","url":"https://arxiv.org/abs/2502.14630","date":1740114000,"author":"","guid":8044,"unread":true,"content":"<article>arXiv:2502.14630v1 Announce Type: new \nAbstract: Solar home systems provide low-cost electricity access for rural off-grid communities. As access to them increases, more long-term data becomes available on how these systems are used throughout their lifetime. This work analyses a dataset of 1,000 systems across sub-Saharan Africa. Dynamic time warping clustering was applied to the load demand data from the systems, identifying five distinct archetypal daily load profiles and their occurrence across the dataset. Temporal analysis reveals a general decline in daily energy consumption over time, with 57% of households reducing their usage after the first year of ownership. On average, there is a 33% decrease in daily consumption by the end of the second year compared to the peak demand, which occurs on the 96th day. Combining the load demand analysis with payment data shows that this decrease in energy consumption is observed even in households that are not experiencing economic hardship, indicating there are reasons beyond financial constraints for decreasing energy use once energy access is obtained.</article>","contentLength":1116,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PEARL: Towards Permutation-Resilient LLMs","url":"https://arxiv.org/abs/2502.14628","date":1740114000,"author":"","guid":8045,"unread":true,"content":"<article>arXiv:2502.14628v1 Announce Type: new \nAbstract: The in-context learning (ICL) capability of large language models (LLMs) enables them to perform challenging tasks using provided demonstrations. However, ICL is highly sensitive to the ordering of demonstrations, leading to instability in predictions. This paper shows that this vulnerability can be exploited to design a natural attack - difficult for model providers to detect - that achieves nearly 80% success rate on LLaMA-3 by simply permuting the demonstrations. Existing mitigation methods primarily rely on post-processing and fail to enhance the model's inherent robustness to input permutations, raising concerns about safety and reliability of LLMs. To address this issue, we propose Permutation-resilient learning (PEARL), a novel framework based on distributionally robust optimization (DRO), which optimizes model performance against the worst-case input permutation. Specifically, PEARL consists of a permutation-proposal network (P-Net) and the LLM. The P-Net generates the most challenging permutations by treating it as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net and the LLM iteratively optimize against each other, progressively improving the LLM's robustness. Experiments on synthetic pre-training and real-world instruction tuning tasks demonstrate that PEARL effectively mitigates permutation attacks and enhances performance. Notably, despite being trained on fewer shots and shorter contexts, PEARL achieves performance gains of up to 40% when scaled to many-shot and long-context scenarios, highlighting its efficiency and generalization capabilities.</article>","contentLength":1719,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors","url":"https://arxiv.org/abs/2502.14627","date":1740114000,"author":"","guid":8046,"unread":true,"content":"<article>arXiv:2502.14627v1 Announce Type: new \nAbstract: Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims to retrieve audio clips or multilingual texts from databases. However, existing ML-ATR schemes suffer from inconsistencies for instance similarity matching across languages. We theoretically analyze the inconsistency in terms of both multilingual modal alignment direction error and weight error, and propose the theoretical weight error upper bound for quantifying the inconsistency. Based on the analysis of the weight error upper bound, we find that the inconsistency problem stems from the data distribution error caused by random sampling of languages. We propose a consistent ML-ATR scheme using 1-to-k contrastive learning and audio-English co-anchor contrastive learning, aiming to mitigate the negative impact of data distribution error on recall and consistency in ML-ATR. Experimental results on the translated AudioCaps and Clotho datasets show that our scheme achieves state-of-the-art performance on recall and consistency metrics for eight mainstream languages, including English. Our code will be available at https://github.com/ATRI-ACL/ATRI-ACL.</article>","contentLength":1187,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Partial Incorrectness Logic","url":"https://arxiv.org/abs/2502.14626","date":1740114000,"author":"","guid":8047,"unread":true,"content":"<article>arXiv:2502.14626v1 Announce Type: new \nAbstract: Reasoning about program correctness has been a central topic in static analysis for many years, with Hoare logic (HL) playing an important role. The key notions in HL are partial and total correctness. Both require that program executions starting in a specified set of initial states (the precondition) reach a designated set of final states (the postcondition). Partial correctness is more lenient in that it does not require termination, effectively deeming divergence acceptable. We explore partial incorrectness logic, which stands in relation to O'Hearn's \"total\" incorrectness logic as partial correctness does to total correctness: Partial correctness allows divergence, partial incorrectness allows unreachability. While the duality between divergence and unreachability may not be immediately apparent, we explore this relationship further. Our chosen formalism is predicate transformers \\`a la Dijkstra. We focus here on deterministic and reversible programs, though the discussion extends to nondeterministic and irreversible computations, both of which introduce additional nondeterminism that must be addressed.</article>","contentLength":1174,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Record Web Page Information Extraction From News Websites","url":"https://arxiv.org/abs/2502.14625","date":1740114000,"author":"","guid":8048,"unread":true,"content":"<article>arXiv:2502.14625v1 Announce Type: new \nAbstract: In this paper, we focused on the problem of extracting information from web pages containing many records, a task of growing importance in the era of massive web data. Recently, the development of neural network methods has improved the quality of information extraction from web pages. Nevertheless, most of the research and datasets are aimed at studying detailed pages. This has left multi-record \"list pages\" relatively understudied, despite their widespread presence and practical significance.\n  To address this gap, we created a large-scale, open-access dataset specifically designed for list pages. This is the first dataset for this task in the Russian language. Our dataset contains 13,120 web pages with news lists, significantly exceeding existing datasets in both scale and complexity. Our dataset contains attributes of various types, including optional and multi-valued, providing a realistic representation of real-world list pages. These features make our dataset a valuable resource for studying information extraction from pages containing many records.\n  Furthermore, we proposed our own multi-stage information extraction methods. In this work, we explore and demonstrate several strategies for applying MarkupLM to the specific challenges of multi-record web pages. Our experiments validate the advantages of our methods.\n  By releasing our dataset to the public, we aim to advance the field of information extraction from multi-record pages.</article>","contentLength":1513,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Online Envy Minimization and Multicolor Discrepancy: Equivalences and Separations","url":"https://arxiv.org/abs/2502.14624","date":1740114000,"author":"","guid":8049,"unread":true,"content":"<article>arXiv:2502.14624v1 Announce Type: new \nAbstract: We consider the fundamental problem of allocating $T$ indivisible items that arrive over time to $n$ agents with additive preferences, with the goal of minimizing envy. This problem is tightly connected to online multicolor discrepancy: vectors $v_1, \\dots, v_T \\in \\mathbb{R}^d$ with $\\| v_i \\|_2 \\leq 1$ arrive over time and must be, immediately and irrevocably, assigned to one of $n$ colors to minimize $\\max_{i,j \\in [n]} \\| \\sum_{v \\in S_i} v - \\sum_{v \\in S_j} v \\|_{\\infty}$ at each step, where $S_\\ell$ is the set of vectors that are assigned color $\\ell$. The special case of $n = 2$ is called online vector balancing. Any bound for multicolor discrepancy implies the same bound for envy minimization. Against an adaptive adversary, both problems have the same optimal bound, $\\Theta(\\sqrt{T})$, but whether this holds for weaker adversaries is unknown.\n  Against an oblivious adversary, Alweiss et al. give a $O(\\log T)$ bound, with high probability, for multicolor discrepancy. Kulkarni et al. improve this to $O(\\sqrt{\\log T})$ for vector balancing and give a matching lower bound. Whether a $O(\\sqrt{\\log T})$ bound holds for multicolor discrepancy remains open. These results imply the best-known upper bounds for envy minimization (for an oblivious adversary) for $n$ and two agents, respectively; whether better bounds exist is open.\n  In this paper, we resolve all aforementioned open problems. We prove that online envy minimization and multicolor discrepancy are equivalent against an oblivious adversary: we give a $O(\\sqrt{\\log T})$ upper bound for multicolor discrepancy, and a $\\Omega(\\sqrt{\\log T})$ lower bound for envy minimization. For a weaker, i.i.d. adversary, we prove a separation: For online vector balancing, we give a $\\Omega\\left(\\sqrt{\\frac{\\log T}{\\log \\log T}}\\right)$ lower bound, while for envy minimization, we give an algorithm that guarantees a constant upper bound.</article>","contentLength":1960,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity","url":"https://arxiv.org/abs/2502.14620","date":1740114000,"author":"","guid":8050,"unread":true,"content":"<article>arXiv:2502.14620v1 Announce Type: new \nAbstract: This paper investigates the efficacy of RWKV, a novel language model architecture known for its linear attention mechanism, for generating sentence embeddings in a zero-shot setting. I conduct a layer-wise analysis to evaluate the semantic similarity captured by embeddings from different hidden layers of a pre-trained RWKV model. The performance is assessed on the Microsoft Research Paraphrase Corpus (MRPC) dataset using Spearman correlation and compared against a GloVe-based baseline. My results indicate that while RWKV embeddings capture some semantic relatedness, they underperform compared to the GloVe baseline in terms of Spearman correlation. I also analyze the inference time and GPU memory usage, highlighting the computational trade-offs associated with RWKV embeddings. The findings suggest that while RWKV offers potential advantages in terms of linear scaling, its zero-shot sentence embedding quality for semantic similarity tasks requires further investigation and potential task-specific fine-tuning to match or exceed simpler baselines.</article>","contentLength":1108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reward Models Identify Consistency, Not Causality","url":"https://arxiv.org/abs/2502.14619","date":1740114000,"author":"","guid":8051,"unread":true,"content":"<article>arXiv:2502.14619v1 Announce Type: new \nAbstract: Reward models (RMs) play a crucial role in aligning large language models (LLMs) with human preferences and enhancing reasoning quality. Traditionally, RMs are trained to rank candidate outputs based on their correctness and coherence. However, in this work, we present several surprising findings that challenge common assumptions about RM behavior. Our analysis reveals that state-of-the-art reward models prioritize structural consistency over causal correctness. Specifically, removing the problem statement has minimal impact on reward scores, whereas altering numerical values or disrupting the reasoning flow significantly affects RM outputs. Furthermore, RMs exhibit a strong dependence on complete reasoning trajectories truncated or incomplete steps lead to significant variations in reward assignments, indicating that RMs primarily rely on learned reasoning patterns rather than explicit problem comprehension. These findings hold across multiple architectures, datasets, and tasks, leading to three key insights: (1) RMs primarily assess coherence rather than true reasoning quality; (2) The role of explicit problem comprehension in reward assignment is overstated; (3) Current RMs may be more effective at ranking responses than verifying logical validity. Our results suggest a fundamental limitation in existing reward modeling approaches, emphasizing the need for a shift toward causality-aware reward models that go beyond consistency-driven evaluation.</article>","contentLength":1521,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Serving Models, Fast and Slow:Optimizing Heterogeneous LLM Inferencing Workloads at Scale","url":"https://arxiv.org/abs/2502.14617","date":1740114000,"author":"","guid":8052,"unread":true,"content":"<article>arXiv:2502.14617v1 Announce Type: new \nAbstract: Large Language Model (LLM) inference workloads handled by global cloud providers can include both latency-sensitive and insensitive tasks, creating a diverse range of Service Level Agreement (SLA) requirements. Managing these mixed workloads is challenging due to the complexity of the inference stack, which includes multiple LLMs, hardware configurations, and geographic distributions. Current optimization strategies often silo these tasks to ensure that SLAs are met for latency-sensitive tasks, but this leads to significant under-utilization of expensive GPU resources despite the availability of spot and on-demand Virtual Machine (VM) provisioning. We propose SAGESERVE, a comprehensive LLM serving framework that employs adaptive control knobs at varying time scales, ensuring SLA compliance while maximizing the utilization of valuable GPU resources. Short-term optimizations include efficient request routing to data center regions, while long-term strategies involve scaling GPU VMs out/in and redeploying models to existing VMs to align with traffic patterns. These strategies are formulated as an optimization problem for resource allocation and solved using Integer Linear Programming (ILP). We perform empirical and simulation studies based on production workload traces with over 8M requests using four open-source models deployed across three regions. SAGESERVE achieves up to 25% savings in GPU-hours while maintaining tail latency and satisfying all SLOs, and it reduces the scaling overhead compared to baselines by up to 80%, confirming the effectiveness of our proposal. In terms of dollar cost, this can save cloud providers up to $2M over the course of a month.</article>","contentLength":1735,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Monocular Depth Estimation and Segmentation for Transparent Object with Iterative Semantic and Geometric Fusion","url":"https://arxiv.org/abs/2502.14616","date":1740114000,"author":"","guid":8053,"unread":true,"content":"<article>arXiv:2502.14616v1 Announce Type: new \nAbstract: Transparent object perception is indispensable for numerous robotic tasks. However, accurately segmenting and estimating the depth of transparent objects remain challenging due to complex optical properties. Existing methods primarily delve into only one task using extra inputs or specialized sensors, neglecting the valuable interactions among tasks and the subsequent refinement process, leading to suboptimal and blurry predictions. To address these issues, we propose a monocular framework, which is the first to excel in both segmentation and depth estimation of transparent objects, with only a single-image input. Specifically, we devise a novel semantic and geometric fusion module, effectively integrating the multi-scale information between tasks. In addition, drawing inspiration from human perception of objects, we further incorporate an iterative strategy, which progressively refines initial features for clearer results. Experiments on two challenging synthetic and real-world datasets demonstrate that our model surpasses state-of-the-art monocular, stereo, and multi-view methods by a large margin of about 38.8%-46.2% with only a single RGB input. Codes and models are publicly available at https://github.com/L-J-Yuan/MODEST.</article>","contentLength":1295,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis","url":"https://arxiv.org/abs/2502.14614","date":1740114000,"author":"","guid":8054,"unread":true,"content":"<article>arXiv:2502.14614v1 Announce Type: new \nAbstract: Retrieval-Augmented Large Language Models (LLMs), which integrate external knowledge into LLMs, have shown remarkable performance in various medical domains, including clinical diagnosis. However, existing RAG methods struggle to effectively assess task difficulty to make retrieval decisions, thereby failing to meet the clinical requirements for balancing efficiency and accuracy. So in this paper, we propose FIND (\\textbf{F}ine-grained \\textbf{In}formation \\textbf{D}ensity Guided Adaptive RAG), a novel framework that improves the reliability of RAG in disease diagnosis scenarios. FIND incorporates a fine-grained adaptive control module to determine whether retrieval is necessary based on the information density of the input. By optimizing the retrieval process and implementing a knowledge filtering module, FIND ensures that the retrieval is better suited to clinical scenarios. Experiments on three Chinese electronic medical record datasets demonstrate that FIND significantly outperforms various baseline methods, highlighting its effectiveness in clinical diagnosis tasks.</article>","contentLength":1136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Behavioral Analysis of Information Salience in Large Language Models","url":"https://arxiv.org/abs/2502.14613","date":1740114000,"author":"","guid":8055,"unread":true,"content":"<article>arXiv:2502.14613v1 Announce Type: new \nAbstract: Large Language Models (LLMs) excel at text summarization, a task that requires models to select content based on its importance. However, the exact notion of salience that LLMs have internalized remains unclear. To bridge this gap, we introduce an explainable framework to systematically derive and investigate information salience in LLMs through their summarization behavior. Using length-controlled summarization as a behavioral probe into the content selection process, and tracing the answerability of Questions Under Discussion throughout, we derive a proxy for how models prioritize information. Our experiments on 13 models across four datasets reveal that LLMs have a nuanced, hierarchical notion of salience, generally consistent across model families and sizes. While models show highly consistent behavior and hence salience patterns, this notion of salience cannot be accessed through introspection, and only weakly correlates with human perceptions of information salience.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enumerating minimal dominating sets and variants in chordal bipartite graphs","url":"https://arxiv.org/abs/2502.14611","date":1740114000,"author":"","guid":8056,"unread":true,"content":"<article>arXiv:2502.14611v1 Announce Type: new \nAbstract: Enumerating minimal dominating sets with polynomial delay in bipartite graphs is a long-standing open problem. To date, even the subcase of chordal bipartite graphs is open, with the best known algorithm due to Golovach, Heggernes, Kant\\'e, Kratsch, Saether, and Villanger running in incremental-polynomial time. We improve on this result by providing a polynomial delay and space algorithm enumerating minimal dominating sets in chordal bipartite graphs. Additionally, we show that the total and connected variants admit polynomial and incremental-polynomial delay algorithms, respectively, within the same class. This provides an alternative proof of a result by Golovach et al. for total dominating sets, and answers an open question for the connected variant. Finally, we give evidence that the techniques used in this paper cannot be generalized to bipartite graphs for (total) minimal dominating sets, unless P = NP, and show that enumerating minimal connected dominating sets in bipartite graphs is harder than enumerating minimal transversals in general hypergraphs.</article>","contentLength":1123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Curiosity Driven Multi-agent Reinforcement Learning for 3D Game Testing","url":"https://arxiv.org/abs/2502.14606","date":1740114000,"author":"","guid":8057,"unread":true,"content":"<article>arXiv:2502.14606v1 Announce Type: new \nAbstract: Recently testing of games via autonomous agents has shown great promise in tackling challenges faced by the game industry, which mainly relied on either manual testing or record/replay. In particular Reinforcement Learning (RL) solutions have shown potential by learning directly from playing the game without the need for human intervention. In this paper, we present cMarlTest, an approach for testing 3D games through curiosity driven Multi-Agent Reinforcement Learning (MARL). cMarlTest deploys multiple agents that work collaboratively to achieve the testing objective. The use of multiple agents helps resolve issues faced by a single agent approach. We carried out experiments on different levels of a 3D game comparing the performance of cMarlTest with a single agent RL variant. Results are promising where, considering three different types of coverage criteria, cMarlTest achieved higher coverage. cMarlTest was also more efficient in terms of the time taken, with respect to the single agent based variant.</article>","contentLength":1067,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Noisy Test-Time Adaptation in Vision-Language Models","url":"https://arxiv.org/abs/2502.14604","date":1740114000,"author":"","guid":8058,"unread":true,"content":"<article>arXiv:2502.14604v1 Announce Type: new \nAbstract: Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing. In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space. Leveraging the zero-shot capability of pre-trained vision-language models (VLMs), this paper introduces Zero-Shot Noisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy samples during test-time in a zero-shot manner. We find existing TTA methods underperform under ZS-NTTA, often lagging behind even the frozen model. We conduct comprehensive experiments to analyze this phenomenon, revealing that the negative impact of unfiltered noisy data outweighs the benefits of clean data during model updating. Also, adapting a classifier for ID classification and noise detection hampers both sub-tasks. Built on this, we propose a framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier frozen. Technically, we introduce the Adaptive Noise Detector (AdaND), which utilizes the frozen model's outputs as pseudo-labels to train a noise detector. To handle clean data streams, we further inject Gaussian noise during adaptation, preventing the detector from misclassifying clean samples as noisy. Beyond the ZS-NTTA, AdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection ability of VLMs. Experiments show that AdaND outperforms in both ZS-NTTA and ZS-OOD detection. On ImageNet, AdaND achieves a notable improvement of $8.32\\%$ in harmonic mean accuracy ($\\text{Acc}_\\text{H}$) for ZS-NTTA and $9.40\\%$ in FPR95 for ZS-OOD detection, compared to SOTA methods. Importantly, AdaND is computationally efficient and comparable to the model-frozen method. The code is publicly available at: https://github.com/tmlr-group/ZS-NTTA.</article>","contentLength":1948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Class Imbalanced Learning with Support Vector Machines via Differential Evolution","url":"https://arxiv.org/abs/2502.14597","date":1740114000,"author":"","guid":8059,"unread":true,"content":"<article>arXiv:2502.14597v1 Announce Type: new \nAbstract: Support vector machine (SVM) is a powerful machine learning algorithm to handle classification tasks. However, the classical SVM is developed for binary problems with the assumption of balanced datasets. Obviously, the multi-class imbalanced classification problems are more complex. In this paper, we propose an improved SVM via Differential Evolution (i-SVM-DE) method to deal with it. An improved SVM (i-SVM) model is proposed to handle the data imbalance by combining cost sensitive technique and separation margin modification in the constraints, which formalize a parameter optimization problem. By using one-versus-one (OVO) scheme, a multi-class problem is decomposed into a number of binary subproblems. A large optimization problem is formalized through concatenating the parameters in the binary subproblems. To find the optimal model effectively and learn the support vectors for each class simultaneously, an improved differential evolution (DE) algorithm is applied to solve this large optimization problem. Instead of the validation set, we propose the fitness functions to evaluate the learned model and obtain the optimal parameters in the search process of DE. A series of experiments are carried out to verify the benefits of our proposed method. The results indicate that i-SVM-DE is statistically superior by comparing with the other baseline methods.</article>","contentLength":1421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"Don't Forget the Teachers\": Towards an Educator-Centered Understanding of Harms from Large Language Models in Education","url":"https://arxiv.org/abs/2502.14592","date":1740114000,"author":"","guid":8060,"unread":true,"content":"<article>arXiv:2502.14592v1 Announce Type: new \nAbstract: Education technologies (edtech) are increasingly incorporating new features built on large language models (LLMs), with the goals of enriching the processes of teaching and learning and ultimately improving learning outcomes. However, the potential downstream impacts of LLM-based edtech remain understudied. Prior attempts to map the risks of LLMs have not been tailored to education specifically, even though it is a unique domain in many respects: from its population (students are often children, who can be especially impacted by technology) to its goals (providing the correct answer may be less important for learners than understanding how to arrive at an answer) to its implications for higher-order skills that generalize across contexts (e.g., critical thinking and collaboration). We conducted semi-structured interviews with six edtech providers representing leaders in the K-12 space, as well as a diverse group of 23 educators with varying levels of experience with LLM-based edtech. Through a thematic analysis, we explored how each group is anticipating, observing, and accounting for potential harms from LLMs in education. We find that, while edtech providers focus primarily on mitigating technical harms, i.e., those that can be measured based solely on LLM outputs themselves, educators are more concerned about harms that result from the broader impacts of LLMs, i.e., those that require observation of interactions between students, educators, school systems, and edtech to measure. Overall, we (1) develop an education-specific overview of potential harms from LLMs, (2) highlight gaps between conceptions of harm by edtech providers and those by educators, and (3) make recommendations to facilitate the centering of educators in the design and development of edtech tools.</article>","contentLength":1848,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data-driven Control of T-Product-based Dynamical Systems","url":"https://arxiv.org/abs/2502.14591","date":1740114000,"author":"","guid":8061,"unread":true,"content":"<article>arXiv:2502.14591v1 Announce Type: new \nAbstract: Data-driven control is a powerful tool that enables the design and implementation of control strategies directly from data without explicitly identifying the underlying system dynamics. While various data-driven control techniques, such as stabilization, linear quadratic regulation, and model predictive control, have been extensively developed, these methods are not inherently suited for multi-linear dynamical systems, where the states are represented as higher-order tensors. In this article, we propose a novel framework for data-driven control of T-product-based dynamical systems (TPDSs), where the system evolution is governed by the T-product between a third-order dynamic tensor and a third-order state tensor. In particular, we offer necessary and sufficient conditions to determine the data informativity for system identification, stabilization by state feedback, and T-product quadratic regulation of TPDSs with detailed complexity analyses. Finally, we validate our framework through numerical examples.</article>","contentLength":1068,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explicit adaptive time stepping for the Cahn-Hilliard equation by exponential Krylov subspace and Chebyshev polynomial methods","url":"https://arxiv.org/abs/2502.14589","date":1740114000,"author":"","guid":8062,"unread":true,"content":"<article>arXiv:2502.14589v1 Announce Type: new \nAbstract: The Cahn-Hilliard equation has been widely employed within various mathematical models in physics, chemistry and engineering. Explicit stabilized time stepping methods can be attractive for time integration of the Cahn-Hilliard equation, especially on parallel and hybrid supercomputers. In this paper, we propose an exponential time integration method for the Cahn-Hilliard equation and describe its efficient Krylov subspace based implementation. We compare the method to a Chebyshev polynomial local iteration modified (LIM) time stepping scheme. Both methods are explicit (i.e., do not involve linear system solution) and tested with both constant and adaptively chosen time steps.</article>","contentLength":734,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Moshi Moshi? A Model Selection Hijacking Adversarial Attack","url":"https://arxiv.org/abs/2502.14586","date":1740114000,"author":"","guid":8063,"unread":true,"content":"<article>arXiv:2502.14586v1 Announce Type: new \nAbstract: Model selection is a fundamental task in Machine Learning~(ML), focusing on selecting the most suitable model from a pool of candidates by evaluating their performance on specific metrics. This process ensures optimal performance, computational efficiency, and adaptability to diverse tasks and environments. Despite its critical role, its security from the perspective of adversarial ML remains unexplored. This risk is heightened in the Machine-Learning-as-a-Service model, where users delegate the training phase and the model selection process to third-party providers, supplying data and training strategies. Therefore, attacks on model selection could harm both the user and the provider, undermining model performance and driving up operational costs.\n  In this work, we present MOSHI (MOdel Selection HIjacking adversarial attack), the first adversarial attack specifically targeting model selection. Our novel approach manipulates model selection data to favor the adversary, even without prior knowledge of the system. Utilizing a framework based on Variational Auto Encoders, we provide evidence that an attacker can induce inefficiencies in ML deployment. We test our attack on diverse computer vision and speech recognition benchmark tasks and different settings, obtaining an average attack success rate of 75.42%. In particular, our attack causes an average 88.30% decrease in generalization capabilities, an 83.33% increase in latency, and an increase of up to 105.85% in energy consumption. These results highlight the significant vulnerabilities in model selection processes and their potential impact on real-world applications.</article>","contentLength":1696,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Stackelberg Game Approach for Signal Temporal Logic Control Synthesis with Uncontrollable Agents","url":"https://arxiv.org/abs/2502.14585","date":1740114000,"author":"","guid":8064,"unread":true,"content":"<article>arXiv:2502.14585v1 Announce Type: new \nAbstract: In this paper, we investigate the control synthesis problem for Signal Temporal Logic (STL) specifications in the presence of uncontrollable agents. Existing works mainly address this problem in a robust control setting by assuming the uncontrollable agents are adversarial and accounting for the worst-case scenario. While this approach ensures safety, it can be overly conservative in scenarios where uncontrollable agents have their own objectives that are not entirely opposed to the system's goals. Motivated by this limitation, we propose a new framework for STL control synthesis within the Stackelberg game setting. Specifically, we assume that the system controller, acting as the leader, first commits to a plan, after which the uncontrollable agents, acting as followers, take a best response based on the committed plan and their own objectives. Our goal is to synthesize a control sequence for the leader such that, for any rational followers producing a best response, the leader's STL task is guaranteed to be satisfied. We present an effective solution to this problem by transforming it into a single-stage optimization problem and leveraging counter-example guided synthesis techniques. We demonstrate that the proposed approach is sound and identify conditions under which it is optimal. Simulation results are also provided to illustrate the effectiveness of the proposed framework.</article>","contentLength":1451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Theory for Conditional Generative Modeling on Multiple Data Sources","url":"https://arxiv.org/abs/2502.14583","date":1740114000,"author":"","guid":8065,"unread":true,"content":"<article>arXiv:2502.14583v1 Announce Type: new \nAbstract: The success of large generative models has driven a paradigm shift, leveraging massive multi-source data to enhance model capabilities. However, the interaction among these sources remains theoretically underexplored. This paper takes the first step toward a rigorous analysis of multi-source training in conditional generative modeling, where each condition represents a distinct data source. Specifically, we establish a general distribution estimation error bound in average total variation distance for conditional maximum likelihood estimation based on the bracketing number. Our result shows that when source distributions share certain similarities and the model is expressive enough, multi-source training guarantees a sharper bound than single-source training. We further instantiate the general theory on conditional Gaussian estimation and deep generative models including autoregressive and flexible energy-based models, by characterizing their bracketing numbers. The results highlight that the number of sources and similarity among source distributions improve the advantage of multi-source training. Simulations and real-world experiments validate our theory. Code is available at: \\url{https://github.com/ML-GSAI/Multi-Source-GM}.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Statistical Case Against Empirical Human-AI Alignment","url":"https://arxiv.org/abs/2502.14581","date":1740114000,"author":"","guid":8066,"unread":true,"content":"<article>arXiv:2502.14581v1 Announce Type: new \nAbstract: Empirical human-AI alignment aims to make AI systems act in line with observed human behavior. While noble in its goals, we argue that empirical alignment can inadvertently introduce statistical biases that warrant caution. This position paper thus advocates against naive empirical alignment, offering prescriptive alignment and a posteriori empirical alignment as alternatives. We substantiate our principled argument by tangible examples like human-centric decoding of language models.</article>","contentLength":537,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-world Troublemaker: A Novel Track Testing Framework for Automated Driving Systems in Safety-critical Interaction Scenarios","url":"https://arxiv.org/abs/2502.14574","date":1740114000,"author":"","guid":8067,"unread":true,"content":"<article>arXiv:2502.14574v1 Announce Type: new \nAbstract: Track testing plays a critical role in the safety evaluation of autonomous driving systems (ADS), as it provides real-world object targets and a safety-controllable interaction environment. However, existing track testing scenarios are often pre-fixed and limited, primarily due to the inflexibility of object target control methods and the lack of intelligent interactive behaviors. To overcome this limitation, we propose a novel track testing framework, Real-world Troublemaker, which can generate adversarial object target motion trajectories and facilitate intelligent interactions with the vehicle under test (VUT), creating a more realistic and dynamic testing environment. To enable flexible motion trajectories, cloud-controlled technology is utilized to remotely and dynamically control object targets to create a realistic traffic environment. To achieve intelligent interactions, an interactive concrete scenario generation method is introduced within a game-theoretic structure. The proposed framework has been successfully implemented at the Tongji University Intelligent Connected Vehicle Evaluation Base. Field test results demonstrate that Troublemaker can perform dynamic interactive testing of ADS accurately and effectively. Compared to traditional track testing methods, Troublemaker improves scenario reproduction accuracy by 65.2\\%, increases the diversity of target vehicle interaction strategies by approximately 9.2 times, and enhances exposure frequency of safety-critical scenarios by 3.5 times in unprotected left-turn scenarios.</article>","contentLength":1607,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining","url":"https://arxiv.org/abs/2502.14573","date":1740114000,"author":"","guid":8068,"unread":true,"content":"<article>arXiv:2502.14573v1 Announce Type: new \nAbstract: Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels. Although this approach simplifies data acquisition compared to supervised methods, it struggles with reflective surfaces, as they violate the assumptions of Lambertian reflectance, leading to inaccurate training on such surfaces. To tackle this problem, we propose a novel training strategy for an SSMDE by leveraging triplet mining to pinpoint reflective regions at the pixel level, guided by the camera geometry between different viewpoints. The proposed reflection-aware triplet mining loss specifically penalizes the inappropriate photometric error minimization on the localized reflective regions while preserving depth accuracy in non-reflective areas. We also incorporate a reflection-aware knowledge distillation method that enables a student model to selectively learn the pixel-level knowledge from reflective and non-reflective regions. This results in robust depth estimation across areas. Evaluation results on multiple datasets demonstrate that our method effectively enhances depth quality on reflective surfaces and outperforms state-of-the-art SSMDE baselines.</article>","contentLength":1331,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Factor Graph-based Interpretable Neural Networks","url":"https://arxiv.org/abs/2502.14572","date":1740114000,"author":"","guid":8069,"unread":true,"content":"<article>arXiv:2502.14572v1 Announce Type: new \nAbstract: Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations. To address this challenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network, which is capable of generating comprehensible explanations under unknown perturbations. Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference. Specifically, we construct the factor graph to express logical rules between explanations and categories. By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic. Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations. Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph. Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines.</article>","contentLength":1655,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Predicting Filter Medium Performances in Chamber Filter Presses with Digital Twins Using Neural Network Technologies","url":"https://arxiv.org/abs/2502.14571","date":1740114000,"author":"","guid":8070,"unread":true,"content":"<article>arXiv:2502.14571v1 Announce Type: new \nAbstract: Efficient solid-liquid separation is crucial in industries like mining, but traditional chamber filter presses depend heavily on manual monitoring, leading to inefficiencies, downtime, and resource wastage. This paper introduces a machine learning-powered digital twin framework to improve operational flexibility and predictive control. A key challenge addressed is the degradation of the filter medium due to repeated cycles and clogging, which reduces filtration efficiency. To solve this, a neural network-based predictive model was developed to forecast operational parameters, such as pressure and flow rates, under various conditions. This predictive capability allows for optimized filtration cycles, reduced downtime, and improved process efficiency. Additionally, the model predicts the filter mediums lifespan, aiding in maintenance planning and resource sustainability. The digital twin framework enables seamless data exchange between filter press sensors and the predictive model, ensuring continuous updates to the training data and enhancing accuracy over time. Two neural network architectures, feedforward and recurrent, were evaluated. The recurrent neural network outperformed the feedforward model, demonstrating superior generalization. It achieved a relative $L^2$-norm error of $5\\%$ for pressure and $9.3\\%$ for flow rate prediction on partially known data. For completely unknown data, the relative errors were $18.4\\%$ and $15.4\\%$, respectively. Qualitative analysis showed strong alignment between predicted and measured data, with deviations within a confidence band of $8.2\\%$ for pressure and $4.8\\%$ for flow rate predictions. This work contributes an accurate predictive model, a new approach to predicting filter medium cycle impacts, and a real-time interface for model updates, ensuring adaptability to changing operational conditions.</article>","contentLength":1921,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linking Science and Industry: Influence of Scientific Research on Technological Innovation through Patent Citations","url":"https://arxiv.org/abs/2502.14570","date":1740114000,"author":"","guid":8071,"unread":true,"content":"<article>arXiv:2502.14570v1 Announce Type: new \nAbstract: This study explores the connection between patent citations and scientific publications across six fields: Biochemistry, Genetics, Pharmacology, Engineering, Mathematics, and Physics. Analysing 117,590 papers from 2014 to 2023, the research emphasises how publication year, open access (OA) status, and discipline influence patent citations. Openly accessible papers, particularly those in hybrid OA journals or green OA repositories, are significantly more likely to be cited in patents, seven times more than those mentioned in blogs, and over twice as likely compared to older publications. However, papers with policy-related references are less frequently cited, indicating that patents may prioritise commercially viable innovations over those addressing societal challenges. Disciplinary differences reveal distinct innovation patterns across sectors. While academic visibility via blogs or platforms like Mendeley increases within scholarly circles, these have limited impact on patent citations. The study also finds that increased funding, possibly tied to applied research trends and fully open access journals, negatively affects patent citations. Social media presence and the number of authors have minimal impact. These findings highlight the complex factors shaping the integration of scientific research into technological innovations.</article>","contentLength":1401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ReVISE: Learning to Refine at Test-Time via Intrinsic Self-Verification","url":"https://arxiv.org/abs/2502.14565","date":1740114000,"author":"","guid":8072,"unread":true,"content":"<article>arXiv:2502.14565v1 Announce Type: new \nAbstract: Self-awareness, i.e., the ability to assess and correct one's own generation, is a fundamental aspect of human intelligence, making its replication in large language models (LLMs) an important yet challenging task. Previous works tackle this by employing extensive reinforcement learning or rather relying on large external verifiers. In this work, we propose Refine via Intrinsic Self-Verification (ReVISE), an efficient and effective framework that enables LLMs to self-correct their outputs through self-verification. The core idea of ReVISE is to enable LLMs to verify their reasoning processes and continually rethink reasoning trajectories based on its verification. We introduce a structured curriculum based upon online preference learning to implement this efficiently. Specifically, as ReVISE involves two challenging tasks (i.e., self-verification and reasoning correction), we tackle each task sequentially using curriculum learning, collecting both failed and successful reasoning paths to construct preference pairs for efficient training. During inference, our approach enjoys natural test-time scaling by integrating self-verification and correction capabilities, further enhanced by our proposed confidence-aware decoding mechanism. Our experiments on various reasoning tasks demonstrate that ReVISE achieves efficient self-correction and significantly improves reasoning performance.</article>","contentLength":1450,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Plan-over-Graph: Towards Parallelable LLM Agent Schedule","url":"https://arxiv.org/abs/2502.14563","date":1740114000,"author":"","guid":8073,"unread":true,"content":"<article>arXiv:2502.14563v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated exceptional abilities in reasoning for task planning. However, challenges remain under-explored for parallel schedules. This paper introduces a novel paradigm, plan-over-graph, in which the model first decomposes a real-life textual task into executable subtasks and constructs an abstract task graph. The model then understands this task graph as input and generates a plan for parallel execution. To enhance the planning capability of complex, scalable graphs, we design an automated and controllable pipeline to generate synthetic graphs and propose a two-stage training scheme. Experimental results show that our plan-over-graph method significantly improves task performance on both API-based LLMs and trainable open-sourced LLMs. By normalizing complex tasks as graphs, our method naturally supports parallel execution, demonstrating global efficiency. The code and data are available at https://github.com/zsq259/Plan-over-Graph.</article>","contentLength":1032,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs","url":"https://arxiv.org/abs/2502.14561","date":1740114000,"author":"","guid":8074,"unread":true,"content":"<article>arXiv:2502.14561v1 Announce Type: new \nAbstract: This work investigates the ability of open Large Language Models (LLMs) to predict citation intent through in-context learning and fine-tuning. Unlike traditional approaches that rely on pre-trained models like SciBERT, which require extensive domain-specific pretraining and specialized architectures, we demonstrate that general-purpose LLMs can be adapted to this task with minimal task-specific data. We evaluate twelve model variations across five prominent open LLM families using zero, one, few, and many-shot prompting to assess performance across scenarios. Our experimental study identifies the top-performing model through extensive experimentation of in-context learning-related parameters, which we fine-tune to further enhance task performance. The results highlight the strengths and limitations of LLMs in recognizing citation intents, providing valuable insights for model selection and prompt engineering. Additionally, we make our end-to-end evaluation framework and models openly available for future use.</article>","contentLength":1074,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Less is More: Improving LLM Alignment via Preference Data Selection","url":"https://arxiv.org/abs/2502.14560","date":1740114000,"author":"","guid":8075,"unread":true,"content":"<article>arXiv:2502.14560v1 Announce Type: new \nAbstract: Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To accurately estimate margins for data selection, we propose a dual-margin guided approach that considers both external reward margins and implicit DPO reward margins. Extensive experiments demonstrate that our method reduces computational cost dramatically while improving performance. Remarkably, by using just 10\\% of the Ultrafeedback dataset, our approach achieves 3\\% to 8\\% improvements across various Llama and Mistral series models on the AlpacaEval 2.0 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data, while further reducing training time. These results highlight the potential of data selection strategies for advancing preference optimization.</article>","contentLength":1257,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Convergence analysis of decoupled mixed FEM for the Cahn-Hilliard-Navier-Stokes equations","url":"https://arxiv.org/abs/2502.14559","date":1740114000,"author":"","guid":8076,"unread":true,"content":"<article>arXiv:2502.14559v1 Announce Type: new \nAbstract: We develop a decoupled, first-order, fully discrete, energy-stable scheme for the Cahn-Hilliard-Navier-Stokes equations. This scheme calculates the Cahn-Hilliard and Navier-Stokes equations separately, thus effectively decoupling the entire system. To further separate the velocity and pressure components in the Navier-Stokes equations, we use the pressure-correction projection method. We demonstrate that the scheme is primitively energy stable and prove the optimal $L^2$ error estimate of the fully discrete scheme in the $P_r\\times P_r\\times P_r\\times P_{r-1}$ finite element spaces, where the phase field, chemical potential, velocity and pressure satisfy the first-order accuracy in time and the $\\left(r+1,r+1,r+1,r\\right)th$-order accuracy in space, respectively. Furthermore, numerical experiments are conducted to support these theoretical findings. Notably, compared to other numerical schemes, our algorithm is more time-efficient and numerically shown to be unconditionally stable.</article>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FUIA: Model Inversion Attack against Federated Unlearning","url":"https://arxiv.org/abs/2502.14558","date":1740114000,"author":"","guid":8077,"unread":true,"content":"<article>arXiv:2502.14558v1 Announce Type: new \nAbstract: With the introduction of regulations related to the ``right to be forgotten\", federated learning (FL) is facing new privacy compliance challenges. To address these challenges, researchers have proposed federated unlearning (FU). However, existing FU research has primarily focused on improving the efficiency of unlearning, with less attention paid to the potential privacy vulnerabilities inherent in these methods. To address this gap, we draw inspiration from gradient inversion attacks in FL and propose the federated unlearning inversion attack (FUIA). The FUIA is specifically designed for the three types of FU (sample unlearning, client unlearning, and class unlearning), aiming to provide a comprehensive analysis of the privacy leakage risks associated with FU. In FUIA, the server acts as an honest-but-curious attacker, recording and exploiting the model differences before and after unlearning to expose the features and labels of forgotten data. FUIA significantly leaks the privacy of forgotten data and can target all types of FU. This attack contradicts the goal of FU to eliminate specific data influence, instead exploiting its vulnerabilities to recover forgotten data and expose its privacy flaws. Extensive experimental results show that FUIA can effectively reveal the private information of forgotten data. To mitigate this privacy leakage, we also explore two potential defense methods, although these come at the cost of reduced unlearning effectiveness and the usability of the unlearned model.</article>","contentLength":1570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multiscale Byte Language Models -- A Hierarchical Architecture for Causal Million-Length Sequence Modeling","url":"https://arxiv.org/abs/2502.14553","date":1740114000,"author":"","guid":8078,"unread":true,"content":"<article>arXiv:2502.14553v1 Announce Type: new \nAbstract: Bytes form the basis of the digital world and thus are a promising building block for multimodal foundation models. Recently, Byte Language Models (BLMs) have emerged to overcome tokenization, yet the excessive length of bytestreams requires new architectural paradigms. Therefore, we present the Multiscale Byte Language Model (MBLM), a model-agnostic hierarchical decoder stack that allows training with context windows of $5$M bytes on single GPU in full model precision. We thoroughly examine MBLM's performance with Transformer and Mamba blocks on both unimodal and multimodal tasks. Our experiments demonstrate that hybrid architectures are efficient in handling extremely long byte sequences during training while achieving near-linear generational efficiency. To the best of our knowledge, we present the first evaluation of BLMs on visual Q\\&amp;A tasks and find that, despite serializing images and the absence of an encoder, a MBLM with pure next token prediction can match custom CNN-LSTM architectures with designated classification heads. We show that MBLMs exhibit strong adaptability in integrating diverse data representations, including pixel and image filestream bytes, underlining their potential toward omnimodal foundation models. Source code is publicly available at: https://github.com/ai4sd/multiscale-byte-lm</article>","contentLength":1379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Position: Graph Learning Will Lose Relevance Due To Poor Benchmarks","url":"https://arxiv.org/abs/2502.14546","date":1740114000,"author":"","guid":8079,"unread":true,"content":"<article>arXiv:2502.14546v1 Announce Type: new \nAbstract: While machine learning on graphs has demonstrated promise in drug design and molecular property prediction, significant benchmarking challenges hinder its further progress and relevance. Current benchmarking practices often lack focus on transformative, real-world applications, favoring narrow domains like two-dimensional molecular graphs over broader, impactful areas such as combinatorial optimization, relational databases, or chip design. Additionally, many benchmark datasets poorly represent the underlying data, leading to inadequate abstractions and misaligned use cases. Fragmented evaluations and an excessive focus on accuracy further exacerbate these issues, incentivizing overfitting rather than fostering generalizable insights. These limitations have prevented the development of truly useful graph foundation models. This position paper calls for a paradigm shift toward more meaningful benchmarks, rigorous evaluation protocols, and stronger collaboration with domain experts to drive impactful and reliable advances in graph learning research, unlocking the potential of graph learning.</article>","contentLength":1155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Entropic Metric for Measuring Calibration of Machine Learning Models","url":"https://arxiv.org/abs/2502.14545","date":1740114000,"author":"","guid":8080,"unread":true,"content":"<article>arXiv:2502.14545v1 Announce Type: new \nAbstract: Understanding the confidence with which a machine learning model classifies an input datum is an important, and perhaps under-investigated, concept. In this paper, we propose a new calibration metric, the Entropic Calibration Difference (ECD). Based on existing research in the field of state estimation, specifically target tracking (TT), we show how ECD may be applied to binary classification machine learning models. We describe the relative importance of under- and over-confidence and how they are not conflated in the TT literature. Indeed, our metric distinguishes under- from over-confidence. We consider this important given that algorithms that are under-confident are likely to be 'safer' than algorithms that are over-confident, albeit at the expense of also being over-cautious and so statistically inefficient. We demonstrate how this new metric performs on real and simulated data and compare with other metrics for machine learning model probability calibration, including the Expected Calibration Error (ECE) and its signed counterpart, the Expected Signed Calibration Error (ESCE).</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-based User Profile Management for Recommender System","url":"https://arxiv.org/abs/2502.14541","date":1740114000,"author":"","guid":8081,"unread":true,"content":"<article>arXiv:2502.14541v1 Announce Type: new \nAbstract: The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.</article>","contentLength":1287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Temporal Connectivity Augmentation","url":"https://arxiv.org/abs/2502.14540","date":1740114000,"author":"","guid":8082,"unread":true,"content":"<article>arXiv:2502.14540v1 Announce Type: new \nAbstract: Connectivity in temporal graphs relies on the notion of temporal paths, in which edges follow a chronological order (either strict or non-strict). In this work, we investigate the question of how to make a temporal graph connected. More precisely, we tackle the problem of finding, among a set of proposed temporal edges, the smallest subset such that its addition makes the graph temporally connected (TCA). We study the complexity of this problem and variants, under restricted lifespan of the graph, i.e. the maximum time step in the graph. Our main result on TCA is that for any fixed lifespan at least 2, it is NP-complete in both the strict and non-strict setting. We additionally provide a set of restrictions in the non-strict setting which makes the problem solvable in polynomial time and design an algorithm achieving this complexity. Interestingly, we prove that the source variant (making a given vertex a source in the augmented graph) is as difficult as TCA. On the opposite, we prove that the version where a list of connectivity demands has to be satisfied is solvable in polynomial time, when the size of the list is fixed. Finally, we highlight a variant of the previous case for which even with two pairs the problem is already NP-hard.</article>","contentLength":1305,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LoRA-GGPO: Mitigating Double Descent in LoRA Fine-Tuning via Gradient-Guided Perturbation Optimization","url":"https://arxiv.org/abs/2502.14538","date":1740114000,"author":"","guid":8083,"unread":true,"content":"<article>arXiv:2502.14538v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have achieved remarkable success in natural language processing, but their full fine-tuning remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), have emerged as a practical solution by approximating parameter updates with low-rank matrices. However, LoRA often exhibits a \"double descent\" phenomenon during fine-tuning, where model performance degrades due to overfitting and limited expressiveness caused by low-rank constraints. To address this issue, we propose LoRA-GGPO (Gradient-Guided Perturbation Optimization), a novel method that leverages gradient and weight norms to generate targeted perturbations. By optimizing the sharpness of the loss landscape, LoRA-GGPO guides the model toward flatter minima, mitigating the double descent problem and improving generalization. Extensive experiments on natural language understanding (NLU) and generation (NLG) tasks demonstrate that LoRA-GGPO outperforms LoRA and its state-of-the-art variants. Furthermore, extended experiments specifically designed to analyze the double descent phenomenon confirm that LoRA-GGPO effectively alleviates this issue, producing more robust and generalizable models. Our work provides a robust and efficient solution for fine-tuning LLMs, with broad applicability in real-world scenarios. The code is available at https://github.com/llm172/LoRA-GGPO.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Preordering: A hybrid of correlation clustering and partial ordering","url":"https://arxiv.org/abs/2502.14536","date":1740114000,"author":"","guid":8084,"unread":true,"content":"<article>arXiv:2502.14536v1 Announce Type: new \nAbstract: We discuss the preordering problem, a joint relaxation of the correlation clustering problem and the partial ordering problem. We show that preordering remains NP-hard even for values in $\\{-1,0,1\\}$. We introduce a linear-time $4$-approximation algorithm and a local search technique. For an integer linear program formulation, we establish a class of non-canonical facets of the associated preorder polytope. By solving a non-canonical linear program relaxation, we obtain non-trivial upper bounds on the objective value. We provide implementations of the algorithms we define, apply these to published social networks and compare the output and efficiency qualitatively and quantitatively.</article>","contentLength":741,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OptiRefine: Densest subgraphs and maximum cuts with $k$ refinements","url":"https://arxiv.org/abs/2502.14532","date":1740114000,"author":"","guid":8085,"unread":true,"content":"<article>arXiv:2502.14532v1 Announce Type: new \nAbstract: Data-analysis tasks often involve an iterative process, which requires refining previous solutions. For instance, when analyzing dynamic social networks, we may be interested in monitoring the evolution of a community that was identified at an earlier snapshot. This task requires finding a community in the current snapshot of data that is ``close'' to the earlier-discovered community of interest. However, classic optimization algorithms, which typically find solutions from scratch, potentially return communities that are very dissimilar to the initial one. To mitigate these issues, we introduce the \\emph{OptiRefine framework}. The framework optimizes initial solutions by making a small number of \\emph{refinements}, thereby ensuring that the new solution remains close to the initial solution and simultaneously achieving a near-optimal solution for the optimization problem. We apply the OptiRefine framework to two classic graph-optimization problems: \\emph{densest subgraph} and \\emph{maximum cut}. For the \\emph{densest-subgraph problem}, we optimize a given subgraph's density by adding or removing $k$~nodes. We show that this novel problem is a generalization of $k$-densest subgraph, and provide constant-factor approximation algorithms for $k=\\Omega(n)$~refinements. We also study a version of \\emph{maximum cut} in which the goal is to improve a given cut. We provide connections to maximum cut with cardinality constraints and provide an optimal approximation algorithm in most parameter regimes under the Unique Games Conjecture for $k=\\Omega(n)$~refinements. We evaluate our theoretical methods and scalable heuristics on synthetic and real-world data and show that they are highly effective in practice.</article>","contentLength":1775,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models","url":"https://arxiv.org/abs/2502.14529","date":1740114000,"author":"","guid":8086,"unread":true,"content":"<article>arXiv:2502.14529v1 Announce Type: new \nAbstract: Large Language Model-based Multi-Agent Systems (LLM-MASs) have demonstrated remarkable real-world capabilities, effectively collaborating to complete complex tasks. While these systems are designed with safety mechanisms, such as rejecting harmful instructions through alignment, their security remains largely unexplored. This gap leaves LLM-MASs vulnerable to targeted disruptions. In this paper, we introduce Contagious Recursive Blocking Attacks (Corba), a novel and simple yet highly effective attack that disrupts interactions between agents within an LLM-MAS. Corba leverages two key properties: its contagious nature allows it to propagate across arbitrary network topologies, while its recursive property enables sustained depletion of computational resources. Notably, these blocking attacks often involve seemingly benign instructions, making them particularly challenging to mitigate using conventional alignment methods. We evaluate Corba on two widely-used LLM-MASs, namely, AutoGen and Camel across various topologies and commercial models. Additionally, we conduct more extensive experiments in open-ended interactive LLM-MASs, demonstrating the effectiveness of Corba in complex topology structures and open-source models. Our code is available at: https://github.com/zhrli324/Corba.</article>","contentLength":1349,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inter-turbine Modelling of Wind-Farm Power using Multi-task Learning","url":"https://arxiv.org/abs/2502.14527","date":1740114000,"author":"","guid":8087,"unread":true,"content":"<article>arXiv:2502.14527v1 Announce Type: new \nAbstract: Because of the global need to increase power production from renewable energy resources, developments in the online monitoring of the associated infrastructure is of interest to reduce operation and maintenance costs. However, challenges exist for data-driven approaches to this problem, such as incomplete or limited histories of labelled damage-state data, operational and environmental variability, or the desire for the quantification of uncertainty to support risk management.\n  This work first introduces a probabilistic regression model for predicting wind-turbine power, which adjusts for wake effects learnt from data. Spatial correlations in the learned model parameters for different tasks (turbines) are then leveraged in a hierarchical Bayesian model (an approach to multi-task learning) to develop a \"metamodel\", which can be used to make power-predictions which adjust for turbine location - including on previously unobserved turbines not included in the training data. The results show that the metamodel is able to outperform a series of benchmark models, and demonstrates a novel strategy for making efficient use of data for inference in populations of structures, in particular where correlations exist in the variable(s) of interest (such as those from wind-turbine wake-effects).</article>","contentLength":1351,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Small Graph Is All You Need: DeepStateGNN for Scalable Traffic Forecasting","url":"https://arxiv.org/abs/2502.14525","date":1740114000,"author":"","guid":8088,"unread":true,"content":"<article>arXiv:2502.14525v1 Announce Type: new \nAbstract: We propose a novel Graph Neural Network (GNN) model, named DeepStateGNN, for analyzing traffic data, demonstrating its efficacy in two critical tasks: forecasting and reconstruction. Unlike typical GNN methods that treat each traffic sensor as an individual graph node, DeepStateGNN clusters sensors into higher-level graph nodes, dubbed Deep State Nodes, based on various similarity criteria, resulting in a fixed number of nodes in a Deep State graph. The term \"Deep State\" nodes is a play on words, referencing hidden networks of power that, like these nodes, secretly govern traffic independently of visible sensors. These Deep State Nodes are defined by several similarity factors, including spatial proximity (e.g., sensors located nearby in the road network), functional similarity (e.g., sensors on similar types of freeways), and behavioral similarity under specific conditions (e.g., traffic behavior during rain). This clustering approach allows for dynamic and adaptive node grouping, as sensors can belong to multiple clusters and clusters may evolve over time. Our experimental results show that DeepStateGNN offers superior scalability and faster training, while also delivering more accurate results than competitors. It effectively handles large-scale sensor networks, outperforming other methods in both traffic forecasting and reconstruction accuracy.</article>","contentLength":1419,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative adversarial networks vs large language models: a comparative study on synthetic tabular data generation","url":"https://arxiv.org/abs/2502.14523","date":1740114000,"author":"","guid":8089,"unread":true,"content":"<article>arXiv:2502.14523v1 Announce Type: new \nAbstract: We propose a new framework for zero-shot generation of synthetic tabular data. Using the large language model (LLM) GPT-4o and plain-language prompting, we demonstrate the ability to generate high-fidelity tabular data without task-specific fine-tuning or access to real-world data (RWD) for pre-training. To benchmark GPT-4o, we compared the fidelity and privacy of LLM-generated synthetic data against data generated with the conditional tabular generative adversarial network (CTGAN), across three open-access datasets: Iris, Fish Measurements, and Real Estate Valuation. Despite the zero-shot approach, GPT-4o outperformed CTGAN in preserving means, 95% confidence intervals, bivariate correlations, and data privacy of RWD, even at amplified sample sizes. Notably, correlations between parameters were consistently preserved with appropriate direction and strength. However, refinement is necessary to better retain distributional characteristics. These findings highlight the potential of LLMs in tabular data synthesis, offering an accessible alternative to generative adversarial networks and variational autoencoders.</article>","contentLength":1175,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigating the Generalizability of ECG Noise Detection Across Diverse Data Sources and Noise Types","url":"https://arxiv.org/abs/2502.14522","date":1740114000,"author":"","guid":8090,"unread":true,"content":"<article>arXiv:2502.14522v1 Announce Type: new \nAbstract: Electrocardiograms (ECGs) are essential for monitoring cardiac health, allowing clinicians to analyze heart rate variability (HRV), detect abnormal rhythms, and diagnose cardiovascular diseases. However, ECG signals, especially those from wearable devices, are often affected by noise artifacts caused by motion, muscle activity, or device-related interference. These artifacts distort R-peaks and the characteristic QRS complex, making HRV analysis unreliable and increasing the risk of misdiagnosis.\n  Despite this, the few existing studies on ECG noise detection have primarily focused on a single dataset, limiting the understanding of how well noise detection models generalize across different datasets. In this paper, we investigate the generalizability of noise detection in ECG using a novel HRV-based approach through cross-dataset experiments on four datasets. Our results show that machine learning achieves an average accuracy of over 90\\% and an AUPRC of more than 0.9. These findings suggest that regardless of the ECG data source or the type of noise, the proposed method maintains high accuracy even on unseen datasets, demonstrating the feasibility of generalizability.</article>","contentLength":1236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance","url":"https://arxiv.org/abs/2502.14520","date":1740114000,"author":"","guid":8091,"unread":true,"content":"<article>arXiv:2502.14520v1 Announce Type: new \nAbstract: 3D Semantic Scene Completion (SSC) provides comprehensive scene geometry and semantics for autonomous driving perception, which is crucial for enabling accurate and reliable decision-making. However, existing SSC methods are limited to capturing sparse information from the current frame or naively stacking multi-frame temporal features, thereby failing to acquire effective scene context. These approaches ignore critical motion dynamics and struggle to achieve temporal consistency. To address the above challenges, we propose a novel temporal SSC method FlowScene: Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance. By leveraging optical flow, FlowScene can integrate motion, different viewpoints, occlusions, and other contextual cues, thereby significantly improving the accuracy of 3D scene completion. Specifically, our framework introduces two key components: (1) a Flow-Guided Temporal Aggregation module that aligns and aggregates temporal features using optical flow, capturing motion-aware context and deformable structures; and (2) an Occlusion-Guided Voxel Refinement module that injects occlusion masks and temporally aggregated features into 3D voxel space, adaptively refining voxel representations for explicit geometric modeling. Experimental results demonstrate that FlowScene achieves state-of-the-art performance on the SemanticKITTI and SSCBench-KITTI-360 benchmarks.</article>","contentLength":1463,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Mobile Robotic Approach to Autonomous Surface Scanning in Legal Medicine","url":"https://arxiv.org/abs/2502.14514","date":1740114000,"author":"","guid":8092,"unread":true,"content":"<article>arXiv:2502.14514v1 Announce Type: new \nAbstract: Purpose: Comprehensive legal medicine documentation includes both an internal but also an external examination of the corpse. Typically, this documentation is conducted manually during conventional autopsy. A systematic digital documentation would be desirable, especially for the external examination of wounds, which is becoming more relevant for legal medicine analysis. For this purpose, RGB surface scanning has been introduced. While a manual full surface scan using a handheld camera is timeconsuming and operator dependent, floor or ceiling mounted robotic systems require substantial space and a dedicated room. Hence, we consider whether a mobile robotic system can be used for external documentation. Methods: We develop a mobile robotic system that enables full-body RGB-D surface scanning. Our work includes a detailed configuration space analysis to identify the environmental parameters that need to be considered to successfully perform a surface scan. We validate our findings through an experimental study in the lab and demonstrate the system's application in a legal medicine environment. Results: Our configuration space analysis shows that a good trade-off between coverage and time is reached with three robot base positions, leading to a coverage of 94.96 %. Experiments validate the effectiveness of the system in accurately capturing body surface geometry with an average surface coverage of 96.90 +- 3.16 % and 92.45 +- 1.43 % for a body phantom and actual corpses, respectively. Conclusion: This work demonstrates the potential of a mobile robotic system to automate RGB-D surface scanning in legal medicine, complementing the use of post-mortem CT scans for inner documentation. Our results indicate that the proposed system can contribute to more efficient and autonomous legal medicine documentation, reducing the need for manual intervention.</article>","contentLength":1923,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MultiSlav: Using Cross-Lingual Knowledge Transfer to Combat the Curse of Multilinguality","url":"https://arxiv.org/abs/2502.14509","date":1740114000,"author":"","guid":8093,"unread":true,"content":"<article>arXiv:2502.14509v1 Announce Type: new \nAbstract: Does multilingual Neural Machine Translation (NMT) lead to The Curse of the Multlinguality or provides the Cross-lingual Knowledge Transfer within a language family? In this study, we explore multiple approaches for extending the available data-regime in NMT and we prove cross-lingual benefits even in 0-shot translation regime for low-resource languages. With this paper, we provide state-of-the-art open-source NMT models for translating between selected Slavic languages. We released our models on the HuggingFace Hub (https://hf.co/collections/allegro/multislav-6793d6b6419e5963e759a683) under the CC BY 4.0 license. Slavic language family comprises morphologically rich Central and Eastern European languages. Although counting hundreds of millions of native speakers, Slavic Neural Machine Translation is under-studied in our opinion. Recently, most NMT research focuses either on: high-resource languages like English, Spanish, and German - in WMT23 General Translation Task 7 out of 8 task directions are from or to English; massively multilingual models covering multiple language groups; or evaluation techniques.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can LLMs Simulate L2-English Dialogue? An Information-Theoretic Analysis of L1-Dependent Biases","url":"https://arxiv.org/abs/2502.14507","date":1740114000,"author":"","guid":8094,"unread":true,"content":"<article>arXiv:2502.14507v1 Announce Type: new \nAbstract: This study evaluates Large Language Models' (LLMs) ability to simulate non-native-like English use observed in human second language (L2) learners interfered with by their native first language (L1). In dialogue-based interviews, we prompt LLMs to mimic L2 English learners with specific L1s (e.g., Japanese, Thai, Urdu) across seven languages, comparing their outputs to real L2 learner data. Our analysis examines L1-driven linguistic biases, such as reference word usage and avoidance behaviors, using information-theoretic and distributional density measures. Results show that modern LLMs (e.g., Qwen2.5, LLAMA3.3, DeepseekV3, GPT-4o) replicate L1-dependent patterns observed in human L2 data, with distinct influences from various languages (e.g., Japanese, Korean, and Mandarin significantly affect tense agreement, and Urdu influences noun-verb collocations). Our results reveal the potential of LLMs for L2 dialogue generation and evaluation for future educational applications.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PLPHP: Per-Layer Per-Head Vision Token Pruning for Efficient Large Vision-Language Models","url":"https://arxiv.org/abs/2502.14504","date":1740114000,"author":"","guid":8095,"unread":true,"content":"<article>arXiv:2502.14504v1 Announce Type: new \nAbstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across a range of multimodal tasks. However, their inference efficiency is constrained by the large number of visual tokens processed during decoding. To address this challenge, we propose Per-Layer Per-Head Vision Token Pruning (PLPHP), a two-level fine-grained pruning method including Layer-Level Retention Rate Allocation and Head-Level Vision Token Pruning. Motivated by the Vision Token Re-attention phenomenon across decoder layers, we dynamically adjust token retention rates layer by layer. Layers that exhibit stronger attention to visual information preserve more vision tokens, while layers with lower vision attention are aggressively pruned. Furthermore, PLPHP applies pruning at the attention head level, enabling different heads within the same layer to independently retain critical context. Experiments on multiple benchmarks demonstrate that PLPHP delivers an 18% faster decoding speed and reduces the Key-Value Cache (KV Cache) size by over 50%, all at the cost of 0.46% average performance drop, while also achieving notable performance improvements in multi-image tasks. These results highlight the effectiveness of fine-grained token pruning and contribute to advancing the efficiency and scalability of LVLMs. Our source code will be made publicly available.</article>","contentLength":1410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LXLv2: Enhanced LiDAR Excluded Lean 3D Object Detection with Fusion of 4D Radar and Camera","url":"https://arxiv.org/abs/2502.14503","date":1740114000,"author":"","guid":8096,"unread":true,"content":"<article>arXiv:2502.14503v1 Announce Type: new \nAbstract: As the previous state-of-the-art 4D radar-camera fusion-based 3D object detection method, LXL utilizes the predicted image depth distribution maps and radar 3D occupancy grids to assist the sampling-based image view transformation. However, the depth prediction lacks accuracy and consistency, and the concatenation-based fusion in LXL impedes the model robustness. In this work, we propose LXLv2, where modifications are made to overcome the limitations and improve the performance. Specifically, considering the position error in radar measurements, we devise a one-to-many depth supervision strategy via radar points, where the radar cross section (RCS) value is further exploited to adjust the supervision area for object-level depth consistency. Additionally, a channel and spatial attention-based fusion module named CSAFusion is introduced to improve feature adaptiveness. Experimental results on the View-of-Delft and TJ4DRadSet datasets show that the proposed LXLv2 can outperform LXL in detection accuracy, inference speed and robustness, demonstrating the effectiveness of the model.</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?","url":"https://arxiv.org/abs/2502.14502","date":1740114000,"author":"","guid":8097,"unread":true,"content":"<article>arXiv:2502.14502v1 Announce Type: new \nAbstract: The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.</article>","contentLength":1307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards a Perspectivist Turn in Argument Quality Assessment","url":"https://arxiv.org/abs/2502.14501","date":1740114000,"author":"","guid":8098,"unread":true,"content":"<article>arXiv:2502.14501v1 Announce Type: new \nAbstract: The assessment of argument quality depends on well-established logical, rhetorical, and dialectical properties that are unavoidably subjective: multiple valid assessments may exist, there is no unequivocal ground truth. This aligns with recent paths in machine learning, which embrace the co-existence of different perspectives. However, this potential remains largely unexplored in NLP research on argument quality. One crucial reason seems to be the yet unexplored availability of suitable datasets. We fill this gap by conducting a systematic review of argument quality datasets. We assign them to a multi-layered categorization targeting two aspects: (a) What has been annotated: we collect the quality dimensions covered in datasets and consolidate them in an overarching taxonomy, increasing dataset comparability and interoperability. (b) Who annotated: we survey what information is given about annotators, enabling perspectivist research and grounding our recommendations for future actions. To this end, we discuss datasets suitable for developing perspectivist models (i.e., those containing individual, non-aggregated annotations), and we showcase the importance of a controlled selection of annotators in a pilot study.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MLGym: A New Framework and Benchmark for Advancing AI Research Agents","url":"https://arxiv.org/abs/2502.14499","date":1740114000,"author":"","guid":8099,"unread":true,"content":"<article>arXiv:2502.14499v1 Announce Type: new \nAbstract: We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.</article>","contentLength":1517,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stories that (are) Move(d by) Markets: A Causal Exploration of Market Shocks and Semantic Shifts across Different Partisan Groups","url":"https://arxiv.org/abs/2502.14497","date":1740114000,"author":"","guid":8100,"unread":true,"content":"<article>arXiv:2502.14497v1 Announce Type: new \nAbstract: Macroeconomic fluctuations and the narratives that shape them form a mutually reinforcing cycle: public discourse can spur behavioural changes leading to economic shifts, which then result in changes in the stories that propagate. We show that shifts in semantic embedding space can be causally linked to financial market shocks -- deviations from the expected market behaviour. Furthermore, we show how partisanship can influence the predictive power of text for market fluctuations and shape reactions to those same shocks. We also provide some evidence that text-based signals are particularly salient during unexpected events such as COVID-19, highlighting the value of language data as an exogenous variable in economic forecasting. Our findings underscore the bidirectional relationship between news outlets and market shocks, offering a novel empirical approach to studying their effect on each other.</article>","contentLength":957,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Language Multi-Agent Learning with Multi-Agent Credit Re-Assignment for Interactive Environment Generalization","url":"https://arxiv.org/abs/2502.14496","date":1740114000,"author":"","guid":8101,"unread":true,"content":"<article>arXiv:2502.14496v1 Announce Type: new \nAbstract: LLM-based agents have made significant advancements in interactive environments, such as mobile operations and web browsing, and other domains beyond computer using. Current multi-agent systems universally excel in performance, compared to single agents, but struggle with generalization across environments due to predefined roles and inadequate strategies for generalizing language agents. The challenge of achieving both strong performance and good generalization has hindered the progress of multi-agent systems for interactive environments. To address these issues, we propose CollabUIAgents, a multi-agent reinforcement learning framework with a novel multi-agent credit re-assignment (CR) strategy, assigning process rewards with LLMs rather than environment-specific rewards and learning with synthesized preference data, in order to foster generalizable, collaborative behaviors among the role-free agents' policies. Empirical results show that our framework improves both performance and cross-environment generalizability of multi-agent systems. Moreover, our 7B-parameter system achieves results on par with or exceed strong closed-source models, and the LLM that guides the CR. We also provide insights in using granular CR rewards effectively for environment generalization, and accommodating trained LLMs in multi-agent systems.</article>","contentLength":1392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nearshore Underwater Target Detection Meets UAV-borne Hyperspectral Remote Sensing: A Novel Hybrid-level Contrastive Learning Framework and Benchmark Dataset","url":"https://arxiv.org/abs/2502.14495","date":1740114000,"author":"","guid":8102,"unread":true,"content":"<article>arXiv:2502.14495v1 Announce Type: new \nAbstract: UAV-borne hyperspectral remote sensing has emerged as a promising approach for underwater target detection (UTD). However, its effectiveness is hindered by spectral distortions in nearshore environments, which compromise the accuracy of traditional hyperspectral UTD (HUTD) methods that rely on bathymetric model. These distortions lead to significant uncertainty in target and background spectra, challenging the detection process. To address this, we propose the Hyperspectral Underwater Contrastive Learning Network (HUCLNet), a novel framework that integrates contrastive learning with a self-paced learning paradigm for robust HUTD in nearshore regions. HUCLNet extracts discriminative features from distorted hyperspectral data through contrastive learning, while the self-paced learning strategy selectively prioritizes the most informative samples. Additionally, a reliability-guided clustering strategy enhances the robustness of learned representations.To evaluate the method effectiveness, we conduct a novel nearshore HUTD benchmark dataset, ATR2-HUTD, covering three diverse scenarios with varying water types and turbidity, and target types. Extensive experiments demonstrate that HUCLNet significantly outperforms state-of-the-art methods. The dataset and code will be publicly available at: https://github.com/qjh1996/HUTD</article>","contentLength":1387,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following","url":"https://arxiv.org/abs/2502.14494","date":1740114000,"author":"","guid":8103,"unread":true,"content":"<article>arXiv:2502.14494v1 Announce Type: new \nAbstract: Multi-turn instruction following capability constitutes a core competency of large language models (LLMs) in real-world applications. Existing evaluation benchmarks predominantly focus on fine-grained constraint satisfaction and domain-specific capability assessment, yet overlook the crucial structural dependency between dialogue turns that distinguishes multi-turn from single-turn interactions. This structural dependency not only reflects user intent but also establishes a second dimension for instruction following evaluation beyond constraint satisfaction. To address this gap, we propose StructFlowBench, a multi-turn instruction following benchmark with structural flow modeling. The benchmark innovatively defines a structural flow framework comprising six fundamental inter-turn relationships, which not only introduces novel structural constraints for model evaluation but also serves as generation parameters for creating customized dialogue flows tailored to specific scenarios. Adopting established LLM-based automatic evaluation methodologies, we conduct systematic evaluations of 13 leading open-source and closed-source LLMs. Experimental results reveal significant deficiencies in current models' comprehension of multi-turn dialogue structures. The code is available at \\url{https://github.com/MLGroupJLU/StructFlowBench}.</article>","contentLength":1392,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CrossFuse: Learning Infrared and Visible Image Fusion by Cross-Sensor Top-K Vision Alignment and Beyond","url":"https://arxiv.org/abs/2502.14493","date":1740114000,"author":"","guid":8104,"unread":true,"content":"<article>arXiv:2502.14493v1 Announce Type: new \nAbstract: Infrared and visible image fusion (IVIF) is increasingly applied in critical fields such as video surveillance and autonomous driving systems. Significant progress has been made in deep learning-based fusion methods. However, these models frequently encounter out-of-distribution (OOD) scenes in real-world applications, which severely impact their performance and reliability. Therefore, addressing the challenge of OOD data is crucial for the safe deployment of these models in open-world environments. Unlike existing research, our focus is on the challenges posed by OOD data in real-world applications and on enhancing the robustness and generalization of models. In this paper, we propose an infrared-visible fusion framework based on Multi-View Augmentation. For external data augmentation, Top-k Selective Vision Alignment is employed to mitigate distribution shifts between datasets by performing RGB-wise transformations on visible images. This strategy effectively introduces augmented samples, enhancing the adaptability of the model to complex real-world scenarios. Additionally, for internal data augmentation, self-supervised learning is established using Weak-Aggressive Augmentation. This enables the model to learn more robust and general feature representations during the fusion process, thereby improving robustness and generalization. Extensive experiments demonstrate that the proposed method exhibits superior performance and robustness across various conditions and environments. Our approach significantly enhances the reliability and stability of IVIF tasks in practical applications.</article>","contentLength":1660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Statistical Scenario Modelling and Lookalike Distributions for Multi-Variate AI Risk","url":"https://arxiv.org/abs/2502.14491","date":1740114000,"author":"","guid":8105,"unread":true,"content":"<article>arXiv:2502.14491v1 Announce Type: new \nAbstract: Evaluating AI safety requires statistically rigorous methods and risk metrics for understanding how the use of AI affects aggregated risk. However, much AI safety literature focuses upon risks arising from AI models in isolation, lacking consideration of how modular use of AI affects risk distribution of workflow components or overall risk metrics. There is also a lack of statistical grounding enabling sensitisation of risk models in the presence of absence of AI to estimate causal contributions of AI. This is in part due to the dearth of AI impact data upon which to fit distributions. In this work, we address these gaps in two ways. First, we demonstrate how scenario modelling (grounded in established statistical techniques such as Markov chains, copulas and Monte Carlo simulation) can be used to model AI risk holistically. Second, we show how lookalike distributions from phenomena analogous to AI can be used to estimate AI impacts in the absence of directly observable data. We demonstrate the utility of our methods for benchmarking cumulative AI risk via risk analysis of a logistic scenario simulations.</article>","contentLength":1171,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"U-index: A Universal Indexing Framework for Matching Long Patterns","url":"https://arxiv.org/abs/2502.14488","date":1740114000,"author":"","guid":8106,"unread":true,"content":"<article>arXiv:2502.14488v1 Announce Type: new \nAbstract: Text indexing is a fundamental and well-studied problem. Classic solutions either replace the original text with a compressed representation, e.g., the FM-index and its variants, or keep it uncompressed but attach some redundancy - an index - to accelerate matching. The former solutions thus retain excellent compressed space, but areslow in practice. The latter approaches, like the suffix array, instead sacrifice space for speed.\n  We show that efficient text indexing can be achieved using just a small extra space on top of the original text, provided that the query patterns are sufficiently long. More specifically, we develop a new indexing paradigm in which a sketch of a query pattern is first matched against a sketch of the text. Once candidate matches are retrieved, they are verified using the original text. This paradigm is thus universal in the sense that it allows us to use any solution to index the sketched text, like a suffix array, FM-index, or r-index.\n  We explore both the theory and the practice of this universal framework. With an extensive experimental analysis, we show that, surprisingly, universal indexes can be constructed much faster than their unsketched counterparts and take a fraction of the space, as a direct consequence of (i) having a lower bound on the length of patterns and (ii) working in sketch space. Furthermore, these data structures have the potential of retaining or even improving query time, because matching against the sketched text is faster and verifying candidates can be theoretically done in constant time per occurrence (or, in practice, by short and cache-friendly scans of the text). Finally, we discuss some important applications of this novel indexing paradigm to computational biology. We hypothesize that such indexes will be particularly effective when the queries are sufficiently long, and so demonstrate applications in long-read mapping.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Temporal Misalignment and Probabilistic Neurons","url":"https://arxiv.org/abs/2502.14487","date":1740114000,"author":"","guid":8107,"unread":true,"content":"<article>arXiv:2502.14487v1 Announce Type: new \nAbstract: Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to Artificial Neural Networks (ANNs) by mimicking biological neural principles, establishing them as a promising approach to mitigate the increasing energy demands of large-scale neural models. However, fully harnessing the capabilities of SNNs remains challenging due to their discrete signal processing and temporal dynamics. ANN-SNN conversion has emerged as a practical approach, enabling SNNs to achieve competitive performance on complex machine learning tasks. In this work, we identify a phenomenon in the ANN-SNN conversion framework, termed temporal misalignment, in which random spike rearrangement across SNN layers leads to performance improvements. Based on this observation, we introduce biologically plausible two-phase probabilistic (TPP) spiking neurons, further enhancing the conversion process. We demonstrate the advantages of our proposed method both theoretically and empirically through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet across a variety of architectures, achieving state-of-the-art results.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Jailbreak Defenses Work and Ensemble? A Mechanistic Investigation","url":"https://arxiv.org/abs/2502.14486","date":1740114000,"author":"","guid":8108,"unread":true,"content":"<article>arXiv:2502.14486v1 Announce Type: new \nAbstract: Jailbreak attacks, where harmful prompts bypass generative models' built-in safety, raise serious concerns about model vulnerability. While many defense methods have been proposed, the trade-offs between safety and helpfulness, and their application to Large Vision-Language Models (LVLMs), are not well understood. This paper systematically examines jailbreak defenses by reframing the standard generation task as a binary classification problem to assess model refusal tendencies for both harmful and benign queries. We identify two key defense mechanisms: safety shift, which increases refusal rates across all queries, and harmfulness discrimination, which improves the model's ability to distinguish between harmful and benign inputs. Using these mechanisms, we develop two ensemble defense strategies-inter-mechanism ensembles and intra-mechanism ensembles-to balance safety and helpfulness. Experiments on the MM-SafetyBench and MOSSBench datasets with LLaVA-1.5 models show that these strategies effectively improve model safety or optimize the trade-off between safety and helpfulness.</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NLoRA: Nystr\\\"om-Initiated Low-Rank Adaptation for Large Language Models","url":"https://arxiv.org/abs/2502.14482","date":1740114000,"author":"","guid":8109,"unread":true,"content":"<article>arXiv:2502.14482v1 Announce Type: new \nAbstract: Parameter-efficient fine-tuning (PEFT) is essential for adapting large language models (LLMs), with low-rank adaptation (LoRA) being the most popular approach. However, LoRA suffers from slow convergence, and some recent LoRA variants, such as PiSSA, primarily rely on Singular Value Decomposition (SVD) for initialization, leading to expensive computation. To mitigate these problems, we use the Nystr\\\"om method, which follows a three-matrix manipulation. We first introduce StructuredLoRA (SLoRA), which investigates adding a small intermediate matrix between the low-rank matrices A and B. Secondly, we propose Nystr\\\"omLoRA (NLoRA), which leverages Nystr\\\"om-based initialization for SLoRA to improve its effectiveness and efficiency. Finally, we propose IntermediateTune (IntTune), which explores fine-tuning exclusively on the intermediate matrix of NLoRA to further boost LLM efficiency. We evaluate our methods on five natural language generation (NLG) tasks and eight natural language understanding (NLU) tasks. On GSM8K, SLoRA and NLoRA achieve accuracies of 56.48% and 57.70%, surpassing LoRA by 33.52% and 36.41%, with only 3.67 million additional trainable parameters. IntTune improves average NLG performance over LoRA by 7.45% while using only 1.25% of its parameters. These results demonstrate the efficiency and effectiveness of our approach in enhancing model performance with minimal parameter overhead.</article>","contentLength":1472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inductive Synthesis of Inductive Heap Predicates -- Extended Version","url":"https://arxiv.org/abs/2502.14478","date":1740114000,"author":"","guid":8110,"unread":true,"content":"<article>arXiv:2502.14478v1 Announce Type: new \nAbstract: We present an approach to automatically synthesise recursive predicates in Separation Logic (SL) from concrete data structure instances using Inductive Logic Programming (ILP) techniques. The main challenges to make such synthesis effective are (1) making it work without negative examples that are required in ILP but are difficult to construct for heap-based structures in an automated fashion, and (2) to be capable of summarising not just the shape of a heap (e.g., it is a linked list), but also the properties of the data it stores (e.g., it is a sorted linked list). We tackle these challenges with a new predicate learning algorithm. The key contributions of our work are (a) the formulation of ILP-based learning only using positive examples and (b) an algorithm that synthesises property-rich SL predicates from concrete memory graphs based on the positive-only learning.\n  We show that our framework can efficiently and correctly synthesise SL predicates for structures that were beyond the reach of the state-of-the-art tools, including those featuring non-trivial payload constraints (e.g., binary search trees) and nested recursion (e.g., n-ary trees). We further extend the usability of our approach by a memory graph generator that produces positive heap examples from programs. Finally, we show how our approach facilitates deductive verification and synthesis of correct-by-construction code.</article>","contentLength":1459,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unshackling Context Length: An Efficient Selective Attention Approach through Query-Key Compression","url":"https://arxiv.org/abs/2502.14477","date":1740114000,"author":"","guid":8111,"unread":true,"content":"<article>arXiv:2502.14477v1 Announce Type: new \nAbstract: Handling long-context sequences efficiently remains a significant challenge in large language models (LLMs). Existing methods for token selection in sequence extrapolation either employ a permanent eviction strategy or select tokens by chunk, which may lead to the loss of critical information. We propose Efficient Selective Attention (ESA), a novel approach that extends context length by efficiently selecting the most critical tokens at the token level to compute attention. ESA reduces the computational complexity of token selection by compressing query and key vectors into lower-dimensional representations. We evaluate ESA on long sequence benchmarks with maximum lengths up to 256k using open-source LLMs with context lengths of 8k and 32k. ESA outperforms other selective attention methods, especially in tasks requiring the retrieval of multiple pieces of information, achieving comparable performance to full-attention extrapolation methods across various tasks, with superior results in certain tasks.</article>","contentLength":1064,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Argument-Based Comparative Question Answering Evaluation Benchmark","url":"https://arxiv.org/abs/2502.14476","date":1740114000,"author":"","guid":8112,"unread":true,"content":"<article>arXiv:2502.14476v1 Announce Type: new \nAbstract: In this paper, we aim to solve the problems standing in the way of automatic comparative question answering. To this end, we propose an evaluation framework to assess the quality of comparative question answering summaries. We formulate 15 criteria for assessing comparative answers created using manual annotation and annotation from 6 large language models and two comparative question asnwering datasets. We perform our tests using several LLMs and manual annotation under different settings and demonstrate the constituency of both evaluations. Our results demonstrate that the Llama-3 70B Instruct model demonstrates the best results for summary evaluation, while GPT-4 is the best for answering comparative questions. All used data, code, and evaluation results are publicly available\\footnote{\\url{https://anonymous.4open.science/r/cqa-evaluation-benchmark-4561/README.md}}.</article>","contentLength":930,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"madupite: A High-Performance Distributed Solver for Large-Scale Markov Decision Processes","url":"https://arxiv.org/abs/2502.14474","date":1740114000,"author":"","guid":8113,"unread":true,"content":"<article>arXiv:2502.14474v1 Announce Type: new \nAbstract: This paper introduces madupite, a high-performance distributed solver for large-scale Markov Decision Processes (MDPs). MDPs are widely used to model complex dynamical systems in various fields, including finance, epidemiology, and traffic control. However, real-world applications often result in extremely high-dimensional MDPs, leading to the curse of dimensionality, which is typically addressed through function approximators like neural networks. While existing solvers such as pymdptoolbox and mdpsolver provide tools for solving MDPs, they either lack scalability, support for distributed computing, or flexibility in solution methods.\n  madupite is designed to overcome these limitations by leveraging modern high-performance computing resources. It efficiently distributes memory load and computation across multiple nodes, supports a diverse set of solution methods, and offers a user-friendly Python API while maintaining a C++ core for optimal performance. With the ability to solve MDPs with millions of states, madupite provides researchers and engineers with a powerful tool to tackle large-scale decision-making problems with greater efficiency and flexibility.</article>","contentLength":1227,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrating Extra Modality Helps Segmentor Find Camouflaged Objects Well","url":"https://arxiv.org/abs/2502.14471","date":1740114000,"author":"","guid":8114,"unread":true,"content":"<article>arXiv:2502.14471v1 Announce Type: new \nAbstract: Camouflaged Object Segmentation (COS) remains a challenging problem due to the subtle visual differences between camouflaged objects and backgrounds. Owing to the exceedingly limited visual cues available from visible spectrum, previous RGB single-modality approaches often struggle to achieve satisfactory results, prompting the exploration of multimodal data to enhance detection accuracy. In this work, we present UniCOS, a novel framework that effectively leverages diverse data modalities to improve segmentation performance. UniCOS comprises two key components: a multimodal segmentor, UniSEG, and a cross-modal knowledge learning module, UniLearner. UniSEG employs a state space fusion mechanism to integrate cross-modal features within a unified state space, enhancing contextual understanding and improving robustness to integration of heterogeneous data. Additionally, it includes a fusion-feedback mechanism that facilitate feature extraction. UniLearner exploits multimodal data unrelated to the COS task to improve the segmentation ability of the COS models by generating pseudo-modal content and cross-modal semantic associations. Extensive experiments demonstrate that UniSEG outperforms existing Multimodal COS (MCOS) segmentors, regardless of whether real or pseudo-multimodal COS data is available. Moreover, in scenarios where multimodal COS data is unavailable but multimodal non-COS data is accessible, UniLearner effectively exploits these data to enhance segmentation performance. Our code will be made publicly available on \\href{https://github.com/cnyvfang/UniCOS}{GitHub}.</article>","contentLength":1647,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Smart Environments with Context-Aware Chatbots using Large Language Models","url":"https://arxiv.org/abs/2502.14469","date":1740114000,"author":"","guid":8115,"unread":true,"content":"<article>arXiv:2502.14469v1 Announce Type: new \nAbstract: This work presents a novel architecture for context-aware interactions within smart environments, leveraging Large Language Models (LLMs) to enhance user experiences. Our system integrates user location data obtained through UWB tags and sensor-equipped smart homes with real-time human activity recognition (HAR) to provide a comprehensive understanding of user context. This contextual information is then fed to an LLM-powered chatbot, enabling it to generate personalised interactions and recommendations based on the user's current activity and environment. This approach moves beyond traditional static chatbot interactions by dynamically adapting to the user's real-time situation. A case study conducted from a real-world dataset demonstrates the feasibility and effectiveness of our proposed architecture, showcasing its potential to create more intuitive and helpful interactions within smart homes. The results highlight the significant benefits of integrating LLM with real-time activity and location data to deliver personalised and contextually relevant user experiences.</article>","contentLength":1134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A multi-model study of the air pollution related to traffic flow in a two-dimensional porous metropolitan area","url":"https://arxiv.org/abs/2502.14466","date":1740114000,"author":"","guid":8116,"unread":true,"content":"<article>arXiv:2502.14466v1 Announce Type: new \nAbstract: In this paper, a useful reinterpretation of the city as a porous medium justifies the application of well-known models on fluid dynamics to develop a multi-model study of urban air pollution due to traffic flow in a large city. Thus, to simulate the traffic flow through the city we use a nonconservative macroscopic traffic model combining the continuity equation with the Darcy-Brinkman-Forchheimer equations. For the air flow, regarding the emission rate of CO$_2$ and its dispersion in the atmosphere, we combine a microscopic model -- based on regression techniques but depending on vehicles' velocity and acceleration -- with a classical convection-diffusion-reaction transport model. To solve numerically above PDEs models, the finite element method of Lagrange $\\rm{P_1}$ type along with suitable time marching schemes (like the strong stability preserving scheme) were sufficient to obtain stable numerical solutions. Several computational tests were run on a realistic scenario inspired by the Metropolitan Area of Guadalajara (Mexico), showing not only the influence of the urban landscape (that is, the porosity) on traffic flow, air flow, and pollution transport, but also other interesting phenomena such as rarefaction traffic waves.</article>","contentLength":1297,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Coverage Types for Resource-Based Policies","url":"https://arxiv.org/abs/2502.14465","date":1740114000,"author":"","guid":8117,"unread":true,"content":"<article>arXiv:2502.14465v1 Announce Type: new \nAbstract: Coverage types provide a suitable type mechanism that integrates underapproximation logic to support property-based testing. They are used to type the return value of a function that represents an input test generator. This allows us to statically assert that an input test generator not only produces valid input tests but also generates all possible ones, ensuring completeness.\n  In this paper, we extend the coverage framework to guarantee the correctness of property-based testing with respect to resource usage in the input test generator. This is achieved by incorporating into coverage types a notion of effect, which represents an overapproximation of operations on relevant resources. Programmers can define resource usage policies through logical annotations, which are then verified against the effect associated with the coverage type.</article>","contentLength":897,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PQBFL: A Post-Quantum Blockchain-based Protocol for Federated Learning","url":"https://arxiv.org/abs/2502.14464","date":1740114000,"author":"","guid":8118,"unread":true,"content":"<article>arXiv:2502.14464v1 Announce Type: new \nAbstract: One of the goals of Federated Learning (FL) is to collaboratively train a global model using local models from remote participants. However, the FL process is susceptible to various security challenges, including interception and tampering models, information leakage through shared gradients, and privacy breaches that expose participant identities or data, particularly in sensitive domains such as medical environments. Furthermore, the advent of quantum computing poses a critical threat to existing cryptographic protocols through the Shor and Grover algorithms, causing security concerns in the communication of FL systems. To address these challenges, we propose a Post-Quantum Blockchain-based protocol for Federated Learning (PQBFL) that utilizes post-quantum cryptographic (PQC) algorithms and blockchain to enhance model security and participant identity privacy in FL systems. It employs a hybrid communication strategy that combines off-chain and on-chain channels to optimize cost efficiency, improve security, and preserve participant privacy while ensuring accountability for reputation-based authentication in FL systems. The PQBFL specifically addresses the security requirement for the iterative nature of FL, which is a less notable point in the literature. Hence, it leverages ratcheting mechanisms to provide forward secrecy and post-compromise security during all the rounds of the learning process. In conclusion, PQBFL provides a secure and resilient solution for federated learning that is well-suited to the quantum computing era.</article>","contentLength":1606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Detecting Metadata-Related Bugs in Enterprise Applications","url":"https://arxiv.org/abs/2502.14463","date":1740114000,"author":"","guid":8119,"unread":true,"content":"<article>arXiv:2502.14463v1 Announce Type: new \nAbstract: When building enterprise applications (EAs) on Java frameworks (e.g., Spring), developers often configure application components via metadata (i.e., Java annotations and XML files). It is challenging for developers to correctly use metadata, because the usage rules can be complex and existing tools provide limited assistance. When developers misuse metadata, EAs become misconfigured, which defects can trigger erroneous runtime behaviors or introduce security vulnerabilities. To help developers correctly use metadata, this paper presents (1) RSL -- a domain-specific language that domain experts can adopt to prescribe metadata checking rules, and (2) MeCheck -- a tool that takes in RSL rules and EAs to check for rule violations.\n  With RSL, domain experts (e.g., developers of a Java framework) can specify metadata checking rules by defining content consistency among XML files, annotations, and Java code. Given such RSL rules and a program to scan, MeCheck interprets rules as cross-file static analyzers, which analyzers scan Java and/or XML files to gather information and look for consistency violations. For evaluation, we studied the Spring and JUnit documentation to manually define 15 rules, and created 2 datasets with 115 open-source EAs. The first dataset includes 45 EAs, and the ground truth of 45 manually injected bugs. The second dataset includes multiple versions of 70 EAs. We observed that MeCheck identified bugs in the first dataset with 100% precision, 96% recall, and 98% F-score. It reported 156 bugs in the second dataset, 53 of which bugs were already fixed by developers. Our evaluation shows that MeCheck helps ensure the correct usage of metadata.</article>","contentLength":1735,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Single-image Reflectance and Transmittance Estimation from Any Flatbed Scanner","url":"https://arxiv.org/abs/2502.14462","date":1740114000,"author":"","guid":8120,"unread":true,"content":"<article>arXiv:2502.14462v1 Announce Type: new \nAbstract: Flatbed scanners have emerged as promising devices for high-resolution, single-image material capture. However, existing approaches assume very specific conditions, such as uniform diffuse illumination, which are only available in certain high-end devices, hindering their scalability and cost. In contrast, in this work, we introduce a method inspired by intrinsic image decomposition, which accurately removes both shading and specularity, effectively allowing captures with any flatbed scanner. Further, we extend previous work on single-image material reflectance capture with the estimation of opacity and transmittance, critical components of full material appearance (SVBSDF), improving the results for any material captured with a flatbed scanner, at a very high resolution and accuracy</article>","contentLength":843,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Llamba: Scaling Distilled Recurrent Models for Efficient Language Processing","url":"https://arxiv.org/abs/2502.14458","date":1740114000,"author":"","guid":8121,"unread":true,"content":"<article>arXiv:2502.14458v1 Announce Type: new \nAbstract: We introduce Llamba, a family of efficient recurrent language models distilled from Llama-3.x into the Mamba architecture. The series includes Llamba-1B, Llamba-3B, and Llamba-8B, which achieve higher inference throughput and handle significantly larger batch sizes than Transformer-based models while maintaining comparable benchmark performance. Furthermore, Llamba demonstrates the effectiveness of cross-architecture distillation using MOHAWK (Bick et al., 2024), achieving these results with less than 0.1% of the training data typically used for models of similar size. To take full advantage of their efficiency, we provide an optimized implementation of Llamba for resource-constrained devices such as smartphones and edge platforms, offering a practical and memory-efficient alternative to Transformers. Overall, Llamba improves the tradeoff between speed, memory efficiency, and performance, making high-quality language models more accessible.</article>","contentLength":1003,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Watch Less, Feel More: Sim-to-Real RL for Generalizable Articulated Object Manipulation via Motion Adaptation and Impedance Control","url":"https://arxiv.org/abs/2502.14457","date":1740114000,"author":"","guid":8122,"unread":true,"content":"<article>arXiv:2502.14457v1 Announce Type: new \nAbstract: Articulated object manipulation poses a unique challenge compared to rigid object manipulation as the object itself represents a dynamic environment. In this work, we present a novel RL-based pipeline equipped with variable impedance control and motion adaptation leveraging observation history for generalizable articulated object manipulation, focusing on smooth and dexterous motion during zero-shot sim-to-real transfer. To mitigate the sim-to-real gap, our pipeline diminishes reliance on vision by not leveraging the vision data feature (RGBD/pointcloud) directly as policy input but rather extracting useful low-dimensional data first via off-the-shelf modules. Additionally, we experience less sim-to-real gap by inferring object motion and its intrinsic properties via observation history as well as utilizing impedance control both in the simulation and in the real world. Furthermore, we develop a well-designed training setting with great randomization and a specialized reward system (task-aware and motion-aware) that enables multi-staged, end-to-end manipulation without heuristic motion planning. To the best of our knowledge, our policy is the first to report 84\\% success rate in the real world via extensive experiments with various unseen objects.</article>","contentLength":1316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Narrative-Driven Travel Planning: Geoculturally-Grounded Script Generation with Evolutionary Itinerary Optimization","url":"https://arxiv.org/abs/2502.14456","date":1740114000,"author":"","guid":8123,"unread":true,"content":"<article>arXiv:2502.14456v1 Announce Type: new \nAbstract: To enhance tourists' experiences and immersion, this paper proposes a narrative-driven travel planning framework called NarrativeGuide, which generates a geoculturally-grounded narrative script for travelers, offering a novel, role-playing experience for their journey. In the initial stage, NarrativeGuide constructs a knowledge graph for attractions within a city, then configures the worldview, character setting, and exposition based on the knowledge graph. Using this foundation, the knowledge graph is combined to generate an independent scene unit for each attraction. During the itinerary planning stage, NarrativeGuide models narrative-driven travel planning as an optimization problem, utilizing a genetic algorithm (GA) to refine the itinerary. Before evaluating the candidate itinerary, transition scripts are generated for each pair of adjacent attractions, which, along with the scene units, form a complete script. The weighted sum of script coherence, travel time, and attraction scores is then used as the fitness value to update the candidate solution set. Experimental results across four cities, i.e., Nanjing and Yangzhou in China, Paris in France, and Berlin in Germany, demonstrate significant improvements in narrative coherence and cultural fit, alongside a notable reduction in travel time and an increase in the quality of visited attractions. Our study highlights that incorporating external evolutionary optimization effectively addresses the limitations of large language models in travel planning.Our codes are available at https://github.com/Evan01225/Narrative-Driven-Travel-Planning.</article>","contentLength":1666,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Efficient Ground-aerial Transportation System for Pest Control Enabled by AI-based Autonomous Nano-UAVs","url":"https://arxiv.org/abs/2502.14455","date":1740114000,"author":"","guid":8124,"unread":true,"content":"<article>arXiv:2502.14455v1 Announce Type: new \nAbstract: Efficient crop production requires early detection of pest outbreaks and timely treatments; we consider a solution based on a fleet of multiple autonomous miniaturized unmanned aerial vehicles (nano-UAVs) to visually detect pests and a single slower heavy vehicle that visits the detected outbreaks to deliver treatments. To cope with the extreme limitations aboard nano-UAVs, e.g., low-resolution sensors and sub-100 mW computational power budget, we design, fine-tune, and optimize a tiny image-based convolutional neural network (CNN) for pest detection. Despite the small size of our CNN (i.e., 0.58 GOps/inference), on our dataset, it scores a mean average precision (mAP) of 0.79 in detecting harmful bugs, i.e., 14% lower mAP but 32x fewer operations than the best-performing CNN in the literature. Our CNN runs in real-time at 6.8 frame/s, requiring 33 mW on a GWT GAP9 System-on-Chip aboard a Crazyflie nano-UAV. Then, to cope with in-field unexpected obstacles, we leverage a global+local path planner based on the A* algorithm. The global path planner determines the best route for the nano-UAV to sweep the entire area, while the local one runs up to 50 Hz aboard our nano-UAV and prevents collision by adjusting the short-distance path. Finally, we demonstrate with in-simulator experiments that once a 25 nano-UAVs fleet has combed a 200x200 m vineyard, collected information can be used to plan the best path for the tractor, visiting all and only required hotspots. In this scenario, our efficient transportation system, compared to a traditional single-ground vehicle performing both inspection and treatment, can save up to 20 h working time.</article>","contentLength":1709,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Exploiting Deblurring Networks for Radiance Fields","url":"https://arxiv.org/abs/2502.14454","date":1740114000,"author":"","guid":8125,"unread":true,"content":"<article>arXiv:2502.14454v1 Announce Type: new \nAbstract: In this paper, we propose DeepDeblurRF, a novel radiance field deblurring approach that can synthesize high-quality novel views from blurred training views with significantly reduced training time. DeepDeblurRF leverages deep neural network (DNN)-based deblurring modules to enjoy their deblurring performance and computational efficiency. To effectively combine DNN-based deblurring and radiance field construction, we propose a novel radiance field (RF)-guided deblurring and an iterative framework that performs RF-guided deblurring and radiance field construction in an alternating manner. Moreover, DeepDeblurRF is compatible with various scene representations, such as voxel grids and 3D Gaussians, expanding its applicability. We also present BlurRF-Synth, the first large-scale synthetic dataset for training radiance field deblurring frameworks. We conduct extensive experiments on both camera motion blur and defocus blur, demonstrating that DeepDeblurRF achieves state-of-the-art novel-view synthesis quality with significantly reduced training time.</article>","contentLength":1110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal word order for non-causal text generation with Large Language Models: the Spanish case","url":"https://arxiv.org/abs/2502.14451","date":1740114000,"author":"","guid":8126,"unread":true,"content":"<article>arXiv:2502.14451v1 Announce Type: new \nAbstract: Natural Language Generation (NLG) popularity has increased owing to the progress in Large Language Models (LLMs), with zero-shot inference capabilities. However, most neural systems utilize decoder-only causal (unidirectional) transformer models, which are effective for English but may reduce the richness of languages with less strict word order, subject omission, or different relative clause attachment preferences. This is the first work that analytically addresses optimal text generation order for non-causal language models. We present a novel Viterbi algorithm-based methodology for maximum likelihood word order estimation. We analyze the non-causal most-likelihood order probability for NLG in Spanish and, then, the probability of generating the same phrases with Spanish causal NLG. This comparative analysis reveals that causal NLG prefers English-like SVO structures. We also analyze the relationship between optimal generation order and causal left-to-right generation order using Spearman's rank correlation. Our results demonstrate that the ideal order predicted by the maximum likelihood estimator is not closely related to the causal order and may be influenced by the syntactic structure of the target sentence.</article>","contentLength":1281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM4FaaS: No-Code Application Development using LLMs and FaaS","url":"https://arxiv.org/abs/2502.14450","date":1740114000,"author":"","guid":8127,"unread":true,"content":"<article>arXiv:2502.14450v1 Announce Type: new \nAbstract: Large language models (LLMs) are powerful tools that can generate code from natural language descriptions. While this theoretically enables non-technical users to develop their own applications, they typically lack the expertise to execute, deploy, and operate generated code. This poses a barrier for such users to leverage the power of LLMs for application development.\n  In this paper, we propose leveraging the high levels of abstraction of the Function-as-a-Service (FaaS) paradigm to handle code execution and operation for non-technical users. FaaS offers function deployment without handling the underlying infrastructure, enabling users to execute LLM-generated code without concern for its operation and without requiring any technical expertise. We propose LLM4FaaS, a novel no-code application development approach that combines LLMs and FaaS platforms to enable non-technical users to build and run their own applications using only natural language descriptions. Specifically, LLM4FaaS takes user prompts, uses LLMs to generate function code based on those prompts, and deploys these functions through a FaaS platform that handles the application's operation. LLM4FaaS also leverages the FaaS infrastructure abstractions to reduce the task complexity for the LLM, improving result accuracy.\n  We evaluate LLM4FaaS with a proof-of-concept implementation based on GPT-4o and an open-source FaaS platform, using real prompts from non-technical users. Our evaluation based on these real user prompts demonstrates the feasibility of our approach and shows that LLM4FaaS can reliably build and deploy code in 71.47% of cases, up from 43.48% in a baseline without FaaS.</article>","contentLength":1725,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LEIT-motifs: Scalable Motif Mining in Multidimensional Time Series","url":"https://arxiv.org/abs/2502.14446","date":1740114000,"author":"","guid":8128,"unread":true,"content":"<article>arXiv:2502.14446v1 Announce Type: new \nAbstract: Time series play a fundamental role in many domains, capturing a plethora of information about the underlying data-generating processes. When a process generates multiple synchronized signals we are faced with multidimensional time series. In this context a fundamental problem is that of motif mining, where we seek patterns repeating twice with minor variations, spanning some of the dimensions. State of the art exact solutions for this problem run in time quadratic in the length of the input time series.\n  We provide a scalable method to find the top-k motifs in multidimensional time series with probabilistic guarantees on the quality of the results. Our algorithm runs in time subquadratic in the length of the input, and returns the exact solution with probability at least $1-\\delta$, where $\\delta$ is a user-defined parameter. The algorithm is designed to be adaptive to the input distribution, self-tuning its parameters while respecting user-defined limits on the memory to use.\n  Our theoretical analysis is complemented by an extensive experimental evaluation, showing that our algorithm is orders of magnitude faster than the state of the art.</article>","contentLength":1210,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PredictaBoard: Benchmarking LLM Score Predictability","url":"https://arxiv.org/abs/2502.14445","date":1740114000,"author":"","guid":8129,"unread":true,"content":"<article>arXiv:2502.14445v1 Announce Type: new \nAbstract: Despite possessing impressive skills, Large Language Models (LLMs) often fail unpredictably, demonstrating inconsistent success in even basic common sense reasoning tasks. This unpredictability poses a significant challenge to ensuring their safe deployment, as identifying and operating within a reliable \"safe zone\" is essential for mitigating risks. To address this, we present PredictaBoard, a novel collaborative benchmarking framework designed to evaluate the ability of score predictors (referred to as assessors) to anticipate LLM errors on specific task instances (i.e., prompts) from existing datasets. PredictaBoard evaluates pairs of LLMs and assessors by considering the rejection rate at different tolerance errors. As such, PredictaBoard stimulates research into developing better assessors and making LLMs more predictable, not only with a higher average performance. We conduct illustrative experiments using baseline assessors and state-of-the-art LLMs. PredictaBoard highlights the critical need to evaluate predictability alongside performance, paving the way for safer AI systems where errors are not only minimised but also anticipated and effectively mitigated. Code for our benchmark can be found at https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard</article>","contentLength":1331,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Enhancement of Jiang, Z., et al.s Compression-Based Classification Algorithm Applied to News Article Categorization","url":"https://arxiv.org/abs/2502.14444","date":1740114000,"author":"","guid":8130,"unread":true,"content":"<article>arXiv:2502.14444v1 Announce Type: new \nAbstract: This study enhances Jiang et al.'s compression-based classification algorithm by addressing its limitations in detecting semantic similarities between text documents. The proposed improvements focus on unigram extraction and optimized concatenation, eliminating reliance on entire document compression. By compressing extracted unigrams, the algorithm mitigates sliding window limitations inherent to gzip, improving compression efficiency and similarity detection. The optimized concatenation strategy replaces direct concatenation with the union of unigrams, reducing redundancy and enhancing the accuracy of Normalized Compression Distance (NCD) calculations. Experimental results across datasets of varying sizes and complexities demonstrate an average accuracy improvement of 5.73%, with gains of up to 11% on datasets containing longer documents. Notably, these improvements are more pronounced in datasets with high-label diversity and complex text structures. The methodology achieves these results while maintaining computational efficiency, making it suitable for resource-constrained environments. This study provides a robust, scalable solution for text classification, emphasizing lightweight preprocessing techniques to achieve efficient compression, which in turn enables more accurate classification.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stochastic Resonance Improves the Detection of Low Contrast Images in Deep Learning Models","url":"https://arxiv.org/abs/2502.14442","date":1740114000,"author":"","guid":8131,"unread":true,"content":"<article>arXiv:2502.14442v1 Announce Type: new \nAbstract: Stochastic resonance describes the utility of noise in improving the detectability of weak signals in certain types of systems. It has been observed widely in natural and engineered settings, but its utility in image classification with rate-based neural networks has not been studied extensively. In this analysis a simple LSTM recurrent neural network is trained for digit recognition and classification. During the test phase, image contrast is reduced to a point where the model fails to recognize the presence of a stimulus. Controlled noise is added to partially recover classification performance. The results indicate the presence of stochastic resonance in rate-based recurrent neural networks.</article>","contentLength":752,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Visual and Auditory Aesthetic Preferences Across Cultures","url":"https://arxiv.org/abs/2502.14439","date":1740114000,"author":"","guid":8132,"unread":true,"content":"<article>arXiv:2502.14439v1 Announce Type: new \nAbstract: Research on how humans perceive aesthetics in shapes, colours, and music has predominantly focused on Western populations, limiting our understanding of how cultural environments shape aesthetic preferences. We present a large-scale cross-cultural study examining aesthetic preferences across five distinct modalities extensively explored in the literature: shape, curvature, colour, musical harmony and melody. Our investigation gathers 401,403 preference judgements from 4,835 participants across 10 countries, systematically sampling two-dimensional parameter spaces for each modality. The findings reveal both universal patterns and cultural variations. Preferences for shape and curvature cross-culturally demonstrate a consistent preference for symmetrical forms. While colour preferences are categorically consistent, relational preferences vary across cultures. Musical harmony shows strong agreement in interval relationships despite differing regions of preference within the broad frequency spectrum, while melody shows the highest cross-cultural variation. These results suggest that aesthetic preferences emerge from an interplay between shared perceptual mechanisms and cultural learning.</article>","contentLength":1251,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"From Bugs to Breakthroughs: Novice Errors in CS2","url":"https://arxiv.org/abs/2502.14438","date":1740114000,"author":"","guid":8133,"unread":true,"content":"<article>arXiv:2502.14438v1 Announce Type: new \nAbstract: Background: Programming is a fundamental skill in computer science and software engineering specifically. Mastering it is a challenge for novices, which is evidenced by numerous errors that students make during programming assignments. Objective: In our study, we want to identify common programming errors in CS2 courses and understand how students evolve over time. Method: To this end, we conducted a longitudinal study of errors that students of a CS2 course made in subsequent programming assignments. Specifically, we manually categorized 710 errors based on a modified version of an established error framework. Result: We could observe a learning curve of students, such that they start out with only few syntactical errors, but with a high number of semantic errors. During the course, the syntax and semantic errors almost completely vanish, but logical errors remain consistently present. Conclusion: Thus, students have only little trouble with learning the programming language, but need more time to understand and express concepts in a programming language.</article>","contentLength":1121,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Natural Language Generation","url":"https://arxiv.org/abs/2502.14437","date":1740114000,"author":"","guid":8134,"unread":true,"content":"<article>arXiv:2502.14437v1 Announce Type: new \nAbstract: This book provides a broad overview of Natural Language Generation (NLG), including technology, user requirements, evaluation, and real-world applications. The focus is on concepts and insights which hopefully will remain relevant for many years, not on the latest LLM innovations. It draws on decades of work by the author and others on NLG.\n  The book has the following chapters: Introduction to NLG; Rule-Based NLG; Machine Learning and Neural NLG; Requirements; Evaluation; Safety, Maintenance, and Testing; and Applications. All chapters include examples and anecdotes from the author's personal experiences, and end with a Further Reading section.\n  The book should be especially useful to people working on applied NLG, including NLG researchers, people in other fields who want to use NLG, and commercial developers. It will not however be useful to people who want to understand the latest LLM technology.\n  There is a companion site with more information at https://ehudreiter.com/book/</article>","contentLength":1045,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Daily Land Surface Temperature Reconstruction in Landsat Cross-Track Areas Using Deep Ensemble Learning With Uncertainty Quantification","url":"https://arxiv.org/abs/2502.14433","date":1740114000,"author":"","guid":8135,"unread":true,"content":"<article>arXiv:2502.14433v1 Announce Type: new \nAbstract: Many real-world applications rely on land surface temperature (LST) data at high spatiotemporal resolution. In complex urban areas, LST exhibits significant variations, fluctuating dramatically within and across city blocks. Landsat provides high spatial resolution data at 100 meters but is limited by long revisit time, with cloud cover further disrupting data collection. Here, we propose DELAG, a deep ensemble learning method that integrates annual temperature cycles and Gaussian processes, to reconstruct Landsat LST in complex urban areas. Leveraging the cross-track characteristics and dual-satellite operation of Landsat since 2021, we further enhance data availability to 4 scenes every 16 days. We select New York City, London and Hong Kong from three different continents as study areas. Experiments show that DELAG successfully reconstructed LST in the three cities under clear-sky (RMSE = 0.73-0.96 K) and heavily-cloudy (RMSE = 0.84-1.62 K) situations, superior to existing methods. Additionally, DELAG can quantify uncertainty that enhances LST reconstruction reliability. We further tested the reconstructed LST to estimate near-surface air temperature, achieving results (RMSE = 1.48-2.11 K) comparable to those derived from clear-sky LST (RMSE = 1.63-2.02 K). The results demonstrate the successful reconstruction through DELAG and highlight the broader applications of LST reconstruction for estimating accurate air temperature. Our study thus provides a novel and practical method for Landsat LST reconstruction, particularly suited for complex urban areas within Landsat cross-track areas, taking one step toward addressing complex climate events at high spatiotemporal resolution.</article>","contentLength":1753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Port-Hamiltonian Neural Networks with Output Error Noise Models","url":"https://arxiv.org/abs/2502.14432","date":1740114000,"author":"","guid":8136,"unread":true,"content":"<article>arXiv:2502.14432v1 Announce Type: new \nAbstract: Hamiltonian neural networks (HNNs) represent a promising class of physics-informed deep learning methods that utilize Hamiltonian theory as foundational knowledge within neural networks. However, their direct application to engineering systems is often challenged by practical issues, including the presence of external inputs, dissipation, and noisy measurements. This paper introduces a novel framework that enhances the capabilities of HNNs to address these real-life factors. We integrate port-Hamiltonian theory into the neural network structure, allowing for the inclusion of external inputs and dissipation, while mitigating the impact of measurement noise through an output-error (OE) model structure. The resulting output error port-Hamiltonian neural networks (OE-pHNNs) can be adapted to tackle modeling complex engineering systems with noisy measurements. Furthermore, we propose the identification of OE-pHNNs based on the subspace encoder approach (SUBNET), which efficiently approximates the complete simulation loss using subsections of the data and uses an encoder function to predict initial states. By integrating SUBNET with OE-pHNNs, we achieve consistent models of complex engineering systems under noisy measurements. In addition, we perform a consistency analysis to ensure the reliability of the proposed data-driven model learning method. We demonstrate the effectiveness of our approach on system identification benchmarks, showing its potential as a powerful tool for modeling dynamic systems in real-world applications.</article>","contentLength":1597,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cardiac Evidence Backtracking for Eating Behavior Monitoring using Collocative Electrocardiogram Imagining","url":"https://arxiv.org/abs/2502.14430","date":1740114000,"author":"","guid":8137,"unread":true,"content":"<article>arXiv:2502.14430v1 Announce Type: new \nAbstract: Eating monitoring has remained an open challenge in medical research for years due to the lack of non-invasive sensors for continuous monitoring and the reliable methods for automatic behavior detection. In this paper, we present a pilot study using the wearable 24-hour ECG for sensing and tailoring the sophisticated deep learning for ad-hoc and interpretable detection. This is accomplished using a collocative learning framework in which 1) we construct collocative tensors as pseudo-images from 1D ECG signals to improve the feasibility of 2D image-based deep models; 2) we formulate the cardiac logic of analyzing the ECG data in a comparative way as periodic attention regulators so as to guide the deep inference to collect evidence in a human comprehensible manner; and 3) we improve the interpretability of the framework by enabling the backtracking of evidence with a set of methods designed for Class Activation Mapping (CAM) decoding and decision tree/forest generation. The effectiveness of the proposed framework has been validated on the largest ECG dataset of eating behavior with superior performance over conventional models, and its capacity of cardiac evidence mining has also been verified through the consistency of the evidence it backtracked and that of the previous medical studies.</article>","contentLength":1357,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Early-Exit and Instant Confidence Translation Quality Estimation","url":"https://arxiv.org/abs/2502.14429","date":1740114000,"author":"","guid":8138,"unread":true,"content":"<article>arXiv:2502.14429v1 Announce Type: new \nAbstract: Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) reducing the cost of quality estimation at scale, and (2) developing an inexpensive uncertainty estimation method for quality estimation. To address the latter, we introduce Instant Confidence COMET, an uncertainty-aware quality estimation model that matches the performance of previous approaches at a fraction of their costs. We extend this to Early-Exit COMET, a quality estimation model that can compute quality scores and associated confidences already at early model layers, allowing us to early-exit computations and reduce evaluation costs. We also apply our model to machine translation reranking. We combine Early-Exit COMET with an upper confidence bound bandit algorithm to find the best candidate from a large pool without having to run the full evaluation model on all candidates. In both cases (evaluation and reranking) our methods reduce the required compute by 50% with very little degradation in performance.</article>","contentLength":1274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Token-Level Density-Based Uncertainty Quantification Methods for Eliciting Truthfulness of Large Language Models","url":"https://arxiv.org/abs/2502.14427","date":1740114000,"author":"","guid":8139,"unread":true,"content":"<article>arXiv:2502.14427v1 Announce Type: new \nAbstract: Uncertainty quantification (UQ) is a prominent approach for eliciting truthful answers from large language models (LLMs). To date, information-based and consistency-based UQ have been the dominant UQ methods for text generation via LLMs. Density-based methods, despite being very effective for UQ in text classification with encoder-based models, have not been very successful with generative LLMs. In this work, we adapt Mahalanobis Distance (MD) - a well-established UQ technique in classification tasks - for text generation and introduce a new supervised UQ method. Our method extracts token embeddings from multiple layers of LLMs, computes MD scores for each token, and uses linear regression trained on these features to provide robust uncertainty scores. Through extensive experiments on eleven datasets, we demonstrate that our approach substantially improves over existing UQ methods, providing accurate and computationally efficient uncertainty scores for both sequence-level selective generation and claim-level fact-checking tasks. Our method also exhibits strong generalization to out-of-domain data, making it suitable for a wide range of LLM-based applications.</article>","contentLength":1226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Data Contamination for Large Language Models","url":"https://arxiv.org/abs/2502.14425","date":1740114000,"author":"","guid":8140,"unread":true,"content":"<article>arXiv:2502.14425v1 Announce Type: new \nAbstract: Recent advancements in Large Language Models (LLMs) have demonstrated significant progress in various areas, such as text generation and code synthesis. However, the reliability of performance evaluation has come under scrutiny due to data contamination-the unintended overlap between training and test datasets. This overlap has the potential to artificially inflate model performance, as LLMs are typically trained on extensive datasets scraped from publicly available sources. These datasets often inadvertently overlap with the benchmarks used for evaluation, leading to an overestimation of the models' true generalization capabilities. In this paper, we first examine the definition and impacts of data contamination. Secondly, we review methods for contamination-free evaluation, focusing on three strategies: data updating-based methods, data rewriting-based methods, and prevention-based methods. Specifically, we highlight dynamic benchmarks and LLM-driven evaluation methods. Finally, we categorize contamination detecting methods based on model information dependency: white-Box, gray-Box, and black-Box detection approaches. Our survey highlights the requirements for more rigorous evaluation protocols and proposes future directions for addressing data contamination challenges.</article>","contentLength":1341,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Routing and Edge Computing in Satellite-Terrestrial Networks: A Column Generation Approach","url":"https://arxiv.org/abs/2502.14422","date":1740114000,"author":"","guid":8141,"unread":true,"content":"<article>arXiv:2502.14422v1 Announce Type: new \nAbstract: Edge computing that enables satellites to process raw data locally is expected to bring further timeliness and flexibility to satellite-terrestrial networks (STNs). In this letter, In this letter, we propose a three-layer edge computing protocol, where raw data collected by satellites can be processed locally, or transmitted to other satellites or the ground station via multi-hop routing for further processing. The overall computing capacity of the proposed framework is maximized by determining the offloading strategy and route formation, subject to channel capacity and hop constraints. Given that the problem scale grows exponentially with the number of satellites and maximum-allowed hops, the column generation approach is employed to obtain the global optimal solution by activating only a subset of variables. Numerical investigations reveal that the proposed three-layer computing protocol improves the computing capacity by 40\\%, compared to the single-layer configuration.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model","url":"https://arxiv.org/abs/2502.14420","date":1740114000,"author":"","guid":8142,"unread":true,"content":"<article>arXiv:2502.14420v1 Announce Type: new \nAbstract: Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained jointly. To overcome these limitations, we propose ChatVLA, a novel framework featuring Phased Alignment Training, which incrementally integrates multimodal data after initial control mastery, and a Mixture-of-Experts architecture to minimize task interference. ChatVLA demonstrates competitive performance on visual question-answering datasets and significantly surpasses state-of-the-art vision-language-action (VLA) methods on multimodal understanding benchmarks. Notably, it achieves a six times higher performance on MMMU and scores 47.2% on MMStar with a more parameter-efficient design than ECoT. Furthermore, ChatVLA demonstrates superior performance on 25 real-world robot manipulation tasks compared to existing VLA methods like OpenVLA. Our findings highlight the potential of our unified framework for achieving both robust multimodal understanding and effective robot control.</article>","contentLength":1437,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing the Longhorn Cloud-native Software Defined Storage Engine for High Performance","url":"https://arxiv.org/abs/2502.14419","date":1740114000,"author":"","guid":8143,"unread":true,"content":"<article>arXiv:2502.14419v1 Announce Type: new \nAbstract: Longhorn is an open-source, cloud-native software-defined storage (SDS) engine that delivers distributed block storage management in Kubernetes environments. This paper explores performance optimization techniques for Longhorn's core component, the Longhorn engine, to overcome limitations in leveraging high-performance server hardware, such as solid-state NVMe disks and low-latency, high-bandwidth networking. By integrating ublk at the frontend, to expose the virtual block device to the operating system, restructuring the communication protocol, and employing DBS, our simplified, direct-to-disk storage scheme, the system achieves significant performance improvements with respect to the default I/O path. Our results contribute to enhancing Longhorn's applicability in both cloud and on-premises setups, as well as provide insights for the broader SDS community.</article>","contentLength":919,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Efficient Automatic Self-Pruning of Large Language Models","url":"https://arxiv.org/abs/2502.14413","date":1740114000,"author":"","guid":8144,"unread":true,"content":"<article>arXiv:2502.14413v1 Announce Type: new \nAbstract: Despite exceptional capabilities, Large Language Models (LLMs) still face deployment challenges due to their enormous size. Post-training structured pruning is a promising solution that prunes LLMs without the need for retraining, reducing computational overhead, and it is hardware-deployment friendly. However, the training-free nature of post-training structured pruning leads to significant performance degradation. We argue that the key to mitigating this issue lies in accurately determining the pruning rate for each layer. Meanwhile, we find that LLMs may have prior knowledge about their own redundancy. Based on this insight, we introduce $\\textbf{Self-Pruner}$ an end-to-end automatic self-pruning framework for LLMs, which efficiently search layer-wise pruning rates. Specifically, $\\textbf{Self-Pruner}$ leverages LLMs to autonomously execute the entire evolutionary search process to search for pruning rate configurations. In this process, LLMs are used to generate populations, select parent solutions from the current population, and perform crossover and mutation operations to produce offspring solutions. In this way, LLMs automatically generate and evaluate a large number of candidate solutions, effectively converging to find the pruning rate configurations with minimal human intervention. Extensive experiments demonstrate $\\textbf{Self-Pruner}$'s better performance compared to existing state-of-the-art methods. Notably, $\\textbf{Self-Pruner}$ prunes LLaMA-2-70B to 49B level with only 0.80$\\%$ drop in accuracy across seven commonsense reasoning tasks, achieving a 1.39$\\times$ speedup on NVIDIA A100 80GB GPU. Further pruning to 35B level resulted in only a 3.80$\\%$ decrease in accuracy while obtaining a 1.70$\\times$ speedup.</article>","contentLength":1805,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Evaluating Precise Geolocation Inference Capabilities of Vision Language Models","url":"https://arxiv.org/abs/2502.14412","date":1740114000,"author":"","guid":8145,"unread":true,"content":"<article>arXiv:2502.14412v1 Announce Type: new \nAbstract: The prevalence of Vision-Language Models (VLMs) raises important questions about privacy in an era where visual information is increasingly available. While foundation VLMs demonstrate broad knowledge and learned capabilities, we specifically investigate their ability to infer geographic location from previously unseen image data. This paper introduces a benchmark dataset collected from Google Street View that represents its global distribution of coverage. Foundation models are evaluated on single-image geolocation inference, with many achieving median distance errors of &lt;300 km. We further evaluate VLM \"agents\" with access to supplemental tools, observing up to a 30.6% decrease in distance error. Our findings establish that modern foundation VLMs can act as powerful image geolocation tools, without being specifically trained for this task. When coupled with increasing accessibility of these models, our findings have greater implications for online privacy. We discuss these risks, as well as future work in this area.</article>","contentLength":1082,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unstructured Evidence Attribution for Long Context Query Focused Summarization","url":"https://arxiv.org/abs/2502.14409","date":1740114000,"author":"","guid":8146,"unread":true,"content":"<article>arXiv:2502.14409v1 Announce Type: new \nAbstract: Large language models (LLMs) are capable of generating coherent summaries from very long contexts given a user query. Extracting and properly citing evidence spans could help improve the transparency and reliability of these summaries. At the same time, LLMs suffer from positional biases in terms of which information they understand and attend to, which could affect evidence citation. Whereas previous work has focused on evidence citation with predefined levels of granularity (e.g. sentence, paragraph, document, etc.), we propose the task of long-context query focused summarization with unstructured evidence citation. We show how existing systems struggle to generate and properly cite unstructured evidence from their context, and that evidence tends to be \"lost-in-the-middle\". To help mitigate this, we create the Summaries with Unstructured Evidence Text dataset (SUnsET), a synthetic dataset generated using a novel domain-agnostic pipeline which can be used as supervision to adapt LLMs to this task. We demonstrate across 5 LLMs of different sizes and 4 datasets with varying document types and lengths that LLMs adapted with SUnsET data generate more relevant and factually consistent evidence than their base models, extract evidence from more diverse locations in their context, and can generate more relevant and consistent summaries.</article>","contentLength":1402,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Differentiable Black-box and Gray-box Modeling of Nonlinear Audio Effects","url":"https://arxiv.org/abs/2502.14405","date":1740114000,"author":"","guid":8147,"unread":true,"content":"<article>arXiv:2502.14405v1 Announce Type: new \nAbstract: Audio effects are extensively used at every stage of audio and music content creation. The majority of differentiable audio effects modeling approaches fall into the black-box or gray-box paradigms; and most models have been proposed and applied to nonlinear effects like guitar amplifiers, overdrive, distortion, fuzz and compressor. Although a plethora of architectures have been introduced for the task at hand there is still lack of understanding on the state of the art, since most publications experiment with one type of nonlinear audio effect and a very small number of devices.\n  In this work we aim to shed light on the audio effects modeling landscape by comparing black-box and gray-box architectures on a large number of nonlinear audio effects, identifying the most suitable for a wide range of devices. In the process, we also: introduce time-varying gray-box models and propose models for compressor, distortion and fuzz, publish a large dataset for audio effects research - ToneTwist AFx https://github.com/mcomunita/tonetwist-afx-dataset - that is also the first open to community contributions, evaluate models on a variety of metrics and conduct extensive subjective evaluation. Code https://github.com/mcomunita/nablafx and supplementary material https://github.com/mcomunita/nnlinafx-supp-material are also available.</article>","contentLength":1388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Macro- and Micro-Hierarchical Transfer Learning Framework for Cross-Domain Fake News Detection","url":"https://arxiv.org/abs/2502.14403","date":1740114000,"author":"","guid":8148,"unread":true,"content":"<article>arXiv:2502.14403v1 Announce Type: new \nAbstract: Cross-domain fake news detection aims to mitigate domain shift and improve detection performance by transferring knowledge across domains. Existing approaches transfer knowledge based on news content and user engagements from a source domain to a target domain. However, these approaches face two main limitations, hindering effective knowledge transfer and optimal fake news detection performance. Firstly, from a micro perspective, they neglect the negative impact of veracity-irrelevant features in news content when transferring domain-shared features across domains. Secondly, from a macro perspective, existing approaches ignore the relationship between user engagement and news content, which reveals shared behaviors of common users across domains and can facilitate more effective knowledge transfer. To address these limitations, we propose a novel macro- and micro- hierarchical transfer learning framework (MMHT) for cross-domain fake news detection. Firstly, we propose a micro-hierarchical disentangling module to disentangle veracity-relevant and veracity-irrelevant features from news content in the source domain for improving fake news detection performance in the target domain. Secondly, we propose a macro-hierarchical transfer learning module to generate engagement features based on common users' shared behaviors in different domains for improving effectiveness of knowledge transfer. Extensive experiments on real-world datasets demonstrate that our framework significantly outperforms the state-of-the-art baselines.</article>","contentLength":1591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bibliometric Analysis of Scientific Production on the COVID-19 Effect in Information Sciences","url":"https://arxiv.org/abs/2502.14402","date":1740114000,"author":"","guid":8149,"unread":true,"content":"<article>arXiv:2502.14402v1 Announce Type: new \nAbstract: This paper analyzes the scientific production on the COVID-19 effect in the area of Information Sciences from a bibliometric perspective. The objectives focused on: 1) determining the most productive authors, countries, institutions and journals; 2) identifying the sources that constitute the core of scientific production; 3) examining the manuscripts with the greatest impact; and 4) visualizing the thematic and conceptual structure of the scientific domain analyzed. Bibliometric indicators and factor analysis techniques were used for data analysis. A total of 1,175 publications indexed in the Web of Science (WoS) core collection from 2020 to 2022 were retrieved. The results showed that the most relevant countries were the United States, United Kingdom, China and Spain. The core of the scientific production was formed by the publications: Journal of the American Medical Informatics Association, Information Professional, Scientometrics and Journal of Health Communication. The papers with the greatest impact were concentrated in those dedicated to the analysis of the role of telemedicine in medical care. The conceptual structure showed the main research fronts, such as the role of telehealth, academic libraries and digital literacy in the fight against the pandemic, the role of social networks in the health crisis, as well as the problem of misinformation and fake news</article>","contentLength":1438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HPS: Hard Preference Sampling for Human Preference Alignment","url":"https://arxiv.org/abs/2502.14400","date":1740114000,"author":"","guid":8150,"unread":true,"content":"<article>arXiv:2502.14400v1 Announce Type: new \nAbstract: Aligning Large Language Model (LLM) responses with human preferences is vital for building safe and controllable AI systems. While preference optimization methods based on Plackett-Luce (PL) and Bradley-Terry (BT) models have shown promise, they face challenges such as poor handling of harmful content, inefficient use of dispreferred responses, and, specifically for PL, high computational costs. To address these issues, we propose Hard Preference Sampling (HPS), a novel framework for robust and efficient human preference alignment. HPS introduces a training loss that prioritizes the most preferred response while rejecting all dispreferred and harmful ones. It emphasizes \"hard\" dispreferred responses--those closely resembling preferred ones--to enhance the model's rejection capabilities. By leveraging a single-sample Monte Carlo sampling strategy, HPS reduces computational overhead while maintaining alignment quality. Theoretically, HPS improves sample efficiency over existing PL methods and maximizes the reward margin between preferred and dispreferred responses, ensuring clearer distinctions. Experiments on HH-RLHF and PKU-Safety datasets validate HPS's effectiveness, achieving comparable BLEU and reward scores while greatly improving reward margins and thus reducing harmful content generation.</article>","contentLength":1365,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimal Popularity-based Transmission Range Selection for D2D-supported Content Delivery","url":"https://arxiv.org/abs/2502.14399","date":1740114000,"author":"","guid":8151,"unread":true,"content":"<article>arXiv:2502.14399v1 Announce Type: new \nAbstract: Considering device-to-device (D2D) wireless links as a virtual extension of 5G (and beyond) cellular networks to deliver popular contents has been proposed as an interesting approach to reduce energy consumption, congestion, and bandwidth usage at the network edge. In the scenario of multiple users in a region independently requesting some popular content, there is a major potential for energy consumption reduction exploiting D2D communications. In this scenario, we consider the problem of selecting the maximum allowed transmission range (or equivalently the maximum transmit power) for the D2D links that support the content delivery process. We show that, for a given maximum allowed D2D energy consumption, a considerable reduction of the cellular infrastructure energy consumption can be achieved by selecting the maximum D2D transmission range as a function of content class parameters such as popularity and delay-tolerance, compared to a uniform selection across different content classes. Specifically, we provide an analytical model that can be used to estimate the energy consumption (for small delay tolerance) and thus to set the optimal transmission range. We validate the model via simulations and study the energy gain that our approach allows to obtain. Our results show that the proposed approach to the maximum D2D transmission range selection allows a reduction of the overall energy consumption in the range of 30% to 55%, compared to a selection of the maximum D2D transmission range oblivious to popularity and delay tolerance.</article>","contentLength":1604,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data","url":"https://arxiv.org/abs/2502.14397","date":1740114000,"author":"","guid":8152,"unread":true,"content":"<article>arXiv:2502.14397v1 Announce Type: new \nAbstract: We introduce PhotoDoodle, a novel image editing framework designed to facilitate photo doodling by enabling artists to overlay decorative elements onto photographs. Photo doodling is challenging because the inserted elements must appear seamlessly integrated with the background, requiring realistic blending, perspective alignment, and contextual coherence. Additionally, the background must be preserved without distortion, and the artist's unique style must be captured efficiently from limited training data. These requirements are not addressed by previous methods that primarily focus on global style transfer or regional inpainting. The proposed method, PhotoDoodle, employs a two-stage training strategy. Initially, we train a general-purpose image editing model, OmniEditor, using large-scale data. Subsequently, we fine-tune this model with EditLoRA using a small, artist-curated dataset of before-and-after image pairs to capture distinct editing styles and techniques. To enhance consistency in the generated results, we introduce a positional encoding reuse mechanism. Additionally, we release a PhotoDoodle dataset featuring six high-quality styles. Extensive experiments demonstrate the advanced performance and robustness of our method in customized image editing, opening new possibilities for artistic creation.</article>","contentLength":1378,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fully spectral scheme for the linear BGK equation on the whole space","url":"https://arxiv.org/abs/2502.14396","date":1740114000,"author":"","guid":8153,"unread":true,"content":"<article>arXiv:2502.14396v1 Announce Type: new \nAbstract: In this article, we design a fully spectral method in both space and velocity for a linear inhomogeneous kinetic equation with mass, momentum and energy conservation. We focus on the linear BGK equation with a confinement potential $\\phi$, even if the method could be applied to different collision operators. It is based upon the projection on Hermite polynomials in velocity and orthonormal polynomials with respect to the weight $e^{-\\phi}$ in space. The potential $\\phi$ is assumed to be a polynomial. It is, to the author's knowledge, the first scheme which preserves hypocoercive behavior in addition to the conservation laws. These different properties are illustrated numerically on both quadratic and double well potential.</article>","contentLength":781,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Portuguese Variety Identification with Cross-Domain Approaches","url":"https://arxiv.org/abs/2502.14394","date":1740114000,"author":"","guid":8154,"unread":true,"content":"<article>arXiv:2502.14394v1 Announce Type: new \nAbstract: Recent advances in natural language processing have raised expectations for generative models to produce coherent text across diverse language varieties. In the particular case of the Portuguese language, the predominance of Brazilian Portuguese corpora online introduces linguistic biases in these models, limiting their applicability outside of Brazil. To address this gap and promote the creation of European Portuguese resources, we developed a cross-domain language variety identifier (LVI) to discriminate between European and Brazilian Portuguese. Motivated by the findings of our literature review, we compiled the PtBrVarId corpus, a cross-domain LVI dataset, and study the effectiveness of transformer-based LVI classifiers for cross-domain scenarios. Although this research focuses on two Portuguese varieties, our contribution can be extended to other varieties and languages. We open source the code, corpus, and models to foster further research in this task.</article>","contentLength":1022,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Leveraging Small LLMs for Argument Mining in Education: Argument Component Identification, Classification, and Assessment","url":"https://arxiv.org/abs/2502.14389","date":1740114000,"author":"","guid":8155,"unread":true,"content":"<article>arXiv:2502.14389v1 Announce Type: new \nAbstract: Argument mining algorithms analyze the argumentative structure of essays, making them a valuable tool for enhancing education by providing targeted feedback on the students' argumentation skills. While current methods often use encoder or encoder-decoder deep learning architectures, decoder-only models remain largely unexplored, offering a promising research direction.\n  This paper proposes leveraging open-source, small Large Language Models (LLMs) for argument mining through few-shot prompting and fine-tuning. These models' small size and open-source nature ensure accessibility, privacy, and computational efficiency, enabling schools and educators to adopt and deploy them locally. Specifically, we perform three tasks: segmentation of student essays into arguments, classification of the arguments by type, and assessment of their quality. We empirically evaluate the models on the Feedback Prize - Predicting Effective Arguments dataset of grade 6-12 students essays and demonstrate how fine-tuned small LLMs outperform baseline methods in segmenting the essays and determining the argument types while few-shot prompting yields comparable performance to that of the baselines in assessing quality. This work highlights the educational potential of small, open-source LLMs to provide real-time, personalized feedback, enhancing independent learning and writing skills while ensuring low computational cost and privacy.</article>","contentLength":1478,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MPPI-DBaS: Safe Trajectory Optimization with Adaptive Exploration","url":"https://arxiv.org/abs/2502.14387","date":1740114000,"author":"","guid":8156,"unread":true,"content":"<article>arXiv:2502.14387v1 Announce Type: new \nAbstract: In trajectory optimization, Model Predictive Path Integral (MPPI) control is a sampling-based Model Predictive Control (MPC) framework that generates optimal inputs by efficiently simulating numerous trajectories. In practice, however, MPPI often struggles to guarantee safety assurance and balance efficient sampling in open spaces with the need for more extensive exploration under tight constraints. To address this challenge, we incorporate discrete barrier states (DBaS) into MPPI and propose a novel MPPI-DBaS algorithm that ensures system safety and enables adaptive exploration across diverse scenarios. We evaluate our method in simulation experiments where the vehicle navigates through closely placed obstacles. The results demonstrate that the proposed algorithm significantly outperforms standard MPPI, achieving a higher success rate and lower tracking errors.</article>","contentLength":923,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tradutor: Building a Variety Specific Translation Model","url":"https://arxiv.org/abs/2502.14385","date":1740114000,"author":"","guid":8157,"unread":true,"content":"<article>arXiv:2502.14385v1 Announce Type: new \nAbstract: Language models have become foundational to many widely used systems. However, these seemingly advantageous models are double-edged swords. While they excel in tasks related to resource-rich languages like English, they often lose the fine nuances of language forms, dialects, and varieties that are inherent to languages spoken in multiple regions of the world. Languages like European Portuguese are neglected in favor of their more popular counterpart, Brazilian Portuguese, leading to suboptimal performance in various linguistic tasks. To address this gap, we introduce the first open-source translation model specifically tailored for European Portuguese, along with a novel dataset specifically designed for this task. Results from automatic evaluations on two benchmark datasets demonstrate that our best model surpasses existing open-source translation systems for Portuguese and approaches the performance of industry-leading closed-source systems for European Portuguese. By making our dataset, models, and code publicly available, we aim to support and encourage further research, fostering advancements in the representation of underrepresented language varieties.</article>","contentLength":1226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rumor Detection by Multi-task Suffix Learning based on Time-series Dual Sentiments","url":"https://arxiv.org/abs/2502.14383","date":1740114000,"author":"","guid":8158,"unread":true,"content":"<article>arXiv:2502.14383v1 Announce Type: new \nAbstract: The widespread dissemination of rumors on social media has a significant impact on people's lives, potentially leading to public panic and fear. Rumors often evoke specific sentiments, resonating with readers and prompting sharing. To effectively detect and track rumors, it is essential to observe the fine-grained sentiments of both source and response message pairs as the rumor evolves over time. However, current rumor detection methods fail to account for this aspect. In this paper, we propose MSuf, the first multi-task suffix learning framework for rumor detection and tracking using time series dual (coupled) sentiments. MSuf includes three modules: (1) an LLM to extract sentiment intensity features and sort them chronologically; (2) a module that fuses the sorted sentiment features with their source text word embeddings to obtain an aligned embedding; (3) two hard prompts are combined with the aligned vector to perform rumor detection and sentiment analysis using one frozen LLM. MSuf effectively enhances the performance of LLMs for rumor detection with only minimal parameter fine-tuning. Evaluating MSuf on four rumor detection benchmarks, we find significant improvements compared to other emotion-based methods.</article>","contentLength":1283,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"S*: Test Time Scaling for Code Generation","url":"https://arxiv.org/abs/2502.14382","date":1740114000,"author":"","guid":8159,"unread":true,"content":"<article>arXiv:2502.14382v1 Announce Type: new \nAbstract: Increasing test-time compute for LLMs shows promise across domains but remains underexplored in code generation, despite extensive study in math. In this paper, we propose S*, the first hybrid test-time scaling framework that substantially improves the coverage and selection accuracy of generated code. S* extends the existing parallel scaling paradigm with sequential scaling to push performance boundaries. It further leverages a novel selection mechanism that adaptively generates distinguishing inputs for pairwise comparison, combined with execution-grounded information to robustly identify correct solutions. We evaluate across 12 Large Language Models and Large Reasoning Model and show: (1) S* consistently improves performance across model families and sizes, enabling a 3B model to outperform GPT-4o-mini; (2) S* enables non-reasoning models to surpass reasoning models - GPT-4o-mini with S* outperforms o1-preview by 3.7% on LiveCodeBench; (3) S* further boosts state-of-the-art reasoning models - DeepSeek-R1-Distill-Qwen-32B with S* achieves 85.7% on LiveCodeBench, approaching o1 (high) at 88.5%. Code will be available under https://github.com/NovaSky-AI/SkyThought.</article>","contentLength":1232,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"dtaianomaly: A Python library for time series anomaly detection","url":"https://arxiv.org/abs/2502.14381","date":1740114000,"author":"","guid":8160,"unread":true,"content":"<article>arXiv:2502.14381v1 Announce Type: new \nAbstract: dtaianomaly is an open-source Python library for time series anomaly detection, designed to bridge the gap between academic research and real-world applications. Our goal is to (1) accelerate the development of novel state-of-the-art anomaly detection techniques through simple extensibility; (2) offer functionality for large-scale experimental validation; and thereby (3) bring cutting-edge research to business and industry through a standardized API, similar to scikit-learn to lower the entry barrier for both new and experienced users. Besides these key features, dtaianomaly offers (1) a broad range of built-in anomaly detectors, (2) support for time series preprocessing, (3) tools for visual analysis, (4) confidence prediction of anomaly scores, (5) runtime and memory profiling, (6) comprehensive documentation, and (7) cross-platform unit testing.\n  The source code of dtaianomaly, documentation, code examples and installation guides are publicly available at https://github.com/ML-KULeuven/dtaianomaly.</article>","contentLength":1066,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Affinity and Diversity: A Unified Metric for Demonstration Selection via Internal Representations","url":"https://arxiv.org/abs/2502.14380","date":1740114000,"author":"","guid":8161,"unread":true,"content":"<article>arXiv:2502.14380v1 Announce Type: new \nAbstract: The performance of In-Context Learning (ICL) is highly sensitive to the selected demonstrations. Existing approaches to demonstration selection optimize different objectives, yielding inconsistent results. To address this, we propose a unified metric--affinity and diversity--that leverages ICL model's internal representations. Our experiments show that both affinity and diversity strongly correlate with test accuracies, indicating their effectiveness for demonstration selection. Moreover, we show that our proposed metrics align well with various previous works to unify the inconsistency.</article>","contentLength":643,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Achieving adaptivity and optimality for multi-armed bandits using Exponential-Kullback Leiblier Maillard Sampling","url":"https://arxiv.org/abs/2502.14379","date":1740114000,"author":"","guid":8162,"unread":true,"content":"<article>arXiv:2502.14379v1 Announce Type: new \nAbstract: We study the problem of Multi-Armed Bandits (MAB) with reward distributions belonging to a One-Parameter Exponential Distribution (OPED) family. In the literature, several criteria have been proposed to evaluate the performance of such algorithms, including Asymptotic Optimality (A.O.), Minimax Optimality (M.O.), Sub-UCB, and variance-adaptive worst-case regret bound. Thompson Sampling (TS)-based and Upper Confidence Bound (UCB)-based algorithms have been employed to achieve some of these criteria. However, none of these algorithms simultaneously satisfy all the aforementioned criteria.\n  In this paper, we design an algorithm, Exponential Kullback-Leibler Maillard Sampling (abbrev. \\expklms), that can achieve multiple optimality criteria simultaneously, including A.O., M.O. with a logarithmic factor, Sub-UCB, and variance-adaptive worst-case regret bound.</article>","contentLength":916,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Extremal Self-Dual Codes and Linear Complementary Dual Codes from Double Circulant Codes","url":"https://arxiv.org/abs/2502.14378","date":1740114000,"author":"","guid":8163,"unread":true,"content":"<article>arXiv:2502.14378v1 Announce Type: new \nAbstract: This paper explores extremal self-dual double circulant (DC) codes and linear complementary dual (LCD) codes of arbitrary length over the Galois field $\\mathbb F_2$. We establish the sufficient and necessary conditions for DC codes and bordered DC codes to be self-dual and identify the conditions for self-dual DC codes of length up to 44 to be extremal or non-extremal. Additionally, The self-duality and extremality between DC codes and bordered DC codes are also examined. Finally, sufficient conditions for bordered DC codes to be LCD codes over $\\mathbb F_2$ under Euclidean inner product are presented.</article>","contentLength":658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers","url":"https://arxiv.org/abs/2502.14377","date":1740114000,"author":"","guid":8164,"unread":true,"content":"<article>arXiv:2502.14377v1 Announce Type: new \nAbstract: The Diffusion Transformer plays a pivotal role in advancing text-to-image and text-to-video generation, owing primarily to its inherent scalability. However, existing controlled diffusion transformer methods incur significant parameter and computational overheads and suffer from inefficient resource allocation due to their failure to account for the varying relevance of control information across different transformer layers. To address this, we propose the Relevance-Guided Efficient Controllable Generation framework, RelaCtrl, enabling efficient and resource-optimized integration of control signals into the Diffusion Transformer. First, we evaluate the relevance of each layer in the Diffusion Transformer to the control information by assessing the \"ControlNet Relevance Score\"-i.e., the impact of skipping each control layer on both the quality of generation and the control effectiveness during inference. Based on the strength of the relevance, we then tailor the positioning, parameter scale, and modeling capacity of the control layers to reduce unnecessary parameters and redundant computations. Additionally, to further improve efficiency, we replace the self-attention and FFN in the commonly used copy block with the carefully designed Two-Dimensional Shuffle Mixer (TDSM), enabling efficient implementation of both the token mixer and channel mixer. Both qualitative and quantitative experimental results demonstrate that our approach achieves superior performance with only 15% of the parameters and computational complexity compared to PixArt-delta. More examples are available at https://relactrl.github.io/RelaCtrl/.</article>","contentLength":1689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Similarity Paradigm Through Textual Regularization Without Forgetting","url":"https://arxiv.org/abs/2502.14376","date":1740114000,"author":"","guid":8165,"unread":true,"content":"<article>arXiv:2502.14376v1 Announce Type: new \nAbstract: Prompt learning has emerged as a promising method for adapting pre-trained visual-language models (VLMs) to a range of downstream tasks. While optimizing the context can be effective for improving performance on specific tasks, it can often lead to poor generalization performance on unseen classes or datasets sampled from different distributions. It may be attributed to the fact that textual prompts tend to overfit downstream data distributions, leading to the forgetting of generalized knowledge derived from hand-crafted prompts. In this paper, we propose a novel method called Similarity Paradigm with Textual Regularization (SPTR) for prompt learning without forgetting. SPTR is a two-pronged design based on hand-crafted prompts that is an inseparable framework. 1) To avoid forgetting general textual knowledge, we introduce the optimal transport as a textual regularization to finely ensure approximation with hand-crafted features and tuning textual features. 2) In order to continuously unleash the general ability of multiple hand-crafted prompts, we propose a similarity paradigm for natural alignment score and adversarial alignment score to improve model robustness for generalization. Both modules share a common objective in addressing generalization issues, aiming to maximize the generalization capability derived from multiple hand-crafted prompts. Four representative tasks (i.e., non-generalization few-shot learning, base-to-novel generalization, cross-dataset generalization, domain generalization) across 11 datasets demonstrate that SPTR outperforms existing prompt learning methods.</article>","contentLength":1660,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"VFL-RPS: Relevant Participant Selection in Vertical Federated Learning","url":"https://arxiv.org/abs/2502.14375","date":1740114000,"author":"","guid":8166,"unread":true,"content":"<article>arXiv:2502.14375v1 Announce Type: new \nAbstract: Federated Learning (FL) allows collaboration between different parties, while ensuring that the data across these parties is not shared. However, not every collaboration is helpful in terms of the resulting model performance. Therefore, it is an important challenge to select the correct participants in a collaboration. As it currently stands, most of the efforts in participant selection in the literature have focused on Horizontal Federated Learning (HFL), which assumes that all features are the same across all participants, disregarding the possibility of different features across participants which is captured in Vertical Federated Learning (VFL). To close this gap in the literature, we propose a novel method VFL-RPS for participant selection in VFL, as a pre-training step. We have tested our method on several data sets performing both regression and classification tasks, showing that our method leads to comparable results as using all data by only selecting a few participants. In addition, we show that our method outperforms existing methods for participant selection in VFL.</article>","contentLength":1143,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CrossVTON: Mimicking the Logic Reasoning on Cross-category Virtual Try-on guided by Tri-zone Priors","url":"https://arxiv.org/abs/2502.14373","date":1740114000,"author":"","guid":8167,"unread":true,"content":"<article>arXiv:2502.14373v1 Announce Type: new \nAbstract: Despite remarkable progress in image-based virtual try-on systems, generating realistic and robust fitting images for cross-category virtual try-on remains a challenging task. The primary difficulty arises from the absence of human-like reasoning, which involves addressing size mismatches between garments and models while recognizing and leveraging the distinct functionalities of various regions within the model images. To address this issue, we draw inspiration from human cognitive processes and disentangle the complex reasoning required for cross-category try-on into a structured framework. This framework systematically decomposes the model image into three distinct regions: try-on, reconstruction, and imagination zones. Each zone plays a specific role in accommodating the garment and facilitating realistic synthesis. To endow the model with robust reasoning capabilities for cross-category scenarios, we propose an iterative data constructor. This constructor encompasses diverse scenarios, including intra-category try-on, any-to-dress transformations (replacing any garment category with a dress), and dress-to-any transformations (replacing a dress with another garment category). Utilizing the generated dataset, we introduce a tri-zone priors generator that intelligently predicts the try-on, reconstruction, and imagination zones by analyzing how the input garment is expected to align with the model image. Guided by these tri-zone priors, our proposed method, CrossVTON, achieves state-of-the-art performance, surpassing existing baselines in both qualitative and quantitative evaluations. Notably, it demonstrates superior capability in handling cross-category virtual try-on, meeting the complex demands of real-world applications.</article>","contentLength":1805,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asymptotic Existence of Class Envy-free Matchings","url":"https://arxiv.org/abs/2502.14371","date":1740114000,"author":"","guid":8168,"unread":true,"content":"<article>arXiv:2502.14371v1 Announce Type: new \nAbstract: We consider a one-sided matching problem where agents who are partitioned into disjoint classes and each class must receive fair treatment in a desired matching. This model, proposed by Benabbou et al. [2019], aims to address various real-life scenarios, such as the allocation of public housing and medical resources across different ethnic, age, and other demographic groups. Our focus is on achieving class envy-free matchings, where each class receives a total utility at least as large as the maximum value of a matching they would achieve from the items matched to another class. While class envy-freeness for worst-case utilities is unattainable without leaving some valuable items unmatched, such extreme cases may rarely occur in practice. To analyze the existence of a class envy-free matching in practice, we study a distributional model where agents' utilities for items are drawn from a probability distribution. Our main result establishes the asymptotic existence of a desired matching, showing that a round-robin algorithm produces a class envy-free matching as the number of agents approaches infinity.</article>","contentLength":1168,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PPO-MI: Efficient Black-Box Model Inversion via Proximal Policy Optimization","url":"https://arxiv.org/abs/2502.14370","date":1740114000,"author":"","guid":8169,"unread":true,"content":"<article>arXiv:2502.14370v1 Announce Type: new \nAbstract: Model inversion attacks pose a significant privacy risk by attempting to reconstruct private training data from trained models. Most of the existing methods either depend on gradient estimation or require white-box access to model parameters, which limits their applicability in practical scenarios. In this paper, we propose PPO-MI, a novel reinforcement learning-based framework for black-box model inversion attacks. Our approach formulates the inversion task as a Markov Decision Process, where an agent navigates the latent space of a generative model to reconstruct private training samples using only model predictions. By employing Proximal Policy Optimization (PPO) with a momentum-based state transition mechanism, along with a reward function balancing prediction accuracy and exploration, PPO-MI ensures efficient latent space exploration and high query efficiency. We conduct extensive experiments illustrates that PPO-MI outperforms the existing methods while require less attack knowledge, and it is robust across various model architectures and datasets. These results underline its effectiveness and generalizability in practical black-box scenarios, raising important considerations for the privacy vulnerabilities of deployed machine learning models.</article>","contentLength":1318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Entropy-UID: A Method for Optimizing Information Density","url":"https://arxiv.org/abs/2502.14366","date":1740114000,"author":"","guid":8170,"unread":true,"content":"<article>arXiv:2502.14366v1 Announce Type: new \nAbstract: Balanced and efficient information flow is essential for optimizing language generation models. In this work, we propose Entropy-UID, a new token selection method that balances entropy and Uniform Information Density (UID) principles for enhanced efficiency of text generation. Our approach adaptively adjusts token selection by jointly minimizing entropy and surprisal, promoting more even information distribution across generated sequences. Theoretical validation demonstrates that Entropy-UID optimally reduces information spikes while maintaining fluency and coherence. The method has been evulated using information-theoretic metrics on multiple benchmark datasets, including WikiText-2, OpenWebText, and WMT. Experimental results show that Entropy-UID achieves lower surprisal and entropy variance compared to standard GPT-2 and alternative heuristics, leading to more balanced and human-like text generation. Our findings point towards the potential of leveraging information-theoretic constraints to refine token selection strategies in autoregressive language models.</article>","contentLength":1126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Is Q-learning an Ill-posed Problem?","url":"https://arxiv.org/abs/2502.14365","date":1740114000,"author":"","guid":8171,"unread":true,"content":"<article>arXiv:2502.14365v1 Announce Type: new \nAbstract: This paper investigates the instability of Q-learning in continuous environments, a challenge frequently encountered by practitioners. Traditionally, this instability is attributed to bootstrapping and regression model errors. Using a representative reinforcement learning benchmark, we systematically examine the effects of bootstrapping and model inaccuracies by incrementally eliminating these potential error sources. Our findings reveal that even in relatively simple benchmarks, the fundamental task of Q-learning - iteratively learning a Q-function from policy-specific target values - can be inherently ill-posed and prone to failure. These insights cast doubt on the reliability of Q-learning as a universal solution for reinforcement learning problems.</article>","contentLength":811,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning","url":"https://arxiv.org/abs/2502.14361","date":1740114000,"author":"","guid":8172,"unread":true,"content":"<article>arXiv:2502.14361v1 Announce Type: new \nAbstract: While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps. However, PRMs still struggle with out-of-distribution (OOD) challenges. This paper identifies key OOD issues, including step OOD, caused by differences in reasoning patterns across model types and sizes, and question OOD, which arises from dataset shifts between training data and real-world problems. To address these issues, we introduce Retrieval-Augmented Process Reward Model (RetrievalPRM), a novel framework designed to tackle these OOD issues. By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps as a warmup, enhancing PRM's ability to evaluate target steps and improving generalization and reasoning consistency across different models and problem types. Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets. Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetrievalPRM model, establishing a new standard for PRM performance.</article>","contentLength":1274,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Weed Detection using Convolutional Neural Network","url":"https://arxiv.org/abs/2502.14360","date":1740114000,"author":"","guid":8173,"unread":true,"content":"<article>arXiv:2502.14360v1 Announce Type: new \nAbstract: In this paper we use convolutional neural networks (CNNs) for weed detection in agricultural land. We specifically investigate the application of two CNN layer types, Conv2d and dilated Conv2d, for weed detection in crop fields. The suggested method extracts features from the input photos using pre-trained models, which are subsequently adjusted for weed detection. The findings of the experiment, which used a sizable collection of dataset consisting of 15336 segments, being 3249 of soil, 7376 of soybean, 3520 grass and 1191 of broadleaf weeds. show that the suggested approach can accurately and successfully detect weeds at an accuracy of 94%. This study has significant ramifications for lowering the usage of toxic herbicides and increasing the effectiveness of weed management in agriculture.</article>","contentLength":851,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests","url":"https://arxiv.org/abs/2502.14359","date":1740114000,"author":"","guid":8174,"unread":true,"content":"<article>arXiv:2502.14359v1 Announce Type: new \nAbstract: We examine three evaluation paradigms: large question-answering benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.</article>","contentLength":1105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An exposition of recent list-size bounds of FRS Codes","url":"https://arxiv.org/abs/2502.14358","date":1740114000,"author":"","guid":8175,"unread":true,"content":"<article>arXiv:2502.14358v1 Announce Type: new \nAbstract: In the last year, there have been some remarkable improvements in the combinatorial list-size bounds of Folded Reed Solomon codes and multiplicity codes. Starting from the work on Kopparty, Ron-Zewi, Saraf and Wootters (SIAM J. Comput. 2023) (and subsequent simplifications due to Tamo (IEEE Trans. Inform. Theory 2024), we have had dramatic improvements in the list-size bounds of FRS codes due to Srivastava (SODA 2025) and Chen &amp; Zhang (STOC 2025). In this note, we give a short exposition of these three results (Tamo, Srivastava and Chen-Zhang).</article>","contentLength":599,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Full-Step-DPO: Self-Supervised Preference Optimization with Step-wise Rewards for Mathematical Reasoning","url":"https://arxiv.org/abs/2502.14356","date":1740114000,"author":"","guid":8176,"unread":true,"content":"<article>arXiv:2502.14356v1 Announce Type: new \nAbstract: Direct Preference Optimization (DPO) often struggles with long-chain mathematical reasoning. Existing approaches, such as Step-DPO, typically improve this by focusing on the first erroneous step in the reasoning chain. However, they overlook all other steps and rely heavily on humans or GPT-4 to identify erroneous steps. To address these issues, we propose Full-Step-DPO, a novel DPO framework tailored for mathematical reasoning. Instead of optimizing only the first erroneous step, it leverages step-wise rewards from the entire reasoning chain. This is achieved by training a self-supervised process reward model, which automatically scores each step, providing rewards while avoiding reliance on external signals. Furthermore, we introduce a novel step-wise DPO loss, which dynamically updates gradients based on these step-wise rewards. This endows stronger reasoning capabilities to language models. Extensive evaluations on both in-domain and out-of-domain mathematical reasoning benchmarks across various base language models, demonstrate that Full-Step-DPO achieves superior performance compared to state-of-the-art baselines.</article>","contentLength":1186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Triply Laplacian Scale Mixture Modeling for Seismic Data Noise Suppression","url":"https://arxiv.org/abs/2502.14355","date":1740114000,"author":"","guid":8177,"unread":true,"content":"<article>arXiv:2502.14355v1 Announce Type: new \nAbstract: Sparsity-based tensor recovery methods have shown great potential in suppressing seismic data noise. These methods exploit tensor sparsity measures capturing the low-dimensional structures inherent in seismic data tensors to remove noise by applying sparsity constraints through soft-thresholding or hard-thresholding operators. However, in these methods, considering that real seismic data are non-stationary and affected by noise, the variances of tensor coefficients are unknown and may be difficult to accurately estimate from the degraded seismic data, leading to undesirable noise suppression performance. In this paper, we propose a novel triply Laplacian scale mixture (TLSM) approach for seismic data noise suppression, which significantly improves the estimation accuracy of both the sparse tensor coefficients and hidden scalar parameters. To make the optimization problem manageable, an alternating direction method of multipliers (ADMM) algorithm is employed to solve the proposed TLSM-based seismic data noise suppression problem. Extensive experimental results on synthetic and field seismic data demonstrate that the proposed TLSM algorithm outperforms many state-of-the-art seismic data noise suppression methods in both quantitative and qualitative evaluations while providing exceptional computational efficiency.</article>","contentLength":1381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment","url":"https://arxiv.org/abs/2502.14354","date":1740114000,"author":"","guid":8178,"unread":true,"content":"<article>arXiv:2502.14354v1 Announce Type: new \nAbstract: Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at \\url{https://github.com/zyttt-coder/SIPO}.</article>","contentLength":989,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eliminating Majority Illusions","url":"https://arxiv.org/abs/2502.14353","date":1740114000,"author":"","guid":8179,"unread":true,"content":"<article>arXiv:2502.14353v1 Announce Type: new \nAbstract: An opinion illusion refers to a phenomenon in social networks where agents may witness distributions of opinions among their neighbours that do not accurately reflect the true distribution of opinions in the population as a whole. A specific case of this occurs when there are only two possible choices, such as whether to receive the COVID-19 vaccine or vote on EU membership, which is commonly referred to as a majority illusion. In this work, we study the topological properties of social networks that lead to opinion illusions and focus on minimizing the number of agents that need to be influenced to eliminate these illusions. To do so, we propose an initial, but systematic study of the algorithmic behaviour of this problem.\n  We show that the problem is NP-hard even for underlying topologies that are rather restrictive, being planar and of bounded diameter. We then look for exact algorithms that scale well as the input grows (FPT). We argue the in-existence of such algorithms even when the number of vertices that must be influenced is bounded, or when the social network is arranged in a ``path-like'' fashion (has bounded pathwidth). On the positive side, we present an FPT algorithm for networks with ``star-like'' structure (bounded vertex cover number). Finally, we construct an FPT algorithm for ``tree-like'' networks (bounded treewidth) when the number of vertices that must be influenced is bounded. This algorithm is then used to provide a PTAS for planar graphs.</article>","contentLength":1537,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SR-LLM: Rethinking the Structured Representation in Large Language Model","url":"https://arxiv.org/abs/2502.14352","date":1740114000,"author":"","guid":8180,"unread":true,"content":"<article>arXiv:2502.14352v1 Announce Type: new \nAbstract: Structured representations, exemplified by Abstract Meaning Representation (AMR), have long been pivotal in computational linguistics. However, their role remains ambiguous in the Large Language Models (LLMs) era. Initial attempts to integrate structured representation into LLMs via a zero-shot setting yielded inferior performance. We hypothesize that such a decline stems from the structure information being passed into LLMs in a code format unfamiliar to LLMs' training corpora. Consequently, we propose SR-LLM, an innovative framework with two settings to explore a superior way of integrating structured representation with LLMs from training-free and training-dependent perspectives. The former integrates structural information through natural language descriptions in LLM prompts, whereas its counterpart augments the model's inference capability through fine-tuning on linguistically described structured representations. Performance improvements were observed in widely downstream datasets, with particularly notable gains of 3.17% and 12.38% in PAWS. To the best of our knowledge, this work represents the pioneering demonstration that leveraging structural representations can substantially enhance LLMs' inference capability. We hope that our work sheds light and encourages future research to enhance the reasoning and interoperability of LLMs by structure data.</article>","contentLength":1427,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SegAnyPET: Universal Promptable Segmentation from Positron Emission Tomography Images","url":"https://arxiv.org/abs/2502.14351","date":1740114000,"author":"","guid":8181,"unread":true,"content":"<article>arXiv:2502.14351v1 Announce Type: new \nAbstract: Positron Emission Tomography (PET) imaging plays a crucial role in modern medical diagnostics by revealing the metabolic processes within a patient's body, which is essential for quantification of therapy response and monitoring treatment progress. However, the segmentation of PET images presents unique challenges due to their lower contrast and less distinct boundaries compared to other structural medical modalities. Recent developments in segmentation foundation models have shown superior versatility across diverse natural image segmentation tasks. Despite the efforts of medical adaptations, these works primarily focus on structural medical images with detailed physiological structural information and exhibit poor generalization ability when adapted to molecular PET imaging. In this paper, we collect and construct PETS-5k, the largest PET segmentation dataset to date, comprising 5,731 three-dimensional whole-body PET images and encompassing over 1.3M 2D images. Based on the established dataset, we develop SegAnyPET, a modality-specific 3D foundation model for universal promptable segmentation from PET images. To issue the challenge of discrepant annotation quality of PET images, we adopt a cross prompting confident learning (CPCL) strategy with an uncertainty-guided self-rectification process to robustly learn segmentation from high-quality labeled data and low-quality noisy labeled data. Experimental results demonstrate that SegAnyPET can correctly segment seen and unseen targets using only one or a few prompt points, outperforming state-of-the-art foundation models and task-specific fully supervised models with higher accuracy and strong generalization ability for universal segmentation. As the first foundation model for PET images, we believe that SegAnyPET will advance the applications to various downstream tasks for molecular imaging.</article>","contentLength":1922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimize Cardinality Estimation Model Pretraining by Simplifying the Training Datasets","url":"https://arxiv.org/abs/2502.14350","date":1740114000,"author":"","guid":8182,"unread":true,"content":"<article>arXiv:2502.14350v1 Announce Type: new \nAbstract: The cardinality estimation is a key aspect of query optimization research, and its performance has significantly improved with the integration of machine learning. To overcome the \"cold start\" problem or the lack of model transferability in learned cardinality estimators, some pre-training cardinality estimation models have been proposed that use learning across multiple datasets and corresponding workloads. These models typically train on a dataset created by uniformly sampling from many datasets, but this approach may not be optimal. By applying the Group Distributionally Robust Optimization (Group DRO) algorithm to training datasets, we find that some specific training datasets contribute more significantly to model performance than others. Based on this observation, we conduct extensive experiments to delve deeper into pre-training cardinality estimators. Our results show how the performance of these models can be influenced by the datasets and corresponding workloads. Finally, we introduce a simplified training dataset, which has been reduced to a fraction of the size of existing pretraining datasets. Sufficient experimental results demonstrate that the pre-trained cardinality estimator based on this simplified dataset can still achieve comparable performance to existing models in zero-shot setups.</article>","contentLength":1373,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FlowAgent: Achieving Compliance and Flexibility for Workflow Agents","url":"https://arxiv.org/abs/2502.14345","date":1740114000,"author":"","guid":8183,"unread":true,"content":"<article>arXiv:2502.14345v1 Announce Type: new \nAbstract: The integration of workflows with large language models (LLMs) enables LLM-based agents to execute predefined procedures, enhancing automation in real-world applications. Traditional rule-based methods tend to limit the inherent flexibility of LLMs, as their predefined execution paths restrict the models' action space, particularly when the unexpected, out-of-workflow (OOW) queries are encountered. Conversely, prompt-based methods allow LLMs to fully control the flow, which can lead to diminished enforcement of procedural compliance. To address these challenges, we introduce FlowAgent, a novel agent framework designed to maintain both compliance and flexibility. We propose the Procedure Description Language (PDL), which combines the adaptability of natural language with the precision of code to formulate workflows. Building on PDL, we develop a comprehensive framework that empowers LLMs to manage OOW queries effectively, while keeping the execution path under the supervision of a set of controllers. Additionally, we present a new evaluation methodology to rigorously assess an LLM agent's ability to handle OOW scenarios, going beyond routine flow compliance tested in existing benchmarks. Experiments on three datasets demonstrate that FlowAgent not only adheres to workflows but also effectively manages OOW queries, highlighting its dual strengths in compliance and flexibility. The code is available at https://github.com/Lightblues/FlowAgent.</article>","contentLength":1512,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Accurate Binary Spiking Neural Networks: Learning with Adaptive Gradient Modulation Mechanism","url":"https://arxiv.org/abs/2502.14344","date":1740114000,"author":"","guid":8184,"unread":true,"content":"<article>arXiv:2502.14344v1 Announce Type: new \nAbstract: Binary Spiking Neural Networks (BSNNs) inherit the eventdriven paradigm of SNNs, while also adopting the reduced storage burden of binarization techniques. These distinct advantages grant BSNNs lightweight and energy-efficient characteristics, rendering them ideal for deployment on resource-constrained edge devices. However, due to the binary synaptic weights and non-differentiable spike function, effectively training BSNNs remains an open question. In this paper, we conduct an in-depth analysis of the challenge for BSNN learning, namely the frequent weight sign flipping problem. To mitigate this issue, we propose an Adaptive Gradient Modulation Mechanism (AGMM), which is designed to reduce the frequency of weight sign flipping by adaptively adjusting the gradients during the learning process. The proposed AGMM can enable BSNNs to achieve faster convergence speed and higher accuracy, effectively narrowing the gap between BSNNs and their full-precision equivalents. We validate AGMM on both static and neuromorphic datasets, and results indicate that it achieves state-of-the-art results among BSNNs. This work substantially reduces storage demands and enhances SNNs' inherent energy efficiency, making them highly feasible for resource-constrained environments.</article>","contentLength":1324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective","url":"https://arxiv.org/abs/2502.14340","date":1740114000,"author":"","guid":8185,"unread":true,"content":"<article>arXiv:2502.14340v1 Announce Type: new \nAbstract: Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model. Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics. To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter. This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment. By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences. Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes. Furthermore, additional experiments on mathematical and reasoning benchmarks (MMLU, GSM8K, and MATH) confirm that our method enhances performance without compromising general capabilities. Our codebase would be available at \\url{https://github.com/LotuSrc/D2PO}.</article>","contentLength":1490,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"English Please: Evaluating Machine Translation for Multilingual Bug Reports","url":"https://arxiv.org/abs/2502.14338","date":1740114000,"author":"","guid":8186,"unread":true,"content":"<article>arXiv:2502.14338v1 Announce Type: new \nAbstract: Accurate translation of bug reports is critical for efficient collaboration in global software development. In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and ChatGPT using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag. To thoroughly assess the accuracy and effectiveness of each system, we employ multiple machine translation metrics, including BLEU, BERTScore, COMET, METEOR, and ROUGE. Our findings indicate that DeepL consistently outperforms the other systems across most automatic metrics, demonstrating strong lexical and semantic alignment. AWS Translate performs competitively, particularly in METEOR, while ChatGPT lags in key metrics. This study underscores the importance of domain adaptation for translating technical texts and offers guidance for integrating automated translation into bug-triaging workflows. Moreover, our results establish a foundation for future research to refine machine translation solutions for specialized engineering contexts. The code and dataset for this paper are available at GitHub: https://github.com/av9ash/gitbugs/tree/main/multilingual.</article>","contentLength":1331,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Information Types in Product Reviews","url":"https://arxiv.org/abs/2502.14335","date":1740114000,"author":"","guid":8187,"unread":true,"content":"<article>arXiv:2502.14335v1 Announce Type: new \nAbstract: Information in text is communicated in a way that supports a goal for its reader. Product reviews, for example, contain opinions, tips, product descriptions, and many other types of information that provide both direct insights, as well as unexpected signals for downstream applications. We devise a typology of 24 communicative goals in sentences from the product review domain, and employ a zero-shot multi-label classifier that facilitates large-scale analyses of review data. In our experiments, we find that the combination of classes in the typology forecasts helpfulness and sentiment of reviews, while supplying explanations for these decisions. In addition, our typology enables analysis of review intent, effectiveness and rhetorical structure. Characterizing the types of information in reviews unlocks many opportunities for more effective consumption of this genre.</article>","contentLength":927,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics","url":"https://arxiv.org/abs/2502.14333","date":1740114000,"author":"","guid":8188,"unread":true,"content":"<article>arXiv:2502.14333v1 Announce Type: new \nAbstract: Recent progress in large language models (LLM) found chain-of-thought prompting strategies to improve the reasoning ability of LLMs by encouraging problem solving through multiple steps. Therefore, subsequent research aimed to integrate the multi-step reasoning process into the LLM itself through process rewards as feedback and achieved improvements over prompting strategies. Due to the cost of step-level annotation, some turn to outcome rewards as feedback. Aside from these training-based approaches, training-free techniques leverage frozen LLMs or external tools for feedback at each step to enhance the reasoning process. With the abundance of work in mathematics due to its logical nature, we present a survey of strategies utilizing feedback at the step and outcome levels to enhance multi-step math reasoning for LLMs. As multi-step reasoning emerges a crucial component in scaling LLMs, we hope to establish its foundation for easier understanding and empower further research.</article>","contentLength":1039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Collaborative Jade Recognition System for Mobile Devices Based on Lightweight and Large Models","url":"https://arxiv.org/abs/2502.14332","date":1740114000,"author":"","guid":8189,"unread":true,"content":"<article>arXiv:2502.14332v1 Announce Type: new \nAbstract: With the widespread adoption and development of mobile devices, vision-based recognition applications have become a hot topic in research. Jade, as an important cultural heritage and artistic item, has significant applications in fields such as jewelry identification and cultural relic preservation. However, existing jade recognition systems still face challenges in mobile implementation, such as limited computing resources, real-time requirements, and accuracy issues. To address these challenges, this paper proposes a jade recognition system based on size model collaboration, aiming to achieve efficient and accurate jade identification using mobile devices such as smartphones.First, we design a size model based on multi-scale image processing, extracting key visual information by analyzing jade's dimensions, shapes, and surface textures. Then, a collaborative multi-model classification framework is built by combining deep learning and traditional computer vision algorithms. This framework can effectively select and adjust models based on different jade characteristics, providing high accuracy results across various environments and devices.Experimental results show that the proposed system can provide high recognition accuracy and fast processing time on mobile devices, while consuming relatively low computational resources. The system not only holds great application potential but also provides new ideas and technical support for the intelligent development of jade identification.</article>","contentLength":1556,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SolSearch: An LLM-Driven Framework for Efficient SAT-Solving Code Generation","url":"https://arxiv.org/abs/2502.14328","date":1740114000,"author":"","guid":8190,"unread":true,"content":"<article>arXiv:2502.14328v1 Announce Type: new \nAbstract: The Satisfiability (SAT) problem is a core challenge with significant applications in software engineering, including automated testing, configuration management, and program verification. This paper presents SolSearch, a novel framework that harnesses large language models (LLMs) to discover and optimize SAT-solving strategies automatically. Leveraging a curriculum-based, trial-and-error process, SolSearch enables the LLM to iteratively modify and generate SAT solver code, thereby improving solving efficiency and performance. This automated SAT-solving paradigm has the advantage of being plug-and-play, allowing integration with any SAT solver and accelerating the development or design process of new SAT solvers (new methods). Our preliminary experimental results are encouraging by demonstrating that the LLM-powered paradigm improves state-of-the-art SAT solvers on general SAT benchmarks and significantly enhances the performance of the widely used Z3 solver (11\\% on PAR-2 score). These results highlight the potential for using LLM-driven methods to advance solver adaptability and effectiveness in real-world software engineering challenges. Future research directions are discussed to further refine and validate this approach, offering a promising avenue for integrating AI with traditional software engineering tasks.</article>","contentLength":1386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChemHTS: Hierarchical Tool Stacking for Enhancing Chemical Agents","url":"https://arxiv.org/abs/2502.14327","date":1740114000,"author":"","guid":8191,"unread":true,"content":"<article>arXiv:2502.14327v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have demonstrated remarkable potential in scientific research, particularly in chemistry-related tasks such as molecular design, reaction prediction, and property estimation. While tool-augmented LLMs have been introduced to enhance reasoning and computation in these domains, existing approaches suffer from tool invocation errors and lack effective collaboration among diverse tools, limiting their overall performance. To address these challenges, we propose ChemHTS (Chemical Hierarchical Tool Stacking), a novel method that optimizes tool invocation pathways through a hierarchical stacking strategy. ChemHTS consists of two key stages: tool self-stacking warmup and multi-layer decision optimization, enabling LLMs to refine tool usage dynamically. We evaluate ChemHTS across four classical chemistry tasks and demonstrate its superiority over strong baselines, including GPT-4o, DeepSeek-R1, and chemistry-specific models, including ChemDFM. Furthermore, we define four distinct tool-stacking behaviors to enhance interpretability, providing insights into the effectiveness of tool collaboration. Our dataset and code are publicly available at \\url{https://github.com/Chang-pw/ChemHTS}.</article>","contentLength":1271,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Browser Fingerprint Detection and Anti-Tracking","url":"https://arxiv.org/abs/2502.14326","date":1740114000,"author":"","guid":8192,"unread":true,"content":"<article>arXiv:2502.14326v1 Announce Type: new \nAbstract: Digital fingerprints have brought great convenience and benefits to many online businesses. However, they pose a significant threat to the privacy and security of ordinary users. In this paper, we investigate the effectiveness of current anti-tracking methods against digital fingerprints and design a browser extension that can effectively resist digital fingerprints and record the website's collection of digital fingerprint-related information.</article>","contentLength":497,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems","url":"https://arxiv.org/abs/2502.14321","date":1740114000,"author":"","guid":8193,"unread":true,"content":"<article>arXiv:2502.14321v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have recently demonstrated remarkable capabilities in reasoning, planning, and decision-making. Building upon these strengths, researchers have begun incorporating LLMs into multi-agent systems (MAS), where agents collaborate or compete through natural language interactions to tackle tasks beyond the scope of single-agent setups. In this survey, we present a communication-centric perspective on LLM-based multi-agent systems, examining key system-level features such as architecture design and communication goals, as well as internal mechanisms like communication strategies, paradigms, objects and content. We illustrate how these communication elements interplay to enable collective intelligence and flexible collaboration. Furthermore, we discuss prominent challenges, including scalability, security, and multimodal integration, and propose directions for future work to advance research in this emerging domain. Ultimately, this survey serves as a catalyst for further innovation, fostering more robust, scalable, and intelligent multi-agent systems across diverse application domains.</article>","contentLength":1173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"It Takes Two to Tango: Serverless Workflow Serving via Bilaterally Engaged Resource Adaptation","url":"https://arxiv.org/abs/2502.14320","date":1740114000,"author":"","guid":8194,"unread":true,"content":"<article>arXiv:2502.14320v1 Announce Type: new \nAbstract: Serverless platforms typically adopt an early-binding approach for function sizing, requiring developers to specify an immutable size for each function within a workflow beforehand. Accounting for potential runtime variability, developers must size functions for worst-case scenarios to ensure service-level objectives (SLOs), resulting in significant resource inefficiency. To address this issue, we propose Janus, a novel resource adaptation framework for serverless platforms. Janus employs a late-binding approach, allowing function sizes to be dynamically adapted based on runtime conditions. The main challenge lies in the information barrier between the developer and the provider: developers lack access to runtime information, while providers lack domain knowledge about the workflow. To bridge this gap, Janus allows developers to provide hints containing rules and options for resource adaptation. Providers then follow these hints to dynamically adjust resource allocation at runtime based on real-time function execution information, ensuring compliance with SLOs. We implement Janus and conduct extensive experiments with real-world serverless workflows. Our results demonstrate that Janus enhances resource efficiency by up to 34.7% compared to the state-of-the-art.</article>","contentLength":1330,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Line Goes Up? Inherent Limitations of Benchmarks for Evaluating Large Language Models","url":"https://arxiv.org/abs/2502.14318","date":1740114000,"author":"","guid":8195,"unread":true,"content":"<article>arXiv:2502.14318v1 Announce Type: new \nAbstract: Large language models (LLMs) regularly demonstrate new and impressive performance on a wide range of language, knowledge, and reasoning benchmarks. Such rapid progress has led many commentators to argue that LLM general cognitive capabilities have likewise rapidly improved, with the implication that such models are becoming progressively more capable on various real-world tasks. Here I summarise theoretical and empirical considerations to challenge this narrative. I argue that inherent limitations with the benchmarking paradigm, along with specific limitations of existing benchmarks, render benchmark performance highly unsuitable as a metric for generalisable competence over cognitive tasks. I also contend that alternative methods for assessing LLM capabilities, including adversarial stimuli and interpretability techniques, have shown that LLMs do not have robust competence in many language and reasoning tasks, and often fail to learn representations which facilitate generalisable inferences. I conclude that benchmark performance should not be used as a reliable indicator of general LLM cognitive capabilities.</article>","contentLength":1176,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ParallelComp: Parallel Long-Context Compressor for Length Extrapolation","url":"https://arxiv.org/abs/2502.14317","date":1740114000,"author":"","guid":8196,"unread":true,"content":"<article>arXiv:2502.14317v1 Announce Type: new \nAbstract: Efficiently handling long contexts is crucial for large language models (LLMs). While rotary position embeddings (RoPEs) enhance length generalization, effective length extrapolation remains challenging and often requires costly fine-tuning. In contrast, recent training-free approaches suffer from the attention sink phenomenon, leading to severe performance degradation. In this paper, we introduce ParallelComp, a novel training-free method for long-context extrapolation that extends LLMs' context length from 4K to 128K while maintaining high throughput and preserving perplexity, and integrates seamlessly with Flash Attention. Our analysis offers new insights into attention biases in parallel attention mechanisms and provides practical solutions to tackle these challenges. To mitigate the attention sink issue, we propose an attention calibration strategy that reduces biases, ensuring more stable long-range attention. Additionally, we introduce a chunk eviction strategy to efficiently manage ultra-long contexts on a single A100 80GB GPU. To further enhance efficiency, we propose a parallel KV cache eviction technique, which improves chunk throughput by 1.76x, thereby achieving a 23.50x acceleration in the prefilling stage with negligible performance loss due to attention calibration. Furthermore, ParallelComp achieves 91.17% of GPT-4's performance on long-context tasks using an 8B model trained on 8K-length context, outperforming powerful closed-source models such as Claude-2 and Kimi-Chat.</article>","contentLength":1562,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Textured 3D Regenerative Morphing with 3D Diffusion Prior","url":"https://arxiv.org/abs/2502.14316","date":1740114000,"author":"","guid":8197,"unread":true,"content":"<article>arXiv:2502.14316v1 Announce Type: new \nAbstract: Textured 3D morphing creates smooth and plausible interpolation sequences between two 3D objects, focusing on transitions in both shape and texture. This is important for creative applications like visual effects in filmmaking. Previous methods rely on establishing point-to-point correspondences and determining smooth deformation trajectories, which inherently restrict them to shape-only morphing on untextured, topologically aligned datasets. This restriction leads to labor-intensive preprocessing and poor generalization. To overcome these challenges, we propose a method for 3D regenerative morphing using a 3D diffusion prior. Unlike previous methods that depend on explicit correspondences and deformations, our method eliminates the additional need for obtaining correspondence and uses the 3D diffusion prior to generate morphing. Specifically, we introduce a 3D diffusion model and interpolate the source and target information at three levels: initial noise, model parameters, and condition features. We then explore an Attention Fusion strategy to generate more smooth morphing sequences. To further improve the plausibility of semantic interpolation and the generated 3D surfaces, we propose two strategies: (a) Token Reordering, where we match approximate tokens based on semantic analysis to guide implicit correspondences in the denoising process of the diffusion model, and (b) Low-Frequency Enhancement, where we enhance low-frequency signals in the tokens to improve the quality of generated surfaces. Experimental results show that our method achieves superior smoothness and plausibility in 3D morphing across diverse cross-category object pairs, offering a novel regenerative method for 3D morphing with textured representations.</article>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension","url":"https://arxiv.org/abs/2502.14315","date":1740114000,"author":"","guid":8198,"unread":true,"content":"<article>arXiv:2502.14315v1 Announce Type: new \nAbstract: Despite the impressive performance of multilingual large language models (mLLMs) in various natural language processing tasks, their ability to understand procedural texts, particularly those with culture-specific content, remains largely unexplored. Texts describing cultural procedures, including rituals, traditional craftsmanship, and social etiquette, require an inherent understanding of cultural context, presenting a significant challenge for mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate mLLMs' ability to process and reason about culturally diverse procedural texts across multiple languages using various methodologies to assess their performance. Our findings indicate that (1) mLLMs face difficulties with culturally contextualized procedural texts, showing notable performance declines in low-resource languages, (2) model performance fluctuates across cultural domains, with some areas presenting greater difficulties, and (3) language models exhibit better performance on multiple-choice tasks within conversational frameworks compared to direct questioning. These results underscore the current limitations of mLLMs in handling culturally nuanced procedural texts and highlight the need for culturally aware benchmarks like CAPTex to enhance their adaptability and comprehension across diverse linguistic and cultural landscapes.</article>","contentLength":1424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ODVerse33: Is the New YOLO Version Always Better? A Multi Domain benchmark from YOLO v5 to v11","url":"https://arxiv.org/abs/2502.14314","date":1740114000,"author":"","guid":8199,"unread":true,"content":"<article>arXiv:2502.14314v1 Announce Type: new \nAbstract: You Look Only Once (YOLO) models have been widely used for building real-time object detectors across various domains. With the increasing frequency of new YOLO versions being released, key questions arise. Are the newer versions always better than their previous versions? What are the core innovations in each YOLO version and how do these changes translate into real-world performance gains? In this paper, we summarize the key innovations from YOLOv1 to YOLOv11, introduce a comprehensive benchmark called ODverse33, which includes 33 datasets spanning 11 diverse domains (Autonomous driving, Agricultural, Underwater, Medical, Videogame, Industrial, Aerial, Wildlife, Retail, Microscopic, and Security), and explore the practical impact of model improvements in real-world, multi-domain applications through extensive experimental results. We hope this study can provide some guidance to the extensive users of object detection models and give some references for future real-time object detector development.</article>","contentLength":1063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Impact and Feasibility of Self-Confidence Shaping for AI-Assisted Decision-Making","url":"https://arxiv.org/abs/2502.14311","date":1740114000,"author":"","guid":8200,"unread":true,"content":"<article>arXiv:2502.14311v1 Announce Type: new \nAbstract: In AI-assisted decision-making, it is crucial but challenging for humans to appropriately rely on AI, especially in high-stakes domains such as finance and healthcare. This paper addresses this problem from a human-centered perspective by presenting an intervention for self-confidence shaping, designed to calibrate self-confidence at a targeted level. We first demonstrate the impact of self-confidence shaping by quantifying the upper-bound improvement in human-AI team performance. Our behavioral experiments with 121 participants show that self-confidence shaping can improve human-AI team performance by nearly 50% by mitigating both over- and under-reliance on AI. We then introduce a self-confidence prediction task to identify when our intervention is needed. Our results show that simple machine-learning models achieve 67% accuracy in predicting self-confidence. We further illustrate the feasibility of such interventions. The observed relationship between sentiment and self-confidence suggests that modifying sentiment could be a viable strategy for shaping self-confidence. Finally, we outline future research directions to support the deployment of self-confidence shaping in a real-world scenario for effective human-AI collaboration.</article>","contentLength":1300,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On Theoretical Limits of Learning with Label Differential Privacy","url":"https://arxiv.org/abs/2502.14309","date":1740114000,"author":"","guid":8201,"unread":true,"content":"<article>arXiv:2502.14309v1 Announce Type: new \nAbstract: Label differential privacy (DP) is designed for learning problems involving private labels and public features. While various methods have been proposed for learning under label DP, the theoretical limits remain largely unexplored. In this paper, we investigate the fundamental limits of learning with label DP in both local and central models for both classification and regression tasks, characterized by minimax convergence rates. We establish lower bounds by converting each task into a multiple hypothesis testing problem and bounding the test error. Additionally, we develop algorithms that yield matching upper bounds. Our results demonstrate that under label local DP (LDP), the risk has a significantly faster convergence rate than that under full LDP, i.e. protecting both features and labels, indicating the advantages of relaxing the DP definition to focus solely on labels. In contrast, under the label central DP (CDP), the risk is only reduced by a constant factor compared to full DP, indicating that the relaxation of CDP only has limited benefits on the performance.</article>","contentLength":1133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"{\\mu}RL: Discovering Transient Execution Vulnerabilities Using Reinforcement Learning","url":"https://arxiv.org/abs/2502.14307","date":1740114000,"author":"","guid":8202,"unread":true,"content":"<article>arXiv:2502.14307v1 Announce Type: new \nAbstract: We propose using reinforcement learning to address the challenges of discovering microarchitectural vulnerabilities, such as Spectre and Meltdown, which exploit subtle interactions in modern processors. Traditional methods like random fuzzing fail to efficiently explore the vast instruction space and often miss vulnerabilities that manifest under specific conditions. To overcome this, we introduce an intelligent, feedback-driven approach using RL. Our RL agents interact with the processor, learning from real-time feedback to prioritize instruction sequences more likely to reveal vulnerabilities, significantly improving the efficiency of the discovery process.\n  We also demonstrate that RL systems adapt effectively to various microarchitectures, providing a scalable solution across processor generations. By automating the exploration process, we reduce the need for human intervention, enabling continuous learning that uncovers hidden vulnerabilities. Additionally, our approach detects subtle signals, such as timing anomalies or unusual cache behavior, that may indicate microarchitectural weaknesses. This proposal advances hardware security testing by introducing a more efficient, adaptive, and systematic framework for protecting modern processors.\n  When unleashed on Intel Skylake-X and Raptor Lake microarchitectures, our RL agent was indeed able to generate instruction sequences that cause significant observable byte leakages through transient execution without generating any $\\mu$code assists, faults or interrupts. The newly identified leaky sequences stem from a variety of Intel instructions, e.g. including SERIALIZE, VERR/VERW, CLMUL, MMX-x87 transitions, LSL+RDSCP and LAR. These initial results give credence to the proposed approach.</article>","contentLength":1816,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient AI in Practice: Training and Deployment of Efficient LLMs for Industry Applications","url":"https://arxiv.org/abs/2502.14305","date":1740114000,"author":"","guid":8203,"unread":true,"content":"<article>arXiv:2502.14305v1 Announce Type: new \nAbstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of industrial applications, from search and recommendations to generative tasks. Although scaling laws indicate that larger models generally yield better generalization and performance, their substantial computational requirements often render them impractical for many real-world scenarios at scale. In this paper, we present methods and insights for training small language models (SLMs) that deliver high performance and efficiency in deployment. We focus on two key techniques: (1) knowledge distillation and (2) model compression via quantization and pruning. These approaches enable SLMs to retain much of the quality of their larger counterparts while significantly reducing training, serving costs, and latency. We detail the impact of these techniques on a variety of use cases at a large professional social network platform and share deployment lessons - including hardware optimization strategies that enhance speed and throughput for both predictive and reasoning-based applications.</article>","contentLength":1135,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models","url":"https://arxiv.org/abs/2502.14302","date":1740114000,"author":"","guid":8204,"unread":true,"content":"<article>arXiv:2502.14302v1 Announce Type: new \nAbstract: Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting \"hard\" category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a \"not sure\" category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.</article>","contentLength":1319,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SEA-HELM: Southeast Asian Holistic Evaluation of Language Models","url":"https://arxiv.org/abs/2502.14301","date":1740114000,"author":"","guid":8205,"unread":true,"content":"<article>arXiv:2502.14301v1 Announce Type: new \nAbstract: With the rapid emergence of novel capabilities in Large Language Models (LLMs), the need for rigorous multilingual and multicultural benchmarks that are integrated has become more pronounced. Though existing LLM benchmarks are capable of evaluating specific capabilities of LLMs in English as well as in various mid- to low-resource languages, including those in the Southeast Asian (SEA) region, a comprehensive and authentic evaluation suite for the SEA languages has not been developed thus far. Here, we present SEA-HELM, a holistic linguistic and cultural LLM evaluation suite that emphasizes SEA languages, comprising five core pillars: (1) NLP Classics, (2) LLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM currently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also introduce the SEA-HELM leaderboard, which allows users to understand models' multilingual and multicultural performance in a systematic and user-friendly manner.</article>","contentLength":1027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generalization Certificates for Adversarially Robust Bayesian Linear Regression","url":"https://arxiv.org/abs/2502.14298","date":1740114000,"author":"","guid":8206,"unread":true,"content":"<article>arXiv:2502.14298v1 Announce Type: new \nAbstract: Adversarial robustness of machine learning models is critical to ensuring reliable performance under data perturbations. Recent progress has been on point estimators, and this paper considers distributional predictors. First, using the link between exponential families and Bregman divergences, we formulate an adversarial Bregman divergence loss as an adversarial negative log-likelihood. Using the geometric properties of Bregman divergences, we compute the adversarial perturbation for such models in closed-form. Second, under such losses, we introduce \\emph{adversarially robust posteriors}, by exploiting the optimization-centric view of generalized Bayesian inference. Third, we derive the \\emph{first} rigorous generalization certificates in the context of an adversarial extension of Bayesian linear regression by leveraging the PAC-Bayesian framework. Finally, experiments on real and synthetic datasets demonstrate the superior robustness of the derived adversarially robust posterior over Bayes posterior, and also validate our theoretical guarantees.</article>","contentLength":1112,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Evaluation of Sakana's AI Scientist for Autonomous Research: Wishful Thinking or an Emerging Reality Towards 'Artificial General Research Intelligence' (AGRI)?","url":"https://arxiv.org/abs/2502.14297","date":1740114000,"author":"","guid":8207,"unread":true,"content":"<article>arXiv:2502.14297v1 Announce Type: new \nAbstract: A major step toward Artificial General Intelligence (AGI) and Super Intelligence is AI's ability to autonomously conduct research - what we term Artificial General Research Intelligence (AGRI). If machines could generate hypotheses, conduct experiments, and write research papers without human intervention, it would transform science. Recently, Sakana.ai introduced the AI Scientist, a system claiming to automate the research lifecycle, generating both excitement and skepticism.\n  We evaluated the AI Scientist and found it a milestone in AI-driven research. While it streamlines some aspects, it falls short of expectations. Literature reviews are weak, nearly half the experiments failed, and manuscripts sometimes contain hallucinated results. Most notably, users must provide an experimental pipeline, limiting the AI Scientist's autonomy in research design and execution.\n  Despite its limitations, the AI Scientist advances research automation. Many reviewers or instructors who assess work superficially may not recognize its output as AI-generated. The system produces research papers with minimal human effort and low cost. Our analysis suggests a paper costs a few USD with a few hours of human involvement, making it significantly faster than human researchers. Compared to AI capabilities from a few years ago, this marks progress toward AGRI.\n  The rise of AI-driven research systems requires urgent discussion within Information Retrieval (IR) and broader scientific communities. Enhancing literature retrieval, citation validation, and evaluation benchmarks could improve AI-generated research reliability. We propose concrete steps, including AGRI-specific benchmarks, refined peer review, and standardized attribution frameworks. Whether AGRI becomes a stepping stone to AGI depends on how the academic and AI communities shape its development.</article>","contentLength":1913,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective","url":"https://arxiv.org/abs/2502.14296","date":1740114000,"author":"","guid":8208,"unread":true,"content":"<article>arXiv:2502.14296v1 Announce Type: new \nAbstract: Generative Foundation Models (GenFMs) have emerged as transformative tools. However, their widespread adoption raises critical concerns regarding trustworthiness across dimensions. This paper presents a comprehensive framework to address these challenges through three key contributions. First, we systematically review global AI governance laws and policies from governments and regulatory bodies, as well as industry practices and standards. Based on this analysis, we propose a set of guiding principles for GenFMs, developed through extensive multidisciplinary collaboration that integrates technical, ethical, legal, and societal perspectives. Second, we introduce TrustGen, the first dynamic benchmarking platform designed to evaluate trustworthiness across multiple dimensions and model types, including text-to-image, large language, and vision-language models. TrustGen leverages modular components--metadata curation, test case generation, and contextual variation--to enable adaptive and iterative assessments, overcoming the limitations of static evaluation methods. Using TrustGen, we reveal significant progress in trustworthiness while identifying persistent challenges. Finally, we provide an in-depth discussion of the challenges and future directions for trustworthy GenFMs, which reveals the complex, evolving nature of trustworthiness, highlighting the nuanced trade-offs between utility and trustworthiness, and consideration for various downstream applications, identifying persistent challenges and providing a strategic roadmap for future research. This work establishes a holistic framework for advancing trustworthiness in GenAI, paving the way for safer and more responsible integration of GenFMs into critical applications. To facilitate advancement in the community, we release the toolkit for dynamic evaluation.</article>","contentLength":1891,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DAG: Deep Adaptive and Generative $K$-Free Community Detection on Attributed Graphs","url":"https://arxiv.org/abs/2502.14294","date":1740114000,"author":"","guid":8209,"unread":true,"content":"<article>arXiv:2502.14294v1 Announce Type: new \nAbstract: Community detection on attributed graphs with rich semantic and topological information offers great potential for real-world network analysis, especially user matching in online games. Graph Neural Networks (GNNs) have recently enabled Deep Graph Clustering (DGC) methods to learn cluster assignments from semantic and topological information. However, their success depends on the prior knowledge related to the number of communities $K$, which is unrealistic due to the high costs and privacy issues of acquisition.In this paper, we investigate the community detection problem without prior $K$, referred to as $K$-Free Community Detection problem. To address this problem, we propose a novel Deep Adaptive and Generative model~(DAG) for community detection without specifying the prior $K$. DAG consists of three key components, \\textit{i.e.,} a node representation learning module with masked attribute reconstruction, a community affiliation readout module, and a community number search module with group sparsity. These components enable DAG to convert the process of non-differentiable grid search for the community number, \\textit{i.e.,} a discrete hyperparameter in existing DGC methods, into a differentiable learning process. In such a way, DAG can simultaneously perform community detection and community number search end-to-end. To alleviate the cost of acquiring community labels in real-world applications, we design a new metric, EDGE, to evaluate community detection methods even when the labels are not feasible. Extensive offline experiments on five public datasets and a real-world online mobile game dataset demonstrate the superiority of our DAG over the existing state-of-the-art (SOTA) methods. DAG has a relative increase of 7.35\\% in teams in a Tencent online game compared with the best competitor.</article>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Graph Anomaly Detection via Adaptive Test-time Representation Learning across Out-of-Distribution Domains","url":"https://arxiv.org/abs/2502.14293","date":1740114000,"author":"","guid":8210,"unread":true,"content":"<article>arXiv:2502.14293v1 Announce Type: new \nAbstract: Graph Anomaly Detection (GAD) has demonstrated great effectiveness in identifying unusual patterns within graph-structured data. However, while labeled anomalies are often scarce in emerging applications, existing supervised GAD approaches are either ineffective or not applicable when moved across graph domains due to distribution shifts and heterogeneous feature spaces. To address these challenges, we present AdaGraph-T3, a novel test-time training framework for cross-domain GAD. AdaGraph-T3 combines supervised and self-supervised learning during training while adapting to a new domain during test time using only self-supervised learning by leveraging a homophily-based affinity score that captures domain-invariant properties of anomalies. Our framework introduces four key innovations to cross-domain GAD: an effective self-supervision scheme, an attention-based mechanism that dynamically learns edge importance weights during message passing, domain-specific encoders for handling heterogeneous features, and class-aware regularization to address imbalance. Experiments across multiple cross-domain settings demonstrate that AdaGraph-T3 significantly outperforms existing approaches, achieving average improvements of over 6.6% in AUROC and 7.9% in AUPRC compared to the best competing model.</article>","contentLength":1354,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Note on Efficient Privacy-Preserving Similarity Search for Encrypted Vectors","url":"https://arxiv.org/abs/2502.14291","date":1740114000,"author":"","guid":8211,"unread":true,"content":"<article>arXiv:2502.14291v1 Announce Type: new \nAbstract: Traditional approaches to vector similarity search over encrypted data rely on fully homomorphic encryption (FHE) to enable computation without decryption. However, the substantial computational overhead of FHE makes it impractical for large-scale real-time applications. This work explores a more efficient alternative: using additively homomorphic encryption (AHE) for privacy-preserving similarity search. We consider scenarios where either the query vector or the database vectors remain encrypted, a setting that frequently arises in applications such as confidential recommender systems and secure federated learning. While AHE only supports addition and scalar multiplication, we show that it is sufficient to compute inner product similarity--one of the most widely used similarity measures in vector retrieval. Compared to FHE-based solutions, our approach significantly reduces computational overhead by avoiding ciphertext-ciphertext multiplications and bootstrapping, while still preserving correctness and privacy. We present an efficient algorithm for encrypted similarity search under AHE and analyze its error growth and security implications. Our method provides a scalable and practical solution for privacy-preserving vector search in real-world machine learning applications.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Drift: Decoding-time Personalized Alignments with Implicit User Preferences","url":"https://arxiv.org/abs/2502.14289","date":1740114000,"author":"","guid":8212,"unread":true,"content":"<article>arXiv:2502.14289v1 Announce Type: new \nAbstract: Personalized alignments for individual users have been a long-standing goal in large language models (LLMs). We introduce Drift, a novel framework that personalizes LLMs at decoding time with implicit user preferences. Traditional Reinforcement Learning from Human Feedback (RLHF) requires thousands of annotated examples and expensive gradient updates. In contrast, Drift personalizes LLMs in a training-free manner, using only a few dozen examples to steer a frozen model through efficient preference modeling. Our approach models user preferences as a composition of predefined, interpretable attributes and aligns them at decoding time to enable personalized generation. Experiments on both a synthetic persona dataset (Perspective) and a real human-annotated dataset (PRISM) demonstrate that Drift significantly outperforms RLHF baselines while using only 50-100 examples. Our results and analysis show that Drift is both computationally efficient and interpretable.</article>","contentLength":1020,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are your apps accessible? A GCN-based accessibility checker for low vision users","url":"https://arxiv.org/abs/2502.14288","date":1740114000,"author":"","guid":8213,"unread":true,"content":"<article>arXiv:2502.14288v1 Announce Type: new \nAbstract: Context: Accessibility issues (e.g., small size and narrow interval) in mobile applications (apps) lead to obstacles for billions of low vision users in interacting with Graphical User Interfaces (GUIs). Although GUI accessibility scanning tools exist, most of them perform rule-based check relying on complex GUI hierarchies. This might make them detect invisible redundant information, cannot handle small deviations, omit similar components, and is hard to extend. Objective: In this paper, we propose a novel approach, named ALVIN (Accessibility Checker for Low Vision), which represents the GUI as a graph and adopts the Graph Convolutional Neural Networks (GCN) to label inaccessible components. Method: ALVIN removes invisible views to prevent detecting redundancy and uses annotations from low vision users to handle small deviations. Also, the GCN model could consider the relations between GUI components, connecting similar components and reducing the possibility of omission. ALVIN only requires users to annotate the relevant dataset when detecting new kinds of issues. Results: Our experiments on 48 apps demonstrate the effectiveness of ALVIN, with precision of 83.5%, recall of 78.9%, and F1-score of 81.2%, outperforming baseline methods. In RQ2, the usefulness is verified through 20 issues submitted to open-source apps. The RQ3 also illustrates the GCN model is better than other models. Conclusion: To summarize, our proposed approach can effectively detect accessibility issues in GUIs for low vision users, thereby guiding developers in fixing them efficiently.</article>","contentLength":1633,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach","url":"https://arxiv.org/abs/2502.14285","date":1740114000,"author":"","guid":8214,"unread":true,"content":"<article>arXiv:2502.14285v1 Announce Type: new \nAbstract: Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.</article>","contentLength":1611,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An efficient and accurate semi-implicit time integration scheme for dynamics in nearly- and fully-incompressible hyperelastic solids","url":"https://arxiv.org/abs/2502.14284","date":1740114000,"author":"","guid":8215,"unread":true,"content":"<article>arXiv:2502.14284v1 Announce Type: new \nAbstract: The choice of numerical integrator in approximating solutions to dynamic partial differential equations depends on the smallest time-scale of the problem at hand. Large-scale deformations in elastic solids contain both shear waves and bulk waves, the latter of which can travel infinitely fast in incompressible materials. Explicit schemes, which are favored for their efficiency in resolving low-speed dynamics, are bound by time step size restrictions that inversely scale with the fastest wave speed. Implicit schemes can enable larger time step sizes regardless of the wave speeds present, though they are much more computationally expensive. Semi-implicit methods, which are more stable than explicit methods and more efficient than implicit methods, are emerging in the literature, though their applicability to nonlinear elasticity is not extensively studied. In this research, we develop and investigate the functionality of two time integration schemes for the resolution of large-scale dynamics in nearly- and fully-incompressible hyperelastic solids: a Modified Semi-implicit Backward Differentiation Formula integrator (MSBDF2) and a forward Euler / Semi-implicit Backward Differentiation Formula Runge-Kutta integrator (FEBDF2). We prove and empirically verify second order accuracy for both schemes. The stability properties of both methods are derived and numerically verified. We find FEBDF2 has a maximum time step size that inversely scales with the shear wave speed and is unaffected by the bulk wave speed -- the desired stability property of a semi-implicit scheme. Finally, we empirically determine that semi-implicit schemes struggle to preserve volume globally when using nonlinear incompressibility conditions, even under temporal and spatial refinement.</article>","contentLength":1828,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC","url":"https://arxiv.org/abs/2502.14282","date":1740114000,"author":"","guid":8216,"unread":true,"content":"<article>arXiv:2502.14282v1 Announce Type: new \nAbstract: In the field of MLLM-based GUI agents, compared to smartphones, the PC scenario not only features a more complex interactive environment, but also involves more intricate intra- and inter-app workflows. To address these issues, we propose a hierarchical agent framework named PC-Agent. Specifically, from the perception perspective, we devise an Active Perception Module (APM) to overcome the inadequate abilities of current MLLMs in perceiving screenshot content. From the decision-making perspective, to handle complex user instructions and interdependent subtasks more effectively, we propose a hierarchical multi-agent collaboration architecture that decomposes decision-making processes into Instruction-Subtask-Action levels. Within this architecture, three agents (i.e., Manager, Progress and Decision) are set up for instruction decomposition, progress tracking and step-by-step decision-making respectively. Additionally, a Reflection agent is adopted to enable timely bottom-up error feedback and adjustment. We also introduce a new benchmark PC-Eval with 25 real-world complex instructions. Empirical results on PC-Eval show that our PC-Agent achieves a 32% absolute improvement of task success rate over previous state-of-the-art methods. The code will be publicly available.</article>","contentLength":1336,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Correcting Noisy Multilabel Predictions: Modeling Label Noise through Latent Space Shifts","url":"https://arxiv.org/abs/2502.14281","date":1740114000,"author":"","guid":8217,"unread":true,"content":"<article>arXiv:2502.14281v1 Announce Type: new \nAbstract: Noise in data appears to be inevitable in most real-world machine learning applications and would cause severe overfitting problems. Not only can data features contain noise, but labels are also prone to be noisy due to human input. In this paper, rather than noisy label learning in multiclass classifications, we instead focus on the less explored area of noisy label learning for multilabel classifications. Specifically, we investigate the post-correction of predictions generated from classifiers learned with noisy labels. The reasons are two-fold. Firstly, this approach can directly work with the trained models to save computational resources. Secondly, it could be applied on top of other noisy label correction techniques to achieve further improvements. To handle this problem, we appeal to deep generative approaches that are possible for uncertainty estimation. Our model posits that label noise arises from a stochastic shift in the latent variable, providing a more robust and beneficial means for noisy learning. We develop both unsupervised and semi-supervised learning methods for our model. The extensive empirical study presents solid evidence to that our approach is able to consistently improve the independent models and performs better than a number of existing methods across various noisy label settings. Moreover, a comprehensive empirical analysis of the proposed method is carried out to validate its robustness, including sensitivity analysis and an ablation study, among other elements.</article>","contentLength":1567,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EpMAN: Episodic Memory AttentioN for Generalizing to Longer Contexts","url":"https://arxiv.org/abs/2502.14280","date":1740114000,"author":"","guid":8218,"unread":true,"content":"<article>arXiv:2502.14280v1 Announce Type: new \nAbstract: Recent advances in Large Language Models (LLMs) have yielded impressive successes on many language tasks. However, efficient processing of long contexts using LLMs remains a significant challenge. We introduce \\textbf{EpMAN} -- a method for processing long contexts in an \\textit{episodic memory} module while \\textit{holistically attending to} semantically relevant context chunks. The output of \\textit{episodic attention} is then used to reweigh the decoder's self-attention to the stored KV cache of the context during training and generation. When an LLM decoder is trained using \\textbf{EpMAN}, its performance on multiple challenging single-hop long-context recall and question-answering benchmarks is found to be stronger and more robust across the range from 16k to 256k tokens than baseline decoders trained with self-attention, and popular retrieval-augmented generation frameworks.</article>","contentLength":942,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OrchardDepth: Precise Metric Depth Estimation of Orchard Scene from Monocular Camera Images","url":"https://arxiv.org/abs/2502.14279","date":1740114000,"author":"","guid":8219,"unread":true,"content":"<article>arXiv:2502.14279v1 Announce Type: new \nAbstract: Monocular depth estimation is a rudimentary task in robotic perception. Recently, with the development of more accurate and robust neural network models and different types of datasets, monocular depth estimation has significantly improved performance and efficiency. However, most of the research in this area focuses on very concentrated domains. In particular, most of the benchmarks in outdoor scenarios belong to urban environments for the improvement of autonomous driving devices, and these benchmarks have a massive disparity with the orchard/vineyard environment, which is hardly helpful for research in the primary industry. Therefore, we propose OrchardDepth, which fills the gap in the estimation of the metric depth of the monocular camera in the orchard/vineyard environment. In addition, we present a new retraining method to improve the training result by monitoring the consistent regularization between dense depth maps and sparse points. Our method improves the RMSE of depth estimation in the orchard environment from 1.5337 to 0.6738, proving our method's validation.</article>","contentLength":1137,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"STeCa: Step-level Trajectory Calibration for LLM Agent Learning","url":"https://arxiv.org/abs/2502.14276","date":1740114000,"author":"","guid":8220,"unread":true,"content":"<article>arXiv:2502.14276v1 Announce Type: new \nAbstract: Large language model (LLM)-based agents have shown promise in tackling complex tasks by interacting dynamically with the environment. Existing work primarily focuses on behavior cloning from expert demonstrations and preference learning through exploratory trajectory sampling. However, these methods often struggle in long-horizon tasks, where suboptimal actions accumulate step by step, causing agents to deviate from correct task trajectories. To address this, we highlight the importance of timely calibration and the need to automatically construct calibration trajectories for training agents. We propose Step-Level Trajectory Calibration (STeCa), a novel framework for LLM agent learning. Specifically, STeCa identifies suboptimal actions through a step-level reward comparison during exploration. It constructs calibrated trajectories using LLM-driven reflection, enabling agents to learn from improved decision-making processes. These calibrated trajectories, together with successful trajectory data, are utilized for reinforced training. Extensive experiments demonstrate that STeCa significantly outperforms existing methods. Further analysis highlights that step-level calibration enables agents to complete tasks with greater robustness. Our code and data are available at https://github.com/WangHanLinHenry/STeCa.</article>","contentLength":1377,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Fact or Guesswork? Evaluating Large Language Model's Medical Knowledge with Structured One-Hop Judgment","url":"https://arxiv.org/abs/2502.14275","date":1740114000,"author":"","guid":8221,"unread":true,"content":"<article>arXiv:2502.14275v1 Announce Type: new \nAbstract: Large language models (LLMs) have been widely adopted in various downstream task domains. However, their ability to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMs' inherent medical knowledge from their reasoning capabilities. Given the high-stakes nature of medical applications, where incorrect information can have critical consequences, it is essential to evaluate how well LLMs encode, retain, and recall fundamental medical facts.\n  To bridge this gap, we introduce the Medical Knowledge Judgment, a dataset specifically designed to measure LLMs' one-hop factual medical knowledge. MKJ is constructed from the Unified Medical Language System (UMLS), a large-scale repository of standardized biomedical vocabularies and knowledge graphs. We frame knowledge assessment as a binary judgment task, requiring LLMs to verify the correctness of medical statements extracted from reliable and structured knowledge sources.\n  Our experiments reveal that LLMs struggle with factual medical knowledge retention, exhibiting significant performance variance across different semantic categories, particularly for rare medical conditions. Furthermore, LLMs show poor calibration, often being overconfident in incorrect answers. To mitigate these issues, we explore retrieval-augmented generation, demonstrating its effectiveness in improving factual accuracy and reducing uncertainty in medical decision-making.</article>","contentLength":1601,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-EvRep: Learning an LLM-Compatible Event Representation Using a Self-Supervised Framework","url":"https://arxiv.org/abs/2502.14273","date":1740114000,"author":"","guid":8222,"unread":true,"content":"<article>arXiv:2502.14273v1 Announce Type: new \nAbstract: Recent advancements in event-based recognition have demonstrated significant promise, yet most existing approaches rely on extensive training, limiting their adaptability for efficient processing of event-driven visual content. Meanwhile, large language models (LLMs) have exhibited remarkable zero-shot capabilities across diverse domains, but their application to event-based visual recognition remains largely unexplored. To bridge this gap, we propose \\textbf{LLM-EvGen}, an event representation generator that produces LLM-compatible event representations \\textbf{LLM-EvRep}, thereby enhancing the performance of LLMs on event recognition tasks. The generator is trained using a self-supervised framework, aligning the generated representations with semantic consistency and structural fidelity. Comprehensive experiments were conducted on three datasets: N-ImageNet, N-Caltech101, and N-MNIST. The results demonstrate that our method, \\textbf{LLM-EvRep}, outperforms the event-to-video method, E2VID, by 15.93\\%, 0.82\\%, and 50.21\\%, respectively, in recognition tasks when evaluated using GPT-4o.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Capturing Nuanced Preferences: Preference-Aligned Distillation for Small Language Models","url":"https://arxiv.org/abs/2502.14272","date":1740114000,"author":"","guid":8223,"unread":true,"content":"<article>arXiv:2502.14272v1 Announce Type: new \nAbstract: Aligning small language models (SLMs) with human values typically involves distilling preference knowledge from large language models (LLMs). However, existing distillation methods model preference knowledge in teacher LLMs by comparing pairwise responses, overlooking the extent of difference between responses. This limitation hinders student SLMs from capturing the nuanced preferences for multiple responses. In this paper, we propose a Preference-Aligned Distillation (PAD) framework, which models teacher's preference knowledge as a probability distribution over all potential preferences, thereby providing more nuanced supervisory signals. Our insight in developing PAD is rooted in the demonstration that language models can serve as reward functions, reflecting their intrinsic preferences. Based on this, PAD comprises three key steps: (1) sampling diverse responses using high-temperature; (2) computing rewards for both teacher and student to construct their intrinsic preference; and (3) training the student's intrinsic preference distribution to align with the teacher's. Experiments on four mainstream alignment benchmarks demonstrate that PAD consistently and significantly outperforms existing approaches, achieving over 20\\% improvement on AlpacaEval 2 and Arena-Hard, indicating superior alignment with human preferences. Notably, on MT-Bench, using the \\textsc{Gemma} model family, the student trained by PAD surpasses its teacher, further validating the effectiveness of our PAD.</article>","contentLength":1551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PaperHelper: Knowledge-Based LLM QA Paper Reading Assistant","url":"https://arxiv.org/abs/2502.14271","date":1740114000,"author":"","guid":8224,"unread":true,"content":"<article>arXiv:2502.14271v1 Announce Type: new \nAbstract: In the paper, we introduce a paper reading assistant, PaperHelper, a potent tool designed to enhance the capabilities of researchers in efficiently browsing and understanding scientific literature. Utilizing the Retrieval-Augmented Generation (RAG) framework, PaperHelper effectively minimizes hallucinations commonly encountered in large language models (LLMs), optimizing the extraction of accurate, high-quality knowledge. The implementation of advanced technologies such as RAFT and RAG Fusion significantly boosts the performance, accuracy, and reliability of the LLMs-based literature review process. Additionally, PaperHelper features a user-friendly interface that facilitates the batch downloading of documents and uses the Mermaid format to illustrate structural relationships between documents. Experimental results demonstrate that PaperHelper, based on a fine-tuned GPT-4 API, achieves an F1 Score of 60.04, with a latency of only 5.8 seconds, outperforming the basic RAG model by 7\\% in F1 Score.</article>","contentLength":1059,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Predicting Fetal Birthweight from High Dimensional Data using Advanced Machine Learning","url":"https://arxiv.org/abs/2502.14270","date":1740114000,"author":"","guid":8225,"unread":true,"content":"<article>arXiv:2502.14270v1 Announce Type: new \nAbstract: Birth weight serves as a fundamental indicator of neonatal health, closely linked to both early medical interventions and long-term developmental risks. Traditional predictive models, often constrained by limited feature selection and incomplete datasets, struggle to achieve overlooking complex maternal and fetal interactions in diverse clinical settings. This research explores machine learning to address these limitations, utilizing a structured methodology that integrates advanced imputation strategies, supervised feature selection techniques, and predictive modeling. Given the constraints of the dataset, the research strengthens the role of data preprocessing in improving the model performance. Among the various methodologies explored, tree-based feature selection methods demonstrated superior capability in identifying the most relevant predictors, while ensemble-based regression models proved highly effective in capturing non-linear relationships and complex maternal-fetal interactions within the data. Beyond model performance, the study highlights the clinical significance of key physiological determinants, offering insights into maternal and fetal health factors that influence birth weight, offering insights that extend over statistical modeling. By bridging computational intelligence with perinatal research, this work underscores the transformative role of machine learning in enhancing predictive accuracy, refining risk assessment and informing data-driven decision-making in maternal and neonatal care. Keywords: Birth weight prediction, maternal-fetal health, MICE, BART, Gradient Boosting, neonatal outcomes, Clinipredictive.</article>","contentLength":1708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MCQA-Eval: Efficient Confidence Evaluation in NLG with Gold-Standard Correctness Labels","url":"https://arxiv.org/abs/2502.14268","date":1740114000,"author":"","guid":8226,"unread":true,"content":"<article>arXiv:2502.14268v1 Announce Type: new \nAbstract: Large Language Models (LLMs) require robust confidence estimation, particularly in critical domains like healthcare and law where unreliable outputs can lead to significant consequences. Despite much recent work in confidence estimation, current evaluation frameworks rely on correctness functions -- various heuristics that are often noisy, expensive, and possibly introduce systematic biases. These methodological weaknesses tend to distort evaluation metrics and thus the comparative ranking of confidence measures. We introduce MCQA-Eval, an evaluation framework for assessing confidence measures in Natural Language Generation (NLG) that eliminates dependence on an explicit correctness function by leveraging gold-standard correctness labels from multiple-choice datasets. MCQA-Eval enables systematic comparison of both internal state-based white-box (e.g. logit-based) and consistency-based black-box confidence measures, providing a unified evaluation methodology across different approaches. Through extensive experiments on multiple LLMs and widely used QA datasets, we report that MCQA-Eval provides efficient and more reliable assessments of confidence estimation methods than existing approaches.</article>","contentLength":1259,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Money Recognition for the Visually Impaired: A Case Study on Sri Lankan Banknotes","url":"https://arxiv.org/abs/2502.14267","date":1740114000,"author":"","guid":8227,"unread":true,"content":"<article>arXiv:2502.14267v1 Announce Type: new \nAbstract: Currency note recognition is a critical accessibility need for blind individuals, as identifying banknotes accurately can impact their independence and security in financial transactions. Several traditional and technological initiatives have been taken to date. Nevertheless, these approaches are less user-friendly and have made it more challenging for blind people to identify banknotes. This research proposes a user-friendly stand-alone system for the identification of Sri Lankan currency notes. A custom-created dataset of images of Sri Lankan currency notes was used to fine-tune an EfficientDet model. The currency note recognition model achieved 0.9847 AP on the validation dataset and performs exceptionally well in real-world scenarios. The high accuracy and the intuitive interface have enabled blind individuals to quickly and accurately identify currency denominations, ultimately encouraging accessibility and independence.</article>","contentLength":988,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game Dynamics","url":"https://arxiv.org/abs/2502.14264","date":1740114000,"author":"","guid":8228,"unread":true,"content":"<article>arXiv:2502.14264v1 Announce Type: new \nAbstract: Deep reinforcement learning agents often face challenges to effectively coordinate perception and decision-making components, particularly in environments with high-dimensional sensory inputs where feature relevance varies. This work introduces SPRIG (Stackelberg Perception-Reinforcement learning with Internal Game dynamics), a framework that models the internal perception-policy interaction within a single agent as a cooperative Stackelberg game. In SPRIG, the perception module acts as a leader, strategically processing raw sensory states, while the policy module follows, making decisions based on extracted features. SPRIG provides theoretical guarantees through a modified Bellman operator while preserving the benefits of modern policy optimization. Experimental results on the Atari BeamRider environment demonstrate SPRIG's effectiveness, achieving around 30% higher returns than standard PPO through its game-theoretical balance of feature extraction and decision-making.</article>","contentLength":1034,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records","url":"https://arxiv.org/abs/2502.14259","date":1740114000,"author":"","guid":8229,"unread":true,"content":"<article>arXiv:2502.14259v1 Announce Type: new \nAbstract: Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.</article>","contentLength":1098,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information","url":"https://arxiv.org/abs/2502.14258","date":1740114000,"author":"","guid":8230,"unread":true,"content":"<article>arXiv:2502.14258v1 Announce Type: new \nAbstract: While the ability of language models to elicit facts has been widely investigated, how they handle temporally changing facts remains underexplored. We discover Temporal Heads, specific attention heads primarily responsible for processing temporal knowledge through circuit analysis. We confirm that these heads are present across multiple models, though their specific locations may vary, and their responses differ depending on the type of knowledge and its corresponding years. Disabling these heads degrades the model's ability to recall time-specific knowledge while maintaining its general capabilities without compromising time-invariant and question-answering performances. Moreover, the heads are activated not only numeric conditions (\"In 2004\") but also textual aliases (\"In the year ...\"), indicating that they encode a temporal dimension beyond simple numerical representation. Furthermore, we expand the potential of our findings by demonstrating how temporal knowledge can be edited by adjusting the values of these heads.</article>","contentLength":1085,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Unified Implementation of Quasi-Monte Carlo Generators, Randomization Routines, and Fast Kernel Methods","url":"https://arxiv.org/abs/2502.14256","date":1740114000,"author":"","guid":8231,"unread":true,"content":"<article>arXiv:2502.14256v1 Announce Type: new \nAbstract: Quasi-random sequences, also called low-discrepancy sequences, have been extensively used as efficient experimental designs across many scientific disciplines. This article provides a unified description and software API for methods pertaining to low-discrepancy point sets. These methods include low discrepancy point set generators, randomization techniques, and fast kernel methods. Specifically, we provide generators for lattices, digital nets, and Halton point sets. Supported randomization techniques include random permutations / shifts, linear matrix scrambling, and nested uniform scrambling. Routines for working with higher-order digital nets and scramblings are also detailed. For kernel methods, we provide implementations of special shift-invariant and digitally-shift invariant kernels along with fast Gram matrix operations facilitated by the bit-reversed FFT, the bit-reversed IFFT, and the FWHT. A new digitally-shift-invariant kernel of higher-order smoothness is also derived. We also describe methods to quickly update the matrix-vector product or linear system solution after doubling the number of points in a lattice or digital net in natural order.</article>","contentLength":1223,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Effects of Prompt Length on Domain-specific Tasks for Large Language Models","url":"https://arxiv.org/abs/2502.14255","date":1740114000,"author":"","guid":8232,"unread":true,"content":"<article>arXiv:2502.14255v1 Announce Type: new \nAbstract: In recent years, Large Language Models have garnered significant attention for their strong performance in various natural language tasks, such as machine translation and question answering. These models demonstrate an impressive ability to generalize across diverse tasks. However, their effectiveness in tackling domain-specific tasks, such as financial sentiment analysis and monetary policy understanding, remains a topic of debate, as these tasks often require specialized knowledge and precise reasoning. To address such challenges, researchers design various prompts to unlock the models' abilities. By carefully crafting input prompts, researchers can guide these models to produce more accurate responses. Consequently, prompt engineering has become a key focus of study. Despite the advancements in both models and prompt engineering, the relationship between the two-specifically, how prompt design impacts models' ability to perform domain-specific tasks-remains underexplored. This paper aims to bridge this research gap.</article>","contentLength":1083,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation","url":"https://arxiv.org/abs/2502.14254","date":1740114000,"author":"","guid":8233,"unread":true,"content":"<article>arXiv:2502.14254v1 Announce Type: new \nAbstract: Recent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have made them powerful tools in embodied navigation, enabling agents to leverage commonsense and spatial reasoning for efficient exploration in unfamiliar environments. Existing LLM-based approaches convert global memory, such as semantic or topological maps, into language descriptions to guide navigation. While this improves efficiency and reduces redundant exploration, the loss of geometric information in language-based representations hinders spatial reasoning, especially in intricate environments. To address this, VLM-based approaches directly process ego-centric visual inputs to select optimal directions for exploration. However, relying solely on a first-person perspective makes navigation a partially observed decision-making problem, leading to suboptimal decisions in complex environments. In this paper, we present a novel vision-language model (VLM)-based navigation framework that addresses these challenges by adaptively retrieving task-relevant cues from a global memory module and integrating them with the agent's egocentric observations. By dynamically aligning global contextual information with local perception, our approach enhances spatial reasoning and decision-making in long-horizon tasks. Experimental results demonstrate that the proposed method surpasses previous state-of-the-art approaches in object navigation tasks, providing a more effective and scalable solution for embodied navigation.</article>","contentLength":1566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pandora3D: A Comprehensive Framework for High-Quality 3D Shape and Texture Generation","url":"https://arxiv.org/abs/2502.14247","date":1740114000,"author":"","guid":8234,"unread":true,"content":"<article>arXiv:2502.14247v1 Announce Type: new \nAbstract: This report presents a comprehensive framework for generating high-quality 3D shapes and textures from diverse input prompts, including single images, multi-view images, and text descriptions. The framework consists of 3D shape generation and texture generation. (1). The 3D shape generation pipeline employs a Variational Autoencoder (VAE) to encode implicit 3D geometries into a latent space and a diffusion network to generate latents conditioned on input prompts, with modifications to enhance model capacity. An alternative Artist-Created Mesh (AM) generation approach is also explored, yielding promising results for simpler geometries. (2). Texture generation involves a multi-stage process starting with frontal images generation followed by multi-view images generation, RGB-to-PBR texture conversion, and high-resolution multi-view texture refinement. A consistency scheduler is plugged into every stage, to enforce pixel-wise consistency among multi-view textures during inference, ensuring seamless integration.\n  The pipeline demonstrates effective handling of diverse input formats, leveraging advanced neural architectures and novel methodologies to produce high-quality 3D content. This report details the system architecture, experimental results, and potential future directions to improve and expand the framework. The source code and pretrained weights are released at: \\url{https://github.com/Tencent/Tencent-XR-3DGen}.</article>","contentLength":1489,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering","url":"https://arxiv.org/abs/2502.14245","date":1740114000,"author":"","guid":8235,"unread":true,"content":"<article>arXiv:2502.14245v1 Announce Type: new \nAbstract: In this paper, we identify a critical problem, \"lost-in-retrieval\", in retrieval-augmented multi-hop question answering (QA): the key entities are missed in LLMs' sub-question decomposition. \"Lost-in-retrieval\" significantly degrades the retrieval performance, which disrupts the reasoning chain and leads to the incorrect answers. To resolve this problem, we propose a progressive retrieval and rewriting method, namely ChainRAG, which sequentially handles each sub-question by completing missing key entities and retrieving relevant sentences from a sentence graph for answer generation. Each step in our retrieval and rewriting process builds upon the previous one, creating a seamless chain that leads to accurate retrieval and answers. Finally, all retrieved sentences and sub-question answers are integrated to generate a comprehensive answer to the original question. We evaluate ChainRAG on three multi-hop QA datasets$\\unicode{x2013}$MuSiQue, 2Wiki, and HotpotQA$\\unicode{x2013}$using three large language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG consistently outperforms baselines in both effectiveness and efficiency.</article>","contentLength":1229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the Contraction Analysis of Nonlinear System with Multiple Equilibrium Points","url":"https://arxiv.org/abs/2502.14242","date":1740114000,"author":"","guid":8236,"unread":true,"content":"<article>arXiv:2502.14242v1 Announce Type: new \nAbstract: In this work, we leverage the 2-contraction theory, which extends the capabilities of classical contraction theory, to develop a global stability framework. Coupled with powerful geometric tools such as the Poincare index theory, the 2-contraction theory enables us to analyze the stability of planar nonlinear systems without relying on local equilibrium analysis. By utilizing index theory and 2-contraction results, we efficiently characterize the nature of equilibrium points and delineate regions in 2-dimensional state space where periodic solutions, closed orbits, or stable dynamics may exist. A key focus of this work is the identification of regions in the state space where periodic solutions may occur, as well as 2-contraction regions that guarantee the nonexistence of such solutions. Additionally, we address a critical problem in engineering the determination of the basin of attraction (BOA) for stable equilibrium points. For systems with multiple equilibria identifying candidate BOAs becomes highly nontrivial. We propose a novel methodology leveraging the 2-contraction theory to approximate a common BOA for a class of nonlinear systems with multiple stable equilibria. Theoretical findings are substantiated through benchmark examples and numerical simulations, demonstrating the practical utility of the proposed approach. Furthermore, we extend our framework to analyze networked systems, showcasing their efficacy in an opinion dynamics problem.</article>","contentLength":1520,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Augmented Reality In-the-Wild: Usage Patterns and Experiences of Working with AR Laptops in Real-World Settings","url":"https://arxiv.org/abs/2502.14241","date":1740114000,"author":"","guid":8237,"unread":true,"content":"<article>arXiv:2502.14241v1 Announce Type: new \nAbstract: Augmented Reality (AR) is increasingly positioned as a tool for knowledge work, providing beneficial affordances such as a virtually limitless display space that integrates digital information with the user's physical surroundings. However, for AR to supplant traditional screen-based devices in knowledge work, it must support prolonged usage across diverse contexts. Until now, few studies have explored the effects, opportunities, and challenges of working in AR outside a controlled laboratory setting and for an extended duration. This gap in research limits our understanding of how users may adapt its affordances to their daily workflows and what barriers hinder its adoption. In this paper, we present findings from a longitudinal diary study examining how participants incorporated an AR laptop -- Sightful's Spacetop EA -- into their daily work routines. 14 participants used the device for 40-minute daily sessions over two weeks, collectively completing 103 hours of AR-based work. Through survey responses, workspace photographs, and post-study interviews, we analyzed usage patterns, workspace configurations, and evolving user perceptions. Our findings reveal key factors influencing participants' usage of AR, including task demands, environmental constraints, social dynamics, and ergonomic considerations. We highlight how participants leveraged and configured AR's virtual display space, along with emergent hybrid workflows that involved physical screens and tasks. Based on our results, we discuss both overlaps with current literature and new considerations and challenges for the future design of AR systems for pervasive and productive use.</article>","contentLength":1714,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No Minima, No Collisions: Combining Modulation and Control Barrier Function Strategies for Feasible Dynamical Collision Avoidance","url":"https://arxiv.org/abs/2502.14238","date":1740114000,"author":"","guid":8238,"unread":true,"content":"<article>arXiv:2502.14238v1 Announce Type: new \nAbstract: As prominent real-time safety-critical reactive control techniques, Control Barrier Function Quadratic Programs (CBF-QPs) work for control affine systems in general but result in local minima in the generated trajectories and consequently cannot ensure convergence to the goals. Contrarily, Modulation of Dynamical Systems (Mod-DSs), including normal, reference, and on-manifold Mod-DS, achieve obstacle avoidance with few and even no local minima but have trouble optimally minimizing the difference between the constrained and the unconstrained controller outputs, and its applications are limited to fully-actuated systems. We dive into the theoretical foundations of CBF-QP and Mod-DS, proving that despite their distinct origins, normal Mod-DS is a special case of CBF-QP, and reference Mod-DS's solutions are mathematically connected to that of the CBF-QP through one equation. Building on top of the unveiled theoretical connections between CBF-QP and Mod-DS, reference Mod-based CBF-QP and on-manifold Mod-based CBF-QP controllers are proposed to combine the strength of CBF-QP and Mod-DS approaches and realize local-minimum-free reactive obstacle avoidance for control affine systems in general. We validate our methods in both simulated hospital environments and real-world experiments using Ridgeback for fully-actuated systems and Fetch robots for underactuated systems. Mod-based CBF-QPs outperform CBF-QPs as well as the optimally constrained-enforcing Mod-DS approaches we proposed in all experiments.</article>","contentLength":1566,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving","url":"https://arxiv.org/abs/2502.14235","date":1740114000,"author":"","guid":8239,"unread":true,"content":"<article>arXiv:2502.14235v1 Announce Type: new \nAbstract: Accurate and realistic 3D scene reconstruction enables the lifelike creation of autonomous driving simulation environments. With advancements in 3D Gaussian Splatting (3DGS), previous studies have applied it to reconstruct complex dynamic driving scenes. These methods typically require expensive LiDAR sensors and pre-annotated datasets of dynamic objects. To address these challenges, we propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with Occupancy Grids (OGs) generated from surround-view camera images using Occupancy Prediction Network (ONet). Our method leverages the semantic information in OGs to separate dynamic vehicles from static street background, converting these grids into two distinct sets of initial point clouds for reconstructing both static and dynamic objects. Additionally, we estimate the trajectories and poses of dynamic objects through a learning-based approach, eliminating the need for complex manual annotations. Experiments on Waymo Open dataset demonstrate that OG-Gaussian is on par with the current state-of-the-art in terms of reconstruction quality and rendering speed, achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while significantly reducing computational costs and economic overhead.</article>","contentLength":1321,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Real-Time Sampling-based Online Planning for Drone Interception","url":"https://arxiv.org/abs/2502.14231","date":1740114000,"author":"","guid":8240,"unread":true,"content":"<article>arXiv:2502.14231v1 Announce Type: new \nAbstract: This paper studies high-speed online planning in dynamic environments. The problem requires finding time-optimal trajectories that conform to system dynamics, meeting computational constraints for real-time adaptation, and accounting for uncertainty from environmental changes. To address these challenges, we propose a sampling-based online planning algorithm that leverages neural network inference to replace time-consuming nonlinear trajectory optimization, enabling rapid exploration of multiple trajectory options under uncertainty. The proposed method is applied to the drone interception problem, where a defense drone must intercept a target while avoiding collisions and handling imperfect target predictions. The algorithm efficiently generates trajectories toward multiple potential target drone positions in parallel. It then assesses trajectory reachability by comparing traversal times with the target drone's predicted arrival time, ultimately selecting the minimum-time reachable trajectory. Through extensive validation in both simulated and real-world environments, we demonstrate our method's capability for high-rate online planning and its adaptability to unpredictable movements in unstructured settings.</article>","contentLength":1276,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Feedforward in Generative AI: Opportunities for a Design Space","url":"https://arxiv.org/abs/2502.14229","date":1740114000,"author":"","guid":8241,"unread":true,"content":"<article>arXiv:2502.14229v1 Announce Type: new \nAbstract: Generative AI (GenAI) models have become more capable than ever at augmenting productivity and cognition across diverse contexts. However, a fundamental challenge remains as users struggle to anticipate what AI will generate. As a result, they must engage in excessive turn-taking with the AI's feedback to clarify their intent, leading to significant cognitive load and time investment. Our goal is to advance the perspective that in order for users to seamlessly leverage the full potential of GenAI systems across various contexts, we must design GenAI systems that not only provide informative feedback but also informative feedforward -- designs that tell users what AI will generate before the user submits their prompt. To spark discussion on feedforward in GenAI, we designed diverse instantiations of feedforward across four GenAI applications: conversational UIs, document editors, malleable interfaces, and automation agents, and discussed how these designs can contribute to a more rigorous investigation of a design space and a set of guidelines for feedforward in all GenAI systems.</article>","contentLength":1145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SleepGMUformer: A gated multimodal temporal neural network for sleep staging","url":"https://arxiv.org/abs/2502.14227","date":1740114000,"author":"","guid":8242,"unread":true,"content":"<article>arXiv:2502.14227v1 Announce Type: new \nAbstract: Sleep staging is a key method for assessing sleep quality and diagnosing sleep disorders. However, current deep learning methods face challenges: 1) postfusion techniques ignore the varying contributions of different modalities; 2) unprocessed sleep data can interfere with frequency-domain information. To tackle these issues, this paper proposes a gated multimodal temporal neural network for multidomain sleep data, including heart rate, motion, steps, EEG (Fpz-Cz, Pz-Oz), and EOG from WristHR-Motion-Sleep and SleepEDF-78. The model integrates: 1) a pre-processing module for feature alignment, missing value handling, and EEG de-trending; 2) a feature extraction module for complex sleep features in the time dimension; and 3) a dynamic fusion module for real-time modality weighting.Experiments show classification accuracies of 85.03% on SleepEDF-78 and 94.54% on WristHR-Motion-Sleep datasets. The model handles heterogeneous datasets and outperforms state-of-the-art models by 1.00%-4.00%.</article>","contentLength":1048,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Designing Parameter and Compute Efficient Diffusion Transformers using Distillation","url":"https://arxiv.org/abs/2502.14226","date":1740114000,"author":"","guid":8243,"unread":true,"content":"<article>arXiv:2502.14226v1 Announce Type: new \nAbstract: Diffusion Transformers (DiTs) with billions of model parameters form the backbone of popular image and video generation models like DALL.E, Stable-Diffusion and SORA. Though these models are necessary in many low-latency applications like Augmented/Virtual Reality, they cannot be deployed on resource-constrained Edge devices (like Apple Vision Pro or Meta Ray-Ban glasses) due to their huge computational complexity. To overcome this, we turn to knowledge distillation and perform a thorough design-space exploration to achieve the best DiT for a given parameter size. In particular, we provide principles for how to choose design knobs such as depth, width, attention heads and distillation setup for a DiT. During the process, a three-way trade-off emerges between model performance, size and speed that is crucial for Edge implementation of diffusion. We also propose two distillation approaches - Teaching Assistant (TA) method and Multi-In-One (MI1) method - to perform feature distillation in the DiT context. Unlike existing solutions, we demonstrate and benchmark the efficacy of our approaches on practical Edge devices such as NVIDIA Jetson Orin Nano.</article>","contentLength":1212,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Pavement Sensor Data Acquisition for AI-Driven Transportation Research","url":"https://arxiv.org/abs/2502.14222","date":1740114000,"author":"","guid":8244,"unread":true,"content":"<article>arXiv:2502.14222v1 Announce Type: new \nAbstract: Effective strategies for sensor data management are essential for advancing transportation research, especially in the current data-driven era, due to the advent of novel applications in artificial intelligence. This paper presents comprehensive guidelines for managing transportation sensor data, encompassing both archived static data and real-time data streams. The real-time system architecture integrates various applications with data acquisition systems (DAQ). By deploying the in-house designed, open-source Avena software platform alongside the NATS messaging system as a secure communication broker, reliable data exchange is ensured. While robust databases like TimescaleDB facilitate organized storage, visualization platforms like Grafana provide real-time monitoring capabilities.\n  In contrast, static data standards address the challenges in handling unstructured, voluminous datasets. The standards advocate for a combination of cost-effective bulk cloud storage for unprocessed sensor data and relational databases for recording summarized analyses. They highlight the role of cloud data transfer tools like FME for efficient migration of sensor data from local storages onto the cloud. Further, integration of robust visualization tools into the framework helps in deriving patterns and trends from these complex datasets.\n  The proposals were applied to INDOT's real-world case studies involving the I-65 and I-69 Greenfield districts. For real-time data collection, Campbell Scientific DAQ systems were used, enabling continuous generation and monitoring of sensor metrics. In the case of the archived I-69 database, summary data was compiled in Oracle, while the unprocessed data was stored in SharePoint. The results underline the effectiveness of the proposed guidelines and motivate their adoption in research projects.</article>","contentLength":1893,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"H3DE-Net: Efficient and Accurate 3D Landmark Detection in Medical Imaging","url":"https://arxiv.org/abs/2502.14221","date":1740114000,"author":"","guid":8245,"unread":true,"content":"<article>arXiv:2502.14221v1 Announce Type: new \nAbstract: 3D landmark detection is a critical task in medical image analysis, and accurately detecting anatomical landmarks is essential for subsequent medical imaging tasks. However, mainstream deep learning methods in this field struggle to simultaneously capture fine-grained local features and model global spatial relationships, while maintaining a balance between accuracy and computational efficiency. Local feature extraction requires capturing fine-grained anatomical details, while global modeling requires understanding the spatial relationships within complex anatomical structures. The high-dimensional nature of 3D volume further exacerbates these challenges, as landmarks are sparsely distributed, leading to significant computational costs. Therefore, achieving efficient and precise 3D landmark detection remains a pressing challenge in medical image analysis.\n  In this work, We propose a \\textbf{H}ybrid \\textbf{3}D \\textbf{DE}tection \\textbf{Net}(H3DE-Net), a novel framework that combines CNNs for local feature extraction with a lightweight attention mechanism designed to efficiently capture global dependencies in 3D volumetric data. This mechanism employs a hierarchical routing strategy to reduce computational cost while maintaining global context modeling. To our knowledge, H3DE-Net is the first 3D landmark detection model that integrates such a lightweight attention mechanism with CNNs. Additionally, integrating multi-scale feature fusion further enhances detection accuracy and robustness. Experimental results on a public CT dataset demonstrate that H3DE-Net achieves state-of-the-art(SOTA) performance, significantly improving accuracy and robustness, particularly in scenarios with missing landmarks or complex anatomical variations. We aready open-source our project, including code, data and model weights.</article>","contentLength":1884,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NDPage: Efficient Address Translation for Near-Data Processing Architectures via Tailored Page Table","url":"https://arxiv.org/abs/2502.14220","date":1740114000,"author":"","guid":8246,"unread":true,"content":"<article>arXiv:2502.14220v1 Announce Type: new \nAbstract: Near-Data Processing (NDP) has been a promising architectural paradigm to address the memory wall problem for data-intensive applications. Practical implementation of NDP architectures calls for system support for better programmability, where having virtual memory (VM) is critical. Modern computing systems incorporate a 4-level page table design to support address translation in VM. However, simply adopting an existing 4-level page table in NDP systems causes significant address translation overhead because (1) NDP applications generate a lot of address translations, and (2) the limited L1 cache in NDP systems cannot cover the accesses to page table entries (PTEs). We extensively analyze the 4-level page table design in the NDP scenario and observe that (1) the memory access to page table entries is highly irregular, thus cannot benefit from the L1 cache, and (2) the last two levels of page tables are nearly fully occupied. Based on our observations, we propose NDPage, an efficient page table design tailored for NDP systems. The key mechanisms of NDPage are (1) an L1 cache bypass mechanism for PTEs that not only accelerates the memory accesses of PTEs but also prevents the pollution of PTEs in the cache system, and (2) a flattened page table design that merges the last two levels of page tables, allowing the page table to enjoy the flexibility of a 4KB page while reducing the number of PTE accesses. We evaluate NDPage using a variety of data-intensive workloads. Our evaluation shows that in a single-core NDP system, NDPage improves the end-to-end performance over the state-of-the-art address translation mechanism of 14.3\\%; in 4-core and 8-core NDP systems, NDPage enhances the performance of 9.8\\% and 30.5\\%, respectively.</article>","contentLength":1802,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigating the Impact of LLM Personality on Cognitive Bias Manifestation in Automated Decision-Making Tasks","url":"https://arxiv.org/abs/2502.14219","date":1740114000,"author":"","guid":8247,"unread":true,"content":"<article>arXiv:2502.14219v1 Announce Type: new \nAbstract: Large Language Models (LLMs) are increasingly used in decision-making, yet their susceptibility to cognitive biases remains a pressing challenge. This study explores how personality traits influence these biases and evaluates the effectiveness of mitigation strategies across various model architectures. Our findings identify six prevalent cognitive biases, while the sunk cost and group attribution biases exhibit minimal impact. Personality traits play a crucial role in either amplifying or reducing biases, significantly affecting how LLMs respond to debiasing techniques. Notably, Conscientiousness and Agreeableness may generally enhance the efficacy of bias mitigation strategies, suggesting that LLMs exhibiting these traits are more receptive to corrective measures. These findings address the importance of personality-driven bias dynamics and highlight the need for targeted mitigation approaches to improve fairness and reliability in AI-assisted decision-making.</article>","contentLength":1025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rethinking Spiking Neural Networks from an Ensemble Learning Perspective","url":"https://arxiv.org/abs/2502.14218","date":1740114000,"author":"","guid":8248,"unread":true,"content":"<article>arXiv:2502.14218v1 Announce Type: new \nAbstract: Spiking neural networks (SNNs) exhibit superior energy efficiency but suffer from limited performance. In this paper, we consider SNNs as ensembles of temporal subnetworks that share architectures and weights, and highlight a crucial issue that affects their performance: excessive differences in initial states (neuronal membrane potentials) across timesteps lead to unstable subnetwork outputs, resulting in degraded performance. To mitigate this, we promote the consistency of the initial membrane potential distribution and output through membrane potential smoothing and temporally adjacent subnetwork guidance, respectively, to improve overall stability and performance. Moreover, membrane potential smoothing facilitates forward propagation of information and backward propagation of gradients, mitigating the notorious temporal gradient vanishing problem. Our method requires only minimal modification of the spiking neurons without adapting the network structure, making our method generalizable and showing consistent performance gains in 1D speech, 2D object, and 3D point cloud recognition tasks. In particular, on the challenging CIFAR10-DVS dataset, we achieved 83.20\\% accuracy with only four timesteps. This provides valuable insights into unleashing the potential of SNNs.</article>","contentLength":1338,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask Me Anything: Exploring children's attitudes toward an age-tailored AI-powered chatbot","url":"https://arxiv.org/abs/2502.14217","date":1740114000,"author":"","guid":8249,"unread":true,"content":"<article>arXiv:2502.14217v1 Announce Type: new \nAbstract: Conversational agents, such as chatbots, have increasingly found their way into many dimensions of our lives, including entertainment and education. In this exploratory study we built a child-friendly chatbot, \"Ask Me Anything\" (AMA), and investigated children's attitudes and trust toward AI-driven conversational agents. To prompt targeted questioning from students and drive engagement, AMA is a specialized chatbot that answers only topic--specific questions in three areas--astronomy, sneakers and shoes, and dinosaurs. We tested AMA with 63 students in a K-8 public school in the Northeast USA. Students worked in small groups, interacted with our tool for three to ten minutes, and completed a post survey. We identified three key themes that emerged from student conversational interactions with AMA: expressing wonder, surprise, and curiosity; building trust and developing confidence; and building relationships and anthropomorphizing. Also, we observed a broad attitude of openness and comfort. Students trusted the chatbot responses in general, indicating a high level of trust in and reliance on AI as a source of information. They described AMA as \"knowledgeable,\" \"smart,\" and that they could \"trust it.\" To confirm their perception of reliability, some students tested the chatbot with questions to which they knew the answers. This behavior illustrated a fundamental aspect of children's cognitive development: the process of actively evaluating the credibility of sources. Our work extends and contributes to the existing body of literature that explores children's interactions with conversational agents.</article>","contentLength":1673,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Secure Program Partitioning for Smart Contracts with LLM's In-Context Learning","url":"https://arxiv.org/abs/2502.14215","date":1740114000,"author":"","guid":8250,"unread":true,"content":"<article>arXiv:2502.14215v1 Announce Type: new \nAbstract: Smart contracts are highly susceptible to manipulation attacks due to the leakage of sensitive information. Addressing manipulation vulnerabilities is particularly challenging because they stem from inherent data confidentiality issues rather than straightforward implementation bugs. To tackle this by preventing sensitive information leakage, we present PartitionGPT, the first LLM-driven approach that combines static analysis with the in-context learning capabilities of large language models (LLMs) to partition smart contracts into privileged and normal codebases, guided by a few annotated sensitive data variables. We evaluated PartitionGPT on 18 annotated smart contracts containing 99 sensitive functions. The results demonstrate that PartitionGPT successfully generates compilable, and verified partitions for 78% of the sensitive functions while reducing approximately 30% code compared to function-level partitioning approach. Furthermore, we evaluated PartitionGPT on nine real-world manipulation attacks that lead to a total loss of 25 million dollars, PartitionGPT effectively prevents eight cases, highlighting its potential for broad applicability and the necessity for secure program partitioning during smart contract development to diminish manipulation vulnerabilities.</article>","contentLength":1340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation","url":"https://arxiv.org/abs/2502.14214","date":1740114000,"author":"","guid":8251,"unread":true,"content":"<article>arXiv:2502.14214v1 Announce Type: new \nAbstract: Source-free unsupervised domain adaptation (SFUDA) has gained significant attention as an alternative to traditional unsupervised domain adaptation (UDA), which relies on the constant availability of labeled source data. However, SFUDA approaches come with inherent limitations that are frequently overlooked. These challenges include performance degradation when the unlabeled target data fails to meet critical assumptions, such as having a closed-set label distribution identical to that of the source domain, or when sufficient unlabeled target data is unavailable-a common situation in real-world applications. To address these issues, we propose an asymmetric co-training (ACT) method specifically designed for the SFFSDA scenario. SFFSDA presents a more practical alternative to SFUDA, as gathering a few labeled target instances is more feasible than acquiring large volumes of unlabeled target data in many real-world contexts. Our ACT method begins by employing a weak-strong augmentation to enhance data diversity. Then we use a two-step optimization process to train the target model. In the first step, we optimize the label smoothing cross-entropy loss, the entropy of the class-conditional distribution, and the reverse-entropy loss to bolster the model's discriminative ability while mitigating overfitting. The second step focuses on reducing redundancy in the output space by minimizing classifier determinacy disparity. Extensive experiments across four benchmarks demonstrate the superiority of our ACT approach, which outperforms state-of-the-art SFUDA methods and transfer learning techniques. Our findings suggest that adapting a source pre-trained model using only a small amount of labeled target data offers a practical and dependable solution. The code is available at https://github.com/gengxuli/ACT.</article>","contentLength":1877,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asynchronous Stochastic Block Projection Algorithm for Solving Linear Systems under Predefined Communication Patterns","url":"https://arxiv.org/abs/2502.14213","date":1740114000,"author":"","guid":8252,"unread":true,"content":"<article>arXiv:2502.14213v1 Announce Type: new \nAbstract: Distributed computation over networks is now receiving an increasing attention in many fields such as engineering and machine learning, where the solution of a linear system of equations is a basic task. This paper presents an asynchronous distributed randomized block Kaczmarz projection algorithm for solving large-scale linear systems over a multi-agent networks, where each agent only holds a part of the problem data. An event-triggered communication mechanism is integrated to minimize the communication overhead and reduce the overall communication costs. This communication mechanism allows each agent to update independently in an asynchronous environment and dynamically regulate communication frequency. In addition, this article analyzes the inefficiency caused by communication in asynchronous algorithms, explores the potential of event triggering mechanisms in alleviating these problems, and provides general conditions for global convergence in such environments. Moreover, a modified stochastic block Kaczmarz algorithm is used for each agent to update their local estimate. Through rigorous mathematical analysis, the exponential convergence rate of the proposed algorithm is established for a consistent system and its computational efficiency, robustness, and communication efficiency is validated through extensive numerical experiments. Furthermore, to address inconsistent systems, the algorithm introduces auxiliary variables to facilitate convergence toward an approximate least-squares solution, accompanied by a formal error analysis. The experimental results demonstrate that the algorithm maintains stability even under extreme asynchrony, communication failures, and node failures, while achieving significantly lower communication overhead and faster convergence rates compared to traditional methods.</article>","contentLength":1882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Less is More: On the Importance of Data Quality for Unit Test Generation","url":"https://arxiv.org/abs/2502.14212","date":1740114000,"author":"","guid":8253,"unread":true,"content":"<article>arXiv:2502.14212v1 Announce Type: new \nAbstract: Unit testing is crucial for software development and maintenance. Effective unit testing ensures and improves software quality, but writing unit tests is time-consuming and labor-intensive. Recent studies have proposed deep learning (DL) techniques or large language models (LLMs) to automate unit test generation. These models are usually trained or fine-tuned on large-scale datasets. Despite growing awareness of the importance of data quality, there has been limited research on the quality of datasets used for test generation. To bridge this gap, we systematically examine the impact of noise on the performance of learning-based test generation models. We first apply the open card sorting method to analyze the most popular and largest test generation dataset, Methods2Test, to categorize eight distinct types of noise. Further, we conduct detailed interviews with 17 domain experts to validate and assess the importance, reasonableness, and correctness of the noise taxonomy. Then, we propose CleanTest, an automated noise-cleaning framework designed to improve the quality of test generation datasets. CleanTest comprises three filters: a rule-based syntax filter, a rule-based relevance filter, and a model-based coverage filter. To evaluate its effectiveness, we apply CleanTest on two widely-used test generation datasets, i.e., Methods2Test and Atlas. Our findings indicate that 43.52% and 29.65% of datasets contain noise, highlighting its prevalence. Finally, we conduct comparative experiments using four LLMs (i.e., CodeBERT, AthenaTest, StarCoder, and CodeLlama7B) to assess the impact of noise on test generation performance. The results show that filtering noise positively influences the test generation ability of the models.</article>","contentLength":1797,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization","url":"https://arxiv.org/abs/2502.14211","date":1740114000,"author":"","guid":8254,"unread":true,"content":"<article>arXiv:2502.14211v1 Announce Type: new \nAbstract: Large language models (LLMs) face significant challenges when balancing multiple high-level objectives, such as generating coherent, relevant, and high-quality responses while maintaining efficient task adaptation across diverse tasks. To address these challenges, we introduce Transfer-Prompting, a novel two-stage framework designed to enhance cross-task adaptation in prompt generation. The framework comprises two key components: (1) source prompt construction, which refines the original prompts on source task datasets to generate source prompts with enhanced generalization ability, and (2) target prompt generation, which enhances cross-task adaptation of target prompts by fine-tuning a set of high-scored source prompts on task-specific datasets. In each optimization cycle, a reference LLM generates candidate prompts based on historical prompt-score pairs and task descriptions in our designed reference prompt. These candidate prompts are refined iteratively, while a scorer LLM evaluates their effectiveness using the multi-dimensional metrics designed in the objective prompts evaluator-a novel contribution in this work that provides a holistic evaluation of prompt quality and task performance. This feedback loop facilitates continuous refinement, optimizing both prompt quality and task-specific outcomes. We validate Transfer-Prompting through extensive experiments across 25 LLMs, including 7 foundational models and 18 specialized models, evaluated on 9 diverse datasets. The results demonstrate that Transfer-Prompting significantly improves task-specific performance, highlighting its potential for enhancing cross-task adaptation in LLMs. The code is available at https://github.com/llm172/Transfer-Prompting.</article>","contentLength":1783,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Spatial and Frequency Domain Adaptive Fusion Network for Image Deblurring","url":"https://arxiv.org/abs/2502.14209","date":1740114000,"author":"","guid":8255,"unread":true,"content":"<article>arXiv:2502.14209v1 Announce Type: new \nAbstract: Image deblurring aims to reconstruct a latent sharp image from its corresponding blurred one. Although existing methods have achieved good performance, most of them operate exclusively in either the spatial domain or the frequency domain, rarely exploring solutions that fuse both domains. In this paper, we propose a spatial-frequency domain adaptive fusion network (SFAFNet) to address this limitation. Specifically, we design a gated spatial-frequency domain feature fusion block (GSFFBlock), which consists of three key components: a spatial domain information module, a frequency domain information dynamic generation module (FDGM), and a gated fusion module (GFM). The spatial domain information module employs the NAFBlock to integrate local information. Meanwhile, in the FDGM, we design a learnable low-pass filter that dynamically decomposes features into separate frequency subbands, capturing the image-wide receptive field and enabling the adaptive exploration of global contextual information. Additionally, to facilitate information flow and the learning of complementary representations. In the GFM, we present a gating mechanism (GATE) to re-weight spatial and frequency domain features, which are then fused through the cross-attention mechanism (CAM). Experimental results demonstrate that our SFAFNet performs favorably compared to state-of-the-art approaches on commonly used benchmarks.</article>","contentLength":1457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Non-Asymptotic Theory of Seminorm Lyapunov Stability: From Deterministic to Stochastic Iterative Algorithms","url":"https://arxiv.org/abs/2502.14208","date":1740114000,"author":"","guid":8256,"unread":true,"content":"<article>arXiv:2502.14208v1 Announce Type: new \nAbstract: We study the problem of solving fixed-point equations for seminorm-contractive operators and establish foundational results on the non-asymptotic behavior of iterative algorithms in both deterministic and stochastic settings. Specifically, in the deterministic setting, we prove a fixed-point theorem for seminorm-contractive operators, showing that iterates converge geometrically to the kernel of the seminorm. In the stochastic setting, we analyze the corresponding stochastic approximation (SA) algorithm under seminorm-contractive operators and Markovian noise, providing a finite-sample analysis for various stepsize choices.\n  A benchmark for equation solving is linear systems of equations, where the convergence behavior of fixed-point iteration is closely tied to the stability of linear dynamical systems. In this special case, our results provide a complete characterization of system stability with respect to a seminorm, linking it to the solution of a Lyapunov equation in terms of positive semi-definite matrices. In the stochastic setting, we establish a finite-sample analysis for linear Markovian SA without requiring the Hurwitzness assumption.\n  Our theoretical results offer a unified framework for deriving finite-sample bounds for various reinforcement learning algorithms in the average reward setting, including TD($\\lambda$) for policy evaluation (which is a special case of solving a Poisson equation) and Q-learning for control.</article>","contentLength":1506,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Mesh Refinement for Variational Inequalities","url":"https://arxiv.org/abs/2502.14206","date":1740114000,"author":"","guid":8257,"unread":true,"content":"<article>arXiv:2502.14206v1 Announce Type: new \nAbstract: Variational inequalities play a pivotal role in a wide array of scientific and engineering applications. This project presents two techniques for adaptive mesh refinement (AMR) in the context of variational inequalities, with a specific focus on the classical obstacle problem.\n  We propose two distinct AMR strategies: Variable Coefficient Elliptic Smoothing (VCES) and Unstructured Dilation Operator (UDO). VCES uses a nodal active set indicator function as the initial iterate to a time-dependent heat equation problem. Solving a single step of this problem has the effect of smoothing the indicator about the free boundary. We threshold this smoothed indicator function to identify elements near the free boundary. Key parameters such as timestep and threshold values significantly influence the efficacy of this method.\n  The second strategy, UDO, focuses on the discrete identification of elements adjacent to the free boundary, employing a graph-based approach to mark neighboring elements for refinement. This technique resembles the dilation morphological operation in image processing, but tailored for unstructured meshes.\n  We also examine the theory of variational inequalities, the convergence behavior of finite element solutions, and implementation in the Firedrake finite element library. Convergence analysis reveals that accurate free boundary estimation is pivotal for solver performance. Numerical experiments demonstrate the effectiveness of the proposed methods in dynamically enhancing mesh resolution around free boundaries, thereby improving the convergence rates and computational efficiency of variational inequality solvers. Our approach integrates seamlessly with existing Firedrake numerical solvers, and it is promising for solving more complex free boundary problems.</article>","contentLength":1849,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Accurate Forgetting for Heterogeneous Federated Continual Learning","url":"https://arxiv.org/abs/2502.14205","date":1740114000,"author":"","guid":8258,"unread":true,"content":"<article>arXiv:2502.14205v1 Announce Type: new \nAbstract: Recent years have witnessed a burgeoning interest in federated learning (FL). However, the contexts in which clients engage in sequential learning remain under-explored. Bridging FL and continual learning (CL) gives rise to a challenging practical problem: federated continual learning (FCL). Existing research in FCL primarily focuses on mitigating the catastrophic forgetting issue of continual learning while collaborating with other clients. We argue that the forgetting phenomena are not invariably detrimental. In this paper, we consider a more practical and challenging FCL setting characterized by potentially unrelated or even antagonistic data/tasks across different clients. In the FL scenario, statistical heterogeneity and data noise among clients may exhibit spurious correlations which result in biased feature learning. While existing CL strategies focus on a complete utilization of previous knowledge, we found that forgetting biased information is beneficial in our study. Therefore, we propose a new concept accurate forgetting (AF) and develop a novel generative-replay method~\\method~which selectively utilizes previous knowledge in federated networks. We employ a probabilistic framework based on a normalizing flow model to quantify the credibility of previous knowledge. Comprehensive experiments affirm the superiority of our method over baselines.</article>","contentLength":1423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On-the-fly Preference Alignment via Principle-Guided Decoding","url":"https://arxiv.org/abs/2502.14204","date":1740114000,"author":"","guid":8259,"unread":true,"content":"<article>arXiv:2502.14204v1 Announce Type: new \nAbstract: With the rapidly expanding landscape of large language models, aligning model generations with human values and preferences is becoming increasingly important. Popular alignment methods, such as Reinforcement Learning from Human Feedback, have shown significant success in guiding models with greater control. However, these methods require considerable computational resources, which is inefficient, and substantial collection of training data to accommodate the diverse and pluralistic nature of human preferences, which is impractical. These limitations significantly constrain the scope and efficacy of both task-specific and general preference alignment methods. In this work, we introduce On-the-fly Preference Alignment via Principle-Guided Decoding (OPAD) to directly align model outputs with human preferences during inference, eliminating the need for fine-tuning. Our approach involves first curating a surrogate solution to an otherwise infeasible optimization problem and then designing a principle-guided reward function based on this surrogate. The final aligned policy is derived by maximizing this customized reward, which exploits the discrepancy between the constrained policy and its unconstrained counterpart. OPAD directly modifies the model's predictions during inference, ensuring principle adherence without incurring the computational overhead of retraining or fine-tuning. Experiments show that OPAD achieves competitive or superior performance in both general and personalized alignment tasks, demonstrating its efficiency and effectiveness compared to state-of-the-art baselines.</article>","contentLength":1657,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Do LLMs Consider Security? An Empirical Study on Responses to Programming Questions","url":"https://arxiv.org/abs/2502.14202","date":1740114000,"author":"","guid":8260,"unread":true,"content":"<article>arXiv:2502.14202v1 Announce Type: new \nAbstract: The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses.</article>","contentLength":1727,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Causal Mean Field Multi-Agent Reinforcement Learning","url":"https://arxiv.org/abs/2502.14200","date":1740114000,"author":"","guid":8261,"unread":true,"content":"<article>arXiv:2502.14200v1 Announce Type: new \nAbstract: Scalability remains a challenge in multi-agent reinforcement learning and is currently under active research. A framework named mean-field reinforcement learning (MFRL) could alleviate the scalability problem by employing the Mean Field Theory to turn a many-agent problem into a two-agent problem. However, this framework lacks the ability to identify essential interactions under nonstationary environments. Causality contains relatively invariant mechanisms behind interactions, though environments are nonstationary. Therefore, we propose an algorithm called causal mean-field Q-learning (CMFQ) to address the scalability problem. CMFQ is ever more robust toward the change of the number of agents though inheriting the compressed representation of MFRL's action-state space. Firstly, we model the causality behind the decision-making process of MFRL into a structural causal model (SCM). Then the essential degree of each interaction is quantified via intervening on the SCM. Furthermore, we design the causality-aware compact representation for behavioral information of agents as the weighted sum of all behavioral information according to their causal effects. We test CMFQ in a mixed cooperative-competitive game and a cooperative game. The result shows that our method has excellent scalability performance in both training in environments containing a large number of agents and testing in environments containing much more agents.</article>","contentLength":1491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Antenna Position and Beamforming Optimization for Movable Antenna Enabled ISAC: Optimal Solutions and Efficient Algorithms","url":"https://arxiv.org/abs/2502.14198","date":1740114000,"author":"","guid":8262,"unread":true,"content":"<article>arXiv:2502.14198v1 Announce Type: new \nAbstract: In this paper, we propose an integrated sensing and communication (ISAC) system enabled by movable antennas (MAs), which can dynamically adjust antenna positions to enhance both sensing and communication performance for future wireless networks. To characterize the benefits of MA-enabled ISAC systems, we first derive the Cram\\'er-Rao bound (CRB) for angle estimation error, which is then minimized for optimizing the antenna position vector (APV) and beamforming design, subject to a pre-defined signal-to-noise ratio (SNR) constraint to ensure the communication performance. In particular, for the case with receive MAs only, we provide a closed-form optimal antenna position solution, and show that employing MAs over conventional fixed-position antennas (FPAs) can achieve a sensing performance gain upper-bounded by 4.77 dB. On the other hand, for the case with transmit MAs only, we develop a boundary traversal breadth-first search (BT-BFS) algorithm to obtain the global optimal solution in the line-of-sight (LoS) channel scenario, along with a lower-complexity boundary traversal depth-first search (BT-DFS) algorithm to find a local optimal solution efficiently. While in the scenario with non-LoS (NLoS) channels, a majorization-minimization (MM) based Rosen's gradient projection (RGP) algorithm with an efficient initialization method is proposed to obtain stationary solutions for the considered problem, which can be extended to the general case with both transmit and receive MAs. Extensive numerical results are presented to verify the effectiveness of the proposed algorithms, and demonstrate the superiority of the considered MA-enabled ISAC system over conventional ISAC systems with FPAs in terms of sensing and communication performance trade-off.</article>","contentLength":1820,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Adaptive Sparsified Graph Learning Framework for Vessel Behavior Anomalies","url":"https://arxiv.org/abs/2502.14197","date":1740114000,"author":"","guid":8263,"unread":true,"content":"<article>arXiv:2502.14197v1 Announce Type: new \nAbstract: Graph neural networks have emerged as a powerful tool for learning spatiotemporal interactions. However, conventional approaches often rely on predefined graphs, which may obscure the precise relationships being modeled. Additionally, existing methods typically define nodes based on fixed spatial locations, a strategy that is ill-suited for dynamic environments like maritime environments. Our method introduces an innovative graph representation where timestamps are modeled as distinct nodes, allowing temporal dependencies to be explicitly captured through graph edges. This setup is extended to construct a multi-ship graph that effectively captures spatial interactions while preserving graph sparsity. The graph is processed using Graph Convolutional Network layers to capture spatiotemporal patterns, with a forecasting layer for feature prediction and a Variational Graph Autoencoder for reconstruction, enabling robust anomaly detection.</article>","contentLength":997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bridging Text and Vision: A Multi-View Text-Vision Registration Approach for Cross-Modal Place Recognition","url":"https://arxiv.org/abs/2502.14195","date":1740114000,"author":"","guid":8264,"unread":true,"content":"<article>arXiv:2502.14195v1 Announce Type: new \nAbstract: Mobile robots necessitate advanced natural language understanding capabilities to accurately identify locations and perform tasks such as package delivery. However, traditional visual place recognition (VPR) methods rely solely on single-view visual information and cannot interpret human language descriptions. To overcome this challenge, we bridge text and vision by proposing a multiview (360{\\deg} views of the surroundings) text-vision registration approach called Text4VPR for place recognition task, which is the first method that exclusively utilizes textual descriptions to match a database of images. Text4VPR employs the frozen T5 language model to extract global textual embeddings. Additionally, it utilizes the Sinkhorn algorithm with temperature coefficient to assign local tokens to their respective clusters, thereby aggregating visual descriptors from images. During the training stage, Text4VPR emphasizes the alignment between individual text-image pairs for precise textual description. In the inference stage, Text4VPR uses the Cascaded Cross-Attention Cosine Alignment (CCCA) to address the internal mismatch between text and image groups. Subsequently, Text4VPR performs precisely place match based on the descriptions of text-image groups. On Street360Loc, the first text to image VPR dataset we created, Text4VPR builds a robust baseline, achieving a leading top-1 accuracy of 57% and a leading top-10 accuracy of 92% within a 5-meter radius on the test set, which indicates that localization from textual descriptions to images is not only feasible but also holds significant potential for further advancement, as shown in Figure 1.</article>","contentLength":1708,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NLP-AKG: Few-Shot Construction of NLP Academic Knowledge Graph Based on LLM","url":"https://arxiv.org/abs/2502.14192","date":1740114000,"author":"","guid":8265,"unread":true,"content":"<article>arXiv:2502.14192v1 Announce Type: new \nAbstract: Large language models (LLMs) have been widely applied in question answering over scientific research papers. To enhance the professionalism and accuracy of responses, many studies employ external knowledge augmentation. However, existing structures of external knowledge in scientific literature often focus solely on either paper entities or domain concepts, neglecting the intrinsic connections between papers through shared domain concepts. This results in less comprehensive and specific answers when addressing questions that combine papers and concepts. To address this, we propose a novel knowledge graph framework that captures deep conceptual relations between academic papers, constructing a relational network via intra-paper semantic elements and inter-paper citation relations. Using a few-shot knowledge graph construction method based on LLM, we develop NLP-AKG, an academic knowledge graph for the NLP domain, by extracting 620,353 entities and 2,271,584 relations from 60,826 papers in ACL Anthology. Based on this, we propose a 'sub-graph community summary' method and validate its effectiveness on three NLP scientific literature question answering datasets.</article>","contentLength":1226,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models","url":"https://arxiv.org/abs/2502.14191","date":1740114000,"author":"","guid":8266,"unread":true,"content":"<article>arXiv:2502.14191v1 Announce Type: new \nAbstract: Reward models play an essential role in training vision-language models (VLMs) by assessing output quality to enable aligning with human preferences. Despite their importance, the research community lacks comprehensive open benchmarks for evaluating multimodal reward models in VLMs. To address this gap, we introduce Multimodal RewardBench, an expert-annotated benchmark covering six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Our dataset comprises 5,211 annotated (prompt, chosen response, rejected response) triplets collected from various VLMs. In evaluating a range of VLM judges, we find that even the top-performing models, Gemini 1.5 Pro and Claude 3.5 Sonnet, achieve only 72% overall accuracy. Notably, most models struggle in the reasoning and safety domains. These findings suggest that Multimodal RewardBench offers a challenging testbed for advancing reward model development across multiple domains. We release the benchmark at https://github.com/facebookresearch/multimodal_rewardbench.</article>","contentLength":1111,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Stereo Image Coding for Machines with Joint Visual Feature Compression","url":"https://arxiv.org/abs/2502.14190","date":1740114000,"author":"","guid":8267,"unread":true,"content":"<article>arXiv:2502.14190v1 Announce Type: new \nAbstract: 2D image coding for machines (ICM) has achieved great success in coding efficiency, while less effort has been devoted to stereo image fields. To promote the efficiency of stereo image compression (SIC) and intelligent analysis, the stereo image coding for machines (SICM) is formulated and explored in this paper. More specifically, a machine vision-oriented stereo feature compression network (MVSFC-Net) is proposed for SICM, where the stereo visual features are effectively extracted, compressed, and transmitted for 3D visual task. To efficiently compress stereo visual features in MVSFC-Net, a stereo multi-scale feature compression (SMFC) module is designed to gradually transform sparse stereo multi-scale features into compact joint visual representations by removing spatial, inter-view, and cross-scale redundancies simultaneously. Experimental results show that the proposed MVSFC-Net obtains superior compression efficiency as well as 3D visual task performance, when compared with the existing ICM anchors recommended by MPEG and the state-of-the-art SIC method.</article>","contentLength":1125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification","url":"https://arxiv.org/abs/2502.14189","date":1740114000,"author":"","guid":8268,"unread":true,"content":"<article>arXiv:2502.14189v1 Announce Type: new \nAbstract: The escalating volume of collected healthcare textual data presents a unique challenge for automated Multi-Label Text Classification (MLTC), which is primarily due to the scarcity of annotated texts for training and their nuanced nature. Traditional machine learning models often fail to fully capture the array of expressed topics. However, Large Language Models (LLMs) have demonstrated remarkable effectiveness across numerous Natural Language Processing (NLP) tasks in various domains, which show impressive computational efficiency and suitability for unsupervised learning through prompt engineering. Consequently, these LLMs promise an effective MLTC of medical narratives. However, when dealing with various labels, different prompts can be relevant depending on the topic. To address these challenges, the proposed approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT, PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and BART provides topics' assignment probabilities, which results in four classifications, all in a 0-shot setting. The outputs are then combined using ensemble learning and processed through a meta-classifier to produce the final MLTC result. The approach is evaluated using three samples of annotated texts, which contrast it with traditional and single-model methods. The results show significant improvements across the majority of the topics in the classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and 80.16% with standard deviations of 0.025 and 0.011, respectively). This research advances MLTC using LLMs and provides an efficient and scalable solution to rapidly categorize healthcare-related text data without further training.</article>","contentLength":1866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Federated Fine-Tuning of Large Language Models: Kahneman-Tversky vs. Direct Preference Optimization","url":"https://arxiv.org/abs/2502.14187","date":1740114000,"author":"","guid":8269,"unread":true,"content":"<article>arXiv:2502.14187v1 Announce Type: new \nAbstract: We evaluate Kahneman-Tversky Optimization (KTO) as a fine-tuning method for large language models (LLMs) in federated learning (FL) settings, comparing it against Direct Preference Optimization (DPO). Using Alpaca-7B as the base model, we fine-tune on a realistic dataset under both methods and evaluate performance using MT-Bench-1, Vicuna, and AdvBench benchmarks. Additionally, we introduce a redistributed dataset setup, where only KTO is applicable due to its ability to handle single-response feedback, unlike DPO's reliance on paired responses. Our results demonstrate that KTO, in both its original (KTOO) and redistributed (KTOR) configurations, consistently outperforms DPO across all benchmarks. In the redistributed setup, KTO further validates its flexibility and resilience by maintaining superior performance in scenarios where DPO cannot be applied. These findings establish KTO as a robust and scalable fine-tuning method for FL, motivating its adoption for privacy-preserving, decentralized, and heterogeneous environments.</article>","contentLength":1090,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"REFLEX Dataset: A Multimodal Dataset of Human Reactions to Robot Failures and Explanations","url":"https://arxiv.org/abs/2502.14185","date":1740114000,"author":"","guid":8270,"unread":true,"content":"<article>arXiv:2502.14185v1 Announce Type: new \nAbstract: This work presents REFLEX: Robotic Explanations to FaiLures and Human EXpressions, a comprehensive multimodal dataset capturing human reactions to robot failures and subsequent explanations in collaborative settings. It aims to facilitate research into human-robot interaction dynamics, addressing the need to study reactions to both initial failures and explanations, as well as the evolution of these reactions in long-term interactions. By providing rich, annotated data on human responses to different types of failures, explanation levels, and explanation varying strategies, the dataset contributes to the development of more robust, adaptive, and satisfying robotic systems capable of maintaining positive relationships with human collaborators, even during challenges like repeated failures.</article>","contentLength":848,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bayesian SegNet for Semantic Segmentation with Improved Interpretation of Microstructural Evolution During Irradiation of Materials","url":"https://arxiv.org/abs/2502.14184","date":1740114000,"author":"","guid":8271,"unread":true,"content":"<article>arXiv:2502.14184v1 Announce Type: new \nAbstract: Understanding the relationship between the evolution of microstructures of irradiated LiAlO2 pellets and tritium diffusion, retention and release could improve predictions of tritium-producing burnable absorber rod performance. Given expert-labeled segmented images of irradiated and unirradiated pellets, we trained Deep Convolutional Neural Networks to segment images into defect, grain, and boundary classes. Qualitative microstructural information was calculated from these segmented images to facilitate the comparison of unirradiated and irradiated pellets. We tested modifications to improve the sensitivity of the model, including incorporating meta-data into the model and utilizing uncertainty quantification. The predicted segmentation was similar to the expert-labeled segmentation for most methods of microstructural qualification, including pixel proportion, defect area, and defect density. Overall, the high performance metrics for the best models for both irradiated and unirradiated images shows that utilizing neural network models is a viable alternative to expert-labeled images.</article>","contentLength":1149,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate","url":"https://arxiv.org/abs/2502.14183","date":1740114000,"author":"","guid":8272,"unread":true,"content":"<article>arXiv:2502.14183v1 Announce Type: new \nAbstract: Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia). Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task. AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range. However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events. This gap highlights the need for proactive behavioral adjustments. We address this need with GLIMMER, Glucose Level Indicator Model with Modified Error Rate, a machine learning approach for forecasting blood glucose levels. GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical. To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (+/-3.77) and a mean absolute error (MAE) of 15.83 (+/-2.09) mg/dL. These results reflect a 23% improvement in RMSE and a 31% improvement in MAE compared to the best-reported error rates.</article>","contentLength":1605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Faceted Studies on Data Poisoning can Advance LLM Development","url":"https://arxiv.org/abs/2502.14182","date":1740114000,"author":"","guid":8273,"unread":true,"content":"<article>arXiv:2502.14182v1 Announce Type: new \nAbstract: The lifecycle of large language models (LLMs) is far more complex than that of traditional machine learning models, involving multiple training stages, diverse data sources, and varied inference methods. While prior research on data poisoning attacks has primarily focused on the safety vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection, rigorous data cleaning, and the multistage nature of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior as intended. Given these challenges, this position paper proposes rethinking the role of data poisoning and argue that multi-faceted studies on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate and address real safety risks to LLMs. From a trustworthiness perspective, data poisoning can be leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs, and hallucinations. Moreover, from a mechanism perspective, data poisoning can provide valuable insights into LLMs, particularly the interplay between data and model behavior, driving a deeper understanding of their underlying mechanisms.</article>","contentLength":1296,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems","url":"https://arxiv.org/abs/2502.14180","date":1740114000,"author":"","guid":8274,"unread":true,"content":"<article>arXiv:2502.14180v1 Announce Type: new \nAbstract: We present a method of generating first-order logic statements whose complexity can be controlled along multiple dimensions. We use this method to automatically create several datasets consisting of questions asking for the truth or falsity of first-order logic statements in Zermelo-Fraenkel set theory. While the resolution of these questions does not require any knowledge beyond basic notation of first-order logic and set theory, it does require a degree of planning and logical reasoning, which can be controlled up to arbitrarily high difficulty by the complexity of the generated statements. Furthermore, we do extensive evaluations of the performance of various large language models, including recent models such as DeepSeek-R1 and OpenAI's o3-mini, on these datasets. All of the datasets along with the code used for generating them, as well as all data from the evaluations is publicly available at https://github.com/bkuckuck/logical-skills-of-llms.</article>","contentLength":1011,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis","url":"https://arxiv.org/abs/2502.14178","date":1740114000,"author":"","guid":8275,"unread":true,"content":"<article>arXiv:2502.14178v1 Announce Type: new \nAbstract: Talking head synthesis is to synthesize a lip-synchronized talking head video using audio. Recently, the capability of NeRF to enhance the realism and texture details of synthesized talking heads has attracted the attention of researchers. However, most current NeRF methods based on audio are exclusively concerned with the rendering of frontal faces. These methods are unable to generate clear talking heads in novel views. Another prevalent challenge in current 3D talking head synthesis is the difficulty in aligning acoustic and visual spaces, which often results in suboptimal lip-syncing of the generated talking heads. To address these issues, we propose Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis (NeRF-3DTalker). Specifically, the proposed method employs 3D prior information to synthesize clear talking heads with free views. Additionally, we propose a 3D Prior Aided Audio Disentanglement module, which is designed to disentangle the audio into two distinct categories: features related to 3D awarded speech movements and features related to speaking style. Moreover, to reposition the generated frames that are distant from the speaker's motion space in the real space, we have devised a local-global Standardized Space. This method normalizes the irregular positions in the generated frames from both global and local semantic perspectives. Through comprehensive qualitative and quantitative experiments, it has been demonstrated that our NeRF-3DTalker outperforms state-of-the-art in synthesizing realistic talking head videos, exhibiting superior image quality and lip synchronization. Project page: https://nerf-3dtalker.github.io/NeRF-3Dtalker.</article>","contentLength":1759,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly","url":"https://arxiv.org/abs/2502.14177","date":1740114000,"author":"","guid":8276,"unread":true,"content":"<article>arXiv:2502.14177v1 Announce Type: new \nAbstract: In recent years, the Shapley value and SHAP explanations have emerged as one of the most dominant paradigms for providing post-hoc explanations of black-box models. Despite their well-founded theoretical properties, many recent works have focused on the limitations in both their computational efficiency and their representation power. The underlying connection with additive models, however, is left critically under-emphasized in the current literature. In this work, we find that a variational perspective linking GAM models and SHAP explanations is able to provide deep insights into nearly all recent developments. In light of this connection, we borrow in the other direction to develop a new method to train interpretable GAM models which are automatically purified to compute the Shapley value in a single forward pass. Finally, we provide theoretical results showing the limited representation power of GAM models is the same Achilles' heel existing in SHAP and discuss the implications for SHAP's modern usage in CV and NLP.</article>","contentLength":1084,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A modal logic translation of the AGM axioms for belief revision","url":"https://arxiv.org/abs/2502.14176","date":1740114000,"author":"","guid":8277,"unread":true,"content":"<article>arXiv:2502.14176v1 Announce Type: new \nAbstract: Building on the analysis of Bonanno (Artificial Intelligence, 2025) we introduce a simple modal logic containing three modal operators: a unimodal belief operator, a bimodal conditional operator and the unimodal global operator. For each AGM axiom for belief revision, we provide a corresponding modal axiom. The correspondence is as follows: each AGM axiom is characterized by a property of the Kripke-Lewis frames considered in Bonanno (Artificial Intelligence, 2025) and, in turn, that property characterizes the proposed modal axiom.</article>","contentLength":586,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction","url":"https://arxiv.org/abs/2502.14171","date":1740114000,"author":"","guid":8278,"unread":true,"content":"<article>arXiv:2502.14171v1 Announce Type: new \nAbstract: Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.</article>","contentLength":1100,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Blockchain-based Framework for Scalable and Incentivized Federated Learning","url":"https://arxiv.org/abs/2502.14170","date":1740114000,"author":"","guid":8279,"unread":true,"content":"<article>arXiv:2502.14170v1 Announce Type: new \nAbstract: Federated Learning (FL) enables collaborative model training without sharing raw data, preserving privacy while harnessing distributed datasets. However, traditional FL systems often rely on centralized aggregating mechanisms, introducing trust issues, single points of failure, and limited mechanisms for incentivizing meaningful client contributions. These challenges are exacerbated as FL scales to train resource-intensive models, such as large language models (LLMs), requiring scalable, decentralized solutions. This paper presents a blockchain-based FL framework that addresses these limitations by integrating smart contracts and a novel hybrid incentive mechanism. The framework automates critical FL tasks, including client registration, update validation, reward distribution, and maintaining a transparent global state. The hybrid incentive mechanism combines on-chain alignment-based rewards, off-chain fairness checks, and consistency multipliers to ensure fairness, transparency, and sustained engagement. We evaluate the framework through gas cost analysis, demonstrating its feasibility for different scales of federated learning scenarios.</article>","contentLength":1206,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Deep learning based infrared small object segmentation: Challenges and future directions","url":"https://arxiv.org/abs/2502.14168","date":1740114000,"author":"","guid":8280,"unread":true,"content":"<article>arXiv:2502.14168v1 Announce Type: new \nAbstract: Infrared sensing is a core method for supporting unmanned systems, such as autonomous vehicles and drones. Recently, infrared sensors have been widely deployed on mobile and stationary platforms for detection and classification of objects from long distances and in wide field of views. Given its success in the vision image analysis domain, deep learning has also been applied for object recognition in infrared images. However, techniques that have proven successful in visible light perception face new challenges in the infrared domain. These challenges include extremely low signal-to-noise ratios in infrared images, very small and blurred objects of interest, and limited availability of labeled/unlabeled training data due to the specialized nature of infrared sensors. Numerous methods have been proposed in the literature for the detection and classification of small objects in infrared images achieving varied levels of success. There is a need for a survey paper that critically analyzes existing techniques in this domain, identifies unsolved challenges and provides future research directions. This paper fills the gap and offers a concise and insightful review of deep learning-based methods. It also identifies the challenges faced by existing infrared object segmentation methods and provides a structured review of existing infrared perception methods from the perspective of these challenges and highlights the motivations behind the various approaches. Finally, this review suggests promising future directions based on recent advancements within this domain.</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"\"It Brought the Model to Life\": Exploring the Embodiment of Multimodal I3Ms for People who are Blind or have Low Vision","url":"https://arxiv.org/abs/2502.14163","date":1740114000,"author":"","guid":8281,"unread":true,"content":"<article>arXiv:2502.14163v1 Announce Type: new \nAbstract: 3D-printed models are increasingly used to provide people who are blind or have low vision (BLV) with access to maps, educational materials, and museum exhibits. Recent research has explored interactive 3D-printed models (I3Ms) that integrate touch gestures, conversational dialogue, and haptic vibratory feedback to create more engaging interfaces. Prior research with sighted people has found that imbuing machines with human-like behaviours, i.e., embodying them, can make them appear more lifelike, increasing social perception and presence. Such embodiment can increase engagement and trust. This work presents the first exploration into the design of embodied I3Ms and their impact on BLV engagement and trust. In a controlled study with 12 BLV participants, we found that I3Ms using specific embodiment design factors, such as haptic vibratory and embodied personified voices, led to an increased sense of liveliness and embodiment, as well as engagement, but had mixed impact on trust.</article>","contentLength":1042,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Structural Parameterizations for Induced and Acyclic Matching","url":"https://arxiv.org/abs/2502.14161","date":1740114000,"author":"","guid":8282,"unread":true,"content":"<article>arXiv:2502.14161v1 Announce Type: new \nAbstract: We revisit the (structurally) parameterized complexity of Induced Matching and Acyclic Matching, two problems where we seek to find a maximum independent set of edges whose endpoints induce, respectively, a matching and a forest. Chaudhary and Zehavi~[WG '23] recently studied these problems parameterized by treewidth, denoted by $\\mathrm{tw}$. We resolve several of the problems left open in their work and extend their results as follows: (i) for Acyclic Matching, Chaudhary and Zehavi gave an algorithm of running time $6^{\\mathrm{tw}}n^{\\mathcal{O}(1)}$ and a lower bound of $(3-\\varepsilon)^{\\mathrm{tw}}n^{\\mathcal{O}(1)}$ (under the SETH); we close this gap by, on the one hand giving a more careful analysis of their algorithm showing that its complexity is actually $5^{\\mathrm{tw}} n^{\\mathcal{O}(1)}$, and on the other giving a pw-SETH-based lower bound showing that this running time cannot be improved (even for pathwidth), (ii) for Induced Matching we show that their $3^{\\mathrm{tw}} n^{\\mathcal{O}(1)}$ algorithm is optimal under the pw-SETH (in fact improving over this for pathwidth is \\emph{equivalent} to falsifying the pw-SETH) by adapting a recent reduction for \\textsc{Bounded Degree Vertex Deletion}, (iii) for both problems we give FPT algorithms with single-exponential dependence when parameterized by clique-width and in particular for \\textsc{Induced Matching} our algorithm has running time $3^{\\mathrm{cw}} n^{\\mathcal{O}(1)}$, which is optimal under the pw-SETH from our previous result.</article>","contentLength":1569,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient Inverse Multiagent Learning","url":"https://arxiv.org/abs/2502.14160","date":1740114000,"author":"","guid":8283,"unread":true,"content":"<article>arXiv:2502.14160v1 Announce Type: new \nAbstract: In this paper, we study inverse game theory (resp. inverse multiagent learning) in which the goal is to find parameters of a game's payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as generative-adversarial (i.e., min-max) optimization problems, for which we develop polynomial-time algorithms to solve, the former of which relies on an exact first-order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent simulacral learning in polynomial time and number of samples. In these problems, we seek a simulacrum, meaning parameters and an associated equilibrium that replicate the given observations in expectation. We find that our approach outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data.</article>","contentLength":911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dual-level Mixup for Graph Few-shot Learning with Fewer Tasks","url":"https://arxiv.org/abs/2502.14158","date":1740114000,"author":"","guid":8284,"unread":true,"content":"<article>arXiv:2502.14158v1 Announce Type: new \nAbstract: Graph neural networks have been demonstrated as a powerful paradigm for effectively learning graph-structured data on the web and mining content from it.Current leading graph models require a large number of labeled samples for training, which unavoidably leads to overfitting in few-shot scenarios. Recent research has sought to alleviate this issue by simultaneously leveraging graph learning and meta-learning paradigms. However, these graph meta-learning models assume the availability of numerous meta-training tasks to learn transferable meta-knowledge. Such assumption may not be feasible in the real world due to the difficulty of constructing tasks and the substantial costs involved. Therefore, we propose a SiMple yet effectIve approach for graph few-shot Learning with fEwer tasks, named SMILE. We introduce a dual-level mixup strategy, encompassing both within-task and across-task mixup, to simultaneously enrich the available nodes and tasks in meta-learning. Moreover, we explicitly leverage the prior information provided by the node degrees in the graph to encode expressive node representations. Theoretically, we demonstrate that SMILE can enhance the model generalization ability. Empirically, SMILE consistently outperforms other competitive models by a large margin across all evaluated datasets with in-domain and cross-domain settings. Our anonymous code can be found here.</article>","contentLength":1447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mixed Signals: A Diverse Point Cloud Dataset for Heterogeneous LiDAR V2X Collaboration","url":"https://arxiv.org/abs/2502.14156","date":1740114000,"author":"","guid":8285,"unread":true,"content":"<article>arXiv:2502.14156v1 Announce Type: new \nAbstract: Vehicle-to-everything (V2X) collaborative perception has emerged as a promising solution to address the limitations of single-vehicle perception systems. However, existing V2X datasets are limited in scope, diversity, and quality. To address these gaps, we present Mixed Signals, a comprehensive V2X dataset featuring 45.1k point clouds and 240.6k bounding boxes collected from three connected autonomous vehicles (CAVs) equipped with two different types of LiDAR sensors, plus a roadside unit with dual LiDARs. Our dataset provides precisely aligned point clouds and bounding box annotations across 10 classes, ensuring reliable data for perception training. We provide detailed statistical analysis on the quality of our dataset and extensively benchmark existing V2X methods on it. Mixed Signals V2X Dataset is one of the highest quality, large-scale datasets publicly available for V2X perception research. Details on the website https://mixedsignalsdataset.cs.cornell.edu/.</article>","contentLength":1027,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Giving AI Personalities Leads to More Human-Like Reasoning","url":"https://arxiv.org/abs/2502.14155","date":1740114000,"author":"","guid":8286,"unread":true,"content":"<article>arXiv:2502.14155v1 Announce Type: new \nAbstract: In computational cognitive modeling, capturing the full spectrum of human judgment and decision-making processes, beyond just optimal behaviors, is a significant challenge. This study explores whether Large Language Models (LLMs) can emulate the breadth of human reasoning by predicting both intuitive, fast System 1 and deliberate, slow System 2 processes. We investigate the potential of AI to mimic diverse reasoning behaviors across a human population, addressing what we call the {\\em full reasoning spectrum problem}. We designed reasoning tasks using a novel generalization of the Natural Language Inference (NLI) format to evaluate LLMs' ability to replicate human reasoning. The questions were crafted to elicit both System 1 and System 2 responses. Human responses were collected through crowd-sourcing and the entire distribution was modeled, rather than just the majority of the answers. We used personality-based prompting inspired by the Big Five personality model to elicit AI responses reflecting specific personality traits, capturing the diversity of human reasoning, and exploring how personality traits influence LLM outputs. Combined with genetic algorithms to optimize the weighting of these prompts, this method was tested alongside traditional machine learning models. The results show that LLMs can mimic human response distributions, with open-source models like Llama and Mistral outperforming proprietary GPT models. Personality-based prompting, especially when optimized with genetic algorithms, significantly enhanced LLMs' ability to predict human response distributions, suggesting that capturing suboptimal, naturalistic reasoning may require modeling techniques incorporating diverse reasoning styles and psychological profiles. The study concludes that personality-based prompting combined with genetic algorithms is promising for enhancing AI's \\textit{human-ness} in reasoning.</article>","contentLength":1963,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Retraction maps: A seed of geometric integrators. Part II: Symmetry and reduction","url":"https://arxiv.org/abs/2502.14152","date":1740114000,"author":"","guid":8287,"unread":true,"content":"<article>arXiv:2502.14152v1 Announce Type: new \nAbstract: In this paper we use retraction and discretization maps (see [Barbero Li\\~n\\'an and Mart\\'in de Diego, 2022]) as a tool for deriving in a systematic way numerical integrators preserving geometric structures (such as symplecticity or Lie-Poisson structure), as well as methods that preserve symmetry and the associated discrete momentum map. The classical notion of a retraction map leads to the notion of discretization map extended here to the Lie algebroid of a Lie groupoid so that the configuration manifold is discretized, instead of the equations of motion. As a consequence, geometric integrators are obtained preserving the Lie-Poisson structure of the corresponding reduced system.</article>","contentLength":739,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Risk-Sensitive Security-Constrained Economic Dispatch: Pricing and Algorithm Design","url":"https://arxiv.org/abs/2502.14150","date":1740114000,"author":"","guid":8288,"unread":true,"content":"<article>arXiv:2502.14150v1 Announce Type: new \nAbstract: We propose a risk-sensitive security-constrained economic dispatch (R-SCED) formulation capturing the tradeoff between dispatch cost and resilience against potential line failures, where risk is modeled via the conditional value at risk (CVaR). In the context of our formulation, we analyze revenue adequacy and side payments of two pricing models, one based on nominal generation costs, and another based on total marginal cost including contingencies. In particular, we prove that the system operator's (SO) merchandising surplus (MS) and total revenue are nonnegative under the latter, while under the former the same does not hold in general. We demonstrate that the proposed R-SCED formulation is amenable to decomposition and describe a Benders' decomposition algorithm to solve it. In numerical examples, we illustrate the differences in MS and total revenue under the considered pricing schemes, and the computational efficiency of our decomposition approach.</article>","contentLength":1016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual Question Answering in Pituitary Surgery","url":"https://arxiv.org/abs/2502.14149","date":1740114000,"author":"","guid":8289,"unread":true,"content":"<article>arXiv:2502.14149v1 Announce Type: new \nAbstract: Vision-Language Models (VLMs) in visual question answering (VQA) offer a unique opportunity to enhance intra-operative decision-making, promote intuitive interactions, and significantly advancing surgical education. However, the development of VLMs for surgical VQA is challenging due to limited datasets and the risk of overfitting and catastrophic forgetting during full fine-tuning of pretrained weights. While parameter-efficient techniques like Low-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MoRA) address adaptation challenges, their uniform parameter distribution overlooks the feature hierarchy in deep networks, where earlier layers, that learn general features, require more parameters than later ones. This work introduces PitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank adaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting GPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames from 25 procedural videos with 745,972 question-answer sentence pairs, covering key surgical elements such as phase and step recognition, context understanding, tool detection, localization, and interactions recognition. Vector-MoLoRA incorporates the principles of LoRA and MoRA to develop a matrix-low-rank adaptation strategy that employs vector ranking to allocate more parameters to earlier layers, gradually reducing them in the later layers. Our approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets, effectively mitigates catastrophic forgetting while significantly enhancing performance over recent baselines. Furthermore, our risk-coverage analysis highlights its enhanced reliability and trustworthiness in handling uncertain predictions. Our source code and dataset is available at~\\url{https://github.com/HRL-Mike/PitVQA-Plus}.</article>","contentLength":1880,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning the P2D Model for Lithium-Ion Batteries with SOH Detection","url":"https://arxiv.org/abs/2502.14147","date":1740114000,"author":"","guid":8290,"unread":true,"content":"<article>arXiv:2502.14147v1 Announce Type: new \nAbstract: Lithium ion batteries are widely used in many applications. Battery management systems control their optimal use and charging and predict when the battery will cease to deliver the required output on a planned duty or driving cycle. Such systems use a simulation of a mathematical model of battery performance. These models can be electrochemical or data-driven. Electrochemical models for batteries running at high currents are mathematically and computationally complex. In this work, we show that a well-regarded electrochemical model, the Pseudo Two Dimensional (P2D) model, can be replaced by a computationally efficient Convolutional Neural Network (CNN) surrogate model fit to accurately simulated data from a class of random driving cycles. We demonstrate that a CNN is an ideal choice for accurately capturing Lithium ion concentration profiles. Additionally, we show how the neural network model can be adjusted to correspond to battery changes in State of Health (SOH).</article>","contentLength":1029,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Efficient and Optimal Policy Gradient Algorithm for Corrupted Multi-armed Bandits","url":"https://arxiv.org/abs/2502.14146","date":1740114000,"author":"","guid":8291,"unread":true,"content":"<article>arXiv:2502.14146v1 Announce Type: new \nAbstract: In this paper, we consider the stochastic multi-armed bandits problem with adversarial corruptions, where the random rewards of the arms are partially modified by an adversary to fool the algorithm. We apply the policy gradient algorithm SAMBA to this setting, and show that it is computationally efficient, and achieves a state-of-the-art $O(K\\log T/\\Delta) + O(C/\\Delta)$ regret upper bound, where $K$ is the number of arms, $C$ is the unknown corruption level, $\\Delta$ is the minimum expected reward gap between the best arm and other ones, and $T$ is the time horizon. Compared with the best existing efficient algorithm (e.g., CBARBAR), whose regret upper bound is $O(K\\log^2 T/\\Delta) + O(C)$, we show that SAMBA reduces one $\\log T$ factor in the regret bound, while maintaining the corruption-dependent term to be linear with $C$. This is indeed asymptotically optimal. We also conduct simulations to demonstrate the effectiveness of SAMBA, and the results show that SAMBA outperforms existing baselines.</article>","contentLength":1062,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLM-Enhanced Dialogue Management for Full-Duplex Spoken Dialogue Systems","url":"https://arxiv.org/abs/2502.14145","date":1740114000,"author":"","guid":8292,"unread":true,"content":"<article>arXiv:2502.14145v1 Announce Type: new \nAbstract: Achieving full-duplex communication in spoken dialogue systems (SDS) requires real-time coordination between listening, speaking, and thinking. This paper proposes a semantic voice activity detection (VAD) module as a dialogue manager (DM) to efficiently manage turn-taking in full-duplex SDS. Implemented as a lightweight (0.5B) LLM fine-tuned on full-duplex conversation data, the semantic VAD predicts four control tokens to regulate turn-switching and turn-keeping, distinguishing between intentional and unintentional barge-ins while detecting query completion for handling user pauses and hesitations. By processing input speech in short intervals, the semantic VAD enables real-time decision-making, while the core dialogue engine (CDE) is only activated for response generation, reducing computational overhead. This design allows independent DM optimization without retraining the CDE, balancing interaction accuracy and inference efficiency for scalable, next-generation full-duplex SDS.</article>","contentLength":1046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"UM_FHS at TREC 2024 PLABA: Exploration of Fine-tuning and AI agent approach for plain language adaptations of biomedical text","url":"https://arxiv.org/abs/2502.14144","date":1740114000,"author":"","guid":8293,"unread":true,"content":"<article>arXiv:2502.14144v1 Announce Type: new \nAbstract: This paper describes our submissions to the TREC 2024 PLABA track with the aim to simplify biomedical abstracts for a K8-level audience (13-14 years old students). We tested three approaches using OpenAI's gpt-4o and gpt-4o-mini models: baseline prompt engineering, a two-AI agent approach, and fine-tuning. Adaptations were evaluated using qualitative metrics (5-point Likert scales for simplicity, accuracy, completeness, and brevity) and quantitative readability scores (Flesch-Kincaid grade level, SMOG Index). Results indicated that the two-agent approach and baseline prompt engineering with gpt-4o-mini models show superior qualitative performance, while fine-tuned models excelled in accuracy and completeness but were less simple. The evaluation results demonstrated that prompt engineering with gpt-4o-mini outperforms iterative improvement strategies via two-agent approach as well as fine-tuning with gpt-4o. We intend to expand our investigation of the results and explore advanced evaluations.</article>","contentLength":1056,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Multi-Agent Risks from Advanced AI","url":"https://arxiv.org/abs/2502.14143","date":1740114000,"author":"","guid":8294,"unread":true,"content":"<article>arXiv:2502.14143v1 Announce Type: new \nAbstract: The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents' incentives, as well as seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment problems, emergent agency, and multi-agent security) that can underpin them. We highlight several important instances of each risk, as well as promising directions to help mitigate them. By anchoring our analysis in a range of real-world examples and experimental evidence, we illustrate the distinct challenges posed by multi-agent systems and their implications for the safety, governance, and ethics of advanced AI.</article>","contentLength":999,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Token Adaptation via Side Graph Convolution for Temporally and Spatially Efficient Fine-tuning of 3D Point Cloud Transformers","url":"https://arxiv.org/abs/2502.14142","date":1740114000,"author":"","guid":8295,"unread":true,"content":"<article>arXiv:2502.14142v1 Announce Type: new \nAbstract: Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloud Transformers has emerged as a promising technique for 3D point cloud analysis. While existing PEFT methods attempt to minimize the number of tunable parameters, they still suffer from high temporal and spatial computational costs during fine-tuning. This paper proposes a novel PEFT algorithm for 3D point cloud Transformers, called Side Token Adaptation on a neighborhood Graph (STAG), to achieve superior temporal and spatial efficiency. STAG employs a graph convolutional side network that operates in parallel with a frozen backbone Transformer to adapt tokens to downstream tasks. STAG's side network realizes high efficiency through three key components: connection with the backbone that enables reduced gradient computation, parameter sharing framework, and efficient graph convolution. Furthermore, we present Point Cloud Classification 13 (PCC13), a new benchmark comprising diverse publicly available 3D point cloud datasets, enabling comprehensive evaluation of PEFT methods. Extensive experiments using multiple pre-trained models and PCC13 demonstrates the effectiveness of STAG. Specifically, STAG maintains classification accuracy comparable to existing methods while reducing tunable parameters to only 0.43M and achieving significant reductions in both computational time and memory consumption for fine-tuning. Code and benchmark will be available at: https://github.com/takahikof/STAG</article>","contentLength":1522,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ModSkill: Physical Character Skill Modularization","url":"https://arxiv.org/abs/2502.14140","date":1740114000,"author":"","guid":8296,"unread":true,"content":"<article>arXiv:2502.14140v1 Announce Type: new \nAbstract: Human motion is highly diverse and dynamic, posing challenges for imitation learning algorithms that aim to generalize motor skills for controlling simulated characters. Previous methods typically rely on a universal full-body controller for tracking reference motion (tracking-based model) or a unified full-body skill embedding space (skill embedding). However, these approaches often struggle to generalize and scale to larger motion datasets. In this work, we introduce a novel skill learning framework, ModSkill, that decouples complex full-body skills into compositional, modular skills for independent body parts. Our framework features a skill modularization attention layer that processes policy observations into modular skill embeddings that guide low-level controllers for each body part. We also propose an Active Skill Learning approach with Generative Adaptive Sampling, using large motion generation models to adaptively enhance policy learning in challenging tracking scenarios. Our results show that this modularized skill learning framework, enhanced by generative sampling, outperforms existing methods in precise full-body motion tracking and enables reusable skill embeddings for diverse goal-driven tasks.</article>","contentLength":1277,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Collaborative Retrieval for Large Language Model-based Conversational Recommender Systems","url":"https://arxiv.org/abs/2502.14137","date":1740114000,"author":"","guid":8297,"unread":true,"content":"<article>arXiv:2502.14137v1 Announce Type: new \nAbstract: Conversational recommender systems (CRS) aim to provide personalized recommendations via interactive dialogues with users. While large language models (LLMs) enhance CRS with their superior understanding of context-aware user preferences, they typically struggle to leverage behavioral data, which have proven to be important for classical collaborative filtering (CF)-based approaches. For this reason, we propose CRAG, Collaborative Retrieval Augmented Generation for LLM-based CRS. To the best of our knowledge, CRAG is the first approach that combines state-of-the-art LLMs with CF for conversational recommendations. Our experiments on two publicly available movie conversational recommendation datasets, i.e., a refined Reddit dataset (which we name Reddit-v2) as well as the Redial dataset, demonstrate the superior item coverage and recommendation performance of CRAG, compared to several CRS baselines. Moreover, we observe that the improvements are mainly due to better recommendation accuracy on recently released movies. The code and data are available at https://github.com/yaochenzhu/CRAG.</article>","contentLength":1152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cluster Analysis and Concept Drift Detection in Malware","url":"https://arxiv.org/abs/2502.14135","date":1740114000,"author":"","guid":8298,"unread":true,"content":"<article>arXiv:2502.14135v1 Announce Type: new \nAbstract: Concept drift refers to gradual or sudden changes in the properties of data that affect the accuracy of machine learning models. In this paper, we address the problem of concept drift detection in the malware domain. Specifically, we propose and analyze a clustering-based approach to detecting concept drift. Using a subset of the KronoDroid dataset, malware samples are partitioned into temporal batches and analyzed using MiniBatch $K$-Means clustering. The silhouette coefficient is used as a metric to identify points in time where concept drift has likely occurred. To verify our drift detection results, we train learning models under three realistic scenarios, which we refer to as static training, periodic retraining, and drift-aware retraining. In each scenario, we consider four supervised classifiers, namely, Multilayer Perceptron (MLP), Support Vector Machine (SVM), Random Forest, and XGBoost. Experimental results demonstrate that drift-aware retraining guided by silhouette coefficient thresholding achieves classification accuracy far superior to static models, and generally within 1% of periodic retraining, while also being far more efficient than periodic retraining. These results provide strong evidence that our clustering-based approach is effective at detecting concept drift, while also illustrating a highly practical and efficient fully automated approach to improved malware classification via concept drift detection.</article>","contentLength":1499,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Self-Regularization with Latent Space Explanations for Controllable LLM-based Classification","url":"https://arxiv.org/abs/2502.14133","date":1740114000,"author":"","guid":8299,"unread":true,"content":"<article>arXiv:2502.14133v1 Announce Type: new \nAbstract: Modern text classification methods heavily rely on contextual embeddings from large language models (LLMs). Compared to human-engineered features, these embeddings provide automatic and effective representations for classification model training. However, they also introduce a challenge: we lose the ability to manually remove unintended features, such as sensitive or task-irrelevant features, to guarantee regulatory compliance or improve the generalizability of classification models. This limitation arises because LLM embeddings are opaque and difficult to interpret. In this paper, we propose a novel framework to identify and regularize unintended features in the LLM latent space. Specifically, we first pre-train a sparse autoencoder (SAE) to extract interpretable features from LLM latent spaces. To ensure the SAE can capture task-specific features, we further fine-tune it on task-specific datasets. In training the classification model, we propose a simple and effective regularizer, by minimizing the similarity between the classifier weights and the identified unintended feature, to remove the impacts of these unintended features toward classification. We evaluate the proposed framework on three real-world tasks, including toxic chat detection, reward modeling, and disease diagnosis. Results show that the proposed framework can significantly improve the classifier's generalizability by regularizing those features that are not semantically correlated to each task. This work pioneers controllable text classification on LLM latent spaces by leveraging interpreted features to address generalizability, fairness, and privacy challenges. We will release our code and data once accepted.</article>","contentLength":1756,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can Community Notes Replace Professional Fact-Checkers?","url":"https://arxiv.org/abs/2502.14132","date":1740114000,"author":"","guid":8300,"unread":true,"content":"<article>arXiv:2502.14132v1 Announce Type: new \nAbstract: Two commonly-employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. In conclusion, our results show that successful community moderation heavily relies on professional fact-checking.</article>","contentLength":1172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Gradients can train reward models: An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model","url":"https://arxiv.org/abs/2502.14131","date":1740114000,"author":"","guid":8301,"unread":true,"content":"<article>arXiv:2502.14131v1 Announce Type: new \nAbstract: We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or $Q^*$ functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-Lojasiewicz (PL) condition -- a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.</article>","contentLength":1344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FILO -- automated unification in $\\mathcal{FL}_0$","url":"https://arxiv.org/abs/2502.14130","date":1740114000,"author":"","guid":8302,"unread":true,"content":"<article>arXiv:2502.14130v1 Announce Type: new \nAbstract: FILO is a java application that decides unifiability for a unification problem formulated in the description logic $\\mathcal{FL}_0$. If the problem is unifiable, it presents a user with an example of a solution. FILO joins a family of similar applications like UEL solving unification problems in the description logic $\\mathcal{EL}$, $\\mathcal{FL}_0$wer a subsumption decider for $\\mathcal{FL}_0$ with TBox, CEL and JCEL subsumption deciders for $\\mathcal{EL}$ with TBox, and others. These systems play an important role in various knowledge representation reasoning problems.</article>","contentLength":626,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GlossGau: Efficient Inverse Rendering for Glossy Surface with Anisotropic Spherical Gaussian","url":"https://arxiv.org/abs/2502.14129","date":1740114000,"author":"","guid":8303,"unread":true,"content":"<article>arXiv:2502.14129v1 Announce Type: new \nAbstract: The reconstruction of 3D objects from calibrated photographs represents a fundamental yet intricate challenge in the domains of computer graphics and vision. Although neural reconstruction approaches based on Neural Radiance Fields (NeRF) have shown remarkable capabilities, their processing costs remain substantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely improves the training efficiency and facilitates to generate realistic rendering in real-time. However, due to the limited ability of Spherical Harmonics (SH) to represent high-frequency information, 3D-GS falls short in reconstructing glossy objects. Researchers have turned to enhance the specular expressiveness of 3D-GS through inverse rendering. Yet these methods often struggle to maintain the training and rendering efficiency, undermining the benefits of Gaussian Splatting techniques. In this paper, we introduce GlossGau, an efficient inverse rendering framework that reconstructs scenes with glossy surfaces while maintaining training and rendering speeds comparable to vanilla 3D-GS. Specifically, we explicitly model the surface normals, Bidirectional Reflectance Distribution Function (BRDF) parameters, as well as incident lights and use Anisotropic Spherical Gaussian (ASG) to approximate the per-Gaussian Normal Distribution Function under the microfacet model. We utilize 2D Gaussian Splatting (2D-GS) as foundational primitives and apply regularization to significantly alleviate the normal estimation challenge encountered in related works. Experiments demonstrate that GlossGau achieves competitive or superior reconstruction on datasets with glossy surfaces. Compared with previous GS-based works that address the specular surface, our optimization time is considerably less.</article>","contentLength":1826,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above","url":"https://arxiv.org/abs/2502.14127","date":1740114000,"author":"","guid":8304,"unread":true,"content":"<article>arXiv:2502.14127v1 Announce Type: new \nAbstract: Multiple choice question answering (MCQA) is popular for LLM evaluation due to its simplicity and human-like testing, but we argue for its reform. We first reveal flaws in MCQA's format, as it struggles to: 1) test generation/subjectivity; 2) match LLM use cases; and 3) fully test knowledge. We instead advocate for generative formats based on human testing-where LLMs construct and explain answers-better capturing user needs and knowledge while remaining easy to score. We then show even when MCQA is a useful format, its datasets suffer from: leakage; unanswerability; shortcuts; and saturation. In each issue, we give fixes from education, like rubrics to guide MCQ writing; scoring methods to bridle guessing; and Item Response Theory to build harder MCQs. Lastly, we discuss LLM errors in MCQA-robustness, biases, and unfaithful explanations-showing how our prior solutions better measure or address these issues. While we do not need to desert MCQA, we encourage more efforts in refining the task based on educational testing, advancing evaluations.</article>","contentLength":1106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Modular Prompt Learning Improves Vision-Language Models","url":"https://arxiv.org/abs/2502.14125","date":1740114000,"author":"","guid":8305,"unread":true,"content":"<article>arXiv:2502.14125v1 Announce Type: new \nAbstract: Pre-trained vision-language models are able to interpret visual concepts and language semantics. Prompt learning, a method of constructing prompts for text encoders or image encoders, elicits the potentials of pre-trained models and readily adapts them to new scenarios. Compared to fine-tuning, prompt learning enables the model to achieve comparable or better performance using fewer trainable parameters. Besides, prompt learning freezes the pre-trained model and avoids the catastrophic forgetting issue in the fine-tuning. Continuous prompts inserted into the input of every transformer layer (i.e. deep prompts) can improve the performances of pre-trained models on downstream tasks. For i-th transformer layer, the inserted prompts replace previously inserted prompts in the $(i-1)$-th layer. Although the self-attention mechanism contextualizes newly inserted prompts for the current layer and embeddings from the previous layer's output, removing all inserted prompts from the previous layer inevitably loses information contained in the continuous prompts. In this work, we propose Modular Prompt Learning (MPL) that is designed to promote the preservation of information contained in the inserted prompts. We evaluate the proposed method on base-to-new generalization and cross-dataset tasks. On average of 11 datasets, our method achieves 0.7% performance gain on the base-to-new generalization task compared to the state-of-the-art method. The largest improvement on the individual dataset is 10.7% (EuroSAT dataset).</article>","contentLength":1579,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding SGD with Exponential Moving Average: A Case Study in Linear Regression","url":"https://arxiv.org/abs/2502.14123","date":1740114000,"author":"","guid":8306,"unread":true,"content":"<article>arXiv:2502.14123v1 Announce Type: new \nAbstract: Exponential moving average (EMA) has recently gained significant popularity in training modern deep learning models, especially diffusion-based generative models. However, there have been few theoretical results explaining the effectiveness of EMA. In this paper, to better understand EMA, we establish the risk bound of online SGD with EMA for high-dimensional linear regression, one of the simplest overparameterized learning tasks that shares similarities with neural networks. Our results indicate that (i) the variance error of SGD with EMA is always smaller than that of SGD without averaging, and (ii) unlike SGD with iterate averaging from the beginning, the bias error of SGD with EMA decays exponentially in every eigen-subspace of the data covariance matrix. Additionally, we develop proof techniques applicable to the analysis of a broad class of averaging schemes.</article>","contentLength":926,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Benchmarking LLMs for Political Science: A United Nations Perspective","url":"https://arxiv.org/abs/2502.14122","date":1740114000,"author":"","guid":8307,"unread":true,"content":"<article>arXiv:2502.14122v1 Announce Type: new \nAbstract: Large Language Models (LLMs) have achieved significant advances in natural language processing, yet their potential for high-stake political decision-making remains largely unexplored. This paper addresses the gap by focusing on the application of LLMs to the United Nations (UN) decision-making process, where the stakes are particularly high and political decisions can have far-reaching consequences. We introduce a novel dataset comprising publicly available UN Security Council (UNSC) records from 1994 to 2024, including draft resolutions, voting records, and diplomatic speeches. Using this dataset, we propose the United Nations Benchmark (UNBench), the first comprehensive benchmark designed to evaluate LLMs across four interconnected political science tasks: co-penholder judgment, representative voting simulation, draft adoption prediction, and representative statement generation. These tasks span the three stages of the UN decision-making process--drafting, voting, and discussing--and aim to assess LLMs' ability to understand and simulate political dynamics. Our experimental analysis demonstrates the potential and challenges of applying LLMs in this domain, providing insights into their strengths and limitations in political science. This work contributes to the growing intersection of AI and political science, opening new avenues for research and practical applications in global governance. The UNBench Repository can be accessed at: https://github.com/yueqingliang1/UNBench.</article>","contentLength":1550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Supervised Machine-Learning Approach For Turboshaft Engine Dynamic Modeling Under Real Flight Conditions","url":"https://arxiv.org/abs/2502.14120","date":1740114000,"author":"","guid":8308,"unread":true,"content":"<article>arXiv:2502.14120v1 Announce Type: new \nAbstract: Rotorcraft engines are highly complex, nonlinear thermodynamic systems that operate under varying environmental and flight conditions. Simulating their dynamics is crucial for design, fault diagnostics, and deterioration control phases, and requires robust and reliable control systems to estimate engine performance throughout flight envelope. However, the development of detailed physical models of the engine based on numerical simulations is a very challenging task due to the complex and entangled physics driving the engine. In this scenario, data-driven machine-learning techniques are of great interest to the aircraft engine community, due to their ability to describe nonlinear systems' dynamic behavior and enable online performance estimation, achieving excellent results with accuracy competitive with the state of the art. In this work, we explore different Neural Network architectures to model the turboshaft engine of Leonardo's AW189P4 prototype, aiming to predict the engine torque. The models are trained on an extensive database of real flight tests featuring a variety of operational maneuvers performed under different flight conditions, providing a comprehensive representation of the engine's performance. To complement the neural network approach, we apply Sparse Identification of Nonlinear Dynamics (SINDy) to derive a low-dimensional dynamical model from the available data, describing the relationship between fuel flow and engine torque. The resulting model showcases SINDy's capability to recover the actual physics underlying the engine dynamics and demonstrates its potential for investigating more complex aspects of the engine. The results prove that data-driven engine models can exploit a wider range of parameters than standard transfer function-based approaches, enabling the use of trained schemes to simulate nonlinear effects in different engines and helicopters.</article>","contentLength":1955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meaning Beyond Truth Conditions: Evaluating Discourse Level Understanding via Anaphora Accessibility","url":"https://arxiv.org/abs/2502.14119","date":1740114000,"author":"","guid":8309,"unread":true,"content":"<article>arXiv:2502.14119v1 Announce Type: new \nAbstract: We present a hierarchy of natural language understanding abilities and argue for the importance of moving beyond assessments of understanding at the lexical and sentence levels to the discourse level. We propose the task of anaphora accessibility as a diagnostic for assessing discourse understanding, and to this end, present an evaluation dataset inspired by theoretical research in dynamic semantics. We evaluate human and LLM performance on our dataset and find that LLMs and humans align on some tasks and diverge on others. Such divergence can be explained by LLMs' reliance on specific lexical items during language comprehension, in contrast to human sensitivity to structural abstractions.</article>","contentLength":747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SALTY: Explainable Artificial Intelligence Guided Structural Analysis for Hardware Trojan Detection","url":"https://arxiv.org/abs/2502.14116","date":1740114000,"author":"","guid":8310,"unread":true,"content":"<article>arXiv:2502.14116v1 Announce Type: new \nAbstract: Hardware Trojans are malicious modifications in digital designs that can be inserted by untrusted supply chain entities. Hardware Trojans can give rise to diverse attack vectors such as information leakage (e.g. MOLES Trojan) and denial-of-service (rarely triggered bit flip). Such an attack in critical systems (e.g. healthcare and aviation) can endanger human lives and lead to catastrophic financial loss. Several techniques have been developed to detect such malicious modifications in digital designs, particularly for designs sourced from third-party intellectual property (IP) vendors. However, most techniques have scalability concerns (due to unsound assumptions during evaluation) and lead to large number of false positive detections (false alerts). Our framework (SALTY) mitigates these concerns through the use of a novel Graph Neural Network architecture (using Jumping-Knowledge mechanism) for generating initial predictions and an Explainable Artificial Intelligence (XAI) approach for fine tuning the outcomes (post-processing). Experiments show 98% True Positive Rate (TPR) and True Negative Rate (TNR), significantly outperforming state-of-the-art techniques across a large set of standard benchmarks.</article>","contentLength":1269,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chasing the Timber Trail: Machine Learning to Reveal Harvest Location Misrepresentation","url":"https://arxiv.org/abs/2502.14115","date":1740114000,"author":"","guid":8311,"unread":true,"content":"<article>arXiv:2502.14115v1 Announce Type: new \nAbstract: Illegal logging poses a significant threat to global biodiversity, climate stability, and depresses international prices for legal wood harvesting and responsible forest products trade, affecting livelihoods and communities across the globe. Stable isotope ratio analysis (SIRA) is rapidly becoming an important tool for determining the harvest location of traded, organic, products. The spatial pattern in stable isotope ratio values depends on factors such as atmospheric and environmental conditions and can thus be used for geographical identification. We present here the results of a deployed machine learning pipeline where we leverage both isotope values and atmospheric variables to determine timber harvest location. Additionally, the pipeline incorporates uncertainty estimation to facilitate the interpretation of harvest location determination for analysts. We present our experiments on a collection of oak (Quercus spp.) tree samples from its global range. Our pipeline outperforms comparable state-of-the-art models determining geographic harvest origin of commercially traded wood products, and has been used by European enforcement agencies to identify illicit Russian and Belarusian timber entering the EU market. We also identify opportunities for further advancement of our framework and how it can be generalized to help identify the origin of falsely labeled organic products throughout the supply chain.</article>","contentLength":1476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Zero loss guarantees and explicit minimizers for generic overparametrized Deep Learning networks","url":"https://arxiv.org/abs/2502.14114","date":1740114000,"author":"","guid":8312,"unread":true,"content":"<article>arXiv:2502.14114v1 Announce Type: new \nAbstract: We determine sufficient conditions for overparametrized deep learning (DL) networks to guarantee the attainability of zero loss in the context of supervised learning, for the $\\mathcal{L}^2$ cost and {\\em generic} training data. We present an explicit construction of the zero loss minimizers without invoking gradient descent. On the other hand, we point out that increase of depth can deteriorate the efficiency of cost minimization using a gradient descent algorithm by analyzing the conditions for rank loss of the training Jacobian. Our results clarify key aspects on the dichotomy between zero loss reachability in underparametrized versus overparametrized DL.</article>","contentLength":715,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Object-centric Binding in Contrastive Language-Image Pretraining","url":"https://arxiv.org/abs/2502.14113","date":1740114000,"author":"","guid":8313,"unread":true,"content":"<article>arXiv:2502.14113v1 Announce Type: new \nAbstract: Recent advances in vision language models (VLM) have been driven by contrastive models such as CLIP, which learn to associate visual information with their corresponding text descriptions. However, these models have limitations in understanding complex compositional scenes involving multiple objects and their spatial relationships. To address these challenges, we propose a novel approach that diverges from commonly used strategies, which rely on the design of hard-negative augmentations. Instead, our work focuses on integrating inductive biases into pre-trained CLIP-like models to improve their compositional understanding without using any additional hard-negatives. To that end, we introduce a binding module that connects a scene graph, derived from a text description, with a slot-structured image representation, facilitating a structured similarity assessment between the two modalities. We also leverage relationships as text-conditioned visual constraints, thereby capturing the intricate interactions between objects and their contextual relationships more effectively. Our resulting model not only enhances the performance of CLIP-based models in multi-object compositional understanding but also paves the way towards more accurate and sample-efficient image-text matching of complex scenes.</article>","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"To Stand on the Shoulders of Giants: Should We Protect Initial Discoveries in Multi-Agent Exploration?","url":"https://arxiv.org/abs/2502.14112","date":1740114000,"author":"","guid":8314,"unread":true,"content":"<article>arXiv:2502.14112v1 Announce Type: new \nAbstract: Exploring new ideas is a fundamental aspect of research and development (R\\&amp;D), which often occurs in competitive environments. Most ideas are subsequent, i.e. one idea today leads to more ideas tomorrow. According to one approach, the best way to encourage exploration is by granting protection on discoveries to the first innovator. Correspondingly, only the one who made the first discovery can use the new knowledge and benefit from subsequent discoveries, which in turn should increase the initial motivation to explore. An alternative approach to promote exploration favors the \\emph{sharing of knowledge} from discoveries among researchers allowing explorers to use each others' discoveries to develop further knowledge, as in the open-source community. With no protection, all explorers have access to all existing discoveries and new directions are explored faster.\n  We present a game theoretic analysis of an abstract research-and-application game which clarifies the expected advantages and disadvantages of the two approaches under full information. We then compare the theoretical predictions with the observed behavior of actual players in the lab who operate under partial information conditions in both worlds.\n  Our main experimental finding is that the no protection approach leads to \\emph{more} investment efforts overall, in contrast to theoretical prediction and common economic wisdom, but in line with a familiar cognitive bias known as `underweighting of rare events'.</article>","contentLength":1543,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Comprehensive Review on the Control of Heat Pumps for Energy Flexibility in Distribution Networks","url":"https://arxiv.org/abs/2502.14111","date":1740114000,"author":"","guid":8315,"unread":true,"content":"<article>arXiv:2502.14111v1 Announce Type: new \nAbstract: Decarbonization plans promote the transition to heat pumps (HPs), creating new opportunities for their energy flexibility in demand response programs, solar photovoltaic integration and optimization of distribution networks. This paper reviews scheduling-based and real-time optimization methods for controlling HPs with a focus on energy flexibility in distribution networks. Scheduling-based methods fall into two categories: rule-based controllers (RBCs), which rely on predefined control rules without explicitly seeking optimal solutions, and optimization models, which are designed to determine the optimal scheduling of operations. Real-time optimization is achieved through model predictive control (MPC), which relies on a predictive model to optimize decisions over a time horizon, and reinforcement learning (RL), which takes a model-free approach by learning optimal strategies through direct interaction with the environment. The paper also examines studies on the impact of HPs on distribution networks, particularly those leveraging energy flexibility strategies. Key takeaways suggest the need to validate control strategies for extreme cold-weather regions that require backup heaters, as well as develop approaches designed for demand charge schemes that integrate HPs with other controllable loads. From a grid impact assessment perspective, studies have focused primarily on RBCs for providing energy flexibility through HP operation, without addressing more advanced methods such as real-time optimization using MPC or RL-based algorithms. Incorporating these advanced control strategies could help identify key limitations, including the impact of varying user participation levels and the cost-benefit trade-offs associated with their implementation.</article>","contentLength":1822,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"On the application of Visibility Graphs in the Spectral Domain for Speaker Recognition","url":"https://arxiv.org/abs/2502.14110","date":1740114000,"author":"","guid":8316,"unread":true,"content":"<article>arXiv:2502.14110v1 Announce Type: new \nAbstract: In this study, we explore the potential of visibility graphs in the spectral domain for speaker recognition. Adult participants were instructed to record vocalizations of the five Spanish vowels. For each vocalization, we computed the frequency spectrum considering the source-filter model of speech production, where formants are shaped by the vocal tract acting as a passive filter with resonant frequencies. Spectral profiles exhibited consistent intra-speaker characteristics, reflecting individual vocal tract anatomies, while showing variation between speakers. We then constructed visibility graphs from these spectral profiles and extracted various graph-theoretic metrics to capture their topological features. These metrics were assembled into feature vectors representing the five vowels for each speaker. Using an ensemble of decision trees trained on these features, we achieved high accuracy in speaker identification. Our analysis identified key topological features that were critical in distinguishing between speakers. This study demonstrates the effectiveness of visibility graphs for spectral analysis and their potential in speaker recognition. We also discuss the robustness of this approach, offering insights into its applicability for real-world speaker recognition systems. This research contributes to expanding the feature extraction toolbox for speaker recognition by leveraging the topological properties of speech signals in the spectral domain.</article>","contentLength":1525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Prediction of Received Power in Low-Power Networks Deployed on the Surface of Rough Waters","url":"https://arxiv.org/abs/2502.14107","date":1740114000,"author":"","guid":8317,"unread":true,"content":"<article>arXiv:2502.14107v1 Announce Type: new \nAbstract: Low-power and cost-effective IoT sensing nodes enable scalable monitoring of different environments. Some of these environments impose rough and extreme operating conditions, requiring continuous adaptation and reconfiguration of physical and link layer parameters. In this paper, we closely investigate the stability of the wireless links established between nodes deployed on the surface of different water bodies and propose a model to predict the received power. Our model is based on Minimum Mean Square Estimation (MMSE) and relies on the statistics of received power and the motion the nodes experience during communication. One of the drawbacks of MMSE is its reliance on matrix inversion, which is at once computationally expensive and difficult to implement with resource constrained devices. We forgo this stage by estimating model parameters using the gradient-descent approach, which is much simpler to implement. The model achieves a prediction accuracy of 91% even with a small number of iterations.</article>","contentLength":1063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Explainable Distributed Constraint Optimization Problems","url":"https://arxiv.org/abs/2502.14102","date":1740114000,"author":"","guid":8318,"unread":true,"content":"<article>arXiv:2502.14102v1 Announce Type: new \nAbstract: The Distributed Constraint Optimization Problem (DCOP) formulation is a powerful tool to model cooperative multi-agent problems that need to be solved distributively. A core assumption of existing approaches is that DCOP solutions can be easily understood, accepted, and adopted, which may not hold, as evidenced by the large body of literature on Explainable AI. In this paper, we propose the Explainable DCOP (X-DCOP) model, which extends a DCOP to include its solution and a contrastive query for that solution. We formally define some key properties that contrastive explanations must satisfy for them to be considered as valid solutions to X-DCOPs as well as theoretical results on the existence of such valid explanations. To solve X-DCOPs, we propose a distributed framework as well as several optimizations and suboptimal variants to find valid explanations. We also include a human user study that showed that users, not surprisingly, prefer shorter explanations over longer ones. Our empirical evaluations showed that our approach can scale to large problems, and the different variants provide different options for trading off explanation lengths for smaller runtimes. Thus, our model and algorithmic contributions extend the state of the art by reducing the barrier for users to understand DCOP solutions, facilitating their adoption in more real-world applications.</article>","contentLength":1428,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach","url":"https://arxiv.org/abs/2502.14100","date":1740114000,"author":"","guid":8319,"unread":true,"content":"<article>arXiv:2502.14100v1 Announce Type: new \nAbstract: Large Language Models (LLMs) enhanced with external contexts, such as through retrieval-augmented generation (RAG), often face challenges in handling imperfect evidence. They tend to over-rely on external knowledge, making them vulnerable to misleading and unhelpful contexts. To address this, we propose the concept of context-robust LLMs, which can effectively balance internal knowledge with external context, similar to human cognitive processes. Specifically, context-robust LLMs should rely on external context only when lacking internal knowledge, identify contradictions between internal and external knowledge, and disregard unhelpful contexts. To achieve this goal, we introduce Grft, a lightweight and plug-and-play gated representation fine-tuning approach. Grft consists of two key components: a gating mechanism to detect and filter problematic inputs, and low-rank representation adapters to adjust hidden representations. By training a lightweight intervention function with only 0.0004\\% of model size on fewer than 200 examples, Grft can effectively adapt LLMs towards context-robust behaviors.</article>","contentLength":1161,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Point Cloud Geometry Scalable Coding Using a Resolution and Quality-conditioned Latents Probability Estimator","url":"https://arxiv.org/abs/2502.14099","date":1740114000,"author":"","guid":8320,"unread":true,"content":"<article>arXiv:2502.14099v1 Announce Type: new \nAbstract: In the current age, users consume multimedia content in very heterogeneous scenarios in terms of network, hardware, and display capabilities. A naive solution to this problem is to encode multiple independent streams, each covering a different possible requirement for the clients, with an obvious negative impact in both storage and computational requirements. These drawbacks can be avoided by using codecs that enable scalability, i.e., the ability to generate a progressive bitstream, containing a base layer followed by multiple enhancement layers, that allow decoding the same bitstream serving multiple reconstructions and visualization specifications. While scalable coding is a well-known and addressed feature in conventional image and video codecs, this paper focuses on a new and very different problem, notably the development of scalable coding solutions for deep learning-based Point Cloud (PC) coding. The peculiarities of this 3D representation make it hard to implement flexible solutions that do not compromise the other functionalities of the codec. This paper proposes a joint quality and resolution scalability scheme, named Scalable Resolution and Quality Hyperprior (SRQH), that, contrary to previous solutions, can model the relationship between latents obtained with models trained for different RD tradeoffs and/or at different resolutions. Experimental results obtained by integrating SRQH in the emerging JPEG Pleno learning-based PC coding standard show that SRQH allows decoding the PC at different qualities and resolutions with a single bitstream while incurring only in a limited RD penalty and increment in complexity w.r.t. non-scalable JPEG PCC that would require one bitstream per coding configuration.</article>","contentLength":1789,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aligned Multi Objective Optimization","url":"https://arxiv.org/abs/2502.14096","date":1740114000,"author":"","guid":8321,"unread":true,"content":"<article>arXiv:2502.14096v1 Announce Type: new \nAbstract: To date, the multi-objective optimization literature has mainly focused on conflicting objectives, studying the Pareto front, or requiring users to balance tradeoffs. Yet, in machine learning practice, there are many scenarios where such conflict does not take place. Recent findings from multi-task learning, reinforcement learning, and LLMs training show that diverse related tasks can enhance performance across objectives simultaneously. Despite this evidence, such phenomenon has not been examined from an optimization perspective. This leads to a lack of generic gradient-based methods that can scale to scenarios with a large number of related objectives. To address this gap, we introduce the Aligned Multi-Objective Optimization framework, propose new algorithms for this setting, and provide theoretical guarantees of their superior performance compared to naive approaches.</article>","contentLength":933,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Retrieving Versus Understanding Extractive Evidence in Few-Shot Learning","url":"https://arxiv.org/abs/2502.14095","date":1740114000,"author":"","guid":8322,"unread":true,"content":"<article>arXiv:2502.14095v1 Announce Type: new \nAbstract: A key aspect of alignment is the proper use of within-document evidence to construct document-level decisions. We analyze the relationship between the retrieval and interpretation of within-document evidence for large language model in a few-shot setting. Specifically, we measure the extent to which model prediction errors are associated with evidence retrieval errors with respect to gold-standard human-annotated extractive evidence for five datasets, using two popular closed proprietary models. We perform two ablation studies to investigate when both label prediction and evidence retrieval errors can be attributed to qualities of the relevant evidence. We find that there is a strong empirical relationship between model prediction and evidence retrieval error, but that evidence retrieval error is mostly not associated with evidence interpretation error--a hopeful sign for downstream applications built on this mechanism.</article>","contentLength":982,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CND-IDS: Continual Novelty Detection for Intrusion Detection Systems","url":"https://arxiv.org/abs/2502.14094","date":1740114000,"author":"","guid":8323,"unread":true,"content":"<article>arXiv:2502.14094v1 Announce Type: new \nAbstract: Intrusion detection systems (IDS) play a crucial role in IoT and network security by monitoring system data and alerting to suspicious activities. Machine learning (ML) has emerged as a promising solution for IDS, offering highly accurate intrusion detection. However, ML-IDS solutions often overlook two critical aspects needed to build reliable systems: continually changing data streams and a lack of attack labels. Streaming network traffic and associated cyber attacks are continually changing, which can degrade the performance of deployed ML models. Labeling attack data, such as zero-day attacks, in real-world intrusion scenarios may not be feasible, making the use of ML solutions that do not rely on attack labels necessary. To address both these challenges, we propose CND-IDS, a continual novelty detection IDS framework which consists of (i) a learning-based feature extractor that continuously updates new feature representations of the system data, and (ii) a novelty detector that identifies new cyber attacks by leveraging principal component analysis (PCA) reconstruction. Our results on realistic intrusion datasets show that CND-IDS achieves up to 6.1x F-score improvement, and up to 6.5x improved forward transfer over the SOTA unsupervised continual learning algorithm. Our code will be released upon acceptance.</article>","contentLength":1384,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A New Framework of Software Obfuscation Evaluation Criteria","url":"https://arxiv.org/abs/2502.14093","date":1740114000,"author":"","guid":8324,"unread":true,"content":"<article>arXiv:2502.14093v1 Announce Type: new \nAbstract: In the domain of practical software protection against man-at-the-end attacks such as software reverse engineering and tampering, much of the scientific literature is plagued by the use of subpar methods to evaluate the protections' strength and even by the absence of such evaluations. Several criteria have been proposed in the past to assess the strength of protections, such as potency, resilience, stealth, and cost. We analyze their evolving definitions and uses. We formulate a number of critiques, from which we conclude that the existing definitions are unsatisfactory and need to be revised. We present a new framework of software protection evaluation criteria: relevance, effectiveness (or efficacy), robustness, concealment, stubbornness, sensitivity, predictability, and cost.</article>","contentLength":839,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hybrid Visual Servoing of Tendon-driven Continuum Robots","url":"https://arxiv.org/abs/2502.14092","date":1740114000,"author":"","guid":8325,"unread":true,"content":"<article>arXiv:2502.14092v1 Announce Type: new \nAbstract: This paper introduces a novel Hybrid Visual Servoing (HVS) approach for controlling tendon-driven continuum robots (TDCRs). The HVS system combines Image-Based Visual Servoing (IBVS) with Deep Learning-Based Visual Servoing (DLBVS) to overcome the limitations of each method and improve overall performance. IBVS offers higher accuracy and faster convergence in feature-rich environments, while DLBVS enhances robustness against disturbances and offers a larger workspace. By enabling smooth transitions between IBVS and DLBVS, the proposed HVS ensures effective control in dynamic, unstructured environments. The effectiveness of this approach is validated through simulations and real-world experiments, demonstrating that HVS achieves reduced iteration time, faster convergence, lower final error, and smoother performance compared to DLBVS alone, while maintaining DLBVS's robustness in challenging conditions such as occlusions, lighting changes, actuator noise, and physical impacts.</article>","contentLength":1038,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Regression in EO: Are VLMs Up to the Challenge?","url":"https://arxiv.org/abs/2502.14088","date":1740114000,"author":"","guid":8326,"unread":true,"content":"<article>arXiv:2502.14088v1 Announce Type: new \nAbstract: Earth Observation (EO) data encompass a vast range of remotely sensed information, featuring multi-sensor and multi-temporal, playing an indispensable role in understanding our planet's dynamics. Recently, Vision Language Models (VLMs) have achieved remarkable success in perception and reasoning tasks, bringing new insights and opportunities to the EO field. However, the potential for EO applications, especially for scientific regression related applications remains largely unexplored. This paper bridges that gap by systematically examining the challenges and opportunities of adapting VLMs for EO regression tasks. The discussion first contrasts the distinctive properties of EO data with conventional computer vision datasets, then identifies four core obstacles in applying VLMs to EO regression: 1) the absence of dedicated benchmarks, 2) the discrete-versus-continuous representation mismatch, 3) cumulative error accumulation, and 4) the suboptimal nature of text-centric training objectives for numerical tasks. Next, a series of methodological insights and potential subtle pitfalls are explored. Lastly, we offer some promising future directions for designing robust, domain-aware solutions. Our findings highlight the promise of VLMs for scientific regression in EO, setting the stage for more precise and interpretable modeling of critical environmental processes.</article>","contentLength":1430,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning from End User Data with Shuffled Differential Privacy over Kernel Densities","url":"https://arxiv.org/abs/2502.14087","date":1740114000,"author":"","guid":8327,"unread":true,"content":"<article>arXiv:2502.14087v1 Announce Type: new \nAbstract: We study a setting of collecting and learning from private data distributed across end users. In the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. This model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy.\n  Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. We use it to privately learn a classifier from the end user data, by learning a private density function per class. Moreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. Our experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP.</article>","contentLength":1190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navigating Semantic Relations: Challenges for Language Models in Abstract Common-Sense Reasoning","url":"https://arxiv.org/abs/2502.14086","date":1740114000,"author":"","guid":8328,"unread":true,"content":"<article>arXiv:2502.14086v1 Announce Type: new \nAbstract: Large language models (LLMs) have achieved remarkable performance in generating human-like text and solving reasoning tasks of moderate complexity, such as question-answering and mathematical problem-solving. However, their capabilities in tasks requiring deeper cognitive skills, such as common-sense understanding and abstract reasoning, remain under-explored. In this paper, we systematically evaluate abstract common-sense reasoning in LLMs using the ConceptNet knowledge graph. We propose two prompting approaches: instruct prompting, where models predict plausible semantic relationships based on provided definitions, and few-shot prompting, where models identify relations using examples as guidance. Our experiments with the gpt-4o-mini model show that in instruct prompting, consistent performance is obtained when ranking multiple relations but with substantial decline when the model is restricted to predicting only one relation. In few-shot prompting, the model's accuracy improves significantly when selecting from five relations rather than the full set, although with notable bias toward certain relations. These results suggest significant gaps still, even in commercially used LLMs' abstract common-sense reasoning abilities, compared to human-level understanding. However, the findings also highlight the promise of careful prompt engineering, based on selective retrieval, for obtaining better performance.</article>","contentLength":1476,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral","url":"https://arxiv.org/abs/2502.14083","date":1740114000,"author":"","guid":8329,"unread":true,"content":"<article>arXiv:2502.14083v1 Announce Type: new \nAbstract: Moral reasoning is a complex cognitive process shaped by individual experiences and cultural contexts and presents unique challenges for computational analysis. While natural language processing (NLP) offers promising tools for studying this phenomenon, current research lacks cohesion, employing discordant datasets and tasks that examine isolated aspects of moral reasoning. We bridge this gap with UniMoral, a unified dataset integrating psychologically grounded and social-media-derived moral dilemmas annotated with labels for action choices, ethical principles, contributing factors, and consequences, alongside annotators' moral and cultural profiles. Recognizing the cultural relativity of moral reasoning, UniMoral spans six languages, Arabic, Chinese, English, Hindi, Russian, and Spanish, capturing diverse socio-cultural contexts. We demonstrate UniMoral's utility through a benchmark evaluations of three large language models (LLMs) across four tasks: action prediction, moral typology classification, factor attribution analysis, and consequence generation. Key findings reveal that while implicitly embedded moral contexts enhance the moral reasoning capability of LLMs, there remains a critical need for increasingly specialized approaches to further advance moral reasoning in these models.</article>","contentLength":1357,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A review of theories and models utilized by empirical studies about mental health help-seeking and implications for future research","url":"https://arxiv.org/abs/2502.14082","date":1740114000,"author":"","guid":8330,"unread":true,"content":"<article>arXiv:2502.14082v1 Announce Type: new \nAbstract: Purpose: With the rise of mental health risks globally, it is urgent to provide effective mental health support. However, a holistic understanding of how people seek help for mental health problems remains limited, impeding the development of evidence-based intervention programs to facilitate help-seeking behavior. This study reviews current theories that guide empirical research on young adults' help-seeking behavior using technologies, identifies limitations in existing frameworks, and proposes directions for future research.\n  Methods: We searched databases that are most likely to contain mental health help-seeking practices in relation to information technology, including PubMed, ACM Digital Library, Web of Science, PsycInfo, ScienceDirect, EBSCO, and Cochrane Library.\n  Results: Of 2443 abstracts reviewed, 43 studies met the criteria and were included in the analysis. We identified 16 theories and models. They represent seven perspectives to view mental health help-seeking and reveal factors such as accessibility, stigma, and social support as key factors influencing help-seeking.\n  Limitations: We summarized the theories and models and categorized them based on their primary perspectives. Cross-perspective connections could be explored in future reviews.\n  Conclusions: A holistic approach to creating culturally sensitive multi-level interventions that consider individual, interpersonal, and community factors is needed to advance effective mental health help-seeking support strategies.</article>","contentLength":1564,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Personalized Education with Generative AI and Digital Twins: VR, RAG, and Zero-Shot Sentiment Analysis for Industry 4.0 Workforce Development","url":"https://arxiv.org/abs/2502.14080","date":1740114000,"author":"","guid":8331,"unread":true,"content":"<article>arXiv:2502.14080v1 Announce Type: new \nAbstract: The Fourth Industrial Revolution (4IR) technologies, such as cloud computing, machine learning, and AI, have improved productivity but introduced challenges in workforce training and reskilling. This is critical given existing workforce shortages, especially in marginalized communities like Underrepresented Minorities (URM), who often lack access to quality education. Addressing these challenges, this research presents gAI-PT4I4, a Generative AI-based Personalized Tutor for Industrial 4.0, designed to personalize 4IR experiential learning. gAI-PT4I4 employs sentiment analysis to assess student comprehension, leveraging generative AI and finite automaton to tailor learning experiences. The framework integrates low-fidelity Digital Twins for VR-based training, featuring an Interactive Tutor - a generative AI assistant providing real-time guidance via audio and text. It uses zero-shot sentiment analysis with LLMs and prompt engineering, achieving 86\\% accuracy in classifying student-teacher interactions as positive or negative. Additionally, retrieval-augmented generation (RAG) enables personalized learning content grounded in domain-specific knowledge. To adapt training dynamically, finite automaton structures exercises into states of increasing difficulty, requiring 80\\% task-performance accuracy for progression. Experimental evaluation with 22 volunteers showed improved accuracy exceeding 80\\%, reducing training time. Finally, this paper introduces a Multi-Fidelity Digital Twin model, aligning Digital Twin complexity with Bloom's Taxonomy and Kirkpatrick's model, providing a scalable educational framework.</article>","contentLength":1682,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Learning Bayesian Game Families, with Application to Mechanism Design","url":"https://arxiv.org/abs/2502.14078","date":1740114000,"author":"","guid":8332,"unread":true,"content":"<article>arXiv:2502.14078v1 Announce Type: new \nAbstract: Learning or estimating game models from data typically entails inducing separate models for each setting, even if the games are parametrically related. In empirical mechanism design, for example, this approach requires learning a new game model for each candidate setting of the mechanism parameter. Recent work has shown the data efficiency benefits of learning a single parameterized model for families of related games. In Bayesian games - a typical model for mechanism design - payoffs depend on both the actions and types of the players. We show how to exploit this structure by learning an interim game-family model that conditions on a single player's type. We compare this approach to the baseline approach of directly learning the ex ante payoff function, which gives payoffs in expectation of all player types. By marginalizing over player type, the interim model can also provide ex ante payoff predictions. This dual capability not only facilitates Bayes-Nash equilibrium approximation, but also enables new types of analysis using the conditional model. We validate our method through a case study of a dynamic sponsored search auction. In our experiments, the interim model more reliably approximates equilibria than the ex ante model and exhibits effective parameter extrapolation. With local search over the parameter space, the learned game-family model can be used for mechanism design. Finally, without any additional sample data, we leverage the interim model to compute piecewise best-response strategies and refine our model to incorporate these strategies, enabling an iterative approach to empirical mechanism design.</article>","contentLength":1690,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CarbonEdge: Leveraging Mesoscale Spatial Carbon-Intensity Variations for Low Carbon Edge Computing","url":"https://arxiv.org/abs/2502.14076","date":1740114000,"author":"","guid":8333,"unread":true,"content":"<article>arXiv:2502.14076v1 Announce Type: new \nAbstract: The proliferation of latency-critical and compute-intensive edge applications is driving increases in computing demand and carbon emissions at the edge. To better understand carbon emissions at the edge, we analyze granular carbon intensity traces at intermediate \"mesoscales,\" such as within a single US state or among neighboring countries in Europe, and observe significant variations in carbon intensity at these spatial scales. Importantly, our analysis shows that carbon intensity variations, which are known to occur at large continental scales (e.g., cloud regions), also occur at much finer spatial scales, making it feasible to exploit geographic workload shifting in the edge computing context. Motivated by these findings, we propose \\proposedsystem, a carbon-aware framework for edge computing that optimizes the placement of edge workloads across mesoscale edge data centers to reduce carbon emissions while meeting latency SLOs. We implement CarbonEdge and evaluate it on a real edge computing testbed and through large-scale simulations for multiple edge workloads and settings. Our experimental results on a real testbed demonstrate that CarbonEdge can reduce emissions by up to 78.7\\% for a regional edge deployment in central Europe. Moreover, our CDN-scale experiments show potential savings of 49.5\\% and 67.8\\% in the US and Europe, respectively, while limiting the one-way latency increase to less than 5.5 ms.</article>","contentLength":1482,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards Vector Optimization on Low-Dimensional Vector Symbolic Architecture","url":"https://arxiv.org/abs/2502.14075","date":1740114000,"author":"","guid":8334,"unread":true,"content":"<article>arXiv:2502.14075v1 Announce Type: new \nAbstract: Vector Symbolic Architecture (VSA) is emerging in machine learning due to its efficiency, but they are hindered by issues of hyperdimensionality and accuracy. As a promising mitigation, the Low-Dimensional Computing (LDC) method significantly reduces the vector dimension by ~100 times while maintaining accuracy, by employing a gradient-based optimization. Despite its potential, LDC optimization for VSA is still underexplored. Our investigation into vector updates underscores the importance of stable, adaptive dynamics in LDC training. We also reveal the overlooked yet critical roles of batch normalization (BN) and knowledge distillation (KD) in standard approaches. Besides the accuracy boost, BN does not add computational overhead during inference, and KD significantly enhances inference confidence. Through extensive experiments and ablation studies across multiple benchmarks, we provide a thorough evaluation of our approach and extend the interpretability of binary neural network optimization similar to LDC, previously unaddressed in BNN literature.</article>","contentLength":1115,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Investigating Non-Transitivity in LLM-as-a-Judge","url":"https://arxiv.org/abs/2502.14074","date":1740114000,"author":"","guid":8335,"unread":true,"content":"<article>arXiv:2502.14074v1 Announce Type: new \nAbstract: Automatic evaluation methods based on large language models (LLMs) are emerging as the standard tool for assessing the instruction-following abilities of LLM-based agents. The most common method in this paradigm, pairwise comparisons with a baseline model, critically depends on the assumption of transitive preferences. However, the validity of this assumption remains largely unexplored. In this study, we investigate the presence of non-transitivity within the AlpacaEval framework and analyze its effects on model rankings. We find that LLM judges exhibit non-transitive preferences, leading to rankings that are sensitive to the choice of the baseline model. To mitigate this issue, we show that round-robin tournaments combined with Bradley-Terry models of preference can produce more reliable rankings. Notably, our method increases both the Spearman correlation and the Kendall correlation with Chatbot Arena (95.0% -&gt; 96.4% and 82.1% -&gt; 86.3% respectively). To address the computational cost of round-robin tournaments, we propose Swiss-Wise Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to capture the benefits of round-robin tournaments while maintaining computational efficiency.</article>","contentLength":1270,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Assessing Simulation Knowledge and Proficiency Among Undergraduate Computing Students in Brazil: Insights and Results from a Survey Research","url":"https://arxiv.org/abs/2502.14072","date":1740114000,"author":"","guid":8336,"unread":true,"content":"<article>arXiv:2502.14072v1 Announce Type: new \nAbstract: This paper reports results of an investigation about the level of knowledge among undergraduate computer science students in Brazil regarding the topic of simulation. Amid rapid technological evolution, simulation emerges as a crucial resource for training professionals capable of facing complex challenges. The research seeks to analyze the presence and effectiveness of simulation education, exploring students' perceptions, the tools used, the challenges faced, and the prospects for deeper study. This report highlights the importance of academic training in a dynamic technological environment, emphasizing the crucial role of simulation education in undergraduate computer science, while exploring the foundations of the methodologies and educational strategies associated with the topic. A survey research approach is adopted. 108 answers were received from 10 Brazilian states. 19 respondents from 15 different institutions said they had some contact with simulation during their studies. Results reveal that MATLAB/Simulink is the most popular formalism/tool used to teach simulation in Brazil.</article>","contentLength":1153,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiffExp: Efficient Exploration in Reward Fine-tuning for Text-to-Image Diffusion Models","url":"https://arxiv.org/abs/2502.14070","date":1740114000,"author":"","guid":8337,"unread":true,"content":"<article>arXiv:2502.14070v1 Announce Type: new \nAbstract: Fine-tuning text-to-image diffusion models to maximize rewards has proven effective for enhancing model performance. However, reward fine-tuning methods often suffer from slow convergence due to online sample generation. Therefore, obtaining diverse samples with strong reward signals is crucial for improving sample efficiency and overall performance. In this work, we introduce DiffExp, a simple yet effective exploration strategy for reward fine-tuning of text-to-image models. Our approach employs two key strategies: (a) dynamically adjusting the scale of classifier-free guidance to enhance sample diversity, and (b) randomly weighting phrases of the text prompt to exploit high-quality reward signals. We demonstrate that these strategies significantly enhance exploration during online sample generation, improving the sample efficiency of recent reward fine-tuning methods, such as DDPO and AlignProp.</article>","contentLength":959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Racing Dataset and Baseline Model for Track Detection in Autonomous Racing","url":"https://arxiv.org/abs/2502.14068","date":1740114000,"author":"","guid":8338,"unread":true,"content":"<article>arXiv:2502.14068v1 Announce Type: new \nAbstract: A significant challenge in racing-related research is the lack of publicly available datasets containing raw images with corresponding annotations for the downstream task. In this paper, we introduce RoRaTrack, a novel dataset that contains annotated multi-camera image data from racing scenarios for track detection. The data is collected on a Dallara AV-21 at a racing circuit in Indiana, in collaboration with the Indy Autonomous Challenge (IAC). RoRaTrack addresses common problems such as blurriness due to high speed, color inversion from the camera, and absence of lane markings on the track. Consequently, we propose RaceGAN, a baseline model based on a Generative Adversarial Network (GAN) that effectively addresses these challenges. The proposed model demonstrates superior performance compared to current state-of-the-art machine learning models in track detection. The dataset and code for this work are available at github.com/RaceGAN.</article>","contentLength":998,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Experiment Design with Gaussian Process Regression with Applications to Chance-Constrained Control","url":"https://arxiv.org/abs/2502.14066","date":1740114000,"author":"","guid":8339,"unread":true,"content":"<article>arXiv:2502.14066v1 Announce Type: new \nAbstract: Learning for control in repeated tasks allows for well-designed experiments to gather the most useful data. We consider the setting in which we use a data-driven controller that does not have access to the true system dynamics. Rather, the controller uses inferred dynamics based on the available information. In order to acquire data that is beneficial for this controller, we present an experimental design approach that leverages the current data to improve expected control performance. We focus on the setting in which inference on the unknown dynamics is performed using Gaussian processes. Gaussian processes not only provide uncertainty quantification but also allow us to leverage structures inherent to Gaussian random variables. Through this structure, we design experiments via gradient descent on the expected control performance with respect to the experiment input. In particular, we focus on a chance-constrained minimum expected time control problem. Numerical demonstrations of our approach indicate our experimental design outperforms relevant benchmarks.</article>","contentLength":1123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Triad: Vision Foundation Model for 3D Magnetic Resonance Imaging","url":"https://arxiv.org/abs/2502.14064","date":1740114000,"author":"","guid":8340,"unread":true,"content":"<article>arXiv:2502.14064v1 Announce Type: new \nAbstract: Vision foundation models (VFMs) are pre-trained on extensive image datasets to learn general representations for diverse types of data. These models can subsequently be fine-tuned for specific downstream tasks, significantly boosting performance across a broad range of applications. However, existing vision foundation models that claim to be applicable to various radiology tasks are mostly pre-trained on 3D computed tomography (CT), which benefits from the availability of extensive 3D CT databases. Significant differences between CT and magnetic resonance imaging (MRI) in imaging principles, signal characteristics, and data distribution may hinder their practical performance and versatility in MRI-specific applications. Here, we propose Triad, a vision foundation model for 3D MRI. Triad adopts a widely used autoencoder architecture to learn robust representations from 131,170 3D MRI volumes and uses organ-independent imaging descriptions to constrain the semantic distribution of the visual modality. The above pre-training dataset is called Triad-131K, which is currently the largest 3D MRI pre-training dataset. We evaluate Triad across three tasks, namely, organ/tumor segmentation, organ/cancer classification, and medical image registration, in two data modalities (within-domain and out-of-domain) settings using 25 downstream datasets. By initializing models with Triad's pre-trained weights, nnUNet-Triad improves segmentation performance by 6.88% compared to nnUNet-Scratch across 17 datasets. Swin-B-Triad achieves a 3.97% improvement over Swin-B-Scratch in classification tasks across five datasets. SwinUNETR-Triad improves by 4.00% compared to SwinUNETR-Scratch in registration tasks across two datasets. Our study demonstrates that pre-training can maximize performance when the data modalities and organs of upstream and downstream tasks are consistent.</article>","contentLength":1931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PedDet: Adaptive Spectral Optimization for Multimodal Pedestrian Detection","url":"https://arxiv.org/abs/2502.14063","date":1740114000,"author":"","guid":8341,"unread":true,"content":"<article>arXiv:2502.14063v1 Announce Type: new \nAbstract: Pedestrian detection in intelligent transportation systems has made significant progress but faces two critical challenges: (1) insufficient fusion of complementary information between visible and infrared spectra, particularly in complex scenarios, and (2) sensitivity to illumination changes, such as low-light or overexposed conditions, leading to degraded performance. To address these issues, we propose PedDet, an adaptive spectral optimization complementarity framework specifically enhanced and optimized for multispectral pedestrian detection. PedDet introduces the Multi-scale Spectral Feature Perception Module (MSFPM) to adaptively fuse visible and infrared features, enhancing robustness and flexibility in feature extraction. Additionally, the Illumination Robustness Feature Decoupling Module (IRFDM) improves detection stability under varying lighting by decoupling pedestrian and background features. We further design a contrastive alignment to enhance intermodal feature discrimination. Experiments on LLVIP and MSDS datasets demonstrate that PedDet achieves state-of-the-art performance, improving the mAP by 6.6% with superior detection accuracy even in low-light conditions, marking a significant step forward for road safety. Code will be available at https://github.com/AIGeeksGroup/PedDet.</article>","contentLength":1363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation","url":"https://arxiv.org/abs/2502.14061","date":1740114000,"author":"","guid":8342,"unread":true,"content":"<article>arXiv:2502.14061v1 Announce Type: new \nAbstract: In industrial applications requiring real-time feedback, such as quality control and robotic manipulation, the demand for high-speed and accurate pose estimation remains critical. Despite advances improving speed and accuracy in pose estimation, finding a balance between computational efficiency and accuracy poses significant challenges in dynamic environments. Most current algorithms lack scalability in estimation time, especially for diverse datasets, and the state-of-the-art (SOTA) methods are often too slow. This study focuses on developing a fast and scalable set of pose estimators based on GDRNPP to meet or exceed current benchmarks in accuracy and robustness, particularly addressing the efficiency-accuracy trade-off essential in real-time scenarios. We propose the AMIS algorithm to tailor the utilized model according to an application-specific trade-off between inference time and accuracy. We further show the effectiveness of the AMIS-based model choice on four prominent benchmark datasets (LM-O, YCB-V, T-LESS, and ITODD).</article>","contentLength":1094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Integrated Telehealth and Extended Reality to Enhance Home Exercise Adherence Following Total Hip and Knee Arthroplasty","url":"https://arxiv.org/abs/2502.14059","date":1740114000,"author":"","guid":8343,"unread":true,"content":"<article>arXiv:2502.14059v1 Announce Type: new \nAbstract: Nearly one million total hip and knee arthroplasties (THA/TKA) are performed annually in the United States, with most patients discharged home and prescribed home exercise programs (HEPs) to enhance lower extremity function. Traditional paper-based HEPs, while accessible and low-cost, often lack engagement and real-time feedback, which are critical for adherence and performance optimization. Extended reality (XR) and telehealth (TH) systems offer promising solutions, combining engagement and feedback, though each has limitations. To address these gaps, we designed and executed a pilot study that compared exercise performance in individuals with THA/TKA using a conventional paper-based HEP versus a proof-of-concept system, dubbed Tele-PhyT, that included the ideal characteristics of a future XR technology that would enable seamless HEP-TH systems, with robust marker-less full body tracking, real-time visual feedback, and performance quantification. The pilot study used a randomized cross-over design and targeted two types of users: therapists and patients. Participants favored Tele- PhyT for its real-time feedback and ease of use, and noted its potential to improve HEP adherence and exercise accuracy.</article>","contentLength":1268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Low-Complexity Scheme for Multi-Message Private Information Retrieval","url":"https://arxiv.org/abs/2502.14054","date":1740114000,"author":"","guid":8344,"unread":true,"content":"<article>arXiv:2502.14054v1 Announce Type: new \nAbstract: Private Information Retrieval (PIR) is a fundamental problem in the broader fields of security and privacy. In recent years, the problem has garnered significant attention from the research community, leading to achievability schemes and converse results for many important PIR settings.\n  This paper focuses on the Multi-message Private Information Retrieval (MPIR) setting, where a user aims to retrieve \\(D\\) messages from a database of \\(K\\) messages, with identical copies of the database available on \\(N\\) remote servers. The user's goal is to maximize the download rate while keeping the identities of the retrieved messages private. Existing approaches to the MPIR problem primarily focus on either scalar-linear solutions or vector-linear solutions, the latter requiring a high degree of subpacketization. Furthermore, prior scalar-linear solutions are restricted to the special case of \\(N = D+1\\). This limitation hinders the practical adoption of these schemes, as real-world applications demand simple, easily implementable solutions that support a broad range of scenarios.\n  In this work, we present a solution for the MPIR problem, which applies to a broader range of system parameters and requires a limited degree of subpacketization. In particular, the proposed scheme applies to all values of \\(N=DL+1\\) for any integer \\(L\\geq 1\\), and requires a degree of subpacketization \\(L\\). Our scheme achieves capacity when \\(D\\) divides \\(K\\), and in all other cases, its performance matches or comes within a small additive margin of the best-known scheme that requires a high degree of subpacketization.</article>","contentLength":1668,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Goggin's corrected Kalman Filter: Guarantees and Filtering Regimes","url":"https://arxiv.org/abs/2502.14053","date":1740114000,"author":"","guid":8345,"unread":true,"content":"<article>arXiv:2502.14053v1 Announce Type: new \nAbstract: In this paper we revisit a non-linear filter for {\\em non-Gaussian} noises that was introduced in [1]. Goggin proved that transforming the observations by the score function and then applying the Kalman Filter (KF) to the transformed observations results in an asymptotically optimal filter. In the current paper, we study the convergence rate of Goggin's filter in a pre-limit setting that allows us to study a range of signal-to-noise regimes which includes, as a special case, Goggin's setting. Our guarantees are explicit in the level of observation noise, and unlike most other works in filtering, we do not assume Gaussianity of the noises.\n  Our proofs build on combining simple tools from two separate literature streams. One is a general posterior Cram\\'er-Rao lower bound for filtering. The other is convergence-rate bounds in the Fisher information central limit theorem.\n  Along the way, we also study filtering regimes for linear state-space models, characterizing clearly degenerate regimes -- where trivial filters are nearly optimal -- and a {\\em balanced} regime, which is where Goggin's filter has the most value. \\footnote{This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</article>","contentLength":1356,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Matter of Perspective(s): Contrasting Human and LLM Argumentation in Subjective Decision-Making on Subtle Sexism","url":"https://arxiv.org/abs/2502.14052","date":1740114000,"author":"","guid":8346,"unread":true,"content":"<article>arXiv:2502.14052v1 Announce Type: new \nAbstract: In subjective decision-making, where decisions are based on contextual interpretation, Large Language Models (LLMs) can be integrated to present users with additional rationales to consider. The diversity of these rationales is mediated by the ability to consider the perspectives of different social actors. However, it remains unclear whether and how models differ in the distribution of perspectives they provide. We compare the perspectives taken by humans and different LLMs when assessing subtle sexism scenarios. We show that these perspectives can be classified within a finite set (perpetrator, victim, decision-maker), consistently present in argumentations produced by humans and LLMs, but in different distributions and combinations, demonstrating differences and similarities with human responses, and between models. We argue for the need to systematically evaluate LLMs' perspective-taking to identify the most suitable models for a given decision-making task. We discuss the implications for model evaluation.</article>","contentLength":1074,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression","url":"https://arxiv.org/abs/2502.14051","date":1740114000,"author":"","guid":8347,"unread":true,"content":"<article>arXiv:2502.14051v1 Announce Type: new \nAbstract: Transformer-based Large Language Models rely critically on KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy designed specifically to reduce both memory bandwidth and capacity demand of KV cache during the decode phase. RocketKV contains two consecutive stages. In the first stage, it performs coarse-grain KV cache eviction on the input sequence tokens with SnapKV++, a method improved upon SnapKV by introducing adaptive pooling size and full compatibility with grouped-query attention. In the second stage, it adopts a hybrid attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensional reductions. Combining these two stages, RocketKV achieves significant KV cache fetching bandwidth and storage savings while maintaining comparable accuracy to full KV cache attention. We show that RocketKV provides end-to-end speedup by up to 3$\\times$ as well as peak memory reduction by up to 31% in the decode phase on an NVIDIA H100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks.</article>","contentLength":1443,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Diversity-driven Data Selection for Language Model Tuning through Sparse Autoencoder","url":"https://arxiv.org/abs/2502.14050","date":1740114000,"author":"","guid":8348,"unread":true,"content":"<article>arXiv:2502.14050v1 Announce Type: new \nAbstract: Current pre-trained large language models typically need instruction tuning to align with human preferences. However, instruction tuning data is often quantity-saturated due to the large volume of data collection and fast model iteration, leaving coreset data selection important but underexplored. On the other hand, existing quality-driven data selection methods such as LIMA (NeurIPS 2023 (Zhou et al., 2024)) and AlpaGasus (ICLR 2024 (Chen et al.)) generally ignore the equal importance of data diversity and complexity. In this work, we aim to design a diversity-aware data selection strategy and creatively propose using sparse autoencoders to tackle the challenge of data diversity measure. In addition, sparse autoencoders can also provide more interpretability of model behavior and explain, e.g., the surprising effectiveness of selecting the longest response (ICML 2024 (Zhao et al.)). Using effective data selection, we experimentally prove that models trained on our selected data can outperform other methods in terms of model capabilities, reduce training cost, and potentially gain more control over model behaviors.</article>","contentLength":1181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Semantic Decomposition and Selective Context Filtering -- Text Processing Techniques for Context-Aware NLP-Based Systems","url":"https://arxiv.org/abs/2502.14048","date":1740114000,"author":"","guid":8349,"unread":true,"content":"<article>arXiv:2502.14048v1 Announce Type: new \nAbstract: In this paper, we present two techniques for use in context-aware systems: Semantic Decomposition, which sequentially decomposes input prompts into a structured and hierarchal information schema in which systems can parse and process easily, and Selective Context Filtering, which enables systems to systematically filter out specific irrelevant sections of contextual information that is fed through a system's NLP-based pipeline. We will explore how context-aware systems and applications can utilize these two techniques in order to implement dynamic LLM-to-system interfaces, improve an LLM's ability to generate more contextually cohesive user-facing responses, and optimize complex automated workflows and pipelines.</article>","contentLength":771,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Towards a Learning Theory of Representation Alignment","url":"https://arxiv.org/abs/2502.14047","date":1740114000,"author":"","guid":8350,"unread":true,"content":"<article>arXiv:2502.14047v1 Announce Type: new \nAbstract: It has recently been argued that AI models' representations are becoming aligned as their scale and performance increase. Empirical analyses have been designed to support this idea and conjecture the possible alignment of different representations toward a shared statistical model of reality. In this paper, we propose a learning-theoretic perspective to representation alignment. First, we review and connect different notions of alignment based on metric, probabilistic, and spectral ideas. Then, we focus on stitching, a particular approach to understanding the interplay between different representations in the context of a task. Our main contribution here is relating properties of stitching to the kernel alignment of the underlying representation. Our results can be seen as a first step toward casting representation alignment as a learning-theoretic problem.</article>","contentLength":918,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Position: There are no Champions in Long-Term Time Series Forecasting","url":"https://arxiv.org/abs/2502.14045","date":1740114000,"author":"","guid":8351,"unread":true,"content":"<article>arXiv:2502.14045v1 Announce Type: new \nAbstract: Recent advances in long-term time series forecasting have introduced numerous complex prediction models that consistently outperform previously published architectures. However, this rapid progression raises concerns regarding inconsistent benchmarking and reporting practices, which may undermine the reliability of these comparisons. Our position emphasizes the need to shift focus away from pursuing ever-more complex models and towards enhancing benchmarking practices through rigorous and standardized evaluation methods. To support our claim, we first perform a broad, thorough, and reproducible evaluation of the top-performing models on the most popular benchmark by training 3,500+ networks over 14 datasets. Then, through a comprehensive analysis, we find that slight changes to experimental setups or current evaluation metrics drastically shift the common belief that newly published results are advancing the state of the art. Our findings suggest the need for rigorous and standardized evaluation methods that enable more substantiated claims, including reproducible hyperparameter setups and statistical testing.</article>","contentLength":1176,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data","url":"https://arxiv.org/abs/2502.14044","date":1740114000,"author":"","guid":8352,"unread":true,"content":"<article>arXiv:2502.14044v1 Announce Type: new \nAbstract: Large multimodal models (LMMs) have shown impressive capabilities in a wide range of visual tasks. However, they often struggle with fine-grained visual reasoning, failing to identify domain-specific objectives and provide justifiable explanations for their predictions. To address this, we propose a novel visual rejection sampling framework to improve the cognition and explainability of LMMs using self-synthesized data. Specifically, visual fine-tuning requires images, queries, and target answers. Our approach begins by synthesizing interpretable answers that include human-verifiable visual features. These features are based on expert-defined concepts, carefully selected based on their alignment with the image content. After each round of fine-tuning, we apply a reward model-free filtering mechanism to select the highest-quality interpretable answers for the next round of tuning. This iterative process of data synthesis and fine-tuning progressively improves the model's ability to generate accurate and reasonable explanations. Experimental results demonstrate the effectiveness of our method in improving both the accuracy and explainability of specialized visual classification tasks.</article>","contentLength":1250,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Asking for Help Enables Safety Guarantees Without Sacrificing Effectiveness","url":"https://arxiv.org/abs/2502.14043","date":1740114000,"author":"","guid":8353,"unread":true,"content":"<article>arXiv:2502.14043v1 Announce Type: new \nAbstract: Most reinforcement learning algorithms with regret guarantees rely on a critical assumption: that all errors are recoverable. Recent work by Plaut et al. discarded this assumption and presented algorithms that avoid \"catastrophe\" (i.e., irreparable errors) by asking for help. However, they provided only safety guarantees and did not consider reward maximization. We prove that any algorithm that avoids catastrophe in their setting also guarantees high reward (i.e., sublinear regret) in any Markov Decision Process (MDP), including MDPs with irreversible costs. This constitutes the first no-regret guarantee for general MDPs. More broadly, our result may be the first formal proof that it is possible for an agent to obtain high reward while becoming self-sufficient in an unknown, unbounded, and high-stakes environment without causing catastrophe or requiring resets.</article>","contentLength":922,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation","url":"https://arxiv.org/abs/2502.14037","date":1740114000,"author":"","guid":8354,"unread":true,"content":"<article>arXiv:2502.14037v1 Announce Type: new \nAbstract: Despite their increasing performance, large language models still tend to reproduce training data, generate several repetitions, and focus on the most common grammatical structures and words. A possible cause is the decoding strategy adopted: the most common ones either consider only the most probable tokens, reducing output diversity, or increase the likelihood of unlikely tokens at the cost of output accuracy and correctness. In this paper, we propose a family of three new decoding methods by leveraging a mathematical analysis of the token probability distribution. In particular, the difference between consecutive, sorted probabilities can be used to avoid incorrect tokens and increase the chance of low-probable but accurate words. Experiments concerning math problem solving, extreme summarization, and the divergent association task show that our approach consistently performs at least as well as current alternatives in terms of quality and diversity.</article>","contentLength":1016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Activation with Knowledge Distillation for Energy-Efficient Spiking NN Ensembles","url":"https://arxiv.org/abs/2502.14023","date":1740114000,"author":"","guid":8355,"unread":true,"content":"<article>arXiv:2502.14023v1 Announce Type: new \nAbstract: While foundation AI models excel at tasks like classification and decision-making, their high energy consumption makes them unsuitable for energy-constrained applications. Inspired by the brain's efficiency, spiking neural networks (SNNs) have emerged as a viable alternative due to their event-driven nature and compatibility with neuromorphic chips. This work introduces a novel system that combines knowledge distillation and ensemble learning to bridge the performance gap between artificial neural networks (ANNs) and SNNs. A foundation AI model acts as a teacher network, guiding smaller student SNNs organized into an ensemble, called Spiking Neural Ensemble (SNE). SNE enables the disentanglement of the teacher's knowledge, allowing each student to specialize in predicting a distinct aspect of it, while processing the same input. The core innovation of SNE is the adaptive activation of a subset of SNN models of an ensemble, leveraging knowledge-distillation, enhanced with an informed-partitioning (disentanglement) of the teacher's feature space. By dynamically activating only a subset of these student SNNs, the system balances accuracy and energy efficiency, achieving substantial energy savings with minimal accuracy loss. Moreover, SNE is significantly more efficient than the teacher network, reducing computational requirements by up to 20x with only a 2% drop in accuracy on the CIFAR-10 dataset. This disentanglement procedure achieves an accuracy improvement of up to 2.4% on the CIFAR-10 dataset compared to other partitioning schemes. Finally, we comparatively analyze SNE performance under noisy conditions, demonstrating enhanced robustness compared to its ANN teacher. In summary, SNE offers a promising new direction for energy-constrained applications.</article>","contentLength":1832,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A General Framework for Augmenting Lossy Compressors with Topological Guarantees","url":"https://arxiv.org/abs/2502.14022","date":1740114000,"author":"","guid":8356,"unread":true,"content":"<article>arXiv:2502.14022v1 Announce Type: new \nAbstract: Topological descriptors such as contour trees are widely utilized in scientific data analysis and visualization, with applications from materials science to climate simulations. It is desirable to preserve topological descriptors when data compression is part of the scientific workflow for these applications. However, classic error-bounded lossy compressors for volumetric data do not guarantee the preservation of topological descriptors, despite imposing strict pointwise error bounds. In this work, we introduce a general framework for augmenting any lossy compressor to preserve the topology of the data during compression. Specifically, our framework quantifies the adjustments (to the decompressed data) needed to preserve the contour tree and then employs a custom variable-precision encoding scheme to store these adjustments. We demonstrate the utility of our framework in augmenting classic compressors (such as SZ3, TTHRESH, and ZFP) and deep learning-based compressors (such as Neurcomp) with topological guarantees.</article>","contentLength":1079,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Where are the marathon Girls?: An Analysis of Female Representation in the Brazilian ICPC Programming Marathons","url":"https://arxiv.org/abs/2502.14020","date":1740114000,"author":"","guid":8357,"unread":true,"content":"<article>arXiv:2502.14020v1 Announce Type: new \nAbstract: Education motivated the encouragement of female participation in several areas of science and technology. Programming marathons have grown over the years and are events where programmers compete to solve coding challenges. However, despite scientific evidence that there is no intellectual difference between genders, women's participation is relatively low. This work seeks to understand the reason for this adherence, considering the gender issue in Programming Marathons over the last years, in a real context. This work aims to understand the context of female representativeness in which the intellectual aspects do not differ in gender. Still, there is a considerable discrepancy in female belonging.</article>","contentLength":755,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems","url":"https://arxiv.org/abs/2502.14019","date":1740114000,"author":"","guid":8358,"unread":true,"content":"<article>arXiv:2502.14019v1 Announce Type: new \nAbstract: As text generation systems' outputs are increasingly anthropomorphic -- perceived as human-like -- scholars have also raised increasing concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourced study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.</article>","contentLength":1046,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Want 'Em All (At Once) -- Ultrametric Cluster Hierarchies","url":"https://arxiv.org/abs/2502.14018","date":1740114000,"author":"","guid":8359,"unread":true,"content":"<article>arXiv:2502.14018v1 Announce Type: new \nAbstract: Hierarchical clustering is a powerful tool for exploratory data analysis, organizing data into a tree of clusterings from which a partition can be chosen. This paper generalizes these ideas by proving that, for any reasonable hierarchy, one can optimally solve any center-based clustering objective over it (such as $k$-means). Moreover, these solutions can be found exceedingly quickly and are themselves necessarily hierarchical. Thus, given a cluster tree, we show that one can quickly access a plethora of new, equally meaningful hierarchies. Just as in standard hierarchical clustering, one can then choose any desired partition from these new hierarchies. We conclude by verifying the utility of our proposed techniques across datasets, hierarchies, and partitioning schemes.</article>","contentLength":830,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cyber security of OT networks: A tutorial and overview","url":"https://arxiv.org/abs/2502.14017","date":1740114000,"author":"","guid":8360,"unread":true,"content":"<article>arXiv:2502.14017v1 Announce Type: new \nAbstract: This manuscript explores the cybersecurity challenges of Operational Technology (OT) networks, focusing on their critical role in industrial environments such as manufacturing, energy, and utilities. As OT systems increasingly integrate with Information Technology (IT) systems due to Industry 4.0 initiatives, they become more vulnerable to cyberattacks, which pose risks not only to data but also to physical infrastructure. The study examines key components of OT systems, such as SCADA (Supervisory Control and Data Acquisition), PLCs (Programmable Logic Controllers), and RTUs (Remote Terminal Units), and analyzes recent cyberattacks targeting OT environments. Furthermore, it highlights the security concerns arising from the convergence of IT and OT systems, examining attack vectors and the growing threats posed by malware, ransomware, and nation-state actors. Finally, the paper discusses modern approaches and tools used to secure these environments, providing insights into improving the cybersecurity posture of OT networks.</article>","contentLength":1087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Appeal prediction for AI up-scaled Images","url":"https://arxiv.org/abs/2502.14013","date":1740114000,"author":"","guid":8361,"unread":true,"content":"<article>arXiv:2502.14013v1 Announce Type: new \nAbstract: DNN- or AI-based up-scaling algorithms are gaining in popularity due to the improvements in machine learning. Various up-scaling models using CNNs, GANs or mixed approaches have been published. The majority of models are evaluated using PSRN and SSIM or only a few example images. However, a performance evaluation with a wide range of real-world images and subjective evaluation is missing, which we tackle in the following paper. For this reason, we describe our developed dataset, which uses 136 base images and five different up-scaling methods, namely Real-ESRGAN, BSRGAN, waifu2x, KXNet, and Lanczos. Overall the dataset consists of 1496 annotated images. The labeling of our dataset focused on image appeal and has been performed using crowd-sourcing employing our open-source tool AVRate Voyager. We evaluate the appeal of the different methods, and the results indicate that Real-ESRGAN and BSRGAN are the best. Furthermore, we train a DNN to detect which up-scaling method has been used, the trained models have a good overall performance in our evaluation. In addition to this, we evaluate state-of-the-art image appeal and quality models, here none of the models showed a high prediction performance, therefore we also trained two own approaches. The first uses transfer learning and has the best performance, and the second model uses signal-based features and a random forest model with good overall performance. We share the data and implementation to allow further research in the context of open science.</article>","contentLength":1570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A double-layer placement algorithm for integrated circuit-based modules on printed circuit board","url":"https://arxiv.org/abs/2502.14012","date":1740114000,"author":"","guid":8362,"unread":true,"content":"<article>arXiv:2502.14012v1 Announce Type: new \nAbstract: Considering that the physical design of printed circuit board (PCB) follows the principle of modularized design, this paper proposes an automatic placement algorithm for functional modules. We first model the placement problem as a mixed-variable optimization problem, and then, developed tailored algorithms of global placement and legalization for the top-layer centralized placement subproblem and the bottom-layer pin-oriented placement subproblem. Numerical comparison demonstrates that the proposed mixed-variable optimization scheme can get optimized total wirelength of placement. Meanwhile, experimental results on several industrial PCB cases show that the developed centralized strategies can well accommodate the requirement of top-layer placement, and the pin-oriented global placement based on bin clustering contributes to optimized placement results meeting the requirement of pin-oriented design.</article>","contentLength":962,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge Devices","url":"https://arxiv.org/abs/2502.14011","date":1740114000,"author":"","guid":8363,"unread":true,"content":"<article>arXiv:2502.14011v1 Announce Type: new \nAbstract: The Internet of Things generates massive data streams, with edge computing emerging as a key enabler for online IoT applications and 5G networks. Edge solutions facilitate real-time machine learning inference, but also require continuous adaptation to concept drifts. Ensemble-based solutions improve predictive performance, but incur higher resource consumption, latency, and memory demands. This paper presents DFDT: Dynamic Fast Decision Tree, a novel algorithm designed for energy-efficient memory-constrained data stream mining. DFDT improves hoeffding tree growth efficiency by dynamically adjusting grace periods, tie thresholds, and split evaluations based on incoming data. It incorporates stricter evaluation rules (based on entropy, information gain, and leaf instance count), adaptive expansion modes, and a leaf deactivation mechanism to manage memory, allowing more computation on frequently visited nodes while conserving energy on others. Experiments show that the proposed framework can achieve increased predictive performance (0.43 vs 0.29 ranking) with constrained memory and a fraction of the runtime of VFDT or SVFDT.</article>","contentLength":1188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Which Attention Heads Matter for In-Context Learning?","url":"https://arxiv.org/abs/2502.14010","date":1740114000,"author":"","guid":8364,"unread":true,"content":"<article>arXiv:2502.14010v1 Announce Type: new \nAbstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to perform new tasks using only a few demonstrations in the prompt. Two different mechanisms have been proposed to explain ICL: induction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task. To better understand which of the two distinct mechanisms drives ICL, we study and compare induction heads and FV heads in 12 language models.\n  Through detailed ablations, we discover that few-shot ICL performance depends primarily on FV heads, especially in larger models. In addition, we uncover that FV and induction heads are connected: many FV heads start as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction facilitates learning the more complex FV mechanism that ultimately drives ICL.</article>","contentLength":980,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures","url":"https://arxiv.org/abs/2502.14008","date":1740114000,"author":"","guid":8365,"unread":true,"content":"<article>arXiv:2502.14008v1 Announce Type: new \nAbstract: The remarkable performance of large language models (LLMs) in various language tasks has attracted considerable attention. However, the ever-increasing size of these models presents growing challenges for deployment and inference. Structured pruning, an effective model compression technique, is gaining increasing attention due to its ability to enhance inference efficiency. Nevertheless, most previous optimization-based structured pruning methods sacrifice the uniform structure across layers for greater flexibility to maintain performance. The heterogeneous structure hinders the effective utilization of off-the-shelf inference acceleration techniques and impedes efficient configuration for continued training. To address this issue, we propose a novel masking learning paradigm based on minimax optimization to obtain the uniform pruned structure by optimizing the masks under sparsity regularization. Extensive experimental results demonstrate that our method can maintain high performance while ensuring the uniformity of the pruned model structure, thereby outperforming existing SOTA methods.</article>","contentLength":1154,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"d-Sketch: Improving Visual Fidelity of Sketch-to-Image Translation with Pretrained Latent Diffusion Models without Retraining","url":"https://arxiv.org/abs/2502.14007","date":1740114000,"author":"","guid":8366,"unread":true,"content":"<article>arXiv:2502.14007v1 Announce Type: new \nAbstract: Structural guidance in an image-to-image translation allows intricate control over the shapes of synthesized images. Generating high-quality realistic images from user-specified rough hand-drawn sketches is one such task that aims to impose a structural constraint on the conditional generation process. While the premise is intriguing for numerous use cases of content creation and academic research, the problem becomes fundamentally challenging due to substantial ambiguities in freehand sketches. Furthermore, balancing the trade-off between shape consistency and realistic generation contributes to additional complexity in the process. Existing approaches based on Generative Adversarial Networks (GANs) generally utilize conditional GANs or GAN inversions, often requiring application-specific data and optimization objectives. The recent introduction of Denoising Diffusion Probabilistic Models (DDPMs) achieves a generational leap for low-level visual attributes in general image synthesis. However, directly retraining a large-scale diffusion model on a domain-specific subtask is often extremely difficult due to demanding computation costs and insufficient data. In this paper, we introduce a technique for sketch-to-image translation by exploiting the feature generalization capabilities of a large-scale diffusion model without retraining. In particular, we use a learnable lightweight mapping network to achieve latent feature translation from source to target domain. Experimental results demonstrate that the proposed method outperforms the existing techniques in qualitative and quantitative benchmarks, allowing high-resolution realistic image synthesis from rough hand-drawn sketches.</article>","contentLength":1753,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Im2SurfTex: Surface Texture Generation via Neural Backprojection of Multi-View Images","url":"https://arxiv.org/abs/2502.14006","date":1740114000,"author":"","guid":8367,"unread":true,"content":"<article>arXiv:2502.14006v1 Announce Type: new \nAbstract: We present Im2SurfTex, a method that generates textures for input 3D shapes by learning to aggregate multi-view image outputs produced by 2D image diffusion models onto the shapes' texture space. Unlike existing texture generation techniques that use ad hoc backprojection and averaging schemes to blend multiview images into textures, often resulting in texture seams and artifacts, our approach employs a trained, feedforward neural module to boost texture coherency. The key ingredient of our module is to leverage neural attention and appropriate positional encodings of image pixels based on their corresponding 3D point positions, normals, and surface-aware coordinates as encoded in geodesic distances within surface patches. These encodings capture texture correlations between neighboring surface points, ensuring better texture continuity. Experimental results show that our module improves texture quality, achieving superior performance in high-resolution texture generation.</article>","contentLength":1036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Smaller But Better: Unifying Layout Generation with Smaller Large Language Models","url":"https://arxiv.org/abs/2502.14005","date":1740114000,"author":"","guid":8368,"unread":true,"content":"<article>arXiv:2502.14005v1 Announce Type: new \nAbstract: We propose LGGPT, an LLM-based model tailored for unified layout generation. First, we propose Arbitrary Layout Instruction (ALI) and Universal Layout Response (ULR) as the uniform I/O template. ALI accommodates arbitrary layout generation task inputs across multiple layout domains, enabling LGGPT to unify both task-generic and domain-generic layout generation hitherto unexplored. Collectively, ALI and ULR boast a succinct structure that forgoes superfluous tokens typically found in existing HTML-based formats, facilitating efficient instruction tuning and boosting unified generation performance. In addition, we propose an Interval Quantization Encoding (IQE) strategy that compresses ALI into a more condensed structure. IQE precisely preserves valid layout clues while eliminating the less informative placeholders, facilitating LGGPT to capture complex and variable layout generation conditions during the unified training process. Experimental results demonstrate that LGGPT achieves superior or on par performance compared to existing methods. Notably, LGGPT strikes a prominent balance between proficiency and efficiency with a compact 1.5B parameter LLM, which beats prior 7B or 175B models even in the most extensive and challenging unified scenario. Furthermore, we underscore the necessity of employing LLMs for unified layout generation and suggest that 1.5B could be an optimal parameter size by comparing LLMs of varying scales. Code is available at https://github.com/NiceRingNode/LGGPT.</article>","contentLength":1558,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Inter3D: A Benchmark and Strong Baseline for Human-Interactive 3D Object Reconstruction","url":"https://arxiv.org/abs/2502.14004","date":1740114000,"author":"","guid":8369,"unread":true,"content":"<article>arXiv:2502.14004v1 Announce Type: new \nAbstract: Recent advancements in implicit 3D reconstruction methods, e.g., neural rendering fields and Gaussian splatting, have primarily focused on novel view synthesis of static or dynamic objects with continuous motion states. However, these approaches struggle to efficiently model a human-interactive object with n movable parts, requiring 2^n separate models to represent all discrete states. To overcome this limitation, we propose Inter3D, a new benchmark and approach for novel state synthesis of human-interactive objects. We introduce a self-collected dataset featuring commonly encountered interactive objects and a new evaluation pipeline, where only individual part states are observed during training, while part combination states remain unseen. We also propose a strong baseline approach that leverages Space Discrepancy Tensors to efficiently modelling all states of an object. To alleviate the impractical constraints on camera trajectories across training states, we propose a Mutual State Regularization mechanism to enhance the spatial density consistency of movable parts. In addition, we explore two occupancy grid sampling strategies to facilitate training efficiency. We conduct extensive experiments on the proposed benchmark, showcasing the challenges of the task and the superiority of our approach.</article>","contentLength":1367,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rectified Lagrangian for Out-of-Distribution Detection in Modern Hopfield Networks","url":"https://arxiv.org/abs/2502.14003","date":1740114000,"author":"","guid":8370,"unread":true,"content":"<article>arXiv:2502.14003v1 Announce Type: new \nAbstract: Modern Hopfield networks (MHNs) have recently gained significant attention in the field of artificial intelligence because they can store and retrieve a large set of patterns with an exponentially large memory capacity. A MHN is generally a dynamical system defined with Lagrangians of memory and feature neurons, where memories associated with in-distribution (ID) samples are represented by attractors in the feature space. One major problem in existing MHNs lies in managing out-of-distribution (OOD) samples because it was originally assumed that all samples are ID samples. To address this, we propose the rectified Lagrangian (RegLag), a new Lagrangian for memory neurons that explicitly incorporates an attractor for OOD samples in the dynamical system of MHNs. RecLag creates a trivial point attractor for any interaction matrix, enabling OOD detection by identifying samples that fall into this attractor as OOD. The interaction matrix is optimized so that the probability densities can be estimated to identify ID/OOD. We demonstrate the effectiveness of RecLag-based MHNs compared to energy-based OOD detection methods, including those using state-of-the-art Hopfield energies, across nine image datasets.</article>","contentLength":1265,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Human-Artificial Interaction in the Age of Agentic AI: A System-Theoretical Approach","url":"https://arxiv.org/abs/2502.14000","date":1740114000,"author":"","guid":8371,"unread":true,"content":"<article>arXiv:2502.14000v1 Announce Type: new \nAbstract: This paper presents a novel perspective on human-computer interaction (HCI), framing it as a dynamic interplay between human and computational agents within a networked system. Going beyond traditional interface-based approaches, we emphasize the importance of coordination and communication among heterogeneous agents with different capabilities, roles, and goals. A key distinction is made between multi-agent systems (MAS) and Centaurian systems, which represent two different paradigms of human-AI collaboration. MAS maintain agent autonomy, with structured protocols enabling cooperation, while Centaurian systems deeply integrate human and AI capabilities, creating unified decision-making entities.\n  To formalize these interactions, we introduce a framework for communication spaces, structured into surface, observation, and computation layers, ensuring seamless integration between MAS and Centaurian architectures, where colored Petri nets effectively represent structured Centaurian systems and high-level reconfigurable networks address the dynamic nature of MAS.\n  Our research has practical applications in autonomous robotics, human-in-the-loop decision making, and AI-driven cognitive architectures, and provides a foundation for next-generation hybrid intelligence systems that balance structured coordination with emergent behavior.</article>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DP-Adapter: Dual-Pathway Adapter for Boosting Fidelity and Text Consistency in Customizable Human Image Generation","url":"https://arxiv.org/abs/2502.13999","date":1740114000,"author":"","guid":8372,"unread":true,"content":"<article>arXiv:2502.13999v1 Announce Type: new \nAbstract: With the growing popularity of personalized human content creation and sharing, there is a rising demand for advanced techniques in customized human image generation. However, current methods struggle to simultaneously maintain the fidelity of human identity and ensure the consistency of textual prompts, often resulting in suboptimal outcomes. This shortcoming is primarily due to the lack of effective constraints during the simultaneous integration of visual and textual prompts, leading to unhealthy mutual interference that compromises the full expression of both types of input. Building on prior research that suggests visual and textual conditions influence different regions of an image in distinct ways, we introduce a novel Dual-Pathway Adapter (DP-Adapter) to enhance both high-fidelity identity preservation and textual consistency in personalized human image generation. Our approach begins by decoupling the target human image into visually sensitive and text-sensitive regions. For visually sensitive regions, DP-Adapter employs an Identity-Enhancing Adapter (IEA) to preserve detailed identity features. For text-sensitive regions, we introduce a Textual-Consistency Adapter (TCA) to minimize visual interference and ensure the consistency of textual semantics. To seamlessly integrate these pathways, we develop a Fine-Grained Feature-Level Blending (FFB) module that efficiently combines hierarchical semantic features from both pathways, resulting in more natural and coherent synthesis outcomes. Additionally, DP-Adapter supports various innovative applications, including controllable headshot-to-full-body portrait generation, age editing, old-photo to reality, and expression editing.</article>","contentLength":1758,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SigStyle: Signature Style Transfer via Personalized Text-to-Image Models","url":"https://arxiv.org/abs/2502.13997","date":1740114000,"author":"","guid":8373,"unread":true,"content":"<article>arXiv:2502.13997v1 Announce Type: new \nAbstract: Style transfer enables the seamless integration of artistic styles from a style image into a content image, resulting in visually striking and aesthetically enriched outputs. Despite numerous advances in this field, existing methods did not explicitly focus on the signature style, which represents the distinct and recognizable visual traits of the image such as geometric and structural patterns, color palettes and brush strokes etc. In this paper, we introduce SigStyle, a framework that leverages the semantic priors that embedded in a personalized text-to-image diffusion model to capture the signature style representation. This style capture process is powered by a hypernetwork that efficiently fine-tunes the diffusion model for any given single style image. Style transfer then is conceptualized as the reconstruction process of content image through learned style tokens from the personalized diffusion model. Additionally, to ensure the content consistency throughout the style transfer process, we introduce a time-aware attention swapping technique that incorporates content information from the original image into the early denoising steps of target image generation. Beyond enabling high-quality signature style transfer across a wide range of styles, SigStyle supports multiple interesting applications, such as local style transfer, texture transfer, style fusion and style-guided text-to-image generation. Quantitative and qualitative evaluations demonstrate our approach outperforms existing style transfer methods for recognizing and transferring the signature styles.</article>","contentLength":1640,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis","url":"https://arxiv.org/abs/2502.13996","date":1740114000,"author":"","guid":8374,"unread":true,"content":"<article>arXiv:2502.13996v1 Announce Type: new \nAbstract: Due to the widespread use of LLMs and the rising critical ethical and safety concerns, LLM unlearning methods have been developed to remove harmful knowledge and undesirable capabilities. In this context, evaluations are mostly based on single-value metrics such as QA accuracy. However, these metrics often fail to capture the nuanced retention of harmful knowledge components, making it difficult to assess the true effectiveness of unlearning. To address this issue, we propose UNCD (UNlearning evaluation via Cognitive Diagnosis), a novel framework that leverages Cognitive Diagnosis Modeling for fine-grained evaluation of LLM unlearning. Our dedicated benchmark, UNCD-Cyber, provides a detailed assessment of the removal of dangerous capabilities. Moreover, we introduce UNCD-Agent, which refines unlearning by diagnosing knowledge remnants and generating targeted unlearning data. Extensive experiments across eight unlearning methods and two base models demonstrate that UNCD not only enhances evaluation but also effectively facilitates the removal of harmful LLM abilities.</article>","contentLength":1132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation","url":"https://arxiv.org/abs/2502.13995","date":1740114000,"author":"","guid":8375,"unread":true,"content":"<article>arXiv:2502.13995v1 Announce Type: new \nAbstract: Tuning-free approaches adapting large-scale pre-trained video diffusion models for identity-preserving text-to-video generation (IPT2V) have gained popularity recently due to their efficacy and scalability. However, significant challenges remain to achieve satisfied facial dynamics while keeping the identity unchanged. In this work, we present a novel tuning-free IPT2V framework by enhancing face knowledge of the pre-trained video model built on diffusion transformers (DiT), dubbed FantasyID. Essentially, 3D facial geometry prior is incorporated to ensure plausible facial structures during video synthesis. To prevent the model from learning copy-paste shortcuts that simply replicate reference face across frames, a multi-view face augmentation strategy is devised to capture diverse 2D facial appearance features, hence increasing the dynamics over the facial expressions and head poses. Additionally, after blending the 2D and 3D features as guidance, instead of naively employing cross-attention to inject guidance cues into DiT layers, a learnable layer-aware adaptive mechanism is employed to selectively inject the fused features into each individual DiT layers, facilitating balanced modeling of identity preservation and motion dynamics. Experimental results validate our model's superiority over the current tuning-free IPT2V methods.</article>","contentLength":1400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Generative Detail Enhancement for Physically Based Materials","url":"https://arxiv.org/abs/2502.13994","date":1740114000,"author":"","guid":8376,"unread":true,"content":"<article>arXiv:2502.13994v1 Announce Type: new \nAbstract: We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to enhance the visual fidelity of materials with detail that is often tedious to author, by adding signs of wear, aging, weathering, etc. As these appearance details are often rooted in real-world processes, we leverage a generative image model trained on a large dataset of natural images with corresponding visuals in context. Starting with a given geometry, UV mapping, and basic appearance, we render multiple views of the object. We use these views, together with an appearance-defining text prompt, to condition a diffusion model. The details it generates are then backpropagated from the enhanced images to the material parameters via inverse differentiable rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the initial noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce geometric consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic of the material model used, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists.</article>","contentLength":1629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SelfAge: Personalized Facial Age Transformation Using Self-reference Images","url":"https://arxiv.org/abs/2502.13987","date":1740114000,"author":"","guid":8377,"unread":true,"content":"<article>arXiv:2502.13987v1 Announce Type: new \nAbstract: Age transformation of facial images is a technique that edits age-related person's appearances while preserving the identity. Existing deep learning-based methods can reproduce natural age transformations; however, they only reproduce averaged transitions and fail to account for individual-specific appearances influenced by their life histories. In this paper, we propose the first diffusion model-based method for personalized age transformation. Our diffusion model takes a facial image and a target age as input and generates an age-edited face image as output. To reflect individual-specific features, we incorporate additional supervision using self-reference images, which are facial images of the same person at different ages. Specifically, we fine-tune a pretrained diffusion model for personalized adaptation using approximately 3 to 5 self-reference images. Additionally, we design an effective prompt to enhance the performance of age editing and identity preservation. Experiments demonstrate that our method achieves superior performance both quantitatively and qualitatively compared to existing methods. The code and the pretrained model are available at https://github.com/shiiiijp/SelfAge.</article>","contentLength":1258,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["arxiv"]}