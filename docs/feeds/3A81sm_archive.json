{"id":"3A81sm","title":"Tech","displayTitle":"Tech","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":98,"items":[{"title":"Intel Killer E5000 Ethernet Support For Linux 6.15","url":"https://www.phoronix.com/news/Intel-Killer-E5000-Linux-6.15","date":1739706300,"author":"Michael Larabel","guid":536,"unread":true,"content":"<article>The upcoming Linux 6.15 kernel cycle will be adding support for Intel Killer E5000 Ethernet...</article>","contentLength":94,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Btrfs-Progs 6.13 Released With \"mkfs.btrfs --compress\" Support","url":"https://www.phoronix.com/news/Btrfs-Progs-6.13","date":1739705883,"author":"Michael Larabel","guid":535,"unread":true,"content":"<article>Btrfs-Progs 6.13 was released this weekend as the newest routine update to the user-space utilities for the Btrfs file-system...</article>","contentLength":128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"134k Lines Of Code Posted As Latest Effort For COBOL Support Within GCC","url":"https://www.phoronix.com/news/134k-Lines-v2-COBOL-For-GCC","date":1739704755,"author":"Michael Larabel","guid":534,"unread":true,"content":"<article>While it's an old language, in recent months there's been a renewed effort over a COBOL language front-end for the GCC compiler. There's been out-of-tree COBOL support for GCC that is working to get into the mainline GNU Compiler Collection codebase. This weekend saw the latest iteration of those patches amounting to 134k lines of new code...</article>","contentLength":344,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The TechBeat: Cybercrooks Are Using Fake Job Listings to Steal Crypto (2/16/2025)","url":"https://hackernoon.com/2-16-2025-techbeat?source=rss","date":1739689860,"author":"Techbeat","guid":523,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/diadkov\">@diadkov</a> [ 4 Min read ] \n Since March 2024, conspiracy theories about TikTok's ban have spread, citing espionage fears and geopolitical influences without solid evidence <a href=\"https://hackernoon.com/is-the-tiktok-ban-a-cover-up-the-internet-thinks-so\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/moonlock\">@moonlock</a> [ 19 Min read ] \n Moonlock Lab dives deep into a campaign tricking blockchain developers with fake job interviews to deploy malware that installs a backdoor and targets MetaMask. <a href=\"https://hackernoon.com/cybercrooks-are-using-fake-job-listings-to-steal-crypto\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/@javar97\">@@javar97</a> [ 7 Min read ] \n According to Stack Overflow's 2024 survey, 76% of developers are using or planning to use AI tools. <a href=\"https://hackernoon.com/ai-coding-tools-are-still-in-the-randd-stage\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bill-achola\">@bill-achola</a> [ 3 Min read ] \n Who really profits in a startup? Our deep dive into startup salaries reveals how executives secure big paydays while employees take on the risk. <a href=\"https://hackernoon.com/i-learned-the-hard-way-that-startup-high-executives-profit-while-employees-struggle\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ntoskrnl\">@ntoskrnl</a> [ 8 Min read ] \n Security mechanisms under the hood of simple file actions <a href=\"https://hackernoon.com/securitys-moving-parts-01-linux-access-control-mechanisms\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bigredeye\">@bigredeye</a> [ 21 Min read ] \n Perforator is a continuous profiling system developed by Yandex, now open-sourced.   <a href=\"https://hackernoon.com/yandexs-high-performance-profiler-is-going-open-source\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/andrei9735\">@andrei9735</a> [ 6 Min read ] \n Link prediction aims to predict the likelihood of a future or missing connection between nodes in a network.  <a href=\"https://hackernoon.com/learn-to-create-an-algorithm-that-can-predict-user-behaviors-using-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/thomascherickal\">@thomascherickal</a> [ 25 Min read ] \n Deep Research Prompts: Explore 30 ambitous, impactful ideas using emerging tech to tackle global crises. Discover research with world-changing potential. <a href=\"https://hackernoon.com/30-world-changing-prompts-openais-ai-singularity-deep-research-has-arrived\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bigmao\">@bigmao</a> [ 6 Min read ] \n The case against content marketing, and how to do inbound marketing in the post-content age.  <a href=\"https://hackernoon.com/no-startup-has-ever-failed-because-it-didnt-have-a-blog\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/vinitabansal\">@vinitabansal</a> [ 9 Min read ] \n While aggressive managers are difficult, they aren’t impossible to work with. With the right strategies, you can turn them around. <a href=\"https://hackernoon.com/dealing-with-an-aggressive-manager-is-simpler-than-you-think\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/silkdrive\">@silkdrive</a> [ 4 Min read ] \n Discover the future of software development with vibe coding—where creativity comes first, and coding happens effortlessly with AI. <a href=\"https://hackernoon.com/vibe-coding-creativity-without-code\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/editingprotocol\">@editingprotocol</a> [ 4 Min read ] \n If you want to become a top writer, here are 3 tips to help you rise to the cream of the crop.  <a href=\"https://hackernoon.com/climb-the-ranks-3-actionable-tips-to-become-a-top-writer\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/abhiyanampally_kob9nse8\">@abhiyanampally_kob9nse8</a> [ 40 Min read ] \n Dive into the comparitive analysis between logarithmic and floating-point arithmetic in neural nets using the commonly used MNIST dataset. <a href=\"https://hackernoon.com/deep-learning-runs-on-floating-point-math-what-if-thats-a-mistake\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/blackheart\">@blackheart</a> [ 6 Min read ] \n In Barbie, Ken struggles with identity, feeling like he exists in Barbie’s shadow. Many cybersecurity specialists can relate. <a href=\"https://hackernoon.com/the-ken-dilemma-in-cybersecurity\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/step\">@step</a> [ 6 Min read ] \n Language is a component of human consciousness. AI has a conversational and relatable language capability, could that be a fraction of consciousness? <a href=\"https://hackernoon.com/so-how-does-one-really-determine-ai-is-conscious\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mesciusinc\">@mesciusinc</a> [ 10 Min read ] \n Learn everything you need to know about the best Blazor UI Components and how to use them in your application. <a href=\"https://hackernoon.com/the-top-blazor-ui-components-everything-you-need-to-know\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/brightdata\">@brightdata</a> [ 8 Min read ] \n Let's see how OpenAI's Operator is handling CAPTCHAs and explore whether this is the best solution! <a href=\"https://hackernoon.com/openais-operator-vs-captchas-whos-winning\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/andrei9735\">@andrei9735</a> [ 7 Min read ] \n In this post we'll continue working on link prediction with the Twitch dataset. <a href=\"https://hackernoon.com/before-ai-predicts-your-next-friend-it-needs-to-do-this-first\">Read More.</a></p>","contentLength":2888,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Bugs Could Delay Upgrades for Both Siri and Alexa","url":"https://apple.slashdot.org/story/25/02/16/0138205/ai-bugs-could-delay-upgrades-for-both-siri-and-alexa?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739684040,"author":"EditorDavid","guid":512,"unread":true,"content":"Bloomberg reports that Apple's long-promised overhaul for Siri \"is facing engineering problems and software bugs, threatening to postpone or limit its release, according to people with knowledge of the matter....\"\n\nLast June, Apple touted three major enhancements coming to Siri: \n\n- the ability to tap into a customer's data to better answer queries and take actions.\n- a new system that would let the assistant more precisely control apps.\n- the capability to see what's currently on a device's screen and use that context to better serve users.... \n\nThe goal is to ultimately offer a more versatile Siri that can seamlessly tap into customers' information and communication. For instance, users will be able to ask for a file or song that they discussed with a friend over text. Siri would then automatically retrieve that item. Apple also has demonstrated the ability for Siri to quickly locate someone's driver's license number by reviewing their photos... Inside Apple, many employees testing the new Siri have found that these features don't yet work consistently... \nThe control enhancements — an upgraded version of something called App Intents — are central to the operation of the company's upcoming smart home hub. That product, an AI device for controlling smart home appliances and FaceTime, is slated for release later this year. \n\nAnd Amazon is also struggling with an AI upgrade for its digital assistant, reports the Washington Post:\nThe \"smarter and more conversational\" version of Alexa will not be available until March 31 or later, the employee said, at least a year and a half after it was initially announced in response to competition from OpenAI's ChatGPT. Internal messages seen by The Post confirmed the launch was originally scheduled for this month but was subsequently moved to the end of March... According to internal documents seen by The Post, new features of the subscriber-only, AI-powered Alexa could include the ability to adopt a personality, recall conversations, order takeout or call a taxi. Some of the new Alexa features are similar to Alexa abilities that were previously available free through partnerships with companies like Grubhub and Uber... \nThe AI-enhanced version of Alexa in development has been repeatedly delayed due to problems with incorrect answers, the employee working on the launch told The Post. As a popular product that is a decade old, the Alexa brand is valuable, and the company is hesitant to risk customer trust by launching a product that is not reliable, the person said.\n\n","contentLength":2551,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Death of OpenAI whistleblower deemed suicide in new autopsy report","url":"https://techcrunch.com/2025/02/15/death-of-openai-whistleblower-deemed-suicide-in-new-autopsy-report/","date":1739682712,"author":"Connie Loizos","guid":511,"unread":true,"content":"<p>Suchir Balaji, a former OpenAI employee, was found dead in his San Francisco apartment on Nov. 26; on Friday, the city’s medical examiner ruled his death a suicide, countering suspicions by his family that had fueled widespread speculation online. Balaji made headlines in October when he accused OpenAI of illegally using copyrighted material to train […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ask Slashdot: What Would It Take For You to Trust an AI?","url":"https://ask.slashdot.org/story/25/02/15/2047258/ask-slashdot-what-would-it-take-for-you-to-trust-an-ai?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739673240,"author":"EditorDavid","guid":500,"unread":true,"content":"Long-time Slashdot reader shanen has been testing AI clients. (They report that China's DeepSeek \"turned out to be extremely good at explaining why I should not trust it. Every computer security problem I ever thought of or heard about and some more besides.\") \n\nThen they wondered if there's also government censorship:\n\nIt's like the accountant who gets asked what 2 plus 2 is. After locking the doors and shading all the windows, the accountant whispers in your ear: \"What do you want it to be...?\" So let me start with some questions about DeepSeek in particular. Have you run it locally and compared the responses with the website's responses? My hypothesis is that your mileage should differ... \n\nIt's well established that DeepSeek doesn't want to talk about many \"political\" topics. Is that based on a distorted model of the world? Or is the censorship implemented in the query interface after the model was trained? My hypothesis is that it must have been trained with lots of data because the cost of removing all of the bad stuff would have been prohibitive... Unless perhaps another AI filtered the data first? \nBut their real question is: what would it take to trust an AI? \"Trust\" can mean different things, including data-collection policies. (\"I bet most of you trust Amazon and Amazon's secret AIs more than you should...\" shanen suggests.) Can you use an AI system without worrying about its data-retention policies?\n \n\nAnd they also ask how many Slashdot readers have read Ken Thompson's \"Reflections on Trusting Trust\", which raises the question of whether you can ever trust code you didn't create yourself. So is there any way an AI system can assure you its answers are accurate and trustworthy, and that it's safe to use? Share your own thoughts and experiences in the comments. \nWhat would it take for you to trust an AI?","contentLength":1846,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreeBSD 13.5 Overcomes UFS Y2038 Problem To Push It Out To Year 2106","url":"https://www.phoronix.com/news/FreeBSD-13.5-Beta-2","date":1739669445,"author":"Michael Larabel","guid":488,"unread":true,"content":"<article>Following last week's FreeBSD 13.5 Beta 1 release to kick off this next FreeBSD 13 point release that will also end the series, FreeBSD 13.5 Beta 2 is out this weekend for testing...</article>","contentLength":182,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Despite Plans for AI-Powered Search, Reddit's Stock Fell 14% Thsi Week","url":"https://tech.slashdot.org/story/25/02/16/007234/despite-plans-for-ai-powered-search-reddits-stock-fell-14-thsi-week?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739664540,"author":"EditorDavid","guid":481,"unread":true,"content":"\"Reddit Answers\" uses generative AI to answer questions using what past Reddittors have posted. Announced in December, Reddit now plans to integrate it into their search results, reports TechCrunch, with Reddit's CEO saying the idea has \"incredible monetization potential.\" \n\nAnd yet Reddit's stock fell 14% this week. CNBC's headline? \"Reddit shares plunge after Google algorithm change contributes to miss in user numbers.\"\n\n\nA Google search algorithm change caused some \"volatility\" with user growth in the fourth quarter, but the company's search-related traffic has since recovered in the first quarter, Reddit CEO Steve Huffman said in a letter to shareholders. \"What happened wasn't unusual — referrals from search fluctuate from time to time, and they primarily affect logged-out users,\" Huffman wrote. \"Our teams have navigated numerous algorithm updates and did an excellent job adapting to these latest changes effectively....\" Reddit has said it is working to convince logged-out users to create accounts as logged-in users, which are more lucrative for its business. \n\n\nAs Yahoo Finance once pointed out, Reddit knew this day would come, acknowledging in its IPO filing that \"changes in internet search engine algorithms and dynamics could have a negative impact on traffic for our website and, ultimately, our business.\" And in the last three months of 2024 Reddit's daily active users dropped, Yahoo Finance reported this week. But logged-in users increased by 400,000 — while logged-out users dropped by 600,000 (their first drop in almost two years). \n\nMarketwatch notes that analyst Josh Beck sees this as a buying opportunity for Reddit's stock:\nBeck pointed to comments from Reddit's management regarding a sharp recovery in daily active unique users. That was likely driven by Google benefiting from deeper Reddit crawling, by the platform uncollapsing comments in search results and by a potential benefit from spam-reduction algorithm updates, according to the analyst. \"While the report did not clear our anticipated bar, we walk away encouraged by international upside,\" he wrote.\n","contentLength":2110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Despite Plans for AI-Powered Search, Reddit's Stock Fell 14% This Week","url":"https://tech.slashdot.org/story/25/02/16/007234/despite-plans-for-ai-powered-search-reddits-stock-fell-14-this-week?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739664540,"author":"EditorDavid","guid":484,"unread":true,"content":"\"Reddit Answers\" uses generative AI to answer questions using what past Reddittors have posted. Announced in December, Reddit now plans to integrate it into their search results, reports TechCrunch, with Reddit's CEO saying the idea has \"incredible monetization potential.\" \n\nAnd yet Reddit's stock fell 14% this week. CNBC's headline? \"Reddit shares plunge after Google algorithm change contributes to miss in user numbers.\"\n\n\nA Google search algorithm change caused some \"volatility\" with user growth in the fourth quarter, but the company's search-related traffic has since recovered in the first quarter, Reddit CEO Steve Huffman said in a letter to shareholders. \"What happened wasn't unusual — referrals from search fluctuate from time to time, and they primarily affect logged-out users,\" Huffman wrote. \"Our teams have navigated numerous algorithm updates and did an excellent job adapting to these latest changes effectively....\" Reddit has said it is working to convince logged-out users to create accounts as logged-in users, which are more lucrative for its business. \n\n\nAs Yahoo Finance once pointed out, Reddit knew this day would come, acknowledging in its IPO filing that \"changes in internet search engine algorithms and dynamics could have a negative impact on traffic for our website and, ultimately, our business.\" And in the last three months of 2024 Reddit's daily active users dropped, Yahoo Finance reported this week. But logged-in users increased by 400,000 — while logged-out users dropped by 600,000 (their first drop in almost two years). \n\nMarketwatch notes that analyst Josh Beck sees this as a buying opportunity for Reddit's stock:\nBeck pointed to comments from Reddit's management regarding a sharp recovery in daily active unique users. That was likely driven by Google benefiting from deeper Reddit crawling, by the platform uncollapsing comments in search results and by a potential benefit from spam-reduction algorithm updates, according to the analyst. \"While the report did not clear our anticipated bar, we walk away encouraged by international upside,\" he wrote.\n","contentLength":2110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"China's 'Salt Typhoon' Hackers Continue to Breach Telecoms Despite US Sanctions","url":"https://it.slashdot.org/story/25/02/15/2244220/chinas-salt-typhoon-hackers-continue-to-breach-telecoms-despite-us-sanctions?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739659620,"author":"EditorDavid","guid":470,"unread":true,"content":"\"Security researchers say the Chinese government-linked hacking group, Salt Typhoon, is continuing to compromise telecommunications providers,\" reports TechCrunch, \"despite the recent sanctions imposed by the U.S. government on the group.\" \n\nTechRadar reports that the Chinese state-sponsored threat actor is \"hitting not just American organizations, but also those from the UK, South Africa, and elsewhere around the world.\"\n\n\n\n\nThe latest intrusions were spotted by cybersecurity researchers from Recorded Future, which said the group is targeting internet-exposed web interfaces of Cisco's IOS software that powers different routers and switches. These devices have known vulnerabilities that the threat actors are actively exploiting to gain initial access, root privileges, and more. More than 12,000 Cisco devices were found connected to the wider internet, and exposed to risk, Recorded Future further explained. However, Salt Typhoon is focusing on a \"smaller subset\" of telecoms and university networks. \n\n\"The hackers attempted to exploit vulnerabilities in at least 1,000 Cisco devices,\" reports NextGov, \"allowing them to access higher-level privileges of the hardware and change their configuration settings to allow for persistent access to the networks they're connected on... Over half of the Cisco appliances targeted by Salt Typhoon were located in the U.S., South America and India, with the rest spread across more than 100 countries.\"\nBetween December and January, the unit, widely known as Salt Typhoon, \"possibly targeted\" — based on devices that were accessed — offices in the University of California, Los Angeles, California State University, Loyola Marymount University and Utah Tech University, according to a report from cyber threat intelligence firm Recorded Future... The Cisco devices were mainly associated with telecommunications firms, but 13 of them were linked to the universities in the U.S. and some in other nations... \"Often involved in cutting-edge research, universities are prime targets for Chinese state-sponsored threat activity groups to acquire valuable research data and intellectual property,\" said the report, led by the company's Insikt Group, which oversees its threat research. \n\nThe cyberspies also compromised Cisco platforms at a U.S.-based affiliate of a prominent United Kingdom telecom operator and a South African provider, both unnamed, the findings added. The hackers also \"carried out a reconnaissance of multiple IP addresses\" owned by Mytel, a telecom operator based in Myanmar... \n\n\"In 2023, Cisco published a security advisory disclosing multiple vulnerabilities in the web UI feature in Cisco IOS XE software,\" a Cisco spokesperson said in a statement. \"We continue to strongly urge customers to follow recommendations outlined in the advisory and upgrade to the available fixed software release.\"\n\n","contentLength":2874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"North Carolina Amazon workers vote against unionizing","url":"https://techcrunch.com/2025/02/15/north-carolina-amazon-workers-vote-against-unionizing/","date":1739657733,"author":"Anthony Ha","guid":433,"unread":true,"content":"<p>Workers at an Amazon warehouse in Garner, North Carolina voted against unionizing in election results announced today. According to Carolina Amazonians United for Solidarity and Empowerment (CAUSE), the worker group seeking to form the union, 3,276 ballots were cast in the election, with 25.3% of votes in favor of unionizing and 74.7% against. The results […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":426,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The World's Most Printed 3D Model, 3DBenchy, Is Now Public Domain","url":"https://hardware.slashdot.org/story/25/02/15/1949206/the-worlds-most-printed-3d-model-3dbenchy-is-now-public-domain?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739655240,"author":"EditorDavid","guid":190,"unread":true,"content":"Hackaday reports:\n\nGood news for everyone who cannot get enough from improbably shaped boats that get referred to as a bench: the current owner (NTI Group) of the copyright has announced that 3DBenchy has been released into the public domain. This comes not too long after Prusa's Printables website had begun to purge all derived models to adhere to the 'no derivatives' license. According to NTI, the removal of these derived models was not requested by NTI, but by a third-party report, unbeknownst to NTI or the original creator of the model. Recognizing its importance to the community, 3DBenchy can now be downloaded &amp; modified freely. \n\n\nNTI worked together with the original creator [Daniel Norée] and former Creative Tools CEO [Paulo Kiefe] to transition 3DBenchy and the associated website to the public domain\n\nMore details at Tom's Hardware and Fabbaloo.","contentLength":867,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Design, Manufacturing and Open-Loop Control of a Soft Pneumatic Arm: Bending Experiments","url":"https://hackernoon.com/design-manufacturing-and-open-loop-control-of-a-soft-pneumatic-arm-bending-experiments?source=rss","date":1739653203,"author":"EScholar: Electronic Academic Papers for Scholars","guid":478,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing </p><p>4 Data Acquisition and Open-Loop Control </p><p>The first experiment consisted of analysing the deflection of a segment versus swelling time. For this purpose, one of the bladders was inflated continuously, in intervals of 100 ms. For each time, PAUL end coord</p><p>\\\nwhere x0 and y0 denote initial position of PAUL end.</p><p>\\\nSince the weight of the subsequent modules influences the behaviour of the first segment, the experiment was repeated by placing first one and then two additional segments. The results are shown in Figure 18.</p><p>\\\nAs can be seen, PAUL is capable of bending up to 40◦ to its vertical axis and the addition of new segments does not cause any noticeable decrease in its bending capacity.</p><p>\\\nAlthough it remains far from the 80◦ of [28] or the 70◦ of [32], Pneunet segments and therefore more flexible, this is an acceptable bending capacity. Moreover, the fact that it does not substantially lose its bending capacity by adding segments makes it possible to concatenate bending movements and thus overcome obstacles that a rigid robot would not be able to overcome.</p><p>\\\nIn conjunction with this, a validation test was proposed whose purpose was to demonstrate PAUL’s ability to flex thanks to its deformable geometry. The aim was to point points in lateral planes. The results of this experiment are shown in Figure 19. The images, extracted from the video of Appendix A, show how the manipulator can adopt different shapes, is able to bend up to 40◦ and adapt, in case of obstacles, to a wide variety of geometries, which undoubtedly makes PAUL a fundamental ally in inspection and exploration operations in very cluttered environments.</p>","contentLength":2317,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Intelligence could arrive on Vision Pro in April","url":"https://techcrunch.com/2025/02/15/apple-intelligence-could-arrive-on-vision-pro-in-april/","date":1739652968,"author":"Anthony Ha","guid":54,"unread":true,"content":"<p>Apple is planning to add Apple Intelligence to its Vision Pro headset in an update that could come as early as April, according to Bloomberg’s Mark Gurman. Just a couple weeks after Apple Intelligence was first announced in June 2024, Gurman reported that Apple was looking to bring its suite of AI tools to the […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"America's Office-Occupancy Rates Drop by Double Digits - and More in San Francisco","url":"https://it.slashdot.org/story/25/02/15/1716204/americas-office-occupancy-rates-drop-by-double-digits---and-more-in-san-francisco?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739651640,"author":"EditorDavid","guid":189,"unread":true,"content":"SFGate shares the latest data on America's office-occupancy rates:\n\nAccording to Placer.ai's January 2025 Office Index, office visits nationwide were 40.2% lower in January 2025 compared with pre-pandemic numbers from January 2019. \n\nBut San Francisco is dragging down the average, with a staggering 51.8% decline in office visits since January 2019 — the weakest recovery of any major metro. Kastle's 10-City Daily Analysis paints an equally grim picture. From Jan. 23, 2025, to Jan. 28, 2025, even on its busiest day (Tuesday), San Francisco's office occupancy rate was just 53.7%, significantly lower than Houston's (74.8%) and Chicago's (70.4%). And on Friday, Jan. 24, office attendance in [San Francisco] was at a meager 28.5%, the worst of any major metro tracked... \n\n\nMeanwhile, other cities are seeing much stronger rebounds. New York City is leading the return-to-office trend, with visits in January down just 19% from 2019 levels, while Miami saw a 23.5% decline, per Placer.ai data. \n\n\"Placer.ai uses cellphone location data to estimate foot traffic, while Kastle Systems measures badge swipes at office buildings with its security systems...\"","contentLength":1159,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Soft Robots and Smart Movement","url":"https://hackernoon.com/soft-robots-and-smart-movement?source=rss","date":1739650503,"author":"EScholar: Electronic Academic Papers for Scholars","guid":477,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing </p><p>4 Data Acquisition and Open-Loop Control </p><p>The size of the table to achieve acceptable kinematic modelling was set experimentally, as no previous references were available and previous works in the literature were very variable in terms of the number of data required. Furthermore, the possibility of an error occurring in the pneumatic system or the buffer of the vision acquisition system collapsing, together with the always present possibility of a leak in the segments, made it advisable to take data in small sessions and subsequently union of all of them. Since the data collection process was automated, this did not pose much of a problem.</p><p>\\\nAlthough the possibility that the ambient temperature was a factor that influenced the kinematics of the robot was considered during the dataset collection process, it was finally proven that small variations in temperature did not affect the behaviour.</p><p>\\\nTable 3 shows the datasets that were taken, the total time necessary to obtain them and the average time per point. It should be taken into account that not all captured positions were finally used, since, if the camera did not correctly detect the positions of the three beacon spheres, it could not calculate the orientation of the trihedral and therefore returned an error code. Of the 1200 samples collected, 5% had to be discarded, leaving 1146 finally usable. The average time per point, considering all the datasets collected, was 6.76 s, with a standard deviation of 0.63 s. The low variability between capture processes proves the effectiveness of the automated method designed.</p><p>\\\nOnce all the datasets were combined, the direct and inverse kinematic models presented here were validated. The validation of the direct model consisted of sending the robot a combination of inflation times and measuring the distance between the position reached, captured by the cameras, and that predicted by the table. Repeating the experiment for 40 points, the histogram of results presented in Figure 15 was observed. The average error is 4.27 mm, the median error 2.72 mm, and the standard deviation 1.99 mm.</p><p>\\\nThe high standard deviation and the shape of the histogram, tilted towards low values and with a very long tail, seem to indicate the existence of points where the model presents notable failures along with others with very good results. A future line of interest could be the detailed analysis of the workspace to locate where those regions of lower precision of the model are located and try to look for failures, perhaps leading to a greater density of points in the dataset.</p><p>\\\nIn the same way, the inverse kinematic model was tested. To do this, PAUL was given a reference position and orientation to achieve, the necessary times were calculated, using the procedures referred to in Equations (16), and (17) inflation was carried out. Subsequently, the position captured with the cameras was compared with the desired one.</p><p>\\\nAs expected, the existence of redundancies, in which equal position values are achieved with very different combinations of inflation times, introduces large uncertainties in the model, which the triangulation presented is not able to capture.</p><p>\\\nSpecifically, the inverse kinematic model has an average error of 10.78 mm, a median error of 9.22 mm and a standard deviation of 5.98 mm. While these errors may seem high, they are compared in Table 4 with other open-loop controllers presented in the literature. It can be clearly seen that they are in line with the results obtained and that they are even better than those obtained by smaller robots, where one would expect, due to the smaller working space, a higher accuracy (at least in data-driven models).</p><p>\\\nIt is worth highlighting, however, two experiments in which PAUL performed very satisfactorily, because the area of operation was restricted to a region where no redundancies were found to exist. They are available in the video of Appendix A.</p><p>\\\nIn the first of them, the robot was forced to reach a set of points located on the horizontal basis plane, also forcing the lower end of the last segment to be parallel to said</p><p>\\\nplane. In all of them, errors less than 7 mm were achieved. Figure 16 shows the results of said experiment. With the aim of facilitating the understanding of the experiment, the beacon was changed for a laser pointer that points to the desired points, on which targets with a radius of 5 mm have been marked, allowing the accuracy achieved to be checked.</p><p>\\\nIn the second experiment, shown in Figure 17, points on the lower horizontal plane were also taken as reference, but without imposing that the lower face of the robot should remain parallel to it. In this case, accuracies of 2 cm were achieved.</p><p>\\\nAlthough the inverse kinematic model therefore presents acceptable results, it must be commented that, in all these experiments, due to the geometry and material of the robot, at the moment in which PAUL reaches the desired position, it tends to acquire a movement damped oscillation. An attempt has been made to reduce it, despite everything, it is a very intrinsic phenomenon to the robot that is difficult to solve. A future line proposed, in this sense, is to try to rigidify the robot by introducing negative pressures that generate vacuum.</p>","contentLength":5938,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"This Week In Techdirt History: February 9th – 15th","url":"https://www.techdirt.com/2025/02/15/this-week-in-techdirt-history-february-9th-15th/","date":1739649600,"author":"Leigh Beadon","guid":310,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Apple Invites Its Users Into Major Years-Long Health Study","url":"https://apple.slashdot.org/story/25/02/15/0610248/apple-invites-its-users-into-major-years-long-health-study?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739648040,"author":"EditorDavid","guid":188,"unread":true,"content":"Can the iPhone, AirPods, or the Apple Watch play a role in improving health? Apple says they want to find out. \n\"In medical research, discoveries are often limited by the number of participants who can be recruited, the amount of data that can be captured, and the duration of a given study,\" the company said in a blog post this week. \"But Apple devices expand the possibilities...\"\nThis new longitudinal, virtual study aims to understand how data from technology — including Apple and third-party devices — can be used to predict, detect, monitor, and manage changes in participants' health. Additionally, researchers will explore connections across different areas of health. \nCNBC reports:\n\n\nThe new study will likely influence future product development. Apple CEO Tim Cook previously said he believes health features will be the company's \"most important contribution to mankind....\" \n\nThe Apple Health Study will be available through the company's Research app, and participation is voluntary. Users will select each data type they are willing to share with researchers, and they can stop sharing or completely discontinue their participation at any time. Apple has no access to participants' identifiable information, the company said... The project will last at least five years and may expand beyond that. \nA Harvard Medical School professor and cardiologist — also a principal investigator on the Apple Health Study — says \"We've only just begun to scratch the surface of how technology can improve our understanding of human health.\"","contentLength":1553,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Proactive IT Career Growth: Take Control of Your Professional Journey","url":"https://hackernoon.com/proactive-it-career-growth-take-control-of-your-professional-journey?source=rss","date":1739646022,"author":"Ekaterina","guid":476,"unread":true,"content":"<p>There are a lot of good articles about possible career tracks that you can pursue in IT, however, I haven’t seen many that might be used as actual guidance to move up the career ladder.</p><p>\\\nCurrently, I am working in a company that has very clear requirements for the engineers’ promotion and what can be used as sufficient evidence of fulfillment of those requirements. A combination of these two factors gave me the idea that additional information on this topic might help other engineers who are employed by companies that do not have it to build a strategy that will allow them to get to the next level.</p><p>\\\nIn almost any more or less mature IT company, a common career track for a software engineer is linear and looks almost the same:</p><p>Associate Software Engineer is optional and may or may not be presented in the common IT department structure for a very simple reason: it’s net-negative for the first 12 months as it requires a lot of hand-holding so not all companies have resources and time to allow such positions in their structure.</p><p>\\\nThe further career track will depend on your inclinations, what you enjoy doing, and whether you are ready for the shift in the way you’re working.</p><p>There’s nothing wrong with staying a senior software engineer if you like to allocate the majority of your time to coding. However, if you feel the need to empower others and lead that’s the right moment to weigh all expectations for each role, your strengths, the things that drive you, and pick the most suitable track for yourself.</p><p>\\\nDespite the visual simplicity of the tracks above, it’s not clear how to get closer to the right end. The following insights will apply to the companies that have:</p><ul><li><p>a hierarchical structure where each employer has a line manager</p></li><li><p>a genuine interest in employee development</p><p>\\n Why is the above-mentioned is important?==The answer is quite simple: from day one you have an ally - your line manager==.</p></li></ul><p>\\\nEach line manager’s efficiency is based on the output of each person reporting to them: the faster you grow - the bigger your output - the better the line manager’s efficiency. Given all this, sooner or later, after you have joined your company, your line manager will approach you with the question: “Where do you see yourself after a certain time?” If it is not happening and you have regular one-to-ones, feel free to add this as a topic for discussion in the agenda.</p><p>\\\nVoicing your intentions and setting a goal is just the first step of your way. The next step is to gather the list of requirements for the higher role and compile a list of achievements that can serve as evidence of your qualifications that you can use as a guide that you should follow to get from point A to point B. In companies with transparent promotion processes, this should be already in place.</p><p>\\\nIf this is not the case, you and your manager could compose one. Remember that this process is beneficial for both sides: you are getting an agreement that after certain achievements, you will be praised with the promotion and your line manager can get increased output from the team, so it’s a win-win case.</p><p>\\\nDifferent companies may have different requirements for certain positions, and I won’t claim that the ones below are universal and will suit everyone. The main purpose is to give you an idea it might look like if you need one that can be further tailored to your needs.</p><p>\\n Guidelines for evidence can be used as a roadmap that brings you to the desired destination. The next steps for the common track might be</p><ul><li>Check the team’s roadmap for suitable projects or change requests that might fit the purpose of the evidence.</li></ul><ul><li>Voice up your intentions to the line manager so they can assist with the suitable project allocation and provide information about its priority, business value, and when it can be picked up for development.</li></ul><ul><li>Spot any potential areas for improvement in code, observability, extensibility, and security perspectives and raise them as ownership tickets.</li></ul><ul><li><p>Familiarize yourself with the current recruitment process in your company and ask for shadowing during recruitment sessions. Ask to switch roles where someone more senior will shadow you and ask for feedback. \\n </p><p>This is a short list of the roles that will be covered from requirements/guidelines for evidence perspectives:</p></li><li><p>Junior Software Engineer Requirements</p></li><li><p>Software Engineer Requirements</p></li><li><p>Senior Software Engineer Requirements</p></li><li><p>Lead Engineer Requirements</p></li><li><p>Senior Engineering Lead Requirements</p></li></ul><h4><strong>==Junior Software Engineer Requirements==</strong></h4><p>|  |  |  |\n|----|----|----|\n|  | Delivers tasks \\n · Clear requirements are needed (business and system) \\n · Designs/implements limited-scope technical solutions \\n · Limited guidance is required | 1. List of tasks completed \\n o Tasks should be complex enough to mention them \\n o Deadlines are met \\n o No major quality issues \\n o Tasks were completed with no handholding \\n 2. Input from the line manager confirming that all the requirements are met. |\n|  | Applies best practices \\n · Learns and constantly applies best practices \\n · Proficient with various dev tools \\n · Investigates and fixes complicated problems/bugs | <strong>Feedback from the line manager and peers confirming that all the requirements are met.</strong> |</p><p>\\\n<strong>==Software Engineer Requirements==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Delivers change requests (features) \\n · Takes business requirements as input \\n · Breaks work into tasks with a sufficient level of detail on the solution (what needs to be done and when it’s done) and the implementation (how it should be done) \\n · Provides accurate estimates on a task/user story level \\n · Pairs with other engineers to deliver faster | List of change requests delivered, conforming to the following requirements: \\n 1. The change request has been fully delivered and the deadline was met. \\n 2. The discovery part was completed by the employee (tickets, estimates). \\n 3. The change request is complex enough from a technical perspective (more than 2 man-weeks for 1 engineer to implement it). \\n 4. The change request provides a meaningful impact on the business. \\n 5. The change request is signed off by the business and is running in production. \\n 6. The employee has demonstrated a sufficient level of autonomy and quality (based on the feedback from the tech lead and the engineering manager). |\n|  | Designs services \\n · Designs and implements smaller services while taking into account all of the non-functional aspects (extensibility, security, observability, etc) \\n · Writes high-quality code with full adoption of engineering practices and methodologies \\n · Participates in code reviews to enforce best practices \\n · Fixes the root causes behind bugs and problems encountered | At least two services designed conforming to the following requirements: \\n 1. It can be a new service or a complete redesign of the existing service. \\n 2. It can be a standalone service, a library, or a component consumed by other services. \\n 3. The service shouldn’t be trivial from a design perspective. \\n 4. The engineer should have followed the formal design process: \\n · Obtain business and system requirements \\n · Identify the bounded context \\n · Identify non-functional requirements \\n · Break down context into services \\n · Get feedback on the solution \\n · Implement it \\n 5. Service is implemented and is running in production. |</p><p>\\\n<strong>==Senior Software Engineer==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Delivers project phases (epics) \\n · Takes requirements and high-level system design as input \\n · Creates system design for the service or the component, decides on the technologies and engineering practices to be used \\n · Breaks work into tasks or user stories with a sufficient level of detail on the solution (what needs to be done and when it’s done) and the implementation (how it should be done) \\n · Provides accurate estimates on task/user story level \\n · Leads a small team to deliver the scope \\n · Unblocks their team, resolves issues, and removes impediments | List of project phases/epics delivered, conforming to the following requirements: \\n 1. The epic/project phase has been fully delivered and the deadline was met. \\n 2. The discovery part was completed by the employee (tickets, estimates). \\n 3. The epic/project phase is complex enough from a technical perspective (requires at least 2 engineers for &gt;= 2 weeks). \\n 4. The epic/project phase provides a meaningful impact on the business. \\n 5. The functionality is signed off by the business and is running in production. \\n 6. The employee has demonstrated a sufficient level of autonomy and quality (based on the feedback from the tech lead and engineering manager). \\n 7. The engineer participated in the implementation as a technical lead. |\n|  | Designs subsystems \\n · It is the same as for a Software Engineer but focuses on more complex services or subsystems \\n · Proficient in the cloud and distributed systems design and implementation | At least 3 services designed conforming to the following requirements: \\n 1. It can be a new service or a complete redesign of the existing service. \\n 2. It can be a standalone service, a library, or a component consumed by other services. \\n 3. The service shouldn’t be trivial from a design perspective. \\n 4. The engineer should have followed the formal design process: \\n a. Obtain business and system requirements \\n b. Identify bounded context \\n c. Identify non-functional requirements \\n d. Break down context into services \\n e. Get feedback on the solution \\n f. Implement it \\n 5. Service is implemented and is running in production. |\n|  | Proposes changes \\n · Challenges the status quo and the assumptions made \\n · Find ways to improve the platform, processes, working environment, and the tech team in general | At least three significant changes were proposed, which can be any of the following: \\n 1. Functionality: proposed a change request that was prioritized and implemented (change request should be substantial enough to be considered as a change, not a cosmetic change). \\n 2. People: interviewed an engineer who was hired and passed probation (junior software engineer or higher, considered as a change to the team). \\n 3. Ownership: proposed an ownership project (included in the ownership roadmap, approved by CTO). |</p><p>\\\n\\\n<strong>==Lead Engineer Requirements==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Tech lead for projects (project proposals) \\n · Takes business requirements as input \\n · Find the most effective solution for the business problem (research alternatives, validate solutions using no-code/low-code approaches) \\n · Creates system design for the new service or subsystem, decides on the technologies and engineering practices to be used \\n · Breaks work into epics with a sufficient level of detail on the solution (what needs to be done and when it’s done) and the implementation (how it should be done) \\n · Provides accurate estimates on the project level, commits to dates \\n · Acts as a tech lead for the entire project \\n · Unblocks their team, resolves issues, and removes impediments \\n · Manages technology, implementation, and operational risks | List of projects delivered, conforming to the following requirements: \\n 1. The solution for the problem was proposed by the employee and it is considered to be effective. I.e. multiple alternatives were evaluated, and the best alternative was chosen based on the low-code/no-code validation. \\n 2. The discovery part was completed by the employee (tickets, estimates). \\n 3. The solution was architected by the employee. \\n 4. The project needs to be a “feature” project initiated through a project proposal. \\n 5. The engineer participated in the implementation as a technical lead (see requirements column for more details). |\n|  | Drives technical changes (squad) \\n · Proposes and implements initiatives to improve system quality and reduce technical debt \\n · Proposes and implements changes to improve developer experience and productivity \\n · Advocates and enforces clean code and clean architecture | List of major changes introduced (usually at least four), conforming to the following requirements: \\n 1. The change provides meaningful improvement to system quality (e.g. platform improvements), developer experience, or developer productivity. The change affects the entire squad. \\n 2. The engineer doesn’t have to be the one who proposed the change. The engineer should be the primary driving force behind the change (e.g. designed, acted as a tech lead, participated in the implementation). The change can be delivered by an engineer or as a team effort. \\n 3. The change should be fully implemented and used by the squad/platform (the change should be “sticky” and provide enough value to keep it). \\n 4. The change should be significant enough to mention. |\n|  | Mentor \\n · Mentors and supports less experienced engineers \\n · Conducts technical interviews effectively \\n · Acts as a “magnet” for great engineers during hiring (be a decisive factor where we are in competition for good talent vs. another company) | Possible evidence: \\n 1. Engineers interviewed, who were hired and passed probation. \\n 2. Feedback from upskilled engineers. \\n 3. Training sessions are organized/delivered for the entire tech team (e.g. Tech Sync, Engineering Dojo). \\n 4. When leading a working group a list of changes proposed/implemented in the scope of the working group can be used as evidence. |</p><p>\\\n<strong>==Senior Engineering Lead==</strong></p><p>|  |  |  |\n|----|----|----|\n|  | Tech Lead for complex projects (project proposals) \\n Same as Lead Engineer, but focuses on problems that are complex from technical, organizational, or business perspectives \\n · The project requires coordination across multiple squads \\n · The project involves 3rd party technology provider or stakeholder (e.g. partnership) \\n · a new product build while the product is in the discovery mode \\n · high priority/urgency project with fixed deadlines and many unknown | List of projects delivered, conforming to the following requirements: \\n 1. The project is considered to be complex (see examples on the left). \\n 2. The project has been fully delivered (all deliverables + DoD) and the deadline was met. \\n 3. The solution for the problem was proposed by the employee and it is considered to be effective (i.e. multiple alternatives were evaluated, and the best alternative was selected based on the low-code/no-code validation). \\n 4. The discovery part was completed by the employee (system requirements, tickets, estimates). \\n 5. The solution was architected by the employee. The project has a high complexity from a system design perspective. \\n 6. An engineer participated in the implementation as a technical lead. |\n|  | Drives technical changes (tech) \\n · Same as E5 but on the tech level \\n · System owner for at least one non-functional aspect (e.g. security, observability, etc). | List of major changes introduced (usually at least 4), conforming to the following requirements: \\n 1. The change provides meaningful improvement to system quality (e.g. platform improvements), developer experience, or developer productivity. The change affects multiple squads (e.g. technology adoption). \\n 2. The engineer doesn’t have to be the one who proposed the change. The engineer should be the primary driving force behind the change (e.g. designed, acted as a tech lead, participated in the implementation). The change itself can be delivered by an engineer or as a team effort. \\n 3. The change should be fully implemented and used by multiple squads (changes should be “sticky” and provide enough value to keep it). \\n 4. The change should be significant enough to mention. It should be tracked on the “upcoming projects” page as an ownership project (ownership in this context means changes to the platform, tooling, processes, etc, not just platform-related changes). \\n 5. At least 2 changes should be related to the non-functional aspect owned by the individual. |\n|  | Recognized expert \\n · Recognized expert within a given area of expertise on a company level, acts as a technical point of contact in tech within their area of expertise \\n · Monitors trends/technologies within the area of expertise and communicates updates and findings \\n · Actively and regularly shares expertise with other engineers (workshops, tech talks, training) \\n · Facilitates collaboration to find solutions for complex problems (working groups, etc) \\n · Conducts technical interviews effectively \\n · Mentors and supports less experienced engineers, guide their career from a professional development perspective \\n · Acts as a “magnet” for great engineers during hiring (be a decisive factor where we are in competition for good talent vs. another company) | Possible evidence: \\n 1. Interviewed engineers, who were hired and passed probation. \\n 2. Feedback from upskilled engineers. \\n 3. Training sessions are organized/delivered for the entire tech team (e.g. Tech Sync, Engineering Dojo). \\n 4. Leading a working group, a list of changes proposed/implemented in the scope of the working group can be used as evidence. |</p><p>|  |  |  |\n|----|----|----|\n|  | Delivers squad roadmap \\n · Leads a squad of 3-6 engineers \\n · Acts as a project manager for multiple concurrent initiatives \\n · Able to deliver results having only business requirements as input (able to create and sign off system requirements) \\n · Focuses on business impact, driven by business value \\n · Communicates commitments, status, and risks to business stakeholders \\n · Ensures that all squad members have all the information they need \\n · Communicates to 3rd parties within the scope of initiatives/ownership \\n · Finds the right balance between feature delivery and system quality \\n · All requirements for Senior Software Engineer | New projects delivered by the squad conforming to the following requirements: \\n 1. Project initiated through a project proposal. \\n 2. The project has met its impact metrics, and the public commitment was met. \\n 3. Projects reported in the previous promotion cycle can’t be included in the list. |\n|  | Drives managerial changes (squad) \\n · Measures and continuously improves squad performance \\n · Identifies and establishes best practices within the squad with a focus on productivity \\n · Maintains high quality of delivery \\n · Ensures transparency on progress, risks, results | 1. Squad productivity (performance) metric values. \\n 2. Major changes (at least 4) introduced, conforming to the following requirements: \\n a. It solves a problem related to the owned squad or tribe, the problem needs to be included in the TOP 5 problems and agreed upon with the line manager. \\n b. The change should be fully implemented and used by the squad (change should be “sticky” and provide enough value to keep it). \\n c. The change should provide meaningful improvement to productivity, engagement, or quality of delivery. \\n d. The manager doesn’t have to be the one who proposed the change. The EM should be the primary driving force behind the change. The change can be delivered by an engineer or as a team effort. |\n|  | Line manager (&gt;=3 direct reports) \\n · Manages 3-6 direct reports \\n · Coaches and supports engineers \\n · Supports and guides career progressions \\n · Reconciles differences of opinion and helps manage and resolve conflicts \\n · Encourages a positive team culture and collaboration | 1. Squad engagement metric values. \\n 2. List of engineers, who were hired and passed probation (can be skipped if we are not hiring, EM should be a hiring manager). |</p><p>\\\n</p><p>|  |  |  |\n|----|----|----|\n|  | Delivers roadmap for multiple squads \\n · Ensures delivery across 2-3 squads \\n · Fulfils Engineering Manager role in one of the squads \\n · Owns partnerships with 3rd parties \\n · All requirements from the Engineering Manager | New projects delivered by the squads conforming to the following requirements: \\n 1. Project initiated through a project proposal (not a BAU activity). \\n 2. The project has met its impact metrics and the public commitment was met. \\n 3. Project results were presented as a Tech Feature session. \\n 4. Projects reported in the previous promotion cycle can’t be included in the list. \\n 5. At least 2 projects should be recognized as key projects on a company level (e.g. a new product, etc, can be confirmed with CTO). |\n|  | Drives managerial changes (multiple squads/tech) \\n · All requirements from Engineering but across multiple squads \\n · System owner for at least one process (e.g. support, etc) | 1. Squads’ productivity (performance) metric values across multiple squads. \\n 2. Major changes (at least 6) introduced, conforming to the following requirements: \\n a. It solves a problem related to the squads or tribe, a problem needs to be included in the TOP 5 problems and agreed upon with the line manager. \\n b. The change should be fully implemented and used by the squads (change should be “sticky” and provide enough value to keep it). \\n c. The change should provide meaningful improvement to productivity, engagement, or quality of delivery. \\n d. The manager doesn’t have to be the one who proposed the change. The Engineering Director should be the primary driving force behind the change. The change can be delivered by an engineer or as a team effort. \\n e. At least 2 changes should be related to the process owned by the director. |\n|  | Line manager (&gt;=10 reports, including indirect reports) \\n · All requirements for Engineering Manager \\n · Coaches and supports engineers \\n · Supports and guides career progressions \\n · Manages churn, reduces “regrettable churn” | 1. Squads’ engagement metric values across multiple squads. \\n 2. List of engineers, who were hired and passed probation (can be skipped if we are not hiring). \\n 3. List of engineers promoted (can be skipped if there is no business need for promotions). |</p>","contentLength":22065,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How a Soft Robot Arm Moves Using Air, Not Motors","url":"https://hackernoon.com/how-a-soft-robot-arm-moves-using-air-not-motors?source=rss","date":1739646019,"author":"EScholar: Electronic Academic Papers for Scholars","guid":475,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing</p><p>4 Data Acquisition and Open-Loop Control</p><p>\\\nAlthough the layout of the pneumatic bench allows working with up to 4 segments, it was thought that using 3 would allow the different problems linked to redundancy to be tackled without increasing the weight of the robot too much or requiring the tubes –which pass through the interior of the segments – to have an excessive amount of space.</p><p>\\\nIt is true that the tubes of the other three could pass through the first module, nevertheless, it was thought that the stiffness they would introduce by being so compressed could make it difficult to bend the initial segment. Since it is also the segment that has to exert the most force, as it is the one that supports the weight of the other segments, the risk of punctures could be increased.</p><p>\\\nTherefore, a robot consisting of three identical modules was assembled, standing at a total height of 390 mm (with each segment measuring 100 mm, intersegment connections 20 mm each, and the vision trihedron rod 30 mm). Under these configurations, the estimated weight of PAUL’s arm is around 600 g. The structure protecting the manipulator is a cube with a side of 500 mm. Pressure of the pneumatic line was established in 1.2 bar.</p><p>\\\nExamples of PAUL reaching different positions are depicted in Figure 13.</p><p>The analysis of the workspace has been carried out experimentally, based on the data taken to generate the dataset. Figure 14 shows the workspace of a segment.</p><p>\\\nAs can be seen, this is a surface, as the segment has two degrees of freedom if the condition that at least one valve should remain deflated is imposed. The surface can be considered as the union of three surfaces intersecting at the central point, which corresponds to the configuration of all deflated bladders. The three surfaces are roughly spherical in shape. If the PCC model were completely valid for the robot, these would be perfect spheres, as the ends of a set of equal-length arcs of circumference with a common origin engrench a circle. Since this is not exactly the case, the generated surfaces only resemble the sphericity predicted by the constant curvature model.</p><p>\\\nThe addition of a second segment already generates a 4-D workspace that is difficult to represent. The generation of this is a consequence of the fact that, from each point on the surface of the workspace of a segment, another similar surface is generated. The</p><p>\\\n\\\nunion of all of these surfaces, which arise from the points on the surface of the first segment, results in the two-segment workspace. This is a volume in which, in addition, each point can be reached from two different orientations, thus leaving latent the four degrees of freedom that PAUL would have with only two modules.</p>","contentLength":3400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"xAI’s “Colossus” supercomputer raises health questions in Memphis","url":"https://techcrunch.com/2025/02/15/xais-colossus-supercomputer-raises-health-questions-in-memphis/","date":1739645584,"author":"Connie Loizos","guid":53,"unread":true,"content":"<p>Elon Musk’s AI startup xAI plans to continue using 15 gas turbines to power its “Colossus” supercomputer in Memphis, Tennessee, according to an operating permit with the Shelby County Health Department for non-stop turbine use from June 2025 to June 2030. Why does it matter? The Commercial Appeal, a news outlet that obtained the documents, […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":416,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elevate Your Night Shift Productivity Levels: 8 Strategies for Thriving - Not Just Surviving","url":"https://hackernoon.com/elevate-your-night-shift-productivity-levels-8-strategies-for-thriving-not-just-surviving?source=rss","date":1739645102,"author":"Beth Rush","guid":474,"unread":true,"content":"<p>Almost every worker will tell you how tough it is to keep productivity levels high at work. However, the struggle is doubled when you work nights. Thankfully, there are a few techniques that can help you survive and thrive in your field, even when working those late hours.</p><h2>The Challenges of Long Hours in the Evening</h2><p>The regular nine-to-five shift is the norm across many industries, especially in the corporate world. However, there are a few outliers, such as the hospitality and manufacturing sectors. Health care, security, and firefighting departments also need hands on deck at all times in emergencies, which requires plenty of alertness.</p><p>\\\nThere are several benefits, like less competition between workers and extra pay. However, it takes some work to get used to it as you have to adjust your body clock. The night shift can <a href=\"https://www.theguardian.com/us-news/2022/nov/18/us-workers-night-shift-takes-toll\">spark plenty of employee turnover</a> since most people can’t sacrifice their daytime hours.</p><p>\\\nThose who decide to take on the night shift find themselves at a crossroads. The hours may seem much longer despite being the same eight hours that workers typically work. It can be hard to keep energy levels up or stay awake.</p><h2><strong>Most Effective Night Shift Tips</strong></h2><p>Dealing with a night shift schedule requires several adjustments, including your sleep schedule, caffeine intake, physical activities and so much more. Here are tried-and-true strategies to help you out.</p><h3><strong>Have a Routine Before the Shift</strong></h3><p>Good sleep should be the highest priority in your pre-work routine. It’s much easier to stay alert and concentrate on your tasks on the job when you’ve had enough shut-eye. Unfortunately, <a href=\"https://www.trustaff.com/blog/tips-for-surviving-night-shift\">over one-third of Americans</a> sleep less than seven hours. Heading to bed in the middle of the day can also seem foreign to your body.</p><p>\\\nThus, the first step is to have a consistent schedule. For example, people in health care may have to start their eight hours of work at 7 p.m. or so. Others might deal with extended hours, bringing the total closer to 12.</p><p>\\\nThus, the ideal night-shift nurse sleep schedule will start at 9 a.m. if you want to achieve eight hours and an extra two hours to prep and commute. The leeway can also be used as additional time to sleep ahead of those longer work times.</p><p>\\\nIf it’s your first time heading into the shift, give your body a week to gradually acclimate to the new sleep schedule. Complete and total sleep deprivation can be too exhausting for your body.</p><p>\\\nBe mindful of your sleeping quarters during the adjustment period. Blackout curtains are ideal for completely darkening the room and tricking your body into thinking it’s nighttime. You should put your phone in silent mode to avoid calls that may disrupt your sleep.</p><p>\\\nYou can also experiment with other sleeping aids. For example, some people find aromatherapy relaxing, easing them into slumber during the daytime. White noise machines can also provide the best background sound as you fall asleep.</p><p>\\\nAfter a long sleep, you should feel recharged. Awaken your senses by preparing a well-balanced dinner before your shift, and incorporate fruits and vegetables into your meals to get your nutrition fix. Afterward, wear your clothing of choice and head out to conquer the night shift.</p><h3>Design a Post-Work Ritual</h3><p>Now that you know how to prepare for a 12-hour night shift, it’s time to move on to winding down. Ideally, the routine gives you something to look forward to, such as having a shower to feel clean. You can also move your skincare routine to this time.</p><p>\\\nTry to give yourself a bit of leisure after work. Some use this time to watch shows, listen to music, or play video games. Social media is also a good outlet for entertainment. Just remember to use it in moderation to avoid doomscrolling.</p><h3><strong>Exercise Caution Around Caffeine</strong></h3><p>Caffeine is a helpful tool to keep your energy levels as high as possible. A cup of your favorite coffee or tea just before work can help you energize throughout the day. However, it’s best to avoid the temptation of getting a second or third drink in the middle of the shift. Too much caffeine will result in quite a crash afterward.</p><p>\\\nIt’s also best to schedule your intake. Prolonged caffeine ingestion <a href=\"https://academic.oup.com/sleepadvances/article/4/1/zpad014/7040153\">can disrupt the circadian rhythm</a>, disrupting your sleep routine. Meanwhile, short exposure boosts sleep fragmentation, increasing your awakenings while sleeping and ruining the quality of your shut-eye. Once a week is a good rule of thumb to follow.</p><p>One thing most people forget to realize when moving into the night shift is that everyone else still maintains their day routines. Your family and friends will likely have lunch, run errands, and do their own jobs when you’re asleep. When you’re awake and working, they’ll likely be sleeping.</p><p>\\\nHuman interaction is essential to enduring the night shift. Try to find overlaps and make time to catch up. You can invite a friend over for dinner before you head to work or chat on the phone while you’re home.</p><p>\\\nYou can also turn to your fellow employees going through the night shift. Camaraderie helps to overcome a challenge. Get to know these people, and build strong working relationships. The bond can even help you increase energy levels and boost productivity.</p><h3><strong>Maintain Physical Activity</strong></h3><p>Night shift workers have limited time and energy. However, it’s still important to maintain some form of physical movement. Exercise <a href=\"https://www.nature.com/articles/s42003-024-05962-8\">may lessen the likelihood of disorders</a> like depression, cardiovascular conditions, and metabolic diseases in chronic shift workers.</p><p>\\\nTry to find breaks to just take a walk. Aside from getting fresh air, you can move your muscles and wake yourself up. If you’re already a veteran for the evening hours, you may even consider heading to the gym right after work.</p><h3><strong>Move Your Responsibilities</strong></h3><p>Keeping productivity levels high throughout the shift can take time and practice. However, one hack you can try is intentionally scheduling the most important responsibilities at the beginning of your shift. After all, you will likely have more energy at the start.</p><p>\\\nOnce you’ve completed the bulk of the most important work, the next goal is to stretch out your energy throughout the rest of the hours and stay awake. You can try to space out your duties and give yourself breaks when you’re faltering.</p><h3><strong>Keep the Same Schedule on Off Days</strong></h3><p>It's important to maintain your current schedule to get your body used to the night shift. This solution is best for working full time, allowing you to fully adjust your body clock and move your most productive times to match your work schedule.</p><p>\\\nIf you have obligations during your off days that need to be done during the daytime, try to reschedule. You can also proceed and readjust afterward with some intentionally planned naps. Minimize deviating and the changes in your productivity should follow suit.</p><h3><strong>Manage Your Mindset and Stress</strong></h3><p>Night shift workers can experience stress throughout the adjustment period. Unfortunately, this pressure <a href=\"https://www.health.harvard.edu/topics/stress\">can alter the immune system</a> and make you more vulnerable to illnesses. It can also build a negative mindset toward your schedule.</p><p>\\\nMake sure to have stress management techniques under your belt. Meditation is a good way to come to terms with your thoughts, while breathing exercises can assist you in calming down systemic turmoil. Activities like journaling and yoga can also improve your mood.</p><h2><strong>Thrive With an Unconventional Schedule</strong></h2><p>Having a night shift schedule is a challenge worth conquering. You get to be present for the people who need aid and assistance, and it brings fulfillment to your long-term career development. You just have to adapt to the circumstances to reap the rewards.</p>","contentLength":7590,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity launches its own freemium ‘deep research’ product","url":"https://techcrunch.com/2025/02/15/perplexity-launches-its-own-freemium-deep-research-product/","date":1739644754,"author":"Anthony Ha","guid":52,"unread":true,"content":"<p>Perplexity has become the latest AI company to release an in-depth research tool, with a new feature announced Friday. Google unveiled a similar feature for its Gemini AI platform in December. Then OpenAI launched its own research agent earlier this month. All three companies even have given the feature the same name: Deep Research. The […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bored With Chess? Magnus Carlsen Wants to Remake the Game","url":"https://games.slashdot.org/story/25/02/15/053254/bored-with-chess-magnus-carlsen-wants-to-remake-the-game?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739644440,"author":"EditorDavid","guid":187,"unread":true,"content":"\"Magnus Carlsen, the world's top chess player, is bored of chess,\" the Washington Post wrote Friday:\n\nCarlsen has spent much of the past year appearing to dismiss the game he has mastered: It was no longer exciting to play, he told a podcast in March. In December, he withdrew from defending a world championship because he was penalized for wearing jeans to the tournament. \nHow would the world's best player spice up the game? Change the rules, and add a touch of reality TV. \n\nTen of the world's top players gathered in a German villa on the Baltic coast this week to play in the first tournament of a new chess circuit, the Freestyle Chess Grand Slam Tour, that Carlsen co-founded. The twist: The tour randomizes the starting positions of the chess board's most important pieces, so each game begins with the queen, rooks and knights in a jumble. [It's sometimes called \"Chess960\" or Fischer random chess — with both players starting with the same arrangement of pieces.] Players have to adapt on the fly. Carlsen is backed by a cadre of investors who see a chance to dramatize chess with the theatrics of a television show. Players wear heart-rate monitors and give confession-booth interviews mid-match where they strategize and fret to the audience. Some purists are skeptical. So is the International Chess Federation, which sent a barrage of legal threats to Freestyle Chess before it launched this week's event. \nAt stake is a lucrative global market of hundreds of millions of chess players that has only continued to grow since the coronavirus pandemic launched a startling chess renaissance — and, perhaps, the authority to decide if and how a centuries-old game should evolve... The format is an antidote to the classical game, where patterns and strategies have been so rigorously studied that it's hard to innovate, Carlsen said. \"It's still possible to get a [competitive] game, but you have to sort of dig deeper and deeper,\" Carlsen said. \"I just find that there's too little scope for creativity.\" \n\nThe article also includes this quote from American grand master Hikaru Nakamura who runs a chess YouTube channel with 2.7 million subscribers). \"An integral part of regular chess is that when you play, you spend hours preparing your opening strategy before the game. But with Fischer Random ... it's a little bit looser and more enjoyable.\" And German entrepreneur Jan Henric Buettner (one of the investors) says they hope to bring the drama of Formula One racecars. (\"Cameras mounted at table level peer up at each player during games,\" the article notes at one point.) \n\nThe first Freestyle Chess Grand Slam Tour (with a $750,000 prize pool) concluded Friday, according to the article, but \"Carlsen did not play in it,\" the Post points out. \"He was upset in the semifinals by German grand master Vincent Keymer.\" Carlsen's reaction? \"I definitely find Freestyle harder.\" \n\nBut Chess.com reports that Carlsen will be back to playing regular chess very soon:\n\nGlobal esports powerhouse Team Liquid has announced the signings of not just one, but two superstars of chess. Five-time World Champion and world number-one Magnus Carlsen and the 2018 challenger, world number-two Fabiano Caruana will represent the club ahead of the 2025 Esports World Cup (EWC)... Carlsen and Caruana, fresh from competing in the Weissenhaus Freestyle Chess Grand Slam, will first represent Team Liquid in the $150,000 Chessable Masters, which begins on February 16 and serves as the first of two qualifying events in the 2025 Champions Chess Tour. The top-12 players from the tour qualify for the EWC. \n\nIn an announcement video Carlsen reportedly trolls the FIDE, according to Indian Express. \"The announcement video sees Carlsen wear a Team Liquid jersey along with a jacket and jeans. He then asks: 'Do I have to change?' To this, someone responds: 'Don't worry, we're pretty chill in esports. Welcome to Team Liquid.'\"","contentLength":3925,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI teases a ‘simplified’ GPT-5 model","url":"https://techcrunch.com/2025/02/15/openai-teases-a-simplified-gpt-5-model/","date":1739642700,"author":"Cody Corrall","guid":51,"unread":true,"content":"<p>Welcome back to Week in Review. This week we’re looking at OpenAI canceling the release of o3; TikTok returning to U.S. app stores nearly a month after it was removed; more complications in Elon Musk’s bid to buy OpenAI for $97.4 billion; and more! Let’s do it. OpenAI effectively canceled the release of o3, which […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Scale AI Infrastructure With Kubernetes and Docker","url":"https://hackernoon.com/how-to-scale-ai-infrastructure-with-kubernetes-and-docker?source=rss","date":1739642406,"author":"Natapong Sornprom","guid":473,"unread":true,"content":"<p>Firms increasingly make use of artificial intelligence (AI) infrastructures to host and manage autonomous workloads. Consequently,<a href=\"https://www.cio.com/article/3577669/as-ai-scales-infrastructure-challenges-emerge.html\"></a> as well as resilient infrastructures that will be able to meet heterogeneous application or cloud requirements. Organizations use<a href=\"https://hackernoon.com/an-intro-to-kubernetes-for-docker-developers\"></a> and<a href=\"https://hackernoon.com/optimizing-docker-images-is-more-than-just-a-one-and-done-thing\"></a> to meet such needs because firms realize that both are highly effective use cases that deliver scalable AI infrastructures.</p><p>\\\nDeploying AI infrastructure typically provides adequate computation power to execute and process large datasets. These demands can translate into the need for scalable methods that enable AI models to run on large workloads without hurting performance.</p><p>, nonetheless, are also resource-intensive, normally demanding both high computing capacity and the ability to process high levels of data. As more advanced AI applications and a larger scale become required, scalability becomes more critical. Scalability ensures that AI systems can handle increasing workloads without any loss of performance.</p><p>The growing amount of data is a concern for AI systems in many facets. Most AI models, especially those based on deep learning, heavily depend on large amounts of data during training and inference. However, without adequate scalable infrastructure, processing and interpreting such<a href=\"https://www.mckinsey.com/west-coast/~/media/mckinsey/featured%20insights/artificial%20intelligence/notes%20from%20the%20ai%20frontier%20applications%20and%20value%20of%20deep%20learning/notes-from-the-ai-frontier-insights-from-hundreds-of-use-cases-discussion-paper.pdf\"></a>.</p><p>Scalable AI hardware supports reliable and stable performance despite drastically overwhelming computational loads. With Kubernetes, horizontal scaling of AI jobs is a breeze, and the dynamic resizing of replica numbers can be done as a function of necessity. In contrast, Docker containers support lean, isolated environments for running AI models where resource conflict is not a performance bottleneck.</p><h3><strong>Effective Resource Management</strong></h3><p>Efficient use of resources is the key to cost-effective and sustainable AI deployment. Kubernetes' resource requests and limits allow for fine-grained CPU and memory resource management by avoiding underprovisioning and overprovisioning. Docker's resource management fills the gap by isolating container resources.</p><h2><strong>Scaling AI Infrastructure With Kubernetes and Docker</strong></h2><p>Containerization is one of the milestones in the evolution of scalable artificial intelligence infrastructure. Containerization of the AI application and its dependencies in a Docker container ensures consistency throughout the development, testing, and deployment environments.</p><p>\\\nFirst, you must define a Dockerfile in order to install the environment. The Dockerfile is a series of instructions about how to build a Docker image. It declares a base image, the dependencies required, and the initial setup commands that apply to your app. The following is a basic Dockerfile for a Python machine-learning model:</p><pre><code># Use an official Python runtime as a parent image\nFROM python:3.9-slim\n\n# Set the working directory in the container\nWORKDIR /usr/src/app\n\n# Copy the current directory contents into the container\nCOPY . .\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Expose the port the app runs on\nEXPOSE 5000\n\n# Define environment variable\nENV NAME World\n\n# Run the app\nCMD [\"python\", \"./app.py\"]\n</code></pre><p>\\\nIf the Dockerfile is ready, then you can build the Docker image and run the container. Run the following commands: \\n </p><pre><code># Build the Docker image\ndocker build -t ml-model:latest .\n\n# Run the container\ndocker run -p 5000:5000 ml-model:latest\n</code></pre><h2><strong>Deploying the Dockerized AI Model to Kubernetes</strong></h2><p> provides a wide range of orchestration features that enable efficient application management in the containerized infrastructure. Deployment of the Docker image on Kubernetes ensures that a specified number of application replicas is always running. The following is an example of deployment.yaml file that you can use to<a href=\"https://dev.to/pavanbelagatti/deploy-any-aiml-application-on-kubernetes-a-step-by-step-guide-2i37\"></a>:</p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ml-model-deployment\nspec:\n  replicas: 3  \n  selector:\n    matchLabels:\n      app: ml-model\n  template:\n    metadata:\n      labels:\n        app: ml-model\n    spec:\n      containers:\n      - name: ml-model-container\n        image: ml-model:latest\n        ports:\n        - containerPort: 5000\n</code></pre><p>\\n The above code snippet shows how to deploy the AI model, but you also need to make the model externally accessible. You will need to expose it by defining a Kubernetes Service. The service.yaml below illustrates an example:</p><pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: ml-model-service\nspec:\n  selector:\n    app: ml-model\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 5000\n  type: LoadBalancer\n</code></pre><p>\\n Use the kubectl command-line tool to apply the deployment and service configurations:</p><pre><code># Deploy the application\nkubectl apply -f deployment.yaml\n\n# Expose the service\nkubectl apply -f service.yaml\n</code></pre><p>Kubernetes provides excellent scaling capabilities to AI environments, maximizing resource utilization and performance. Horizontal scaling is done by adding additional containers, and vertical scaling involves adding additional resources like CPU or memory to a container.</p><p>Horizontal scaling is used to scale up the number of replicas (Pods) of an AI system to handle a higher workload. The process requires enabling dynamic scaling depending on the number of replicas. The command used to enable such a process is `kubectl scale`. The particular command is used to set up the deployment to function up to a maximum of five replicas:</p><p>\\\n`kubectl scale --replicas=5 deployment/ml-model-deployment`</p><p>\\\nThe command scales up the ml-model-deployment to use five replicas of the machine-learning model container. The system dynamically provisions more Pods to meet the required number afterward.</p><h3><strong>Automatic Scaling using the Horizontal Pod Autoscaler (HPA)</strong></h3><p>Kubernetes facilitates auto-scaling using the Horizontal Pod Autoscaler (HPA). The HPA dynamically adjusts the number of replicas based on resource use, i.e., CPU or memory, in relation to set limits. The YAML configuration shown below is a relevant example of an HPA that dynamically scales for ml-model-deployment in response to CPU use:</p><pre><code>apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: ml-model-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: ml-model-deployment\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 50\n</code></pre><p>\\n In this setup, scaleTargetRef is used to define the Deployment to be scaled, i.e., ml-model-deployment. The minimum replica count is set using MinReplicas, while the maximum replica count is controlled using maxReplicas. In addition, the CPU utilization percentage is set using targetCPUUtilizationPercentage, i.e., to 50%. </p><p>\\\nCPU utilization of more than 50% across all Pods results in scaling up the replica count to a maximum of 10 automatically. As soon as CPU utilization drops below the set percentage, Kubernetes automatically reduces the replica count in order to release resources.</p><p>Horizontal scaling is mainly to cope with more traffic, whereas vertical scaling provides more resources (such as CPU or memory) to existing containers. The process is to scale up or down resource requests and limits in the Kubernetes Deployment. In order to scale up the CPU and memory limits of the ml-model-deployment, one would need to open the deployment.yaml file:</p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ml-model-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: ml-model\n  template:\n    metadata:\n      labels:\n        app: ml-model\n    spec:\n      containers:\n      - name: ml-model-container\n        image: ml-model:latest\n        ports:\n        - containerPort: 5000\n        resources:\n          requests:\n            cpu: \"1\"\n            memory: \"2Gi\"\n          limits:\n            cpu: \"2\"\n            memory: \"4Gi\"\n</code></pre><p>\\\nIn this updated configuration:</p><ul><li>requests specify the minimum resources required for the container.</li><li>limits define the maximum resources the container can use.</li></ul>","contentLength":7854,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"'Mass Theft': Thousands of Artists Call for AI Art Auction to be Cancelled","url":"https://slashdot.org/story/25/02/15/0351257/mass-theft-thousands-of-artists-call-for-ai-art-auction-to-be-cancelled?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739640840,"author":"EditorDavid","guid":186,"unread":true,"content":"An anonymous reader shared this report from the Guardian:\n\nThousands of artists are urging the auction house Christie's to cancel a sale of art created with artificial intelligence, claiming the technology behind the works is committing \"mass theft\". The Augmented Intelligence auction has been described by Christie's as the first AI-dedicated sale by a major auctioneer and features 20 lots with prices ranging from $10,000 to $250,000... \nThe British composer Ed Newton-Rex, a key figure in the campaign by creative professionals for protection of their work and a signatory to the letter, said at least nine of the works appearing in the auction appeared to have used models trained on artists' work. However, other pieces in the auction do not appear to have used such models. \nA spokesperson for Christie's said that \"in most cases\" the AI used to create art in the auction had been trained on the artists' \"own inputs\". \n\nMore than 6,000 people have now signed the letter, which states point-blank that \"Many of the artworks you plan to auction were created using AI models that are known to be trained on copyrighted work without a license.\"\n\nThese models, and the companies behind them, exploit human artists, using their work without permission or payment to build commercial AI products that compete with them. Your support of these models, and the people who use them, rewards and further incentivizes AI companies' mass theft of human artists' work. We ask that, if you have any respect for human artists, you cancel the auction. \n\nLast week ARTnews spoke to Nicole Sales Giles, Christie's vice-president and director of digital art sales (before the open letter was published). And Giles insisted one of the major themes of the auction is \"that AI is not a replacement for human creativity.\"\n\"You can see a lot of human agency in all of these works,\" Giles said. \"In every single work, you're seeing a collaboration between an AI model, a robot, or however the artist has chosen to incorporate AI. It is showing how AI is enhancing creativity and not becoming a substitute for it.\" \n\nOne of the auction's headline lots is a 12-foot-tall robot made by Matr Labs that is guided by artist Alexander Reben's AI model. It will paint a new section of a canvas live during the sale every time the work receives a bid. Reben told ARTnews that he understands the frustrations of artists regarding the AI debate, but he sees \"AI as an incredible tool... AI models which are trained on public data are done so under the idea of 'fair use,' just as search engines once faced scrutiny for organizing book data (which was ultimately found to fall under fair use),\" he said.... \"AI expands creative potential, offering new ways to explore, remix, and evolve artistic expression rather than replace it. The future of art isn't about AI versus artists — it's about how artists wield AI to push boundaries in ways we've never imagined before....\" \n\nDigital artist Jack Butcher has used the open letter to create a minted digital artwork called Undersigned Artists. On X he wrote that the work \"takes a collective act of dissent — an appeal to halt an AI art auction — and turns it into the very thing it resists: a minted piece of digital art. The letter, originally a condemnation of AI-generated works trained on unlicensed human labor, now becomes part of the system it critiques.\" \n\nChristie's will accept cryptocurrency payments for the majority of lots in the sale.\n","contentLength":3474,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Programmer's Guide to Game Design: The Major Ingredients You Should Know","url":"https://hackernoon.com/a-programmers-guide-to-game-design-the-major-ingredients-you-should-know?source=rss","date":1739638805,"author":"Chenuli J.","guid":117,"unread":true,"content":"<p>Software developers spend their whole time with complicated problems, and they try to learn almost everything about algorithms, structures, frameworks, and blah-blah-blah. While playing games and coffee have become stress-busters in our lives, why can't building games be the best one?</p><p>\\\nEven though some people find game development insane, it’s easy for software developers because they have almost every skill needed: math, Programming, UX/UI, and the usual stuff. If a normal person takes 6 months to learn a Game Engine, the Developer will take a maximum of 3 months or less.</p><p>\\\nMany people don’t know this, but I started my tech career as a game developer, although I later turned my back on game development and became a Python developer. And no lies, they were a few of the best years in my life.</p><p>\\\nThis article is dedicated to any developer who wants to try something new or exciting (which is game development, yes). Scroll down!</p><p>Software developers who are interested in game development have a remarkable head start. Programming skills are the core of game development. Even if you’re more comfortable with Swift or Ruby, which is not commonly used in game development, you can quickly pick up other Object-oriented programming languages that are much more commonly used for game development, like C# or C++, easier than anyone!</p><p>\\\nIf you're a Python lover, you will love to hear this: There are really good, AAA games made with Python such as Battlefield 2, EVE Online, Civilization IV, and more!</p><p>Not only Python, but almost every commonly used Programming language has libraries that support making games. For example:</p><ul><li>Flutter has Flame, a game engine that supports Flutter language.</li><li>Ruby has Gosu, a library that makes it easy to develop 2D games.</li><li>Python has PyGame, a library that empowers you to create both 2D and 3D games.</li><li>Phaser allows you to create games with JavaScript and HTML5.</li></ul><p>For every beginner, either a software engineer or a novice, I give one common piece of advice— Start as small as possible. It doesn't matter how small pieces of code it has or how messy it is, you've won the challenge!</p><p>\\\nIf you love simple 2D games, I would recommend that you make a Pong game. Pong features simple graphics (2 rectangles and a circle) you can create on your own, minimal sounds, and a game loop. If you want to learn about making multiplayer games, allow matches to occur between two human players over a network. If you want to learn about AI, allow the player to challenge the computer.</p><p>\\\nAnd if you love 3D games like me, start with Cube Run. I haven't made it without a Game engine because, 3D becomes a bit harder with Python or others but with Unity, it's the best game I recommend to take a start.</p><h2><strong>Major Ingredients of A Game</strong></h2><p>If you make a cake without Sugar, no one will eat it. Except for diabetic people, of course, like my mom.</p><p>\\\nThe same goes for video games. It’s full of ingredients, some of which are required for every game and some of which are optional. I’ll introduce them briefly.</p><p>In many games, the level itself is a challenge, trickier than the smartest AI enemies. Series like Tomb Raider also emphasize complex and challenging level design.</p><p>\\\nWhile the advent of open-world games like GTA may make the level design seem less important than in bygone times, it’s worth noting that even open-world games have ‘levels’, such as a particular building, structure, or map area you must enter to achieve a goal.</p><p>\\\nTo reduce player feelings of being railroaded, levels will ideally have multiple possible paths through them.</p><p>In games, you can’t really rely on natural light sources to illuminate your video game.</p><p>\\\n(You don’t think there’s a sun inside a game engine, do you?)</p><p>\\\nEvery light source in a video game must be added by hand and light manipulation is incredibly important. Light can be used for all of the following:</p><ul><li>Controlling the player’s ability . In horror and survival games, light is a resource that must be carefully managed.</li></ul><ul><li>Controlling a player’s ability . In games with an element of stealth, dark areas can provide cover while well-lit areas represent a difficult challenge.</li></ul><ul><li>Setting the mood. The quality of light can be used to set the mood, with sunny and bright lighting associated with happy times and brooding light associated with dark times.</li></ul><ul><li>Lighting the way. Light can be used to direct the player’s attention. The best-designed levels in video games often make clever use of light to guide the player in the right direction when they might otherwise be lost.</li></ul><p>Game art is the medium through which the game world is presented to the player. In a sense, all the programming effort that goes into making video games is an attempt to turn game art into something that feels responsive and alive. Game art is an umbrella term that includes textures, 3D models, sprites, particle effects, and lighting.</p><p>\\\nAnd yes, it’s a broad area, best to not cover it in this article.</p><p>Unlike in the real world, video game sounds cannot be made by accident. Every sound in the game universe must be added by hand, and it is through layering these sounds that the game world starts to feel lifelike. You also need to be mindful of sounds triggered by the player, by other characters, and ambient sounds that create the game world environment.</p><p>\\\nAs an example, the player accidentally hits a metal object; if it doesn't emit a sound, it doesn't feel natural, or the scientist who claimed that metals have sonorous properties got it wrong.</p><p>\\\nAnother ever-present fact of video games is music, used to create an emotional response in the player or removed entirely to leave behind an eerie silence. Unlike most compositions, video game music must loop seamlessly. It must also transition smoothly to new compositions based on in-game events, such as being spotted by an enemy.</p><p>\\\nHere are some of my favorite places to find sound:</p><p>You have made graphics, levels, and everything but the whole game feels like a dead body. If you want your game to be alive, you need some lines of code.</p><p>\\\nMost of my readers are probably a developer already, so you probably know the importance of programming anywhere. I'm not going to give a lecture on \"what is programming\", but here’s a bit about programming in game design.</p><p>\\\nFirst of all, you need to decide one thing: Are you making a game by using a Game Engine + Language or building a game from scratch with Python or something? It's your choice but let me help you: If you're going to build games for fun, choose the first option because it's easier. </p><p>\\\nAs I said in the beginning, programmers always juggle complex problems so you probably shouldn't take more stress with that too.</p><p>\\\n(Game Engines are the software used to create video games.)</p><p>\\\nThere are many of them, but the most popular ones are:</p><ul><li><p>Unity (Great for beginners, recommended)</p></li></ul><p>Next, choose a language to get started. Mostly, C# and C are used. Don't worry, you already got a headstart in knowing at least one programming language; most people start even before they know what the word “programming“stands for.</p><p>\\\nAnd that’s about it. Well, of course, there are many other optional ingredients that video games consist of but in general, the above are basically the starter pack.</p><p>Starting game development is easier for software developers than anyone. As someone with programming skills, you have a massive head-start on the average video game hobbyist who wants to learn how to make a game. If I scroll to the top of the article, I can list the below points as key takeaways.</p><ul><li><p>It's a lot easier to find a game dev library in your comfortable programming language to get started.</p></li><li><p>Start with a small game, maybe a clone of an existing game.</p></li><li><p>Game Engines make your life a lot easier.</p></li><li><p>If you want to make a big, impressive game but don’t have a lot of time to spare, consider teaming up with others or joining a modding community.</p></li></ul><p>And that's all for now, Happy Designing! 🏎</p><p>\\\nIf you loved this article, make sure to subscribe using your email, so you can read all my content inside your inbox without missing any!</p><p>It’s totally free of charge and I don’t even have time to send spam emails.</p>","contentLength":8132,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Marc Andreessen dreams of making a16z a lasting company, beyond partnerships","url":"https://techcrunch.com/2025/02/15/marc-andreessen-dreams-of-making-a16z-a-lasting-company-beyond-partnerships/","date":1739638800,"author":"Marina Temkin","guid":50,"unread":true,"content":"<p>Many venture industry observers have wondered whether Andreessen Horowitz, a firm that manages $45 billion, has its sights on eventually becoming a publicly traded company. Co-founder Marc Andreessen said he isn’t “chomping at the bit to take the firm public,” on this week’s Invest Like the Best podcast. But he discussed his goal of building […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ISS Astronauts Give Space-to-Earth Interview Weeks Before Finally Returning to Earth","url":"https://science.slashdot.org/story/25/02/15/033223/iss-astronauts-give-space-to-earth-interview-weeks-before-finally-returning-to-earth?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739637240,"author":"EditorDavid","guid":185,"unread":true,"content":"Last June two NASA astronauts flew to the International Space Station on the first crewed test flight of Boeing's Starliner. But they aren't stranded there, and they weren't abandoned, the astronauts reminded CNN this week in a rare space-to-earth interview:\n\n\"That's been the rhetoric. That's been the narrative from day one: stranded, abandoned, stuck — and I get it. We both get it,\" [NASA astronaut Butch] Wilmore said. \"But that is, again, not what our human spaceflight program is about. We don't feel abandoned, we don't feel stuck, we don't feel stranded.\" Wilmore added a request: \"If you'll help us change the rhetoric, help us change the narrative. Let's change it to 'prepared and committed.' \n\n\"That's what we prefer,\" he said... \n[NASA astronaut Suni] Williams also reiterated a sentiment she has expressed on several occasions, including in interviews conducted before she left Earth. \"Butch and I knew this was a test flight,\" she told CNN's Cooper, acknowledging the pair has been prepared for contingencies and understood that the stay in space might be extended. \"We knew that we would probably find some things (wrong with Starliner) and we found some stuff, and so that was not a surprise,\" she said. \nWhen Cooper opened the interview by asking the astronauts how they're doing, Williams answers \"We're doing pretty darn good, actually,\" pointing out they had plenty of food and great crew members. And Wilmore added that crews come to the space station on a careful cycle, and \"to alter that cycle sends ripple effects all the way down the chain. We would never expect to come back just special for us or anyone unless it was a medical issue or something really out of the circumstances along those lines. So we need to come back and keep the normal cycle going...\" \n\nCNN's article notes a new announcement from NASA Tuesday that the astronauts might return a couple weeks early \"after opting to change the SpaceX Crew Dragon capsule it will use.\" That mission's targeted launch date is now March 12. \n\nIn the meantime, Williams says in the interview, \"We do have some internet connection up here, so we can get some internet live. We've gotten football. It's been this crew's go-to this past fall. Also YouTube or something like that. It's not continuous — it has chunks of time that we get it. And we use that same system also to make phone calls home, so we can talk to our families, and do videoconferences even on the weekends as well. This place is a pretty nice place to live, for the most part.\" \n\nAnd they're also \"working on with folks on the ground\" to test the NASA's cube-shaped, free-flying robotic Astrobees.","contentLength":2649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NTSYNC Driver Fix Being Worked On For Proper User Permissions","url":"https://www.phoronix.com/news/Linux-NTSYNC-Permissions-Issue","date":1739635532,"author":"Michael Larabel","guid":367,"unread":true,"content":"<article>One of the great new features of Linux 6.14 is the NTSYNC driver being completed for better emulating the Microsoft Windows NT synchronization primitives so that software like Wine and Proton (Steam Play) can provide for better performance when running Windows games on Linux. But it turns out an oversight up to now has meant that in practice it's not really too usable out-of-the-box...</article>","contentLength":388,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: No Startup Has Ever Failed Because it Didn’t Have a Blog (2/15/2025)","url":"https://hackernoon.com/2-15-2025-newsletter?source=rss","date":1739635457,"author":"Noonification","guid":116,"unread":true,"content":"<p>🪐 What’s happening in tech today, February 15, 2025?</p><p>By <a href=\"https://hackernoon.com/u/realgpp\">@realgpp</a> [ 9 Min read ] Learn how to read thread dumps and take control of your application’s runtime behaviour.\n <a href=\"https://hackernoon.com/how-to-expose-and-fix-hidden-bottlenecks-in-adobe-experience-manager\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/bigmao\">@bigmao</a> [ 6 Min read ] The case against content marketing, and how to do inbound marketing in the post-content age.  <a href=\"https://hackernoon.com/no-startup-has-ever-failed-because-it-didnt-have-a-blog\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/loadbalancer\">@loadbalancer</a> [ 5 Min read ] Researchers have optimized Layer-7 load balancing using programmable SmartNICs to improve efficiency, cost, and energy use in cloud data centers. <a href=\"https://hackernoon.com/cloud-giants-spend-a-fortune-on-load-balancersthis-research-could-change-that\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jeep Claims 'Software Glitch' Disabled Opting-Out of In-Vehicle Pop-Up Ads in 'a Few' Cases","url":"https://tech.slashdot.org/story/25/02/15/0149202/jeep-claims-software-glitch-disabled-opting-out-of-in-vehicle-pop-up-ads-in-a-few-cases?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739633640,"author":"EditorDavid","guid":184,"unread":true,"content":"Remember Jeep's new in-dash pop-up ads which reportedly appeared every time you stopped? \n\"Since I'm a journalist, or at least close enough, I decided that I should at least get Stellantis/Jeep's side of things,\" writes car-culture site The Autopian:\n\n\nWould Stellantis do something so woefully misguided and annoying? I reached out to our Stellantis/Jeep contact to ask and was initially told that they were \"investigating\" on their end, which to me felt like a stalling tactic while the proper ass-covering plans were conceived. I eventually got this response from a Stellantis spokesperson: \n\n \"This was an in-vehicle message designed to inform Jeep customers about Mopar extended vehicle care options. A temporary software glitch affected the ability to instantly opt out in a few isolated cases, though instant opt-out is the standard for all our in-vehicle messages. Our team had already identified and corrected the error, and we are following up directly with the customer to ensure the matter is fully resolved...\" \n\nI suppose a glitch is possible, though I've not seen any examples of this ad popping up with the instant opt-out option available, but I guess it must exist, since not all Jeep owners seem to have had to deal with these ads. I suspect if this was happening to more people than these \"few isolated cases\" we'd still be cleaning up from the aftermath of the riots and uprisings. \n\nBecause, as they write, \"Really, I can't think of a quicker way to incur the wrath of nearly every human...\"","contentLength":1513,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is an encryption backdoor?","url":"https://techcrunch.com/2025/02/15/what-is-an-encryption-backdoor/","date":1739631600,"author":"Natasha Lomas","guid":49,"unread":true,"content":"<p>Talk of backdoors in encrypted services is once again doing the rounds after reports emerged that the U.K. government is seeking to force Apple to open up iCloud’s end-to-end encrypted (E2EE) device backup offering. Officials were said to be leaning on Apple to create a “backdoor” in the service that would allow state actors to […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":404,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Antarctica's Only Insect","url":"https://www.404media.co/antarcticas-only-insect/","date":1739628013,"author":"Becky Ferreira","guid":389,"unread":true,"content":"<img src=\"https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-09.17.41@2x.png\" alt=\"Antarctica's Only Insect\"><p>Welcome <a href=\"https://www.404media.co/neanderthals-would-rather-die-than-talk-to-you-3/\" rel=\"noreferrer\">back to the Abstract</a>, 404 Media's weekly roundup of scientific studies to distract us from our present dystopia!</p><p>This week, we are traveling back in time to 16th century Transylvania, so please make sure you are up to date on your bubonic plague shots. A study reconstructed wild weather events through the eyes of record-keepers during this fraught period, opening a tantalizing window into climate extremes unleashed by a vengeful God (according to contemporary reports).</p><p>Then: making love the medaka way (get those anal fins ready). Next, the chillest insect in Antarctica (also: the only one). Finally, these turtles will dance for food, and yes, it’s very cute.</p><h3><strong>The Haunting Weather Reports of 16th Century Transylvania</strong></h3><p>Rejoice, for this week has delivered one of the best varieties of study: Science via historical documents. Sure, ice cores and geological strata are great for reconstructing past climates, but nobody can bitch about the weather better than a good old-fashioned red-blooded member of team .&nbsp;</p><p>To that end, researchers searched for mentions of weird weather across a trove of diaries, monastery records, travel notes, and other documents from 16th century Transylvania, during a “pivotal moment in climate history” when a centuries-long cooling event called the Little Ice Age intensified, according to researchers led by Ovidiu Răzvan Gaceu of the University of Oradea.&nbsp;</p><p>These types of studies are packed with colorful human testimonies that can corroborate natural records. More importantly, though, they are just fun to read, especially during such an evocative time and place, freshly haunted by the vampiric spirit of Vlad the Impaler. Some highlights:</p><p>In August 1526, heavy rainfall caused freak floods in Braşov that “washed the walls of the fortress, demolished the main gate, and the fish also got caught in the big church,” according to the Annals of Brașov. Fish in the church! The ultimate baptism.&nbsp;</p><p>&nbsp;In autumn 1553, people in the city of Cluj reported unusual weather events including “October strawberries.” For real, October is for pumpkins, get out of here with the strawbs. Turned out it was a bad omen—there was a plague the following winter. Keep that in mind if you see any late autumn strawberries: Kill on sight.</p><p>Naturally, a lot of these accounts are heartbreaking. Locusts “sometimes covered the whole sky and destroyed grain crops” and caused terrible famines. A storm-related fire “killed 14 people and made 60 poor.” On September 29, 1582, “there was such a big storm, as it was said that it had never been seen before in the city of Cluj, which uprooted the trees and raised the roofs of the houses, people believed that it is sent by divinity to punish the crimes committed by them.”&nbsp;</p><p>I mean, I’m not saying these people weren’t doing crimes. It’s 16th century Transylvania. Do what you gotta do. But that's not why there is extreme weather. You’re just in the Little Ice Age.&nbsp;</p><p>The study ultimately identified “multiple pieces of evidence associated with extreme weather events, including 40 unusually warm summers and several years of excess precipitation or drought.” Taken together with natural archives, the documents paint a picture of troubled times, exacerbated by an unstable climate and possible emergent vampires. Relatable!&nbsp;</p><p>Valentine’s Day is over, but the romantic mood is still in the air—or in the water, if you’re a medaka (flawless segue). Scientists have discovered that wild medaka, also known as Japanese rice fish, are fans of late-night booty calls, which is a behavior that has not been observed in captivity.</p><p>“Although medaka and other model organisms are invaluable in laboratories, their ecology in the wild remains largely unknown,” said researchers led by Yuki Kondo of Osaka Metropolitan University. “This study showed that medaka in the wild initiate spawning during late nocturnal hours and exhibit vigorous courtship behavior at midnight.”</p><p>Kondo and her colleagues recorded this vigorous courtship by placing GoPros into streams over the course of several summer nights in Gifu, Japan. The tapes revealed that medaka like to spawn in the dark, possibly to avoid predators during copulation. The results “provide the first empirical evidence that medaka mating begins significantly earlier than previously reported in the laboratory.”&nbsp;&nbsp;</p><p>For anyone who feels clueless about courtship, may I offer a page from the Medaka Sutra:&nbsp;</p><p>“The spawning behavior of medaka follows a sequence of events: the male chases the female (following), the male swims rapidly around the female (quick circle), the male wraps his dorsal and anal fins around the female (wrapping), the female releases eggs, the male releases sperm (egg and sperm release), and the male leaves the female (leaving),” according to Kondo’s team.</p><p>The only true love language is, indeed, spoken with anal fins.</p><p>Major bonus points also go to Osaka Metropolitan University’s press team for <a href=\"https://www.eurekalert.org/multimedia/1058890?ref=404media.co\"><u>throwing together this version</u></a> of Edward Hopper’s famous “Nighthawks” painting with medaka getting drinks at a bar that is also named Medaka. It is genuinely one of the most inspired public relations efforts I have ever seen, and I’m going to get a print of it to hang on my wall.</p><h3><strong>The Insect at the Edge of Earth</strong></h3><p>, or the Antarctic midge, is the only insect that lives year-round on its namesake continent. Do you know how weird you have to be to be the  insect somewhere? But this midge doesn’t care. It just lives out its bug life, which lasts two years, in an otherwise bugless wasteland.&nbsp;</p><p>Humans definitely care about the midge, though—how could we not? What is it doing there? How is it not dead? What can it teach us about cryopreservation? These questions are addressed in a new study that resolved mysteries about the animal’s interesting life cycle.</p><p>“Freeze tolerance and cryoprotective dehydration are cold tolerance strategies used by various invertebrate species in polar regions and indeed,  utilises both for overwintering,” said researchers led by Mizuki Yoshida of the Ohio State University, who completed the project while at Osaka Metropolitan University (OMU killing it this week).&nbsp;</p><p>“Larvae that are frozen in ice and cryoprotectively dehydrated readily survived 32 days of simulated overwintering,” the team said. “Unlike many insects restricted to highly specific microhabitats,  larvae inhabit a remarkably diverse range of substrates that differ in vegetation, substrate type, slope, drainage, and thermal and hydric conditions.”</p><p>I love the phrasing of “readily survived” as if the midges were eager to show off their cryoprotective superpowers. After this 32-day period they emerged with “That all you got?” energy. By studying the bugs in these simulated conditions, the researchers confirmed that they rely on multiple overwintering strategies, including a state of arrested development called “obligate diapause.”&nbsp;</p><p>“Diapause has long been assumed to be uncommon in Antarctic species, but the present study reveals that  utilises diapause for seasonal adaptation, as in many temperate species,” Yoshida and her colleagues said.&nbsp;</p><p>In addition to being the only endemic Antarctic insect, this midge has the smallest genome of any known insect while also being the largest fully terrestrial animal on the continent, even though it’s only a few millimeters long. In other words, it is the biggest animal in Antarctica that doesn’t fly or swim. Okay, Antarctic midge. You just keep doing you.</p><p>Last, turtles do a little victory dance when they find food. Yes, it is cute. Yes, there is a video.</p><p>The footage (along with <a href=\"https://youtu.be/IcI6yHr6JXo?si=DAGsav7HY4S3uhKn&amp;ref=404media.co\"></a>) is part of a study that tested if turtles could distinguish the magnetic signatures of two geographical areas. When the turtles were exposed to signatures associated with an area they associated with food, they danced in anticipation of a meal, demonstrating that they could tell the signals apart—and party accordingly.&nbsp;&nbsp;</p><p>“Hallmarks of the behaviour include some or all of the following: tilting the body vertically, holding the head near or above water, opening the mouth, rapid alternating movement of the front flippers, and, occasionally, even spinning in place, hence the name ‘turtle dance,’” said researchers led by Kayla Goforth of Texas A&amp;M University. “Turtles exhibited significantly higher levels of turtle dance behaviour when experiencing the field in which they had been fed.”</p><p>With that, let’s all tilt vertically, spin in place, and shell-abrate the long weekend.&nbsp;</p><p>Thanks for reading! See you next week.&nbsp;&nbsp;</p>","contentLength":8613,"flags":null,"enclosureUrl":"https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-09.17.41@2x.png","enclosureMime":"","commentsUrl":null},{"title":"Paragraf Is Building a \"Blank Canvas\" Graphene Foundry","url":"https://spectrum.ieee.org/paragraf-graphene-foundry","date":1739628003,"author":"Liam Critchley","guid":78,"unread":true,"content":"<p>The company wants to make graphene sensors more accessible to industries</p>","contentLength":72,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjQ2NTMyMC9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4MzQyMzgzM30.wJGF4-_y2VSVjXLsYFhhL9DuC-xHiTHWB-Ciq4DHQTU/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"These Google Photos alternatives offer tons of storage options at a reasonable price","url":"https://techcrunch.com/2025/02/15/these-google-photos-alternatives-offer-tons-of-storage-options-at-a-reasonable-price/","date":1739628000,"author":"Ivan Mehta","guid":48,"unread":true,"content":"<p>Google Photos is a great service for storing images across devices. But Google Drive and Gmail only offer 15GB of storage for free. Google Photos used to offer free unlimited storage of images, but that is not the case anymore. If you are looking for a better photo storage plan, different features, or just want […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The IRS Is Buying an AI Supercomputer From Nvidia","url":"https://tech.slashdot.org/story/25/02/15/0540249/the-irs-is-buying-an-ai-supercomputer-from-nvidia?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739624400,"author":"BeauHD","guid":183,"unread":true,"content":"According to The Intercept, the IRS is set to purchase an Nvidia SuperPod AI supercomputer to enhance its machine learning capabilities for tasks like fraud detection and taxpayer behavior analysis. From the report: With Elon Musk's so-called Department of Government Efficiency installing itself at the IRS amid a broader push to replace federal bureaucracy with machine-learning software, the tax agency's computing center in Martinsburg, West Virginia, will soon be home to a state-of-the-art Nvidia SuperPod AI computing cluster. According to the previously unreported February 5 acquisition document, the setup will combine 31 separate Nvidia servers, each containing eight of the company's flagship Blackwell processors designed to train and operate artificial intelligence models that power tools like ChatGPT. The hardware has not yet been purchased and installed, nor is a price listed, but SuperPod systems reportedly start at $7 million. The setup described in the contract materials notes that it will include a substantial memory upgrade from Nvidia.\n \nThough small compared to the massive AI-training data centers deployed by companies like OpenAI and Meta, the SuperPod is still a powerful and expensive setup using the most advanced technology offered by Nvidia, whose chips have facilitated the global machine-learning spree. While the hardware can be used in many ways, it's marketed as a turnkey means of creating and querying an AI model. Last year, the MITRE Corporation, a federally funded military R&amp;D lab, acquired a $20 million SuperPod setup to train bespoke AI models for use by government agencies, touting the purchase as a \"massive increase in computing power\" for the United States.\n \nHow exactly the IRS will use its SuperPod is unclear. An agency spokesperson said the IRS had no information to share on the supercomputer purchase, including which presidential administration ordered it. A 2024 report by the Treasury Inspector General for Tax Administration identified 68 different AI-related projects underway at the IRS; the Nvidia cluster is not named among them, though many were redacted. But some clues can be gleaned from the purchase materials. \"The IRS requires a robust and scalable infrastructure that can handle complex machine learning (ML) workloads,\" the document explains. \"The Nvidia Super Pod is a critical component of this infrastructure, providing the necessary compute power, storage, and networking capabilities to support the development and deployment of large-scale ML models.\"\n \nThe document notes that the SuperPod will be run by the IRS Research, Applied Analytics, and Statistics division, or RAAS, which leads a variety of data-centric initiatives at the agency. While no specific uses are cited, it states that this division's Compliance Data Warehouse project, which is behind this SuperPod purchase, has previously used machine learning for automated fraud detection, identity theft prevention, and generally gaining a \"deeper understanding of the mechanisms that drive taxpayer behavior.\"","contentLength":3057,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Karol Herbst Steps Down As Nouveau Maintainer Due To Linux Kernel's Toxic Environment","url":"https://www.phoronix.com/news/Karol-Herbst-Nouveau-No","date":1739619627,"author":"Michael Larabel","guid":366,"unread":true,"content":"<article>Karol Herbst has been a Nouveau driver developer for over a decade working on this open-source, reverse-engineered NVIDIA Linux graphics driver. He went on to become employed by Red Hat. While he's known more these days for his work on Mesa and the Rusticl OpenCL driver for it, he's still remained a maintainer of the Nouveau kernel driver. But today he announced he's resigning as a Nouveau driver maintainer due to differences with the upstream Linux kernel developer community...</article>","contentLength":483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KDE Developers Addressing Early Bugs From Plasma 6.3","url":"https://www.phoronix.com/news/KDE-Plasma-6.3-Early-Bugs","date":1739618860,"author":"Michael Larabel","guid":365,"unread":true,"content":"<article>KDE Plasma 6.3 released this week as the newest step forward for the KDE desktop. While it was smooth on the whole, there were some early bugs that KDE developers were dealing with this week. KDE developer Nate Graham is out with his usual weekly development summary for the Plasma desktop...</article>","contentLength":292,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Eating From Plastic Takeout Containers Can Increase Heart Failure Risk, Study Finds","url":"https://science.slashdot.org/story/25/02/15/0555235/eating-from-plastic-takeout-containers-can-increase-heart-failure-risk-study-finds?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739613600,"author":"BeauHD","guid":182,"unread":true,"content":"A new study suggests that frequent consumption of food from plastic takeout containers significantly increases the risk of congestive heart failure due to gut biome changes that trigger inflammation and circulatory damage. The Guardian reports: The authors used a two-part approach, first looking into the frequency with which over 3,000 people in China ate from plastic takeout containers, and whether they had heart disease. They then exposed rats to plastic chemicals in water that was boiled and poured in carryout containers to extract chemicals. \"The data revealed that high-frequency exposure to plastics is significantly associated with an increased risk of congestive heart failure,\" the authors wrote. [...] They put boiling water in the containers for one, five or 15 minutes because plastic chemicals leach at much higher rates when hot contents are placed in containers -- the study cited previous research that found as many as 4.2m microplastic particles per sq cm can leach from plastic containers that are microwaved.\n \nThe authors then gave rats the water contaminated with leachate to drink for several months, then analyzed the gut biome and metabolites in the feces. It found notable changes. \"It indicated that ingestion of these leachates altered the intestinal microenvironment, affected gut microbiota composition, and modified gut microbiota metabolites, particularly those linked to inflammation and oxidative stress,\" the authors wrote. They then checked the rats' heart muscle tissue and found it had been damaged. The study did not find a statistical difference in the changes and damage among rats that were exposed to water that had been in contact with plastic for one minute versus five or fifteen. The study has been published in the journal Ecotoxicology and Environmental Safety.","contentLength":1816,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The TechBeat: Futures of Ethereum II - Censorship Resistance (2/15/2025)","url":"https://hackernoon.com/2-15-2025-techbeat?source=rss","date":1739603464,"author":"Techbeat","guid":115,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/rootstock_io\">@rootstock_io</a> [ 3 Min read ] \n Rootstock merges Bitcoin’s security with Ethereum’s flexibility, enabling AI-driven blockchain apps for trustless governance, security, and fraud detection. <a href=\"https://hackernoon.com/ai-meets-bitcoin-how-rootstock-powers-the-future-of-trustless-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/boxhero\">@boxhero</a> [ 13 Min read ] \n Discover how AI-powered sentiment analysis tools deliver accurate insights from customer reviews and feedback to help improve your business strategy. <a href=\"https://hackernoon.com/sentiment-analysis-and-ai-everything-you-need-to-know-in-2025\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/lumoz\">@lumoz</a> [ 3 Min read ] \n On February 13, Lumoz announced the official launch of Lumoz Chain and released the migration guide and reward plan for Verifier nodes.  <a href=\"https://hackernoon.com/lumoz-flips-the-switch-on-its-ai-powered-blockchain\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/stellar\">@stellar</a> [ 5 Min read ] \n Regulatory shifts in 2025 will shape crypto wallets. Learn how compliance, DeFi, and Stellar’s Soroban ecosystem will impact the future of Web3 wallets. <a href=\"https://hackernoon.com/regulatory-clarity-on-wallets-will-shape-defi-in-2025\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/adambakay\">@adambakay</a> [ 22 Min read ] \n By understanding market microstructure, you might be able to add more precision into your trading. <a href=\"https://hackernoon.com/thinking-of-pursuing-trading-full-time-then-you-need-to-know-what-market-microstructures-are\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mexcmedia\">@mexcmedia</a> [ 6 Min read ] \n Liquidity is key to crypto trading, ensuring price stability, seamless transactions, and reduced slippage. Learn how MEXC excels in liquidity management. <a href=\"https://hackernoon.com/the-importance-of-liquidity-how-it-impacts-crypto-trading\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 11 Min read ] \n Explore the evolution of crypto options and perpetual futures, diving into innovations like panoptions, liquidity challenges, and decentralized trading. <a href=\"https://hackernoon.com/neverending-options-trading-options-to-infinity-and-beyond\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/2077research\">@2077research</a> [ 17 Min read ] \n The article explores Ethereum's efforts to ensure censorship resistance, focusing on solutions like PBS and encrypted mempools amid regulatory pressures. <a href=\"https://hackernoon.com/futures-of-ethereum-ii-censorship-resistance\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hayday\">@hayday</a> [ 4 Min read ] \n Is the rise of vibe coding also the end of software engineering? How will vibeware change the nature of the software entrepreneur, and the meaning of work? <a href=\"https://hackernoon.com/vibe-coding-a-new-system-of-the-world\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mexcmedia\">@mexcmedia</a> [ 5 Min read ] \n Discover key crypto trends of 2025, from Bitcoin’s surge to MEXC’s role in shaping the future of digital asset trading with liquidity, security, &amp; innovation. <a href=\"https://hackernoon.com/2025-crypto-landscape-emerging-trends-and-exchange-platforms-shaping-the-future\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/noda\">@noda</a> [ 4 Min read ] \n 2025 is the tipping point for pay-by-bank. Lower fees, instant payments, and new regulations make it the future of digital transactions.  <a href=\"https://hackernoon.com/card-payments-are-slow-costly-and-dyingheres-what-consumers-are-turning-to-instead\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/matteopisani91\">@matteopisani91</a> [ 34 Min read ] \n A cybersecurity engineer built a sci-fi bass from scratch, packed with a synth, a wireless transmitter, a hacked built-in tuner and voltmeters. <a href=\"https://hackernoon.com/diy-enthusiast-hacks-his-way-into-building-his-own-musical-instrument-and-it-rocks\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ishanpandey\">@ishanpandey</a> [ 4 Min read ] \n Discover how CrossFi is revolutionizing crypto payments in this exclusive interview with CEO Alex Mamasidikov. <a href=\"https://hackernoon.com/crypto-without-banks-crossfis-ceo-on-the-future-of-decentralized-payments\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/menaskop\">@menaskop</a> [ 6 Min read ] \n Ethereum has many so-called \"killers,\" though most of them look more like self-destructive projects. <a href=\"https://hackernoon.com/is-ethereum-in-trouble-no-so-why-does-everyone-say-otherwise\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/wezam\">@wezam</a> [ 4 Min read ] \n There’s plenty of disagreement on how AI will change the product management landscape.  <a href=\"https://hackernoon.com/is-ai-making-product-managers-obsolete\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/awsmarketplace\">@awsmarketplace</a> [ 8 Min read ] \n Discover the best endpoint protection solutions, top platforms to consider, and key evaluation criteria to enhance your organization's cybersecurity defenses. <a href=\"https://hackernoon.com/evaluating-top-5-endpoint-protection-solutions\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/vitae\">@vitae</a> [ 5 Min read ] \n We present a deterministic game incorporating two key mechanisms: Controlled Chaos Shifts (CCS) and Accepting Loss of Control (ALC). <a href=\"https://hackernoon.com/the-first-provable-ai-proof-game-introducing-butterfly-wings-4\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/edwinliavaa\">@edwinliavaa</a> [ 3 Min read ] \n While Bitcoin's design brilliantly enables decentralization, human nature consistently pulls us toward centralization. <a href=\"https://hackernoon.com/bitcoin-is-eerily-resembling-the-financial-system-it-was-meant-to-replace\">Read More.</a></p>","contentLength":3218,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Used To Design a Multi-Step Enzyme That Can Digest Some Plastics","url":"https://science.slashdot.org/story/25/02/15/0549201/ai-used-to-design-a-multi-step-enzyme-that-can-digest-some-plastics?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739602800,"author":"BeauHD","guid":181,"unread":true,"content":"Leveraging AI tools like RFDiffusion and PLACER, researchers were able to design a novel enzyme capable of breaking down plastic by targeting ester bonds, a key component in polyester. Ars Technica reports: The researchers started out by using the standard tools they developed to handle protein design, including an AI tool named RFDiffusion, which uses a random seed to generate a variety of protein backgrounds. In this case, the researchers asked RFDiffusion to match the average positions of the amino acids in a family of ester-breaking enzymes. The results were fed to another neural network, which chose the amino acids such that they'd form a pocket that would hold an ester that breaks down into a fluorescent molecule so they could follow the enzyme's activity using its glow.\n \nOf the 129 proteins designed by this software, only two of them resulted in any fluorescence. So the team decided they needed yet another AI. Called PLACER, the software was trained by taking all the known structures of proteins latched on to small molecules and randomizing some of their structure, forcing the AI to learn how to shift things back into a functional state (making it a generative AI). The hope was that PLACER would be trained to capture some of the structural details that allow enzymes to adopt more than one specific configuration over the course of the reaction they were catalyzing. And it worked. Repeating the same process with an added PLACER screening step boosted the number of enzymes with catalytic activity by over three-fold.\n \nUnfortunately, all of these enzymes stalled after a single reaction. It turns out they were much better at cleaving the ester, but they left one part of it chemically bonded to the enzyme. In other words, the enzymes acted like part of the reaction, not a catalyst. So the researchers started using PLACER to screen for structures that could adopt a key intermediate state of the reaction. This produced a much higher rate of reactive enzymes (18 percent of them cleaved the ester bond), and two -- named \"super\" and \"win\" -- could actually cycle through multiple rounds of reactions. The team had finally made an enzyme.\n \nBy adding additional rounds alternating between structure suggestions using RFDiffusion and screening using PLACER, the team saw the frequency of functional enzymes increase and eventually designed one that had an activity similar to some produced by actual living things. They also showed they could use the same process to design an esterase capable of digesting the bonds in PET, a common plastic. The research has been published in the journal Science.","contentLength":2629,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reading Documentation Shouldn't Be a Chore","url":"https://hackernoon.com/reading-documentation-shouldnt-be-a-chore?source=rss","date":1739602109,"author":"Rami James","guid":114,"unread":true,"content":"<p>As a developer, documentation is both your greatest ally and your worst nemesis. It's the key to unlocking the power of libraries, frameworks, APIs, and even entire programming languages. Yet, many developers struggle to effectively navigate and extract the information they need from documentation. It's a skill that can be learned, and it's one that can make a huge difference in your productivity and the quality of your work. New developers often lack this critical skill, and find themselves stopped in their progress towards becoming a better dev because of it.</p><p>\\\nI'm currently writing the docs for Vewrite, a project management tool for writing teams. It occurs to me that I've been writing documentation for a long time, and I've gotten pretty good at it. I've also gotten pretty good at reading documentation, and I think that this is a skill that is often overlooked.</p><p>\\\nLet's talk about how to equip you with the skills to read documentation better, turning this often daunting task into a smooth and efficient process.</p><h3>Why is reading documentation challenging?</h3><p>\\\nGood documentation is essential. It provides the definitive guide to how software works, saving you countless hours of pointless, frustrating trial and error. Bad documentation is a nightmare. It's like trying to navigate a maze blindfolded, with no map and no sense of direction, guided by a liar.</p><p>\\\nIf you have access to the code you are working against, that's great and you should use it. Poke around and see what you're supposed to do by reading the code. But, in many cases (and almost all of the time with proprietary software), you're going to have to rely on the documentation to get you through. However, documentation can vary wildly in quality. Some is meticulously crafted, while others can be uninformative, outdated, or even misleading. This inconsistency is one of the reasons why reading documentation can be challenging.</p><p>Some docs are just a dumping ground for information, with no clear structure or organization. This can make it difficult to find what you're looking for, and you end up spending more time searching than actually learning. A well-structured set of documentation will have a clear table of contents, with sections and subsections that guide you through the material in a logical order. This makes it easier to navigate and find the information you need.In general, I'll also recommend some Getting Started section which points different types of users to the information which is most critical to them.</p><p>Information overload is another common hurdle. Docs can cover a vast range of features and functionalities, making it difficult to find the specific information you need. For example, a large framework might have hundreds of classes and methods, and trying to find the one you need can feel like searching for a needle in a haystack.</p><p>Unclear technical jargon can also be a barrier, as documentation is inherently technical, and sometimes the terminology can be confusing, especially for newcomers. This often stems from the assumption that the reader has a certain level of prior knowledge as it is largely written by the team who has developed the software and has an intimate knowledge of its innner workings.</p><p>Outdated information can be a major problem. Software evolves rapidly, and documentation can sometimes lag behind, leading to frustration and wasted time. A library might have deprecated a certain function, but the documentation might still describe it as the primary way to perform a task. The teams who produce software should be including accurate documentation creation and upkeep in their backlog as part of their process. Unfortunately, in the haste to get features shipped to market, corners are often cut. I've spent the better part of the last decade advocating for better processes that lead to better docs at a number of companies, and the biggest challenge here has been explaining to stakeholders where the value is.</p><h3>How to read documentation effectively</h3><p>Over the years, I've developed a structured approach to how I tackle reading new documentation. I've found that this approach helps me quickly get up to speed with new tools and technologies, and I hope it can help you too.</p><p>\\\nBy and large this method has three parts: understanding the structure of the documentation itself, understanding the way that the system I'm learning works, and lastly engaging with the material in the docs. First I get the lay of the land, and then I try to focus in on the details.</p><h4>Understand the structure of the documentation</h4><p>Docs are written by people, and every sysyem is going to be different. That means that every set of docs is going to be different. Some docs are organized by feature, others by use case, and still others by API endpoint. Understanding the structure of the documentation is key to finding the information you need quickly. Start by scanning the table of contents to get an overview of the topics covered. This will help you get a sense of the scope of the documentation and where to find specific information. You'll want to quickly click through the docs and get a sense of how things work, how they are organized, and where you can find reference materials if there are any.</p><p>\\\nI find that this initial run through also trickles some information about the system and its components into my brain. It's not comprehensive at all, but it's a good start.</p><p>Every piece of software is a system, and understanding how that system works is key to understanding how to use it. This is where the documentation can be a huge help. Good documentation will provide you with a high-level overview of the system, explaining how the different components fit together and how they interact. This can help you understand the context in which a particular feature or function is used, and how it relates to the rest of the system. This is where you start to get a sense of how the system is supposed to work, and how you can use it to solve your problems.</p><p>\\\nA lot of the problems that you'll encounter when building software will be because you fundamentally don't understand how what you're building with actually works. This is where the documentation can be a huge help. It can provide you with the context you need to understand how the system is supposed to work, and how you can use it to solve your problems. Use that to your advantage and you'll be able to build better software, faster.</p><p>This is often the most tedious and boring part. You have to actually take a few hours and read through the entire docs. For smaller libraries or APIs, this isn't going to be a big deal and you'll be able to fit most of it into your head. For larger systems, this is going to be a multi-day process. You'll want to take notes, ask questions, and engage with the material in a way that helps you understand it. This is where you start to get a sense of how the system is supposed to work, and how you can use it to solve your problems. If it doesn't all fit into your head now, that's ok. You'll remember more than you think you will, and when you're actually doing development it will be an order of magnitude faster to find an answer if you've already seen it once.</p><p>I've actually found that modern AIs are absolutely awesome for this step. You can use them as you would a more experienced developer and ask them questions about the system, the functionality, and the docs themselves. You can't trust everything that they say, but you can't really trust what another developer always tells you. I've found that being able to have a discussion with an AI about a library or API really helps me understand how stuff works at a deeper level.</p><h3>Strategies for effectively reading docs</h3><p>Despite these challenges, mastering the art of reading documentation is doable. Here are some strategies to keep in mind while you are digging around in docs, looking for an answer.</p><p>If you don't know where you're going, you may never get there. Before you dive into a set of docs, you may want to set a clear objective for yourself. Are you trying to understand a specific function, integrate a new library, or troubleshoot an error? Having a clear objective will help you focus your search.</p><p>\\\nFor example, if you're trying to fix a bug where a specific function isn't returning the expected value, and your code matches what the docs say that you're supposed to do, your goal is to understand the function's place in the wider system. The problem is going to be a few levels up and the docs are likely going to be the key to understanding how the software works at a higher level.</p><p>\\\nDon't immediately jump into the nitty-gritty details. Begin with the introductory sections, tutorials, or quick start guides to get a high-level understanding of the system. Your assumptions will often be wrong, and it is good to base your work on a foundation of truth. Think of it like reading the abstract of a research paper before diving into the full text. These introductory materials often provide a roadmap to the rest of the documentation.</p><h4>Use the structure of the docs to your advantage</h4><p>Use the table of contents and search function (these are your best friends) to quickly locate the sections you need. Don't be afraid to experiment with different search terms. Scan through the entire table of contents. Poke around and get a really good feel for how the documentation is structured. Sometimes you will find that your answers are hidden adjacent to where you thought that they would be.</p><p>Real-world examples are invaluable, and the best docs are going to include them. They demonstrate how to use the code in practice and can often clarify confusing concepts. Pay close attention to the context in which the examples are used. A code snippet demonstrating how to use a particular function is much more helpful if it's accompanied by an explanation of what the function does and why it's being used in that specific scenario.</p><h4>Pay attention to the details</h4><p>When looking at function or method definitions, carefully examine the data types of the parameters and return values. This will help you understand how to use them correctly. Sometimes, crucial information is hidden in footnotes, warnings, or less prominent sections. Make sure you scan the entire page, not just the headlines. A seemingly minor note might contain a critical piece of information that can save you a lot of trouble. Documentation often contains links to related topics or other parts of the documentation. Don't hesitate to follow these links to gain a deeper understanding. These links can often lead you to more detailed explanations, examples, or even the source code itself.</p><p>This is one of the biggest reasons why I believe that open-source can be such a boon for companies who want to build an ecosystem of developers around their software. Sometimes, the best way to understand how something works is to look at the source code itself. Don't be afraid to dive in, especially if the documentation is unclear. Reading the source code can give you a much clearer picture of what's happening under the hood. Try out the code examples and modify them to see how they behave. This is a great way to solidify your understanding and discover edge cases. Experimenting with the code is like conducting your own little science experiment. You can change the inputs, observe the outputs, and gain a deeper understanding of how the code works. If you're still stuck, don't hesitate to reach out to the community for help. Forums, mailing lists, and online communities are valuable resources for getting your questions answered. The developer community is generally very helpful, and there are often people who have encountered the same problems you're facing.</p><p>Reading documentation shouldn't be a passive activity. Engage with the material by taking notes, jotting down key points, examples, and any questions you have. Annotating the documentation (if possible), highlighting important sections and adding your own comments can also be helpful. Creating your own examples, writing small programs that use the features you're learning about, is a great way to reinforce your understanding. It's like practicing a musical instrument – the more you do it, the better you get.</p><p>Reading documentation is a skill that improves with practice. By adopting these strategies and actively engaging with the material, you can transform documentation from a source of frustration into a powerful tool for learning and development. So, embrace the documentation, and unlock the full potential of the software you're working with!</p>","contentLength":12550,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Expose (And Fix) Hidden Bottlenecks in Adobe Experience Manager","url":"https://hackernoon.com/how-to-expose-and-fix-hidden-bottlenecks-in-adobe-experience-manager?source=rss","date":1739601941,"author":"Giuseppe Baglio","guid":113,"unread":true,"content":"<h2><em>Learn how to read thread dumps and take control of your application’s runtime behaviour.</em></h2><p>\\\nWhen your Adobe Experience Manager (or in general any JAVA application) instance shows signs of sluggishness, it’s time to roll up your sleeves and dive into the world of thread dumps. IBM Thread Analyzer (TDA) is here to help you untangle the web of threads and pinpoint performance bottlenecks. In this guide, we’ll walk you through how to use IBM TDA to diagnose performance issues in AEM like a pro.</p><p>Before you can start analyzing thread dumps, you’ll need to download and install <a href=\"https://www.ibm.com/support/pages/ibm-thread-and-monitor-dump-analyzer-java-tmda\">IBM Thread Analyzer</a>. Head over to the official IBM website or your organization’s repository to grab the latest version. Once downloaded, follow the installation instructions for your operating system. It’s quick, easy, and sets the stage for some serious troubleshooting.</p><p>Thread dumps are snapshots of all the threads running in your AEM instance at a specific moment. To capture them:</p><ol><li>Use tools like , , or AEM’s built-in functionality to generate thread dumps. There is a well-documented page on <a href=\"http://erience-cloud-kcs/kbarticles/ka-17452\">Adobe Docs</a>.</li><li>Save the thread dump files to your local machine.</li></ol><p>\\\nPro Tip: Capture multiple thread dumps at intervals (e.g., every 10 seconds) to get a clearer picture of long-running issues.</p><p>Launch IBM TDA and open the thread dump files you’ve captured. Simply drag and drop the files into the application or use the “Open” option to load them. Once loaded, you’ll see a list of thread dumps on the left-hand panel.</p><p>To analyze a specific thread dump:</p><ol><li><p>Select the file from the listing.</p></li><li><p>Click the Thread Detail button at the top</p></li></ol><p>\\\nThis will display a detailed view of all the threads in that dump. Now, let’s sort the threads by Stack Depth, ensuring the longest stacks appear at the top. Why? Threads with deeper stacks often indicate more complex operations, which are usually where performance issues hide.</p><p>Focus on threads with a stack depth of 10 lines or longer. These threads are typically the ones consuming the most resources. Take notes on any threads that stand out — whether due to their names, states, or stack traces.</p><p>Next, sort the threads by their State. Scroll down to the Runnable threads. These are the threads that were actively using CPU time when the dump was taken. Keep an eye out for application-specific threads, such as:</p><ul><li><p>Background job threads: Handling tasks like indexing or replication.</p></li><li><p>Request threads: Named like <code>127.0.0.1 [timestamp] GET /path HTTP/1.1</code>.</p></li></ul><p>For each request thread, extract the timestamp from its name (e.g., ). This Unix epoch timestamp tells you when the user’s browser made the request. Convert it to a human-readable date/time using a tool like <a href=\"https://www.epochconverter.com/\">https://www.epochconverter.com/</a>. Compare this with the thread dump’s timestamp to calculate how long the request has been active.</p><p>If the difference is unusually large (e.g., several seconds or minutes), it could indicate a bottleneck in your application.</p><p>\\\nPro Tip: Keep an eye out for patterns. Are certain types of requests consistently taking longer? For example, requests involving complex queries or resource-heavy operations might be worth optimizing. Additionally, if you notice that specific URLs or endpoints are frequently associated with long-running threads, consider profiling those areas of your codebase.</p><p>Thread analysis requires a nuanced approach that goes beyond simple waiting states. While the IBM Thread Analyzer (TDA) interface provides valuable insights into thread relationships, understanding the full context of thread behavior helps create a more complete picture of your application’s performance characteristics.</p><h2>Understanding Thread States</h2><p>When examining threads in TDA, you’ll encounter several important states:</p><p>: These threads are either currently executing or ready to execute when CPU time becomes available. A Runnable state doesn’t necessarily indicate a problem — it’s the natural state for actively working threads.</p><p>: These threads have temporarily paused execution while waiting for a condition to be met. The waiting state can occur for many legitimate reasons, including:</p><ul><li>Resource availability (database connections, file handles)</li><li>Task completion in other threads</li></ul><p>\\\n: These threads are specifically waiting to acquire a monitor or lock. While similar to waiting, blocked states specifically indicate synchronization-related pauses.</p><h2>Analyzing Thread Relationships</h2><p>When you identify a thread of interest, examine its relationships with other threads using this systematic approach:</p><ol><li>Direct Lock Relationships:</li></ol><ul><li>Examine the Waiting Threads panel for immediate dependencies</li><li>Review the stack traces of waiting threads to understand why they’re blocked</li><li>Note the duration of the wait states if available</li></ul><p>\\\n2. Resource Usage Patterns:</p><ul><li>Look for patterns in resource acquisition and release</li><li>Identify potential resource bottlenecks</li><li>Consider alternative resource management strategies</li></ul><p>\\\n3. Architectural Implications:</p><ul><li>Evaluate if the observed behaviour aligns with the system’s design</li><li>Consider if the current threading model is appropriate</li><li>Assess the impact on scalability</li></ul><h2>Understanding Lock Types and Visibility</h2><p>Thread dumps may not show all types of contention. Modern Java applications use various synchronization mechanisms:</p><ol><li>Intrinsic Locks (synchronized keyword):</li></ol><ul><li>Show clear owner-waiter relationships</li><li>Stack traces indicate synchronization points</li></ul><p>\\\n2. Explicit Locks (java.util.concurrent):</p><ul><li>May require additional tooling to visualize</li></ul><p>\\\n3. Non-blocking Mechanisms (Don’t appear as traditional locks but can impact performance):</p><ul></ul><p>When you identify genuine contention issues, consider these approaches:</p><ul><li>Implement finer-grained locking</li><li>Consider non-blocking alternatives</li></ul><ul><li>Implement backoff strategies</li><li>Consider caching solutions</li></ul><p>\\\n3. Architectural Changes</p><ul><li>Evaluate asynchronous processing</li><li>Consider parallel execution paths</li><li>Implement queue-based approaches</li></ul><p>\\\nRemember that thread analysis is an iterative process. Patterns that emerge in one thread dump might not represent consistent behaviour. Always validate your findings across multiple dumps and different time periods before making significant changes to your application.</p><p>Comparing thread dumps across time reveals important performance patterns in your AEM instance. Start by establishing a baseline during normal operation, including peak usage periods and maintenance windows. This baseline provides context for identifying abnormal thread behaviour.</p><p>To determine if a thread is persistent across time:</p><ol><li>Select multiple thread dumps from different points in time.</li><li>Click the Compare Threads button in IBM TDA.</li><li>Look for threads that remain in the Runnable state across all dumps, especially those with consistently long stack traces.</li></ol><p>\\\nUse IBM TDA’s Compare Threads feature to analyze dumps from different time points. Focus on threads that persist across multiple dumps, examining their states, stack depths, and resource usage. Remember that thread persistence alone doesn’t automatically indicate a problem — background services naturally run continuously, while request threads should complete within expected timeframes.</p><p>\\\nWhen analyzing persistent Runnable threads, correlate their behaviour with system metrics like CPU usage, memory consumption, and response times. Consider the thread’s purpose: background services, request processing, or maintenance tasks each have different expected patterns. For request threads, compare their duration against defined service level agreements and business requirements.</p><p>\\\nGot a suspicious thread pattern? Don’t jump to conclusions just yet! Try to recreate the issue in your test environment first — it’s like having a dress rehearsal before the main show. Take a good look at your code, double-check those config settings, and consider what else might be stirring up trouble in your environment. Keep track of what you find with real performance numbers and test results — you’ll thank yourself later.</p><p>\\\nOnce you’re sure you’ve caught a real performance culprit (backed by solid evidence, of course), it’s time to fix it.</p><p>If analyzing threads doesn’t yield actionable insights, switch to the Monitor Detail view:</p><ol><li><p>Go back to the thread listing.</p></li><li><p>Select a thread dump and click the Monitor Detail button.</p></li><li><p>IBM TDA will display a tree view of monitor-owning threads and their waiting threads.</p></li></ol><p>\\\nThis view helps you identify threads that are holding monitors and causing contention. Understanding thread monitors is like viewing the nervous system of your application. These synchronization mechanisms control how threads access shared resources, preventing potential conflicts and ensuring smooth operation.</p><p>\\\nMonitor interactions can reveal critical performance insights. Some threads will be actively processing requests, while others wait for resource acquisition or participate in coordinated activities. Not all waiting or idle threads indicate a problem — they’re often part of the application’s natural resource management strategy.</p><p>\\\nHowever, not all threads are equally important:</p><ul><li>Ignore idle thread pool threads: These threads typically have ≤10 stack lines and are part of thread pools like the servlet engine. They’re usually harmless unless they dominate the thread pool.</li><li>Focus on application-specific monitors: Look for monitors tied to your application’s business logic, such as database connections, caching mechanisms, or custom synchronization blocks.</li></ul><p>\\\nRemember that thread and monitor analysis is both an art and a science. Each application has unique characteristics, so approach performance optimization with curiosity and a holistic perspective. The goal is not to eliminate all waiting threads but to understand and optimize their interactions.</p><p>\\\nAdvanced Tip: If you notice certain monitors are frequently contended, consider refactoring your code to reduce lock granularity. For example:</p><ul><li>Replace coarse-grained locks with fine-grained ones.</li><li>Use non-blocking algorithms or concurrent data structures where possible.</li><li>Optimize database queries to reduce the time threads spend waiting for locks.</li></ul><p>In some thread dumps, you might notice the  appearing frequently. This service handles tasks like Garbage Collection, memory management, and resource cleanup. While the Collector Service might seem like a mysterious background process, understanding its behaviour is key to maintaining optimal system performance — think of it like a diligent janitor in a large office building.</p><p>\\\nWhen you notice frequent Collector Service activity, don’t immediately assume disaster. It’s normal for the Collector Service to show up occasionally, but excessive activity could indicate underlying issues:</p><ul><li>Memory leaks: Objects that are not being garbage collected can cause frequent GC cycles.</li><li>High object churn: Rapid creation and destruction of objects can overwhelm the garbage collector.</li><li>Improper JVM settings: Misconfigured heap sizes or GC algorithms can lead to inefficiencies.</li></ul><p>\\\nHere are some considerations to optimize resource usage:</p><ul><li>Tuning your JVM settings (e.g., increasing heap size, switching to G1GC).</li><li>Reviewing your application’s memory allocation patterns to reduce unnecessary object creation.</li></ul><p>\\\nGarbage Collection is not a problem to be solved, but a dynamic system to be understood and optimized. Each application has unique characteristics, and there’s no universal solution.</p><p>Thread dump analysis is a developer’s superpower — transforming you from a code writer to a performance detective. IBM Thread Analyzer (TDA) is your key to understanding complex system behaviours, revealing hidden bottlenecks that impact your Java/AEM instance’s performance.</p><p>\\\nLike learning an instrument, your skill improves with practice. Each thread dump becomes clearer, revealing intricate patterns of system interactions. The more you analyze, the more intuitive performance optimization becomes.</p><p>\\\nRemember, practice makes perfect — the more you analyze thread dumps, the sharper your diagnostic skills will become. 📊💪</p><p>\\\n🛠 ️Happy troubleshooting! And don’t forget to share your findings with your team to keep your Java/AEM instance running smoothly.</p>","contentLength":12075,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Launch a Product in a New Market Is Hard, So I Made a 4-Step Framework for Success","url":"https://hackernoon.com/launch-a-product-in-a-new-market-is-hard-so-i-made-a-4-step-framework-for-success?source=rss","date":1739601806,"author":"Roman Shimanskiy","guid":112,"unread":true,"content":"<p> because there’s just no real demand for what they’re offering. A staggering number and a logical reason behind it – and it’s the main one, according to CB Insights. It’s not about a lack of funding, bad hires, or tough competition. The real question is whether people actually need what you’re about to offer them.</p><p>\\\nPlus, having a great product isn’t enough. To win, you need to understand the competitive landscape, your audience, and how to get your brand in front of the right people. Drawing from my experience launching Yango Play in MENA — where we developed an AI-powered Entertainment SuperApp that quickly became one of the region’s leading streaming platforms — I want to share the key steps we took to make it the success it is now.</p><p><strong>Selecting the right market</strong> is a balance between <strong>competition and profitability.</strong> Some regions offer low competition but limited revenue potential, while others may be more lucrative but very, very saturated. Sub-Saharan Africa, for example, has a large population and low competition, making it an appealing early-stage entry option. But, lower consumer spending restricts growth. MENA, on the other hand, has a smaller population but significantly higher consumer purchasing power, making it an attractive place for expansion.</p><p>\\\n is just as critical as market selection. Entering too early may mean slow adoption while entering too late risks facing intense competition.</p><blockquote><p>If competition is low, it provides an advantage and accelerates market growth. The key is to enter before the market experiences a surge in adoption.</p></blockquote><p>\\\nIn the CIS region, the subscription economy remained relatively flat at 1–2 million subscribers for several years before suddenly accelerating to 100% annual growth. The same pattern emerged in MENA, indicating that the region was nearing a similar tipping point, making it the optimal time to enter.</p><p>\\\nMarket data is here to reinforce this statement. MENA’s total population is around 500 million, with 13.5 million already subscribed to video streaming services. Just for reference, penetration of subscription services in CIS region stands at 15-20%, with slight differences between countries. Even without complex economic modeling, these figures indicated a major opportunity for subscription-based services in MENA. So, an ideal market for expansion. In this case, the potential formed the perfect ground for selecting the market and strategy – the choice was easy.</p><p>\\\nStill, the region was already highly competitive, with more than 50 video streaming platforms and many global and local music services. We had strong competitors in each category, so for us entering the market required more than just identifying demand.</p><h3><strong>Product and Content Differentiation</strong></h3><p>Creating a strong differentiator was key, and competing head-on with zero brand awareness would have been very much an uphill battle.</p><p>\\\nTake YANGO’s case study on product differentiation. We simultaneously implemented two strategies — with **differentiation through the product and content. \\n </p><ol><li><p>An Entertainment SuperApp — <strong>a single subscription combining music, movies, and games.</strong> At the time, there wasn’t anything similar, so we gladly took the opportunity for this strategic entry point. Users typically juggle multiple subscriptions: one for music, another for video, and yet another for gaming. We built everything into one app under a single subscription. This way, we got on the growing global trend toward bundled entertainment services and also filled a gap in our target market. Win-win.</p></li><li><p>Netflix has long become the go-to platform for global content. We took a different route and built an ecosystem tailored to local tastes. Our SuperApp has regional music, Arabic-language films, and localized gaming options, making it much more relevant to MENA audiences. This became a critical differentiator, making our product not just another streaming service but a culturally attuned entertainment hub.</p></li></ol><p>\\\nConclusion: <em>To successfully differentiate a product, the best strategy is to identify weaknesses or unmet needs in competing products — and fill those gaps.</em></p><p>\\\nIn crowded markets, pricing is as crucial as the product. It’s not the features that make or break the product – it’s about making your product the most logical choice.</p><p>\\\nSo, we introduced a long trial period and a bundled pricing model, allowing users to get music and movies for the price of one. Instead of discounting, we maximized perceived value, making the subscription an easy, rational decision.</p><p>\\\nExpanding into a new market meant building a local team and adapting to language and cultural nuances. With deep market research, perfect timing, and a product tailored to real consumer needs, the SuperApp secured a strong foothold in a high-growth market. How did we tackle these challenges? That’s a story for another article.</p><h3><strong>Audience Differentiation: Defining the Target User</strong></h3><blockquote><p><em>Users have mental “boxes” where they sort every new product they see. Food delivery? That goes into one box. A streaming service? Another box. But if your product doesn’t fit neatly into a box, it triggers confusion — people don’t know what to do with it. And when something feels unfamiliar, the default reaction is to ignore it.</em></p></blockquote><p>\\\nThat’s why crystal-clear positioning isn’t optional — it’s survival. If users can’t instantly grasp who your product is for and why they need it, you’re fighting an uphill battle. But just fitting into a category isn’t enough. You have to own it. When people think about entertainment subscriptions, they should immediately think of your brand.</p><p>\\\nHow do you make that happen? Precision. <strong>Messaging needs to be sharp, clear, and unmistakable.</strong> Instead of vague positioning, spell it out: “This is a service for modern Arabic-speaking users.” That clarity eliminates confusion and makes it easier for potential subscribers to see how the product fits into their everyday world.</p><p>\\\nBrand anchoring is everything— <strong>the simpler and more memorable the positioning, the stronger the audience connection.</strong> If users struggle to define what the product is or who it’s for, they’ll tune it out. That’s a death sentence for any brand. The fix? Build an instant mental link. The moment someone thinks “entertainment subscription,” your SuperApp should be the first thing they recall.</p><p>For startups that can’t outspend competitors, traditional marketing funnels — awareness → consideration → purchase — might not be the best play. Instead, skip the awareness stage and go straight for user acquisition.</p><p>\\\nHere’s the logic: if someone downloads your app, they already know it exists. There’s no need to waste resources on brand awareness before conversion. Yes, this approach raises the cost per acquisition (CPA) since users are coming in cold, but the trade-off is greater control over spending and a more direct path to growth.</p><p>\\\n<strong>The real power lies in performance-driven marketing, which focuses on conversions rather than broad exposure.</strong> Even brands with large media budgets rely on performance marketing as the most efficient way to drive actual user engagement. For lean startups, this strategy isn’t just viable — it’s essential.</p><p><strong>A product launch is a one-time shot</strong> at grabbing audience attention —  over a long, drawn-out campaign. The smartest play? Go loud.</p><p>\\\n<em>The launch moment is your biggest marketing asset — a rare window where PR and buzz can be built just around your existence. Instead of slow, passive brand-building, focus on making a splash that gets people talking.</em></p><p>\\\nOnce the initial hype kicks in, then comes the next phase — brick by brick, reinforcing brand awareness and earning user trust. We also saw this proven in the case of Kinopoisk (video streaming platform). We focused our marketing around major premieres such as <em>Zack Snyder's Justice League</em>, , and big exclusive original premieres.</p><h3><strong>Shifting from Traditional Advertising to Alternative Channels</strong></h3><p>From experience, <em>buying direct media ads is often far less effective than leveraging influencers, word of mouth, and strong PR.</em></p><p>\\\nSpending time on finding the right influencer, crafting a viral PR angle, or developing creative content that resonates delivers higher ROI than just pouring money into paid ads. <strong>In a budget-constrained environment, the smarter play is a two-step approach:</strong></p><ul><li><p>Performance marketing first — drive installs and conversions while simultaneously learning how users interact with the product.</p></li><li><p>Organic &amp; influencer-driven marketing second — building credibility and momentum through earned media, viral content, and strategic partnerships.</p></li></ul><p>With influencers, you’re hitting two targets at once. First, their audience gets exposed to your service, boosting brand awareness. But the real power of influencer marketing is in its high conversion rates — it doesn’t just build visibility; it drives action. It works like performance marketing in many ways. Even better? The creative content influencers produce often performs exceptionally well in paid performance campaigns, making it a valuable asset beyond the initial collaboration. So, really, you’re getting even more value out of every campaign.</p><p>\\\nThis keeps acquisition costs under control and ensures real engagement and long-term user retention.</p><p>\\\nOne of the most successful cases of leveraging PR instead of direct advertising is Netflix. When expanding into new regions, the company rarely invests in traditional media ads. Instead, it focuses on localized content, influencer collaborations, and large-scale PR campaigns. This approach has allowed Netflix to dominate new markets without massive ad spending, making the brand recognizable through organic user discussions and viral interest.</p><p>\\\nThere are plenty of examples of viral brands and services — take Dubai Chocolate or Clubhouse, for instance. These apps and platforms managed to gain massive audiences with little to no marketing spend, relying purely on organic growth and word of mouth.</p><h3><strong>The Founder as the Brand’s Leading Ambassador</strong></h3><p>In a competitive market, PR isn’t just about press releases and media coverage — it’s about people. A founder holds a unique advantage: their voice becomes the product’s most powerful marketing tool.</p><p>\\\nPeople don’t engage with companies — they engage with other people. Instead of relying on corporate statements or press releases, modern PR is about creating shareable content and building influence through authentic, human-driven narratives.</p><p>\\n Whether you are a startup or an established company, entering a new market requires more than a strong product — it asks for adaptability to evolving industry rules and dynamics. The strategies that worked in the past may no longer be effective in an environment where competition is fierce, consumer behavior is shifting, and digital ecosystems are rapidly changing. Success hinges on thorough market research, precise audience targeting, strategic product differentiation, and a well-structured promotional approach.</p>","contentLength":11021,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Every New Apple Device Expected in 2025","url":"https://hackernoon.com/every-new-apple-device-expected-in-2025?source=rss","date":1739600519,"author":"David Perru","guid":111,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Hospitals Use AI to Boost Efficiency in Medical Imaging Technology","url":"https://hackernoon.com/how-hospitals-use-ai-to-boost-efficiency-in-medical-imaging-technology?source=rss","date":1739600517,"author":"Beth Rush","guid":110,"unread":true,"content":"<p>\\\nAdvancing health care technologies are crucial for doctors and patients. They could make medical services easier to provide while giving people better results, but only if everyone understands the bigger picture. Artificial intelligence (AI) is a critical tool that may become part of your health care services in the near future. Check out how physicians are using AI in medical imaging and diagnostics to learn why it could revolutionize the industry.</p><h2><strong>What Is Medical Imaging Technology?</strong></h2><p>Medical imaging technology is any tool that  to monitor normal and abnormal cases. The term includes various services that assist health care providers in making diagnoses. While they’ve existed long before automation, artificial intelligence in medical imaging is becoming more widespread through software updates.</p><h2><strong>Different Types of Imaging Techniques</strong></h2><p>You’ll better understand why AI could be an essential scientific tool if you know which imaging techniques people undergo. Learn more about each form of technology to understand AI’s advantages and challenges more easily.</p><p>X-ray machines are penetrative imaging tools that look at targeted areas inside the body before sending the results to imaging technology. The machine sends ionizing radiation into a patient to gather pictures for diagnosis. X-rays are especially useful for bone inquiries because the radiation passes through them and makes them bright white.</p><h3><strong>Magnetic Resonance Imaging (MRI)</strong></h3><p>If you need an MRI, a doctor might inject a contrast agent into your bloodstream  before you sit in the machine. Radiowaves, a magnet and a computer processor look inside the selected part of your body to reveal everything from tissues to joints.</p><p>Ultrasound imaging  to see what’s in your body. The machinery bounces soundwaves off of your organs and tissues instead. This isn’t a penetrative imaging technique, even though tools like transvaginal wands do work inside the vaginal canal.</p><p>Doctors needing more complex imaging may order CT scans. They combine an X-ray with a computer to  before it bounces back on a plate behind the patient. The machinery scans in a circle, making it more useful for 3D images of internal damage or tumors.</p><h2><strong>How Each Technique Benefits From AI</strong></h2><p>If health care experts have used imaging techniques since before AI existed, why would they benefit from it? There are a few key ways AI in medical imaging and diagnostics could improve each technique for providers and patients.</p><h3><strong>Algorithms Can Process Images Faster</strong></h3><p>When doctors order penetrative imaging, they wait for specialized hospital staff to conduct the process and return the pictures. The physician needs time to sit and analyze the image, which may not happen immediately if they have a busy schedule. Given that the U.S. will have a health care worker shortage , the high work demand on doctors is a long-term challenge.</p><p>\\\nYou don’t need to worry about not getting medical imaging results back quickly if your local hospital uses machine-learning algorithms to process patient results. AI can review results , providing recommendations for doctors when they have time to check the scans. A health care provider will still review your results, but they could get back to you faster with computerized assistance.</p><h3>AI May Catch Small Details Better</h3><p>Artificial intelligence in medical imaging can do much more than summarize scans. The algorithms can also delineate structures in the human body. Advanced programs , tumors, and blood vessels and catch abnormalities in imaging results.</p><p>\\\nDoctors might accidentally overlook similar developments if they’re too small to see with the human eye. Instead of going back for additional imaging in a few weeks or months, you could get earlier and more precise results with AI-supported medical imaging techniques.</p><p>\\\nRemember — your doctor will still have the final say over your diagnosis. AI provides additional insights, but you’ll always need to talk with your health care provider before moving forward with treatments.</p><h3>Predictive Analytics May Provide Personalized Treatment Suggestions</h3><p>Personalized treatment plans start with diagnostic tools like medical imaging before combining your results with your health history. AI may assist with that process in more hospitals over time. Advanced algorithms can  while checking your scan results. Afterward, your doctor will review its recommended treatment options and use that data to start conversations with you.</p><p>\\\nTogether, you’ll decide the next best steps. AI won’t determine how you get medical care. Machine learning programs are an evergrowing tool that could make those conversations more straightforward by supplying potentially more accurate data.</p><p>Some surgeries require medical imagery techniques for real-time data. Machine learning  in specialties like cardiothoracic, ophthalmology and general surgery. When health care professionals use it in real time, the algorithms can review images guiding surgeons through a procedure and assist in augmented visualization. Decision support may also be helpful for instant recommendations based on the patient’s collective medical history, like the likelihood of complications.</p><h2>Does AI Medical Imaging Guarantee Increased Efficiency?</h2><p>There’s no guarantee that AI medical imaging will increase the efficiency of real-time surgical decisions, diagnoses or treatment plans. Algorithms are only as good as their programming. Your doctor will always need to consider the data provided by any machine learning program before using their comprehensive medical knowledge and understanding of your needs to give an informed opinion.</p><h2>Potential Challenges Hospitals May Face</h2><p>As more hospitals start using AI programs to read results like medical scans, it’s important to understand the barriers that may prevent implementation. Keep those factors in mind if you’re hoping to see AI-assisted medical services where you live.</p><h3>Data Storage and Protection</h3><p>Everything processed and produced by artificial intelligence in medical imaging consists of sensitive data. Hospitals need robust storage capabilities to utilize AI-assisted medical imaging and keep patient information safe. Experts estimate data breaches  in 2024 and may get worse in years to come.</p><p>\\\nHospital administrative teams may need to partner with information technology experts internally or externally to set up data storage methods with extra protection. The process may delay AI implementation, depending on where each facility’s current cybersecurity measures stand.</p><h3>Accurate Interpretability</h3><p>Medical imaging technology with AI programs is relatively new. Hospital staff will likely need some form of training before they can use it with patients. Human error could cause less accurate interpretability as medical providers get used to reading and considering AI results while making recommendations.</p><h3>Integrating AI With Current Workflow Systems</h3><p>Adding a software system to a new service is much less complicated than integrating it into something people are already doing. Hospitals are serving patients around the clock, so adding AI to their medical imaging processes without compromising patient care or service speed is challenging.</p><p>\\\nResearchers also point out that health care systems  due to things like mismatching software programs or outdated technology. Analyzing, collecting and storing imaging data may require other updates before it can happen smoothly. Communicating AI information across departments or between hospitals may create additional struggles for health care providers.</p><h2>Anticipate More AI in Medical Imaging and Diagnostics</h2><p>Health care professionals are interested in artificial intelligence in medical imaging because it could help them work faster, provide personalized results and give more accurate treatment plans. It can also require complex integration planning before patients benefit from it. Staying up to date on those factors will keep you in the know with the ever-changing health care industry.</p>","contentLength":7965,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ikey Doherty's Serpent OS Rebranding As AerynOS","url":"https://www.phoronix.com/news/Serpent-OS-To-AerynOS","date":1739595600,"author":"Michael Larabel","guid":364,"unread":true,"content":"<article>The nearly three year old Serpent OS Linux distribution started by Ikey Doherty of Solus fame is going to re-brand as AerynOS...</article>","contentLength":128,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Phenomenology of Dark Matter Explained","url":"https://hackernoon.com/the-phenomenology-of-dark-matter-explained?source=rss","date":1739591096,"author":"Phenomenology Technology","guid":109,"unread":true,"content":"<h2>3.4 Phenomenology of dark matter</h2><p>\\\n• The relic density coming from Planck satellite data [49]</p><p>\\\nThe total relic abundance of DM in our model is given by the sum of the scalar (𝜒) and fermion (𝑁3) relic abundances:</p><p>\\\nOnly for solutions falling exactly within the band given in Eq. (3.31) the totality of the DM can be explained by 𝜒 and 𝑁3.</p><p>\\\n• Direct detection cross-section of DM scattering of nucleon set by various experiments such as XENON1T [66], LUX [65] and PandaX-II [174]</p><p>\\\nWe implemented the model in the SARAH package [175] to calculate all the vertices, mass matrices, tadpole equations etc. The thermal cross sections and DM relic abundance are determined using micrOMEGAS-5.0.8 [176]. Even though the model introduces new free parameters, not all of them are important to DM analysis. For example, self-quartic coupling 𝜆𝜒 does not play any role in DM phenomenology. Hence we choose to fix 𝜆𝜒 = 0.1 in our analysis. The remaining free parameters relevant for DM analysis can be chosen as:</p><p>\\\nIn the next sections, we will study how the DM phenomenology of this model depends on the free parameters and to do that we choose the following benchmark points which are allowed from all the above-mentioned constraints:</p><p>\\\nwhich we can be utilized to compute the relic density of both the components,</p><p>The direct detection study of our DM candidates 𝜒𝑅 and 𝑁3 are done here. The current experimental constraints on the DM direct detection assume the existence of only one DM candidate. As in our model, two-component DM candidates are predicted, and the contribution of each candidate to the direct detection cross-section should be rescaled by the fraction contributing to the total relic density. Hence it is convenient to define the fraction of the mass density of 𝑖th DM in the case of multi-component DM [156,157,178,179]</p><p>\\\nThe upper limit on the direct detection now can be recast as</p><p>\\\nThe above formula in Eq. (3.40) is an extension of the expression corresponding to the singlet scalar DM case [180]. The relative negative sign between the ℎ1 and ℎ2 contributions arises in our considered model as the couplings get modified according to Eq. (3.42). Due to the presence of the two different channels, depending on the parameter space, we can have destructive interference between these two channels, and direct detection can be very small.</p><p>(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.</p>","contentLength":2451,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No Personal Liability For DOGE Yet, But With Two More Lawsuits We Get Closer","url":"https://www.techdirt.com/2025/02/14/no-personal-liability-for-doge-yet-but-with-two-more-lawsuits-we-get-closer/","date":1739590740,"author":"Cathy Gellis","guid":309,"unread":true,"content":"<p>I’m going to keep <a href=\"https://www.techdirt.com/2025/02/12/doge-may-now-be-aware-of-the-cfaa-but-its-still-violating-it-along-with-lots-of-other-laws/\">pounding the drum for personal liability</a> against Musk and DOGE, partly to scare them into backing off from their unlawful seizure of our government, and eventually to compensate us for the immense harm they’ve caused. So far it doesn’t seem like anyone has tried to personally sue them for damages, but several lawsuits are taking what might be a predicate step to establish the lawlessness of their claimed power, upon which liability claims would later be based. In addition to the AFGE litigation against OPM we already <a href=\"https://www.techdirt.com/2025/02/13/at-last-doge-and-musk-are-finally-named-in-a-lawsuit-albeit-officially/\">wrote about</a>, which also names OPM itself for it wrongfully giving DOGE access to its systems, and the <a href=\"https://www.techdirt.com/2025/02/10/court-blocks-doges-treasury-access-adding-credence-to-cfaa-questions/\">states’ lawsuit against the Treasury department</a> for giving DOGE access to theirs, now we have (at this writing at least) two more lawsuits. But while those lawsuits were directed at specific agencies and the wrongfulness of Musk and DOGE’s misuse of power at these agencies, these new lawsuits come gunning for Musk and DOGE and their illegal seizure of power generally.</p><p>They both base this argument on the Appointments Clause of the Constitution, but we’ll use the states’ <a href=\"https://www.courtlistener.com/docket/69638651/2/state-of-new-mexico-v-musk/\">complaint</a> to illustrate how. As it sets forth:</p><blockquote><p><em>The Founders of this country fought for independence from the British monarchy due in no small part to the King’s despotic power to create an unlimited number of governmental offices and to fill those offices with the King’s supporters. In fact, this practice so severely undermined the Founders’ freedoms that it is a listed grievance in the Declaration of Independence. Informed by that history, the Framers of the Constitution crafted the Appointments Clause to protect against such tyranny in our system of government. The Appointments Clause was designed to buttress the separation of powers in two ways: first by requiring that Congress create an office before the President can fill it, and second by requiring that the Senate confirm a nominee to an office created by law. These limitations on the President’s power make executive appointments accountable to Congress and make the Senate’s confirmation decisions accountable to the people. See United States v. Arthrex, 594 U.S. 1, 12 (2021). In this way, the Appointments Clause serves a vital role in curbing Executive abuses of power.</em></p></blockquote><p>Yet here we have Musk wielding a shocking amount of power, the complaint continues:</p><blockquote><p><em>Mr. Musk’s seemingly limitless and unchecked power to strip the government of its workforce and eliminate entire departments with the stroke of a pen or click of a mouse would have been shocking to those who won this country’s independence. There is no office of the United States, other than the President, with the full power of the Executive Branch, and the sweeping authority now vested in a single unelected and unconfirmed individual is antithetical to the nation’s entire constitutional structure.</em></p></blockquote><p>We have an Appointments Clause for this very reason, the complaint reminds, “because it prevents one branch from “aggrandizing its power” or “dispensing it too freely . . . to inappropriate members of the Executive Branch.” It explains that there are three types of personnel that can work for the Executive Branch, “Principle Officers,” “Inferior Officers,” and employees. The last category doesn’t require Senate confirmation, but it also isn’t endowed with the sort of executive power that Musk has been claiming. The other categories are, which is why they require nominations by the President and Senate approval, unless Congress has already passed a law foregoing that process. But Congress can only do that for inferior officers, it has not done so here, and in any case Musk is acting more like a Principle Officer anyway.</p><p>Furthermore, even for Principle Officers the President simply can’t make up a job with such power and appoint someone to it. And <em>even Justice Thomas agrees</em>! The complaint cites what he wrote less than a year ago in :</p><blockquote><p><em>Importantly, the Appointments Clause only grants the President the power to nominate officers to offices that Congress has already “established by Law.” U.S. Const. art. II, § 2, cl. 2. “If Congress has not reached a consensus that a particular office should exist, the Executive lacks the power to unilaterally create and then fill that office.” Trump v. United States, 603 U.S. 593, 650 (2024) (Thomas, J., concurring). “By keeping the ability to create offices out of the President’s hands, the Founders ensured that no President could unilaterally create an army of officer positions to then fill with his supporters. Instead, our Constitution leaves it in the hands of the people’s elected representatives to determine whether new executive offices should exist.” Id. at 646 (Thomas, J., concurring).</em></p></blockquote><p>Yet here we are, with Trump having done exactly what Justice Thomas said he could not, for the very reasons Justice Thomas himself said he could not.</p><p>The complaint then takes 30 pages to document Musk and DOGE’s unlawful rampage through executive branch agencies, in what is surely only scratching the surface of the full depth of how he has abused his unlawful power, and still continues to abuse it.</p><p>And so the lawsuit asks for this power to be enjoined so that Musk and DOGE are forced to stop their destruction. In fact, it’s also now asked for a temporary restraining order to get Musk and DOGE to stop what they are doing immediately:</p><blockquote><p><em>[T]he States ask the court to issue a temporary restraining order that immediately and temporarily, until such time as the Court may hear a motion for preliminary injunction, orders Mr. Musk to identify all ways in which any data obtained through unlawful agency access was used, including whether it was used to train any algorithmic models or create or obtain derivative data, orders Mr. Musk to destroy any copies or any derivative data from such unauthorized access in his or DOGE’s possession, custody, or control, and bars Mr. Musk and personnel associated with DOGE from:(a) ordering any change in the disbursement of public funds by agencies;<p>(b) extending offers on behalf of the United States that would bind the government to an</p>appropriation that has not been authorized by law;<p>(c) cancelling government contracts;</p>(d) disposing of government property;<p>(e) ordering the rescission or amendment of regulations;</p>(f) making personnel decisions for agency employees;<p>(g) taking steps to dismantle agencies created by law or otherwise asserting control over</p>such agencies, including, e.g., placing employees on administrative leave;<p>(h) accessing sensitive and confidential agency data, using agency data for other than its</p>authorized purpose;<p>(i) altering agency data systems without authorization by law and without taking all</p>appropriate protections against cybersecurity risks;<p>(j) engaging in any other conduct that violates the Appointments Clause or exceeds</p>statutory authority.</em></p></blockquote><p>But beyond that the complaint also asks for declaratory relief such that a court finally speaks to the unlawful nature of Musk’s power (as well as that of its DOGE agency, which, as the complaint explains, is also malformed if it is to claim the sort of supervisory power that it has, which is a power that can only be endowed by Congress):</p><blockquote><p><em>DOGE has purported to exercise authority of its own, and not merely to have acted as an adviser to the President. “Administrative agencies are creatures of statute. They accordingly possess only the authority that Congress has provided.” Nat’l Fed’n of Indep. Bus. v. Dep’t of Lab., Occupational Safety &amp; Health Admin., 595 U.S. 109, 117 (2022) (per curiam). Congress has not provided any authority to DOGE. The Constitution does not provide any authority to DOGE. The temporary organization statute, 5 U.S.C. § 3161, [that Trump claimed in his Executive Order was empowering DOGE] does not provide DOGE with the authority it has purported to exercise. That statute provides that a “temporary organization” is defined as an organization “established by law or Executive order for a specific period not in excess of three years for the purpose of performing a specific study or other project.” 5 U.S.C. § 3161(a)(1) (emphasis added). There is no plausible definition of “project” that would include DOGE’s attempt to remake the entire Executive Branch, as described above, or to destroy agencies, fire personnel, halt funding, or dispose of government property.</em></p></blockquote><p>In asking for declaratory relief a few things are accomplished. For one thing, it gives those whom Musk and DOGE are bossing around the ability to say no. In fact, if gives them the obligation to say no, because what they are being asked to do would be an unlawful order and thus unlawful for them to do. (Of course, the injunction/TRO would also restrict Musk and DOGE from even making such demands.)</p><p>But it also inches us forward to the real prize here: holding everyone involved with DOGE personally liable for the harm they have caused. By establishing that what they have done has been unlawful it provides the predicate basis for potentially all sorts of forms of liability, including the <a href=\"https://www.techdirt.com/2025/02/04/when-its-not-just-a-coup-but-a-cfaa-violation-too/\">CFAA</a>, which the USAID workers suit also provides more evidence of liability for, including in its allegations that . See page 6-7 of the <a href=\"https://www.courtlistener.com/docket/69636722/4/does-1-26-v-musk/\">complaint</a>:*</p><blockquote><p><em>J. Doe 2 understands that the DOGE personnel had administrative privileges into all the USAID systems and tools and that DOGE personnel took information out of the agency and sent it elsewhere. DOGE’s actions have caused J. Doe 2 emotional injury, as J. Doe 2 is aware of the extent of confidential information that has been breached and the privacy laws broken.</em></p></blockquote><p>And the declaratory judgment would help overcome another legal issue: whether anyone associated with DOGE would be entitled to any sort of governmental immunity for the harm they’ve caused. This will be an issue to analyze further, because we have, and would normally want to have, some immunity shielding government officials from liability for doing their jobs, if we are going to leave them sufficiently free to do their jobs. But here no one in DOGE actually had a job that would have entitled them to do what they have done. Which is what these 14 states are asking a court to finally and declaratively say.</p>","contentLength":10237,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Brake Pad Dust Can Be More Toxic Than Exhaust Emissions, Study Says","url":"https://tech.slashdot.org/story/25/02/15/0016236/brake-pad-dust-can-be-more-toxic-than-exhaust-emissions-study-says?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739590200,"author":"BeauHD","guid":180,"unread":true,"content":"Bruce66423 shares a report from The Guardian: Microscopic particles emitted from brake pads can be more toxic than those emitted in diesel vehicle exhaust, a study has found. This research shows that even with a move to electric vehicles, pollution from cars may not be able to be eradicated. The researchers found that a higher concentration of copper in some commonly used brake pads was associated with increased harmful effects on sensitive cells from people's lungs, as a result of particles being breathed in.\n \nExposure to pollution generated by cars, vans and lorries has been previously been linked to an increased risk of lung and heart disease. While past attention has mainly concentrated on exhaust emissions, particles are also released into the air from tyre, road and brake pad wear. These emissions are largely unregulated by legislation and the study found that these âoenon-exhaustâ pollution sources are now responsible for the majority of vehicle particulate matter emissions in the UK and parts of Europe, with brake dust the main contributor among them.\n \n[...] The scientists examined the effects on lung health of particulate matter from four different types of brake pad with differing chemical compositions; low metallic, semi-metallic, non-asbestos organic and hybrid-ceramic. Results showed that of the four types of brake pads, non-asbestos organic pads were the most potent at inducing inflammation and other markers of toxicity, and were found to be more toxic to human lung cells than diesel exhaust particles. Ceramic pads were the second most toxic. Dr. Ian Mudway, senior lecturer at the school of public health at Imperial College London, cautioned that while the research on brake pad emissions appears sound, it is premature to conclude they are worse than diesel exhaust due to \"uncontrolled variables\" like brake disc types and particle composition.\n \nSlashdot reader Bruce66423 also notes it \"doesn't discuss the significance of regenerative breaking, which is a feature of at least some electric cars [that reduces brake pad wear by using the electric motor to slow down the vehicle and recover energy].\"\n \nThe research has been published in the journal Particle and Fibre Technology.","contentLength":2229,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Theoretical and Experimental Constraints: Discussing Different Constraints on the Model Parameters","url":"https://hackernoon.com/theoretical-and-experimental-constraints-discussing-different-constraints-on-the-model-parameters?source=rss","date":1739588801,"author":"Phenomenology Technology","guid":108,"unread":true,"content":"<h2>3.3 Theoretical and experimental constraints</h2><p>We discuss different constraints on the model parameters such as𝑈(1)𝑋 gauge coupling and scalar mixing angle. To estimate the constraints we consider vacuum stability, perturbative unitarity, and collider searches of BSM Higgs and 𝑍′ boson respectively.</p><p>The above scalar potential must be bounded from below. To determine the conditions for 𝑉(𝐻, Φ, 𝜒) to be bounded from below, we need to check the following symmetric matrix which comes from the quadratic part of the potential,</p><p>\\\nRequiring such a matrix to be positive-definite, we obtain the following conditions,</p><h3>3.3.2 Higgs Invisible decay</h3><p>\\\nHence the total invisible decay width of SM Higgs boson ℎ1 is given a</p><p>\\\nAccordingly, the invisible branching ratio for ℎ1 is given b</p><h3>3.3.4 Bounds on the mixing parameter between physical mass eigenstates</h3><p>(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.</p>","contentLength":928,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Phenomenological Study of WIMP Models: Scalar Sector, Gauge Sector, and More","url":"https://hackernoon.com/a-phenomenological-study-of-wimp-models-scalar-sector-gauge-sector-and-more?source=rss","date":1739588422,"author":"Phenomenology Technology","guid":107,"unread":true,"content":"<p>\\\nIn the next subsections, we discuss various parts of the lagrangian of the model,</p><p>\\\nwhere 𝛼 is the mixing angle. The rotation matrix satisfies</p><p>\\\nThe real and imaginary components of 𝜒 have the following masses</p><p>To determine the gauge boson spectrum, we have to expand the scalar kinetic terms and replace</p><p>The Yukawa sector of the model can be written in a gauge-invariant way as</p><p>(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.</p>","contentLength":446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Studying a Two-Component Dark Matter Model: An Introduction","url":"https://hackernoon.com/studying-a-two-component-dark-matter-model-an-introduction?source=rss","date":1739588136,"author":"Phenomenology Technology","guid":106,"unread":true,"content":"<p>In this chapter, we study a two-component DM model interacting with SM via Higgs and Z portals. The results are based on the work: Arindam Das, Shivam Gola, Sanjoy Mondal, Nita Sinha, \"Two-Components Scalar and Fermionic Dark Matter candidates in a generic U(1)𝑋 model, Phys.Lett.B 2022.137117”.</p><p>Underpinning the origin of neutrino mass and elucidating the nature of DM would constitute a major step forward in particle physics. Several simple extensions of the SM that can account for the DM have already been studied [136–143]. In these models, the SM particle content is extended by additional fields, and a discrete symmetry is usually introduced to guarantee the stability of the DM particle in cosmological scale. In recent years, a class of models has been proposed to incorporate the neutrino mass generation and the existence of DM in a unified framework. Motivated by this, people have studied well-motivated BSM framework based on the gauged 𝑈(1)𝑋 model [144–146]. The most intriguing aspect of this model is that including three generations of right-handed neutrinos, as in the type-I seesaw process for creating light neutrino masses, is no longer an option, but emerges as the simplest solution to eliminate the gauge and mixed gauge-gravity anomalies [147]. The scalar DM can be inherently stable in such models due to its 𝑈(1)𝑋 charge, but the fermionic DM cannot be realized in the simplest 𝑈(1)𝑋 model. Additional discrete symmetries can be introduced, which can stabilize one of the right-handed neutrinos to play the role of DM, while the other two neutrinos participate in the type I seesaw process to generate the required light neutrino masses and flavor mixing. Also, there are many models proposed in the literature, where neutrino mass generation is intimately connected with DM [148–153]. In these types of models, DM is a mediator of neutrino mass generation.</p><p>\\\nA single-particle DM model may not be sufficient to account for the relic density of DM observed in the universe. Many such models face strong constraints from direct detection experiments and other observations. Therefore, it is reasonable to consider multi-particle DM scenarios, where two or more particles contribute to the DM abundance. [24,62,126]. Multi-component DM refers to a situation in which two or more particles contribute to the measured DM density. This has been already studied in many BSM scenarios [31, 56, 57, 154–166]. The multi-component DM model has also some benefits over the single dark matter scenario. For instance, it can avoid some stringent constraints arising from various experiments that probe the properties and interactions of dark matter and other particles. A multi-component DM model can also accommodate different observational features of dark matter, such as its distribution and abundance in the universe.</p><p>(1) Shivam Gola, The Institute of Mathematical Sciences, Chennai.</p>","contentLength":2933,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI and the Future of Work: Transforming Industries","url":"https://hackernoon.com/ai-and-the-future-of-work-transforming-industries?source=rss","date":1739585602,"author":"Ombir Sharma","guid":105,"unread":true,"content":"<p>AI or artificial intelligence is the capability and ability of a machine to imitate human intelligence. Intelligent systems are capable of learning, reasoning, and self-correcting. AI includes a number of technologies such as machine learning, natural language processing, robotics, and computer vision. It is able to perform a wide array of functions from simple repetitive work to making complex decisions which changes the very fabric of industries.</p><h2>The Rise of Robotics and AI</h2><p>The combination of <a href=\"https://hackernoon.com/artificial-intelligence-and-robotics-whos-at-fault-when-robots-kill-a01u3ear\">AI and robotics</a> is changing the world at an unprecedented rate; human errors are greatly reduced and proficiency is greatly improved across multiple industries. Robots powered by AI can be programmed to accomplish tasks that previously only humans could perform. From AI-trained chatbots at customer service to assembly lines in factories, this newly developed technology changes the workplace alongside societal norms.</p><h3>Effect of Artificial Intelligence on Employment</h3><p>AI is a double-edged sword with its impact on employment. The technology has created several new opportunities and increased overall productivity in numerous ways, however, it has also posed great threats to certain jobs and industries which can lead to displacement of employees.</p><p>An important shift is on the horizon as the job market is about to undergo a role reversal change. As AI technology evolves, it is taking over the more labor-intensive tasks and as a result, human workers are becoming more strategic and analytic. AI alleviates repetitive tasks and allows employees to focus on creative problem-solving.</p><p>The new approach AI takes centers around<a href=\"https://www.intelligenceinsights.net/artificial-intelligence/autogpt-the-future-of-autonomous-ai-assistants/\"></a>, just like the Industrial Revolution. While focusing on increasing productivity, ChatGPT and AutoGPT have the potential to perform tasks such as data entry, scheduling, and even more complex problem-solving, increasing overall demand for customer service representatives and support agents. In every age, these shifts in productivity have led to increased job opportunities, just like the multiple industries that formed alongside the technology integrations.</p><h3>Facilitation of Employment</h3><p>The introduction of ChatGPT can aid everyday business operations, serving as virtual assistants that communicate primarily with customers. By streamlining business functions for flexible resource allocation, productivity is sure to increase. Efficient use of ChatGPT makes it incredibly simple for businesses to automate their customer inquiries, increasing profit margins.</p><p>ChatGPT encourages an environmentally friendly approach by seamlessly optimizing supply chains with effective waste reduction strategies. AI, without a doubt, would optimize ChatGPT-provided decision-making to improve overall energy consumption and ensure economic goals are catered towards too.</p><p>In the absence of people, ChatGPT opens doors to new interactions that drastically improve service and business operations through automation. Communication via virtual assistants and chatbots allows for effortless collaboration and automated customer interactions on a new level.</p><p>\\\nIn every way possible, AI improves business decision-making by offering adequate utility and data for correct allocation. Business AI helps set prices, predict market movement, and analyze how much risk varies. Keeping track of tools like ChatGPT can aid project-based decisions by providing up-to-date information.</p><h3>Elimination of Junior and Mid-level Staff Positions</h3><p>As a result of automation, junior and mid-level roles, particularly in data entry and administrative duties and tasks, are being skinned out. More companies seem to be using AI, which, in turn, is leading to an increased need for highly specialized and senior positions that require a supervisor to think strategically.</p><p>Across different industries, AI is changing the future of work. From performing simple services to making complex decisions, AI has a broad range of applications, and as time goes by, the list only seems to grow.</p><h2>The Use Of AI In Business</h2><p>Artificial intelligence is being adopted in all sectors to boost productivity, accuracy, and satisfaction of customers. It is used in recruitment HR, in<a href=\"https://www.thenexthint.com/a-new-era-in-customer-service-with-automation/\"></a> through chatbots, and in marketing through brand campaigns. Work processes are now being automated with AI, thereby, minimizing the costs of running the business.</p><h3>Impact Of AI On Business Operations</h3><p>Software developments with regard to Artificial intelligence utilizing machine learning and automation present the future of the workplace, resulting in increased productivity. Businesses will continuously adopt AI to better their decisions, improve work processes, and promote creativity. Because of this, more skills in AI will be needed which will translate to new positions being created.</p><p>In the field of finance, AI handles fraud detection, automates trading, and enhances risk assessments. Robo-advisors make investment suggestions based on a client’s unique profile, and AI chatbots provide further engagement with customers.</p><p>AI assists in diagnosing diseases and personalizes treatments for patients in healthcare. With the help of <a href=\"https://hackernoon.com/the-future-of-robotics-development-with-eva-li-of-vincross-dd8d85256d4e\">AI-led robots</a>, humans can make fewer mistakes while performing surgeries, which helps enhance patient care.</p><p>In the automotive sector, AI enables self-driving cars, predictive maintenance, and smart traffic control. AI technologies also help improve vehicle safety and fuel efficiency.</p><p><a href=\"https://hackernoon.com/how-the-us-government-plans-to-ensure-the-safety-and-security-of-ai-technology\">Cybersecurity</a> is enhanced by AI technologies that assess new threats, detect anomalies, and provide assistance during the occurrence of cyber-attacks. Data security is also improved and business risks are suppressed by AI technologies.</p><p>In the e-commerce sector, AI improves customer experiences, automates supply chains, and increases the level of service. Recommendation systems based on users' behavior increase the total sales and satisfaction level of customers.</p><p>We see the growth of AI in hospitality in chatbots, smart booking systems, and customer experience applications. AI analytics also help with effective guest price settings and greater guest satisfaction.</p><p>AI greatly assists in providing relevant lists of jobs that best suit a candidate's skills. AI technology helps to read a CV, understand the candidate's level of skills, and determine his success.</p><h2>Are My Working Opportunities On The Line Due To AI?</h2><p>Filling positions for repetitive work is indeed scalable with AI which is why it poses a threat for employment. In reality, AI also creates new possibilities by taking up low-productivity tasks. It is expected that new jobs that require creative thinking, emotions, and strategy-based decision-making will still exist. To ensure that they remain employed, workers must learn new skills and adjust to the new reality.</p><p>AI is enabling the future of work by performing tasks that require time and effort, growing productivity, and changing the existing categorization of industries. Some roles may be lost, but there are always new positions that have to be filled. Being able to utilize AI together with other novel technologies is crucial to be successful for business professionals in the new world.</p>","contentLength":7063,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"'Please Stop Inviting AI Notetakers To Meetings'","url":"https://slashdot.org/story/25/02/15/006253/please-stop-inviting-ai-notetakers-to-meetings?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739584920,"author":"BeauHD","guid":179,"unread":true,"content":"Most virtual meeting platforms these days include AI-powered notetaking tools or bots that join meetings as guests, transcribe discussions, and/or summarize key points. \"The tech companies behind them might frame it as a step forward in efficiency, but the technology raises troubling questions around etiquette and privacy and risks undercutting the very communication it's meant to improve (paywalled; alternative source),\" writes Chris Stokel-Walker in a Weekend Essay for Bloomberg. From the article: [...] The push to document every workplace interaction and utterance is not new. Having a paper trail has long been seen as a useful thing, and a record of decisions and action points is arguably what makes a meeting meaningful. The difference now is the inclusion of new technology that lacks the nuance and depth of understanding inherent to human interaction in a meeting room. In some ways, the prior generation of communication tools, such as instant messaging service Slack, created its own set of problems. Messaging that previously passed in private via email became much more transparent, creating a minefield where one wrong word or badly chosen emoji can explode into a dispute between colleagues. There is a similar risk with notetaking tools. Each utterance documented and analyzed by AI includes the potential for missteps and misunderstandings.\n \nAnyone thinking of bringing an AI notetaker to a meeting must consider how other attendees will respond, says Andrew Brodsky, assistant professor of management at the McCombs School of Business, part of the University of Texas at Austin. Colleagues might think you want to better focus on what is said without missing out on a definitive record of the discussion. Or they might think, \"You can't be bothered to take notes yourself or remember what was being talked about,\" he says. For the companies that sell these AI interlopers, the upside is clear. They recognize we're easily nudged into different behaviors and can quickly become reliant on tools that we survived without for years. [...] There's another benefit for tech companies getting us hooked on AI notetakers: Training data for AI systems is increasingly hard to come by. Research group Epoch AI forecasts there will be a drought of usable text possibly by next year. And with publishers unleashing lawsuits against AI companies for hoovering up their content, the tech firms are on the hunt for other sources of data. Notes from millions of meetings around the world could be an ideal option.\n \nFor those of us who are the source of such data, however, the situation is more nuanced. The key question is whether AI notetakers make office meetings more useless than so many already are. There's an argument that meetings are an important excuse for workers to come together and talk as human beings. All that small talk is where good ideas often germinate -- that's ostensibly why so many companies are demanding staff return to the office. But if workers trade in-person engagement for AI readbacks, and colleagues curb their words and ideas for fear of being exposed by bots, what's left? If the humans step back, all that remains is a series of data points and more AI slop polluting our lives.","contentLength":3228,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uber sues DoorDash, alleging anti-competitive tactics","url":"https://techcrunch.com/2025/02/14/uber-sues-doordash-alleging-anti-competitive-tactics/","date":1739583337,"author":"Kirsten Korosec, Maxwell Zeff","guid":47,"unread":true,"content":"<p>Ride-share giant Uber filed a lawsuit Friday against DoorDash, accusing the delivery outfit of stifling competition by intimidating restaurant owners into exclusive deals. Uber alleges in the lawsuit, filed in Superior Court of California, that its chief rival bullied restaurants into only working with DoorDash. Uber claims that DoorDash, which holds the largest share of […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":442,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NYC Is Giving Free E-Bikes To Delivery Workers Using Unsafe Models","url":"https://hardware.slashdot.org/story/25/02/14/2336232/nyc-is-giving-free-e-bikes-to-delivery-workers-using-unsafe-models?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739582700,"author":"BeauHD","guid":178,"unread":true,"content":"New York City's Department of Transportation is offering delivery workers the opportunity to swap out uncertified e-bikes for safer UL-compliant models. \"Millions of people rely on such workers for timely deliveries, yet the low wages and brutal conditions of the job have forced many riders to seek out low-cost electric bicycles to perform the work -- exactly the kind of e-bikes that are least likely to have received safety certifications,\" reports Electrek. From the report: The NYC DOT has already begun accepting applications for the new E-Bike Trade-In Program, which is open to delivery workers with non-compliant electric bicycles as well as the often-seen electric scooters/mopeds that don't really qualify as e-bikes, despite their ubiquitous use in the industry. Interestingly, the program even accepts gasoline-powered mopeds that are not able to be legally registered with the DMV, including those that lack VINs. In exchange for trading in a non-certified vehicle, the delivery worker will receive a new UL-certified electric bike with a spare UL-certified battery.\n \nThere are a few requirements for eligibility. The worker has to have earned at least US $1,500 by working in the food delivery industry last year in 2024, live in one of the five New York City boroughs, be at least 18 years old, and own/use one of the eligible devices for trade-in. The program is free to participate in with no additional cost for the delivery workers. However, the supply of free electric bicycles is described as \"limited.\" Those interested need to submit an application before the window closes on March 10, 2025.","contentLength":1618,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.24 Brings Performance Improvements, Better WebAssembly Support","url":"https://www.phoronix.com/news/Go-1.24-Released","date":1739582673,"author":"Michael Larabel","guid":363,"unread":true,"content":"<article>Go 1.24 was released this week by Google engineers as the newest step forward for this popular programming language...</article>","contentLength":118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft Study Finds Relying on AI Kills Your Critical Thinking Skills","url":"https://slashdot.org/story/25/02/14/2320203/microsoft-study-finds-relying-on-ai-kills-your-critical-thinking-skills?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739580300,"author":"BeauHD","guid":177,"unread":true,"content":"A new study (PDF) from researchers at Microsoft and Carnegie Mellon University found that increased reliance on AI tools leads to a decline in critical thinking skills. Gizmodo reports: The researchers tapped 319 knowledge workers -- a person whose job involves handling data or information -- and asked them to self-report details of how they use generative AI tools in the workplace. The participants were asked to report tasks that they were asked to do, how they used AI tools to complete them, how confident they were in the AI's ability to do the task, their ability to evaluate that output, and how confident they were in their own ability to complete the same task without any AI assistance.\n \nOver the course of the study, a pattern revealed itself: the more confident the worker was in the AI's capability to complete the task, the more often they could feel themselves letting their hands off the wheel. The participants reported a \"perceived enaction of critical thinking\" when they felt like they could rely on the AI tool, presenting the potential for over-reliance on the technology without examination. This was especially true for lower-stakes tasks, the study found, as people tended to be less critical. While it's very human to have your eyes glaze over for a simple task, the researchers warned that this could portend to concerns about \"long-term reliance and diminished independent problem-solving.\"\n \nBy contrast, when the workers had less confidence in the ability of AI to complete the assigned task, the more they found themselves engaging in their critical thinking skills. In turn, they typically reported more confidence in their ability to evaluate what the AI produced and improve upon it on their own. Another noteworthy finding of the study: users who had access to generative AI tools tended to produce \"a less diverse set of outcomes for the same task\" compared to those without.","contentLength":1915,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI’s Hallucinations Are Over","url":"https://hackernoon.com/ais-hallucinations-are-over?source=rss","date":1739579978,"author":"Oleksandr Zabashnyi","guid":104,"unread":true,"content":"<p>First of all, let me describe the problem. I am a software developer, and I don't use AI to write code just because of hallucinations. For creating pictures or writing texts they are not so critical, but for the task of writing code, they are overkill. I have identified two subproblems. Firstly, it is difficult to identify the errors made by AI. It is quite easy for a designer as he counts the fingers in the pictures and that's it; the picture is accepted as the result. </p><p>\\\nHowever, I have to deal with the code and seek the mistakes in it. I hate to do it. Secondly, it is the irreproducibility of the result. For example, I tried to write unit tests: the first generated unit test is successful, but the second one is not, even for the same method. This makes working with it impossible.</p><p>\\\nNow, about the solution. Our hypothesis was as follows: what if we could assign the corresponding result data to a certain set of input information? And do that at least sometimes during the task-solving process. Would that reduce the number of hallucinations? Oh yes! After thorough research, we have come to the conclusion that the presence of a critical mass of such ‘rigidly fixed’ nodes almost completely removed hallucinations from the output. All that remained was to find the criteria for identifying these correspondences between input and output information and to modify the math apparatus to enable the use of this approach. We called this method the \"preset landscape.\"</p><p>\\\nCertainly, our approach has several limitations. First of all, the subject area should imply the existence of only one correct answer for a given set of output data. Fortunately, software development is exactly such an area. The second limitation is that at the stage of landscape formation, the participation of an expert in the subject field is required. In other words, it is impossible to apply the math apparatus to the areas where a human cannot articulate the rules. These limitations greatly narrow the scope of application of our approach. However, its usage in such areas as software development, law, healthcare, and engineering tasks is more than enough.</p><p>\\\nTo demonstrate the capabilities of the math apparatus, we have developed <a href=\"https://drive.google.com/file/d/1mRhGzEeyhcT4EUZj4a4wAZ8TNYSvByRW/view?usp=sharing\">a plugin</a> for IntelliJ Idea (JetBrains). You can install it and make sure that there are no hallucinations. Here, you can find <a href=\"https://drive.google.com/file/d/1mRhGzEeyhcT4EUZj4a4wAZ8TNYSvByRW/view?usp=sharing\">the instructions</a>.</p><p>\\\nWhile we were working on the plugin, AI services came into vogue, which provides API and you can use them in your own solutions. Therefore, we are planning to make such a platform for software development tasks. We will most likely start with Java. If you have an insight into how this approach can be implemented for lawyers or healthcare professionals - feel free to share.</p>","contentLength":2743,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PIN AI Launches Mobile App Letting You Make Your Own Personalized, Private AI Model","url":"https://mobile.slashdot.org/story/25/02/14/2227222/pin-ai-launches-mobile-app-letting-you-make-your-own-personalized-private-ai-model?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1739577720,"author":"BeauHD","guid":176,"unread":true,"content":"An anonymous reader quotes a report from VentureBeat: A new startup PIN AI (not to be confused with the poorly reviewed hardware device the AI Pin by Humane) has emerged from stealth to launch its first mobile app, which lets a user select an underlying open-source AI model that runs directly on their smartphone (iOS/Apple iPhone and Google Android supported) and remains private and totally customized to their preferences. Built with a decentralized infrastructure that prioritizes privacy, PIN AI aims to challenge big tech's dominance over user data by ensuring that personal AI serves individuals -- not corporate interests. Founded by AI and blockchain experts from Columbia, MIT and Stanford, PIN AI is led by Davide Crapis, Ben Wu and Bill Sun, who bring deep experience in AI research, large-scale data infrastructure and blockchain security. [...]\n \nPIN AI introduces an alternative to centralized AI models that collect and monetize user data. Unlike cloud-based AI controlled by large tech firms, PIN AI's personal AI runs locally on user devices, allowing for secure, customized AI experiences without third-party surveillance. At the heart of PIN AI is a user-controlled data bank, which enables individuals to store and manage their personal information while allowing developers access to anonymized, multi-category insights -- ranging from shopping habits to investment strategies. This approach ensures that AI-powered services can benefit from high-quality contextual data without compromising user privacy.\n[...]\nThe new mobile app launched in the U.S. and multiple regions also includes key features such as: - The \"God model\" (guardian of data): Helps users track how well their AI understands them, ensuring it aligns with their preferences. - Ask PIN AI: A personalized AI assistant capable of handling tasks like financial planning, travel coordination and product recommendations. - Open-source integrations: Users can connect apps like Gmail, social media platforms and financial services to their personal AI, training it to better serve them without exposing data to third parties. - \"With our app, you have a personal AI that is your model,\" Crapis added. \"You own the weights, and it's completely private, with privacy-preserving fine-tuning.\" Davide Crapis, co-founder of PIN AI, told VentureBeat that the app currently supports several open-source AI models, including small versions of DeepSeek and Meta's Llama. \"With our app, you have a personal AI that is your model,\" Crapis added. \"You own the weights, and it's completely private, with privacy-preserving fine-tuning.\"\n \nYou can sign up for early access to the PIN AI app here.","contentLength":2669,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ctrl-Alt-Speech: Backdoors And Backsteps","url":"https://www.techdirt.com/2025/02/14/ctrl-alt-speech-backdoors-and-backsteps/","date":1739577663,"author":"Mike Masnick","guid":308,"unread":true,"content":"<p>In this week’s round-up of the latest news in online speech, content moderation and internet regulation, Mike and Ben are joined by a group of students from the Media Law and Policy class at the American University School of Communication. Together they cover:</p><p>This episode is brought to you with financial support from the Future of Online Trust &amp; Safety Fund.</p>","contentLength":362,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Court filings show Meta paused efforts to license books for AI training","url":"https://techcrunch.com/2025/02/14/court-filings-show-meta-paused-efforts-to-license-books-for-ai-training/","date":1739576133,"author":"Kyle Wiggers","guid":46,"unread":true,"content":"<p>New court filings in an AI copyright case against Meta add credence to earlier reports that the company “paused” discussions with book publishers on licensing deals to supply some of its generative AI models with training data. The filings are related to the case&nbsp;Kadrey v. Meta Platforms — one of many such cases winding through […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Alexa and AI Siri face bugs and delays","url":"https://techcrunch.com/2025/02/14/ai-alexa-and-ai-siri-face-bugs-and-delays/","date":1739575487,"author":"Maxwell Zeff","guid":45,"unread":true,"content":"<p>Amazon and Apple are struggling to put generative AI technology in their digital assistants — Alexa and Siri, respectively — according to a pair of reports that came out on Friday. Amazon hoped to release its new Alexa during an event in New York on February 26. Now Amazon plans to delay the release of […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":376,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pull Request Testing on Kubernetes: How to Test Locally and on GitHub Workflows","url":"https://hackernoon.com/pull-request-testing-on-kubernetes-how-to-test-locally-and-on-github-workflows?source=rss","date":1739574357,"author":"Nicolas Fränkel","guid":103,"unread":true,"content":"<p>Imagine an organization with the following practices:</p><ul><li>Runs its CI/CD pipelines with GitHub Actions</li><li>Runs its production workload on Kubernetes</li></ul><p>\\\nA new engineer manager arrives and asks for the following:</p><blockquote><p>On every PR, run integration tests in a Kubernetes cluster similar to the production one.</p></blockquote><p>\\\nIn this series of posts, I'll show how you can do it. My plan is the following:</p><ul><li>This blog post focuses on the app, the basic GitHub workflow setup, and testing both locally and during the workflow run.</li><li>The second blog post will detail the setup of a Google Kubernetes Engine instance and how to adapt the workflow to use it.</li></ul><h2>Unit Testing vs. Integration Testing</h2><blockquote><p>Integration Testing is a strategy to test the collaboration of at least two components.</p></blockquote><p>\\\nI translated it in Object-Oriented Programming as:</p><blockquote><p>Integration Testing is a strategy to test the collaboration of at least two classes.</p></blockquote><blockquote><p>Let’s consider the making of a car. Single-class testing is akin to testing each nut and bolt separately. Imagine testing of such components brought no issue to light. Still, it would be very risky to mass manufacture the car without having built a prototype and sent it to a test drive.</p></blockquote><p>\\\nHowever, technology has evolved since that time.</p><p>I use the word \"technology\" very generally, but I have <a href=\"https://testcontainers.com/\">Testcontainers</a> in mind:</p><blockquote><p><em>Unit tests with real dependencies</em></p><p>\\\n  Testcontainers is an open source library for providing throwaway, lightweight instances of databases, message brokers, web browsers, or just about anything that can run in a Docker container.</p></blockquote><p>\\\nIn effect, Testcontainers replaces  with \"real\" dependencies-containerized. It's a real game-changer: instead of painfully writing mocking code to stub dependencies just set them up regularly.</p><p>\\\nFor example, without Testcontainers, you'd need to provide mocks for your  in tests; with it, you only need to start a database container, and off you go.</p><p>\\\nAt the time, the cost of having a local Docker daemon in your testing environment offset many benefits. It's not the case anymore, as Docker daemons are available (nearly) everywhere.</p><p>\\\nMy definition of Integration Testing has changed a bit:</p><blockquote><p>Integration Testing is testing that requires significant setup.</p></blockquote><p>\\\nThe definition is vague on purpose, as significance has a different meaning depending on the organization, the team, and the individual. Note that Google defines two categories of tests: fast and slow. Their definition is equally vague, meant to adapt to different contexts.</p><p>In any case, the golden rule still applies: the closer you are to the final environment, the more risks you cover, and the more valuable your tests are. </p><p>\\\nIf our target production environment is Kubernetes, we will reap the most benefits from running the app on Kubernetes and testing it as a black box. It doesn't mean that white box testing in a more distant environment is not beneficial; it means that the more significant the gap between the testing environment and the target environment, the fewer issues we will uncover.</p><p>\\\nFor the purposes of this blog post, we will use GitHub as the base testing environment for unit testing and a full-fledged Kubernetes cluster for integration testing. There is no absolute truth regarding what is the best practice™, as contexts vary widely across organizations and even across teams within the same organization. It's up to every engineer to decide within their specific context the ROI of setting up such an environment because the closer you are to production, the more complex and, thus, expensive it will be.</p><h2>Use-case: Application With Database</h2><p>Let's jump into how to test an app that uses a database to store its data. I don't want anything fancy, just solid, standard engineering practices. I'll be using a CRUD(Create Read Update Delete) JVM-based app, but most of the following can easily apply to other stacks as well. The following blog posts will involve less language-specific content.</p><ul><li>Kotlin, because I love the language</li><li>Spring Boot: it's the most widespread framework for JVM-based applications</li><li>Maven-there's nothing else</li><li>Project Reactor and coroutines, because it makes things more interesting</li><li>PostgreSQL-at the moment, it's a very popular database, and it's well-supported by Spring</li></ul><p>\\\nIf you don't know Flyway, it allows you to track database schemas and data in a code repository and manage changes, known as migrations, between versions. Each migration has a unique version, , v1.0, v1.1, v2.1.2, etc. Flyway tries to apply migration in order. If it has already applied a migration, it skips it. Flyway stores its data in a dedicated table to track the applied migrations.</p><p>\\\nThis approach is a must-have; <a href=\"https://www.liquibase.com/\">Liquibase</a> is an alternative that follows the same principles.</p><p>\\\nSpring Boot fully integrates Flyway and Liquibase. When the app starts, the framework will kickstart them. If a pod is killed and restarted, Flyway will first check the migrations table to apply only the one that didn't run previously.</p><p>\\\nI don't want to bore you with the app details; you can find the code on <a href=\"https://github.com/ajavageek/vcluster-pipeline\">GitHub</a>.</p><p>Per my definition above, unit testing should be easy to set up. With Testcontainers, it is.</p><p>\\\nThe testing code counts the number of items in a table, inserts a new item, and counts the number of items again. It then checks that:</p><ul><li>There's one additional item compared to the initial count</li><li>That the new item is the one we inserted</li></ul><pre><code>@SpringBootTest                                                              //1\nclass VClusterPipelineTest @Autowired constructor(private val repository: ProductRepository) { //2\n\n    @Test\n    fun `When inserting a new Product, there should be one more Product in the database and the last inserted Product should be the one inserted`() { //3\n        runBlocking {                                                        //4\n            val initialCount = repository.count()                            //5\n            // The rest of the test\n        }\n    }\n}\n</code></pre><ol><li>Initialize the Spring context</li><li>Praise Kotlin for allowing for descriptive function names</li><li>Run non-blocking code in a blocking function</li></ol><p>\\\nWe now need a PostgreSQL database; Testcontainers can provide one for us. However, to avoid conflicts, it will choose a random port until it finds an unused one. We need it to connect to the database, run the Flyway migration, and run the testing code.</p><p>\\\nFor this reason, we must write a bit of additional code:</p><pre><code>@Profile(\"local\")                                                              //1\nclass TestContainerConfig {\n\n    companion object {\n        val name = \"test\"\n        val userName = \"test\"\n        val pass = \"test\"\n        val postgres = PostgreSQLContainer&lt;Nothing&gt;(\"postgres:17.2\").apply {   //1\n            withDatabaseName(name)\n            withUsername(userName)\n            withPassword(pass)\n            start()\n        }\n    }\n}\n\nclass TestContainerInitializer : ApplicationContextInitializer&lt;ConfigurableApplicationContext&gt; {\n    override fun initialize(applicationContext: ConfigurableApplicationContext) {\n        if (applicationContext.environment.activeProfiles.contains(\"local\")) {\n            TestPropertyValues.of(                                             //2\n                \"spring.r2dbc.url=r2dbc:postgresql://${TestContainerConfig.postgres.host}:${TestContainerConfig.postgres.firstMappedPort}/$name\",\n                \"spring.r2dbc.username=$name\",\n                \"spring.r2dbc.password=$pass\",\n                \"spring.flyway.url=jdbc:postgresql://${TestContainerConfig.postgres.host}:${TestContainerConfig.postgres.firstMappedPort}/$name\",\n                \"spring.flyway.user=$name\",\n                \"spring.flyway.password=$pass\"\n            ).applyTo(applicationContext.environment)\n        }\n    }\n}\n</code></pre><ol><li><p>Start the container, but only if the Spring Boot profile  is active.</p></li><li><p>Override the configuration values.</p></li></ol><p>\\\nWe need to specify neither the  nor the  if we hacked the  to reuse the R2BC parameters of the same name:</p><pre><code>spring:\n  application:\n    name: vcluster-pipeline\n  r2dbc:\n    username: test\n    password: test\n    url: r2dbc:postgresql://localhost:8082/flyway-test-db\n  flyway:\n    user: ${SPRING_R2DBC_USERNAME}                                             #1\n    password: ${SPRING_R2DBC_PASSWORD}                                         #1\n    url: jdbc:postgresql://localhost:8082/flyway-test-db\n</code></pre><ol><li>Smart hack to DRY configuration further down.</li></ol><p>\\\nWe also annotate the previous test class to use the initializer:</p><pre><code>@SpringBootTest\n@ContextConfiguration(initializers = [TestContainerInitializer::class])\nclass VClusterPipelineTest @Autowired constructor(private val repository: ProductRepository) {\n\n    // No change\n}\n</code></pre><p>\\\nSpring Boot offers a couple of options to <a href=\"https://docs.spring.io/spring-boot/reference/features/external-config.html\">activate profiles</a>. For local development, we can use a simple JVM property, , <code>mvn test -Dspring.profiles.active=local</code>; in the CI pipeline, we will use environment variables instead.</p><p>I'll also use Flyway to create the database structure for integration testing. In the scope of this example, the System Under Test will be the entire app; hence, I'll test from the HTTP endpoints. It's end-to-end testing for APIs. The code will test the same behavior, albeit treating the System Under Test as a black box.</p><pre><code>class VClusterPipelineIT {\n\n    val logger = LoggerFactory.getLogger(this::class.java)\n\n    @Test\n    fun `When inserting a new Product, there should be one more Product in the database and the last inserted Product should be the one inserted`() {\n\n        val baseUrl = System.getenv(\"APP_BASE_URL\") ?: \"http://localhost:8080\" //1\n\n        logger.info(\"Using base URL: $baseUrl\")\n\n        val client = WebTestClient.bindToServer()                              //2\n            .baseUrl(baseUrl)\n            .build()\n\n        val initialResponse: EntityExchangeResult&lt;List&lt;Product?&gt;?&gt; = client.get() //3\n            .uri(\"/products\")\n            .exchange()\n            .expectStatus().isOk\n            .expectBodyList(Product::class.java)\n            .returnResult()\n\n        val initialCount = initialResponse.responseBody?.size?.toLong()        //4\n\n        val now = LocalDateTime.now()\n        val product = Product(\n            id = UUID.randomUUID(),\n            name = \"My awesome product\",\n            description = \"Really awesome product\",\n            price = 100.0,\n            createdAt = now\n        )\n\n        client.post()                                                          //5\n            .uri(\"/products\")\n            .bodyValue(product)\n            .exchange()\n            .expectStatus().isOk\n            .expectBody(Product::class.java)\n\n        client.get()                                                           //6\n            .uri(\"/products\")\n            .exchange()\n            .expectStatus().isOk\n            .expectBodyList(Product::class.java)\n            .hasSize((initialCount!! + 1).toInt())\n    }\n}\n</code></pre><ol><li>Get the deployed app URL.</li><li>Create a web client that uses the former.</li><li>Get the initial item list.</li><li>Get the size; we definitely should offer a count function if there are too many items.</li><li>Insert a new item and assert everything works out fine.</li><li>Get the list of items and assert the item count is higher by one.</li></ol><p>\\\nBefore going further, let's run the tests in a GitHub workflow.</p><p>I'll assume you're familiar with <a href=\"https://docs.github.com/en/actions/writing-workflows\">GitHub workflows</a>. If you aren't, a GitHub workflow is a declarative description of an automated job. A job consists of several steps. GitHub offers several triggers: Manual, scheduled, or depending on an event.</p><p>\\\nWe want the workflow to run on each Pull Request to verify that tests run as expected.</p><pre><code>name: Test on PR                                                               #1\n\non:\n  pull_request:\n    branches: [ \"master\" ]                                                     #2\n</code></pre><ol><li><p>Trigger on a PR to the master branch.</p></li></ol><p>\\\nThe first steps are pretty standard:</p><pre><code>jobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n      - name: Install JRE\n        uses: actions/setup-java@v4\n        with:\n          distribution: temurin\n          java-version: 21\n          cache: maven                                                         #1\n</code></pre><ol><li>The  action includes a caching option for build tools. Here, it will cache dependencies across runs, speeding up consecutive runs. Unless you have good reasons not to, I recommend using this option.</li></ol><p>\\\nFor the same reason, we should cache our built artifacts. While researching for this post, I learned that GitHub discards them across runs <strong>and steps in the same run</strong>. Hence, we can speed up the runs by caching them explicitly:</p><pre><code>      - name: Cache build artifacts\n        uses: actions/cache@v4                                                 &lt;1&gt;\n        with:\n          path: target\n          key: ${{ runner.os }}-build-${{ github.sha }}                        &lt;2&gt;\n          restore-keys:\n            ${{ runner.os }}-build                                             &lt;3&gt;\n</code></pre><ol><li><p>Use the same action that  uses under the hood.</p></li><li><p>Compute the cache key. In our case, the  should be immutable, but this should be how you run matrices across different operating systems.</p></li><li><p>Reuse the cache if it's the same OS.</p></li></ol><pre><code>      - name: Run \"unit\" tests\n        run: ./mvnw -B test\n        env:\n          SPRING_PROFILES_ACTIVE: local                                        &lt;1&gt;\n</code></pre><ol><li>Activate the local profile. The workflow's environment provides a Docker daemon. Hence, Testcontainer successfully downloads and runs the database container.</li></ol><p>\\\nAt this point, we should run the integration test. Yet, we need the app deployed to run this test. For this, we need available infrastructure.</p><h2>Alternative \"Unit testing\" on GitHub</h2><p>The above works perfectly on GitHub, but we can move closer to the deployment setup by leveraging GitHub <a href=\"https://docs.github.com/en/actions/use-cases-and-examples/using-containerized-services/about-service-containers\">service containers</a>. Let's migrate PostgreSQL from Testcontainers to a GitHub service container.</p><p>\\\nRemoving Testcontainers is pretty straightforward: we do not activate the  profile.</p><p>\\\nUsing GitHub's service container requires an additional section in our workflow:</p><pre><code>jobs:\n  build:\n    runs-on: ubuntu-latest\n    env:\n      GH_PG_USER: testuser                                                     #1\n      GH_PG_PASSWORD: testpassword                                             #1\n      GH_PG_DB: testdb                                                         #1\n    services:\n      postgres:\n        image: postgres:15\n        options: &gt;-                                                            #2\n          --health-cmd \"pg_isready -U $POSTGRES_USER\"\n          --health-interval 10s\n          --health-timeout 5s\n          --health-retries 5\n        ports:\n            - 5432/tcp                                                         #3\n        env:\n          POSTGRES_USER: ${{ env.GH_PG_USER }}                                 #4\n          POSTGRES_PASSWORD: ${{ env.GH_PG_PASSWORD }}                         #4\n          POSTGRES_DB: ${{ env.GH_PG_DB }}                                     #4\n</code></pre><ol><li><p>Define environment variables at the job level to use them across steps. You can use secrets, but in this case, the database instance is not exposed outside the workflow and will be switched off when the latter finishes. Environment variables are good enough to avoid adding unnecessary secrets.</p></li><li><p>Make sure that PostgreSQL works before going further.</p></li><li><p>Assign a random port and map it to the underlying  port.</p></li><li><p>Use the environment variables.</p></li></ol><p>\\\nTo run the tests using the above configuration is straightforward.</p><pre><code>      - name: Run \"unit\" tests\n        run: ./mvnw -B test\n        env:\n          SPRING_FLYWAY_URL: jdbc:postgresql://localhost:${{ job.services.postgres.ports['5432'] }}/${{ env.GH_PG_DB }} #1\n          SPRING_R2DBC_URL: r2dbc:postgresql://localhost:${{ job.services.postgres.ports['5432'] }}/${{ env.GH_PG_DB }} #1\n          SPRING_R2DBC_USERNAME: ${{ env.GH_PG_USER }}\n          SPRING_R2DBC_PASSWORD: ${{ env.GH_PG_PASSWORD }}\n</code></pre><ol><li><p>GitHub runs PostgreSQL on a local Docker, so the host is . We can get the random port with the <code>${{ job.services.postgres.ports['5432'] }}</code> syntax.</p></li></ol><p>In this post, we laid the ground for a simple app's unit- and integration-testing, leveraging Testcontainers in the local environment. We then proceeded to automate unit testing via a GitHub workflow with the help of GitHub service containers. In the next post, we will prepare the Kubernetes environment on a Cloud provider infrastructure, build the image, and deploy it to the latter.</p><p>\\\nThe complete source code for this post can be found on <a href=\"https://github.com/ajavageek/vcluster-pipeline\">GitHub</a>.</p><p><em>Originally published on <a href=\"https://blog.frankel.ch/pr-testing-kubernetes/1/\">A Java Geek</a> on February 9th, 2025</em></p>","contentLength":16407,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Figure AI is in talks to raise $1.5B at 15x its last valuation","url":"https://techcrunch.com/2025/02/14/figure-ai-is-in-talks-to-raise-1-5b-at-15x-its-last-valuation/","date":1739573509,"author":"Charles Rollet","guid":44,"unread":true,"content":"<p>Robotics startup Figure AI is raising $1.5 billion at a $39.5 billion valuation, a whopping 15 times higher than before.</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":183,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SailPoint’s dull debut did little to loosen the stuck IPO window, expert says","url":"https://techcrunch.com/2025/02/14/sailpoints-dull-debut-did-little-to-loosen-the-stuck-ipo-window-expert-says/","date":1739570453,"author":"Julie Bort","guid":43,"unread":true,"content":"<p>SailPoint’s IPO on Thursday was a disappointment for anyone hoping it would indicate that tech IPOs are hot again. The first day’s trading ended below the $23 initial price. The stock fared a tad better Friday, closing at over $24. But that’s nothing close to the big bang companies and VCs hope for. For instance, […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":389,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trump’s DOJ Corruption Laid Bare… By His Own Conservative Prosecutors","url":"https://www.techdirt.com/2025/02/14/trumps-doj-corruption-laid-bare-by-his-own-conservative-prosecutors/","date":1739569769,"author":"Mike Masnick","guid":307,"unread":true,"content":"<p><em> make sure you read the update at the end of this story.</em></p><p>Here’s a fun thing about corruption investigations: Usually when prosecutors uncover one quid pro quo, they don’t resolve it by offering an even bigger quid pro quo. And yet, that appears to be exactly what’s happening with NYC Mayor Eric Adams, who was <a href=\"https://www.justice.gov/usao-sdny/pr/new-york-city-mayor-eric-adams-charged-bribery-and-campaign-finance-offenses\">indicted last fall</a> for allegedly trading favors with Turkish officials, and is now watching those charges evaporate in exchange for helping the Trump administration with its immigration agenda.</p><p>The twist — and there’s always a twist — is that the people most effectively pointing out this corruption aren’t the usual suspects. Instead, it’s coming from a bunch of dyed-in-the-wool conservative prosecutors at SDNY who are <a href=\"https://www.theguardian.com/us-news/2025/feb/14/justice-department-officials-resign-eric-adams-charges\">resigning en masse</a> rather than participate in what they see as a perversion of justice. When the Federalist Society crowd starts quitting over corruption, you know something interesting is happening.</p><p>The apparent corruption here isn’t just brazen — it’s documented in black and white. The Justice Department’s <a href=\"https://www.nytimes.com/live/2025/02/10/nyregion/eric-adams-charges\">order to drop the case</a> doesn’t even pretend to assess the merits of the charges. Instead, Acting Deputy Attorney General Emil Bove explicitly tied the dismissal to Adams’ willingness to <a href=\"https://www.theguardian.com/us-news/2025/feb/13/nyc-eric-adams-rikers-ice\">assist with federal deportation efforts</a> — a textbook example of weaponizing prosecutorial discretion for political ends.</p><p>Even more disturbing is the mechanism: the dismissal is “without prejudice,” meaning charges could be refiled at any time. This isn’t just prosecutorial discretion — it’s prosecutorial extortion. The Trump administration has effectively created a sword of Damocles to hang over Adams’ head, ensuring his continued compliance with their immigration agenda. The message is clear: step out of line, and those charges might suddenly become relevant again. It’s the kind of institutional corruption that would make a banana republic blush.</p><p>It means that Adams’ personal freedom now outweighs the best interests of the people of New York City.</p><p>The system’s response to this corruption has been revealing. For several days after the initial order, an unusual silence descended over the Southern District office — a silence that spoke volumes about the internal struggle taking place. Then came something remarkable: a scathing letter from Acting US Attorney Danielle Sassoon to Attorney General Pam Bondi. Sassoon — a Federalist Society stalwart and former Scalia clerk who’s about as far from a “progressive prosecutor” as you can get — laid bare the rot at the core of this decision in a document that reads like a conservative legal scholar’s manifesto against institutional corruption.</p><blockquote><p><em>Because the law does not support a dismissal, and because I am confident that Adams has committed the crimes with which he is charged, I cannot agree to seek a dismissal driven by improper considerations. As Justice Robert Jackson explained, “the prosecutor at his best is one of the most beneficent forces in our society, when he acts from malice or other base motives, he is one of the worst.” The Federal Prosecutor, 24 J. Am. Jud. Soc’y 18 (“This authority has been granted by people who really wanted the right thing done—wanted crime eliminated— but also wanted the best in our American traditions preserved. “). I understand my duty as a prosecutor to mean enforcing the law impartially, and that includes prosecuting a validly returned indictment regardless whether its dismissal would be politically advantageous, to the defendant or to those who appointed me. A federal prosecutor “is the representative not of an ordinary party to a controversy, but of a sovereignty whose obligation to govern impartially is as compelling as its obligation to govern at all.” Berger v. United States, 295 U.S. 78, 88 (1935).</em></p><p><em>For the reasons explained above, I do not believe there are reasonable arguments in support of a Rule 48(a) motion to dismiss a case that is well supported by the evidence and the law. I understand that Mr. Bove disagrees, and I am mindful of your recent order reiterating prosecutors’ duty to make good-faith arguments in support of the Executive Branch’s positions. See Feb. 5, 2025 Mem. “General Policy Regarding Zealous Advocacy on Behalf of the United States.” But because I do not see any good-faith basis for the proposed position, I cannot make such arguments consistent with my duty of candor. N.Y.R.P.C.3.3; id. cmt. 2 (“A lawyer acting as an advocate in an adjudicative proceeding has an obligation to present the client’s case with persuasive force. Performance of that duty while maintaining confidences of the client, however, is qualified by the advocate’s duty of candor to the tribunal. ” ).</em></p><p><em>In particular, the rationale given by Mr. Bove—an exchange between a criminal defendant and the Department of Justice akin to the Bout exchange with Russia— is, as explained above, a bargain that a prosecutor should not make. Moreover, dismissing without prejudice and with the express option of again indicting Adams in the future creates obvious ethical problems, by implicitly threatening future prosecution if Adams’s cooperation with enforcing the immigration laws proves unsatisfactory to the Department. See In re Christoff, 690 N.E.2d 1135 (Ind. 1997) (disciplining prosecutor for threatening to renew a dormant criminal investigation against a potential candidate for public office in order to dissuade the candidate from running); Bruce A. Green &amp; Rebecca Roiphe, Who Should Police Politicization of the DOJ?, 35 Notre Dame J.L. Ethics &amp; Pub. Pol’y 671, 681 (2021) (noting that the Arizona Supreme Court disbarred the elected chief prosecutor of Maricopa County, Arizona, and his deputy, in part, for misusing their power to advance the chief prosecutor’s partisan political interests) . Finally, given the highly generalized accusations of weaponization, weighed against the strength of the evidence against Adams, a court will likely question whether that basis is pretextual. See, e.g. , United States v. Greater Blouse, Skirt &amp; Neckwear Contractors, 228 F. Supp. 483, 487 (S.D.N.Y. 1964)(courts “ should be satisfied that the reasons advanced for the proposed dismissal are substantial and the real grounds upon which the application is based”)</em></p><p><em>I remain baffled by the rushed and superficial process by which this decision was reached, in seeming collaboration with Adams’s counsel and without my direct input on the ultimate stated rationales for dismissal. Mr. Bove admonished me to be mindful of my obligation to zealously defend the interests of the United States and to advance good-faith arguments on behalf of the Administration. I hope you share my view that soliciting and considering the concerns of the U.S. Attorney overseeing the case serves rather than hinders that goal, and that we can find time to meet.</em></p></blockquote><p>But wait, it gets better! There’s a footnote in Sassoon’s letter that tells you everything you need to know about how modern corruption works. The old-school way was to have your shady meetings in smoke-filled back rooms. The new way, apparently, is to have them in official conference rooms while actively preventing anyone from taking notes:</p><blockquote><p><em>I attended a meeting on January 31, 2025, with Mr. Bove, Adams’s counsel, and members of my office. Adams’s attorneys repeatedly urged what amounted to a quid pro quo, indicating that Adams would be in a position to assist with the Department’s enforcement priorities only if the indictment were dismissed. Mr. Bove admonished a member of my team who took notes during that meeting and directed the collection of those notes at the meeting’s conclusion</em></p></blockquote><p>Nothing quite says you know you’re engaging in some shady ass shit like demanding you collect the notes of anyone in attendance.</p><p>What makes this story particularly significant is who’s blowing the whistle. Sassoon isn’t some “woke prosecutor” that the MAGA world can easily dismiss. She’s a card-carrying member of the conservative legal establishment who, until this week, was seen as a rising star in those circles. Her willingness to sacrifice her standing in that world to uphold basic constitutional principles reveals just how far the corruption has spread — and perhaps offers a glimmer of hope that some institutional guardrails still hold.</p><p>Sassoon’s stand has triggered a cascade of resignations within SDNY, with seven prosecutors (and counting) choosing to walk away rather than participate in this corruption of justice. The latest resignation letter, <a href=\"https://s3.documentcloud.org/documents/25536146/hagan-scotten-resignation-letter.pdf\">a scorching indictment from lead prosecutor Hagan Scotten</a>, is particularly noteworthy. Scotten — who clerked for both Justices Roberts and Kavanaugh and explicitly states his support for the Trump administration — makes it clear that this isn’t about politics; it’s about fundamental principles of justice being trampled for political gain.</p><blockquote><p><em>There is a tradition in public service of resigning in a last-ditch effort to head off a serious mistake. Some will view the mistake you are committing here in the light of their generally negative views of the new Administration. I do not share those views. I can even understand how a Chief Executive whose background is in business and politics might see the contemplated dismissal-with-leverage as a good, if distasteful, deal. But any assistant U.S. attorney would know that our laws and traditions do not allow using the prosecutorial power to influence other citizens, much less elected officials, in this way.</em><strong><em>If no lawyer within earshot of the President is willing to give him that advice, then I expect you will eventually find someone who is enough of a fool, or enough of a coward, to file your motion. But it was never going to be me.</em></strong></p></blockquote><p>Scotten’s prediction proved grimly prophetic. As <a href=\"https://www.theguardian.com/us-news/2025/feb/14/justice-department-officials-resign-eric-adams-charges\">reported just hours ago</a>, Bove and Bondi found their willing executioner — though the circumstances reveal yet another layer of institutional corruption:</p><blockquote><p><em>The prosecutor acquiesced to file the motion in an attempt to spare other career staff from potentially being fired by Emil Bove, the acting US deputy attorney general and former personal lawyer to Trump, sources briefed on the matter told Reuters. The news agency named the lawyer as Ed Sullivan, a veteran career prosecutor, who agreed to alleviate pressure on his colleagues in the department’s public integrity section of 30 attorneys, two sources said, after his team was given an hour by Bove to decide between them who would file the motion.</em></p><p><em>“This is not a capitulation – this is a coercion,” one of the people briefed on the meeting later told Reuters. “That person, in my mind, is a hero.” The whole section had reportedly discussed resigning en masse.</em></p></blockquote><p>The cruel irony of forcing the Public Integrity Section to compromise its own integrity isn’t lost on anyone. This is how institutions die — not with a bang, but with an ultimatum.</p><p>There’s a special kind of institutional poetry here: The Public Integrity Section was given an hour to decide who would compromise their integrity. And someone did, not out of cowardice or foolishness, but to protect their colleagues. “A hero,” his colleague called him, and maybe that’s right. But it’s the kind of heroism that only exists in broken systems.</p><p>The NY Times has revealed even more disturbing details about the behind-the-scenes machinations. In what reads like a playbook for corrupting justice, Bove apparently <a href=\"https://www.nytimes.com/2025/02/13/nyregion/adams-lawyers-justice-department-dismissal.html\"><em>coached Adams’ legal team</em></a> (including Alex Spiro, better known as Elon Musk’s go-to counsel) in a wink-wink-nudge-nudge fashion on exactly what political commitments would make the charges disappear.</p><blockquote><p><em>During the meeting, Mr. Bove signaled that the decision about whether to dismiss the case had nothing to do with its legal merits.</em></p><p><em>Instead, Mr. Bove said he was interested in whether the case was hindering Mr. Adams’s leadership, particularly with regard to the city’s ability to cooperate with the federal government on Mr. Trump’s crackdown on illegal immigration.</em></p><p><em>Mr. Bove also said he was interested in whether the case, brought by the former U.S. attorney, Damian Williams, was a politically motivated prosecution meant to hurt Mr. Adams’s re-election prospects.</em></p><p><em>In her letter to Ms. Bondi, Ms. Sassoon said that she was “baffled by the rushed and superficial process by which this decision was reached, in seeming collaboration with Adams’s counsel and without my direct input on the ultimate stated rationales for dismissal.”</em></p></blockquote><p>There’s something almost elegant about it, in a horrifying sort of way. The Justice Department has managed to transform a corruption prosecution into what amounts to a compliance manual for corruption. It’s like they’ve created a template: “Here’s how to trade criminal charges for political favors while maintaining plausible deniability.” And the really wild part? This is all happening after years of the MAGA world screaming about supposed “lawfare” against conservatives. Turns out they weren’t complaining about weaponized justice — they were planning how to do it themselves.</p><p>History rhymes: While mass resignations of principled lawyers helped topple Nixon’s presidency, in Trump’s second term they’ve become just another item in the daily digest of institutional erosion. The difference this time? It’s not the usual suspects sounding the alarm. Instead, it’s career conservatives — products of the Federalist Society pipeline — who are putting their careers on the line to preserve what’s left of prosecutorial independence.</p><p>As we’ve <a href=\"https://www.techdirt.com/2025/02/12/gop-is-quietly-freaking-out-about-elon-time-for-them-to-take-a-stand/\">previously discussed</a>, any path through this constitutional crisis requires principled conservatives to find their voice. The fact that it’s taking career prosecutors to do what elected Republicans won’t speaks volumes about where the real courage in conservative circles resides.</p><p>The question now isn’t just whether our institutions can survive this assault, but whether these acts of principled resistance can inspire others before the machinery of justice is fully converted into a tool of political control. The American experiment has survived previous challenges through the courage of individuals willing to place principle above party. We’re about to find out if enough of those individuals still exist.</p><p> Incredibly, that report that a prosecutor had agreed to file the dismissal turned out to not be accurate. Many hours later, after no such filing was actually made a few very bizarre things happened. First, Emil Bove <a href=\"https://storage.courtlistener.com/recap/gov.uscourts.nysd.628916/gov.uscourts.nysd.628916.121.0_2.pdf\">filed a notice of appearance</a> in the case. That is… not normal.</p><p>Finally, the “nolle prosequi” (a notice saying “we no longer want to prosecute”) <a href=\"https://storage.courtlistener.com/recap/gov.uscourts.nysd.628916/gov.uscourts.nysd.628916.122.0_4.pdf\">was filed</a>. But even the way it was filed is weird and somewhat unprecedented. Two lawyers, including Ed Sullivan (who was mentioned above as effectively agreeing to be the fool to protect his coworkers) signed , but they did not sign the final statement. Instead, there was a further “order” from the DOJ, signed by Bove alone, telling the Court to effectively dismiss the case: </p><p>Even the language here is bizarre. The prosecutors don’t get to “direct” the Court to do anything. That’s likely why Bacon and Sullivan signed the part about “respectfully requests” that the Court issue an order. But Bove leaps in, acting like he gets to order around the judge, and separately signs that part.</p><p>What will be interesting now, is to see what Judge Dale Ho does.</p>","contentLength":15455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI says its board of directors ‘unanimously’ rejects Elon Musk’s bid","url":"https://techcrunch.com/2025/02/14/openai-says-its-board-of-directors-unanimously-rejects-musks-bid/","date":1739569507,"author":"Kyle Wiggers","guid":42,"unread":true,"content":"<p>OpenAI’s board of directors has “unanimously” rejected billionaire Elon Musk’s offer to buy the nonprofit that effectively governs OpenAI, the company said on Friday. In a statement shared via OpenAI’s press account on X, Bret Taylor, board chair, called Musk’s bid “an attempt to disrupt his competition.” “OpenAI is not for sale, and the board […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bluesky gets growth and analytics tools with BlueSkyHunter launch","url":"https://techcrunch.com/2025/02/14/bluesky-gets-growth-and-analytics-tools-with-blueskyhunter-launch/","date":1739569355,"author":"Sarah Perez","guid":41,"unread":true,"content":"<p>A new startup is addressing the need for an all-in-one toolset built for people who want to grow, manage, and track their Bluesky presence and following. The subscription service BlueSkyHunter, which launched Friday, introduces an online dashboard that combines access to analytics and other tools to schedule posts and automate DMs (direct messages), alongside other […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Global electricity demand expected to grow 4% annually through 2027","url":"https://techcrunch.com/2025/02/14/global-electricity-demand-expected-to-grow-4-annually-through-2027/","date":1739569205,"author":"Tim De Chant","guid":40,"unread":true,"content":"<p>Meeting that demand will require adding more generating capacity than all of Japan —&nbsp;every year.&nbsp;</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":164,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How this weekend’s ‘Tesla Takeover’ protests against Elon Musk came together on Bluesky","url":"https://techcrunch.com/2025/02/14/how-this-weekends-tesla-takeover-protests-against-elon-musk-came-together-on-bluesky/","date":1739568849,"author":"Sean O'Kane","guid":39,"unread":true,"content":"<p>As Elon Musk and his acolytes rip through the federal government looking for agencies to throw into the “wood chipper,” a grassroots effort to hit the world’s richest man where it hurts is picking up steam. The courts are busy contesting the actions of Musk’s Department of Government Efficiency, but the judicial system is slow […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is device code phishing, and why are Russian spies so successful at it?","url":"https://arstechnica.com/information-technology/2025/02/russian-spies-use-device-code-phishing-to-hijack-microsoft-accounts/","date":1739567771,"author":"Dan Goodin","guid":298,"unread":true,"content":"<p>Researchers have uncovered a sustained and ongoing campaign by Russian spies that uses a clever phishing technique to hijack Microsoft 365 accounts belonging to a wide range of targets, researchers warned.</p><p>The technique is known as device code phishing. It exploits “device code flow,” a form of authentication formalized in the industry-wide <a href=\"https://tools.ietf.org/html/draft-ietf-oauth-device-flow-07#section-3.4\">OAuth standard</a>. Authentication through device code flow is designed for logging printers, smart TVs, and similar devices into accounts. These devices typically don’t support browsers, making it difficult to sign in using more standard forms of authentication, such as entering user names, passwords, and two-factor mechanisms.</p><p>Rather than authenticating the user directly, the input-constrained device displays an alphabetic or alphanumeric device code along with a link associated with the user account. The user opens the link on a computer or other device that’s easier to sign in with and enters the code. The remote server then sends a token to the input-constrained device that logs it into the account.</p>","contentLength":1058,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2022/03/phishing.jpeg","enclosureMime":"","commentsUrl":null},{"title":"DeepSeek founder Liang Wenfeng is reportedly set to meet with China’s Xi Jinping","url":"https://techcrunch.com/2025/02/14/deepseek-founder-liang-wenfeng-is-reportedly-set-to-meet-with-chinas-xi-jinping/","date":1739565917,"author":"Kyle Wiggers","guid":38,"unread":true,"content":"<p>Chinese AI startup DeepSeek founder Liang Wenfeng is reportedly set to meet with China’s top politicians, including Chinese leader Xi Jinping, during a summit that Alibaba founder Jack Ma is also expected to attend. The summit, which could happen as soon as next week, may be intended as a signal by China’s Communist Party that […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why PAUL Needs a Massive Dataset to Improve Its Movements","url":"https://hackernoon.com/why-paul-needs-a-massive-dataset-to-improve-its-movements?source=rss","date":1739565002,"author":"EScholar: Electronic Academic Papers for Scholars","guid":102,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing</p><p>4 Data Acquisition and Open-Loop Control</p><h2>4.3 Dataset Generation: Table-Based Models</h2><p>Due to the complexity of the robot, model-based methodologies, such as PCC or the ones based on Cosserat Rod Theory were discarded. Although the usage of FEM is an avenue that will not be closed in future work, the large number of parameters to be set experimentally for each segment (Young’s modulus, moment of inertia. . . ), given that the manufacturing process is so variable meant that, in this first phase, we opted to use some type of PAUL modelling based on data collection.</p><p>\\\nThe output of the system is taken as the position and orientation reached by the final end –thus, at this stage, all the positions of the intermediate segments are ignored– and as input, the inflation times of each of the bladders. As there were not enough pressure sensors available at the time of the construction of the robot, it was decided to take inflation time as an input variable. As the working pressure is limited by the pressure limiting valve and the flow rate into each bladder can be assumed to be constant, the time is equivalent to the volume of air introduced into each cavity.</p><p>\\\nAll the control options considered have in common the need for a large amount of empirical data, which leads to the need to develop an experimental design to systematise the collection of this data. Since the capture of this information is done in different phases and the datasets have to represent the behaviour of the robot in an objective way, the re-applicability of the experiment takes on special importance.</p><p>\\\nThe data stored in the datasets was the position of the robot tip and the set of inflation times that achieve this configuration. The aforementioned limitation that only two of the three bladders in the segment are inflated reduces redundancies. As previously stated, more than two segments lead to redundancies, which implies that the inverse kinematic model of the robot can have multiple solutions.</p><p>\\\nThe data collection process involves several sequential steps. Initially, a set number of samples is determined. For each sample, Matlab commands dispatch a random combination of nine inflation times, corresponding to each valve of PAUL, to the actuation bench. Times are generated below a maximum time limit Tmax, and ensuring that only two cavities per segment are inflated. Following this, the robot’s bladders are inflated based on the sent times. Subsequently, the vision system’s two cameras capture images to determine the position and orientation of the robot’s end. This entire procedure is repeated for the specified number of iterations, and upon completion, the collected data is stored in the dataset</p><p>\\\nThe information on swelling times is stored as a percentage, with a value of 0 corresponding to a zero swelling of that segment and 100 corresponding to Tmax, the swelling for the maximum number of milliseconds defined for this data collection session. This value Tmax is stored, together with the values, in the dataset, in order to be able to compare different datasets. The reason for this coding comes from the lack of information, a priori, on what is the maximum pressure that a PAUL bladder admits. Although it is true that it was experimentally determined that inflation times of more than 1500 ms in a row led to punctures, the application of lower times during a repeated number of cycles also generated leaks. On this basis, it was decided never to inflate any valve, either in one or several steps, more than 1000 ms.</p><p>\\\nAlong with the inflation times of each bladder, the position and orientation reached by the end tip is stored, based on the camera readings. In particular, the position of the green marker and the orientation of the trihedron are stored. The latter is expressed in Euler angles, as it is a much efficient form of storage than a rotation matrix. In addition, the dataset also contains metadata from the collection process that are believed to influence the results, such as the pneumatic line pressure or the ambient temperature.</p><p>\\\nSome aspects in the pneumatic system merit attention. Initially, bladder inflation and deflation are not symmetrical processes. Geometric constraints in the pneumatic components result in a lower deflation rate compared to inflation. Consequently, when the PAUL receives a deflation time, it multiplies it by an empirically derived factor, approximately 1.45 for a 1.2 bar working pressure. This multiplier compensates for the discrepancy between inflation and deflation times of a singular group of bladders, ensuring that the deflation time aligns with the time required to reach the same inflation point.</p><p>\\\nSimilarly, although it is physically possible to inflate several valves at the same time, it has been shown that this parallel flow distribution means that the effective fillings of each valve are not the same as if they were inflated individually. To prevent this phenomenon, it was decided to inflate each bladder individually both during the data acquisition process and utterly, when PAUL was asked to reach certain positions.</p><p>\\\nFinally, there are hysteresis phenomena in the silicone that cause the position reached by inflating for a time t to be different from the position reached by inflating first for a time t1 and then for a time t2 = t − t1. The strategy employed to tackle this problem was to capture the dataset bringing PAUL back to its zero position between each sample. Nevertheless, when controlling the robot in open-loop this is not possible, or, at least, not desirable, as one may wish to follow trajectories or travel through a sequence of points. Therefore, transitioning from position x1 to x2 requires an additional factor of 1.2, also derived experimentally, to account for hysteresis effects.</p><p>Once the dataset is generated, it can be used to model the behaviour of PAUL for open-loop control. It is foreseen, as a future line, to train a neural network for the direct kinematics and another one for the inverse kinematics. However, given the large amount of data that may be required (in [62] 24389 samples are used for a three-segment robot like this one), a table look-up method has been used for this work.</p><p>\\\nThe method for direct kinematics –which allows obtaining the position and orientation of the final end of the robot from the inflation times of the nine bladders– consists of searching, in the generated dataset in the previous step, the three inflation time values located at a shorter distance from the inflation time given as a reference. Obviously, if the set of inflation times sought were in the table, the value associated with these times would be returned as a result of the direct kinematic model. Otherwise, the average of the position and orientation values associated with the three closest inflation times, weighted by the distance (Euclidean norm) existing between each of them and the values of reference inflation times, is returned as the position and orientation value of the robot.</p><p>\\\n\\\nwith them, it is possible to calculate the position returned by the direct kinematic model using the expression:</p>","contentLength":7812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"NATO backs its first cohort of European dual-use startups","url":"https://techcrunch.com/2025/02/14/nato-backs-its-first-cohort-of-european-dual-use-startups/","date":1739564172,"author":"Mike Butcher","guid":37,"unread":true,"content":"<p>With both Vice President J.D. Vance and U.S. Defense Secretary Pete Hegseth making loud noises Friday about Europe stepping up to the plate in spending more on its own defense, it might come as a surprise that Europe is already on the path toward far greater investment in defense, especially in tech. Not only has […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":383,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How PAUL the Robot Tracks Its Own Movements Using Cameras and LEDs","url":"https://hackernoon.com/how-paul-the-robot-tracks-its-own-movements-using-cameras-and-leds?source=rss","date":1739564107,"author":"EScholar: Electronic Academic Papers for Scholars","guid":101,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing</p><p>4 Data Acquisition and Open-Loop Control</p><h2>4 Data Acquisition and Open-Loop Control</h2><p>In order to provide the manipulator with a solid and stable fastening system, which would also allow reliable and predictable data capture of the positions and orientations of its end, the metal structure shown in Figure 10 was built. It is a cube made of steel profiles with methacrylate sheets on the walls. The pneumatic bench, the power supply and the microcontroller were placed on top of the structure.</p><p>\\\nThe aim of the data acquisition system is to be able to measure, whenever required, the position and orientation of the end of the robot in order to be able to relate it to the inflation times of each bladder and thus be able to create an open-loop model of PAUL. For this purpose, three elements are available: the cameras, the calibration grid and the trihedron.</p><p>\\\nTwo Spedal AF926H USB cameras with 1920 x 1080 px, 80◦ field of view and a frequency of 60 fps are used to capture the images. These have been placed on two tripods external to the robot’s structure. They are calibrated with a checkerboard of 11 x 8 squares of 20 mm each, which can be seen in Figure 11a.</p><p>\\\nThe vision beacon, on the other hand, has the task of being recognised in space to determine the position and orientation of the mobile system with respect to the fixed system. The trihedron, displayed in Figure 11b, consists of three spheres, manufactured by 3D printing in PLA, inside which three LED diodes have been embedded. Thanks to these, it is possible to vary the luminosity of the spheres by means of software, keeping the system functioning correctly when the workplace or the environmental or lighting conditions vary.</p><p>\\\nThe existence of the central rod, which moves the luminous spheres away from the base of the robot end, makes possible the spheres to be visible to the cameras in all the poses that the robot can adopt. If the spheres were otherwise directly attached to the</p><p>\\\nend of the robot, there would be numerous poses in which it would not be possible to determine the position, as the spheres would be hidden by the robot itself.</p><h3>4.2 Vision Capture System</h3><p>\\\n\\\nBecause coordinates of the real world are independent of the camera, if Equation (1) is applied for both cameras and rk vector cleared in the two equations, it can be said that:</p><p>\\\n\\\nSystem of equations (2) can be solved using the Least Squares Method:</p><p>\\\n\\\nand then use the Rodrigues’ rotation formula to obtain it, respect to the real world base in the form of a rotation matrix:</p><p>\\\n\\\nand I denotes the identity matrix of size 3.</p>","contentLength":3247,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Drugs Have Won The War On Drugs: Drugged-Up Rat Infestation Edition","url":"https://www.techdirt.com/2025/02/14/drugs-have-won-the-war-on-drugs-drugged-up-rat-infestation-edition/","date":1739563659,"author":"Tim Cushing","guid":306,"unread":true,"content":"<p>50+ years of hardline prohibition have only resulted in better prices, better purity, and a slew of states legalizing or decriminalizing personal use amounts of any number of drugs, with marijuana leading the way in terms of overall legalization. </p><p>Treating drugs users as just as terrible as drug dealers has led to a ton of incarceration and a  ton of collected evidence that is apparently being consumed by things cops and crooks are used to dealing with: rats.</p><blockquote><p><em>“Drug-addicted rats” are eating narcotics seized and stored by Houston police, prompting a change in how long the police department is required to store the evidence, officials said.</em></p><p><em>Houston Mayor John Whitmire, Harris County District Attorney Sean Teare and Houston Police Chief J. Noe Diaz announced new steps Friday to dispose of drugs and other evidence kept at police headquarters downtown, some of which has been sitting there for decades, attracting rodents, even though cases they are linked to have long been adjudicated.</em></p></blockquote><p>This is where it becomes clear that drugs have won the Drug War. This single evidence locker contains enough weed to start a drug empire. </p><blockquote><p><em>“We got 400,000 pounds of marijuana in storage,” Whitmire said. “The rats are the only ones enjoying it.”</em></p></blockquote><p>The Houston PD (which has <a href=\"https://www.techdirt.com/2024/10/17/houston-cop-gerald-goines-gets-60-year-sentence-for-leading-bogus-drug-raid-that-ended-with-cops-killing-two-people/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2024/10/17/houston-cop-gerald-goines-gets-60-year-sentence-for-leading-bogus-drug-raid-that-ended-with-cops-killing-two-people/\">more than its share</a> of <a href=\"https://www.techdirt.com/2022/11/10/houston-pd-drops-cases-tainted-by-corrupt-narcotics-officers-but-decides-it-can-still-keep-seized-cash/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2022/11/10/houston-pd-drops-cases-tainted-by-corrupt-narcotics-officers-but-decides-it-can-still-keep-seized-cash/\">corrupt drug cops</a>) is sitting on 200  of marijuana it has amassed over the years. And yet, I would wager that no Houston resident has any trouble obtaining this product on short notice.</p><p>The same goes for the rest of the stuff in the warehouse, which apparently (until recently) contained cocaine seized in 1996. As police chief J. Noe Diaz pointed out, the evidence has outlasted the case. The suspect pled guilty, served his time, and was free to go long before the evidence used to convict him was.</p><p>It’s apparently a nationwide problem, according to some of the forensic experts interviewed by NBC News, <a href=\"https://www.nbcnews.com/news/us-news/rats-are-high-marijuana-stored-infested-new-orleans-police-evidence-ro-rcna143249\" data-type=\"link\" data-id=\"https://www.nbcnews.com/news/us-news/rats-are-high-marijuana-stored-infested-new-orleans-police-evidence-ro-rcna143249\">which carried a story early last year</a> about a similar drug-eating rat problem in New Orleans. </p><p>New rules are being put in place to destroy this evidence more frequently. On one hand, it makes sense to destroy evidence after defendants serve their time in jail. On the other hand, I wouldn’t get  carried away giving PDs permission to destroy evidence, since that’s the sort of thing that lends itself to cover-ups and the disappearance of evidence prisoners might use to challenge their convictions — a process that can take years, thanks to the justice system’s reluctance to reconsider its own calls and the byzantine processes convicted people are expected to navigate just to have (a very looooong) shot at having their cases heard. </p><p>But above all that, there’s the sheer amount of drugs being held in evidence warehouses. If there’s nearly a half-million pounds of weed just laying around at any given time, it would seem law enforcement isn’t <a href=\"https://www.techdirt.com/2024/12/27/florida-sheriff-decided-it-might-be-a-good-idea-to-manufacture-crack-and-now-2600-convictions-might-be-vacated/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2024/12/27/florida-sheriff-decided-it-might-be-a-good-idea-to-manufacture-crack-and-now-2600-convictions-might-be-vacated/\">scoring many meaningful wins</a> in the Drug War. At best, it’s a game of Whack-a-mole that won’t generate enough tickets to make it worth visiting the merchandise booth at the arcade. At worst, it’s just cops looking busy, a meaningless waste of time that unfortunately results in people losing years of their lives to a system that not only can’t fix what’s broken, but clearly prefers doing the things <a href=\"https://www.techdirt.com/2024/10/03/lapd-raids-medical-lab-for-nonexistent-weed-get-gun-stuck-in-an-mri-machine/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2024/10/03/lapd-raids-medical-lab-for-nonexistent-weed-get-gun-stuck-in-an-mri-machine/\">that don’t work</a> as often as possible in perpetuity. </p>","contentLength":3327,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elon Musk’s AI company, xAI, said to be in talks to raise $10B","url":"https://techcrunch.com/2025/02/14/elon-musks-ai-company-xai-said-to-be-in-talks-to-raise-10b/","date":1739562841,"author":"Kyle Wiggers","guid":36,"unread":true,"content":"<p>Elon Musk’s AI company, xAI, is said to be in talks to raise $10 billion in a round that would value xAI at $75 billion. Bloomberg reported Friday that xAI is canvassing existing investors, including Sequoia Capital, Andreessen Horowitz, and Valor Equity Partners for the round, which would bring xAI’s total raised to $22.4 billion, […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Researcher Captures Contents of ‘DEI.gov’ Before It Was Hidden Behind a Password","url":"https://www.404media.co/dei-waste-gov-doge-list-behind-password/","date":1739561563,"author":"Samantha Cole","guid":388,"unread":true,"content":"<img src=\"https://www.404media.co/content/images/2025/02/Screenshot-2025-02-14-at-2.25.54-PM.png\" alt=\"Researcher Captures Contents of ‘DEI.gov’ Before It Was Hidden Behind a Password\"><p>A German researcher captured the contents of the White House’s “DEI.gov” during a brief period when it was not password protected.</p><p>The capture shows that the site contains a list of vague, alleged government-funded tasks and their costs, without sources or context, like “$1.3 million to Arab and Jewish photographers,\" “$1.5 million for ‘art for inclusion of people with disabilities,’” and \"$3.4 million for Malaysian drug-fueled gay sex app.” DEI.gov redirects to waste.gov and is currently inaccessible without a password; Elon Musk told reporters on Tuesday that his Department of Government Efficiency (DOGE) is “trying to be as transparent as possible.”</p><p>⁨The researcher is Henrik Schönemann⁩, a historian who started the <a href=\"https://safeguarding-research.discourse.group/?ref=404media.co\"><u>Safeguarding Research &amp; Culture archivalist project</u></a>, <a href=\"https://fedihum.org/@lavaeolus/113998679846185670?ref=404media.co\" rel=\"noreferrer\">posted screenshots on Mastodon</a> showing the contents. Schönemann⁩ also shared the specific site scrapes that he was able to capture, which showed the contents of the site. He told 404 Media he set up a change detection app using PikaPods, and is monitoring changes across hundreds of government websites. When the dei.gov and waste.gov sites were registered 10 days ago, he started tracking them, too.&nbsp;</p><p>Before the site administrators added a Wordpress template to the pages, the list was online at those URLs. This list was only online for a maximum of 30 minutes, starting around 4:50 p.m. EST; by 5:23 p.m. on February 11, it was gone from public view, according to the snapshots Schönemann’s app⁩ captured.&nbsp;</p><p>According to the screenshots provided by Schönemann⁩, the list includes (all of the following are direct quotes):&nbsp;</p><ul><li>$78,000 to Palestinian activist group whose chairman was photographed attending an anniversary event celebrating the founding of the Popular Front for the Liberation of the Palestine terrorist group</li><li>$1 Million for foreign DEI programs, including ‘indigenous language technology’ in Guatemala, per non-public funding docs reviewed by WFB</li><li>$5 million for effort to treat eating disorders by “affirming” LGBTQIA+ patients’ sexual orientation and gender claims</li><li>Up to $3 million to defund the police advocacy group to pursue “climate justice” for convicts</li><li>Funded performances of play “Angels in America: A Gay Fantasia on National Themes,” in which God is bisexual and communists are good, in North Macedonia</li><li>Disbursed $15,000 to “queer” Muslim writers in India</li><li>Shelled out tens of thousands to create army of 2,500 LGBTQI+ allies</li><li>Up to $10 million worth of USAID-funded meals went to al Qaeda-linked terrorist group the Nusra Front</li><li>$500,000 to group that “empowers women” in attempt to solve sectarian violence in Israel just ten days before Hamas’ Oct. 7 attacks</li><li>$4.67 million to EcoHealth Alliance – one of the key NGOs funding bat virus research at Wuhan Institute of Virology — in late 2021. Later refused to answer key questions about the funding.</li><li>$7.9 million to a project that would teach Sri Lankan journalists to avoid “binary-gendered language”</li><li>$1.3 million to Arab and Jewish photographers</li><li>$1.5 million for “art for inclusion of people with disabilities”</li><li>$2 million to promote “LGBT equality through entrepreneurship…in developing Latin American countries.”</li><li>Education Week: “Biden Administration Cites 1619 Project as Inspiration in History Grant Proposal”</li><li>VA took at least a dozen actions aimed at bolstering DEI during the Biden-Harris administration while the number of homeless veterans increased and the amount of claims in the VA’s backlog grew from ~211,000 to ~378,000</li><li>NASA has allocated roughly $10 million to grants advancing DEI and “environmental justice” since 2020</li><li>Following President Trump’s executive order on DEI at federal agencies, the ATF “quietly changing the job title of its former diversity officer… to ‘senior executive’ with the ATF.</li><li>The Department of Labor requested additional funding in 2023 for “The Chief Evaluation Office for a new rigorous interagency evaluation of actions aimed at improving Diversity, Equity, Inclusivity, and Accessibility across the federal workforce,” more than $6.5 million “to restore employee benefits programs that will advance equity by specifically addressing how opportunities can be expanded for underserved communities and vulnerable populations,” and $5 million “to evaluate actions aimed at improving diversity, equity, inclusion, and accessibility (DEIA) within the federal workforce.”</li><li>Fox Business: “FOX Business’ ‘Trouble in the Skies,’ a six month investigation of the FAA’s new hiring practices, uncovered changes that may put the nation’s flying public at risk as well as allegations that the newest air traffic control recruits had access to answers on a key test that helped them gain jobs with the FAA…Also uncovered was an FAA effort to promote diversity that discarded 3000 qualified college graduates with degrees in air traffic control despite their following FAA procedure and obtaining FAA accredited degrees.”</li></ul><p>Schönemann⁩ told 404 Media he wanted to share a sentiment alongside his find: “People all around the world care, you are not alone. And: #TransRights.” </p><p>Earlier this week, we reported that the Trump administration had set up a website called waste.gov, which was live on the internet with a sample page from a default WordPress template. Both DEI.gov and waste.gov were created at the same time, according to Reuters, and <a href=\"http://dei.gov/?ref=404media.co\"><u>DEI.gov was recently set up to redirect to waste.gov</u></a>. After our reporting, both websites were put behind a password wall.</p>","contentLength":5563,"flags":null,"enclosureUrl":"https://www.404media.co/content/images/2025/02/Screenshot-2025-02-14-at-2.25.54-PM.png","enclosureMime":"","commentsUrl":null},{"title":"Meta’s next big bet may be humanoid robotics","url":"https://techcrunch.com/2025/02/14/metas-next-big-bet-may-be-humanoid-robotics/","date":1739560571,"author":"Kyle Wiggers","guid":35,"unread":true,"content":"<p>Meta is forming a new team within its Reality Labs hardware division to build robots that can assist with physical tasks, Bloomberg reported. The team will be responsible for developing humanoid robotics hardware, potentially including hardware that can perform household chores. Meta’s new robotics group, which will be led by Marc Whitten, driverless car startup […]</p><p>© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":435,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How PAUL, a Soft Robot, is Designed and Built","url":"https://hackernoon.com/how-paul-a-soft-robot-is-designed-and-built?source=rss","date":1739559606,"author":"EScholar: Electronic Academic Papers for Scholars","guid":100,"unread":true,"content":"<p>(1) Jorge Francisco Garcia-Samartın, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain (jorge.gsamartin@upm.es);</p><p>(2) Adrian Rieker, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain;</p><p>(3) Antonio Barrientos, Centro de Automatica y Robotica (UPM-CSIC), Universidad Politecnica de Madrid — Consejo Superior de Investigaciones Cientıficas, Jose Gutierrez Abascal 2, 28006 Madrid, Spain.</p><p>3 PAUL: Design and Manufacturing</p><p>4 Data Acquisition and Open-Loop Control</p><p>The first step in the manufacturing process is to obtain the wax cores which, when inserted into the mould, are used to create the holes for what, in the finished segment, will be the bladders. These are made by pouring paraffin wax into previously made female moulds (Figure 5a).</p><p>\\\nAfter half an hour, the wax has solidified and the cores can be removed and inserted into the mould (Figure 5b). The mould consists of four 3D printed parts (two sides, a bottom cap and a top grip on which the cores rest) which are screwed together and then sealed with a hot silicone bead to prevent leakage during subsequent curing (Figure 5c).</p><p>\\\nThe silicone can then be poured into the mould, which must be filled to the top to counteract the aforementioned shrinkage. In particular, TinSil8015 requires a mass ratio of 10:1 liquid to catalyst. For the dimensions of the segment, about 175 g of total mixture are required.</p><p>\\\nThe curing process lasts 24 hours at ambient temperature (Figure 5d), after which it can be removed from the mould. It may be necessary to use a scalpel to remove the silicone burrs (Figure 5e).</p><p>\\\nOnce the segment has been built, the cores that have been used to create the bladders are removed. While the wood can be removed by pulling, it is necessary to apply heat to the segment to remove the wax. Thus, it is first placed in an oven at 110 ◦C (Figure 5f) and then immersed in a boiling water bath for 15 minutes, which ensures the elimination of the residual traces of wax (Figure 5g).</p><p>\\\nSince the males are through, it is required to close the lower part of the segment. To do this, a layer of silicone is poured onto the plate of Figure 5h, glued onto the segment and left to cure for 24 hours. Finally, the pneumatic tubes are joined to the segment, adhering them with cyanoacrylate and strengthening the tightness with the usage of plastic flanges (Figure 5i).</p><p>\\\nThe final result, a functional segment is depicted in Figure 6. Experimentally, it is found that its weight is 161 g and that, as designed, it has a height of 100 mm and an external diameter of 45 mm.</p><p>Within the robot, the function of the pneumatic bench is to control the flow of compressed air from the compressor according to the control signals. Specifically, the PAUL bench consists of 6 pairs of 2/2 valves (SMC VDW20BZ1D model) and 3/2 valves (SMC Y100 model) placed in series, which will therefore allow up to 12 degrees of freedom. Both are shown in Figure 7. The physical characteristics of the 2/2 valves limited the total pressure of the assembly to 4 bar, but to reduce the risk of segment leakage, it was reduced with a flow regulator to 2 bar. Figure 8 presents a schematic of the pneumatic circuit.</p><p>\\\n\\\nThe valves are operated via 24 V voltage signals. A MOSFET (model IRF540) is the switch in charge of managing them. Initially, the use of relays was considered, but the high current they would consume made their use unfeasible. An Arduino Due was chosen as the bench controller. A PC power supply, capable of supplying up to 8.5 A, is responsible for powering the unit, whose final layout is illustrated in Figure 9.</p>","contentLength":3825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"No Startup Has Ever Failed Because it Didn’t Have a Blog","url":"https://hackernoon.com/no-startup-has-ever-failed-because-it-didnt-have-a-blog?source=rss","date":1739559603,"author":"susie liu","guid":99,"unread":true,"content":"<p>\\\nSomewhere along the way, startup marketing got hijacked by creator economy logic.</p><ul><li>“If I build an audience, I’ll have an easier time selling.” </li><li>“If I post valuable content, customers will come to me.” </li><li>“Content is how you earn trust.”</li></ul><blockquote><p>No startup has ever failed because it didn’t have a blog. Name one high-growth startup that failed because it didn’t have a content marketing strategy. </p><p>\\\n  . Because it’s never happened. </p></blockquote><p>\\\nStartups fail because: 1) They didn’t find product-market fit. 2) They spent money like fools. 3) A combo of shaky financials plus bad operations. <strong>Not because they skipped the “10 SaaS Growth Hacks” Medium post.</strong></p><h2><strong>The Case Against Content Marketing</strong></h2><p>\\\nContent isn’t just eating up your time—<strong><em>it’s going to eat your startup for lunch</em></strong>. You think you’re building something, but really, you’re just shuffling pixels around and calling it momentum. <strong>==You’ll die because you’re wasting your best thinking on being interesting instead of being necessary.==</strong></p><p>\\\nDon’t try to be a media company when you haven’t even figured out how to be a business.</p><h3><strong>You Ignore Your Most Powerful Asset: Your Product</strong></h3><p>\\\nThe best startups don’t need a perfect LinkedIn strategy. They need a product so good that people can’t shut up about it.</p><ul><li>Tesla didn’t become Tesla because Musk wrote aspirational posts. (Tesla happened, then people started dissecting every Elon tweet.) </li><li>Superhuman didn’t win because they ranked for “best email productivity tool”.</li><li>Notion didn’t explode because they had a “high-value” newsletter.</li></ul><p>\\\nThey each built something so good that content wasn’t necessary.</p><p>\\\n<strong>==But content marketing makes you spend more time talking about your product than making it actually worth talking about.==</strong> If you’re creating Canva graphics explaining why your product is amazing, ask yourself: <em>Why isn’t it obvious already?</em></p><h3><strong>You’re Playing To Your Weakness</strong></h3><p>\\\nIf you’re a tech founder, chances are your biggest strength isn’t crafting “valuable content.” But content marketing throws you into the most crowded, attention-starved battlefield on earth that’s populated by all the creative genius humanity has to offer: the Internet. </p><p>\\\nContent marketing pulls you away from your strengths. It forces you into a slow, bloated, competitive game where you’re fighting against seasoned creators and media companies—people whose  is content. <em>And you’re out here thinking you can compete?</em></p><p>\\\nWhile you’re wasting time learning how to be a copywriter, you’re neglecting the strengths that gave you the guts to build a company in the first place. </p><p>\\\n<strong>Content marketing is a wait-and-see game.</strong></p><ul></ul><p>Do this long enough, and <strong>you'll start convincing yourself that the reason you're not making progress is because you haven’t published enough</strong>. So, you rinse and repeat, cranking out more content—and waiting for something to happen. <strong><em>I promise you, nothing will</em></strong>. <strong>==Unless you’re a media company, the real problem won’t be solved through content volume==</strong>. Look at the Airbnb guys. They tackled crappy check-in rates by bringing their cameras into homes and taking better photos, not by blogging about the future of holiday stays. </p><p>\\\nFounders love content marketing because it’s <strong><em>a safe way to feel productive without doing the uncomfortable shit</em></strong>. Well, if you want to stay away from the uncomfortable, you should consider going back to your desk job. </p><h3><strong>You’re Sprinting For The Wrong Finish Line</strong></h3><p>\\\n==Content marketing subtly shifts the goalpost of what success looks like.== </p><ul><li>Instead of $$$ in the bank, founders start measuring social impressions. </li><li>Instead of customers using the product, they’re tracking newsletter signups. </li><li>Instead of winning market share, they’re focused on LinkedIn comments.</li></ul><p>\\\n Awareness and engagement doesn’t pay the bills. Real growth is customers in, cash in. If you don’t know the difference between attention and action, content marketing is eating your brain. <strong><em>Are you really growing, or have you just gotten more people to consume free content?</em></strong></p><h3><strong>You’re Killing Your Brand</strong></h3><p>\\\nWeird, right? You’d think viral content will help you build your brand. Nope, because going viral means you’ve got to jump on every buzzword, every new format, and every other trend because it’s what everyone’s talking about. Well, trends don’t care two cents about what your brand stands for. </p><p>\\\n<strong>Every time you chase a trend, you’re stretching your brand thin, trying to force it into a mold that doesn’t fit.</strong> You’re watering down your purpose for a quick hit, all while ignoring the fact that <strong>==<em>building a memorable brand means sticking to your guns, even if it’s what the algorithm despises.</em>==</strong></p><h2><strong>Inbound’s Still In. You Just Don’t Need Content To Do It.</strong></h2><blockquote><p>The content marketing trend is a byproduct of people misunderstanding HubSpot’s whole inbound marketing thing. </p><p>\\\n  Inbound was and still is genius, but inbound marketing was never about “just create content.” It was about <strong>==creating undeniable value before asking for the sale.==</strong> (That, and not relying on paid ads for awareness.) </p></blockquote><p>\\\nHubSpot made content work because information was scarce in 2006—they gave away what no one else would. Today, everyone’s got an “ultimate guide” to something. And if they don’t, GPT can make something up based on a prompt. The problem now isn’t lack of content. </p><p>\\\n<strong><em>Here’s how to do inbound marketing in the post-content age.</em></strong></p><h3><strong>1. Figure Out If Content Even Makes Sense for You</strong></h3><p>\\\nBefore you waste another hour on a blog post, ask yourself:</p><ul><li>Is my audience actively looking for information, but struggling to find it? </li><li>Is there a knowledge gap that makes them hesitate to buy? </li><li>Are my competitors hoarding insights instead of sharing them?</li></ul><p>\\\nIf the answer is yes, fine. Write. Publish. Own the space. If the answer is no, STOP. No one needs your “Top 10 Trends” list. <em>They have Google and chatbots for that.</em></p><p>\\\nKnow what’s better than teaching people how to do something? ***Making sure they never have to learn it in the first place. ***</p><p>\\\nYou can write another 1,500-word thesis. <strong>Or, you can build something—a tool, a feature, an experience—that solves the same problem but without needing people to read a bunch of text</strong>. Look at the content that’s dominating your space—the posts that get all the likes, shares, and comments. Then ask yourself: <strong><em>How can I make this educational BS obsolete?</em></strong></p><blockquote><p>Here’s a thought: Tell your audience exactly how much time and money you’re burning on content—and then tell them you’re ditching it to build something free that actually helps them. \"We were going to spend $20K this quarter on content, but instead, we’re using it to build a free AI-powered email draft generator for sales teams. Tell us what you need, and we’ll make it happen.\" </p></blockquote><p>\\\nPeople will talk about it. They’ll share it, they’ll use it, <strong><em>and they’ll remember the one who actually did something instead of just saying something.</em></strong></p><h3><strong>3. Content Isn’t The Only Thing That Gets Organic Attention</strong></h3><p>\\\nThe other principle of inbound marketing?  Yes, content generates organic awareness and conversation. So do a lot of other things. . This is a last resort because stunts could backfire (but for a startup, any PR is good PR), but at least you’re exercising your creative brain cells. </p><blockquote><p>If you have to force people to see your content, why not just skip the middle step and force them to see your product instead? </p></blockquote><p>\\\nStop badgering GPT for content ideas, and use that time to hijack attention—starting fights on X, sitting on the doorsteps of potential clients for weeks on end with a cardboard sign, <strong><em>anything that gets people talking about your company now, not in six months when Google decides to bless you with traffic.</em></strong></p><p>\\\n(Tip: When pulling a stunt, ==just make sure it barely costs you anything to execute.== That way you won’t hate yourself if you don’t get returns.) </p><h2><strong>Final Thoughts: Why Are You Marketing Like a Coward?</strong></h2><p>\\\nYou built something from nothing. You ignored the safe path, told the doubters to shove it, and bet on yourself. You made a move.</p><p>\\\n<strong>So where’d that nerve go?</strong></p><blockquote><p>The second it came time to market, you swapped instinct for “best practices.” </p><p>\\\n  ==Innovation doesn’t come from following rules, and attention won’t be earned through compliance.== </p><p>\\\n  It’s taken. It’s ripped out of the market’s hands by people willing to do what no one else is doing.</p></blockquote><h3>Treat marketing like you treat your product—or get used to being ignored.</h3>","contentLength":8455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Police Union Still Insists NY Misconduct Records Are Secret Despite Court Decisions Saying Otherwise","url":"https://www.techdirt.com/2025/02/14/police-union-still-insists-ny-misconduct-records-are-secret-despite-court-decisions-saying-otherwise/","date":1739559300,"author":"Tim Cushing","guid":305,"unread":true,"content":"<p>When you’re playing with house money, playing one losing hand after another isn’t a sign of tenacity. It’s just a way of signaling you can’t be trusted with the house’s money.</p><p>That’s why appeal after appeal from government entities don’t tend to indicate that they’re in the right. It just means they don’t care how much money they spend because it’s not coming from their own pockets. </p><p>The same principle applies to police unions. The money they use to litigate comes from the officers they’re supposed to be serving. At some point, you’d think they would experience some fleeting shame against lighting <a href=\"https://www.techdirt.com/2023/04/24/new-york-court-rules-state-police-cant-keep-hiding-its-misconduct-records-from-the-public/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2023/04/24/new-york-court-rules-state-police-cant-keep-hiding-its-misconduct-records-from-the-public/\">their contributors’ money</a> on fire repeatedly, but the sad fact is that most cops represented by New York’s Police Benevolent Association (PBA) are more than happy to keep burning their own money if it means there’s even the most remote chance their past misdeeds won’t come back to haunt them.</p><p>There have been plenty of legal challenges against the repeal of 50-a, the shorthand that refers to the law that — <a href=\"https://www.techdirt.com/2020/06/12/new-york-legislators-dump-law-that-allowed-pds-to-withhold-officers-disciplinary-records/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2020/06/12/new-york-legislators-dump-law-that-allowed-pds-to-withhold-officers-disciplinary-records/\">up until June 2020</a> — gave law enforcement agencies the option to withhold misconduct records requested by members of the public.</p><blockquote><p><em>The Police Benevolent Association of the City of New York asked a seven-judge panel of the New York Court of appeals to apply retroactivity analysis on the question of whether or not state lawmakers intended a confidentiality provision enacted in 1976 to be conferred to police officers as a vested right after it was repealed in 2020.</em></p><p><em>“We’re just stuck, at best, with a very ambiguous record,” Police Benevolent Association lawyer Matthew Daly told the appeals court on Thursday afternoon. “Policy arguments can be made on both sides — there are policy arguments for disclosure, but there’s also policy to protect rights.”</em></p><p><em>Fighting off document requests from the New York Post, the union says the mandate to retroactively make those old records public would infringe vested rights of police officers and other covered employees “who for more than four decades relied on the statutory confidentiality in deciding how to respond to disciplinary matters.”</em></p></blockquote><p>PBA attorney Matthew Daly is, at best, being disingenuous here. The PBA is asking this court to basically codify something that isn’t present in the law: a limitation that prevents disclosure of records generated before the law’s repeal in 2020. </p><p>As the court <a href=\"https://www.techdirt.com/2023/12/08/ny-appeals-court-says-law-enforcement-cant-withhold-misconduct-records-containing-only-unproven-allegations/\" data-type=\"link\" data-id=\"https://www.techdirt.com/2023/12/08/ny-appeals-court-says-law-enforcement-cant-withhold-misconduct-records-containing-only-unproven-allegations/\">pointed out in late 2023</a>, there’s nothing in the law that forbids access to records created prior to this repeal. </p><blockquote><p><em>By their nature, FOIL requests seek records that were generated prior to the request date. In amending the Public Officers Law to provide for the disclosure of records relating to law enforcement disciplinary proceedings, </em><strong><em>the Legislature did not limit disclosure under FOIL to records generated after June 12, 2020, and we will not impose such a limitation ourselves</em>. </strong></p></blockquote><p>Yet, the PBA persists, guided by little more than its access to other people’s money and a burning desire to progress the rot in its barrel of apples by pretending there’s no way the legislature intended to make this law retroactive. It’s a stupid point to make, as NY Post attorney, Jeremy Chase, told the court:</p><blockquote><p><em>“The Legislature, if they wanted to carve out this period from 1976 to June 2020, they easily could have done that, they didn’t do that,” he said.</em></p></blockquote><p>This leaves the PBA as the last bulwark against… um… serving the public trust. If the PBA (or the NYPD officers it represents) actually cared about rebuilding trust and setting it back on the path towards earning the nickname “New York’s Finest,” it wouldn’t be blowing cash in court trying to keep its dirty laundry buried under empty body cam boxes in the back of the metaphorical closet. Instead, it has chosen to spend nearly a half-decade fighting this small move towards greater transparency despite having lost at every judicial level to this point. </p>","contentLength":3931,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Daily Deal: Linux/UNIX Training Bundle","url":"https://www.techdirt.com/2025/02/14/daily-deal-linux-unix-training-bundle/","date":1739558729,"author":"Daily Deal","guid":304,"unread":true,"content":"<p>Linux and UNIX operating systems have become increasingly popular in commercial computing environments. Due to their rapid growth in today’s businesses, Linux/UNIX administrators have also become very much in demand. This <a href=\"https://deals.techdirt.com/sales/linux-certification-training-bundle?utm_campaign=affiliaterundown\">Linux/UNIX Training Bundle</a> will help you learn the knowledge and skills to install, configure, &amp; support a Linux/UNIX server, and more. It’s on sale for $50.</p><p><em>Note: The Techdirt Deals Store is powered and curated by StackCommerce. A portion of all sales from Techdirt Deals helps support Techdirt. The products featured do not reflect endorsements by our editorial team.</em></p>","contentLength":594,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dynamic Triple Buffering Merged For GNOME 48","url":"https://www.phoronix.com/news/GNOME-48-Triple-Buffering","date":1739558110,"author":"Michael Larabel","guid":362,"unread":true,"content":"<article>As quite a Valentine's Day treat, the long-in-development dynamic triple buffering support for GNOME's Mutter compositor was just merged ahead of next month's GNOME 48 desktop release!..</article>","contentLength":186,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jay Xiao on the Power of Technology and Surety Through SuretyNow","url":"https://hackernoon.com/jay-xiao-on-the-power-of-technology-and-surety-through-suretynow?source=rss","date":1739557864,"author":"Jon Stojan Journalist","guid":98,"unread":true,"content":"<p>\\\n has always believed in the power of technology to improve people's lives. As a millennial, Xiao witnessed life before the internet as we know it and how much easier things became after it became more accessible to everyday people. He is now the co-founder and president of SuretyNow, a company that works to help protect the public from dishonest businesses.</p><h3><strong>A Heart and Talent for Tech</strong></h3><p>\\\nAfter graduating high school, Xiao attended Queen's University in Canada for commerce and computer science. Though his original goal was to go into consulting and finance, he made a decision to switch fully into tech after an unfulfilling internship at an asset management firm his first summer. During this time, he was balancing the internship while working on a startup with friends, eventually leading him to leave the asset management internship behind to dedicate his time to that startup.</p><p>\\\nHowever, Xiao did not know how to code, so he began taking computer science classes to expand his horizons. He interned at a Toronto startup called Nuology and then parlayed that internship into full-time roles at Google, where he worked on the Google Ads and Google Stadia teams.</p><h2><strong>Making an Impact with the Biggest Names in Tech</strong></h2><p>Breaking into the tech field was difficult for Xiao, partly because he was new to the industry and didn't go to a school with a strong history of tech company recruitment. As a result, he worked to better himself in technology through education and went from zero tech experience to interning with Google within three years. At Google Stadia, he helped build Stadia's publisher analytics platform from zero to launch. He helped facilitate an analytics platform used by around 30 publishers at its peak, including Ubisoft and Electronic Arts.</p><p>As the driving force behind <a href=\"https://suretynow.com/\">SuretyNow</a>, Xiao's role extends far beyond the title of co-founder and president. He single-handedly built the company's website, developed the internal tooling and infrastructure, and now manages multiple teams. His strategic vision and hands-on approach have been instrumental in the company's success, making it a prime example of his entrepreneurial prowess.</p><p>\\\nAt SuretyNow, Xiao says that their agents can produce two times the industry average due to the automation they've built to make them more efficient. SuretyNow serves businesses who wish to buy surety to work like other modern tools they love, all with fast and affordable customer service. With exceptional customer service and a commitment to innovation through technology, they aim to educate and deliver peace of mind with their expertise in surety.</p><p>\\\nXiao's future goals for SuretyNow are simple: to lead and inspire others by hiring and mentoring interns. Ultimately, Xiao wants to make buying surety bonds and insurance as easy as purchasing a keychain on Amazon—a fact that can be difficult for some and challenging for others.</p>","contentLength":2882,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Trump Admin Adds Note Rejecting ‘Gender Ideology’ on Sites Court Ordered Them to Restore","url":"https://www.404media.co/trump-admin-adds-note-rejecting-gender-ideology-on-sites-court-ordered-them-to-restore/","date":1739557545,"author":"Emanuel Maiberg","guid":387,"unread":true,"content":"<img src=\"https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-10.31.48@2x.png\" alt=\"Trump Admin Adds Note Rejecting ‘Gender Ideology’ on Sites Court Ordered Them to Restore\"><p>After being forced by a court order to <a href=\"https://www.cbsnews.com/news/judge-orders-hhs-cdc-fda-restore-deleted-webpages-health-information/?ref=404media.co\"></a> about gender and diversity to government websites, the Trump administration has added a note to the top of those pages saying “Any information on this page promoting gender ideology is extremely inaccurate, and disconnected from the immutable biological reality that there are two sexes, male and female.”</p><div><div>Per a court order, HHS is required to restore this website as of February 14, 2025 at 11:59 p.m. Any information on this page promoting gender ideology is extremely inaccurate, and disconnected from the immutable biological reality that there are two sexes, male and female. The Trump Administration rejects gender ideology and condemns the harms it causes to children, by promoting their chemical and surgical mutilation, and to women, by depriving them of their dignity, safety, well-being, and opportunities. This page does not reflect biological reality and therefore the Administration and this Department rejects it.</div></div><p>The note essentially seems like a way for the current administration to legally comply with a court order while still signaling that it entirely rejects any government funded or endorsed research or policy sympathetic to LGBTQ+ community and diversity, equity, and inclusion, which Trump and Elon Musk’s Department of Government Efficiency have been purging from government websites.&nbsp;</p><p>Earlier this week, <a href=\"https://www.404media.co/federal-judge-orders-cdc-fda-to-bring-back-web-pages-before-midnight/\"></a> that a federal judge ordered the Department of Health and Human Services, Centers for Disease Control, and Food and Drug Administration to restore several webpages they removed as a result of Trump’s executive order attacking diversity, equity, and inclusion. The agencies were given until 11:59 p.m. on February 11 to restore the webpages.&nbsp;</p><p>The court ordered the administration to restore the webpages “to their versions as of January 30, 2025, meaning they were supposed to revert the webpages to what they looked like on January 30 with no changes. The versions that have been restored now have this additional disclaimer.</p><p><a href=\"https://storage.courtlistener.com/recap/gov.uscourts.dcd.277069/gov.uscourts.dcd.277069.13.0_1.pdf?ref=404media.co\"><u>A joint status update filed Thursday</u></a> by lawyers for the Department of Justice and the Public Citizen Litigation Group says that the government has provided the court with a list of websites that it has restored, though the list of websites is not available. It also specifically says that the government is refusing to restore the website reproductiverights.gov: “Defendants have objected to restoring the website ‘reproductiverights.gov.’ Plaintiff’s counsel is conferring with their client,” it says.</p><p>“Plaintiff’s lists include websites from Department of Health and Human Services (HHS) components other than the Centers for Disease Control and Prevention and the Food and Drug Administration. The parties disagree about whether such websites properly fall within the scope of the Order. However, given Plaintiff’s forthcoming amended complaint and to avoid further emergency motions practice, Defendants will restore those websites consistent with the Order,” it adds.</p>","contentLength":2996,"flags":null,"enclosureUrl":"https://www.404media.co/content/images/2025/02/CleanShot-2025-02-14-at-10.31.48@2x.png","enclosureMime":"","commentsUrl":null},{"title":"DOGE’s ‘Genius’ Coders Launch Website So Full Of Holes, Anyone Can Write To It","url":"https://www.techdirt.com/2025/02/14/doges-genius-coders-launch-website-so-full-of-holes-anyone-can-write-to-it/","date":1739553900,"author":"Mike Masnick","guid":303,"unread":true,"content":"<p>If you want to write something on the U.S. government’s official DOGE website, apparently you can just… do that. Not in the usual way of submitting comments through a form, mind you, but by directly injecting content into their database. This seems suboptimal.</p><p>The story here is that DOGE — Elon Musk’s collection of supposed coding “geniuses” brought in to “disrupt” government inefficiency — finally launched their official website. And what they delivered is a masterclass in how not to build government infrastructure. One possibility is that they’re brilliant disruptors breaking all the rules to make things better. Another possibility is that they have no idea what they’re doing.</p><p>The latter seems a lot more likely.</p><p>Last week, it was reported that the <a href=\"https://www.npr.org/2025/02/06/nx-s1-5289337/elon-musk-doge-treasury\">proud racist</a> 25-year-old Marko Elez had been given admin access and was <a href=\"https://www.techdirt.com/2025/02/05/a-25-year-old-is-writing-backdoors-into-the-treasurys-6-trillion-payment-system-what-could-possibly-go-wrong/\">pushing untested code</a> to the US government’s $6 trillion/year payment system. While the Treasury Department initially claimed (including in court filings!) that Elez had “read-only” access, others reported he had write access. After those reports came out, the Treasury Dept. “corrected” itself and said Elez had been <a href=\"https://www.zetter-zeroday.com/court-documents-shed-new-light-on-doge-access-and-activity-at-treasury-department/\">“accidentally” given write privileges</a> for the payments database, but only for the data, not the code. Still, they admitted that while they had put in place some security protections, it’s possible that Elez did copy some private data which “may have occasionally included screenshots of payment systems data or records.”</p><p>Now, you might think that having a racist twenty-something with admin access to trillion-dollar payment systems would concern people. But Musk’s defenders had a compelling counterargument: he must be a genius! Because… well, because Musk hired him, and Musk only hires geniuses. Or so we’re told.</p><p>The DOGE team’s actual coding prowess is turning out to be quite something. First, they decided that government transparency meant <a href=\"https://www.techdirt.com/2025/02/11/musk-promised-government-transparency-doge-delivers-maximum-secrecy/\">hiding everything from FOIA requests</a>. When questioned about this interesting interpretation of “transparency,” Musk explained that actually DOGE was being super transparent by putting everything on their website and ExTwitter account.</p><p>There was just one small problem with this explanation. At the time he said it, the DOGE website looked like this:</p><p>That was it. That was the whole website.</p><p>On Thursday, they finally launched a real website. Sort of. If by “real website” you mean “a collection of already-public information presented in misleading ways by people who don’t seem to understand what they’re looking at.” But that’s not even the interesting part.</p><p>These supposed technical geniuses managed to build what might be the least secure government website in history. Let’s start with something basic: where does the website actually live? According to <a href=\"https://www.wired.com/story/doge-website-is-just-one-big-x-ad/\">Wired</a>, the source code actually tells search engines that ExTwitter, not <a href=\"http://DOGE.gov\">DOGE.gov</a>, is the real home of this government information:</p><blockquote><p><em>A WIRED review of the page’s source code shows that the promotion of Musk’s own platform went deeper than replicating the posts on the homepage. The source code shows that the site’s</em><a href=\"https://moz.com/learn/seo/canonicalization\"></a><a href=\"http://x.com\"></a><a href=\"http://DOGE.gov\"></a></p><p><em>A canonical tag is a snippet of code that tells search engines what the authoritative version of a website is. It is typically used by sites with multiple pages as a search engine optimization tactic, to avoid their search ranking being diluted.</em></p><p><em>In DOGE’s case, however, the code is informing search engines that when people search for content found on</em><a href=\"http://DOGE.gov\"></a><em>, they should not show those pages in search results, but should instead display the posts on X.</em></p><p><em>“It is promoting the X account as the main source, with the website secondary,” Declan Chidlow,</em><a href=\"https://vale.rocks/\"></a><em>, tells WIRED. “This isn’t usually how things are handled, and it indicates that the X account is taking priority over the actual website itself.”</em></p></blockquote><p>If you’re not a web developer, here’s what that means: When you build a website, you can tell search engines “hey, if you find copies of this content elsewhere, this version here is the real one.” It’s like telling Google “if someone copied my site, mine is the original.”</p><p>But DOGE did the opposite. They told search engines “actually, ExTwitter has the real version of this government information. Our government website is just a copy.” Which is… an interesting choice for a federal agency? It’s a bit like the Treasury Department saying “don’t look at our official reports, just check Elon’s tweets.”</p><p>You might think that a government agency directing people away from its official website and toward the private company of its leader would raise some conflict-of-interest concerns. And you’d be right!</p><p>But wait, it gets better. Or worse. Actually, yeah, it’s worse.</p><p>Who built this government website? Through some sloppy coding, security researcher Sam Curry <a href=\"https://x.com/samwcyo/status/1889527715029557607\">figured out</a> it was DOGE employee Kyle Shutt. The same Kyle Shutt who, according to Drop Site News, <a href=\"https://www.dropsitenews.com/p/doge-fema-funding-access-social-security-numbers\">has admin access to the FEMA payments system</a>. The same Kyle Shutt who used the exact same Cloudflare ID to build Musk’s America PAC Trump campaign website. Because why maintain separate secure credentials for government systems and political campaigns when you can just… not do that?</p><p>But the real cherry on top came Thursday when people discovered something amazing about the DOGE site database:  Not “anyone with proper credentials.” Not “anyone who passes security checks.” Just… anyone. As 404 Media <a href=\"https://www.404media.co/anyone-can-push-updates-to-the-doge-gov-website-2/\">reported</a>, if you know basic database operations, you too can be a government website administrator:</p><blockquote><p><em>The <a href=\"http://doge.gov\">doge.gov</a> website that was spun up to track Elon Musk’s cuts to the federal government is insecure and pulls from a database that can be edited by anyone, according to two separate people who found the vulnerability and shared it with 404 Media. One coder added at least two database entries that are visible on the live site and say “<a href=\"https://doge.gov/workforce?orgId=1&amp;ref=404media.co\">this is a joke of a .gov site</a>” and “<a href=\"https://doge.gov/workforce?orgId=7cd300eb-cf3f-47f5-90f1-9e66a8bc8d07&amp;ref=404media.co\">THESE ‘EXPERTS’ LEFT THEIR DATABASE OPEN -roro</a>.”&nbsp;</em></p></blockquote><p>While I imagine those will be taken down shortly, for now, the insertions are absolutely visible:</p><p>Look, there’s a reason <a href=\"https://www.techdirt.com/2025/02/03/musks-takeover-of-the-governments-computer-systems-needs-to-be-understood-as-a-cyberattack-or-worse/\">we called this whole thing a cyberattack</a>. When someone takes over your computer systems and leaves them wide open to anyone who wants to mess with them, we usually don’t call that “disruption” or “innovation.” We call it a cybersecurity breach.</p><blockquote><p><em>“Feels like it was completely slapped together,” they added. “Tons of errors and details leaked in the page source code.”</em></p><p><em>Both sources said that the way the site is set up suggests that it is not running on government servers.</em></p><p><a href=\"http://doge.gov\"></a><em>has its codebase, probably through GitHub or something,” the other developer who noticed the insecurity said. “They’re deploying the website on Cloudflare Pages from their codebase, and</em><a href=\"http://doge.gov\"></a><em>is a custom domain that their</em><a href=\"http://pages.dev\"></a><em>URL is set to. So rather than having a physical server or even something like Amazon Web Services, they’re deploying using Cloudflare Pages which supports custom domains.”</em></p></blockquote><p>Here’s the thing about government computer systems: They’re under constant attack from foreign adversaries. Yes, they can be inefficient. Yes, they can be bloated. But you know what else they usually are? Not completely exposed to the entire internet. It turns out that some of that inefficient “bureaucracy” involves basic things like “security” and “not letting random people write whatever they want in federal databases.”</p><p>This isn’t some startup where “move fast and break things” is a viable strategy. This is the United States government. And it’s been handed over to people whose main qualification appears to be “posts spicy memes on 4chan.” The implications go far beyond embarrassing database injections — this level of technical negligence in federal systems creates genuine national security concerns. When your “disruption” involves ignoring decades of hard-learned lessons about government systems security, you’re not innovating — you’re inviting disaster.</p>","contentLength":7997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video Friday: PARTNR","url":"https://spectrum.ieee.org/video-friday-partnr","date":1739552404,"author":"Evan Ackerman","guid":77,"unread":true,"content":"<p>Your weekly selection of awesome robot videos</p>","contentLength":45,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjQ5NjM2MS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc0MDk5OTA2Nn0.hMaiDCMfAle2DiX2_S3CZKODrsXEYRxu4BQGU0eByc4/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"Behind the Blog: Backdoors and the Miracle of Wikipedia","url":"https://www.404media.co/behind-the-blog-backdoors-and-the-miracle-of-wikipedia/","date":1739551201,"author":"Samantha Cole","guid":386,"unread":true,"content":"<img src=\"https://www.404media.co/content/images/2025/02/nl2.14--1-.png\" alt=\"Behind the Blog: Backdoors and the Miracle of Wikipedia\"><p><em>This is Behind the Blog, where we share our behind-the-scenes thoughts about how a few of our top stories of the week came together. This week, we discuss Apple's iCloud, Wikipedia is a miracle of humankind, and good soup.</em></p><p>After <a href=\"https://www.404media.co/behind-the-blog-getting-political/\" rel=\"noreferrer\">our relatively unhinged BTBs last week</a>, many of you left extremely nice comments, reached out individually, or otherwise gave us encouragement. You all are the best, and it made us feel very good. Thank you!</p>","contentLength":435,"flags":null,"enclosureUrl":"https://www.404media.co/content/images/2025/02/nl2.14--1-.png","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.15 To Ensure PlayStation 5 Controllers Use The Correct Driver","url":"https://www.phoronix.com/news/Linux-6.15-Ensures-PS5-Driver","date":1739548149,"author":"Michael Larabel","guid":361,"unread":true,"content":"<article>A change queued up by an Amazon engineer ahead of the upcoming Linux 6.15 kernel cycle will ensure that PlayStation 5 controllers on Linux load with the correctly desired driver...</article>","contentLength":180,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Largest Sofa You Can Move Around a Corner","url":"https://www.quantamagazine.org/the-largest-sofa-you-can-move-around-a-corner-20250214/","date":1739543753,"author":"Richard Green","guid":81,"unread":true,"content":"<p>If you’ve ever moved into a new home, then you know how difficult it can be to steer bulky furniture through narrow hallways or around awkward corners. Mathematicians have been trying to solve this problem, too, ever since 1966, when Leo Moser framed it in quantitative terms. Say you want to move a two-dimensional shape — your sofa (disregarding its height) — through an L-shaped hallway.</p>","contentLength":396,"flags":null,"enclosureUrl":"https://www.quantamagazine.org/wp-content/uploads/2025/02/Moving-sofas_crTommy-Parker-Default.webp","enclosureMime":"","commentsUrl":null},{"title":"Fwupd 2.0.6 Adds Support For HPE Gen10/Gen10+ Servers","url":"https://www.phoronix.com/news/Fwupd-2.0.6-Released","date":1739541176,"author":"Michael Larabel","guid":360,"unread":true,"content":"<article>Fwupd 2.0.6 is out today as the newest update to this widely-used open-source solution for system and peripheral device firmware updating under Linux...</article>","contentLength":152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Price Hikes, Enshittification Trigger 700K Customer Losses At Disney+, ESPN+","url":"https://www.techdirt.com/2025/02/14/price-hikes-enshittification-trigger-700k-customer-losses-at-disney-espn/","date":1739539743,"author":"Karl Bode","guid":302,"unread":true,"content":"<p>That has involved chasing pointless “growth of growth’s sake” megamergers and imposing bottomless price hikes and new <a href=\"https://www.techdirt.com/2023/09/22/the-enshittification-of-streaming-continues-as-amazon-starts-charging-prime-video-customers-even-more-money-to-avoid-ads/\">annoying restrictions</a> (like equating password sharing with “piracy”) — all while simultaneously cutting corners on product quality in a bid to give Wall Street that sweet, impossible, unlimited quarterly growth it demands.</p><p>Disney+, Hulu, and ESPN+ (all now owned by the same company thanks to consolidation) all recently raised prices to access <a href=\"https://arstechnica.com/gadgets/2024/08/so-tired-disney-hulu-espn-prices-increase-by-up-to-25-percent-in-october/\">streaming catalogs of deteriorating quality</a>. Some of Disney’s price hikes were as much as 25 percent, hitting ad-based and ad-free versions alike. Customers were <a href=\"https://arstechnica.com/gadgets/2024/08/so-tired-disney-hulu-espn-prices-increase-by-up-to-25-percent-in-october/\">quick to complain</a>.</p><p>So not surprisingly, Disney+ has now seen the first quarterly subscriber loss in the streaming platform’s history, with <a href=\"https://www.fastcompany.com/91273357/disney-plus-subscriber-decline-price-hikes-earnings-q1-2025\">700,000 customers cancelling service</a>. ESPN+, ESPN’s streaming service, also saw a 700,000 subscriber loss:</p><blockquote><p><em>“Total paid Disney+ subscriptions currently rest at 124.6 million compared with 125.3 million at the end of the fiscal fourth quarter. ESPN+ also saw a loss of 700,000 subscribers, currently at 24.9 million, compared with 25.6 million at the end of last quarter.”</em></p></blockquote><p>Publicly-traded companies can’t just provide a quality, affordable service people like. That’s simply not allowed. </p><p>They have to provide Wall Street ever-escalating quarterly returns in the pursuit of scale, even if that pursuit proves disastrous. If it’s not possible to achieve those returns through innovation and subscriber growth (which is no longer possible now that the streaming market is saturated), that’s when big companies get in trouble and start creatively nickel-and-diming their user base.</p><p>Traditional cable TV, of course, went through this exact life cycle. And despite the fact many of those executives have shifted over to streaming, they’ve learned nothing from history or experience because they’re not financially incentivized to learn from experience. They’re incentivized to make stock values climb at any cost, then flee when things get rough; fat executive or investor compensation in hand.</p><p>Which is to say don’t expect things to change, even if the economy tightens and customers increasingly balk at higher streaming video prices.</p><p>Two, I suspect companies will work tirelessly to make cancelling streaming services (a major advantage over traditional bloated cable TV) more difficult, whether that means complicated wireless/broadband bundling that makes dumping services a confusing hassle (is your Hulu subscription discount tied to your Amazon or wireless bill?), or some creative new restrictions we haven’t seen previously.</p>","contentLength":2623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["tech"]}