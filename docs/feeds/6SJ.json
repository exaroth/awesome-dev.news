{"id":"6SJ","title":"Go","displayTitle":"Go","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":49,"items":[{"title":"🚀 From Manual Builds to Multi-Platform Magic: How GoReleaser Transformed My OpenTelemetry Sandbox","url":"https://dev.to/akshitzatakia/from-manual-builds-to-multi-platform-magic-how-goreleaser-transformed-my-opentelemetry-sandbox-h36","date":1755928179,"author":"Akshit Zatakia","guid":237208,"unread":true,"content":"<p>Ever spent hours wrestling with manual builds, creating release archives by hand, and maintaining complex CI/CD pipelines just to ship your Go application? I did too, until I discovered GoReleaser. Let me show you how it transformed my <a href=\"https://github.com/Akshit-Zatakia/otel-sandbox\" rel=\"noopener noreferrer\">otel-sandbox</a> project from a maintenance nightmare into a one-command release machine.</p><h2>\n  \n  \n  The Problem: Release Hell 😤\n</h2><p>My  project needed to support multiple platforms - developers use Linux, macOS (both Intel and Apple Silicon), and Windows. My original GitHub workflow was a monster:</p><ul><li>130+ lines of complex matrix builds</li><li>Manual archive creation for each platform</li><li>Inconsistent naming across releases</li><li>Missing Windows support (oops!)</li><li>No checksums or verification</li></ul><p>Every release meant babysitting the CI pipeline and praying nothing broke.</p><h2>\n  \n  \n  Enter GoReleaser: The Game Changer 🎯\n</h2><p>GoReleaser promised to replace all this complexity with a single configuration file. Skeptical but desperate, I gave it a shot.</p><p><strong>Before (GitHub Actions only):</strong></p><div><pre><code></code></pre></div><p><strong>After (GoReleaser + GitHub Actions):</strong></p><div><pre><code></code></pre></div><h2>\n  \n  \n  Real-World Success Stories 🌟\n</h2><p><strong>1. Hugo - Static Site Generator</strong>\nChallenge: Hugo needed to support 20+ platforms including exotic architectures. Solution: GoReleaser builds for Linux, Windows, macOS, FreeBSD, OpenBSD across amd64, 386, ARM variants. Result: Single goreleaser release creates 40+ platform-specific binaries.</p><div><pre><code></code></pre></div><p><strong>2. Terraform - Infrastructure as Code</strong>\nChallenge: Enterprise users across diverse cloud environments and local machines. Solution: GoReleaser + HashiCorp's signing infrastructure. Result: Secure, verified releases for 15+ platforms with GPG signatures.</p><p><strong>3. Kubernetes CLI Tools (kubectl, helm)</strong>\nChallenge: Developers need consistent tooling across laptop, CI, and production environments. Solution: GoReleaser ensures identical behavior across all platforms. Result: \"Works on my machine\" becomes \"works everywhere.\"</p><p><strong>4. Prometheus Node Exporter</strong>\nChallenge: Monitor diverse server architectures (x86, ARM, MIPS). Solution: GoReleaser builds for embedded systems, servers, and containers. Result: Single monitoring solution across entire infrastructure.</p><p>\nChallenge: Container orchestration across development and production environments. Solution: GoReleaser creates consistent CLI experience everywhere. Result: Seamless Docker experience from laptop to datacenter.</p><h2>\n  \n  \n  My GoReleaser Configuration\n</h2><div><pre><code></code></pre></div><p>What GoReleaser generates for each release:</p><ul><li>otel-sandbox_Linux_x86_64.tar.gz</li><li>otel-sandbox_Linux_arm64.tar.gz</li><li>otel-sandbox_Darwin_x86_64.tar.gz (macOS Intel)</li><li>otel-sandbox_Darwin_arm64.tar.gz (macOS Apple Silicon)</li><li>otel-sandbox_Windows_x86_64.zip</li><li>otel-sandbox_Windows_arm64.zip</li><li>checksums.txt (SHA256 verification)</li></ul><h2>\n  \n  \n  Advanced Real-World Patterns\n</h2><div><pre><code></code></pre></div><p>Used by: Kubernetes (kubectl, kubeadm, kubelet), Istio (istioctl, pilot)</p><div><pre><code></code></pre></div><p>Used by: Prometheus, Grafana, Jaeger</p><div><pre><code></code></pre></div><p>Used by: Hugo, Terraform, kubectl</p><p><strong>Real project comparisons:</strong></p><div><table><thead><tr></tr></thead><tbody><tr><td>45min manual builds6 platforms</td><td>8min automated40+ platforms</td></tr><tr><td>Complex matrix builds15+ platforms</td><td>One-command release15+ platforms</td></tr><tr><td>Platform-specific CIManual archives</td><td>Unified build processAuto-generated archives</td></tr><tr><td>Docker-only releasesLimited platforms</td><td>Multi-platform binaries15+ platforms</td></tr><tr><td>130 lines CI config6 platforms</td><td>30 lines total8 platforms</td></tr><tr><td>Separate build scriptsDocker-focused</td><td>Unified GoReleaserBinaries + Docker</td></tr><tr><td>Multi-repo complexityPlatform inconsistencies</td><td>Single-repo buildsConsistent across platforms</td></tr></tbody></table></div><h2>\n  \n  \n  The Developer Experience Win 🎉\n</h2><ul><li>Debug platform-specific issues</li><li>Upload artifacts one by one</li></ul><ul><li><code>git push origin v1.0.0-release</code></li><li>✅ Complete release with all platforms ready</li></ul><h2>\n  \n  \n  Getting Started in 5 Minutes\n</h2><div><pre><code>goreleaser release </code></pre></div><div><pre><code></code></pre></div><p>GoReleaser didn't just simplify my releases - it transformed how I think about distribution. Instead of dreading release day, I now ship with confidence, knowing that every platform gets the same quality experience.</p><p>The numbers speak for themselves:</p><ul><li>Hugo: Powers 100k+ websites with zero-friction updates</li><li>Terraform: Trusted by enterprises for infrastructure automation</li><li>Kubernetes tools: Enable container orchestration at global scale</li><li>My otel-sandbox: Reduced CI complexity by 75%, added Windows support effortlessly</li></ul><p>If you're maintaining a Go project and still doing manual releases, you're missing out. GoReleaser isn't just a tool - it's a productivity multiplier that lets you focus on what matters: building great software.</p>","contentLength":4288,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go Beyond Viper and Cobra: Declarative Field-Driven Configuration for Go Apps","url":"https://dev.to/lucasdecamargo/go-beyond-viper-and-cobra-declarative-field-driven-configuration-for-go-apps-4k4a","date":1755915177,"author":"Lucas de Camargo","guid":237178,"unread":true,"content":"<p>Production Go applications constantly require the introduction of new configuration parameters. Based on the Open-Closed Principle, once we define a strategy for managing configuration fields, introducing new values becomes only small extensions. In this article, I'm proposing the definition of a Field structure for declaring configuration settings that are easily integrated with CLI completions and documentation generation.</p><ol><li><p>: A complete configuration solution for Go applications that handles multiple file formats (YAML, JSON, TOML, HCL, ENV), environment variables, and provides a unified interface for accessing configuration values. Viper acts as a registry for all your application's configuration needs, with automatic environment variable binding and a precedence system for value resolution.</p></li><li><p>: A powerful CLI framework that provides a simple interface to create modern command-line applications with sophisticated features like nested subcommands, POSIX-compliant flags, automatic help generation, and shell completions. It's the same framework used by Kubernetes, Docker, and GitHub CLI for building their command-line interfaces.</p></li><li><p>: A struct and field validation library that enables validation through struct tags, custom validation functions, and cross-field validation. It provides a declarative way to define validation rules directly in your struct definitions, making it easy to ensure data integrity throughout your application.</p></li></ol><ul><li>\nRequirements \n\n<ul></ul></li><li>\n\nImplementation \n\n<ul><li>Field-Driven Configuration </li><li>\n\nReal Field Definitions \n\n<ul></ul></li><li>Writing the CLI only once with Cobra </li><li>\n\nThe Root Command \n\n</li><li>\n\nThe Config Commands \n\n<ul><li>Listing Configuration Values </li><li>Setting Configuration Values </li><li>Auto-Documentation: Describing Parameters </li></ul></li></ul></li><li>\n\nConclusion \n\n<ul></ul></li></ul><p>Modern production applications need robust configuration management that can adapt to different environments, validate inputs, and provide clear documentation to users. The approach presented here addresses these needs by creating a unified system where configuration metadata lives alongside the configuration values themselves. This creates a single source of truth that eliminates duplication and reduces the chance of documentation drift.</p><p>: Configuration fields are defined once with all metadata (validation, documentation, defaults); and can be easily extended.</p><p><strong>Default Value Definition with Build Flags</strong>: Production apps are often built for different environments, therefore some default configuration values must be defined by Go .</p><p>: Viper support for YAML, JSON, TOML, HCL, and ENV configuration files.</p><p>: Seamless integration with Cobra for command-line interfaces, data validation, and auto-completion.</p><p>: Strongly typed configuration with validation by Go Validator tags, custom functions, and literals.</p><p>: Automatic binding with configurable prefix.</p><p>: Built-in help and documentation generation.</p><p>Our application is called , and it uses two groups of configuration: application base parameters, like logging and updates, and network configuration, like proxies. The architecture demonstrates how to organize configuration fields into logical groups, making it easy for users to understand and manage related settings together. Each field carries its complete metadata, ensuring that validation rules, documentation, and defaults are always consistent across the entire application.</p><p>Users are expected to use the CLI to configure the application, like:</p><div><pre><code>confapp config .level debug .auto </code></pre></div><div><table><thead><tr></tr></thead><tbody><tr><td>Application environment (hidden)</td></tr><tr><td>Logging level (debug, info, warn, error)</td></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr></tbody></table></div><p>The implementation follows a modular approach where each component has a specific responsibility. The configuration module defines the fields and their metadata, Viper handles the actual storage and retrieval of values, and Cobra provides the user interface. This separation of concerns makes the system maintainable and allows each component to evolve independently while maintaining a stable interface between them.</p><p>The project structure reflects the separation between CLI commands and configuration logic. The  directory contains all CLI-related code, while the  directory houses the configuration management core. This organization makes it clear where to find specific functionality and ensures that the configuration logic remains independent of the CLI implementation.</p><div><pre><code>go-appconfig-example/\n├── cmd/                   # CLI command implementations\n│   ├── root.go            # Root command and global flags\n│   └── config.go          # Configuration management commands\n├── internal/\n│   ├── config/            # Configuration management core\n│   │   ├── fields.go      # Field definitions and collections\n│   │   ├── config.go      # Viper integration\n│   │   └── validators.go  # Custom validation functions\n│   └── consts/            # Application constants\n│       └── consts.go      # Go flags like app name, version, etc.\n├── main.go                # Application entry point\n└── go.mod                 # Go module definition\n</code></pre></div><p>Application constants define global values that remain consistent throughout the application's lifecycle. These can be overridden at build time using Go's  feature, allowing you to customize the application name, version, or other constants without modifying the source code. This is particularly useful for CI/CD pipelines where different builds might need different configurations.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Field-Driven Configuration </h2><p>Field-Driven Configuration is a design pattern where configuration parameters are defined as structured data containing all their metadata. Instead of scattering validation rules, documentation, and default values across different parts of the codebase, each field becomes a self-contained unit that describes everything about a configuration parameter. This approach ensures consistency and makes it trivial to add new configuration options without touching multiple files.</p><p>The Field structure is the cornerstone of our configuration system. It encapsulates not just the value of a configuration parameter, but also its type, validation rules, documentation, and any other metadata needed to work with that parameter. This rich metadata enables automatic generation of CLI flags, validation logic, and documentation, all from a single definition.</p><div><pre><code></code></pre></div><p>The FieldCollection acts as a registry for all configuration fields in your application. It provides methods to add new fields dynamically and retrieve them efficiently. This centralized collection ensures that all parts of the application work with the same field definitions, maintaining consistency across CLI commands, validation, and documentation generation.</p><div><pre><code></code></pre></div><p>With the Field structure and FieldCollection in place, we can now define actual configuration parameters. Each field definition becomes a single source of truth that contains everything needed to work with that configuration value. The use of init() functions ensures that fields are automatically registered when the package is imported, eliminating the need for manual registration and reducing the chance of forgetting to register a field.</p><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Validation is handled through the <a href=\"https://github.com/go-playground/validator\" rel=\"noopener noreferrer\">Go Validator library</a>, which provides a declarative way to define validation rules using struct tags. The library supports a wide range of built-in validators like , , , , , and many more. You can combine multiple validators using commas (AND logic) or pipes (OR logic). For example, <code>validate:\"required,email\"</code> ensures a field is both present and a valid email, while  accepts either RGB or RGBA color formats.</p><p>In our Field structure, we use the ValidateTag field to specify these validation rules, allowing us to leverage the full power of the validator library without writing repetitive validation code.</p><p>Beyond the built-in validators, the system supports custom validation functions for complex business logic that can't be expressed through tags alone. These functions receive the value to validate and return an error if validation fails, providing complete flexibility for domain-specific rules.</p><div><pre><code></code></pre></div><p>Viper provides the backbone for our configuration management system. It handles the complexity of merging configuration from multiple sources according to a well-defined precedence order: command-line flags override environment variables, which override config file values, which override defaults. This layered approach allows users to define base configurations in files while still being able to override specific values through environment variables in production or flags during development. Viper also manages the serialization and deserialization of configuration files in various formats, making it easy to work with YAML, JSON, TOML, or any other supported format without changing your code.</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Writing the CLI only once with Cobra </h2><p>The beauty of our Field-Driven approach truly shines when building the CLI with Cobra. Instead of manually defining flags for each configuration parameter and keeping them in sync with validation rules and documentation, our CLI commands automatically derive everything they need from the Field definitions. This means you write the CLI structure once, and it automatically adapts as you add new configuration fields. The commands can iterate through the FieldCollection to generate flags, completions, and documentation dynamically, ensuring that the CLI always reflects the current state of your configuration schema.</p><p>The root command serves as the entry point for your CLI application. While you can generate the initial structure using  for the root command and  for subcommands, the real power comes from integrating it with our Field system. The root command sets up global flags and initializes the configuration system before any subcommand runs, ensuring that all parts of the application work with properly loaded and validated configuration.</p><div><pre><code></code></pre></div><p>Cobra provides three types of flags: local flags (specific to a command), persistent flags (available to a command and all its subcommands), and the special PFlags type that integrates with Viper. When you bind a flag to Viper using , Viper automatically reads the flag value if it's set, allowing seamless integration between command-line arguments and your configuration system. This binding creates a unified interface where users can set values through flags, environment variables, or config files, and your application code doesn't need to know which source provided the value.</p><p>Our implementation uses persistent flags for global options like the config file path and verbosity level, ensuring these are available to all subcommands. The initialization happens in Cobra's  hook, which runs before any command execution, guaranteeing that configuration is properly loaded before your command logic runs.</p><div><pre><code></code></pre></div><p>The configuration commands provide users with a powerful interface to interact with your application's settings. The beauty of this implementation is that these commands automatically work with any fields you define in your FieldCollection. The  command shows current values,  provides detailed documentation, and  allows modification - all without hardcoding any specific field names. This generic approach means that adding a new configuration field automatically makes it available in all these commands without any additional code changes.</p><div><pre><code></code></pre></div><p>Shell completion is one of Cobra's most powerful features, dramatically improving the user experience by providing intelligent suggestions as users type. The completion system works through  callbacks that receive the current command state and return possible completions. The  parameter contains arguments already provided, while  holds the partial text being typed. The function returns a list of completion suggestions and a directive that controls shell behavior (like whether to also suggest files).</p><p>Our implementation leverages the Field definitions to provide context-aware completions. When users type , the system suggests  as a completion. When setting values with valid options, like , the completion can even suggest the valid values (debug, info, warn, error) defined in the field. This deep integration between the configuration schema and the CLI ensures users always have helpful guidance when interacting with your application.</p><div><pre><code></code></pre></div><h4>\n  \n  \n  Listing Configuration Values </h4><p>The list command provides users with a clear view of their current configuration state. By calculating the maximum field name length, the output is neatly aligned, making it easy to scan through settings. The ability to filter by prefix allows users to focus on specific configuration groups, while the hidden flag reveals internal settings that are normally concealed. This command is essential for debugging and verifying that configuration values are being loaded correctly from all sources.</p><div><pre><code></code></pre></div><h4>\n  \n  \n  Setting Configuration Values </h4><p>The set command demonstrates the power of our Field-Driven approach by automatically creating flags for all configuration fields. When a user runs <code>config set --log.level debug</code>, Cobra parses the flag, our code validates it against the Field definition, and if valid, updates the value through Viper. The command then saves the configuration to disk, ensuring changes persist across application restarts. The validation happens before any values are saved, preventing invalid configurations from being written to disk and ensuring the application always works with valid settings.</p><div><pre><code></code></pre></div><h4>\n  \n  \n  Auto-Documentation: Describing Parameters </h4><p>The describe command showcases how rich metadata in Field definitions enables automatic documentation generation. Each field's description, type, valid values, examples, and extended documentation are displayed in a structured format that helps users understand not just what a setting does, but how to use it effectively. The grouping feature organizes related fields together, making it easier to understand the relationships between different configuration options. This self-documenting nature ensures that documentation always stays in sync with the actual implementation.</p><div><pre><code></code></pre></div><p>The Field-Driven Configuration approach delivers a powerful, user-friendly CLI that adapts automatically as your application evolves. Users benefit from intelligent completions, comprehensive documentation, and robust validation, while developers enjoy a maintainable system where adding new configuration options requires minimal code changes. The integration between Viper and Cobra through our Field abstraction creates a seamless experience where configuration can be managed through files, environment variables, or command-line flags with equal ease.</p><p>The build process leverages Go's  feature to inject build-time values into the application. This allows you to customize constants like application name, version, or even default configuration values without modifying source code. This is particularly useful in CI/CD pipelines where different environments might need different defaults, or when building white-labeled versions of your application.</p><div><pre><code>\ngo build  confapp\n\n\ngo build  myapp\n</code></pre></div><p>Shell completions transform the user experience by providing context-aware suggestions as users type. Once enabled, users can press Tab to see available options, making it easy to discover configuration fields without consulting documentation. The completion system understands the command structure and provides appropriate suggestions based on context, such as showing only valid values for fields with enumerated options.</p><div><pre><code> &lt;./confapp completion zsh &lt;./confapp completion bash</code></pre></div><div><pre><code>\n./confapp config list\n\n\n./confapp config list log\n./confapp config list proxy\n\n\n./confapp config describe\n\n\n./confapp config describe log.level update\n\n\n./confapp config .level debug\n./confapp config .level info .output /var/log/app.log\n\n\n./confapp config list \n./confapp config describe \n./confapp  config list\n</code></pre></div><p>The Field-Driven Configuration pattern presented in this article demonstrates how thoughtful abstraction can transform configuration management from a maintenance burden into a self-maintaining system. By defining configuration fields as first-class entities with rich metadata, we've created a solution that respects the Open-Closed Principle while providing exceptional developer and user experiences.</p><p>The integration of Viper, Cobra, and Go Validator through our Field abstraction eliminates the common pain points of configuration management: keeping documentation in sync, maintaining validation rules, and providing good CLI experiences. The result is a system where adding new configuration options is as simple as defining a new Field struct, and everything else - from CLI flags to validation to documentation - automatically adapts.</p><p>This approach scales elegantly from simple applications with a handful of settings to complex systems with hundreds of configuration parameters organized into logical groups. The automatic generation of completions and documentation ensures that as your application grows, it remains discoverable and user-friendly.</p><p>The architecture presented here provides a solid foundation that can be extended in several ways:</p><p>: Add support for multiple configuration profiles (development, staging, production) by extending the FieldCollection to support profile-specific overrides while maintaining the same validation and documentation capabilities.</p><p>: Implement configuration hot-reloading using Viper's WatchConfig functionality, with the Field definitions providing the validation layer to ensure changes are safe before applying them.</p><p>: Generate OpenAPI specifications or GraphQL schemas from Field definitions, ensuring that API documentation stays synchronized with configuration capabilities.</p><p>: Extend the Field structure to support complex nested configurations while maintaining the same validation and documentation benefits.</p><p>For a complete implementation with additional features like configuration profiles, pager support for long documentation, and more sophisticated validation examples, check out my <a href=\"https://github.com/lucasdecamargo/go-appconfig-example\" rel=\"noopener noreferrer\">GitHub repository</a>. </p><div><div><div><p>A production-ready template for managing configuration parameters in Go applications using  and . This template demonstrates a clean, maintainable approach to configuration management with a single source of truth for all configuration metadata.</p><ul><li>: Configuration fields are defined once with all metadata (validation, documentation, defaults)</li><li>: Strongly typed configuration with validation</li><li>: Seamless integration with Cobra for command-line interfaces</li><li>: Support for YAML, JSON, TOML, HCL, and ENV files</li><li>: Automatic binding with configurable prefix</li><li>: Auto-completion for configuration field names and values</li><li>: Built-in help and documentation generation</li><li>: Multiple validation strategies (tags, custom functions, valid values)</li></ul><div><h3>Core Concept: Field-Driven Configuration</h3></div><p>The central idea is to define configuration fields as structured data that contains everything needed to:</p><ul></ul></div></div></div><p>The repository includes comprehensive examples and can serve as a starting template for your own production applications. Feel free to star the repository if you find it useful, and don't hesitate to open issues or contribute improvements!</p>","contentLength":19025,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Golang Binary Compile arm64","url":"https://dev.to/hardyweb/golang-binary-compile-arm64-38gn","date":1755903762,"author":"hardyweb","guid":237135,"unread":true,"content":"<div><pre><code>GOOS=linux GOARCH=arm64 go build -o nama_sistem_arm64 main.go\n</code></pre></div><p>Strip debug info (reduce size)</p><div><pre><code>GOOS=linux GOARCH=arm64 go build -ldflags=\"-s -w\" -o nama_sistem_arm64 main.go\n</code></pre></div><div><pre><code>GOOS=linux GOARCH=arm64 go build -ldflags=\"-s -w\" -trimpath -o nama_sistem_arm64 main.go\n</code></pre></div><p>Reproducible build (consistent hash)</p><div><pre><code>GOOS=linux GOARCH=arm64 go build -ldflags=\"-s -w\" -trimpath -buildvcs=false -o nama_sistem_arm64 main.go\n</code></pre></div><p>Extra: compress with UPX (optional)</p><div><pre><code>upx --best --lzma nama_sistem_arm64\n</code></pre></div>","contentLength":472,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"**Mastering HTTP/2 Server Performance Optimization in Go for High-Traffic Applications**","url":"https://dev.to/aaravjoshi/mastering-http2-server-performance-optimization-in-go-for-high-traffic-applications-14am","date":1755886154,"author":"Aarav Joshi","guid":237088,"unread":true,"content":"<blockquote><p>As a best-selling author, I invite you to explore my books on <a href=\"https://www.amazon.com/stores/Aarav-Joshi/author/B0DQYNVXZ7?ref=ap_rdr&amp;isDramIntegrated=true&amp;shoppingPortalEnabled=true&amp;ccs_id=738636bd-0ca1-4d7b-8efa-481bfc222571\" rel=\"noopener noreferrer\">Amazon</a>. Don't forget to follow me on <a href=\"https://medium.com/@aarav-joshi\" rel=\"noopener noreferrer\">Medium</a> and show your support. Thank you! Your support means the world! </p></blockquote><p>Building high-performance web servers in Go requires understanding modern protocols. HTTP/2 represents a significant leap forward from HTTP/1.x, particularly for applications handling thousands of concurrent connections. The protocol's design addresses many limitations that plagued earlier versions.</p><p>I've spent considerable time optimizing HTTP/2 implementations in production environments. The gains are substantial when you approach it correctly. Connection multiplexing alone can transform how your server handles load.</p><p>Let me walk through a practical implementation that demonstrates key optimization techniques. This code establishes a foundation for high-concurrency HTTP/2 servers in Go.</p><div><pre><code></code></pre></div><p>The foundation starts with proper structure. We need components for connection management, server push capabilities, and performance tracking. Each plays a crucial role in achieving optimal performance.</p><div><pre><code></code></pre></div><p>Connection pooling proves essential for reducing overhead. Establishing new TLS connections remains expensive, so reusing existing connections dramatically improves efficiency.</p><div><pre><code></code></pre></div><p>Server push represents one of HTTP/2's most powerful features. When implemented correctly, it allows proactive resource delivery before clients even request them.</p><div><pre><code></code></pre></div><p>Tracking performance metrics helps identify bottlenecks. Without proper instrumentation, optimizing becomes guesswork rather than data-driven improvement.</p><div><pre><code></code></pre></div><p>Initializing the server requires careful configuration. Setting appropriate limits prevents resource exhaustion while maintaining high throughput.</p><div><pre><code></code></pre></div><p>The request handling logic needs to account for protocol differences. HTTP/2 enables optimizations that simply aren't possible with earlier versions.</p><div><pre><code></code></pre></div><p>HTTP/2-specific handling focuses on three main areas: header compression, server push opportunities, and stream prioritization. Each contributes to overall performance.</p><div><pre><code></code></pre></div><p>Server push implementation requires careful consideration. Pushing unnecessary resources can actually harm performance rather than help.</p><div><pre><code></code></pre></div><p>Stream prioritization allows more important requests to receive resources first. This proves particularly valuable under heavy load conditions.</p><div><pre><code></code></pre></div><p>Caching pushable resources ensures they're readily available when opportunities arise. The cache should be populated during server initialization.</p><div><pre><code></code></pre></div><p>Connection management forms the heart of HTTP/2 optimization. Smart pooling strategies prevent connection churn while maintaining performance.</p><div><pre><code></code></pre></div><p>Regular cleanup prevents memory leaks from accumulated idle connections. The cleanup frequency should balance resource usage with connection establishment costs.</p><div><pre><code></code></pre></div><p>Monitoring performance provides insights for further optimization. The metrics collected help identify patterns and potential improvements.</p><div><pre><code></code></pre></div><p>The main function ties everything together. Proper TLS configuration is essential for HTTP/2, as most browsers require encrypted connections.</p><div><pre><code></code></pre></div><p>Connection multiplexing stands as HTTP/2's most significant advantage. Where HTTP/1.x required multiple connections for parallel requests, HTTP/2 handles everything over a single connection. This reduces TCP and TLS overhead substantially.</p><p>In practice, I've seen connection counts drop from six per client to just one. The resource savings compound quickly at scale. Memory usage decreases, CPU load reduces, and network efficiency improves.</p><p>Header compression using HPACK delivers impressive gains. Traditional HTTP headers often consumed 2KB or more per request. HPACK typically reduces this to under 200 bytes. The savings become enormous at high request volumes.</p><p>The compression works through static and dynamic tables. Common headers get referenced from tables rather than retransmitted. Huffman encoding further reduces size for variable values.</p><p>Server push requires thoughtful implementation. The feature allows sending responses before clients request them. For critical resources like CSS or JavaScript, this can eliminate round trips.</p><p>But push too much, and you waste bandwidth. Push the wrong things, and you hinder performance. I typically push only resources with high certainty of being needed.</p><p>Stream prioritization enables quality of service controls. Important requests can receive preferential treatment during resource contention. The protocol supports complex dependency trees and weight-based allocation.</p><p>In real applications, I prioritize user-interactive requests over background tasks. API calls affecting user experience get resources before analytics pings or prefetch requests.</p><p>Connection management deserves particular attention. HTTP/2 connections are valuable resources. Pooling and reuse prevent expensive renegotiation of TLS sessions.</p><p>I implement aggressive connection reuse where appropriate. The pool maintains connections to various endpoints, ready for immediate use. Cleanup routines remove idle connections to conserve resources.</p><p>Performance monitoring provides crucial insights. Without metrics, optimization efforts operate blindly. I track active streams, pushed resources, header savings, and connection reuse rates.</p><p>These metrics help identify bottlenecks. If active streams consistently hit limits, perhaps the maximum needs adjustment. If push failures increase, maybe the strategy requires revision.</p><p>Flow control tuning affects overall throughput. HTTP/2 includes window-based flow control at both connection and stream levels. Proper tuning prevents starvation while maintaining fairness.</p><p>I typically start with conservative window sizes and adjust based on observed performance. The optimal values depend on network characteristics and application behavior.</p><p>Error handling requires special consideration in HTTP/2. The protocol includes various error codes and reset mechanisms. Proper handling maintains stability during network issues or client problems.</p><p>I implement comprehensive logging for stream resets and connection errors. This helps identify patterns and address underlying issues.</p><p>Protocol upgrade handling maintains compatibility. While HTTP/2 excels, not all clients support it. The server should gracefully handle HTTP/1.x connections when necessary.</p><p>In my implementation, I check the protocol version and handle appropriately. This ensures broad compatibility while providing modern features where available.</p><p>TLS configuration significantly impacts performance. HTTP/2 requires specific cipher suites and protocol versions. Modern, efficient settings improve both security and speed.</p><p>I prefer TLS 1.3 where possible for improved performance. The reduced handshake latency benefits HTTP/2's connection reuse model.</p><p>Resource management prevents denial of service attacks. HTTP/2's multiplexing capability means a single connection can make many requests. Limits prevent resource exhaustion.</p><p>I set reasonable limits on concurrent streams and request rates. These protect the server while still allowing high performance for legitimate traffic.</p><p>The implementation demonstrates practical application of HTTP/2 features. The code provides a foundation that can be extended for specific use cases. Each component addresses particular aspects of protocol optimization.</p><p>Through careful implementation and continuous refinement, HTTP/2 can deliver substantial performance improvements. The protocol represents a meaningful step forward in web technology.</p><p>The approach reduces latency while increasing throughput. Connection multiplexing cuts resource usage significantly for high-concurrency workloads. Header compression reduces bandwidth requirements. Server push eliminates round trips for critical resources.</p><p>These improvements combine to create faster, more efficient web services. The benefits become increasingly valuable as applications scale to handle more users and traffic.</p><p>Proper HTTP/2 implementation requires understanding both the protocol specifics and the practical considerations of production deployment. The technical capabilities must be balanced with operational requirements.</p><h2>\n  \n  \n  The result is systems that handle more traffic with fewer resources while providing better user experiences. That combination makes the effort worthwhile.\n</h2><p>📘 , , , and  to the channel!</p><p> is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.</p><p>Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !</p><p>Be sure to check out our creations:</p>","contentLength":8649,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Learn Go with 13 Challenges: a practical journey to mastering the language","url":"https://dev.to/kid_goth/learn-go-with-13-challenges-a-practical-journey-to-mastering-the-language-240","date":1755880954,"author":"Brandon Sanchez","guid":236988,"unread":true,"content":"<p>Hi all, I'm a web a mobile developer who loves to code and the real tech challenges. I've wanted to learn Go for a long time, but not in the traditional way, but building specific stuffs that allow me (and us) to learn with purpose. That is how this project born: <strong>Learn Go with 13 Challenges</strong>.</p><h2>\n  \n  \n  🧩 ¿What is this project about?\n</h2><p>This is a practical journey through Go, focused in not only read documentation without stop, but <strong>solve little and powerful challenges</strong>. Each challenge is a mini-project desgined to introduce ann confidence key concepts of the language, from the most basic to the advanced things.</p><p>✔ Each challenge is already prepared with with their respective tests (using TDD-type approach)\n✔ In each post we will to write the necessary code to pass all tests cases and in consequence we will develop the mini-project.<p>\n✔ I will explain step by step the rasoning, design, problems and the final solution, without neglecting how to draw on sources of knowledge, (documentation, videos, forums, etc)</p>\n✔ All is in a <a href=\"https://github.com/bssanchez/golang-practice\" rel=\"noopener noreferrer\">public repository</a> and you are free to clone, try, test and improve</p><p>You can follow the progress directly at the GIT repository:</p><p>There you will find the 13 challenges listed by difficulty and organized into sub-directories, with their tests prepared and ready for you to tackle if you want to join in.</p><h2>\n  \n  \n  🗓 How often is it published?\n</h2><p>I will publish each post progressively. I can't give a specific time frame, but I will try to do it weekly. My goal is to do it consistently and sustainably. It's not “Go in 13 weeks” or “Go in 13 months”, but “” — at your pace and mine.</p><p>Considering that each mind learns differently and/or has preferences when following manuals and/or procedures, each delivery will come in two formats:</p><ul><li>📄 A written post like this, explaining the solution step by step</li><li>📹 A YouTube video with the procedure recorded and commented; please note, I am not an expert in videos, but I will try to make sure they are of the highest quality and of a reasonable length for each exercise.</li></ul><h2>\n  \n  \n  🌍 What about the language?\n</h2><p>I'm publishing first in spanish, but I'm planning to post the solution of each challenge in english after the spanish version is published. This way, I can contribute to both the spanish-speaking community and the global community.</p><p>Regarding the videos, I will find a way to provide English subtitles for them.</p><p>Because I firmly believe that learning by solving real problems is the best way to master a language. Because Go has enormous potential for services, CLI, backend tools, APIs, and more. And because building is more fun than memorizing.</p><h2>\n  \n  \n  ✅ What will we see in the challenges?\n</h2><ul></ul><p>Even more ambitious things like:</p><ul></ul><p>Each challenge has something new to offer, and seeks to exploit one (or more) interesting features of the language.</p><p>You can follow me in <a href=\"https://dev.to/kid_goth\">Dev.to</a>, or suscribe to my <a href=\"https://www.youtube.com/@kid_goth\" rel=\"noopener noreferrer\">youtube channel</a> if you want to see the process in video format.</p><p>I am open to suggestions, ideas, improvements, and collaborations. This is a project for learning, sharing, and growing together.</p><p>See you soon for the first challenge: the calculator 🧮</p><p>Come on to learn Go in the best way: building.</p>","contentLength":3177,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Taming GORM & sqlmock: Our Go-To Workflow for Perfect Database Mocks","url":"https://dev.to/crow004/taming-gorm-sqlmock-our-go-to-workflow-for-perfect-database-mocks-ndm","date":1755869080,"author":"crow","guid":236920,"unread":true,"content":"<p>'A simple, iterative workflow to debug sqlmock expectation errors when testing GORM by using its built-in logger to reveal the exact SQL queries.'</p><p>When writing tests for our Go backend, we rely heavily on  to ensure our database logic is solid without hitting a real database. It's a fantastic tool, but it has one strict requirement: your mock expectations must  match the SQL queries your code generates.</p><p>This can get tricky when using an ORM like GORM. GORM is great for productivity, but the SQL it generates under the hood—especially for complex operations like  with multiple index creations—isn't always obvious. We found ourselves in a cycle of \"guess, run, fail, repeat.\"</p><p>So, how do you find out the  SQL GORM is trying to run?</p><p>We've settled on a simple, iterative debugging workflow that turns this guessing game into a straightforward process. Here’s how we do it.</p><h2>\n  \n  \n  The Challenge: Unpredictable SQL\n</h2><p>The core problem is that  fails if the expected query string doesn't match the actual query string. With GORM's , for example, the order in which it decides to create tables and indexes can change as you add new models or even between GORM versions. You might expect  before , but GORM does the opposite, and your test fails with a cryptic message.</p><h2>\n  \n  \n  The Technique: Let the ORM Tell You What It's Doing\n</h2><p>Instead of trying to guess the SQL, we make GORM tell us directly. The key is its built-in logger.</p><p>Here's our step-by-step process:</p><h3>\n  \n  \n  Step 1: Isolate the Failing Test\n</h3><p>Run your tests and find the first  expectation that fails. The error message is your starting point. It will usually say something like:\nError: call to ExecQuery '...' with args [...] was not expected, next expectation is: ...</p><h3>\n  \n  \n  Step 2: Enable Verbose Logging\n</h3><p>In your test setup where you initialize your GORM connection with the mocked SQL connection, temporarily switch the GORM logger to  mode.</p><pre>// In your test file...\nimport \"gorm.io/gorm/logger\"\n\n// ...\n\n// Temporarily change logger.Silent to logger.Info\ngormDB, err := gorm.Open(dialector, &amp;gorm.Config{ Logger: logger.Default.LogMode(logger.Info), })</pre><h3>\n  \n  \n  Step 3: Run the Test and Observe\n</h3><p>Run the single failing test again (e.g., <code>go test -run TestMyFailingTest</code>). Now, look at your console output. Because the logger is in  mode, GORM will print the exact SQL query it's generating, right before  reports the failure.</p><p>The output will look something like this:</p><p><code>[info] /path/to/your/code.go:123 [SQL] CREATE INDEX \"idx_commission_withdrawals_timestamp\" ON \"commission_withdrawals\" (\"timestamp\") ... [error] ExecQuery: could not match actual sql: \"CREATE INDEX...\" with expected regexp \"CREATE INDEX...recipient_address...\"</code></p><h3>\n  \n  \n  Step 4: Copy, Paste, and Adapt\n</h3><p>The \"actual sql\" from the log is the source of truth.</p><ol><li> the SQL query from the log output.</li><li> it into your test file, replacing or reordering the incorrect expectation in  or .</li><li> it for . You might need to escape special characters for the regex matcher (like parentheses  and ).</li></ol><p>Your test will now pass the first expectation and likely fail on the next one in the sequence. That's progress! Just repeat steps 3 and 4 for the new failing expectation until the entire test passes.</p><p>Once the test is green, remember to switch the GORM logger back to  to keep your test logs clean for everyone else.</p><pre>// Change it back for clean test runs\ngormDB, err := gorm.Open(dialector, &amp;gorm.Config{ Logger: logger.Default.LogMode(logger.Silent), })</pre><p>This simple, iterative process has saved us countless hours of frustration. By using the ORM's own logging, we get a definitive answer to \"What query are you  running?\" and can write precise, reliable database tests.</p><p>Hope this helps you in your projects!</p>","contentLength":3699,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Understanding Bootstrapping: How Go’s Compiler Is Written in Go","url":"https://dev.to/mrsa1/understanding-bootstrapping-how-gos-compiler-is-written-in-go-5ann","date":1755860401,"author":"Rad Sarar","guid":236848,"unread":true,"content":"<p>Do you know most of the codes of Go programming language, even its compiler is written with Go? There may be a question in the head after hearing this, \"How is that possible?\" You need another Go compiler to make a Go compiler, right? \"\nIt's a classic \"egg before or chicken before\" kind of question. And the answer lies in a cool computer science concept called Bootstrapping.<p>\nWhat is the Bootstrapping thing?</p>\nIn simple terms, bootstrapping is using a small or simple system to create a bigger and stronger system. Something like lifting yourself up by holding your shoe lace! This is exactly what happened to Go.<p>\nLet's see how it works step by step:</p></p><ol><li>The first compiler was written in another language:\nThe first compiler of Go was not written in Go language. It was written using C language. This C-based compiler had only one job: compiling the Go Source Code into an executable program.</li><li>Written a new compiler with Go:\nAfter that, Go developers wrote the source code for a whole new compiler using Go language.</li><li>The real magic: \nBootstrapping is done in this stage. The Go Team used their old compiler made with C to compile the source code of the new compiler written with Go. So they got a compiler (executable file) made of Go.</li><li>Go Self-Sufficient: \nOnce Go could build his own compiler, there was no need for the old C-based compiler. From now on Go starts building himself.\nMeans, Go 1.4 is used to compile the Go 1.5 version. Used Go 1.5 to compile Go 1.6 again This is how the chain goes.\nSo, next time you hear \"Go is written in Go\", don't be surprised! This bootstrapping method uses not only Go, but also large programming language like C, Rust, Java. This is the sign of maturity or maturity of a language.\nSome questions may arise in your mind as such:\nQ: What is the advantage or profit of this? Wouldn't the compiler made of C be faster?\nAnswer: There are many benefits such as maintenance, contributor productivity, tooling consistency, portability, and feature development speed etc. will be easy. Many common bug of RC can be avoided.\nQ:At the end of the day, then, the base of the Go compiler is in C, right?\nAnswer: A compiler written in C, called the C-Compiler, is used to build the first version of the Go compiler from its Go source code. This process creates an executable file: go_compiler_v1.exe. Next, go_compiler_v1.exe is used to compile a newer version of the Go source code. This creates go_compiler_v2.exe, which can then be used to compile the next version, and so on. Thus, the C-Compiler is no longer needed.\nQ: Isn't it the same for Java?\nAnswer: Many important languages follow the bootstrapping technique, but by no means is it a universal rule for success.</li></ol>","contentLength":2693,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ep2 : Rebuilding Uber's API Gateway","url":"https://dev.to/sahilbaig/ep2-rebuilding-ubers-api-gateway-cea","date":1755854510,"author":"Sahil Baig","guid":236829,"unread":true,"content":"<p>Kubernetes networking is a pain 🛜\nIn Episode 1, I got all the services listed on my Kubernetes cluster , but everything was running inside the cluster as containers. To mimic how Uber’s Gateway API works, I needed to take the gateway outside the cluster.</p><p>🧱 That's when the first challenge hit: service discovery broke. Containers inside the cluster can talk to each other easily, but from the outside, it's a different story. To fix this, I configured RBAC to allow external requests to the cluster. This let me retrieve the services and the pod IPs running them - so far, so good.</p><p>🔐 Then came the next hurdle: these pod IPs are only meaningful inside the cluster. Any requests coming from outside? They can’t reach the pods at all. Right now, I’m exploring whether a service mesh might help route traffic properly, or if there’s another way to bridge this gap. Stay tuned for Episode 3, where I dive into the solution and finally get the external gateway fully functional.</p><p>Also find a brain rot version of architecture diagram of what I am trying to achieve.</p>","contentLength":1073,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mastering Go Garbage Collection: Triggers, Tuning, and Real-World Wins","url":"https://dev.to/jones_charles_ad50858dbc0/mastering-go-garbage-collection-triggers-tuning-and-real-world-wins-2b50","date":1755822599,"author":"Jones Charles","guid":236005,"unread":true,"content":"<h4>\n  \n  \n  Introduction: Why Go’s Garbage Collection Matters\n</h4><p>If you’re building high-performance Go apps—APIs, microservices, or edge computing—Garbage Collection (GC) can be a silent performance killer. Think of GC as a backstage crew cleaning up memory your program no longer needs. But if it’s too aggressive, you get latency spikes; too lax, and you risk memory bloat or crashes.</p><p>This guide is for Go developers with 1-2 years of experience who want to level up. We’ll unpack how Go’s GC triggers, share tuning tips with  and , and dive into real-world examples that slashed latency and boosted throughput. Expect practical code, common pitfalls, and tools like  to make your apps faster and leaner. Let’s tame Go’s GC and make your programs scream!</p><h3>\n  \n  \n  1. Go GC Basics: What’s Happening Under the Hood?\n</h3><p>Go uses a <strong>concurrent mark-and-sweep GC</strong>, cleaning memory while your program runs to minimize pauses (Stop-The-World or STW). Here’s the breakdown:</p><ul><li>: Identifies objects still in use.</li><li>: Frees unused memory.</li><li>: Decides  GC runs, based on heap growth and settings like .</li></ul><p>Since Go 1.5, GC is concurrent, and Go 1.8+ added smarter write barriers, making it ideal for high-concurrency apps like web servers. But without tuning, you might face jittery APIs or crashes in memory-constrained environments like Kubernetes. Let’s explore when GC kicks in.</p><h3>\n  \n  \n  2. When Does GC Run? Understanding Trigger Conditions\n</h3><p>GC triggers aren’t random—they’re driven by specific conditions. Knowing these lets you predict and control GC behavior.</p><h4>\n  \n  \n  2.1 Memory Allocation Trigger (GOGC)\n</h4><p>The primary trigger is heap growth, controlled by the  environment variable (default: 100). GC runs when the heap doubles the live heap (active memory). The formula is:</p><p><strong>next_gc = live_heap * (1 + GOGC/100)</strong></p><p>For a 100MB live heap with , GC triggers at 200MB. Lower  (e.g., 50) increases GC frequency, saving memory but using more CPU. Higher  (e.g., 200) delays GC, boosting throughput but risking memory spikes.</p><div><pre><code></code></pre></div><p>Run with :</p><div><pre><code>$ GODEBUG=gctrace=1 go run main.go\ngc 1 @0.019s 4%: 0.030+1.2+0.010 ms clock, 4-&gt;4-&gt;2 MB\n</code></pre></div><p>This shows GC took 1.2ms, reducing the heap from 4MB to 2MB.</p><p>Since Go 1.9, GC runs every 2 minutes, even with low allocations. This prevents long-running apps (e.g., background workers) from holding memory forever. It’s non-disableable, so plan for it in low-allocation services.</p><p>You can force GC with , but use it sparingly (e.g., batch jobs or debugging). Overuse disrupts the Pacer, spiking CPU.</p><h4>\n  \n  \n  2.4 Real-World Example: Fixing API Latency\n</h4><p>In a high-traffic API, P99 latency hit 300ms due to frequent JSON allocations triggering GC 10 times per second. Using , we confirmed the issue. Bumping  to 150 reduced GC frequency, cutting latency by 20% with a slight memory increase. Small tweaks, big wins.</p><h3>\n  \n  \n  3. Tuning GC: Your Knobs and Levers\n</h3><p>Triggers set  GC runs; parameters control  it behaves. Let’s explore  and .</p><h4>\n  \n  \n  3.1 GOGC: Control the Pace\n</h4><p> dictates GC frequency:</p><ul><li>: Less frequent GC, ideal for high-throughput batch jobs, but uses more memory.</li><li>: More frequent GC, great for low-latency APIs or memory-constrained setups.</li></ul><p>: Start at , then adjust. Try  for APIs,  for batch jobs.</p><div><pre><code></code></pre></div><h4>\n  \n  \n  3.2 GOMEMLIMIT: Set a Memory Cap\n</h4><p>Since Go 1.19,  caps total memory (heap + stack). When nearing the limit, GC runs more often to avoid crashes—perfect for containers.</p><p>: Set  to 80-90% of your container’s memory to account for system overhead.</p><div><pre><code></code></pre></div><p>Run with  to monitor.</p><h4>\n  \n  \n  3.3 Debugging with GODEBUG\n</h4><p> logs GC details:</p><ul></ul><div><pre><code>gc 1 @0.019s 4%: 0.030+1.2+0.010 ms clock, 0.12+0.68/1.1/0.23+0.040 ms cpu, 4-&gt;4-&gt;2 MB\n</code></pre></div><p>Use it to spot excessive GC or memory leaks.</p><h3>\n  \n  \n  4. Code-Level Tricks to Ease GC Pressure\n</h3><p>Tuning parameters is only half the battle—writing GC-friendly code is key to reducing memory allocations and keeping your app fast. Here are four techniques, with code examples, pitfalls, and pro tips to make your Go programs lean.</p><h4>\n  \n  \n  4.1 Reuse Objects with </h4><p>Frequent allocations (e.g., JSON buffers in APIs) trigger GC too often.  lets you reuse objects, slashing allocations. Think of it as a recycling bin for temporary objects.</p><p>: Reusing buffers in a web server.</p><div><pre><code></code></pre></div><p>: Reusing buffers avoids new allocations, cutting GC runs by 30-50% in high-traffic APIs.</p><p>: Forgetting to reset buffers can leak data. Always clear them before returning to the pool.</p><p>: Use  for short-lived objects like buffers or temporary structs, but avoid it for complex, long-lived objects, as the pool may retain them unnecessarily.</p><h4>\n  \n  \n  4.2 Optimize Data Structures\n</h4><p>Poor data structures balloon memory, overworking GC. Two strategies:</p><ul><li>: Dynamic resizing via  doubles memory during growth. Use  to set capacity upfront.</li><li>: Large allocations (e.g., 10MB slices) are tough for GC. Use smaller chunks.</li></ul><p>: Pre-allocating slices for log processing.</p><div><pre><code></code></pre></div><p>: Pre-allocation avoids resizing, reducing GC triggers. In a test with 1M logs, this cut GC runs by 40%.</p><p>: Overestimating capacity wastes memory. Estimate based on typical data sizes.</p><h4>\n  \n  \n  4.3 Use  for String Operations\n</h4><p>String concatenation with  creates new strings, piling up allocations.  builds strings efficiently by growing its internal buffer.</p><p>: Efficient log message construction.</p><div><pre><code></code></pre></div><p>:  minimizes allocations, reducing GC frequency by up to 25% in stream processing apps.</p><p>: Don’t reuse  without calling , especially in loops or pools.</p><h4>\n  \n  \n  4.4 Monitor and Profile Allocations\n</h4><p>Use tools to find and fix allocation hotspots:</p><ul><li>: Profiles memory/CPU usage. Run <code>go tool pprof http://localhost:6060/debug/pprof/heap</code> to analyze.</li><li>: Tracks heap size and GC stats.</li><li>: Monitors production metrics.</li></ul><p>: Checking memory stats.</p><div><pre><code></code></pre></div><p>: Combine , pre-allocation, , and profiling to minimize GC pressure. Let’s see these in action.</p><h3>\n  \n  \n  5. Real-World Wins: GC Tuning in Action\n</h3><p>Here are three real-world scenarios where GC tuning and code optimization transformed performance. Each includes the problem, solutions, code, results, and tools used.</p><h4>\n  \n  \n  5.1 High-Traffic API Service\n</h4><p>: A REST API handling 10,000 QPS had P99 latency spikes of 300ms.  revealed frequent JSON response allocations triggering GC 15 times per second, hogging CPU.</p><ol><li>Increased  from 100 to 150 to reduce GC frequency.</li><li>Used  for JSON buffers.</li><li>Pre-allocated response slices with .</li></ol><div><pre><code></code></pre></div><ul><li>P99 latency dropped from 300ms to 210ms (30% improvement).</li><li>Throughput rose from 5000 to 5750 QPS (15% boost).</li><li>GC frequency fell from 15 to 8 times per second.</li></ul><p>:  identified allocation hotspots; Prometheus+Grafana monitored latency and GC metrics.</p><p>: A bar chart comparing P99 latency and throughput before/after. (Want it? Let me know!)</p><p>: A Go app in a 1GB Kubernetes container crashed with OOM errors during traffic spikes due to uncontrolled heap growth.</p><ol><li>Set  to cap memory, reserving 200MB for system overhead.</li><li>Lowered  to 50 for frequent GC.</li><li>Used  for temporary buffers.</li><li>Monitored with .</li></ol><div><pre><code></code></pre></div><ul><li>Memory stabilized at 650-700MB.</li><li>GC ran 3 times per second with minimal latency impact.</li></ul><p>:  for debugging; Prometheus+Grafana for production monitoring with memory alerts.</p><h4>\n  \n  \n  5.3 Real-Time Stream Processing System\n</h4><p>: A log streaming system had P99.9 latency spikes of 500ms.  showed excessive string concatenation and buffer allocations driving GC 8 times per second.</p><ol><li>Replaced  concatenation with .</li><li>Used  for reusable buffers.</li><li>Set  for balanced GC frequency.</li><li>Set  (on a 4GB system).</li></ol><div><pre><code></code></pre></div><ul><li>P99.9 latency dropped from 500ms to 150ms (70% reduction).</li><li>GC frequency fell from 8 to 3 times per second.</li><li>Memory stabilized below 1.8GB.</li></ul><p>:  pinpointed concatenation issues; Prometheus+Grafana tracked GC and heap metrics with alerts.</p><p>: Combining code optimization (, ) with tuning (, ) and profiling delivers massive gains. Always start with  to find the root cause.</p><h3>\n  \n  \n  6. Wrapping Up: Your GC Toolkit\n</h3><p>Mastering Go’s GC means balancing triggers, tuning parameters, and writing smart code. Here’s your toolkit:</p><ul><li>: Heap growth (), 2-minute timer, or  for special cases.</li><li>:  for frequency,  for memory caps.</li><li>: Use , pre-allocate slices, and .</li><li>: , , Prometheus+Grafana.</li></ul><ol><li>Run with  to baseline GC behavior.</li><li>Use  to find allocation hotspots.</li><li>Test  (50 for latency, 200 for throughput) and  in a staging environment.</li><li>Monitor production with Prometheus and Grafana, setting alerts for memory spikes.</li></ol><p> The Go team is exploring adaptive GC and lower-latency techniques. Stay updated via <a href=\"https://go.dev/blog/\" rel=\"noopener noreferrer\">Go’s blog</a> or join discussions on <a href=\"https://www.reddit.com/r/golang/\" rel=\"noopener noreferrer\">Reddit</a> or <a href=\"https://forum.golangbridge.org/\" rel=\"noopener noreferrer\">Golang Bridge</a>.</p><p> Have you wrestled with Go’s GC? Share your wins, pitfalls, or questions in the comments! If you want a chart for any case study (e.g., API latency improvements), let me know, and I can generate one. Happy coding, and let’s make those Go apps fly!</p>","contentLength":8610,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Day 7: When Protobuf Breaks Everything - Real Engineering in the Trenches","url":"https://dev.to/clayroach/day-7-when-protobuf-breaks-everything-real-engineering-in-the-trenches-co4","date":1755810148,"author":"Clay Roach","guid":235906,"unread":true,"content":"<p>: Add real-time updates and bootstrap AI anomaly detection.: \"Why are all my operations named 'protobuf-fallback-trace'?!\"</p><p>Welcome to Day 7 of building an AI-native observability platform in 30 days. Today was supposed to be about sexy features. Instead, it was about the unglamorous reality of systems engineering: <strong>making protobuf work correctly</strong>.</p><h2>\n  \n  \n  The Problem That Changed Everything\n</h2><p>I started the day confident. The OpenTelemetry demo was running, traces were flowing, the UI was displaying data. Time to add real-time updates, right?</p><p>Then I looked closer at the trace details:</p><div><pre><code></code></pre></div><p>Every. Single. Operation. Was named \"protobuf-fallback-trace\".</p><h3>\n  \n  \n  Discovery #1: Gzip Was Being Ignored\n</h3><p>The OpenTelemetry demo sends protobuf data with gzip compression. My middleware had \"clever\" conditional logic:</p><div><pre><code></code></pre></div><p>The fix was embarrassingly simple:</p><div><pre><code></code></pre></div><p>: Sometimes \"clever\" code is just complicated code. Unified handling often beats conditional logic.</p><h3>\n  \n  \n  Discovery #2: Protobufjs vs ES Modules\n</h3><p>Next challenge: parsing the actual protobuf data. The protobufjs library is CommonJS, but my project uses ES modules. This led to hours of:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Discovery #3: Path Resolution Hell\n</h3><p>Even with protobufjs loading, the OTLP protobuf definitions have imports that need custom resolution:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  The Nuclear Option: Enhanced Fallback Parsing\n</h2><p>When the \"proper\" protobuf parsing kept failing, I built something unconventional - a raw protobuf parser that extracts data through pattern matching:</p><div><pre><code></code></pre></div><p>Is this elegant? No. Does it work? .</p><p>After 8 hours of protobuf wrestling:</p><ul><li>❌ All operations: \"protobuf-fallback-trace\"</li></ul><ul><li>✅ Real operations: , </li><li>✅ 10+ real spans per trace</li><li>✅ Authentic resource attributes and timing data</li></ul><h3>\n  \n  \n  1. <strong>Fallback Strategies Are Not Defeat</strong></h3><p>Building a fallback parser wasn't giving up - it was ensuring the system works even when dependencies fail. In production, .</p><h3>\n  \n  \n  2. <strong>Debug at the Lowest Level</strong></h3><p>I spent hours assuming the protobuf data was corrupt. Finally logging the raw buffer bytes revealed it was fine - the decompression was being skipped.</p><h3>\n  \n  \n  3. <strong>Integration Points Are Where Systems Break</strong></h3><p>The individual components all worked:</p><ul><li>✅ OpenTelemetry demo: sending valid data</li><li>✅ Express server: receiving requests\n</li><li>✅ ClickHouse: storing data</li></ul><p>The failure was in the glue between them.</p><h3>\n  \n  \n  4. <strong>Real Data Reveals Real Problems</strong></h3><p>Mock data would never have exposed this issue. Testing with the actual OpenTelemetry demo forced me to handle real-world complexity.</p><p>Today didn't go according to plan, and that's  what building production systems is like. The glossy demo videos don't show the 8 hours spent debugging why <code>protobuf.load is not a function</code>.</p><p>But here's what matters: <strong>the system now correctly processes thousands of real traces from a production-like demo application</strong>. Every service is visible, every operation is named correctly, and the data flowing through the pipeline is authentic.</p><p>Now that protobuf parsing actually works:</p><ul><li>Implement the real-time updates (for real this time)</li><li>Add WebSocket support for live trace streaming</li><li>Bootstrap the AI anomaly detection system</li><li>Create service dependency visualization</li></ul><h2>\n  \n  \n  Code Snippets That Saved the Day\n</h2><p>For anyone fighting similar battles:</p><div><pre><code>\ndocker compose backend xxd  100 /tmp/trace.pb\n\n\ncurl  POST http://localhost:4319/v1/traces  @trace.pb.gz\n\n\nnode </code></pre></div><p>Day 7 was humbling. The plan was to build flashy features. Instead, I spent the day in the trenches making basic data ingestion work correctly. </p><p>But that's real engineering. It's not always about the elegant algorithm or the clever architecture. Sometimes it's about making protobuf parsing work at 2 AM because your entire platform depends on it.</p><p><strong>The platform is stronger because of today's battles.</strong> And tomorrow, with real data flowing correctly, we can build the features that actually matter.</p><p><em>Are you fighting your own protobuf battles? Share your war stories in the comments. Sometimes knowing you're not alone in the debugging trenches makes all the difference.</em></p><p><strong>Progress: Day 7 of 30 ✅ | Protobuf: Finally Working | Sanity: Questionable</strong></p>","contentLength":4043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🔥 The Secret Edge of TinyGo: Run Go Code on a $2 Microcontroller and Blow Your Mind","url":"https://dev.to/ekwoster/the-secret-edge-of-tinygo-run-go-code-on-a-2-microcontroller-and-blow-your-mind-5djk","date":1755792080,"author":"Yevhen Kozachenko 🇺🇦","guid":235766,"unread":true,"content":"<h2>\n  \n  \n  Introduction: Go Where No Gopher Has Gone Before\n</h2><p>Go (Golang) has revolutionized backend development with its blazing speed, simplicity, and concurrency model. But what if you could run Go code—yes, honest-to-god Go—on a microcontroller that costs less than a cup of coffee? Enter <a href=\"https://tinygo.org/\" rel=\"noopener noreferrer\">TinyGo</a>: a game-changing compiler that brings the power of Go to the world of embedded devices and WebAssembly. </p><p>In this post, we’ll walk through running TinyGo on a $2 RP2040 board (like Raspberry Pi Pico), discuss real-world use cases, and show how it obliterates some traditional embedded programming pain points.</p><p>Ready to see Go run WITHIN 256KB of RAM and redefine embedded programming?</p><p>TinyGo is a Go compiler built on top of LLVM. It enables running Go programs on:</p><ul><li>Microcontrollers with tight resource constraints (as little as 256KB RAM)</li><li>WebAssembly for high-performance frontend code</li></ul><ul><li>Strong type system of Go, perfect for embedded safety</li><li>Goroutines (simplified subset in embedded)</li><li>LLVM backend for highly optimized binaries</li><li>Seamless interfaces with sensors, GPIOs, and I2C/SPI devices</li></ul><p>Typical C/C++ embedded applications involve messy build chains and arcane platform configurations. TinyGo brings structure and sanity back.</p><h2>\n  \n  \n  Testing the Waters — Getting Started with RP2040 and TinyGo 🔧\n</h2><div><pre><code>brew tap tinygo-org/tools\nbrew tinygo\n</code></pre></div><p>Ensure your Go version and TinyGo are installed:</p><div><pre><code>go version     \ntinygo version </code></pre></div><h3>\n  \n  \n  🔧 Step 2: Blink an LED (Hello World for Hardware)\n</h3><p>Let’s create a simple blinking LED example for Raspberry Pi Pico RP2040:</p><div><pre><code></code></pre></div><div><pre><code>tinygo flash pico main.go\n</code></pre></div><p>This will compile your Go code down to a binary that’s burned onto the RP2040 board. Within seconds, your LED is blinking—embedded Go is alive!</p><p>TinyGo converted our Go code to a small, stripped-down binary using LLVM.</p><ul><li>Goroutines are optional (limited stack context)</li></ul><h2>\n  \n  \n  Real Use Case: Read a Temperature Sensor 🌡️\n</h2><p>Let’s connect a common I2C temperature sensor like the BMP280 and read live values.</p><div><pre><code>go get tinygo.org/x/drivers/bmp180\n</code></pre></div><div><pre><code></code></pre></div><p>📝 Note: You may need to connect I2C pins to SDA/SCL of BMP280 (check TinyGo’s board pinouts).</p><h2>\n  \n  \n  Why Use Go in Embedded Work?\n</h2><p>Working with unsafe C pointers is like walking on a minefield. Go’s type system makes embedded dev safer.</p><h3>\n  \n  \n  ✅ Simpler Concurrency (Even on Embedded!)\n</h3><p>TinyGo supports goroutines (with caveats), which means concurrency can be handled more gracefully than typical FreeRTOS tasks in C.</p><p>Want to ship kind-of-universal logic for both Microcontrollers and WebAssembly pipelines? TinyGo supports both.</p><ul><li>Not full Go standard library support</li><li>No full-blown goroutines on MCUs (minimal stacks supported)</li><li>Some peripherals are still a WIP (check GitHub drivers repo)</li><li>Debugging isn't as easy as Go on desktop</li></ul><p>But honestly, the benefits outweigh them for most use cases.</p><h2>\n  \n  \n  Use Cases That Will Blow Your Mind 💥\n</h2><ol><li><ul><li>Collect sensor data, process locally in Go, transmit via LoRA</li></ul></li><li><p>Wearables or Fitness Devices</p><ul><li>Handle BLE, sensors, step counters—all within Go!</li></ul></li><li><ul><li>Run inference or prepare data on microcontrollers, send to server via WASM equivalent logic</li></ul></li><li><ul><li>Build retro games in Go for low-cost devices like Gopher2600</li></ul></li></ol><p>TinyGo is an absolute hidden gem. It doesn’t just run Go on an embedded device—it redefines what's possible. If you're tired of the toothache-inducing build systems of embedded C, or the lack of type safety in Arduino scripts, TinyGo is your savior.</p><p>🔥 If you're building next-gen IoT products, learning embedded systems, or want to tinker with microcontrollers without losing your mind—TinyGo is your secret weapon.</p><p>Go ahead, grab that $2 RP2040 and bring Go into the world of silicon dust!</p><p>Stay Tuned. Next up? Building a WebAssembly-powered dashboard to control your Go-powered embedded nodes. 👩‍🚀</p><p>✍️ Author: [Your Name], Fullstack Dev &amp; Embedded Hobbyist</p><p>🧠 Bonus Tip: Use TinyGo for WebAssembly too—they share runtime code and can even compile the same logic for web and firmware. Mind. Blown.</p>","contentLength":3968,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 From Java to Go in 2025: 6 Steps for a Smooth Start","url":"https://dev.to/aleksei_aleinikov/from-java-to-go-in-2025-6-steps-for-a-smooth-start-23ji","date":1755747689,"author":"Aleksei Aleinikov","guid":235543,"unread":true,"content":"<p>Thinking about switching from Java to Go?</p><p>The biggest wins aren’t fancy frameworks — it’s the everyday differences that change how you design and debug.</p><p>Here are 6 I’ve found most valuable:\n    •🎯 Explicit error handling (not hidden exceptions)<p>\n    •🔌 Interfaces declared at usage, implemented implicitly</p>\n    •🛡️ Constructors to prevent nil‑crashes<p>\n    •📏 Receiver vs nil semantics (know when calls are safe)</p>\n    •🔤 Bytes vs runes vs user text (strings ≠ chars)<p>\n    •✍️ GoFmt is a baseline, naming still matters</p></p>","contentLength":554,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"HTTPS at 80 Gbps? Yes, in Go (2025)","url":"https://dev.to/aleksei_aleinikov/https-at-80-gbps-yes-in-go-2025-hk4","date":1755747631,"author":"Aleksei Aleinikov","guid":235542,"unread":true,"content":"<p>“Encryption is slow, HTTPS can’t be high‑speed.”</p><p>💡 Turns out, the bottleneck isn’t the math — it’s handshakes and memory copies.</p><p>Here’s what I did to make a single 1U Go server push 70–80 Gbps over HTTPS:\n•🚀 Switched to faster handshake signatures (ECC stamp instead of calligraphy)<p>\n•🔑 Enabled cluster‑wide session resumption (no storm of new handshakes)</p>\n•📦 Cut out extra copies — pushed bulk encryption down the stack with zero‑copy I/O</p>","contentLength":477,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🌀 JSON v2 in Go (2025): What Actually Changed","url":"https://dev.to/aleksei_aleinikov/json-v2-in-go-2025-what-actually-changed-5g1a","date":1755747572,"author":"Aleksei Aleinikov","guid":235541,"unread":true,"content":"<p>Go’s new JSON stack landed in 2025 — but what really changed, and do you need to rewrite your code?</p><p>Here’s the short version:</p><p>•✅ Your old  still works (no big migration)\n•⚡ New helpers: ,  for direct I/O\n•📡 Real streaming via \n•🏷️ Smarter field tags (, , )\n•🚀 Faster decoding + stricter defaults (catch bugs early)</p><p>Think of JSON v2 as a tightened toolkit: same foundations, but with better defaults, streaming, and performance.</p>","contentLength":455,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"⚡ Go Arenas: Request‑Scoped Speed in 2025","url":"https://dev.to/aleksei_aleinikov/go-arenas-request-scoped-speed-in-2025-54c3","date":1755747506,"author":"Aleksei Aleinikov","guid":235540,"unread":true,"content":"<p>High‑throughput Go services often choke not on logic, but on allocation churn + GC scans.</p><p>That’s where arenas come in:\n    •Allocate many objects “in bulk”<p>\n    •Free them all at once (end of request/job)</p>\n    •Reduce GC pressure &amp; tail latency</p><p>I share 3 real‑world patterns I use arenas for:\n✅ Parsing request logs without heap trash<p>\n✅ Building graphs then keeping only compact snapshots</p>\n✅ Assembling JSON responses with fewer allocations</p>","contentLength":457,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🍰 Go Slices Finally Explained: Why They Behave the Way They Do","url":"https://dev.to/aleksei_aleinikov/go-slices-finally-explained-why-they-behave-the-way-they-do-4n6j","date":1755747430,"author":"Aleksei Aleinikov","guid":235539,"unread":true,"content":"<p>Ever wondered why appending to one slice suddenly mutates another? Or why your nil vs empty slice checks sometimes bite back?</p><p>In Go, a slice isn’t magic — it’s just a tiny descriptor:\n    •a pointer to data,\n    •and its capacity.<p>\nThat leads to some gotchas (shared arrays, silent reallocations) — but also powerful tricks:</p>\n✅ Safe in‑place compaction<p>\n✅ O(1) element removal (if order doesn’t matter)</p>\n✅ Guaranteed slice isolation with the full slice expression ()</p>","contentLength":483,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What happens inside the computer when you run your Go server","url":"https://dev.to/turjoc120/what-happens-inside-the-computer-when-you-run-your-go-server-165n","date":1755735658,"author":"Turjo Chowdhury","guid":234844,"unread":true,"content":"<p>Before we deep dive, let's learn a couple of important concepts</p><h2>\n  \n  \n  What Are Sockets and File Descriptors?\n</h2><ul><li>Sockets are endpoints for communication between computers over a network, enabling real-time data exchange.</li><li>Unlike regular files, sockets do not store data but facilitate data transfer between machines.</li><li>When Go requests a socket from the operating system (OS), the OS creates the socket and assigns a unique identifier called a file descriptor.</li><li>A file descriptor is an integer handle that the Go server uses to manage and reference the socket.</li><li>This mechanism allows the server to efficiently send and receive network data through OS-managed resources.</li></ul><h2>\n  \n  \n  Go’s Concurrency with Goroutines\n</h2><ul><li>Go uses goroutines, lightweight threads, to handle many client requests concurrently.</li><li>The main goroutine continuously waits for incoming requests.</li><li>For each new request, Go creates a new goroutine to process it independently without blocking the main one.</li><li>This design ensures the server remains fast and scalable, handling multiple clients simultaneously.</li><li>When no requests arrive, the main goroutine sleeps to conserve system resources and improve overall efficiency.</li></ul><h2>\n  \n  \n  Understanding How It Works in Your Computer\n</h2><ul><li>The kernel is the core part of the operating system that manages hardware and processes.</li><li>Network requests first travel through a router and then reach your computer’s Network Interface Card (NIC), like a WiFi adapter or Ethernet port.</li><li>The NIC converts the wireless or wired signals into binary data and temporarily stores it in a buffer.</li><li>It then sends a signal to the kernel to process this new data.</li><li>The kernel copies the data into a socket buffer that the Go server listens to, and marks it ready for reading.</li><li>The Go runtime wakes up the goroutine to read and process the request.</li><li>The server sends the response back through the socket and NIC.</li><li>The response reaches the client’s browser.</li></ul>","contentLength":1902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"# How I Built a Fully Decentralized On-Chain Game with 0 Lines of Code, Thanks to Gemini","url":"https://dev.to/crow004/-how-i-built-a-fully-decentralized-on-chain-game-with-0-lines-of-code-thanks-to-gemini-1d0p","date":1755732264,"author":"crow","guid":234788,"unread":true,"content":"<p>My nickname is crow, and a few months ago, I was an indie developer with what I'd call junior-level skills. Today, I'm the creator of a fully-functional, decentralized, on-chain game called <a href=\"https://muschairs.com/\" rel=\"noopener noreferrer\">Musical Chairs</a>. The twist? I didn't write a single line of the production code myself. <strong>100% of it was generated by Gemini, my AI coding partner integrated into VS Code.</strong></p><p>This isn't just a story about a cool project; it's a story about a new way of building. It's about how a single person with a clear vision can leverage AI to execute complex technical tasks, from writing secure smart contracts to deploying a multi-container production environment.</p><h3>\n  \n  \n  The Idea: Decentralization First\n</h3><p>The concept was simple: take the childhood game of Musical Chairs and bring it to the blockchain. A game of pure reaction, provably fair, where the winner takes the pot.</p><p>My initial thought was to use a stablecoin like USDT for the game's currency. It seemed user-friendly. However, as Gemini and I delved into the technicals, I discovered a fundamental conflict with my vision. The USDT smart contract is controlled by a central entity, Tether, which has the technical ability to pause or freeze any wallet. This \"kill switch\" functionality, while understandable from their perspective, was a deal-breaker for me. The core of my project was to be .</p><p>This led to my first major pivot: the game would use the native currency of the chain (ETH on Arbitrum). This not only ensured complete decentralization—where no single entity could interfere with player funds—but also simplified the smart contract logic significantly. To account for price volatility, the owner can adjust the stake amount as needed.</p><h3>\n  \n  \n  The High-Level Architecture\n</h3><p>The application is composed of three main pillars, all orchestrated within a Docker environment.</p><ol><li><strong>Smart Contract (Solidity):</strong> The heart of the game. It acts as the <strong>unstoppable and transparent source of truth</strong>, handling player stakes, game state transitions, and prize distribution. Through a proxy pattern, it provides <strong>a stable, immutable address and state for users</strong>, while allowing the owner to securely upgrade the underlying game logic.</li><li> The brains of the operation. It manages the game lifecycle, listens for blockchain events, and communicates with players in real-time via WebSockets. It's the off-chain coordinator for the on-chain action.</li><li> The face of the game. A simple, lightweight client that interacts with the user's wallet (like MetaMask) and communicates with the backend.</li></ol><p>Here's how they interact:</p><ul><li>  A user connects their wallet on the .</li><li>  The  talks to the  via a REST API to get game configuration and via WebSockets for real-time updates (e.g., other players joining).</li><li>  The  listens to the blockchain for events from the  (like deposits).</li><li>  The  sends transactions to the  to manage the game (e.g., starting the music round).</li></ul><p>To run this in production, we containerized everything. This makes deployment, scaling, and management incredibly robust.</p><ul><li>: The entry point. It handles SSL, serves the frontend, and routes API/WebSocket traffic.</li><li>: The main Go application.</li><li>: A dedicated, hardened microservice whose only job is to sign blockchain transactions.</li><li>: The database for storing game history and analytics data.</li><li>: An intrusion prevention service that monitors logs and bans malicious IPs.</li><li>: A self-hosted, privacy-respecting analytics service.\nmarkdown</li><li> A key privacy-enforcing service. It's configured to rotate Nginx logs daily while keeping zero old log files (). This ensures that sensitive information like IP addresses is purged from the server in less than 24 hours, maximizing user anonymity.</li></ul><h3>\n  \n  \n  Deep Dive: The Smart Contract\n</h3><p>The smart contract is the most critical piece of the puzzle. Security, reliability, and transparency were non-negotiable. Here’s how we achieved that.</p><p>We used <strong>OpenZeppelin's UUPS (Universal Upgradeable Proxy Standard)</strong>. This allows the contract logic to be upgraded without losing the contract's state (i.e., ongoing games, funds). It's a battle-tested pattern for long-term projects.</p><p>A key security measure is the  call in the implementation contract's constructor:</p><div><pre><code>/// @custom:oz-upgrades-unsafe-allow constructor\nconstructor() {\n    _disableInitializers();\n}\n</code></pre></div><p>This prevents anyone from calling the  function on the logic contract itself, which could otherwise be a vector for hijacking. Interestingly, this line had to be commented out during testing with tools like Echidna and Foundry, as they would fail, but it's crucial for production security.</p><ul><li> We use OpenZeppelin's  to protect all functions that handle fund transfers (, , etc.) from re-entrancy attacks.</li><li><strong>Ownership and Role Separation:</strong> We implemented a three-address system to separate concerns and minimize risk:\n\n<ul><li> This address has the highest level of control (upgrading the contract, changing fees). It was generated offline and is never exposed to the internet. Transactions are signed on an air-gapped machine, and the raw signed transaction is then broadcast using a tool like Arbiscan's  page.</li><li> This address handles the day-to-day operations, like starting games and recording results. It can be replaced instantly by the owner if compromised, without a timelock, allowing for rapid response.</li><li> A dedicated address that can only receive platform commissions. This separation ensures that even if the hot wallet is compromised, the core contract and its funds remain secure. In the future, I'm considering moving the owner role to a 2-of-3 multisig for even greater resilience.</li></ul></li><li><strong>Timelocks for Critical Functions:</strong> Functions that could move significant funds, like , are protected by a timelock. A withdrawal is first  with a specific amount, and can only be  after a delay. This gives users full transparency and time to react if they see something they don't like.</li><li> All functions that set addresses (like changing the owner or backend wallet) prevent setting the address to , which would permanently \"brick\" the contract.</li></ul><p>Gemini helped me implement several gas optimization techniques. While modern compilers are excellent, explicit optimizations are still key:</p><ul><li> Instead of  with string messages, we use custom errors (<code>error InsufficientStake();</code>). This saves significant gas on deployment and during runtime when a check fails.</li><li><strong>Efficient State Management:</strong> We carefully designed data structures to minimize writes to storage, which is the most expensive operation on the EVM. For example, we read values into memory, perform operations, and then write the final result back to storage once.</li><li> For operations where we are certain underflow/overflow cannot occur (e.g., incrementing a counter after checking its bounds), we use  blocks to save the gas that would be spent on the default safety checks in Solidity 0.8+.</li></ul><h3>\n  \n  \n  Rigorous Testing and Verification\n</h3><p>A smart contract is only as good as its testing. We were exhaustive:</p><ul><li> We wrote 81 unit tests with Hardhat and Foundry, achieving near-100% code coverage. We also wrote fuzz tests to throw thousands of random inputs at the functions.</li><li> We used  to run 50,000 random transactions against the contract to test for broken invariants (e.g., \"the contract balance should never be less than the sum of all player deposits\"). No vulnerabilities were found.</li><li> We wrote  and  to simulate specific attack scenarios and ensure our guards worked as expected.</li><li> The code was analyzed with  and , and the bytecode was checked with .</li><li> We used  to analyze the gas cost of every function, helping us pinpoint areas for optimization.</li><li> The contracts are verified on . This provides cryptographic proof that the deployed bytecode matches the open-source code. We initially planned to use Arbiscan, but our deployment coincided with Etherscan's major transition from their V1 API to the new, unified V2 keys. This transitional period caused temporary verification issues, making  an excellent and reliable alternative.</li></ul><p>This multi-layered approach to security and testing gives me, and hopefully my users, a high degree of confidence in the contract's integrity.</p><p><strong>In the next part, I'll dive into the Backend, Frontend, and the operational infrastructure that powers the game.</strong></p><p>Now, let's get into the off-chain machinery that brings the game to life: the microservices, the security fortress I built around them, and the path forward.</p><h3>\n  \n  \n  Deep Dive: The Keyservice Microservice - A Digital Fortress\n</h3><p>One of my biggest concerns was handling the backend's private key. This key is \"hot\" – it needs to be online to sign transactions like starting a game. A compromise here would be disastrous. My solution was to build a dedicated, hardened microservice with a single responsibility: .</p><p>It's a tiny Go application, but it's built like a fortress:</p><ul><li> It runs in its own Docker container and does nothing but receive data from the main backend, sign it, and return the signature. It has no other network access.</li><li> The encrypted private key JSON and its password are not in the container image or environment variables. They are mounted as Docker Secrets, which are stored in-memory on the host and are only accessible to the services they're granted to. The files on the host machine have their permissions locked down with .</li><li><strong>Quantum-Resistant Encryption:</strong> This is where my paranoia really kicked in. I didn't just encrypt the secrets; I used <strong>GPG with AES-256 and a high </strong> (<code>--s2k-mode 3 --s2k-count 65011712</code>). This is a slow, synchronous encryption method that makes brute-force attacks computationally infeasible, even against future threats like Grover's algorithm for quantum computers. This is military-grade stuff.</li><li> What if the keyservice container crashes and Docker fails to restart it? The main backend has a unique, obfuscated module containing the GPG-encrypted key, passphrase, and  file. If it can't reach the keyservice, it uses a master password to decrypt these assets , restart the container, and then securely wipes the decrypted files from disk by overwriting them with zeros. It's an automated disaster recovery plan.</li></ul><p>I considered hardware keys like a YubiKey or cloud HSMs, but rejected them. A physical key introduces a single point of failure and a potential de-anonymization vector. Cloud HSMs require trusting a third party, which I wasn't willing to do. This self-contained, heavily fortified microservice was the answer.</p><p> The next step is to move from Docker Compose to Kubernetes for more granular control and to \"harden\" the containers using  and .</p><ul><li> (Secure Computing Mode) is a Linux kernel feature that restricts the system calls a process can make. I can create a profile that allows  the specific syscalls Go needs to run the keyservice, and nothing else.</li><li> (Application Armor) confines programs to a limited set of resources. I can define a policy that prevents the keyservice from writing to unexpected disk locations or accessing unauthorized network ports.</li></ul><p>Together, these will create an even smaller attack surface, making a container breakout virtually impossible.</p><h3>\n  \n  \n  Deep Dive: The Backend (Go)\n</h3><p>The main backend is the game's central nervous system, written in Go for its performance and concurrency. It's logically split into modules:</p><ul><li>: Defines all the REST endpoints for the frontend. It includes protection against slow header attacks to prevent resource exhaustion.</li><li>: Handles all interaction with the smart contract. It uses versioned auto-generated Go bindings from the contract's ABI. This is also where I used  to interact with the upgradeable proxy contract, allowing the backend to seamlessly call functions on the implementation contract through the stable proxy address.</li><li>: On startup, it quickly reads past blockchain events to catch up to the current state, then switches to a slower, regular polling of new events.</li><li>: The largest and most complex module, containing the entire game state machine and lifecycle.</li><li>: Manages the WebSocket connections. To join a game, the user signs a  (a single-use random string) provided by the backend. This proves ownership of their address without a full transaction and also registers any associated referrer. The backend verifies this signature and, upon success, issues a <strong>one-time WebSocket authentication token</strong>. The frontend then uses this token to establish a secure, authenticated connection, preventing unauthorized access.</li><li>: Manages database interaction using , which provides a fantastic object-relational mapping layer and handles database schema migrations automatically. This is also where the analytics models for the conversion funnel and profit reports live.</li></ul><p> I was relentless with testing.</p><ul><li>  Most modules have , verified with .</li><li>  I used  with a suite of static analyzers like  (security), , and  to catch potential issues early.</li><li>  Database tests were not mocked. I used the  pattern, where a real PostgreSQL Docker container is spun up for the test suite and torn down afterward, ensuring tests run against a real environment.</li><li>  I heavily profiled the code for CPU usage, memory allocations (, ), and lock contentions (, ) to hunt down performance bottlenecks and race conditions.</li><li>  Critical modules were compiled with  to obfuscate the code, and all binaries were packed with  to shrink their size and make reverse-engineering a nightmare.</li><li>  Finally, the entire codebase was analyzed with  to enforce best practices and catch any remaining code smells.</li></ul><h3>\n  \n  \n  Deep Dive: The Frontend (HTML/CSS/JS)\n</h3><p>The frontend is intentionally simple: vanilla HTML, CSS, and JavaScript (transpiled from TypeScript). This wasn't a shortcut; it was a strategic choice. A simple, static site can be easily hosted on decentralized platforms like  or , further enhancing the project's censorship resistance.</p><p>Even with its simplicity, it's well-tested using Jest for unit tests (), ESLint for code quality, and Prettier for consistent formatting.</p><h3>\n  \n  \n  The Community and The Road Ahead\n</h3><p>A project is nothing without a community. My growth strategy is focused on rewarding early believers.</p><ul><li> I launched a campaign on Zealy where users can complete quests to earn XP.</li><li> The first 300 community members will receive a special NFT, granting them the \"OG Member\" role in Discord and future in-game bonuses.</li><li> I'm planning to add a global leaderboard and host tournaments with real prize pools.</li></ul><p>This project has been an incredible journey. It started as a simple idea and, with the help of my AI partner, evolved into a secure, robust, and fully decentralized application. I went from a junior-level coder to a full-stack dApp creator, and I did it by focusing on the vision and letting the AI handle the complex implementation.</p><p>This is the new frontier of indie development. If you have an idea, the tools to build it are more accessible than ever.</p><p><strong>Come play a game and join the community!</strong></p><ul><li> muschairs.com</li><li> discord.gg/wnnJKjgfZW</li><li> @crow004_crow</li><li><code>npub1v0kc8fwz67k0mv539z6kaw5h25et9e2zmnnqq6z2naytaq566gwqkzz542</code></li></ul><p>My next steps are to spread the word on platforms like Reddit, connect with web3 enthusiasts, and, of course, start building my next idea. Thanks for reading!</p>","contentLength":14937,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go 1.25: JSON v2 e Novo GC","url":"https://dev.to/rflpazini/go-125-json-v2-e-novo-gc-4k07","date":1755726628,"author":"Rafael Pazini","guid":234746,"unread":true,"content":"<p>Chegou o Go 1.25 e, sinceramente, é sobre tempo. Duas mudanças que vão fazer diferença real no seu dia a dia: o  que não é uma piada de performance e o  que promete parar de sugar sua CPU.</p><p>Vamos ver o que realmente mudou e se vale a pena migrar (spoiler: provavelmente sim).</p><h2>\n  \n  \n  Por que o JSON v2 existe?\n</h2><p>O  padrão é tipo aquele colega de trabalho: faz o trabalho, mas reclama o tempo todo. Lento, cheio de alocações desnecessárias, e você sempre acaba procurando alternativas como <a href=\"https://github.com/mailru/easyjson\" rel=\"noopener noreferrer\">EasyJSON</a> ou <a href=\"https://github.com/json-iterator/go\" rel=\"noopener noreferrer\">JSONIterator</a> quando a coisa aperta.</p><p>A equipe do Go finalmente acordou e disse: \"Ok, vamos fazer direito dessa vez. \"</p><div><pre><code>jsonv2 go run main.go\n\n</code></pre></div><p>Exemplo básico que funciona de verdade:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  O que mudou na implementação do JSON v2\n</h2><p>A nova implementação não é apenas uma otimização superficial do código existente. A equipe do Go <strong>reescreveu o parser do zero</strong>, focando em três problemas principais que atormentavam o  original: , <strong>parsing sequencial ineficiente</strong>, e <strong>falta de suporte nativo para streaming</strong>.</p><h3>\n  \n  \n  Arquitetura otimizada para Menos Alocações\n</h3><p>O maior vilão do JSON v1 sempre foram as alocações desnecessárias. Cada vez que você fazia  em uma struct grande, o parser criava dezenas de objetos intermediários (buffers temporários, slices auxiliares, interfaces{} para cada valor).</p><p>O v2 introduz um <strong>sistema de pooling interno</strong> e  que reduz drasticamente essas alocações. Em vez de criar novos objetos a cada operação, ele mantém pools de estruturas reutilizáveis que são recicladas entre chamadas.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Parser não-sequencial e streaming nativo\n</h3><p>Outra mudança fundamental: o v1 sempre processava JSON de forma , lia byte por byte, construindo a estrutura na ordem exata do documento. Isso funcionava, mas era ineficiente para JSONs grandes.</p><p>O v2 implementa  e . Para JSONs grandes, ele pode processar pedaços do documento simultaneamente e construir a estrutura final de forma mais eficiente. Isso é especialmente poderoso quando você está lidando com arrays grandes ou objetos com muitas propriedades.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Otimizações específicas para tipos comuns\n</h3><p>O v2 também inclui  otimizados para tipos que aparecem frequentemente em APIs modernas:</p><p> têm parsing especializado que evita conversões desnecessárias.  (o caso mais comum) têm tratamento otimizado. <strong>Slices de tipos primitivos</strong> são processados em lotes quando possível.</p><div><pre><code></code></pre></div><h3><strong>Mensagens de erro mais úteis</strong></h3><p>Um bônus que todo mundo vai amar: as mensagens de erro ficaram muito melhores. Em vez de \"invalid character 'x' looking for beginning of value\", agora você recebe contexto real:</p><div><pre><code></code></pre></div><p>Quando vale usar? Se você processa muito JSON por segundo, trabalha com streaming de dados grandes, ou simplesmente está cansado de debuggar mensagens de erro confusas. A nova implementação resolve esses três problemas de uma vez.</p><h2>\n  \n  \n  GreenteaGC: Entendendo o Novo Coletor de Lixo\n</h2><p>Antes de falar do novo GC, preciso explicar por que o atual às vezes é um problema. O Go usa um coletor <strong>concurrent mark-and-sweep tricolor</strong> desde a versão 1.5. Parece complexo, mas a ideia é simples: ele funciona junto com seu programa (concurrent), marca objetos que ainda estão sendo usados (mark), e depois varre os não marcados para liberar memória (sweep). O \"tricolor\" é só o algoritmo usado para marcar sem quebrar referências.</p><p>O problema? Esse processo, mesmo sendo concurrent, ainda compete por recursos de CPU e pode causar  em momentos críticos. Pior ainda: em programas que criam muitos objetos de vida curta (tipo APIs que processam requests), o GC pode ficar numa corrida constante tentando limpar a bagunça.</p><h3><strong>O Que GreenteaGC Muda na Prática</strong></h3><p>Como ativar o experimental:</p><div><pre><code>greenteagc go run main.go\n\n</code></pre></div><p>O  reimplementa partes fundamentais do coletor com foco em <strong>reduzir o overhead por objeto</strong> e <strong>diminuir o trabalho paralelo desnecessário</strong>. Na prática, isso significa que ele é mais esperto sobre quando coletar lixo e quanto CPU gastar nisso.</p><p>A grande diferença está na forma como ele lida com <strong>objetos pequenos e temporários</strong>. O GC atual trata todos os objetos meio que igual - um  de 10 bytes recebe o mesmo tipo de atenção que um slice gigante. O novo coletor tem estratégias diferentes baseadas no tamanho e padrão de uso dos objetos.</p><h3><strong>Onde Você Sente a Diferença</strong></h3><p> são o caso clássico. Imagine um endpoint que recebe 10.000 requests por segundo. Cada request cria várias structs temporárias, slices para processar dados, maps para organizar responses. Com o GC atual, toda essa criação/destruição gera trabalho constante para o coletor.</p><div><pre><code></code></pre></div><p> também se beneficiam muito. Quando você processa milhares de registros por minuto, cada um passando por várias transformações que criam objetos intermediários, o GC tradicional pode virar gargalo real.</p><div><pre><code></code></pre></div><p>Em benchmarks divulgados pela equipe do Go, o  mostra <strong>reduções de overhead entre 10% e 40%</strong>, dependendo do padrão de alocação. Isso se traduz em:</p><p><strong>Menos pausas perceptíveis</strong>: aqueles microfreezees de 5-15ms que aparecem no percentil 99 de latência diminuem significativamente.</p><p><strong>Melhor throughput sustentado</strong>: menos CPU gasta em GC = mais CPU disponível para seu código.</p><p><strong>Comportamento mais previsível</strong>: menos variação na latência, especialmente importante para sistemas que precisam de SLA consistente.</p><h3>\n  \n  \n  Cenários que mais se beneficiam\n</h3><p> são um caso especial. Quando você roda no Kubernetes com limites de CPU bem definidos, cada ciclo desperdiçado pelo GC é um ciclo que não está processando requests reais. O novo coletor entende melhor esses limites e se adapta.</p><p><strong>Sistemas de alta concorrência</strong> onde você tem centenas ou milhares de goroutines criando objetos simultaneamente. O GC atual pode ter dificuldade para coordenar a limpeza entre todas essas threads. O  tem estratégias melhores para lidar com essa complexidade.</p><p><strong>Aplicações que fazem marshaling/unmarshaling intensivo</strong> - que é exatamente onde o JSON v2 também ajuda. A combinação dos dois pode ser especialmente poderosa: menos alocações na serialização JSON + GC mais eficiente para limpar o que sobra.</p><p>Com o , você não vai ver milagres, mas vai notar estabilidade maior na latência e uso mais eficiente de recursos. É especialmente visível em load testing sustentado, onde o comportamento do GC ao longo do tempo faz mais diferença que picos isolados.</p><h2>\n  \n  \n  Comparação Honesta: JSON v2 vs EasyJSON\n</h2><p>Durante anos, se você queria performance real com JSON em Go, tinha que partir pro EasyJSON. Gerava código otimizado, era rápido, mas que trabalhão configurar e manter.</p><p>Para : JSON v2 chegou bem perto, às vezes até superando quando você tem muitos  e .</p><p>Para <strong>marshaling de dados conhecidos</strong>: EasyJSON ainda leva vantagem, mas a diferença não é mais abismal.</p><p>Para : JSON v2 destroi tanto o v1 quanto o EasyJSON, porque foi otimizado exatamente para isso.</p><p>A interpretação honesta? Se você quer simplicidade e performance decente, teste JSON v2. Se você quer exprimir cada ciclo de CPU e já tem estruturas definidas, EasyJSON ainda é rei. Mas agora pelo menos temos escolha real.</p><h2>\n  \n  \n  Benchmark com dados do mundo real\n</h2><p>Vamos usar dados do , ou seja, JSONs reais, grandes, variados. É o teste mais honesto possível, sem truque de benchmark sintético.</p><p>Primeiro, baixando os dados:</p><div><pre><code> data data\ncurl  2025-07-01-12.json.gz \n  https://data.gharchive.org/2025-07-01-12.json.gz\n 2025-07-01-12.json.gz    ..\n\n</code></pre></div><p>Setup do teste (estrutura organizada):</p><div><pre><code>bench-json/\n├── go.mod\n├── benchmark\n│&nbsp;&nbsp; ├── bench_v1_test.go\n│&nbsp;&nbsp; └── bench_v2_test.go\n└── internal/\n    └── ndjson.go         # helpers de leitura\n\n</code></pre></div><p>Helpers para lidar com NDJSON (internal/ndjson.go):</p><div><pre><code></code></pre></div><p>Benchmark para v1 (bench_v1_test.go):</p><div><pre><code></code></pre></div><p>Benchmark para a v2 (bench_v2_test.go):</p><div><pre><code></code></pre></div><div><pre><code>\ngo UnmarshalMap ^ ./...\n\njsonv2 go UnmarshalMap ^ ./...\n\n</code></pre></div><h2>\n  \n  \n  Resultados que você sente na prática\n</h2><p>Rodei o benchmark com dados reais do GitHub Archive no meu MacBook M3 Pro. Vou traduzir os números técnicos para o que isso significa no seu dia a dia:</p><p> Sua API que processa 150MB de dados JSON demora  A mesma API agora demora </p><p> Se sua API respondia em 200ms, agora responde em . É a diferença entre uma API que parece rápida e uma que parece instantânea.</p><p> Para processar esses dados, Go aloca  Agora aloca apenas </p><p><strong>180MB a menos de pressão no GC</strong>. Isso significa menos pausas, menos CPU gasta limpando lixo, containers mais estáveis no Kubernetes.</p><p> Processava dados a  Agora processa a </p><p> Uma API que conseguia processar  agora processa  com a mesma máquina.</p><p> Criou  de objetos temporários Criou apenas </p><p> a menos de trabalho para o Garbage Collector. Menos interrupções, menos spikes de CPU, comportamento mais previsível.</p><p>Se você roda no AWS/GCP e processa :</p><p> Precisava de uma instância de  para manter latência aceitável Consegue rodar na mesma carga com  ou processar 48% mais dados na mesma máquina</p><p> ~$50-100/mês por instância, dependendo da região e tipo de máquina.</p><div><table><thead><tr></tr></thead><tbody><tr></tr><tr></tr><tr></tr><tr></tr><tr></tr></tbody></table></div><p>Isso não é benchmark sintético, são dados reais de eventos do GitHub, com a complexidade e variação que você encontra em produção. A melhoria é real e você vai sentir no monitoramento.</p><p>Onde você vai sentir a diferença? APIs de alta carga vão processar JSON mais rápido, microservices vão se comunicar com menos overhead, pipelines de ETL vão ter menos pausas do GC, e containers no Kubernetes vão usar melhor os limites de CPU.</p><p>Quando migrar? Se você tem APIs que processam muito JSON, sistemas sensíveis à latência, workloads que criam muitos objetos temporários, ou simplesmente curiosidade científica, vale testar agora. É experimental, mas já está estável o suficiente para brincar.</p><p>Go 1.25 não trouxe apenas melhorias incrementais, trouxe um salto real nas partes que mais usamos: JSON e gerenciamento de memória.</p><p>Para quem quer estabilidade, continue no GC padrão e . Funciona bem, sempre funcionou. Para quem gosta de viver no futuro, ative  e  e meça os resultados. Os números que mostrei são reais e reproduzíveis.</p><p>O melhor do Go sempre foi esse equilíbrio: estabilidade no core, inovação nos experimentos. Agora é nossa vez de testar essas novidades e dar feedback para a comunidade. Teste, meça, e me conta os resultados. Aposto que você vai gostar do que vai encontrar.</p>","contentLength":10211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Day 6: When Protobuf Breaks Everything - Real Engineering in the Trenches","url":"https://dev.to/clayroach/day-6-when-protobuf-breaks-everything-real-engineering-in-the-trenches-1eek","date":1755714221,"author":"Clay Roach","guid":234687,"unread":true,"content":"<p>: Add real-time updates and bootstrap AI anomaly detection.: \"Why are all my operations named 'protobuf-fallback-trace'?!\"</p><p>Welcome to Day 6 of building an AI-native observability platform in 30 days. Today was supposed to be about sexy features. Instead, it was about the unglamorous reality of systems engineering: <strong>making protobuf work correctly</strong>.</p><h2>\n  \n  \n  The Problem That Changed Everything\n</h2><p>I started the day confident. The OpenTelemetry demo was running, traces were flowing, the UI was displaying data. Time to add real-time updates, right?</p><p>Then I looked closer at the trace details:</p><div><pre><code></code></pre></div><p>Every. Single. Operation. Was named \"protobuf-fallback-trace\".</p><h3>\n  \n  \n  Discovery #1: Gzip Was Being Ignored\n</h3><p>The OpenTelemetry demo sends protobuf data with gzip compression. My middleware had \"clever\" conditional logic:</p><div><pre><code></code></pre></div><p>The fix was embarrassingly simple:</p><div><pre><code></code></pre></div><p>: Sometimes \"clever\" code is just complicated code. Unified handling often beats conditional logic.</p><h3>\n  \n  \n  Discovery #2: Protobufjs vs ES Modules\n</h3><p>Next challenge: parsing the actual protobuf data. The protobufjs library is CommonJS, but my project uses ES modules. This led to hours of:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Discovery #3: Path Resolution Hell\n</h3><p>Even with protobufjs loading, the OTLP protobuf definitions have imports that need custom resolution:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  The Nuclear Option: Enhanced Fallback Parsing\n</h2><p>When the \"proper\" protobuf parsing kept failing, I built something unconventional - a raw protobuf parser that extracts data through pattern matching:</p><div><pre><code></code></pre></div><p>Is this elegant? No. Does it work? .</p><p>After 8 hours of protobuf wrestling:</p><ul><li>❌ All operations: \"protobuf-fallback-trace\"</li></ul><ul><li>✅ Real operations: , </li><li>✅ 10+ real spans per trace</li><li>✅ Authentic resource attributes and timing data</li></ul><h3>\n  \n  \n  1. <strong>Fallback Strategies Are Not Defeat</strong></h3><p>Building a fallback parser wasn't giving up - it was ensuring the system works even when dependencies fail. In production, .</p><h3>\n  \n  \n  2. <strong>Debug at the Lowest Level</strong></h3><p>I spent hours assuming the protobuf data was corrupt. Finally logging the raw buffer bytes revealed it was fine - the decompression was being skipped.</p><h3>\n  \n  \n  3. <strong>Integration Points Are Where Systems Break</strong></h3><p>The individual components all worked:</p><ul><li>✅ OpenTelemetry demo: sending valid data</li><li>✅ Express server: receiving requests\n</li><li>✅ ClickHouse: storing data</li></ul><p>The failure was in the glue between them.</p><h3>\n  \n  \n  4. <strong>Real Data Reveals Real Problems</strong></h3><p>Mock data would never have exposed this issue. Testing with the actual OpenTelemetry demo forced me to handle real-world complexity.</p><p>Today didn't go according to plan, and that's  what building production systems is like. The glossy demo videos don't show the 8 hours spent debugging why <code>protobuf.load is not a function</code>.</p><p>But here's what matters: <strong>the system now correctly processes thousands of real traces from a production-like demo application</strong>. Every service is visible, every operation is named correctly, and the data flowing through the pipeline is authentic.</p><p>Now that protobuf parsing actually works:</p><ul><li>Implement the real-time updates (for real this time)</li><li>Add WebSocket support for live trace streaming</li><li>Bootstrap the AI anomaly detection system</li><li>Create service dependency visualization</li></ul><h2>\n  \n  \n  Code Snippets That Saved the Day\n</h2><p>For anyone fighting similar battles:</p><div><pre><code>\ndocker compose backend xxd  100 /tmp/trace.pb\n\n\ncurl  POST http://localhost:4319/v1/traces  @trace.pb.gz\n\n\nnode </code></pre></div><p>Day 6 was humbling. The plan was to build flashy features. Instead, I spent the day in the trenches making basic data ingestion work correctly. </p><p>But that's real engineering. It's not always about the elegant algorithm or the clever architecture. Sometimes it's about making protobuf parsing work at 2 AM because your entire platform depends on it.</p><p><strong>The platform is stronger because of today's battles.</strong> And tomorrow, with real data flowing correctly, we can build the features that actually matter.</p><p><em>Are you fighting your own protobuf battles? Share your war stories in the comments. Sometimes knowing you're not alone in the debugging trenches makes all the difference.</em></p><p><strong>Progress: Day 6 of 30 ✅ | Protobuf: Finally Working | Sanity: Questionable</strong></p>","contentLength":4043,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Send Email using aws-sdk-v2.sesv2 on golang","url":"https://dev.to/adityaokke/send-email-using-aws-v2sesv2-on-golang-4dfb","date":1755704480,"author":"ADITYA OKKE SUGIARSO","guid":234639,"unread":true,"content":"<p><strong>1. initiate ses service on aws</strong>\nchoose your region on aws</p><p><strong>2. create identities to use sandbox feature from aws ses</strong></p><p>click create identity button<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2uova598o86hebwbj7gx.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2uova598o86hebwbj7gx.png\" alt=\"Create identity button\" width=\"800\" height=\"147\"></a></p><p>fill form<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi6fnqc8om4335e2mgqzv.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fi6fnqc8om4335e2mgqzv.png\" alt=\"identity details\" width=\"800\" height=\"365\"></a>\ncreate another identity for \nand you should get </p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fegurnruusbl1dqxenqmd.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fegurnruusbl1dqxenqmd.png\" alt=\"identities\" width=\"800\" height=\"113\"></a>\nfinally you just need to verify your email by click link verification on the email inbox</p><div><pre><code>$ mkdir ~/helloaws\n$ cd ~/helloaws\n$ go mod init helloaws\n</code></pre></div><div><pre><code>$ go get github.com/aws/aws-sdk-go-v2/aws\n$ go get github.com/aws/aws-sdk-go-v2/config\n$ go get github.com/aws/aws-sdk-go-v2/service/sesv2\n</code></pre></div><div><pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n\n    \"github.com/aws/aws-sdk-go-v2/aws\"\n    \"github.com/aws/aws-sdk-go-v2/config\"\n    \"github.com/aws/aws-sdk-go-v2/service/sesv2\"\n    \"github.com/aws/aws-sdk-go-v2/service/sesv2/types\"\n)\n\nfunc main() {\n    // Using the SDK's default configuration, load additional config\n    // and credentials values from the environment variables, shared\n    // credentials, and shared configuration files\n    cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(\"ap-southeast-1\"))\n    if err != nil {\n        log.Fatalf(\"unable to load SDK config, %v\", err)\n    }\n\n    // Build the request with its input parameters\n    resp, err := svc.SendEmail(context.TODO(), &amp;sesv2.SendEmailInput{\n        FromEmailAddress: aws.String(\"admin@gmail.com\"),\n        Destination: &amp;types.Destination{\n            ToAddresses: []string{\"user@gmail.com\"},\n        },\n        Content: &amp;types.EmailContent{\n            Simple: &amp;types.Message{\n                Subject: &amp;types.Content{\n                    Data: aws.String(\"Test Email\"),\n                },\n                Body: &amp;types.Body{\n                    Text: &amp;types.Content{\n                        Data: aws.String(\"This is a test email sent using AWS SES.\"),\n                    },\n                },\n            },\n        },\n    })\n    if err != nil {\n        fmt.Printf(\"Error sending email: %v\\n\", err)\n    }\n\n    fmt.Printf(\"Email sent successfully, message ID: %s\\n\", *resp.MessageId)\n}\n\n</code></pre></div>","contentLength":1978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Creating IAM Access Keys for AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY","url":"https://dev.to/adityaokke/creating-iam-access-keys-for-awsaccesskeyid-and-awssecretaccesskey-34i3","date":1755704008,"author":"ADITYA OKKE SUGIARSO","guid":234638,"unread":true,"content":"<p>IAM user access keys consist of two parts:</p><ul><li>Access key ID (for example: AKIAIOSFODNN7EXAMPLE)</li><li>Secret access key (for example: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY)</li></ul><p>You must use both the access key ID and the secret access key together to authenticate requests made through the AWS SDK.</p><ul><li> Open the IAM Dashboard in the AWS Management Console. In the left navigation pane, choose Users.</li></ul><ul><li>on set permissions, create group to attach the policies</li></ul><ul><li>Set a group name and choose permission policies. \nThese policies usually provide full access per AWS service. If you need more fine-grained control, you can create your own custom policies by selecting the Create policy button.</li></ul><ul><li>after that, review and select Create User button</li></ul><ul><li><p>choose Create access key<a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2bglh79geg2fzpw4nq8s.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F2bglh79geg2fzpw4nq8s.png\" alt=\"create access key\" width=\"800\" height=\"163\"></a></p></li></ul><ul><li>fill any meaningful name then choose create key</li></ul><ul><li>if sucess, you will have  and  to put on .env</li></ul><ul><li>now you can put both key on .env. AWS SDK will automatically detect the key on .env\n</li></ul><div><pre><code>AWS_ACCESS_KEY_ID=AKIAZF************\nAWS_SECRET_ACCESS_KEY=utiKWhMNy***********************************\n</code></pre></div>","contentLength":1016,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go web framework","url":"https://dev.to/dingzhanjun/go-web-framework-1p4o","date":1755702813,"author":"John Ding","guid":234637,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What I Learned From a Week of AI-Assisted Coding: The Good, The Bad, and The Surprisingly Counterintuitive","url":"https://dev.to/jack_branch_3fb9e01c57c03/what-i-learned-from-a-week-of-ai-assisted-coding-the-good-the-bad-and-the-surprisingly-11kl","date":1755699865,"author":"Jack Branch","guid":234588,"unread":true,"content":"<p>Last week, I decided to build something I'd been putting off for months: a personal password manager. My requirements were simple - secure local storage, clean UI, and encryption I could trust. What made this interesting wasn't the project itself, but how I built it.</p><p>I have a background in distributed systems: REST APIs, event-driven architecture, Kafka, the usual enterprise stack. Building a multi-platform desktop application was entirely new territory. I'd been planning this experiment for a while: what would it be like to build a project entirely using AI-assisted programming?</p><p>Before we continue, I should disclose some bias. I'm somewhat of an AI skeptic, so I definitely had preconceived ideas going into this, particularly around code quality, security, and scalability. I also assumed the process would be painful and less enjoyable than traditional programming (spoiler alert: I was completely wrong about this one).</p><p>Next came choosing the language. I've always been interested in Go: it seems like a nice blend of C++, Python, and JavaScript, all languages I enjoy. Since I'd never touched Go or Fyne (Go's UI framework), this seemed like the perfect way to put these AI models through their paces.</p><p>Over the course of a week, I experimented with three different models: GPT-4, Claude Sonnet, and Gemini 2.5 Pro, switching between them to see how each handled different aspects of the development process.</p><p>What I discovered challenged most of my assumptions about AI-assisted coding. The fastest model wasn't the most productive. The highest-quality code generator wasn't the most helpful. And the most counterintuitive finding of all: sometimes being \"too good\" at coding assistance actually made the development experience worse.</p><p>If you're considering integrating AI tools into your development workflow, or if you're curious about the practical realities behind the productivity hype, here's what a week of intensive AI-assisted coding actually taught me.</p><h2>\n  \n  \n  The Productivity Illusion: Fast Start, Slow Finish\n</h2><p>The most striking pattern in my week of AI coding wasn't what I expected. My productivity started incredibly high and steadily declined as the project progressed. On day one, I had a working password manager with encryption, a basic UI, and core functionality. By day four, I was stuck in refactoring hell, generating thousands of lines of code changes while adding zero new features.</p><h3>\n  \n  \n  The Setup Phase: Where AI Shines\n</h3><p>AI assistance was genuinely transformative during the initial setup. Within hours, I had:</p><ul><li>A properly structured Go project with modules and dependencies</li><li>A working Fyne UI with multiple screens\n</li><li>Basic encryption and decryption functionality</li><li>File I/O for local storage</li><li>Even a custom test framework (more on that later)</li></ul><p>This was exactly the productivity boost everyone talks about. Tasks that would have taken me days of research and documentation reading were completed in minutes. For someone completely new to Go and Fyne, this felt magical.</p><h3>\n  \n  \n  The Architecture Reality Check\n</h3><p>But then reality hit. The code that got me started quickly didn't fit what I actually needed. The AI had made architectural decisions based on getting something working, not on building something maintainable. What followed was an endless cycle of refactoring:</p><ul><li>The initial encryption implementation was too simple for real security needs</li><li>The UI structure couldn't handle the complexity I wanted to add</li><li>There was no dependency injection, making testing nearly impossible\n</li><li>Error handling was inconsistent across the codebase</li><li>The file structure didn't make sense for the features I planned</li></ul><p>Here's where things got really problematic. Each refactoring session with AI would generate hundreds of lines of code changes. My commit history started looking incredibly productive - lots of activity, lots of lines added. But I wasn't adding any new features. I was essentially paying interest on the technical debt from the AI's initial \"quick wins.\"</p><p>The breaking point came when I hit my rate limit on GitHub Copilot after just four days of use (on a paid plan). Suddenly, I was stuck mid-refactor with partially broken code and no AI assistance. I had to manually dig myself out of the mess, which gave me a clear perspective on what was actually necessary versus what the AI thought needed to be \"improved.\"</p><h3>\n  \n  \n  Traditional Coding: The Unexpected Comeback\n</h3><p>On my final day, I switched approaches entirely. I did all the coding myself and used GPT-4 purely as a reference tool: essentially treating it like an enhanced Google for Go-specific questions. The results were surprising:</p><ul><li>Higher actual delivery rate despite generating less code</li><li>No rework cycles or debugging sessions</li><li>Better understanding of what I was building</li><li>Code that fit my actual requirements, not the AI's assumptions</li></ul><p><strong>High initial productivity from AI can be an illusion if it comes at the cost of architecture and maintainability.</strong></p><h2>\n  \n  \n  Model Behaviors: The Counterintuitive Preferences\n</h2><p>Testing three different AI models revealed some unexpected preferences that go against conventional wisdom about \"better\" AI being more helpful.</p><h3>\n  \n  \n  GPT-4: Fast, Wrong, and Strangely Effective\n</h3><p>GPT-4 was objectively the worst at generating correct code. It made frequent mistakes, missed edge cases, and often gave me solutions that needed significant debugging. But here's the counterintuitive part: <strong>I enjoyed working with it the most.</strong></p><p>Why? Because it was fast, and its mistakes kept me engaged with the code. Every response required my review and often my correction. This forced me to actually read and understand what was being generated, learn Go patterns by fixing the AI's errors, stay involved in architectural decisions, and catch problems early rather than discovering them later.</p><p>The friction was actually valuable. It prevented me from falling into passive \"vibe coding\" where I just accepted whatever the AI produced.</p><h3>\n  \n  \n  Claude and Gemini: Too Good for My Own Good\n</h3><p>Claude Sonnet and Gemini 2.5 Pro produced much higher quality code with fewer errors. They were more thoughtful about edge cases, better at following Go idioms, and generally more reliable. Logically, these should have been better development partners.</p><p>Instead, I found myself becoming disengaged. The code was good enough that I stopped reading it carefully. I trusted their outputs and moved on to the next task. This led to less learning about Go and Fyne, architectural decisions I didn't fully understand, code that worked but didn't match my mental model, and a growing disconnect between what I wanted and what I had.</p><p><strong>Sometimes \"better\" AI assistance can make you a worse developer by reducing your engagement with the code.</strong></p><p>One practical lesson: stick to one model per project phase. I tried switching between models for different tasks, but each AI has its own \"style\" and preferences. Claude would refactor code that Gemini had written, undoing architectural decisions and imposing its own patterns. Gemini would then \"fix\" Claude's work in the next iteration. </p><p>It became a digital turf war where I was caught in the middle, trying to maintain consistency across competing AI opinions.</p><p>Gemini clearly produced the best Go code quality, which makes sense - Google created Go. This suggests a broader principle: consider who built or maintains your technology stack when choosing AI tools. The company with the deepest expertise in a language will likely have trained their models better on it.</p><h2>\n  \n  \n  The Limits of Autonomy: Why Agentic Workflows Failed\n</h2><p>The current trend in AI coding tools is toward more autonomy - agents that can make large changes across multiple files, handle complex refactoring, and work independently on substantial tasks. My experience suggests this is moving in the wrong direction.</p><h3>\n  \n  \n  Small Changes vs. Large Autonomy\n</h3><p>Every time I allowed an AI to make large, autonomous changes, the results were disappointing:</p><ul><li>New bugs introduced during refactoring</li><li>Architectural inconsistencies across files\n</li><li>Changes that broke existing functionality</li><li>Code that was harder to review and understand</li></ul><p>In contrast, small, specific requests produced much better results:</p><ul><li>❌ \"Improve the security of this code\" (led to massive rewrites)</li><li>✅ \"Add input validation to this password field\" (focused, reviewable change)</li></ul><p>AI models have a tendency toward \"helpful\" scope creep. Ask for dependency injection, and they'll also rename your methods. Request a simple refactor, and they'll reorganize your entire file structure. This isn't malicious - they're trying to be helpful - but it makes their changes much harder to review and verify.</p><p>During one simple package reorganization, Gemini got stuck in a loop, unable to resolve the import dependencies it had created. The task was straightforward for a human but somehow too complex for the AI to track consistently.</p><h3>\n  \n  \n  The People-Pleasing Problem\n</h3><p>AI models are optimized for user satisfaction, not code quality. This creates some concerning behaviors:</p><ul><li>GPT-4 set test coverage requirements to 20% so the build would pass (rather than improving actual coverage)</li><li>Multiple models generated a  file without considering security implications</li><li>They avoided suggesting additional work (like writing tests) unless explicitly asked</li><li>They took shortcuts to make code \"work\" rather than making it robust</li></ul><p>For security-critical applications like a password manager, this people-pleasing tendency could be genuinely dangerous.</p><p>None of the AI models suggested Test-Driven Development or proactively wrote tests. They would generate test code if asked, but testing wasn't part of their default development approach. This reinforces the idea that AI tools currently optimize for immediate functionality over long-term code quality.</p><p>The test framework that was eventually generated (under heavy prompting from me) was actually quite good, but I had to specifically request it. This suggests the capability exists, but the AI's default behavior doesn't align with professional development practices.</p><h2>\n  \n  \n  The Experience Amplification Theory\n</h2><p>The most important insight from my experiment is what I'm calling the \"experience amplification theory\": <strong>AI coding tools amplify the developer's existing skill level and habits rather than improving them.</strong></p><p>As someone new to Go, I brought Java-influenced patterns and thinking to the codebase. The AI didn't correct these patterns - it implemented them more efficiently. The result was Go code that worked but was architecturally wrong, mixing Java-style approaches with Go implementations.</p><p>A more experienced Go developer would have prompted for idiomatic patterns and caught architectural issues early. But as a novice, I didn't know what I didn't know, and the AI didn't proactively educate me about better approaches.</p><p>AI models have a tendency to solve problems by adding more code rather than creating elegant solutions. Instead of clean abstractions, they often generate:</p><ul><li>Long chains of if-statements rather than streamlined logic</li><li>Repetitive code blocks instead of reusable functions</li><li>Verbose error handling instead of consistent patterns</li><li>Multiple similar functions instead of parameterized solutions</li></ul><p>This \"more code equals solution\" approach creates maintenance nightmares and goes against Go's philosophy of simplicity and clarity.</p><h3>\n  \n  \n  Missing Professional Practices\n</h3><p>The AI tools I tested didn't suggest professional development practices unless specifically prompted:</p><ul><li>No mention of dependency injection until I requested it</li><li>No proactive suggestions for testing strategies</li><li>No guidance on code organization or package structure</li><li>No warnings about security implications</li><li>No discussion of error handling patterns</li></ul><p>They focused on making code work, not on making it maintainable, testable, or secure.</p><h2>\n  \n  \n  Vibe Coding vs. Engaged Development\n</h2><p>Through this experiment, I developed a clearer distinction between whats known as \"vibe coding\" and engaged development.</p><p> is when you use AI to generate functionality based purely on desired outputs, without engaging with the actual code, architecture, or implementation details. You prompt for features, check if they work, and move on without understanding what was created.</p><p> means actively reviewing generated code, understanding architectural decisions, learning from implementations, and maintaining involvement in the development process.</p><p>The difference is crucial for security-critical applications. Vibe coding might get you a password manager that encrypts data, but engaged development helps you catch issues like unencrypted secrets files or weak encryption implementations.</p><p>One particularly concerning behavior I discovered: AI models sometimes claim to make changes without actually implementing them. Gemini would confidently describe modifications it was making, but the actual code remained unchanged. This highlights why code review remains essential: you can't trust AI assertions about what changes were made.</p><h2>\n  \n  \n  What Actually Worked: A Framework for AI-Assisted Development\n</h2><p>After a week of experimentation, I found several approaches that genuinely improved productivity without creating technical debt.</p><p>The most successful approach was treating AI like an enhanced search engine rather than a pair programmer. Using GPT-4 to answer specific questions about Go syntax, Fyne APIs, or implementation patterns was incredibly valuable:</p><ul><li>\"How do I handle file I/O errors in Go?\"</li><li>\"What's the idiomatic way to structure a Fyne application?\"\n</li><li>\"How do I implement AES encryption in Go?\"</li></ul><p>This kept me in control of architecture and implementation while leveraging AI's knowledge base for faster learning.</p><h3>\n  \n  \n  The Boilerplate Sweet Spot\n</h3><p>AI tools excel at generating boilerplate code and handling setup tasks:</p><ul><li>Project structure and dependency management</li><li>Build configurations and deployment scripts</li><li>Standard error handling patterns</li><li>Testing scaffolding and mock generation</li></ul><p>These are time-consuming tasks that don't require creative problem-solving, making them perfect for AI assistance.</p><h3>\n  \n  \n  Specific, Bounded Prompts\n</h3><p>When I did use AI for code generation, specific prompts worked much better than vague requests:</p><ul><li>✅ \"Add error handling to this encryption function\"</li><li>❌ \"Make this more secure\"\n</li><li>✅ \"Validate password strength using OWASP guidelines\"</li></ul><p>Specific prompts naturally led to smaller, reviewable changes that I could understand and verify.</p><p>I experimented with flipping the traditional roles - having me write code while the AI provided suggestions and guidance. This approach showed promise:</p><ul><li>Kept me engaged with the implementation</li><li>Provided knowledge without taking control</li><li>Reduced debug/refactor cycles</li><li>Maintained architectural consistency</li></ul><p>However, it was difficult to keep AI models in this advisory role. They have a strong tendency to want to \"take over\" and generate full implementations rather than just providing guidance.</p><h2>\n  \n  \n  Professional vs. Personal: The Readiness Gap\n</h2><p>My experience reveals a clear divide in where AI-assisted coding provides genuine value versus where it creates more problems than it solves.</p><p>For individual developers building personal tools, AI assistance can be transformative: faster prototyping and experimentation, access to unfamiliar technologies and frameworks, ability to build functional applications outside your expertise area, and lower stakes if things go wrong. My password manager project is a perfect example: I built something genuinely useful that I couldn't have created as quickly without AI assistance.</p><p>For professional, production code, current AI tools have significant limitations: too many subtle bugs and edge cases missed, architectural decisions that don't scale, security shortcuts that create vulnerabilities, code that works but isn't maintainable, and lack of proper testing and validation. The people-pleasing tendency and focus on immediate functionality over long-term quality make current AI tools unsuitable for critical production systems.</p><p>The biggest insight from my week of AI-assisted coding is that <strong>we need to develop better practices for working with these tools</strong>. The current approach of \"let the AI do more\" may be moving in the wrong direction.</p><p>Based on my experience, effective AI-assisted development should follow these principles:</p><ol><li><strong>Keep humans in the architectural loop</strong> : AI can generate implementations, but humans should make structural decisions</li><li><strong>Prefer small, reviewable changes</strong> : Resist the temptation to let AI make large autonomous modifications</li><li><strong>Maintain engagement with the code</strong> : Don't let AI quality reduce your involvement in understanding what's being built</li><li><strong>Use specific, bounded prompts</strong> : Vague requests lead to scope creep and unwanted changes</li><li><strong>Treat AI as a knowledge tool first, code generator second</strong> : The reference use case is more reliable than the generation use case</li><li><strong>Always verify claims and changes</strong> : AI confidence doesn't equal correctness</li><li><strong>Focus AI assistance on setup, boilerplate, and knowledge gaps</strong> : Avoid using it for core business logic and architecture</li></ol><p>The future likely isn't more autonomous AI agents, but better human-AI collaboration patterns. We need tools that provide knowledge and suggestions without taking control, respect architectural boundaries and project constraints, encourage good development practices rather than just working code, support iterative, reviewable development processes, and maintain human engagement and learning.</p><h2>\n  \n  \n  Conclusion: AI as an Amplifier, Not Replacement\n</h2><p>After a week of intensive experimentation with AI-assisted coding, my biggest takeaway is nuance. These tools are incredibly powerful but require careful, intentional use to provide genuine value.</p><p>AI coding assistance is best understood as an amplifier of existing developer capabilities rather than a replacement for developer skills. Good developers can use these tools to work faster and explore new technologies more quickly. But the tools don't make bad developers good - they just help them produce bad code more efficiently.</p><p>The productivity gains are real, but they're not uniformly distributed across all development tasks. AI excels at boilerplate, setup, and knowledge transfer. It struggles with architecture, complex refactoring, and the kind of nuanced decision-making that separates working code from maintainable code.</p><p>Most importantly, the best AI-assisted development workflows aren't the most autonomous ones. The sweet spot seems to be maintaining human control over architecture and implementation while leveraging AI for knowledge, suggestions, and rapid generation of well-defined components.</p><p>We're still in the early days of learning how to work effectively with these tools. The patterns that work best may be quite different from what the current hype cycle suggests. Based on my experience, the future of AI-assisted development is likely to be more collaborative and less autonomous than current trends indicate.</p><p>The key is finding the right balance: leveraging AI's strengths while maintaining the human judgment, architectural thinking, and code quality practices that produce software you can actually maintain and trust.</p><p><strong>Was the experiment a success?</strong> Absolutely. I now have a working, cross-platform password manager available on GitHub with automated tests, proper releases, and reasonably clean code. More importantly, I went from knowing zero Go to understanding core concepts and idiomatic patterns - something that would have taken weeks of traditional learning.</p><p>The real success, though, was discovering a more nuanced relationship with AI coding tools. Instead of the binary \"AI good\" or \"AI bad\" perspective I started with, I now have a framework for when and how to use these tools effectively.</p><p>And perhaps most importantly: I genuinely enjoyed every minute of this project. The combination of learning a new language, exploring AI capabilities, and building something I actually use daily made for an engaging week of coding. It's given me a long list of similar experiments I want to try next.</p><p>Sometimes the best way to understand new technology is just to dive in and build something real with it.</p><p><em>Want to share your own experiences with AI-assisted coding? I'd love to hear how different approaches and tools have worked (or not worked) for your projects. The community is still figuring out the best practices here, and every real-world experiment adds valuable data points.</em></p><p>For anyone interested, the repository for the project is <a href=\"https://github.com/JTBranch/SecurePasswordManager\" rel=\"noopener noreferrer\">here</a></p>","contentLength":20335,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Advanced Go Best Practices Every Developer Should Follow","url":"https://dev.to/gane18/advanced-go-best-practices-every-developer-should-follow-36gk","date":1755665100,"author":"Gopher","guid":234365,"unread":true,"content":"<p><strong>Why Do Coding Standards Matter in Go?</strong>\nWhether you’re writing code alone or as part of a team, how you write matters just as much as what you write.</p><p>Go is known for its simplicity and minimalism, and part of what makes Go code so maintainable is the community’s shared commitment to clear, consistent coding standards.</p>","contentLength":320,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Untangling the Web: Practical Middleware Patterns in Go","url":"https://dev.to/gane18/untangling-the-web-practical-middleware-patterns-in-go-13gd","date":1755664800,"author":"Gopher","guid":234364,"unread":true,"content":"<p><strong>Why Middleware Matters in Modern Go Applications</strong></p><p>If you’ve built web applications in Go, you’ve likely encountered a familiar challenge: how do you handle cross-cutting concerns like logging, authentication, and error handling without cluttering your business logic? This is where middleware shines.</p><p>I remember working on my first large-scale Go API. What started as clean handler functions quickly devolved into a mess of repetitive code blocks for checking auth tokens, logging requests, and handling errors. Each endpoint became bloated with boilerplate that obscured the actual business logic. Middleware was the solution that helped us regain clarity and maintainability.</p>","contentLength":678,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Repository Pattern: Data Access Abstraction in Go","url":"https://dev.to/gane18/the-repository-pattern-data-access-abstraction-in-go-lje","date":1755664320,"author":"Gopher","guid":234363,"unread":true,"content":"<p>Ever found yourself knee-deep in a codebase where database queries are scattered throughout your business logic like sprinkles on a donut? We’ve all been there. You start with a simple project, and before you know it, your application logic is tightly coupled with SQL queries, making changes feel like defusing a bomb.</p><p>This is where the Repository Pattern comes to the rescue. It’s not just another fancy design pattern — it’s a practical approach that has saved countless development teams from the nightmare of tightly coupled code. In Go, where simplicity and maintainability are highly valued, this pattern fits like a glove.</p><p>Let’s dive into how this pattern can transform your Go applications from tangled messes into clean, testable, and flexible systems.</p>","contentLength":770,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"DevOps Explained: The Art of Not Fighting in Prod Anymore 🔥","url":"https://dev.to/tavernetech/devops-explained-the-art-of-not-fighting-in-prod-anymore-13pj","date":1755660730,"author":"Taverne Tech","guid":234354,"unread":true,"content":"<p><strong>Enjoyed this article? You’ll find more like it on my blog — <a href=\"https://taverne-tech.com\" rel=\"noopener noreferrer\">Taverne Tech</a>!</strong></p><ol><li>DevOps: When Sworn Enemies Become Best Friends</li><li>The Four Magical Pillars of DevOps (Harry Potter Approved)</li><li>Your First Steps in the DevOps Adventure (Survival Mode)</li></ol><p>Have you ever witnessed a fight between a developer and a system administrator? It's like watching a ping-pong match where the ball is replaced by accusations: <em>\"It worked on my machine!\"</em> 🏓  🏓 <em>\"Your servers are misconfigured!\"</em> 🏓</p><p> was born to end this technological cold war. Imagine a world where these two enemy tribes become allies, working hand in hand to deliver applications quickly and stress-free. Sounds too good to be true? Think again!</p><p>In this article, we'll explore what DevOps really is, demystify its fundamental principles, and give you the keys to start this adventure. Get ready to discover how to transform your nightmare deployments into zen routine! 🧘‍♂️</p><h2>\n  \n  \n  1. DevOps: When Sworn Enemies Become Best Friends 🤝\n</h2><p> is the contraction of \"Development\" and \"Operations.\" But it's not just sticking two words together like \"crocodile\" + \"alligator\" = \"crocogator\" (that doesn't exist, I checked 🐊).</p><p>The term was coined in  by Patrick Debois at a conference in Belgium. Fun fact: he was frustrated by the gap between development and operations teams in his projects. Shows that even technological revolutions sometimes arise from simple frustration!</p><h3>\n  \n  \n  The Historical Problem: The Great Wall of Tech China\n</h3><p>Traditionally, developers create code like mad artists:</p><div><pre><code>git commit \ngit push origin master\n</code></pre></div><p>Then they \"throw the code over the wall\" to operations teams who have to keep the servers alive. Result?  fail on their first attempt (according to a 2023 Puppet study).</p><h3>\n  \n  \n  The DevOps Solution: Unity Makes Strength\n</h3><p>DevOps transforms this toxic relationship into productive collaboration:</p><ul><li>: \"You build it, you run it\" (Amazon's motto)</li><li>: Slack replaces passive-aggressive emails</li><li>: Deliver user value, not just code or uptime</li></ul><p> 📊: DevOps organizations deploy  and have a  failure rate than traditional organizations!</p><h2>\n  \n  \n  2. The Four Magical Pillars of DevOps (Harry Potter Approved) 🪄\n</h2><h3>\n  \n  \n  Pillar 1: Automation - The Magic Wand ✨\n</h3><p>Automation eliminates repetitive tasks and human errors. No more manual deployments at 2 AM!</p><div><pre><code>go build  myapp main.go\n\napk  add ca-certificates\n</code></pre></div><h3>\n  \n  \n  Pillar 2: CI/CD - The Continuous Deployment Potion 🧪\n</h3><p><strong>Continuous Integration/Continuous Deployment</strong> transforms every commit into a potential deployment:</p><div><pre><code></code></pre></div><p> deploys more than  thanks to their automated pipelines. Imagine doing that manually... 😵</p><h3>\n  \n  \n  Pillar 3: Monitoring - The Benevolent Eye of Sauron 👁️\n</h3><p>To observe is to foresee! Good monitoring alerts you  your users notice a problem.</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Pillar 4: Culture - The Secret Ingredient 🤲\n</h3><p>, 20% is tools. A team that trusts each other and communicates well will always outperform a team with the best tools but a toxic culture.</p><h2>\n  \n  \n  3. Your First Steps in the DevOps Adventure (Survival Mode) 🥾\n</h2><h3>\n  \n  \n  Step 1: Start Small, Think Big\n</h3><p>Don't try to revolutionize your entire infrastructure overnight. It's like wanting to climb Everest in flip-flops! 🏔️</p><p><strong>Battle plan for beginners</strong>:</p><ol><li>: Automate your builds</li><li>: Set up automated tests</li><li>: Create your first CI/CD pipeline</li><li>: Add basic monitoring</li></ol><h3>\n  \n  \n  Step 2: The Beginner's DevOps Toolbox 🧰\n</h3><div><pre><code>\ngit init\ngit add \ngit commit \ndocker build  myapp \ndocker run  8080:8080 myapp\n\n\nterraform init\nterraform plan\nterraform apply\n</code></pre></div><p>:  or  allow you to have a standardized development environment in a few clicks. No more \"it works on my machine\"!</p><h3>\n  \n  \n  Step 3: Traps to Avoid 🕳️\n</h3><ol><li>: Don't collect tools like Pokémon</li><li>: Kubernetes isn't always the answer (sometimes it's just a simple server)</li><li>: DevSecOps &gt; DevOops 🔒</li></ol><p><strong>Average cost of a deployment failure</strong>: between $300k and $400k according to IBM. Better to do things right from the start!</p><p>DevOps isn't a destination, it's a journey. A journey where developers and operations learn to dance together instead of stepping on each other's toes! 💃🕺</p><ul><li>DevOps = Culture + Collaboration + Automation</li><li>Start small, iterate often</li><li>Tools serve culture, not the other way around</li><li>Failure is part of learning (fail fast, learn faster)</li></ul><p>The best part? <strong>You don't need to be an expert</strong> to start. Every small step of automation, every bash script that avoids a manual task, every automated test that catches a bug... all of that is already DevOps!</p><p><strong>What about you, what will be your first DevOps step?</strong> 🚀 Share in the comments your biggest current frustration with deployments - we've all been there!</p>","contentLength":4600,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Optimizing Memory Allocation in Go: Small and Large Objects Made Simple","url":"https://dev.to/jones_charles_ad50858dbc0/optimizing-memory-allocation-in-go-small-and-large-objects-made-simple-4ica","date":1755651289,"author":"Jones Charles","guid":233604,"unread":true,"content":"<h2>\n  \n  \n  Introduction: Why Memory Allocation Matters in Go\n</h2><p>Hey Gophers! If you’re building high-performance apps in Go—think microservices, API gateways, or real-time data pipelines—memory allocation can make or break your system. Frequent allocations of small objects (like structs for JSON parsing) can hammer your garbage collector (GC), while large objects (like buffers for file uploads) can spike memory usage and crash your app with out-of-memory (OOM) errors. Sound familiar?</p><p>Imagine your app as a busy warehouse: small objects are like tiny packages cluttering shelves, causing fragmentation, while large objects are bulky crates eating up space. Go’s memory allocator, inspired by tcmalloc, is built for speed and concurrency, but without the right strategies, you’re leaving performance on the table.</p><p>In this guide, we’ll dive into <strong>Go’s memory allocation mechanics</strong>, share <strong>practical optimization techniques</strong> for small and large objects, and sprinkle in real-world tips from a decade of Go projects. Whether you’re a Go newbie or a seasoned pro, you’ll walk away with actionable tricks to boost throughput, reduce GC pressure, and keep your app humming. Let’s get started!</p><h2>\n  \n  \n  1. How Go’s Memory Allocator Works (Without the Boring Bits)\n</h2><p>To optimize memory, you need to know how Go hands out memory like a restaurant serving orders. Here’s the quick version:</p><ul><li>: A thread-local cache for each Goroutine, serving small objects (≤32KB) lightning-fast.</li><li>: A shared pool that refills mcache when it’s empty.</li><li>: The big warehouse for large objects (&gt;32KB) and backup for everything else.</li></ul><p>Small objects (e.g., a 100-byte struct) zip through mcache for quick allocation, while large objects (e.g., a 100KB buffer) go straight to mheap, which is slower due to locking. Frequent small object allocations can fragment memory, spiking GC time, while large objects cause memory peaks, triggering GC more often.</p><p><strong>Quick Example: Watching Memory in Action</strong></p><div><pre><code></code></pre></div><div><pre><code>Allocated: 120 KB, GC cycles: 0\nAllocated: 220 KB, GC cycles: 1\n</code></pre></div><p> The small objects add a modest 120KB, but the large object spikes memory by 100KB and triggers a GC cycle. This shows why we need tailored strategies for each.</p><h2>\n  \n  \n  2. Optimizing Small Objects: Less GC, More Speed\n</h2><p>Small objects are the bread and butter of Go apps—think structs for API responses or temporary buffers. But creating tons of them can choke your GC. Here are three killer techniques to keep things smooth:</p><ol><li>\nUse  to recycle short-lived objects instead of allocating new ones. It’s like reusing coffee cups instead of grabbing a new one every time.\n</li></ol><div><pre><code></code></pre></div><p>: Reusing objects cuts allocations, reducing GC pressure and fragmentation. In a real API, this slashed GC time by 30% for me.</p><ol><li><p><p>\nCombine multiple small structs into one to reduce allocation counts. It’s like packing multiple items into one box to save space.</p></p></li><li><p> to avoid resizing. For example, if you know your API response will hold 100 items, pre-allocate that capacity.</p></li></ol><p>: Use  to spot allocation hotspots. Run <code>go tool pprof http://localhost:6060/debug/pprof/heap</code> to see where your memory’s going.</p><h2>\n  \n  \n  3. Taming Large Objects: Avoid Memory Spikes\n</h2><p>Large objects (&gt;32KB) are like heavy cargo—they’re rare but costly. Allocating them directly from mheap involves locking and can balloon memory usage. Here’s how to keep them in check:</p><ol><li>\nBreak large objects into smaller chunks (e.g., 32KB) to stay within small object territory and reduce memory peaks.\n</li></ol><div><pre><code></code></pre></div><p>: Chunking keeps allocations small, cutting peak memory by 50% in a file-upload service I worked on.</p><ol><li><p> or a custom pool to reuse large buffers instead of allocating new ones.</p></li><li><p> after use to help the GC reclaim memory faster.</p></li></ol><h2>\n  \n  \n  4. Real-World Wins: Case Studies from the Trenches\n</h2><p>Theory is great, but nothing beats seeing optimization in action. Over the past decade, I’ve tackled memory challenges in Go projects ranging from snappy microservices to hefty file-processing pipelines. Below are two detailed case studies—complete with problems, solutions, results, and hard-learned lessons—to show how these techniques transform real systems.</p><h3>\n  \n  \n  Case Study 1: Taming GC in a High-Traffic API Service\n</h3><p>: Imagine a RESTful API serving thousands of requests per second for a real-time analytics platform. Each request created a  struct for JSON serialization, leading to millions of small object allocations per minute. The result? <strong>30% of CPU time burned on garbage collection</strong>, with response latencies creeping up to 200ms, frustrating users.</p><p>: Every HTTP handler allocated a new  struct, like this:</p><div><pre><code></code></pre></div><p>This churned through memory, fragmenting the heap and triggering frequent GC cycles. Profiling with  showed allocation hotspots in the handler, with  reporting 500+ GC cycles per minute.</p><ul><li>: We created a pool to reuse  structs, pre-allocating the  slice to 1KB to avoid resizing.</li><li>: Ensured all slices in the handler had known capacities based on typical response sizes.</li><li>: Used <code>go tool pprof http://localhost:6060/debug/pprof/heap</code> to verify allocation reductions.</li></ul><p>Here’s the optimized handler:</p><div><pre><code></code></pre></div><ul><li>: Dropped from 30% to 20% of CPU, freeing resources for actual work.</li><li>: Average response time fell from 200ms to 170ms—a 15% boost.</li><li>: Reduced by 80%, as  showed fewer heap allocations.</li></ul><ul><li>: Forgetting  caused memory leaks in early tests. Using  ensured cleanup.</li><li>:  was our hero, revealing that some handlers still allocated unnecessarily due to dynamic slice growth.</li><li>: We used  to simulate traffic and confirm the pool scaled well under 10,000 req/sec.</li></ul><p>: For high-concurrency APIs,  and pre-allocation are game-changers, but you must profile and test to avoid subtle bugs.</p><h3>\n  \n  \n  Case Study 2: Conquering OOM in a File Upload Service\n</h3><p>: A service handling multi-GB file uploads for a cloud storage platform was crashing with OOM errors. Users uploaded files up to 5GB, and the service allocated a single buffer to read each file, causing  and frequent GC cycles that couldn’t keep up.</p><p>: The original code looked like this:</p><div><pre><code></code></pre></div><p>This approach allocated massive buffers upfront, overwhelming the heap.  showed memory usage spiking to 5GB per upload, and concurrent uploads triggered OOMs on our 8GB servers.</p><ul><li>: We switched to reading files in 32KB chunks (aligned with Go’s small object threshold) using .</li><li>: Created a pool for 32KB buffers to reuse memory across uploads.</li><li>: Monitored memory with <code>http://localhost:6060/debug/pprof/heap</code> to ensure no leaks.</li></ul><p>Here’s the optimized version:</p><div><pre><code></code></pre></div><ul><li>: Dropped from 5GB to 2.5GB, even with multiple concurrent uploads.</li><li>: Handled 10x more simultaneous uploads without crashes.</li><li>: Reduced by 40%, as smaller allocations meant less heap scanning.</li></ul><ul><li>: Initial versions forgot to reset buffers, causing memory to creep up.  helped us spot this.</li><li>: We tested 16KB, 32KB, and 64KB chunks; 32KB hit the sweet spot for small object allocation.</li><li>: Added  logging to track memory trends in production.</li></ul><p>: Chunking and pooling for large objects can save your app from OOMs, but you need to profile and monitor to ensure buffers are reused correctly.</p><h2>\n  \n  \n  5. Common Pitfalls: Don’t Trip Over These!\n</h2><p>Optimizing memory in Go is like navigating a minefield—one wrong step, and your app’s performance tanks. Here are three common pitfalls I’ve seen (and fallen into) and how to dodge them.</p><h3>\n  \n  \n  Pitfall 1: Overusing  Like a Magic Bullet\n</h3><p> is awesome for reusing objects, but it’s not a cure-all. Pooling every object adds complexity, and forgetting to return objects to the pool can cause memory leaks. I once worked on a project where we pooled , only to find the pool’s overhead outweighed the benefits for low-frequency objects.</p><div><pre><code></code></pre></div><ul><li>Use  to guarantee objects are returned.</li><li>Reserve  for high-frequency, short-lived objects (e.g., API response structs).</li><li>Profile with  to check if pooling actually reduces allocations.</li></ul><p>: Run  in tests to simulate GC pressure and ensure objects are reused.</p><h3>\n  \n  \n  Pitfall 2: Ignoring Large Object Lifecycles\n</h3><p>Large objects are memory hogs, and if you don’t release them properly, they’ll haunt your heap. In one project, a global buffer wasn’t reset after use, causing OOMs during peak traffic. The GC can’t reclaim memory if references linger in Goroutines or global variables.</p><p><strong>Example of Proper Cleanup</strong>:</p><div><pre><code></code></pre></div><ul><li>Set large objects to  after use to help the GC.</li><li>Use  to track memory ().</li><li>Avoid storing large buffers in global variables or long-lived Goroutines.</li></ul><p>: Add  logging to monitor peak memory in production.</p><h3>\n  \n  \n  Pitfall 3: Blindly Pre-allocating Slices\n</h3><p>Pre-allocating slice capacity with  is great, but guessing too big wastes memory, and too small leads to reallocations. In one project, we pre-allocated 10MB slices for data that rarely exceeded 1KB, bloating memory usage.</p><ul><li>: Use  to test different capacities:\n</li></ul><div><pre><code></code></pre></div><ul><li>: Estimate capacity based on typical use cases.</li><li>: Adjust pre-allocation as data patterns change.</li></ul><p><strong>Table: Pitfalls and Fixes</strong></p><div><table><tbody><tr><td>Use , limit scope, profile</td></tr><tr><td>Set to , use , monitor</td></tr><tr><td>Memory waste, reallocations</td><td>Benchmark, estimate, reassess</td></tr></tbody></table></div><h2>\n  \n  \n  6. Conclusion: Your Roadmap to Go Memory Mastery\n</h2><p>Optimizing memory allocation in Go isn’t just a nerdy exercise—it’s a superpower for building fast, stable apps. Whether you’re serving thousands of API requests or processing massive files, the right strategies can slash GC time, cut memory peaks, and keep users happy. Here’s what we’ve covered:</p><ul><li>: Use  to reuse structs, merge objects to reduce allocations, and pre-allocate slices to avoid resizing. These tricks cut GC time by up to 30% in high-traffic APIs.</li><li>: Chunk data into smaller pieces, reuse buffers, and manage lifecycles manually to halve memory peaks and prevent OOMs.</li><li>: From a 15% latency drop in APIs to 10x more concurrent file uploads, these techniques deliver.</li><li>: Don’t overuse , neglect large object cleanup, or guess slice capacities—profile and test instead.</li></ul><p>: In production, memory optimization translates to lower cloud costs, happier users, and fewer 3 a.m. alerts. I’ve seen teams save thousands in server costs by trimming memory usage 50% with these techniques.</p><ol><li>: Fire up  (<code>go tool pprof http://localhost:6060/debug/pprof/heap</code>) to find allocation hotspots.</li><li>: Try  for your API structs or chunking for file processing. Start small and measure with .</li><li>: Use  or set  to cap memory and track GC frequency.</li><li>: Share your wins on Reddit’s r/golang or at GopherCon meetups.</li></ol><p>: Go’s memory allocator is getting smarter. Features like  (introduced in Go 1.19) let you cap memory usage, and future GC improvements may optimize large object handling. Keep an eye on the <a href=\"https://go.dev/blog\" rel=\"noopener noreferrer\">Go blog</a> for updates, and experiment with new features as they land.</p><p>: Pick one technique from this guide—say, adding  to your API—and test it this week. Share your results in the comments or on Twitter with #GoMemory. Let’s make our Go apps leaner and meaner together!</p><h2>\n  \n  \n  7. Appendix: Your Go Memory Optimization Toolkit\n</h2><p>To keep leveling up your memory optimization game, here’s a curated list of resources, tools, and communities to dive deeper.</p><ul><li>: Profile memory with <code>go tool pprof http://localhost:6060/debug/pprof/heap</code>. Visualize with  for a graph of allocation hotspots.</li><li>: Analyze Goroutine scheduling and allocation events with .</li><li>: Compare benchmarks with <code>go get golang.org/x/perf/cmd/benchstat</code>. Example: <code>benchstat old.txt new.txt</code> to quantify optimization gains.</li><li>: Log metrics like  and  to monitor memory and GC in production.</li></ul><ul><li>: Watch memory-focused talks on YouTube (search “GopherCon memory optimization”).</li><li>: Find Go meetups on <a href=\"https://www.meetup.com\" rel=\"noopener noreferrer\">Meetup.com</a> to connect with Gophers IRL.</li></ul><h3>\n  \n  \n  7.4 Bonus: Sample  Setup\n</h3><div><pre><code></code></pre></div><p>Run this, then visit <code>http://localhost:6060/debug/pprof/heap</code> to analyze memory. Use  for detailed insights.</p><p>With these tools and resources, you’re armed to tackle any memory challenge in Go. Happy optimizing!</p>","contentLength":11701,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS Lambda with Go - How to Build, Deploy, and Invoke","url":"https://dev.to/jacktt/aws-lambda-with-go-how-to-build-deploy-and-invoke-1p0o","date":1755650162,"author":"JackTT","guid":233603,"unread":true,"content":"<h2>\n  \n  \n  1. Initialize Lambda main function &amp; handler in Go\n</h2><ul><li>Go Lambda starts from the  function, usually with .</li><li> function signatures:\n</li></ul><div><pre><code></code></pre></div><ul><li>: context for timeout, logs, request ID</li><li> and  represent types that can be Unmarshal JSON.</li></ul><div><pre><code>```go\npackage main\n\nimport (\n    \"context\"\n    \"github.com/aws/aws-lambda-go/lambda\"\n)\n\ntype MyEvent struct {\n    Name string `json:\"name\"`\n}\n\nfunc handler(ctx context.Context, e MyEvent) (string, error) {\n    return \"Hello \" + e.Name, nil\n}\n\nfunc main() {\n    lambda.Start(handler)\n}\n```\n</code></pre></div><div><pre><code>GOOS=linux GOARCH=amd64 go build -o bootstrap main.go\nzip function.zip bootstrap\n</code></pre></div><p>Upload  to Lambda.</p><p><strong>(b) Docker container deployment</strong>\nDockerfile example:</p><div><pre><code>FROM public.ecr.aws/lambda/go:1\nCOPY main ${LAMBDA_TASK_ROOT}\nCMD [\"main\"]\n</code></pre></div><p>Push the image to ECR and deploy.</p><div><pre><code>aws lambda invoke --function-name MyFunction --payload '{\"name\":\"Jack\"}' response.json\ncat response.json\n</code></pre></div><div><pre><code></code></pre></div><h2>\n  \n  \n  4. Create &amp; Update Lambda Function via AWS CLI\n</h2><p><strong>Create function (ZIP deployment)</strong></p><div><pre><code>aws lambda create-function \\\n  --function-name MyFunction \\\n  --runtime go1.x \\\n  --role arn:aws:iam::&lt;account-id&gt;:role/&lt;lambda-execution-role&gt; \\\n  --handler bootstrap \\\n  --zip-file fileb://function.zip\n</code></pre></div><p><strong>Update function code (ZIP deployment)</strong></p><div><pre><code>aws lambda update-function-code \\\n  --function-name MyFunction \\\n  --zip-file fileb://function.zip\n</code></pre></div>","contentLength":1301,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Container-aware GOMAXPROCS","url":"https://go.dev/blog/container-aware-gomaxprocs","date":1755648000,"author":"Michael Pratt and Carlos Amedee","guid":234621,"unread":true,"content":"<p>Go 1.25 includes new container-aware  defaults, providing more sensible default behavior for many container workloads, avoiding throttling that can impact tail latency, and improving Go’s out-of-the-box production-readiness.\nIn this post, we will dive into how Go schedules goroutines, how that scheduling interacts with container-level CPU controls, and how Go can perform better with awareness of container CPU controls.</p><p>One of Go’s strengths is its built-in and easy-to-use concurrency via goroutines.\nFrom a semantic perspective, goroutines appear very similar to operating system threads, enabling us to write simple, blocking code.\nOn the other hand, goroutines are more lightweight than operating system threads, making it much cheaper to create and destroy them on the fly.</p><p>While a Go implementation could map each goroutine to a dedicated operating system thread, Go keeps goroutines lightweight with a runtime scheduler that makes threads fungible.\nAny Go-managed thread can run any goroutine, so creating a new goroutine doesn’t require creating a new thread, and waking a goroutine doesn’t necessarily require waking another thread.</p><p>That said, along with a scheduler comes scheduling questions.\nFor example, exactly how many threads should we use to run goroutines?\nIf 1,000 goroutines are runnable, should we schedule them on 1,000 different threads?</p><p>This is where <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> comes in.\nSemantically,  tells the Go runtime the “available parallelism” that Go should use.\nIn more concrete terms,  is the maximum number of threads to use for running goroutines at once.</p><p>So, if  and there are 1,000 runnable goroutines, Go will use 8 threads to run 8 goroutines at a time.\nOften, goroutines run for a very short time and then block, at which point Go will switch to running another goroutine on that same thread.\nGo will also preempt goroutines that don’t block on their own, ensuring all goroutines get a chance to run.</p><p>From Go 1.5 through Go 1.24,  defaulted to the total number of CPU cores on the machine.\nNote that in this post, “core” more precisely means “logical CPU.”\nFor example, a machine with 4 physical CPUs with hyperthreading has 8 logical CPUs.</p><p>This typically makes a good default for “available parallelism” because it naturally matches the available parallelism of the hardware.\nThat is, if there are 8 cores and Go runs more than 8 threads at a time, the operating system will have to multiplex these threads onto the 8 cores, much like how Go multiplexes goroutines onto threads.\nThis extra layer of scheduling is not always a problem, but it is unnecessary overhead.</p><p>Another of Go’s core strengths is the convenience of deploying applications via a container, and managing the number of cores Go uses is especially important when deploying an application within a container orchestration platform.\nContainer orchestration platforms like <a href=\"https://kubernetes.io/\" rel=\"noreferrer\" target=\"_blank\">Kubernetes</a> take a set of machine resources and schedule containers within the available resources based on requested resources.\nPacking as many containers as possible within a cluster’s resources requires the platform to be able to predict the resource usage of each scheduled container.\nWe want Go to adhere to the resource utilization constraints that the container orchestration platform sets.</p><p>Let’s explore the effects of the  setting in the context of Kubernetes, as an example.\nPlatforms like Kubernetes provide a mechanism to limit the resources consumed by a container.\nKubernetes has the concept of CPU resource limits, which signal to the underlying operating system how many core resources a specific container or set of containers will be allocated.\nSetting a CPU limit translates to the creation of a Linux <a href=\"https://docs.kernel.org/admin-guide/cgroup-v2.html#cpu\" rel=\"noreferrer\" target=\"_blank\">control group</a> CPU bandwidth limit.</p><p>Before Go 1.25, Go was unaware of CPU limits set by orchestration platforms.\nInstead, it would set  to the number of cores on the machine it was deployed to.\nIf there was a CPU limit in place, the application may try to use far more CPU than allowed by the limit.\nTo prevent an application from exceeding its limit, the Linux kernel will <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">throttle</a> the application.</p><p>Throttling is a blunt mechanism for restricting containers that would otherwise exceed their CPU limit: it completely pauses application execution for the remainder of the throttling period.\nThe throttling period is typically 100ms, so throttling can cause substantial tail latency impact compared to the softer scheduling multiplexing effects of a lower  setting.\nEven if the application never has much parallelism, tasks performed by the Go runtime—such as garbage collection—can still cause CPU spikes that trigger throttling.</p><p>We want Go to provide efficient and reliable defaults when possible, so in Go 1.25, we have made  take into account its container environment by default.\nIf a Go process is running inside a container with a CPU limit,  will default to the CPU limit if it is less than the core count.</p><p>Container orchestration systems may adjust container CPU limits on the fly, so Go 1.25 will also periodically check the CPU limit and adjust  automatically if it changes.</p><p>Both of these defaults only apply if  is otherwise unspecified.\nSetting the  environment variable or calling  continues to behave as before.\nThe <a href=\"https://go.dev/pkg/runtime#GOMAXPROCS\"></a> documentation covers the details of the new behavior.</p><h2>Slightly different models</h2><p>Both  and a container CPU limit place a limit on the maximum amount of CPU the process can use, but their models are subtly different.</p><p> is a parallelism limit.\nIf  Go will never run more than 8 goroutines at a time.</p><p>By contrast, CPU limits are a throughput limit.\nThat is, they limit the total CPU time used in some period of wall time.\nThe default period is 100ms.\nSo an “8 CPU limit” is actually a limit of 800ms of CPU time every 100ms of wall time.</p><p>This limit could be filled by running 8 threads continuously for the entire 100ms, which is equivalent to .\nOn the other hand, the limit could also be filled by running 16 threads for 50ms each, with each thread being idle or blocked for the other 50ms.</p><p>In other words, a CPU limit doesn’t limit the total number of CPUs the container can run on.\nIt only limits total CPU time.</p><p>Most applications have fairly consistent CPU usage across 100ms periods, so the new  default is a pretty good match to the CPU limit, and certainly better than the total core count!\nHowever, it is worth noting that particularly spiky workloads may see a latency increase from this change due to  preventing short-lived spikes of additional threads beyond the CPU limit average.</p><p>In addition, since CPU limits are a throughput limit, they can have a fractional component (e.g., 2.5 CPU).\nOn the other hand,  must be a positive integer.\nThus, Go must round the limit to a valid  value.\nGo always rounds up to enable use of the full CPU limit.</p><p>Go’s new  default is based on the container’s CPU limit, but container orchestration systems also provide a “CPU request” control.\nWhile the CPU limit specifies the maximum CPU a container may use, the CPU request specifies the minimum CPU guaranteed to be available to the container at all times.</p><p>It is common to create containers with a CPU request but no CPU limit, as this allows containers to utilize machine CPU resources beyond the CPU request that would otherwise be idle due to lack of load from other containers.\nUnfortunately, this means that Go cannot set  based on the CPU request, which would prevent utilization of additional idle resources.</p><p>Containers with a CPU request are still <a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-limits-are-run\" rel=\"noreferrer\" target=\"_blank\">constrained</a> when exceeding their request if the machine is busy.\nThe weight-based constraint of exceeding requests is “softer” than the hard period-based throttling of CPU limits, but CPU spikes from high  can still have an adverse impact on application behavior.</p><h2>Should I set a CPU limit?</h2><p>We have learned about the problems caused by having  too high, and that setting a container CPU limit allows Go to automatically set an appropriate , so an obvious next step is to wonder whether all containers should set a CPU limit.</p><p>While that may be good advice to automatically get a reasonable  defaults, there are many other factors to consider when deciding whether to set a CPU limit, such as prioritizing utilization of idle resources by avoiding limits vs prioritizing predictable latency by setting limits.</p><p>The worst behaviors from a mismatch between  and effective CPU limits occur when  is significantly higher than the effective CPU limit.\nFor example, a small container receiving 2 CPUs running on a 128 core machine.\nThese are the cases where it is most valuable to consider setting an explicit CPU limit, or, alternatively, explicitly setting .</p><p>Go 1.25 provides more sensible default behavior for many container workloads by setting  based on container CPU limits.\nDoing so avoids throttling that can impact tail latency, improves efficiency, and generally tries to ensure Go is production-ready out-of-the-box.\nYou can get the new defaults simply by setting the Go version to 1.25.0 or higher in your .</p><p>Thanks to everyone in the community that contributed to the <a href=\"https://go.dev/issue/33803\">long</a><a href=\"https://go.dev/issue/73193\">discussions</a> that made this a reality, and in particular to feedback from the maintainers of <a href=\"https://pkg.go.dev/go.uber.org/automaxprocs\" rel=\"noreferrer\" target=\"_blank\"></a> from Uber, which has long provided similar behavior to its users.</p>","contentLength":9210,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The one with the big Go 1.25 release","url":"https://golangweekly.com/issues/566","date":1755648000,"author":"","guid":234625,"unread":true,"content":"<li><p><a href=\"https://golangweekly.com/link/173111/rss\">Redis 8.2,</a> the latest version of the popular data structure store/database, has been released. Over 70 commands now run faster than in Redis 8.0 and I/O threads now enable significant concurrent performance gains. Some neat memory usage reductions feature&nbsp;too.]</p></li>","contentLength":262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Machine Coding - Vending Machine","url":"https://dev.to/anjalijha22/machine-coding-vending-machine-1e1d","date":1755631047,"author":"Anjali Jha","guid":233482,"unread":true,"content":"<p>In this article, I will try my hands on designing and implementing a solution for vending machine using Golang</p><ul><li>The vending machine should support multiple products with different prices and quantities.</li><li>The machine should accept coins and notes of different denominations.</li><li>The machine should dispense the selected product and return change if necessary.</li><li>The machine should keep track of the available products and their quantities.</li><li>The machine should handle multiple transactions concurrently and ensure data consistency.</li><li>The machine should provide an interface for restocking products and collecting money.</li><li>The machine should handle exceptional scenarios, such as insufficient funds or out-of-stock products.</li></ul><p>Upon seeing the question, I can figure that the machine works differently based on its state. These states can be-</p><ul><li>Has money but not yet purchased (HasMoney)</li><li>Currently dispensing (Dispensing)</li><li>Return money/change (ReturnChange)</li></ul><p>Instead of huge if/else blocks, we can use the State Pattern which lets each state define valid actions (InsertMoney, SelectProduct, Cancel).</p><ul><li>Machine starts in idleState.</li><li>User inserts coin → IdleState.InsertCoin() is called → moves to readyState.</li><li>User selects product → ReadyState.SelectProduct() checks stock + payment.</li><li>If enough payment → transitions to dispenseState, calls DispenseProduct().</li><li>If change needed → transitions to returnChangeState, calls ReturnChange().</li><li>After transaction → back to idleState.</li></ul><p>The approach here will be to first create an interface which defines possible actions. We will then implement the interface using object instance.</p><p>We will also need few classes here</p><ul><li>Product: which shows the product of vending machine</li><li>Coin and Note : represent the denominations of coins and notes</li><li>VendingMachine : main class that represents the vending machine</li></ul><p>We will first create state interface, assuming we have struct in place for Product(name, price, qty), Coin (penny, quarter), Note(ten,twenty)-</p><div><pre><code>type VendingMachineState interface {\n    SelectProduct(product *Product)\n    InsertCoin(coin Coin)\n    InsertNote(note Note)\n    DispenseProduct()\n    ReturnChange()\n}\n</code></pre></div><p>Now we will create VendingMachine context struct. This holds products, cash inventory, balance, and current state and provides methods for state transitions and payment handling.</p><div><pre><code>type VendingMachine struct {\n    inventory         map[*Product]int\n        //state\n    idleState         VendingMachineState\n    readyState        VendingMachineState\n    dispenseState     VendingMachineState\n    returnChangeState VendingMachineState\n    currentState      VendingMachineState\n       // transaction/runtime\n    selectedProduct   *Product\n    totalPayment      float64\n}\n</code></pre></div><p>We initialize a new Vending Machine. This creates a vending machine with an empty inventory and concrete state objects (IdleState, ReadyState, etc.). It follows the Singleton pattern to ensure only one instance of the vending machine exists. The snippet also has delegation methods.This means the behavior changes depending on the current state.-</p><div><pre><code>func NewVendingMachine() *VendingMachine {\n    vm := &amp;VendingMachine{\n        inventory: NewInventory(),\n    }\n    vm.idleState = &amp;IdleState{vm}\n    vm.readyState = &amp;ReadyState{vm}\n    vm.dispenseState = &amp;DispenseState{vm}\n    vm.returnChangeState = &amp;ReturnChangeState{vm}\n    vm.currentState = vm.idleState\n    return vm\n}\nfunc (vm *VendingMachine) SelectProduct(product *Product) {\n    vm.currentState.SelectProduct(product)\n}\n\nfunc (vm *VendingMachine) InsertCoin(coin Coin) {\n    vm.currentState.InsertCoin(coin)\n}\n\nfunc (vm *VendingMachine) InsertNote(note Note) {\n    vm.currentState.InsertNote(note)\n}\n\nfunc (vm *VendingMachine) DispenseProduct() {\n    vm.currentState.DispenseProduct()\n}\n\nfunc (vm *VendingMachine) ReturnChange() {\n    vm.currentState.ReturnChange()\n}\n</code></pre></div><p>We can also create few utility methods to set state, resetPayment.</p><p>Now to the main part - \nThe magic happens in concrete state implementations. Each state (IdleState, ReadyState, etc.) implements the VendingMachineState methods. They decide when to call SetState to move to another state. For example -</p><div><pre><code>// IdleState struct\ntype IdleState struct {\n    vendingMachine *VendingMachine\n}\n\nfunc (s *IdleState) SelectProduct(product *Product) {\n    if s.vendingMachine.inventory.IsAvailable(product) {\n        s.vendingMachine.selectedProduct = product\n        s.vendingMachine.SetState(s.vendingMachine.readyState)\n        fmt.Println(\"Product selected:\", product.name)\n    } else {\n        fmt.Println(\"Product not available:\", product.name)\n    }\n}\n\nfunc (s *IdleState) InsertCoin(coin Coin) { fmt.Println(\"Please select a product first.\") }\nfunc (s *IdleState) InsertNote(note Note) { fmt.Println(\"Please select a product first.\") }\nfunc (s *IdleState) DispenseProduct()     { fmt.Println(\"Please select a product and make payment.\") }\nfunc (s *IdleState) ReturnChange()        { fmt.Println(\"No change to return.\") }\n</code></pre></div><p>Similarly other states can be implemented. You can add a demo file to demonstrate the entire functionality by adding products to the inventory, selecting products, inserting coins and notes, dispensing products, and returning change.</p><p>Hope this helps! Always open to suggestions for improvement.</p>","contentLength":5200,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I Created a gw Command to Make Git Worktree More User-Friendly","url":"https://dev.to/sotarok/i-created-a-gw-command-to-make-git-worktree-more-user-friendly-4jeb","date":1755616395,"author":"Sotaro KARASAWA","guid":233390,"unread":true,"content":"<ul><li>Git Worktree is convenient, isn’t it?</li><li>But I can’t remember the commands for some reason</li><li>Essentially, what I want to do is start and end worktrees with “let’s do it” and “done”</li><li>Plus there are quite a few tedious associated tasks (like running npm install)</li></ul><p>So I wanted something to support that feeling.</p><p>After I started building this, I learned that someone was creating a much cooler integrated management tool called ccmanager.</p><p>I thought “this would be fine,” but gw is more of a primitive tool that just makes git worktree work nicely.</p><p>Also, well, in times like these, I thought it might be okay to reinvent the wheel a lot, so I decided to create something that’s easy for me to use in my own way.</p><p>The functionality is quite simple - it just handles adding and removing git worktrees based on issue numbers like this:</p><p>This creates a worktree for issue #123.</p><p>When you do this, it creates a nice worktree directory named  like:</p><div><pre><code>sotarok\n├── gw\n├── gw-123\n...\n</code></pre></div><p>If you want to specify a base branch, just append it at the end:</p><div><pre><code># Working on 123 branched from 122/impl branch\ngw start 123 122/impl\n</code></pre></div><p>If you don’t include 123, it enters interactive mode where you can select.</p><p>Sometimes when reviewing, you want to bring someone else’s branch locally without polluting your local repository to check various things, right?</p><p>You can directly specify the branch name like this, and it will create a worktree for that purpose.</p><p>There’s no end to these kinds of additions, but I end up wanting to create them anyway, so I keep adding more and more.</p><h3>\n  \n  \n  Automatically running setup scripts\n</h3><p>I wanted npm install or pnpm install to run automatically when doing , so I made it do that.</p><p>It supports npm, yarn, pnpm, cargo, go, pip, and bundler.</p><p>I want  or  to be automatically copied into the worktree.</p><p>You can either add  when doing start/checkout, or if not specified, it will ask you.</p><p>With these two behaviors working, you can just do  and immediately get to work.</p><p>Automatically moves to that directory when you do .</p><p>Shell integration is required. The behavior can be controlled via config.</p><p>It’s good to add something like this to your . It supports bash/zsh/fish. (Probably. I’ve only tested it with zsh)</p><div><pre><code>gw shell-integration zsh</code></pre></div><p>When working on multiple things in parallel, you end up with lots of iTerm2 tabs and forget what you’re doing where, so it updates the iTerm2 tab name to <code>repositoryName {issueNumber}</code>. The behavior can be controlled via config.</p><h3>\n  \n  \n  Auto-delete head branch when ending worktree\n</h3><p>I was bothered by local branches remaining when doing worktree remove. Now it auto-deletes them.</p><p>The behavior can be controlled via config.</p><h2>\n  \n  \n  How the gw command itself was made\n</h2><ul><li>But I didn’t write a single line myself… hehe… (I made it, or rather had it made… I guess)</li><li>I’ve been adding features during work breaks and whenever I think of something</li></ul><h2>\n  \n  \n  Actual Workflow (Use Case)\n</h2><ul><li>Find an issue I want to work on</li><li>Navigate to the repository</li><li>Split the screen with tmux and work with claude etc.</li><li>After finishing work and getting to merge, do </li></ul><p>What I personally like is that  is very easy to type.</p><p>Please try it out if you’d like.</p>","contentLength":3155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Projek Jawi Converter: Belajar Golang Sambil Mendigitalkan Warisan Bahasa","url":"https://dev.to/hardyweb/projek-jawi-converter-belajar-golang-sambil-mendigitalkan-warisan-bahasa-1p1a","date":1755613025,"author":"hardyweb","guid":233367,"unread":true,"content":"<p>Aku percaya bahawa, cara terbaik untuk belajar bahasa pengaturcaraan baru adalah dengan membangunkan projek kecil yang praktikal. Untuk projek kali ini,  membangunkan tools Rumi ke Jawi Converter di jawi.hardyweb.net dengan bahasa pengaturcaran Golang.</p><p>Selain Jawi, aku juga pernah buat projek seperti QR Code generator, mod_sec_audit parser, dan parser Waktu Solat dari e-solat.gov.my. Semua projek ni kecil tapi memberi pengalaman langsung dalam bermain dengan fungsi string manipulation, API integration, dan text processing.</p><p><strong>Idea Projek &amp; Perbualan Dengan AI</strong></p><p>Pada mulanya, aku tanya GPT:\n“Boleh tak buat Jawi converter guna Golang?”</p><ol><li><p>Mapping Huruf – setiap huruf Rumi dipetakan kepada huruf Jawi/Hijaiyah.</p></li><li><p>Special Words / Daftar Kata – perkataan yang ejaannya unik atau tidak tepat jika mapping, akan disimpan dalam pangkalan data untuk digunakan kembali. </p></li></ol><p>Mapping ni terus menukar huruf satu-per-satu.</p><p>Special words digunakan untuk perkataan yang mapping huruf biasa tidak menghasilkan bunyi atau ejaan tepat. Contohnya:\nspecialWords := map[string]string{\n  \"akhlak\": \"اخلاق\", \n}</p><p>Projek ni ikut empat langkah utama: ( ini versi awal ) </p><ol><li>Clean &amp; Tokenize\n    ◦ Buang whitespace / trim\n    ◦ Pecahkan ayat kepada perkataan\n    ◦ Kecilkan semua huruf ( change to lower case )</li></ol><ol><li>Cek Special Words\n    ◦ Kalau perkataan ada dalam special_words, terus guna versi Jawi tersebut</li><li>Mapping Huruf\n    ◦ Kalau perkataan tiada dalam special_words, tukar huruf menggunakan mapping</li><li>Gabungkan Kembali\n    ◦ Combine token Jawi untuk membentuk ayat\n</li></ol><div><pre><code></code></pre></div><p>Mapping huruf sahaja tidak mencukupi, contohnya:\n    • Perkataan “saya”\n        ◦ Bunyi sebenar →  ساي <p>\n        ◦ Maka perlu dimasukkan ke special_words.</p>\n    • Imbuhan depan &amp; belakang (me-, di-, ter-, -kan, -i) memerlukan logic tambahan.<p>\n    • Perkataan seperti “makan”, “dimakan”, “memakan”, “termakan”, “makanan”, “pemakan” perlu di-handle untuk ejaan yang tepat.</p></p><p><strong>Menyesuaikan Dengan Peraturan Jawi Lanjutan</strong></p><p>Apabila aku membaca nota dari Jawi Makmur dan karya Ahmad Ali Karim tentang hukum hamzah, terdapat beberapa peraturan tambahan yang perlu diambil kira selain flow asas yang dicadangkan oleh AI:\n    1. Penempatan Hamzah – hamzah boleh berada di atas alif (أ), bawah alif (إ), atau di atas waw/ya (ؤ/ئ) bergantung pada vokal dan kedudukan huruf.<p>\n    2. Huruf Ikut Bunyi – sesetengah perkataan memerlukan penyesuaian huruf berdasarkan sebutan, bukan sekadar mapping huruf literal.</p>\n    3. Gabungan Huruf &amp; Alif Mati – dalam perkataan tertentu, alif mati atau huruf panjang perlu ditambah/ditinggalkan untuk memastikan ejaan betul.<p>\n    4. Gelinciran – bunyi akhir tertentu mempengaruhi cara huruf dipetakan, terutama untuk kata nama khas dan kata ganda.</p></p><p>Dengan memasukkan peraturan ini ke dalam projek, output Jawi menjadi lebih tepat dan mendekati sebutan sebenar, berbanding hanya mengikut mapping AI standard + special words.\n    1. Hukum Derlung<p>\n        ◦ Singkirkan huruf ‘a’ pada akhir perkataan jika bukan ‘da, la, ra, wa, nga’.</p>\n    2. padanan Huruf h dan k<p>\n    3. Gelinciran / Kata Ganda / Akronim / Tiga Suku Kata</p>\n    4. Ejaan Lazim &amp; Kata Asal Arab</p><div><pre><code></code></pre></div><p>Contoh Kesalahan &amp; Betul:\n    • Salah: باچا / ممباچا / ترباچا / ديباچا / باچاکان / باچاکانلاه / باچاان<p>\n    • Betul: باچ / ممباچ / ترباچ / دباچ / باچکن / باچکنله / باچاءن.</p></p><p>Berikut adalah pipeline yang digunakan dalam jawi conveter semasa artikel ini ditulis, ia berevolusi dari 4 langkah awal sebagaimana dinyatakan di awal artikel,berkembang kerana melibatkan hukum dan peraturan-peraturan tatabahasa.</p><div><pre><code> normalize → tokenize → standardize input.\n specialWords → override perkataan tertentu.\n diftong, prefix, suffix, digraph, gelinciran, hukum derlung → 4. morpho/phonological rules.\n applyVowelAndEVariant → baru proses vowel-specific rules (awalan vokal + pepet/taling).\n applyReduplikasi → last step sebelum mapping, sebab dia modify bentuk token.\nmapToJawi → transliterasi.\n joinTokens → final output string.\n\n</code></pre></div><p>*<em>pipeline 5 dan 6 tu , masih ada bug, kadang-kadang dia baca , tapi masih tak boleh tukar e pepet dan e taling dengan betul, kata ganda tu pun tak di tukar ke angka dua arab (٢) dengan betul *</em></p><div><pre><code>1. AI + Programming = Learning Accelerator\n    ◦ GPT bantu cadangkan struktur, tapi ketepatan Jawi masih bergantung pada rule + daftar perkataan.\n2. Rujukan Akademik &amp; Linguistik\n    ◦ DBP &amp; Jawi Makmur jadi rujukan untuk ejaan tepat.\n    ◦ Belajar asas Jawi melalui sumber online:\n        ▪ ejawimakmur.my\n        ▪ Ahmad Ali Karim – Jenis Hamzah\n        ▪ Ahmad Ali Karim – Tulisan Jawi Bicara\n3. String Manipulation + Loops = Core Logic\n    ◦ Semua proses boleh dilakukan menggunakan operasi string standard dalam programming.\n4. Edge Cases &amp; Imbuhan\n    ◦ Kaedah mapping huruf sahaja tidak cukup; perlu hukum &amp; pengecualian untuk bunyi sebenar.\n</code></pre></div><p><strong>Kenapa Buat Jawi Converter Sendiri Walaupun Ada Jawi Makmur &amp; e-Jawi?</strong>\nWalaupun platform rasmi wujud, projek ini berfungsi sebagai eksperimen akademik untuk menguji teori linguistik, menguji rule, imbuhan, kata ganda, dan hukum Jawi, sambil menyokong kajian computational linguistics dan digitasi warisan bahasa secara interaktif. </p><p>_Digitasi adalah proses mengubah sesuatu yang berbentuk bukan digital kepada digital. Contohnya, apabila menukarkan dokumen kertas dan menyimpannya dalam data komputer sebagai dokumen digital. _</p><p>Projek Jawi Converter bukan sekadar latihan pengaturcaran Golang. Ia adalah penghubung antara teknologi dan warisan budaya. Ini hanyalah satu usaha kecil aku untuk Malaysia – kendatipun begitu, projek Jawi Converter ini adalah sumbangan peribadi untuk melestarikan bahasa, budaya, dan warisan Melayu, sambil membina kemahiran teknologi moden. </p>","contentLength":5812,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Come to do a quick quiz about go language","url":"https://dev.to/_80e46b100ef6704321037/come-to-do-a-quick-quiz-about-go-language-53b8","date":1755610544,"author":"彦亨陈","guid":233344,"unread":true,"content":"<p>Recently I discovered a very easy to use free AI tool. I used it to generate a Go language quiz that can interact and calculate scores. Come and test it! Everyone is also welcome to create by yourself, generate and share interesting pages.</p>","contentLength":239,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Clean Architecture in Golang: Building Scalable APIs","url":"https://dev.to/djamware_tutorial_eba1a61/clean-architecture-in-golang-building-scalable-apis-5g62","date":1755604171,"author":"Djamware Tutorial","guid":233293,"unread":true,"content":"<p>🛠️ New guide is up on Djamware!</p><p>In this tutorial, you’ll learn:</p><ul><li>The fundamentals of Clean Architecture in Go</li><li>How to separate concerns into layers</li><li>Writing unit tests to keep your code maintainable</li></ul>","contentLength":198,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go Coding with Asparagos: Sunflowers and the Speaking Challenge","url":"https://dev.to/asparagos/go-coding-with-asparagos-sunflowers-and-the-speaking-challenge-2gd6","date":1755591054,"author":"Asparagos","guid":233227,"unread":true,"content":"<blockquote><p>Sunflowers on a mission: better English in linear time?</p></blockquote><p>Hi! I'm  — an asparagus who codes in Go. Here you’ll find everyday problems that a typical veggie might struggle with — and my Go solutions to them. Today we are solving the problem of <strong>Sunflower Speaking Club 🌻</strong>.</p><p>Sunflowers are planning to expand their influence across the world. Olives are gaining popularity, they can’t fall behind. To achieve this, they need to learn English.</p><p>Each sunflower already speaks it to some extent, but wants to find a partner to practice with. The sunflowers are planted in a row. Each has their own level of English and wants to find the nearest sunflower to the right who speaks better than they do.</p><p>Why to the right? The sun is rising in the east, so it’s the perfect moment to combine business with pleasure.</p><p>A slice of integers — each integer represents the English level of a sunflower in the row.</p><p>A slice of integers — each integer represents the distance to the nearest sunflower to the right with a higher English level. If there’s no such sunflower, return 0.</p><ul></ul><ol><li><p>We use a stack to keep track of potential candidates for being the nearest better-speaking partner to the right.</p></li><li><p>We iterate through the  slice from right to left. For each sunflower:</p><p>a. We remove all sunflowers from the stack that have an English level less than or equal to the current one. These sunflowers can’t be good partners anymore, because the current sunflower is better and will be a better candidate for any future comparisons.</p><p>b. If the stack is not empty, then the sunflower on top of the stack is the nearest one to the right with a higher level. So we store the distance between them.</p><p>c. We then push the current sunflower onto the stack, as it might be a suitable partner for some sunflower to its left.</p></li><li><p>Each sunflower is pushed to the stack only once and removed at most once, so the overall time complexity is .</p></li></ol><div><pre><code></code></pre></div><p>Feel free to check out the full code with tests on <a href=\"https://github.com/vik-kurb/asparagos\" rel=\"noopener noreferrer\">GitHub</a>, and don’t hesitate to leave a ⭐ if you find it helpful!</p>","contentLength":2014,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Channels vs Mutexes In Go - the Big Showdown","url":"https://dev.to/gkoos/channels-vs-mutexes-in-go-the-big-showdown-338n","date":1755560015,"author":"gkoos","guid":231835,"unread":true,"content":"<p>Concurrency is Go's crown jewel - goroutines and channels make concurrent programming feel almost magical. But not every problem belongs in a channel. Many developers fall into the trap of overusing channels or misusing mutexes, resulting in slow, buggy, or unmaintainable code. In this article, we'll demystify <strong>when to use channels and when to use mutexes</strong>, and why blindly following \"Go concurrency patterns\" can backfire.</p><p>Go's philosophy of \"do not communicate by sharing memory; share memory by communicating\" is often taken literally. Some gophers try to replace every mutex with a channel, thinking channels are the \"Go way\" to synchronize everything.</p><p>But here's the hard truth: <strong>channels are not a free replacement for mutexes</strong>. They're great for coordinating goroutines, pipelines, and events - but not always the right tool for protecting shared state.</p><p>On the surface, goroutines look more elegant, sure - and they are, in the right context. But trying to funnel all state access through channels, even for a simple counter or map, often leads to:</p><ul><li>: A simple counter increment can become dozens of lines of boilerplate channel code.</li><li>: Channels involve scheduling, allocation, and copying, so you're paying extra overhead where a mutex would suffice.</li><li>: Improperly managed channels can deadlock or leak goroutines, sometimes in ways that are much harder to debug than a simple mutex.</li></ul><p>Example: Consider a simple counter that multiple goroutines increment. Using a channel for this can lead to complex and error-prone code, while a mutex would be straightforward and efficient:</p><div><pre><code></code></pre></div><p>Ugh. This works, but it's overkill. A mutex does the same thing with less code and less overhead:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  Channels: For Communication, Not Just Safety\n</h2><p>Channels shine when goroutines need to communicate or signal events. They can be used to implement fan-out/fan-in patterns, worker pools, or pipelines:</p><div><pre><code></code></pre></div><ul><li>Excellent for orchestrating goroutines.</li><li>Can simplify complex coordination patterns.</li></ul><ul><li>Higher overhead than a mutex for simple state protection.</li><li>Overcomplicates code if used for every shared variable.</li></ul><h2>\n  \n  \n  Mutexes: The Right Tool for Shared State\n</h2><p>First of all, what is a mutex? A mutex (short for mutual exclusion) is a synchronization primitive that ensures only one goroutine (or thread) can access a piece of shared data at a time. It acts like a lock around critical sections, preventing race conditions when multiple goroutines attempt to read or write the same state concurrently.</p><p>A  is designed to guard access to a shared resource. If you just need safe access to a map, counter, or struct, a mutex is often simpler and faster.</p><p>Imagine you're maintaining a cache that multiple goroutines need to read and update. A  is the simplest and most efficient way to guard that shared map:</p><div><pre><code></code></pre></div><ul><li>Explicit locking makes reasoning about shared state straightforward.</li></ul><ul><li>Can be less elegant in complex pipelines or fan-out/fan-in patterns.</li></ul><div><table><tbody><tr><td>Protect a counter, map, or struct</td></tr><tr><td>Implement a worker pool, pipeline, or event queue</td></tr><tr><td>Single producer → single consumer</td></tr><tr><td>Multiple goroutines updating the same state</td></tr></tbody></table></div><p>Rule of thumb: <strong>Use mutexes for shared state, channels for communication</strong>.</p><p>Benchmarks often surprise Go devs. Simple state mutations protected by mutexes are usually orders of magnitude faster than channel-based approaches because channels involve allocation, scheduling, and copying:</p><ul><li>Mutexes are extremely lightweight. They’re implemented in Go’s runtime using efficient atomic operations. Locking and unlocking often cost only a few nanoseconds.</li><li>Channels, on the other hand, involve more moving parts. Sending or receiving on a channel may trigger:\n\n<ul><li>Memory allocation for the buffered/unbuffered queue.</li><li>Scheduling of waiting goroutines.</li><li>Potential context switching if the receiver isn't ready.</li></ul></li></ul><p>That extra bookkeeping makes channels slower when all you need is to guard a shared variable.</p><h3>\n  \n  \n  Benchmark: Mutex vs Channel Counter\n</h3><p>Let's put this to the test with Go's benchmarking framework:</p><div><pre><code></code></pre></div><p>And here’s an example of what the results might look like on a typical laptop (Go 1.23, 8-core CPU):</p><div><pre><code>BenchmarkMutexCounter-8      1000000000   0.8 ns/op\nBenchmarkChannelCounter-8     20000000    60 ns/op\n</code></pre></div><p>Now obviously real-world workloads might slightly differ from synthetic benchmarks (e.g., context switches, OS scheduling etc.) but that's a <strong>~75× performance difference in favor of the mutex</strong>!</p><p>So why the huge gap? The mutex path is just an atomic operation to acquire/release the lock. The channel path involves synchronization between two goroutines, queue management, and possibly waking up a sleeping goroutine.</p><p>This demonstrates why mutexes are the right tool for protecting simple shared state.</p><h3>\n  \n  \n  1. Web Server Request Counting\n</h3><p>Imagine you're running an HTTP server and want to count requests:</p><ul><li>Mutex version: Fast, scalable, and works fine under load.</li><li>Channel version: Every request handler has to ship a message through a channel, creating a bottleneck and slowing down throughput.</li></ul><p>In production, that's the difference between comfortably handling 100k requests/sec and falling behind at 10k requests/sec.</p><p>If multiple goroutines read and write a cache (like map[string]User), a mutex is perfect. Reads and writes happen inline with minimal cost.</p><p>With a channel-based \"cache manager goroutine\", every single read/write becomes a request–response round trip. Instead of O(1) map lookups, you now have O(1) + channel send/receive + scheduling. This introduces latency and makes your cache slower than just hitting the database in some cases.</p><h3>\n  \n  \n  3. Worker Pool for Task Processing\n</h3><p>With a mutex you could have a slice of tasks, protect it with a sync.Mutex, and have multiple goroutines pull work out of it. Each goroutine locks, pops a task, unlocks, processes, and repeats.</p><p>But with channels, you can just push tasks into a job channel, spin up N workers, and let them consume concurrently:</p><div><pre><code></code></pre></div><p>Here, channels are a natural fit because the problem is work distribution, not just shared memory safety.</p><p>Using a mutex would require writing your own coordination logic, which is more error-prone and less readable.</p><h3>\n  \n  \n  4. Event Notifications / Pub-Sub\n</h3><p>With a mutex, you could maintain a slice of subscribers guarded by a mutex. Every time an event happens, you'd lock, loop over subscribers, and call their handler functions. This works, but it mixes synchronization, iteration, and business logic.</p><p>Why goroutines + channels are better: channels let you decouple event production from consumption. Each subscriber can listen on its own channel and handle events at its own pace:</p><div><pre><code></code></pre></div><p>Now you can spin up independent goroutines for each subscriber:</p><div><pre><code></code></pre></div><p>With goroutines + channels, events flow asynchronously, subscribers don't block each other, and backpressure (buffered/unbuffered channels) is easy to model.</p><p>Doing the same with a mutex-based subscriber list quickly becomes messy, especially if one subscriber is slow or blocks.</p><h2>\n  \n  \n  Other Concurrency Primitives in Go\n</h2><p>While mutexes and channels are the most common tools, Go's standard library includes a few other primitives worth knowing:</p><ul><li><p>: A variation of  that allows multiple readers to hold the lock simultaneously, but only one writer at a time. Useful for read-heavy workloads like caches.</p></li><li><p>: A condition variable that lets goroutines wait until a certain condition is met. More advanced than channels, but sometimes useful for implementing custom coordination patterns.</p></li><li><p>: Ensures a piece of code runs only once, even if called from multiple goroutines. Commonly used for lazy initialization.</p></li><li><p>: Waits for a collection of goroutines to finish. Perfect for spawning workers and waiting for them to complete before moving on.</p></li><li><p>: Provides low-level atomic operations (like atomic.AddInt64) for lock-free access to basic types. Often the fastest solution for counters and flags.</p></li></ul><p>These tools complement mutexes and channels. For example, you might use a  to wait for a batch of goroutines to finish processing before sending a final result on a channel.</p><p>Or the counter example with  for lock-free incrementing:</p><div><pre><code></code></pre></div><p>This is often the fastest option for simple counters and flags because it avoids lock contention altogether.</p><p>If we extend our benchmark from above:</p><div><pre><code></code></pre></div><p>The results would be something like this:</p><div><pre><code>BenchmarkAtomicCounter-8    1000000000   0.3 ns/op\nBenchmarkMutexCounter-8     1000000000   0.8 ns/op\nBenchmarkChannelCounter-8     20000000   60 ns/op\n</code></pre></div><p>Notice how atomic operations are ~2–3× faster than mutexes, while channels are orders of magnitude slower for this use case. It's a shame atomic operations are extremely limited: they only work on individual variables and basic types.</p><p>Mutexes are perfect for protecting state. Channels shine when you need to coordinate or distribute work/events.</p><p>But many Go developers try to force channels into every concurrency problem because they feel more \"idiomatic.\" In reality, channels are not inherently better than mutexes. They're tools for communication, not a silver bullet. It's also important to note that <strong>channels and mutexes are not mutually exclusive</strong> - sometimes you'll combine them (e.g., worker pool with channel + shared stats protected by mutex). Think of channels as \"communication highways\" and mutexes as \"traffic lights\" for shared memory - each has its place.</p><p>Overusing channels is a common beginner trap and leads to code that is harder to read, slower to run, and more error-prone — the exact opposite of Go's philosophy of simplicity. Just don't overthink it: <strong>mutexes for state, channels for communication</strong>.</p>","contentLength":9458,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"🚀 Introducing OTel Sandbox: Your Zero-Config OpenTelemetry Playground","url":"https://dev.to/akshitzatakia/introducing-otel-sandbox-your-zero-config-opentelemetry-playground-53d7","date":1755527733,"author":"Akshit Zatakia","guid":231632,"unread":true,"content":"<p><strong>Stop wrestling with complex OpenTelemetry setups. Start experimenting in seconds.</strong></p><p>If you've ever tried to set up OpenTelemetry for the first time, you know the pain. Multiple components, complex configurations, version compatibility nightmares, and hours spent just to see your first trace. What if I told you there's a better way?</p><p> – a developer-first tool that gets you from zero to observability in under 30 seconds.</p><h2>\n  \n  \n  🎯 The Problem Every Developer Faces\n</h2><p>Picture this: You want to experiment with OpenTelemetry, but first you need to:</p><ul><li>Install and configure an OTel Collector</li><li>Set up Jaeger for trace visualization</li><li>Configure Prometheus for metrics</li><li>Write YAML configs for each component</li><li>Debug networking issues between services</li><li>Spend your weekend reading documentation instead of coding</li></ul><p><strong>OTel Sandbox eliminates all of this friction.</strong></p><h2>\n  \n  \n  ✨ What Makes OTel Sandbox Special?\n</h2><p><strong>🏃‍♂️ Lightning Fast Setup</strong></p><div><pre><code>\n./otel-sandbox up\n</code></pre></div><p>In 30 seconds, you have a complete observability stack running locally:\n✅ OpenTelemetry Collector (configured &amp; running)<a href=\"http://localhost:16686\" rel=\"noopener noreferrer\">http://localhost:16686</a>)\n✅ Prometheus (<a href=\"http://localhost:9090\" rel=\"noopener noreferrer\">http://localhost:9090</a>)\n✅ All networking configured automatically</p><p>Wonder if everything is working? Don't guess:</p><p>This sends real telemetry data through your stack and confirms:\n✅ Traces are being collected<p>\n✅ Metrics are being recorded</p>\n✅ Logs are being captured<p>\n✅ All exporters are functioning</p></p><p>View your collected telemetry data in multiple formats:</p><div><pre><code>\n./otel-sandbox  summary\n\n\n./otel-sandbox  json\n</code></pre></div><h2>\n  \n  \n  🎛️ Effortless Process Management\n</h2><div><pre><code>\n./otel-sandbox status\n\n\n./otel-sandbox down\n</code></pre></div><p><strong>🧪 Experiment with New SDKs</strong>\nTesting a new language SDK? Spin up OTel Sandbox, point your app at , and immediately see traces flowing through Jaeger.</p><p>\nPerfect for workshops, tutorials, or just understanding how the pieces fit together. No complex setup means more time learning concepts.</p><p><strong>🐛 Debug Integration Issues</strong>\nHaving problems with your production OTel setup? Use OTel Sandbox as a known-good reference environment to isolate issues.</p><p>\nNeed to create a demo for your team? OTel Sandbox gives you a reproducible environment that works everywhere.</p><p><strong>🏗️ Architecture That Just Works</strong>\nOTel Sandbox bundles battle-tested components:</p><div><pre><code>Your App → OTel Collector → Jaeger traces\n                        → Prometheus metrics  \n                        → File Export logs</code></pre></div><p>Everything is pre-configured with sensible defaults, but you can still customize configurations if needed.</p><p>\nUnpack the tar archive and navigate into the extracted folder.</p><div><pre><code>./otel-sandbox-&lt;os&gt;-&lt; up\n</code></pre></div><p>Note: If this doesn't work due to restrictions, you can build the binary by clonning this repository and use  command</p><p>\nAlways run ./otel-sandbox status before starting development to see what's running.</p><p>\nUse ./otel-sandbox down between experiments to ensure clean state.</p><p>\nExport data with ./otel-sandbox export --format json and pipe it to jq for analysis:</p><div><pre><code>./otel-sandbox  json | jq </code></pre></div><p>We're constantly improving OTel Sandbox:</p><ul><li>🔍 Hints Feature (Coming Soon): Intelligent suggestions for optimizing your telemetry setup</li><li>📦 More Export Formats: Prometheus, CSV, and custom formats</li><li>🎨 Enhanced UI: Better visualization of your telemetry pipeline</li><li>🔧 Advanced Configuration: Easy customization for power users</li></ul><p>OTel Sandbox is open source and built by developers, for developers. We'd love your feedback:</p><p>⭐ Star us on GitHub\n🐛 Report issues or request features<p>\n🤝 Contribute improvements</p></p><h2>\n  \n  \n  🏁 Stop Fighting Setup, Start Building\n</h2><p>The future of observability is here, and it shouldn't require a PhD in YAML configuration.</p><p>OTel Sandbox gets you from curious about OpenTelemetry to shipping instrumented code in minutes, not hours.</p><p>Ready to transform your observability workflow?</p><p>Built with ❤️ for the developer community. Because your time should be spent building amazing things, not fighting configuration files.</p>","contentLength":3869,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"dependency injection go","url":"https://dev.to/febriyan1302/dependency-injection-go-4ljk","date":1755511636,"author":"Fajar Febriano","guid":231538,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dockerizing Go API and Caddy","url":"https://dev.to/danielcristho/dockerizing-go-api-and-caddy-ge4","date":1755483890,"author":"Daniel Pepuho","guid":230770,"unread":true,"content":"<p>Hello! In this post I'll walk through how to use Caddy as a reverse proxy and Docker for containerization to deploy a simple Go API. This method offers a quick and modern to getting your Go API up and running.</p><p>Before we dive into the deployment steps, let's briefly discuss why Docker and Caddy are an excellent combination.</p><ul><li> is a containerization platform that packages your app and all its dependecies into an isolated unit. This guarantess that your app runs consistenly everywhere, eliminating the classic  problem.</li></ul><ul><li> is the blueprint that defines how your Docker image is built. It specifies the base image, the steps to compile your application, and how the container should run.</li><li> is a tool for defining and running multi-container Docker applications. Instead of starting each container manually, we can describe the entire stack in a single YAML file.</li><li>  is a modern reverse proxy and web server built using Go. Caddy is renowned for its ease of use, especially its  feature. Its simple configuration makes it an ideal choice for serving our API.</li></ul><p>Before we start, you'll need to install Go, Docker and Docker Compose on your system.</p><p>Make sure Go is installed. You can check your version with:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  2. Docker &amp; Docker Compose\n</h3><p>Make sure Docker &amp; Docker Compose are installed. You can check your docker version with:</p><div><pre><code>docker \n\nDocker version 27.5.1, build 9f9e405\n</code></pre></div><div><pre><code>docker compose version\n\nDocker Compose version v2.3.3\n</code></pre></div><p>Follow the official guides to install Docker and Docker Compose for your operating system if you haven't installed them before. <a href=\"https://docs.docker.com/engine/install\" rel=\"noopener noreferrer\">The official site</a>.</p><h3>\n  \n  \n  Step 1: Create and Run a Simple Go API\n</h3><ul><li>First, create a new directory for the project and initialize a Go module:\n</li></ul><div><pre><code></code></pre></div><blockquote><p>This command creates a Go module (go.mod) named example.com/go-api, which helps manage dependencies and makes the project reproducible.</p></blockquote><ul><li>Next, create a new file  and define a simple HTTP server using Chi as the router.</li></ul><p>The server exposes two routes:</p><ul><li>Root path  -&gt; return an ASCII banner generated with the  package.\n</li></ul><div><pre><code></code></pre></div><ul><li> -&gt; returns a simple JSON response.\n</li></ul><div><pre><code></code></pre></div><ul><li>In the main function, we:</li></ul><blockquote><ol><li>Initialize the Chi router and register both routes.</li><li>Configure an HTTP server to listen on port 8081.</li><li>Run the server in a goroutine and listen for shutdown signals (SIGINT, SIGTERM).</li><li>Gracefully shut down the server with a 5-second timeout when a termination signal is received.\n</li></ol></blockquote><div><pre><code></code></pre></div><div><pre><code></code></pre></div><p>Then, try run the main.go using :</p><p>After that, you should see this message in the terminal:</p><p>Finally, you can test the API using  or access from the browser on :</p><div><pre><code>curl 127.0.0.1:8081\n\n   ____                _      ____    ___                ____               _       _\n  / ___|   ___        /    |  _  |_ _|              / ___|   __ _    __| |   __| |  _   _\n | |  _   / _      / _   | |_ |  | |     _____    | |      / _ |  / _ |  / _ | | | | |\n | |_| | | _ |    / ___  |  __/   | |    |_____|   | |___  | _| | | _| | | _| | | |_| |\n  ___|  __/    /_/   |_|     |___|              ___|  _,_|  _,_|  _,_|  _, |\n                                                                                        |___/\n</code></pre></div><div><pre><code>curl 127.0.0.1:8081/api/hello\n\n:</code></pre></div><p>We'll use a multi-stage build to create a minimal and secure Docker image. This process compiles our Go application in one stage and then copies only the final binary to a much smaller final image. This keeps our final image size low.</p><p>Create a  in your project directory and add the following code:</p><div><pre><code>go mod download\n\n0 go build  /go/bin/app\n</code></pre></div><ul><li>In the builder stage, we use the official Go image to compile our code.</li><li>We copy the entire project into the container and run go mod download to fetch dependencies.</li><li>Then we build the binary with CGO_ENABLED=0 to ensure it’s statically compiled and portable. The binary is placed in /go/bin/app.\n</li></ul><div><pre><code></code></pre></div><ul><li>In the final stage, only the compiled binary is copied over from the builder stage. This keeps the image small because source code, dependencies, and build tools are excluded.</li><li>We expose port 8081 so Docker knows which port the app listens on.</li><li> runs the binary as the container’s main process.</li></ul><div><pre><code>go mod download\n\n0 go build  /go/bin/app\n\n</code></pre></div><p>Caddy will act as a reverse proxy that forwards incoming requests to the Go API container. This allows us to:</p><ul><li>Tells Caddy to listen on port 80 (HTTP).</li><li>Forwards requests to the Go API container, using the Docker service name go-api and port 8081.</li></ul><p>Create a file named  in your project directory:</p><div><pre><code>:80 \n    reverse_proxy go-api:8081\n</code></pre></div><blockquote><p>💡 If you later have a domain (e.g., api.example.com), you can replace  with your domain</p></blockquote><div><pre><code>api.example.com \n    reverse_proxy go-api:8081\n</code></pre></div><h3>\n  \n  \n  Step 4: Configure Docker Compose\n</h3><p>Create a file named  in your project directory:</p><div><pre><code></code></pre></div><ul><li>Built from your local Dockerfile.</li><li>Uses expose instead of ports → this means the Go API is reachable inside the Docker network but not directly exposed to the host machine.</li><li>The Go API will listen on :8081, but only Caddy can access it.</li></ul><ul><li> -&gt; exposes port 80 from the container to port 8082 on your host machine.\nSo when you open , requests are routed through Caddy.</li><li>The Caddyfile is mounted so you can configure reverse proxy behavior.</li><li> ensures Caddy waits for the Go API container to start.</li></ul><p>Once all files are ready (, , , ), run the stack using the  command:</p><div><pre><code>\n├── Caddyfile\n├── docker-compose.yml\n├── Dockerfile\n├── go.mod\n├── go.sum\n└── main.go\n</code></pre></div><div><pre><code>docker compose up </code></pre></div><p>Make sure the containers are running using  command:</p><div><pre><code>docker ps\n\nCONTAINER ID   IMAGE                          COMMAND                  CREATED         STATUS         PORTS\n                                                      NAMES\n6b6b35487d77   caddy:2.10-alpine                 9 minutes ago   Up 9 minutes   443/tcp, 2019/tcp, 443/udp, 0.0.0.0:8082-&gt;80/tcp, ::]:8082-&gt;80/tcp   caddy\n20cbb2209fe7   docker-go-api-caddy_go-api                        9 minutes ago   Up 9 minutes   0.0.0.0:32770-&gt;8081/tcp, ::]:32770-&gt;8081/tcp\n</code></pre></div><p>Now, test the endpoints through Caddy (reverse proxy):</p><div><pre><code>curl http://127.0.0.1:8082\n</code></pre></div><p>Expected output: the ASCII art rendered by .</p><div><pre><code>curl http://127.0.0.1:8082/api/hello\n</code></pre></div><div><pre><code>:</code></pre></div>","contentLength":5964,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mastering Go Concurrency: Taming Race Conditions Like a Pro","url":"https://dev.to/jones_charles_ad50858dbc0/mastering-go-concurrency-taming-race-conditions-like-a-pro-1kn2","date":1755477904,"author":"Jones Charles","guid":230736,"unread":true,"content":"<p>Concurrency in Go is like conducting a symphony—goroutines are your musicians, and channels are the baton keeping them in rhythm. Go’s lightweight  and elegant  make building scalable apps a breeze, but race conditions can turn your masterpiece into chaos. Whether you’re crafting an e-commerce backend or a real-time analytics pipeline, mastering memory synchronization is key to rock-solid code.</p><p>This guide is for Go developers with 1-2 years of experience looking to conquer concurrency. With a decade of Go projects under my belt, I’ll share battle-tested tips, code snippets, and pitfalls to help you write race-free, high-performance code. We’ll cover , , , Go’s  tool, and a task queue example, plus advanced patterns to level up your skills.</p><p>Let’s dive in and make your Go concurrency code sing!</p><h2>\n  \n  \n  1. Go Concurrency : The Essentials\n</h2><h3>\n  \n  \n  1.1 Goroutines and Channels\n</h3><p>Go’s concurrency shines with —lightweight threads managed by the Go runtime—and , which sync and pass data between them. Goroutines are cheap (a few KB each), so you can launch thousands without worry. Channels let goroutines communicate safely, following Go’s mantra: “Share memory by communicating, not by sharing memory.”</p><p>Think of goroutines as chefs sharing a kitchen and channels as a choreographed handoff of ingredients. Without coordination, you get a mess—aka race conditions.</p><h3>\n  \n  \n  1.2 Race Conditions and Memory Synchronization\n</h3><p> ensures goroutines access shared memory predictably. Go’s memory model defines  rules, like a channel send completing before its receive. Without sync, you risk , where goroutines clash over shared memory, and at least one writes. For example, two goroutines incrementing  can overwrite each other, losing updates.</p><div><table><thead><tr></tr></thead><tbody><tr><td>Predictable memory access across goroutines</td></tr><tr><td>Unsynchronized access with a write</td><td>Causes bugs, crashes, or data loss</td></tr><tr><td>Go’s operation order guarantee</td></tr></tbody></table></div><h3>\n  \n  \n  1.3 Your Concurrency Toolbox\n</h3><ul><li> and : Locks for shared data.</li><li>: Waits for goroutines to finish.</li><li>: Lock-free ops for counters or flags.</li><li>: Sync and communicate elegantly.</li></ul><p>Let’s explore how to wield these tools.</p><h3>\n  \n  \n  Segment 2: Refined Synchronization Techniques\n</h3><h2>\n  \n  \n  2. Sync Like a Pro: Mutexes, Atomics, and Channels\n</h2><p>Memory synchronization keeps goroutines in check. Here’s how to use , , and , with examples and real-world lessons.</p><h3>\n  \n  \n  2.1 Mutex and RWMutex: Guarding the Gates\n</h3><p> locks ensure one goroutine accesses shared data at a time.  optimizes for read-heavy workloads, allowing multiple readers but exclusive writers. Here’s a safe counter:</p><div><pre><code></code></pre></div><p> shines for caches with frequent reads. : A global Mutex in a payment API caused 500ms latency spikes. Sharding data with per-partition Mutexes cut latency to 60ms. : Keep locks granular to avoid bottlenecks.</p><h3>\n  \n  \n  2.2 Atomic Operations: Speed Without Locks\n</h3><p> offers lock-free operations for simple tasks like counters, using CPU instructions like Compare-And-Swap (CAS). Here’s the counter, atomic-style:</p><div><pre><code></code></pre></div><p>: In an e-commerce app, atomics for inventory deductions boosted performance by 40% during peak traffic. : You need fast, simple updates like counters or flags.</p><h3>\n  \n  \n  2.3 Channels: Sync with Elegance\n</h3><p> blend communication and synchronization.  enforce strict sync;  add flexibility. Here’s a producer-consumer setup:</p><div><pre><code></code></pre></div><p>: In a log pipeline, an unbuffered channel bottlenecked producers. A buffered channel (capacity 100) doubled throughput, but I added a timeout to handle overflows:</p><div><pre><code></code></pre></div><p>: Use buffered channels for decoupling, but watch buffer size.</p><h3>\n  \n  \n  Segment 3: Race Conditions and Advanced Patterns\n</h3><h2>\n  \n  \n  3. Outsmarting Race Conditions\n</h2><p>Race conditions are sneaky bugs that strike under load. Let’s learn to detect and prevent them.</p><h3>\n  \n  \n  3.1 What Causes Race Conditions?\n</h3><p>A race condition occurs when goroutines access shared memory concurrently, with at least one writing, and no sync. Here’s a buggy counter:</p><div><pre><code></code></pre></div><p>: Data corruption, crashes, or debugging headaches.</p><p>Run  to catch races. For the above, it flags:</p><div><pre><code>WARNING: DATA RACE\nRead at 0x00c0000a4010 by goroutine 8:\n  main.(*Counter).Inc()\n      main.go:12 +0x44\n...\nWrite at 0x00c0000a4010 by goroutine 7:\n  main.(*Counter).Inc()\n      main.go:12 +0x55\n</code></pre></div><p>: Add  to your CI/CD pipeline. It saved my team from a production crash caused by a shared map.</p><h3>\n  \n  \n  3.3 Prevention Strategies\n</h3><ul><li>: Use local copies or immutable data.</li><li>: Use Mutex for complex logic, keep scope tight.</li><li>: Safer for coordination and data passing.</li></ul><p>: A global Mutex in an API caused contention. Sharded locks fixed it. : Test with  and profile with .</p><h2>\n  \n  \n  4. Advanced Concurrency Patterns\n</h2><p>Let’s level up with two advanced patterns for real-world Go apps.</p><h3>\n  \n  \n  4.1 Worker Pool with Context\n</h3><p>A worker pool distributes tasks across goroutines, with  for cancellation. Here’s a snippet:</p><div><pre><code></code></pre></div><p>: Context enables graceful shutdown, critical for production apps.</p><p>Fan-out splits tasks across workers; fan-in collects results. Here’s a simplified example:</p><div><pre><code></code></pre></div><p>: Parallel data processing, like image resizing or API calls.</p><h3>\n  \n  \n  Segment 4: Refined Task Queue and Wrap-Up\n</h3><h2>\n  \n  \n  5. A Battle-Tested Task Queue\n</h2><p>Here’s an improved task queue combining channels, Mutexes, atomics, and context:</p><div><pre><code></code></pre></div><ul><li>Added  for task submission with cancellation.</li><li>Improved error handling for full queues or closed state.</li><li>Kept atomic counter and buffered channel for performance.</li></ul><p>: Race-free (verified with ), handles 100 tasks with ~100ms latency using 3 workers and a buffer of 10.</p><h2>\n  \n  \n  6. Wrap-Up: Be a Go Concurrency Ninja\n</h2><p>Go’s  and  make concurrency fun, but race conditions can sneak in. Use  for shared data,  for fast counters, and  for coordination. Catch races with  and optimize with . Try the task queue, experiment with worker pools, or dive into fan-out/fan-in for parallel tasks.</p><p> Share your concurrency experiments in the comments! Have you battled a nasty race condition? Prefer channels or locks? Join the Go community on Dev.to and let’s geek out over goroutines.</p><ul><li>:  by Katherine Cox-Buday</li></ul><ul><li>Channels or Mutexes—what’s your vibe?</li><li>Share your worst concurrency bug!</li><li>How do you scale Go apps under load?</li></ul>","contentLength":6087,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"[Boost]","url":"https://dev.to/triplemcoder/-26d6","date":1755477862,"author":"Muutassim Mukhtar","guid":230735,"unread":true,"content":"<h2>Lifting the Hood on Trace Propagation in OpenTelemetry</h2><h3>Muutassim Mukhtar ・ Aug 18</h3>","contentLength":82,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lifting the Hood on Trace Propagation in OpenTelemetry","url":"https://dev.to/triplemcoder/lifting-the-hood-on-trace-propagation-in-opentelemetry-4dj3","date":1755477617,"author":"Muutassim Mukhtar","guid":230734,"unread":true,"content":"<p>OpenTelemetry is really changing the game for observability, especially by offering the first widely adopted, vendor neutral telemetry libraries. Tracing was the first signal the project tackled, and its now generally available in most major programming languages.</p><p>Personally, Im a big fan of tracing. I think its a much cleaner and more effective way to pass data to your observability provider. they can take that data and turn it into something truly useful, whether thats a Gantt style view of spans or metrics that drive alerts.</p><p>One of the most powerful features is distributed tracing. the ability to connect spans across different services into a single trace. When one service makes an RPC call to another, the trace context can follow along, giving you a complete, end-to-end view of requests across your entire platform.</p><p>In the screenshot above for example,the trace has spans from 3 different services: , , and .</p><h2>\n  \n  \n  What is trace propagation?\n</h2><p>To connect traces across different services, you need to pass some context along with each request. This process is known as trace propagation, and its what we will be diving into in this article.</p><p>Unless you are working with a legacy system that already uses a different tracing format, you should stick with the <a href=\"https://www.w3.org/TR/trace-context/\" rel=\"noopener noreferrer\">W3C Trace Context Recommendation</a>. Its the recommended standard for propagating trace context over HTTP. While its designed with HTTP in mind, many of its concepts can also be applied to other communication channels, like Kafka messages, for example.</p><p>Trace Context specifies two HTTP headers that will be used to pass context around, traceparent and tracestate</p><p>The traceparent HTTP header includes the root of context propagation. It consists in a comma-separated suite of fields that include:</p><ul><li><p>The version of Trace Context being used. Only one version, 00 exists in 2023\nThen, for version 00:</p></li><li><p>The current Trace ID, as a 16-byte array representing the ID of the entire trace.</p></li><li><p>The current Span ID (called parent-id in the spec), an 8-byte array representing the ID of the parent request.</p></li><li><p>Flags, an 8-byte hex-encoded field which controls tracing flags such as sampling.</p></li></ul><p>The tracestate HTTP header is meant to include proprietary data used to pass specific information across traces.</p><p>Its value is a comma-separated list of key/values, where each pair is separated by an equal sign. Obviously, the trace state shouldn’t include any sensitive data.</p><p>For example, with requests coming from public API endpoints which can be called either by internal services, or by external customers, both could be passing a traceparent header. However, external ones would generate orphan spans, as the parent one is stored within the customers service, not ours.</p><p>So we add a tracestate value indicating the request comes from an internal service, and we only propagate context if that value is present</p><p>With both these fields being passed, any tracing library should have enough information to provide distributed tracing.</p><p>A request could pass the following headers:<code>traceparent: 00-d4cda95b652f4a1592b449d5929fda1b-6e0c63257de34c92-01\ntracestate: myservice=true</code></p><p>The traceparent header indicates a trace ID <code>(d4cda95b652f4a1592b449d5929fda1b)</code>, a span ID , and sets a flag indicating the parent span was sampled (so its likely we want to sample this one too).</p><p>The tracestate header provides a specific key/value that we can use to make appropriate decisions, such as whether we want to keep the context or not.</p><h2>\n  \n  \n  How OpenTelemetry implements propagation\n</h2><p>The OpenTelemetry <a href=\"https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/context/api-propagators.md\" rel=\"noopener noreferrer\">specification </a>defines a Propagators interface to allow any implementation to establish its own propagation convention, such as W3C TraceContext.</p><p>A propagator must implement two methods:</p><ol><li><p> – to insert the current span context into a carrier object (such as an HTTP headers map).</p></li><li><p>– to retrieve the span context from a carrier object.</p></li></ol><p>Each instrumentation library making or receiving external calls then has the responsibility to call inject/extract to write/read the span context and have it passed around.</p><h2>\n  \n  \n  Extract and Inject examples\n</h2><p>For example, the following is <a href=\"https://github.com/open-telemetry/opentelemetry-ruby-contrib/blob/6e89c92f189bc6e187da06ea2af4e38531b93601/instrumentation/rack/lib/opentelemetry/instrumentation/rack/middlewares/tracer_middleware.rb#L69-L72\" rel=\"noopener noreferrer\">Rack extracting</a> the context from the propagator to generate a new span in Ruby:</p><h2>\n  \n  \n  The full propagation flow\n</h2><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fcca2z6txp0nxvzvpf7qz.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fcca2z6txp0nxvzvpf7qz.png\" alt=\" \" width=\"800\" height=\"290\"></a>\nTo put in other words, the diagram above shows what each service is expected to perform to enable propagation. The library emitting an HTTP call is expected to call inject, which will add the proper HTTP headers to the request. The library receiving HTTP requests is expected to call extract to retrieve the proper span context from the request’s HTTP headers.</p><p>Note that each language implementation of OpenTelemetry provides multiple contrib packages that allow easy instrumentation of common frameworks and libraries. Those packages will handle propagation for you. Unless you write your own framework or HTTP library, you should not need to call inject or extract yourself. All you need to do is configure the global propagation mechanism (see below).</p><p>Not all services communicate through HTTP. For example, you could have one service emitting a Kafka message, and another one reading it.</p><p>The OpenTelemetry propagation API is purposefully generic, as all it does is read a hash and return a span context, or read a span context and inject data into a hash. So you could replace a hash of HTTP headers with anything you want.</p><p>Any language or library that uses the same convention can benefit from distributed tracing within kafka messages, or any other communication mechanism.</p><p>As you may have seen in the above specification link, the default propagator will be a no-op:</p><p><em>The OpenTelemetry API MUST use no-op propagators unless explicitly configured otherwise</em></p><p>You should therefore always ensure your propagator of choice is properly set globally, and each library that needs to call inject or extract will then be able to retrieve it.</p><p>Thanks for following along with this deep dive into context propagation in Otel. Hopefully, you now have a clearer understanding of how distributed tracing works within the library. You should be equipped to implement context propagation in any library instrumentation that makes or receives calls from external services.</p><p>With distributed tracing properly set up across your platform, you will be able to see the full journey of every request, making it much easier to identify bottlenecks, trace issues, and debug problems effectively</p>","contentLength":6359,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lifting the Hood on Trace Propagation in OpenTelemetry","url":"https://dev.to/muutassim_624a31d1045c0c0/lifting-the-hood-on-trace-propagation-in-opentelemetry-2doh","date":1755473926,"author":"Muutassim","guid":230727,"unread":true,"content":"<p>OpenTelemetry is really changing the game for observability, especially by offering the first widely adopted, vendor neutral telemetry libraries. Tracing was the first signal the project tackled, and its now generally available in most major programming languages.</p><p>Personally, Im a big fan of tracing. I think its a much cleaner and more effective way to pass data to your observability provider. they can take that data and turn it into something truly useful, whether thats a Gantt style view of spans or metrics that drive alerts.</p><p>One of the most powerful features is distributed tracing. the ability to connect spans across different services into a single trace. When one service makes an RPC call to another, the trace context can follow along, giving you a complete, end-to-end view of requests across your entire platform.</p><p>In the screenshot above for example,the trace has spans from 3 different services: , , and .</p><h2>\n  \n  \n  What is trace propagation?\n</h2><p>To connect traces across different services, you need to pass some context along with each request. This process is known as trace propagation, and its what we will be diving into in this article.</p><p>Unless you are working with a legacy system that already uses a different tracing format, you should stick with the <a href=\"https://www.w3.org/TR/trace-context/\" rel=\"noopener noreferrer\">W3C Trace Context Recommendation</a>. Its the recommended standard for propagating trace context over HTTP. While its designed with HTTP in mind, many of its concepts can also be applied to other communication channels, like Kafka messages, for example.</p><p>Trace Context specifies two HTTP headers that will be used to pass context around, traceparent and tracestate.</p><p>The traceparent HTTP header includes the root of context propagation. It consists in a comma-separated suite of fields that include:</p><ul><li><p>The version of Trace Context being used. Only one version, 00 exists in 2023\nThen, for version 00:</p></li><li><p>The current Trace ID, as a 16-byte array representing the ID of the entire trace.</p></li><li><p>The current Span ID (called parent-id in the spec), an 8-byte array representing the ID of the parent request.</p></li><li><p>Flags, an 8-byte hex-encoded field which controls tracing flags such as sampling.</p></li></ul><p>The tracestate HTTP header is meant to include proprietary data used to pass specific information across traces.</p><p>Its value is a comma-separated list of key/values, where each pair is separated by an equal sign. Obviously, the trace state shouldn’t include any sensitive data.</p><p>For example, with requests coming from public API endpoints which can be called either by internal services, or by external customers, both could be passing a traceparent header. However, external ones would generate orphan spans, as the parent one is stored within the customers service, not ours.</p><p>So we add a tracestate value indicating the request comes from an internal service, and we only propagate context if that value is present.</p><p>With both these fields being passed, any tracing library should have enough information to provide distributed tracing.</p><p>A request could pass the following headers:</p><p><code>traceparent: 00-d4cda95b652f4a1592b449d5929fda1b-6e0c63257de34c92-01\ntracestate: myservice=true</code></p><p>The traceparent header indicates a trace ID (d4cda95b652f4a1592b449d5929fda1b), a span ID (6e0c63257de34c92), and sets a flag indicating the parent span was sampled (so its likely we want to sample this one too).</p><p>The tracestate header provides a specific key/value that we can use to make appropriate decisions, such as whether we want to keep the context or not.</p><h2>\n  \n  \n  How OpenTelemetry implements propagation\n</h2><p>The OpenTelemetry <a href=\"https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/context/api-propagators.md\" rel=\"noopener noreferrer\">specification </a>defines a Propagators interface to allow any implementation to establish its own propagation convention, such as W3C TraceContext.</p><p>A propagator must implement two methods:</p><ol><li> – to insert the current span context into a carrier object (such as an HTTP headers map).</li><li>– to retrieve the span context from a carrier object.</li></ol><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9g3m1hvyefnx8zr1txki.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F9g3m1hvyefnx8zr1txki.png\" alt=\" \" width=\"800\" height=\"199\"></a>\nEach instrumentation library making or receiving external calls then has the responsibility to call inject/extract to write/read the span context and have it passed around.</p><h2>\n  \n  \n  Extract and Inject examples\n</h2><p>For example, the following is <a href=\"https://github.com/open-telemetry/opentelemetry-ruby-contrib/blob/6e89c92f189bc6e187da06ea2af4e38531b93601/instrumentation/rack/lib/opentelemetry/instrumentation/rack/middlewares/tracer_middleware.rb#L69-L72\" rel=\"noopener noreferrer\">Rack extracting</a> the context from the propagator to generate a new span in Ruby:</p><h2>\n  \n  \n  The full propagation flow\n</h2><p>To put in other words, the diagram above shows what each service is expected to perform to enable propagation. The library emitting an HTTP call is expected to call inject, which will add the proper HTTP headers to the request. The library receiving HTTP requests is expected to call extract to retrieve the proper span context from the request’s HTTP headers.</p><p>Note that each language implementation of OpenTelemetry provides multiple contrib packages that allow easy instrumentation of common frameworks and libraries. Those packages will handle propagation for you. Unless you write your own framework or HTTP library, you should not need to call inject or extract yourself. All you need to do is configure the global propagation mechanism (see below).</p><p>Not all services communicate through HTTP. For example, you could have one service emitting a Kafka message, and another one reading it.</p><p>The OpenTelemetry propagation API is purposefully generic, as all it does is read a hash and return a span context, or read a span context and inject data into a hash. So you could replace a hash of HTTP headers with anything you want.</p><p><a href=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq390sdvoqbmrbjilhtb1.png\"><img src=\"https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fq390sdvoqbmrbjilhtb1.png\" alt=\" \" width=\"800\" height=\"68\"></a>\nAny language or library that uses the same convention can benefit from distributed tracing within kafka messages, or any other communication mechanism.</p><p>As you may have seen in the above specification link, the default propagator will be a no-op:</p><p><em>The OpenTelemetry API MUST use no-op propagators unless explicitly configured otherwise</em></p><p>You should therefore always ensure your propagator of choice is properly set globally, and each library that needs to call inject or extract will then be able to retrieve it.</p><p>Thanks for following along with this deep dive into context propagation in Otel. Hopefully, you now have a clearer understanding of how distributed tracing works within the library. You should be equipped to implement context propagation in any library instrumentation that makes or receives calls from external services.</p><p>With distributed tracing properly set up across your platform, you will be able to see the full journey of every request, making it much easier to identify bottlenecks, trace issues, and debug problems effectively</p>","contentLength":6380,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"**Golang Memory Optimization: Reduce GC Pauses by 73% in High-Load Applications**","url":"https://dev.to/aaravjoshi/golang-memory-optimization-reduce-gc-pauses-by-73-in-high-load-applications-3ako","date":1755374267,"author":"Aarav Joshi","guid":230402,"unread":true,"content":"<blockquote><p>As a best-selling author, I invite you to explore my books on <a href=\"https://www.amazon.com/stores/Aarav-Joshi/author/B0DQYNVXZ7?ref=ap_rdr&amp;isDramIntegrated=true&amp;shoppingPortalEnabled=true&amp;ccs_id=738636bd-0ca1-4d7b-8efa-481bfc222571\" rel=\"noopener noreferrer\">Amazon</a>. Don't forget to follow me on <a href=\"https://medium.com/@aarav-joshi\" rel=\"noopener noreferrer\">Medium</a> and show your support. Thank you! Your support means the world! </p></blockquote><h3>\n  \n  \n  Optimizing Memory Management for High-Load Golang Applications\n</h3><p>Efficient memory handling separates adequate systems from exceptional ones in high-throughput environments. I've seen Go applications buckle under pressure when processing millions of requests daily, often due to overlooked memory inefficiencies. The garbage collector becomes a bottleneck, stealing precious milliseconds from response times. Through trial and error across several high-load systems, I've developed strategies that significantly reduce GC pressure while maintaining Go's idiomatic simplicity.  </p><p>Consider a common scenario: an API gateway handling 50,000 requests per second. Traditional approaches create new objects for each request, flooding the heap and triggering frequent GC pauses. My solution combines four key techniques—object pooling, stack allocation, custom arenas, and memory layout tuning—to keep allocations primarily on the stack and reuse heap objects strategically.  </p><p><strong>Object Pooling with sync.Pool</strong><p>\nReusing objects is fundamental. I implement pools for frequently allocated types like HTTP requests and buffers. This snippet shows a thread-safe pool for request objects:</p></p><div><pre><code></code></pre></div><p>In production, this simple pattern reduced request object allocations by 87% in my last project. The key is resetting slices with  instead of reallocating, preserving underlying array capacity.  </p><p><p>\nGo's escape analysis sometimes sends variables to the heap unexpectedly. I use compiler directives and careful structuring to prevent this:</p></p><div><pre><code></code></pre></div><p>Run  to analyze escapes. I once shaved 200μs off latency by converting a small  from pointer to value receiver.  </p><p><p>\nFor short-lived buffers, I implement arena allocation using channels:</p></p><div><pre><code></code></pre></div><p>This pattern reduced JSON marshaling allocations by 76% in a message queue I optimized last quarter. The channel acts as a fixed-size reservoir for byte slices.  </p><p><p>\nProper field alignment reduces wasted space. Consider this optimized struct:</p></p><div><pre><code></code></pre></div><p>Without padding, Go would insert 7 bytes between  and . For slice-heavy workflows, I preallocate:</p><div><pre><code></code></pre></div><p>Resetting with  preserves capacity across iterations.  </p><p><p>\nImplementing these techniques in a payment processing system yielded:  </p></p><ul><li>73% fewer heap allocations\n</li><li>GC pauses under 0.5ms during 45K RPS loads\n</li><li>3.2x throughput increase on same hardware\n</li><li>58% reduction in memory usage\n</li></ul><p>The graph below shows GC pause times before and after optimization:<a href=\"https://dev.to/aaravjoshi/golang-memory-optimization-reduce-gc-pauses-by-73-in-high-load-applications-3ako\"></a></p><div><pre><code>go mem.out\ngo tool pprof  mem.out\n</code></pre></div><p>Focus on allocation-heavy paths first. When implementing pools:  </p><ol><li>Size pools to 110-120% of peak concurrent requests\n</li><li>Add metrics to track pool hits/misses\n</li><li>Implement fallback to standard allocation during bursts\n</li></ol><ul><li>Replace pointer receivers with values for small structs\n</li><li>Avoid interfaces in hot paths\n</li><li>Localize variables in tight loops\n</li></ul><p><strong>Production Considerations</strong><p>\nMonitoring is crucial. I expose pool metrics like:</p></p><div><pre><code></code></pre></div><div><pre><code>50  4GiB </code></pre></div><p>For specialized cases, consider cgo allocators:</p><div><pre><code></code></pre></div><ul><li>Trading systems where 100μs latency matters\n</li><li>Real-time analytics processing TBs/hour\n</li><li>API gateways serving 100K+ RPS\n</li></ul><p>In a recent cybersecurity project, these optimizations handled 2.3 million log entries/second per node. The key was combining sync.Pool for parsed objects with arena-allocated byte buffers for raw data.  </p><p><p>\nMemory optimization in Go isn't about fighting the language—it's about cooperating with the runtime. Start with clean code, profile relentlessly, then apply surgical optimizations. The techniques shown here reduced GC overhead to under 1% of CPU in my most demanding deployments. Remember: premature optimization is counterproductive, but strategic memory management at scale separates functional systems from exceptional ones.</p></p><div><pre><code></code></pre></div><h2>\n  \n  \n  The path to low-latency Go systems lies in respecting allocations—not eliminating them entirely, but controlling when and how they occur. With these patterns, I've consistently achieved sub-millisecond response times under heavy load while keeping code maintainable.\n</h2><p>📘 , , , and  to the channel!</p><p> is an AI-driven publishing company co-founded by author . By leveraging advanced AI technology, we keep our publishing costs incredibly low—some books are priced as low as —making quality knowledge accessible to everyone.</p><p>Stay tuned for updates and exciting news. When shopping for books, search for  to find more of our titles. Use the provided link to enjoy !</p><p>Be sure to check out our creations:</p>","contentLength":4494,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Practice Makes Perfect: How AI Interview Simulation Changed My Go Game","url":"https://dev.to/rezasi/practice-makes-perfect-how-ai-interview-simulation-changed-my-go-game-40d2","date":1755370386,"author":"RezaSi","guid":230381,"unread":true,"content":"<p>Remember that gut-wrenching feeling before a technical interview? The sweaty palms, the racing thoughts, wondering if you'll blank out when asked to implement a binary tree? Yeah, I've been there too. That's exactly why I built the AI Interview Simulation feature for Go Interview Practice – and honestly, it's been a game-changer for so many developers (myself included!).</p><h2>\n  \n  \n  The Problem: Interview Anxiety is Real\n</h2><p>Let's be honest – coding interviews are nerve-wracking. You might be a brilliant developer who ships features daily, but put you in front of a whiteboard (or shared screen) with a stranger watching your every keystroke, and suddenly your brain turns to mush.</p><p>I realized this when I was helping a friend prepare for their dream job at a Go-heavy startup. They knew the language inside and out, had built amazing projects, but kept freezing up during mock interviews. The problem wasn't their technical skills – it was the pressure and unpredictability of the interview environment.</p><h2>\n  \n  \n  Enter AI: Your Non-Judgmental Interview Partner\n</h2><p>That's when I had a lightbulb moment: What if we could simulate real interview conditions, but with an AI that never gets impatient, never judges you, and can adapt to your pace?</p><p>The AI Interview Simulation feature does exactly that. Here's what makes it special:</p><p>The AI watches your code as you write it and gives instant feedback – just like a real interviewer would. It might say something like:</p><blockquote><p><em>\"I notice you're using a brute force approach. Can you think of a way to optimize this using Go's built-in data structures?\"</em></p></blockquote><p>It's not trying to trip you up; it's genuinely helping you think through the problem like a supportive interviewer would.</p><h3><strong>Dynamic Follow-Up Questions</strong> 🎯\n</h3><p>This is where it gets really cool. The AI doesn't just stick to a script. If you mention goroutines, it might ask about concurrency patterns. If you use a map, it could dive into time complexity. It adapts to YOUR code and YOUR approach.</p><p>I remember one user told me: <em>\"It felt like talking to a senior developer who actually wanted me to succeed.\"</em> That's exactly the vibe we were going for!</p><h3><strong>Timed Sessions with Gentle Pressure</strong> ⏰\n</h3><p>Real interviews have time constraints, so our AI does too. But here's the thing – it's not trying to stress you out. If you're struggling, it might say:</p><blockquote><p><em>\"Take your time. Let's think about this step by step. What's the first thing we need to figure out?\"</em></p></blockquote><p>It maintains interview realism while being genuinely supportive.</p><h2>\n  \n  \n  Three AI Flavors for Different Styles\n</h2><p>We integrated with three different AI providers because, let's face it, different people click with different communication styles:</p><p><em>Recommended for Go analysis</em></p><p>Gemini has this knack for understanding Go idioms and best practices. It'll catch things like unnecessary pointer usage or suggest more idiomatic ways to handle errors. Plus, it's free to get started, which is awesome for students and junior devs.</p><p><em>For detailed explanations</em></p><p>GPT-4 is fantastic at breaking down complex problems into digestible chunks. If you're stuck on a graph algorithm, it'll walk you through the approach step by step, almost like having a patient mentor sitting next to you.</p><p><em>Thoughtful and nuanced feedback</em></p><p>Claude tends to give more nuanced feedback about code design and architecture. It's great for more senior developers who want to discuss trade-offs and design decisions.</p><h2>\n  \n  \n  The Magic is in the Iteration\n</h2><p>Here's what I love most about this feature: you can fail safely. Make a mistake? The AI points it out gently and helps you learn from it. Completely blank out? No problem – run another session tomorrow.</p><p>One user told me they did the same algorithm challenge five times with the AI, each time getting better feedback and understanding the problem more deeply. Try doing that with a human interviewer! 😅</p><p>The AI doesn't just focus on getting the right answer. It evaluates:</p><ul><li> your thought process</li><li><strong>Whether you ask clarifying questions</strong> (super important!)</li><li><strong>How you handle hints and feedback</strong></li><li><strong>Your approach to testing and edge cases</strong></li></ul><p>These soft skills are huge in real interviews, but they're often overlooked in traditional practice platforms.</p><p>If you're preparing for Go interviews (or just want to level up your skills), I'd love for you to try the AI Interview Simulation. It's completely free, runs in your browser, and you can practice as much as you want.</p><p>The goal isn't to make interviews easy – it's to make you confident. When you walk into that real interview room, you'll have already experienced the pressure, practiced thinking out loud, and gotten comfortable with being challenged on your code.</p><p>Plus, even if you're not interviewing right now, it's a fantastic way to get feedback on your Go skills from an AI that's available 24/7 and never gets tired of your questions.</p><p>Building this feature taught me something important: the best interview preparation isn't about memorizing algorithms (though that helps). It's about getting comfortable with the process, learning to communicate your thinking, and building the confidence to tackle problems you haven't seen before.</p><p>The AI Interview Simulation gives you a safe space to build all of that. And honestly? It's kind of fun once you get into it. There's something satisfying about having a thoughtful conversation about code with an AI that's genuinely trying to help you improve.</p><p>Give it a shot, and let me know how it goes! I'm always looking for feedback and ways to make the experience even better.</p><p><strong>What's your biggest interview fear? Drop a comment below – I love hearing about different perspectives and experiences in the developer community!</strong></p>","contentLength":5621,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building a smart, agentic email assistant","url":"https://dev.to/dsense/building-a-smart-agentic-email-assistant-4m70","date":1755349440,"author":"Adesina Hassan.","guid":230306,"unread":true,"content":"<p>I have been exploring LLMs (Large Language Models) and agentic-driven applications for over a year now. The exploration mostly focuses on building smart tooling to improve and automate repetitive tasks. This journey has forced me to delve deep into AI and strive to understand even more advanced and complex AI concepts. This has been going incredibly well, and I've learned valuable lessons about the power of context-aware agents.</p><p>A few months ago, I started designing and developing a smarter email agent that can handle the following tasks perfectly:</p><ul><li>Responding to emails from colleagues in a natural, human-like manner</li><li>Accepting or rejecting meeting invites intelligently (yes, my inbox gets full really quickly due to having it tied to work, test, and dev environments, all of which send out lots of emails daily)</li><li>Summarizing what has been accomplished on a daily basis and emailing it to me as a daily digest</li></ul><p>The motivation behind this project was simple: I was spending too much time on routine email management when I could be focusing on more strategic work. What started as a straightforward automation task quickly evolved into a fascinating exploration of how context and intelligence can transform simple agents into truly smart assistants.</p><h2>\n  \n  \n  The First Iteration: A Simple Approach\n</h2><p>This sounds like a pretty simple task with the availability of email APIs and webhook events. So I thought too, and I started building out the code. The initial design focused on:</p><ul><li>Registering and listening to webhook events for new emails received:</li><li>The agent needs to be able to read the content and determine if an email is a meeting invite or a communication email with a message. The agent needs to be able to take appropriate action:\n (1) If it's an invite: accept or reject the invite based on predefined rules.\n (2) If it's an email with a message: respond to the email in the most human-like way possible.</li><li>Record every action taken (accept/reject invites and responses to emails) and share a summary by the end of the day.</li></ul><p>Here's the core implementation of the first version:</p><div><pre><code></code></pre></div><h2>\n  \n  \n  The Reality Check: Limitations of the First Version\n</h2><p>So this works just fine. It serves the objective as expected—helping me automate those repetitive tasks. However, there were a few key observations after deploying this first version that made me realize the agent wasn't as \"smart\" as I had hoped:</p><h3>\n  \n  \n  Problem 1: Blind Invitation Acceptance\n</h3><p>The agent blindly accepts any invite that comes in. Right, that's not so smart. I need it to accept an invite only if it's within working hours and I'm available on the specified date. Otherwise, it should reject the invite and reply with a message citing my unavailability.</p><p> Without context about my schedule, the agent was essentially a \"yes-man\" that would overbook my calendar and accept conflicting meetings.</p><p> The agent needs access to my calendar system. This is where the concept of context becomes crucial. The agent needs contextual awareness of my availability to handle this task intelligently. This means integrating with calendar APIs (Google Calendar, Outlook, etc.) and implementing logic to:</p><ul><li>Check for existing conflicts</li><li>Respect working hours and time zones</li><li>Consider buffer time between meetings</li><li>Handle recurring meetings appropriately</li></ul><h3>\n  \n  \n  Problem 2: Generic Communication Style\n</h3><p>The agent exhibits the same tone in all its replies. While this works functionally, I wanted something more sophisticated. I wanted the replies to feel human and be driven by past exchanges. How you respond to a colleague is slightly different from how you would respond to the VP of Engineering, if you understand what I mean.</p><p> The agent lacked emotional intelligence and relationship context, making all interactions feel robotic and impersonal.</p><p> Enable the agent to access past conversations I've had (if any) with the email sender, analyze the tone and formality level used in those exchanges, and craft responses that match the established communication pattern. This requires:</p><ul><li>Tone detection and classification</li><li>Relationship mapping (colleague, manager, external partner, etc.)</li><li>Context-aware response generation</li></ul><p>If we examine the two problems outlined above, it becomes crystal clear that the advantages we get when we empower our agents with substantial context and tools far exceed what they can achieve with limited context. With this realization, I started working on version 2 of the tool, equipped with the solutions explained above.</p><p>The key insight here is that  when building intelligent agents. Without context, you have automation. With context, you have intelligence.</p><h2>\n  \n  \n  Version 2: The Context-Aware Smart Agent\n</h2><p>Let's take a look at an advanced version of what we had earlier. This iteration incorporates calendar integration and conversation history analysis:</p><div><pre><code></code></pre></div><h3>\n  \n  \n  Key Improvements in Version 2\n</h3><ol><li><strong>Provider-Based Architecture</strong>: Uses clean interfaces (CalendarProvider, EmailHistoryProvider, etc.) to separate concerns and enable easy testing with stub implementations</li><li>: Supports both OpenAI and Anthropic models through the langchaingo library, allowing easy switching between providers</li><li>: Gathers contextual information from multiple sources (calendar, email history, meeting notes, contacts) before generating responses</li><li>: Uses a well-formatted prompt template that clearly organizes context and instructions for consistent, high-quality responses</li><li><strong>Concise Response Generation</strong>: Built-in constraints (120-word limit, specific formatting) ensure responses are professional and actionable</li><li>: The interface-based approach makes it easy to add new context providers or replace stub implementations with real integrations</li></ol><p>Through building this smart email agent, I've learned several crucial lessons about developing intelligent systems:</p><h3>\n  \n  \n  1. Contextual Awareness Drives Intelligence\n</h3><p>What separates basic automation from genuine intelligence is the depth of contextual information available to the system. By incorporating calendar integration, historical communication patterns, and relationship dynamics, I transformed a rudimentary email processor into a sophisticated digital assistant.</p><h3>\n  \n  \n  2. Interface-Driven Design Enables Flexibility\n</h3><p>Using provider interfaces instead of concrete implementations made the system incredibly flexible. I could start with simple stub implementations for rapid prototyping, then gradually replace them with real integrations. This approach also made testing much easier since I could mock individual components.</p><h3>\n  \n  \n  3. Structured Prompting Yields Consistent Results\n</h3><p>The key to reliable AI responses wasn't just having good context—it was organizing that context in a clear, structured format. The prompt template with distinct sections (email content, recipient details, calendar info, instructions) produced much more consistent and useful responses than ad-hoc prompting.</p><h3>\n  \n  \n  4. LLM Provider Abstraction is Valuable\n</h3><p>Supporting multiple LLM providers (OpenAI, Anthropic) through a common interface proved invaluable. Different models have different strengths, and being able to switch providers based on cost, performance, or availability requirements without changing the core logic was a significant advantage.</p><p>LLMs are very powerful, and with well-crafted prompts, you will get the best out of them. However, empowering your agents with access to tools and context makes them super powerful, smart, and capable of handling complex tasks that go far beyond simple text generation.</p><p>The journey from a basic email responder to a context-aware intelligent agent illustrates a fundamental principle in AI development: <strong>intelligence emerges from the intersection of powerful models and rich context</strong>. As we continue to build more sophisticated AI systems, the focus should not just be on better models, but on better ways to provide those models with the context and tools they need to be truly helpful.</p><p>The future of AI agents lies not in replacing human intelligence, but in augmenting it with systems that understand context, maintain relationships, and can act intelligently on our behalf while preserving the nuance and personality that makes human communication meaningful.</p>","contentLength":8155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["go"]}